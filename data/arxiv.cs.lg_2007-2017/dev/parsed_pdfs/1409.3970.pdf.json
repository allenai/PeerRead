{
  "name" : "1409.3970.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data",
    "authors" : [ "Yin Zheng", "Yu-Jin Zhang" ],
    "emails" : [ "y-zheng09@mails.tsinghua.edu.cn", "zhang-yj@tsinghua.edu.cn", "hugo.larochelle@usherbrooke.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7]. One of the leading approaches is based on topic modelling, the most popular model being latent Dirichlet allocation or LDA [8]. LDA is a generative model for documents that originates from the natural language processing community, but has had great success in computer vision [8, 9]. LDA models a document as a multinomial distribution over topics, where a topic is itself a multinomial distribution over words. While the distribution over topics is specific for each document, the topic-dependent distributions over words are shared across all documents. Topic models can thus extract a meaningful, semantic representation from a document by inferring its latent distribution over topics from the words it contains. In the context of computer vision, LDA can be used by first extracting so-called “visual words” from images, convert the images into visual word documents and training an LDA topic model on the bags of visual words. To deal with multimodal data, some variants of LDA have been proposed recently [2, 5, 4, 9]. For instance, Correspondence LDA (Corr-LDA) [2] was proposed to discover the relationship between images and annotation modalities, by assuming each image topic must have a corresponding text topic. Multimodal LDA [5] generalizes Corr-LDA by learning a regression module relating the topics from the different\nar X\niv :1\n40 9.\n39 70\nv3 [\ncs .C\nV ]\nmodalities. Multimodal Document Random Field Model (MDRF) [4] was also proposed to deal with multimodal data, which learns cross-modality similarities from a document corpus containing multinomial data. Besides the annotation words, the class label modality can also be embedded into LDA, such as in supervised LDA (sLDA) [10, 9]. By modeling the image visual words, annotation words and their class labels, the discriminative power of the learned image representations could thus be improved.\nAt the heart of most topic models is a generative story in which the image’s latent representation is generated first and the visual words are subsequently produced from this representation. The appeal of this approach is that the task of extracting the representation from observations is easily framed as a probabilistic inference problem, for which many general purpose solutions exist. The disadvantage however is that as a model becomes more sophisticated, inference becomes less trivial and more computationally expensive. In LDA for instance, inference of the distribution over topics does not have a closed-form solution and must be approximated, either using variational approximate inference or MCMC sampling. Yet, the model is actually relatively simple, making certain simplifying independence assumptions such as the conditional independence of the visual words given the image’s latent distribution over topics.\nAnother approach to model the statistical structure of words is through the use of distributed representations modeled by artificial neurons. In the realm of document modeling, Salakhutdinov and Hinton [11] proposed a so-called Replicated Softmax (RS) model for bags of words. The RS model was later used for multimodal data modeling [12], where pairs of images and text annotations were modeled jointly within a deep Boltzmann machine (DBM) [13]. This deep learning approach to the generative modeling of multimodal data achieved state-of-the-art performance on the MIR Flickr data set [14]. On the other hand, it also shares with LDA and its different extensions the reliance on a stochastic latent representation of the data, requiring variational approximations and MCMC sampling at training and test time. Another neural network based state-of-the-art multimodal data modeling approach is Multimodal Deep Recurrent Neural Network (MDRNN) [15] which aims at predicting missing data modalities through the rest of data modalities by minimizing the variation of information rather than maximizing likelihood.\nRecently, an alternative generative modeling approach for documents was proposed in Larochelle and Lauly [16]. In this work, a Document Neural Autoregressive Distribution Estimator (DocNADE) is proposed, which models directly the joint distribution of the words in a document by decomposing it as a product of conditional distributions (through the probability chain rule) and modeling each conditional using a neural network. Hence, DocNADE doesn’t incorporate any latent random variables over which potentially expensive inference must be performed. Instead, a document representation can be computed efficiently in a simple feed-forward fashion, using the value of the neural\nnetwork’s hidden layer. Larochelle and Lauly [16] also show that DocNADE is a better generative model of text documents than LDA and the RS model, and can extract a useful representation for text information retrieval.\nIn this paper, we consider the application of DocNADE to deal with multimodal data in computer vision. More specifically, we first propose a supervised variant of DocNADE (SupDocNADE), which can be used to model the joint distribution over an image’s visual words, annotation words and class label. The model is illustrated in Figure 1. We investigate how to successfully incorporate spatial information about the visual words and highlight the importance of calibrating the generative and discriminative components of the training objective. Our results confirm that this approach can outperform other topic models, such as the supervised variant of LDA. Moreover, we propose a deep extension of SupDocNADE, that learns a deep and discriminative representation of pairs of images and annotation words. The deep version of SupDocNADE, which is illustrated in Figure 2, outperforms its shallow one and achieves state-of-the-art performance on the challenging MIR Flickr data set."
    }, {
      "heading" : "2 Related Work",
      "text" : "As previously mentioned, multimodal data is often modeled using extensions of the basic LDA topic model, such as Corr-LDA [2], Multimodal LDA [5] and MDRF [4]. In this paper, we focus on learning a joint representation from three different modalities: image visual words, annotations, and class labels. The class label describes the image globally with a single descriptive label (such as coast, outdoor, inside city, etc.), while the annotation focuses on tagging the local content within the image. Wang et al. [9] proposed a supervised LDA formulation to tackle this problem. Wang et al. [17] opted instead for a maximum margin formulation of LDA (MMLDA). Our work also belongs to this line of work, extending topic models to a supervised variant: our first contribution in this paper is thus\nto extend a different topic model, DocNADE, to this context for multimodal data modeling. What distinguishes DocNADE from other topic models is its reliance on an autoregressive neural network architecture. Recently, deep neural networks are increasingly used for the probabilistic modeling of images and text (see [18] for a review). The work of Srivastava and Salakhutdinov [12] on DBMs and Sohn et al. [15] on MDRNN are good recent examples. Ngiam et al. [19] also proposed deep autoencoder networks for multimodal learning, though this approach was recently shown to be outperformed by DBMs [13] and MDRNN [15]. Although DocNADE shows favorable performance over other topic models, the lack of an efficient deep formulation reduces its ability of modeling multimodal data, especially compared with the deep neural network based models [19, 12, 13]. Thus, the second contribution of this paper is to propose an efficient deep version of DocNADE and its supervised variant. As we’ll see, the deep version of our DocNADE model will outperform the DBM approach of Srivastava and Salakhutdinov [13]."
    }, {
      "heading" : "3 Document NADE",
      "text" : "In this section, we describe the original DocNADE model. In Larochelle and Lauly [16], DocNADE was used to model documents of real words, belonging to some predefined vocabulary. To model image data, we assume that images have first been converted into a bag of visual words. A standard approach is to learn a vocabulary of visual words by performing K-means clustering on SIFT descriptors densely exacted from all training images. See Section 6.1.2 for more details about this procedure. From that point on, any image can thus be represented as a bag of visual words v = [v1, v2, . . . , vDv ], where each vi is the index of the closest K-means cluster to the i\nth SIFT descriptor extracted from the image and Dv is the number of extracted descriptors for image v.\nDocNADE models the joint probability of the visual words p(v) by rewriting it as\np (v) = Dv∏ i=1 p (vi|v<i) (1)\nand modeling instead each conditional p(vi|v<i), where v<i is the subvector containing all vj such that j < i1. Notice that Equation 1 is true for any distribution, based on the probability chain rule. Hence, the main assumption made by DocNADE is in the form of the conditionals. Specifically, DocNADE assumes that each conditional can be modeled and learned by a feedforward neural network.\nOne possibility would be to model p(vi|v<i) with the following architecture:\nhi (v<i) = g\n( c +\n∑ k<i W:,vk\n) (2)\np (vi = w|v<i) = exp (bw + Vw,:hi (v<i))∑ w′ exp (bw′ + Vw′,:hi (v<i))\n(3)\nwhere g(·) is an element-wise non-linear activation function, W ∈ RH×Q and V ∈ RQ×H are the connection parameter matrices, c ∈ RN and b ∈ RQ are bias parameter vectors and H,Q are the number of hidden units (topics) and vocabulary size, respectively.\nComputing the distribution p(vi = w|v<i) of Equation 3 requires time linear in Q. In practice, this is too expensive, since it must be computed for each of the Dv visual words vi. To address this issue, Larochelle and Lauly [16] propose to use a balanced binary tree to decompose the computation of the conditionals and obtain a complexity logarithmic inQ. This is achieved by randomly assigning all visual words to a different leaf in a binary tree. Given this tree, the probability of a word is modeled as the probability of reaching its associated leaf from the root. Larochelle and Lauly [16] model each left/right transition probabilities in the binary tree using a set of binary logistic regressors taking the hidden layer hi(v<i) as input. The probability of a given word can then be obtained by multiplying the probabilities of each left/right choices of the associated tree path.\n1 We use a random ordering of the visual words in Equation 1 for each image, and we find it works well in practice. See the discussion in Section 4.1 for more details.\nSpecifically, let l (vi) be the sequence of tree nodes on the path from the root to the leaf of vi and let π (vi) be the sequence of binary left/right choices at the internal nodes along that path. For example, l (vi)1 will always be the root node of the binary tree, and π (vi)1 will be 0 if the word leaf vi is in the left subtree or 1 otherwise. Let V ∈ RT×H now be the matrix containing the logistic regression weights and b ∈ RT be a vector containing the biases, where T is the number of inner nodes in the binary tree and H is the number of hidden units. The probability p(vi = w|v<i) is now modeled as\np(vi = w|v<i) = |π(vi)|∏ k=1 p(π (vi)k |v<i) , (4)\nwhere p(π (vi)k = 1|v<i) = sigm ( bl(vi)m + Vl(vi)m,:hi (v<i) ) (5) are the internal node logistic regression outputs and sigm(x) = 1/(1 + exp(−x)) is the sigmoid function. By using a balanced tree, we are guaranteed that computing Equation 4 involves only O(log2Q) logistic regression outputs. One could attempt to optimize the organization of the words within the tree, but a random assignment of the words to leaves works well in practice [16].\nThus, by combining Equations 2, 4 and 5, we can compute the probability p (v) = ∏ i=1 p (vi|v<i) for any document under DocNADE. To train the parameters θ = {W,V,b, c} of DocNADE, we simply optimize the average negative log-likelihood of the training set documents using stochastic gradient descent.\nEquations 4,5 indicate that the conditional probability of each word vi requires computing the position dependent hidden layer hi (v<i), which extracts a representation out of the bag of previous visual words v<i. Since computing hi (v<i) is in O(HDv) on average, and there are Dv hidden layers hi (v<i) to compute, then a naive procedure for computing all hidden layers would be in O(HD2v).\nHowever, noticing that\nhi+1 (v<i+1) = g\n( c +\n∑ k<i+1 W:,vk\n) (6)\n= g ( W:,vi + c + ∑ k<i W:,vk ) (7)\nand exploiting that fact that the weight matrix W is the same across all conditionals, the linear transformation c +∑ k<iW:,vk can be reused from the computation of the previous hidden layer hi(v<i) to compute hi+1(v<i+1). With this procedure, computing all hidden layers hi(v<i) sequentially from i = 1 to i = Dv becomes in O(HDv). Finally, since the computation complexity of each of theO(log2Q) logistic regressions in Equation 4 isO(H), the total complexity of computing p(vi = w|v<i) is O(log2(Q)HDv). In practice, the length of document Dv and the number of hidden units H tends to be small, while log2(Q) will be small even for large vocabularies. Thus DocNADE can be used and trained efficiently.\nOnce the model is trained, a latent representation can be extracted from a new document v∗ as follows:\nhy (v ∗) = g ( c +\nDv∑ i W:,v∗i\n) . (8)\nThis representation could be fed to a standard classifier to perform any supervised computer vision task. The index y is used to highlight that it is the representation used to predict the class label y of the image."
    }, {
      "heading" : "4 SupDocNADE for Multimodal Data",
      "text" : "In this section, we describe the approach of this paper, inspired by DocNADE, to learn jointly from multimodal data. Here, we will concentrate on the single layer version of our model and discuss its deep extension later, in Section 5.\nFirst, we describe a supervised extension of DocNADE (SupDocNADE), which incorporates the class label modality into training to learn more discriminative hidden features for classification. Then we describe how we exploit the\nspatial position information of the visual words. Finally, we describe how to jointly model the text annotation modality with SupDocNADE."
    }, {
      "heading" : "4.1 Supervised DocNADE",
      "text" : "It has been observed that learning image feature representations using unsupervised topic models such as LDA can perform worse than training a classifier directly on the visual words themselves, using an appropriate kernel such as a pyramid kernel [20]. One reason is that the unsupervised topic features are trained to explain as much of the entire statistical structure of images as possible and might not model well the particular discriminative structure we are after in our computer vision task. This issue has been addressed in the literature by devising supervised variants of LDA, such as Supervised LDA or sLDA [10]. DocNADE also being an unsupervised topic model, we propose here a supervised variant of DocNADE, SupDocNADE, in an attempt to make the learned image representation more discriminative for the purpose of image classification.\nSpecifically, given an image v = [v1, v2, . . . , vDv ] and its class label y ∈ {1, . . . , C}, SupDocNADE models the full joint distribution as\np(v, y) = p(y|v) Dv∏ i=1 p (vi|v<i) . (9)\nAs in DocNADE, each conditional is modeled by a neural network. We use the same architecture for p (vi|v<i) as in regular DocNADE. We now only need to define the model for p(y|v).\nSince hy (v) is the image representation that we’ll use to perform classification, we propose to model p (y|v) as a multiclass logistic regression output computed from hy (v):\np (y|v) = softmax (d + Uhy (v))y (10)\nwhere softmax(a)i = exp(ai)/ ∑C j=1 exp(aj), d ∈ RC is the bias parameter vector in the supervised layer and U ∈ RC×H is the connection matrix between hidden layer hy and the class label. Put differently, p (y|v) is modeled as a regular multiclass neural network, taking as input the bag of visual words v. The crucial difference however with a regular neural network is that some of its parameters (namely the hidden unit parameters W and c) are also used to model the visual word conditionals p (vi|v<i).\nMaximum likelihood training of this model is performed by minimizing the negative log-likelihood\n− log p (v, y) = − log p (y|v) + Dv∑ i=1 − log p(vi|v<i) (11)\naveraged over all training images. This is known as generative learning [21]. The first term is a purely discriminative term, while the second is unsupervised and can be understood as a regularizer, that encourages a solution which also explains the unsupervised statistical structure within the visual words. In practice, this regularizer can bias the solution too strongly away from a more discriminative solution that generalizes well. Hence, similarly to previous work on hybrid generative/discriminative learning, we propose instead to weight the importance of the generative term\nL(v, y; θ) = − log p (y|v) + λ Dv∑ i=1 − log p(vi|v<i) (12)\nwhere λ is treated as a regularization hyper-parameter. Optimizing the training set average of Equation 12 is performed by stochastic gradient descent, using backpropagation to compute the parameter derivatives. As in regular DocNADE, computation of the training objective and its gradient requires that we define an ordering of the visual words. Though we could have defined an arbitrary path across the image to order the words (e.g. from left to right, top to bottom in the image), we follow Larochelle and Lauly [16] and randomly permute the words before every stochastic gradient update. The implication is that the model is effectively trained to be a good inference model of any conditional p(vi|v<i), for any ordering of the words in v.\nAlgorithm 1 Computing p (v, y) using SupDocNADE Input: bag of words representation v, target y Output: p (v, y) act← c p (v)← 1 for i from 1 to Dv do hi ← g (act) p (vi|v<i) = 1 for m from 1 to |π (vi) | do p (vi|v<i)← p (vi|v<i) p (π (vi)m |v<i)\nend for p (v)← p (v) p (vi|v<i) act← act + W:,vi\nend for hc (v)← max(0,act) p (y|v)← softmax (d + Uhc (v)))|y p (v, y)← p (v) p (y|v)\nThis again helps fighting against overfitting and better regularizes our model. One could thus think of SupDocNADE as learning from a sequence of random fixations performed in a visual scene.\nIn our experiments, we used the rectified linear function as the activation function\ng(a) = max(0,a) = [max(0, a1), . . . ,max(0, aH)] (13)\nwhich often outperforms other activation functions [22] and has been shown to work well for image data [23]. Since this is a piece-wise linear function, the (sub-)gradient with respect to its input, needed by backpropagation to compute the parameter gradients, is simply\n1(g(a)>0) = [1(g(a1)>0), . . . , 1(g(aH)>0)] (14)\nwhere 1P is 1 if P is true and 0 otherwise. Algorithms 1 and 2 give pseudocodes for efficiently computing the joint distribution p (v, y) and the parameter gradients of Equation 12 required for stochastic gradient descent training."
    }, {
      "heading" : "4.2 Dealing with Multiple Regions",
      "text" : "Spatial information plays an important role for understanding an image. For example, the sky will often appear on the top part of the image, while a car will most often appear at the bottom. A lot of previous work has exploited this intuition successfully. For example, in the seminal work on spatial pyramids [20], it is shown that extracting different visual word histograms over distinct regions instead of a single image-wide histogram can yield substantial gains in performance.\nWe follow a similar approach, whereby we model both the presence of the visual words and the identity of the region they appear in. Specifically, let’s assume the image is divided into several distinct regionsR = {R1, R2, . . . , RM}, where M is the number of regions. The image can now be represented as\nvR = [vR1 , v R 2 , . . . , v R D ] (15)\n= [(v1, r1) , (v2, r2) , . . . , (vDv , rDv)]\nwhere ri ∈ R is the region from which the visual word vi was extracted. To model the joint distribution over these visual words, we decompose it as p(vR) = ∏ i p((vi, ri)|vR<i) and treat each Q×M possible visual word/region pair as a distinct word. One implication of this is that the binary tree of visual words must be larger so as to have a leaf for each possible visual word/region pair. Fortunately, since computations grow logarithmically with the size of the tree, this is not a problem and we can still deal with a large number of regions.\nAlgorithm 2 Computing SupDocNADE training gradients Input: training vector v, target y,\nunsupervised learning weight λ Output: gradients of Equation 12 w.r.t. parameters f (v)← softmax (d + Uhc (v))) δd← (f (v)− 1y) δact← (Uᵀδd) ◦ 1hy>0 δU← δd hcᵀ δc← 0, δb← 0, δV← 0, δW← 0 for i from Dv to 1 do δhi ← 0 for m from 1 to |π (vi) | do δt← λ (p (π (vi)m |v<i)− π (vi)m) δbl(vi)m ← δbl(vi)m + δt δVl(vi)m,: ← δVl(vi)m,: + δt h ᵀ i\nδhi ← δhi + δtVᵀl(vi)m,: end for δact← δact + δhi ◦ 1hi>0 δc← δc + δhi ◦ 1hi>0 δW:,vi ← δW:,vi + δact\nend for"
    }, {
      "heading" : "4.3 Dealing with Annotations",
      "text" : "So far, we’ve described how to model the visual word and class label modalities. In this section, we now describe how we also model the annotation word modality with SupDocNADE.\nSpecifically, let A be the predefined vocabulary of all annotation words, we will note the annotation of a given image as a = [a1, a2, . . . , aL] where ai ∈ A, with L being the number of words in the annotation. Thus, the image with its annotation can be represented as a mixed bag of visual and annotation words:\nvA = [vA1 , . . . , v A Dv , v A Dv+1, . . . , , v A Dv+L] (16)\n= [vR1 , . . . , v R Dv , a1, . . . , aL] .\nTo embed the annotation words into the SupDocNADE framework, we treat each annotation word the same way we deal with visual words. Specifically, we use a joint indexing of all visual and annotation words and use a larger binary word tree so as to augment it with leaves for the annotation words. By training SupDocNADE on this joint image/annotation representation vA, it can learn the relationship between the labels, the spatially-embedded visual words and the annotation words.\nAt test time, the annotation words are not given and we wish to predict them. To achieve this, we compute the document representation hy(vR) based only on the visual words and compute for each possible annotation word a ∈ A the probability that it would be the next observed word p(vAi = a|vA = vR), based on the tree decomposition as in Equation 4. In other words, we only compute the probability of paths that reach a leaf corresponding to an annotation word (not a visual word). We then rank the annotation words in A in decreasing order of their probability and select the top 5 words as our predicted annotation."
    }, {
      "heading" : "5 Deep Extension of SupDocNADE",
      "text" : "Although SupDocNADE has achieved better performance than the other topic models in our previous work [24], the lack of an efficient deep formulation of SupDocNADE reduces its capability of modeling multimodal data, especially compared with other models based on deep neural network [13, 12].\nRecently, Uria et al. [25] proposed an efficient extension of the original NADE model [26] for binary vector observations, from which DocNADE was derived. We take inspiration from Uria et al. [25] and propose SupDeepDocNADE, i.e. a supervised deep autoregressive neural topic model for multimodal data modeling.\nIn this section, we introduce the deep extension of DocNADE (DeepDocNADE) and then describe how to incorporate supervised information into its training. We also discuss how to deal with the inbalance between the number of visual words and annotation words, in order to obtain good performances. Before we start the discussion, we note that the notation v, which denotes the words of an image, includes both visual words and annotation words of an image in the following section, as is discussed in Section 4.3"
    }, {
      "heading" : "5.1 DocNADE revisited",
      "text" : "We first revisit the training procedure for DocNADE. We will concentrate on the unsupervised version of DocNADE for now and discuss the supervised case later.\nIn Section 4.1 we mentioned that words are randomly permuted before every stochastic gradient update, to make DocNADE be a good inference model for any ordering of the words. As Uria et al. [25] notice, we can think of the use of many orderings as the instantiation of many different DocNADE models, one for each distinct ordering. From that point of view, by training a single set of parameters (connection matrices and biases) on all these orderings, we are effectively employing a parameter sharing strategy across these models and the training process can be interpreted as training a factorial number of DocNADE models simultaneously.\nWe will now make the notion of ordering more explicit in our notation. Following Uria et al. [25], we now denote p (v|θ, o) as the joint distribution of the DocNADE model over the words of an image given the parameters θ and ordering o. We will also note p ( vod |vo<d , θ, o ) as the conditional distribution described in Equation 3 or 4, where vo<d is the subvector of the previous d− 1 words extracted from an ordered word vector vo, and vod is the dth word of vo. Notice that the ordering o is now treated explicitly as a random variable.\nThus, training DocNADE on stochastically sampled orderings corresponds, in expectation, to minimize the negative log-likelihood − log p (v|θ, o) across all possible orderings, for each training example v:\nL (v; θ) = E o∈O − log p (v|θ, o) (17)\nwhere O is the set of all orderings. Applying DocNADE’s autoregressive expression for the conditionals in Equation 1, Equation 17 can be rewritten as: L (v; θ) = E\no∈O ∑ d − log p ( vod |vo<d , θ, o ) (18)\nBy moving the expectation over orderings, E o∈O , inside the summation over the conditionals, the expectation can be split into three parts2: one over o<d, standing for the first d− 1 indices in the ordering o; one over od, which is the dth index of the ordering o; and one over o>d, standing for the remaining indices of the ordering.\nHence, the loss function can be rewritten as: L (v; θ) = ∑ d E o<d E od E o>d − log p ( vod |vo<d , θ, o<d, od, o>d ) (19)\nNoting that the value of each conditional does not depend on o>d, Equation 19 can then be simplified as:\nL (v; θ) = ∑ d E o<d E od − log p ( vod |vo<d , θ, o<d, od ) . (20)\nIn practice, Equation 20 still sums over a number of terms of too large to be performed exhaustively. For training, we thus use a stochastic estimation and replace the expectations/sums over d and o<d with samples. On the other hand, the innermost expectation over od can be obtained cheaply. Indeed, for a given value of d and o<d, all terms\n2 The split is done in a modality-agnostic way, i.e. the visual words and annotations words are mixed together and are treated equally when training the model.\np ( vod |vo<d , θ, o<d, od ) require the computation of the same hidden layer representation hd ( vo<d ) from the subvector vo<d . Therefore, L (v, θ) can be estimated by:\nL̂ (v, θ) = Dv Dv − d+ 1 ∑ od − log p ( vod |vo<d , θ, o<d, od ) (21)\nwhere Dv is the number of words (including both visual and annotation words) in v. In words, Equation 21 measures the ability of the model to predict, from a fixed and random context of d− 1 words vo<d, any of the remaining words in the image/annotation.\nFrom this, training of DocNADE can be performed by stochastic gradient descent. For a given training example v, a training update is performed as follows3:\n1). Shuffle v to specify an ordering o;\n2). Sample d uniformly from [0, Dv], which separates v into two parts: vo<d as inputs and vo≥d as outputs;\n3). Compute each of the conditionals in Equation 21, where od ∈ vo≥d ;\n4). Compute and sum the gradients for each of the conditionals in Equation 21, and rescale by DvDv−d+1 .\nIt should be noticed that, since the number of words in an image/annotation pair can vary across examples, the value of Dv will vary between updates, unlike in Uria et al. [25] will models binary vectors of fixed size.\nWe can contrast this procedure from the one described in Section 4.1, which prescribed a stochastic estimation with respect to the possible orderings of the words and an exhaustive sum in predicting all the words in the sequence. Here, we have the opposite: it is stochastic by predicting a subset of the words but is (partially) exhaustive by implicitly summing the gradient contributions over several orderings sharing the same permutation up to position d."
    }, {
      "heading" : "5.2 Deep Document NADE",
      "text" : "As shown in Section 5.1, training of DocNADE can be performed by randomly splitting the words v into two parts, vo<d and vo≥d , and applying stochastic gradient descent on the loss function of Equation 21. Thus, the training procedure now corresponds to a neural network, with vo<d being the input and vo≥d as the output’s target. The advantage of this approach is that DocNADE can more easily be extended to a deep version this way, which we will refer to as DeepDocNADE.\nIndeed, as mentioned in the previous section, all conditionals p ( vod |vo<d , θ, o<d, od ) in the summation of Equa-\ntion 21 require the computation of a single hidden layer representation:\nh (1) d (vo<d) = g\n( c(1) + ∑ k<dW (1) :,vok ) (22)\n= g ( c(1) + W(1)x ( vo<d )) (23)\nwhere x ( vo<d ) is the histogram vector representation of the word sequence vo<d and where the exponent (1) is used to index the first hidden layer and its parameters. So, unlike in the original training procedure for DocNADE, a training update now requires the computation of a single hidden layer, instead of Dv hidden layers. This way, adding more hidden layers only has an additive, instead of multiplicative, effect on the complexity of each training update. Hidden layers are added as in regular deep feedforward neural networks, as follows:\nh(n) = g ( c(n) + W(n)h(n−1) ) (24)\n3In experiments, both visual words and annotation words are represented in Bag of Words (BoW) fashion. As is shown in Section 5.2, the training processing actually equals to generating a word vector v from BoW, shuffling the word vector v and splitting it, and then regenerating the histogram x ( vo<d ) and x ( vo≥d ) , which is inefficient for processing samples in a mini-batch fashion. Hence, in practice, we split the original histogram x (v) directly by uniformly sampling how many are put in the left of the split (the others are put on the right of the split) for each individual word. This is not equivalent to the one mentioned in this paper, but it works well in practice.\nwhere W(n) and c(n) are the connection matrix and bias for hidden layer h(n), n = 1, . . . , N , where N is the number of hidden layers.\nTo compute the conditional p ( vod |vo<d , θ, o<d, od ) in Equation 21 after obtaining the hidden representation h(N), the binary tree introduced in Section 3 could be used for an efficient implementation. However, in cases where the histogram x ( vo≥d ) of future words is not sparse, the binary tree output model might not be the most efficient\napproach. For example, suppose x ( vo≥d ) is full (has no zero entries) and the vocabulary size is Q, the computation of Equation 21 via the binary tree is in O (Q log2Q), since it has to compute O (logQ) logistic regressions for each of the Q words in x ( vo≥d ) . In this specific scenario however, going back to a softmax model of the conditionals is preferable. Indeed, since all conditionals in Equation 21 share the same hidden representation h(N) and thus the normalization term in the softmax is the same for all future words, it is only in O (Q). Another advantage of the softmax over the binary tree is that the softmax is more amenable to an efficient implementation on the GPU, which will also speed up the training process.\nIn the end, for the experiments with the deep extension of DocNADE of this paper, we opted for the softmax model as we’ve found it to be more efficient. We emphasize however that the binary tree is still the most efficient option for the loss function of Equation 12 or when the histogram of future words x ( vo≥d ) is sparse."
    }, {
      "heading" : "5.3 Supervised Deep Document NADE",
      "text" : "Deep Document NADE can also be extended to a supervised variant, which is referred to as SupDeepDocNADE, following the formulation in Section 4.1.\nSpecifically, to add the supervised information into DeepDocNADE, the negative log-likelihood function in Equation 17 could be extended as follows:\nL (v, y; θ) = E o∈O − log p (v, y|θ, o) (25)\n= E o∈O − log p (y|v, θ)− log p (v|θ, o) (26)\nSince p (y|v, θ) is independent of o, Equation 26 can be rewritten as:\nL (v, y; θ) = − log p (y|v, θ)− E o∈O log p (v|θ, o) (27)\nThen L (v, y; θ) can be approximated by sampling v, d and o<d as follows:\nL̂ (v, y; θ) = − log p (y|v, θ) (28)\n− Dv Dv − d+ 1 ∑ od log p ( vod |vo<d , θ, o<d, od ) Similar to Equation 12, the first term in Equation 28 is supervised, while the second term is unsupervised and can be interpreted as a regularizer. Thus, we can also weight the importance of the unsupervised part by a hyperparameter λ and obtain a hybrid cost function:\nL̂ (v, y; θ) = − log p (y|v, θ) (29)\n− λ Dv Dv − d+ 1 ∑ od log p ( vod |vo<d , θ, o<d, od ) Equation 29 can then be used as the per-example loss and optimized over the training set using stochastic gradient descent."
    }, {
      "heading" : "5.4 Weighting the Annotation Words",
      "text" : "As mentioned in Section 4.3, the annotation words can be embedded into the framework of SupDocNADE by treating them the same way we deal with visual words. In practice, however, the number of visual words could be much larger\nthan that of the annotation words. For example, in the MIR Flickr data set, with the experimental setup of Srivastava and Salakhutdinov [12], the average number of visual words for an image is about 69 011, which is much larger than the average number of annotation words for an image (5.15). The imbalance of visual words and annotation words might cause some problems. For example, the contribution to the hidden representation from the annotation words is so small that it might be ignored compared with the contribution from the huge mount of visual words, and the gradients coming from the annotation words might also be too small to have any meaningful effect for increasing the conditionals probability of the annotation words.\nTo deal with this problem, we propose to weight the annotation words in the histogram x ( vo<d ) and x ( vo≥d ) . More specifically, let ω (ρ) ∈ RQ be a vector containingQ components, whereQ is the vocabulary size (including both visual and annotation words), each component corresponding to a word (either visual or annotation). The components corresponding to the visual words is set to 1 and the components corresponding to the annotation word is set to ρ. Then the new histogram of x̃ ( vo<d ) and x̃ ( vo≥d ) is computed as\nx̃ ( vo<d ) = x ( vo<d ) ω (ρ) (30)\nx̃ ( vo≥d ) = x ( vo≥d ) ω (ρ) (31)\nwhere is element-wise multiplication. Moreover, the hybrid cost function of Equation 29 is rewritten as:\nL̂ (v, y; θ) = − log p (y|v, θ) (32)\n− λDv Dv − d+ 1 ∑ od Φod (ρ) log p̃ ( vod |vo<d , θ, o<d, od ) where p̃ ( vod |vo<d , θ, o<d, od ) is a conditional probability obtained by replacing x ( vo<d ) with x̃ ( vo<d ) in Equation 23, and Φod (ρ) is a function that assigns weight ρ if od is an annotation word, and 1 otherwise. By weighting annotation words in the histogram, the model will pay more attention to the annotation words, reducing the problem caused by the imbalance between visual and annotation words. In practice, the weight ρ is a hyper-parameter and can be selected by cross-validation. As we’ll see in Section 6.2.4, weighting annotation words more heavily can significantly improve the performance."
    }, {
      "heading" : "5.5 Exploiting Global Image Features",
      "text" : "Besides the spatial information and annotation which are embedded into the framework of DocNADE in Section 4.2 and Section 4.3, bottom-up global features, such as Gist [27] and MPEG-7 descriptors [28], can also play an important role in multimodal data modeling [12]. Global features can, among other things, complement the local information extracted from patch-based visual words. In this section, we describe how to embed such features into the framework of our model.\nSpecifically, let f ∈ RNf be the global feature vector extracted from an image, whereNf is the length of the global feature vector. One possibility for embedding f into the model could be to condition the hidden representation on the global feature f as follows:\nh(1) = g ( c(1) + W(1)x ( vo<d ) + Pf ) (33)\nwhere P is a connection matrix specific to the global features. This can be understood as a hidden layer whose hidden unit biases are conditioned on the image’s global features vector f . Thus, the whole model is conditioned not only on previous words but also on the global features f ."
    }, {
      "heading" : "6 Experiments and Results",
      "text" : "In this section, we compare the performance of our model over the other models for multimodal data modeling. Specifically, we first test the ability of the single hidden layer SupDocNADE to learn from multimodal data on two real-world\ndata sets which are widely used in the research on other topic models. Then we test the performance of SupDeepDocNADE on the largescale multimedia informaton retrieval (MIR) Flickr data set and show that SupDeepDocNADE achieves state-of-the-art performance. The code to download the data sets and for SupDocNADE and SupDeepDocNADE is available at https://sites.google.com/site/zhengyin1126/home/supdeepdocnade."
    }, {
      "heading" : "6.1 Experiments for SupDocNADE",
      "text" : "To test the ability of the single hidden layer SupDocNADE to learn from multimodal data, we measured its performance under simultaneous image classification and annotation tasks. We tested our model on 2 real-world data sets: a subset of the LabelMe data set [29] and the UIUC-Sports data set [30]. LabelMe and UIUC-Sports come with annotations and are popular classification and annotation benchmarks. We performed extensive quantitative comparisons of SupDocNADE with the original DocNADE model and supervised LDA (sLDA)4 [10, 9]. We also provide some comparisons with MMLDA [17] and a Spatial Pyramid Matching (SPM) approach [20]."
    }, {
      "heading" : "6.1.1 Data sets Description",
      "text" : "Following Wang et al. [9], we constructed our LabelMe data set using the online tool to obtain images of size 256×256 pixels from the following 8 classes: highway, inside city, coast, forest, tall building, street, open country and mountain. For each class, 200 images were randomly selected and split evenly in the training and test sets, yielding a total of 1600 images.\nThe UIUC-Sports data set contains 1792 images, classified into 8 classes: badminton (313 images), bocce (137 images), croquet (330 images), polo (183 images), rockclimbing (194 images), rowing (255 images), sailing (190 images), snowboarding (190 images). Following previous work, the maximum side of each image was resized to 400 pixels, while maintaining the aspect ratio. We randomly split the images of each class evenly into training and test sets. For both LabelMe and UIUC-Sports data sets, we removed the annotation words occurring less than 3 times, as in Wang et al. [9]."
    }, {
      "heading" : "6.1.2 Experimental Setup for SupDocNADE",
      "text" : "Following Wang et al. [9], 128 dimensional, densely extracted SIFT features were used to extract the visual words. The step and patch size of the dense SIFT extraction was set to 8 and 16, respectively. The dense SIFT features from the training set were quantized into 240 clusters, to construct our visual word vocabulary, using K-means. We divided each image into a 2 × 2 grid to extract the spatial position information, as described in Section 4.2. This produced 2× 2× 240 = 960 different visual word/region pairs.\nWe use classification accuracy to evaluate the performance of image classification and the average F-measure of the top 5 predicted annotations to evaluate the annotation performance, as in previous work. The F-measure of an image is defined as\nF -measure = 2× Precision× Recall\nPrecision + Recall (34)\nwhere recall is the percentage of correctly predicted annotations out of all ground-truth annotations for an image, while the precision is the percentage of correctly predicted annotations out of all predicted annotations5. We used 5 random train/test splits to estimate the average accuracy and F-measure.\nImage classification with SupDocNADE is performed by feeding the learned document representations to a RBF kernel SVM. In our experiments, all hyper-parameters (learning rate, unsupervised learning weight λ in SupDocNADE, C and γ in RBF kernel SVM), were chosen by cross validation. We emphasize that, again from following Wang et al. [9], the annotation words are not available at test time and all methods predict an image’s class based solely on its bag of visual words.\n4We mention that [9] has shown that sLDA performs better than Corr-LDA[2]. Moreover, [4] found that Multimodal LDA [5] did not improve on the performance of Corr-LDA. Finally, sLDA distinguishes itself from the other models in the fact that it also supports the class label modality and has code available online. Hence, we compare directly with sLDA only.\n5When there are repeated words in the ground-truth annotations, the repeated terms were removed to calculate the F-measure."
    }, {
      "heading" : "6.1.3 Quantitative Comparison",
      "text" : "In this section, we describe our quantitative comparison between SupDocNADE, DocNADE and sLDA. We used the implementation of sLDA available at http://www.cs.cmu.edu/˜chongw/slda/ in our comparison, to which we fed the same visual (with spatial regions) and annotation words as for DocNADE and SupDocNADE.\nThe classification results are illustrated in Figure 3. Similarly, we observe that SupDocNADE outperforms DocNADE and sLDA. Tuning the trade-off between generative and discriminative learning and exploiting position information is usually beneficial. There is just one exception, on LabelMe, with 200 hidden topic units, where using a 1×1 grid slightly outperforms a 2× 2 grid.\nAs for image annotation, we computed the performance of our model with 200 topics. As shown in Table 1, SupDocNADE obtains an F -measure of 43.87% and 46.95% on the LabelMe and UIUC-Sports data sets respectively. This is slightly superior to regular DocNADE. Since code for performing image annotation using sLDA is not publicly available, we compare directly with the results found in the corresponding paper [9]. Wang et al. [9] report F -measures of 38.7% and 35.0% for sLDA, which is below SupDocNADE by a large margin.\nWe also compare with MMLDA [17], which has been applied to image classification and annotation separately. The reported classification accuracy for MMLDA is less than SupDocNADE as shown in Table 1. The performance for annotation reported in Wang et al. [17] is better than SupDocNADE on LabelMe but worse on UIUC-Sports. We highlight that MMLDA did not deal with the class label and annotation word modalities jointly, the different modalities being treated separately.\nThe spatial pyramid approach of Lazebnik et al. [20] could also be adapted to perform both image classification and annotation. We used the code from Lazebnik et al. [20] to generate two-layer SPM representations with a vocabulary size of 240, which is the same configuration as used by the other models. For image classification, an SVM with\nHistogram Intersection Kernel (HIK) is adopted as the classifier, as in Lazebnik et al. [20]. For annotation, we used a k nearest neighbor (KNN) prediction of the annotation words for the test images. Specifically, the top 5 most frequent annotation words among the k nearest images (based on the SPM representation with HIK similarity) in the training set were selected as the prediction of a test image’s annotation words. The number k was selected by cross validation, for each of the 5 random splits. As shown in Table 1, SPM achieves a classification accuracy of 80.88% and 72.33% for LabelMe and UIUC-Sports, which is lower than SupDocNADE. As for annotation, the F -measure of SPM is also lower than SupDocNADE, with 43.68% and 41.78% for LabelMe and UIUC-Sports, respectively.\nFigure 4 illustrates examples of correct and incorrect predictions made by SupDocNADE on the LabelMe data set."
    }, {
      "heading" : "6.1.4 Visualization of Learned Representations",
      "text" : "Since topic models are often used to interpret and explore the semantic structure of image data, we looked at how we could observe the structure learned by SupDocNADE.\nWe extracted the visual/annotation words that were most strongly associated with certain class labels within SupDocNADE as follows. Given a class label street, which corresponds to a column U:,i in matrix U, we selected the top 3 topics (hidden units) having the largest connection weight in U:,i. Then, we averaged the columns of matrix W\ncorresponding to these 3 hidden topics and selected the visual/annotation words with largest averaged weight connection. The results of this procedure for classes street, sailing, forest and highway is illustrated in Figure 5. To visualize the visual words, we show 16 image patches belonging to each visual word’s cluster, as extracted by K-means. The learned associations are intuitive: for example, the class street is associated with the annotation words “building”, “buildings”, “window”, “person walking” and “sky”, while the visual words showcase parts of buildings and windows."
    }, {
      "heading" : "6.2 Experiments for SupDeepDocNADE",
      "text" : "We now test the performance of SupDeepDocNADE, the deep extension of SupDocNADE, on the large-scale MIR Flickr data set [14]. MIR Flickr is a challenging benchmark for multimodal data modeling task. In this section, we will show that SupDeepDocNADE achieves state-of-the-art performance on the MIR Flickr data set over strong baselines : the DBM apporach of Srivastava and Salakhutdinov [13], MDRNN [15], TagProp [6] and the multiple kernel learning approach of Verbeek et al. [31]."
    }, {
      "heading" : "6.2.1 MIR Flickr Data Set",
      "text" : "The MIR Flickr data set contains 1 million real images that are collected from the image hosting website Flickr. The social tags of each image are also collected and used as annotations in our experiments. Among the 1 million images, there are 25 000 images that is labeled into 38 classes, such as sky, bird, people, animals, car, etc., giving us a subset of labeled images. Each image in the labeled subset can have multiple class labels. In our experiments, we used 15 000 images for training and 10 000 images for testing. The remaining 975 000 images do not have labels and thus were used for the unsupervised pretraining of SupDeepDocNADE (see next section). The most frequent 2000 tags are collected for the annotation vocabulary, following previous work [12, 13]. The averaged number of annotations for an\nimage is 5.15. In the whole data set, 128 501 images do not have annotations, out of which 4551 images are in the labeled subset."
    }, {
      "heading" : "6.2.2 Experimental Setup for SupDeepDocNADE",
      "text" : "In order to compare directly with the DBM approach of Srivastava and Salakhutdinov [13], we use the same experimental configuration. Specifically, the images in MIR Flickr are first rescaled to make the maximum side of each image be 480 pixels, keeping the aspect ratio. Then, 128 dimensional SIFT features are densely sampled on these images to extract the visual words. Following Srivastava and Salakhutdinov [13], we used 4 different scales of patch size, which are 4, 6, 8, 10 pixels, respectively, and the patch step is fixed to 3 pixels. The SIFT features from the unlabeled images were quantized into 2000 clusters, which is used as the visual word vocabulary. Thus, the image modality is represented by the bag of visual words representation using this vocabulary. As preliminary experiments suggested that spatial information (see Section 4.2) wasn’t useful on the Flickr data set, we opted for not using it here. Similarly, the text modality for SupDeepDocNADE is represented using the annotation vocabulary, which is built upon the most frequent 2000 tags, as is mentioned in Section 6.2.1. The visual words and annotation words are combined together and treated as the input of SupDeepDocNADE.\nAs for the global features (Section 5.5), a combination of Gist [27] and MPEG-7 descriptors [28](EHD, HTD, CSD, CLD, SCD) is adopted in our experiments, as in Srivastava and Salakhutdinov [13]. The length of the global features is 1857.\nWe used a 3 hidden layers architecture in our experiments, with the size of each hidden layer being 2048. Note that the DBM [12, 13] also use 3 hidden layers with 2048 hidden units for each layer, thus our comparison with the DBM is fair. The activation function for the hidden units is the rectified linear function. We used a softmax output layer instead of a binary tree to compute the conditionals p ( vod |vo<d , θ, o<d, od ) for SupDeepDocNADE, as discussed in Section 5.2. For the prediction of class labels, since images in MIR Flickr could have multiple labels, we used a sigmoid output layer instead of the softmax to compute the probability that an image belongs to a specific class ci\np (ci = 1|v, θ) = sigmoid ( dci + Uci,:h (N) )\n(35)\nwhere h(N) is the hidden representation of the top layer. As a result, the supervised cost part in Equation 29 is replaced by the cross entropy ∑C i=1−ci log p (ci = 1|v, θ)− (1− ci) log p (ci = 0|v, θ), where C is the number of classes.\nIn all experiments, the unlabeled images are used for unsupervised pretraining. This is achieved by first training a DeepDocNADE model, without any output layer predicting class labels. The result of this training is then used to initialize the parameters of a SupDeepDocNADE model, which is finetuned on the labeled training set based on the loss of Equation 32.\nOnce training is finalized, the hidden representation from the top hidden layer after observing all words (both visual words and annotation words) of an image is feed to a linear SVM [32] to compute confidences of an image belonging to each class. The average precision (AP) for each class is obtained based on these confidences, where AP is the area under the precision-recall curve. After that, the mean average precision (MAP) over all classes is computed and used as the metric to measure the performance of the model. We used the same 5 training/validation/test set splits on the labeled subset of MIR Flickr as Srivastava and Salakhutdinov [13] and report the average performance on the 5 splits.\nTo initialize the connection matrices, we followed the recommendation of Glorot and Bengio [33] used a uniform distribution: Θ ∼ U [ − √ 6√\nlΘ + wΘ ,\n√ 6√\nlΘ + wΘ\n] (36)\nwhere Θ ∈ {W,U,V} is a connection matrix, lΘ,wΘ are the number of rows and columns respectively of matrix Θ, respectively, andU is the uniform distribution. In practice, we’ve also found it useful to normalize the input histograms x̃ ( vo<d ) for each image, by rescaling them to have unit variance.\nThe hyper-parameters (learning rate, unsupervised weight λ, and the parameter for linear SVM, etc.) are chosen by cross-validation. To prevent overfitting, dropout [34] is adopted during training, with a dropout rate of 0.5 for all\nhidden layers. We also maintained an exponentially decaying average of the parameter values throughout the gradient decent training procedure and used the averaged parameters at test time. This corresponds to Polyak averaging [35], but where the linear average is replaced by a weighting that puts more emphasis on recent parameter values. For the annotation weight, it was fixed to 12 000, which is approximately the ratio of the averaged visual words and annotation words of the data set. We will investigate the impact of the annotation weight on the performance in Section 6.2.4."
    }, {
      "heading" : "6.2.3 Comparison with other baselines",
      "text" : "Table 2 presents a comparison of the performance of SupDeepDocNADE with the DBM approach of Srivastava and Salakhutdinov [13] and MDRNN of Sohn et al. [15] as well as other strong baselines, in terms of MAP performance. We also provide the simple and popular TF-IDF baseline in Table 2 to make the comparison more complete. The TF-IDF baseline is conducted only on the bag-of-words representations of images without global features. We feed the TF-IDF representations to a linear SVM to obtain confidences of an image belonging to each class and then we compute the Mean AP, as for SupDeepDocNADE.\nWe can see that SupDeepDocNADE achieves the best performance among all methods. More specifically, we first pretrained the model for 625 epochs on the unlabeled data with 1, 2 and 3 hidden layers. The results illustrated in Table 2 show that SupDeepDocNADE outperforms the DBM baseline by a large margin. Moreover, we can see that SupDeepDocNADE with 2 and 3 hidden layers performs better than with only 1 hidden layer, with 625 epochs of pretraining. We then pretrained the model for more epochs on the unlabeled data (2325 epochs). As shown in Table 2, with more pretraining epochs, the deeper model (3 hidden layers) performs even better. This confirms that the use of a deep architecture is beneficial. When the number of pretraining epochs reaches 4125, the SupDeepDocNADE model with 3 hidden layers achieves a MAP of 0.691, which outperforms all the strong baselines and increases the performance gap with the 2-hidden-layers model.\nFrom Tabel 2 we can also see that the performance of 2-layers SupDeepDocNADE does not improve as much as 3-layers SupDeepDocNADE when the number of pretraining epochs increases from 2325 to 4125. Figure 6 shows the the performance of SupDeepDocNADE w.r.t the number of pretraining epochs. We can see from Figure 6 that with more epochs of pretraining, the performance of 3-layers SupDeepDocNADE increases faster than the 2-layers models, which indicates that the capacity of 3-layers SupDeepDocNADE is bigger than the 2-layers model and the capacity could be leveraged by more pretraining. Figure 6 also suggests that the performance of SupDeepDocNADE could be even better than 0.691 with more pretraining epochs.\nFigure 7 illustrates some failed predictions of SupDeepDocNADE, where the reasons for failure are shown on the\nleft-side of each row. One of the reasons for failure is that the local texture/color is ambiguous or misleading. For example, in the first image of the top row, the blue color in the upper side of the wall misleads the model to predict ”sky” with a confidence of 0.995. Another type of failure, which is shown in the middle row of Figure 7, is caused by images of an abstract illustration of the class. For instance, the model fails to recognize the bird, car and tree in the images of the middle row, respectively, as these images are merely abstract illustrations of these concepts. The third reason illustrated in the bottom row is that the class takes a small portion of the image, making it more likely to be ignored. For example, the female face on the stamp in the first image of the bottom row is too small to be recognized by the model. Note that we just illustrated some failed examples and there might be other kinds of failures. In practice, we also find that some images are not correctly labeled, which might also cause some failures.\nHaving established that SupDeepDocNADE achieves state-of-the-art performance on the MIR Flickr data set and also discussed some failed examples, we now explore in more details some of its properties in the following sections."
    }, {
      "heading" : "6.2.4 The Impact of the Annotation Weight",
      "text" : "In Section 6.2.4, we proposed to weight differently the annotation words to deal with the problem of imbalance in the number of visual and annotation words. In this part, we investigate the influence of the annotation weight on the performance. Specifically, we set the annotation weight to {1, 4000, 8000, 12 000, 16 000}, and show the performance for each of the annotation weight values. Note that when the annotation weight equals 1, there is no compensation for the imbalance of visual words and annotation words. The other experimental configurations are the same as in Section 6.2.2.\nFigure 8 shows the performance comparison between different annotation weights. As expected, SupDeepDocNADE performs extremely bad when the annotation equals to 1, When the annotation weight is increased, the performance gets better. Among all the chosen annotation weights, 12 000 performs best, which achieves a MAP of 0.671. The other annotation weights also achieves good performance compared with the DBM model [13]: MAP of 0.658, 0.669 and 0.670 for annotation weight values of 4000, 8000 and 16 000, respectively."
    }, {
      "heading" : "6.2.5 Visualization of the Retrieval Results",
      "text" : "Since SupDeepDocNADE is used for multimodal data modeling, we illustrate here some results for multimodal data retrieval tasks. More specifically, we show some qualitative results in two multimodal data retrieval scenarios: multimodal data query and generation of text from images. Multimodal Data Query: Given a query corresponding to an image/annotation pair, the task is to retrieve other similar pairs from a collection, using the hidden representation learned by SupDeepDocNADE. In this task, the cosine similarity is adopted as the similarity metric. In this experiment, each query corresponds to an individual test example and the collection corresponds to the rest of the test set. Figure 10 illustrates the retrieval results for multimodal data query task, where we show the 4 most similar images to the query input in the testset. Generating Text from Image: As SupDeepDocNADE learns the relationship between the image and text modalities, we test its ability to generate text from given images. This task is implemented by feeding SupDeepDocNADE only the bag of visual words and selecting the annotation words according to their probability of being the next word, similarly to Section 4.3. Figure 9 illustrates the ground truth annotation and the most probable 8 annotations generated by SupDeepDocNADE. We can see that SupDeepDocNADE generated very meaningful texts according to the image modality, which shows that it effectively learned about the statistical structure between the two modalities."
    }, {
      "heading" : "7 Conclusion and Discussion",
      "text" : "In this paper, we proposed SupDocNADE, a supervised extension of DocNADE, which can learn jointly from visual words, annotations and class labels. Moreover, we proposed a deep extension of SupDocNADE which outperforms its shallow version and can be trained efficiently. Although both SupDocNADE and SupDeepDocNADE are the same in nature, SupDeepDocNADE differs from the single layer version in its training process. Specifically, the\ntraining process of SupDeepDocNADE is performed over a subset of the words by summing the gradients over several orderings sharing the same permutation up to a randomly selected position d, while the single layer version does the opposite and exploits a single randomly selected ordering but updates all the conditionals on the words.\nLike all topic models, our model is trained to model the distribution of the bag-of-word representations of images and can extract a meaningful representation from it. Unlike most topic models however, the image representation is not modeled as a latent random variable in a model, but instead as the hidden layer of a neural autoregressive network. A distinctive advantage of SupDocNADE is that it does not require any iterative, approximate inference procedure to compute an image’s representation. Our experiments confirm that SupDocNADE is a competitive approach for multimodal data modeling and SupDeepDocNADE achieves state-of-the-art performance on the challenging multimodal data benchmark MIR Flickr."
    } ],
    "references" : [ {
      "title" : "Matching words and pictures",
      "author" : [ "K. Barnard", "P. Duygulu", "D. Forsyth" ],
      "venue" : "JMLR, 2003.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Modeling annotated data",
      "author" : [ "D.M. Blei", "M.I. Jordan" ],
      "venue" : "ACM SIGIR, 2003.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora",
      "author" : [ "R. Socher", "L. Fei-Fei" ],
      "venue" : "CVPR, 2010.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning cross-modality similarity for multinomial data",
      "author" : [ "Y. Jia" ],
      "venue" : "ICCV, 2011.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Topic regression multi-modal latent dirichlet allocation for image annotation",
      "author" : [ "D. Putthividhy" ],
      "venue" : "CVPR, 2010.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multimodal semi-supervised learning for image classification",
      "author" : [ "M. Guillaumin" ],
      "venue" : "CVPR, 2010.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A new approach to cross-modal multimedia retrieval",
      "author" : [ "N. Rasiwasia" ],
      "venue" : "ACM-MM, 2010.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D. Blei" ],
      "venue" : "JMLR, 2003.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Simultaneous image classification and annotation",
      "author" : [ "C. Wang", "D. Blei", "F.-F. Li" ],
      "venue" : "CVPR, 2009.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Replicated softmax: an undirected topic model",
      "author" : [ "R. Salakhutdinov", "G.E. Hinton" ],
      "venue" : "NIPS, 2009.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multimodal learning with deep boltzmann machines.",
      "author" : [ "N. Srivastava", "R. Salakhutdinov" ],
      "venue" : "in NIPS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Discriminative transfer learning with tree-based priors",
      "author" : [ "N. Srivastava", "R.R. Salakhutdinov" ],
      "venue" : "NIPS, 2013.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The mir flickr retrieval evaluation",
      "author" : [ "M.J. Huiskes", "M.S. Lew" ],
      "venue" : "ACM MIR, 2008.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Improved multimodal deep learning with variation of information",
      "author" : [ "K. Sohn", "W. Shang", "H. Lee" ],
      "venue" : "NIPS, 2014.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A neural autoregressive topic model",
      "author" : [ "H. Larochelle", "S. Lauly" ],
      "venue" : "NIPS 25, 2012.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Max-margin latent dirichlet allocation for image classification and annotation",
      "author" : [ "Y. Wang" ],
      "venue" : "BMVC, 2011.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1206.5538, 2012.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
      "author" : [ "S. Lazebnik" ],
      "venue" : "CVPR, 2006.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The tradeoff between generative and discriminative classifiers",
      "author" : [ "G. Bouchard", "B. Triggs" ],
      "venue" : "COMPSTAT, 2004.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Deep sparse rectifier networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "AISTATS, 2011.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "ICML, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Topic modeling of multimodal data: an autoregressive approach",
      "author" : [ "Y. Zheng", "Y.-J. Zhang", "H. Larochelle" ],
      "venue" : "CVPR, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A deep and tractable density estimator",
      "author" : [ "B. Uria", "I. Murray", "H. Larochelle" ],
      "venue" : "ICML, 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The neural autoregressive distribution estimator",
      "author" : [ "H. Larochelle", "I. Murray" ],
      "venue" : "Journal of Machine Learning Research, 2011.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Modeling the shape of the scene: A holistic representation of the spatial envelope",
      "author" : [ "A. Oliva", "A. Torralba" ],
      "venue" : "IJCV, 2001.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Color and texture descriptors",
      "author" : [ "B.S. Manjunath" ],
      "venue" : "Circuits and Systems for Video Technology, IEEE Transactions on, 2001.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Labelme: a database and web-based tool for image annotation",
      "author" : [ "B.C. Russell" ],
      "venue" : "IJCV, 2008.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "What, where and who? classifying events by scene and object recognition",
      "author" : [ "L.-J. Li", "L. Fei-Fei" ],
      "venue" : "ICCV, 2007. 23",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Image annotation with tagprop on the mirflickr set",
      "author" : [ "J. Verbeek" ],
      "venue" : "ACM MIR, 2010.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "R.-E. Fan" ],
      "venue" : "The Journal of Machine Learning Research, 2008.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "AISTATS, 2010.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "G.E. Hinton" ],
      "venue" : "arXiv preprint arXiv:1207.0580, 2012.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A tutorial on stochastic approximation algorithms for training restricted boltzmann machines and deep belief nets",
      "author" : [ "K. Swersky" ],
      "venue" : "Information Theory and Applications Workshop. IEEE, 2010. 24",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 5,
      "context" : "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "Multimodal data modeling, which combines information from different sources, is increasingly attracting attention in computer vision [1, 2, 3, 4, 5, 6, 7].",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "One of the leading approaches is based on topic modelling, the most popular model being latent Dirichlet allocation or LDA [8].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "LDA is a generative model for documents that originates from the natural language processing community, but has had great success in computer vision [8, 9].",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "LDA is a generative model for documents that originates from the natural language processing community, but has had great success in computer vision [8, 9].",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "To deal with multimodal data, some variants of LDA have been proposed recently [2, 5, 4, 9].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "To deal with multimodal data, some variants of LDA have been proposed recently [2, 5, 4, 9].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "To deal with multimodal data, some variants of LDA have been proposed recently [2, 5, 4, 9].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "To deal with multimodal data, some variants of LDA have been proposed recently [2, 5, 4, 9].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "For instance, Correspondence LDA (Corr-LDA) [2] was proposed to discover the relationship between images and annotation modalities, by assuming each image topic must have a corresponding text topic.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "Multimodal LDA [5] generalizes Corr-LDA by learning a regression module relating the topics from the different",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "Multimodal Document Random Field Model (MDRF) [4] was also proposed to deal with multimodal data, which learns cross-modality similarities from a document corpus containing multinomial data.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "Besides the annotation words, the class label modality can also be embedded into LDA, such as in supervised LDA (sLDA) [10, 9].",
      "startOffset" : 119,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "In the realm of document modeling, Salakhutdinov and Hinton [11] proposed a so-called Replicated Softmax (RS) model for bags of words.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "The RS model was later used for multimodal data modeling [12], where pairs of images and text annotations were modeled jointly within a deep Boltzmann machine (DBM) [13].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : "The RS model was later used for multimodal data modeling [12], where pairs of images and text annotations were modeled jointly within a deep Boltzmann machine (DBM) [13].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 12,
      "context" : "This deep learning approach to the generative modeling of multimodal data achieved state-of-the-art performance on the MIR Flickr data set [14].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "Another neural network based state-of-the-art multimodal data modeling approach is Multimodal Deep Recurrent Neural Network (MDRNN) [15] which aims at predicting missing data modalities through the rest of data modalities by minimizing the variation of information rather than maximizing likelihood.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "Recently, an alternative generative modeling approach for documents was proposed in Larochelle and Lauly [16].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : "Larochelle and Lauly [16] also show that DocNADE is a better generative model of text documents than LDA and the RS model, and can extract a useful representation for text information retrieval.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "As previously mentioned, multimodal data is often modeled using extensions of the basic LDA topic model, such as Corr-LDA [2], Multimodal LDA [5] and MDRF [4].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "As previously mentioned, multimodal data is often modeled using extensions of the basic LDA topic model, such as Corr-LDA [2], Multimodal LDA [5] and MDRF [4].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "As previously mentioned, multimodal data is often modeled using extensions of the basic LDA topic model, such as Corr-LDA [2], Multimodal LDA [5] and MDRF [4].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : "[9] proposed a supervised LDA formulation to tackle this problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "[17] opted instead for a maximum margin formulation of LDA (MMLDA).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Recently, deep neural networks are increasingly used for the probabilistic modeling of images and text (see [18] for a review).",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "The work of Srivastava and Salakhutdinov [12] on DBMs and Sohn et al.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : "[15] on MDRNN are good recent examples.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[19] also proposed deep autoencoder networks for multimodal learning, though this approach was recently shown to be outperformed by DBMs [13] and MDRNN [15].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "[19] also proposed deep autoencoder networks for multimodal learning, though this approach was recently shown to be outperformed by DBMs [13] and MDRNN [15].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "Although DocNADE shows favorable performance over other topic models, the lack of an efficient deep formulation reduces its ability of modeling multimodal data, especially compared with the deep neural network based models [19, 12, 13].",
      "startOffset" : 223,
      "endOffset" : 235
    }, {
      "referenceID" : 11,
      "context" : "Although DocNADE shows favorable performance over other topic models, the lack of an efficient deep formulation reduces its ability of modeling multimodal data, especially compared with the deep neural network based models [19, 12, 13].",
      "startOffset" : 223,
      "endOffset" : 235
    }, {
      "referenceID" : 11,
      "context" : "As we’ll see, the deep version of our DocNADE model will outperform the DBM approach of Srivastava and Salakhutdinov [13].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "In Larochelle and Lauly [16], DocNADE was used to model documents of real words, belonging to some predefined vocabulary.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "To address this issue, Larochelle and Lauly [16] propose to use a balanced binary tree to decompose the computation of the conditionals and obtain a complexity logarithmic inQ.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "Larochelle and Lauly [16] model each left/right transition probabilities in the binary tree using a set of binary logistic regressors taking the hidden layer hi(v<i) as input.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "One could attempt to optimize the organization of the words within the tree, but a random assignment of the words to leaves works well in practice [16].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : "1 Supervised DocNADE It has been observed that learning image feature representations using unsupervised topic models such as LDA can perform worse than training a classifier directly on the visual words themselves, using an appropriate kernel such as a pyramid kernel [20].",
      "startOffset" : 269,
      "endOffset" : 273
    }, {
      "referenceID" : 18,
      "context" : "This is known as generative learning [21].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "from left to right, top to bottom in the image), we follow Larochelle and Lauly [16] and randomly permute the words before every stochastic gradient update.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "which often outperforms other activation functions [22] and has been shown to work well for image data [23].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "which often outperforms other activation functions [22] and has been shown to work well for image data [23].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "For example, in the seminal work on spatial pyramids [20], it is shown that extracting different visual word histograms over distinct regions instead of a single image-wide histogram can yield substantial gains in performance.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 21,
      "context" : "Although SupDocNADE has achieved better performance than the other topic models in our previous work [24], the lack of an efficient deep formulation of SupDocNADE reduces its capability of modeling multimodal data, especially compared with other models based on deep neural network [13, 12].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "Although SupDocNADE has achieved better performance than the other topic models in our previous work [24], the lack of an efficient deep formulation of SupDocNADE reduces its capability of modeling multimodal data, especially compared with other models based on deep neural network [13, 12].",
      "startOffset" : 282,
      "endOffset" : 290
    }, {
      "referenceID" : 10,
      "context" : "Although SupDocNADE has achieved better performance than the other topic models in our previous work [24], the lack of an efficient deep formulation of SupDocNADE reduces its capability of modeling multimodal data, especially compared with other models based on deep neural network [13, 12].",
      "startOffset" : 282,
      "endOffset" : 290
    }, {
      "referenceID" : 22,
      "context" : "[25] proposed an efficient extension of the original NADE model [26] for binary vector observations, from which DocNADE was derived.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] proposed an efficient extension of the original NADE model [26] for binary vector observations, from which DocNADE was derived.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "[25] and propose SupDeepDocNADE, i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[25] notice, we can think of the use of many orderings as the instantiation of many different DocNADE models, one for each distinct ordering.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[25], we now denote p (v|θ, o) as the joint distribution of the DocNADE model over the words of an image given the parameters θ and ordering o.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[25] will models binary vectors of fixed size.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "For example, in the MIR Flickr data set, with the experimental setup of Srivastava and Salakhutdinov [12], the average number of visual words for an image is about 69 011, which is much larger than the average number of annotation words for an image (5.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 24,
      "context" : "3, bottom-up global features, such as Gist [27] and MPEG-7 descriptors [28], can also play an important role in multimodal data modeling [12].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 25,
      "context" : "3, bottom-up global features, such as Gist [27] and MPEG-7 descriptors [28], can also play an important role in multimodal data modeling [12].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "3, bottom-up global features, such as Gist [27] and MPEG-7 descriptors [28], can also play an important role in multimodal data modeling [12].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 26,
      "context" : "We tested our model on 2 real-world data sets: a subset of the LabelMe data set [29] and the UIUC-Sports data set [30].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 27,
      "context" : "We tested our model on 2 real-world data sets: a subset of the LabelMe data set [29] and the UIUC-Sports data set [30].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "We performed extensive quantitative comparisons of SupDocNADE with the original DocNADE model and supervised LDA (sLDA)4 [10, 9].",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "We also provide some comparisons with MMLDA [17] and a Spatial Pyramid Matching (SPM) approach [20].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : "We also provide some comparisons with MMLDA [17] and a Spatial Pyramid Matching (SPM) approach [20].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "[9], we constructed our LabelMe data set using the online tool to obtain images of size 256×256 pixels from the following 8 classes: highway, inside city, coast, forest, tall building, street, open country and mountain.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9], 128 dimensional, densely extracted SIFT features were used to extract the visual words.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9], the annotation words are not available at test time and all methods predict an image’s class based solely on its bag of visual words.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "4We mention that [9] has shown that sLDA performs better than Corr-LDA[2].",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "4We mention that [9] has shown that sLDA performs better than Corr-LDA[2].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "Moreover, [4] found that Multimodal LDA [5] did not improve on the performance of Corr-LDA.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "Moreover, [4] found that Multimodal LDA [5] did not improve on the performance of Corr-LDA.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "LabelMe UIUC-Sports Model Accuracy% F-measure% Accuracy% F-measure% SPM [20] 80.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "78 MMLDA [17] 81.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "51† sLDA [9] 81.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "Since code for performing image annotation using sLDA is not publicly available, we compare directly with the results found in the corresponding paper [9].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "[9] report F -measures of 38.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "We also compare with MMLDA [17], which has been applied to image classification and annotation separately.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "[17] is better than SupDocNADE on LabelMe but worse on UIUC-Sports.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[20] could also be adapted to perform both image classification and annotation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[20] to generate two-layer SPM representations with a vocabulary size of 240, which is the same configuration as used by the other models.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[20].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "2 Experiments for SupDeepDocNADE We now test the performance of SupDeepDocNADE, the deep extension of SupDocNADE, on the large-scale MIR Flickr data set [14].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 11,
      "context" : "In this section, we will show that SupDeepDocNADE achieves state-of-the-art performance on the MIR Flickr data set over strong baselines : the DBM apporach of Srivastava and Salakhutdinov [13], MDRNN [15], TagProp [6] and the multiple kernel learning approach of Verbeek et al.",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 13,
      "context" : "In this section, we will show that SupDeepDocNADE achieves state-of-the-art performance on the MIR Flickr data set over strong baselines : the DBM apporach of Srivastava and Salakhutdinov [13], MDRNN [15], TagProp [6] and the multiple kernel learning approach of Verbeek et al.",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "In this section, we will show that SupDeepDocNADE achieves state-of-the-art performance on the MIR Flickr data set over strong baselines : the DBM apporach of Srivastava and Salakhutdinov [13], MDRNN [15], TagProp [6] and the multiple kernel learning approach of Verbeek et al.",
      "startOffset" : 214,
      "endOffset" : 217
    }, {
      "referenceID" : 28,
      "context" : "[31].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "The most frequent 2000 tags are collected for the annotation vocabulary, following previous work [12, 13].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "The most frequent 2000 tags are collected for the annotation vocabulary, following previous work [12, 13].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "2 Experimental Setup for SupDeepDocNADE In order to compare directly with the DBM approach of Srivastava and Salakhutdinov [13], we use the same experimental configuration.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : "Following Srivastava and Salakhutdinov [13], we used 4 different scales of patch size, which are 4, 6, 8, 10 pixels, respectively, and the patch step is fixed to 3 pixels.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 24,
      "context" : "5), a combination of Gist [27] and MPEG-7 descriptors [28](EHD, HTD, CSD, CLD, SCD) is adopted in our experiments, as in Srivastava and Salakhutdinov [13].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "5), a combination of Gist [27] and MPEG-7 descriptors [28](EHD, HTD, CSD, CLD, SCD) is adopted in our experiments, as in Srivastava and Salakhutdinov [13].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "5), a combination of Gist [27] and MPEG-7 descriptors [28](EHD, HTD, CSD, CLD, SCD) is adopted in our experiments, as in Srivastava and Salakhutdinov [13].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 10,
      "context" : "Note that the DBM [12, 13] also use 3 hidden layers with 2048 hidden units for each layer, thus our comparison with the DBM is fair.",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "Note that the DBM [12, 13] also use 3 hidden layers with 2048 hidden units for each layer, thus our comparison with the DBM is fair.",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 29,
      "context" : "Once training is finalized, the hidden representation from the top hidden layer after observing all words (both visual words and annotation words) of an image is feed to a linear SVM [32] to compute confidences of an image belonging to each class.",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 11,
      "context" : "We used the same 5 training/validation/test set splits on the labeled subset of MIR Flickr as Srivastava and Salakhutdinov [13] and report the average performance on the 5 splits.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 30,
      "context" : "To initialize the connection matrices, we followed the recommendation of Glorot and Bengio [33] used a uniform distribution: Θ ∼ U [ − √ 6 √ lΘ + wΘ , √ 6 √ lΘ + wΘ ] (36)",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 31,
      "context" : "To prevent overfitting, dropout [34] is adopted during training, with a dropout rate of 0.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "004 Multiple Kernel Learning SVMs [6] 0.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : "623 TagProp [31] 0.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 11,
      "context" : "640 Multimodal DBM [13] 0.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "005 MDRNN [15] 0.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 32,
      "context" : "This corresponds to Polyak averaging [35], but where the linear average is replaced by a weighting that puts more emphasis on recent parameter values.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 11,
      "context" : "3 Comparison with other baselines Table 2 presents a comparison of the performance of SupDeepDocNADE with the DBM approach of Srivastava and Salakhutdinov [13] and MDRNN of Sohn et al.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 13,
      "context" : "[15] as well as other strong baselines, in terms of MAP performance.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "The other annotation weights also achieves good performance compared with the DBM model [13]: MAP of 0.",
      "startOffset" : 88,
      "endOffset" : 92
    } ],
    "year" : 2015,
    "abstractText" : "Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to deal with multimodal data, such as in image annotation tasks. Another popular approach to model the multimodal data is through deep neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-theart performance for text document modeling. In this work, we show how to successfully apply and extend this model to multimodal data, such as simultaneous image classification and annotation. First, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the learned hidden topic features and show how to employ it to learn a joint representation from image visual words, annotation words and class label information. We test our model on the LabelMe and UIUC-Sports data sets and show that it compares favorably to other topic models. Second, we propose a deep extension of our model and provide an efficient way of training the deep model. Experimental results show that our deep model outperforms its shallow version and reaches state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flickr data set.",
    "creator" : "LaTeX with hyperref package"
  }
}