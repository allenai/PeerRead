{
  "name" : "1702.08159.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "F2F: A Library for Fast Kernel Expansions",
    "authors" : [ "Joachim Curto", "Irene Zarza", "Feng Yang", "Alexander J. Smola", "Luc Van Gool" ],
    "emails" : [ "curto@vision.ee.ethz.ch", "zarza@vision.ee.ethz.ch", "fengyang@google.com", "alex@smola.org", "vangool@vision.ee.ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Approximate kernel expansions, Fast Walsh-Hadamard Transform, SIMD, Gaussian RBF kernel, Matern kernel"
    }, {
      "heading" : "1. Introduction",
      "text" : "Kernel methods offer state-of-the-art estimation performance. They provide function classes that are flexible and easy to control in terms of regularization properties. However, the use of kernels in large-scale machine learning problems has been beset with difficulty. This is because using kernel expansions in large datasets is too expensive in terms of computation and storage. In order to solve this problem, Le et al. (2013) proposed an approximation algorithm, called Fastfood, based on Random Kitchen Sinks by Rahimi and Recht (2007), that speeds up the computation of a large range of kernel functions, allowing us to use them in big data.\nAt its heart, Fastfood requires scalar multiplications, a permutation, access to trigonometric functions, and two Walsh-Hadamard Transforms (WHT) for implementation. The key computational bottleneck here is the WHT. We provide a fast, cache friendly SIMD oriented implementation that outperforms state-of-the-art codes such as Spiral Johnson and\nar X\niv :1\n70 2.\n08 15\n9v 1\n[ cs\nPuschel (2000). To allow for very compact distribution of models, we use hash functions and a Pseudorandom Permutation Generator for portability. In this way, for each feature dimension, we only need one floating point number. In summary, our implementation serves as a drop-in feature generator for linear methods where attributes are generated on the fly, such as for regression, classification, or two-sample tests. This obviates the need for explicit kernel computations, particularly on large amounts of data.\nOutline: We begin with a brief operational description of the Fastfood feature construction in Section 2. This is followed by a discussion of the computational issues for a fast SIMD implementation in Section 3. The concepts governing the API are described in Section 4. A detailed reference for all classes and methods is relegated to the appendix."
    }, {
      "heading" : "2. Fastfood and its Computational Complexity",
      "text" : "Kernel methods work by defining a kernel function k(x, x′) on a domain X . We can write k as inner product between feature maps, as follows\nk(x, x′) = 〈 φ(x), φ(x′) 〉 (1)\nfor some suitably chosen φ. Random Kitchen Sinks (Rahimi and Recht, 2007) approximate this feature mapping φ by a Fourier expansion in the case of radial basis function kernels, i.e. whenever k(x, x′) = κ(x− x′). This is possible since the Fourier transform diagonalizes the corresponding integral operator (Smola et al., 1988). This leads to\nk(x, x′) = ∫ exp i 〈w, x〉 exp−i 〈w, x〉dρ(w) (2)\nfor some L2 measurable function ρ(ω) ≥ 0 that is given by the Fourier transform of κ. Random Kitchen Sinks exploit this by replacing the integral by sampling ωj ∼ ρ(ω)/ ‖ρ‖1. This allows for finite dimensional expansions but it is costly due to the large number of inner products required. Fastfood resolves this for rotationally invariant κ by providing a fast approximation of the matrix W := [ω1, . . . ωn].\nThis is best seen for the Gaussian RBF kernel where k(x, x′) = exp ( − 1\n2σ2 ‖x− x′‖2\n) . Since\nFourier transforms of Gaussians are Gaussians, albeit with inverse covariance, it follows that ρ(ω) ∝ exp ( −σ22 ‖ω‖ 2 ) and that W contains i.i.d Gaussian random variables. It is this matrix that Fastfood approximates via\nV := SHGΠHB (3)\nHere S,G and B are diagonal matrices, Π is a random permutation matrix and H is the Hadamard matrix. Whenever the number of rows in W exceeds the dimensionality of the data, we can simply generate multiple instances of V , drawn i.i.d, until the required number of dimensions is obtained.\nDiagonal Binary Matrix B This is a matrix with entries Bii ∈ {±1}, drawn from the uniform distribution. To avoid memory footprint, we simply use Murmurhash1 as hash function and extract bits from h(i, j) with j ∈ {0, . . . N}.\n1. https://code.google.com/p/smhasher/\nWalsh Hadamard Matrix H This matrix is iteratively composed ofH2n = [ Hn Hn −Hn Hn ] .\nIt is fixed and matrix-vector products are carried out efficiently in O(n log n) time using the Fast Walsh-Hadamard Transform (FWHT). We will discuss implementation details for a fast variant below.\nPermutation Matrix Π We generate a random permutation using the Fisher-Yates shuffle. That is, given a list L = {1, . . . n} we generate permutations recursively as follows: pick a random element from L. Use this as the image of n and move n to the position where the element was removed. The algorithm runs in linear time and its coefficients can be stored in O(n) space. Moreover, to obtain a deterministic mapping, replace the random number generator with calls to the hash function. Gaussian Scaling G This is a diagonal matrix with i.i.d Gaussian entries. We generate the random variates using the Box-Muller transform Box and Muller (1958) while substituting the random number generator by calls to the hash function to allow us to recompute the values at any time without the need to store random numbers. Diagonal Scaling S This is a random scaling operator whose behavior depends on the type of kernel chosen, such as the Matern kernel, the Gaussian RBF kernel or any other radial spectral distribution."
    }, {
      "heading" : "3. Fast Walsh-Hadamard Transform Implementation",
      "text" : "A key part of the library is an efficient implementation of the Fast Walsh-Hadamard Transform. In particular, F2F offers considerable improvement over the Spiral2 library, due to automatic code generation, the use of Intel SIMD intrinsics (SSE2 using 128 bit registers) and loop unrolling. This decreases the memory overhead.\nF2F proceeds with vectorized sums and subtractions iteratively for the first n 2k\ninput vector positions (where n is the length of the input vector and k the iteration starting from 1), computing the intermediate operations of the Cooley-Tukey algorithm till a small Hadamard routine that fits in cache. Then the algorithm continues in the same way but starting from the smallest length and doubling on each iteration the input dimension until the whole FWHT is done in-place.\nFor instance, on an intel i5-4200 CPU @ 1.60GHz laptop the following performance can be obtained\n2. http://www.spiral.net\n103 104 105 106\n10−1\n100\n101\nSize of Hadamard matrix Hn\nT im\ne in\nm il\nli se\nco n\nd s\nSpiral FWHT F2F FWHT\nOur code outperforms Spiral by a factor of 2 consistently throughout the range of arguments. Furthermore, Spiral needs to precompute trees and by default can only perform the computation up to matrix size n = 220. On the other hand, our implementation works for any size since it computes the high-level partitioning dynamically."
    }, {
      "heading" : "4. Software Description",
      "text" : "The API for the fastfood implementation follows the factory design pattern. That is, while the fastfood object is fairly generic in terms of feature computation, we have a factory that acts as a means of instantiating the parameters according to pre-specified sets of parameters for e.g. a Gaussian RBF or a Matern kernel. The so-chosen parameters are deterministic, given by the values of a hash function. The advantage of this approach is that there is no need to save the coefficients generated for fastfood when deploying the functions. That said, we also provide a constructor that consumes B,Π, G and S to allow for arbitrary kernels.\nvoid fwht_opt(float* data, unsigned long lgn)\nComputes the in place Fast Hadamard-Walsh Transform on data. The length of the vector it operates on is given by 2lgn. Note that fwht opt does not check whether data points to a sufficiently large amount of memory.\n4.1 Customizing F2F: Matern kernel\nFor instance, to generate each Sii entry for Matern kernel we draw t iid samples from the n-dimensional unit ball Sd, add them and compute its Euclidean norm. To draw efficiently samples from Sd we use the algorithm provided below.\nLet X = (X1, . . . , Xn) be a vector of iid random variables drawn from N (0, 1), and let ||X|| be the Euclidean norm of X, then Y = ( X1||X|| , . . . , Xn ||X||) is uniformly distributed over the n-sphere, where it is obvious to see that Y is the projection of X onto the surface of the n-dimensional sphere. To draw uniform random variables in the n-ball, we multiply\nY by U1/n where U ∼ U(0, 1). This can be proved as follows: Let Z = (Z1, . . . , Zn) be a random vector uniformly distributed in the unit ball. Then, the radius R = ||Z|| satisfies P(R ≤ r) = rn. Now, by the inverse transform method we get R = U1/n.Therefore to sample uniformly from the n-ball the following algorithm is used: Z = rU1/n X||X|| ."
    }, {
      "heading" : "4.2 Design",
      "text" : "F2F class instance initializes the object by computing and storing B and Π diagonal matrices. The Factory design lets you specify the creation of the matrix G and S by calling the proper kernel constructor.\nF2F objects contain the following methods:\n• ff_fastfood(float* data, float* features) computes the features by using the real version of the complex feature map φ in Rahimi and Recht (2007), φ′(x) =\nn− 1 2 {cos([V x]j), sin([V x]j)}. SIMD vectorized instructions and cache locality are used to increase speed performance. To avoid a bottleneck in the basic trigonometric function computation a SSE2 sincos function3 is used. These allow an speed improvement of 18x times for a 224 dimension input matrix.\n• float ff_eval (float* data, float* weights) computes the inner product between the input vector and the weights."
    }, {
      "heading" : "4.3 Practical Use",
      "text" : "To illustrate how to embedded F2F in the code header definitions and examples are included in Appendix B.\nF2F can be embedded in front of a linear classifier. For instance, first doing feature extraction, then F2F and finally going through Linear SVM. Another example is to embedded F2F in a deep neural network architecture either as non linear mapping to the activation function, or as a separate Fastfood layer."
    }, {
      "heading" : "Appendix A: How to use FWHT",
      "text" : "If you are only interested in using FWHT and no other functionalities of this library, then enclose #include \"hpp/fast2food.hpp\" in your test file and consider the following usage guidelines."
    }, {
      "heading" : "Description",
      "text" : "fwht_opt computes in place the Fast Walsh Hadamard Transform of a given input vector."
    }, {
      "heading" : "Header",
      "text" : "void fwht_opt(float* data, unsigned long lgn)"
    }, {
      "heading" : "Parameters",
      "text" : "• data: input data vector\n• lgn: log2 input vector length"
    }, {
      "heading" : "Usage",
      "text" : "1 // In−p lace FWHT input vec to r with dimension dim 2 fwht opt ( data , l og2 (dim) ) ;\nIf you are interested in using the function for a n-input vector matrix, do as follows\n1 // In−p lace FWHT n−input vec to r matrix with dimension dim 2 for (unsigned long i = 0 ; i < n ; ++i ) 3 fwht opt ( data + i ∗ dim , log2 (dim) ) ;\nAppendix B: How to use F2F\nTo embedded F2F in the code, enclose #include \"hpp/fast2food_factory.hpp\" in your test file and consider the following usage guidelines.\nF2F Factory\nF2F Factory allows you to generate F2F objects according to your particular kernel choice."
    }, {
      "heading" : "Standard version",
      "text" : "The standard version is intended for single machine system.\nDescription\ncreateFast2food initializes the Fast2food object given the initialization parameters"
    }, {
      "heading" : "Header",
      "text" : "static Fast2food* createFast2food(Fast2foodType fast2foodType, float* data, const unsigned long nvec, const unsigned long dim, const unsigned long D, const float sigma = 1.0, const unsigned long t = 5);"
    }, {
      "heading" : "Parameters",
      "text" : "• fast2foodType: Fast2foodFactory::RBF or Fast2foodFactory::Matern\n• data: input data matrix\n• nvec: number of feature vectors\n• dim: feature dimension\n• D: number of basis kernel expansions\n• sigma: RBF sigma parameter\n• t: experimental Matern kernel parameter"
    }, {
      "heading" : "Class atributes",
      "text" : "• ff_nvec: number of input feature vectors, type unsigned long.\n• ff_dim: feature dimension, type unsigned long.\n• ff_dimpad: padded feature dimension, type unsigned long.\n• ff_D: integer part of the quotient between the input number of basis kernel expansions D and ff_dimpad, type unsigned long.\n• ff_dimD: number of kernel basis expansions defined as the product ff_D * ff_dimpad, type unsigned long.\n• ff_data: pointer input data matrix, type float.\n• ff_dataout: pointer internal data matrix, type float.\n• diag_Pi: random permutation diagonal matrix Π, type unsigned long vector.\n• diag_G: random gaussian diagonal matrix G, type float vector.\n• diag_S: scaling diagonal matrix S, type float vector.\n• diag_B: random diagonal matrix B with ±1 entries, type unsigned long vector."
    }, {
      "heading" : "Methods",
      "text" : "• ff_fastfood(): computes fastfood and stores the result in ff_dataout.\n• ff_features(): computes fastfood features and return the result in a float pointer."
    }, {
      "heading" : "Usage",
      "text" : "1 // Fast2food with Gaussian RBF ke rne l 2 Fast2food ∗ f a s t 2 f o od = 3 Fast2foodFactory : : c r ea t eFas t2 food ( Fast2foodFactory : :RBF, datain , nvec , dim , D, sigma ) ; 4 5 f loat ∗ da t a f e a t u r e s ; 6 f a s t2 f ood−> f f f a s t f o o d ( ) ; // f a s t f o od computation 7 da t a f e a t u r e s = fa s t2 f ood−> f f f e a t u r e s ( ) ; // complex map computation\n1 // Fast2food with Matern RBF ke rne l 2 Fast2food ∗ f a s t 2 f o od = 3 Fast2foodFactory : : c r ea t eFas t2 food ( Fast2foodFactory : : Matern , datain , nvec , dim , D, sigma , t ) ; 4 5 f loat ∗ da t a f e a t u r e s ; 6 f a s t2 f ood−> f f f a s t f o o d ( ) ; // f a s t f o od computation 7 da t a f e a t u r e s = fa s t2 f ood−> f f f e a t u r e s ( ) ; // complex map computation"
    }, {
      "heading" : "Distributed oriented version",
      "text" : "The distributed oriented version is intended to be embedded in a distributed system and allows to compute random numbers by the use of hashes."
    }, {
      "heading" : "Description",
      "text" : "createFast2food initializes the Fast2food object given the initialization parameters"
    }, {
      "heading" : "Header",
      "text" : "static Fast2food* createFast2food(Fast2foodType fast2foodType, float* data, const unsigned long nvec, const unsigned long dim, const unsigned long D, const unsigned long seed, const float sigma = 1.0, const unsigned long t = 5);"
    }, {
      "heading" : "Parameters",
      "text" : "• fast2foodType: Fast2foodFactory::RBF or Fast2foodFactory::Matern.\n• data: input data matrix\n• nvec: number of feature vectors\n• dim: feature dimension\n• D: number of basis kernel expansions\n• seed: seed to generate random distributions\n• sigma: RBF sigma parameter\n• t: experimental Matern kernel parameter"
    }, {
      "heading" : "Class atributes",
      "text" : "• ff_nvec: number of input feature vectors, type unsigned long.\n• ff_dim: feature dimension, type unsigned long.\n• ff_dimpad: padded feature dimension, type unsigned long.\n• ff_D: integer part of the quotient between the input number of basis kernel expansions D and ff_dimpad, type unsigned long.\n• ff_dimD: number of kernel basis expansions defined as the product ff_D * ff_dimpad, type unsigned long.\n• ff_data: pointer to input data matrix, type float.\n• ff_dataout: pointer to internal data matrix, type float.\n• diag_Pi: random permutation diagonal matrix Π, type unsigned long vector.\n• diag_G: random gaussian diagonal matrix G, type float vector.\n• diag_S: scaling diagonal matrix S, type float vector.\n• diag_B: random diagonal matrix B with ±1 entries, type unsigned long vector."
    }, {
      "heading" : "Methods",
      "text" : "• ff_fastfood(): computes fastfood and stores the result in ff_dataout.\n• ff_features(): computes fastfood features and return the result in a float pointer."
    }, {
      "heading" : "Usage",
      "text" : "1 // Fast2food with Gaussian RBF ke rne l 2 3 random device rd ; 4 5 // seed random d i s t r i b u t i o n s 6 const unsigned long seed = (unsigned long ) rd ( ) ; 7 8 Fast2food ∗ f a s t 2 f o od = 9 Fast2food ∗ f a s t 2 f o od = Fast2foodFactory : : c r ea t eFas t2 food ( Fast2foodFactory : :RBF\n, datain , nvec , dim , D, seed , sigma ) ; 10 11 f loat ∗ da t a f e a t u r e s ; 12 fa s t2 f ood−> f f f a s t f o o d ( ) ; // f a s t f o od computation 13 da t a f e a t u r e s = fa s t2 f ood−> f f f e a t u r e s ( ) ; // complex map computation\n1 // Fast2food with Matern RBF ke rne l 2 3 random device rd ; 4 5 // seed random d i s t r i b u t i o n s 6 const unsigned long seed = (unsigned long ) rd ( ) ; 7 8 Fast2food ∗ f a s t 2 f o od = 9 Fast2food ∗ f a s t 2 f o od = Fast2foodFactory : : c r ea t eFas t2 food ( Fast2foodFactory : :\nMatern , datain , nvec , dim , D, seed , sigma , t ) ; 10 11 f loat ∗ da t a f e a t u r e s ; 12 fa s t2 f ood−> f f f a s t f o o d ( ) ; // f a s t f o od computation 13 da t a f e a t u r e s = fa s t2 f ood−> f f f e a t u r e s ( ) ; // complex map computation"
    } ],
    "references" : [ {
      "title" : "A Note on the Generation of Random Normal Deviates",
      "author" : [ "G.E.P. Box", "Mervin E. Muller" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Box and Muller.,? \\Q1958\\E",
      "shortCiteRegEx" : "Box and Muller.",
      "year" : 1958
    }, {
      "title" : "In search of the optimal Walsh-Hadamard Transform",
      "author" : [ "Jeremy Johnson", "Markus Puschel" ],
      "venue" : null,
      "citeRegEx" : "Johnson and Puschel.,? \\Q2000\\E",
      "shortCiteRegEx" : "Johnson and Puschel.",
      "year" : 2000
    }, {
      "title" : "Fastfood - Approximating Kernel Expansions in Loglinear Time",
      "author" : [ "Quoc Le", "Tamas Sarlos", "Alex Smola" ],
      "venue" : null,
      "citeRegEx" : "Le et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2013
    }, {
      "title" : "Random Features for Large-Scale Kernel Machines",
      "author" : [ "Ali Rahimi", "Ben Recht" ],
      "venue" : null,
      "citeRegEx" : "Rahimi and Recht.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rahimi and Recht.",
      "year" : 2007
    }, {
      "title" : "General cost functions for support vector regression",
      "author" : [ "Alexander J. Smola", "Bernhard Schökopf", "Klaus Robert Müller" ],
      "venue" : "Ninth Australian Conference on Neural Networks,",
      "citeRegEx" : "Smola et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Smola et al\\.",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "It contains a CPU optimized implementation of the Fastfood algorithm in Le et al. (2013), that allows the computation of approximated kernel expansions in loglinear time.",
      "startOffset" : 72,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "In order to solve this problem, Le et al. (2013) proposed an approximation algorithm, called Fastfood, based on Random Kitchen Sinks by Rahimi and Recht (2007), that speeds up the computation of a large range of kernel functions, allowing us to use them in big data.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "In order to solve this problem, Le et al. (2013) proposed an approximation algorithm, called Fastfood, based on Random Kitchen Sinks by Rahimi and Recht (2007), that speeds up the computation of a large range of kernel functions, allowing us to use them in big data.",
      "startOffset" : 32,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "Random Kitchen Sinks (Rahimi and Recht, 2007) approximate this feature mapping φ by a Fourier expansion in the case of radial basis function kernels, i.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "This is possible since the Fourier transform diagonalizes the corresponding integral operator (Smola et al., 1988).",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "We generate the random variates using the Box-Muller transform Box and Muller (1958) while substituting the random number generator by calls to the hash function to allow us to recompute the values at any time without the need to store random numbers.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "• ff_fastfood(float* data, float* features) computes the features by using the real version of the complex feature map φ in Rahimi and Recht (2007), φ′(x) = n− 1 2 {cos([V x]j), sin([V x]j)}.",
      "startOffset" : 124,
      "endOffset" : 148
    } ],
    "year" : 2017,
    "abstractText" : "F2F is a C++ library for large-scale machine learning. It contains a CPU optimized implementation of the Fastfood algorithm in Le et al. (2013), that allows the computation of approximated kernel expansions in loglinear time. The algorithm requires to compute the product of Walsh-Hadamard Transform (WHT) matrices. A cache friendly SIMD Fast Walsh-Hadamard Transform (FWHT) that achieves compelling speed and outperforms current state-of-the-art methods has been developed. F2F allows to obtain non-linear classification combining Fastfood and a linear classifier.",
    "creator" : "LaTeX with hyperref package"
  }
}