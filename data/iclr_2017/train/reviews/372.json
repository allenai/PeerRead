{"conference": "ICLR 2017 conference submission", "title": "Learning to Remember Rare Events", "abstract": "Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision.  It operates in a life-long manner, i.e., without the need to reset it during training.  Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task.", "reviews": [{"is_meta_review": true, "comments": "The paper proposes a new memory module to be used as an addition to existing neural network models.\n\nPros:\n* Clearly written and original idea.\n* Useful memory module, shows nice improvements.\n* Tested on some big tasks.\n\nCons:\n* No comparisons to other memory modules such as associative LSTMs etc.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The primary contribution of this paper is showing that k-nearest-neighbor method based memory can be usefully incorporated in a variety of architectures and supervised learning tasks. The presentation is clear, and results are good. I like the synthetic task and analysis. For the Omniglot task, running and reporting results on the original splits used by Lake would be good, as the splits used in matching nets are considerably easier and result in ceiling effects.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Question about update memory during training", "OTHER_KEYS": "(anonymous)", "comments": "what does \"introduce some randomness in the choice so as to avoid race conditions in asynchronous multi-replica training\" mean specifically?\nIs there any reference paper to let the readers to get better understanding of it? Thank you very much. ", "IS_META_REVIEW": false, "DATE": "17 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "is_meta_review": false, "comments": "This paper proposes a new memory module for large scale life-long and one-shot learning. The module is general enough that the authors apply the module to several neural network architectures and show improvements in performance.\n\nUsing k-nearest neighbors for memory access is not completely new. This has been recently explored in Rae et al., 2016 and Chandar et al., 2016. K-nearest neighbors based memory for one-shot learning has also been explored in [R1]. This paper provides experimental evidence that such an approach can be applied to a variety of architectures.\n\nAuthors have addressed all my pre-review questions and I am ok with their response.\n\nAre the authors willing to release the source code to reproduce the results? At least for omniglot experiments and synthetic task experiments?\n\nReferences:\n\n[R1] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, Demis Hassabis: Model-Free Episodic Control. CoRR abs/1606.04460 (2016)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "20 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "No Title", "is_meta_review": false, "comments": "A new memory module based on k-NN is presented.\nThe paper is very well written and the results are convincing. \n\nOmniglot is a good sanity test and the performance is surprisingly good.\nThe artificial task shows us that the authors claims hold and highlight the need for better benchmarks in this domain.\nAnd the translation task eventually makes a very strong point on practical usefulness of the proposed model.\n\nI am not a specialist in memory networks so I trust the authors to double-check if all relevant references have been included (another reviewer mentioned associative LSTM). But besides that I think this is a very nice and useful paper. I hope the authors will publish their code.", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "interesting new memory module", "is_meta_review": false, "comments": "The paper proposes a new memory module to be used as an addition to existing neural network models.\n\nPros:\n* Clearly written and original idea.\n* Useful memory module, shows nice improvements.\n* Tested on some big tasks.\n\nCons:\n* No comparisons to other memory modules such as associative LSTMs etc.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "New Revision", "OTHER_KEYS": "Lukasz Kaiser", "comments": "We are very grateful for the reviewers' questions, and we uploaded a new revision that clarifies the paper to answer them.\n\nIn addition to the answers, we updated the definition of the memory module for the cases when no positive neighbour is found in the top-k. We now get any vector from memory instead of using all 0s, which prevents the loss from jumping in such cases. This improved our results on Omniglot, they are state-of-the-art now.", "IS_META_REVIEW": false, "DATE": "12 Dec 2016", "is_meta_review": false}, {"TITLE": "Few questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Context Set", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "memory module update", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Lukasz Kaiser, Ofir Nachum, Aurko Roy, Samy Bengio", "KEYWORDS": "We introduce a memory module for life-long learning that adds one-shot learning capability to any supervised neural network.", "accepted": true, "id": ""}
