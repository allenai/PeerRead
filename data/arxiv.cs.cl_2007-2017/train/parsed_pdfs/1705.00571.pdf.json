{
  "name" : "1705.00571.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines",
    "authors" : [ "Andrew Moore", "Paul Rayson" ],
    "emails" : [ "initial.surname@lancaster.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The objective of Task 5 Track 2 of SemEval (2017) was to predict the sentiment of news headlines with respect to companies mentioned within the headlines. This task can be seen as a financespecific aspect-based sentiment task (Nasukawa and Yi, 2003). The main motivations of this task is to find specific features and learning algorithms that will perform better for this domain as aspect based sentiment analysis tasks have been conducted before at SemEval (Pontiki et al., 2014).\nDomain specific terminology is expected to play a key part in this task, as reporters, investors and analysts in the financial domain will use a specific set of terminology when discussing financial performance. Potentially, this may also vary across different financial domains and industry sectors. Therefore, we took an exploratory approach and investigated how various features and learning algorithms perform differently, specifically SVR and BLSTMs. We found that BLSTMs outperform an SVR without having any knowledge of the company that the sentiment is with respect to. For replicability purposes, with this paper\nwe are releasing our source code1 and the finance specific BLSTM word embedding model2."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is a growing amount of research being carried out related to sentiment analysis within the financial domain. This work ranges from domainspecific lexicons (Loughran and McDonald, 2011) and lexicon creation (Moore et al., 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016). Peng and Jiang (2016) used a multi layer neural network to predict the stock market and found that incorporating textual features from financial news can improve the accuracy of prediction. Kazemian et al. (2016) showed the importance of tuning sentiment analysis to the task of stock market prediction. However, much of the previous work was based on numerical financial stock market data rather than on aspect level financial textual data.\nIn aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016 task 5 (Pontiki et al., 2014). The winning system (Brun et al., 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al., 2016) used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Deep learning methods have also been applied to aspect polarity prediction. Ruder et al. (2016) created a hierarchical BLSTM with a sentence level BLSTM inputting into a review level BLSTM thus allowing them to take into account inter- and intra-sentence context. They used only word embeddings making their system less dependent on extensive feature engineering or manual feature creation. This system outperformed all others on certain languages\n1https://github.com/apmoore1/semeval 2https://github.com/apmoore1/semeval/\ntree/master/models/word2vec_models\nar X\niv :1\n70 5.\n00 57\n1v 1\n[ cs\n.C L\n] 1\nM ay\n2 01\n7\non the SemEval-2016 task 5 dataset (Pontiki et al., 2014) and on other languages performed close to the best systems. Wang et al. (2016) also created an LSTM based model using word embeddings but instead of a hierarchical model it was a one layered LSTM with attention which puts more emphasis on learning the sentiment of words specific to a given aspect."
    }, {
      "heading" : "3 Data",
      "text" : "The training data published by the organisers for this track was a set of headline sentences from financial news articles where each sentence was tagged with the company name (which we treat as the aspect) and the polarity of the sentence with respect to the company. There is the possibility that the same sentence occurs more than once if there is more than one company mentioned. The polarity was a real value between -1 (negative sentiment) and 1 (positive sentiment).\nWe additionally trained a word2vec (Mikolov et al., 2013) word embedding model3 on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva4. The articles stem from a range of sources including the Financial Times and relate to companies from the United States only. We trained the model on domain specific data as it has been shown many times that the financial domain can contain very different language."
    }, {
      "heading" : "4 System description",
      "text" : "Even though we have outlined this task as an aspect based sentiment task, this is instantiated in only one of the features in the SVR. The following two subsections describe the two approaches, first SVR and then BLSTM. Key implementation details are exposed here in the paper, but we have released the source code and word embedding models to aid replicability and further experimentation."
    }, {
      "heading" : "4.1 SVR",
      "text" : "The system was created using ScitKit learn (Pedregosa et al., 2011) linear Support Vector Regression model (Drucker et al., 1997). We exper-\n3For reproducibility, the model can be downloaded, however the articles cannot be due to copyright and licence restrictions.\n4https://global.factiva.com/ factivalogin/login.asp?productname= global\nimented with the following different features and parameter settings:"
    }, {
      "heading" : "4.1.1 Tokenisation",
      "text" : "For comparison purposes, we tested whether or not a simple whitespace tokeniser can perform just as well as a full tokeniser, and in this case we used Unitok5."
    }, {
      "heading" : "4.1.2 N-grams",
      "text" : "We compared word-level uni-grams and bi-grams separately and in combination."
    }, {
      "heading" : "4.1.3 SVR parameters",
      "text" : "We tested different penalty parameters C and different epsilon parameters of the SVR."
    }, {
      "heading" : "4.1.4 Word Replacements",
      "text" : "We tested replacements to see if generalising words by inserting special tokens would help to reduce the sparsity problem. We placed the word replacements into three separate groups:\n1. Company - When a company was mentioned in the input headline from the list of companies in the training data marked up as aspects, it was replaced by a company special token.\n2. Positive - When a positive word was mentioned in the input headline from a list of positive words (which was created using the N most similar words based on cosine distance) to ‘excellent’ using the pre-trained word2vec model.\n3. Negative - The same as the positive group however the word used was ‘poor’ instead of ‘excellent’.\nIn the positive and negative groups, we chose the words ‘excellent’ and ‘poor’ following Turney (2002) to group the terms together under nondomain specific sentiment words."
    }, {
      "heading" : "4.1.5 Target aspect",
      "text" : "In order to incorporated the company as an aspect, we employed a boolean vector to represent the sentiment of the sentence. This was done in order to see if the system could better differentiate the sentiment when the sentence was the same but the company was different.\n5http://corpus.tools/wiki/Unitok"
    }, {
      "heading" : "4.2 BLSTM",
      "text" : "We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al., 2016). We choose an LSTM model as it solves the vanishing gradients problem of Recurrent Neural Networks. We used a bidirectional model as it allows us to capture information that came before and after instead of just before, thereby allowing us to capture more relevant context within the model. Practically, a BLSTM is two LSTMs one going forward through the tokens the other in reverse order and in our models concatenating the resulting output vectors together at each time step.\nThe BLSTM models take as input a headline sentence of size L tokens6 where L is the length of the longest sentence in the training texts. Each word is converted into a 300 dimension vector using the word2vec model trained over the financial text7. Any text that is not recognised by the word2vec model is represented as a vector of zeros; this is also used to pad out the sentence if it is shorter than L.\nBoth BLSTM models have the following similar properties:\n1. Gradient clipping value of 5 - This was to help with the exploding gradients problem.\n2. Minimised the Mean Square Error (MSE) loss using RMSprop with a mini batch size of 32.\n3. The output activation function is linear.\nThe main difference between the two models is the use of drop out and when they stop training over the data (epoch). Both models architectures can be seen in figure 1."
    }, {
      "heading" : "4.2.1 Standard LSTM (SLSTM)",
      "text" : "The BLSTMs do contain drop out in both the input and between the connections of 0.2 each. Finally the epoch is fixed at 25."
    }, {
      "heading" : "4.2.2 Early LSTM (ELSTM)",
      "text" : "As can be seen from figure 1, the drop out of 0.5 only happens between the layers and not the\n6Tokenised by Unitok 7See the following link for detailed implementation details https://github.com/apmoore1/semeval# finance-word2vec-model\nconnections as in the SLSTM. Also the epoch is not fixed, it uses early stopping with a patience of 10. We expect that this model can generalise better than the standard one due to the higher drop out and that the epoch is based on early stopping which relies on a validation set to know when to stop training."
    }, {
      "heading" : "5 Results",
      "text" : "We first present our findings on the best performing parameters and features for the SVRs. These were determined by cross validation (CV) scores on the provided training data set using cosine similarity as the evaluation metric.8 We found that using uni-grams and bi-grams performs best and using only bi-grams to be the worst. Using the Unitok tokeniser always performed better than simple whitespace tokenisation. The binary presence of tokens over frequency did not alter performance.\n8All the cross validation results can be found here https://github.com/apmoore1/semeval/ tree/master/results\nThe C parameter was tested for three values; 0.01, 0.1 and 1. We found very little difference between 0.1 and 1, but 0.01 produced much poorer results. The eplison parameter was tested for 0.001, 0.01 and 0.1 the performance did not differ much but the lower the higher the performance but the more likely to overfit. Using word replacements was effective for all three types (company, positive and negative) but using a value N=10 performed best for both positive and negative words. Using target aspects also improved results. Therefore, the best SVR model comprised of: Unitok tokenisation, uni- and bi- grams, word representation, C=0.1, eplison=0.01, company, positive, and negative word replacements and target aspects.∑N\nn=1 Cosine similarity(ŷn, yn) N\n(1)\nThe main evaluation over the test data is based on the best performing SVR and the two BLSTM models once trained on all of the training data. The result table 1 shows three columns based on the three evaluation metrics that the organisers have used. Metric 1 is the original metric, weighted cosine similarity (the metric used to evaluate the final version of the results, where we were ranked 5th; metric provided on the task website9). This was then changed after the evaluation deadline to equation 110 (which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in Cortis et al. (2017) (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th).\nAs you can see from the results table 1, the difference between the metrics is quite substantial. This is due to the system’s optimisation being based on metric 1 rather than 2. Metric 2 is a classification metric for sentences with one aspect as\n9http://alt.qcri.org/semeval2017/ task5/index.php?id=evaluation\n10Where N is the number of unique sentences, ŷn is the predicted and yn are the true sentiment value(s) of all sentiments in sentence n.\nit penalises values that are of opposite sign (giving -1 score) and rewards values with the same sign (giving +1 score). Our systems are not optimised for this because it would predict scores of -0.01 and true value of 0.01 as very close (within vector of other results) with low error whereas metric 2 would give this the highest error rating of -1 as they are not the same sign. Metric 3 is more similar to metric 1 as shown by the results, however the crucial difference is that again if you get opposite signs it will penalise more.\nWe analysed the top 50 errors based on Mean Absolute Error (MAE) in the test dataset specifically to examine the number of sentences containing more than one aspect. Our investigation shows that no one system is better at predicting the sentiment of sentences that have more than one aspect (i.e. company) within them. Within those top 50 errors we found that the BLSTM systems do not know which parts of the sentence are associated to the company the sentiment is with respect to. Also they do not know the strength/existence of certain sentiment words."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this short paper, we have described our implemented solutions to SemEval Task 5 track 2, utilising both SVR and BLSTM approaches. Our results show an improvement of around 5% when using LSTM models relative to SVR. We have shown that this task can be partially represented as an aspect based sentiment task on a domain specific problem. In general, our approaches acted as sentence level classifiers as they take no target company into consideration. As our results show, the choice of evaluation metric makes a great deal of difference to system training and testing. Future work will be to implement aspect specific information into an LSTM model as it has been shown to be useful in other work (Wang et al., 2016)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We are grateful to Nikolaos Tsileponis (University of Manchester) and Mahmoud El-Haj (Lancaster University) for access to headlines in the corpus of financial news articles collected from Factiva. This research was supported at Lancaster University by an EPSRC PhD studentship."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
      "author" : [ "Martı́n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin" ],
      "venue" : null,
      "citeRegEx" : "Abadi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Abadi et al\\.",
      "year" : 2016
    }, {
      "title" : "Xrce at semeval-2016 task 5: Feedbacked ensemble modelling on syntactico-semantic knowledge for aspect based sentiment analysis",
      "author" : [ "Caroline Brun", "Julien Perez", "Claude Roux." ],
      "venue" : "Proceedings of SemEval pages 277–281.",
      "citeRegEx" : "Brun et al\\.,? 2016",
      "shortCiteRegEx" : "Brun et al\\.",
      "year" : 2016
    }, {
      "title" : "Keras",
      "author" : [ "François Chollet." ],
      "venue" : "https://github. com/fchollet/keras.",
      "citeRegEx" : "Chollet.,? 2015",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2015
    }, {
      "title" : "Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news",
      "author" : [ "Keith Cortis", "Andre Freitas", "Tobias Daudert", "Manuela Huerlimann", "Manel Zarrouk", "Brian Davis." ],
      "venue" : "Proceedings of SemEval .",
      "citeRegEx" : "Cortis et al\\.,? 2017",
      "shortCiteRegEx" : "Cortis et al\\.",
      "year" : 2017
    }, {
      "title" : "Support vector regression machines. Advances in neural information processing systems 9:155–161",
      "author" : [ "Harris Drucker", "Christopher JC Burges", "Linda Kaufman", "Alex Smola", "Vladimir Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Drucker et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Drucker et al\\.",
      "year" : 1997
    }, {
      "title" : "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "author" : [ "Alex Graves", "Jürgen Schmidhuber." ],
      "venue" : "Neural Networks 18(5):602–610.",
      "citeRegEx" : "Graves and Schmidhuber.,? 2005",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2005
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Evaluating sentiment analysis in the context of securities trading",
      "author" : [ "Siavash Kazemian", "Shunan Zhao", "Gerald Penn." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association",
      "citeRegEx" : "Kazemian et al\\.,? 2016",
      "shortCiteRegEx" : "Kazemian et al\\.",
      "year" : 2016
    }, {
      "title" : "Iit-tuda at semeval-2016 task 5: Beyond sentiment lexicon: Combining domain dependency and distributional semantics features for aspect based sentiment analysis",
      "author" : [ "Ayush Kumar", "Sarah Kohail", "Amit Kumar", "Asif Ekbal", "Chris Biemann." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Kumar et al\\.,? 2016",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2016
    }, {
      "title" : "When is a liability not a liability? textual analysis, dictionaries, and 10-ks",
      "author" : [ "Tim Loughran", "Bill McDonald." ],
      "venue" : "The Journal of Finance 66(1):35–65.",
      "citeRegEx" : "Loughran and McDonald.,? 2011",
      "shortCiteRegEx" : "Loughran and McDonald.",
      "year" : 2011
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781 .",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Domain adaptation using stock market prices to refine sentiment dictionaries",
      "author" : [ "Andrew Moore", "Paul Rayson", "Steven Young." ],
      "venue" : "Proceedings of the 10th edition of Language Resources and Evaluation",
      "citeRegEx" : "Moore et al\\.,? 2016",
      "shortCiteRegEx" : "Moore et al\\.",
      "year" : 2016
    }, {
      "title" : "Sentiment analysis: Capturing favorability using natural language processing",
      "author" : [ "Tetsuya Nasukawa", "Jeonghee Yi." ],
      "venue" : "Proceedings of the 2nd international conference on Knowledge capture. ACM, pages 70–77.",
      "citeRegEx" : "Nasukawa and Yi.,? 2003",
      "shortCiteRegEx" : "Nasukawa and Yi.",
      "year" : 2003
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg" ],
      "venue" : "Journal of Machine",
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Leverage financial news to predict stock price movements using word embeddings and deep neural networks",
      "author" : [ "Yangtuo Peng", "Hui Jiang." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Peng and Jiang.,? 2016",
      "shortCiteRegEx" : "Peng and Jiang.",
      "year" : 2016
    }, {
      "title" : "Semeval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "Proceedings of SemEval pages 27–35.",
      "citeRegEx" : "Pontiki et al\\.,? 2014",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "A hierarchical model of reviews for aspect-based sentiment analysis",
      "author" : [ "Sebastian Ruder", "Parsa Ghaffari", "G. John Breslin." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Ruder et al\\.,? 2016",
      "shortCiteRegEx" : "Ruder et al\\.",
      "year" : 2016
    }, {
      "title" : "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews",
      "author" : [ "Peter D Turney." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics,",
      "citeRegEx" : "Turney.,? 2002",
      "shortCiteRegEx" : "Turney.",
      "year" : 2002
    }, {
      "title" : "Attention-based lstm for aspectlevel sentiment classification",
      "author" : [ "Yequan Wang", "Minlie Huang", "xiaoyan zhu", "Li Zhao" ],
      "venue" : "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "This task can be seen as a financespecific aspect-based sentiment task (Nasukawa and Yi, 2003).",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "The main motivations of this task is to find specific features and learning algorithms that will perform better for this domain as aspect based sentiment analysis tasks have been conducted before at SemEval (Pontiki et al., 2014).",
      "startOffset" : 207,
      "endOffset" : 229
    }, {
      "referenceID" : 9,
      "context" : "This work ranges from domainspecific lexicons (Loughran and McDonald, 2011) and lexicon creation (Moore et al.",
      "startOffset" : 46,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "This work ranges from domainspecific lexicons (Loughran and McDonald, 2011) and lexicon creation (Moore et al., 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al.",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : ", 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016).",
      "startOffset" : 42,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : ", 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016).",
      "startOffset" : 42,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : ", 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016). Peng and Jiang (2016) used a multi layer neural network to predict the stock market and found that incorporating textual features from financial news can improve the accuracy of prediction.",
      "startOffset" : 65,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : ", 2016) to stock market prediction models (Peng and Jiang, 2016; Kazemian et al., 2016). Peng and Jiang (2016) used a multi layer neural network to predict the stock market and found that incorporating textual features from financial news can improve the accuracy of prediction. Kazemian et al. (2016) showed the importance of tuning sentiment analysis to the task of stock market prediction.",
      "startOffset" : 65,
      "endOffset" : 302
    }, {
      "referenceID" : 15,
      "context" : "In aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016 task 5 (Pontiki et al., 2014).",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "The winning system (Brun et al., 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : ", 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al., 2016) used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM).",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "The winning system (Brun et al., 2016) used many different linguistic features and an ensemble model, and the runner up (Kumar et al., 2016) used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Deep learning methods have also been applied to aspect polarity prediction. Ruder et al. (2016) created a hierarchical BLSTM with a sentence level BLSTM inputting into a review level BLSTM thus allowing them to take into account inter- and intra-sentence context.",
      "startOffset" : 20,
      "endOffset" : 333
    }, {
      "referenceID" : 15,
      "context" : "on the SemEval-2016 task 5 dataset (Pontiki et al., 2014) and on other languages performed close to the best systems.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "on the SemEval-2016 task 5 dataset (Pontiki et al., 2014) and on other languages performed close to the best systems. Wang et al. (2016) also created an LSTM based model using word embeddings but instead of a hierarchical model it was a one layered LSTM with attention which puts more emphasis on learning the sentiment of words specific to a given aspect.",
      "startOffset" : 36,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "We additionally trained a word2vec (Mikolov et al., 2013) word embedding model3 on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva4.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "The system was created using ScitKit learn (Pedregosa et al., 2011) linear Support Vector Regression model (Drucker et al.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : ", 2011) linear Support Vector Regression model (Drucker et al., 1997).",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "In the positive and negative groups, we chose the words ‘excellent’ and ‘poor’ following Turney (2002) to group the terms together under nondomain specific sentiment words.",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al.",
      "startOffset" : 39,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al.",
      "startOffset" : 93,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al.",
      "startOffset" : 159,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "We created two different Bidirectional (Graves and Schmidhuber, 2005) Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) using the Python Keras library (Chollet, 2015) with tensor flow backend (Abadi et al., 2016).",
      "startOffset" : 200,
      "endOffset" : 220
    }, {
      "referenceID" : 3,
      "context" : "This was then changed after the evaluation deadline to equation 110 (which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in Cortis et al. (2017) (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th).",
      "startOffset" : 261,
      "endOffset" : 282
    }, {
      "referenceID" : 18,
      "context" : "Future work will be to implement aspect specific information into an LSTM model as it has been shown to be useful in other work (Wang et al., 2016).",
      "startOffset" : 128,
      "endOffset" : 147
    } ],
    "year" : 2017,
    "abstractText" : "This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between -1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.",
    "creator" : "LaTeX with hyperref package"
  }
}