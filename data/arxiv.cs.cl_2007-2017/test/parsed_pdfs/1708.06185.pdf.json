{
  "name" : "1708.06185.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Seernet at EmoInt-2017: Tweet Emotion Intensity Estimator",
    "authors" : [ "Venkatesh Duppada" ],
    "emails" : [ "sushant.hiray}@seernet.io" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Twitter, a micro-blogging and social networking site has emerged as a platform where people express themselves and react to events in real-time. It is estimated that nearly 500 million tweets are sent per day 1. Twitter data is particularly interesting because of its peculiar nature where people convey messages in short sentences using hashtags, emoticons, emojis etc. In addition, each tweet has meta data like location and language used by the sender. It’s challenging to analyze this data because the tweets might not be grammatically correct and the users tend to use informal and slang words all the time. Hence, this poses an interesting problem for NLP researchers. Any advances in using this abundant and diverse data can help understand and analyze information about a person, an event, a product, an organization or a country as a whole. Many notable use cases of the twitter can be found here2.\nAlong the similar lines, The Task 1 of WASSA2017 (Mohammad and Bravo-Marquez, 2017c) poses a problem of finding emotion intensity of\n1https://en.wikipedia.org/wiki/Twitter 2https://en.wikipedia.org/wiki/\nTwitter_usage\nfour emotions namely anger, fear, joy, sadness from tweets. In this paper, we describe our approach and experiments to solve this problem. The rest of the paper is laid out as follows: Section 2 describes the system architecture, Section 3 reports results and inference from different experiments, while Section 4 points to ways that the problem can be further explored."
    }, {
      "heading" : "2 System Description",
      "text" : ""
    }, {
      "heading" : "2.1 Preprocessing",
      "text" : "The preprocessing step modifies the raw tweets before they are passed to feature extraction. Tweets are processed using tweetokenize tool3. Twitter specific features are replaced as follows: username handles to USERNAME, phone numbers to PHONENUMBER, numbers to NUMBER, URLs to URL and times to TIME. A continuous sequence of emojis is broken into individual tokens. Finally, all tokens are converted to lowercase."
    }, {
      "heading" : "2.2 Feature Extraction",
      "text" : "Many tasks related to sentiment or emotion analysis depend upon affect, opinion, sentiment, sense and emotion lexicons. These lexicons associate words to corresponding sentiment or emotion metrics. On the other hand, the semantic meaning of words, sentences, and documents are preserved\n3https://www.github.com/jaredks/ tweetokenize\nar X\niv :1\n70 8.\n06 18\n5v 1\n[ cs\n.C L\n] 2\n1 A\nug 2\n01 7\nand compactly represented using low dimensional vectors (Mikolov et al., 2013) instead of one hot encoding vectors which are sparse and high dimensional. Finally, there are traditional NLP features like word N-grams, character N-grams, PartOf-Speech N-grams and word clusters which are known to perform well on various tasks.\nBased on these observations, the feature extraction step is implemented as a union of different independent feature extractors (featurizers) in a light-weight and easy to use Python program EmoInt 4. It comprises of all features available in the baseline model (Mohammad and BravoMarquez, 2017a) 5 along with additional feature extractors and bi-gram support. Fourteen such feature extractors have been implemented which can be clubbed into 3 major categories:\n• Lexicon Features • Word Vectors • Syntax Features\nLexicon Features: AFINN (Nielsen, 2011) word list are manually rated for valence with an integer between -5 (Negative Sentiment) and +5 (Positive Sentiment). Bing Liu (Hu and Liu, 2004) opinion lexicon extract opinion on customer reviews. +/-EffectWordNet (Choi and Wiebe, 2014) by MPQA group are sense level lexicons. The NRC Affect Intensity (Mohammad, 2017) lexicons provide real valued affect intensity. NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2010) contains 8 sense level associations (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and 2 sentiment level associations (negative and positive). Expanded NRC Word-Emotion Association Lexicon (BravoMarquez et al., 2016) expands the NRC wordemotion association lexicon for twitter specific language. NRC Hashtag Emotion Lexicon (Mohammad and Kiritchenko, 2015) contains emotion word associations computed on emotion labeled twitter corpus via Hashtags. NRC Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Mohammad et al., 2013) contains sentiment word associations computed on twitter corpus via Hashtags and Emoticons. SentiWordNet (Baccianella et al., 2010) assigns to each synset of WordNet\n4To enable replicability, the code is open sourced at https://github.com/SEERNET/EmoInt.\n5https://www.github.com/felipebravom/ AffectiveTweets\nthree sentiment scores: positivity, negativity, objectivity. Negation lexicons collections are used to count the total occurrence of negative words. In addition to these, SentiStrength (Thelwall et al., 2010) application which estimates the strength of positive and negative sentiment from tweets is also added.\nWord Vectors: We focus primarily on the word vector representations (word embeddings) created specifically using the twitter dataset. GloVe (Pennington et al., 2014) is an unsupervised learning algorithm for obtaining vector representations for words. 200-dimensional GloVe embeddings trained on 2 Billion tweets are integrated. Edinburgh embeddings (Bravo-Marquez et al., 2015) are obtained by training skip-gram model on Edinburgh corpus (Petrovic et al., 2010). Since tweets are abundant with emojis, Emoji embeddings (Eisner et al., 2016) which are learned from the emoji descriptions have been used. Embeddings for each tweet are obtained by summing up individual word vectors and then dividing by the number of tokens in the tweet.\nSyntactic Features: Syntax specific features such as Word N-grams, Part-Of-Speech N-grams (Owoputi et al., 2013), Brown Cluster N-grams (Brown et al., 1992) obtained using TweetNLP 6 project have been integrated into the system. The final feature vector is the concatenation of all the individual features. For example, we concatenate average word vectors, sum of NRC Affect Intensities, number of positive and negative Bing Liu lexicons, number of negation words and so on to get final feature vector. The scaling of final features is not required when used with gradient boosted trees. However, scaling steps like standard scaling (zero mean and unit normal) may be beneficial for neural networks as the optimizers work well when the data is centered around origin.\nA total of fourteen different feature extractors have been implemented, all of which can be enabled or disabled individually to extract features from a given tweet."
    }, {
      "heading" : "2.3 Regression",
      "text" : "The dev data set (Mohammad and BravoMarquez, 2017b) in the competition was small hence, the train and dev sets were merged to perform 10-fold cross validation. On each fold, a model was trained and the predictions were col-\n6http://www.cs.cmu.edu/˜ark/TweetNLP/\nlected on the remaining dataset. The predictions are averaged across all the folds to generalize the solution and prevent over-fitting. As described in Section 2.2, different combinations of feature extractors were used. After performing feature extraction, the data was then passed to various regressors Support Vector Regression, AdaBoost, RandomForestRegressor, and, BaggingRegressor of sklearn (Pedregosa et al., 2011). Finally, the chosen top performing models had the least error on evaluation metrics namely Pearson’s Correlation Coefficient and Spearman’s rank-order correlation."
    }, {
      "heading" : "2.4 Parameter Optimization",
      "text" : "In order to find the optimal parameter values for the EmoInt system, an extensive grid search was performed through the scikit-Learn framework over all subsets of the training set (shuffled), using stratified 10-fold cross validation and optimizing the Pearson’s Correlation score. Best cross-validation results were obtained using AdaBoost meta regressor with base regressor as XGBoost (Chen and Guestrin, 2016) with 1000 estimators and 0.1 learning rate. Experiments and analysis of results are presented in the next section."
    }, {
      "heading" : "3 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental Results",
      "text" : "As described in Section 2.2 various syntax features were used namely, Part-of-Speech tags, brown clusters of TweetNLP project. However, these didn’t perform well in cross validation. Hence, they were dropped from the final system. While performing grid-search as mentioned in Section 2.4, keeping all the lexicon based features same, choice of combination of emoji vector and word vectors are varied to minimize cross validation metric. Table 1 describes the results for experiments conducted with different combinations of word vectors. Emoji embeddings (Eisner et al., 2016) give better results than using plain GloVe and Edinburgh embeddings. Edinburgh embeddings outperform GloVe embeddings in Joy and Sadness category but lag behind in Anger and Fear category. The official submission comprised of the top-performing model for each emotion category. This system ranked 3rd for the entire test dataset and 2nd for the subset of the test data formed by taking every instance with a gold emo-\ntion intensity score greater than or equal to 0.5. Post competition, experiments were performed on ensembling diverse models for improving the accuracy. An ensemble obtained by averaging the results of the top 2 performing models outperforms all the individual models."
    }, {
      "heading" : "3.2 Feature Importance",
      "text" : "The relative feature importance can be assessed by the relative depth of the feature used as a decision node in the tree. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. By averaging the measure over several randomized trees, the variance of the estimate can be reduced and used as a measure of relative feature importance. In Figure 2 feature importance graphs are plotted for each emotion to infer which features are playing the major role in identifying emotional intensity in tweets. +/-EffectWordNet (Choi and Wiebe, 2014), NRC Hashtag Sentiment Lexicon, Sentiment140 Lexicon (Mohammad et al., 2013) and NRC Hashtag Emotion Lexicon (Mohammad and Kiritchenko, 2015) are playing the most important role."
    }, {
      "heading" : "3.3 System Limitations",
      "text" : "It is important to understand how the model performs in different scenarios. Table 2 analyzes when the system performs the best and worst for each emotion. Since the features used are mostly lexicon based, the system has difficulties in capturing the overall sentiment and it leads to amplifying or vanishing intensity signals. For instance, in example 4 of fear louder and shaking lexicons imply fear but overall sentence doesn’t imply fear. A similar pattern can be found in the 4th example of Anger and 3rd example of Joy. The system has difficulties in understanding of sarcastic tweets, for instance, in the 3rd tweet of Anger the user expressed anger but used lol which is used in a positive sense most of the times and hence the system did a bad job at predicting intensity. The system also fails in predicting sentences having deeper emotion and sentiment which humans can understand with a little context. For example, in sample 4 of sadness, the tweet refers to post travel blues which humans can understand. But with little context, it is difficult for the system to accurately estimate the intensity. The performance is\npoor with very short sentences as there are fewer indicators to provide a reasonable estimate."
    }, {
      "heading" : "4 Future Work & Conclusion",
      "text" : "The paper studies the effectiveness of various affect lexicons word embeddings to estimate emotional intensity in tweets. A light-weight easy to use affect computing framework (EmoInt) to facilitate ease of experimenting with various lexicon features for text tasks is open-sourced. It provides plug and play access to various feature extractors and handy scripts for creating ensembles.\nFew problems explained in the analysis section can be resolved with the help of sentence embeddings which take the context information into consideration. The features used in the system are generic enough to use them in other affective computing tasks on social media text, not just tweet data. Another interesting feature of lexicon-based systems is their good run-time performance during prediction, future work to benchmark the performance of the system can prove vital for deploying in a real-world setting."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We would like to thank the organizers of the WASSA-2017 Shared Task on Emotion Intensity, for providing the data, the guidelines and timely support."
    } ],
    "references" : [ {
      "title" : "Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining",
      "author" : [ "Stefano Baccianella", "Andrea Esuli", "Fabrizio Sebastiani" ],
      "venue" : "In LREC",
      "citeRegEx" : "Baccianella et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Baccianella et al\\.",
      "year" : 2010
    }, {
      "title" : "Determining word–emotion associations from tweets by multilabel classification",
      "author" : [ "Felipe Bravo-Marquez", "Eibe Frank", "Saif M Mohammad", "Bernhard Pfahringer." ],
      "venue" : "WI’16. IEEE Computer Society, pages 536–539.",
      "citeRegEx" : "Bravo.Marquez et al\\.,? 2016",
      "shortCiteRegEx" : "Bravo.Marquez et al\\.",
      "year" : 2016
    }, {
      "title" : "From unlabelled tweets to twitterspecific opinion words",
      "author" : [ "Felipe Bravo-Marquez", "Eibe Frank", "Bernhard Pfahringer." ],
      "venue" : "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM,",
      "citeRegEx" : "Bravo.Marquez et al\\.,? 2015",
      "shortCiteRegEx" : "Bravo.Marquez et al\\.",
      "year" : 2015
    }, {
      "title" : "Class-based n-gram models of natural language",
      "author" : [ "Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai." ],
      "venue" : "Computational linguistics 18(4):467–479.",
      "citeRegEx" : "Brown et al\\.,? 1992",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1992
    }, {
      "title" : "Xgboost: A scalable tree boosting system",
      "author" : [ "Tianqi Chen", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, pages 785–794.",
      "citeRegEx" : "Chen and Guestrin.,? 2016",
      "shortCiteRegEx" : "Chen and Guestrin.",
      "year" : 2016
    }, {
      "title" : "+/effectwordnet: Sense-level lexicon acquisition for opinion inference",
      "author" : [ "Yoonjung Choi", "Janyce Wiebe." ],
      "venue" : "EMNLP. pages 1181–1191.",
      "citeRegEx" : "Choi and Wiebe.,? 2014",
      "shortCiteRegEx" : "Choi and Wiebe.",
      "year" : 2014
    }, {
      "title" : "emoji2vec: Learning emoji representations from their description",
      "author" : [ "Ben Eisner", "Tim Rocktäschel", "Isabelle Augenstein", "Matko Bošnjak", "Sebastian Riedel." ],
      "venue" : "arXiv preprint arXiv:1609.08359 .",
      "citeRegEx" : "Eisner et al\\.,? 2016",
      "shortCiteRegEx" : "Eisner et al\\.",
      "year" : 2016
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proceedings of the tenth",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Word affect intensities",
      "author" : [ "Saif M Mohammad." ],
      "venue" : "arXiv preprint arXiv:1704.08798 .",
      "citeRegEx" : "Mohammad.,? 2017",
      "shortCiteRegEx" : "Mohammad.",
      "year" : 2017
    }, {
      "title" : "Emotion intensities in tweets",
      "author" : [ "Saif M. Mohammad", "Felipe Bravo-Marquez" ],
      "venue" : null,
      "citeRegEx" : "Mohammad and Bravo.Marquez.,? \\Q2017\\E",
      "shortCiteRegEx" : "Mohammad and Bravo.Marquez.",
      "year" : 2017
    }, {
      "title" : "Emotion intensities in tweets",
      "author" : [ "Saif M. Mohammad", "Felipe Bravo-Marquez." ],
      "venue" : "Proceedings of the sixth joint conference on lexical and computational semantics (*Sem). Vancouver, Canada.",
      "citeRegEx" : "Mohammad and Bravo.Marquez.,? 2017b",
      "shortCiteRegEx" : "Mohammad and Bravo.Marquez.",
      "year" : 2017
    }, {
      "title" : "Wassa-2017 shared task on emotion intensity",
      "author" : [ "Saif M. Mohammad", "Felipe Bravo-Marquez." ],
      "venue" : "EMNLP 2017 Workshop on Computational Approaches to Subjectivity, Sentiment, and Social Media (WASSA), Copenhagen, Denmark.",
      "citeRegEx" : "Mohammad and Bravo.Marquez.,? 2017c",
      "shortCiteRegEx" : "Mohammad and Bravo.Marquez.",
      "year" : 2017
    }, {
      "title" : "Using hashtags to capture fine emotion categories from tweets",
      "author" : [ "Saif M Mohammad", "Svetlana Kiritchenko." ],
      "venue" : "Computational Intelligence 31(2):301–326.",
      "citeRegEx" : "Mohammad and Kiritchenko.,? 2015",
      "shortCiteRegEx" : "Mohammad and Kiritchenko.",
      "year" : 2015
    }, {
      "title" : "Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets",
      "author" : [ "Saif M Mohammad", "Svetlana Kiritchenko", "Xiaodan Zhu." ],
      "venue" : "arXiv preprint arXiv:1308.6242 .",
      "citeRegEx" : "Mohammad et al\\.,? 2013",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2013
    }, {
      "title" : "Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon",
      "author" : [ "Saif M Mohammad", "Peter D Turney." ],
      "venue" : "Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and gen-",
      "citeRegEx" : "Mohammad and Turney.,? 2010",
      "shortCiteRegEx" : "Mohammad and Turney.",
      "year" : 2010
    }, {
      "title" : "A new anew: Evaluation of a word list for sentiment analysis in microblogs",
      "author" : [ "Finn Årup Nielsen." ],
      "venue" : "arXiv preprint arXiv:1103.2903 .",
      "citeRegEx" : "Nielsen.,? 2011",
      "shortCiteRegEx" : "Nielsen.",
      "year" : 2011
    }, {
      "title" : "Improved part-of-speech tagging for online conversational text with word",
      "author" : [ "Olutobi Owoputi", "Brendan O’Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A Smith" ],
      "venue" : null,
      "citeRegEx" : "Owoputi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Owoputi et al\\.",
      "year" : 2013
    }, {
      "title" : "Scikit-learn: Machine learning",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP. volume 14, pages 1532– 1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "The edinburgh twitter corpus",
      "author" : [ "Sasa Petrovic", "Miles Osborne", "Victor Lavrenko." ],
      "venue" : "Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media. pages 25–26.",
      "citeRegEx" : "Petrovic et al\\.,? 2010",
      "shortCiteRegEx" : "Petrovic et al\\.",
      "year" : 2010
    }, {
      "title" : "Sentiment strength detection in short informal text",
      "author" : [ "Mike Thelwall", "Kevan Buckley", "Georgios Paltoglou", "Di Cai", "Arvid Kappas." ],
      "venue" : "Journal of the American Society for Information Science and Technology 61(12):2544–2558.",
      "citeRegEx" : "Thelwall et al\\.,? 2010",
      "shortCiteRegEx" : "Thelwall et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Along the similar lines, The Task 1 of WASSA2017 (Mohammad and Bravo-Marquez, 2017c) poses a problem of finding emotion intensity of",
      "startOffset" : 49,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "and compactly represented using low dimensional vectors (Mikolov et al., 2013) instead of one hot encoding vectors which are sparse and high dimensional.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "Lexicon Features: AFINN (Nielsen, 2011) word list are manually rated for valence with an integer between -5 (Negative Sentiment) and +5 (Positive Sentiment).",
      "startOffset" : 24,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "Bing Liu (Hu and Liu, 2004) opinion lexicon extract opinion on customer reviews.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "+/-EffectWordNet (Choi and Wiebe, 2014) by MPQA group are sense level lexicons.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "The NRC Affect Intensity (Mohammad, 2017) lexicons provide real valued affect intensity.",
      "startOffset" : 25,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2010) contains 8 sense level associations (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and 2 sentiment level associations (negative and positive).",
      "startOffset" : 37,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "NRC Hashtag Emotion Lexicon (Mohammad and Kiritchenko, 2015) contains emotion word associations computed on emotion labeled twitter corpus via Hashtags.",
      "startOffset" : 28,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "NRC Hashtag Sentiment Lexicon and Sentiment140 Lexicon (Mohammad et al., 2013) contains sentiment word associations computed on twitter corpus via Hashtags and Emoticons.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "SentiWordNet (Baccianella et al., 2010) assigns to each synset of WordNet",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 21,
      "context" : "In addition to these, SentiStrength (Thelwall et al., 2010) application which estimates the strength of positive and negative sentiment from tweets is also added.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "GloVe (Pennington et al., 2014) is an unsupervised learning algorithm for obtaining vector representations for words.",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "Edinburgh embeddings (Bravo-Marquez et al., 2015) are obtained by training skip-gram model on Edinburgh corpus (Petrovic et al.",
      "startOffset" : 21,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : ", 2015) are obtained by training skip-gram model on Edinburgh corpus (Petrovic et al., 2010).",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "Since tweets are abundant with emojis, Emoji embeddings (Eisner et al., 2016) which are learned from the emoji descriptions have been used.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "Syntactic Features: Syntax specific features such as Word N-grams, Part-Of-Speech N-grams (Owoputi et al., 2013), Brown Cluster N-grams (Brown et al.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : ", 2013), Brown Cluster N-grams (Brown et al., 1992) obtained using TweetNLP 6 project have been integrated into the system.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : "After performing feature extraction, the data was then passed to various regressors Support Vector Regression, AdaBoost, RandomForestRegressor, and, BaggingRegressor of sklearn (Pedregosa et al., 2011).",
      "startOffset" : 177,
      "endOffset" : 201
    }, {
      "referenceID" : 4,
      "context" : "Best cross-validation results were obtained using AdaBoost meta regressor with base regressor as XGBoost (Chen and Guestrin, 2016) with 1000 estimators and 0.",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "Emoji embeddings (Eisner et al., 2016) give better results than using plain GloVe and Edinburgh embeddings.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "+/-EffectWordNet (Choi and Wiebe, 2014), NRC Hashtag Sentiment Lexicon, Sentiment140 Lexicon (Mohammad et al.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "+/-EffectWordNet (Choi and Wiebe, 2014), NRC Hashtag Sentiment Lexicon, Sentiment140 Lexicon (Mohammad et al., 2013) and NRC Hashtag Emotion Lexicon (Mohammad and Kiritchenko, 2015) are playing the most important role.",
      "startOffset" : 93,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : ", 2013) and NRC Hashtag Emotion Lexicon (Mohammad and Kiritchenko, 2015) are playing the most important role.",
      "startOffset" : 40,
      "endOffset" : 72
    } ],
    "year" : 2017,
    "abstractText" : "The paper describes experiments on estimating emotion intensity in tweets using a generalized regressor system. The system combines lexical, syntactic and pretrained word embedding features, trains them on general regressors and finally combines the best performing models to create an ensemble. The proposed system stood 3rd out of 22 systems in the leaderboard of WASSA-2017 Shared Task on Emotion Intensity.",
    "creator" : "LaTeX with hyperref package"
  }
}