{
  "name" : "1602.02505.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Binarized Neural Networks",
    "authors" : [ "Itay Hubara", "Daniel Soudry" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "BDNNs are trained using a novel binarized back propagation algorithm (BBP), which uses binary weights and binary neurons during the forward and backward propagation, while retaining precision of the stored weights in which gradients are accumulated. At test phase, BDNNs are fully binarized and can be implemented in hardware with low circuit complexity. The proposed binarized networks can be implemented using binary convolutions and proxy matrix multiplications with only standard binary XNOR and population count (popcount) operations. BBP is expected to reduce energy consumption by at least two orders of magnitude when compared to the hardware implementation of existing training algorithms. We obtained near state-of-the-art results with BDNNs on the permutation-invariant MNIST, CIFAR-10 and SVHN datasets."
    }, {
      "heading" : "1 Introduction",
      "text" : "The success of deep neural networks (DNNs) and, in particular, convolutional neural networks (CNNs) for large scale object recognition (Krizhevsky et al., 2012) has motivated on going exploration of alternative architectures, optimization and regularization techniques, that enable better accuracy and/or reduce computational footprint. The common pattern used for CNNs for object recognition is alternating convolution, max-pooling layers followed by non-linearity and a small number of fully connected layers. Very often deep networks are over-specified (the number of parametersexceed the number required) and regularized during training using dropout Hinton (2014)and `2 or `1 norms of the weights. More current research, has focused on improving the convergence speed and on reducing the computational complexity. Training or even just using neural networks (NN) algorithms on conventional generalpurpose digital hardware, namely, Von Neumann architecture has been found highly inefficient due to the massive amount of multiply-accumulate operations (MACs) in neurons, used when computing the weighted sums of their inputs. Currently, the number of neurons employed in typical CNNs for solving common tasks is 1e6− 1e9. By\nar X\niv :1\n60 2.\n02 50\n5v 1\n[ cs\n.L G\n] 8\nreducing many of these MAC operations, for example, by binarizing the floating point numbers involved, one can potentially improve computational complexity by orders of magnitude.\nRecent works have shown that it is possible to construct more computationally efficient DNNs by quantizing some of the parameters involved. So far, however, this was only partially achieved. In one study weights and neurons were binarized only during inference (test phase) (Soudry et al., 2014), and in anotheronly the weights were binraized during training propagation and inference stages (Courbariaux et al., 2015a). This studyproposes a more advanced techniquereferred to as binarized back propagation (BBP) for the complete binarizationof neurons and weights during inference and training. The proposed solution allows for completly binarized deep neural networks (BDNNs) in which all MAC operations are replaced with XNOR and population count (i.e., counting the number of ones in the binary number) operations. The proposed method is particularly beneficial for implementing large convolutional networks whose neuron to weight ratio is very large.\nWe argue that the proposed BBP algorithm can be implemented in hardware and argue that it is expected to be much more efficient in terms of area, speed, and energy consumption than full precision DNNs, which used floating-point multiply-accumulators. This was recently demonstrated (Esser & Arthur, 2015) in hardware that implemented binary neural networks at the inference phase, results with significant improvements in energy efficiency."
    }, {
      "heading" : "2 Related Work",
      "text" : "Until recently, the use of extremely low-precision networks (binary in the extreme case)was believed to be highly destructive to the network performance (Courbariaux et al., 2015b). Soudry et al. (2014) proved the contrary by using a variational Bayesian approach, that one can infer networks with binary weights and neurons by updating the posterior distributions over the weights. These distributions are updated by differentiating their parameters (e.g., mean values) via the back propagation (BP) algorithm. The drawback of this procedure, termed Expectation BackPropagation (EBP), is that the binarized parameters were only used during inference.\nThe probabilistic idea behind EBP was extended in the BinnaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification, (Spang, 1962)). The effect of this noise on the next neuron’s input would be small since the input was a summation over many weighted neurons. Thus, the real-valued version could be updated by the back propagated error by simply ignoring the binarization noise in the update. Using this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets. Courbariaux et al. (2015) also argued that noisy weights provide a form of regularization, which could help to improve generalization, as previously shown inWan et al. (2013) study. While binarizing weights this method kept full precision neurons.\nLin et al. (2015) study carried over the work of Courbariaux et al. (2015) to the back-propagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons values of power-of-two integers. Lin et al. work seems to share similar characteristics as ours. However, their approach continues to use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons only during back propagation process, and not in forward propagation. Other research (Judd et al., 2015; Gong et al., 2014) aimed to compress a fully trained high precision network by using a quantization or matrix factorization method. These methods required training the network with full precision weights and neurons thus require numerous MAC operations which the proposed BBP algorithm avoids. Hwang & Sung (2014) focused on Fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture results. Hwang & Sung (2014) provided evidence that DNNs with ternary weights, used on a dedicated circuit, consume very low power and can be operated with only on-chip memory, at test phase. Sung et al. (2015) study also indicated satisfactory empirical performance of neural networks with 8-bit precision. So far, to the best of the authors knowledge, no work has succeeded to binarize weights and as well as neurons at the inference and training phases.\nOne of the ideas in this work is that binarization can be treated as random noise. Following this idea, the authors introduce a new technique for injecting noise to hidden neurons by stochastically binarizing them during forward and backward propagation. The idea was derived from the successful dropout procedure of Hinton (2014), which randomly substitutes a portion of the hidden units with zeros, that noisy hidden neurons also add a form of regularization. The procedure proposed in the present study extends the practical applications of Courbariaux et al. (2015a) and creates a fully binarized network with no multiplications. This study showsthat even if we are not expanding the number of parameters in comparison to Courbariaux et al. (2015a) study, BBP algorithm can still provide near state-of-the-art results on three very popular datasets while keeping binaryrepresentations and weights."
    }, {
      "heading" : "2.1 Binary Connect",
      "text" : "Our work expands the BinaryConnect approach of Courbariaux et al. (2015a). We now summarize their ideas, and introduce our extension in the next section. BinaryConnect (Courbariaux et al., 2015a), as well as DropConnect (Wan et al., 2013) which share the same idea. During the training phase these methods add a form of noise to the model parameters while keeping the clean model parameters as a reference point. Whereas DropConnect zeros out a portion of the weights, BinaryConnect binarizes them. Courbariaux et al. (2015a) introduced and described two procedures:\n• Deterministic\nwb =\n{ +1 σ(w) > 0.5\n−1 otherwise , (1)\n• Stochastic\nwb =\n{ +1 w.p p = σ(w)\n−1 w.p p = 1− σ(w) , (2)\nwhere σ(·) is the hard sigmoid function, i.e.\nσ(x) = max(min( x+ 1\n2 , 0), 1),\nwith w being the full precision weight, and wb is the binarized weight. In both procedures, the binarized weight wb is used during the forward and backward propagation phase, while the full precision weight w is updated after the propagation.\nBoth procedures help regularize the model and achieved state-of-the-art results on several classic benchmarks (Courbariaux et al., 2015a). Courbariaux et al. (2015a) also observred the need to add certain edge constraints to w. Therefore, after each update, they used clipping, to force w values to be in the interval[−1, 1]."
    }, {
      "heading" : "3 Binarized Back Propagation",
      "text" : "In this section the BBP algorithm ispresenteds It shows the procedures that were used including: how to binarize the neurons (deterministic vs. stochastic implementation); how to reduce the impact of the weights and hidden units binarization without batch normalization; and finally, how to train and perform inference."
    }, {
      "heading" : "3.1 Stochastic and Deterministic Binarization",
      "text" : "The binarization operation used in the present work transforms real-valued weights into two possible values. At training time a stochastic binarization is applied to facilitate a finer, more informative binarization noise in comparison to the standard sign function.\nhb (x) =\n{ +1 w.p p = σ(x)\n−1 w.p p = 1− σ(x) (3)\nwhere σ(x) = (HT(x) + 1) /2 and HT(x) is the well known “hard tanh”\nHT(x) =  +1 x > 1\nx x ∈ [1,−1] −1 x < 1\n(4)\nNote that this clipping operation can be implemented with a simple comparison operator. Similarly to the relation between BinaryConnect and DropConnect, these neuron masks are related to dropout Hinton (2014): By adding quantization noise to the hidden neurons a regularization mechanism is created without preventing the model to converge and thus may help to avoid overfitting. At test phase deterministic binarization was used based on the sign function:\nhb(x) = { +1 x ≥ 0 −1 x < 0\n(5)"
    }, {
      "heading" : "3.2 Forward and Backward Propagation",
      "text" : "In the process of forward propagation we clipped the input via HT(x) defined in Eq. (4) and then binarieze it using Eq. (3) (or Eq. (5) for inference). However, in order to implement the backward propagation phase, we needed to first differentiate through these binary, non-differentiable hidden neurons. To achieve this goal, we used the stochastic binarization scheme Eq. (3), and examined the input to the next layer\nWbhb (x) = WbHT (x) + n (x) ,\nWe used the fact that HT (x) is the expectation over hb (x) (from Eqs. (3) and (4)), and defined n (x) as binarization noise with mean equal to zero. When the layer is wide, we expected the deterministic mean term HT (x) to dominate, as the noise term n (x) is a sum of many independent binarizations from all the neurons in the previous layer. Thus, we reasoned that the binarization noise n (x) could be ignored, when performing the differentiation in the backward propagation. Therefore, we replaced ∂hb(x)∂x (which cannot be computed) with:\n∂HT (x)\n∂x =  0 x > 1;\n1 x ∈ [1,−1] 0 x < 1, ; (6)\nNote that (6) is the derivative of HT(x) (Eq. 4). Therefore, in the process of backward propagation through the neurons, all we had to dowas mask out the gradients when the neuron is saturated (x > 1 or x < 1), while passing the rest of the gradients (if x ∈ [1,−1]). This masking is computationally cheap. However, to make this method work properly, the use of batch-normalization (BN) was required, since we would like that the mean value of the activation would be near zero and most of the valuable information would reside in [−1, 1]."
    }, {
      "heading" : "3.3 Batch Normalization and Clipping",
      "text" : "As shown by Ioffe & Szegedy (2015), due to the constant change of the distribution of each layer’s input, training neural networks can be a very noisy procedure which strongly depends on the weights’ initialization and the learning rate, and requires long convergence time. Batch normalization (BN) aims to solve all of these problems by performing a simple normalization for each mini-batch. BN usually allows high learning rates, and makes the model less sensitive to initialization. Additionally, it acts as a regularizer, in some cases eliminating the need for dropout. Moreover, according to Courbariaux et al. (2015a), batch normalization is necessary to reduce the overall\nAlgorithm 1 Binarized BackPropagation (BBP). C is the cost function. binarize(W ) and clip(W ) stands for binarize and clip methods. L is the number of layers.\nRequire: a deep model with parameters W, b at each layer. Input data x, its corresponding targets y, and learning rate η Initialize W, b = uniform(−1, 1).\nForward Propagation for i = 1 : L do Wb ← binarizeWeight(W ) hb ← binarizeNeuron(Wbhi−1) Eq. 5,3,4 end for Backward Propagation Initialize output layer’s error signal δ = ∂C∂hL for i = 1 : L do\nCompute ∆W and ∆b using Wb and hb (Eq.6) Update W : W ← clip(W −∆W ) Update b : b← b−∆b\nend for\nimpact of the weights’ scale. One of the drawbacks of BN is that it requires many multiplications both during the training (calculating the standard deviation and dividing by it) and during testing, namely, dividing by the running variance (the weighted mean of the training set activation variance). Although the number of scaling calculations is the same as the number of neurons, in the case of CNNs this number is quite large. For example in the CIFAR-10 dataset (using our architecture) the first convolution layer, consisting of only 128 × 3 × 3 kernel masks, converts an image of size 3 × 32 × 32 to size 3 × 128 × 28 × 28, which is two orders of magnitude larger than the number of weights. In our experiments we also noticed that BN improved accuracy and accelerated the convergence speed. To proxy BN we used shift based batch normalization technique which approximates BN almost without multiplications.\nStandard BN perform the following normalization:\nC(x) = x− 〈x〉\nσ−1(x) = 1√ 〈C2(x)〉\n(7)\nBN(x) = C(x)σ−1(x)γ + β , (8)\nwhere x is the input to a layer, on a minibatch of size B, 〈x〉 = 1B ∑B\ni=1 xi is an average over the minibatch samples, and γ and β are learnable parameters that perform an affine transformation.\nTo reduce the computational complexity, we suggest an alternative procedure. We define AP2(z) as the approximate power-of-2 proxy of z (i.e., the index of the most significant bit (MSB)), and stands for both left and right binary shift. Then, at\neach minibatch, we approximate the inverse standard deviation (Eq. 7)\nσ−1p2 (x) =AP2\n( 1√\n〈C(x) AP2(C(x)〉\n) , (9)\nand the normalization BNAP2(x) = (( C(x) σ−1p2 (x) ) AP2(γ) ) +β; . (10)\nTo obtain (9) we replaced in (7) the squaring operation of C (x) by a binary shifting of C (x) according to its own power-of-2 proxy. This saves the many MAC operations. . To obtain(10) we again replaced multiplication by a shift operation with powers-of-2 proxies.\nThe only operation which is not a binary shift or an add is the inverse square root in Eq. (9). From the early work of Lomont (2003) we know that this could be done at approximately the same complexity as multiplication. There are also faster methods, which involve lookup table tricks that typically achieve smaller accuracy (this may not be an issue, since our precedure already add a lot of noise). However, the number of values on which we apply the inverse-square operation is rather small, since it is done after averaging involved in the variance calculation (for a more thorough calculation, see the BN analysis in Lin et al. (2015). Forthermore size of the standard deviation vectors is relatively small . For example, the number of these values consist of just 0.3% of the network size (i.e., the number of learnable parameters), in the Cifar-10 network we used in our experiments."
    }, {
      "heading" : "3.4 Additional Implementation Details",
      "text" : "Throughout our work we restricted ourselves to use only adders, bitwise and shift operations. The comparison operation is also cheap, since adding and comparing two variables requires the same amount of energy. One of most common ways to compare two values is by subtracting them and looking at the sign bit. Hence, even if we use the simplest approach, the complexity is approximately the same as adding. For optimization technique we used a variant of AdaMax algorithmKingma & Ba (2014) we named shift based-AdaMax (S-AdaMax) which implement AdaMax only with learning rate and deviations which are power-of-2 integer, hence equal to shift. No momentum or weight decay were used.\n."
    }, {
      "heading" : "4 Expected Efficiency Gains",
      "text" : "Recently new technologies enable us to increase computing performance greatly. Improving computing performance has always been a challenge, a number of factors have made this process difficult in this last decade, and made power the main constraint on performance(Horowitz, 2014). This is why many researches tried to find a solution for how to reduce the energy consumption neural network require. In this section we would try to quantify the energy and complexity gain of using the BBP algorithm.\nThroughout this section we assume that the amount of energy required to add two 8-bit integers is 0.03 Pico Joule (pJ) (see table 1), and this will serve as our basic energy unit. Further more we assume that the addition of integers is linear in complexity (i.e. addition of 2-bit integers will require a quarter of this basic energy unit and so on)."
    }, {
      "heading" : "4.1 Energy Efficiency Estimates",
      "text" : ". Horowitz (2014) provides rough numbers for the energy consumption 1 as summarized in table 1 and 2. As can be seen in Table 1, while floating-point multipicators demand 1.1pJ-3.7pJ, floating point adders require only 0.4pJ-0.9pJ. Courbariaux et al. (2015b) replaced approximately two thirds of the multiplication operationswith addition, thus reducing the energy demand by roughly 2. BBP replaces as well two thirds of the multiplications, by using 2-bit integer adders (-1,+1 are typically represented by 2- bit although they actually require only one bit) which require only 0.03pJ - an order of magnitude smaller. Therefore even if we assume that most of the neural networks require less than 16-bit floating point by replacing the multiplication with integer adders we gain energy reduction by approximately two orders of magnitude. Moreover, similarly to Lin et al. (2015) we eliminate the multiplication in the back propagation process as well, thus reducing the energy consumption even further.\nTable 2 shows that the memory requires a great amount of energy (due to hardware leakage problems (Horowitz, 2014)). This is a major issue since CNNs use massive amount of neurons (much more than weight parameters) consequently by binarizing the neurons we reduce memory complexity which results with huge energy reduction."
    }, {
      "heading" : "4.2 Exploiting Kernel Repetitions",
      "text" : "When using a CNN architecture with binary weights, the maximum amount of unique kernels is bounded by the kernel size. For example, in our implementation we use kernels of size 3 × 3, so the maximum number of unique 2D kernels is 29 = 512.\n1The given number are for 45nm technology\nHowever, this does not mean that there is no point in expanding the number of feature maps beyond this number, since the actual kernel is a 3D matrix. Assuming we have M` kernels in the ` convolutional layer, we have to store a 4D weight matrix of size M` ×M`−1 × k × k. Consequently, the number of unique kernels is 2k\n2M`−1 . When needed, we apply each kernel on the map and perform the necessary MAC operations (in our case it is performed by XNOR and popcount operations). Since we now have binary kernels, many 2D kernels of size k × k repeat themselves. By using dedicated hardware/software we can apply only the unique 2D kernels on each feature map and sum the result wisely to receive each 3D kernel convolutional result. Note that an inverse kernel (i.e., [-1,1,-1] is the inverse of [1,-1,1]) can also be treated as a repetition considering it is merely a multiplication of the original kernel by -1. For example, in our CNN architecture trained on the CIFAR-10 benchmark, on average there are only 37% unique kernels per layer. Hence we can reduce the amount of the XNOR-popcount operations by 3."
    }, {
      "heading" : "5 Benchmark Results",
      "text" : "This section, report on empirical results showing that BBP obtains near state-of-theart results with fully binary networks on the permutation-invariant MNIST, CIFAR-10 and SVHN datasets. In all of our experiments we used an identical architecture as the BinarryConnect does. We use the L2-SVM output layer and opted square hinge loss and Shift based-AdaMax (section 3.4). We initialized the weight and bias using a uniform(−1, 1) distribution. The learning rate was initialized using Glorot et al. (2011) technique (again rounded to be integer of power 2). Since we can not use a standard decaying learning rate we shifted the learning rate to the right (multiplied by 0.5) every 50 iterations. Our networks were implemented in Torch which is a widely used environment for neural network algorithms."
    }, {
      "heading" : "5.1 Datasets",
      "text" : ""
    }, {
      "heading" : "5.1.1 CIFAR-10",
      "text" : "The well known CIFAR-10 is an image classification benchmark dataset. Containing 50,000 training images and 10,000 test images of 32 × 32 color images in 10 classes (airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships and trucks). For this dataset, we apply the same global contrast normalization and ZCA whitening as used by Goodfellow et al. (2013) and Lin et al. (2013). No data-augmentation was applied (which was shown to be very helpful for this data set Graham (2014) ). The architecture of our CNN was inspired by BinarryConnect and contains three alternating stages of two 3x3 convolution filters followed by 2x2 max pooling with a stride of 2 with increasing numbers of maps 128, 256, and 512 respectively. The output is then concatenated into one vector of size 8192 which served as the input to a two stage fully connected layer with 1024 hidden units each. For the final classification we used a L2-SVM output layer. A binary shift based batch-normalization (section 3.3) with a\nmini-batch of size 100 was used to speed up the training. We report results after 500 iterations."
    }, {
      "heading" : "5.1.2 Permutation Invariant MNIST",
      "text" : "The MNIST database of handwritten digits is one of the most studied datasets benchmark for image classification. The dataset contains 60,000 examples of digits from 0 to 9 for training and 10,000 examples for testing. Each sample is a 28 x 28 pixel gray level image. For the basic version of the MNIST learning task, no knowledge of geometry is provided and there is no special preprocessing or enhancement of the training set, so an unknown but fixed random permutation of the pixels would not affect the learning algorithm. The MLP we trained on MNIST has a similar architecture as the BinaryConnect and consists of 3 binary hidden layers of 1024 and a L2-SVM output layer. We used a mini-batch with a size of 200 to speed up the training and avoid Batch Normalization. We report results after 1000 iterations."
    }, {
      "heading" : "5.1.3 SVHN",
      "text" : "SVHN is an image classification dataset benchmark obtained from house numbers in Google Street View images. Similarly to MNIST, it contains images representing digits ranging from 0 to 9 but incorporates one order of magnitude more labeled data and is considered significantly more difficult. It consists of a training set of 604K instances and a test set of 26K instances where each instance is a 32 × 32 color images. We applied the same procedure we used for CIFAR-10, with an architecture similar to that of BinnaryConnect. We report results after 500 iterations."
    }, {
      "heading" : "5.2 Results",
      "text" : "As can be seen in Table 3, BBP algorithm using the architecture stated above received 10.15% error rate on CIFAR10, 2.53% on SVHN and 1.4% on permutation invariant MNIST. It is somehow surprising that despite the binarization noise and the other rough power-of-2 estimation (shift base BN and S-AdaMax section 3.3 and 3.4 respectively) BDNN still receives near state-of-the-art results. Note that we did not exhaustively search for different architecture or enlarged the number of parameters in comparison to Courbariaux et al. (2015a); Lin et al. (2015). Moreover, as can be seen in figure 5.2 the training set didn’t overfit the data, hence perhaps by increasing the network size some improvement may be achieved.\n11"
    }, {
      "heading" : "6 Discussion and Future Work",
      "text" : "In this work, we have introduced binary back propagation (BBP), a novel binarization scheme for weights and neurons during forward and backward propagation. We have shown that it is possible to train BDNNs on the permutation invariant MNIST, CIFAR10 and SVHN data sets and achieve nearly state-of-the-art results. These findings have wide-ranging implications for specialized hardware implementations of deep networks by removing the need for almost all multiplications, and thus potentially allowing to speed-up the process by two orders of magnitude. The impact at test phase could be even more important, getting rid of the multiplications altogether, reducing by a factor of at least 16 (from 16 bits single-float precision to single bit precision) the memory requirement of deep networks and reduce the energy consummation by two orders of magnitude. This has a major effect on the memory and computation bandwidth and thus on the size of the models that can be deployed. As a byproduct, we introduced an approximate, computationally cheap, batch normalization method with no multiplication.\nWe believe that with the proper hardware, which has the ability to process fast binary convolution, BBP would enable for a wide variety of DNNs to run on mobile devices. Such BDNNs may enable also interpretable binary representations (Wu et al., 2015) and efficient hashing (Ginkel & Connor, 2015). Another potential benefit is scalable training of spiking neuronal networks (which are recurrent neural nets with\nbinary neurons) for computational neuroscience research purposes, so far a non-trivial task ((DePasquale et al., 2016), and references therein). We are currently working on extending this work to other models and bigger more complex data sets such as ImageNet sets (Insert citation). Moreover, like other researchers Soudry et al. (2014); Hwang & Sung (2014); Courbariaux et al. (2015a); Lin et al. (2015), at training timethe value of the full precision weights was kept (note this is not the case for the hidden neurons which can be stored in their binary format). We encourage the search towards an ideal algorithm that does not need to store those values.\nCurrently, saving the full precision requires relatively large energy resources (however, novel memory devices might be used to alleviate this issue in the future Soudry et al. (2015)). As shown in figure 5.2 many weights (75-90%) reach the saturation level. Those weights can theoretically be saved with one bit which simply means that we do not care about the accuracy of the other bits. Furthermore, as mentioned in section 4.2, approximately 63% of the and and popcount operations can be saved (at inference time) due to the vast amount of binary kernels repetitions. This however require dedicated hardware/software implementation (for more details see section 4.2). We hope that this work would encourage the development of dedicated binary convolution hardware which would lead to very fast training and testing of neural networks."
    } ],
    "references" : [ {
      "title" : "Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropgation",
      "author" : [ "Cheng", "Zhiyong", "Soudry", "Daniel", "Mao", "Zexi", "Lan", "Zhenzhong" ],
      "venue" : null,
      "citeRegEx" : "Cheng et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2012
    }, {
      "title" : "BinaryConnect: Training Deep Neural Networks with binary weights during propagations",
      "author" : [ "Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre" ],
      "venue" : "Nips, pp. 1–9,",
      "citeRegEx" : "Courbariaux et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Courbariaux et al\\.",
      "year" : 2015
    }, {
      "title" : "Training deep neural networks with low precision multiplications",
      "author" : [ "Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre" ],
      "venue" : "Iclr, (Section 5):10,",
      "citeRegEx" : "Courbariaux et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Courbariaux et al\\.",
      "year" : 2015
    }, {
      "title" : "Using Firing-Rate Dynamics to Train Recurrent Networks of Spiking Model Neurons",
      "author" : [ "DePasquale", "Brian", "Churchland", "Mark M", "L.F. Abbott" ],
      "venue" : null,
      "citeRegEx" : "DePasquale et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "DePasquale et al\\.",
      "year" : 2016
    }, {
      "title" : "Backpropagation for Energy-Efficient Neuromorphic Computing",
      "author" : [ "Esser", "Steve K", "Arthur", "John V" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Esser et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Esser et al\\.",
      "year" : 2015
    }, {
      "title" : "Discrete Parameter Autoencoders for Semantic Hashing",
      "author" : [ "Ginkel", "Robbert Van", "Connor", "Peter O" ],
      "venue" : null,
      "citeRegEx" : "Ginkel et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ginkel et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep Sparse Rectifier",
      "author" : [ "Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua" ],
      "venue" : "Neural Networks. Aistats,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Compressing Deep Convolutional Networks using Vector Quantization",
      "author" : [ "Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir" ],
      "venue" : "pp. 1–10,",
      "citeRegEx" : "Gong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2014
    }, {
      "title" : "Spatially-sparse convolutional neural networks",
      "author" : [ "Graham", "Benjamin" ],
      "venue" : "pp. 1–13,",
      "citeRegEx" : "Graham and Benjamin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Graham and Benjamin.",
      "year" : 2014
    }, {
      "title" : "A Simple Way to Prevent Neural Networks from Overfitting",
      "author" : [ "Hinton. Dropout" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Dropout,? \\Q2014\\E",
      "shortCiteRegEx" : "Dropout",
      "year" : 2014
    }, {
      "title" : "Computing’s Energy Problem (and what we can do about it)",
      "author" : [ "Horowitz", "Mark" ],
      "venue" : "IEEE Interational Solid State Circuits Conference,",
      "citeRegEx" : "Horowitz and Mark.,? \\Q2014\\E",
      "shortCiteRegEx" : "Horowitz and Mark.",
      "year" : 2014
    }, {
      "title" : "Fixed-point feedforward deep neural network design using weights",
      "author" : [ "Hwang", "Kyuyeon", "Sung", "Wonyong" ],
      "venue" : "IEEE Workshop on Signal Processing Systems (SiPS), pp",
      "citeRegEx" : "Hwang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hwang et al\\.",
      "year" : 2014
    }, {
      "title" : "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "author" : [ "Ioffe", "Sergey", "Szegedy", "Christian" ],
      "venue" : null,
      "citeRegEx" : "Ioffe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe et al\\.",
      "year" : 2015
    }, {
      "title" : "Reduced-Precision Strategies for Bounded Memory in Deep",
      "author" : [ "Judd", "Patrick", "Albericio", "Jorge", "Hetherington", "Tayler", "Aamodt", "Tor", "Jerger", "Natalie Enright", "Urtasun", "Raquel", "Moshovos", "Andreas" ],
      "venue" : "Neural Nets. pp",
      "citeRegEx" : "Judd et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Judd et al\\.",
      "year" : 2015
    }, {
      "title" : "Bitwise Neural Networks",
      "author" : [ "Kim", "Minje", "Paris", "Smaragdis" ],
      "venue" : "ICML Workshop on Resource-Efficient Machine Learning,",
      "citeRegEx" : "Kim et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Kingma", "Diederik", "Ba", "Jimmy" ],
      "venue" : "[cs], pp",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "Krizhevsky", "Alex", "Sulskever", "IIya", "Hinton", "Geoffret E" ],
      "venue" : "Advances in Neural Information and Processing Systems (NIPS), pp",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Network In Network",
      "author" : [ "Lin", "Min", "Chen", "Qiang", "Yan", "Shuicheng" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "Lin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural Networks with Few Multiplications",
      "author" : [ "Lin", "Zhouhan", "Courbariaux", "Matthieu", "Memisevic", "Roland", "Bengio", "Yoshua" ],
      "venue" : "Iclr, pp",
      "citeRegEx" : "Lin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast Inverse Square Root",
      "author" : [ "Lomont", "Chris" ],
      "venue" : "Indiana: Purdue University,",
      "citeRegEx" : "Lomont and Chris.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lomont and Chris.",
      "year" : 2006
    }, {
      "title" : "Expectation Backpropagation: parameter-free training of multilayer neural networks with real and discrete weights",
      "author" : [ "D Soudry", "I Hubara", "R. Meir" ],
      "venue" : "Neural Information Processing Systems 2014,",
      "citeRegEx" : "Soudry et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Soudry et al\\.",
      "year" : 2014
    }, {
      "title" : "Memristor-Based Multilayer Neural Networks With Online Gradient Descent Training",
      "author" : [ "Soudry", "Daniel", "Di Castro", "Dotan", "Gal", "Asaf", "Kolodny", "Avinoam", "Kvatinsky", "Shahar" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems,",
      "citeRegEx" : "Soudry et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Soudry et al\\.",
      "year" : 2015
    }, {
      "title" : "Reduction by Feedback",
      "author" : [ "H.A. Spang" ],
      "venue" : "IRE Transactions on Communications Systems, pp",
      "citeRegEx" : "Spang,? \\Q1962\\E",
      "shortCiteRegEx" : "Spang",
      "year" : 1962
    }, {
      "title" : "Resiliency of Deep Neural Networks under Quantization",
      "author" : [ "Sung", "Wonyong", "Shin", "Sungho", "Hwang", "Kyuyeon" ],
      "venue" : null,
      "citeRegEx" : "Sung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sung et al\\.",
      "year" : 2014
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "LeCun", "Yann", "Fergus", "Rob" ],
      "venue" : "Icml, (1):109–111,",
      "citeRegEx" : "Wan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2013
    }, {
      "title" : "Adjustable Bounded Rectifiers: Towards Deep Binary Representations",
      "author" : [ "Wu", "Zhirong", "Lin", "Dahua", "Tang", "Xiaoou" ],
      "venue" : "arXiv preprint, pp",
      "citeRegEx" : "Wu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "The success of deep neural networks (DNNs) and, in particular, convolutional neural networks (CNNs) for large scale object recognition (Krizhevsky et al., 2012) has motivated on going exploration of alternative architectures, optimization and regularization techniques, that enable better accuracy and/or reduce computational footprint.",
      "startOffset" : 135,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "The success of deep neural networks (DNNs) and, in particular, convolutional neural networks (CNNs) for large scale object recognition (Krizhevsky et al., 2012) has motivated on going exploration of alternative architectures, optimization and regularization techniques, that enable better accuracy and/or reduce computational footprint. The common pattern used for CNNs for object recognition is alternating convolution, max-pooling layers followed by non-linearity and a small number of fully connected layers. Very often deep networks are over-specified (the number of parametersexceed the number required) and regularized during training using dropout Hinton (2014)and `2 or `1 norms of the weights.",
      "startOffset" : 136,
      "endOffset" : 669
    }, {
      "referenceID" : 20,
      "context" : "In one study weights and neurons were binarized only during inference (test phase) (Soudry et al., 2014), and in anotheronly the weights were binraized during training propagation and inference stages (Courbariaux et al.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification, (Spang, 1962)).",
      "startOffset" : 166,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "Until recently, the use of extremely low-precision networks (binary in the extreme case)was believed to be highly destructive to the network performance (Courbariaux et al., 2015b). Soudry et al. (2014) proved the contrary by using a variational Bayesian approach, that one can infer networks with binary weights and neurons by updating the posterior distributions over the weights.",
      "startOffset" : 154,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "Until recently, the use of extremely low-precision networks (binary in the extreme case)was believed to be highly destructive to the network performance (Courbariaux et al., 2015b). Soudry et al. (2014) proved the contrary by using a variational Bayesian approach, that one can infer networks with binary weights and neurons by updating the posterior distributions over the weights. These distributions are updated by differentiating their parameters (e.g., mean values) via the back propagation (BP) algorithm. The drawback of this procedure, termed Expectation BackPropagation (EBP), is that the binarized parameters were only used during inference. The probabilistic idea behind EBP was extended in the BinnaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process.",
      "startOffset" : 154,
      "endOffset" : 761
    }, {
      "referenceID" : 1,
      "context" : "Until recently, the use of extremely low-precision networks (binary in the extreme case)was believed to be highly destructive to the network performance (Courbariaux et al., 2015b). Soudry et al. (2014) proved the contrary by using a variational Bayesian approach, that one can infer networks with binary weights and neurons by updating the posterior distributions over the weights. These distributions are updated by differentiating their parameters (e.g., mean values) via the back propagation (BP) algorithm. The drawback of this procedure, termed Expectation BackPropagation (EBP), is that the binarized parameters were only used during inference. The probabilistic idea behind EBP was extended in the BinnaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification, (Spang, 1962)). The effect of this noise on the next neuron’s input would be small since the input was a summation over many weighted neurons. Thus, the real-valued version could be updated by the back propagated error by simply ignoring the binarization noise in the update. Using this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets.",
      "startOffset" : 154,
      "endOffset" : 1374
    }, {
      "referenceID" : 1,
      "context" : "Until recently, the use of extremely low-precision networks (binary in the extreme case)was believed to be highly destructive to the network performance (Courbariaux et al., 2015b). Soudry et al. (2014) proved the contrary by using a variational Bayesian approach, that one can infer networks with binary weights and neurons by updating the posterior distributions over the weights. These distributions are updated by differentiating their parameters (e.g., mean values) via the back propagation (BP) algorithm. The drawback of this procedure, termed Expectation BackPropagation (EBP), is that the binarized parameters were only used during inference. The probabilistic idea behind EBP was extended in the BinnaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification, (Spang, 1962)). The effect of this noise on the next neuron’s input would be small since the input was a summation over many weighted neurons. Thus, the real-valued version could be updated by the back propagated error by simply ignoring the binarization noise in the update. Using this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets. Courbariaux et al. (2015) also argued that noisy weights provide a form of regularization, which could help to improve generalization, as previously shown inWan et al.",
      "startOffset" : 154,
      "endOffset" : 1511
    }, {
      "referenceID" : 1,
      "context" : "Until recently, the use of extremely low-precision networks (binary in the extreme case)was believed to be highly destructive to the network performance (Courbariaux et al., 2015b). Soudry et al. (2014) proved the contrary by using a variational Bayesian approach, that one can infer networks with binary weights and neurons by updating the posterior distributions over the weights. These distributions are updated by differentiating their parameters (e.g., mean values) via the back propagation (BP) algorithm. The drawback of this procedure, termed Expectation BackPropagation (EBP), is that the binarized parameters were only used during inference. The probabilistic idea behind EBP was extended in the BinnaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification, (Spang, 1962)). The effect of this noise on the next neuron’s input would be small since the input was a summation over many weighted neurons. Thus, the real-valued version could be updated by the back propagated error by simply ignoring the binarization noise in the update. Using this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets. Courbariaux et al. (2015) also argued that noisy weights provide a form of regularization, which could help to improve generalization, as previously shown inWan et al. (2013) study.",
      "startOffset" : 154,
      "endOffset" : 1660
    }, {
      "referenceID" : 13,
      "context" : "Other research (Judd et al., 2015; Gong et al., 2014) aimed to compress a fully trained high precision network by using a quantization or matrix factorization method.",
      "startOffset" : 15,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "Other research (Judd et al., 2015; Gong et al., 2014) aimed to compress a fully trained high precision network by using a quantization or matrix factorization method.",
      "startOffset" : 15,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "(2015) study carried over the work of Courbariaux et al. (2015) to the back-propagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons values of power-of-two integers.",
      "startOffset" : 38,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "(2015) study carried over the work of Courbariaux et al. (2015) to the back-propagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons values of power-of-two integers. Lin et al. work seems to share similar characteristics as ours. However, their approach continues to use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons only during back propagation process, and not in forward propagation.",
      "startOffset" : 38,
      "endOffset" : 467
    }, {
      "referenceID" : 1,
      "context" : "(2015) study carried over the work of Courbariaux et al. (2015) to the back-propagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons values of power-of-two integers. Lin et al. work seems to share similar characteristics as ours. However, their approach continues to use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons only during back propagation process, and not in forward propagation. Other research (Judd et al., 2015; Gong et al., 2014) aimed to compress a fully trained high precision network by using a quantization or matrix factorization method. These methods required training the network with full precision weights and neurons thus require numerous MAC operations which the proposed BBP algorithm avoids. Hwang & Sung (2014) focused on Fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture results.",
      "startOffset" : 38,
      "endOffset" : 907
    }, {
      "referenceID" : 1,
      "context" : "(2015) study carried over the work of Courbariaux et al. (2015) to the back-propagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons values of power-of-two integers. Lin et al. work seems to share similar characteristics as ours. However, their approach continues to use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons only during back propagation process, and not in forward propagation. Other research (Judd et al., 2015; Gong et al., 2014) aimed to compress a fully trained high precision network by using a quantization or matrix factorization method. These methods required training the network with full precision weights and neurons thus require numerous MAC operations which the proposed BBP algorithm avoids. Hwang & Sung (2014) focused on Fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture results. Hwang & Sung (2014) provided evidence that DNNs with ternary weights, used on a dedicated circuit, consume very low power and can be operated with only on-chip memory, at test phase.",
      "startOffset" : 38,
      "endOffset" : 1066
    }, {
      "referenceID" : 1,
      "context" : "(2015) study carried over the work of Courbariaux et al. (2015) to the back-propagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons values of power-of-two integers. Lin et al. work seems to share similar characteristics as ours. However, their approach continues to use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons only during back propagation process, and not in forward propagation. Other research (Judd et al., 2015; Gong et al., 2014) aimed to compress a fully trained high precision network by using a quantization or matrix factorization method. These methods required training the network with full precision weights and neurons thus require numerous MAC operations which the proposed BBP algorithm avoids. Hwang & Sung (2014) focused on Fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture results. Hwang & Sung (2014) provided evidence that DNNs with ternary weights, used on a dedicated circuit, consume very low power and can be operated with only on-chip memory, at test phase. Sung et al. (2015) study also indicated satisfactory empirical performance of neural networks with 8-bit precision.",
      "startOffset" : 38,
      "endOffset" : 1248
    }, {
      "referenceID" : 1,
      "context" : "(2015) study carried over the work of Courbariaux et al. (2015) to the back-propagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons values of power-of-two integers. Lin et al. work seems to share similar characteristics as ours. However, their approach continues to use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons only during back propagation process, and not in forward propagation. Other research (Judd et al., 2015; Gong et al., 2014) aimed to compress a fully trained high precision network by using a quantization or matrix factorization method. These methods required training the network with full precision weights and neurons thus require numerous MAC operations which the proposed BBP algorithm avoids. Hwang & Sung (2014) focused on Fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture results. Hwang & Sung (2014) provided evidence that DNNs with ternary weights, used on a dedicated circuit, consume very low power and can be operated with only on-chip memory, at test phase. Sung et al. (2015) study also indicated satisfactory empirical performance of neural networks with 8-bit precision. So far, to the best of the authors knowledge, no work has succeeded to binarize weights and as well as neurons at the inference and training phases. One of the ideas in this work is that binarization can be treated as random noise. Following this idea, the authors introduce a new technique for injecting noise to hidden neurons by stochastically binarizing them during forward and backward propagation. The idea was derived from the successful dropout procedure of Hinton (2014), which randomly substitutes a portion of the hidden units with zeros, that noisy hidden neurons also add a form of regularization.",
      "startOffset" : 38,
      "endOffset" : 1825
    }, {
      "referenceID" : 1,
      "context" : "(2015) study carried over the work of Courbariaux et al. (2015) to the back-propagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons values of power-of-two integers. Lin et al. work seems to share similar characteristics as ours. However, their approach continues to use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons only during back propagation process, and not in forward propagation. Other research (Judd et al., 2015; Gong et al., 2014) aimed to compress a fully trained high precision network by using a quantization or matrix factorization method. These methods required training the network with full precision weights and neurons thus require numerous MAC operations which the proposed BBP algorithm avoids. Hwang & Sung (2014) focused on Fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture results. Hwang & Sung (2014) provided evidence that DNNs with ternary weights, used on a dedicated circuit, consume very low power and can be operated with only on-chip memory, at test phase. Sung et al. (2015) study also indicated satisfactory empirical performance of neural networks with 8-bit precision. So far, to the best of the authors knowledge, no work has succeeded to binarize weights and as well as neurons at the inference and training phases. One of the ideas in this work is that binarization can be treated as random noise. Following this idea, the authors introduce a new technique for injecting noise to hidden neurons by stochastically binarizing them during forward and backward propagation. The idea was derived from the successful dropout procedure of Hinton (2014), which randomly substitutes a portion of the hidden units with zeros, that noisy hidden neurons also add a form of regularization. The procedure proposed in the present study extends the practical applications of Courbariaux et al. (2015a) and creates a fully binarized network with no multiplications.",
      "startOffset" : 38,
      "endOffset" : 2065
    }, {
      "referenceID" : 1,
      "context" : "(2015) study carried over the work of Courbariaux et al. (2015) to the back-propagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons values of power-of-two integers. Lin et al. work seems to share similar characteristics as ours. However, their approach continues to use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons only during back propagation process, and not in forward propagation. Other research (Judd et al., 2015; Gong et al., 2014) aimed to compress a fully trained high precision network by using a quantization or matrix factorization method. These methods required training the network with full precision weights and neurons thus require numerous MAC operations which the proposed BBP algorithm avoids. Hwang & Sung (2014) focused on Fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture results. Hwang & Sung (2014) provided evidence that DNNs with ternary weights, used on a dedicated circuit, consume very low power and can be operated with only on-chip memory, at test phase. Sung et al. (2015) study also indicated satisfactory empirical performance of neural networks with 8-bit precision. So far, to the best of the authors knowledge, no work has succeeded to binarize weights and as well as neurons at the inference and training phases. One of the ideas in this work is that binarization can be treated as random noise. Following this idea, the authors introduce a new technique for injecting noise to hidden neurons by stochastically binarizing them during forward and backward propagation. The idea was derived from the successful dropout procedure of Hinton (2014), which randomly substitutes a portion of the hidden units with zeros, that noisy hidden neurons also add a form of regularization. The procedure proposed in the present study extends the practical applications of Courbariaux et al. (2015a) and creates a fully binarized network with no multiplications. This study showsthat even if we are not expanding the number of parameters in comparison to Courbariaux et al. (2015a) study, BBP algorithm can still provide near state-of-the-art results on three very popular datasets while keeping binaryrepresentations and weights.",
      "startOffset" : 38,
      "endOffset" : 2247
    }, {
      "referenceID" : 24,
      "context" : ", 2015a), as well as DropConnect (Wan et al., 2013) which share the same idea.",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "Our work expands the BinaryConnect approach of Courbariaux et al. (2015a). We now summarize their ideas, and introduce our extension in the next section.",
      "startOffset" : 47,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "Our work expands the BinaryConnect approach of Courbariaux et al. (2015a). We now summarize their ideas, and introduce our extension in the next section. BinaryConnect (Courbariaux et al., 2015a), as well as DropConnect (Wan et al., 2013) which share the same idea. During the training phase these methods add a form of noise to the model parameters while keeping the clean model parameters as a reference point. Whereas DropConnect zeros out a portion of the weights, BinaryConnect binarizes them. Courbariaux et al. (2015a) introduced and described two procedures:",
      "startOffset" : 47,
      "endOffset" : 526
    }, {
      "referenceID" : 1,
      "context" : "Both procedures help regularize the model and achieved state-of-the-art results on several classic benchmarks (Courbariaux et al., 2015a). Courbariaux et al. (2015a) also observred the need to add certain edge constraints to w.",
      "startOffset" : 111,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "Moreover, according to Courbariaux et al. (2015a), batch normalization is necessary to reduce the overall",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : "However, the number of values on which we apply the inverse-square operation is rather small, since it is done after averaging involved in the variance calculation (for a more thorough calculation, see the BN analysis in Lin et al. (2015). Forthermore size of the standard deviation vectors is relatively small .",
      "startOffset" : 221,
      "endOffset" : 239
    }, {
      "referenceID" : 1,
      "context" : "Courbariaux et al. (2015b) replaced approximately two thirds of the multiplication operationswith addition, thus reducing the energy demand by roughly 2.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "Courbariaux et al. (2015b) replaced approximately two thirds of the multiplication operationswith addition, thus reducing the energy demand by roughly 2. BBP replaces as well two thirds of the multiplications, by using 2-bit integer adders (-1,+1 are typically represented by 2- bit although they actually require only one bit) which require only 0.03pJ - an order of magnitude smaller. Therefore even if we assume that most of the neural networks require less than 16-bit floating point by replacing the multiplication with integer adders we gain energy reduction by approximately two orders of magnitude. Moreover, similarly to Lin et al. (2015) we eliminate the multiplication in the back propagation process as well, thus reducing the energy consumption even further.",
      "startOffset" : 0,
      "endOffset" : 648
    }, {
      "referenceID" : 6,
      "context" : "The learning rate was initialized using Glorot et al. (2011) technique (again rounded to be integer of power 2).",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "(2013) and Lin et al. (2013). No data-augmentation was applied (which was shown to be very helpful for this data set Graham (2014) ).",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "(2013) and Lin et al. (2013). No data-augmentation was applied (which was shown to be very helpful for this data set Graham (2014) ).",
      "startOffset" : 11,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "Note that we did not exhaustively search for different architecture or enlarged the number of parameters in comparison to Courbariaux et al. (2015a); Lin et al.",
      "startOffset" : 122,
      "endOffset" : 149
    }, {
      "referenceID" : 1,
      "context" : "Note that we did not exhaustively search for different architecture or enlarged the number of parameters in comparison to Courbariaux et al. (2015a); Lin et al. (2015). Moreover, as can be seen in figure 5.",
      "startOffset" : 122,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "15% Binarized weights, during training and test BinaryConnect Courbariaux et al. (2015a) 1.",
      "startOffset" : 62,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "9% Binarized neurons+weights, during test EPB Cheng et al. (2015) 2.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 17,
      "context" : "68% Network in NetworkLin et al. (2013) 2.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 17,
      "context" : "68% Network in NetworkLin et al. (2013) 2.35% 10.41% DropConnectWan et al. (2013) 1.",
      "startOffset" : 22,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "Such BDNNs may enable also interpretable binary representations (Wu et al., 2015) and efficient hashing (Ginkel & Connor, 2015).",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "binary neurons) for computational neuroscience research purposes, so far a non-trivial task ((DePasquale et al., 2016), and references therein).",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "binary neurons) for computational neuroscience research purposes, so far a non-trivial task ((DePasquale et al., 2016), and references therein). We are currently working on extending this work to other models and bigger more complex data sets such as ImageNet sets (Insert citation). Moreover, like other researchers Soudry et al. (2014); Hwang & Sung (2014); Courbariaux et al.",
      "startOffset" : 94,
      "endOffset" : 338
    }, {
      "referenceID" : 1,
      "context" : "binary neurons) for computational neuroscience research purposes, so far a non-trivial task ((DePasquale et al., 2016), and references therein). We are currently working on extending this work to other models and bigger more complex data sets such as ImageNet sets (Insert citation). Moreover, like other researchers Soudry et al. (2014); Hwang & Sung (2014); Courbariaux et al.",
      "startOffset" : 94,
      "endOffset" : 359
    }, {
      "referenceID" : 1,
      "context" : "(2014); Hwang & Sung (2014); Courbariaux et al. (2015a); Lin et al.",
      "startOffset" : 29,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "(2014); Hwang & Sung (2014); Courbariaux et al. (2015a); Lin et al. (2015), at training timethe value of the full precision weights was kept (note this is not the case for the hidden neurons which can be stored in their binary format).",
      "startOffset" : 29,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "(2014); Hwang & Sung (2014); Courbariaux et al. (2015a); Lin et al. (2015), at training timethe value of the full precision weights was kept (note this is not the case for the hidden neurons which can be stored in their binary format). We encourage the search towards an ideal algorithm that does not need to store those values. Currently, saving the full precision requires relatively large energy resources (however, novel memory devices might be used to alleviate this issue in the future Soudry et al. (2015)).",
      "startOffset" : 29,
      "endOffset" : 513
    } ],
    "year" : 2017,
    "abstractText" : "In this work we introduce a binarized deep neural network (BDNN) model. BDNNs are trained using a novel binarized back propagation algorithm (BBP), which uses binary weights and binary neurons during the forward and backward propagation, while retaining precision of the stored weights in which gradients are accumulated. At test phase, BDNNs are fully binarized and can be implemented in hardware with low circuit complexity. The proposed binarized networks can be implemented using binary convolutions and proxy matrix multiplications with only standard binary XNOR and population count (popcount) operations. BBP is expected to reduce energy consumption by at least two orders of magnitude when compared to the hardware implementation of existing training algorithms. We obtained near state-of-the-art results with BDNNs on the permutation-invariant MNIST, CIFAR-10 and SVHN datasets.",
    "creator" : "TeX"
  }
}