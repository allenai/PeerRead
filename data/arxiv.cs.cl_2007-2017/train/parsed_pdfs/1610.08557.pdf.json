{
  "name" : "1610.08557.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Knowledge-Based Biomedical Word Sense Disambiguation with Neural Concept Embeddings",
    "authors" : [ "AKM Sabbir", "Antonio Jimeno-Yepes", "Ramakanth Kavuluru" ],
    "emails" : [ "akm.sabbir@uky.edu", "antonio.jimeno@gmail.com", "rvkavu2@uky.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nBiomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]–[5], clinical narratives [6]–[8] and health related social media posts [9]–[11]. Specialized components including named entity recognition (NER) programs, syntactic parsers, and relation extractors form the backbone of many high level information extraction and knowledge discovery applications. For most components in an NLP application pipeline, there is a clear snowball effect of errors in a component in the beginning of the pipeline leading to more errors in other subsequent components and the final results of the full application.\nResolving lexical mentions in text to correct named entities from a fixed terminology is essential in building an effective NER program, which is typically the next step after sentence segmentation and part-of-speech tagging in most NLP pipelines. All other downstream components and the\n1AKM. Sabbir is with the Department of Computer Science, University of Kentucky, Lexington, KY, USA. akm.sabbir@uky.edu\n2A. Jimeno-Yepes is with IBM Research Australia, Melbourne, VIC, Australia. antonio.jimeno@gmail.com\n3R. Kavuluru is the corresponding author and is with the Division of Biomedical Informatics (Department of Internal Medicine) and the Department of Computer Science, University of Kentucky, Lexington, KY, USA. rvkavu2@uky.edu\nfull application suffer if lexical ambiguities are not correctly resolved. Recent research also shows that resolving ambiguities provides performance gains in information retrieval and search system design [12]. In this effort, we employ knowledge-based methods, neural concept and word vectors learned through unsupervised deep learning approaches, and a straightforward nearest neighbor approach to achieve new state-of-the-art results over a public gold standard dataset [1] in biomedical word sense disambiguation (WSD).\nFor this effort, WSD specifically deals with identifying the correct sense of a term, among a set of given candidate senses for that term, when it is presented in a brief narrative along with its context (surrounding text). For example, consider the ambiguous term ‘discharge’. It has two unique senses in biomedicine – (S1). The first is the administrative process of releasing a patient from a healthcare facility following an in-patient stay for some treatment or procedure. (S2). The second sense pertains to bodily secretions of certain fluids from an orifice or wound. In our task the ambiguous term ‘discharge’ is specified along with the sense set {S1, S2} and an example context – “Low risk patients identified using CADILLAC risk score with STEMI treated successfully with primary PCI have a low adverse event rate on the third day or later of hospitalization suggesting that an earlier discharge is safe in properly selected patients.” Our goal is to identify the correct sense S1 for this specific occurrence of ‘discharge’.\nNext, we outline the organization of the rest of the paper. In Section II, we discuss earlier efforts in biomedical WSD and recent approaches that incorporate word embeddings. Our main methods that employ concept/word embeddings including the nearest neighbor approach are detailed in Section III. We then present our main results and takeaways in Section IV. Finally, in Section V, we conclude with some future research directions involving recurrent neural architectures for biomedical WSD."
    }, {
      "heading" : "II. BACKGROUND AND RELATED WORK",
      "text" : "For a thorough overview of approaches to WSD, we direct the readers to the survey by Navigli [13], which suggests mainly three categories – supervised, knowledgebased, and unsupervised approaches. Supervised approaches for WSD [14], [15] use a labeled dataset along with interesting lexical/syntactic features derived from the context around the term to build machine learned models that predict the correct sense in unseen test contexts. Knowledge-based approaches [1], [16] do not use any corpus but solely rely on thesauri or sense inventories such as WordNet and the Unified Medical Language System (UMLS) that contain brief definitions of different senses and corresponding synonyms. Unsupervised approaches may employ topic modeling [17] based methods to disambiguate when the senses are known ahead of time. Some unsupervised approaches [18] are often referred to as performing word sense discrimination or\nar X\niv :1\n61 0.\n08 55\n7v 5\n[ cs\n.C L\n] 3\n0 Se\np 20\n17\ninduction as opposed to disambiguation because they employ clustering approaches where different clusters are expected to represent the different senses, which are not known a priori."
    }, {
      "heading" : "A. WSD in Biomedicine",
      "text" : "In biomedicine, knowledge-based word sense disambiguation efforts mostly relied on the UMLS knowledge base [19], which contains over 3.4 million unique concepts expertly sourced from ≈ 200 different terminologies in biomedicine and allied fields. The UMLS is maintained by the US National Library of Medicine (NLM) and is updated every year to reflect new concepts and other changes. For each concept in the UMLS, there is usually a brief definition and sometimes additional relations (both hierarchical and associative) connecting it with other concepts. Each concept has a unique ID called the concept unique identifier (CUI), an alphanumeric string that starts with a ‘C’. For example, the sense S1 (administrative process) for ‘discharge’ in Section I is represented by CUI C0030685 and sense S2 (body substance) is represented by the CUI C0012621. S1 has a short definition “The administrative process of discharging the patient, alive or dead, from hospitals or other health facilities”. For S2 we notice the definition – “In medicine, a fluid that comes out of the body. Discharge can be normal or a sign of disease.” In the MSH WSD dataset that we use in this effort, the candidate senses for each ambitious word are represented in the form of these unique CUIs. The task is to identify the correct CUI given a particular context (few sentences) containing an ambiguous term. For the rest of the manuscript, we use the three terms CUI, concept, and sense synonymously as they refer to the same notion.\nSchuemie et al. [20] present a nice survey of approaches and efforts in biomedical WSD until 2005 including the wellknown NLM WSD dataset [21], which has 50 ambiguous terms with 5000 test instances. Disambiguation efforts were also focused on a small set of 10–15 ambiguous abbreviations [22], [23] using combinations of supervised and unsupervised approaches. More recent approaches [24], [25] used supervised models including Naive Bayes, SVMs, logistic regressors, decision lists with a variety of features using both subsets of the NLM WSD dataset and other smaller datasets. Berster et al. [26] encoded senses, contexts, and ambiguous terms using random indexing and conducted supervised ten-fold cross validation experiments on the NLM WSD dataset using the binary splatter code method. McInnes and Pedersen [16] used the network structure of the UMLS (specifically the hypernymic trees) and concept definitions to devise concept relatedness measures which are in turn used for WSD for the MSH WSD dataset. Chasin et al. [27] demonstrated the application of topic modeling for a clinical WSD dataset of 50 ambiguous terms curated from the Mayo Clinic [25]. Recently, Wang et al. [28] used an active learning strategy to involve domain experts in an interactive supervised machine learning framework for biomedical WSD. Among all the datasets available, the MSH WSD that we use in our current effort is the largest publicly available dataset [1] for biomedical WSD (more in Section III) and also has the least skewed distribution (the average percentage of majority sense is 54% [28]).\nIn a recent approach Jimeno-Yepes and Berlanga [2] used a hybrid approach that combined a knowledge-based component that exploits the UMLS definitions and synonyms for different\nconcepts with unlabeled biomedical narratives (from Medline/PubMed) to derive word-concept probability estimates P (w|c) for any word w and UMLS concept c. They exploited the Naive Bayes formulation and selected the correct sense as the CUI c that maximizes P (T |c) = ∏ i P (wi|c), where wi is the i-th word in the test context T that contains the ambiguous term. With this approach they achieved an accuracy of 89.1% on the MSH WSD dataset [1]. This result corresponds to the best performance thus far on the MSH WSD dataset without using supervised models. Given we employ this method as a component of our best model, for completeness, we provide its high level summary in the Appendix.\nIn this effort, we use recent advances in neural word embeddings to generate new state-of-the-art results on the MSH WSD dataset achieved without supervised cross validation experiments. Our methods can be classified as weakly supervised given we employ the well-known biomedical concept mapping tool MetaMap [29] to generate concept vectors and employ them in combination with the knowledge-based method from Jimeno-Yepes and Berlanga [2]."
    }, {
      "heading" : "B. Neural Embeddings for WSD",
      "text" : "Neural word representations have been shown to capture both semantic and syntactic information and a few recent approaches learn word vectors [30]–[32] (as elements of Rd, where d is the dimension) in an unsupervised fashion from textual corpora. These dense word vectors obviate the sparsity issues inherent to the so called one-hot representations of words1. Chen et al. [34] adapted neural word embeddings to compute different sense embeddings (of the same word) and showed competitive performance on the SemEval 2007 WSD dataset [35]. Disambiguation is achieved by picking the sense that maximizes the cosine similarity of the corresponding sense vector with the context vector for an ambiguous term. Recently, Iacobacci et al. [36] evaluated and demonstrated the superiority of neural word embeddings as features in supervised WSD models on the same SemEval dataset.\nIn a very recent effort Pakhomov et al. [37] use word embeddings (without corpus enhanced concept embeddings) for the MSH WSD dataset but only report 77% accuracy although the central aim of their paper is not limited to WSD. Their approach relies on vectors of words that co-occur with words in the definitions of different senses (CUIs) in the UMLS. In our effort, we use a similar framework as Chen et al. [34] to directly learn sense vectors using a pure distributional semantics framework that does not rely on word vectors. Additionally, we employ complementary evidences beyond cosine similarity to achieve further improvements that rival performances typically reported using supervised approaches."
    }, {
      "heading" : "III. OUR APPROACH",
      "text" : "There are 203 ambiguous terms in the MSH WSD dataset [1] with a total of 424 unique CUIs (from the UMLS), each of which is a unique sense. Thus, on average, the dataset has 424/203 = 2.08 senses. There are a total of 38,495 test instances of contexts (a few sentences) each with one\n1One-hot representations lead to very large dimensionality (typically the size of the vocabulary) resulting in further issues in similarity computations, a phenomenon often termed as the curse of dimensionality [33, Chapter 1.4]\nof the 203 ambiguous terms along with the correct sense (CUI). Besides being the largest biomedical WSD dataset, it also includes a richer set of ambiguities including 106 ambiguous abbreviations, 88 ambiguous noun phrases, and 9 that are combinations of both. Due to these features, the NLM encourages researchers to use this dataset over their older dataset (please see https://wsd.nlm.nih.gov). Our goal is to directly test on this dataset by employing weakly supervised approaches. For this, we learn vector representations of words and CUIs using well-known approaches that apply deep neural networks to NLP tasks."
    }, {
      "heading" : "A. Neural Word and Concept Embeddings",
      "text" : "We ran the well-known word2vec [32] word embedding program (the skip-gram model) from Google on over 20 million biomedical citations (titles and abstracts) from PubMed to obtain word vector representations with a word window size of ten words and dimensionality d = 300 with all other parameters set to the default settings. To learn concept or CUI vectors of the same dimensionality, we curated a dataset of five million randomly chosen citations (published between 1998 and 2014). For this subset of PubMed, we ran MetaMap [29] with its WSD option turned on so we obtain unique CUIs for potential ambiguous terms2. The text was passed through MetaMap two adjacent non-stop words at a time, to capture as many CUIs as possible. Next, we treated these sequences of CUIs in each citation thus obtained through NER as a semantic version of the free text corpus. We ran word2vec on this corpus of CUI texts, just like how we ran it on free text articles with the same parameters. As a result we obtained 300 dimensional dense vectors for each CUI, including all 424 CUIs corresponding to the 203 ambiguous terms in our test set. This component of our methodology to derive dense concept vectors involves weak supervision because although MetaMap with its WSD option is in and of itself not a powerful solution (see Section IV), it nevertheless was useful to learn concept vectors that in turn helped us achieve state-ofthe-art results. This deep neural network based distributional semantics approach to learning CUI vectors aids in modeling complementary aspects of similarity. This is because we use, as a component, the CUI definition based information via the word-probability estimate based approach [2] outlined earlier."
    }, {
      "heading" : "B. WSD with Word/Concept Embeddings and KnowledgeBased Approaches",
      "text" : "Our main idea is that besides comparing pairs of word vectors and concept vectors, we can also compare a word vector with a concept vector given at a high level there is a direct connection between words and concepts – words are often lexical manifestations of high level concepts. The fact that we simply replaced word sequences in free text with the corresponding concept sequences to generate CUI vectors of the same dimensionality as the word vectors also makes it feasible to compare word vectors and their compositions to concept vectors. As will see in Section IV, this intuition appears to work as well as other state-of-the-art approaches [2].\n2MetaMap uses the UMLS knowledge base of concept synonyms along with shallow linguistic parsing to map free text to UMLS CUIs. It also has a WSD option which is based on concept profiles generated through words co-occurring with different concepts in biomedical literature [38].\nWe establish some notation for the rest of the paper. In any WSD problem, a test instance corresponds to a three tuple (T,w,C(w)) where T is a context, typically a few sentences, that contains the ambiguous term w and C(w) is the set of different senses that w can assume depending upon the context T 3. Specifically, C(w) in this effort is the set of different CUIs that capture the different senses for w. Our WSD goal is to construct a function f(T,w,C(w)) that maps T to the CUI c ∈ C(w) that corresponds to the correct sense in which w was used in T . We have four approaches that apply the embeddings from Section III-A to our test set. We specify them in terms of functions f?(T,w,C(w)) where ? indicates symbols that identify the underlying method(s) used, made clear as follows.\n1) Our first approach uses vector cosine similarity with\nf c(T,w,C(w)) = argmax c∈C(w) cos(~Tavg,~c),\nwhere ~Tavg is the average of non-stop words’ vectors in the context T and ~c is the context vector for c. This formulation is well-known given cosine similarity is a popular approach to measure semantic similarity of entities (words, concepts, . . . ) represented by the corresponding vectors.\n2) Our second approach is based on vector projections with\nfp = argmax c∈C(w)\n[ ρ[cos(~Tavg,~c)] ·\n‖P(~Tavg,~c)‖ ‖~c‖\n] ,\nwhere P(~r,~s) refers to the projection of ~r on to ~s, ‖ ‖ is the Euclidean norm, and ρ is the sign function. Using straightforward manipulation based on vector projections in Euclidean spaces [39, Chapter 5], we have\n‖P(~Tavg,~c)‖ = |~Tavg • ~c| ‖~c‖ ,\nwhich is what we used in our implementation (with • denoting vector dot product). Although f c (approach one) accounts for the overall directional similarity (thematic orientation) of the vectors, it does not account for the strength or magnitude of association, an aspect that seems ignored in others’ efforts we reviewed for this paper. By considering the vector projection of the context vector onto the CUI vector ~c, in fp we also account for the magnitude of the context vector’s projection in relation to that of the CUI vector. The sign function ρ is essentially to account for situations when 90 < θ ≤ 180, the angle between ~Tavg and ~c.\n3) Our third approach is based on the first two approaches where we set\nf c,p = argmax c∈C(w)\n[ cos(~Tavg,~c) ·\n‖P(~Tavg,~c)‖ ‖~c‖\n] .\nWe simply incorporate both evidences (magnitude and orientation of association) to compare different CUIs.\n3In practice, there might be cases where the context in T is deemed insufficient even for human judges to pick the right sense. However, for this paper we assess our performance based on MSH WSD dataset where each instance is assigned a unique sense.\n4) Our final approach uses a probabilistic model developed in an earlier effort by Jimeno-Yepes and Berlanga [2] (as outlined in Section II-A and elaborated in the Appendix) that selects the c that maximizes P (T |c). We involve this knowledge-based approach as a third scoring component and set\nf c,p,k\n= argmax c∈C(w)\n[ cos(~Tavg,~c) ·\n‖P(~Tavg,~c)‖ ‖~c‖ + P (T |c)\n] .\nAlthough there are different ways of combining evidences from multiple sources of predictive information, we rely on this straightforward combination as a form of unsupervised rank aggregation from two different sources.\nThe methods discussed thus far can be summarized using the schematic in Figure 1."
    }, {
      "heading" : "C. WSD with Weak Supervision",
      "text" : "From methods in Section III-B, we have multiple ways of disambiguating CUIs for any ambiguous term given a sample context. We exploit them to build a weakly supervised dataset for the 203 ambiguous terms in our test dataset. For each sentence in an independent corpus of biomedical citations that contains any ambiguous term from our dataset, we employ methods in Section III-B to assign the predicted CUI. Thus we can create a weakly supervised dataset for each ambiguous term with thousands of examples if we choose a large corpus. These examples can then be used to train traditional discriminative models or nearest neighbor models. We emphasize here that we are proposing to label arbitrary sentences (not our test sentences) in an external corpus based on our methods in Section III-B. Hence we still have our full MSH WSD dataset to finally test the approach we propose here with other models in a fair way.\nFor the k nearest neighbor (k-NN) model, let Dw ⊆ D be the set of instances for the ambiguous term w in the weakly supervised dataset D. We rank instances (D,w, c) ∈ Dw for a given test instance T based on cos(~Tavg, ~Davg), where c is the sense assigned to D from C(w) based on methods in\nSection III-B. Let Rk(Dw) be the set of top k instances in Dw when ranked in descending order based on cos(~Tavg, ~Davg). Now the predicted sense for T is chosen based on\nfk−NN = argmax c∈C(w)  ∑ (D,w,c)∈Rk(Dw) cos(~Tavg, ~Davg)  . The expression in the argmax boils down to summing up the similarities of the test context with those contexts in the training dataset that have the same assigned CUI c. We subsequently pick the particular c that maximizes that summation. Intuitively, our approach aggregates evidence from training instances that are semantically most similar to our test instance. The choice of k also plays an important role in the performance of k-NN approaches as we observe in the next section."
    }, {
      "heading" : "IV. RESULTS AND DISCUSSION",
      "text" : "Our results are shown in Table I based on methods introduced in the previous section. MetaMap does not perform as well on this dataset (row 1) even with the WSD option achieving an accuracy4 of 81.77%. However, it may not be fair to compare MetaMap with our methods given it does not try to particularly disambiguate our specific 203 terms, for each of which we are already given candidate concepts that contain the correct sense. In row 2 of the table, we show the performance achieved by Jimeno-Yepes and Berlanga using word-concept probability estimates P (w|c) derived from synonymous names of concepts in the UMLS Metathesaurus.\nRows 3–6 show performances of our methods that leverage neural word/concept embeddings from Section III-B. The cosine similarity and projection approaches both score above 85% but when used together, they achieve an accuracy of 89.26% which is slightly better than the current best result [2] achieved through unsupervised and knowledge-based approaches. Row 6 shows an accuracy of 92.24% achieved by\n4Accuracy is the ratio of total number of correctly assigned senses to the total of number of occurrences of the 203 ambiguous terms in the MSH WSD dataset. The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.\nour ensemble method that combines our word/concept vector approach with the knowledge-based method by Jimeno-Yepes and Berlanga [2]. The time complexity of these methods is linear in terms of the number of words in the test context T and the number of candidate senses |C(w)|, considering the computation of ~Tavg and evaluation of the argmax expressions for each c ∈ C(w).\nWe created a weakly supervised dataset as outlined in Section III-C with the same corpus of five million biomedical citations used for training word and concept vectors (Section III-A). From this corpus, we considered the so called utterances that represent clauses (from the input text) that MetaMap outputs as distinct fragments with the corresponding CUIs. For each utterance that contains an ambiguous term in our test set, we apply our best linear method f c,p,k (corresponding to row\n6 of Table I) to assign one specific CUI from all possible candidates. There were seven million such utterances, with an average length of 18 words, that contained an ambiguous term out of a total of 78 million utterances from the corpus. Given our prior experiences in convolutional neural networks (CNNs) in biomedical text classification [40] that proved superior over traditional linear classifiers such as support vector machines and logistic regression models, we built 203 multiclass CNN models, one for each ambiguous term based on this weakly supervised dataset. The configuration of the CNN and its various hyper parameters were determined as per our prior effort [40, Sections 3.2 and 4.2]. This setup however resulted in accuracy of 87.78% which does not match the performance of simpler approaches (rows 4–6 of Table I).\nWe finally applied the k-NN approach outlined in Section III-C with the weakly supervised dataset with the number of nearest neighbors k ∈ {20, 50, 100, 200, 300, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000}. The corresponding accuracies are plotted as shown in Figure 2. We obtained the best accuracy of 94.34% when k = 3500 as shown in the last row of Table I. Overall, the accuracy rapidly increases as the numbers of neighbors used increase. The gains become smaller as more neighbors are added, reaching the top score at k = 3500 after which the accuracy descends abruptly. At k = 5000, the accuracy is same as that achieved with k = 300. This phenomenon is not surprising – at first more neighbors contribute to additional evidence, consistency, and robustness against noise in comparing the candidate concepts. However, considering an increasing number of neighbors at some point also leads to the semantic drift of their content from that of the test context. So neighbors ranked further down the list negatively affect the prediction given they are not as related to the test context, thus lowering overall accuracy. We realize that the value of k = 3500 is specific to this biomedical\ndataset and that there could be a value 3000 < k < 4000 that achieves a slightly higher accuracy. Our analysis is essentially a proof of concept for the high-level nearly monotonous nature of performance of k-NN based approaches. Given that there are over 38,000 test instances in our dataset, we believe k ≈ 3500 is appropriate in domains with similar characteristics (e.g., average number of senses per word, distributions of senses, and average length of test contexts). However, researchers may be able to derive more appropriate k values for their domains if they have access to relevant datasets.\nFinally, it is well known that k-NN approaches are infamous for high test time complexity because of the nearest neighbor search in high dimensional space. Our implementation involves cosine similarity computation with all training instances for the corresponding ambiguous term. In this effort, on average there are nearly 40,000 training instances created through weak supervision per ambiguous term. So given a new test instance (T,w,C(w)), cosine similarity (of 300 dimensional vectors) needs to be computed for the test instance T with about 40,000 contexts to impose the ranking on these potential neighbors. The threshold of a chosen k (say, 3500) can only be applied after this ranking is created. However, this similarity computation can be parallelized in a straightforward manner by distributing the similarity computations across multiple processors and pooling the results to incrementally build the ranked neighbor list. Although real time disambiguation may not be feasible, having the k-NN models run overnight every day to address disambiguation in new articles may be practical. Alternative approaches such as locality sensitive hashing [41] that address the dimensionality problems without having to compute cosine similarities may be helpful to alleviate the situation. Overall, however, it is clear that k-NN based approaches with weakly supervised datasets offer an interesting alternative to purely supervised approaches in biomedical WSD."
    }, {
      "heading" : "V. CONCLUSION",
      "text" : "Biomedical WSD is an important task with implications for downstream components in NLP applications. In this effort, we applied recent approaches in neural word embeddings to construct concept embeddings. Our linear time method uses these embeddings to combine cosine similarity, projection magnitude proportion, and a prior knowledge-based approach to produce an accuracy of 92.24%. This is an absolute 3% improvement over just using the knowledge-based approach, which generated the previous best result obtained without supervised learning. Based on predictions from our best linear method, we created a new weakly supervised dataset and built a k-NN model that achieves an accuracy of 94.34%.\nOur results rival performances achieved by supervised approaches – the best published supervised result achieves 93% macro accuracy over ten fold cross validation experiments on the MSH WSD dataset with the Naive Bayes model [1]. Based on additional ten fold cross validation experiments with support vector machines that use neural word vector features, Jimeno-Yepes [42] was able to achieve close to 96% macro accuracy. However, we cannot directly compare these cross validation results (from the supervised experiments) to the 94.34% accuracy we obtained without supervision in this paper. Specifically, in each iteration, the cross validation\nframework tests only on one-tenth of MSH WSD dataset by training on the remaining nine folds of the dataset. In our method, we test on the full MSH WSD dataset without using any of it for training.\nOverall, our results in this paper contribute new evidence that dense neural embeddings function as useful representations of textual data for biomedical NLP applications. Furthermore, they also showcase the potential of knowledge-based approaches in learning better dense vector representations (via MetaMap that uses UMLS) and their complementary contributions to WSD tasks. We conclude with some limitations and future research directions.\n• Although linear method’s accuracy is above 92.24%, there is still room for improvement in terms of incorporating modifications that account for sense level errors. That is, in addition to accuracy, for each ambiguous term, if we study the errors (false positives, false negatives) associated with each of its senses, we might be able to modify our approaches to account for any common patterns in which errors manifest. This task involves manual qualitative analysis with over 400 unique senses in the MSH WSD dataset and is a important future research direction.\n• In Section III-B, the test context ~Tavg is the vector formed by element-wise averaging of the word vectors of the corresponding words in the test context T . Although averaging is simple and intuitive, it may not be the best representation of the semantic content of the narrative in the test context. As such, more powerful alternatives that can better represent information in the context sentences might be helpful. To this end, one option is to directly model paragraphs as fixed size vectors using a word2vec style unsupervised learning architecture as demonstrated by Le and Mikolov [43] where paragraph vectors are learned along with word vectors.\n• A second approach is to consider a weighted average of the word vectors corresponding to tokens in the context vector where the weight selected for a word is inversely proportional to its distance from the ambiguous term w in the test context. Besides word vectors, we can also compute the weighted average of concept vectors associated with the CUIs (other than those associated with w) in the test context. The weighted averages of the words and contextual CUIs can then be compared separately with the candidate concept vectors from C(w) to generate two different scores ∈ [0, 1] whose sum can form the final score to select the correct sense.\n• Both the paragraph vector approach [43] and the weighted averaging approach discussed earlier do not explicitly model word order when composing test context words to form fixed size vectors that better capture the semantics of the full context. Recurrent neural networks (RNNs [44, Chapter 3]), especially with long short-term memory units [45], are a more suitable alternative for such scenarios but would need training data to set the parameters of the recurrent layer. The dataset created using weak supervision in Section III-C can be used here to estimate RNN parameters corresponding to the model for each ambiguous term."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "Our work is primarily supported by the National Library of Medicine through grant R21LM012274. We are also supported by the National Center for Advancing Translational Sciences through grant UL1TR001998 and the Kentucky Lung Cancer Research Program through grant PO2 41514000040001. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH."
    } ],
    "references" : [ {
      "title" : "Exploiting MeSH indexing in MEDLINE to generate a data set for word sense disambiguation",
      "author" : [ "A. Jimeno-Yepes", "B.T. McInnes", "A.R. Aronson" ],
      "venue" : "BMC bioinformatics, vol. 12, no. 223, 2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Knowledge based word-concept model estimation and refinement for biomedical text mining",
      "author" : [ "A. Jimeno-Yepes", "R. Berlanga" ],
      "venue" : "Journal of biomedical informatics, vol. 53, pp. 300–307, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bridging semantics and syntax with graph algorithm – state-of-the-art of extracting biomedical relations",
      "author" : [ "Y. Luo", "Ö. Uzuner", "P. Szolovits" ],
      "venue" : "Briefings in bioinformatics, p. bbw001, 2016.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Context-driven automatic subgraph creation for literature-based discovery",
      "author" : [ "D. Cameron", "R. Kavuluru", "T.C. Rindflesch", "A.P. Sheth", "K. Thirunarayan", "O. Bodenreider" ],
      "venue" : "Journal of biomedical informatics, vol. 54, pp. 141–157, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An up-to-date knowledge-based literature search and exploration framework for focused bioscience domains",
      "author" : [ "R. Kavuluru", "C. Thomas", "A.P. Sheth", "V. Chan", "W. Wang", "A. Smith", "A. Soto", "A. Walters" ],
      "venue" : "Proc. of the 2nd ACM SIGHIT Health Informatics Symposium. ACM, 2012, pp. 275–284.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Mayo clinical text analysis and knowledge extraction system (cTAKES)",
      "author" : [ "G.K. Savova", "J.J. Masanz", "P.V. Ogren", "J. Zheng", "S. Sohn", "K.K. Schuler", "C.G. Chute" ],
      "venue" : "Journal of the American Medical Informatics Association, vol. 17, no. 5, pp. 507–513, 2010.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text",
      "author" : [ "Ö. Uzuner", "B.R. South", "S. Shen", "S.L. DuVall" ],
      "venue" : "Journal of the American Medical Informatics Association, vol. 18, no. 5, pp. 552–556, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records",
      "author" : [ "R. Kavuluru", "A. Rios", "Y. Lu" ],
      "venue" : "Artificial intelligence in medicine, vol. 65, no. 2, pp. 155–166, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Utilizing social media data for pharmacovigilance: a review",
      "author" : [ "A. Sarker", "R. Ginn", "A. Nikfarjam", "K. O’Connor", "K. Smith", "S. Jayaraman", "T. Upadhaya", "G. Gonzalez" ],
      "venue" : "Journal of biomedical informatics, vol. 54, pp. 202–212, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Toward automated e-cigarette surveillance: Spotting e-cigarette proponents on Twitter",
      "author" : [ "R. Kavuluru", "A. Sabbir" ],
      "venue" : "Journal of biomedical informatics, vol. 61, pp. 19–26, 2016.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Social media mining for public health monitoring and surveillance.",
      "author" : [ "M. Paul", "A. Sarker", "J. Brownstein", "A. Nikfarjam", "M. Scotch", "K. Smith", "G. Gonzalez" ],
      "venue" : "Pacific Symposium on Biocomputing.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Word sense disambiguation improves information retrieval",
      "author" : [ "Z. Zhong", "H.T. Ng" ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 273–282.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Word sense disambiguation: A survey",
      "author" : [ "R. Navigli" ],
      "venue" : "ACM Computing Surveys (CSUR), vol. 41, no. 2, p. 10, 2009.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "It makes sense: A wide-coverage word sense disambiguation system for free text",
      "author" : [ "Z. Zhong", "H.T. Ng" ],
      "venue" : "Proceedings of the ACL 2010 System Demonstrations. Association for Computational Linguistics, 2010, pp. 78–83.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Disambiguation of biomedical text using diverse sources of information",
      "author" : [ "M. Stevenson", "Y. Guo", "R. Gaizauskas", "D. Martinez" ],
      "venue" : "BMC bioinformatics, vol. 9, no. 11, 2008.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Evaluating measures of semantic similarity and relatedness to disambiguate terms in biomedical text",
      "author" : [ "B.T. McInnes", "T. Pedersen" ],
      "venue" : "Journal of biomedical informatics, vol. 46, no. 6, pp. 1116–1124, 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Link-topic model for biomedical abbreviation disambiguation",
      "author" : [ "S. Kim", "J. Yoon" ],
      "venue" : "Journal of biomedical informatics, vol. 53, pp. 367– 380, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A sense-topic model for word sense induction with unsupervised data enrichment",
      "author" : [ "J. Wang", "M. Bansal", "K. Gimpel", "B.D. Ziebart", "T.Y. Clement" ],
      "venue" : "Transactions of the Association for Computational Linguistics, vol. 3, pp. 59–71, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Word sense disambiguation in the biomedical domain: an overview",
      "author" : [ "M.J. Schuemie", "J.A. Kors", "B. Mons" ],
      "venue" : "Journal of Computational Biology, vol. 12, no. 5, pp. 554–565, 2005.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Developing a test collection for biomedical word sense disambiguation.",
      "author" : [ "M. Weeber", "J.G. Mork", "A.R. Aronson" ],
      "venue" : "Proceedings of the AMIA Symposium. American Medical Informatics Association,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2001
    }, {
      "title" : "Abbreviation and acronym disambiguation in clinical discourse",
      "author" : [ "S. Pakhomov", "T. Pedersen", "C.G. Chute" ],
      "venue" : "AMIA Annual Symposium Proceedings, vol. 2005. American Medical Informatics Association, 2005, p. 589.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Combining corpus-derived sense profiles with estimated frequency information to disambiguate clinical abbreviations",
      "author" : [ "H. Xu", "P.D. Stetson", "C. Friedman" ],
      "venue" : "AMIA Annual Symposium Proceedings, vol. 2012. American Medical Informatics Association, 2012, p. 1004.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A multi-aspect comparison study of supervised word sense disambiguation",
      "author" : [ "H. Liu", "V. Teller", "C. Friedman" ],
      "venue" : "Journal of the American Medical Informatics Association, vol. 11, no. 4, pp. 320–331, 2004.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Word sense disambiguation across two domains: biomedical literature and clinical notes",
      "author" : [ "G.K. Savova", "A.R. Coden", "I.L. Sominsky", "R. Johnson", "P.V. Ogren", "P.C. De Groen", "C.G. Chute" ],
      "venue" : "Journal of biomedical informatics, vol. 41, no. 6, pp. 1088–1100, 2008.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Hyperdimensional computing approach to word sense disambiguation",
      "author" : [ "B.-T. Berster", "J.C. Goodwin", "T. Cohen" ],
      "venue" : "AMIA Annual Symposium Proceedings. American Medical Informatics Association, 2012, pp. 1129–1138.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Word sense disambiguation in the clinical domain: a comparison of knowledge-rich and knowledge-poor unsupervised methods",
      "author" : [ "R. Chasin", "A. Rumshisky", "O. Uzuner", "P. Szolovits" ],
      "venue" : "Journal of the American Medical Informatics Association, vol. 21, no. 5, pp. 842–849, 2014.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Clinical word sense disambiguation with interactive search and classification",
      "author" : [ "Y. Wang", "K. Zheng", "H. Xu", "Q. Mei" ],
      "venue" : "AMIA Annual Symposium Proceedings. American Medical Informatics Association, 2016, pp. 2062–2071.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "An overview of MetaMap: historical perspective and recent advances",
      "author" : [ "A.R. Aronson", "F.-M. Lang" ],
      "venue" : "Journal of the American Medical Informatics Association, vol. 17, no. 3, pp. 229–236, 2010.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin" ],
      "venue" : "The Journal of Machine Learning Research, vol. 3, pp. 1137–1155, 2003.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "R. Collobert", "J. Weston" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160–167.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 3111–3119.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Pattern recognition and machine",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2006
    }, {
      "title" : "A unified model for word sense representation and disambiguation",
      "author" : [ "X. Chen", "Z. Liu", "M. Sun" ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. ACL, 2014, pp. 1025–1035.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Semeval-2007 task 07: Coarse-grained english all-words task",
      "author" : [ "R. Navigli", "K.C. Litkowski", "O. Hargraves" ],
      "venue" : "Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics, 2007, pp. 30–35.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Embeddings for word sense disambiguation: An evaluation study",
      "author" : [ "I. Iacobacci", "M.T. Pilehvar", "R. Navigli" ],
      "venue" : "Proceedings of the  54th Annual Meeting of the Association for Computational Linguistics. ACL, 2016, pp. 897–907.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Corpus domain effects on distributional semantic modeling of medical terms",
      "author" : [ "S.V. Pakhomov", "G. Finley", "R. McEwan", "Y. Wang", "G.B. Melton" ],
      "venue" : "Bioinformatics, p. In Press, 2016.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Integration of UMLS and Medline in unsupervised word sense disambiguation",
      "author" : [ "A. Jimeno-Yepes", "A.R. Aronson" ],
      "venue" : "2012 AAAI fall symposium series, 2012, pp. 26–31.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Elementary Linear Algebra",
      "author" : [ "R. Larson", "D.C. Falvo" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2008
    }, {
      "title" : "Convolutional neural networks for biomedical text classification: application in indexing biomedical articles",
      "author" : [ "A. Rios", "R. Kavuluru" ],
      "venue" : "Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics. ACM, 2015, pp. 258–267.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Locality-sensitive hashing for finding nearest neighbors [lecture notes",
      "author" : [ "M. Slaney", "M. Casey" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 128–131, 2008.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Word embeddings and recurrent neural networks based on long-short term memory nodes in supervised biomedical word sense disambiguation",
      "author" : [ "A. Jimeno-Yepes" ],
      "venue" : "arXiv preprint arXiv:1604.02506, 2016.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Q. Le", "T. Mikolov" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1188–1196.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Supervised Sequence Labelling with Recurrent Neural Networks, ser",
      "author" : [ "A. Graves" ],
      "venue" : "Studies in Computational Intelligence. Springer,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In this paper, we employ knowledgebased approaches that also exploit recent advances in neural word/concept embeddings to improve over the state-of-the-art in biomedical WSD using the public MSH WSD dataset [1] as the test set.",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 1,
      "context" : "24% which is a 3% improvement over the best known results [2] obtained via unsupervised means.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]–[5], clinical narratives [6]–[8] and health related social media posts [9]–[11].",
      "startOffset" : 237,
      "endOffset" : 240
    }, {
      "referenceID" : 4,
      "context" : "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]–[5], clinical narratives [6]–[8] and health related social media posts [9]–[11].",
      "startOffset" : 241,
      "endOffset" : 244
    }, {
      "referenceID" : 5,
      "context" : "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]–[5], clinical narratives [6]–[8] and health related social media posts [9]–[11].",
      "startOffset" : 266,
      "endOffset" : 269
    }, {
      "referenceID" : 7,
      "context" : "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]–[5], clinical narratives [6]–[8] and health related social media posts [9]–[11].",
      "startOffset" : 270,
      "endOffset" : 273
    }, {
      "referenceID" : 8,
      "context" : "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]–[5], clinical narratives [6]–[8] and health related social media posts [9]–[11].",
      "startOffset" : 312,
      "endOffset" : 315
    }, {
      "referenceID" : 10,
      "context" : "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]–[5], clinical narratives [6]–[8] and health related social media posts [9]–[11].",
      "startOffset" : 316,
      "endOffset" : 320
    }, {
      "referenceID" : 11,
      "context" : "Recent research also shows that resolving ambiguities provides performance gains in information retrieval and search system design [12].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "In this effort, we employ knowledge-based methods, neural concept and word vectors learned through unsupervised deep learning approaches, and a straightforward nearest neighbor approach to achieve new state-of-the-art results over a public gold standard dataset [1] in biomedical word sense disambiguation (WSD).",
      "startOffset" : 262,
      "endOffset" : 265
    }, {
      "referenceID" : 12,
      "context" : "For a thorough overview of approaches to WSD, we direct the readers to the survey by Navigli [13], which suggests mainly three categories – supervised, knowledgebased, and unsupervised approaches.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "Supervised approaches for WSD [14], [15] use a labeled dataset along with interesting lexical/syntactic features derived from the context around the term to build machine learned models that predict the correct sense in unseen test contexts.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 14,
      "context" : "Supervised approaches for WSD [14], [15] use a labeled dataset along with interesting lexical/syntactic features derived from the context around the term to build machine learned models that predict the correct sense in unseen test contexts.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "Knowledge-based approaches [1], [16] do not use any corpus but solely rely on thesauri or sense inventories such as WordNet and the Unified Medical Language System (UMLS) that contain brief definitions of different senses and corresponding synonyms.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "Knowledge-based approaches [1], [16] do not use any corpus but solely rely on thesauri or sense inventories such as WordNet and the Unified Medical Language System (UMLS) that contain brief definitions of different senses and corresponding synonyms.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 16,
      "context" : "Unsupervised approaches may employ topic modeling [17] based methods to disambiguate when the senses are known ahead of time.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "Some unsupervised approaches [18] are often referred to as performing word sense discrimination or ar X iv :1 61 0.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 18,
      "context" : "[20] present a nice survey of approaches and efforts in biomedical WSD until 2005 including the wellknown NLM WSD dataset [21], which has 50 ambiguous terms with 5000 test instances.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] present a nice survey of approaches and efforts in biomedical WSD until 2005 including the wellknown NLM WSD dataset [21], which has 50 ambiguous terms with 5000 test instances.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 20,
      "context" : "Disambiguation efforts were also focused on a small set of 10–15 ambiguous abbreviations [22], [23] using combinations of supervised and unsupervised approaches.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : "Disambiguation efforts were also focused on a small set of 10–15 ambiguous abbreviations [22], [23] using combinations of supervised and unsupervised approaches.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : "More recent approaches [24], [25] used supervised models including Naive Bayes, SVMs, logistic regressors, decision lists with a variety of features using both subsets of the NLM WSD dataset and other smaller datasets.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 23,
      "context" : "More recent approaches [24], [25] used supervised models including Naive Bayes, SVMs, logistic regressors, decision lists with a variety of features using both subsets of the NLM WSD dataset and other smaller datasets.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 24,
      "context" : "[26] encoded senses, contexts, and ambiguous terms using random indexing and conducted supervised ten-fold cross validation experiments on the NLM WSD dataset using the binary splatter code method.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "McInnes and Pedersen [16] used the network structure of the UMLS (specifically the hypernymic trees) and concept definitions to devise concept relatedness measures which are in turn used for WSD for the MSH WSD dataset.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 25,
      "context" : "[27] demonstrated the application of topic modeling for a clinical WSD dataset of 50 ambiguous terms curated from the Mayo Clinic [25].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[27] demonstrated the application of topic modeling for a clinical WSD dataset of 50 ambiguous terms curated from the Mayo Clinic [25].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 26,
      "context" : "[28] used an active learning strategy to involve domain experts in an interactive supervised machine learning framework for biomedical WSD.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Among all the datasets available, the MSH WSD that we use in our current effort is the largest publicly available dataset [1] for biomedical WSD (more in Section III) and also has the least skewed distribution (the average percentage of majority sense is 54% [28]).",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 26,
      "context" : "Among all the datasets available, the MSH WSD that we use in our current effort is the largest publicly available dataset [1] for biomedical WSD (more in Section III) and also has the least skewed distribution (the average percentage of majority sense is 54% [28]).",
      "startOffset" : 259,
      "endOffset" : 263
    }, {
      "referenceID" : 1,
      "context" : "In a recent approach Jimeno-Yepes and Berlanga [2] used a hybrid approach that combined a knowledge-based component that exploits the UMLS definitions and synonyms for different concepts with unlabeled biomedical narratives (from Medline/PubMed) to derive word-concept probability estimates P (w|c) for any word w and UMLS concept c.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "1% on the MSH WSD dataset [1].",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "Our methods can be classified as weakly supervised given we employ the well-known biomedical concept mapping tool MetaMap [29] to generate concept vectors and employ them in combination with the knowledge-based method from Jimeno-Yepes and Berlanga [2].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "Our methods can be classified as weakly supervised given we employ the well-known biomedical concept mapping tool MetaMap [29] to generate concept vectors and employ them in combination with the knowledge-based method from Jimeno-Yepes and Berlanga [2].",
      "startOffset" : 249,
      "endOffset" : 252
    }, {
      "referenceID" : 28,
      "context" : "Neural word representations have been shown to capture both semantic and syntactic information and a few recent approaches learn word vectors [30]–[32] (as elements of R, where d is the dimension) in an unsupervised fashion from textual corpora.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 30,
      "context" : "Neural word representations have been shown to capture both semantic and syntactic information and a few recent approaches learn word vectors [30]–[32] (as elements of R, where d is the dimension) in an unsupervised fashion from textual corpora.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 32,
      "context" : "[34] adapted neural word embeddings to compute different sense embeddings (of the same word) and showed competitive performance on the SemEval 2007 WSD dataset [35].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34] adapted neural word embeddings to compute different sense embeddings (of the same word) and showed competitive performance on the SemEval 2007 WSD dataset [35].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 34,
      "context" : "[36] evaluated and demonstrated the superiority of neural word embeddings as features in supervised WSD models on the same SemEval dataset.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[37] use word embeddings (without corpus enhanced concept embeddings) for the MSH WSD dataset but only report 77% accuracy although the central aim of their paper is not limited to WSD.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[34] to directly learn sense vectors using a pure distributional semantics framework that does not rely on word vectors.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "There are 203 ambiguous terms in the MSH WSD dataset [1] with a total of 424 unique CUIs (from the UMLS), each of which is a unique sense.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 30,
      "context" : "We ran the well-known word2vec [32] word embedding program (the skip-gram model) from Google on over 20 million biomedical citations (titles and abstracts) from PubMed to obtain word vector representations with a word window size of ten words and dimensionality d = 300 with all other parameters set to the default settings.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 27,
      "context" : "For this subset of PubMed, we ran MetaMap [29] with its WSD option turned on so we obtain unique CUIs for potential ambiguous terms2.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "This is because we use, as a component, the CUI definition based information via the word-probability estimate based approach [2] outlined earlier.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "As will see in Section IV, this intuition appears to work as well as other state-of-the-art approaches [2].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 36,
      "context" : "It also has a WSD option which is based on concept profiles generated through words co-occurring with different concepts in biomedical literature [38].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : "4) Our final approach uses a probabilistic model developed in an earlier effort by Jimeno-Yepes and Berlanga [2] (as outlined in Section II-A and elaborated in the Appendix) that selects the c that maximizes P (T |c).",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "26% which is slightly better than the current best result [2] achieved through unsupervised and knowledge-based approaches.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 22,
      "context" : "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 35,
      "context" : "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 25,
      "context" : "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "Jimeno-Yepes and Berlanga [2] 89.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "Combining f , f, and [2] (f ) 92.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "our ensemble method that combines our word/concept vector approach with the knowledge-based method by Jimeno-Yepes and Berlanga [2].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 38,
      "context" : "Given our prior experiences in convolutional neural networks (CNNs) in biomedical text classification [40] that proved superior over traditional linear classifiers such as support vector machines and logistic regression models, we built 203 multiclass CNN models, one for each ambiguous term based on this weakly supervised dataset.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 39,
      "context" : "Alternative approaches such as locality sensitive hashing [41] that address the dimensionality problems without having to compute cosine similarities may be helpful to alleviate the situation.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "Our results rival performances achieved by supervised approaches – the best published supervised result achieves 93% macro accuracy over ten fold cross validation experiments on the MSH WSD dataset with the Naive Bayes model [1].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 40,
      "context" : "Based on additional ten fold cross validation experiments with support vector machines that use neural word vector features, Jimeno-Yepes [42] was able to achieve close to 96% macro accuracy.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 41,
      "context" : "To this end, one option is to directly model paragraphs as fixed size vectors using a word2vec style unsupervised learning architecture as demonstrated by Le and Mikolov [43] where paragraph vectors are learned along with word vectors.",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "The weighted averages of the words and contextual CUIs can then be compared separately with the candidate concept vectors from C(w) to generate two different scores ∈ [0, 1] whose sum can form the final score to select the correct sense.",
      "startOffset" : 167,
      "endOffset" : 173
    }, {
      "referenceID" : 41,
      "context" : "• Both the paragraph vector approach [43] and the weighted averaging approach discussed earlier do not explicitly model word order when composing test context words to form fixed size vectors that better capture the semantics of the full context.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 43,
      "context" : "Recurrent neural networks (RNNs [44, Chapter 3]), especially with long short-term memory units [45], are a more suitable alternative for such scenarios but would need training data to set the parameters of the recurrent layer.",
      "startOffset" : 95,
      "endOffset" : 99
    } ],
    "year" : 2017,
    "abstractText" : "Biomedical word sense disambiguation (WSD) is an important intermediate task in many natural language processing applications such as named entity recognition, syntactic parsing, and relation extraction. In this paper, we employ knowledgebased approaches that also exploit recent advances in neural word/concept embeddings to improve over the state-of-the-art in biomedical WSD using the public MSH WSD dataset [1] as the test set. Our methods involve weak supervision – we do not use any hand-labeled examples for WSD to build our prediction models; however, we employ an existing concept mapping program, MetaMap, to obtain our concept vectors. Over the MSH WSD dataset, our linear time (in terms of numbers of senses and words in the test instance) method achieves an accuracy of 92.24% which is a 3% improvement over the best known results [2] obtained via unsupervised means. A more expensive approach that we developed relies on a nearest neighbor framework and achieves accuracy of 94.34%, essentially cutting the error rate in half. Employing dense vector representations learned from unlabeled free text has been shown to benefit many language processing tasks recently and our efforts show that biomedical WSD is no exception to this trend. For a complex and rapidly evolving domain such as biomedicine, building labeled datasets for larger sets of ambiguous terms may be impractical. Here, we show that weak supervision that leverages recent advances in representation learning can rival supervised approaches in biomedical WSD. However, external knowledge bases (here sense inventories) play a key role in the improvements achieved.",
    "creator" : "LaTeX with hyperref package"
  }
}