{
  "name" : "1302.4935.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generating the Structure of a Fuzzy Rule under Uncertainty",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nIf we want to describe a system, it is necessary to know which are the inputs and the outputs of the system, and, more importantly, the relationship between them. This function, in most cases, is not easy to achieve, and in many others, it contains highly complicated mathematical relationships.\nSo, it would be interesting, if this input-output conec tion could be obtained directly from the performance of the system. Several approaches to automatic learn ing have been analysed (Michalski 1984), (Quinlan 1986).\nOver the last few years, the identification of systems has been carried out using fuzzy logic. This is possible since either we can not describe the system easily or for reasons of simplicity and efficiency. A set of fuzzy rules shall be worked out from a set of input-output ex amples of the system. This approach has been widely discussed: (De Mori 1980), (Sugeno 1991), (Campos 1993), (Delgado 1993), (Gonza.lez 199.5).\nBasically, the problem is finding a finite set of fuzzy rules able to reproduce the system's input-output be haviour. The rules shall be given in the form of:\nR: if X1 is A1 and . .. and Xn is An then Y is Bj.\nAs an input-output example, we shall take E = {e1, ... ,en,en+1}, where (e1, ... ,en) E (P (DE1) X ... x P(DEn )), and en+1 E P(Ds ), with P being the Power set and DE; and Ds are domains of the X; and Y variables, respectively.\nThe learning problem of a set of fuzzy rules from a set of input-output examples has two main aspects: structure identification and parameter identification. The former, tries to detect the linguistic label of the variables in the rule, and the latter gives a degree for each rule, indicating how well this rule matchies the examples.\nThe minimal set of variables, which must be present in the rule shall be obtained in this research. For in stance, let us suppose that the following two rules had been learnt:\nR1 : X1 is A 1\\ X2 is A'- Y is B [a uncertainty] R2 : X 1 is A 1\\ X 2 is A\" -> Y is B' [,B uncertainty ] ,\nthe X 2 variable shall be detected as being useless in the R1 rule, and so, the above rules shall be presented as:\nR� : X 1 is A - Y is B R2 : X 1 is A 1\\ X 2 is A\" -> Y is B' [I' uncertainty ] [,B uncertainty ] ,\nwhere the R1 rule has been substituted by the R� rule. Thus, the algorithm shows that, the certainty in which the R1 rule matchies the whole set of examples, is in cluded in the rule R�.\nThe method we propose, does not make any explicit distinction between parameter and structure identifi cation, but both are obtained simultaneously in the learning algorithm.\n64 Castro and Zurita\nThe identification model idea is based on the basic principles of an ATMS (De Kleer 1986). We shall use the assumption and premise, contradiction and ATMS node terms.\nTruth Maintenance Systems (TMS) are common tools in artificial intelligence for managing logical relation ships between beliefs and statements. A more recently proposed ATMS (De Kleer 1986) maintains multiple sets of beliefs simultaneously, thus allowing inferences in multiple contexts at the same time. The problem solver builds records of all the inferences mades (justi fications) and hypotheses it introduces (assumptions).\nThe task of the ATMS is to efficiently determine, given the inferences that have been made so far, all the pos sible contexts and their contents. An environment is a set of assumptions. An assumption may participate in more than one environment, thus environments may overlap and even subsume one another. Therefore the set of assumptions defines an environment lattice.\nEach environment defines a context. The context con sists of all and only the ATMS data nodes that, given the justifications in the ATMS, can be justified entirely on the basis of the nodes in the environment. Thus, all the nodes in a given environment are also members of the derived context.\nThe label of each ATMS data node is a set of environ ments, which is required to be: Consistent: No envi ronment in the label supports the derivation of false. Sound: For each environment in the label, the node must be included in the context defined by that en vironment; Complete: Any environment in the lattice whose context includes the node is a superset of some environment in the label. (Inconsistent environments have empy contexts); Minimal: No environment in the label is a subset of another.\nEvery ATMS node is a triple of the form (N, L, J). The first element is the name of the datum being managed by the problem solver. The second element L is the label. Finally, the third element is the set of justifica tions, J, having N as a consequence.\n2 DESCRIPTION OF THE APPROACH\nIf we have an input-output set of examples, simulat ing the behaviour of the system, a set of fuzzy rules and a certainty (uncertainty) value in each of them, reflecting how good is the rule with regards to the performance of the system. should be found using the learning algorithm.\nThe main idea is the following:\nThe ATMS allows the problem-solver to assume dif ferent fuzzy rules (assumptions), each of them shall be studied in relation to its identification with the set of examples taken, and, it could be rejected at any time if we find another rule which is \"better\" than the\nprevious one.\nThe algorithm has two important aspects. Firstly, the whole environments does not have to be kept in mem ory, due to the ATMS minimality property, some en vironments (fuzzy rules), shall be removed from the label in the presence of any other one recently exam ined. Secondly, there is no need to examine all the rules, since, if we obtain one rule, with a matching degree to the set of examples equal to 1, we shall ob viate, any other which is \"subsuming\" in it without even considering it.\nThe number of different restrictions (fuzzy linguistic labels) associated with a variable usually ranges from 5 to 9. We shall use seven, which represent, from the impossibility of the proposition to the maximun cred ibility in it. We can show that in Figure 1.\nHN MN SN z SP MP HP\nFigure 1: A set of seven linguistic labels.\nLet Y and Et = {HN,MN,SN.Z,SP,MP,HP} be the output classification variable and the set of linguis tic labels, respectively. Let S be the Power set of Et, denoted as P( Et), this represents the set of environ ments which describe the ATMS lattice. Let a be the size of S.\nThe learning process consists of:\nAll the fuzzy rules having for Y both Eti E Et as con sequent and any t E S as antecedent shall be worked out. With this aim in mind, an Eti shall be established for the consequent and each t E S shall be assumed (taken as an assumption) by the problem solver. The matching degree of the rule t __,. Yet;, with every one of the examples e j, j = 1 ... k is calculated. This de gree is obtained only in those examples that match (although it could be minimally) the set of predictive variables with the antecedent part of the rule. It is obvious that only these examples are considered, be cause the others are not identified at all by this fuzzy rule. But, of course, the latter should be identified by any other that must be found. Each Et; shall form an ATMS node (NyE, ), that has a matching degree greater than zero. Fin�lly, minimality criteria shall be established among the elements of the NyE,, label.\nLet us look at the set of input-output examples de picted in Table 1.\nWhen the problem solver assumes the environment t, E 5, the ATMS shall update a node for Yet, made up of:\nGenerating the Structure of a Fuzzy Rule under Uncertainty 65\nin which, the sequence of b1, ... , b,, ... , bi values means the matching degree between both the environments t1, ... , t,, ... , lj and the Examples1, ... , Examples., ... , Examplesi sets of examples, respectively. That is to say, with this node, the following sets of fuzzy rules are represented as:\nis____. YEt, [b8]; Vs = 1 ... j.\nThe b, is worked out by the problem-solver in the fol lowing way:\n• Cardinality of t, equal to n. Let us suposse that t, is:\nts = {Xt is Ett, ... , X, is Et11}, Et; E Et,\nthen, with each {e1,e2, ... ,f11,ey} example, the inference:\nshall be made.\nXn is Etn en\nY is Et;\nY is B\nThe fuzzy set B' is calculated by means of the compositional rule of inference ( CRI). In this case, with crisp values, it shall be reduced to:\nj.lB•(y) = J.lEt1(el) 1\\ · · . 1\\ JlEt.,(En ) 1\\pyE,,(y).\n• Cardinality of t, less than 11. Let us suppose that t, is:\nt, = {Xt is Eft, ... , Xp is Etp}, Et; E Et,\nSince p < n, then, only those variables which ap pear in (,, shall be chosen in the example, making the inference:\nXp is Etp ---.. Y is Et;\nY is B\nThe fuzzy set B' is calculated again as:\nJ..lB'(Y) =\nJ..lEt,(el) 1\\ · · . I\\ J..lEtp(ep) 1\\ J..lYEt, (y).\nDefinition 2.1.\nLet t E S be an environment. Let e be an input-output example. The matching degree oft to e, that will be annoted as M D1(e), is defined as:\nwith B' being the fuzzy set obtained as mentioned in the earlier detailed inferences.\nDefinition 2.2.\nLet t E S be an environment. Let M be a subset of the given input-output set of examples. The matching degree of t to M, that shall be an noted as MD M ( t), is defined as:\nGiven t. E S, Et; E Et and the oputput variable Y, the learning algorithm computes:\nTherefore the node relating to ex presion ( 1) is:\nNyEI;: ((Nf DExamples1 (tl)), · · · , NJ DExamples,(t,), · . . , Af D Examples j ( lj) )( l 1, . . . , is, ... , l j) (E:�.:amples1, ... , Examples., ... , Examplesj )).\nDefinition 2.3.\nThe label of a node NyE,; is said to be minimal, if and only if all the environments that subsume in others are removed from the label. An environment t; subsumes in another tj, if:\nsubsume(t;,tj) '¢:?\n{ t. c t· J - 1 1\\J De:ramp/es1(ij )) > M DExamples;(ii))\nfor each t;, lj E Label(NyE1.). That is to say, the environments giving a certainty in the proposition \"Y is Et;\" less than others (subsets of them) have, are removed.\nThe i; have different numbers of elements, therefore it is possible to consider t; instead of other t; which have\n66 Castro and Zurita\na greater quantity of variables. \\Vhen the problem solver assumes every t E 5, the corresponding fuzzy rule, with respect to t, is worked out by the ATMS. Thus, we should establish when we can obviate a \"longer\" rule, and taking instead another \"smaller\" one. In other words, when may we say that the R learning fuzzy rules set is minimal?\nDefinition 2.4.\nLet R be a learning fuzzy rules set. R. is said to be minimal if every rule, that subsumes in others, 1s re moved. Given two rules:\nRi : ti --> Y es Ets [o:] Ri : t i --. Y es Et s [,B]\nRi subsumes in Rj is defined as:\nsubsume( R;, RJ) ¢:> { tj � t;\n;3 > (�\nCorollary.\nR is minimal¢:> Label(NyE,,) is minimal.\nThe proof is evident, taking o: = Af DExamples1(tj) and (3 = A/ DExamples, (t; ).\n3 EXAMPLE\nThe goodness of the algorithm has been tested on the Iris Plant Database (Fisher 1936). This data base has been employed in many publications. Relevant infor mation about it, may be seen in Table 2.\nTable 2: Relevant Information of Iris Plant Database.\nNo. Cla::;ses No. Instances No. Predictive Attributes XO:Sepal Length (in em) Xl:Sepal \\Vidth (in em) X2:Petal Length (in em) X3:Sepal Width (in em) 3 150, 50 in each class 4 Min:4.3 Max:7.9 Ivlin:2.0 Max:4.4 Min:l.O Max:6.9 l\\Iin:0.1 l\\Iax:2.5\n\\Vhat we are trying to do is learn the characteristics of each plant. This will allow us to classify the four predictive attributes as belonging to one of the three classes. In order to do so, we shall use a subset, of the whole set of examples, as a training set (80 %) (120 entries), and the remaining examples shall be used as a testing set ( 20 %) ( 30 entries).\nThus, 120 examples have been chosen at random for training, and the following process has been carried out: the attributes of the iris-setosa plant have been\nlearnt from those that are not iris-setosa, the at tributes of the iris-versicolor plant have been learnt from those that are not iris-versicolor, and the at tributes of the iris-virginica plant have been learnt from those that are not iris-virginica. Therefore, three fuzzy rule bases have been obtained. Each of them must be able to classify every plant.\nBy using the algorithm proposed, 28 of the 30 exam ples given in the testing set ( ��) have been classified correctly, in other words, 93.33% is the learning rate of the algorithm. What is more, 1 example cannot be distinguished as belonging to one of the three classes, therefore only 1 example of the 30 supplied is classified erroneously.\nThe comparison with other classic algorithms is shown in Table 3.\nThis paper attempts to offer an approach for identi fying a fuzzy system from a set of data (input-output entries of the system). For this purpose, an ATMS was necessary. The assistance given by the ATMS (partic ularly because of its minimality property), allows the minimal structure of a rule to be found. So a learning algorithm has been developed.\nFinally, the model shown here, has been implemented (in C language) using a SUN-4 workstation. The proofs for the latter point have been accomplished with its help.\nAcknowledgements\nThe authors acknowledge the invaluable help of Pro fessor M. Delgado. They are also grateful to Professor .J .L. Verdegay and Professor E. Trillas for their valu able advice.\nReferences\nL. M. Campos and S. Moral (1993). Learning rules for a fuzzy inference model. Fuzzy Sets and Systems 59: 247-257.\nJ. L. Castro and J. M. Zurita (1995). An Inductive Learning Algorithm In Fuzzy Systems. Submitted to Fuzzy Sets and Systems.\nM. Delgado and A. Gonzalez (1993). An inductive learning procedure to identify fuzzy systems. Fuzzy Sets and Systems 55: 121-132.\nJ. De Kleer (1986). An assumption-based TMS. Arti ficial Intelligence 28: 127-162.\nGenerating the Structure of a Fuzzy Rule under Uncertainty 67\nR. De Mori and L. Saitta ( 1980). Automatic Learning of fuzzy naming relations over finite languages. Infor mation Sciences 21: 93-139.\nR. A. Fisher (1936). The use of multiple measurements in taxonomic problems. Annual Eugenics 7 (Part II): 179-188.\nA. Gonzalez (1995 ). A learning methodology in uncer tain and imprecise environments. International Jour nal of Intelligence Systems, (to appear).\nR. S. Michalski (1984). A theory and methodology of inductive reasoning. In: R.S. Michalski, J. Carbonell, T. Mitchel (ed.), In: Machine Learning: An Artificial Intelligence Approach 1. San Mateo, CA.\nJ. R. Quinlan (1986). Induction of decision trees. Ma chine Learning 1(1): 81-106.\nlVI. Sugeno and K. Tanaka (1991). Successive identifi cation of a fuzzy model and its applications to predic tion of a complex system. Fu:::y Sets and Systems 42: 315-334.\nJ. M. Zurita ( 1994). Disei'io de un ATl\\IS Generico. Aplicaciones a! Aprendizaje y a Ia Validaci6n de Bases de Conocimient.o. PhD. di.s.s .. Universidad de Granada. Spain."
    } ],
    "references" : [ {
      "title" : "Learning rules for a fuzzy inference model",
      "author" : [ "S.L.M. Campos" ],
      "venue" : "Moral",
      "citeRegEx" : "Campos,? \\Q1993\\E",
      "shortCiteRegEx" : "Campos",
      "year" : 1993
    }, {
      "title" : "An Inductive Learning Algorithm",
      "author" : [ "J.L. Castro", "J.M. Zurita" ],
      "venue" : "In Fuzzy Systems. Submitted to Fuzzy Sets and Systems",
      "citeRegEx" : "Castro and Zurita,? \\Q1995\\E",
      "shortCiteRegEx" : "Castro and Zurita",
      "year" : 1995
    }, {
      "title" : "An inductive learning procedure to identify fuzzy systems",
      "author" : [ "M. Delgado", "A. Gonzalez" ],
      "venue" : "Fuzzy Sets and Systems",
      "citeRegEx" : "Delgado and Gonzalez,? \\Q1993\\E",
      "shortCiteRegEx" : "Delgado and Gonzalez",
      "year" : 1993
    }, {
      "title" : "Automatic Learning of fuzzy naming relations over finite languages",
      "author" : [ "R. De Mori", "L. Saitta" ],
      "venue" : "Infor­ mation Sciences",
      "citeRegEx" : "Mori and Saitta,? \\Q1980\\E",
      "shortCiteRegEx" : "Mori and Saitta",
      "year" : 1980
    }, {
      "title" : "The use of multiple measurements in taxonomic problems",
      "author" : [ "A. R" ],
      "venue" : "Annual Eugenics",
      "citeRegEx" : "R.,? \\Q1936\\E",
      "shortCiteRegEx" : "R.",
      "year" : 1936
    }, {
      "title" : "A theory and methodology of inductive reasoning",
      "author" : [ "R.S. Michalski" ],
      "venue" : null,
      "citeRegEx" : "Michalski,? \\Q1984\\E",
      "shortCiteRegEx" : "Michalski",
      "year" : 1984
    }, {
      "title" : "Induction of decision trees",
      "author" : [ "J.R. Quinlan" ],
      "venue" : "Ma­ chine Learning",
      "citeRegEx" : "Quinlan,? \\Q1986\\E",
      "shortCiteRegEx" : "Quinlan",
      "year" : 1986
    }, {
      "title" : "Successive identifi­ cation of a fuzzy model and its applications to predic­ tion of a complex system",
      "author" : [ "lVI. Sugeno", "K. Tanaka" ],
      "venue" : "Fu:::y Sets and Systems",
      "citeRegEx" : "Sugeno and Tanaka,? \\Q1991\\E",
      "shortCiteRegEx" : "Sugeno and Tanaka",
      "year" : 1991
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Several approaches to automatic learn­ ing have been analysed (Michalski 1984), (Quinlan 1986).",
      "startOffset" : 62,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "Several approaches to automatic learn­ ing have been analysed (Michalski 1984), (Quinlan 1986).",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "This approach has been widely discussed: (De Mori 1980), (Sugeno 1991), (Campos 1993), (Delgado 1993), (Gonza.",
      "startOffset" : 72,
      "endOffset" : 85
    } ],
    "year" : 2011,
    "abstractText" : "The aim of this paper is to present a method for identifying the structure of a rule in a fuzzy model. For this purpose, an ATMS shall be used (Zurita 1994). An a.lgorithm obtaining the identification of the structure will be suggested (Castro 1995). The minimal structure of the rule (with re­ spect to the number of variables that must appear in the rule) will be found by this al­ gorithm. Furthermore, the identification pa­ rameters shall be obtained simultaneously. The proposed method shall be applied for classification to an example. The Iris Plant Database shall be learnt for all three kinds of plants.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}