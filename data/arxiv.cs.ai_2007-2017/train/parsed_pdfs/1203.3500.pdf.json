{
  "name" : "1203.3500.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Comparative Analysis of Probabilistic Models for Activity Recognition with an Instrumented Walker",
    "authors" : [ "Farheen Omar", "Mathieu Sinn", "Jakub Truszkowski", "Pascal Poupart", "James Tung", "Allan Caine", "David R. Cheriton" ],
    "emails" : [ "f2omar@uwaterloo.ca,", "msinn@uwaterloo.ca,", "jmtruszk@uwaterloo.ca,", "ppoupart@uwaterloo.ca,", "j6tung@uwaterloo.ca,", "adcaine@alumni.uwaterloo.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Rollating walkers are popular mobility aids used by older adults to improve balance control. There is a need to automatically recognize the activities performed by walker users to better understand activity patterns, mobility issues and the context in which falls are more likely to happen. We design and compare several techniques to recognize walker related activities. A comprehensive evaluation with control subjects and walker users from a retirement community is presented."
    }, {
      "heading" : "1 Introduction",
      "text" : "Improving the quality of life of the ever increasing elderly population is one of the key concerns for health care provision. Limitations to independent mobility for these individuals have a significant impact on the quality of their life. Devices such as rollating walkers are often used to improve the independence and mobility of older adults. Our long-term goal is to improve the utility of these devices, by enabling them to perceive their environment and actively provide assistance to their users.\nWe are collaborating with a multidisciplinary group that is studying the usage of rollating walkers. We have access to a walker [9] that has been instrumented with various sensors and cameras to monitor the user. We are developing automated techniques to recognize the activities performed by users with respect to their walker (e.g., walking, standing, turning, etc.) based on the non-video sensors. This problem is significant for Kinesiologists who are studying the usage of walkers by elderly people. Currently they have to hand label the data by looking at video feeds of the user, which is\n∗Allan Caine is currently at Research in Motion, Waterloo\nnot only time consuming but may not be accurate due to synchronization issues between various sensors. An automated activity recognition system would enable clinicians to gather statistics about the activity patterns of users, their level of mobility and the context in which falls are more likely to occur. This will also be useful for the development of smart walkers that can assist users with navigation and braking while taking into account their intended activity.\nIn this paper we describe a comparative analysis of activity recognition techniques based on hidden Markov models (HMMs) and conditional random fields (CRFs) trained by supervised and unsupervised learning for rollating walkers instrumented with various sensors. Our contributions are:\n• the first fully automated system to automatically recognize activities performed by walker users;\n• design and training (supervised and unsupervised) of probabilistic models (HMMs and CRFs) tailored to activity recognition with instrumented walkers;\n• comprehensive analysis of these techniques with real data collected with control subjects and regular walker users living in a retirement community;\n• comprehensive analysis of the ease/difficulty to recognize common walker user activities with existing algorithms.\nThe paper is organized as follows. Section 2 summarizes related work. Section 3 describes the walker, the experimental setup and our hypotheses regarding the ease/difficulty of recognizing common user activities. Section 4 describes the recognition models (HMMs and CRFs) and their training procedures (supervised and unsupervised). Sections 5 and 6 present the results of the experiments and analyze each approach. Finally Section 7 concludes and discusses future work."
    }, {
      "heading" : "2 Related Work",
      "text" : "In [1], Alwan et al. describe a method that assesses basic walker-assisted gait characteristics, including heel strikes, toe-off events, as well as stride time, double support and right and left single support phases. These statistics are based on the measurement of weight transfer between the user and the walker by two load cells in the handles of the walker. A simple thresholding approach is used to detect peaks and valleys in the load measurements, which are assumed to be indicative of certain events in the gait cycle. This work focuses on low level gait statistics where as we are interested to recognize complex high level behaviours.\nHirata et al. [3] instrumented a walker with sensors and actuators. They recognize three user states: walking, stopped and emergency (including falling). These states are inferred based on the distance between the user and the walker (measured by a laser range finder) and the velocity of the walker. This work is limited to the three states mentioned above and would not be able to differentiate between activities that exhibit roughly the same velocity and distance measurements (e.g., walking, turning, going up a ramp).\nA significant amount of work has been done on activity recognition in other contexts. In particular Liao et al. [6] use a Hierarchical Markov Model to learn and infer a user’s daily movements through an urban community. The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a user’s destination and mode of transportation. They use Rao-Blackwellized particle filters for state estimation. In [5], they also recognize activities and places from GPS traces by Hierarchical CRFs. This work assumes that the high level goals and routines of the user as well as the map of the area are known and stable."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : "We have access to a walker developed by Tung et al. [9]. A picture of the walker is shown in Fig. 2. The walker is equipped with various sensors including a 3-D accelerometer in the seat, a load-cell in each leg and a wheel encoder, which measures the wheel’s displacement. The sensor readings vary between 0 and 216−1. The data is channeled via blue-tooth to a PDA for acquisition at 50 Hz. In addition to these sensors, there are two cameras on the walker. One is facing backwards and provides the video feed of the user’s legs. The other is looking forwards and provides the video feed of the environment. The video frame rate is approximately 30 frames/second. In order to collect data for our models, we designed and conducted two exper-\niments that are described below:"
    }, {
      "heading" : "3.1 Experiment 1",
      "text" : "17 healthy young subjects (age between 19 and 53) were asked to go through the course shown in Figure 1 twice with the walker. The behaviours exhibited by the participants are shown in Table 1."
    }, {
      "heading" : "3.2 Experiment 2",
      "text" : "In a second experiment, we asked 8 regular walker users (age 84 to 97) to follow the course shown in Figure 3. This experiment was conducted at the Village of Winston Park (retirement community in Kitchener, Ontario) and the participants were residents of that facility. We also asked 12 adults (age 80 to 89) who do not live in a retirement community and are not regular walker users to follow the same course. The behaviours exhibited during this experiment include those of Tables 1 and 2."
    }, {
      "heading" : "3.3 Recognizing Behaviours",
      "text" : "Our goal is to perform behaviour recognition based on the non-video sensors.1 Since the accelerometers, load cells and wheel encoder only measure indirectly the activities of the person, it is not obvious a priori which activities can easily recognized. We formulated the following hypotheses.\n1The use of video data is subject to future work.\nNot Touching Walker (NTW) should be easy to predict as there is no load on the walker and the walker is not moving. Standing (ST) should be differentiable from NTW based on load cell readings as load fluctuations are expected when the person touches the walker. Walking forward (WF) and walking backward (WB) should be differentiable from ST based on the wheel encoder measurements, which increase with forward movements and decrease with backward movements. Sitting on walker (SW) should also be easily distinguishable since the value of the load sensors is much higher than any other behaviour.\nWe expect turns to be more difficult to predict. We hypothesize that the speed of the person is lower when he/she is turning. We also expect a higher load on the side of the turn and some mild acceleration in the opposite direction of the turn. However this will likely vary with each person. Similarly, going up or down ramps and curbs are not expected to be easy to detect. We expect to see some fluctuations in the vertical acceleration when there is an immediate change of elevation and a sustained mild acceleration corresponding to gravity in the forward or backward direction depending on the inclination of ramps. We hypothesize that both these behaviours will be noticeable. However, they may be difficult to distinguish from WF in general. Furthermore, going up and down curbs may be particularly difficult to recognize due to the wide\nrange of strategies used by people to lift or lower the walker.\nTransfers (sit to stand or stand to sit) are also expected to be difficult to recognize due to a wide range of strategies. In theory, the load of the walker should decrease as the person goes from standing to sitting and increase for sit to stand. However, some people leave the walker to hold other supports such as the arms of a chair. Some people also engage the walker’s brakes as they do a transfer while others move the walker. Another difficult behaviour is the reaching task. It involves behaviours such as opening a door or picking something from ground. The wheel encoder is not useful as people’s habits of engaging the brakes during these behaviours are variable. In reaching tasks, however, the person usually keeps one hand on the walker while he/she uses the other hand to reach for the object. This can be captured by the load cells as there will be more weight on one side.\nIn the next section, we discuss various probabilistic models to recognize activities as accurately as possible despite the wide range of strategies for some behaviours."
    }, {
      "heading" : "4 Activity Recognition Models",
      "text" : "We assume that the set of all possible behaviours is B whose cardinality is m. The behaviour at time t is represented by the random variable Bt. The reading on sensor k at time t is given by the random variable Skt where k ∈ {1, . . . , n}. For notational convenience, we denote the sequence of behaviours from time i to j by Bi:j and the observation sequence on sensor k by Ski:j . The readings on all sensors observed between time i and j is denoted by S1:ni:j and the actual observed sequence is denoted by s1:ni:j . The total length of a sequence is T . The actual behaviour at time t is denoted by b̂t and the predicted behaviour at time t is b̃t. Lower case letters denote the assignment of a value to a random variable."
    }, {
      "heading" : "4.1 Hidden Markov Model",
      "text" : "We construct a hidden Markov model in which the behaviour is the hidden variable and the sensor readings are the observations (Fig. 4). The parameters include θb′,b ≡ Pr(Bt = b′|Bt−1 = b) (probability that the behaviour at time t is b′ given that the behaviour at time t−1 is b), φs,k,b ≡ Pr(Skt = s|Bt = b) (probability that the value measured by the kth sensor at time t is s given that the behaviour is b) and πb ≡ Pr(B0 = b) (probability that the initial behaviour is b)."
    }, {
      "heading" : "4.1.1 Maximum Likelihood (ML) Learning",
      "text" : "For supervised learning, we manually label the data based on the video feed. We learn θb′,b by counting the number of times behaviour b is followed by behaviour b′ in the labeled data:\nθb′,b = PT\nt=1 δ(Bt=b′&Bt−1=b)PT t=1 δ(Bt−1=b)\n∀b, b′ ∈ B\nHere δ(x) = 1 if x is true and 0 otherwise. However, in some of our experiments, to avoid the bias introduced by using a fixed course, we use a simple transition model that reflects the fact that behaviours are τ times more likely to persist than to change.\nθb′,b =\n{ τ/m+ τ − 1 if b = b′\n1/m+ τ − 1 otherwise ∀b, b′ ∈ B\nWe can learn the prior πi from data using πb = ∑T t=1 δ (Bt = b)/T\nAgain, in some of our experiments, to avoid the bias introduced by using a fixed course, we consider all behaviours to be equally likely initially. Hence\nπb = 1/m ∀b ∈ B\nWe can model φs,k,b with a parametric density function such as a Gaussian. However, on close analysis of the data, we found that the distribution of φs,i,j does not follow a Gaussian. Therefore, we discretize the sensor readings by dividing the range into D discrete intervals. We learn φs,k,b using\nφs,k,b = PT\nt=1 δ(Bt=b&Sit=s)PT t=1 δ(Bt=b) ∀b ∈ B, ∀s ∈ {1, . . . , D} ∀k ∈ {1, . . . , n}\nML with EM algorithm For unsupervised learning, we use Expectation Maximization (EM) [2] to learn the parameters. The algorithm alternates between com-\nputing the expectations\nE0(b) = Pr(B0 = b|s1:n1:t , π, θ, φ), E(b, b′) = ∑T−1 t=1 Pr(Bt = b, Bt+1 = b\n′|s1:n1:t , π, θ, φ), Ek(b, s) = ∑T t=1 Pr(Bt = b, S k t = s|s1:n1:t , π, θ, φ)\nand updating the parameters\nπb = E0(b), θb′,b = E(b, b′)/ ∑ b′ E(b, b\n′), φs,k,b = Ek(b, s)/ ∑ sE(b, s).\nPrediction: Note that since the sensor readings at any given time are conditionally independent given the behaviour at that time, therefore, Pr(s1:nt |Bt) =∏ i I φst,i,b. We use maximum a posteriori filtering to infer the most likely behaviour given past observations: b̃t = maxb∈B Pr ( Bt = b|s1:n1:t ) . This computation can be performed online as only past observations are used."
    }, {
      "heading" : "4.1.2 Bayesian Learning",
      "text" : "Bayesian learning is an alternative to maximum likelihood learning. We start from a prior distribution and update it using Bayes rule to obtain the full posterior distribution over the variables of interest. By considering the full posterior over the parameters, we hope to avoid the over-fitting issues, often experienced by the EM algorithm. Unfortunately, in most cases the posterior does not have a tractable form, so we cannot sample from it directly. Consequently, we resort to sampling techniques based on simulating a Markov chain whose stationary distribution is the posterior distribution of interest.\nA convenient choice of a prior distribution over the parameters is a product of Dirichlet distributions of the form Dir(θ1, . . . , θk; c1, . . . , ck) = Γ( P ci)Q\nΓ(ci)\n∏ θc11 θ c2 2 . . . θ ck k where ∑ i θi = 1. Each Dirich-\nlet is a prior for the corresponding multinomial distribution of transitions or emissions from some state. The values ci can be understood as counts indicating how many times a particular transition/emission has been observed. The Dirichlet distribution is a conjugate prior for the multinomial distribution. Therefore, if the hidden states are known, the posterior distribution of the parameters will also be a product of Dirichlets with the counts updated by the number of observed transitions or emissions. If the hidden states are not known, the posterior becomes a mixture of exponentially many products of Dirichlets, each product corresponding to one possible state path.\nWe use Gibbs sampling to estimate the posterior over hidden states and parameters. We repeatedly sample each variable from the conditional distribution given all the other variables. It can be shown that the resulting Markov chain converges to the joint distribution of\nall variables. Here, we use the collapsed Gibbs sampler [7] which samples the hidden states and integrates out the parameters to speed up the convergence.\nSince we use a Dirichlet prior, the posterior probability Pr(B1:T , s1:n1:T ) can be computed analytically:\nPr(B1:T , s1:n1:T ) (1)\n= ˆ πθφ Pr(B1:T , s1:n1:T |π, θ, φ) Pr(π, θ, φ)dπdθdφ (2)\n= c\n∏ B1\nΓ(γB1) Γ( ∑ B1 γB1) ∏ b ∏ b′ Γ(αbb′) Γ( ∑ b′ αbb′) ∏ b,k ∏ sk Γ(βbsk) Γ( ∑ sk βbsk) (3)\nwhere α’s, β’s and γ’s are transition, emission and initial state counts. By taking Pr(Bt|B1:t−1, Bt+1:T , s1:n1:T ) = Pr(Bt, B1:t−1, Bt+1:T , s 1:n 1:T )/ P Bt Pr(Bt, B1:t−1, Bt+1:T , s 1:n 1:T ) and simplifying, we get:\nPr(Bt|B1:t−1, Bt+1:T , s1:n1:T ) ∝ αBt−1BtP\nbt αBt−1bt · αBtBt+1P bt+1 αBtbt+1 · ∏n k=1 β Bts k tP s βBts\nBy repeatedly sampling each hidden state according to the distribution above, we are guaranteed to converge to the posterior distribution Pr(B1:T |s1:n1:T ).\nAlthough we integrate out the parameters analytically, we can sample efficiently from the distribution over the parameters. For any assignment to B1:T , the conditional distribution Pr(π, θ, φ|B1:T , s1:n1:T ) is a product of Dirichlet distributions. Since the Gibbs sampler provides us with a way to sample from Pr(B1:T |s1:n1:T ), sampling from Pr(π, θ, φ|s1:n1:T ) can be done efficiently.\nGibbs sampling can be used both for learning and prediction. Here, we only use it for learning to simplify the comparison to other methods."
    }, {
      "heading" : "4.2 Conditional Random Field",
      "text" : "Conditional Random Fields (CRFs) are probabilistic models for segmenting and labeling sequential data [4]. We consider the special case of linear-chain CRFs. A CRF specifies the distribution of a sequence of labels (B1:t) conditioned on a sequence of observations (S1:n1:t ). In our experiments, the labels are the behaviour of the user, and the observations are the sensor measurements at each time. The probability of B1:t = b1:t conditioned on S1:n1:t = s1:n1:t is given by\nPλ(B1:t = b1:t |S1:n1:t = s1:n1:t ) = 1\nZ(s1:n1:t ) ×\n( T∏ t=1 eµ·f(s 1:n t ,bt) ) × ( T∏ t=2 eν·g(bt−1,bt) ) (4)\nwhere Z(s1:n1:t ) is a normalizing constant, f(s1:nt , bt) is a state feature function (possibly vector-valued) with the corresponding weights µ, and g(bt−1, bt)) is a transition feature function with the corresponding weights ν. Intuitively, the feature functions allow to incorporate which observations and labels are likely to occur together. Usually, the feature functions are kept fixed and the weights are learned from training data. Given a sequence of labeled training data, the most common approach to find the model weights is minimizing the negative log-likelihood. Writing λ for the stacked weights (µ, ν), the objective function is given by\nL (λ) = − T∑ t=1 µ · f ( s1:nt , bt ) − T∑ t=2 ν · g (bt−1, bt)\n+ logZ ( s1:n1:T ) + λTλ\n2σ2 (5)\nwhere the last term on the right hand side is a shrinkage prior to penalize large weights. In our experiments\nwe chose σ2 = 1, However, we found that scaling σ2 by factors up to 10 and 10−1, does not yield big differences in the accuracy of the resulting models. Since the objective function is convex, its unique minimum can be found using gradient-based search. The term Z(s1:n1:t ), which also depends on λ, can be efficiently evaluated using dynamic programming [8]. We use conjugate gradients to minimize the negative log-likelihood and stop training after 100 iterations.\nTo predict behaviours, we consider the speed of the walker and the acceleration in x, y and z-direction. Instead of using the raw data of the load sensors, we consider the following measurements: the total load (which is just the sum of the loads on each wheel), the frontal plane center of pressure (which is the difference between the loads on the left and on the right side divided by the total load) and the sagittal plane center of pressure (the difference between the loads on the rear and front wheels divided by the total load).\nOur state feature functions are based on thresholding: for each pair of behaviours b, b′ and each sensor k, we compare the actual observation to a fixed threshold value. If the value is exceeded, we add some model weight µ(e)bb′k, otherwise, we add some weight µ (n) bb′k. We choose the threshold values manually by a visual inspection of the data. If the labels b, b′ cannot be well discriminated by looking at the data from sensor k, the threshold is chosen as the average value from sensor k; later on, such “irrelevant” thresholds will be given very low weights in the training of the model.\nWe use a very simple transition model to avoid a bias towards certain transitions due to the design of the walker course. In particular, the transition feature function is given by g(bt−1, bt) = δ(bt−1 = bt), hence the corresponding weight ν is a scalar.\nGiven the model parameters and an observation sequence, the predicted behaviour sequence b̃1:T maximizes the a-posteriori probability of B1:T ,\nb̃1:T = argmax b1:T P (B1:T = b1:T |S1:n1:T = s1:n1:T)\nwhere the maximization is over all label sequences of length T . Note that b̃1:T can be computed efficiently similar to the Viterbi algorithm for HMMs."
    }, {
      "heading" : "5 Results",
      "text" : "We present results for the HMM and CRF for both experiments. We use leave one out cross validation for each round of training and prediction. Specifically, if we want to predict the behaviour sequence for a certain participant in Experiment 2, we learn the parameters from all other participants in Experiment 2. For Experiment 1, each person goes through the course twice. We included one instance of the course in the training data along with data from other participants while testing for the same person.\nSince the range of sensor readings is too large (all integers from 0 to 216 − 1), we divide the range into 20 intervals and set their length in such a way that the same number of readings fall into each interval. Since the participants’ weight varies, we also normalize the load cell readings as follows: normalizedV alue = (value−min)/(max−min).\nGround truth is established by hand labeling the data based on the video. This process is not perfect since the labeler can make mistakes while identifying behaviour transitions. For example, the labeler may interpret that a left turn started at time t, while the turn may actually start some time before t, but it only becomes evident in the video at time t. Therefore, in order to calculate our error, we introduce the concept\nof window size. If the window size is x, then for the behaviour at time t, if we find the same behaviour in the window between time t − x and time t + x in the predicted sequence, we count it as a correct prediction. We vary our window size from 0 to 50 in intervals of 5. If we make a correct prediction within a window width of 25, then we are only off by half a second. Since older people perform behaviours at a rate that is much slower than half a second, this may still be considered accurate.\nTables 3 and 4 show the results in the form of confusion matrices for Experiment 2 when performing supervised learning with an HMM and a CRF. In both cases, accelerometer measurements, speed, frontal and sagittal center of pressure (COP) and total weight are used as observations instead of the raw measurements (see Sect. 4.2 for more details). Each entry at row i and column j indicates how many times behaviour i was confused as behaviour j, assuming a window of size 25.\nDue to a lack of space, we did not include the confusion matrices for Experiment 1 and for the unsupervised learning algorithms, however Tables 5 and 6 summarize the recognition accuracy of all the algorithms for each experiment. We define accuracy as∑T t=1 δ ( b̂t = b̃t ) /T . Note that random predictions\nwould yield an accuracy of 1/7 = 14% in Experiment 1 and 1/13 = 7% in Experiment 2.\nIn some situations, identifying transitions from some behaviour to another is what really matters. Hence, Fig. 5 shows the precision and recall of behaviour transitions (in addition to recognition accuracy) as a function of the window size. We calculate the number of actual transitions (i.e., AT = ∑T t=1 δ ( b̂t 6= b̂t−1 ) ), the number of pre-\ndicted transitions (i.e., PT = ∑|T | t=1 δ ( b̃t 6= b̃t−1 ) ) and the number of correctly predicted transitions (i.e., CPT = ∑T t=1 δ ( b̃t 6= b̃t−1 & b̂t = b̃t & b̂t−1 = b̃t−1 ) ). Then precision = CPT/AT and recall = CPT/PT ."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this section, we analyze the results presented in the previous section.\nExperiment 1 vs. Experiment 2: It is obvious from Tables 5 and 6 that the overall accuracy is much higher for Experiment 1. Difficult behaviours such as TR, TL and TRS are also predicted accurately. In fact, the accuracy for TRS is much higher than expected. One reason for the high accuracy may be that in Experiment 1, each person executes the course twice\nand one instance of the course execution is included in the training data while testing for the same person. Therefore, the training data may include information that is more specific to the particular person e.g., how people put load on the walker for different behaviours. Secondly, for Experiment 1, behaviours such as NTW, ST, WF and WB are easily distinguishable from each other. Confusion is only likely between WF, TL and TR and between ST and TRS. There is a larger number of behaviours in Experiment 2 that are difficult to distinguish from each other based on sensor information e.g. WF, TL, TR, GUR, GDR, GUC and GDC. Similarly it is difficult to distinguish between RT and ST.\nCenter of Pressure (COP) vs. Normalized Load Values: In addition to the accelerometer values and the walker speed, we considered two alternative sets of features for prediction. One set includes the normalized load cell measurements while the other includes the frontal plane COP, the sagittal plane COP and the total load on the walker. It is evident from Tables 5 and 6 that both sets of features predict different behaviours well. When the COP features are used, we note that the prediction accuracy is lower for TR, TL, GUR, GDR, GUC and GDC and higher for the other behaviours.\nCRF vs. HMM: Tables 5 and 6 show that the total accuracy of the CRF is much higher than that of the HMM. This is largely due to the fact that the CRF models Pr(Behaviours|Observations) directly and optimizes the parameters of this distribution. On the other hand, the HMM models Pr(Observations|Behaviours) as well as Pr(Behaviours), and then uses Bayes rule to calculate\nPr(Behaviours|Observations). Therefore, the HMM model is more complex and the number of parameters that we have to learn is larger. Also, the HMM makes an explicit assumption about the conditional independence of sensor measurements over time. The CRF avoids this (problematic) assumption since it does not model any distribution over the observations. However, techniques for unsupervised learning are better established for HMMs than CRFs. This becomes an important advantage since we do not need to label data in unsupervised learning.\nWe were surprised to see that the HMM was able to predict certain behaviours better than the CRF. In general, the HMM seems to favor behaviours that occur infrequently. For example, in Table 3, we can see that the prediction accuracy of GUR and GUC is higher than that of WF. We expected that it would be difficult to accurately predict infrequent behaviours such as GUR and GUC. Note also that WF is often predicted as GUR and GUC. We suspect that this is due to the assumption of conditional independence between different sensors.\nLearning Transition Model from Data: As discussed in Sect. 3, the experiments include a pre-defined walking course that biases the behaviour transitions. This is why we considered two scenarios when learning an HMM: i) fixed transition model where each behaviour is τ times more likely to persist than to transition to some other behaviour with uniform probability (denoted by LOD in Tables 5 and 6) and ii) learned transition model (denoted by LOTD). Naturally, the accuracy is higher when the transition model is learned from data. In future work, we plan to collect data with participants in their daily activities instead of a\nscripted walking course, which will allow us to learn realistic and personalized transition models.\nML vs. Bayesian Learning: As explained previously, manually labeling the data is a time consuming and error-prone. Unsupervised learning algorithms avoid this problem. However, they take longer to converge and the solutions are usually approximations to the optimal parameters. It is interesting to note from Table 5 that for Experiment 1, the Gibbs sampling accuracy for NTW and ST is actually higher than other algorithms. One important difficulty with unsupervised learning is state matching. On one hand, unsupervised learning algorithms may pick sub-behaviours of composite behaviours as a state. For example, the ramp transition can be broken up into getting on the ramp (corresponding to a blip in the vertical acceleration), and walking on the ramp. The algorithm may find a state that matches either one or both instead of the behaviours going up/down ramp. On the other hand, if two behaviours are similar, the algorithm might treat them as the same state. We observe that in Table 5, TRS is almost never predicted correctly. This may be due to the fact that TRS is very similar to ST and hence they are merged into one state.\nFor EM and Gibbs sampling, once a model is learned, we manually associate each latent state with the behaviour that is the most frequent. In general we used a number of latent states equal to the number of behaviours, except for Gibbs sampling in Experiment 1 (Table 5) where we used more latent states (11) than behaviours (7). As a result, the accuracy improved. In future work, we would like to investigate more thoroughly whether using a larger number of latent states generally improves the accuracy.\nSince EM often gets stuck in local optima, we did 20 random restarts and showed the results of the model with the highest likelihood. In contrast, Gibbs sampling does not suffer from this problem. It is evident from Tables 5 and 6 as well as Fig. 5 that Gibbs sampling performs better than EM."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "This paper presented a novel and significant application of activity recognition in the context of instrumented walkers. We designed several algorithms based on HMMs and CRFs, and tested them with real users at the Village of Winston Park (retirement community in Kitchener, Ontario). A comprehensive analysis of the results showed that behaviours associated with walker usage tend to induce load, speed and acceleration patterns that are sufficient to distinguish them with reasonable accuracy. In the future, we would like to further improve the recognition accuracy, by using\nthe video data and exploring various feature extraction techniques. We are also working on improving the battery life and memory capacity of the walker to collect data over long periods of time. In particular, this will allow us to loan the walker to users, record their daily usage and learn realistic and personalized behaviour transition models. Finally, we hope to turn this work into a clinical tool that can be used to assess the mobility patterns of walker users and the contexts in which they are more likely to fall."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by funds from CIHR, NSERC, the Ontario Ministry of Research and Innovation (Pascal Poupart’s ERA) and the Canadian government (Mathieu Sinn’s postdoctoral fellowship). We also thank the UW-Schlegel Research Institute for Aging and the Village of Winston Park as well as all the volunteers who participated in the experiments."
    } ],
    "references" : [ {
      "title" : "Passive derivation of basic walker-assisted gait characteristics from measured forces and moments",
      "author" : [ "M. Alwan", "G. Wasson", "P. Sheth", "A. Ledoux", "C. Huang" ],
      "venue" : "In Proc. Int. Conf of the IEEE EMBS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Maximum likelihood from incomplete data with the em algorithm",
      "author" : [ "A. Dempster", "N. Laird", "D. Rubin" ],
      "venue" : "J. of the Royal Statistical Society, Series B,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1977
    }, {
      "title" : "Motion control of intelligent passive-type walker for fall prevention function based on estimation of user state",
      "author" : [ "Y. Hirata", "A. Muraki", "K. Kasuge" ],
      "venue" : "In Proc. of the IEEE ICRA,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J. Lafferty", "A. McCallum", "F. Pereira" ],
      "venue" : "In Proc. ICML,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Extracting places and activities from gps traces using hierarchical conditional random fields",
      "author" : [ "L. Liao", "D. Fox", "H. Kautz" ],
      "venue" : "International Journal of Robotics Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Learning and inferring transportation",
      "author" : [ "L. Liao", "D.J. Patterson", "D. Fox", "H. Kautz" ],
      "venue" : "routines. AI,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "The collapsed Gibbs sampler in Bayesian computations with applications to a gene regulation problem",
      "author" : [ "J.S. Liu" ],
      "venue" : "J. of the Am. Statistical Association,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1994
    }, {
      "title" : "Introduction to Statistical Relational Learning, chapter An Introduction to Conditional Random Fields for Relational Learning",
      "author" : [ "C. Sutton", "A. McCallum" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "Frontal plane balance control with rollators: Perturbed stance and walking",
      "author" : [ "J. Tung", "K. Zabjek", "W. Gage", "B. Maki", "andW. McIlroy" ],
      "venue" : "Archives of Physical Medicine and Rehabilitation,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "We have access to a walker [9] that has been instrumented with various sensors and cameras to monitor the user.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "In [1], Alwan et al.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "[3] instrumented a walker with sensors and actuators.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] use a Hierarchical Markov Model to learn and infer a user’s daily movements through an urban community.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "In [5], they also recognize activities and places from GPS traces by Hierarchical CRFs.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "[9].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "ML with EM algorithm For unsupervised learning, we use Expectation Maximization (EM) [2] to learn the parameters.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "Here, we use the collapsed Gibbs sampler [7] which samples the hidden states and integrates out the parameters to speed up the convergence.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Conditional Random Fields (CRFs) are probabilistic models for segmenting and labeling sequential data [4].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "The term Z(s 1:t ), which also depends on λ, can be efficiently evaluated using dynamic programming [8].",
      "startOffset" : 100,
      "endOffset" : 103
    } ],
    "year" : 2010,
    "abstractText" : "Rollating walkers are popular mobility aids used by older adults to improve balance control. There is a need to automatically recognize the activities performed by walker users to better understand activity patterns, mobility issues and the context in which falls are more likely to happen. We design and compare several techniques to recognize walker related activities. A comprehensive evaluation with control subjects and walker users from a retirement community is presented.",
    "creator" : "TeX"
  }
}