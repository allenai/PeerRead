{
  "name" : "1509.02213.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "UNSUPERVISED SPOKEN TERM DETECTION WITH SPOKEN QUERIES BY MULTI-LEVEL ACOUSTIC PATTERNS WITH VARYING MODEL GRANULARITY",
    "authors" : [ "Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee" ],
    "emails" : [ "b97901182@gmail.com,", "chunanchan@gmail.com,", "lslee@gate.sinica.edu.tw" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— zero resource speech recognition, unsupervised learning, dynamic time warping, hidden Markov models, spoken term detection"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "The fast growing quantity of video and audio content over the Internet implies a very high demand for efficient and accurate approaches to search through the spoken contents. Spoken term detection (STD) usually refers to the task of finding all occurrences of the text query term from a large spoken archive [1]. Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5]. This implies annotated training corpora properly matched to the spoken content are necessary. When the input query is spoken, it becomes possible to directly match the spoken content with the spoken query without conventional ASR. In this way, the difficulties in conventional ASR such as recognition errors and need for annotated training data may be bypassed, which is especially attractive for languages with very limited annotated data [6][7] or spoken content with unknown languages. This leads to recent efforts in unsupervised STD with spoken queries without using annotated data during training [8] [9], which is also the focus of this work. Hereafter we assume all queries are in spoken, and no annotated speech data is available.\nPrevailing approaches to the task considered here rely on dynamic time warping (DTW) to directly match the spoken query to the spoken documents. However, a major limitation of DTW-based approaches is that the DTW distances are easily affected by speaker mismatch and varying acoustic conditions. Many related works focused on feature\nrepresentations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13]. The other limitation for DTW-based approaches is that the computation load for the matching process is linear to the number of frames to be searched through. Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].\nIn recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22]. In this paper, we propose a new approach for unsupervised STD with spoken queries using multilevel acoustic patterns automatically discovered from the target corpus with varying model granularity discovery from the corpus of interest. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a threedimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to on another, thus can jointly capture the characteristics of the spoken terms. By converting the spoken content and query into parallel sequences of acoustic patterns with different model granularity, token matching can be performed with pattern indices representing highly varying signals. Very encouraging results were obtained the preliminary experiments."
    }, {
      "heading" : "2. ACOUSTIC PATTERNS WITH VARYING MODEL GRANULARITY",
      "text" : ""
    }, {
      "heading" : "2.1. Pattern Discovery for a Given Model Configuration",
      "text" : "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set ψ that determines the topology of the HMMs [19][20][21]. This can be achieved by first finding an initial label ω0 based on a set of assumed patterns for all observations in the corpus χ as in (1)[17]. Then in each iteration t the HMM parameter set θψt can be trained with the label ωt−1 obtained in the previous iteration as in (2), and the new label ωt can be obtained by free-pattern decoding with the obtained parameter set θψt as in (3).\nω0 = initialization(χ), (1) θψt = arg max θψ P (χ|θψ , ωt−1), (2)\nωt = arg max ω\nP (χ|θψt , ω). (3)\nThe training process can be repeated with enough number of iterations until the difference between ωt−1 and ωt becomes insignificant. This\nar X\niv :1\n50 9.\n02 21\n3v 1\n[ cs\n.C L\n] 7\nS ep\n2 01\ngives a converged set of acoustic pattern HMMs which we denote as Θψ ."
    }, {
      "heading" : "2.2. Model Granularity Space",
      "text" : "The above process can be performed with many different HMM configurations, each characterized by three hyperparameters: the number of states m in each acoustic pattern HMM, the total number of acoustic patterns n during initialization, and the number of Gaussians l in each HMM state, ψ = (m,n, l). The transcription of a speech signal decoded with these acoustic pattern HMMs may be considered as a temporal segmentation of the signal, so the HMM length (or number of states in each HMM) m represents the temporal granularity. The set of all acoustic pattern HMMs may be considered as a segmentation of the phonetic space, so the total number n of acoustic pattern HMMs represents the phonetic granularity. The different Gaussians in each state then jointly model the distributions of the signals in the acoustic feature space represented by MFCCs, so the number of Gaussians l in each state represents the acoustic granularity. This gives a three-dimensional representation of the acoustic pattern configurations in terms of temporal, phonetic and acoustic granularities as in Fig. 1. Any point in the three-dimensional space in Fig. 1 corresponds to an acoustic pattern configuration.\nAlthough the hyperparameters ψ = (m,n, l) are difficult to determine for a given corpus of unknown language and unknown linguistic characteristics, it is possible to have many different sets of acoustic patterns with hyperparameters and HMMs {Θψk , ψk = (mk, nk, lk), k = 1, 2, ...K} independently learned in parallel, considered as on different levels. Because different model granularities (m,n, l) give different characteristics to the acoustic patterns as mentioned above, with enough number of pattern sets and the model granularities (m,n, l) properly distributed in the three-dimensional space, this multi-level set of acoustic pattern may jointly represent the behavior of the signal for the given corpus. There can be a variety of applications for these patterns, but here in the work below we use these patterns in spoken term detection, assuming the characteristics of the spoken terms can be captured by different sets of these patterns."
    }, {
      "heading" : "3. SPOKEN TERM DETECTION AND SEARCH METHODS",
      "text" : ""
    }, {
      "heading" : "3.1. Off-line Processing",
      "text" : "All the spoken documents in the archive are first off-line decoded into sequences of acoustic patterns using each level of acoustic pattern HMM set Θ(mk,nk,lk). Let {pr, r = 1, 2, 3, .., nk} denote the nk acoustic\npatterns in the set Θψk . We further construct a similarity matrix S of size nk × nk for every ψk, for which the component S(i, j) is the similarity between any two pattern HMMs pi and pj in the set Θψk . Two similarity matrices used for this work are in (4).\nS(i, j) =\n{ δ(i, j), for hard similarity, or (4a)\nexp(−KL(i, j)/β, for soft similarity. (4b)\nThe matrix in (4a) is simply the identity matrix. The KL-divergence KL(i, j) between two pattern HMMs in (4b) is defined as the KLdivergence between the states based on the variational approximation [23] summed over the states. To transform the KL divergence into a similarity measure between 0 and 1, a negative exponential was applied [24] with a scaling factor β. When β is small, similarity between distinct patterns in (4b) approaches zero, so (4b) becomes similar to (4a). β can be determined with a held out data set, but here we simply set it to 100 times the number of states mk."
    }, {
      "heading" : "3.2. On-line Matching Matrix Construction",
      "text" : "In the on-line phase, we perform the following for each entered spoken query q and each document d in the archive. Assume for the pattern set Θψk a document d is decoded into a sequence of D acoustic patterns with indices (d1, d2, ..., dD) and the query q into a sequence of Q patterns with indices (q1, ..., qQ). We thus construct a matching matrix W of size D ×Q for every document-query pair, in which each entry (i, j) is the similarity between acoustic patterns with indices di and qj as in (5a).\nW (i, j) =\n{ S(di, qj), for 1-best sequences, or (5a)\nPTi SPj , for N-best sequences. (5b)\nAlternatively, we can also match the N-best sequences of documents to N-best sequences of queries as depicted in Fig (2) and (5b). We extend each pattern position i in a sequence into a posteriorgram vector Pi of size nk × 1 by accumulating the duration for each of the nk patterns within the pattern boundaries of the best transcription, across the N-best transcriptions considered, uniformly weighted and normalized(Pi for d and Pj for q). When only the best transcription is considered, (5b) reduces to (5a). The price paid here is the increased computation time by a factor of O(n2k) over that in (5a). One can also choose to use the posteriorgram only on the query, which increases the load by O(nk)."
    }, {
      "heading" : "3.3. On-line Matching Policy",
      "text" : "There can be two methods to calculate the relevance score between document d and query q. In the sub-sequence matching(SUB) method in (6a) , we sum the elements in the matrixW along the diagonal direction, generating the accumulated similarities for all sub-sequences starting at all pattern positions in d as shown in Fig (3)(a). The maximum is selected to represent the relevance between document d and query q on the\npattern set Θψk as in (6a).\nR(d, q) =  max i:index Q∑ j=1 W (i+ j, j), for SUB, or (6a)\nmax u: path |u|∑ j=1 W (ud(j), uq(j)), for DTW. (6b)\nIn order to alleviate the problem of insertion/deletion, we can also perform dynamic time warping (DTW) on the matrix W as in (6b) and Fig 3(b), refered to as the pattern-based DTW here. This is an extended version of (6a), except now the elements W are accumulated along any allowed DTW path in the matrix W . The summation in (6b) is over a single DTW path, while the maximization in (6b) is over all DTW paths. Although DTW takes longer time, performing DTW in the matrix W on-line is significantly faster than the conventional frame-based DTW, because most of the calculation was performed offline when evaluating (4). Since table lookup is constant time operation, asymptotically speaking, the computational load online is reduced by a factor of O(FT 2) = O(Fm2k), where F is the feature dimension in the framebased DTW and T is the duration of the acoustic patterns in frames which scales linearly with number of states mk in HMMs."
    }, {
      "heading" : "3.4. Overall Relevance Score",
      "text" : "In each of (4)(5)(6) there are two options, leading to a total 8 search methods. We thus use three binary digits to specify these methods in reporting experimental results below: γ = (Soft,Nbest,DTW), i.e. Soft=1 for soft similarity in (4b) and 0 for (4a); Nbest=1 for N-best sequence in (5b) and 0 for (5a); DTW=1 for DTW in (6b) and 0 for (6a). These search methods,{γs, s = 1, ..., 8} give different relevance scores for each pattern set Θψk , R(ψk,γs)(d, q) as in (6). The overall relevance score R̄(d, q) between d and q is then simply the weighted sum of (6) over all K different sets of acoustic patterns Θψk and the 8 search methods with weights λ(ψk, γs) as in (7). Below, we simply set the weights λ(ψk, γs) to either 0 or 1.\nR̄(d, q) = K∑ k=1 8∑ s=1 λ(ψk, γs)R (ψk,γs)(d, q). (7)\nNote that the acoustic pattern sets {Θψk , k = 1, 2, ...,K} for different model granularities are complementary to one another. By adding the scores obtained via different pattern sets as in (7) the signal characteristics can be better captured. In addition, the limitation caused by temporal granularity (model length mk) may be alleviated to some degree by the pattern-based DTW in (6b), the limitation caused by phonetic granularity (number of patterns nk) may be alleviated to some degree by the Nbest sequences in (5b), and the limitation caused by acoustic granularity (number of Gaussian lk) may be taken care to some degree by the soft similarity in (4b). Therefore, the choices of ψk and γs are correlated for good scores of R̄(d, q) in (7). Also, the process above, including constructing the KL-divergence matrices and decoding the spoken documents into acoustic patterns off-line, and generating the pattern sequence for the query on-line can all be performed in a highly parallel manner, so\nthe computational load can be scalable with the computational resources available."
    }, {
      "heading" : "4. EXPERIMENTS",
      "text" : "The proposed approach was tested in preliminary experiments performed on the TIMIT corpus. 20 sets of acoustic patterns with number of states m=3, 5, 7, 9, 11 and number of HMMs n=50, 100, 200, 300 were first generated with l=1 Gaussian per state on the TIMIT training set. Then we increased the number of Gaussians per state by 1 and perform (2) and (3) until the sets converge. We repeated the process until we had l=1, 2, 3, 4 for all the 20 pattern sets, obtaining K=80 pattern HMM sets in the end. With the 8 search methods mentioned in section 3, this gave a total of 640 scores for every query-document pair in (7). The TIMIT training set was also taken as the spoken archive from which we wish to detect the spoken terms.\nThe query set consisted of 32 spoken words randomly selected from the TIMIT testing set. An instance of every query word was randomly selected from the testing set, and used as the spoken query to search for other instances in the training set. Note that although choices of λ(ψk, γs) can be based on the optimization with respect to an evaluation metric [25][26], the preliminary experiments in this work were to verify the feasibility of the proposed frameworks instead. The baseline we compared to was frame-based DTW on MFCC sequences. The same acoustic features were used for training the pattern HMMs. In principle, the framework should generalize to other features as well. For spoken term detection the performance measures we used were the mean average precision(MAP), precision at 5 and 10(P@5 and P@10). All three measures gave very similar trends. Below we only report results for MAP due to space limitation."
    }, {
      "heading" : "4.1. Feature Selection and Achievable Performance",
      "text" : "In this experiment, our goal was to learn the 640 weights λ(ψ, γ) in (7) to be either 1 or 0 in order to optimize the MAP. We randomly split the query set into 2 disjoint sets A and B, each containing 16 queries. We use set B as the development set for learning these weights and focus our discussion on the performance on set A. Starting with every λ(ψ, γ)=0, we greedily select the next (ψ, γ) that would yield the best MAP and set it to 1. This process was repeated until 20 pairs of (ψ, γ) were selected. The results are shown in pink in Fig. 4, in which the oracle results by learning on set A itself are also shown in cyan. The baseline of framebased DTW was 10.16% for set A. It was low probably because most queries unexpectedly had less than 10 relevant documents in the training set, and the various dialects of TIMIT made it even more difficult for simply comparing the feature sequences. We can clearly see with only 20 out of 640 scores selected based on set B, the MAP reaches 25.88%, significantly higher than the baseline. The oracle results learning on set A itself reached as high as 35.60%, which implied that if a more elaborate learning algorithm was applied, there was still much room for improvement.\nThe parameter sets (ψ, γ) for the 20 scores selected based on set B are also printed on Fig. 4. Some observations can be made here. In all the 20 cases Soft=1, so Soft=1 was certainly better. For the selection of N-best sequences or DTW there were no clear trends. However, some correlation between search methods and pattern configurations can be observed. For example, it can be found that Nbest=1 were preferred when l=1 or l=2, probably because too few number of Gaussians limited the accuracies in the 1-best sequence. Also, DTW=1 was preferred very often when m=5 or 3, probably because for shorter patterns it made better sense to merge more than one patterns into a longer pattern, which is actually what the pattern-based DTW did. These results seem to imply\nthe need to learn from a development set, which is not always available. This will be further discussed in the next section."
    }, {
      "heading" : "4.2. Performance Analysis without a Development Set",
      "text" : "Here we consider the case without a development set. We first consider the different search method γ by summing all the 80 scores for all combinations of m,n,l, but not γ as shown in Part (A) of Fig. (5). Several conclusions may be drawn: (a) Soft similarity brought massive improvement (Soft=1 > Soft=0 for all combinations of Nbest and DTW), (b) N-best Sequences brought only a negligible improvement (Nbest=1 ∼ Nbest=0 for all combinations of Soft and DTW), (c) DTW degraded the performance in general (DTW=1<DTW=0 for all combinations of Soft and Nbest). Conclusion (a) is consistent with the conclusion drawn from the top 20 scores in Fig 4. Since generating a soft similarity metric is also the only part of the search that could be conducted off-line, it is certainly attractive. Conclusion (b) and (c) may be a little surprising, since intuitively N-best sequences and DTW can help and quite several of the top 20 scores in Fig 4 had Nbest=1 or DTW=1. As discussed previously with Fig 4, Nbest and DTW could be helpful for specific pattern configurations ψ but not necessarily all. When such specific configurations cannot be properly chosen with a development set, these improvements could be diluted when averaged with all possible configurations. Because the different pattern sets carried complementary information, jointly considering the 80 1-best sequences obtained from the 80 pattern sets itself can be viewed as considering a single large lattice best representing the utterance with all time warping and N-best information included. This is probably why N-best and DTW didn’t help here. The MAP obtained by summing all 640 scores without any selection was 20.62% as shown in Part(A) of Fig (5), which was also significantly higher than the baseline.\nTherefore below we focus our discussion on the selection of pattern configurations of ψ assuming γ=(1,0,0) without using Nbest or DTW. The 80 scores for different ψ forms a 3 dimensional space over (m,n, l). We summed the relevance score over 2 of the dimensions and plotted the performance on the remaining dimension. Summing over (m,n), (m, l), (n, l), we get Parts (B)(C)(D) of Fig. (5) respectively. The average of the MAP values for Parts (B)(C)(D) of Fig. (5) are also listed for comparison between the dimensions. Several additional conclusions may be drawn: (d) the performance is better for larger l as shown in Part(B) of Fig. 5, (e) Model combination on the (m,n) plane was the most effective (the average MAP of Part(B) was much higher than those in Part (C)(D) of Fig. 5), (f) several optimal values seem to exist for m and n (the maximum occurs for m=3 in Part (D) and n=100 in Part (C)). We highly suspect that these optimal points for m, n as mentioned in Conclusion (f) are inert characteristics of the underlying language that should stay approximately the same for a different corpus of the same\nlanguage. Conclusion (d) will probably hold until the phenomenon of over-fitting begins to happen, since the number of Gaussians is limited by the training corpus size. Conclusion (e) implies when blending score with respect to (m,n), a good policy may be to have m and n as diverse as possible, if there is no information regarding the selection of m, and n. From conclusion (f), m=3 seemed close to the average phoneme duration of TIMIT, while n=100 seemed close to the number of phonemes with some context dependency considered. This may imply selection of (m,n) is a language dependent characteristic although we do not have strong evidence yet to back up this claim. This would be useful if verified to be true, especially for under-resourced languages, since in that case the learned weights could be similarly useful for different corpora on the same language.\nWe further plot the MAP of R(ψ,γ)(d, q) for γ=(1,0,0) and l=3 for sets A and B in Fig 6, the best performing γ and l in Parts (A)(B) of Fig 5. The 20 points for (m,n) were interpolated with a 2D spline function to show the smoothed MAP distributions over the plane. As can be seen, the score distributions look similar to a good degree even for completely different query sets in Fig 6(a) and (b). When we simply selected the 20 scores for set A here (m=3,5,7,9,11; n=50,100,200,300; l=3,γ=(1,0,0)), the MAP is 26.32%, slightly higher than 25.88% achieved above by the 20 scores selected greedily from a development set."
    }, {
      "heading" : "5. CONCLUSION",
      "text" : "In this work, we present a new approach for unsupervised spoken term detection using multi-level acoustic patterns discovered from the target corpus. The different pattern sets with different model configurations are complementary, thus can jointly capture the information for the spoken terms. Significantly better performance than frame-based DTW on TIMIT corpus as obtained."
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] David RH Miller, Michael Kleber, Chia-Lin Kao, Owen Kimball, Thomas Colthurst, Stephen A Lowe, Richard M Schwartz, and Herbert Gish, “Rapid and accurate spoken term detection.,” in INTERSPEECH, 2007, pp. 314–317.\n[2] Jonathan Mamou, Bhuvana Ramabhadran, and Olivier Siohan, “Vocabulary independent spoken term detection,” in Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 615–622.\n[3] Roy G Wallace, Robert J Vogt, and Sridha Sridharan, “A phonetic search approach to the 2006 nist spoken term detection evaluation,” 2007.\n[4] Yi-cheng Pan and Lin-shan Lee, “Performance analysis for lattice-based speech indexing approaches using words and subword units,” Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562–1574, 2010.\n[5] Murat Saraclar and Richard Sproat, “Lattice-based search for spoken utterance retrieval,” Urbana, vol. 51, pp. 61801, 2004.\n[6] Lou Boves, Rolf Carlson, Erhard W Hinrichs, David House, Steven Krauwer, Lothar Lemnitzer, Martti Vainio, and Peter Wittenburg, “Resources for speech research: present and future infrastructure needs.,” in INTERSPEECH. Citeseer, 2009, pp. 1803–1806.\n[7] Arun Kumar, Nitendra Rajput, Dipanjan Chakraborty, Sheetal K Agarwal, and Amit A Nanavati, “Wwtw: the world wide telecom web,” in Proceedings of the 2007 workshop on Networked systems for developing regions. ACM, 2007, p. 7.\n[8] Florian Metze, Nitendra Rajput, Xavier Anguera, Marelie Davel, Guillaume Gravier, Charl Van Heerden, Gautam V Mantena, Armando Muscariello, Kishore Prahallad, Igor Szoke, et al., “The spoken web search task at mediaeval 2011,” in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5165–5168.\n[9] Chun-An Chan and Lin-Shan Lee, “Unsupervised hidden markov modeling of spoken queries for spoken term detection without speech recognition.,” in INTERSPEECH, 2011, pp. 2141–2144.\n[10] Michael A Carlin, Samuel Thomas, Aren Jansen, and Hynek Hermansky, “Rapid evaluation of speech representations for spoken term discovery.,” in INTERSPEECH, 2011, pp. 821–824.\n[11] Yaodong Zhang and James R Glass, “Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams,” in Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398–403.\n[12] Marijn Huijbregts, Mitchell McLaren, and David van Leeuwen, “Unsupervised acoustic sub-word unit detection for query-byexample spoken term detection,” in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4436–4439.\n[13] Haipeng Wang, Cheung-Chi Leung, Tan Lee, Bin Ma, and Haizhou Li, “An acoustic segment modeling approach to query-by-example spoken term detection,” in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5157–5160.\n[14] Yaodong Zhang and James R Glass, “A piecewise aggregate approximation lower-bound estimate for posteriorgram-based dynamic time warping.,” in INTERSPEECH, 2011, pp. 1909–1912.\n[15] Yaodong Zhang, Kiarash Adl, and James Glass, “Fast spoken query detection using lower-bound dynamic time warping on graphical processing units,” in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5173–5176.\n[16] Aren Jansen and Benjamin Van Durme, “Indexing raw acoustic features for scalable zero resource search.,” in INTERSPEECH, 2012.\n[17] Cheng-Tao Chung, Chun-an Chan, and Lin-shan Lee, “Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization,” in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081–8085.\n[18] Aren Jansen, Kenneth Church, and Hynek Hermansky, “Towards spoken term discovery at scale with zero resources.,” in INTERSPEECH, 2010, pp. 1676–1679.\n[19] Aren Jansen and Kenneth Church, “Towards unsupervised training of speaker independent acoustic models.,” in INTERSPEECH, 2011, pp. 1693–1692.\n[20] Herbert Gish, Man-hung Siu, Arthur Chan, and William Belfield, “Unsupervised training of an hmm-based speech recognizer for topic classification.,” in INTERSPEECH, 2009, pp. 1935–1938.\n[21] Mathias Creutz and Krista Lagus, “Unsupervised models for morpheme segmentation and morphology learning,” ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.\n[22] Hung-yi Lee, Yun-Chiao Li, Cheng-Tao Chung, and Lin-shan Lee, “Enhancing query expansion for semantic retrieval of spoken content with automatically discovered acoustic patterns,” in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8297–8301.\n[23] John R Hershey and Peder A Olsen, “Approximating the kullback leibler divergence between gaussian mixture models,” in Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on. IEEE, 2007, vol. 4, pp. IV–317.\n[24] Marcin Marszałek and Cordelia Schmid, “Constructing category hierarchies for visual recognition,” in Computer Vision–ECCV 2008, pp. 479–491. Springer, 2008.\n[25] Filip Radlinski and Thorsten Joachims, “Query chains: learning to rank from implicit feedback,” in Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. ACM, 2005, pp. 239–248.\n[26] Tie-Yan Liu, “Learning to rank for information retrieval,” Foundations and Trends in Information Retrieval, vol. 3, no. 3, pp. 225– 331, 2009."
    } ],
    "references" : [ {
      "title" : "Rapid and accurate spoken term detection",
      "author" : [ "David RH Miller", "Michael Kleber", "Chia-Lin Kao", "Owen Kimball", "Thomas Colthurst", "Stephen A Lowe", "Richard M Schwartz", "Herbert Gish" ],
      "venue" : "INTERSPEECH, 2007, pp. 314–317.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Vocabulary independent spoken term detection",
      "author" : [ "Jonathan Mamou", "Bhuvana Ramabhadran", "Olivier Siohan" ],
      "venue" : "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 615–622.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A phonetic search approach to the 2006 nist spoken term detection evaluation",
      "author" : [ "Roy G Wallace", "Robert J Vogt", "Sridha Sridharan" ],
      "venue" : "2007.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Performance analysis for lattice-based speech indexing approaches using words and subword units",
      "author" : [ "Yi-cheng Pan", "Lin-shan Lee" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562–1574, 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Lattice-based search for spoken utterance retrieval",
      "author" : [ "Murat Saraclar", "Richard Sproat" ],
      "venue" : "Urbana, vol. 51, pp. 61801, 2004.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1801
    }, {
      "title" : "Resources for speech research: present and future infrastructure needs",
      "author" : [ "Lou Boves", "Rolf Carlson", "Erhard W Hinrichs", "David House", "Steven Krauwer", "Lothar Lemnitzer", "Martti Vainio", "Peter Wittenburg" ],
      "venue" : "INTERSPEECH. Citeseer, 2009, pp. 1803–1806.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Wwtw: the world wide telecom web",
      "author" : [ "Arun Kumar", "Nitendra Rajput", "Dipanjan Chakraborty", "Sheetal K Agarwal", "Amit A Nanavati" ],
      "venue" : "Proceedings of the 2007 workshop on Networked systems for developing regions. ACM, 2007, p. 7.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The spoken web search task at mediaeval 2011",
      "author" : [ "Florian Metze", "Nitendra Rajput", "Xavier Anguera", "Marelie Davel", "Guillaume Gravier", "Charl Van Heerden", "Gautam V Mantena", "Armando Muscariello", "Kishore Prahallad", "Igor Szoke" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5165–5168.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Unsupervised hidden markov modeling of spoken queries for spoken term detection without speech recognition",
      "author" : [ "Chun-An Chan", "Lin-Shan Lee" ],
      "venue" : "INTERSPEECH, 2011, pp. 2141–2144.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Rapid evaluation of speech representations for spoken term discovery",
      "author" : [ "Michael A Carlin", "Samuel Thomas", "Aren Jansen", "Hynek Hermansky" ],
      "venue" : "INTERSPEECH, 2011, pp. 821–824.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams",
      "author" : [ "Yaodong Zhang", "James R Glass" ],
      "venue" : "Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398–403.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Unsupervised acoustic sub-word unit detection for query-byexample spoken term detection",
      "author" : [ "Marijn Huijbregts", "Mitchell McLaren", "David van Leeuwen" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4436–4439.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An acoustic segment modeling approach to query-by-example spoken term detection",
      "author" : [ "Haipeng Wang", "Cheung-Chi Leung", "Tan Lee", "Bin Ma", "Haizhou Li" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5157–5160.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A piecewise aggregate approximation lower-bound estimate for posteriorgram-based dynamic time warping",
      "author" : [ "Yaodong Zhang", "James R Glass" ],
      "venue" : "INTERSPEECH, 2011, pp. 1909–1912.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Fast spoken query detection using lower-bound dynamic time warping on graphical processing units",
      "author" : [ "Yaodong Zhang", "Kiarash Adl", "James Glass" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5173–5176.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Indexing raw acoustic features for scalable zero resource search",
      "author" : [ "Aren Jansen", "Benjamin Van Durme" ],
      "venue" : "INTERSPEECH, 2012.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization",
      "author" : [ "Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081–8085.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Towards spoken term discovery at scale with zero resources",
      "author" : [ "Aren Jansen", "Kenneth Church", "Hynek Hermansky" ],
      "venue" : "INTER- SPEECH, 2010, pp. 1676–1679.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Towards unsupervised training of speaker independent acoustic models",
      "author" : [ "Aren Jansen", "Kenneth Church" ],
      "venue" : "INTERSPEECH, 2011, pp. 1693–1692.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Unsupervised training of an hmm-based speech recognizer for topic classification",
      "author" : [ "Herbert Gish", "Man-hung Siu", "Arthur Chan", "William Belfield" ],
      "venue" : "INTERSPEECH, 2009, pp. 1935–1938.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Unsupervised models for morpheme segmentation and morphology learning",
      "author" : [ "Mathias Creutz", "Krista Lagus" ],
      "venue" : "ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Enhancing query expansion for semantic retrieval of spoken content with automatically discovered acoustic patterns",
      "author" : [ "Hung-yi Lee", "Yun-Chiao Li", "Cheng-Tao Chung", "Lin-shan Lee" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8297–8301.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Approximating the kullback leibler divergence between gaussian mixture models",
      "author" : [ "John R Hershey", "Peder A Olsen" ],
      "venue" : "Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on. IEEE, 2007, vol. 4, pp. IV–317.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Constructing category hierarchies for visual recognition",
      "author" : [ "Marcin Marszałek", "Cordelia Schmid" ],
      "venue" : "Computer Vision–ECCV 2008, pp. 479–491. Springer, 2008.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Query chains: learning to rank from implicit feedback",
      "author" : [ "Filip Radlinski", "Thorsten Joachims" ],
      "venue" : "Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. ACM, 2005, pp. 239–248.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "Tie-Yan Liu" ],
      "venue" : "Foundations and Trends in Information Retrieval, vol. 3, no. 3, pp. 225– 331, 2009.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Spoken term detection (STD) usually refers to the task of finding all occurrences of the text query term from a large spoken archive [1].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 5,
      "context" : "In this way, the difficulties in conventional ASR such as recognition errors and need for annotated training data may be bypassed, which is especially attractive for languages with very limited annotated data [6][7] or spoken content with unknown languages.",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 6,
      "context" : "In this way, the difficulties in conventional ASR such as recognition errors and need for annotated training data may be bypassed, which is especially attractive for languages with very limited annotated data [6][7] or spoken content with unknown languages.",
      "startOffset" : 212,
      "endOffset" : 215
    }, {
      "referenceID" : 7,
      "context" : "This leads to recent efforts in unsupervised STD with spoken queries without using annotated data during training [8] [9], which is also the focus of this work.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "This leads to recent efforts in unsupervised STD with spoken queries without using annotated data during training [8] [9], which is also the focus of this work.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].",
      "startOffset" : 247,
      "endOffset" : 251
    }, {
      "referenceID" : 11,
      "context" : "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].",
      "startOffset" : 285,
      "endOffset" : 289
    }, {
      "referenceID" : 12,
      "context" : "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].",
      "startOffset" : 289,
      "endOffset" : 293
    }, {
      "referenceID" : 8,
      "context" : "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 16,
      "context" : "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 18,
      "context" : "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 20,
      "context" : "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 18,
      "context" : "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set ψ that determines the topology of the HMMs [19][20][21].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set ψ that determines the topology of the HMMs [19][20][21].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 20,
      "context" : "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set ψ that determines the topology of the HMMs [19][20][21].",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 16,
      "context" : "This can be achieved by first finding an initial label ω0 based on a set of assumed patterns for all observations in the corpus χ as in (1)[17].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 22,
      "context" : "The KL-divergence KL(i, j) between two pattern HMMs in (4b) is defined as the KLdivergence between the states based on the variational approximation [23] summed over the states.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 23,
      "context" : "To transform the KL divergence into a similarity measure between 0 and 1, a negative exponential was applied [24] with a scaling factor β.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "Note that although choices of λ(ψk, γs) can be based on the optimization with respect to an evaluation metric [25][26], the preliminary experiments in this work were to verify the feasibility of the proposed frameworks instead.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 25,
      "context" : "Note that although choices of λ(ψk, γs) can be based on the optimization with respect to an evaluation metric [25][26], the preliminary experiments in this work were to verify the feasibility of the proposed frameworks instead.",
      "startOffset" : 114,
      "endOffset" : 118
    } ],
    "year" : 2015,
    "abstractText" : "This paper presents a new approach for unsupervised Spoken Term Detection with spoken queries using multiple sets of acoustic patterns automatically discovered from the target corpus. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a three-dimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to one another, thus can jointly capture the characteristics of the spoken terms. By representing the spoken content and spoken query as sequences of acoustic patterns, a series of approaches for matching the pattern index sequences while considering the signal variations are developed. In this way, not only the on-line computation load can be reduced, but the signal distributions caused by different speakers and acoustic conditions can be reasonably taken care of. The results indicate that this approach significantly outperformed the unsupervised feature-based DTW baseline by 16.16% in mean average precision on the TIMIT corpus.",
    "creator" : "LaTeX with hyperref package"
  }
}