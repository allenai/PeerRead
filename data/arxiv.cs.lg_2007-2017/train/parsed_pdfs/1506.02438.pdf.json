{
  "name" : "1506.02438.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
    "authors" : [ "John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel" ],
    "emails" : [ "joschu@eecs.berkeley.edu", "pcmoritz@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu", "jordan@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Policy gradient methods provide a conceptually straightforward approach to policy learning by directly optimizing the expected cost. Simple Monte Carlo estimators of the policy gradient [20, 14] are unbiased and do not require a value function, but their variance scales unfavorably with the time horizon, since the effect of an action is confounded with the effects of past and future actions. Actor-critic methods avoid this issue by making use of a value function instead of Monte Carlo estimates [9], at the cost of introducing bias when the value function is not exact, which may cause the algorithm to diverge or converge to a suboptimal solution.\nWe propose a family of policy gradient estimators that interpolate between these two cases, allowing the biasvariance tradeoff to be balanced for the problem. We call this estimation scheme, parameterized by γ ∈ [0, 1] and λ ∈ [0, 1], the generalized advantage estimator (GAE). GAE uses a discounted sum of temporal difference (TD) residuals as an estimate of the advantage function. Related methods have been proposed in the context of online actorcritic methods [8, 19]. We provide a more general analysis, which is applicable in both the online and batch settings, and discuss an interpretation of our method as an instance of cost shaping [10], where the approximate value function is used to shape the cost.\nEmpirically, GAE achieves significantly faster policy improvement than both REINFORCE [20] and actor-critic methods, which are both special cases of GAE. We present experimental results on a number of highly challenging 3D locomotion tasks, where we show that our approach can learn complex gaits using high-dimensional, general purpose neural network function approximators for both the policy and the value function, each with over 104 parameters. The policies perform torque-level control of simulated 3D robots with up to 33 state dimensions and 10 actuators.\nThe contributions of this paper are summarized as follows:\n1. We provide justification and intuition for an effective variance reduction scheme for policy gradients, which we call generalized advantage estimation (GAE). While the formula has been proposed in prior work [8, 19], our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments.\n2. We propose the use of a trust region optimization method for the value function, which we find is a robust and efficient way to train neural network value functions with thousands of parameters.\nar X\niv :1\n50 6.\n02 43\n8v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\n5\n3. By combining (1) and (2) above, we obtain an algorithm that empirically is effective at learning neural network policies for challenging control tasks. The results extend the state of the art in using reinforcement learning for high-dimensional continuous control. Videos are available at https://sites.google.com/site/ gaepapersupp."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We consider an undiscounted, episodic formulation of the policy optimization problem. The initial state s1 is sampled from ρ1. A trajectory (s1, a1, s2, a2, . . . , sT , aT ) is generated by sampling actions according to the policy at ∼ π(at | st) and sampling the states according to the dynamics st+1 ∼ P (st+1 | st, at). A cost ct = c(st, at, st+1) is incurred at each step. The goal is to minimize the expected total cost ∑T t=1 ct. Although we use episodes of length T , for notational convenience (and to avoid boundary terms) we will often write sums that go up to infinity, where it is implied that ct = V (st) = 0 for t > T . Policy gradient methods optimize the expected cost by using the gradient g := ∇θE [∑T t=1 ct ] . There are several different related expressions for the policy gradient, which have the form\ng = E [ T∑ t=1 Ψt∇θ log πθ(at | st) ] , (1)\nwhere Ψt may be one of the following:\n1. ∑T t=1 ct: total cost of the trajectory.\n2. ∑T t′=t ct′ : cost following action at.\n3. ∑T t′=t ct′−b(st): baselined version of previous for-\nmula.\n4. Qπ(st, at): state-action value function.\n5. Aπ(st, at): advantage function.\n6. ct + V π(st+1)− V π(st): TD residual.\nThese formulas can be derived from each other through by manipulating the nested expectations and by using the fact that Eat [∇θ log πθ(at | st)] = 0. The latter formulas use the definitions\nV π(st) := Est+1:T , at:T [ ∞∑ l=0 ct+l ] Qπ(st, at) := Est+1:T , at+1:T [ ∞∑ l=0 ct+l ] (2) Aπ(st, at) := Q π(st, at)− V π(st), (Advantage function) (3)\nwhere the subscript of E enumerates the variables being integrated over. In general, the formula based on total cost (Item 1) has the highest variance, and the the formula based on Aπ (Item 5) has the lowest (but requires that advantage function is known, which isn’t the case in practice).\nWe will introduce a parameter γ that allows us to reduce variance by downweighting costs corresponding to delayed effects, at the cost of introducing bias. This parameter is related to the discount factor used in discounted formulations of MDPs, but we treat it as a variance reduction parameter, following prior work [16, 7], and analyze different settings in our experiments. The discounted value functions are given by:\nV π,γ(st) := Est+1:T , at:T [ ∞∑ l=0 γlct+l ] Qπ,γ(st, at) := Est+1:T , at+1:T [ ∞∑ l=0 γlct+l ] (4) Aπ,γ(st, at) := Q π,γ(st, at)− V π,γ(st). (5)\nThe discounted approximation to the policy gradient is defined as follows:\ngγ := Es1:T a1:T [ T∑ t=1 Aπ,γ(st, at)∇θ log πθ(at | st) ] . (6)\n= Es1:T a1:T [ T∑ t=1 Q̂γt∇θ log πθ(at | st) ] , (7)\nwhere Q̂γt = ct + γct+1 + γ 2ct+2 + . . . , and V is a function.\nIn general, the quantity inside the expectation of Equation (6) has lower variance than the quantity inside the expectation of Equation (7), since subtracting the value function acts as a baseline [5]. Thus, it would give a much better estimate of the policy gradient. Unfortunately, the advantage functionAπ,γ(st, at) is unknown and very difficult to estimate, whereas Q̂t can be computed at each timestep from a single sample trajectory. The following section tries to achieve the low-variance ideal of Equation (6) by using a value function to help approximate the advantage function Aπ,γ ."
    }, {
      "heading" : "3 Advantage function estimation",
      "text" : "This section will be concerned with producing an accurate estimate Ât of the true advantage function Aπ,γ(st, at), which will then be used to construct a policy gradient estimator of the following form:\ngγ ≈ E [ T∑ t=1 Ât∇θ log πθ(at | st) ] . (8)\nLet V be an approximate value function. Define δVt = ct + γV (st+1) − V (st), i.e., the TD residual of V [13], and consider the discounted sum of these δV terms. (Recall that we are using the convention that ct = V (st) = 0 for t > T .)\n∞∑ l=0 γlδVt+l = ∞∑ l=0 γl(ct+l + γV (st+l+1)− V (st+l)) = ∞∑ l=0 γlct+l − V (st) = Q̂γt − V (st). (9)\nHence, the sum ∑∞ l=0 γ lδVt+l is an unbiased estimator of the advantage functionA π,γ(st, at), since Est+1:T\nat+1:T\n[∑∞ l=0 γ lδVt+l ] =\nEst+1:T at+1:T\n[ Q̂γt − V (st) ] = Qπ,γ(st, at)− V π,γ(st) = Aπ,γ(st, at). This result is an identity and does not require V to\nbe accurate. Now, suppose that we happen to know the exact value function, V = V π,γ . Then we only need to use the first term\nin the sum of TD residuals, since Est+1 [ δVt ]\n= Est+1 [ct + γV π,γ(st+1)− V π,γ(st)] = Est+1:Tat+1:T\n[ Q̂γt − V (st) ] =\nAπ,γ(st, at). Moreover, all terms after the first term contribute nothing to the advantage function:\nEst+1:T at+1:T\n[ δVt+l ] = 0 for all l > 0 . (10)\nHowever, in general when V 6= V π,γ , δVt is a biased estimator: Est+1 [ δVt ] 6= Aπ,γ(st, at).\nWe have now introduced two estimators for the advantage function: one that use the entire sum of TD residuals, and one that just uses the first residual:\nÂt := ∞∑ l=0 γlδt+l = ∞∑ l=0 γlct+l − V (st) (11)\nÂt := δt = ct + γV (st+1)− V (st) (12)\nThe former is unbiased, but it has high variance due to the sum of terms. The latter is biased for any V 6= V π , but it typically has much lower variance. Our generalized advantage estimator makes a compromise between bias and variance, controlled by parameter λ. It is defined as an exponentially-weighted sum of the TD residuals:\nÂ GAE(γ,λ) t := ∞∑ l=0 (γλ)lδVt+l = ∞∑ l=0 (γλ)l[ct+l + γ(1− λ)V (st+l+1)]− V (st). (13)\nNote that this expression is unbiased for V = V π , via Equation (10), and it smoothly interpolates between Equation (11) (λ = 1) and Equation (12) (λ = 0). The TD(λ) method for value function estimation [13] uses a similar γλ-discounted sum of TD errors, though the goal in that case is to estimate the state-value function rather than the advantage function.\nWe’ve described an advantage estimator with two separate parameters γ and λ, both of which contribute to the bias-variance tradeoff when using an approximate value function. γ determines the scale of the value function V π,γ , which does not depend on λ. Taking γ < 1 introduces bias into the policy gradient estimate, regardless of the value function’s accuracy. On the other hand, λ < 1 introduces bias only when the value function is inaccurate. Empirically, we find that the best value of λ is much lower than the best value of γ, likely because λ introduces far less bias than γ for a reasonably accurate value function.\nUsing the generalized advantage estimator, we can form an estimate of the policy gradient:\ngγ ≈ E [ T∑ t=1 ∇θ log πθ(at | st)ÂGAE(γ,λ)t ] = E [ T∑ t=1 ∇θ log πθ(at | st) ∞∑ l=0 (γλ)lδVt+l ] , (14)\nwhere, as above, δt = ct = V (st) = 0 for t ≥ T . This estimator reduces to baselined REINFORCE [20] (with advantage estimate from Equation (11)) when λ = 1, and to the actor-critic method [9] (with advantage estimate from Equation (12)) when λ = 0."
    }, {
      "heading" : "4 Interpretation as Cost Shaping",
      "text" : "In this section, we discuss how one can interpret λ as an extra discount factor applied after performing a cost shaping transformation on the MDP. We also introduce the notion of a response function to help understand the bias introduced by γ and λ.\nCost shaping refers to the following transformation of the cost function of an MDP: let Φ : S → R be an arbitrary scalar-valued function on state space, and define the transformed cost function c̃ by\nc̃(s, a, s′) = c(s, a, s′) + γΦ(s′)− Φ(s). (15)\nAs pointed out by Ng et al. [10], this shaping transformation does not alter the optimal policy (in the discounted setting). In fact, it leaves the advantage functionAπ,γ unchanged for any policy π. This follows from the observation that if we consider any two trajectories starting from the same state, the difference in their discounted costs is unchanged by the shaping transformation. Note that if Φ happens to be the value function V π,γ (in the original MDP), then the new MDP has the interesting property that V π,γ is zero at every state. Now it becomes clear that the terms δVt in the policy gradient estimators of Section 2 can be interpreted as reshaped costs, using the approximate value function Φ = V for shaping. To further analyze the effect of this shaping transformation, it will be useful to introduce the notion of a response function χ, which we define as follows:\nχ(l; st, at) = E [ct+l | st, at]− E [ct+l | st] . (16) Note that Aπ,γ(s, a) = ∑∞ l=0 γ\nlχ(l; s, a), hence the response function decomposes the advantage function across timesteps. The response function lets us quantify the temporal credit assignment problem: long range dependencies between actions and costs correspond to nonzero values of the response function for l 0.\nGiven the shaped cost function c̃ using Φ = V π,γ , we have E [c̃t+l | st, at] = E [c̃t+l | st] = 0 for l > 0, i.e., the response function is only nonzero at l = 0. Therefore, this shaping transformation turns a temporally extended response into an immediate response.\nGiven that V π,γ completely reduces the temporal spread of the response function, we would hope that a good approximation V would partially reduce it. This observation suggests an interpretation of Equation (13): take the original Monte Carlo estimator for the policy gradient in Equation (7), reshape the costs using V to shrink the temporal extent of the response function, and then introduce a “steeper” discount γλ to cut off the terms corresponding to long temporal delays.\nHaving defined response functions, let us revisit the discount factor γ and the approximation we are making by using Aπ,γ rather than Aπ,1. The discounted policy gradient estimator from Equation (6) has a sum of terms of the form\nAπ,γ(st, at)∇θ log πθ(at | st) = ∞∑ l=0 γlχ(l; s, a)∇θ log πθ(at | st). (17)\nUsing a discount γ < 1 corresponds to dropping the terms with l 1/(1 − γ). Thus, the error introduced by this approximation will be small if χ rapidly decays as l increases, i.e., if the effect of an action on costs is “forgotten” after ≈ 1/(1− γ) timesteps."
    }, {
      "heading" : "5 Value Function Estimation",
      "text" : "A variety of different methods can be used to estimate the value function (see, e.g., [2]). When using a nonlinear function approximator to represent the value function, the simplest approach is to solve a nonlinear regression problem:\nminimize φ N∑ n=1 ‖Vφ(sn)− V̂n‖2, (18)\nwhere V̂t = ∑T l=0 γ\nlct+l is the discounted sum of costs, and n indexes over all timesteps in a batch of trajectories. This is sometimes called the Monte Carlo or TD(1) approach for estimating the value function [13].1\nFor the experiments in this work, we used a trust region method to optimize the value function in each iteration of a policy iteration procedure. The trust region helps us to avoid overfitting to the most recent batch of data. To formulate the trust region problem, we first compute σ2 = 1N ∑N n=1‖Vφold(sn)− V̂n‖2, where φold is the parameter vector before optimization. Then we solve the following constrained optimization problem:\nminimize φ N∑ n=1 ‖Vφ(sn)− V̂n‖2\nsubject to 1\nN N∑ n=1 ‖Vφ(sn)− Vφold(sn)‖2 2σ2 ≤ . (19)\nWe compute an approximate solution to the trust region problem using the conjugate gradient algorithm [21]."
    }, {
      "heading" : "6 Experiments",
      "text" : "We designed a set of experiments to investigate the following questions:\n1. What is the empirical effect of varying λ ∈ [0, 1] and γ ∈ [0, 1] when optimizing episodic total cost using generalized advantage estimation?\n2. Can generalized advantage estimation, along with trust region algorithms for policy and value function optimization, be used to optimize large neural network policies for challenging control problems?"
    }, {
      "heading" : "6.1 Policy Optimization Algorithm",
      "text" : "While generalized advantage estimation can be used along with a variety of different policy gradient methods, for these experiments, we performed the policy updates using trust region policy optimization (TRPO) [12]. In particular, we used the instantiation that minimizes gT (θ−θold) subject to a quadratic constraint, which yields a step in the direction θ − θold ∝ −A−1g, where A is the average Fisher information matrix. Hence, the policy optimization algorithm is similar to the natural policy gradient [6] / natural actor-critic [11]. However, it uses the methods specified in [12] for optimization and stepsize determination, and it uses ÂGAE(γ,λ) in the gradient estimator.\nSince prior work [12] compared TRPO to a variety of different policy optimization algorithms, we will not repeat these comparisons; rather, we will focus on varying the γ, λ parameters of policy gradient estimator while keeping the underlying algorithm fixed."
    }, {
      "heading" : "6.2 Experimental Setup",
      "text" : "We evaluated our approach on the classic cart-pole balancing problem, as well as several challenging 3D locomotion tasks: (1) bipedal locomotion; (2) quadrupedal locomotion; (3) dynamically standing up, for the biped, which starts off laying on its back. The models are shown in Figure 1.\n1Another natural choice is to compute target values with an estimator based on the TD(λ) backup [2, 13], mirroring the expression we use for policy gradient estimation: V̂ λt = Vφold (sn) + ∑∞ l=0(γλ)\nlδt+l. While we experimented with this choice, we did not notice a difference in performance from the λ = 1 estimator in Equation (18)."
    }, {
      "heading" : "6.2.1 Architecture",
      "text" : "We used the same architecture for all of the 3D robot tasks, which was a feedforward network with three hidden layers, with 100, 50 and 25 tanh units respectively. The final output layer had linear activation. The value function estimator used the same architecture, but with only one scalar output. For the simpler cart-pole task, we used a linear policy, and a neural network with one 20-unit hidden layer as the value function."
    }, {
      "heading" : "6.2.2 Task details",
      "text" : "The simulated robot tasks were simulated using the MuJoCo physics engine [17]. The humanoid model has 33 state dimensions and 10 actuated degrees of freedom, while the quadruped model has 29 state dimensions and 8 actuated degrees of freedom. The initial state for these tasks consisted of a uniform distribution centered on a reference configuration. The cart-pole task was implemented according to the description by Barto et al. [1].\nFor the cart-pole balancing task, we collected 20 trajectories per batch, whereas we used 50000 timesteps per batch for bipedal locomotion, and 200000 timesteps per batch for quadrupedal locomotion and bipedal standing.\nThe cost functions are provided in the table below.\nTask Cost 3D biped locomotion −vfwd + 10−5‖u‖2 + 10−5‖fimpact‖2 − 0.2\nQuadruped locomotion −vfwd + 10−6‖u‖2 + 10−3‖fimpact‖2 − 0.05 Biped getting up (hhead − 1.5)2 + 10−5‖u‖2\nHere, vfwd := forward velocity, u := vector of joint torques, fimpact := impact forces, hhead := height of the head. In the locomotion tasks, the episode is terminated if the center of mass of the actor falls below a predefined height:\n.8 m for the biped, and .2 m for the quadruped. The constant offset in the cost function encourages longer episodes; otherwise the quadratic cost terms might lead lead to a policy that ends the episodes as quickly as possible."
    }, {
      "heading" : "6.3 Experimental Results",
      "text" : ""
    }, {
      "heading" : "6.3.1 Cart-pole",
      "text" : "The results are averaged across 21 experiments with different random seeds. Results are shown in Figure Figure 2, and indicate that the best results are obtained at intermediate values of the parameters: γ ∈ [0.96, 0.99] and λ ∈ [0.92, 0.99]."
    }, {
      "heading" : "6.3.2 3D bipedal locomotion",
      "text" : "Each optimization took about 2 hours to run on a 16-core machine. Here, the results are averaged across 9 trials with different random seeds. The best performance is again obtained using intermediate values of γ ∈ [0.99, 0.995], λ ∈ [0.96, 0.99]."
    }, {
      "heading" : "6.3.3 Other 3D robot tasks",
      "text" : "The other two motor behaviors considered are quadrupedal locomotion and getting up off the ground for the 3D biped. Again, we performed 5 trials per experimental condition, with different random seeds (and initializations). The experiments took about 4 hours per trial on a 32-core machine. We performed a more limited comparison on these domains (due to the substantial computational resources required to run these experiments), fixing γ = 0.995 but varying λ = {1, 0.96}, as well as an experimental condition with no value function. For quadrupedal locomotion, the best results are obtained using a value function with λ = 0.96 Section 6.3.2. For 3D standing, the value function always helped, but the results are roughly the same for λ = 0.96 and λ = 1."
    }, {
      "heading" : "7 Discussion",
      "text" : "Our main experimental validation of generalized advantage estimation is in the domain of simulated robotic locomotion, which is challenging due to the high dimensionality of the state and action space and the complexity of the function approximators for the policy and value function, which we represent with large neural networks. In these domains, the λ = 0 estimator has excessively high bias, and the λ = 1 estimator has high variance. As shown in our experiments, choosing an appropriate intermediate value of λ in the range [0.9, 0.99] usually results in the best performance. A possible topic for future work is how to adjust the estimator parameters γ, λ in an adaptive or automatic way.\nOur method learned all of the gaits and motor behaviors with general-purpose policies and simple cost functions, using minimal prior knowledge. This is in contrast with most prior methods for learning locomotion controllers, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping [15, 4, 18].\nOne question that merits future investigation is the relationship between value function estimation error and policy gradient estimation error. If this relationship were known, we could choose an error metric for value function fitting that is well-matched to the quantity of interest, which is typically the accuracy of the policy gradient estimation. Some candidates for such an error metric might include the Bellman error or projected Bellman error, as described in [3].\nAnother enticing possibility is to use a shared function approximation architecture for the policy and the value function, while optimizing the policy using generalized advantage estimation. While formulating this problem in a way that is suitable for numerical optimization and provides convergence guarantees remains an open question, such an approach could allow the value function and policy representations to share useful features of the input, resulting in even faster learning."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Emo Todorov for providing the simulator as well as insightful discussions, and we thank Greg Wayne, Yuval Tassa, Dave Silver, and others at Google DeepMind for insightful discussions. This research was funded in part by the Office of Naval Research through a Young Investigator Award and under grant number N00014-11-1-0688, DARPA through a Young Faculty Award, by the Army Research Office through the MAST program."
    } ],
    "references" : [ {
      "title" : "Neuronlike adaptive elements that can solve difficult learning control problems",
      "author" : [ "Andrew G Barto", "Richard S Sutton", "Charles W Anderson" ],
      "venue" : "Systems, Man and Cybernetics, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1983
    }, {
      "title" : "Dynamic programming and optimal control, volume 2",
      "author" : [ "Dimitri P Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Convergent temporal-difference learning with arbitrary smooth function approximation",
      "author" : [ "Shalabh Bhatnagar", "Doina Precup", "David Silver", "Richard S Sutton", "Hamid R Maei", "Csaba Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Fast biped walking with a reflexive controller and realtime policy searching",
      "author" : [ "T. Geng", "B. Porr", "F. Wörgötter" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Variance reduction techniques for gradient estimates in reinforcement learning",
      "author" : [ "Evan Greensmith", "Peter L Bartlett", "Jonathan Baxter" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "A natural policy gradient",
      "author" : [ "Sham Kakade" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "Optimizing average reward using discounted rewards. In Computational Learning Theory, pages 605–615",
      "author" : [ "Sham Kakade" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "An analysis of actor/critic algorithms using eligibility traces: Reinforcement learning with imperfect value function",
      "author" : [ "Hajime Kimura", "Shigenobu Kobayashi" ],
      "venue" : "In ICML,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "On actor-critic algorithms",
      "author" : [ "Vijay R Konda", "John N Tsitsiklis" ],
      "venue" : "SIAM journal on Control and Optimization,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "Policy invariance under reward transformations: Theory and application to reward shaping",
      "author" : [ "Andrew Y Ng", "Daishi Harada", "Stuart Russell" ],
      "venue" : "In ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel" ],
      "venue" : "arXiv preprint arXiv:1502.05477,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Introduction to reinforcement learning",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1999
    }, {
      "title" : "Stochastic policy gradient reinforcement learning on a simple 3d biped",
      "author" : [ "R. Tedrake", "T. Zhang", "H. Seung" ],
      "venue" : "In IEEE/RSJ International Conference on Intelligent Robots and Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Bias in natural actor-critic algorithms",
      "author" : [ "Philip Thomas" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Mujoco: A physics engine for model-based control",
      "author" : [ "Emanuel Todorov", "Tom Erez", "Yuval Tassa" ],
      "venue" : "In Intelligent Robots and Systems (IROS),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Optimal gait and form for animal locomotion",
      "author" : [ "Kevin Wampler", "Zoran Popović" ],
      "venue" : "In ACM Transactions on Graphics (TOG),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Real-time reinforcement learning by sequential actor–critics and experience replay",
      "author" : [ "Paweł Wawrzyński" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1992
    }, {
      "title" : "Numerical optimization",
      "author" : [ "Stephen J Wright", "Jorge Nocedal" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Simple Monte Carlo estimators of the policy gradient [20, 14] are unbiased and do not require a value function, but their variance scales unfavorably with the time horizon, since the effect of an action is confounded with the effects of past and future actions.",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "Simple Monte Carlo estimators of the policy gradient [20, 14] are unbiased and do not require a value function, but their variance scales unfavorably with the time horizon, since the effect of an action is confounded with the effects of past and future actions.",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "Actor-critic methods avoid this issue by making use of a value function instead of Monte Carlo estimates [9], at the cost of introducing bias when the value function is not exact, which may cause the algorithm to diverge or converge to a suboptimal solution.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "We call this estimation scheme, parameterized by γ ∈ [0, 1] and λ ∈ [0, 1], the generalized advantage estimator (GAE).",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "We call this estimation scheme, parameterized by γ ∈ [0, 1] and λ ∈ [0, 1], the generalized advantage estimator (GAE).",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Related methods have been proposed in the context of online actorcritic methods [8, 19].",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "Related methods have been proposed in the context of online actorcritic methods [8, 19].",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "We provide a more general analysis, which is applicable in both the online and batch settings, and discuss an interpretation of our method as an instance of cost shaping [10], where the approximate value function is used to shape the cost.",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 18,
      "context" : "Empirically, GAE achieves significantly faster policy improvement than both REINFORCE [20] and actor-critic methods, which are both special cases of GAE.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "While the formula has been proposed in prior work [8, 19], our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments.",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "While the formula has been proposed in prior work [8, 19], our analysis is novel and enables GAE to be applied with a more general set of algorithms, including the batch trust-region algorithm we use for our experiments.",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "This parameter is related to the discount factor used in discounted formulations of MDPs, but we treat it as a variance reduction parameter, following prior work [16, 7], and analyze different settings in our experiments.",
      "startOffset" : 162,
      "endOffset" : 169
    }, {
      "referenceID" : 6,
      "context" : "This parameter is related to the discount factor used in discounted formulations of MDPs, but we treat it as a variance reduction parameter, following prior work [16, 7], and analyze different settings in our experiments.",
      "startOffset" : 162,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "In general, the quantity inside the expectation of Equation (6) has lower variance than the quantity inside the expectation of Equation (7), since subtracting the value function acts as a baseline [5].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 11,
      "context" : ", the TD residual of V [13], and consider the discounted sum of these δ terms.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "The TD(λ) method for value function estimation [13] uses a similar γλ-discounted sum of TD errors, though the goal in that case is to estimate the state-value function rather than the advantage function.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : "This estimator reduces to baselined REINFORCE [20] (with advantage estimate from Equation (11)) when λ = 1, and to the actor-critic method [9] (with advantage estimate from Equation (12)) when λ = 0.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "This estimator reduces to baselined REINFORCE [20] (with advantage estimate from Equation (11)) when λ = 1, and to the actor-critic method [9] (with advantage estimate from Equation (12)) when λ = 0.",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "[10], this shaping transformation does not alter the optimal policy (in the discounted setting).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : ", [2]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 11,
      "context" : "This is sometimes called the Monte Carlo or TD(1) approach for estimating the value function [13].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "We compute an approximate solution to the trust region problem using the conjugate gradient algorithm [21].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "What is the empirical effect of varying λ ∈ [0, 1] and γ ∈ [0, 1] when optimizing episodic total cost using generalized advantage estimation? 2.",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "What is the empirical effect of varying λ ∈ [0, 1] and γ ∈ [0, 1] when optimizing episodic total cost using generalized advantage estimation? 2.",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "1 Policy Optimization Algorithm While generalized advantage estimation can be used along with a variety of different policy gradient methods, for these experiments, we performed the policy updates using trust region policy optimization (TRPO) [12].",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 5,
      "context" : "Hence, the policy optimization algorithm is similar to the natural policy gradient [6] / natural actor-critic [11].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "However, it uses the methods specified in [12] for optimization and stepsize determination, and it uses Â in the gradient estimator.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "Since prior work [12] compared TRPO to a variety of different policy optimization algorithms, we will not repeat these comparisons; rather, we will focus on varying the γ, λ parameters of policy gradient estimator while keeping the underlying algorithm fixed.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "1Another natural choice is to compute target values with an estimator based on the TD(λ) backup [2, 13], mirroring the expression we use for policy gradient estimation: V̂ λ t = Vφold (sn) + ∑∞ l=0(γλ) δt+l.",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "1Another natural choice is to compute target values with an estimator based on the TD(λ) backup [2, 13], mirroring the expression we use for policy gradient estimation: V̂ λ t = Vφold (sn) + ∑∞ l=0(γλ) δt+l.",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "2 Task details The simulated robot tasks were simulated using the MuJoCo physics engine [17].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "This is in contrast with most prior methods for learning locomotion controllers, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping [15, 4, 18].",
      "startOffset" : 192,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "This is in contrast with most prior methods for learning locomotion controllers, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping [15, 4, 18].",
      "startOffset" : 192,
      "endOffset" : 203
    }, {
      "referenceID" : 16,
      "context" : "This is in contrast with most prior methods for learning locomotion controllers, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping [15, 4, 18].",
      "startOffset" : 192,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "Some candidates for such an error metric might include the Bellman error or projected Bellman error, as described in [3].",
      "startOffset" : 117,
      "endOffset" : 120
    } ],
    "year" : 2015,
    "abstractText" : "This paper is concerned with developing policy gradient methods that gracefully scale up to challenging problems with high-dimensional state and action spaces. Towards this end, we develop a scheme that uses value functions to substantially reduce the variance of policy gradient estimates, while introducing a tolerable amount of bias. This scheme, which we call generalized advantage estimation (GAE), involves using a discounted sum of temporal difference residuals as an estimate of the advantage function, and can be interpreted as a type of automated cost shaping. It is simple to implement and can be used with a variety of policy gradient methods and value function approximators. Along with this variance-reduction scheme, we use trust region algorithms to optimize the policy and value function, both represented as neural networks. We present experimental results on a number of highly challenging 3D locomotion tasks, where our approach learns complex gaits for bipedal and quadrupedal simulated robots. We also learn controllers for the biped getting up off the ground. In contrast to prior work that uses hand-crafted low-dimensional policy representations, our neural network policies map directly from raw kinematics to joint torques.",
    "creator" : "LaTeX with hyperref package"
  }
}