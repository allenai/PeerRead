{
  "name" : "1702.05398.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Experiment Segmentation in Scientific Discourse as Clause-level Structured Prediction using Recurrent Neural Networks",
    "authors" : [ "Pradeep Dasigi", "Gully A.P.C. Burns", "Eduard Hovy", "Anita de Waard" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "An important part of science is communicating results. There are well established rhetorical guidelines (Alley, 1996) for scientific writing that are used across disciplines and consequently, narratives describing evidence within a scientific investigation are expected to have a certain structure. Typically, the description begins with certain background information which has already been proved, followed by some motivating hypotheses to introduce the experiment, the methods inferences made based on those results. Understanding this structure is important since it enables the higher-level construction of the general argument of the paper. The reader assembles the pieces in order to understand what was done, why it was done, what prior knowledge it builds upon and/or refutes, and with what certainty the final conclusions should be accepted. Without such an overall model of the experiment, the reader has nothing but basic assertions.\nIn this work, our aim is to identify these discourse elements given an experiment narrative. We view the task at hand as a sequence labeling problem: Given a sequence of clauses from a paragraph describing an experiment, we seek to label the clauses with their discourse type. There exist several proposals for experiment discourse models (Liakata, 2010; Nawaz, Thompson, and Ananiadou, 2010; Mizuta and Collier, 2004; Nwogu, 1997). We adopt the discourse type taxonomy for biological papers suggested by De Waard and\nPander Maat (2012), and define our problem as identifying the discourse type of each clause in a given experiment description. The taxonomy contains seven types (Table 1) and Figure 1 shows an example paragraph1 broken down into clauses and tagged with discourse types.\nWhile there has been some variation in the level of granularity of text in prior discourse processing work, for our task the appropriate level of processing is clearly at the clauselevel. As shown in Figure 1, there are many sentences in our data containing clauses of different kinds. For example, a pattern we observe frequently is when an author writes “To understand phenomenon X, we performed experiment Y” yielding a ‘goal’ followed by a ‘method’ clause in a single sentence. Using the main and subordinate clauses from a Stanford parse provided good segregation of this structure.\nFor this work, we focus on Systems Biology (SB) papers concerning signaling pathways in cancer cells. Typically, researchers in this field use a number of small-scale experimental assays to investigate molecular events, see Voit (2012) and Svoboda and Reenstra (2002) for textbook and review introductions. There can easily be as many as 20-30 separate small experiments in any study that each provide evidence for the interpretive assertions being made. Our goal is to partition the text of SB papers to identify smallscale passages that describe the goals, methods, results and implications of each experiment. By convention, subfigures denoted by ‘1A’, ‘1B’, ‘1C’ etc. each describe data from a separate experiment and are directly referenced in the narrative (see Figure 1)."
    }, {
      "heading" : "Related Work",
      "text" : "Identifying structure within scientific papers There is a significant amount of prior work that is aimed at scientific discourse processing. Teufel and Moens (2002) and Teufel and Moens (1999) describe argumentative zoning (AZ), a way of classifying scientific papers at a sentence level, into zones, thus extracting the structure from entire papers. Hirohata et al. (2008) use a 4-way classification scheme for abstracts of scientific papers for identifying objectives, methods, results and conclusions. Liakata (2010)\n1From Angers-Loustau et al. (1999) “Protein tyrosine phosphatase-PEST regulates focal adhesion disassembly, migration, and cytokinesis in fibroblasts” J. Cell Bio. 144:1019-31\nar X\niv :1\n70 2.\n05 39\n8v 1\n[ cs\n.C L\n] 1\n7 Fe\nb 20\n17\ndescribed a three-layer finer grained annotation scheme for sentence-level annotation (with 11 separate categorical labels) for identifying the core scientific concepts of papers. Classification performance for machine learning systems to automatically tag scientific sentences was F-Score=0.51 for LibSVM classifiers (Liakata et al., 2012). There is extensive overlap between leaf elements of the CoreSC schema and our simpler discourse type model (‘Hypothesis’, ‘Goal’, ‘Method’, and ‘Result’ are shared between both annotation sets and tags like ‘Background’ and ‘Conclusion’ map to our ’fact’ and ’implication’ tags).\nGuo et al. (2010) used SVM and Naı̈ve Bayes classifiers to compare the three schemes described above. Gupta and Manning (2011) also studied the problem of extracting focus, techniques and the domain of research papers to identify the influence of research communities over each other.\nIn these studies, the focus of research is largely centered on modeling the discourse being used to construct a scientific argument, driving towards understanding “sentiment expressed towards cited work, ownership of ideas, and speech acts which express rhetorical statements typical for scientific argumentation” (Teufel, 2000). These are driven by human-to-human communication processes of the scientific literature rather than using discourse elements to support machine reading of a semantic representation of scientific findings from primary experimental research papers. Our focus is specifically on attempting to identify text pertaining to experimental evidence for scientific IE rather focusing on authors’ interpretations of those findings.\nDeep Learning for structured prediction and text classification There is a great amount of work in classification and structured prediction over text and other modalities, that uses deep learning. Particularly in sequence labeling tasks in text, (Collobert et al., 2011) words are represented as vectors\nand used as features to train a tagger. One advantage of using this approach is the reusability of pre-trained word vectors (Mikolov et al., 2014; Pennington, Socher, and Manning, 2014) as features in various tasks. In our task, the sequences being labeled are clauses instead of words. We obtain vector representations of clauses by summarizing those of words in the clauses.\nAttention has been used for complex tasks like question answering (Hermann et al., 2015) and machine translation (Bahdanau, Cho, and Bengio, 2014). In sequence-tosequence learning problems like machine translation (Bahdanau, Cho, and Bengio, 2014), parsing (Vinyals et al., 2015) and image caption generation (Xu et al., 2015), one network is used to encode the input modality, and a different network to decode into the output modality, with the decoder using attention to learn parts of the input to attend to for generating a given part of the output sequence. While our work does use two different models, one for encoding clause representations as a function of word representations and another for decoding clause labels from clause representations, the two models operate at different granularities.\nComparison with RST based discourse parsing General domain discourse parsing is a well-studied problem. While there are many discourse theories (see Marcu (2000), chapter 2 for an overview), Rhetorical Structure Theory (RST) by Mann and Thompson (1988), received a lot of attention. It is generally accepted that relations between non-overlapping chunks of text need to be considered to account for the overall meaning (Marcu, 2000). Accordingly, rhetorical relations are central in RST for marking the structure. In contrast, the taxonomy we use applies to the clauses themselves, instead of the relations between them. This is made possible by the specificity of our domain: In the general case, it may not be possible to identify the type of a clause in isolation. However, it has to be noted that the information conveyed by our clause-centric formalism may also be expressed using a relation-centric discourse formalism like RST. Figure 2 shows one possible RST tree for the text shown in Figure 1."
    }, {
      "heading" : "Approach",
      "text" : "We call our system Scientific Discourse Tagger (SciDT). Our pipeline is shown in Figure 3. The input to the tagger is a set of clauses from a paragraph. They are first embedded to obtain a tensor D ∈ Rc×w×d where c is the number of clauses , and w is the number of words in each clause, and each word is represented as a d dimensional vector. The tensors are zero-padded along the clause and word dimensions dimensions if needed. The next step is to summarize the clause representations to obtain Dsumm ∈ Rc×d, a matrix corresponding to the entire paragraph, with one vector per clause. Finally, Dsumm is fed to the Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) Hochreiter and Schmidhuber (1997) cells, to label the clauses. We propose two ways of summarizing the clause representations below. Both variants use attention to learn the weights of words within a clause based on their importance for the labeling task, and compute a weighted average of the word representations using those weights, to get the clause representation. The attention component and the LSTM-RNN are trained jointly. We use pretrained representations for words and fix them during training.\nAttention with and without context Both variants take as input the tensor D. The output in both cases is a matrix A ∈ Rc×w, which contains the attention weights of all the words in the paragraph. We first project the input words to a lower dimensional space in both cases using a projection operator P ∈ Rd×p.\nDl = tanh(D.P ) ∈ Rc×w×p (1)\nThe low dimension representations are then scored differently by each variant.\nOut of context This model defines a simple scoring operator ss ∈ Rp, that scores each word based only on its low\ndimension representation. The scoring is out of context because each word is scored in isolation.\nDil = Dl[i, :, :] ∈ Rw×p (2) ais = softmax(D i l .ss) ∈ Rw (3)\nAs = [ a1s a 2 s · · · aws ] ∈ Rc×w (4)\nEquation 2 corresponds to selecting all the words in the ith clause of the paragraph. Equation 3 shows the computation of attention scores for all words in the clause, and equation 4 simply puts the clause scores together to get the paragraph level attention values.\nClause context In this variant, we score words in a clause in the context of other words that occur in the clause. Concretely, as shown in the equations below, this is a recurrent scoring mechanism that uses a RNN to score each word in a clause as a function of its low-dimension representation and its previous context in the clause given by the hidden layer in the RNN. It has to be noted that the recurrence in this scoring model is over words in a clause while that in the LSTM described previously is over clauses.\nDil = Dl[i, :, :] ∈ Rw×p (5) hij = tanh(D i l [j, :].WIH + h i j−1.WHH) ∈ Rw×p (6)\nair = softmax(h i j .sr) ∈ Rw (7) Ar = [ a1r a 2 r · · · awr ] ∈ Rc×w (8)\nEquation 5, equation 7 and equation 8 are similar to equation 2, equation 3 and equation 4 respectively. The operator sr ∈ Rp is similar to ss from simple attention. In equation 6, we apply the standard RNN recurrence to update the hidden state, using the parameters WIH ∈ Rp×p, operating on the input word at the current timestep j, and WHH ∈ Rp×p, operating on the hidden state from the previous timestep j− 1."
    }, {
      "heading" : "Input to LSTM",
      "text" : "A weighted sum of the input tensor is computed, with the weights coming from the attention model, and it is fed to the LSTM.\nDsumm[i, :] = A[i, :].D[i, :, :] ∈ Rc×d (9)\nThe above equation shows the composed representation of the ith clause stored as the ith row in Dsumm."
    }, {
      "heading" : "Experiments",
      "text" : ""
    }, {
      "heading" : "Implementation Details",
      "text" : "We used the 200 dimension vectors trained on Pubmed Central data by Pyysalo et al. (2013) as input representations and projected them down to 50d to keep the parameter space of the entire model under control. The projection operator is also trained along with the rest of the pipeline. LSTMs were implemented 2 using Keras (Chollet, 2015) and attention using Theano (Bergstra et al., 2010). We trained for\n2Code is publicly available at https://github.com/ edvisees/sciDT\natmost 100 epochs, while monitoring accuracy on held-out validation data for early stopping. We used ADAM (Kingma and Ba, 2014) as the optimization algorithm. Dropout with p = 0.5 was used on input to the attention layer."
    }, {
      "heading" : "Data Preprocessing, Annotations and Pipelines",
      "text" : "We created a scientific discourse marked dataset from 75 papers in the area of intercellular cancer pathways taken from the Open Access3 subset of Pubmed Central. Using a multithreaded preprocessing pipeline, we extracted the Results sections of each of those papers, and parsed all the sentences using the Stanford Parser (Socher et al., 2013). This process separated the main and subordinate clauses of each sentence that we process as a sequence over separate paragraphs. We asked domain experts to label each of those clauses using the seven label taxonomy suggested by De Waard and Pander Maat (2012). We also added a None label for those clauses that do not fall under any category. Each sequence in the dataset corresponds to the clauses extracted from a paragraph. So we make the assumption that paragraphs are minimal experiment narratives. On the whole, our dataset4 consists of 392 experiment descriptions with a total of 4497 clauses. This is an ongoing annotation effort, and we intend to make a bigger dataset in the future.\nFigure 4 shows p(position|type) values of discourse types at various positions in a paragraph, estimated from the entire annotated dataset. It can be seen that goal, fact, problem and hypothesis are more likely at the beginning of a paragraph compared to other locations, whereas method peaks before the middle, result at the middle, and implication clearly towards the end of a paragraph. This trend supports the expected narrative structure described in Section 1.\n3http://www.ncbi.nlm.nih.gov/pmc/tools/ openftlist/\n4Please contact the authors if you would like to use this dataset for your research."
    }, {
      "heading" : "Results and Analysis",
      "text" : "A baseline model we compare against is a Conditional Random Field (Lafferty, McCallum, and Pereira, 2001) that uses as features part of speech tags, the identities of verbs and adverbs, presence of figure references and citations, and hand-crafted lexicon features5 that indicate specific discourse types. In addition, we also test a variant of our model that does not use attention in the input layer and the clause vectors are obtained simply as an average of the vectors of words in them. Table 2 shows accuracy scores and weighted averages6 of f-scores from 5-fold cross validation of the two baseline models and the two attention based models. The performance of the averaged input SciDT is comparable to the CRF model, whereas the two attention models perform better. The performance of the recurrent attention SciDT model shows the importance of modeling context in attention. On closer examination of the attention weights assigned to words in unseen paragraphs, we noticed stronger trends in the recurrent attention based model. Particularly, the main verbs of the sentences received the highest attention in many cases in the recurrent model. That the verb form is an important indicator of discourse type identification was shown by De Waard and Pander Maat (2012).\nFigure 5 shows examples of parts of clauses and the attention weights assigned to the words. These indicate the general trends of words relevant to discourse classes being given higher attention: investigate whether indicates Goal, analysis is a Method word, and strongly suggest is a phrase expected in Implication. While some of these errors made by the LSTM may be attributed to the model itself, there were also some exceptions to the assumption that clauses are the smallest units of discourse. There are some infrequent cases where clauses had components of multiple discourse types in them. Moreover, the syntactic parser we used to separate clauses was sometimes inaccurate, resulting in incorrect clause splits.\n5For example, words like demonstrate and suggest indicate implication; phrases like data not shown indicate result\n6F-scores are of all classes were averaged weighted by the number of points within that class in the gold standard."
    }, {
      "heading" : "Discussion and Conclusion",
      "text" : "We introduced a sequence labeling approach for identifying the discourse elements within experiment descriptions. Our model uses an attention mechanism over word representations to obtain clause representations. The results show that our attention based composition mechanism used to encode clauses adds value to the LSTM model. Visualizations show that the clause context model does indeed learn to attend to words important for the final tagging decision. In the future, we shall extend the idea of contextual attention to attend to words based on context at the paragraph level.\nOur system can complement existing IE tools that operate on scientific literature, and provide useful epistemic and contextual features. Identifying the structure of experiments provides additional context information that can help various downstream tasks. For event co-reference, one can use the structure to accurately resolve anaphora links. For example, a reference made in an implication statement is likely to some entity in a result statement it follows. It has to be noted that the taxonomy we used also provides epistemic information which is helpful in information extraction (IE): IE systems need not process clauses labeled as hypothesis or goal since they do not contain events that actually occurred. Going forward, our goal is to read, assemble and model mechanisms describing complex phenomena from collections of relevant scientific documents.\nThe application of our methods can reveal a small-scale discourse structure to contextualize, report and interpret evidence from individual experiments in a fine-grained context. This could be used to support cyclic models of scientific reasoning where data from individual experiments can be placed in an appropriate interpretive context within an informatics system that synthesizes knowledge across many papers. Beyond the scope of direct applications to IE, this work may be applied to Semantic Web representations of scientific knowledge and biocuration pipelines to accelerate knowledge acquisition."
    } ],
    "references" : [ {
      "title" : "The craft of scientific writing",
      "author" : [ "M. Alley" ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Alley,? 1996",
      "shortCiteRegEx" : "Alley",
      "year" : 1996
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano: A cpu and gpu math compiler in python",
      "author" : [ "J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : "van der Walt, S., and Millman, J., eds., Proceedings of the 9th Python in Science Conference, 3 – 10.",
      "citeRegEx" : "Bergstra et al\\.,? 2010",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Keras",
      "author" : [ "F. Chollet" ],
      "venue" : "https://github.com/ fchollet/keras.",
      "citeRegEx" : "Chollet,? 2015",
      "shortCiteRegEx" : "Chollet",
      "year" : 2015
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : "Journal of Machine Learning Research 12(Aug):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Verb form indicates discourse segment type in biological research",
      "author" : [ "A. De Waard", "H. Pander Maat" ],
      "venue" : null,
      "citeRegEx" : "Waard and Maat,? \\Q2012\\E",
      "shortCiteRegEx" : "Waard and Maat",
      "year" : 2012
    }, {
      "title" : "Identifying the information structure of scientific abstracts: an investigation of three different schemes",
      "author" : [ "Y. Guo", "A. Korhonen", "M. Liakata", "I.S. Karolinska", "L. Sun", "U. Stenius" ],
      "venue" : "Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, 99–",
      "citeRegEx" : "Guo et al\\.,? 2010",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2010
    }, {
      "title" : "Analyzing the dynamics of research by extracting key aspects of scientific papers",
      "author" : [ "S. Gupta", "C. Manning" ],
      "venue" : "Proceedings of 5th International Joint Conference on Natural Language Processing, 1–9. Chiang Mai, Thailand: Asian Federation of Natural Language Processing.",
      "citeRegEx" : "Gupta and Manning,? 2011",
      "shortCiteRegEx" : "Gupta and Manning",
      "year" : 2011
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom" ],
      "venue" : "Advances in Neural Information Processing Systems, 1684–1692.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Identifying sections in scientific abstracts using conditional random fields",
      "author" : [ "K. Hirohata", "N. Okazaki", "S. Ananiadou", "M. Ishizuka", "M.I. Biocentre" ],
      "venue" : null,
      "citeRegEx" : "Hirohata et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hirohata et al\\.",
      "year" : 2008
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba,? 2014",
      "shortCiteRegEx" : "Kingma and Ba",
      "year" : 2014
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J. Lafferty", "A. McCallum", "F.C. Pereira" ],
      "venue" : null,
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Automatic recognition of conceptualization zones in scientific articles and two life science applications",
      "author" : [ "M. Liakata", "S. Saha", "S. Dobnik", "C. Batchelor", "D. Rebholz-Schuhmann" ],
      "venue" : "Bioinformatics (Oxford, England) 28(7):991–1000.",
      "citeRegEx" : "Liakata et al\\.,? 2012",
      "shortCiteRegEx" : "Liakata et al\\.",
      "year" : 2012
    }, {
      "title" : "Zones of conceptualisation in scientific papers: a window to negative and speculative statements",
      "author" : [ "M. Liakata" ],
      "venue" : "Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, 1–4. Association for Computational Linguistics.",
      "citeRegEx" : "Liakata,? 2010",
      "shortCiteRegEx" : "Liakata",
      "year" : 2010
    }, {
      "title" : "Rhetorical structure theory: Toward a functional theory of text organization",
      "author" : [ "W.C. Mann", "S.A. Thompson" ],
      "venue" : "Text-Interdisciplinary Journal for the Study of Discourse 8(3):243–281.",
      "citeRegEx" : "Mann and Thompson,? 1988",
      "shortCiteRegEx" : "Mann and Thompson",
      "year" : 1988
    }, {
      "title" : "The theory and practice of discourse parsing and summarisation",
      "author" : [ "D. Marcu" ],
      "venue" : null,
      "citeRegEx" : "Marcu,? \\Q2000\\E",
      "shortCiteRegEx" : "Marcu",
      "year" : 2000
    }, {
      "title" : "An annotation scheme for a rhetorical analysis of biology articles",
      "author" : [ "Y. Mizuta", "N. Collier" ],
      "venue" : "LREC, 1737– 1740.",
      "citeRegEx" : "Mizuta and Collier,? 2004",
      "shortCiteRegEx" : "Mizuta and Collier",
      "year" : 2004
    }, {
      "title" : "Evaluating a meta-knowledge annotation scheme for bio-events",
      "author" : [ "R. Nawaz", "P. Thompson", "S. Ananiadou" ],
      "venue" : "Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, 69–77. Association for Computational Linguistics.",
      "citeRegEx" : "Nawaz et al\\.,? 2010",
      "shortCiteRegEx" : "Nawaz et al\\.",
      "year" : 2010
    }, {
      "title" : "The medical research paper: Structure and functions",
      "author" : [ "K.N. Nwogu" ],
      "venue" : "English for specific purposes 16(2):119– 138.",
      "citeRegEx" : "Nwogu,? 1997",
      "shortCiteRegEx" : "Nwogu",
      "year" : 1997
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "J. Pennington", "R. Socher", "C.D. Manning" ],
      "venue" : "EMNLP, volume 14, 1532–43.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributional semantics resources for biomedical text processing",
      "author" : [ "S. Pyysalo", "F. Ginter", "H. Moen", "T. Salakoski", "S. Ananiadou" ],
      "venue" : "Proceedings of Languages in Biology and Medicine.",
      "citeRegEx" : "Pyysalo et al\\.,? 2013",
      "shortCiteRegEx" : "Pyysalo et al\\.",
      "year" : 2013
    }, {
      "title" : "Parsing with compositional vector grammars",
      "author" : [ "R. Socher", "J. Bauer", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "In Proceedings of the ACL conference. Citeseer.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Approaches to studying cellular signaling: a primer for morphologists",
      "author" : [ "K.K.H. Svoboda", "W.R. Reenstra" ],
      "venue" : "The Anatomical record 269(2):123–139.",
      "citeRegEx" : "Svoboda and Reenstra,? 2002",
      "shortCiteRegEx" : "Svoboda and Reenstra",
      "year" : 2002
    }, {
      "title" : "Discourse-level argumentation in scientific articles: human and automatic annotation",
      "author" : [ "S. Teufel", "M. Moens" ],
      "venue" : "Towards Standards and Tools for Discourse Tagging, 84–93.",
      "citeRegEx" : "Teufel and Moens,? 1999",
      "shortCiteRegEx" : "Teufel and Moens",
      "year" : 1999
    }, {
      "title" : "Summarizing scientific articles: experiments with relevance and rhetorical status",
      "author" : [ "S. Teufel", "M. Moens" ],
      "venue" : "Computational linguistics 28(4):409–445.",
      "citeRegEx" : "Teufel and Moens,? 2002",
      "shortCiteRegEx" : "Teufel and Moens",
      "year" : 2002
    }, {
      "title" : "Argumentative Zoning: Information Extraction from Scientific Text",
      "author" : [ "S. Teufel" ],
      "venue" : "Ph.D. Dissertation, School of Cognitive Science, University of Edinburgh, Edinburg.",
      "citeRegEx" : "Teufel,? 2000",
      "shortCiteRegEx" : "Teufel",
      "year" : 2000
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "O. Vinyals", "Ł. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems, 2755–2763.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "A First Course in Systems Biology",
      "author" : [ "E. Voit" ],
      "venue" : "Garland Science, 1st edition.",
      "citeRegEx" : "Voit,? 2012",
      "shortCiteRegEx" : "Voit",
      "year" : 2012
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "There are well established rhetorical guidelines (Alley, 1996) for scientific writing that are used across disciplines and consequently, narratives describing evidence within a scientific investigation are expected to have a certain structure.",
      "startOffset" : 49,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "There exist several proposals for experiment discourse models (Liakata, 2010; Nawaz, Thompson, and Ananiadou, 2010; Mizuta and Collier, 2004; Nwogu, 1997).",
      "startOffset" : 62,
      "endOffset" : 154
    }, {
      "referenceID" : 17,
      "context" : "There exist several proposals for experiment discourse models (Liakata, 2010; Nawaz, Thompson, and Ananiadou, 2010; Mizuta and Collier, 2004; Nwogu, 1997).",
      "startOffset" : 62,
      "endOffset" : 154
    }, {
      "referenceID" : 19,
      "context" : "There exist several proposals for experiment discourse models (Liakata, 2010; Nawaz, Thompson, and Ananiadou, 2010; Mizuta and Collier, 2004; Nwogu, 1997).",
      "startOffset" : 62,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "There exist several proposals for experiment discourse models (Liakata, 2010; Nawaz, Thompson, and Ananiadou, 2010; Mizuta and Collier, 2004; Nwogu, 1997). We adopt the discourse type taxonomy for biological papers suggested by De Waard and Pander Maat (2012), and define our problem as identifying the discourse type of each clause in a given experiment description.",
      "startOffset" : 63,
      "endOffset" : 260
    }, {
      "referenceID" : 27,
      "context" : "Typically, researchers in this field use a number of small-scale experimental assays to investigate molecular events, see Voit (2012) and Svoboda and Reenstra (2002) for textbook and review introductions.",
      "startOffset" : 122,
      "endOffset" : 134
    }, {
      "referenceID" : 23,
      "context" : "Typically, researchers in this field use a number of small-scale experimental assays to investigate molecular events, see Voit (2012) and Svoboda and Reenstra (2002) for textbook and review introductions.",
      "startOffset" : 138,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "Teufel and Moens (2002) and Teufel and Moens (1999) describe argumentative zoning (AZ), a way of classifying scientific papers at a sentence level, into zones, thus extracting the structure from entire papers.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 22,
      "context" : "Teufel and Moens (2002) and Teufel and Moens (1999) describe argumentative zoning (AZ), a way of classifying scientific papers at a sentence level, into zones, thus extracting the structure from entire papers.",
      "startOffset" : 0,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "Hirohata et al. (2008) use a 4-way classification scheme for abstracts of scientific papers for identifying objectives, methods, results and conclusions.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 9,
      "context" : "Hirohata et al. (2008) use a 4-way classification scheme for abstracts of scientific papers for identifying objectives, methods, results and conclusions. Liakata (2010)",
      "startOffset" : 0,
      "endOffset" : 169
    }, {
      "referenceID" : 13,
      "context" : "51 for LibSVM classifiers (Liakata et al., 2012).",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 26,
      "context" : "In these studies, the focus of research is largely centered on modeling the discourse being used to construct a scientific argument, driving towards understanding “sentiment expressed towards cited work, ownership of ideas, and speech acts which express rhetorical statements typical for scientific argumentation” (Teufel, 2000).",
      "startOffset" : 314,
      "endOffset" : 328
    }, {
      "referenceID" : 6,
      "context" : "Guo et al. (2010) used SVM and Naı̈ve Bayes classifiers to compare the three schemes described above.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "Guo et al. (2010) used SVM and Naı̈ve Bayes classifiers to compare the three schemes described above. Gupta and Manning (2011) also studied the problem of extracting focus, techniques and the domain of research papers to identify the influence of research communities over each other.",
      "startOffset" : 0,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "Particularly in sequence labeling tasks in text, (Collobert et al., 2011) words are represented as vectors Type Definition",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "Attention has been used for complex tasks like question answering (Hermann et al., 2015) and machine translation (Bahdanau, Cho, and Bengio, 2014).",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 27,
      "context" : "In sequence-tosequence learning problems like machine translation (Bahdanau, Cho, and Bengio, 2014), parsing (Vinyals et al., 2015) and image caption generation (Xu et al.",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 29,
      "context" : ", 2015) and image caption generation (Xu et al., 2015), one network is used to encode the input modality, and a different network to decode into the output modality, with the decoder using attention to learn parts of the input to attend to for generating a given part of the output sequence.",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "It is generally accepted that relations between non-overlapping chunks of text need to be considered to account for the overall meaning (Marcu, 2000).",
      "startOffset" : 136,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : "While there are many discourse theories (see Marcu (2000), chapter 2 for an overview), Rhetorical Structure Theory (RST) by Mann and Thompson (1988), received a lot of attention.",
      "startOffset" : 45,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "While there are many discourse theories (see Marcu (2000), chapter 2 for an overview), Rhetorical Structure Theory (RST) by Mann and Thompson (1988), received a lot of attention.",
      "startOffset" : 124,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : "Finally, Dsumm is fed to the Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) Hochreiter and Schmidhuber (1997) cells, to label the clauses.",
      "startOffset" : 95,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "LSTMs were implemented 2 using Keras (Chollet, 2015) and attention using Theano (Bergstra et al.",
      "startOffset" : 37,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "LSTMs were implemented 2 using Keras (Chollet, 2015) and attention using Theano (Bergstra et al., 2010).",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "We used the 200 dimension vectors trained on Pubmed Central data by Pyysalo et al. (2013) as input representations and projected them down to 50d to keep the parameter space of the entire model under control.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "We used ADAM (Kingma and Ba, 2014) as the optimization algorithm.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "Using a multithreaded preprocessing pipeline, we extracted the Results sections of each of those papers, and parsed all the sentences using the Stanford Parser (Socher et al., 2013).",
      "startOffset" : 160,
      "endOffset" : 181
    }, {
      "referenceID" : 22,
      "context" : "Using a multithreaded preprocessing pipeline, we extracted the Results sections of each of those papers, and parsed all the sentences using the Stanford Parser (Socher et al., 2013). This process separated the main and subordinate clauses of each sentence that we process as a sequence over separate paragraphs. We asked domain experts to label each of those clauses using the seven label taxonomy suggested by De Waard and Pander Maat (2012). We also added a None label for those clauses that do not fall under any category.",
      "startOffset" : 161,
      "endOffset" : 443
    } ],
    "year" : 2017,
    "abstractText" : "We propose a deep learning model for identifying structure within experiment narratives in scientific literature. We take a sequence labeling approach to this problem, and label clauses within experiment narratives to identify the different parts of the experiment. Our dataset consists of paragraphs taken from open access PubMed papers labeled with rhetorical information as a result of our pilot annotation. Our model is a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells that labels clauses. The clause representations are computed by combining word representations using a novel attention mechanism that involves a separate RNN. We compare this model against LSTMs where the input layer has simple or no attention and a feature rich CRF model. Furthermore, we describe how our work could be useful for information extraction from scientific literature.",
    "creator" : "LaTeX with hyperref package"
  }
}