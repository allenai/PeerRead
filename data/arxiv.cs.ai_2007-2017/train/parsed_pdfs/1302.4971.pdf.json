{
  "name" : "1302.4971.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Complexity of Solving Markov Decision Problems",
    "authors" : [ "Michael L. Littman", "Thomas L. Dean", "Leslie Pack Kaelbling" ],
    "emails" : [ "}@cs." ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nA Markov decision process is a controlled stochastic process satisfying the Markov property with costs as signed to state transitions. A Markov decision prob lem is a Markov decision process together with a per formance criterion. A solution to a Markov decision problem is a policy, mapping states to actions, that (perhaps stochastically) determines state transitions to minimize the cost according to the performance criterion. Markov decision problems (MDPs) pro vide the theoretical foundations for decision-theoretic planning, reinforcement learning, and other sequential decision-making tasks of interest to researchers and practitioners in artificial intelligence and operations re search (Dean et a!., 1995b). MDPs employ dynamical models based on well-understood stochastic processes and performance criteria based on established theory in operations research, economics, combinatorial opti mization, and the social sciences (Puterman, 1994) . It would seem that MDPs exhibit special structure that might be exploited to expedite their solution. In investment planning, for example, often the ini tial state is known with certainty (the current price for a stock or commodity) and as a result the set of likely reachable states (future prices) and viable invest-\nment strategies in the near-term future is considerably restricted. In general, notions of time, action, and reachability in state space are inherent characteristics of MDPs that might be exploited to produce efficient algorithms for solving them. It is important that we understand the computational issues involved in these sources of structure to get some idea of the prospects for efficient sequential and parallel algorithms for com puting both exact and approximate solutions.\nThis paper summarizes some of what is known (and unknown but worth knowing) about the computa tional complexity of solving MDPs. Briefly, any MDP can be represented as a linear program (LP) and solved in polynomial time. However, the order of the poly nomials is large enough that the theoretically efficient algorithms are not efficient in practice. Of the algo rithms specific to solving MDPs, none is known to run in worst-case polynomial time. However, algo rithms and analyses to date have made little use of MDP-specific structure, and results in related areas of Monte Carlo estimation and Markov chain theory sug gest promising avenues for future research. We begin by describing the basic class of problems.\n2 MARKOV DECISION PROBLEMS\nFor our purposes, a Markov decision process is a four tuple (S1s, S1A, p, c) , where S1s is the state space, S1A is the action space, p is the state-transition probability distribution function, and c is the instantaneous-cost function.\nThe state-transition function is defined as follows: for all i, j E S1s, k E S1A,\nP7j = Pr(St = jiSt-1 = i, At = k) where St (At) is a random variable denoting the state (action) at timet. The cost c7 is defined to be the cost of taking action k from state i.\nLet N = IS1sl and M = IS1AI· For some of the com plexity results, it will be necessary to assume that p and c are encoded using N X N x M tables of ratio nal numbers. We let B be the maximum number of bits required to represent any component of p or c. In\nOn the Complexity of Solving Markov Decision Problems 395\nthis paper, we restrict our attention to discrete-time processes in which both fls and f!A are finite. A Markov decision process describes the dynamics of an agent interacting with a stochastic environment. Given an initial state or distribution over states and a sequence of actions, the Markov decision process de scribes the subsequent evolution of the system state over a (possibly infinite) sequence of times referred to as the stages of the process. This paper focuses on the infinite-horizon case, in which the sequence of stages in infinite.\nA policy 1r is a mapping from states to actions. If the policy is independent of the current stage, it is said to be stationary.\nA Markov decision problem (MDP) is a Markov de cision process together with a performance criterion. The performance criterion enables us to assign a total cost to each state for a given policy. A policy and a Markov decision process, together with an initial state, determine a probability distribution over sequences of state/action pairs called trajectories. The performance criterion assigns to each such trajectory a cost (deter mined in part by the instantaneous-cost function) and the probability-weighted sum of these costs determine the policy's total cost for that state.\nA policy 1r1 is said to dominate policy 1r2 if, for every state i E fls the total cost of performing 1r1 starting in state i is less than or equal to the total cost of perform ing 1r2 starting in state i, and if there is at least one state j E fls from which the total cost of performing 1r1 is strictly less than that of 1r2• A fundamental result in the theory of MDPs is that there exists a stationary policy that dominates or has equal total cost to every other policy (Bellman, 19.57). Such a policy is termed an optimal policy and the total cost it attaches to each state is said to be the optimal total cost for that state. An <-optimal solution to a Markov decision problem is a policy whose total cost, for every state, is within E of the optimal total cost. For the problems we are interested in, the optimal total-cost function (mapping from states to their optimal total costs) is unique but the optimal policy need not be.\nWe briefly consider three popular performance crite ria: expected cost to target, expected discounted cumu lative cost, and average expected cost per stage. In the expected cost-to-target criterion, a subset of f!s is des ignated as a target and the cost assigned to a trajec tory is the sum of the instantaneous costs until some state in the target set is reached. In the expected dis counted cumulative cost criterion, the cost of a trajec tory is the sum over all t of It times the instantaneous cost at time t, where 0 < 1 < 1 is the discount rate and t indicates the stage.1 Under reasonable assump tions (Derman, 19 70), the expected cost to target and expected discounted cumulative cost criteria give rise\n1 When r is considered part of the input, it is assumed to be encodable in B bits.\nto equivalent computational problems. The average expected cost per stage criterion is attractive because it does not require the introduction of a seemingly ar bitrary discount rate, nor the specification of a set of target states. However, it is often a difficult criterion to analyze and work with.\nThis paper focuses on the expected discounted cumu lative cost criterion. To simplify the notation, suppose that the instantaneous costs are dependent only upon the initial state and action so that, for each i E fls and k E nA, c7 = C;j(k) for all j E fls. The expected discounted cumulative cost with respect to a state i for a particular policy 1r and fixed discount 1 is defined by the following system of equations: for all i E fls,\nErr(�,li) = c; (i) +I L P7t)Err(�,lj). (1) jEO.s\nThe optimal total-cost function E* (�,I·) is defined as\nE*(�,li) = minErr(�,li), i E fls, 7r which can be shown to satisfy the following optimality equations: for all i E fls,\nE*(�,li) = min [c7 + 1 L P7jE*(�,Ij)] . (2) kEO.A\njEO.s\nThis family of equations, due to Bellman {1957), is the basis for several practical algorithms for solving MDPs. There is a policy, 7r*, called the optimal pol icy , which achieves the optimal total-cost function. It can be found from the optimal total-cost function as follows: for all i E fls,\n1r*(i) = arg min [c7 + 1 \"P7jE*(�, Ij)l (3) kEO.A �\njEO.s\n3 GENERAL COMPLEXITY\nRESULTS\nThere is no known algorithm that can solve general MDPs in a number of arithmetic operations polyno mial in N and M . Such an algorithm would be called strongly polynomial. Using linear programming, how ever, the problem can be solved in a number of arith metic operations polynomial in N , M , and B.\nPapadimitriou and Tsitsiklis (1987) analyzed the com putational complexity of MDPs. They showed that, under any of the three cost criteria mentioned earlier, the problem is P-complete. This means that, although it is solvable in polynomial time, if an efficient parallel algorithm were available, then all problems in P would be solvable efficiently in parallel (an outcome consid ered unlikely by researchers in the field). Since the linear programming problem is also P-complete, this result implies that in terms of parallelizability, MDPs\n396 Littman, Dean, and Kaelbling\nand LPs are equivalent: a fast parallel algorithm for solving one would yield a fast parallel algorithm for solving the other. It is not known whether the two problems are equivalent with respect to strong polyno miality: although it is clear that a strongly polynomial algorithm for solving linear programs would yield one for MDPs, the inverse is still open.\nPapadimitriou and Tsitsiklis also show that for MDPs with deterministic transition functions (the compo nents of pare all O's and 1's), optimal total-cost func tions can be found efficiently in parallel for all three cost criteria (i. e . , the problem is in NC). Further, the algorithms they give are strongly polynomial. This suggests that the stochastic nature of some MDPs has important consequences for complexity and that not all MDPs are equally difficult to solve.\n4 ALGORITHMS AND ANALYSIS\nThis section describes the basic algorithms used to solve MDPs and analyzes their running times.\n4.1 LINEAR PROGRAMMING\nThe problem of computing an optimal total-cost func tion for an infinite-horizon discounted MDP can be for mulated as a linear program (LP) (D'Epenoux, 1963). Linear programming is a very general technique and does not appear to take advantage of the special struc ture of MDPs. Nonetheless, this reduction is currently the only proof that MDPs are solvable in polynomial time.\nThe primal linear program involves maximizing the sum\nL Vj jEOs\nsubject to the constraints\nv; S c7 +I L P7jvj, (4) jEO.s\nfor all i E ns, k E nA, where Vj for i E ns are the variables that we are solving for and which, for an optimal solution to the linear program, determine the optimal total-cost function for the original MDP. The intuition here is that, for each state i, the optimal total cost from i is no greater than what would be achieved by first taking action k, for each k E nA. The maximization insists that we choose the greatest lower bound for each of the v; variables.\nIt is also of interest to consider the dual of the above program which involves minimizing the sum\nL L x7c7 iEO.s kEOA\nsubject to the constraints\nL xj = 1 +I L L P7jx7, (5) kEOA iEOs kEOA\nfor all j E ns. The xj variables can be thought of as indicating the amount of \"policy flow\" through state j that exits via action k. Under this interpretation, the constraints are flow conservation constraints that say that the total flow exiting state j is equal to the flow beginning at state j (always 1) plus the flow enter ing state j via all possible combinations of states and actions weighted by their probability. The objective, then, is to minimize the cost of the flow.\nIf { xn is a feasible solution to the dual, then LiEn LkEn x7c7 can be interpreted as the total cost ;f the stationary stochastic policy that chooses action k in state i with probability\nx7/ L x7. kEOA\nThis solution can be converted into a deterministic optimal policy as follows:\n1r* ( i) = arg max x7. kEOA\nThe primal LP as N M constraints and N variables and the dual N constraints and N M variables. In both LPs, the coefficients have a number of bits polynomial in B. There are algorithms for solving rational LPs that take time polynomial in the number of variables and constraints as well as the number of bits used to represent the coefficients (Karmarkar, 1984; Khachian, 1979). Thus, MDPs can be solved in time polynomial in N , M, and B. A drawback of the existing polyno mial time algorithms is that they run extremely slowly in practice and so are rarely used.\nThe most popular (and practical) methods for solving linear programs are variations of Dantzig's (1963) sim plex method. The simplex method works by choosing subsets of the constraints to satisfy with equality and solving the resulting linear equations for the values of the variables. The algorithm proceeds by iteratively swapping constraints in and out of the selected sub set, continually improving the value of the objective function. When no swaps can be made to improve the objective function, the optimal solution has been found. Simplex methods differ as to their choice of pivot rule, the rule for choosing which constraints to swap in and out at each iteration.\nAlthough simplex methods seem to perform well in practice, Klee and Minty (1972) showed that one of Dantzig's choices of pivoting rule could lead the sim plex algorithm to take an exponential number of it erations on some problems. Since then, other piv oting rules have been suggested and almost all have been shown to result in exponential running times in the worst case. None has been shown to result in a polynomial-time implementation of simplex. Note that these results may not apply directly to the use of linear programming to solve MDPs since the set of lin ear programs resulting from MDPs might not include the counterexample linear programs. This is an open ISSUe.\nOn the Complexity of Solving Markov Decision Problems 397\nThere are two ways to consider speeding up the so lutions of MDPs: finding improved methods for solv ing LPs or using solution methods that are specific to MDPs. While progress has been made on speeding up linear programming algorithms (such as a subexponen tial simplex algorithm which uses a randomized pivot ing rule (Bland, 1977; Kalai, 1992)), MDP-specific al gorithms hold more promise for efficient solution. We address such algorithms, specifically policy iteration and value iteration, in the following sections.\n4.2 POLICY ITERATION\nThe most widely used algorithms for solving MDPs are iterative methods. One of the best known of these algorithms is due to Howard (1960) and is known as policy iteration. Policy iteration alternates between a value determination phase, in which the current policy is evaluated, and a policy improvement phase, in which an attempt is made to improve the current policy.\nPolicy improvement can be performed in O(M N2) arithmetic operations (steps), and value determina tion in O�N3) steps by solving a system of linear equations. The total running time, therefore, is poly nomial if and only if the number of iterations required to find an optimal or f-optimal policy is polynomial. This question is addressed later in the section.\nThe basic policy iteration algorithm works as follows:\n1. Let 1r1 be a deterministic stationary policy. 2. Loop\n(a) Set 1r to be 7r1• (b) Determine, for all i E ns , E1r p::;, li) by solv\ning the set of N equations in N unknowns described by Equation 1.\n(c) For each i Ens, if there exists some k E nA such that\n[cf + 1 .L P 7jE11\"(�11J)l < E71\"(�1li), JEOs\nthen set 1r1 ( i) to be k, otherwise set 1r1 ( i) to be 1r(i).\n(d) Repeat loop if 1r '1- 1r1 3. Return 1r.\nStep 2b is the value determination phase and Step 2c is the policy improvement phase.\nSince there are only M N distinct policies, and each new policy dominates the previous one (Puterman, 1994), it is obvious that policy iteration terminates in at most an exponential number of steps. We are interested in finding a polynomial upper bound or in\n2ln theory, value determination can probably be done somewhat faster, since it primarily requires inverting a N X N matrix, which can be done in 0( N2·376) steps (Cop persmith and Winograd, 1987).\nFigure 1: Simple policy iteration requires an expo nential number of iterations to generate an optimal solution to the family of MDPs illustrated here (af ter (Melekopoglou and Condon, 1990)) .\nshowing that no such upper bound exists ( i .e . , that the number of iterations can be exponential in the worst case).\nWhile direct analyses of policy iteration have been scarce, several researchers have examined a sequential improvement variant of policy iteration, in which the current policy is improved for at most one state in Step 2c. A detailed analogy can be constructed be tween the choice of state to update in sequential improvement policy iteration and the choice of pivot rule in simplex. Denardo ( 1982) shows that the feasi ble bases for the primal LP (Equation 4) are in one-to one correspondence with the stationary deterministic policies.\nAs with simplex, examples have been constructed to make sequential-improvement policy iteration re quire exponentially many iterations. Melekopoglou and Condon (1990) examine the problem of solving expected cost-to-target MDPs using several variations on the sequential improvement policy iteration algo rithm. In a version they call simple policy iteration, every state is labeled with a unique index and, at each iteration, the policy is updated at the state with the smallest index of those at which the policy can be im proved. They show that the family of counterexamples suggested by Figure 1, from a particular starting pol icy, takes an exponential number of iterations to solve using simple policy iteration.\nA counterexample can be constructed for each even number, N (N = 10 in the figure). The states are divided into three classes: decision states (labeled 0 through N/2- 1), random states (labeled 1' through (N/2- 1)'), and an absorbing state. From each de cision state i, there are two actions available: action 0 (heavy solid lines) results in a transition to decision state i + 1 and action 1 (dashed lines) results in a tran sition to random state (i + 1)'. From random state i', there is no choice of action and instead a random tran sition with probability 1/2 of reaching random state ( i + 1 ) ' and probability 1/2 of reaching decision state i + 1 takes place. Actions from decision state N /2 - 1 and random state N /2 - 1 both result in a transition to the absorbing state. This transition has a cost of + 1 in the case of decision state N /2 - 1 and all other transitions have zero cost.\nThe initial policy is 7ro ( i) = 0, so every decision state i\n398 Littman, Dean, and Kaelbling\nselects the action which takes it to decision state i + 1. In the optimal policy, 1r* ( i) = 0 for i # N /2 - 2 and 1r*(Nj2 - 2) = 1. Although these two policies are highly similar, Melekopoglou and Condon show that simple policy iteration steps through 2Nfz-z policies before arriving at the optimal policy. We remark that although this example was constructed with the ex pected cost-to-target criterion in mind, it also holds for the discounted cumulative cost criterion regardless of discount rate.\nWhen the policy is improved at all states in parallel, policy iteration no longer has a direct simplex ana logue. It is an open question whether this can lead to exponential running time in the worst case or whether the resulting algorithm is guaranteed to converge in polynomial time. However, we can show that this more popular version of policy iteration is strictly more effi cient than the simple policy iteration algorithm men tioned above.\nLet 1r n be the policy found after n iterations of policy iteration. Let E,.., (:E1 Ji) be the total-cost function associated with 71\" n. Let En (:E1 li) be the total-cost function found by value iteration (Section 4.3) starting with E,.0 (:E1 Ji) as an initial total-cost function. Puter man (1994) (Theorem 6.4.6) shows that E,.., (:E1Ii) al ways dominates or is equal to En (E, Ji) and therefore that policy iteration converges no more slowly than value iteration for discounted infinite-horizon MDPs. When combined with a result by Tseng (1990) (de scribed in more detail in the next section) which bounds the time needed for value iteration to find an optimal policy, this shows that policy iteration takes polynomial time, for a fixed discount rate. Further more, if the discount rate is included as part of the input as a rational number with the denominator writ ten in unary, policy iteration takes polynomial time. This makes policy iteration a pseudo-polynomi al-ti me algorithm.\nThus, whereas policy iteration runs in polynomial time for a fixed discount rate, simple policy iteration can take exponential time, regardless of discount rate. This novel observation stands in contrast to a com ment by Denardo (1982). He argues that block piv oting in simplex achieves the same goal as parallel policy improvement in policy iteration and therefore that one should prefer commercial implementations of simplex to home-grown implementations of policy it eration. His argument is based on the misconception that one step of policy improvement on N states is equivalent in power to N iterations of simple policy iteration. In fact, one policy improvement step on N states is more like 2N iterations of simple policy itera tion, in the worst case. Thus, policy iteration has not yet been ruled out as the preferred solution method for MDPs. More empirical study is needed.\n4.3 VALUE ITERATION\nBellman (1957) devised a successive approximation al gorithm for MDPs called value i terati on which works by computing the optimal total-cost function assum ing first a one-stage finite horizon, then a two-stage finite horizon, and so on. The total-cost functions so computed are guaranteed to converge in the limit to the optimal total-cost function. In addition, the policy associated with the successive total-cost functions will converge to the optimal policy in a finite number of iterations (Bertsekas, 1987), and in practice this con vergence can be quite rapid.\nThe basic value-iteration algorithm is described as fol lows:\n1. For each i E Os, initialize E0(:E1Ji). 2. Set n to be 1. 3. While n < maximum number of iterations,\n(a) For each i E Os and k E OA, let\nEn(E1Ji,k) = [c7 +1 2: p�En-1(E,Jj)l jEOs\n(b) Set n to n + 1. 4. For each i E Os,\n5. Return 71\".\n7r(i) = arg min En(E,Ii,k). kEOA\nThe maximum number of iterations is either set in advance or determined automatically using an appro priate stoppi ng rule. The Bellman resi dual at step n is defined to be\nmaxJEn(:E,Ji)- En-l(:E,Ii)J. tEOs\nBy examining the Bellman residual during value itera tion and stopping when it gets below some threshold, f1 = E(1-1)/(21), we can guarantee that the resulting policy will be t:-optimal (Williams and Baird, 1993). The running time for each iteration is O(M N2), thus, once again, the method is polynomial if and only if the total number of iterations required is polynomial. We sketch an analysis of the number of iterations required for convergence to an optimal policy below; more de tailed discussion can be found in Tseng's paper.\n1. Bound the distance from the initial total-cost function to the optimal total-cost function. Let M = maJCiens,kEnA Jc7J, the magnitude of the largest instantaneous cost. The total-cost func tion for any policy will have components in the range (-M/(1 -1), M/(1 -1)]. Thus any choice of initial total-cost function with components in this range cannot differ from the optimal total cost function by more than 2 M/ ( 1 -1) at any state.\nOn the Complexity of Solving Markov Decision Problems 399\n2. Show that each iteration results in an improve ment of a factor of at least 1 in the distance be tween the estimated and optimal total-cost func tions. This is the standard \"contraction mapping\" result for discounted MDPs (Puterman, 1994).\n3. Give an expression for the distance between es timated and optimal total-cost functions after n iterations. Show how this gives a bound on the number of iterations required for an f-optimal pol icy. After n iterations the estimated total-cost func tion can differ from the optimal total-cost function by no more than 2M In 1(1- {) at any state. Solv ing for nand using the result relating the Bellman residual to the total cost of the associated policy, we can express the maximum number of iterations needed to find an f-optimal policy as\nn* < B + log(1l«') + log(11(1- 1)) + 1\n- 1- \"Y . (6)\n4. Argue that there is a value for f > 0 for which an f-optimal policy is, in fact, optimal. The optimal total-cost function can be expressed as the solution of a linear program with rational components of no more than B bits each (Sec tion 4.1). A standard result in the theory of linear programming is that the solution to such a linear program can be written as rational numbers where each component is represented using a number of bits polynomial in the size of the system and B, B* (Schrijver, 1986). This means that if we can find a policy that is\nf = 1128.+1-optimal, the policy must be optimal. 5. Substitute this value off into the bound to get a\nbound on the number of iterations needed for an exact answer. Substituting for f in Equation 6 reveals that run ning value iteration for a number of iterations polynomial in N, M, B, and 1 I ( 1 - 1) guaran tees an optimal policy.\nThis analysis shows that, for fixed {, value iteration takes polynomial time. It is also useful for constructing an upper bound for policy iteration (see Section 4.2). Although it is not known whether the dependence on 11(1- \"Y) (which can be quite large as 1 approaches 1) can be dropped for policy iteration, we can show that value iteration can indeed take that long.\nFigure 2 illustrates a family of MDPs for which discov ering the optimal policy via value iteration takes time proportional to 11(1 - 1) log(11(1 - 1)). It consists of 3 states, labeled 0 through 2. From state 0, ac tion 1 causes a deterministic transition to state 1 and action 2 causes a deterministic transition to state 2. Action 1 has no instantaneous cost but once in state 1, there is a cost of+ 1 for every time step thereafter. Action 2 has an instantaneous cost of 12 1(1- 1) but\n+1\n... 0 +0 ,. \" 2\n�� ,. ... �\n_]_\n� 1-y\nFigure 2: Value iteration requires number of iterations proportional to 1 I ( 1 -1) log( 1 I ( 1 - 1)) to generate an optimal solution for this family of MDPs.\nstate 2 is a zero-cost absorbing state.3 The discounted infinite-horizon cost of choosing action 1 from state 0 is 1 I ( 1 - 1) whereas the total cost for action 2 is 12 1(1- 1) (smaller, since 1 < 1). If we initialize value iteration to the zero total-cost function, the estimate of the costs of these two choices are: {(1-\"Yn)I(1-\"Y) and 12 1(1- 1) at iteration n > 1. Therefore, value it eration will continue to choose the suboptimal action until iteration n* where: * log(1-\"Y) 11 ( 1 ) 1 n > >- og -- . - log1 - 2 1- 1 (1- 1) Thus, in the worst case, value iteration has a running time that grows faster than 1 I ( 1 - 1).\n5 ALTERNATIVE METHODS OF\nANALYSIS\nWe know that MDPs can be solved in time polynomial in N, M and B. Unfortunately, the degree of the poly nomial is nontrivial and the methods that are guar anteed to achieve such polynomial-time performance do not make any significant use of the structure of MDPs. Furthermore, as with the multi-commodity flow problem (Leighton et a!., 1991), the existence of a linear programming solution does not preclude the need for more efficient algorithms, even if it means finding only approximately optimal solutions. This section sketches some directions that could be pursued to find improved algorithms for MDPs.\nAn in-depth empirical study of existing MDP algo rithms might be fruitful. In addition to the solution methods discussed earlier, there are numerous elabora tions and hybrids that have been proposed to improve the convergence speed or running time. Puterman and Shin (1978) describe a general method called modified policy iteration that includes policy iteration and value iteration as special cases. The structure of modified policy iteration is essentially that of policy iteration where the value determination step is replaced with\n3Note that these costs can be specified by B ::::: log(l /(1- -y)) = O(log(1/(1- -y))) bits.\n400 Littman, Dean, and Kaelbling\nan approximation that closely resembles value itera tion with a fixed policy. Bertsekas (1987) describes variations on value and policy iteration, called asyn chronous dynamic programming algorithms, that in terleave improving policies and estimating the value of policies. These methods resemble techniques used in the reinforcement-learning field where MDPs are solved by performing cost update computations along high probability trajectories. A promising approach from this literature involves a heuristic for dynami cally choosing which states to update in value itera tion according to how likely such an update would be to improve the estimated total-cost function (Moore and Atkeson, 1993; Peng and Williams, 1993). Before embarking on such a study, we need to compile a suite of benchmark MDPs that reflects interesting classes of problems.\nFast f-approximation algorithms could be very useful in trading off solution accuracy for time. For exam ple, approximation algorithms have been designed for solving linear programs. One is designed for finding f optimal solutions to a certain class of linear programs which includes the primal linear program given in Sec tion 4.1 (Plotkin et al., 1991). Although this partic ular scheme is unlikely to yield practical implementa tions (it is most useful for solving linear programs with exponentially many constraints) the application of ap proximate linear-programming approaches to MDPs is worth more study.\nProbabilistic approximations might also be desirable in some applications, say if we could find an t optimal solution with probability 1 - J, in some low order polynomial in N, M, 1/f, 1/J, and 1/(1 - 1). Fully-polynomial randomized approximation schemes (FPRAS) such as this are generally designed for prob lems that cannot be computed exactly in polynomial time (e.g. , (Dagum and Luby, 1993)), but researchers are now developing iterative algorithms with tight probabilistic performance bounds that provide reliable estimates (e.g ., the Dagum et al. (1995) optimal stop ping rule for Monte Carlo estimation).\nWork on FPRAS has identified properties of graphs and Markov chains (e.g ., the rapid mixing property for Markov chains used by Jerrum and Sinclair (1988) in approximating the permanent) that may allow us to classify MDPs into easy and hard problems. A related observation is made by Bertsekas (1987) in the context of an algorithm that combines value iteration with a rule for maintaining error bounds. He notes that the convergence of this algorithm is controlled by the dis count rate in conjunction with the magnitude of the subdominant eigenvalue of the Markov chain induced by the optimal policy (if it is unique). This value could be used to help characterize hard and easy MDPs.\nSome work has already been done to characterize MDPs with respect to their computational proper ties, including experimental comparisons that illus trate that there are plenty of easy problems mixed in\nwith extraordinarily hard ones (Dean et al., 1995a), and categorization schemes that attempt to relate measurable attributes of MDPs such as the amount of uncertainty in actions to the type of solution method that works best (Kirman, 1994).\nOne thing not considered by any of the algorithms mentioned above is that, in practice, the initial state is often known. Thus it may be possible to find near optimal solutions without considering the entire state space (e.g ., consider the case in which 1 is relatively small and it takes many stages to reach more than log (N) states from the initial state). Dean, Kaelbling, Kirman, and Nicholson (1993) solve MDPs using an algorithm that exploits this property but provide no error bounds on its performance. Barto, Bradtke, and Singh's RTDP (real-time dynamic programming) al gorithm (1995) exploits a similar intuition to find an optimal policy without necessarily considering the en tire state space.\nStructure in the underlying dynamics should allow us to aggregate states and decompose problems into weakly-coupled subproblems, thereby simplifying com putation. Aggregation has long been an active topic of research in operations research and optimal con trol (Schweitzer, 1984). In particular, Bertsekas and Castanon (1989) describe adaptive aggregation tech niques that might be very important for large, struc tured state spaces, and Kushner and Chen (1974) de scribe how to use Dantzig-Wolfe LP decomposition techniques (1960) for solving large MDPs. More re cently, researchers in planning (Boutilier et al., 1995b; Dean and Lin, 1995) and reinforcement learning (Kael bling, 1993; Moore and Atkeson, 1995) have been exploring aggregation and decomposition techniques for solving large MDPs. What is needed is a clear mathematical characterization of the classes of MDPs for which these techniques guarantee good approxima tions in low-order polynomial time.\nFinally, our preoccupation with computational com plexity is not unjustified. Although, in theory, MDPs can be solved in polynomial time in the size of the state space, action space, and bits of precision, this only holds true for so-called flat representations of the system dynamics in which the states are explicitly enu merated. Boutilier et al. (1995), consider the advan tages of structured state spaces in which the represen tation of the dynamics is some log factor of the size of the state space. An efficient algorithm for these MDPs would therefore need to run in time bounded by a poly nomial in the log arithm of the number of the number of states-a considerably more challenging endeavor.\n6 SUMMARY AND CONCLUSIONS\nIn this paper, we focus primarily on the class of MDPs with an expected-discounted-cumulative-cost perfor mance criterion and discount rate I· These MDPs can be solved using linear programming in a number\nOn the Complexity of Solving Markov Decision Problems 401\nof arithmetic operations polynomial in N (the num ber of states), M (the number of actions) , and B (the maximum number of bits required to encode instanta neous costs and state-transition probabilities as ratio nal numbers) . There is no known strongly-polynomial algorithm for solving MDPs. The general problem is P-complete and hence equivalent to the problem of solving linear programs with respect to the prospects for exploiting parallelism.\nThe best known practical algorithms for solving MDPs appear to be dependent on the discount rate I· Both value iteration and policy iteration can be shown to perform in polynomial time for fixed 1, but value it eration can take a number of iterations proportional to 1/(1- 1) log (1/(1- 1)) in the worst case. In ad dition, a version of policy iteration in which policies are improved one state at a time can be shown to re quire an exponential number of iterations, regardless of 1, giving some indication that the standard algorithm for policy iteration is strictly more powerful than this variant. We note that neither value iteration nor pol icy iteration makes significant use of the structure of the underlying dynamical model.\nThe fact that the linear programming formulation of MDPs can be solved in polynomial time is not par ticularly comforting. Existing algorithms for solving LPs with provable polynomial-time performance are impractical for most MDPs. Practical algorithms for solving LPs based on the simplex method appear prone to the same sort of worst-case behavior as policy iter ation and value iteration.\nWe suggest two avenues of attack on MDPs: first, we relax our requirements for performance, and, second, we focus on narrower classes of MDPs that have ex ploitable structure. The goal is to address problems that are representative of the types of applications and performance expectations found in practice in order to produce theoretical results that are of interest to prac titioners.\nIn conclusion, we find the current complexity results of marginal use to practitioners. We call on theoreticians and practitioners to generate a set of alternative ques tions whose answers will inform practice and challenge current theory.\nAcknowledgments\nThanks to Justin Boyan, Tony Cassandra, Anne Con don, Paul Dagum, Michael Jordan, Philip Klein, Hsueh-1 Lu, Walter Ludwig, Satinder Singh, John Tsitsiklis, and Marty Puterman for pointers and help ful discussion.\nReferences\nBarto, A. G., Bradtke, S. J., and Singh, S. P. (1995) . Learning to act using real-time dynamic programming. Artificial Intelligence, 72 ( 1 ) :81-138.\nBellman, R. (1957) . Dynamic Programming. Princeton University Press. Bertsekas, D. P. (1987) . Dynamic Programming: De terministic and Stochastic Models. Prentice-Hall, En glewood Cliffs, N.J. Bertsekas, D. P. and Castanon, D. A. (1989). Adaptive aggregation for infinite horizon dynamic programming. IEEE Transactions on Automatic Control, 34 (6) :589- 598. Bland, R. G. (1977) . New finite pivoting rules for the simplex method. Mathematics of Operations Research, 2:103-107. Boutilier, C., Dean, T., and Hanks, S. (1995a) . Plan ning under uncertainty: Structural assumptions and computational leverage. Submitted to the Second Eu ropean Workshop on Planning. Boutilier, C., Dearden, R., and Goldszmidt, M. (1995b) . Exploiting structure in policy construction. In Proceedings of the 1995 International Joint Confer ence on Artificial Intelligence.\nCoppersmith, D. and Winograd, S. (1987) . Matrix multiplication via arithmetic progressions. In Proceed ings of 19th Annual ACM Symposium on Theory of Computing, pages 1-6. Dagum, P., Karp, R., Luby, M., and Ross, S. M. ( 1995) . An optimal stopping rule for Monte Carlo es timation. Submitted to FOCS-95. Dagum, P. and Luby, M. (1993) . Approximating prob abilistic inference in bayesian belief networks is NP hard. Artificial Intelligence, 60:141-153. Dantzig, G. (1963) . Linear Programming and Exten sions. Princeton University Press. Dantzig, G. and Wolfe, P. (1960). Decomposition principle for dynamic programs. Operations Research, 8 (1) :101-111. Dean, T., Kaelbling, L., Kirman, J., and Nicholson, A. (1993) . Planning with deadlines in stochastic domains. In Proceedings AAAI-93, pages 574-579. AAAI. Dean, T., Kaelbling, L., Kirman, J., and Nicholson, A. (1995a) . Planning under time constraints in stochastic domains. To appear in Artificial Intelligence. Dean, T., Kanazawa, K., Koller, D., and Russell, S. (1995b) . Decision theoretic planning: Part I. Submit ted to the Journal for Artificial Intelligence Research. Dean, T. and Lin, S.-H. (1995) . Decomposition tech niques for planning in stochastic domains. In Proceed ings of the 1995 International Joint Conference on A r tificial Intelligence.\nDenardo, E. (1982) . Dynamic Programming: Models and Applications. Prentice-Hall, Inc. D'Epenoux, F. (1963) . A probabilistic production and inventory problem. Management Science, 10:98-108. Derman, C. (1970) . Finite State Markovian Decision Processes. Cambridge University Press, New York.\n402 Littman, Dean, and Kaelbling\nHoward, R. A. ( 1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge, Mas sachusetts. Jerrum, M. and Sinclair, A. (1988) . Conductance and the rapid mixing property for Markov chains: the ap proximation of the permanent resolved (preliminary version) . In ACM Symposium on Theoretical Comput ing, pages 235-244. Kaelbling, L. P. ( 1993) . Hierarchical learning in stochastic domains: A preliminary report. In Pro ceedings Tenth International Conference on Machine Learning, page 1993. Kalai, G. (1992). A subexponential randomized sim plex algorithm. In Proceedings of 24th Annual ACM Symposium on the Theory of Computing, pages 475- 482. Karmarkar, N. (1984) . A new polynomial time al gorithm for linear programming. Combinatorica, 4(4):373-395. Khachian, L. G. (1979). A polynomial algorithm for linear programming. Soviet Math Dokl. , 20:191-194. Kirman, J. ( 1994). Predicting Real-Time Planner Per formance by Domain Characterization. PhD thesis, Department of Computer Science, Brown University. Klee, V. and Minty, G. J. (1972). How good is the simplex algorithm? In Shisha, 0., editor, Inequalities, III, pages 159-175. Academic Press, New York. Kushner, H. J. and Chen, C.-H. (1974). Decomposition of systems governed by Markov chains. IEEE Trans actions on Automatic Control, AC-19(5):501-507. Leighton, T., Makedon, F. , Plotkin, S., Stein, C., Tar dos, E., and Tragoudas, S. (1991) . Fast approximation algorithms for multicommodity flow problems. In Pro ceedings of 23rd Annual ACM Symposium on Theory of Computing, pages 101-111. Melekopoglou, M. and Condon, A. (1990). On the complexity of the policy iteration algorithm for stochastic games. Technical Report CS-TR-90-941, Computer Sciences Department, University of Wiscon sin Madison. To appear in the ORSA Journal on Com puting.\nMoore, A. W. and Atkeson, C. G. (1993). Memory based reinforcement learning: Efficient computation with prioritized sweeping. In Advances in Neural In formation Processing Systems 5, pages 263-270, San Mateo, California. Morgan Kaufmann. Moore, A. W. and Atkeson, C. G. (1995). The parti game algorithm for variable resolution reinforcement learning in multidimensional state spaces. To appear in Machine Learning. Papadimitriou, C. H. and Tsitsiklis, J. N. (1987). The complexity of Markov chain decision processes. Math ematics of Operations Research, 12(3):441-450. Peng, J. and Williams, R. J. (1993). Efficient learning and planning within the dyna framework. Adaptive Behavior, 1(4):437-454.\nPlotkin, S. A., Shmoys, D. B., and Tardos, E. (1991). Fast approximation algorithms for fractional packing and covering problems. In 32nd Annual Symposium on Foundations of Computer Science, pages 495-504. Puterman, M. L. (1994). Markov Decision Processes. John Wiley & Sons, New York. Puterman, M. L. and Shin, M. C. (1978). Modified policy iteration algorithms for discounted Markov de cision processes. Management Science, 24: 1 127-1137. Schrijver, A. (1986). Theory of linear and integer pro gramming. Wiley-Interscience. Schweitzer, P. J. (1984) . Aggregation methods for large Markov chains. In Iazola, G., Contois, P. J., and Hordijk, A., editors, Mathematical Computer Perfor mance and Reliability, pages 275-302. Elsevier, Ams terdam, Holland. Tseng, P. (1990). Solving H-horizon, stationary Markov decision problems in time proportional to log(H). Operations Research Letters, 9(5):287-297. Williams, R. J. and Baird, L. C. I. (1993). Tight per formance bounds on greedy policies based on imper fect value functions. Technical Report NU-CCS-93-13, Northeastern University, College of Computer Science, Boston, MA."
    } ],
    "references" : [ {
      "title" : "Dynamic Programming",
      "author" : [ "R. Bellman" ],
      "venue" : null,
      "citeRegEx" : "Bellman,? \\Q1987\\E",
      "shortCiteRegEx" : "Bellman",
      "year" : 1987
    }, {
      "title" : "Adaptive aggregation for infinite horizon dynamic programming",
      "author" : [ "D.P. Bertsekas", "D.A. Castanon" ],
      "venue" : "IEEE Transactions on Automatic Control, 34 (6) :589598.",
      "citeRegEx" : "Bertsekas and Castanon,? 1989",
      "shortCiteRegEx" : "Bertsekas and Castanon",
      "year" : 1989
    }, {
      "title" : "Linear Programming and Exten­ sions",
      "author" : [ "G. Dantzig" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Dantzig,? \\Q1960\\E",
      "shortCiteRegEx" : "Dantzig",
      "year" : 1960
    }, {
      "title" : "Decomposition tech­ niques for planning in stochastic domains",
      "author" : [ "T. Dean", "Lin", "S.-H" ],
      "venue" : "In Proceed­ ings of the 1995 International Joint Conference on A",
      "citeRegEx" : "Dean et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 1995
    }, {
      "title" : "Dynamic Programming and Markov Processes",
      "author" : [ "R.A. Howard" ],
      "venue" : null,
      "citeRegEx" : "Howard,? \\Q1960\\E",
      "shortCiteRegEx" : "Howard",
      "year" : 1960
    }, {
      "title" : "A subexponential randomized sim­ plex algorithm",
      "author" : [ "G. Kalai" ],
      "venue" : "Proceedings of 24th Annual ACM Symposium on the Theory of Computing, pages 475482.",
      "citeRegEx" : "Kalai,? 1992",
      "shortCiteRegEx" : "Kalai",
      "year" : 1992
    }, {
      "title" : "A polynomial algorithm for linear programming",
      "author" : [ "L.G. Khachian" ],
      "venue" : "Soviet Math Dokl. , 20:191-194.",
      "citeRegEx" : "Khachian,? 1979",
      "shortCiteRegEx" : "Khachian",
      "year" : 1979
    }, {
      "title" : "How good is the simplex algorithm",
      "author" : [ "V. Klee", "G.J. Minty" ],
      "venue" : "In Shisha,",
      "citeRegEx" : "Klee and Minty,? \\Q1972\\E",
      "shortCiteRegEx" : "Klee and Minty",
      "year" : 1972
    }, {
      "title" : "Decomposition of systems governed by Markov chains",
      "author" : [ "H.J. Kushner", "Chen", "C.-H." ],
      "venue" : "IEEE Trans­ actions on Automatic Control, AC-19(5):501-507.",
      "citeRegEx" : "Kushner et al\\.,? 1974",
      "shortCiteRegEx" : "Kushner et al\\.",
      "year" : 1974
    }, {
      "title" : "On the complexity of the policy iteration algorithm for stochastic games",
      "author" : [ "M. Melekopoglou", "A. Condon" ],
      "venue" : "Technical Report CS-TR-90-941, Computer Sciences Department, University of Wiscon­ sin Madison. To appear in the ORSA Journal on Com­",
      "citeRegEx" : "Melekopoglou and Condon,? 1990",
      "shortCiteRegEx" : "Melekopoglou and Condon",
      "year" : 1990
    }, {
      "title" : "Memory­ based reinforcement learning: Efficient computation with prioritized sweeping",
      "author" : [ "A.W. Moore", "C.G. Atkeson" ],
      "venue" : "Advances in Neural In­ formation Processing Systems 5, pages 263-270, San Mateo, California. Morgan Kaufmann.",
      "citeRegEx" : "Moore and Atkeson,? 1993",
      "shortCiteRegEx" : "Moore and Atkeson",
      "year" : 1993
    }, {
      "title" : "The parti­ game algorithm for variable resolution reinforcement learning in multidimensional state spaces",
      "author" : [ "A.W. Moore", "C.G. Atkeson" ],
      "venue" : "To appear in Machine Learning.",
      "citeRegEx" : "Moore and Atkeson,? 1995",
      "shortCiteRegEx" : "Moore and Atkeson",
      "year" : 1995
    }, {
      "title" : "The complexity of Markov chain decision processes",
      "author" : [ "C.H. Papadimitriou", "J.N. Tsitsiklis" ],
      "venue" : "Math­ ematics of Operations Research, 12(3):441-450.",
      "citeRegEx" : "Papadimitriou and Tsitsiklis,? 1987",
      "shortCiteRegEx" : "Papadimitriou and Tsitsiklis",
      "year" : 1987
    }, {
      "title" : "Efficient learning and planning within the dyna framework",
      "author" : [ "J. Peng", "R.J. Williams" ],
      "venue" : "Adaptive Behavior, 1(4):437-454.",
      "citeRegEx" : "Peng and Williams,? 1993",
      "shortCiteRegEx" : "Peng and Williams",
      "year" : 1993
    }, {
      "title" : "Fast approximation algorithms for fractional packing and covering problems",
      "author" : [ "S.A. Plotkin", "D.B. Shmoys", "E. Tardos" ],
      "venue" : "32nd Annual Symposium on Foundations of Computer Science, pages 495-504.",
      "citeRegEx" : "Plotkin et al\\.,? 1991",
      "shortCiteRegEx" : "Plotkin et al\\.",
      "year" : 1991
    }, {
      "title" : "Markov Decision Processes",
      "author" : [ "M.L. Puterman" ],
      "venue" : "John Wiley & Sons, New York.",
      "citeRegEx" : "Puterman,? 1994",
      "shortCiteRegEx" : "Puterman",
      "year" : 1994
    }, {
      "title" : "Modified policy iteration algorithms for discounted Markov de­ cision processes",
      "author" : [ "M.L. Puterman", "M.C. Shin" ],
      "venue" : "Management Science, 24: 1 127-1137.",
      "citeRegEx" : "Puterman and Shin,? 1978",
      "shortCiteRegEx" : "Puterman and Shin",
      "year" : 1978
    }, {
      "title" : "Theory of linear and integer pro­ gramming",
      "author" : [ "A. Schrijver" ],
      "venue" : "Wiley-Interscience.",
      "citeRegEx" : "Schrijver,? 1986",
      "shortCiteRegEx" : "Schrijver",
      "year" : 1986
    }, {
      "title" : "Aggregation methods for large Markov chains",
      "author" : [ "P.J. Schweitzer" ],
      "venue" : "Mathematical Computer Perfor­ mance and Reliability,",
      "citeRegEx" : "Schweitzer,? \\Q1984\\E",
      "shortCiteRegEx" : "Schweitzer",
      "year" : 1984
    }, {
      "title" : "Solving H-horizon, stationary Markov decision problems in time proportional to log(H)",
      "author" : [ "P. Tseng" ],
      "venue" : "Operations Research Letters, 9(5):287-297.",
      "citeRegEx" : "Tseng,? 1990",
      "shortCiteRegEx" : "Tseng",
      "year" : 1990
    }, {
      "title" : "Tight per­ formance bounds on greedy policies based on imper­ fect value functions",
      "author" : [ "R.J. Williams", "L.C.I. Baird" ],
      "venue" : "Technical Report NU-CCS-93-13, Northeastern University, College of Computer Science, Boston, MA.",
      "citeRegEx" : "Williams and Baird,? 1993",
      "shortCiteRegEx" : "Williams and Baird",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "MDPs employ dynamical models based on well-understood stochastic processes and performance criteria based on established theory in operations research, economics, combinatorial opti­ mization, and the social sciences (Puterman, 1994) .",
      "startOffset" : 217,
      "endOffset" : 233
    }, {
      "referenceID" : 6,
      "context" : "There are algorithms for solving rational LPs that take time polynomial in the number of variables and constraints as well as the number of bits used to represent the coefficients (Karmarkar, 1984; Khachian, 1979).",
      "startOffset" : 180,
      "endOffset" : 213
    }, {
      "referenceID" : 2,
      "context" : "The most popular (and practical) methods for solving linear programs are variations of Dantzig's (1963) sim­ plex method.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "Although simplex methods seem to perform well in practice, Klee and Minty (1972) showed that one of Dantzig's choices of pivoting rule could lead the sim­ plex algorithm to take an exponential number of it­ erations on some problems.",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "While progress has been made on speeding up linear programming algorithms (such as a subexponen­ tial simplex algorithm which uses a randomized pivot­ ing rule (Bland, 1977; Kalai, 1992)), MDP-specific al­ gorithms hold more promise for efficient solution.",
      "startOffset" : 160,
      "endOffset" : 186
    }, {
      "referenceID" : 4,
      "context" : "One of the best known of these algorithms is due to Howard (1960) and is known as policy iteration.",
      "startOffset" : 52,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "Since there are only M N distinct policies, and each new policy dominates the previous one (Puterman, 1994), it is obvious that policy iteration terminates in at most an exponential number of steps.",
      "startOffset" : 91,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "Figure 1: Simple policy iteration requires an expo­ nential number of iterations to generate an optimal solution to the family of MDPs illustrated here (af­ ter (Melekopoglou and Condon, 1990)) .",
      "startOffset" : 161,
      "endOffset" : 192
    }, {
      "referenceID" : 9,
      "context" : "Melekopoglou and Condon (1990) examine the problem of solving expected cost-to-target MDPs using several variations on the sequential improvement policy iteration algo­ rithm.",
      "startOffset" : 0,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : "When combined with a result by Tseng (1990) (de­ scribed in more detail in the next section) which bounds the time needed for value iteration to find an optimal policy, this shows that policy iteration takes polynomial time, for a fixed discount rate.",
      "startOffset" : 31,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "By examining the Bellman residual during value itera­ tion and stopping when it gets below some threshold, f1 = E(1-1)/(21), we can guarantee that the resulting policy will be t:-optimal (Williams and Baird, 1993).",
      "startOffset" : 187,
      "endOffset" : 213
    }, {
      "referenceID" : 15,
      "context" : "This is the standard \"contraction mapping\" result for discounted MDPs (Puterman, 1994).",
      "startOffset" : 70,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "A standard result in the theory of linear programming is that the solution to such a linear program can be written as rational numbers where each component is represented using a number of bits polynomial in the size of the system and B, B* (Schrijver, 1986).",
      "startOffset" : 241,
      "endOffset" : 258
    }, {
      "referenceID" : 15,
      "context" : "Puterman and Shin (1978) describe a general method called modified policy iteration that includes policy iteration and value iteration as special cases.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "A promising approach from this literature involves a heuristic for dynami­ cally choosing which states to update in value itera­ tion according to how likely such an update would be to improve the estimated total-cost function (Moore and Atkeson, 1993; Peng and Williams, 1993).",
      "startOffset" : 227,
      "endOffset" : 277
    }, {
      "referenceID" : 13,
      "context" : "A promising approach from this literature involves a heuristic for dynami­ cally choosing which states to update in value itera­ tion according to how likely such an update would be to improve the estimated total-cost function (Moore and Atkeson, 1993; Peng and Williams, 1993).",
      "startOffset" : 227,
      "endOffset" : 277
    }, {
      "referenceID" : 14,
      "context" : "1 (Plotkin et al., 1991).",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 18,
      "context" : "Aggregation has long been an active topic of research in operations research and optimal con­ trol (Schweitzer, 1984).",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : ", 1995b; Dean and Lin, 1995) and reinforcement learning (Kael­ bling, 1993; Moore and Atkeson, 1995) have been exploring aggregation and decomposition techniques for solving large MDPs.",
      "startOffset" : 56,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "In particular, Bertsekas and Castanon (1989) describe adaptive aggregation tech­ niques that might be very important for large, struc­ tured state spaces, and Kushner and Chen (1974) de­ scribe how to use Dantzig-Wolfe LP decomposition techniques (1960) for solving large MDPs.",
      "startOffset" : 15,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "In particular, Bertsekas and Castanon (1989) describe adaptive aggregation tech­ niques that might be very important for large, struc­ tured state spaces, and Kushner and Chen (1974) de­ scribe how to use Dantzig-Wolfe LP decomposition techniques (1960) for solving large MDPs.",
      "startOffset" : 15,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : "In particular, Bertsekas and Castanon (1989) describe adaptive aggregation tech­ niques that might be very important for large, struc­ tured state spaces, and Kushner and Chen (1974) de­ scribe how to use Dantzig-Wolfe LP decomposition techniques (1960) for solving large MDPs.",
      "startOffset" : 15,
      "endOffset" : 254
    } ],
    "year" : 2011,
    "abstractText" : "Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying au­ tomated planning and reinforcement learn­ ing. In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution al­ gorithms. We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly. To encourage future research, we sketch some alternative methods of analysis that rely on the struc­ ture of MDPs.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}