{
  "name" : "1301.6716.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lazy Evaluation of Symmetric Bayesian Decision Problems",
    "authors" : [ "Anders L. Madsen", "Finn V. Jensen" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Solving symmetric Bayesian decision prob lems is a computationally intensive task to perform regardless of the algorithm used. In this paper we propose a method for improv ing the efficiency of algorithms for solving Bayesian decision problems. The method is based on the principle of lazy evalua tion - a principle recently shown to improve the efficiency of inference in Bayesian net works. The basic idea is to maintain de compositions of potentials and to postpone computations for as long as possible. The efficiency improvements obtained with the lazy evaluation based method is emphasized through examples. Finally, the lazy evalu ation based method is compared with the HUGIN and valuation-based systems architec tures for solving symmetric Bayesian decision problems.\n1 INTRODU CTION\nBayesian decision theory provides a solid foundation for assessing and thinking about actions under un certainty. A symmetric Bayesian decision problem is specified with a set of decision variables, a set of chance variables, a multiplicative decomposition of the joint probability distribution of the chance variables given the decision variables, and a utility function speci fying the preferences of the decision maker. Solving Bayesian decision problems is, unfortunately, a com putationally intensive task to perform.\nInfluence diagrams [Howard and Matheson, 1984] is an effective modeling framework for analysis of sym metric Bayesian decision making under uncertainty. The influence diagram is a natural representation for capturing the semantics of decision making with a minimum of clutter and confusion for the decision\nmaker [Shachter and Peot, 1992]. An influence dia gram is essentially a Bayesian network augmented with decision variables and utility nodes.\nValuation-based systems (VBS) [Shenoy, 1992] is a framework different from influence diagrams for solv ing symmetric Bayesian decision problems. Influence diagrams are based on the semantics of conditional independence relations while VBS are based on a fac torization of the joint probability distribution of the chance variables and a decomposition of the utility function. A graphical illustration of a VBS is referred to as a valuation network. In valuation networks vari ables, potentials and the precedence constraints are specified explicitly.\nSolving a symmetric Bayesian decision problem amounts to computing an optimal strategy maxi mizing the expected utility for the decision maker and computing the maximal expected utility of this strategy. An optimal strategy specifies the optimal decision for each decision variable given the past. A number of different methods for solving symmetric Bayesian decision problems ex ists, see for instance [Shachter, 1986, Shenoy, 1992, Ndilikilikesha, 1994, Jensen et a!., 1994, Zhang, 1998].\nRecently, the lazy propagation architecture for prob abilistic inference in Bayesian networks was pro posed [Madsen and Jensen, 1998]. The method we propose is based on applying the principles of lazy evaluation to solving decision problems. Bayesian net work inference involves elimination of chance variables only. Solving symmetric Bayesian decision problems, on the other hand, involves elimination of both chance and decision variables. Elimination of a chance vari able is performed by summation while elimination of a decision variable is performed by maximization. Ex tending the lazy propagation architecture for solving decision problems involves extending both the on-line triangulation heuristic, the elimination algorithm, and the potential combination algorithm.\nLazy Evaluation of Symmetric Bayesian Decision Problems 383\nIn general, we assume the reader to be familiar with Bayesian networks, influence diagrams, and VBS.\n2 PRELIMINARIES\nThe set of chance variables of a symmetric Bayesian decision problem is partitioned into disjoint informa tion sets Io, I1 , ... .In relative to the decision vari ables. The partition induces a partial ordering -< on the variables of the decision problem. The set of vari ables observed between decision D, and DH 1 precedes DH 1 and succeeds D, in the ordering: Io -< D 1 -< I1 · · · -< Dn -< In.\nThe set of legal elimination orderings is constrained to include only orderings where all variables in infor mation set h are eliminated before decision D,. This implies that Io is the set of chance variables initially observed and In is the set of chance variables never observed or observed after the last decision has been made. A total ordering on the decision variables is usu ally assumed. This assumption can, however, be re laxed. (Nielsen and Jensen, 1999] describes when de cision problems with only a partial ordering on the decision variables is well-defined.\nOne of the main difficulties in solving symmetric Bayesian decision problems is that elimination of chance and decision variables do not, in general, com mute. Variables within the same information set can, however, be eliminated in any order and consecutive decision variables 0;. and 0;.+ 1 can be eliminated in any order if the information set I;. is empty.\nA decision rule for decision variable D;. is a function 6;. : dom(no,) -) dom(D;.) where no, is the relevant past of 0;.. The relevant past of decision variable 0;. is the subset of the informational parents of 0;. relevant for making decision 0;. (see [Nielsen and Jensen, 1999] for a structural definition).\nA strategy is an ordered set of decision rules t. = ( 61, . . . , 6n) including a decision rule for each decision. An optimal strategy 3; returns the optimal decision for the decision maker to make at each decision. To solve a decision problem is to compute an optimal strat egy 3; maximizing the expected utility for the decision maker and to compute the maximum expected utility MEU{6) of 3;. The utility function specifying the preferences of the decision maker is most often assumed to decompose additively as when the utility function decomposes multiplicatively solving the decision problem reduces to a task similar to performing probabilistic inference in Bayesian networks. We return to this point later.\nThe perfect recall assumption of Bayesian decision\nproblems implies that at the time of any decision, the decision maker remembers all past decisions and all previously known information. This implies that a decision variable and all its parents are informa tional parents of all subsequent decision variables (non-forgetting arcs will not be depicted in any figure, but they will always be assumed present).\nFor ease of exposition and limited space we focus on the influence diagram representation of symmet ric Bayesian decision problems. An influence diagram ID is a triple ( G, P, U) where G is a directed acyclic graph with chance, decision, and utility nodes. Each chance node corresponds to a chance variable which is associated with a conditional probability distribution. The set of conditional probability distributions is de noted P. Each value node is associated with a utility potential and the set of utility potentials is denoted U. Each decision node is associated with a decision variable. Arcs into chance nodes denote probabilistic dependence relations and an arc from a node X into a decision node D indicates that the state of X is known when decision D is to be made.\nIn the rest of the paper the generalized marginaliza tion operator M introduced by (Jensen et a!., 1994] is used. The marginalization operator works differently for marginalization of chance and decision variables:\nM P = L p and M p = max p, X X D D\nwhere X is a chance and D is a decision variable.\n3 THE HUGIN ARCHITECTURE\nThe HUGIN architecture for solving influence diagrams is based on message passing in strong junction trees. A strong junction tree representation T of an influence diagram ID is constructed based on moralization, re moval of the information arcs, and removal of the di rection on the remaining arcs of ID. Let rom denote the resulting graph. The nodes of T are cliques of a strong triangulation of rom and T has the property that for any two adjacent cliques C;. and Ci with C;. closest to the strong root R ofT there exists an ordering of the variables of Ci respecting -< with the variables of C;. n Ci preceding the variables of Ci \\ C;.. Once T is constructed, the probability and utility po tentials of ID are associated with cliques of T such that the domain of the potential is a subset of the clique domain. When all potentials have been associ ated with cliques, the probability potentials associated with a clique C are combined by multiplication and the utility potentials associated with C are combined by\n384 Madsen and Jensen\nsummation to form the initial clique probability and utility potentials <l>c and tJ>c, respectively.\nMessages are passed from the leaf cliques ofT to R and messages in different branches ofT can be passed inde pendently. The message passed over a separator S con necting cliques Ci and Ci consists of two potentials, a probability potential <1>5 computed by marginalization of <l>c; down to S and a utility potential tl>5 computed by marginalization of the combination of <1> c; and tj> c 1 down to S:\n<1>5 = M <l>c, and tl>5 = M <l>c, tl>c,. C;\\S C;\\S\nWhen a clique C receives messages from neighboring separators the clique probability and utility potentials <l>c and tl>c are updated with the potentials received:\n<I>C: = <l>c I1 <1>5 and SEch(C) ,,.. ,,, ' tl>5 'I'C = 'I'C + L <I>* , SEch(C) S\nwhere ch( C) is the set of separators between C and all adjacent cliques further away from R.\nThe relevant past of a decision variable is not simple to read off from the strong junction tree structure. How ever, let C be the clique containing decision variables 0 closest to R and let Yo be the set of variables pre ceding 0 in C. If and only if X E Yo, then there is a path between X and 0 in rom such that when 0 is to be eliminated all variables on the path between X and 0 have already been eliminated. This implies that all past variables relevant for computing the optimal deci sion rule b is a subset of Yo where b returns the max imizing alternatives of the utility potential tj>(O, Yo).\nThe partial ordering imposed by the structure of the influence diagram IO is extended to a total ordering by the heuristic method used for the strong triangulation of rom. The strong junction tree T constructed from the strong triangulation imposes a partial order on the elimination of variables during message passing. The fact that T only imposes a partial order on the set of legal elimination orderings cannot be exploited fully in the HuGIN architecture as the potentials initially associated with cliques are combined to form initial clique probability and utility potentials.\n4 VARIABLE ELIMINATION\nThe maximum expected utility of a symmetric Bayesian decision problem is calculated by eliminat ing all variables of the decision problem in the reverse order of the partial ordering imposed by the informa tion constraints. A formula representing the task of computing MEU(Ll) by eliminating a set of variables\nY consisting of chance variables {X 1 , .. . , Xn} and de cision variables {0 1, . . . , 0 0} from a set of probability potentials <I> = {P(Xi I pa(X;) 11 -:; i::; n} and a set of utility potentials '¥ = {U 1 , . • • , Um} is:\nAssume the first variable to eliminate according to the strong elimination order is Y. The set of utility poten tials '¥ can be divided into two subsets. Let 'Dy be the subset of utility potentials including Y in the domain: Vy = {Uj I Y E dom(Ui ), 1 ::; j ::; , m}, let Nv be the set of utility potentials not including Y in the domain: Nv = {Ui 11 ::; j ::; m} \\ Vy, and let 1-lv = Y U ch(Y) where ch(Y) is the set of children of Y. Furthermore, let <l>v be the potential obtained by eliminating Y from the combination of all probability potentials with Y in the domain and let tj>y be the potential obtained by eliminating Y from the combination of all probability and utility potentials with Y in the domain:\n<l>v = M IJ P(X;Ipa(X;)),\ntl>v = M ( IJ P(X;Ipa(Xdl) L. uk. (2) Y X1E1l.v UKEVv\nWith these definitions, equation 1 is rewritten as:\nMEU(Li) = t; [ (DP(Xilpa(Xdl) ( L. ui + L. uk)]\nU;ENv U,EDv\nM [ ( IJ P(Xi lpa(Xj))) XEV\\{Y} X; EV\\'H.v\nM ( IJ P(XiJpa(Xj)l) ( L. ui + L. uk)] Y X;E'H.v U;ENv U,EVv\nM [( IJ P(Xjlpa(Xill) XEV\\{Y} X;EV\\'H.v\n( c�v Ui) <l>v + tl>v)] M [( IJ P(Xilpa(Xill)<l>v\nXEV\\{Y} X;EV\\'H.v\n(3)\n(4)\nLazy Evaluation of Symmetric Bayesian Decision Problems 385\nEquation 4 shows that elimination of Y from <1> and '!' produces a probability potential <fly and a utility potential �. When Y is eliminated, all potentials of <l>y and '!'y including Y in their domain are removed from <1> and '!', respectively. Instead, <fly and � are added. The union of the updated sets <1>* U '!'* is a value preserving reduction of <1> U '!' where Y has been eliminated.\nThe above derivation is similar to the derivation pre sented in [Dechter, 1996] with the exception that we do not assume the decision variables to be root vari ables in the influence diagram. Equation 4 is essen tially a general specification of the fusion operation of [Shenoy, 1992]. The fusion operation, however, ex plicitly differentiates between four different cases de pending on the variable Y being eliminated and the set of potentials with Y in the domain.\nOne of the main difficulties in solving symmetric Bayesian decision problems is that an additive decom position of the utility function makes combination of utility and probability potentials non-associative. If the utility function decomposes multiplicatively, then combination of probability and utility potentials is as sociative, and the need for division and summation of potentials is eliminated. This implies that the task of solving a symmetric Bayesian decision problem is reduced to a task similar to performing probabilistic inference in a Bayesian network 1.\n5 LAZY PROPAGATION\nFor ease of exposition the lazy evaluation based method is described in the context of influence dia grams and strong junction trees, but notice that the principles of lazy evaluation can be applied to other representations and computational structures such as VBS. In the context of strong junction trees we refer to the method as the lazy propagation algorithm.\nLazy propagation is based on message passing in a strong junction tree representation, say T, of a deci sion problem represented as an influence diagram, say ID. Initially, the probability and utility potentials of ID are associated with cliques of T. The potentials associated with cliques are not combined, so during message passing each clique and separator holds two sets of potentials. Messages are passed from the leaves ofT towards the strong root R by recursively invoking the CollectMessage algorithm (defined below). When CollectMessage is invoked on a clique Ci from an adja cent clique Ci, clique Ci invokes CollectMessage on all other adjacent cliques. When these cliques have fin-\n1 A multiplicative decomposition of the utility function is obtainable using the exponential function, for instance.\nished their CollectMessage, Cj absorbs messages from each of them and C absorbs from Ci: Algorithm 5.1 [CollectMessage] Let Ci and Ci be adjacent cliques. If CollectMessage is invoked on Ci from Ci, then:\n1. Ci invokes CollectM essage on all adjacent cliques except C.\n2. The message from Ci to Ci is absorbed (algo rithm 5.2).\nD\nFrom the description of the CollectMessage algorithm it is clear that information is passed between adjacent cliques by absorption. Consider two adjacent cliques Ci and Ci separated by S. Absorption from Ci to Ci over S amounts to eliminating the variables of Ci \\ S from the sets of probability and uti.]ity potentials <1> and '!' associated with Ci and the separators of ch( Ci) and then associating the obtained potentials with S:\nAlgorithm 5.2 [Absorption] Let Ci and Ci be adjacent cliques and let S be the separator between Ci and Ci. If Absorption is invoked on Ci from Ci, then:\nu S'Ech(C;)\n2. Marginalize out all variables {Y E dom(p) I p E Rs, Y if. S} (algorithm 5.3). Let <1>$ and '¥$ be the sets of probability and utility potentials obtained.\n3. Associate <1>$ and '¥$ with S as the sets of poten tials passed from Ci to Ci.\nD\nAlgorithm 5.2 uses the algorithm for marginalization of variables from two sets of potentials:\nAlgorithm 5.3 [Marginalization] Let <1> and '!' be sets of probability and utility poten tials, respectively. If Marginalization of variable Y is invoked on <1> U '!', then:\n1. Set <l>y = { <fJ E <1> I Y E dom( <fJ)} and '!'y = N> E '!'I Y E dom(l)J)}.\n2. Calculate\n<ll� M IT <tJ, Y <PE<I>v\nl)J� M IT <tJ .L 1)!, Y <PE<I>v >l>E'l'v\n386 Madsen and Jensen\n3. Return <I>* = <I> U { <l>v} \\ <l>v and '1'* = '1' U { *f} \\ '!'y.\n0\nThe maximizing alternatives of the utility potential \"lj.>(D, Yo) from which D is eliminated during evalua tion of the influence diagram should be recorded as the optimal decision rule for D.\nTheorem 5.1 [Lazy Propagation] Lazy propagation computes the optimal strategy l maximizing the expected utility for the decision maker and the maximum expected utility MEU(l).\nProof. Nothing to prove. It is just a change of repre sentation. 0\n5.1 EFFICIENCY IMPROVEMENTS\nA number of adjustments to the basic lazy propaga tion algorithm can improve the efficiency of the algo rithm considerably. Consider absorption of informa tion between adjacent cliques C, and Ci. In step 1 of algorithm 5.2 all potentials associated with clique Ci and separators of ch( Ci), <I> and '1' say, are as sumed relevant for calculating the message to pass to clique Ci· The subset of potentials Rs that can be relevant for computing the message to pass over 5 can be determined in time linear in the size of the domain graph induced by <I> U '1' using the d-separation crite rion [Geiger et a!., 1990]:\nAlgorithm 5.4 [Find Potentials That Can Be Relevant] Let <I> and '1' be sets of probability and utility poten tials and let 5 be a set of variables. If Find Potentials That Can Be Relevant for calculating the joint of 5 is invoked on <I> U '1', then:\n1. Return Rs { p E <I> U '1' 3X E dom(p) and X d-connected toY E 5}.\n0\nBarren variables in Bayesian networks are defined as variables which are neither evidence nor target vari ables and have only barren descendants. We define barren variables in influence diagrams to have the same properties as barren variables in Bayesian networks with the additional property that a variable with a directed path to a utility potential cannot be barren. If a variable, X say, is barren when only the set of probability potentials are considered, we refer to X as a probabilistic barren variable. If a decision variable D is barren, then any alternative for D is optimal. The set of potentials Rs returned by algorithm 5.4 can be\nreduced by recursively removing all potentials includ ing only barren variables as head variables.\nIf a variable X is probabilistic barren, then <!> = Mx ncj>E<I>x <!> is a unity-potential (referred to as vac uous valuations in [Shenoy, 1992]). During calculation of messages unity-potentials are not calculated and if the denominator of the division introduced in step 3 of algorithm 5.3 is a unity-potential, then the division is not performed. If the denominator is not a unity potential, then the division should be performed on the domain of ncj>E<I>v <!> and not on the domain of Wy as the domain size of Wv is at least the size of the do main of ncj>E<I>v Q>. A division should not be performed when it is introduced, but should be postponed for as long as possible.\nStep 3 of algorithm 5.3 corresponds to equation 4, but in some cases it is more efficient to use equation 3. If, for example, the set Nv is known to be empty, then it is more efficient to use equation 3. Whether it is more efficient to use equation 3 or equation 4 cannot always be determined locally in each clique.\nConsider the calculation of \"lj.>y in equation 2. This calculation can also be performed as:\nwv = M L ( IT P(Xilpa(Xd)) uk. (5) Y U,eVv X;E1lv\nWhether to distribute the combination of all probabil ity potentials depends on the structure of the utility potentials and whether or not the division in step 3 of algorithm 5.3 is necessary. If all utility potentials have the same domain, then it is more efficient not to distribute. If, on the other hand, the utility potentials only share the variable Y, then it is more efficient to distribute.\nDue to the partial ordering of the variables imposed by the information constraints, a variable X E Ik cannot be instantiated by evidence when calculating the max imum expected utility of a decision variable Di where k :0:: j. This implies that the probability potential <!>x calculated when eliminating X is not a unity-potential only if X is non-descendent of Di. In general, evidence instantiating a variable X is exploited by reducing the domain of potentials including X in the domain.\nSubject to the limitations of the information con straints, the internal elimination order in a clique C is determined based on the set of potentials associated with C and separators of ch(C). Let V be the set of variables to eliminate when calculating a message. If a variable X E V is a probabilistic barren variable, then X is eliminated first unless elimination of another non probabilistic barren variable Y E V reduces the domain\nLazy Evaluation of Symmetric Bayesian Decision Problems 387\nsize of a utility potential and the elimination of Y does not make a barren variable non-barren.\n5.2 PROPERTIES\nIn this section some properties of the lazy propagation architecture are exemplified.\nExample 5.2 The influence diagram ID of [Jensen et a!., 1994] and a corresponding junction tree T are shown in figures 1 and 2, respectively. Figure 3 shows the flow of mes sages in T during CallectMessage invoked on BD1 EFD (sets including only unity-potentials are not shown).\nFigure 1: An influence diagram ID.\nAs 4>(D4, I) is a unity-potential it is not computed and the division introduced in step 3 of algorithm 5.3 is not performed. In fact, no divisions are performed when solving ID in the lazy propagation architecture. Furthermore, as a benefit of the decomposition of po tentials and exploitation of unity-potentials consider ably fewer arithmetic operations are performed in the\nlazy propagation architecture than in the HUGIN ar chitecture. If we assume all variables to be binary, then 575 and 161 arithmetic operations are performed in the HUGIN and the lazy propagation architectures, respectively. This does not include the operations re quired to compute the initial clique probability and utility potentials in the HUGIN architecture.\nFigure 3: The flow of messages towards BD1 EFD.\n0\nUsing the HUG!N architecture it is difficult to handle variables which the decision maker might or might not have evidence on. It is, in fact, necessary to create a strong junction for each scenario. The lazy propaga tion architecture supports certain types of changes in the structure of the influence diagram without requir ing construction of a new strong junction tree. The types of changes supported follows from the way in which variables are eliminated. Variables are elimi nated one at a time according to the reverse partial order imposed by the structure of the strong junction tree. If a change made to the structure of the influ ence diagram moves a variable X from one information set It to another information set Ij where j < i, then X has to be eliminated at a later time than specified in the original influence diagram. This is readily han dled in the lazy propagation architecture as this just amounts to postponing the elimination of X until in formation set Ij is reached during message passing in the junction tree. The opposite situation, however, cannot be handled as easily. To facilitate the possibil ity of changing the time of elimination of a variable X the strong junction tree can be constructed under the assumption that X Ein.\nExample 5.3 (Myopic Value of Information] [Dittmer and Jensen, 1997] introduces a method for myopic value of information analysis (VOl) on a pre determined set of variables V in influence diagrams. The method is based on introducing additional con trol structures. The control structures can be inter preted as adding V to all cliques of the junction tree, but due to the introduction of additional control struc-\n388 Madsen and Jensen\ntures the decrease in performance is not as serious as this. Similarly to the above described approach, clique and separator potentials are only extended as needed. The method facilitates VOl for different information scenarios within the same junction tree.\nFigure 4: The original junction tree T.\nConsider the junction tree T with root 0 1 C shown in figure 4 (borrowed from [Dittmer and Jensen, 1997]). To support VOl on B control structures corresponding to extending all cliques and separators of T with B are added to T, see figure 5. The additional control structures facilitate VOl where the time of elimination of B can be chosen arbitrarily.\nFigure 5: The junction tree T' supporting VOl on B.\nPerforming VOl on B in the lazy propagation architec ture is based on T. By postponing the elimination of B as explained above it is possible to choose the time of elimination of B arbitrarily. 0\nMuch effort has been put into developing various meth ods for reducing the complexity of Bayesian network inference. Methods for approximation, exploitation of independence of causal influence, and exploitation of context specific independence, for instance, exists for Bayesian networks. Very little effort, however, has been put into developing similar methods for influ ence diagrams. The next example shows that inde pendence of causal influence (ICI) is readily exploited in the lazy propagation architecture for solving influ ence diagrams.\nExample 5.4 [Independence of Causal Influence] Consider the influence diagram 10 and the corre sponding strong junction tree T shown in figure 6. As sume that 01, c2, and c3 are causally independent with respect to E 1 and that all variables are binary with states f and t. In order to compute MEU(.&) all variables have to be eliminated according to the partial order D1 -< {E1}-< Dz-< {Ez, Cz, C3}.\nFigure 6: A simple influence diagram and a corre sponding strong junction tree T.\nIf lj.>(E1) is the messa�e passed from E1 DzEz to D1 E1 CzC3, then MEU(t.) is calculated as:\nMEU(.&) = !If,� L lj.>(E1) L P(Cz) E1 Cz\nL P(C3)P(E1I D1, Cz, C3). c,\nBy exploiting ICl the complexity of calculating MEU(.&) can be reduced to linear in the num ber of parents of E 1 . Assume that the lazy par ent divorcing method developed for Bayesian net works [Madsen and D'Ambrosio, 1'999] is used to ex ploit ICI. One hidden variable Y is introduced as a child of c2 and c3 (P(Ef I C) is the contribution from C E {D1, Cz, C3} to E1 when Vc-1-C' C' = f) :\nMEU(.&) =max L lj.>(E1) L P(Cz) L P(C3) o, E, c2 c,\nL P(Ef' I DJ) {E�1 ,YIE,=E�1 •Y}\nL P(Ef21Cz)P(Ef'l C3) {Eil ,Ei3 )Y=Ei2*£i3}\nL L P(Cz)P(Ef21Cz) {Ei2 ,E�31Y=E�2*Ei3} Cz\nL P(C3)P(Ef'IC3). c,\nWhen the ICl is exploited, the domain size of the largest potential created when computing MEU(.&) is 3 instead of 4 variables. 0\n6 COMPARISON\nThe HuGIN architecture for solving symmetric Bayesian decision problems represented as influence di agrams is based on constructing a strong junction tree representation of the influence diagram. The decision problem is solved by message passing in the strong junction tree as explained in section 3. Combining the probability and utility potentials associated with each clique to form the initial clique and utility potentials makes it impossible to benefit from determining the internal elimination order on-line and to take advan tage of independence relations induced by evidence. Furthermore, it also makes it impossible to reduce the\nLazy Evaluation of Symmetric Bayesian Decision Problems 389\ncomplexity of inference by exploiting barren and prob abilistic barren variables. Barren variables can, how ever, be determined in a preprocessing step before the strong junction tree is constructed.\n[Jensen et a!., 1994] argues that the contribution from the division operation plays a smaller role in the HUGIN architecture than in VBS since divisions are performed on separators only. We will not discuss this statement in detail here, but only note that a substan tial number of the divisions performed in the HUGIN architecture are unnecessary (see example 5.2). The lazy propagation architecture often detects the situa tions in which the divisions are unnecessary. We be lieve that it is only rarely necessary to actually per form most of the divisions as indicated in example 5.2. A number of empirical tests or an in depth analysis should be performed to dissolve this. The divisions performed in the lazy propagation architecture are not performed on separators, but are introduced when variables are eliminated. This implies that when a di vision in fact is performed the operation might have a higher computational cost than in the HuGIN archi tecture. It is, however, also possible that the converse is the case. That is, because divisions are introduced each time a variable is eliminated and only performed when it cannot be detected to be unnecessary, the op eration is likely to play a smaller role in the lazy prop agation architecture than in the HUGIN architecture.\nRecently, a method for solving symmetric Bayesian decision problems represented as influence dia grams by reducing influence diagram evaluation to Bayesian network inference problems has been pro posed [Zhang, 1998]. The reduction is performed by repeated use of Cooper's trick and the method induces simpler inference problems than the methods proposed by [Cooper, 1988] and [Shachter and Peot, 1992]. Re ducing influence diagram evaluation to Bayesian net work inference problems makes it possible and easy to exploit existing methods for improving the efficiency of Bayesian network inference to improve the efficiency of decision problem solving. We have argued that it is unnecessary to convert to Bayesian inference problems in order to exploit methods for improving the efficiency of decision problem solving. Exploitation of ICI in the lazy propagation architecture is just one example.\nThe VBS architecture for solving decision problems is based on the fusion algorithm. The marginalization algorithm (algorithm 5.3) of the basic lazy propaga tion architecture is essentially the fusion algorithm of VBS. When the marginalization algorithm is extended with the efficiency improvements of section 5.1 the al gorithms become different:\nExample 6.1 [Valuation-based systems]\nConsider the valuation network shown in figure 7 (bor rowed from [Shenoy, 1992]). The valuation network specifies a joint probability distribution for C 1 and C 2 (the triangle in the figure), a utility function de composing additively into utility potentials U( C 1) and U(D, Cz), and the precedence constraints.\nThe division performed in equation 6 is avoided if the division is postponed until it can be determined that it is unnecessary to perform it. The above derivation shows how to avoid the division. The number of arith metic operations performed to calculate equation 6 is 25 while only 21 are operations performed to compute equation 7 (the four divisions are avoided). D\n390 Madsen and Jensen\nWhen described in the context of valuation networks the lazy evaluation based method and the VBS archi tecture are very similar, but they are as example 6.1 shows different. If we consider the lazy evaluation based method in the context of strong junction trees, the differences between the two methods become clear. The main focus of VBS is on elimination of variables while the main focus of lazy propagation is on mes sage passing. The messages are calculated by elimina tion of variables and the order in which variables are eliminated is controlled by the structure of the strong junction tree and the algorithms described in section 5.\nLazy propagation exploits the structure of the strong junction tree to simplify tasks such as finding the set of potentials relevant for eliminating a particular vari able. The fusion operation, on the other hand, is ap plied directly to the entire set of potentials of the VBS without any preprocessing.\n7 CONCLUS ION\nWe have presented a lazy evaluation based method for solving symmetric Bayesian decision problems. The lazy evaluation based method is not developed to a particular computational structure as the principles of the method applies to various different computational structures. We have, however, in detail described how the principles of lazy evaluation can be applied to in fluence diagrams and strong junction trees. We have only briefly indicated how the principles of lazy eval uation can be applied to VBS. The lazy evaluation principle offers the opportunity to reduce the number of arithmetic operations performed during solution of symmetric Bayesian decision problems.\nThe computational efficiency of solving symmetric Bayesian decision problems with the lazy evaluation based method has been emphasized through examples.\nReferences\n[Cooper, 1988] Cooper, G. F. (1988). A method for using belief networks as influence diagrams. In Proc. of the 4th Conference on UAI, pages 55-63.\n[Dechter, 1996] Dechter, R. (1996). Bucket elimina tion: A unifying framework for probabilistic infer ence. In Proc. of the 12th Conference on UAI, pages 211-219.\n[Dittmer and Jensen, 1997] Dittmer, S. L. and Jensen, F. V. (1997). Myopic value of informa tion for influence diagrams. In Proc. of the 13th Conference on UAI, pages 142-149.\n[Geiger et al., 1990] Geiger, D., Verma, T. S., and Pearl, J. (1990). d-separation: From theorems to\nalgorithms. Uncertainty in Artificial Intelligence, 5:139-148.\n[Howard and Matheson, 1984] Howard, R. A. and Matheson, J. E. (1984). Influence Diagrams. The Principles and Applications of Decision Analysis, Vol. II.\n[Jensen et a!., 1994] Jensen, F., Jensen, F. V., and Dittmer, S. L. (1994). From Influence Diagrams to Junction Trees. In Proc. of the 10th Conference on UAI, pages 367-373.\n[Madsen and D'Ambrosio, 1999] Madsen, A. L. and D'Ambrosio, B. {1999). Independence of Causal In fluence and Lazy Propagation. In Proc. of the 5th ECSQARU. To appear.\n[Madsen and Jensen, 1998] Madsen, A. L. and Jensen, F. V. (1998). Lazy Propagation in Junction Trees. In Proc. of the 14th Conference on UAI, pages 362-369.\n[Ndilikilikesha, 1994] Ndilikilikesha, P. (1994). Poten tial influence diagrams. International Journal of Approximate Reasoning, 11(1):251-285.\n[Nielsen and Jensen, 1999] Nielsen, T. D. and Jensen, F. V. (1999). Welldefined decision scenarios. This conference.\n[Shachter, 1986] Shachter, R. (1986). Evaluating in fluence diagrams. Operations Research, 34(6):871- 882.\n[Shachter and Peot, 1992] Shachter, R. D. and Peot, M. A. (1992). Decision making using probabilistic inference methods. In Proc. of the 8th Conference on UAI, pages 276-283.\n[Shenoy, 1992] Shenoy, P. P. (1992). Valuation-Based Systems for Bayesian Decision Analysis. Operations Research, 40(3):463-484.\n[Zhang, 1998] Zhang, N. L. (1998). Probabilistic in ference in influence diagrams. In Proc. of the 14th Conference on UAI, pages 514-522."
    } ],
    "references" : [ {
      "title" : "Myopic value of informa­ tion for influence diagrams",
      "author" : [ "Dittmer", "Jensen", "S.L. 1997] Dittmer", "F.V. Jensen" ],
      "venue" : "In Proc. of the 13th Conference on UAI,",
      "citeRegEx" : "Dittmer et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Dittmer et al\\.",
      "year" : 1997
    }, {
      "title" : "d-separation: From theorems",
      "author" : [ "Geiger et al", "D. 1990] Geiger", "T.S. Verma", "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1990
    }, {
      "title" : "From Influence Diagrams to Junction Trees",
      "author" : [ "F. Jensen", "F.V. Jensen", "S.L. Dittmer" ],
      "venue" : "[Jensen et a!.,",
      "citeRegEx" : "Jensen et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Jensen et al\\.",
      "year" : 1994
    }, {
      "title" : "Independence of Causal In­ fluence and Lazy Propagation",
      "author" : [ "Madsen", "D'Ambrosio", "A.L. 1999] Madsen", "B. D'Ambrosio" ],
      "venue" : "In Proc. of the 5th ECSQARU. To appear",
      "citeRegEx" : "Madsen et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Madsen et al\\.",
      "year" : 1999
    }, {
      "title" : "Lazy Propagation in Junction Trees",
      "author" : [ "Madsen", "Jensen", "A.L. 1998] Madsen", "F.V. Jensen" ],
      "venue" : "In Proc. of the 14th Conference on UAI,",
      "citeRegEx" : "Madsen et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Madsen et al\\.",
      "year" : 1998
    }, {
      "title" : "Welldefined decision scenarios",
      "author" : [ "Nielsen", "Jensen", "T.D. 1999] Nielsen", "F.V. Jensen" ],
      "venue" : null,
      "citeRegEx" : "Nielsen et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Nielsen et al\\.",
      "year" : 1999
    }, {
      "title" : "Decision making using probabilistic inference methods",
      "author" : [ "Shachter", "Peot", "R.D. 1992] Shachter", "M.A. Peot" ],
      "venue" : "In Proc. of the 8th Conference on UAI,",
      "citeRegEx" : "Shachter et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Shachter et al\\.",
      "year" : 1992
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "Solving symmetric Bayesian decision prob­ lems is a computationally intensive task to perform regardless of the algorithm used. In this paper we propose a method for improv­ ing the efficiency of algorithms for solving Bayesian decision problems. The method is based on the principle of lazy evalua­ tion a principle recently shown to improve the efficiency of inference in Bayesian net­ works. The basic idea is to maintain de­ compositions of potentials and to postpone computations for as long as possible. The efficiency improvements obtained with the lazy evaluation based method is emphasized through examples. Finally, the lazy evalu­ ation based method is compared with the HUGIN and valuation-based systems architec­ tures for solving symmetric Bayesian decision problems.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}