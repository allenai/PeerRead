{"conference": "ICLR 2017 conference submission", "title": "Capacity and Trainability in Recurrent Neural Networks", "abstract": "Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.", "histories": [], "reviews": [{"IS_META_REVIEW": true, "comments": "The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.\n\nThe experimental setups look sound. To generalize comparisons between different architectures it\u2019s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups. \n\nThe descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it\u2019s hard to interpret the squared error scores in Figure 2c. It\u2019s not clear to me what the term \u2018unrollings\u2019 refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.\n\nNovelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I\u2019d argue that the paper is original enough for that reason alone.\n\nThe paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.\n\nPros:\n* Thorough analysis.\n* Seemingly proper experiments.\n* The way of quantifying capacity in neural networks adds to the novelty of the paper.\n* The results have some practical value and suggest similar analysis of other architectures.\n* The results provide useful insights into the relative merits of different RNN architectures.\n\nCons:\n* It\u2019s hard to isolate the most important findings (some plots seem redundant).\n* Some relevant experimental details are missing."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers all agreed that this paper should appear at the conference. The experiments seem to confirm interesting intuition about the capacity of recurrent nets and how difficult they are to train, and the reviewers appreciated the experimental rigor. This is certainly of interest and useful to the ICLR community and will lead to fruitful discussion. The reviewers did request more fine details related to the experiments for reproducibility (thank you for adding more detail to the appendix). The authors are recommended to steer clear of making any strong but unsubstantiated references to neuroscience.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Updated paper", "IS_META_REVIEW": false, "comments": "We have just uploaded the newest version of our paper. Based on the reviewers' helpful comments, the changes include:\n\n- Minimal Gated Unit for Recurrent Neural Networks citation\n- Improved descriptions of training tasks in Appendix (added missing details, such as time steps unrolled before computing loss)\n- Added range of perceptron capacity HP b and plot showing that optimal dataset size selected by the HP tuner is only slightly more than mutual information calculated (Figure 1 of Appendix)\n- Small text changes to emphasize that our results indicate the promise of the UGRNN and +RNN, but of course much more extensive testing is needed to declare that they are comparable to well-known, tried and true architectures\n- Welch\u2019s t-test statistics to test the significance of differences between architecture losses as a result of randomly selected hyperparameters (the experiment shown in Figure 5 and Table 1, and test statistics are reported in Table 2 of Appendix)\n- Softening of our recommendation regarding the +RNN in the discussion.\n\nWe are also in the process of releasing a dataset of the HPs and associated losses for all experiments described in the paper. While we can not be positive how long the release process will take, we plan to have the HP dataset publicly available by February 2017!\n", "OTHER_KEYS": "Jasmine L Collins"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "review of \"CAPACITY AND TRAINABILITY IN RECURRENT NEURAL NETWORKS\"", "comments": "This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular.   \n\nPros:\n\n* This paper addresses an important question I and many others would have liked to know the answer to but didn't have the computational resources to thoroughly attack it.   This is a nice use of Google's resources to help the community. \n\n* The work appears to have been done carefully so that the results can be believed.\n\n* The basic answer arrived at (that, in the \"typical training environment\" LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful.   Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper.\n\n* The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work.  In sum, they're much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity.\n\n* The point about the near-equivalence of capacity at equal numbers of parameters is very useful.   \n\n* The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures.\n\n* The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it's a practical problem that everyone working with these networks has but often isn't addressed. \n\n* The paper text is very clearly written.\n\nCons:\n\n* The work on the UGRNNs and the +RNNs seems a bit preliminary.  I don't think that the authors have clearly shown that the +RNN should be \"recommended\" with the same generality as the GRU.   I'd at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel).   In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important.   I don't really mind having them in the paper though.   I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don't know what the ICLR area chairs will think. \n\n* The paper gives short shrift to the details of the HP algorithm itself.  They do say: \n\n     \"Our setting of the tuner\u2019s internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a   \n     Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs\"  \n\nand give some good references, but I expect that actually trying to replicate this involves a lot of missing details.   \n\n* I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness.   \n\n* The neuroscience reference (\"4.7 bits per synapse\") seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained.  I guess it's just in the discussion, but it seems gratuitous.   Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be \"in agreement\" here between computational architectures and neuroscience, but perhaps they could say something like -- \"We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience.\")\n\n\n\n", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "21 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review: Capacity and Trainability in Recurrent Neural Networks", "comments": "CONTRIBUTIONS\nLarge-scale experiments are used to measure the capacity and trainability of different RNN architectures. Capacity experiments suggest that across all architectures, RNNs can store between three and six bits of information per parameter, with ungated RNNs having the highest per-parameter capacity. All architectures are able to store approximately one floating point number per hidden unit. Trainability experiments show that ungated architectures (RNN, IRNN) are much harder to train than gated architectures (GRU, LSTM, UGRNN, +RNN). The paper also proposes two novel RNN architectures (UGRNN and +RNN); experiments suggest that the UGRNN has similar per-parameter capacity as the ungated RNN but is much easier to train, and that deep (8-layer) +RNN models are easier to train than existing architectures.\n\nCLARITY\nThe paper is well-written and easy to follow.\n\nNOVELTY\nThis paper is the first to my knowledge to empirically measure the number of bits of information that can be stored per learnable parameter. The idea of measuring network capacity by finding the dataset size and other hyperparameters that maximizes mutual information is a particularly novel experimental setup.\n\nThe proposed UGRNN is similar but not identical to the minimal gated unit proposed by Zhou et al, \u201cMinimal Gated Unit for Recurrent Neural Networks\u201d, International Journal of Automation and Computing, 2016.\n\nSIGNIFICANCE\nI have mixed feelings about the significance of this paper. I found the experiments interesting, but I don\u2019t feel that they reveal anything particularly surprising or unexpected about recurrent networks; it is hard to see how any of the experimental results will change the way either that I think about RNNs, or the way that I will use them in my own future work. On the other hand it is valuable to see intuitive results about RNNs confirmed by rigorous experiments, especially since few researchers have the computational resources to perform such large-scale experiments.\n\nThe capacity experiments (both per-parameter capacity and per-unit capacity) essentially force the network to model random data. For most applications of RNNs, however, we do not expect them to work with random data; instead when applied in machine translation or language modeling or image captioning or any number of real-world tasks, we hope that RNNs can learn to model data that is anything but random. It is not clear to me that an architecture\u2019s ability to model random data should be beneficial in modeling real-world data; indeed, the experiments in Section 2.1 show that architectures vary in their capacity to model random data, but the text8 experiments in Section 3 show that these same architectures do not significantly vary in their capacity to model real-world data.\n\nI do not think that the experimental results in the paper are sufficient to prove the significance of the proposed UGRNN and +RNN architectures. It is interesting that the UGRNN can achieve comparable bits per parameter as the ungated RNN and that the deep +RNNs are more easily trainable than other architectures, but the only experiments on a real-world task (language modeling on text8) do not show these architectures to be significantly better than GRU or LSTM.\n\nSUMMARY\nI wish that the experiments had revealed more surprising insights about RNNs, though there is certainly value in experimentally verifying intuitive results. The proposed UGRNN and +RNN architectures show some promising results on synthetic tasks, but I wish that they showed more convincing performance on real-world tasks. Overall I think that the good outweighs the bad, and that the ideas of this paper are of value to the community.\n\nPROS\n- The paper is the first of my knowledge to explicitly measure the bits per parameter that RNNs can store\n- The paper experimentally confirms several intuitive ideas about RNNs:\n    - RNNs of any architecture can store about one number per hidden unit from the input\n    - Different RNN architectures should be compared by their parameter count, not their hidden unit count\n    - With very careful hyperparameter tuning, all RNN architectures perform about the same on text8 language modeling\n    - Gated architectures are easier to train than non-gated RNNs\n\nCONS\n- Experiments do not reveal anything particularly surprising or unexpected\n- The UGRNN and +RNN architectures do not feel well-motivated\n- The utility of the UGRNN and +RNN architectures is not well-established", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review: Capacity and trainability in RNNs", "comments": "The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.\n\nThe experimental setups look sound. To generalize comparisons between different architectures it\u2019s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups. \n\nThe descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it\u2019s hard to interpret the squared error scores in Figure 2c. It\u2019s not clear to me what the term \u2018unrollings\u2019 refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.\n\nNovelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I\u2019d argue that the paper is original enough for that reason alone.\n\nThe paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.\n\nPros:\n* Thorough analysis.\n* Seemingly proper experiments.\n* The way of quantifying capacity in neural networks adds to the novelty of the paper.\n* The results have some practical value and suggest similar analysis of other architectures.\n* The results provide useful insights into the relative merits of different RNN architectures.\n\nCons:\n* It\u2019s hard to isolate the most important findings (some plots seem redundant).\n* Some relevant experimental details are missing.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Insight from experiments about hyperparameters?", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"IMPACT": 4, "TITLE": "From wich range was the number of samples chosen in the capacity experiments?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016"}, {"IS_META_REVIEW": true, "comments": "The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.\n\nThe experimental setups look sound. To generalize comparisons between different architectures it\u2019s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups. \n\nThe descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it\u2019s hard to interpret the squared error scores in Figure 2c. It\u2019s not clear to me what the term \u2018unrollings\u2019 refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.\n\nNovelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I\u2019d argue that the paper is original enough for that reason alone.\n\nThe paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.\n\nPros:\n* Thorough analysis.\n* Seemingly proper experiments.\n* The way of quantifying capacity in neural networks adds to the novelty of the paper.\n* The results have some practical value and suggest similar analysis of other architectures.\n* The results provide useful insights into the relative merits of different RNN architectures.\n\nCons:\n* It\u2019s hard to isolate the most important findings (some plots seem redundant).\n* Some relevant experimental details are missing."}, {"DATE": "06 Feb 2017", "TITLE": "ICLR committee final decision", "IS_META_REVIEW": false, "comments": "The reviewers all agreed that this paper should appear at the conference. The experiments seem to confirm interesting intuition about the capacity of recurrent nets and how difficult they are to train, and the reviewers appreciated the experimental rigor. This is certainly of interest and useful to the ICLR community and will lead to fruitful discussion. The reviewers did request more fine details related to the experiments for reproducibility (thank you for adding more detail to the appendix). The authors are recommended to steer clear of making any strong but unsubstantiated references to neuroscience.", "OTHER_KEYS": "ICLR 2017 pcs"}, {"DATE": "18 Jan 2017", "TITLE": "Updated paper", "IS_META_REVIEW": false, "comments": "We have just uploaded the newest version of our paper. Based on the reviewers' helpful comments, the changes include:\n\n- Minimal Gated Unit for Recurrent Neural Networks citation\n- Improved descriptions of training tasks in Appendix (added missing details, such as time steps unrolled before computing loss)\n- Added range of perceptron capacity HP b and plot showing that optimal dataset size selected by the HP tuner is only slightly more than mutual information calculated (Figure 1 of Appendix)\n- Small text changes to emphasize that our results indicate the promise of the UGRNN and +RNN, but of course much more extensive testing is needed to declare that they are comparable to well-known, tried and true architectures\n- Welch\u2019s t-test statistics to test the significance of differences between architecture losses as a result of randomly selected hyperparameters (the experiment shown in Figure 5 and Table 1, and test statistics are reported in Table 2 of Appendix)\n- Softening of our recommendation regarding the +RNN in the discussion.\n\nWe are also in the process of releasing a dataset of the HPs and associated losses for all experiments described in the paper. While we can not be positive how long the release process will take, we plan to have the HP dataset publicly available by February 2017!\n", "OTHER_KEYS": "Jasmine L Collins"}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "review of \"CAPACITY AND TRAINABILITY IN RECURRENT NEURAL NETWORKS\"", "comments": "This paper performs a very important service: exploring in a clear and systematic way the performance and trainability characteristics of a set of neural network architectures -- in particular, the basic RNN motifs that have recently been popular.   \n\nPros:\n\n* This paper addresses an important question I and many others would have liked to know the answer to but didn't have the computational resources to thoroughly attack it.   This is a nice use of Google's resources to help the community. \n\n* The work appears to have been done carefully so that the results can be believed.\n\n* The basic answer arrived at (that, in the \"typical training environment\" LSTMs are reliable but basically GRUs are the answer) seems fairly decisive and practically useful.   Of course the real answer is more complicated than my little summary here, but the subtleties are discussed nicely in the paper.\n\n* The insistence on a strong distinction between capacity and trainability helps nicely clear up a misconception about the reasons why gated architectures work.  In sum, they're much more easily trainable but somewhat lower capacity than vanilla RNNs, and in hard tasks, the benefits of better trainability far outweigh the costs of mildly lower capacity.\n\n* The point about the near-equivalence of capacity at equal numbers of parameters is very useful.   \n\n* The paper makes it clear the importance of HP tuning, something that has sometimes gotten lost in the vast flow of papers about new architectures.\n\n* The idea of quantifying the fraction of infeasible parameters (e.g. those that diverge) is nice, because it's a practical problem that everyone working with these networks has but often isn't addressed. \n\n* The paper text is very clearly written.\n\nCons:\n\n* The work on the UGRNNs and the +RNNs seems a bit preliminary.  I don't think that the authors have clearly shown that the +RNN should be \"recommended\" with the same generality as the GRU.   I'd at the least want some better statistics on the significance of differences between +RNN and GRU performances quantifying the results in Figure 4 (8-layer panel).   In a way the high standards for declaring an architecture useful that are set in the paper make the UGRNNs and +RNN contributions seem less important.   I don't really mind having them in the paper though.   I guess the point of this paper is not really to be novel in the first place -- which is totally fine with me, though I don't know what the ICLR area chairs will think. \n\n* The paper gives short shrift to the details of the HP algorithm itself.  They do say: \n\n     \"Our setting of the tuner\u2019s internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a   \n     Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs\"  \n\nand give some good references, but I expect that actually trying to replicate this involves a lot of missing details.   \n\n* I found some of the figures a bit hard to read at first, esp. Fig 4, mostly due to the panels being small, having a lot of details, and bad choices for visual cleanliness.   \n\n* The neuroscience reference (\"4.7 bits per synapse\") seems a little bit of a throw-away to me, because the connection between these results and the experimental neuroscience is very tenuous, or at any rate, not well explained.  I guess it's just in the discussion, but it seems gratuitous.   Maybe it should couched in slightly less strong terms (nothing is really strongly shown to be \"in agreement\" here between computational architectures and neuroscience, but perhaps they could say something like -- \"We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience.\")\n\n\n\n", "SOUNDNESS_CORRECTNESS": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "21 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review: Capacity and Trainability in Recurrent Neural Networks", "comments": "CONTRIBUTIONS\nLarge-scale experiments are used to measure the capacity and trainability of different RNN architectures. Capacity experiments suggest that across all architectures, RNNs can store between three and six bits of information per parameter, with ungated RNNs having the highest per-parameter capacity. All architectures are able to store approximately one floating point number per hidden unit. Trainability experiments show that ungated architectures (RNN, IRNN) are much harder to train than gated architectures (GRU, LSTM, UGRNN, +RNN). The paper also proposes two novel RNN architectures (UGRNN and +RNN); experiments suggest that the UGRNN has similar per-parameter capacity as the ungated RNN but is much easier to train, and that deep (8-layer) +RNN models are easier to train than existing architectures.\n\nCLARITY\nThe paper is well-written and easy to follow.\n\nNOVELTY\nThis paper is the first to my knowledge to empirically measure the number of bits of information that can be stored per learnable parameter. The idea of measuring network capacity by finding the dataset size and other hyperparameters that maximizes mutual information is a particularly novel experimental setup.\n\nThe proposed UGRNN is similar but not identical to the minimal gated unit proposed by Zhou et al, \u201cMinimal Gated Unit for Recurrent Neural Networks\u201d, International Journal of Automation and Computing, 2016.\n\nSIGNIFICANCE\nI have mixed feelings about the significance of this paper. I found the experiments interesting, but I don\u2019t feel that they reveal anything particularly surprising or unexpected about recurrent networks; it is hard to see how any of the experimental results will change the way either that I think about RNNs, or the way that I will use them in my own future work. On the other hand it is valuable to see intuitive results about RNNs confirmed by rigorous experiments, especially since few researchers have the computational resources to perform such large-scale experiments.\n\nThe capacity experiments (both per-parameter capacity and per-unit capacity) essentially force the network to model random data. For most applications of RNNs, however, we do not expect them to work with random data; instead when applied in machine translation or language modeling or image captioning or any number of real-world tasks, we hope that RNNs can learn to model data that is anything but random. It is not clear to me that an architecture\u2019s ability to model random data should be beneficial in modeling real-world data; indeed, the experiments in Section 2.1 show that architectures vary in their capacity to model random data, but the text8 experiments in Section 3 show that these same architectures do not significantly vary in their capacity to model real-world data.\n\nI do not think that the experimental results in the paper are sufficient to prove the significance of the proposed UGRNN and +RNN architectures. It is interesting that the UGRNN can achieve comparable bits per parameter as the ungated RNN and that the deep +RNNs are more easily trainable than other architectures, but the only experiments on a real-world task (language modeling on text8) do not show these architectures to be significantly better than GRU or LSTM.\n\nSUMMARY\nI wish that the experiments had revealed more surprising insights about RNNs, though there is certainly value in experimentally verifying intuitive results. The proposed UGRNN and +RNN architectures show some promising results on synthetic tasks, but I wish that they showed more convincing performance on real-world tasks. Overall I think that the good outweighs the bad, and that the ideas of this paper are of value to the community.\n\nPROS\n- The paper is the first of my knowledge to explicitly measure the bits per parameter that RNNs can store\n- The paper experimentally confirms several intuitive ideas about RNNs:\n    - RNNs of any architecture can store about one number per hidden unit from the input\n    - Different RNN architectures should be compared by their parameter count, not their hidden unit count\n    - With very careful hyperparameter tuning, all RNN architectures perform about the same on text8 language modeling\n    - Gated architectures are easier to train than non-gated RNNs\n\nCONS\n- Experiments do not reveal anything particularly surprising or unexpected\n- The UGRNN and +RNN architectures do not feel well-motivated\n- The utility of the UGRNN and +RNN architectures is not well-established", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "CLARITY": 5, "REVIEWER_CONFIDENCE": 4}, {"IMPACT": 4, "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review: Capacity and trainability in RNNs", "comments": "The authors investigate a variety of existing and two new RNN architectures to obtain more insight about the effectiveness at which these models can store task information in their parameters and activations.\n\nThe experimental setups look sound. To generalize comparisons between different architectures it\u2019s necessary to consider multiple tasks and control for the effect of the hyperparameters. This work uses multiple tasks of varying complexities, principled hyperparameter tuning methodology and a number of tuning iterations that can currently only be achieved by the computational resources of some of the larger industrial research groups. \n\nThe descriptions of the models and the objective where very clear to me. The descriptions of the experiments and presentation of the results were not always clear to me at times, even with the additional details in the appendix available. Most of these issues can easily be resolved by editing the text. For example, in the memory task the scaling of the inputs (and hence also outputs) is not provided so it\u2019s hard to interpret the squared error scores in Figure 2c. It\u2019s not clear to me what the term \u2018unrollings\u2019 refers to in Figure 2b. Is this a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output? Since the perceptron capacity task is somewhat central to the paper, I think a slightly more precise description of how and when the predictions are computed would be helpful. Due to the large number of graphs, it can be somewhat hard to find the most relevant results. Perhaps some of the more obvious findings (like Figure 1(b-d) given Figure 1a) could move to the appendix to make space for more detailed task descriptions.\n\nNovelty is not really the aim of this paper since it mostly investigates existing architectures. To use the mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is new to me. The paper also proposed to new architectures that seem to have practical value. The paper adds to the currently still somewhat neglected research effort to employ the larger computational resources we currently have towards a better understanding of architectures which were designed when such resources were not yet available. I\u2019d argue that the paper is original enough for that reason alone.\n\nThe paper provides some interesting new insights into the properties of RNNs. While observed before, it is interesting to see the importance of gated units for maintaining trainability of networks with many layers. It is also interesting to see a potential new use for vanilla RNNs for simpler tasks where a high capacity per parameter may be required due to hardware constraints. The proposed +RNN may turn out to have practical value as well and the hyperparameter robustness results shed some light on the popularity of certain architectures when limited time for HP tuning is available. The large body of results and hyperparameter analysis should be useful to many researchers who want to use RNNs in the future. All in all, I think this paper would make a valuable addition to the ICLR conference but would benefit from some improvements to the text.\n\nPros:\n* Thorough analysis.\n* Seemingly proper experiments.\n* The way of quantifying capacity in neural networks adds to the novelty of the paper.\n* The results have some practical value and suggest similar analysis of other architectures.\n* The results provide useful insights into the relative merits of different RNN architectures.\n\nCons:\n* It\u2019s hard to isolate the most important findings (some plots seem redundant).\n* Some relevant experimental details are missing.", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "15 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Insight from experiments about hyperparameters?", "comments": "", "ORIGINALITY": 2, "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "CLARITY": 5}, {"IMPACT": 4, "TITLE": "From wich range was the number of samples chosen in the capacity experiments?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_ANNOTATED": true, "IS_META_REVIEW": false, "DATE": "30 Nov 2016"}], "authors": "Jasmine Collins, Jascha Sohl-Dickstein, David Sussillo", "accepted": true, "id": "376"}