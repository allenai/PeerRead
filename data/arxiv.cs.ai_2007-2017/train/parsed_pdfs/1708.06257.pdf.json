{
  "name" : "1708.06257.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Notes: A Continuous Model of Neural Networks Part I: Residual Networks",
    "authors" : [ "Zhen Li", "Zuoqiang Shi" ],
    "emails" : [ "lishen03@gmail.com", "zqshi@tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this part, we start with a linear transport equation (with nonlinear transport velocity field) and obtain a class of residual type neural networks. If the transport velocity field has a special form, the obtained network is found similar to the original ResNet (He et al., 2016). This neural network can be regarded as a discretization of the continuous flow defined by the transport flow.\nIn the end, a summary of the correspondence between neural networks and transport equations is presented, followed by some general discussions."
    }, {
      "heading" : "1 Transport Equation",
      "text" : "Consider the following terminal value problem (TVP) for linear transport equation:{ ∂tu + v(t, x) · ∇u = 0, x ∈ Rd, t ∈ [0, T ]\nu(T, x) = f(x), x ∈ Rd. (1)\nHere v is a Rd-valued function, called the transport velocity field. It can be chosen in different ways. We will consider firstly the general form, then a special type:\nv(t, x) = W (2)(t)a ( W (1)(t) + b(1)(t) ) + b(2)(t), (2)\nwhere W (1)(t),W (2)(t) ∈ Rd×d, b(1)(t), b(2)(t) ∈ Rd. The activation a is a Rd-valued nonlinear function, which is Lipschitz continuous.\n∗Department of Mathematics, HKUST. Email: lishen03@gmail.com †Yau Mathematical Sciences Center, Tsinghua University. Email: zqshi@tsinghua.edu.cn\nar X\niv :1\n70 8.\n06 25\n7v 1\n[ cs\n.L G\n] 2\n1 A\nIt is well known that the solution of equation (1) is transported along characteristics. The characteristics are defined as solutions of the initial value problems (IVP) of the ODE: {\nẋ = v(t, x), t ∈ [0, T ] x(0) = x0,\n(3)\nwhere x0 ∈ Rd. Along the solution curve x = q(t), it is easy to verify that\nd dt u(t, q(t)) = (∂tu(t, x) + q̇(t) · ∇u(t, x))x=q(t) (4)\n= (∂tu(t, x) + v(t, q(t)) · ∇u(t, x))x=q(t) = 0. (5)\nSo u remains unchanged along the curve. See Figure 1 for a conceptual illustration. Therefore\nu(0, x0) = u(t, q(t)) = u(T, q(T )) = f(q(T )). (6)"
    }, {
      "heading" : "2 Method of Characteristics",
      "text" : "In this part we will use the method of characteristics to solve (1). In order to make the following approximations reasonable, we assume that the change of v(t, x) with t and x is regular enough. Especially, we assume that the solution of (1) and (3) exist for the posed conditions, and they are regular enough.\nLet {tk}Lk=0 with t0 = 0 and tL = T be a partition of [0, T ] ⊂ R such that for any k = 1, . . . , L, sk = tk − tk−1 is small enough. Let x = q(t) be a characteristic of the transport equation (1), i.e. the solution of (3), and denote xk = q(tk). Denote Vk(x) = v(tk, x) and uk(x) = u(tk, x) for any x ∈ R. See Figure 2 for a illustration of the discretization.\nNear time tk, the ODE (3) is approximately\nẋ = Vk(xk) ≈ Vk(xk−1). (7)\nUse Euler method to integrate this ODE from tk−1 to tk, we get xk ≈ xk−1 + ∫ tk tk−1 Vk(xk−1)dt (8)\n≈ xk−1 + skVk(xk−1) (9) = (id + skVk)(xk−1). (10)\nTherefore\nxL = (id + sLVL)(xL−1) (11)\n= (id + sLVL) ◦ · · · ◦ (id + s1V1)(x0) (12)\nIf the terminal value function of u is given as uL = f , we might be able to use (12) to get the initial value u0. According to (6),\nu0(x0) = uL(xL) = f ◦ (id + sLVL) ◦ · · · ◦ (id + s1V1)(x0) (13)"
    }, {
      "heading" : "3 Neural Network Representation",
      "text" : ""
    }, {
      "heading" : "3.1 General form",
      "text" : "The discrete solution (13) of the terminal value problem of transport equation (1) is valid for any x0 ∈ Rd. Its basic structure is shown in Figure 3. This structure reminds us of the ResNet (He et al., 2016), but it is merely a formal one. In order to see the actual structure, we need to specify the definition of Vk’s."
    }, {
      "heading" : "3.2 A Special Type",
      "text" : "In order to get a ResNet with explicit 2-layer block, consider the special type of transport velocity field given by (2). Denote\nW (1) k = W (1)(tk), b (1) k = b (1)(tk), (14) W (2) k = W (2)(tk), b (2) k = b (2)(tk). (15)\nW (2) k = skW (2) k , b (2) k = skb (2) k . (16)\nBy using the method of characteristics as before, we can get\nxk = xk−1 + W (2) k a\n( W\n(1) k xk−1 + b (1) k\n) + b\n(2) k . (17)\nIt generates a 2-layer ResNet block, which is much more like the original ResNet. 1 Figure 4 illustrates its basic structure.\nAt a first glance, it appears that directly define the transport velocity as (2) is not natural. But it is actually reasonable. The inner weights W (1) and b(1) are used to adjust the distribution of the transport velocity field according to the location of input. Notice that the activation a is usually non-negative, or even restricted to [0, 1]. Thus the outer weights W (2) and b(2) are needed to adjust the direction and magnitude of the transport velocity. Both inner weights and outer weights are necessary ingredients of the velocity field."
    }, {
      "heading" : "3.3 Discussions",
      "text" : "The ResNets obtained here are special. Firstly, as we can see in (10) and (17), due to the time step sk, the residual term can be made sufficiently small comparing with the leading term xk. This is a necessary condition for the ResNet to be modeled by transport equation.\nSecondly, the weights of the ResNet changes slowly from block to block. More specifically, the weights on the same positions of adjacent ResNet blocks should be close to each other, because they are assumed to be discretizations of continuous functions of time. For example, W (1) k is close to W (1) k−1, W (2) k is close to W (2) k−1, and so on.\n1One concern is that the layers of this ResNet are of the same width. This may not be a series problem in theory, because they can always be embedded in to the same high dimensional space."
    }, {
      "heading" : "3.4 Summary",
      "text" : "Clearly, there is a correspondence between the ResNets (10) (17) and the transport equation (1). This correspondence also exists in plain nets (neural networks without residual shortcuts). It is summarized in Table 1 and illustrated in Figure 5."
    }, {
      "heading" : "4 Discussions",
      "text" : "Claim: The discussions here are not intended to provide solution to any problem in theory and applications of neural networks, nor be able to answer any concern. They only present our ideas about some of candidate directions that could be explored.\n1. In the viewpoint of solving terminal value problem of transport equation, we can also consider other numerical methods intentionally designed for PDEs, besides of the method of characteristics. For example, we can increase regularity of solutions by adding dissipative terms to the transport equation.\n2. The training of neural networks could be considered as solving inverse problem of transport equation. It means that both initial value (on samples) and terminal value are posed. The task is to find a transport velocity field (depending on time) that transports the initial value to the terminal value. It will be made clearer in the rest parts of this series of notes.\n3. The correspondence provides one way to partially explain why deep is good for neural networks. Usually, in order to learn a good prediction map, the dataset needs to be distorted significantly. But the distortion provided by each layer is very limited. So it needs more layers to accomplish the total distortion. In the language of differential equations, deep means fine partition and small time step size. It allows the discretization to be more smooth and regular, such that each layer makes only a small progress. At the same time, the initial and terminal conditions can get a better match through the solution, hence the learnt prediction map is more accurate and can generalize better."
    }, {
      "heading" : "Acknowledgement",
      "text" : "Zhen Li would like to show his gratitude to the support of professors Yuan Yao and Yang Wang from the Department of Mathematics, HKUST."
    } ],
    "references" : [ {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "If the transport velocity field has a special form, the obtained network is found similar to the original ResNet (He et al., 2016).",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 0,
      "context" : "This structure reminds us of the ResNet (He et al., 2016), but it is merely a formal one.",
      "startOffset" : 40,
      "endOffset" : 57
    } ],
    "year" : 2017,
    "abstractText" : "In this series of notes, we try to model neural networks as as discretizations of continuous flows on the space of data, which can be called flow model. The idea comes from an observation of their similarity in mathematical structures. This conceptual analogy has not been proven useful yet, but it seems interesting to explore. In this part, we start with a linear transport equation (with nonlinear transport velocity field) and obtain a class of residual type neural networks. If the transport velocity field has a special form, the obtained network is found similar to the original ResNet (He et al., 2016). This neural network can be regarded as a discretization of the continuous flow defined by the transport flow. In the end, a summary of the correspondence between neural networks and transport equations is presented, followed by some general discussions. 1 Transport Equation Consider the following terminal value problem (TVP) for linear transport equation: { ∂tu + v(t, x) · ∇u = 0, x ∈ R, t ∈ [0, T ] u(T, x) = f(x), x ∈ R. (1) Here v is a Rd-valued function, called the transport velocity field. It can be chosen in different ways. We will consider firstly the general form, then a special type: v(t, x) = W (t)a ( W (t) + b(t) ) + b(t), (2) where W (1)(t),W (2)(t) ∈ Rd×d, b(1)(t), b(2)(t) ∈ Rd. The activation a is a Rd-valued nonlinear function, which is Lipschitz continuous. ∗Department of Mathematics, HKUST. Email: lishen03@gmail.com †Yau Mathematical Sciences Center, Tsinghua University. Email: zqshi@tsinghua.edu.cn 1 ar X iv :1 70 8. 06 25 7v 1 [ cs .L G ] 2 1 A ug 2 01 7 It is well known that the solution of equation (1) is transported along characteristics. The characteristics are defined as solutions of the initial value problems (IVP) of the ODE: { ẋ = v(t, x), t ∈ [0, T ] x(0) = x0, (3) where x0 ∈ Rd. Along the solution curve x = q(t), it is easy to verify that d dt u(t, q(t)) = (∂tu(t, x) + q̇(t) · ∇u(t, x))x=q(t) (4) = (∂tu(t, x) + v(t, q(t)) · ∇u(t, x))x=q(t) = 0. (5) So u remains unchanged along the curve. See Figure 1 for a conceptual illustration. Therefore u(0, x0) = u(t, q(t)) = u(T, q(T )) = f(q(T )). (6) Figure 1: Illustration of characteristics. Here x, u(t, x) ∈ R. 2 Method of Characteristics In this part we will use the method of characteristics to solve (1). In order to make the following approximations reasonable, we assume that the change of v(t, x) with t and x is regular enough. Especially, we assume that the solution of (1) and (3) exist for the posed conditions, and they are regular enough. Let {tk}k=0 with t0 = 0 and tL = T be a partition of [0, T ] ⊂ R such that for any k = 1, . . . , L, sk = tk − tk−1 is small enough. Let x = q(t) be a characteristic of the transport equation (1), i.e. the solution of (3), and denote xk = q(tk). Denote Vk(x) = v(tk, x) and uk(x) = u(tk, x) for any x ∈ R. See Figure 2 for a illustration of the discretization.",
    "creator" : "LaTeX with hyperref package"
  }
}