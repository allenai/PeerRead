{
  "name" : "1510.06786.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Freshman or Fresher? Quantifying the Geographic Variation of Language in Online Social Media",
    "authors" : [ "Vivek Kulkarni", "Steven Skiena" ],
    "emails" : [ "vvkulkarni@cs.stonybrook.edu}", "bperozzi@cs.stonybrook.edu}", "skiena@cs.stonybrook.edu}" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Detecting and analyzing regional variation in language is central to the field of socio-variational linguistics and dialectology (eg. [25, 31, 40, 41]). Since online content is an agglomeration of material originating from all over the world, language on the Internet demonstrates geographic variation. The abundance of geo-tagged online text enables a study of geographic linguistic variation at scales that are unattainable using classical methods like surveys and questionnaires.\nCharacterizing and detecting such variation is challenging since it takes different forms: lexical, syntactic and semantic. Most existing work has focused on detecting lexical variation\nCopyright © 2016This is the authors draft of the work. It is posted here for your personal use. Not for redistribution.\nprevalent in geographic regions [4, 13, 15, 16]. However, regional linguistic variation is not limited to lexical variation.\nIn this paper we address this gap. Our method, GEODIST, is the first computational approach for tracking and detecting statistically significant linguistic shifts of words across geographical regions. GEODIST detects syntactic and semantic variation in word usage across regions, in addition to purely lexical differences. GEODIST builds on recently introduced neural language models that learn word representations (word embeddings), extending them to capture region-specific semantics. Since observed regional variation could be due to chance, GEODIST explicitly introduces a null model to ensure detection of only statistically significant differences between regions.\nFigure 1 presents a visualization of the semantic variation captured by GEODIST for the word test between the United States, the United Kingdoms, Canada, and India. In the majority of English speaking countries, test almost always means an exam, but in India (where cricket is a popular sport) test almost always refers to a lengthy form of cricket match. One might argue that simple baseline methods like (analyzing part of speech) might be sufficient to identify regional variation. However because these methods capture different modalities, they detect different types of changes as\nar X\niv :1\n51 0.\n06 78\n6v 2\n[ cs\n.C L\n] 7\nM ar\n2 01\n6\nwe illustrate in Figure 2. We use our method in two novel ways. First, we evaluate our methods on several large datasets at multiple geographic resolutions. We investigate linguistic variation across Twitter at multiple scales: (a) between four English speaking countries and (b) between fifty states in USA. We also investigate regional variation in the Google Books Ngram Corpus data. Our methods detect a variety of changes including regional dialectical variations, region specific usages, words incorporated due to code mixing and differing semantics.\nSecond, we apply our method to analyze distances between language dialects. In order to do this, we propose a measure of semantic distance between languages. Our analysis of British and American English over a period of 100 years reveals that semantic variation between these dialects is shrinking potentially due to cultural mixing and globalization (see Figure 3).\nSpecifically, our contributions are as follows:\n• Models and Methods: We present our new method GEODIST which extends recently proposed neural language models to capture semantic differences between regions (Section 3.2). GEODIST is a new statistical method that explicitly incorporates a null model to ascertain statistical significance of observed semantic changes.\n• Multi-Resolution Analysis: We apply our method on multiple domains (Books and Tweets) across geographic scales (States and Countries). Our analysis of these large corpora (containing billions of words) reveals interesting facets of language change at multiple scales of geographic resolution – from neighboring states to distant continents (Section 5).\n• Semantic Distance: We propose a new measure of semantic distance between languages which we use to characterize distances between various dialects of English and analyze their convergent and divergent patterns over time (Section 6).\n2 Problem Definition We seek to quantify shift in word meaning (usage) across different geographic regions. Specifically, we are given a corpus C that spans R regions where Cr corresponds to the corpus specific to region r. We denote the vocabulary of the corpus by V . We want to detect words in V that have region specific semantics (not including trivial instances of words exclusively used in one region). For each region r, we capture statistical properties of a wordw’s usage in that region. Given a pair of regions (ri, rj), we then reduce the problem of detecting words that are used differently across these regions to an outlier detection problem using the statistical properties captured.\nIn summary, we answer the following questions: 1. In which regions does the word usage drastically differ\nfrom other regions? 2. How statistically significant is the difference observed\nacross regions? 3. Given two regions, how close are their corresponding di-\nalects semantically?"
    }, {
      "heading" : "3 Methods",
      "text" : "In this section we discuss methods to model regional word usage."
    }, {
      "heading" : "3.1 Baseline Methods",
      "text" : "Frequency Method. One standard method to detect which words vary across geographical regions is to track their frequency of usage. Formally, we track the change in probability of a word across regions as described in [24]. To characterize the difference in frequency usage of w between a region pair (ri, rj), we compute the ratio SCORE(w) = Pri (w)\nPrj (w) where\nPri(w) is the probability of w occurring in region ri. An example of the information we capture by tracking word frequencies over regions is shown in Figure 4. Observe that touchdown (an American football term) is used much more frequently in the US than in UK. While this naive method is easy to implement and identifies words which differ in their usage patterns, one limitation is an overemphasis on rare words. Furthermore frequency based methods overlook the fact that word usage or meaning changes are not exclusively associated with a change in frequency.\nSyntactic Method. A method to capture syntactic variation in word usage through time was proposed by [24]. Along similar lines, we can capture regional syntactic variation of words. The word lift is a striking example of such variation: In the US, lift is dominantly used as a verb (in the sense: “to lift an object”), whereas in the UK lift also refers to an elevator, thus predominantly used as a common noun. Given a word w and a pair of regions (ri, rj) we adapt the method outlined in [24] and compute the Jennsen-\nShannon Divergence between the part of speech distributions for word w corresponding to the regions.\nFigure 5 shows the part of speech distribution for a few words that differ in syntactic usage between the US and UK. In the US, remit is used primarily as a verb (as in “to remit a payment”). However in the UK, remit can refer “to an area of activity over which a particular person or group has authority, control or influence” (used as “A remit to report on medical services”)1. The word curb is used mostly as a noun (as ”I should put a curb on my drinking habits.”) in the UK but it is used dominantly as a verb in the US (as in “We must curb the rebellion.”).\nWhereas the Syntactic method captures a deeper variation than the frequency methods, it is important to observe that semantic changes in word usage are not limited to syntactic variation as we illustrated before in Figure 2."
    }, {
      "heading" : "3.2 Distributional Method: GEODIST",
      "text" : "As we noted in the previous section, linguistic variation is not restricted only to syntactic variation. In order to detect subtle semantic changes, we need to infer cues based on the contextual usage of a word. To do so, we use distributional methods which learn a latent semantic space that maps each word w ∈ V to a continuous vector space Rd.\nWe differentiate ourselves from the closest related work to our method [5], by explicitly accounting for random variation between regions, and proposing a method to detect statistically significant changes.\nLearning region specific word embeddings Given a corpus C with R regions, we seek to learn a region specific word embedding φr : V, Cr 7→ Rd using a neural language model. For each word w ∈ V the neural language model learns:\n1. A global embedding δMAIN(w) for the word ignoring all region specific cues. 2. A differential embedding δr(w) that encodes differences from the global embedding specific to region r. 1http://www.oxfordlearnersdictionaries.com/us/\ndefinition/english/remit_1\nThe region specific embedding φr(w) is computed as: φr(w) = δMAIN(w) + δr(w). Before training, the global word embeddings are randomly initialized while the differential word embeddings are initialized to 0. During each training step, the model is presented with a set of words w and the region r they are drawn from. Given a word wi, the context words are the words appearing to the left or right of wi within a window of size m. We define the set of active regions A = {r,MAIN} where MAIN is a placeholder location corresponding to the global embedding and is always included in the set of active regions. The training objective then is to maximize the probability of words appearing in the context of word wi conditioned on the active set of regions A. Specifically, we model the probability of a context word wj given wi as:\nPr(wj | wi) = exp (wTj wi)∑\nwk∈V exp (wTkwi)\n(1)\nwhere wi is defined as wi = ∑ a∈A δa(wi).\nDuring training, we iterate over each word occurrence in C to minimize the negative log-likelihood of the context words. Our objective function J is thus given by:\nJ = ∑ wi∈C i+m∑ j=i−m j!=i − log Pr(wj | wi) (2)\nWhen |V| is large, it is computationally expensive to compute the normalization factor in Equation 1 exactly. Therefore, we approximate this probability by using hierarchical soft-max [32, 34] which reduces the cost of computing the normalization factor from O(|V|) to O(log |V|). We optimize the model parameters using stochastic gradient descent [8], as φt(wi) = φt(wi)−α× ∂J∂φt(wi) where α is the learning rate. We calculate the derivatives using the back-propagation algorithm [38]. We set α = 0.025, context window size m to 10 and size of the word embedding d to be 200 unless stated otherwise.\nDistance Computation between regional embeddings After learning word embeddings for each word w ∈ V , we then compute the distance of a word between any two regions (ri, rj) as SCORE(w) = COSINEDISTANCE(φri(w), φrj (w)) where COSINEDISTANCE(u, v) is defined by 1− u\nT v ‖u‖2‖v‖2\n. Figure 6 illustrates the information captured by our\nGEODIST method as a two dimensional projection of the latent semantic space learned, for the word theatre. In the US, the British spelling theatre is typically used only to refer to the performing arts. Observe how the word theatre in the US is close to other subjects of study: sciences, literature, anthropology, but theatre as used in UK is close to places showcasing performances (like opera, studio, etc). We emphasize that these regional differences detected by GEODIST are inherently semantic, the result of a level of language understanding unattainable by methods which focus solely on lexical variation [17]."
    }, {
      "heading" : "3.3 Statistical Significance of Changes",
      "text" : "In this section, we outline our method to quantify whether an observed change given by SCORE(w) is significant. When one is operating on an entire population (or in the absence of stochastic processes), one fairly standard method to identify outliers is the Z-value test [1] (obtained by standardizing the raw scores) and marking samples whose Z-value exceeds a threshold β (typically set to the 95th percentile) as outliers.\nHowever since in our method, SCORE(w) could vary due random stochastic processes (even possibly pure chance), whether an observed score is significant or not depends on two factors: (a) the magnitude of the observed score (effect size) and (b) probability of obtaining a score more extreme than the observed score, even in the absence of a true effect.\nSpecifically, given a word w with a score E(w) = SCORE(w) between regions (ri, rj) we ask the question: “What is the chance of observing E(w) or a more extreme value assuming the absence of an effect?”\nFirst our method explicitly models the scenario when there is no effect, which we term as the null model. Next we characterize the distribution of scores under the null model. Our method then compares the observed score with this distribution of scores to ascertain the significance of the observed score. The details of our method are described in Algorithm 1 and below.\nWe simulate the null model by observing that under the null model, the labels of the text are exchangeable. Therefore, we generate a corpusC ′ by a random assignment of the labels (regions) of the given corpus C. We then learn a model using C ′ and estimate SCORE(w) under this model. By repeating this procedure B times we estimate the distribution of scores for each word under the null model (Lines 1 to 10).\nAfter we estimate the distribution of scores we then compute the 100α% confidence interval on SCORE(w) under the null model. Thus for each word w, we specify two measures: (a) observed effect size and (b) 100α% confidence interval (we typically set α = 0.95) corresponding to the null distribution (Lines 16-17). When the observed effect is not contained in the confidence interval obtained for the null distribution,\nthe effect is statistically significant at the 1− α significance level.\nEven though p-values have been traditionally used to report significance, recently researchers have argued against their use as p-values themselves do not indicate what the observed effect size was and hence even very small effects can be deemed statistically significant [14, 39]. In contrast, reporting effect sizes and confidence intervals enables us to factor in the magnitude of effect size while interpreting significance. In a nutshell therefore, we deem a change observed for w as statistically significant when:\n1. The effect size exceeds a threshold β which ensures the effect size is large enough. One typically standardizes the effect size and typically sets β to the 95th percentile (which is usually around 3). 2. It is rare to observe this effect as a result of pure chance. This is captured by our comparison to the null model and the confidence intervals computed. Figure 7 illustrates this for two words: hand and\nbuffalo. Observe that for hand, the observed score is smaller than the higher confidence interval, indicating that hand has not changed significantly. In contrast buffalo which is used differently in New York (since buffalo refers to a place in New York) has a score well above the higher confidence interval under the null model.\nAs we will also see in Section 5, the incorporation of the null model and obtaining confidence estimates enables our method to efficaciously tease out effects arising due to random chance from statistically significant effects."
    }, {
      "heading" : "4 Datasets",
      "text" : "Here we outline the details of two online datasets that we consider - Tweets from various geographic locations on Twitter and Google Books Ngram Corpus.\nAlgorithm 1 SCORESIGNIFICANCE (C, B, α) Input: C: Corpus of text with R regions, B: Number of\nbootstrap samples, α: Confidence Interval threshold Output: E: Computed effect sizes for each word w, CI:\nComputed confidence intervals for each word w // Estimate the NULL distribution.\n1: BS ← ∅ {Corpora from the NULL Distribution}. NULLSCORES(w) {Store the scores for w under null model.} 2: repeat 3: Permute the labels assigned to text of C uniformly at\nrandom to obtain corpus C ′ 4: BS ← BS ∪ C ′ 5: Learn a model N using C ′ as the text. 6: for w ∈ V do 7: Compute SCORE(w) using N . 8: Append SCORE(w) to NULLSCORES(w) 9: end for\n10: until |BS| = B // Estimate the actual observed effect and compute confidence intervals. 11: Learn a model M using C as the text. 12: for w ∈ V do 13: Compute SCORE(w) using M . 14: E(w)← SCORE(w) 15: Sort the scores in NULLSCORES(w). 16: HCI(w)← 100α percentile in NULLSCORES(w) 17: LCI(w) ← 100(1 − α) percentile in\nNULLSCORES(w) 18: CI(w)← (LCI(w),HCI(w)) 19: end for 20: return E,CI\nThe Google Books Ngram Corpus The Google Books Ngram Corpus corpus [27] contains frequencies of short phrases of text (ngrams) which were taken from books spanning eight languages over five centuries. While these ngrams vary in size from 1 − 5, we use the 5-grams in our experi-\nments. Specifically we use the Google Books Ngram Corpus corpora for American English and British English and use a random sample of 30 million ngrams for our experiments. Here, we show a sample of 5-grams along with their region:\n• drive a coach and horses (UK) • years as a football coach (US)\nWe obtained the POS Distribution of each word in the above corpora using Google Syntactic Ngrams[18, 26].\nTwitter Data This dataset consists of a sample of Tweets spanning 24 months starting from September 2011 to October 2013. Each Tweet includes the Tweet ID, Tweet and the geo-location if available. We partition these tweets by their location in two ways:\n1. States in the USA: We consider Tweets originating in the United States and group the Tweets by the state in the United States they originated from. The joint corpus consists of 7 million Tweets. 2. Countries: We consider 11 million Tweets originating from USA, UK, India (IN) and Australia (AU) and partition the Tweets among these four countries.\nSome sample Tweet text is shown below:\n• Someone come to golden with us! (CA) • Taking the subway with the kids\n...(NY)\nIn order to obtain part of speech tags, for the tweets we use the TweetNLP POS Tagger[36]."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "In this section, we apply our methods to various data sets described above to identify words that are used differently across various geographic regions. We describe the results of our experiments below."
    }, {
      "heading" : "5.1 Geographical Variation Analysis",
      "text" : "Table 1 shows words which are detected by the Frequency method. Note that zucchini is used rarely in the UK because a zucchini is referred to as a courgette in the UK. Yet another example is the word freshman which refers to a student in their first year at college in the US. However in the UK a freshman is known as a fresher. The Frequency method also detects terms that are specific to regional cultures like touchdown, an American football term and hence used very frequently in the US.\nAs we noted in Section 3.1, the Syntactic method detects words which differ in their syntactic roles. Table 2 shows words like lift, cuddle which are used as verbs in the US but predominantly as nouns in the UK. In particular lift in the UK also refers to an elevator. While in the USA, the word cracking is typically used as a verb (as in “the ice is cracking”), in the UK cracking is also used as an adjective and means “stunningly beautiful”. The Frequency method in\ncontrast would not be able to detect such syntactic variation since it focuses only on usage counts and not on syntax.\nIn Tables 3a and 3b we show several words identified by our GEODIST method. While theatre refers primarily to a building (where events are held) in the UK, in the US theatre also refers primarily to the study of the performing arts. The word extract is yet another example: extract in the US refers to food extracts but is used primarily as a verb in the UK. While in the US, the word test almost always refers to an exam, in India test has an additional meaning of a cricket match that is typically played over five days. An example usage of this meaning is “We are going to see the test match between India and Australia” or the “The test was drawn.”. We reiterate here that the GEODIST method picks up on finer distributional cues that the Syntactic or the Frequency method cannot detect. To illustrate this, observe that theatre is still used predominantly as a noun in both UK and the USA, but they differ in semantics which the Syntactic method fails to detect.\nAnother clear pattern that emerges are “code-mixed words”, which are regional language words that are incorporated into the variant of English (yet still retaining the meaning in the regional language). Examples of such words include main and hum which in India also mean “I” and “We” respectively in addition to their standard meanings. In Indian English, one can use main as “the main job is done” as well as “main free at noon. what about you?”. In the second sentence main refers to “I” and means “I am free at noon. what about you?”.\nFurthermore, we demonstrate that our method is capable of detecting changes in word meaning (usage) at finer scales (within states in a country). Table 4 shows a sample of the words in states of the USA which differ in semantic usage markedly from their overall semantics globally across the country.\nNote that the usage of buffalo significantly differs in New York as compared to the rest of the USA. buffalo typically would refer to an animal in the rest of USA, but it refers to a place named Buffalo in New York. The word queens is yet another example where people in New York almost always refer to it as a place.\nOther clear trends evident are words that are typically associated with states. Examples of such words include golden, space and twins. The word golden in California almost always refers to The golden gate bridge and space in Washington refers to The space needle. While twins in the rest of the country is dominantly associated with twin babies (or twin brothers), in the state of Minnesota, twins also refers to the state’s baseball team Minnesota Twins.\nTable 4 also illustrates the significance of incorporating the null model to detect which changes are significant. Observe how incorporating the null model renders several observed changes as being not significant thus highlighting statistically significant changes. Without incorporating the null model, one would erroneously conclude that hand has different semantic usage in several states. However on incorporating the null model, we notice that these are very likely due to random chance thus enabling us to reject this as signifying a true change.\nThese examples demonstrate the capability of our method to detect wide variety of variation across different scales of geography spanning regional differences to code-mixed words."
    }, {
      "heading" : "6 Semantic Distance",
      "text" : "In this section we investigate the following question: Are British and American English converging or diverging over time semantically?\nIn order to measure semantic distance between languages through time, we propose a measure of semantic distance between two variants of the language at a given point t. Specifically, at a given time t, we are given a corpus C and a pair of regions (ri, rj). Using our method (see Section 3.2) we compute the standardized distance Zt(w) for each word w between the regions at time point t. Then, we construct the\nintersection of the set of words W that have been deemed to have changed significantly at each time point t. We do this so that (a) we focus on only the words that were significantly different between the language dialects at time point t and (b) the words identified as different are stable across time, allowing us to track the usage of the same set of divergent words over time. Our measure of the semantic distance between the two language dialects at time t is then Semt(ri, rj) = 1|W| ∑ w∈W Zt(w), the mean of the distances of words inW . In our experiment, we considered the Google Books Ngram Corpus for UK English and US English within a time span of 1900 − 2005 using a window of 5 years. We computed the semantic distance between these dialects as described above, which we present in Figure 3. We clearly observe a trend showing both British and American English"
    }, {
      "heading" : "1900 1920 1940 1960 1980 2000",
      "text" : "are converging. Figure 8 shows one such word acts, where the usage in the UK starts converging to the usage in the US. Before the 1950’s, acts in British English was primarily used as a legal term (with ordinances, enactments, laws etc). American English on the other hand used acts to refer to actions (as in acts of vandalism, acts of sabotage). However in the 1960’s British English started adopting the American usage.\nWe hypothesize that this effect is observed due to globalization (the invention of radio, tv and the Internet), but leave a rigorous investigation of this phenomenon to future work.\nWhile our measure of semantic distance between languages does not capture lexical variation, introduction of new words etc, our work opens the door for future research to design better metrics for measuring semantic distances while also accounting for other forms of variation."
    }, {
      "heading" : "7 Related Work",
      "text" : "Most of the related work can be organized into two areas: (a) Socio-variational linguistics (b) Word embeddings\nSocio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22–24].\nWhile previous work like [7, 9, 20, 22, 23] focus on temporal analysis of language variation, our work centers on methods to detect and analyze linguistic variation according to geography. A majority of these works also either restrict themselves to two time periods or do not outline methods to detect when changes are significant. Recently [24] proposed methods to detect statistically significant linguistic change over time that hinge on timeseries analysis. Since their methods explicitly model word evolution as a time series, their methods cannot be trivially applied to detect geographical variation.\nSeveral works on geographic variation [4, 13, 15, 35] focus on lexical variation. Bamman and others [4] study lexical variation in social media like Twitter based on gender identity. Eisenstein et al. [15] describe a latent variable model to capture geographic lexical variation. Eisenstein et al. [16] outline a model to capture diffusion of lexical variation in social media. Different from these studies, our work seeks\nto identify semantic changes in word meaning (usage) not limited to lexical variation. The work that is most closely related to ours is that of Bamman, Dyer, and Smith [5]. They propose a method to obtain geographically situated word embeddings and evaluate them on a semantic similarity task that seeks to identify words accounting for geographical location. Their evaluation typically focuses on named entities that are specific to geographic regions. Our work differs in several aspects: Unlike their work which does not explicitly seek to identify which words vary in semantics across regions, we propose methods to detect and identify which words vary across regions. While our work builds on their work to learn region specific word embeddings, we differentiate our work by proposing an appropriate null model, quantifying the change and assessing its significance. Furthermore our work is unique in the fact that we evaluate our method comprehensively on multiple web-scale datasets at different scales (both at a country level and state level).\nMeasures of semantic distance have been developed for units of language (words, concepts etc) which [33] provide an excellent survey. Cooper [12] study the problem of measuring semantic distance between languages, by attempting to capture the relative difficulty of translating various pairs of languages using bi-lingual dictionaries. Different from their work, we measure semantic distance between language dialects in an unsupervised manner (using word embeddings) and also analyze convergence patterns of language dialects over time.\nWord Embeddings The concept of using distributed representations to learn a mapping from symbolic data to continuous space dates back to Hinton [21]. In a landmark paper, Bengio et al. [6] proposed a neural language model to learn word embeddings and demonstrated that they outperform traditional n-gram based models. Mikolov et al. [29] proposed Skipgram models for learning word embeddings and demonstrated that they capture fine grained structures and linguistic regularities [28, 30]. Also [37] induce language networks over word embeddings to reveal rich but varied community structure. Finally these embeddings have been demonstrated to be useful features for several NLP tasks [2, 3, 10, 11].\n8 Conclusions In this work, we proposed a new method to detect linguistic change across geographic regions. Our method explicitly accounts for random variation, quantifying not only the change but also its significance. This allows for more precise detection than previous methods.\nWe comprehensively evaluate our method on large datasets at different levels of granularity – from states in a country to countries spread across continents. Our methods are capable of detecting a rich set of changes attributed to word semantics, syntax, and code-mixing. Using our method, we are able to characterize the semantic distances between dialectical variants over time. Specifically, we are able to observe the semantic convergence between British and American English over time, potentially an effect of globalization. This promising (although preliminary) result points to exciting research directions for future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank David Bamman for sharing the code for training situated word embeddings. We thank Yingtao Tian for valuable comments."
    } ],
    "references" : [ {
      "title" : "Polyglot-ner: Massive multilingual named entity recognition",
      "author" : [ "R. Al-Rfou", "V. Kulkarni", "B. Perozzi", "S. Skiena" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Polyglot: Distributed word representations for multilingual nlp",
      "author" : [ "R. Al-Rfou", "B. Perozzi", "S. Skiena" ],
      "venue" : "In Proceedings of the Seventeenth Conference on Computational Natural Language Learning",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Gender identity and lexical variation in social media",
      "author" : [ "D Bamman" ],
      "venue" : "Journal of Sociolinguistics",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Distributed representations of geographically situated language",
      "author" : [ "D. Bamman", "C. Dyer", "N.A. Smith" ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Neural probabilistic language models",
      "author" : [ "Y. Bengio", "H. Schwenk", "J.-S. Senecal", "F. Morin", "J.-L. Gauvain" ],
      "venue" : "In Innovations in Machine Learning",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Stochastic gradient learning in neural networks",
      "author" : [ "L. Bottou" ],
      "venue" : "In Proceedings of Neuro-Nı̂mes",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1991
    }, {
      "title" : "Analyzing discourse communities with distributional semantic models",
      "author" : [ "I. Brigadir", "D. Greene", "P. Cunningham" ],
      "venue" : "In ACM Web Science",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "The expressive power of word embeddings",
      "author" : [ "Y. Chen", "B. Perozzi", "R. Al-Rfou", "S. Skiena" ],
      "venue" : "arXiv preprint arXiv:1301.3226",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Natural language processing (almost) from scratch. JMLR",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Measuring the semantic distance between languages from a statistical analysis of bilingual dictionaries",
      "author" : [ "M.C. Cooper" ],
      "venue" : "Journal of Quantitative Linguistics",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Mapping dialectal variation by querying social media",
      "author" : [ "G. Doyle" ],
      "venue" : "In EACL",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Confidence interval or p-value?: part 4 of a series on evaluation of scientific publications. Deutsches Ärzteblatt International",
      "author" : [ "J.-B. du Prel", "G. Hommel", "B. Röhrig", "M. Blettner" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "A latent variable model for geographic lexical variation",
      "author" : [ "J. Eisenstein", "B. O’Connor", "N.A. Smith", "E.P. Xing" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Diffusion of lexical change in social media",
      "author" : [ "J. Eisenstein", "B. O’Connor", "N.A. Smith", "E.P. Xing" ],
      "venue" : "PLoS ONE",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Discovering sociolinguistic associations with structured sparsity",
      "author" : [ "J. Eisenstein", "N. A Smith" ],
      "venue" : "ACL-HLT",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "A dataset of syntacticngrams over time from a very large corpus of english books",
      "author" : [ "Y. Goldberg", "J. Orwant" ],
      "venue" : "*SEM",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Crowdsourcing dialect characterization through twitter",
      "author" : [ "B. Gonçalves", "D. Sánchez" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "A distributional similarity approach to the detection of semantic change in the google books ngram corpus. In GEMS",
      "author" : [ "K. Gulordava", "M. Baroni" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Learning distributed representations of concepts",
      "author" : [ "G.E. Hinton" ],
      "venue" : "In Proceedings of the eighth annual conference of the cognitive science society",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1986
    }, {
      "title" : "Ad hoc monitoring of vocabulary shifts over time",
      "author" : [ "T. Kenter", "M. Wevers", "P Huijnen" ],
      "venue" : "In CIKM. ACM",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Temporal analysis of language through neural language models",
      "author" : [ "Y. Kim", "Y.-I. Chiu", "K. Hanaki", "D. Hegde", "S. Petrov" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Statistically significant detection of linguistic change",
      "author" : [ "V. Kulkarni", "R. Al-Rfou", "B. Perozzi", "S. Skiena" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Locating language in time and space / edited by William Labov",
      "author" : [ "W. Labov" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1980
    }, {
      "title" : "Syntactic annotations for the google books ngram corpus",
      "author" : [ "Y. Lin", "J.-B. Michel", "E. L Aiden" ],
      "venue" : "In Proceedings of the ACL 2012 system demonstrations",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Quantitative analysis of culture using millions of digitized books",
      "author" : [ "Michel", "J.-B" ],
      "venue" : "Science",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "T Mikolov" ],
      "venue" : "In Proceedings of NAACL-HLT",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2013
    }, {
      "title" : "Linguistic variation and change: on the historical sociolinguistics of English",
      "author" : [ "J. Milroy" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1992
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "A. Mnih", "G.E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2009
    }, {
      "title" : "Distributional measures of semantic distance: A survey",
      "author" : [ "S.M. Mohammad", "G. Hirst" ],
      "venue" : "arXiv preprint arXiv:1203.1858",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2012
    }, {
      "title" : "Hierarchical probabilistic neural network language model",
      "author" : [ "F. Morin", "Y. Bengio" ],
      "venue" : "In Proceedings of the international workshop on artificial intelligence and statistics",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2005
    }, {
      "title" : "Discovering demographic language variation",
      "author" : [ "B. O’Connor", "J. Eisenstein", "E.P. Xing", "N.A. Smith" ],
      "venue" : "In Proc. of NIPS Workshop on Machine Learning for Social Computing",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "Improved part-ofspeech tagging for online conversational text with word clusters. Association for Computational Linguistics",
      "author" : [ "O. Owoputi", "B O’Connor" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2013
    }, {
      "title" : "Inducing language networks from continuous space word representations",
      "author" : [ "B. Perozzi", "R Al-Rfou" ],
      "venue" : "In Complex Networks V",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2014
    }, {
      "title" : "Learning representations by back-propagating errors. Cognitive modeling 1:213",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : null,
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2002
    }, {
      "title" : "Using effect size-or why the p value is not enough",
      "author" : [ "G.M. Sullivan", "R. Feinn" ],
      "venue" : "Journal of graduate medical education",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2012
    }, {
      "title" : "Analysing Sociolinguistic Variation",
      "author" : [ "S.A. Tagliamonte" ],
      "venue" : null,
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "[25, 31, 40, 41]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 28,
      "context" : "[25, 31, 40, 41]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 37,
      "context" : "[25, 31, 40, 41]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "prevalent in geographic regions [4, 13, 15, 16].",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "prevalent in geographic regions [4, 13, 15, 16].",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : "prevalent in geographic regions [4, 13, 15, 16].",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "prevalent in geographic regions [4, 13, 15, 16].",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 21,
      "context" : "Formally, we track the change in probability of a word across regions as described in [24].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 21,
      "context" : "A method to capture syntactic variation in word usage through time was proposed by [24].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "Given a word w and a pair of regions (ri, rj) we adapt the method outlined in [24] and compute the JennsenremitUK remitUS curbUK curbUS wadUK wadUS 0.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "We differentiate ourselves from the closest related work to our method [5], by explicitly accounting for random variation between regions, and proposing a method to detect statistically significant changes.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 29,
      "context" : "Therefore, we approximate this probability by using hierarchical soft-max [32, 34] which reduces the cost of computing the normalization factor from O(|V|) to O(log |V|).",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "Therefore, we approximate this probability by using hierarchical soft-max [32, 34] which reduces the cost of computing the normalization factor from O(|V|) to O(log |V|).",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "We optimize the model parameters using stochastic gradient descent [8], as φt(wi) = φt(wi)−α× ∂J ∂φt(wi) where α is the learning rate.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 35,
      "context" : "We calculate the derivatives using the back-propagation algorithm [38].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "We emphasize that these regional differences detected by GEODIST are inherently semantic, the result of a level of language understanding unattainable by methods which focus solely on lexical variation [17].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 11,
      "context" : "Even though p-values have been traditionally used to report significance, recently researchers have argued against their use as p-values themselves do not indicate what the observed effect size was and hence even very small effects can be deemed statistically significant [14, 39].",
      "startOffset" : 272,
      "endOffset" : 280
    }, {
      "referenceID" : 36,
      "context" : "Even though p-values have been traditionally used to report significance, recently researchers have argued against their use as p-values themselves do not indicate what the observed effect size was and hence even very small effects can be deemed statistically significant [14, 39].",
      "startOffset" : 272,
      "endOffset" : 280
    }, {
      "referenceID" : 24,
      "context" : "The Google Books Ngram Corpus The Google Books Ngram Corpus corpus [27] contains frequencies of short phrases of text (ngrams) which were taken from books spanning eight languages over five centuries.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "We obtained the POS Distribution of each word in the above corpora using Google Syntactic Ngrams[18, 26].",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "We obtained the POS Distribution of each word in the above corpora using Google Syntactic Ngrams[18, 26].",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 33,
      "context" : "In order to obtain part of speech tags, for the tweets we use the TweetNLP POS Tagger[36].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22–24].",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22–24].",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22–24].",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22–24].",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22–24].",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22–24].",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22–24].",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 21,
      "context" : "Socio-variational linguistics A large body of work studies how language varies according to geography and time [4, 5, 15, 17, 19, 22–24].",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "While previous work like [7, 9, 20, 22, 23] focus on temporal analysis of language variation, our work centers on methods to detect and analyze linguistic variation according to geography.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "While previous work like [7, 9, 20, 22, 23] focus on temporal analysis of language variation, our work centers on methods to detect and analyze linguistic variation according to geography.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "While previous work like [7, 9, 20, 22, 23] focus on temporal analysis of language variation, our work centers on methods to detect and analyze linguistic variation according to geography.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : "While previous work like [7, 9, 20, 22, 23] focus on temporal analysis of language variation, our work centers on methods to detect and analyze linguistic variation according to geography.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : "Recently [24] proposed methods to detect statistically significant linguistic change over time that hinge on timeseries analysis.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 2,
      "context" : "Several works on geographic variation [4, 13, 15, 35] focus on lexical variation.",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "Several works on geographic variation [4, 13, 15, 35] focus on lexical variation.",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "Several works on geographic variation [4, 13, 15, 35] focus on lexical variation.",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 32,
      "context" : "Several works on geographic variation [4, 13, 15, 35] focus on lexical variation.",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "Bamman and others [4] study lexical variation in social media like Twitter based on gender identity.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "[15] describe a latent variable model to capture geographic lexical variation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[16] outline a model to capture diffusion of lexical variation in social media.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "The work that is most closely related to ours is that of Bamman, Dyer, and Smith [5].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 30,
      "context" : "Measures of semantic distance have been developed for units of language (words, concepts etc) which [33] provide an excellent survey.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "Cooper [12] study the problem of measuring semantic distance between languages, by attempting to capture the relative difficulty of translating various pairs of languages using bi-lingual dictionaries.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 18,
      "context" : "Word Embeddings The concept of using distributed representations to learn a mapping from symbolic data to continuous space dates back to Hinton [21].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "[6] proposed a neural language model to learn word embeddings and demonstrated that they outperform traditional n-gram based models.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 26,
      "context" : "[29] proposed Skipgram models for learning word embeddings and demonstrated that they capture fine grained structures and linguistic regularities [28, 30].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[29] proposed Skipgram models for learning word embeddings and demonstrated that they capture fine grained structures and linguistic regularities [28, 30].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 27,
      "context" : "[29] proposed Skipgram models for learning word embeddings and demonstrated that they capture fine grained structures and linguistic regularities [28, 30].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 34,
      "context" : "Also [37] induce language networks over word embeddings to reveal rich but varied community structure.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "Finally these embeddings have been demonstrated to be useful features for several NLP tasks [2, 3, 10, 11].",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "Finally these embeddings have been demonstrated to be useful features for several NLP tasks [2, 3, 10, 11].",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "Finally these embeddings have been demonstrated to be useful features for several NLP tasks [2, 3, 10, 11].",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "Finally these embeddings have been demonstrated to be useful features for several NLP tasks [2, 3, 10, 11].",
      "startOffset" : 92,
      "endOffset" : 106
    } ],
    "year" : 2016,
    "abstractText" : "In this paper we present a new computational technique to detect and analyze statistically significant geographic variation in language. While previous approaches have primarily focused on lexical variation between regions, our method identifies words that demonstrate semantic and syntactic variation as well. Our meta-analysis approach captures statistical properties of word usage across geographical regions and uses statistical methods to identify significant changes specific to regions. We extend recently developed techniques for neural language models to learn word representations which capture differing semantics across geographical regions. In order to quantify this variation and ensure robust detection of true regional differences, we formulate a null model to determine whether observed changes are statistically significant. Our method is the first such approach to explicitly account for random variation due to chance while detecting regional variation in word meaning. To validate our model, we study and analyze two different massive online data sets: millions of tweets from Twitter spanning not only four different countries but also fifty states, as well as millions of phrases contained in the Google Book Ngrams. Our analysis reveals interesting facets of language change at multiple scales of geographic resolution – from neighboring states to distant continents. Finally, using our model, we propose a measure of semantic distance between languages. Our analysis of British and American English over a period of 100 years reveals that semantic variation between these dialects is shrinking.",
    "creator" : "LaTeX with hyperref package"
  }
}