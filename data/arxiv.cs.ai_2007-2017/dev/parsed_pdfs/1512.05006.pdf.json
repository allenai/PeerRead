{
  "name" : "1512.05006.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "BayesDB: A probabilistic programming system for querying the probable implications of data",
    "authors" : [ "Vikash Mansinghka", "Richard Tibbetts", "Jay Baxter" ],
    "emails" : [ "vkm@mit.edu", "tibbetts@mit.edu", "jbaxter@mit.edu", "p.shafto@louisville.edu", "b0eave01@louisville.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Acknowledgements: VKM would like to thank Alexey Radul, Feras Saad, and Taylor Campbell for helpful discussions and contributions to a prototype implementation. This research was supported by DARPA (under the XDATA and PPAML programs), IARPA (under research contract 2015-15061000003), the Office of Naval Research (under research contract N000141310333), the Army Research Office (under agreement number W911NF-131-0212), the Bill & Melinda Gates Foundation, and gifts from Analog Devices and Google."
    }, {
      "heading" : "1. Introduction",
      "text" : "Is it possible to make statistical inference broadly accessible to non-statisticians without sacrificing mathematical rigor or inference quality? This paper describes BayesDB, a system that enables users to query the probable implications of their data as directly as SQL\nar X\niv :1\n51 2.\n05 00\n6v 1\n[ cs\n.A I]\ndatabases enable them to query the data itself. By combining ordinary SQL with three new primitives — SIMULATE, INFER, and ESTIMATE — users of BayesDB can detect predictive relationships between variables, retrieve statistically similar data items, identify anomalous data points and variables, infer missing values, and synthesize hypothetical subpopulations. The default modeling assumptions that BayesDB makes are suitable for a broad class of problems (Mansinghka et al., 2015; Wasserman, 2011), but statisticians can customize these assumptions when necessary. BayesDB also enables domain experts that lack statistical expertise to perform qualitative model checking (Gelman et al., 1995) and encode simple forms of qualitative prior knowledge.\nBayesDB consists of four components, integrated into a single probabilistic programming system:\n1. The Bayesian Query Language (BQL), an SQL-like query language for Bayesian data analysis. BQL programs can solve a broad class of data analysis problems using statistically rigorous formulations of cleaning, exploration, confirmatory analysis, and predictive modeling. BQL defines these primitive operations for these workflows in terms of Bayesian model averaging over results from an implicit set of multivariate probabilistic models.\n2. A mathematical interface that enables a broad class of multivariate probabilistic models, called generative population models, to be used to implement BQL. According to this interface, a data generating process defined over a fixed set of variables is represented by (i) an infinite array of random realizations of the process, including any observed data, and (ii) algorithms for simulating from arbitrary conditional distributions and calculating arbitrary conditional densities. This interface permits many statistical operations to be implemented once, independent of the specific models that will be used to apply these operations in the context of a particular data table.\n3. The BayesDB Meta-modeling Language (MML), a minimal probabilistic programming language. MML includes constructs that enable statisticians to integrate custom statistical models — including arbitrary algorithmic models contained in external software — with the output of a broad class of Bayesian model building techniques. MML also includes constructs for specifying qualitative dependence and independence constraints.\n4. A hierarchical, semi-parametric Bayesian “meta-model” that automatically builds ensembles of generalized mixture models from database tables. These ensembles serve as baseline data generators that BQL can use for data cleaning, initial exploration, and other routine applications.\nThis design insulates end users from most statistical considerations. Queries are posed in a qualitative probabilistic programming language for Bayesian data analysis that hides the details of probabilistic modeling and inference. Baseline models can be built automatically and customized by statisticians when necessary. All models can be critically assessed and qualitatively validated via predictive checks that compare synthetic rows (generated via BQL’s SIMULATE operation) with rows from the original data. Instead of hypothesis testing, dependencies between variables are obtained via Bayesian model selection.\nBayesDB is “Bayesian” in two ways:\n1. In BQL, the objects of inference are rows, and the underlying probability model forms a “prior” probability distribution on the fields of these rows. This is then constrained by row-specific observations to create a posterior distribution of field values. Without this prior, it would be impossible to simulate rows or infer missing values from partial observations.\n2. In MML, the default meta-model is Bayesian in that it assigns a prior probability to a very broad class of probabilistic models and narrows down on probable models via Bayesian inference. This prior is unusual in that it encodes a state of ignorance rather than a strong inductive constraint. MML also provides instructions for augmenting this prior to incorporate qualitative and quantitative domain knowledge.\nIn practice, it is useful to use BQL for Bayesian queries against models built using nonBayesian or only partially Bayesian techniques. For example, MML supports composing the default meta-model with modeling techniques specified in external code that need not be Bayesian. However, the default is to be Bayesian for both model building and query interpretation, as this ensures the broadest applicability of the results.\nThis paper focuses on the technical details of BQL, the data generator interface, the meta-model, and the MML. It also illustrates the capabilities of BayesDB using three applications: cleaning and exploring a public database of Earth satellites, discovering relationships in measurements of macroeconomic development of countries, and analyzing salary survey data. Empirical results are based on a prototype implementation that embeds BQL into sqlite3, a lightweight, open-source, in-memory database."
    }, {
      "heading" : "1.1 A conceptual illustration",
      "text" : "This section illustrates data analysis using the MML and BQL on a synthetic example based on analysis of electronic health records. SQL databases make it easy to load data from disk and run queries that filter and retrieve the contents. The first step in using BayesDB is to load data that describes a statistical (sub)population into a table, with one row per member of the population, and one column per variable:\nCREATE POPULATION patients WITH DATA FROM patients.csv;\nSELECT age, has_heart_disease FROM patients WHERE age > 30 LIMIT 3;\nage has_heart_disease 66 ??? 44 yes 31 ???\nOnce data has been loaded, a population schema needs to be specified. This schema specifies the statistical characteristics of each example. For example, whether it is categorical or numerical, and if it is categorical, how many outcomes are there and how is each outcome represented. After an initial schema has been specified — using a mix of automatic inference\nand manual specification — the schema can be customized using instructions in the Metamodeling Language (MML).\nGUESS POPULATION SCHEMA FOR patients;\nALTER POPULATION SCHEMA FOR patients SET DATATYPE FOR num_hosp_visits TO COUNT;\nCREATE DEFAULT METAMODEL FOR patients; ALTER METAMODEL FOR patients ENSURE will_readmit DEPENDENT ON dialysis;\nALTER METAMODEL FOR patients MODEL infarction GIVEN gender, age, weight, height, cholesterol, bp USING CUSTOM MODEL FROM infarction_regression.py;\nOne distinctive feature of MML is that it includes instructions for qualitative probabilistic programming. These instructions control the behavior of the automatic modeling machinery in the MML runtime. In this example, these constraints include the assertion of a dependence between the presence of a chronic kidney condition and future hospital readmissions. They also include the specification of a custom statistical model for the infarction variable, illustrating one way that discriminative and non-probabilistic approaches to inference can be integrated into BayesDB.\nThe next step is to use the MML to build an ensemble of general-purpose models for the data, subject to the specified constraints:\nINITIALIZE 100 MODELS FOR patients; ANALYZE patients FOR 3 HOURS CHECKPOINT EVERY 10 MINUTES;\nEach of these 100 models is a generative population model (GPM) that represents the joint distribution on all possible measurements of an infinite population with the given population schema. These models are initially drawn accordingt to a broad prior probability distribution over a large hypothesis space of possible GPMs. Until the observed data has been analyzed, BQL will thus report broad uncertainty for all its query responses.\nOnce the models are sufficiently adapted to the data, it is possible to query its probable implications. The following query quantifies over columns, rather than rows, and retrieves the probability of a marginal dependence between three (arbitrary) variables and height:\nESTIMATE COLUMN NAME, PROBABILITY OF DEPENDENCE WITH height FROM COLUMNS OF patients LIMIT 3;\ncolumn name p( dep. with height ) height 1.0\ninfarction 0.08 gender 0.99\nPoint predictions can be accessed by using the INFER instruction, a natural generalization of SELECT from SQL:\nINFER age, has_heart_disease FROM patients WHERE age > 30 WITH CONFIDENCE 0.8 LIMIT 3;\nage has_heart_disease 66 yes 44 yes 31 ???\nIn this example, only one of the missing values could be inferred with the specified confidence level. The probabilistic semantics of CONFIDENCE will be discussed later in this paper.\nBQL also makes it straightforward to generate synthetic sub-populations subject to a broad class of constraints:\nSIMULATE height, weight, blood_pressure FROM patients 3 TIMES GIVEN gender = male AND age < 10\nheight weight blood_pressure 46 80 110 38 60 80 39 119 120\nThe SIMULATE operator gives BQL users access to samples from the posterior predictive distribution induced by the implicit underlying set of models. This is directly useful for predictive modeling and also decision-theoretic choice implemented using Monte Carlo estimation of expected utility (Russell and Norvig, 2003). It also enables predictive checking: samples from SIMULATE can be compared to the results returned by SELECT. Finally, domain experts can use SIMULATE to scrutinize the implications of the underlying model ensemble, both quantitatively and qualitatively."
    }, {
      "heading" : "2. Example Analyses",
      "text" : "This section describes three applications of the current BayesDB prototype:\n1. Exploring and cleaning a public database of Earth satellites.\n2. Assessing the evidence for dependencies between indicators of global poverty\n3. Analyzing data from a salary survey.\nBQL and MML constructs are introduced via real-world uses; a discussion of their formal interpretation is provided in later sections."
    }, {
      "heading" : "2.1 Exploring and cleaning a public database of Earth satellites",
      "text" : "The Union of Concerned Scientists maintains a database of 1000 Earth satellites. For the majority of satellites, it includes kinematic, material, electrical, political, functional, and economic characteristics, such as dry mass, launch date, orbit type, country of operator, and purpose. Here we show a sequence of interactions with a snapshot of this database using the bayeslite implementation of BayesDB."
    }, {
      "heading" : "2.1.1 Inspecting the data.",
      "text" : "We start by loading the data and looking at a sample. This process uses a combination of ordinary SQL and convenience functions built into bayeslite. The first step is to create a population from the raw data:\nCREATE POPULATION satellites FROM ucs_database.csv\nOne natural query is to find the International Space Station, a well-known satellite:\nSELECT * FROM satellites WHERE Name LIKE ’International Space Station%’\nVariable Value Name International Space Station (ISS ...) Country_of_Operator Multinational Operator_Owner NASA/Multinational Users Government Purpose Scientific Research Class_of_Orbit LEO Type_of_Orbit Intermediate Perigee_km 401 Apogee_km 422 Eccentricity 0.00155 Period_minutes 92.8 Launch_Mass_kg NaN Dry_Mass_kg NaN Power_watts NaN Date_of_Launch 36119 Anticipated_Lifetime 30 Contractor Boeing Satellite Systems (prime)/Multinational Country_of_Contractor Multinational Launch_Site Baikonur Cosmodrome Launch_Vehicle Proton Source_Used_for_Orbital_Data www.satellitedebris.net 12/12 longitude_radians_of_geo NaN Inclination_radians 0.9005899\nThis result row illustrates typical characteristics of real-world databases such as heterogeneous data types and missing values."
    }, {
      "heading" : "2.1.2 Building baseline models.",
      "text" : "Before exploring the implications of the data, it is necessary to obtain a collection of probabilistic models. The next two MML instructions produce a collection of 16 models, using roughly 4 minutes of analysis total.\nINITIALIZE 16 MODELS FOR satellites; ANALYZE satellites FOR 4 MINUTES WAIT;\nEach of the 16 models is a separate GPM produced by an independent Markov chain for approximate posterior sampling in the default semi-parametric factorial mixture metamodel described earlier. This number of models and amount of computation is typical for the exploratory analyses done with our prototype implementation; this is sufficient for roughly 100 full sweeps of all latent variables."
    }, {
      "heading" : "2.1.3 Answering hypotheticals.",
      "text" : "The satellites database should in principle inform the answers to a broad class of hypothetical or “what if?” questions. For example, consider the following question:\nSuppose you receive a report indicating the presence of a previously undetected satellite in geosynchronous orbit with a dry mass of 500 kilograms. What countries are most likely to have launched it, and what are its likely purposes?\nAnswering this question requires knowledge of satellite engineering, orbital mechanics, and the geopolitics of the satellite industry. It is straightforward to answer this question using BQL. The key step is to generate a synthetic population of satellites that reflect the given constraints:\nSIMULATE country_of_operator, purpose FROM satellites GIVEN Class_of_orbit = GEO, Dry_mass_kg = 500 LIMIT 1000;\nFigure 1 shows the results of a simple aggregation of these results, counting the marginal frequencies of various countries and purposes and sorting accordingly. The most probable explanation, carrying roughly 25% of the probability mass, is that it is a communications satellite launched by the USA. It is also plausible that it might have been launched other major space powers such as Russia or China, and that it might have a military purpose.\nThe satellites data are too sparse and ambiguous for frequency counting to be a viable alternative. Consider an approach based on finding satellites that match the discrete GEO constraint and are within some ad-hoc tolerance around the observed dry mass:\nSELECT country_of_operator, purpose, Class_of_orbit, Dry_mass_kg FROM satellites WHERE Class_of_orbit = \"GEO\" AND Dry_Mass_kg BETWEEN 400 AND 600;\nThis SQL query returns just 2 satellites, both Indian:\nCountry_of_Operator Purpose Class_of_Orbit Dry_Mass_kg 0 India Communications GEO 559 1 India Meteorology GEO 500\nPresuming our intuition about satellite mass is flawed, we might issue another query to look at a broader range of satellites:\nSELECT country_of_operator, purpose, Class_of_orbit, Dry_mass_kg FROM satellites WHERE Class_of_orbit = ’GEO’ AND Dry_Mass_kg BETWEEN 300 AND 700\nThe results still do not give any real insight into the likely purpose of this satellite:\nCountry_of_Operator Purpose Class_of_Orbit Dry_Mass_kg 0 Malaysia Communications GEO 650 1 Israel Communications GEO 646 2 Luxembourg Communications GEO 700 3 Russia Communications GEO 620 4 China (PR) Earth Science GEO 620 5 China (PR) Earth Science GEO 620 6 China (PR) Earth Science GEO 620 7 India Communications GEO 559 8 India Navigation GEO 614 9 India Meteorology GEO 500 10 Malaysia Communications GEO 650 11 Multinational Earth Science/Meteorology GEO 320 12 United Kingdom Communications GEO 660 13 Norway Communications GEO 646\nWithout deep expertise in satellites, and significant expertise in statistics, it is difficult to know whether or not these results can be trusted. How does the set of satellites vary as the thresholds on Dry_Mass_kg are adjusted? How locally representative and comprehensive is the coverage afforded by the data? Are there indirect, multivariate dependencies that ought to be taken into account, to determine which satellites are most similar? How should existing satellites be weighted to make an appropriate weighted sample against which to calculate frequencies? In fact, small modifications to the tolerance on Dry_Mass_kg yield large changes in the result set."
    }, {
      "heading" : "2.1.4 Identifying predictive relationships between variables.",
      "text" : "A key exploratory task is to identify those variables in the database that appear to predict one another. This is closely related to the key confirmatory analysis question of assessing the evidence for a predictive relationship between any two particular variables.\nTo quantify the evidence for (or against) a predictive relationship between two pairs of variables, BQL relies on information theory. The notion of dependence between two variables A and B is taken to be mutual information; the amount of evidence for dependence is then the probability that the mutual information between A and B is nonzero. If the population models are obtained by posterior inference in a meta-model — as is the case with MML — then this probability approximates the posterior probability (or strength of evidence) that the mutual information is nonzero.\nESTIMATE DEPENDENCE PROBABILITY FROM PAIRWISE COLUMNS OF satellites;\nFigure 2 shows the results from this query. There are several groups of variables with high probability of mutual interdependence. For example, we see a definite block of geopolitically related variables, such as the country of contractor & operator, the contractor’s identity,\nand the location of the satellite (if it is in geosynchronous orbit). The kinematic variables describing orbits, such as perigee, apogee, period, and orbit class, are also shown as strongly interdependent. A domain expert with sufficiently confident domain knowledge can use this overview of the predictive relationships to assess the value of the data and the validity of the baseline models.\nIt is also instructive to compare the heatmap of pairwise dependence probabilities with alternatives from statistics. Figure 2 also shows heatmap that results from datatypeappropriate measures of correlation. The results from correlation are sufficiently noisy that it would be difficult to trust inferences from techniques that use correlation to select variables. Furthermore, the most causally unambiguous relationships, such as the kinematic constraints relating perigee, apogee, and orbital period, not detected by correlation."
    }, {
      "heading" : "2.1.5 Detecting multivariate anomalies.",
      "text" : "Another key aspect of exploratory analysis is identifying anomalous values, including both (univariate) outliers and multivariate anomalies. Anomalies can arise due to errors in data acquisition, bugs in upstream preprocessing software (including binning of continuous variables or translating between different discrete outcomes), and runtime failures. Anomalies can also arise due to genuine surprises or changes in the external environment.\nUsing BQL, multivariate anomalies can be detected by assessing the predictive probability density of each measurement, and ordering from least to most probable. Here we illustrate this using a simple example: ordering the geosynchronous satellites according to the probability of their recorded orbital period:\nESTIMATE name, class_of_orbit, period_minutes AS TAU, PREDICTIVE PROBABILITY OF period_minutes AS \"Pr[TAU]\" FROM satellites ORDER BY “Pr[TAU]” ASCENDING LIMIT 10\nThis BQL query produces the following table of results: Name Class_of_Orbit TAU Pr[TAU]\n0 AEHF-3 (Advanced Extremely High Frequency sate... GEO 1306.29 0.001279 1 AEHF-2 (Advanced Extremely High Frequency sate... GEO 1306.29 0.001292 2 DSP 20 (USA 149) (Defense Support Program) GEO 142.08 0.002656 3 Intelsat 903 GEO 1436.16 0.003239 4 BSAT-3B GEO 1365.61 0.003440 5 Intelsat 902 GEO 1436.10 0.003492 6 SDS III-6 (Satellite Data System) NRO L-27, Gr... GEO 14.36 0.003811 7 Advanced Orion 6 (NRO L-15, USA 237) GEO 23.94 0.003938 8 SDS III-7 (Satellite Data System) NRO L-38, Dr... GEO 23.94 0.003938 9 QZS-1 (Quazi-Zenith Satellite System, Michibiki) GEO 1436.00 0.004446\nRecall that a geosynchronous orbit should take 24 hours or 1440 minutes. Rows 7 and 8 appear to be unit conversion errors (hours rather than minutes). Rows 2 and 6 appear to be decimal placement errors. Note that row 2 is not an outlier: some satellites have an orbital\nperiod of roughly two hours. It is only anomalous in the context of the other variables that are probably predictive of orbital period, such as orbit class."
    }, {
      "heading" : "2.1.6 Inferring missing values.",
      "text" : "A key application of predictive modeling is inferring point predictions for missing measurements. This can be necessary for cleaning data before downstream processing. It can also be of intrinsic interest, e.g. in classification problems. The satellites database has many missing values. Here we show an INFER query that infers missing orbit types and returns both a point estimate and the confidence in that point estimate:\nINFER EXPLICIT anticipated_lifetime, perigee_km, period_minutes, class_of_orbit, PREDICT type_of_orbit AS inferred_orbit_type CONFIDENCE inferred_orbit_type_conf FROM satellites WHERE type_of_orbit IS NULL;\nThis form of INFER uses the EXPLICIT modifier that exposes both predicted values and their associated confidence levels to be included in the output. Figure 3 shows a visualization of the results. The panel on the bottom left shows that the confidence depends on the orbit class and on the predicted value for the inferred orbit type. For example, there is typically moderate to high confidence for the orbit type of LEO satellites — and high confidence (but some variability in confidence) for those with Sun-Synchronous orbits. Satellites with Elliptical orbits may be assigned a Sun-Synchronous type with moderate confidence, but for other target labels confidence is generally lower. After examining the overall distribution on confidences, it can be natural to filter INFER results based on a manually specified confidence threshold. Note that many standard techniques for imputation from statistics correspond to INFER ... WITH CONFIDENCE 0."
    }, {
      "heading" : "2.1.7 Integrating a kinematic model for elliptical orbits.",
      "text" : "Can we improve over the baseline models by integrating causal knowledge about satellites? MML can be used to compose GPMs built by the default model builder with algorithmic and/or statistical models specified as external software. Here we integrate a simple model for elliptical orbits:\nALTER METAMODEL FOR satellites MODEL perigee_km, apogee_km GIVEN period_minutes, eccentricity USING CUSTOM MODEL FROM stochastic_kepler.py; ANALYZE FOREIGN PREDICTORS FOR 1 MINUTE;\nThe underlying foreign predictor implements Kepler’s laws:\nRmin = τ 2 3 (1.0− )−RGEO\nRmax = τ 2 3 (1.0 + )−RGEO\nX(r∗,apogee_km) ∼ N(Rmin, σ2) X(r∗,perigee_km) ∼ N(Rmax, σ2)\nHere, is set via the eccentricity measurement for row r∗, and τ is set via the period_minutes measurement. RGEO is a fixed constant inside the foreign predictor representing the radius of the Earth in kilometers. σ is a parameter that determines the noise and is set when the variable subset for the foreign predictor instantiation is ANALYZEd. Note that foreign predictors are essentially GPMs and accordingly must implement generic simulate(...) and logpdf(...) methods. For algorithmic forward models with numerical outputs, MML provides a default wrapper that uses importance sampling with resampling to approximately generate conditional samples and estimate marginal densities. This is how Kepler’s laws — in the form of a forward simulation — are turned into a generative model for kinematic variables that can be conditionally simulated in arbitrary directions.\nFigure 4 and Figure 5 show the results. A detailed discussion of the relative merits of empirical versus analytical modeling is beyond the scope of this paper. However, it is clear that neither the empirical approach nor the analytical approach is universally dominant. The empirical approach is able to correctly locate the empirical probability mass — including multiple modes — but underfits. The orbital mechanics approach yields inferences that are typically in much tighter accord with the kinematic data. This is unsurprising: these are the patterns of covariation that led to the development of quantitative models and “hard” natural sciences. However, there are many satellites for which Kepler’s laws are not in accord with the data. This reflects many factors, including data quality errors as well as legitimate gaps between idealized mathematical laws and fine-grained empirical records of real-world phenomena. For example, at the time Kepler’s laws were formulated, orbiting bodies lacked engines."
    }, {
      "heading" : "2.1.8 Combing random forests, causal models, and nonparametric Bayes.",
      "text" : "Because MML supports model composition, it is straightforward to build hybrid models that integrate techniques from subfields of machine learning that might seem to be in conflict. Figure 6 shows the transcript of a complete MML session that builds such a hybrid model. Random forests are used to classify orbits into types; Kepler’s Laws are used to relate period, perigee, and apogee; and the default semi-parametric Bayesian meta-model is used for all remaining variables (with two variables coming with overridden datatypes).\nLater sections of this paper explain how these three modeling approaches are combined to answer individual BQL queries."
    }, {
      "heading" : "2.2 Assessing the evidence for dependencies between indicators of global poverty",
      "text" : "In the early 21st century it is widely believed that resolving extreme poverty around the world will be accomplished by empowering individuals to resolve their poverty. Governments and NGOs encourage this process through a variety of interventions, many of them combining material assistance with policy changes. In principle, policies should be driven by quantitative data-driven understanding of international economic development. In practice, international economic data is sparse, unreliable, and highly aggregated. These data limitations create substantial obstacles to understanding the situational context of successful or unsuccessful interventions and policies.\nSIMULATE period_minutes, apogee_km FROM satellites_kepler LIMIT 100;\nSIMULATE perigee_km, apogee_km FROM satellites_kepler ASSUMING period_minutes = 1436 LIMIT 100;\n25000 30000 35000 40000 45000 50000 Apogee [km]\n25000\n30000\n35000\n40000\nPe ri\nge e\n[k m\n]\nSimulated and Observed (Apogee, Perigee) Given Period = 1436 Eccentricity [-1,1] Orbital Model Simulation Default Model Simulation Emperical Data\nResiduals of Derived Orbital Statistics From Simulated Apogee, Perigee\nThe “Gapminder” data set, collected and curated by Hans Rosling at the Karolinska Institutet, is the most well known and extensive sets of longitudinal global developmental indicators. Representing over 500 indicators, 400 countries, and 500 years of data, it covers the colonial era, industrial revolution, socio-political upheavals around the world in the 20th century, and the first decade of the 21st. Containing over 2 million observations, the data has been used as the basis for a compelling set of data animations and the most widely viewed TED talk on statistics.\nTo date, analysis of this data has been minimal, as it requires intensive preprocessing and cleaning. Different analytical methdologies require different approaches to imputation and variable selection; as a result, results from different teams are difficult to compare. Here we show how to explore the data with BayesDB and assess the evidence for predictive relationships between different macroeconomic measures of development."
    }, {
      "heading" : "2.2.1 Exploring the Data with SQL",
      "text" : "The raw form of the data is ∼500 Excel spreadsheets, each containing longitudinal data for ∼300 countries over ∼100 years. However, the dataset only contains ∼2 million observations, i.e. 97% of the data is missing. Figure 7 shows key indicators of the data around size, missing records, and the relationship between data availability and countries, records, and years. The primary data is mmodeled in SQL as a âĂĲfactâĂİ table structure. This relatively-normalized representation easily models the sparse matrix and allows us to use a combination of SQL and Python data science tools to craft our population structures.\nThe histograms in Figure 7 show the breadth and also the variability of the data. The histogram by year in Figure 7a shows that data is complete for only recent history, and in fact that some predicted data continues into the future, and that data for some indicators is only available every 10 years. The histogram by country in Figure 7a shows that data availability varies by country (the most described country is Sweden), that many countries have reasonably complete data, but that there is a long tail of countries with sparse data, including countries that no longer exist and with inconsistent or disputed naming. Figure 7c shows that there is also a variance by indicators, because different measurements are collected by different agencies with different expectations and data policies.\nThe data has already been subject to extensive visualization and descriptive analytics by the Gapminder project. This paper focuses on the use of MML to model the data and BQL to query its probable implications."
    }, {
      "heading" : "2.2.2 Detecting Basic and Longitudinal Dependence",
      "text" : "Our analysis focuses on the 53 variables with most complete data for the years 1999-2008. It is straightforward to create an ensemble of models for this subset:\nGUESS POPULATION SCHEMA FOR dense_gapminder; INITIALIZE 64 MODELS FOR dense_gapminder; ANALYZE todo FOR 300 MINUTES WAIT;\nThe probability of dependence heatmap that results is shown in Figure 8. Indicators such as total population and urban percentage form blocks containing their values for all 10 years contained in the dataset. This shows that the default GPM was able to extract the\ntemporal dependence in these indicators. In other cases, such as measurements of the number of people killed in floods, the year to year dependence is much weaker. The heatmap also shows dependence between indicators, such as the block in the top right corner combining indicators of stress, urbanization, and fertility rate. Finally, it segregates data according to type sof indicators, as can be seen in Figure 8b where there is a sharp break from total measurements to per-capital.\nIf we analyze just the data for the year 2002, using 32 models for 3 minutes, we get a heatmap that highlights the dependence and independence between indicators. Figure 9 shows the details."
    }, {
      "heading" : "2.2.3 Measuring the Similarity of Countries",
      "text" : "In order to help with the delivery of international aid and the design and analysis of interventions, decision makers often want a richer understanding of the similarities between countries. With BQL we can formulate these queries in general or against specific attributes. Figure 10 shows country similarities for different indicators. As expected, changing the indicator of interest can produce a very different similarity structure. Analyses that presume a single global similarity measure cannot pick up this context-specific structure.\nThe authors are involved in an ongoing research partnership with the Bill and Melinda Gates Foundation aimed at integrating the Gapminder data with other relevant sources, including qualitative knowledge from domain eperts, and using it to drive empirically grounded policy and aid interventions."
    }, {
      "heading" : "2.3 Analyzing a salary survey",
      "text" : "Surveys are a common source of multivariate data and a potentially appealing application for BayesDB. Here we show a preliminary analysis of a web-administered anonymous salary survey. Participants shared their compensation details along with information about their title, years of service, acheivements, employer, and geography."
    }, {
      "heading" : "2.4 Controlling Models with Qualitative Assumptions",
      "text" : "This salary population provides an instructive example of applying qualitiative assumptions to a model. In this case, the first analysis of compensation data finds that geographic location (state, region) is not a factor in compensation. Domain experts suggest that is implausible, that cost of living and the competitive market in different cities is a significant factor in compensation of the survey participants. The following code can be used to apply this qualitative assumption:\nALTER METAMODEL FOR salary ENSURE total, equity, base, bonus DEPENDENT"
    }, {
      "heading" : "ON state;",
      "text" : "Without asserting the dependence, state is inferred to be dependent on region and independent of performance. After asserting a qualitative constraint, the probability of dependence heat map changes. Not only are the squares implied by that depencence colored to 1.0,\nbut other columns have re-aligned in their modeling. In particular, given this assumption, there appears to be more evidence of dependence between the 2012 and 2013 measures and core indicators such as years in the job, bonus, the presence of an equity stake, etc. Also, there appears to be less evidence that job title impacts the key compensation variables."
    }, {
      "heading" : "3. The Bayesian Query Language",
      "text" : "The Bayesian Query Language (BQL) formalizes Bayesian data analysis without exposing the end user to model parameters, priors, and posteriors. This section describes the statistical operations that are implemented by the core BQL instruction set. It also describes the modeling formalism that is used to implement BQL."
    }, {
      "heading" : "3.1 Generative Population Models",
      "text" : "BQL programs are executed against a weighted collection of generative population models (GPMs). At present, GPMs can be built in two ways:\n1. Specified directly as external software libraries.\n2. Inferred from data via probabilistic inference in a meta-model written in BayesDB’s Meta-modeling Language.\nGPMs can respond to queries about the joint distribution of the underlying data generating process as a whole or about the predictive distribution for a specific member of the population. The population can be thought of as a table, where individual members are specified by row indexes.\nEach GPM induces a random table with a finite number of columns and an infinite number of rows, where each cell contains a random variable. BQL treats each BayesDB generator as a model of the data generating process underlying its associated table of observations. It is sometimes useful to query a GPM about hypothetical members of the population. This can be performed by using a row whose index r∗ may not be associated with any actual member; this can be guaranteed by generating a unique row index.\nEach GPM is described by a schema S that must be compatible with the population schema for the population to which it is being applied. This schema is a tuple containing (typed-outputs, typed-inputs, body). The typed-outputs component specifies the column indexes and statistical types of each column that the data generator will be responsible for producing. The typed-inputs component specifies the indexes and statistical types of each column that the data generator can read from. The body is an opaque binary that contains any GPM-specific configuration information, such as a probabilistic program.\nMathematically, the internals of a GPM G = (Θ,Z, O) consists of three parts:\n1. Measurement-specific latent variables Z = ∪z(r,c).\nThere may be overlap between the latent variables for different measurements. If a GPM cannot track dependencies internally — or if it is based on a model class in which all measurements are coupled — then z(r,c) = Z.\n2. Population-level latent variables Θ.\nThese are all latent variables that remain well-defined in the absence of all measurements. Examples include hyper-parameters and mixture component parameters.\n3. Observations O = {(ri, ci, x(ri,ci))}. These correspond to the observed measurements.\nFor example, a naive Bayesian GPM lacks any measurement-specific variables, i.e. z(r,c) = ∅, and is completely characterized by a single vector of parameters Θ = ~θc for the probability models for each column. A finite mixture GPM would have z(r,c) = {zr} be the cluster assignment for each row, and have Θ = {θ(c,l)|l ∈ Z} be the component model parameters for each cluster.\nGenerative population models are required to satisfy the following conditional independence constraint:\nx(r,c)|Θ, z(r,c) x(r′,c′)|Θ, z(r′,c′) unless (r, c) = (r′, c′) Note in particular that the observations O need not be conditioned on directly, given Θ and z(r,c). This formalizes the requirement that the dependencies between the measurements in the population are completely mediated by the population-level latent variables and all relevant measurement-specific latent variables. In general, no other independence constraints are enforced by the interface. GPMs can thus be built around dense, highly-coupled model families such as low-dimensional latent spaces and convolutional neural networks."
    }, {
      "heading" : "3.1.1 An interface to generative population models",
      "text" : "A GPM must implement the following interface:\n1. G = {Θ,Z} = initialize( schema = S ) Initialize a data generator with the given schema and return the resulting data generator G. It ensures that storage has been allocated for the random variables Θ and Z, storing the global latent variables and the local latent variables, respectively.\n2. ~si = simulate(G, givens = {(rj , cj , x(r,cj))}, targets = {rk, ck}, N) Generate N sampled values {~si} from the specified distribution:\n{~si} ∼ {X(rk,ck)}|{X(rj ,cj) = x(rj ,cj)},Θ, {zr,c|(r, c) ∈ {rj , cj} ∪ {rk, ck}}\nThe set of valid distributions includes all finite-dimensional joint distributions obtainable by conditioning on the values of arbitrary measurements and marginalizing over another arbitrary set.\n3. log p = logpdf(G, givens = {(rj , cj , x(rj ,cj))}, query = {(rk, ck, q(r,ck))}) Evaluate the log probability density of the specified conditional/marginal distribution at a target point:\nlog p = log p({X(rk,ck) = q(rk,ck)}|{X(rj ,cj) = x(rj ,cj)},Θ, {zr,c|(r, c) ∈ {rj , cj}∪{rk, ck}})\n4. d= kl-divergence-given-G(G, measurements_A = {(ri, cai )}, measurements_B = {(rj , cbj)}, conditions_C = {(rk, cck, xk)})\nThis estimates the KL divergence of the set of measurements A from the set of measurements B, conditioned on the given constraints C. KL calculations are central to model-independent data analysis. For example, to detect predictive relationships, it suffices to check for non-zero mutual information, which can be reduced to calculating the KL between the joint distribution over two variables and the product of the marginals.\nIt is included in the GPM interface because that allows a GPM implementer to supply an optimized implementation. Where such an implementation is not available, the KL can be estimated via simple Monte Carlo estimation:\nDGKL({X(ri,cai )}, {X(rj ,cbj)}) = ∑\n{xi}∈dom({X(ri,cai )})\np ( {X(ri,cai )} = {xi} |G ) log p ( {X(rj ,cbj)} = {xi} |G ) p ( {X(ri,cai )} = {xi} |G ) \n≈ ∑ {xi}k log\n p ( {X(rj ,cbj)} = {xi} k |G )\np ( {X(ri,cai )} = {xi} k) |G ) \nwith{xi}k ∼ {X(ri,cai )}\nThis interface is intentionally quite general. It needs to support an open set of primitives for Bayesian data analysis. This paper focuses on the subset of this interface where all measurements come from the same row. All the BQL operations used in this paper can be reduced to explicit invocations of simulate, logpdf , and to Monte Carlo estimates of Kullback-Leibler divergences implemented in terms them. Some GPMs can significantly optimize some of these operations relative to Monte Carlo baselines; such optimizations are likely to be important in practice but are beyond the scope of this paper."
    }, {
      "heading" : "3.1.2 Weighted collections of generative population models.",
      "text" : "BQL is executed against a weighted collection of GPMsM:\nM = {(wi,Gi)}\nIn principle, these collections can include GPMs drawn from different model classes. The weights are treated as prior probabilities. This paper focuses on the case where the GPMs come from a single meta-model, each produced by independent runs of a single Markov chain for posterior inference in the meta-model given all available measurements. In this case, assigning unit weights to all models wi = 1 results in BQL queries based on a Monte Carlo approximation to Bayesian model averaging.\n3.2 Core instructions: SIMULATE, ESTIMATE, and INFER\nData analysis workflows in BQL are built around three core classes of statistical operations:\n1. Generating samples from predictive probability distributions, including both completions of existing rows in a data table as well as predictive distributions over hypothetical rows.\n2. Estimating predictive probability densities and approximating derived informationtheoretic quantities.\n3. Summarizing multi-modal probability distributions with single values.\nThese capabilities are exposed via three basic extensions to SQL that each combine results from individual GPMs in different ways. They can be composed with ordinary SQL to solve a broad range of data analysis tasks:\n1. Detecting predictive relationships between variables: ESTIMATE COLUMN PROBABILITY OF DEPENDENCE WITH ...\nThis yields an estimate of the marginal probability of dependence between the specified columns. This is equivalent to the probability that the mutual information between those two variables is nonzero, integrating over the weighted collection of GPMs that BayesDB maintains. If the GPMs are produced by an asymptotically consistent estimator of the joint distribution, then these probabilities will reflect non-linear, heteroscedastic, or context-specific dependencies that statistical aggregates (such as correlation or linear regression coefficients) will not.\n2. Regression, classification, semi-supervised learning, and imputation: INFER ...\nEach of these predictive modeling tasks requires filling in point estimates in different conditions. All of these can be viewed as special cases of INFER, which handles arbitrary patterns of missing values and both continuous and discrete prediction targets.\n3. Anomaly/outlier detection: ORDER BY PROBABILITY OF col ASCENDING LIMIT k\nAnomalous cells can be found by predictive checking: identify the cells that are least likely under the inferred constellation of models. These may not be outliers in the standard univariate sense: the low probability may be due to interactions between several variables, even though each variable on its own is marginally typical.\n4. Retrieving similar rows: ORDER BY SIMILARITY TO row\nA broad class of structured search operations can be performed via informationtheoretic measures of similarity between rows. These are useful in both data exploration and in more targeted search.\n5. Predictive model checking: SIMULATE ...\nBy comparing aggregates from the output of SIMULATE to the output of the analogous SELECT statements, it is possible to do predictive checking without having to mention models, parameters, priors, or posteriors.\n3.2.1 SIMULATE: generating samples from arbitrary predictive distributions.\nThe first, called SIMULATE, provides a flexible interface to sampling from posterior predictive distributions:\nSIMULATE target columns FROM population [WHERE row filter] [ASSUMING constraint] [k TIMES]\nThe WHERE clause is interpreted as a constraint to test against all members of the population that have been observed so far. If it is not supplied, the SIMULATE command is executed against an arbitrary as-yet-unobserved member of the population, i.e. a unique row id from the standpoint of the GPM interface. The ASSUME clause is interpreted as an additional set of constraints to condition each row on before generating the simulations.\nFor example, to generate a proxy dataset of two variables varA and varB, one can write SIMULATE varA, varB FROM population 100 TIMES. As another example, consider the BQL command SIMULATE varA, varD FROM population 20 TIMES WHERE varB = True AND varC IS MISSING ASSUMING varC = 3.4. This generates 20 simulated values from p(varA, varD|varC = 3.4) for each member of the population where varB is equal to True and varC is missing. This behavior may seem non-intuitive. For example, a SIMULATE invocation with WHERE true returns Rk rows, where R is the number of rows in the database and k is the number of output samples specified with the query. On the other hand, WHERE false yields an empty result set, always. However, this semantics allows SQL aggregates to reduce the predictions for individual source rows by grouping on the row identifiers.\nTo formally describe the meaning of simulate, we first introduce some notation. Let w({x(r,c)|c ∈ G}) be the predicate denoted by the WHERE clause, i.e. w(·) = 1 if the predicate is satisfied and 0 otherwise. Let R be the set of rows for which there is at least one measurement, i.e. R = {ri|(ri, ·, ·) ∈ O}, and let W = {ri|w({x(ri,c)|c ∈ G}) = 1} be the set of rows that satisfy the WHERE clause’s filter. If a WHERE clause is not provided, then w({x(r,c)|c ∈ G}) = 0 for all existing rows r ∈ R, and W = {r∗} be a set containing a single distinguished row about which no measurements are known. Let T = {ci} be the set of target columns, and let A = (cj , x(r,cj)) be the set of assumed equality constraints. Also let TA = T ∪ {c|(c, ·) ∈ A} be the set of all columns referenced in the SIMULATE command.\nFor each r∗ ∈ W , the SIMULATE primitive produces a set of k returned realizations Sr∗ = {si} of the following generative process:\nGi ∼ Discrete({Gj);wj}) si ∼ {Xr∗,c|c ∈ T}|{Xr∗,c′ = xc′ |(c′, xc′) ∈ A},Θi, {zi(r∗,cm)|cm ∈ TA}\nThis corresponds to choosing a GPM at random according to the probabilities given by their weights and then generating si from the conditioned distribution in that model. If the models are equally weighted, i.e. wi = 1, and if all the GPMs are drawn from their posterior distribution given the observations p(G|O), then this procedure implements sampling from the Bayesian posterior predictive distribution over the targets given all the observed data plus the additional constraints from the ASSUME clause:\np({Xr∗,c|c ∈ T}|{Xr∗,c′ = xc′ |(c′, xc′) ∈ A}, O) ∝ p({Xr∗,c|c ∈ T}|{Xr∗,c′ = xc′ |(c′, xc′) ∈ A}|G)p(G|O)\n3.2.2 ESTIMATE: approximating posterior averages.\nThe second core BQL primitive, called ESTIMATE, allows clients to query the posterior expectations of stochastic functions that are defined over the rows and the columns:\nESTIMATE target properties FROM [COLUMNS OF] table [WHERE row/col filter]\nRow-wise estimands provided by BQL. Consider the case where the rows are being queried, i.e. COLUMNS OF does not occur in the query. Let P = {fi(x(r,ci),G)} be the set of properties whose values are requested. These properties can depend on observed measurements as well as latent components of the GPM. Let w(·) implement the WHERE clause’s filter, as with SIMULATE. If a WHERE clause is not provided, then w({x(r,c)|c ∈ G}) = 0 for all existing rows r ∈ R.\nGiven these definitions, each row in the output of this class of ESTIMATE invocations is defined as follows: {ei} with ei = ∑ k wkfi(x(r,ci),Gk)\nThe total set of returned rows is defined by the where clause:\n{{ei}r} for r ∈W = {ri|w({x(ri,c)|c ∈ G}) = 1}\n1. log p = predictive-probability(G, row = r, col = c) This estimand is denoted PREDICTIVE PROBABILITY OF col, and applied against an implicitly specified row, thus picking out a single measurement in the population. It can be implemented by delegation to the underlying GPM:\npredictive-probability(G, r, c)) = logpdf(G, ∅, {(r, c, x(rj ,cj) from OG)})\nThis can be used to identify outliers — measurements that are unlikely under their marginal distribution — as well as anomalous measurements that are marginally likely but unlikely given the other measurements for the same row.\n2. sim(a,b) = generative-similarity(G, context = {ci}, rowA = ra, rowB = rb) Data analysts frequently want to retrieve rows from a table that are “statistically similar” to some pre-existing or hypothetical row. This is a key problem in data exploration. It is also useful when trying to explain surprising inference results or when trying to diagnose and repair data or inference quality issues. Many machine learning techniques treat similarity as a central primitive, and use a metric formulation of similarity as the basis for inductive generalization.\nInformation theory provides appealing alternatives: measure similarity in terms of the amount of information one row contains about the values in another. This can be assessed against all variables or just against a “context” that is defined by a particular subset of variables. One approach, leading to a directional measure, is to measure the divergence of the distribution over values in one row from the distribution over values in another:\nPr[DGKL(~xG,ra |{ci}||~xG,rb |{ci}) = 0]\nColumn-wise estimands provided by BQL. Another use of ESTIMATE is to query properties of the columns, via ESTIMATE ... FROM COLUMNS OF .... Let r∗ be a distinguished row about which no measurements are known, i.e. (r∗, ·, ·) /∈ O. Let gi(x(r∗,c),G) be a function of a set of measurements from a fresh row and the underlying GPM. It is then straightforward to define the set G of values needed to check the WHERE filter, the set of columns Cs that satisfy the filter, and the set E of returned values containing all the target expressions for each satisfying column.\nG = {gc(x(r∗,c), {x(r,c)|(r, c, ·) ∈ O},Gk)|c ∈ Gk} Cs = {g|g ∈ G and w(g) = 1} E = ∪t {gt(x(r∗,c), {(x(r,c)|(r, c, ·) ∈ O}, }k)|c ∈ Cs}\n1. p = marginal-dependence-prob(G, colA = ci, colB = cj) This estimand characterizes the amount of evidence for the existence of a predictive relationship between the pair of variables ci and cj . It is defined according to the information-theoretic definition of conditional independence:\nPr[X(r∗,ci) X(r∗,cj)] = ∑ G Pr[I(X(r∗,ci);X(r∗,cj)) = 0|G]Pr[G]\nIf each weighted GPM Gk is sampled approximately from some Bayesian posterior Pr[G|O] (and wk = 1 identically), then simple Monte Carlo estimation of the marginal dependence probability yields an estimate of the posterior marginal dependence probability:\nPr[X(r∗,ci) X(r∗,cj)|O]\n2. b = mutual-information(G, colA = ci, colB = cj) The mutual information between two columns can be estimated by the standard reduction to KL divergence (Cover and Thomas, 2012). This complements the marginal dependence probability, providing one measure of the strength of a dependence.\nFor convenience, some of the quantities that are ordinarily accessed via ESTIMATE are also made available via SELECT.\n3.2.3 INFER: summarizing distributions with point estimates.\nPredictive modeling applications sometimes require access to point predictions rather than samples from predictive distributions. BQL provides these capabilities using the INFER primitive. The difference between INFER and SELECT is that INFER incorporates automatic implicit imputation from the underlying collection of GPMs, plus filtering based on userspecified confidence thresholds. For simplicity, this paper describes a simplified version with a single threshold:\nINFER target columns FROM table [WHERE row filter] WITH CONFIDENCE confidence level\nThis operation returns a set of measurements {xinf(r,c)} where unobserved measurements are filled in with point estimates x̂(r,c) if a prescribed confidence threshold p(conf(X(r,c) = x̂(r,c)) ≥ q) is reached. More formally:\nxinf(r,c) =  x(r,c) foreach (r, c, ·) ∈ O x̂(r,c) foreach (r, c, ·) /∈ O and p(conf(X(r,c) = x̂(r,c)) ≥ q) null otherwise\nFor discrete measurements, BQL implements conf(·) in terms of predictive probability:\nconf(X(r,c) = x̂(r,c)) = p(X(r,c) = x̂(r,c)) = ∑ G p(X(r,c) = x̂(r,c)|G)p(G) = ∑ G p(X(r,c) = x̂(r,c)|G)wi\nOptimal candidate estimates can be found by optimization, implemented via enumeration:\nx̂(r,c) = arg max x p(X(r,c) = x)\nFor continuous measurements, there is no canonical definition of confidence that applies to all GPMs. Here we define conf(X(r,c) = x) = q as the probability that there is a useful unimodal summary of the distribution of X(r,c) that captures at least 100q percent of the predictive probability mass. This can be formalized in terms of mixture modeling. Let φl be the parameters of mixture component l; for continuous data, we will use Gaussian component models, so φl = (µl, σl). Let πl be the mass associated with component l. We will choose conf(·) and x̂ as follows:\n{(φl, πl)} ∼ p({(φl, πl)}|{Xk(r,c)}) for 0 ≤ k ≤ K +\nl∗ = arg max l πl\nX̂(r,c) = µl∗\nconf(X(r,c) = x̂(r,c)) = πl∗\nNote that this approach can recover the behavior of the chosen strategy for discrete data by using component models that place all their probability mass on single values. The current prototype implementation of BayesDB uses a standard Gibbs sampler for a Dirichlet process mixture model (Mansinghka et al., 2015; Neal, 1998; Rasmussen, 2000) to sample {(φl, πl)}|{Xk(r,c)}, with K\n+ = 1000 by default. Adjusting K+ and the amount of inference done in this mixture model can yield a broad class of tradeoffs between time, accuracy, and variance; the current values are chosen for simplicity."
    }, {
      "heading" : "3.3 Model and data independence",
      "text" : "Relational databases revolutionized the processing and analysis of business data by enabling a single centrally managed data base to shared by multiple applications and also shared between operational and analytic workloads. This in turn accelerated the development of high\nperformance and efficient databases, because the common abstraction became a target for researchers and industrial practitioners looking to build high performance system software with a broad impact. The relational model enabled sharing and infrastructure reuse because interactions with the data, queries, are expressed in a notation (most popularly SQL) that is independent of the physical representation of the data (Codd, 1970). Without this independence, physical data layout must be carefully tailored to particular workloads, specialized code written to manipulate the layout, and these data formats and access methods cannot easily be shared.\nBayesDB aims to provide additional abstraction barriers that insulate clients from the statistical underpinnings of data analysis. Clients need to be able to specify data analysis steps and workflows in a notation that is independent of the models and runtime inference strategies used to implement individual primitives, and (where possible) the modeling strategies used to produce models from the original data.\nRecall that the complete persistent state of a single population in BayesDB is characterized by two mathematical objects:\n1. The complete set of observed measurements O = {(ri, ci, x(ri,ci))}.\n2. The weighted collection of GPMs {(wk,Gk)}. Note that this notation makes no commitment as to the content of the GPMs, the weights, or the procedures by which they were obtained.\nThe independencies provided by BayesDB can be described in terms of these objects:\n1. Physical data independence. The notation for O makes no commitment as to the physical representation of the measurements. The definitions of BQL primitives given above therefore do not depend on details of the data representation to define their values. However, as with SQL, small changes in representation may yield large changes in runtime performance.\n2. Physical model independence. The notation for each (wk,Gk) makes no commitment as to the specific probability distribution induced over the set of random measurements X = {X(ri,ci)}. The definitions of BQL primitives given above therefore do not depend on the details of the probabilistic models used to define the random result set for each query. In principle, the mathematical properties of the models as well as their software implementation (or even implementing platform) can be changed without invalidating end user queries. However, small changes in the generative population model may yield large changes in the results of simulate and logpdf .\nDatabases provide other finer-grained independence properties that may have useful analogs in BayesDB. For example, let us partition the random variables induced by a given GPM into two subsets XA = {X(rai ,cai )} and X\nB = {X(rbj ,cbj)}. An example of a desirable data-dependent independence property is that if XA|O XB|O in the “true” GPM, then Q|XA, XB = Q|XA in any inferred models. Informally, this rests on the model-building strategy: if the model-builder recovers the correct independencies, then the independence of query results follows. This can be thought of as an analogue of logical data independence,\nwhich stipulates that e.g. adding new features should not affect the behavior of existing applications whose results do not depend on the value of these new features. Formalizing and verifying these properties is an important challenge for future research."
    }, {
      "heading" : "4. Modeling with the Meta-Modeling Language",
      "text" : "BayesDB also provides the Meta-Modeling Language (MML), a probabilistic programming language for building models of data tables. MML programs consist of modeling tactics that control the behavior of an automatic model-building engine. These tactics take several forms: statistical datatypes; initialization of weighted collections of random models; approximately Bayesian updating of the model collection; qualitative assertions about dependence and independence between variables; and the use of custom statistical models for specific conditional distributions. All these tactics are currently implemented in terms of a unifying semi-parametric Bayesian model that fills in all unspecified aspects."
    }, {
      "heading" : "4.1 Statistical datatypes",
      "text" : "This metadata constrains the probability models that will be used for each column of data and can also be used to choose appropriate visualizations. It is straightforward to support several different kinds of data:\n1. Categorical values from a closed set. This datatype includes a dictionary that maps the raw data values (often strings) to canonical numerical indexes for efficient storage and processing. This information can also be used to inform modeling tactics. For example, in the current version of MML, closed-set categorical variables are modeled generatively via a multinomial component model with a symmetric Dirichlet prior on the parameters (Mansinghka et al., 2015). Discriminative models for closed-set categorical columns could potentially use a multinomial logit link function, or an appropriate multi-class classification scheme.\n2. Binary data. Data of this type is generatively modeled using an asymmetric BetaBernoulli model (Mansinghka et al., 2015) that can better handle sparse or marginally biased variables than a symmetric alternative. Also, a broad class of discriminative learning techniques can natively handle the binary classification problems induced by binary variables.\n3. Count data. Non-negative counts can be naturally modeled generatively by a PoissonGamma model or discriminatively by a GLM with the appropriate link function.\n4. Numerical data. By default, data of this type is generatively modeled using a standard Normal-Gamma model. It is straightforward to add numerical ranges to enforce truncation post-hoc, and to add numerical pre-transformations that are appropriate for data that is naturally viewed as normal only on a log scale.\nWe have performed preliminary experiments on other datatypes built on standard statistical models. For example, cyclic data can be handled via a von Mises model (Gopal and Yang, 2014). Many other datatypes can be handled by the appropriate generalized linear\nmodel (McCullagh and Nelder, 1989). Broadening the set of primitive data types and assessing coverage on a representative corpus of real-world databases will be a key research challenge going forwards."
    }, {
      "heading" : "4.2 Bayesian generative population meta-models",
      "text" : "Some data generators can be learned from data. Often the learning mechanism will be based on approximate probabilistic inference in a meta-model: a probabilistic model defined over a space of data generators, each of which is also a probabilistic model in its own right. Thus far, all BayesDB meta-models have been Markov chain meta-models. These meta-models internally maintain a single sample from an approximate posterior, and provide a Markov chain transition operator that updates this sample stochastically.\n1. G = (θ0G ,XG) = initialize(meta-schema = Λ)\nInitializes a new meta-model with arbitrary parameters and an associated tabular data store.\n2. incorporate(id = r, values = {(cj , x(r,cj))})\nCreates a new member of the population with the given row index and values and stores it. Errors result from duplicate indexes or variables cj whose values x(r,cj) are not compatible with the meta-schema Λ (e.g. because the expected data type is incompatible with a provided value).\n3. remove(id = r)\nRemoves a member of the population from the data store.\n4. infer(program = P)\nSimulate an internal Markov chain transition operator T to improve the quality of the current sampled model representation:\nθi+1G = T (θ i G)\nSome Markov chain meta-models are asymptotically Bayesian, i.e. the distribution that results from sequences of T updates converges to the posterior over meta-models as T goes to infinity:\nlim t→∞\nDKL(p(θG |XG)||p(T t(θG))) = 0\nA sufficiently expressive Markov chain meta-model may also be asymptotically consistent in the usual sense. The default semi-parametric GPM provided by BayesDB is designed to be both asymptotically consistent and asymptotically Bayesian; these invariants are crucial for its robustness, broad applicability, and suitability for use by non-experts. Formally specifying and validating these properties is an important challenge for future research."
    }, {
      "heading" : "4.2.1 Controlling inference via INITIALIZE and ANALYZE.",
      "text" : "The MML allows users to control the process by which models are created and updated to reflect the data. These capabilities are exposed via two commands:\n1. INITIALIZE k MODELS FOR population\nThis command creates models by sampling their structure and parameters from the underlying GPM’s prior. This is implemented by delegation to initialize(Λ) where Λ is the entire MML schema so far.\n2. ANALYZE [variable subset OF] population FOR timelimit\nThis command performs approximately Bayesian updating of the models in the weighted collection by delegating to the infer() procedure from the underlying GPM. Here is a typical invocation:\nANALYZE my_population FOR 10 MINUTES\nWhen no variable subset is provided, analysis is done on all the latent variables associated with every GPM in the weighted collection. Finer-grained control is also possible using variable subset specifiers that pick out particular portions of the latent state in the GPM; these details are beyond the scope of this paper."
    }, {
      "heading" : "4.3 Qualitative constraints",
      "text" : "The BayesDB MML provides constructs for specifying qualitative constraints on the dependence and independence relationships (Pearl, 1988). The model-building engine attempts to enforce them in all GPMs1. These constraints are specified as follows:\nALTER METAMODEL FOR population ENSURE colA IS [NOT] MARGINALLY DEPENDENT"
    }, {
      "heading" : "ON colB",
      "text" : "It is also possible to INITIALIZE and ANALYZE models that do not respect the constraints, and then enforce them after the fact:\nALTER MODELS FOR population ENSURE colA IS [NOT] MARGINALLY DEPENDENT"
    }, {
      "heading" : "ON colB",
      "text" : "These commands enable domain experts to apply qualitative knowledge to make better use of sparse data. This can be crucial for improving analysis and model credibility in the eyes of domain experts. They also create the opportunity for false or unjustified knowledge to influence the results of analysis. This can reduce credibility in the eyes of statisticians or domain skeptics who want to see all assumptions in an analysis scrutinized empirically.\n1. The current implementation does not attempt to detect contradictions."
    }, {
      "heading" : "4.4 Incorporating foreign statistical models",
      "text" : "A crucial aspect of MML is that it permits experts to override the automatic model-building machinery using custom-built statistical models. Feedforward networks of such models can be specified as follows:\nALTER SCHEMA FOR population MODEL output variables GIVEN input variables USING FOREIGN PREDICTOR FROM source file\nPresently these models are presumed to be discriminative. They are only required to be able to simulate from a probability distribution over the output variables conditioned on the inputs, and to evaluate the probability density induced by this distribution."
    }, {
      "heading" : "4.5 A semi-parametric factorial mixture GPM",
      "text" : "The current implementation of MML implements all the above commands in terms of approximate inference in single, unusually flexible, semi-parametric Bayesian meta-model. This GPM is closely related to CrossCat (Mansinghka et al., 2015). The CrossCat model is a factorial Dirichlet process mixture model, where variables are assigned to specific Dirichlet process mixtures by inference in another Dirichlet process mixture model over the columns. The version used for implementing MML adds two key components:\n1. Deterministic constraints on model structure. Users can specify constraints on the marginal dependence or independence of arbitrary pairs of variables.\n2. Feedforward networks of discriminative models conditioned on the outputs of the generative model. This allows users to combine general-purpose density estimation with standard statistical techniques such as regression as well as complex computational models with noisy outputs.\nThus in MML, the CrossCat probability model is used as the root node in a directed graphical model. Each other node in the graph corresponds to specific discriminative model, directly conditioned on the inputs of its immediate ancestors. Undirected terms attached to the root node enforce deterministic constraints.\nIt is helpful to view this model in terms of a “divide and conquer” modeling strategy that bottoms out in foreign predictors and other standard parametric models from Bayesian statistics:\n1. All variables not explicitly assigned to a custom model are divided into marginally independent groups. Variables in the same group are assumed to be marginally dependent. Partitions of variables that do not respect the given marginal dependence and independence constraints are rejected. Each group of variables induces an independent subproblem that will typically be far lower dimensional than the original high-dimensional problem.\n2. For each subproblem, divide the rows into clusters whose values are marginally dependent given any variable-specific hyperparameters.\n3. For each cluster, use a simple product of parametric models — i.e. a “naive Bayes” approach (Duda et al., 2001) — to estimate the joint distribution.\nInference in the GPM thus addresses modeling tradeoffs that resemble the decisions faced in exploratory analysis, confirmatory analysis, and predictive modeling. The most crucial decisions involve defining which subset of the data is relevant for answering each question. A secondary issue is what probabilistic model to use for each subset; absent prior knowledge, these are chosen generically, based on the type of the data in the column."
    }, {
      "heading" : "4.5.1 A “divide-and-conquer” generative process",
      "text" : "The generative process that induces the default GPM can be described using the following notation:\nName Description αD Concentration hyperparameter for CRP that slices the columns ~λd Hyperparameters for column d (datatype-dependent) zd Slice (column partition) assigned to column d αv Concentration hyperparameter for CRP that clusters rows for slice v yvr Cluster assigned to row r with respect to slice v ~θdc Model parameters for column d cluster c (datatype-dependent) ~xc(·,d) Values in cluster c for column d, i.e. {x(r,d) | y| zd r = c} ud An indicator such that ud = 1 iff d is modeled by a foreign predictor par(d) The set of input dimensions for the foreign predictor conditionally modeling variable d ~φd Parameters for the foreign predictor conditionally modeling variable l md(x(r,d); ~φd, ~xp) The stochastic model for the foreign predictor used for variable d (with density mdensd (·)) with ~xp = {x(r,p)|p ∈ par(d)}) δm~z Characteristic function enforcing marginal (in)dependence constraint m Vd(·) A generic hyper-prior of the appropriate type for variable or dimension d. Md(·) and LD(·) A datatype-appropriate parameter prior (e.g. a Beta prior for binary data, ∀ d s.t. ud = 1 Normal-Gamma for continuous data, or Dirichlet for discrete data),\nand likelihood model (e.g. Bernoulli, Normal or Multinomial).\nUsing this notation, the unconstrained generative process for the default meta-model can be concisely described in statistician’s notation as follows:\nαD ∼ Gamma(k = 1, θ = 1) ~λd ∼ Vd(·) foreach d ∈ {1, · · · , D} zd ∼ CRP({zi | i 6= d};αD) foreach d ∈ {1, · · · , D} αv ∼ Gamma(k = 1, θ = 1) foreach v ∈ ~z yvr ∼ CRP({yvi | i 6= r};αv) foreach v ∈ ~z and\nr ∈ {1, · · · , R} ~θdc ∼ Md(·;~λd)\n~xc(·,d) = {x(r,d) | y zd r = c} ∼ ∏ r Ld( ~θdc ) if ud = 0\n~x(·,d) = {x(r,d)} ∼ md( ~φd; {x(r,p)|p ∈ par(d)}) if ud = 1 cm ∼ δm(~z) foreach (in)dependence constraint\nThe true generative process also must ensure that cm = 1 for all of theM (in)dependence constraints. This is enforced by conditioning on the event {cm = 1}. A generative model for this constrained process can be given trivially by embedding the unconstrained generative process in the inner loop of a rejection sampler for {cm} (Mansinghka, 2009; Murray et al., 2009)."
    }, {
      "heading" : "4.5.2 The joint density",
      "text" : "Here we use θG to denote all the latent information in a semi-parametric GPM G needed to capture its dependence on the data O. This includes the concentration parameter αD for the CRP over columns, the variable-specific hyper-parameters {~λd}, the column partition ~z, the column-partition-specific concentration parameters {αv} and row partition {~yv}, and the category-specific parameters {θdc}. Note that in this section, Md, Vd, Ld, and CRP each represent probability density functions rather than stochastic simulators.\nGiven this notation, we have:\nP (θG , O) = P (X, {~θdc}, {~yv, αv}, {~λd}, ~z, αD) = e−αD ( ∏ d∈D Vd(~λd) ) CRP(~z;αD) (∏ v∈~z e−αvCRP(~yv;αv) )\n× (∏ v∈~z ∏ c∈~yv ∏ d∈{i s.t. zi=v} Md(~θ d c ; ~λd) ∏ r∈c Ld(x(r,d); ~θdc ) )(∏ m δm~z )\n× ( ∏ d with ud=1 ∏ r mdensd (x(r,d); ~φd, {x(r,p)|p ∈ par(d)) )"
    }, {
      "heading" : "4.5.3 Inference via sequential Monte Carlo with Gibbs proposals and Gibbs rejuvenation",
      "text" : "Inference in this meta-model is performed via a sequential Monte Carlo scheme, in which each row is incorporated incrementally, with all latent variables proposed from their conditional\ndistribution. Additionally, clients can control the frequency and target latent variables for rejuvenation kernels based on Gibbs sampling, turning the overall scheme into a resamplemove algorithm (Andrieu et al., 2003; Smith et al., 2013). This combination enables parallel inference and estimation of marginal probabilities while allowing the bulk of the inferential work to be done via a suitable Markov chain.\n1. incorporate(id = r, values = {(cj , x(r,cj))}) Each row is incorporated via a single Gibbs step that numerically marginalizes out all the latent variables associated with the row (Smith et al., 2013; Murphy, 2002). The associated weight is the marginal probability of the measurements to be incorporated:\nw′i = wi ∗ p({(cj , x(r,cj))}|G〉)\nThis operation is linear in the number of observed cells for the record being incorporated, the number of total slices, and the maximum number of clusters associated with any slice.\n2. infer(iterations = N , type = rows | columns | parameters | hyperparameters | foreign | resample, slice = j | NA, cluster = k | NA, foreign_predictor = l | NA) This operation applies a particular transition operator, specified by the arguments, to a selected subset of the latent variables. Each invocation affects all particles in the sequential Monte Carlo scheme. By varying the type parameter, a client can control whether inference is performed over the row-cluster assignment variables, the columnslice assignment variables, the cluster parameters, the column-specific hyperparameters, or all latent variables associated with a specific foreign predictor. An invocation with type = resample applies multinomial resampling to the weighted collection of models. This allows for a limited form of inference programming (Mansinghka et al., 2014), as follows. By varying the slice, cluster, or foreign_predictor variables, clients can instruct the GPM to only perform inference on a specific subset of the latent variables. Computational effort can thus be focused on those latent variables that are most relevant for a given analysis, rather than uniformly distributed across all latent variables in the GPM. This is most useful when the queries of interest focus on a subset of the variables, or when the clusters are well-separated. The prototype implementation of BayesDB uses row-cluster, column-slice, clusterparameter, and column-hyperparameter transition operators from Mansinghka et al. (2015). The only modification is that the log joint density now includes terms for enforcing each of the (in)dependence constraints, and also terms for the likelihood induced by each foreign predictor, as described above.\nThis interface allows clients to specify multiple MCMC, SMC and hybrid strategies for inference. The default inference program that is invoked by the ANALYZE command in BQL does no resampling and selects slices and clusters to do inference on via systematic scans. It thus can be thought of as an MCMC scheme with multiple parallel chains. This approach is conservative and makes it easier to assess the stability and reproducibility of inference, although it is unlikely to be the most efficient approach in some cases."
    }, {
      "heading" : "5. Discussion",
      "text" : "This paper has described BayesDB, a probabilistic programming platform that allows users to directly query the probable implications of statistical data. The query language can solve statistical inference problems such as detecting predictive relationships between variables, inferring missing values, simulating probable observations, and identifying statistically similar database entries. Statisticians and domain experts can incorporate (in)dependence constraints and custom models using a qualitative language for probabilistic models. The default meta-model frees users from needing to know how to choose modeling approaches, remove records with missing values, detect outliers, or tune model parameters. The prototype implementation is suitable for analyzing complex, heterogeneous data tables with up to tens of thousands of rows and hundreds of variables."
    }, {
      "heading" : "5.1 Related work in probabilistic programming",
      "text" : "Most probabilistic programming languages are intended for model specification (Goodman et al., 2008; Stan Development Team, 2015; Milch et al., 2007; Pfeffer, 2009). This is fundamentally different from BQL and MML:\n1. In BQL, probabilistic models are never explicitly specified. Instead, an implicit set of models is averaged over (or sampled from) as needed.\n2. With MML, users specify constraints on an algorithm for model discovery and need not explicitly select any specific models. These constraints generally do not uniquely identify the structure of the model that will ultimately be used.\nIn contrast, with languages such as Stan (Stan Development Team, 2015), each program corresponds to a specific probabilistic model whose structure is fixed by the program source. Tabular (Gordon et al., 2014), a probabilistic language designed for embedding into spreadsheets that applies user-specified factor graph models defined in terms of observed and latent variables to datasets represented as sub-tables, seems closest in structure to BQL. However, like BUGS and Stan, Tabular does not aim to hide the conceptual vocabulary of probabilistic modeling from its end users, and it focuses on user-specified models. Other integrations of probabilistic modeling with databases such as (Singh and Graepel, 2013) are similarly focused on sophisticated modeling but do not provide a model-independent abstraction for queries or support for general Bayesian data analysis.\nIt is straightforward to extend MML to allow syntactic escapes into all these languages that allow external probabilistic programs to be used as foreign predictors."
    }, {
      "heading" : "5.2 Related work in probabilistic databases",
      "text" : "BayesDB takes a complementary approach to several recent projects that integrate aspects of probabilistic inference with databases. The most closely related systems are MauveDB (Deshpande and Madden, 2006) and BBQ (Deshpande et al., 2004). They provide modelbased views that enable users to run standard SQL queries on the outputs of statistical models. These models must be explicitly specified as part of the schema. This is useful for some machine learning applications but does not address the core problems of applied\ninference, such as data exploration, data cleaning, and confirmatory analysis. Both systems also use restricted model classes that can easily introduce substantial for ad-hoc predictive queries.\nOther systems such as MLBase (Kraska et al., 2013) and GraphLab (Low et al., 2012) aim to simplify at-scale development and deployment of machine learning algorithms. MLBase and GraphLab host data in a distributed database environment and provide operators for scalable ML algorithms. Systems such as SimSQL (Cai et al., 2013) and its ancestor, MCDB (Jampani et al., 2008), provide SQL operators for efficient Monte Carlo sampling. In principle, several of these systems could serve as runtime platforms for optimized implementations of BQL and the MML."
    }, {
      "heading" : "5.2.1 Uncertain data versus uncertain inference",
      "text" : "The database research community has proposed several probabilistic databases that aim to simplify the management and querying of data that is “uncertain” or “imprecise” (Dalvi et al., 2009). This “data uncertainty” is different from the inferential uncertainty that motivates BayesDB. Even when the data is known with certainty, it is rarely possible to uniquely identify a single model that can be used with complete certainty. Second, each probable model is likely to have uncertain implications. Extensions of BayesDB that augment GPMs with probabilistically coherent treatments of data uncertainty are an important area for future research."
    }, {
      "heading" : "5.3 Limitations and future work",
      "text" : "Additional GPMs and meta-models are needed for some applications. There are specialized SQL databases that strike different tradeoffs between query latency, workload variability, and storage efficiency. Similarly, we expect that future GPMs and meta-models will strike different tradeoffs between prediction speed, prediction accuracy, statistical model capacity, and the amount of available data. In some cases, the semi-parametric meta-model presented here may be adequate in principle but producing an appropriate implementation is a significant systems research project. For example, it may be possible to build versions suitable for ad-hoc exploration of distributed databases such as Dremel (Melnik et al., 2010) or Spark (Zaharia et al., 2010). In other cases, fundamentally different model classes may be more appropriate. For example, it seems appealing to jointly model populations of web browsing sessions and web assets with low-dimensional latent space models (Stern et al., 2009).\nIt will be challenging to develop query planners that can handle GPMs given by arbitrary probabilistic programs. A key issue is that the full GPM interface allows for complex conditional queries over composite GPMs that may require data-dependent inference strategies. One potential approach is to specify GPMs as probabilistic programs in a language with programmable inference; currently, the only such language is VentureScript. The inference strategy needed to answer a given query could then be assembled on-demand.\nBQL and MML have yet to incorporate key ideas from several significant subfields of statistics. For example, neither language has explicit support for causal relationships and arbitrary counterfactuals (Pearl, 1988, 2009, 2001). Both BQL and MML make the standard, simplistic assumption that data is missing at random. Neither BQL nor MML has native support for longitudinal or panel data or for time-series; instead, users must apply standard\nworkarounds or implement custom data types. A minor limitation is that hierarchical models are currently supported by merging subpopulations, retaining an indicator variable, and treating any variables unique to a given subpopulation as missing. It should instead be possible to build GPMs that jointly model subpopulations that are separately represented (and that therefore may not share the same set of observable variables). It will also be important to develop a formal semantics and cost model for both BQL and MML. Qualitative probabilistic programming. BQL and MML are qualitative languages for quantitative reasoning. They make it possible for users to perform Bayesian data analysis without needing to know how to specify quantitative probabilities or model parameters. However, the set of qualitative constructs that they support is limited, and needs to be expanded. For example, in MML, it will be important to support conditional dependence constraints. These could be specified generatively, e.g. by defining a directed acyclic graph over subsets of variables, and leaving the model builder to fill in the (conditional) joint distributions over each subset of variables. In BQL, it would be interesting to explore the addition of commands for optimization and decision-theoretic choice, with objective functions specified both explicitly and implicitly. Finally, it will be interesting to explore elicitation strategies based on “programming by example”. For example, users could create datasets by iteratively specifying prototypical examples and turn them into large datasets by treating each as the seed for a separate synthetic population, produced via SIMULATE."
    }, {
      "heading" : "5.4 Conclusion",
      "text" : "Traditional databases protect consumers of data from “having to know how the data is organized in the machine” Codd (1970) and provide automated data representations and retrieval algorithms that perform well enough for a broad class of applications. Although this abstraction barrier is only imperfectly achieved, it has proved useful enough to serve as the basis of multiple generations of software and data systems. This decoupling of task specification from implementation made it possible to improve performance and reliability — of individual database indexes, and in some cases of entire database systems — without needing to notify end users. It also created a simple conceptual vocabulary and query language for data management and data processing that spread far farther than the systems programming knowledge needed to implement it.\nBayesDB aims to insulate consumers of statistical inference from the concepts of modeling and statistics and provide a simple, qualitative interface for solving problems that currently seem quantitative and complex. It also allows models, analyses, and data resources to be improved independently. It is not yet clear how deeply the analogy with traditional databases will run. However, we hope that BayesDB represents a significant step towards making statistically rigorous empirical inference more credible, transparent and ubiquitous."
    } ],
    "references" : [ {
      "title" : "An introduction to mcmc for machine learning",
      "author" : [ "Christophe Andrieu", "Nando De Freitas", "Arnaud Doucet", "Michael I Jordan" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Andrieu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Andrieu et al\\.",
      "year" : 2003
    }, {
      "title" : "A relational model of data for large shared data banks",
      "author" : [ "Edgar F Codd" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Codd.,? \\Q1970\\E",
      "shortCiteRegEx" : "Codd.",
      "year" : 1970
    }, {
      "title" : "Elements of information theory",
      "author" : [ "Thomas M Cover", "Joy A Thomas" ],
      "venue" : null,
      "citeRegEx" : "Cover and Thomas.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cover and Thomas.",
      "year" : 2012
    }, {
      "title" : "Probabilistic databases: diamonds in the dirt",
      "author" : [ "Nilesh Dalvi", "Christopher Ré", "Dan Suciu" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Dalvi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2009
    }, {
      "title" : "Mauvedb: supporting model-based user views in database systems",
      "author" : [ "Amol Deshpande", "Samuel Madden" ],
      "venue" : "In Proceedings of the 2006 ACM SIGMOD international conference on Management of data,",
      "citeRegEx" : "Deshpande and Madden.,? \\Q2006\\E",
      "shortCiteRegEx" : "Deshpande and Madden.",
      "year" : 2006
    }, {
      "title" : "Model-driven data acquisition in sensor networks",
      "author" : [ "Amol Deshpande", "Carlos Guestrin", "Samuel R Madden", "Joseph M Hellerstein", "Wei Hong" ],
      "venue" : "In Proceedings of the Thirtieth international conference on Very large data bases-Volume",
      "citeRegEx" : "Deshpande et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Deshpande et al\\.",
      "year" : 2004
    }, {
      "title" : "Bayesian Data Analysis",
      "author" : [ "Andrew Gelman", "John B. Carlin", "Hal S. Stern", "Donald B. Rubin" ],
      "venue" : null,
      "citeRegEx" : "Gelman et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Gelman et al\\.",
      "year" : 1995
    }, {
      "title" : "Church: a language for generative models",
      "author" : [ "Noah D. Goodman", "Vikash K. Mansinghka", "Daniel Roy", "Keith Bonawitz", "Joshua B. Tenenbaum" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Goodman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Goodman et al\\.",
      "year" : 2008
    }, {
      "title" : "Tabular: a schema-driven probabilistic programming language",
      "author" : [ "Andrew D Gordon", "Thore Graepel", "Nicolas Rolland", "Claudio Russo", "Johannes Borgstrom", "John Guiver" ],
      "venue" : "In ACM SIGPLAN Notices,",
      "citeRegEx" : "Gordon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2014
    }, {
      "title" : "Mcdb: a monte carlo approach to managing uncertain data",
      "author" : [ "Ravi Jampani", "Fei Xu", "Mingxi Wu", "Luis Leopoldo Perez", "Christopher Jermaine", "Peter J Haas" ],
      "venue" : "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,",
      "citeRegEx" : "Jampani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jampani et al\\.",
      "year" : 2008
    }, {
      "title" : "Mlbase: A distributed machine-learning system",
      "author" : [ "Tim Kraska", "Ameet Talwalkar", "John C Duchi", "Rean Griffith", "Michael J Franklin", "Michael I Jordan" ],
      "venue" : "In CIDR,",
      "citeRegEx" : "Kraska et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kraska et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed graphlab: a framework for machine learning and data mining in the cloud",
      "author" : [ "Yucheng Low", "Danny Bickson", "Joseph Gonzalez", "Carlos Guestrin", "Aapo Kyrola", "Joseph M Hellerstein" ],
      "venue" : "Proceedings of the VLDB Endowment,",
      "citeRegEx" : "Low et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2012
    }, {
      "title" : "Venture: a higher-order probabilistic programming platform with programmable inference",
      "author" : [ "Vikash Mansinghka", "Daniel Selsam", "Yura Perov" ],
      "venue" : "arXiv preprint arXiv:1404.0099,",
      "citeRegEx" : "Mansinghka et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mansinghka et al\\.",
      "year" : 2014
    }, {
      "title" : "Crosscat: A fully bayesian nonparametric method for analyzing heterogeneous, high dimensional data",
      "author" : [ "Vikash Mansinghka", "Patrick Shafto", "Eric Jonas", "Cap Petschulat", "Max Gasner", "Joshua Tenenbaum" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Mansinghka et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mansinghka et al\\.",
      "year" : 2015
    }, {
      "title" : "Natively probabilistic computation",
      "author" : [ "Vikash Kumar Mansinghka" ],
      "venue" : "PhD thesis, Massachusetts Institute of Technology,",
      "citeRegEx" : "Mansinghka.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mansinghka.",
      "year" : 2009
    }, {
      "title" : "Generalized linear models, volume 37",
      "author" : [ "Peter McCullagh", "John A Nelder" ],
      "venue" : "CRC press,",
      "citeRegEx" : "McCullagh and Nelder.,? \\Q1989\\E",
      "shortCiteRegEx" : "McCullagh and Nelder.",
      "year" : 1989
    }, {
      "title" : "Dremel: interactive analysis of web-scale datasets",
      "author" : [ "Sergey Melnik", "Andrey Gubarev", "Jing Jing Long", "Geoffrey Romer", "Shiva Shivakumar", "Matt Tolton", "Theo Vassilakis" ],
      "venue" : "Proceedings of the VLDB Endowment,",
      "citeRegEx" : "Melnik et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Melnik et al\\.",
      "year" : 2010
    }, {
      "title" : "blog: Probabilistic models with unknown objects",
      "author" : [ "Brian Milch", "Bhaskara Marthi", "Stuart Russell", "David Sontag", "Daniel L Ong", "Andrey Kolobov" ],
      "venue" : "Statistical relational learning,",
      "citeRegEx" : "Milch et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2007
    }, {
      "title" : "Dynamic bayesian networks: representation, inference and learning",
      "author" : [ "Kevin Patrick Murphy" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Murphy.,? \\Q2002\\E",
      "shortCiteRegEx" : "Murphy.",
      "year" : 2002
    }, {
      "title" : "The gaussian process density sampler",
      "author" : [ "Iain Murray", "David MacKay", "Ryan P Adams" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Murray et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2009
    }, {
      "title" : "Markov chain sampling methods for dirichlet process mixture models",
      "author" : [ "R. Neal" ],
      "venue" : null,
      "citeRegEx" : "Neal.,? \\Q1998\\E",
      "shortCiteRegEx" : "Neal.",
      "year" : 1998
    }, {
      "title" : "Probabilistic reasoning in intelligent systems: Networks of plausible inference",
      "author" : [ "Judea Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl.,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl.",
      "year" : 1988
    }, {
      "title" : "Bayesianism and causality, or, why i am only a half-bayesian",
      "author" : [ "Judea Pearl" ],
      "venue" : "In Foundations of bayesianism,",
      "citeRegEx" : "Pearl.,? \\Q2001\\E",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2001
    }, {
      "title" : "Figaro: An object-oriented probabilistic programming language. Charles River Analytics",
      "author" : [ "Avi Pfeffer" ],
      "venue" : "Technical Report,",
      "citeRegEx" : "Pfeffer.,? \\Q2009\\E",
      "shortCiteRegEx" : "Pfeffer.",
      "year" : 2009
    }, {
      "title" : "The infinite gaussian mixture model",
      "author" : [ "C. Rasmussen" ],
      "venue" : "In Advances in Neural Processing Systems",
      "citeRegEx" : "Rasmussen.,? \\Q2000\\E",
      "shortCiteRegEx" : "Rasmussen.",
      "year" : 2000
    }, {
      "title" : "Artificial intelligence: A modern approach",
      "author" : [ "Stuart Russell", "Peter Norvig" ],
      "venue" : null,
      "citeRegEx" : "Russell and Norvig.,? \\Q2003\\E",
      "shortCiteRegEx" : "Russell and Norvig.",
      "year" : 2003
    }, {
      "title" : "ACM, 2013",
      "author" : [ "Sameer Singh", "Thore Graepel. Automated probabilistic modeling for relational data. In Proceedings of the ACM of Information", "Knowledge Management CIKM" ],
      "venue" : "URL http://research.microsoft.com/apps/pubs/default.aspx?id=200220.",
      "citeRegEx" : "Singh et al\\.,? 2013",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequential Monte Carlo methods in practice",
      "author" : [ "Adrian Smith", "Arnaud Doucet", "Nando de Freitas", "Neil Gordon" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Smith et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2013
    }, {
      "title" : "Matchbox: large scale online bayesian recommendations",
      "author" : [ "David H Stern", "Ralf Herbrich", "Thore Graepel" ],
      "venue" : "In Proceedings of the 18th international conference on World wide web,",
      "citeRegEx" : "Stern et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Stern et al\\.",
      "year" : 2009
    }, {
      "title" : "Low assumptions, high dimensions",
      "author" : [ "L. Wasserman" ],
      "venue" : "Rationality, Markets and Morals,",
      "citeRegEx" : "Wasserman.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wasserman.",
      "year" : 2011
    }, {
      "title" : "Spark: cluster computing with working sets",
      "author" : [ "Matei Zaharia", "Mosharaf Chowdhury", "Michael J Franklin", "Scott Shenker", "Ion Stoica" ],
      "venue" : "In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing,",
      "citeRegEx" : "Zaharia et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zaharia et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "The default modeling assumptions that BayesDB makes are suitable for a broad class of problems (Mansinghka et al., 2015; Wasserman, 2011), but statisticians can customize these assumptions when necessary.",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 29,
      "context" : "The default modeling assumptions that BayesDB makes are suitable for a broad class of problems (Mansinghka et al., 2015; Wasserman, 2011), but statisticians can customize these assumptions when necessary.",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "BayesDB also enables domain experts that lack statistical expertise to perform qualitative model checking (Gelman et al., 1995) and encode simple forms of qualitative prior knowledge.",
      "startOffset" : 106,
      "endOffset" : 127
    }, {
      "referenceID" : 25,
      "context" : "This is directly useful for predictive modeling and also decision-theoretic choice implemented using Monte Carlo estimation of expected utility (Russell and Norvig, 2003).",
      "startOffset" : 144,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "b = mutual-information(G, colA = ci, colB = cj) The mutual information between two columns can be estimated by the standard reduction to KL divergence (Cover and Thomas, 2012).",
      "startOffset" : 151,
      "endOffset" : 175
    }, {
      "referenceID" : 13,
      "context" : "The current prototype implementation of BayesDB uses a standard Gibbs sampler for a Dirichlet process mixture model (Mansinghka et al., 2015; Neal, 1998; Rasmussen, 2000) to sample {(φl, πl)}|{X (r,c)}, with K + = 1000 by default.",
      "startOffset" : 116,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : "The current prototype implementation of BayesDB uses a standard Gibbs sampler for a Dirichlet process mixture model (Mansinghka et al., 2015; Neal, 1998; Rasmussen, 2000) to sample {(φl, πl)}|{X (r,c)}, with K + = 1000 by default.",
      "startOffset" : 116,
      "endOffset" : 170
    }, {
      "referenceID" : 24,
      "context" : "The current prototype implementation of BayesDB uses a standard Gibbs sampler for a Dirichlet process mixture model (Mansinghka et al., 2015; Neal, 1998; Rasmussen, 2000) to sample {(φl, πl)}|{X (r,c)}, with K + = 1000 by default.",
      "startOffset" : 116,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "The relational model enabled sharing and infrastructure reuse because interactions with the data, queries, are expressed in a notation (most popularly SQL) that is independent of the physical representation of the data (Codd, 1970).",
      "startOffset" : 219,
      "endOffset" : 231
    }, {
      "referenceID" : 13,
      "context" : "For example, in the current version of MML, closed-set categorical variables are modeled generatively via a multinomial component model with a symmetric Dirichlet prior on the parameters (Mansinghka et al., 2015).",
      "startOffset" : 187,
      "endOffset" : 212
    }, {
      "referenceID" : 13,
      "context" : "Data of this type is generatively modeled using an asymmetric BetaBernoulli model (Mansinghka et al., 2015) that can better handle sparse or marginally biased variables than a symmetric alternative.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "model (McCullagh and Nelder, 1989).",
      "startOffset" : 6,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : "3 Qualitative constraints The BayesDB MML provides constructs for specifying qualitative constraints on the dependence and independence relationships (Pearl, 1988).",
      "startOffset" : 150,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "This GPM is closely related to CrossCat (Mansinghka et al., 2015).",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : "A generative model for this constrained process can be given trivially by embedding the unconstrained generative process in the inner loop of a rejection sampler for {cm} (Mansinghka, 2009; Murray et al., 2009).",
      "startOffset" : 171,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : "A generative model for this constrained process can be given trivially by embedding the unconstrained generative process in the inner loop of a rejection sampler for {cm} (Mansinghka, 2009; Murray et al., 2009).",
      "startOffset" : 171,
      "endOffset" : 210
    }, {
      "referenceID" : 0,
      "context" : "Additionally, clients can control the frequency and target latent variables for rejuvenation kernels based on Gibbs sampling, turning the overall scheme into a resamplemove algorithm (Andrieu et al., 2003; Smith et al., 2013).",
      "startOffset" : 183,
      "endOffset" : 225
    }, {
      "referenceID" : 27,
      "context" : "Additionally, clients can control the frequency and target latent variables for rejuvenation kernels based on Gibbs sampling, turning the overall scheme into a resamplemove algorithm (Andrieu et al., 2003; Smith et al., 2013).",
      "startOffset" : 183,
      "endOffset" : 225
    }, {
      "referenceID" : 27,
      "context" : "incorporate(id = r, values = {(cj , x(r,cj))}) Each row is incorporated via a single Gibbs step that numerically marginalizes out all the latent variables associated with the row (Smith et al., 2013; Murphy, 2002).",
      "startOffset" : 179,
      "endOffset" : 213
    }, {
      "referenceID" : 18,
      "context" : "incorporate(id = r, values = {(cj , x(r,cj))}) Each row is incorporated via a single Gibbs step that numerically marginalizes out all the latent variables associated with the row (Smith et al., 2013; Murphy, 2002).",
      "startOffset" : 179,
      "endOffset" : 213
    }, {
      "referenceID" : 12,
      "context" : "This allows for a limited form of inference programming (Mansinghka et al., 2014), as follows.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "This allows for a limited form of inference programming (Mansinghka et al., 2014), as follows. By varying the slice, cluster, or foreign_predictor variables, clients can instruct the GPM to only perform inference on a specific subset of the latent variables. Computational effort can thus be focused on those latent variables that are most relevant for a given analysis, rather than uniformly distributed across all latent variables in the GPM. This is most useful when the queries of interest focus on a subset of the variables, or when the clusters are well-separated. The prototype implementation of BayesDB uses row-cluster, column-slice, clusterparameter, and column-hyperparameter transition operators from Mansinghka et al. (2015). The only modification is that the log joint density now includes terms for enforcing each of the (in)dependence constraints, and also terms for the likelihood induced by each foreign predictor, as described above.",
      "startOffset" : 57,
      "endOffset" : 738
    }, {
      "referenceID" : 7,
      "context" : "1 Related work in probabilistic programming Most probabilistic programming languages are intended for model specification (Goodman et al., 2008; Stan Development Team, 2015; Milch et al., 2007; Pfeffer, 2009).",
      "startOffset" : 122,
      "endOffset" : 208
    }, {
      "referenceID" : 17,
      "context" : "1 Related work in probabilistic programming Most probabilistic programming languages are intended for model specification (Goodman et al., 2008; Stan Development Team, 2015; Milch et al., 2007; Pfeffer, 2009).",
      "startOffset" : 122,
      "endOffset" : 208
    }, {
      "referenceID" : 23,
      "context" : "1 Related work in probabilistic programming Most probabilistic programming languages are intended for model specification (Goodman et al., 2008; Stan Development Team, 2015; Milch et al., 2007; Pfeffer, 2009).",
      "startOffset" : 122,
      "endOffset" : 208
    }, {
      "referenceID" : 8,
      "context" : "Tabular (Gordon et al., 2014), a probabilistic language designed for embedding into spreadsheets that applies user-specified factor graph models defined in terms of observed and latent variables to datasets represented as sub-tables, seems closest in structure to BQL.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "The most closely related systems are MauveDB (Deshpande and Madden, 2006) and BBQ (Deshpande et al.",
      "startOffset" : 45,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "The most closely related systems are MauveDB (Deshpande and Madden, 2006) and BBQ (Deshpande et al., 2004).",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "Other systems such as MLBase (Kraska et al., 2013) and GraphLab (Low et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 11,
      "context" : ", 2013) and GraphLab (Low et al., 2012) aim to simplify at-scale development and deployment of machine learning algorithms.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : ", 2013) and its ancestor, MCDB (Jampani et al., 2008), provide SQL operators for efficient Monte Carlo sampling.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "1 Uncertain data versus uncertain inference The database research community has proposed several probabilistic databases that aim to simplify the management and querying of data that is “uncertain” or “imprecise” (Dalvi et al., 2009).",
      "startOffset" : 213,
      "endOffset" : 233
    }, {
      "referenceID" : 16,
      "context" : "For example, it may be possible to build versions suitable for ad-hoc exploration of distributed databases such as Dremel (Melnik et al., 2010) or Spark (Zaharia et al.",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 30,
      "context" : ", 2010) or Spark (Zaharia et al., 2010).",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 28,
      "context" : "For example, it seems appealing to jointly model populations of web browsing sessions and web assets with low-dimensional latent space models (Stern et al., 2009).",
      "startOffset" : 142,
      "endOffset" : 162
    } ],
    "year" : 2015,
    "abstractText" : "Is it possible to make statistical inference broadly accessible to non-statisticians without sacrificing mathematical rigor or inference quality? This paper describes BayesDB, a probabilistic programming platform that aims to enable users to query the probable implications of their data as directly as SQL databases enable them to query the data itself. This paper focuses on four aspects of BayesDB: (i) BQL, an SQL-like query language for Bayesian data analysis, that answers queries by averaging over an implicit space of probabilistic models; (ii) techniques for implementing BQL using a broad class of multivariate probabilistic models; (iii) a semi-parametric Bayesian model-builder that auomatically builds ensembles of factorial mixture models to serve as baselines; and (iv) MML, a “meta-modeling” language for imposing qualitative constraints on the model-builder and combining baseline models with custom algorithmic and statistical models that can be implemented in external software. BayesDB is illustrated using three applications: cleaning and exploring a public database of Earth satellites; assessing the evidence for temporal dependence between macroeconomic indicators; and analyzing a salary survey.",
    "creator" : "LaTeX with hyperref package"
  }
}