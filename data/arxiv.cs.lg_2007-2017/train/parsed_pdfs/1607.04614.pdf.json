{
  "name" : "1607.04614.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Guided Policy Search as Approximate Mirror Descent",
    "authors" : [ "William Montgomery", "Sergey Levine" ],
    "emails" : [ "wmonty@cs.washington.edu", "svlevine@cs.washington.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Policy search algorithms based on supervised learning from a computational or human “teacher” have gained prominence in recent years due to their ability to optimize complex policies for autonomous flight [16], video game playing [15, 4], and bipedal locomotion [11]. Among these methods, guided policy search algorithms [6] are particularly appealing due to their ability to adapt the teacher to produce data that is best suited for training the final policy with supervised learning. Such algorithms have been used to train complex deep neural network policies for vision-based robotic manipulation [6], as well as a variety of other tasks [19, 11]. However, convergence results for these methods typically follow by construction from their formulation as a constrained optimization, where the teacher is gradually constrained to match the learned policy, and guarantees on the performance of the final policy only hold at convergence if the constraint is enforced exactly. This is problematic in practical applications, where such algorithms are typically executed for a small number of iterations.\nIn this paper, we show that guided policy search algorithms can be interpreted as approximate variants of mirror descent under constraints imposed by the policy parameterization, with supervised learning corresponding to a projection onto the constraint manifold. Based on this interpretation, we can derive a new, simplified variant of guided policy search, which corresponds exactly to mirror descent under linear dynamics and convex policy spaces. When these convexity and linearity assumptions do not hold, we can show that the projection step is approximate, up to a bound that depends on the step size of the algorithm, which suggests that for a small enough step size, we can achieve continuous improvement. The form of this bound provides us with intuition about how to adjust the step size in practice, so as to obtain a simple algorithm with a small number of hyperparameters.\nThe main contribution of this paper is a simple new guided policy search algorithm that can train complex, high-dimensional policies by alternating between trajectory-centric reinforcement learning\nar X\niv :1\n60 7.\n04 61\n4v 1\n[ cs\n.L G\n] 1\n5 Ju\nl 2 01\nAlgorithm 1 Generic guided policy search method 1: for iteration k ∈ {1, . . . ,K} do 2: C-step: improve each pi(ut|xt) based on surrogate cost ˜̀i(xt,ut), return samples Di 3: S-step: train πθ(ut|xt) with supervised learning on the dataset D = ∪iDi 4: Modify ˜̀i(xt,ut) to enforce agreement between πθ(ut|xt) and each p(ut|xt) 5: end for\nand supervised learning, as well as a connection between guided policy search methods and mirror descent. We also extend previous work on bounding policy cost in terms of KL divergence [15, 17] to derive a bound on the cost of the policy at each iteration, which provides guidance on how to adjust the step size of the method. We provide empirical results on several simulated robotic navigation and manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters."
    }, {
      "heading" : "2 Guided Policy Search Algorithms",
      "text" : "We first review guided policy search methods and background. Policy search algorithms aim to optimize a parameterized policy πθ(ut|xt) over actions ut conditioned on the state xt. Given stochastic dynamics p(xt+1|xt,ut) and cost `(xt,ut), the goal is to minimize the expected cost under the policy’s trajectory distribution, given by J(θ) = ∑T t=1Eπθ(xt,ut)[`(xt,ut)], where we overload\nnotation to use πθ(xt,ut) to denote the marginals of πθ(τ) = p(x1) ∏T t=1 p(xt+1|xt,ut)πθ(ut|xt), where τ = {x1,u1, . . . ,xT ,uT } denotes a trajectory. A standard reinforcement learning (RL) approach to policy search is to compute the gradient∇θJ(θ) and use it to improve J(θ) [18, 14]. The gradient is typically estimated using samples obtained from the real physical system being controlled, and recent work has shown that such methods can be applied to very complex, high-dimensional policies such as deep neural networks [17, 10]. However, for complex, high-dimensional policies, such methods tend to be inefficient, and practical real-world applications of such model-free policy search techniques are typically limited to policies with about one hundred parameters [3].\nInstead of directly optimizing J(θ), guided policy search algorithms split the optimization into a “control phase” (which we’ll call the C-step) that finds multiple simple local policies pi(ut|xt) that can solve the task from different initial states xi1 ∼ p(x1), and a “supervised phase” (S-step) that optimizes the global policy πθ(ut|xt) to match all of these local policies using standard supervised learning. In fact, a variational formulation of guided policy search [7] corresponds to the EM algorithm, where the C-step is actually the E-step, and the S-step is the M-step. The benefit of this approach is that the local policies pi(ut|xt) can be optimized separately using domain-specific local methods. Trajectory optimization might be used when the dynamics are known [19, 11], while local RL methods might be used with unknown dynamics [5, 6], which still requires samples from the real system, though substantially fewer than the direct approach, due to the simplicity of the local policies. This sample efficiency is the main advantage of guided policy search, which can train policies with nearly a hundred thousand parameters for vision-based control using under 200 episodes [6], in contrast to direct deep RL methods that might require orders of magnitude more experience [17, 10].\nA generic guided policy search method is shown in Algorithm 1. The C-step invokes a local policy optimizer (trajectory optimization or local RL) for each pi(ut|xt) on line 2, and the S-step uses supervised learning to optimize the global policy πθ(ut|xt) on line 3 using samples from each pi(ut|xt), which are generated during the C-step. On line 4, the surrogate cost ˜̀i(xt,ut) for each pi(ut|xt) is adjusted to ensure convergence. This step is crucial, because supervised learning does not in general guarantee that πθ(ut|xt) will achieve similar long-horizon performance to pi(ut|xt) [15]. The local policies might not even be reproducible by a single global policy in general. To address this issue, most guided policy search methods have some mechanism to force the local policies to agree with the global policy, typically by framing the entire algorithm as a constrained optimization that seeks at convergence to enforce equality between πθ(ut|xt) and each pi(ut|xt). The form of the\noverall optimization problem resembles dual decomposition, and usually looks something like this:\nmin θ,p1,...,pN N∑ i=1 T∑ t=1 Epi(xt,ut)[`(xt,ut)] such that pi(ut|xt) = πθ(ut|xt) ∀xt,ut, t, i. (1)\nSince xi1 ∼ p(x1), we have J(θ) ≈ ∑N i=1 ∑T t=1Epi(xt,ut)[`(xt,ut)] when the constraints are enforced exactly. The particular form of the constraint varies depending on the method: prior works have used dual gradient descent [8], penalty methods [11], ADMM [12], and Bregman ADMM [6]. We omit the derivation of these prior variants due to space constraints."
    }, {
      "heading" : "2.1 Efficiently Optimizing Local Policies",
      "text" : "A common and simple choice for the local policies pi(ut|xt) is to use time-varying linear-Gaussian controllers of the form pi(ut|xt) = N (Ktxt + kt,Ct), though other options are also possible [12, 11, 19]. Linear-Gaussian controllers represent individual trajectories with linear stabilization and Gaussian noise, and are convenient in domains where each local policy can be trained from a different (but consistent) initial state xi1 ∼ p(x1). This represents an additional assumption beyond standard RL, but allows for an extremely efficient and convenient local model-based RL algorithm based on iterative LQR [9]. The algorithm proceeds by generating N samples on the real physical system from each local policy pi(ut|xt) during the C-step, using these samples to fit local linear-Gaussian dynamics for each local policy of the form pi(xt+1|xt,ut) = N (fxtxt + futut + fct,Ft) using linear regression, and then using these fitted dynamics to improve the linear-Gaussian controller via a modified LQR algorithm [5]. This modified LQR method solves the following optimization problem:\nmin Kt,kt,Ct T∑ t=1 Epi(xt,ut)[ ˜̀ i(xt,ut)] such that DKL(pi(τ)‖p̄i(τ)) ≤ , (2)\nwhere we again use pi(τ) to denote the trajectory distribution induced by pi(ut|xt) and the fitted dynamics pi(xt+1|xt,ut). Here, p̄i(ut|xt) denotes the previous local policy, and the constraint ensures that the change in the local policy is bounded, as proposed also in prior works [1, 14, 13]. This is particularly important when using linearized dynamics fitted to local samples, since these dynamics are not valid outside of a small region around the current controller. In the case of linearGaussian dynamics and policies, the KL-divergence constraint DKL(pi(τ)‖p̄i(τ)) ≤ can be shown to simplify, as shown in prior work [5] and Appendix A: DKL(pi(τ)‖p̄i(τ))= T∑ t=1 DKL(pi(ut|xt)‖p̄i(ut|xt))= T∑ t=1 −Epi(xt,ut)[log p̄i(ut|xt)]−H(pi(ut|xt)),\nand the resulting Lagrangian of the problem in Equation (2) can be optimized with respect to the primal variables using the standard LQR algorithm, which suggests a simple method for solving the problem in Equation (2) using dual gradient descent [5]. The surrogate objective ˜̀i(xt,ut) = `(xt,ut)+φi(θ) typically includes some term φi(θ) that encourages the local policy pi(ut|xt) to stay close to the global policy πθ(ut|xt), such as a KL-divergence of the form DKL(pi(ut|xt)‖πθ(ut|xt))."
    }, {
      "heading" : "2.2 Prior Convergence Results",
      "text" : "Prior work on guided policy search typically shows convergence by construction, by framing the C-step and S-step as block coordinate ascent on the (augmented) Lagrangian of the problem in Equation (1), with the surrogate cost ˜̀i(xt,ut) for the local policies corresponding to the (augmented) Lagrangian, and the overall algorithm being an instance of dual gradient descent [8], ADMM [12], or Bregman ADMM [6]. Since these methods enforce the constraint pi(ut|xt) = πθ(ut|xt) at convergence (up to linearization or sampling error, depending on the method), we know that 1 N ∑N i=1Epi(xt,ut)[`(xt,ut)] ≈ Eπθ(xt,ut)[`(xt,ut)] at convergence.1 However, prior work does not say anything about πθ(ut|xt) at intermediate iterations, and the constraints of policy search in the real world might often preclude running the method to full convergence. We propose a simplified variant of guided policy search, and present an analysis that sheds light on the performance of both the new algorithm and prior guided policy search methods.\n1As mentioned previously, the initial state xi1 of each local policy pi(ut|xt) is assumed to be drawn from p(x1), hence the outer sum corresponds to Monte Carlo integration of the expectation under p(x1).\nAlgorithm 2 Mirror descent guided policy search (MDGPS): convex linear variant 1: for iteration k ∈ {1, . . . ,K} do 2: C-step: pi ← arg minpi Epi(τ) [∑T t=1 `(xt,ut) ] such that DKL(pi(τ)‖πθ(τ)) ≤\n3: S-step: πθ ← arg minθ ∑ iDKL(pi(τ)‖πθ(τ)) (via supervised learning) 4: end for"
    }, {
      "heading" : "3 Mirror Descent Guided Policy Search",
      "text" : "In this section, we propose our new simplified guided policy search, which we term mirror descent guided policy search (MDGPS). This algorithm uses the constrained LQR optimization in Equation (2) to optimize each of the local policies, but instead of constraining each local policy pi(ut|xt) against the previous local policy p̄i(ut|xt), we instead constraint it directly against the global policy πθ(ut|xt), and simply set the surrogate cost to be the true cost, such that ˜̀i(xt,ut) = `(xt,ut). The method is summarized in Algorithm 2. In the case of linear dynamics and a quadratic cost (i.e. the LQR setting), and assuming that supervised learning can globally solve a convex optimization problem, we can show that this method corresponds to an instance of mirror descent [2] on the objective J(θ). In this formulation, the optimization is performed on the space of trajectory distributions, with a constraint that the policy must lie on the manifold of policies with the chosen parameterization. Let ΠΘ be the set of all possible policies πθ for a given parameterization, where we overload notation to also let ΠΘ denote the set of trajectory distributions that are possible under the chosen parameterization. The return J(θ) can be optimized according to πθ ← arg minπ∈ΠΘ Eπ(τ)[ ∑T t=1 `(xt,ut)]. Mirror descent solves this optimization by alternating between two steps at each iteration k:\npk ← arg min p Ep(τ) [ T∑ t=1 `(xt,ut) ] s. t. D ( p, πk ) ≤ , πk+1 ← arg min π∈ΠΘ D ( pk, π ) .\nThe first step finds a new distribution pk that minimizes the cost and is close to the previous policy πk in terms of the divergence D ( p, πk ) , while the second step projects this distribution onto the\nconstraint set ΠΘ, with respect to the divergence D(pk, π). In the linear-quadratic case with a convex supervised learning phase, this corresponds exactly to Algorithm 2: the C-step optimizes pk, while the S-step is the projection. Monotonic improvement of the global policy πθ follows from the monotonic improvement of mirror descent [2]. In the case of linear-Gaussian dynamics and policies, the S-step, which minimizes KL-divergence between trajectory distributions, in fact only requires minimizing the KL-divergence between policies. Using the identity in Appendix A, we know that\nDKL(pi(τ)‖πθ(τ)) = T∑ t=1 Epi(xt) [DKL(pi(ut|xt)‖πθ(ut|xt))] . (3)"
    }, {
      "heading" : "3.1 Implementation for Nonlinear Global Policies and Unknown Dynamics",
      "text" : "In practice, we aim to optimize complex policies for nonlinear systems with unknown dynamics. This requires a few practical considerations. The C-step requires a local quadratic cost function, which can be obtained via Taylor expansion, as well as local linear-Gaussian dynamics p(xt+1|xt,ut) = N (fxtxt + futut + fct,Ft), which we can fit to samples as in prior work [5]. We also need a local time-varying linear-Gaussian approximation to the global policy πθ(ut|xt), denoted π̄θi(ut|xt). This can be obtained either by analytically differentiating the policy, or by using the same linear regression method that we use to estimate p(xt+1|xt,ut), which is the approach in our implementation. In both cases, we get a different global policy linearization around each local policy. Following prior work [5], we use a Gaussian mixture model prior for both the dynamics and global policy fit.\nThe S-step can be performed approximately in the nonlinear case by using the samples collected for dynamics fitting to also train the global policy. Following prior work [6], our S-step minimizes2∑\ni,t\nEpi(xt) [DKL(πθ(ut|xt)‖pi(ut|xt))] ≈ 1 |Di| ∑ i,t,j DKL(πθ(ut|xt,i,j)‖pi(ut|xt,i,j)),\n2Note that we flip the KL-divergence inside the expectation, following [6]. We found that this produced better results. The intuition behind this is that, because log pi(ut|xt) is proportional to the Q-function of pi(ut|xt) (see Appendix B.1), DKL(πθ(ut|xt,i,j)‖pi(ut|xt,i,j) minimizes the cost-to-go under pi(ut|xt) with respect to πθ(ut|xt), which provides for a more informative objective than the unweighted likelihood in Equation (3).\nAlgorithm 3 Mirror descent guided policy search (MDGPS): unknown nonlinear dynamics 1: for iteration k ∈ {1, . . . ,K} do 2: Generate samples Di = {τi,j} by running either pi or πθi 3: Fit linear-Gaussian dynamics pi(xt+1|xt,ut) using samples in Di 4: Fit linearized global policy π̄θi(ut|xt) using samples in Di 5: C-step: pi ← arg minpi Epi(τ)[ ∑T t=1 `(xt,ut)] such that DKL(pi(τ)‖π̄θi(τ)) ≤\n6: S-step: πθ ← arg minθ ∑ t,i,j DKL(πθ(ut|xt,i,j)‖pi(ut|xt,i,j)) (via supervised learning) 7: Adjust (see Section 4.2) 8: end for\nwhere xt,i,j is the jth sample from pi(xt) obtained by running pi(ut|xt) on the real system. For linear-Gaussian pi(ut|xt) and (nonlinear) conditionally Gaussian πθ(ut|xt) = N (µπ(xt),Σπ(xt)), where µπ and Σπ can be any function (such as a deep neural network), the KL-divergence DKL(πθ(ut|xt,i,j)‖pi(ut|xt,i,j)) can easily be evaluated and differentiated in closed form [6]. However, in the nonlinear setting, minimizing this objective no longer minimizes the KL-divergence between trajectory distributions DKL(πθ(τ)‖pi(τ)) exactly, which means that MDGPS does not correspond exactly to mirror descent: although the C-step can still be evaluated exactly, the S-step now corresponds to an approximate projection onto the constraint manifold. In the next section, we discuss how we can bound the error in this projection. A summary of the nonlinear MDGPS method is provided in Algorithm 4, and additional details are in Appendix B. The samples for linearizing the dynamics and policy can be obtained by running either the last local policy pi(ut|xt), or the last global policy πθ(ut|xt). Both variants produce good results, and we compare them in Section 6."
    }, {
      "heading" : "3.2 Analysis of Prior Guided Policy Search Methods as Approximate Mirror Descent",
      "text" : "The main distinction between the proposed method and prior guided policy search methods is that the constraint DKL(pi(τ)‖π̄θi(τ)) ≤ is enforced on the local policies at each iteration, while in prior methods, this constraint is iteratively enforced via a dual descent procedure over multiple iterations. This means that the prior methods perform approximate mirror descent with step sizes that are adapted (by adjusting the Lagrange multipliers) but not constrained exactly. In our empirical evaluation, we show that our approach is somewhat more stable, though sometimes slower than these prior methods. This empirical observation agrees with our intuition: prior methods can sometimes be faster, because they do not exactly constrain the step size, but our method is simpler, requires less tuning, and always takes bounded steps on the global policy in trajectory space."
    }, {
      "heading" : "4 Analysis in the Nonlinear Case",
      "text" : "Although the S-step under nonlinear dynamics is not an optimal projection onto the constraint manifold, we can bound the additional cost incurred by this projection in terms of the KL-divergence between pi(ut|xt) and πθ(ut|xt). This analysis also reveals why prior guided policy search algorithms, which only have asymptotic convergence guarantees, still attain good performance in practice even after a small number of iterations. We will drop the subscript i from pi(ut|xt) in this section for conciseness, though the same analysis can be repeated for multiple local policies pi(ut|xt)."
    }, {
      "heading" : "4.1 Bounding the Global Policy Cost",
      "text" : "The analysis in this section is based on the following lemma, which we prove in Appendix C.1, building off of earlier results by Ross et al. [15] and Schulman et al. [17]: Lemma 4.1 Let t = maxxt DKL(p(ut|xt)‖πθ(ut|xt). Then DTV(p(xt)‖πθ(xt)) ≤ 2 ∑T t=1 √ 2 t.\nThis means that if we can bound the KL-divergence between the policies, then the total variation divergence between their state marginals (given by DTV(p(xt)‖πθ(xt)) = 12‖p(xt)−πθ(xt)‖1) will also be bounded. This bound allows us in turn to relate the total expected costs of the two policies to each other according to the following lemma, which we prove in Appendix C.2:\nLemma 4.2 If DTV(p(xt)‖πθ(xt)) ≤ 2 ∑T t=1 √ 2 t, then we can bound the total cost of πθ as\nT∑ t=1 Eπθ(xt,ut)[`(xt,ut)] ≤ T∑ t=1 [ Ep(xt,ut)[`(xt,ut)] + √ 2 t max xt,ut `(xt,ut) + 2 √ 2 tQmax,t ] where Qmax,t = ∑T t′=t maxxt′ ,ut′ `(xt′ ,ut′), the maximum total cost from time t to T .\nThis bound on the cost of πθ(ut|xt) tells us that if we update p(ut|xt) so as to decrease its total cost or decrease its KL-divergence against πθ(ut|xt), we will eventually reduce the cost of πθ(ut|xt). For the MDGPS algorithm, this bound suggests that we can ensure improvement of the global policy within a small number of iterations by appropriately choosing the constraint during the C-step. Recall that the C-step constrains ∑T t=1 t ≤ , so if we choose to be small enough, we can close the gap between the local and global policies. Optimizing the bound directly turns out to produce very slow learning in practice, because the bound is very loose. However, it tells us that we can either decrease toward the end of the optimization process or if we observe the global policy performing much worse than the local policies. We discuss how this idea can be put into action in the next section."
    }, {
      "heading" : "4.2 Step Size Selection",
      "text" : "In prior work [8], the step size in the local policy optimization is adjusted by considering the difference between the predicted change in the cost of the local policy p(ut|xt) under the fitted dynamics, and the actual cost obtained when sampling from that policy. The intuition is that, because the linearized dynamics are local, we incur a larger cost the further we deviate from the previous policy. We can adjust the step size by estimating the rate at which the additional cost is incurred and choose the optimal tradeoff. Let `k−1k−1 denote the expected cost under the previous local policy p̄(ut|xt), `kk−1 the cost under the current local policy p(ut|xt) and the previous fitted dynamics (which were estimated using samples from p̄(ut|xt) and used to optimize p(ut|xt)), and `kk the cost of the current local policy under the dynamics estimated using samples from p(ut|xt) itself. Each of these can be computed analytically under the linearized dynamics. We can view the difference `kk − `kk−1 as the additional cost we incur from imperfect dynamics estimation. Previous work suggested modeling the change in cost as a function of as following: `kk − ` k−1 k−1 = a\n2 + b , where b is the change in cost per unit of KL-divergence, and a is additional cost incurred due to inaccurate dynamics [8]. This model is reasonable because the integral of a quadratic cost under a linear-Gaussian system changes roughly linearly with KL-divergence. The additional cost due to dynamics errors is assumes to scale superlinearly, allowing us to solve for b by looking at the difference `kk − `kk−1 and then solving for a new optimal ′ according to ′ = −b/2a, resulting in the update ′ = (`kk−1 − ` k−1 k−1)/2(` k k−1 − `kk).\nIn MDGPS, we propose to use two step size adjustment rules. The first rule simply adapts the previous method to the case where we constrain the new local policy p(ut|xt) against the global policy πθ(ut|xt), instead of the previous local policy p̄(ut|xt). In this case, we simply replace `k−1k−1 with the expected cost under the previous global policy, given by ` k−1,π k−1 , obtained using its linearization π̄θ(ut|xt). We call this the “classic” step size: ′ = (`kk−1 − ` k−1,π k−1 )/2(` k k−1 − `kk).\nHowever, we can also incorporate intuition from the bound in the previous section to obtain a more conservative step adjustment that reduces not only when the obtained local policy improvement doesn’t meet expectations, but also when we detect that the global policy is unable to reproduce the behavior of the local policy. In this case, reducing reduces the KL-divergence between the global and local policies which, as shown in the previous section, tightens the bound on the global policy return. As mentioned previously, directly optimizing the bound tends to perform poorly because the bound is quite loose. However, if we estimate the cost of the global policy using its linearization, we can instead adjust the step size based on a simple model of global policy cost. We use the same model for the change in cost, given by `k,πk − ` k−1,π k−1 = a\n2 + b . However, for the term `kk, which reflects the actual cost of the new policy, we instead use the cost of the new global policy `k,πk , so that a now models the additional loss due to both inaccurate dynamics and inaccurate projection: if `k,πk is much worse than ` k k−1, then either the dynamics were too local, or S-step failed to match the\nperformance of the local policies. In either case, we decrease the step size.3 As before, we can solve for the new step size ′ according to ′ = (`kk−1 − ` k−1,π k−1 )/2(` k k−1 − ` k,π k ). We call this the “global” step size. Details of how each quantity in this equation is computed are provided in Appendix B.3."
    }, {
      "heading" : "5 Relation to Prior Work",
      "text" : "While we’ve discussed the connections between MDGPS and prior guided policy search methods, in this section we’ll also discuss the connections between our method and other policy search methods. One popular supervised policy learning methods is DAGGER [15], which also trains the policy using supervised learning, but does not attempt to adapt the teacher to provide better training data. MDGPS removes the assumption in DAGGER that the supervised learning stage has bounded error against an arbitrary teacher policy. MDGPS does not need to make this assumption, since the teacher can be adapted to the limitations of the global policy learning. This is particularly important when the global policy has computational or observational limitations, such as when learning to use camera images for partially observed control tasks or, as shown in our evaluation, blind peg insertion.\nWhen we sample from the global policy πθ(ut|xt), our method resembles policy gradient methods with KL-divergence constraints [14, 13, 17]. However, policy gradient methods update the policy πθ(ut|xt) at each iteration by linearizing with respect to the policy parameters, which often requires small steps for complex, nonlinear policies, such as neural networks. In contrast, we linearize in the space of time-varying linear dynamics, while the policy is optimized at each iteration with many steps of supervised learning (e.g. stochastic gradient descent). This makes MDGPS much better suited for quickly and efficiently training highly nonlinear, high-dimensional policies."
    }, {
      "heading" : "6 Experimental Evaluation",
      "text" : "We compare several variants of MDGPS and a prior guided policy search method based on Bregman ADMM (BADMM) [6]. We evaluate all methods on one simulated robotic navigation task and two manipulation tasks. Guided policy search code, including BADMM and MDGPS methods, is available at https://www.github.com/cbfinn/gps. Obstacle Navigation. In this task, a 2D point mass (grey) must navigate around obstacles to reach a target (shown in green), using velocities and positions relative to the target. We use N = 5 initial states, with 5 samples per initial state per iteration. The target and obstacles are fixed, but the starting position varies. Peg Insertion. This task, which is more complex, requires controlling a 7 DoF 3D arm to insert a tight-fitting peg into a hole. The hole can be in different positions, and the state consists of joint angles, velocities, and end-effector positions relative to the target. This task is substantially more challenging physically. We use N = 9 different hole positions, with 5 samples per initial state per iteration. Blind Peg Insertion. The last task is a blind variant of the peg insertion task, where the target-relative end effector positions are provided to the local policies, but not to the global policy πθ(ut|xt). This requires the global policy to search for the hole, since no input to the global policy can distinguish between the different initial state xi1. This makes it much more challenging to adapt the global and local policies to each other, and makes it impossible for the global learner to succeed without adaptation of the local policies. We use N = 4 different hole positions, with 5 samples per initial state per iteration.\nThe global policy for each task consists of a fully connected neural network with two hidden layers with 40 rectified linear units. The same settings are used for MDGPS and the prior BADMM-based method, except for the difference in surrogate costs, constraints, and step size adjustment methods discussed in the paper. Results are presented in Figure 1. On the easier point mass and peg tasks, all\n3Although we showed before that the discrepancy depends on ∑T t=1 √ 2 t, here we use\n2. This is a simplification, but the net result is the same: when the global policy is worse than expected, is reduced.\nof the methods achieve similar performance. However, the MDGPS methods are all substantially easier to apply to these tasks, since they have very few free hyperparameters. An initial step size must be selected, but the adaptive step size adjustment rules make this choice less important. In contrast, the BADMM method requires choosing an initial weight on the augmented Lagrangian term, an adjustment schedule for this term, a step size on the dual variables, and a step size for local policies, all of which have a substantial impact on the final performance of the method (the reported results are for the best setting of these parameters, identified with a hyperparameter sweep).\nOn the harder blind peg task, MDGPS consistently outperforms BADMM when sampling from the local policies (“off policy”), with both the classic and global step sizes. This is particularly apparent in the success rates in Table 1, which shows that the MDGPS policies succeed at actually inserting the peg into the hole more often and on more conditions. This suggests that our method is better able to improve global policies particularly in situations where informational or representational constraints make naïve imitation of the local policies insufficient to solve the task. On-policy sampling tends to learn slower, since the approximate projection causes the global policy to lag behind the local policy in performance, but this method is still able to consistently improve the global policies. Sampling from the global policies may be desirable in practice, since the global policies can directly use observations at runtime instead of requiring access to the state [6]. The global step size also tends to be more conservative, but produces more consistent and monotonic improvement."
    }, {
      "heading" : "7 Discussion and Future Work",
      "text" : "We presented a new guided policy search method that corresponds to mirror descent under linearity and convexity assumptions, and showed how prior guided policy search methods can be seen as approximating mirror descent. We provide a bound on the return of the global policy in the nonlinear case, and argue that an appropriate step size can provide improvement of the global policy in this case also. Our analysis provides us with the intuition to design an automated step size adjustment rule, and we illustrate empirically that our method achieves good results on a complex simulated robotic manipulation task while requiring substantially less tuning and hyperparameter optimization than prior guided policy search methods. Manual tuning and hyperparameter searches are a major challenge across a range of deep reinforcement learning algorithms, and developing scalable policy search methods that are simple and reliable is vital to enable further progress.\nAs discussed in Section 5, MDGPS has interesting connections to other policy search methods. Like DAGGER [15], MDGPS uses supervised learning to train the policy, but unlike DAGGER, MDGPS does not assume that the learner is able to reproduce an arbitrary teacher’s behavior with bounded error, which makes it very appealing for tasks with partial observability or other limits on information, such as learning to use camera images for robotic manipulation [6]. When sampling directly from the global policy, MDGPS also has close connections to policy gradient methods that take steps of fixed KL-divergence [14, 17], but with the steps taken in the space of trajectories rather than policy parameters, followed by a projection step. In future work, it would be interesting to explore this connection further, so as to develop new model-free policy gradient methods."
    }, {
      "heading" : "A KL Divergence Between Gaussian Trajectory Distributions",
      "text" : "In this appendix, we derive the KL-divergence between two Gaussian trajectory distributions corresponding to time-varying linear-Gaussian dynamics p(xt+1|xt,ut) and two policies p(ut|xt) and q(ut|xt). The two policies induce Gaussian trajectory distributions (with block-diagonal covariances) according to\np(τ) = p(x1) T∏ t=1 p(xt+1|xt,ut)p(ut|xt), q(τ) = p(x1) T∏ t=1 p(xt+1|xt,ut)q(ut|xt).\nWe can therefore derive their KL-divergence as\nDKL(p(τ)‖q(τ)) = Ep(τ) [log p(τ)− log q(τ)]\n= Ep(τ) [ T∑ t=1 log p(ut|xt)− log q(ut|xt) ]\n= T∑ t=1 Ep(xt,ut) [log p(ut|xt)− log q(ut|xt)]\n= T∑ t=1 −Ep(xt,ut) [log q(ut|xt)]− Ep(xt)[H(p(ut|xt))]\n= T∑ t=1 −Ep(xt,ut) [log q(ut|xt)]−H(p(ut|xt))\nwhere the second step follows because the dynamics and initial state distribution cancel, the third step follows by linearity of expectations, the fourth step from the definition of differential entropy, and the last step follows from the fact that the entropy of a conditional Gaussian distribution is independent on the quantity that it is conditioned on, since it depends only on the covariance and not the mean. We therefore have\nDKL(p(τ)‖q(τ)) = T∑ t=1 −Ep(xt,ut) [log q(ut|xt)]−H(p(ut|xt)).\nBy the definition of KL-divergence, we can also write this as\nDKL(p(τ)‖q(τ)) = T∑ t=1 Ep(xt,ut) [DKL(p(ut|xt)‖q(ut‖xt))] ."
    }, {
      "heading" : "B Details of the MDGPS Algorithm",
      "text" : "A summary of the MDGPS algorithm appears in Algorithm 2, and is repeated below for convenience:\nAlgorithm 4 Mirror descent guided policy search (MDGPS): unknown nonlinear dynamics 1: for iteration k ∈ {1, . . . ,K} do 2: Generate samples Di = {τi,j} by running either pi or πθi 3: Fit linear-Gaussian dynamics pi(xt+1|xt,ut) using samples in Di 4: Fit linearized global policy π̄θi(ut|xt) using samples in Di 5: C-step: pi ← arg minpi Epi(τ)[ ∑T t=1 `(xt,ut)] such that DKL(pi(τ)‖π̄θi(τ)) ≤\n6: S-step: πθ ← arg minθ ∑ t,i,j DKL(πθ(ut|xt,i,j)‖pi(ut|xt,i,j)) (via supervised learning) 7: Adjust (see Section 4.2) 8: end for\nB.1 C-Step Details\nThe C-step solves the following constrained optimization problem:\npi ← arg min pi Epi(τ) [ T∑ t=1 `(xt,ut) ] such that DKL(pi(τ)‖π̄θi(τ)) ≤ .\nThe solution to this problem follows prior work [5], and is reviewed here for completeness. First, the Lagrangian of this problem is given by\nL(pi, η) = Epi(τ) [ T∑ t=1 `(xt,ut) ] + η(DKL(pi(τ)‖π̄θi(τ))− )\n= T∑ t=1 Epi(xt,ut)[`(xt,ut)− η log π̄θi(ut|xt)]− ηH(p(ut|xt))− η ,\nwhere equality follows from the identity in Appendix A. As discussed in prior work [5], we can minimize this Lagrangian with respect to pi by solving an LQR problem (assuming a quadratic expansion of `(xt,ut)) with a surrogate cost\n˜̀(xt,ut) = 1\nη `(xt,ut)− log π̄θi(ut|xt).\nThis follows because LQR can be shown to solve the following problem [5]\npi = arg min pi T∑ t=1 Epi(xt,ut) [ ˜̀(xt,ut) ] −H(pi(ut|xt))\nif we set pi(ut|xt) = N (Ktxt + kt, Q−1u,ut), where Kt and kt are the optimal feedback and feedforward terms, respectively, andQu,ut is the action component of the Q-function matrix computed by LQR, where the full Q-function is given by\nQ(xt,ut) = 1\n2 xTt Qx,xtxt +\n1 2 uTt Qu,utut + u T t Qu,xtxt + x T t Qxt + u T t Qut.\nThis maximum entropy LQR solution also directly from the so-called Kalman duality, which describes a connection between LQR and Kalman smoothing.\nOnce we can minimize the Lagrangian with respect to pi, we can solve the original constrained problem by using dual gradient descent to iteratively adjust the dual variable η. Since there is only a single dual variable, we can find it very efficiently by using a bracketing line search, exploiting the fact that the dual function is convex.\nAs discussed in the paper, the dynamics pi(xt+1|xt,ut) are estimated by using samples (drawn from either the local policy or the global policy) and linear regression. Following prior work [5], the dynamics at each step are fitted using linear regression with a Gaussian mixture model prior. This prior incorporates samples from other time steps and previous iterations to allow the regression procedure to use a very small number of sampled trajectories.\nB.2 S-Step Details\nThe step solves the following optimization problem:\nπθ ← arg min θ ∑ t,i,j DKL(πθ(ut|xt,i,j)‖pi(ut|xt,i,j)).\nSince both πθ(ut|xt) = N (µπ(xt),Σπ(xt)) and pi(ut|xt) = N (Ktixt + kti,Cti) are assumed to be conditionally Gaussian, this objective can be rewritten in closed form as\nπθ ← arg min θ ∑ t,i,j tr[C−1ti Σ π(xt,i,j)]− log |Σπ(xt,i,j)|+\n(µπ(xt,i,j)− µpti(xt,i,j))C −1 ti (µ π(xt,i,j)− µpti(xt,i,j)).\nNote that the last term is simply a weighted quadratic cost on the policy mean µπ(xt,i,j , which lends itself to simple and straightforward optimization using stochastic gradient descent. In our implementation, we use a policy where the covariance Σπ(xt) is independent of the state xt, and therefore we can solve for the covariance in closed form, as discussed in prior work [6]. However, in general, the covariance could also be optimized using stochastic gradient descent.\nB.3 Step Size Adjustment\nAs discussed in Section 4.2, the step size adjustment procedure requires estimating quantities of the type `km = ∑T t=1Epk(xt,ut)[`(xt,ut)], where p\nk(xt,ut) is the marginal of the local policy used to generate samples at iteration k and the dynamics fitted at iteration m (not to be confused with pi, which we use to denote the local policy for the ith initial state, independent of the iteration number). We also use terms of the form `k,πm = ∑T t=1Eπ̄kθ (xt,ut)[`(xt,ut)], which give the expected cost under the dynamics at iteration m and the linearized global policy at iteration k. Specifically, we require `k−1k−1, ` k k−1, and ` k k, as well as the corresponding global policy terms ` k−1,π k−1 , ` k,π k−1, and ` k,π k .\nAll of these terms can be computed analytically, since the fitted dynamics, local policies, and linearized global policy π̄θ(ut|xt) are all linear-Gaussian. The state-action marginals p(xt,ut) in linear-Gaussian policies can be computed simply by propagating Gaussian densities forward in time, according to\nµxt,ut = [ µxt Ktµxt + kt ] Σxt,ut = [ Σxt ΣxtK T t KtΣxt KtΣxtK T t + Ct ] µxt+1 = ftµxt,ut + fct Σxt+1 = ftΣxt,utf T t + Ft\nwhere we have p(xt+1|xt,ut) = N (ft(xt,ut)T + fct,Ft) and p(ut|xt) = N (Ktxt +kt,Ct), and then we can estimate the expectation of the cost at time t simply by integrating the quadratic cost under the Gaussian state-action marginals."
    }, {
      "heading" : "C Global Policy Cost Bounds",
      "text" : "In this appendix, we prove the bound on the policy cost discussed in Section 4.1. The proof combines the earlier results from Ross et al. [15] and Schulman et al. [17], and extends them to the case of finite-horizon episodic tasks.\nC.1 Policy State Distribution Bound\nWe begin by proving Lemma 4.1, which we restate below with slightly simplified notation, replacing πθ by q:\nLemma C.1 Let t = maxxt DKL(p(ut|xt)‖q(ut|xt). Then DTV(p(xt)‖q(xt)) ≤ 2 ∑T t=1 √ 2 t.\nThe proof first requires introducing a lemma that relates the total variation divergence maxxt ‖p(ut|xt)− q(ut|xt)‖1 between two policies to the probability that the policies will take the same action in a discrete setting (extensions to the continuous setting are also possible):\nLemma C.2 Assume that maxxt ‖p(ut|xt)− q(ut|xt)‖1 ≤ √\n2 t, then the probability that p and q take the same action at time step t is 1− √ 2 t.\nThe proof for this lemma was presented by Schulman et al. [17]. We can use it to bound the state distribution difference as following. First, we are acting according to p(ut|xt), the probability that the same action would have been taken by q(ut|xt), based on Lemma C.2, is (1 − √ 2 t), so the probability that all actions up to time t would have been taken by q(ut|xt) is given by∏t t′=1(1− √ 2 t′). We can therefore express the state distribution p(xt) as\np(xt) =\n[ t∏\nt′=1\n(1− √\n2 t′) ] q(xt) + ( 1−\nt∏ t′=1 (1− √ 2 t′)\n) p̃(xt)\n=\n[ t∏\nt′=1\n(1− √\n2 t′) ] [q(xt)− p̃(xt)] + p̃(xt),\nwhere p̃(xt) is some other distribution. In order to bound DTV(p(xt)‖q(xt)) = ‖p(xt)− q(xt)‖1, we can substitute this equation into ‖p(xt)− q(xt)‖1 to get\n‖p(xt)− q(xt)‖1 = ∥∥∥∥∥ [ t∏ t′=1 (1− √ 2 t′) ] [q(xt)− p̃(xt)] + p̃(xt)− q(xt) ∥∥∥∥∥ = ∥∥∥∥∥ [ 1− t∏\nt′=1\n(1− √\n2 t′) ] [q(xt)− p̃(xt)] ∥∥∥∥∥ = [ 1−\nt∏ t′=1 (1− √ 2 t′)\n] ‖q(xt)− p̃(xt)‖\n≤ 2 [ 1−\nt∏ t′=1 (1− √ 2 t′)\n] ,\nwhere the last inequality comes from the fact that ‖q(xt)− p̃(xt)‖ ≤ 2 for discrete distributions. With continuous densities, we could extend the result by taking the limit of an infinitely fine discretization. Next, we note that\nt∏ t′=1 (1− √ 2 t′) ≥ 1− t∑ t′ √ 2 t′ ,\nand therefore we have\n‖p(xt)− q(xt)‖1 ≤ 2 t∑\nt′=1\n√ 2 t′\nThis completes the proof.\nC.2 Total Policy Cost Bound\nIn this appendix, we use the result above to prove Lemma 4.2. This result is based on Ross et al. [15], but extends the proof to the case of time-varying finite-horizon systems. We first restate the lemma under the same notation as the previous appendix: Lemma C.3 If DTV(p(xt)‖πθ(xt)) ≤ 2 ∑T t=1 √ 2 t, then we can bound the total cost of πθ as\nT∑ t=1 Eπθ(xt,ut)[`(xt,ut)] ≤ T∑ t=1 [ Ep(xt,ut)[`(xt,ut)] + 2 √ tQmax,t ] ,\nwhere Qmax,t = ∑T t′=t maxxt′ ,ut′ `(xt′ ,ut′), the maximum total cost from time t to T .\nWe bound the cost of q at time step t according to\nEq(xt,ut)[`(xt,ut)] = 〈q(xt,ut), `(xt,ut)〉 = 〈q(xt,ut)− p(xt)q(ut|xt), `(xt,ut)〉+ 〈p(xt)q(ut|xt), `(xt,ut)〉 = 〈q(ut|xt)[q(xt)− p(xt)], `(xt,ut)〉+ 〈p(xt)[q(ut|xt)− p(ut|xt)], `(xt,ut)〉+ Ep(xt,ut)[`(xt,ut)] ≤ Ep(xt,ut)[`(xt,ut)] + ‖q(xt)− p(xt)‖1 maxxt,ut `(xt,ut) + ‖q(ut|xt)− p(ut|xt)‖1 max xt,ut `(xt,ut)\n≤ Ep(xt,ut)[`(xt,ut)] + maxxt,ut `(xt,ut)\n√ 2 t + 2 max\nxt,ut `(xt,ut) t∑ t′=1 √ 2 t′\nIf we add up the above quantity over all time t, we get T∑ t=1 Eq(xt,ut)[`(xt,ut)] ≤ T∑ t=1 Ep(xt,ut)[`(xt,ut)]+ T∑ t=1 √ 2 t max xt,ut `(xt,ut)+2 T∑ t=1 max xt,ut `(xt,ut) t∑ t′=1 √ 2 ′t\nwhich we can rewrite as T∑ t=1 Eq(xt,ut)[`(xt,ut)] ≤ T∑ t=1 [ Ep(xt,ut)[`(xt,ut)] + √ 2 t max xt,ut `(xt,ut) + 2 √ 2 tQmax,t ] where Qmax,t = ∑T t′=t maxxt′ ,ut′ `(xt′ ,ut′)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Guided policy search algorithms can be used to optimize complex nonlinear poli-<lb>cies, such as deep neural networks, without directly computing policy gradients<lb>in the high-dimensional parameter space. Instead, these methods use supervised<lb>learning to train the policy to mimic a “teacher” algorithm, such as a trajectory<lb>optimizer or a trajectory-centric reinforcement learning method. Guided policy<lb>search methods provide asymptotic local convergence guarantees by construction,<lb>but it is not clear how much the policy improves within a small, finite number of<lb>iterations. We show that guided policy search algorithms can be interpreted as an<lb>approximate variant of mirror descent, where the projection onto the constraint<lb>manifold is not exact. We derive a new guided policy search algorithm that is sim-<lb>pler and provides appealing improvement and convergence guarantees in simplified<lb>convex and linear settings, and show that in the more general nonlinear setting, the<lb>error in the projection step can be bounded. We provide empirical results on several<lb>simulated robotic navigation and manipulation tasks that show that our method is<lb>stable and achieves similar or better performance when compared to prior guided<lb>policy search methods, with a simpler formulation and fewer hyperparameters.",
    "creator" : "LaTeX with hyperref package"
  }
}