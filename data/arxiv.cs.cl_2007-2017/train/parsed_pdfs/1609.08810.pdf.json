{
  "name" : "1609.08810.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Effective Combination of Language and Vision Through Model Composition and the R-CCA Method",
    "authors" : [ "Hagar Loeub", "Roi Reichart" ],
    "emails" : [ "hagar.loeub@gmail.com", "roiri@ie.technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We address the problem of integrating textual and visual information in vector space models for word meaning representation. We first present the Residual CCA (R-CCA) method, that complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared, max correlation, space. We then show that constructing visual and textual representations and then post-processing them through composition of common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (a.k.a sequential modeling) yields high quality models. On five standard semantic benchmarks our sequential models outperform recent multimodal representation learning alternatives, including ones that rely on joint representation learning. For two of these benchmarks our R-CCA method is part of the Best configuration our algorithm yields."
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, vector space models (VSMs), deriving word meaning representations from word cooccurrence patterns in text, have become prominent in lexical semantics research (Turney et al., 2010; Clark, 2012). Recent work has demonstrated that when other modalities, particularly the visual, are exploited together with text, the resulting multimodal representations outperform strong textual models on a variety of tasks (Baroni, 2016).\nModels that integrate text and vision can be largely divided to two types. Sequential models\nfirst separately construct visual and textual representations and then merge them using a variety of techniques: concatenation (Bruni et al., 2011; Silberer et al., 2013; Kiela and Bottou, 2014), linear weighted combination of vectors (Bruni et al., 2012; Bruni et al., 2014) or linear interpolation of model scores (Bruni et al., 2014), Canonical Correlation Analysis and its kernalized version (CCA, (Hill et al., 2014; Silberer and Lapata, 2012; Silberer et al., 2013)), Singular Value Decomposition (SVD, (Bruni et al., 2014)) and Weighted Gram Matrix Combination (Reichart and Korhonen, 2013; Hill et al., 2014)). Joint models directly learn a joint representation from textual and visual resources using Bayesian modeling (Andrews et al., 2009; Feng and Lapata, 2010; Roller and Im Walde, 2013) and various neural network (NN) techniques: autoencoders (Silberer and Lapata, 2014), extensions of word2vec skip-gram (Hill and Korhonen, 2014; Lazaridou et al., 2015) and others (e.g. (Howell et al., 2005)).\nThe focused contribution of this short paper is two-fold. First, we advocate the sequential approach for text and vision combination and show that when a systematic search in the space of configurations of composition of common modeling motifs (Grosse, 2014) is employed, this approach outperforms recent joint models as well as sequential models that do not thoroughly search the space of configurations. This finding has important implications for future research as it advocates the development of efficient search techniques in configuration spaces of the type we explore.\nParticularly, we experiment with unimodal dimensionality reduction with Principal Component\nar X\niv :1\n60 9.\n08 81\n0v 2\n[ cs\n.C L\n] 4\nO ct\n2 01\nAnalysis ((PCA, (Jolliffe, 2002)), multimodal fusion with Canonical Correlation Analysis (CCA, (Hardoon et al., 2004)) and model score combination with linear interpolation (LI, (Bruni et al., 2014)). The composed models outperform strong alternatives on semantic benchmarks for word pair similarity and association: MEN (Bruni et al., 2014), WordSim353 (WS, (Finkelstein et al., 2001)), SimLex999 (SL (Hill et al., 2015)), SemSim and VisSim (SSim, VSim, (Silberer and Lapata, 2014)).\nOur second contribution is in proposing the Residual CCA (R-CCA) method for multimodal fusion. This method complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared space. Since CCA aims to maximize the correlation between the projected signals, the residual signals intuitively represent uncorrelated components of the original signals. Empirically, including R-CCA in the configuration space improves results on two evaluation benchmarks. Moreover, for all five benchmarks R-CCA substantially outperforms CCA."
    }, {
      "heading" : "2 Multimodal Composition",
      "text" : ""
    }, {
      "heading" : "2.1 Modeling Motifs",
      "text" : "PCA is a standard dimensionality reduction method. We hence do not describe its details here and refer the interested reader to (Jolliffe, 2002).\nCCA finds two projection vectors, one for each original vector, such that projecting the original vectors yields the highest possible correlation under linear projection. In short, given an n word vocabulary, with representations X ∈ Rn×d1 and Y ∈ Rn×d2 , CCA seeks two sets of projection vectors V ∈ Rd1×d and W ∈ Rd2×d that maximize the correlation (ρ) between the projected vectors of each of the words: V,W = argmaxV ′ ,W ′ ρ(XV ′ , Y W ′ ). The final projection is: X ′ = XV and Y ′ = YW .\nResidual-CCA (R-CCA) CCA aims to project the involved representations into a shared space where the correlation between them is maximized. The underlying assumption of this method is hence that multiple modalities can facilitate learning through exploitation of their shared signal. A complementary point of view would suggest that important in-\nformation can also be found in the dissimilar components of the monomodal signals.\nWhile there may be many ways to implement this idea, we explore here a simple one which we call the residuals approach. Denoting the original monomodal signals with X and Y and their CCA projections with X ′ and Y ′ respectively, the residual signals are defined as: Rx = X − X ′ and Ry = Y −Y ′. Notice that a monomodal signal (e.g. X) and its CCA projection (e.g. X ′) may not be of the same dimension. In such cases we first project the original signal (X) to the dimensionality of the projected signal (X ′) with PCA.\nLI combines the scores produced by two VSMs for a word pair, scm1(wi, wj) and scm2(wi, wj), using the linear equation (α ∈ [0, 1]): Score(wi, wj) = α · scm1(wi, wj) + (1− α) · scm2(wi, wj)."
    }, {
      "heading" : "2.2 Motif Composition",
      "text" : "We divide the above modeling motifs to three layers, to facilitate an efficient systematic optimal configuration search (Figure 1): (a) Data: (a.1) original vectors; or (a.2) original vectors projected with unimodal PCA; (b) Fusion: (b.1) CCA and (b.2) RCCA, each method outputting two projected vectors per word, one for each modality; (c) Combination: (c.1) vector concatenation; and (c.2) linear interpolation (LI) of model scores. In our search, a higher\nlayer method considers inputs from all lower layer methods as long as both inputs are the output of the same method. That is, CCA (layer b.1) is ap-\nplied to original textual and visual vector pairs (output of a.1) as well as to PCA-transformed vectors (a.2), but not, e.g., to PCA-transformed visual vectors (a.2) paired with original textual vectors (a.1). Vector concatenation (c.1) and linear score interpolation (c.2), in turn, are applied to all the inputs and outputs of CCA and of R-CCA.\nThe only exception is that we allow an output of CCA (e.g. projected visual vectors) and an output of R-CCA (e.g. residual textual vectors) as input to layer c, as two projections or two residuals may convey very similar information. To facilitate efficiency further, CCA and R-CCA are only applied to textual and visual vectors of the same dimensionality. We leave the exploration of other, possibly more complex, search spaces to future work.\nFor each benchmark (Section 3) we search for its Best configuration: the optimal sequence of the above motifs, at most one from each layer, together with the optimal assignment of their parameters. We do not aim to develop efficient algorithms for optimal configuration inference, but rather employ an exhaustive grid search approach. The high quality configurations we find, advocate future development of efficient search algorithms."
    }, {
      "heading" : "3 Data and Experiments",
      "text" : "Input Vectors Our textual VSM is word2vec skipgram (Mikolov et al., 2013), 1 trained on the 8G words corpus generated by the word2vec script.2 We followed the hyperparameter setting of (Schwartz et al., 2015) and, particularly, set vector dimensionality to 500. For the visual modality, we used the 5100 4096-dimensional vectors of Lazaridou et al. (2015), extracted with a pre-trained Convolutional Neural Network (CNN, (Krizhevsky et al., 2012)) and the Caffe toolkit (Jia et al., 2014) from 100 pictures sampled for each word from its ImageNet (Deng et al., 2009) entry. While there are various alternatives for both textual and visual representations, those we chose are based on state-of-the-art techniques.\nBenchmarks We report the Spearman rank correlation (ρ) between model and human scores, for the word pairs in five benchmarks: MEN, WS, SL, SSim and VSim. While all the words in our benchmarks appear in our textual corpus, only a fraction of them appears in ImageNet, our source of visual input. Hence, following Lazaridou et al. (2015),\n1https://code.google.com/p/word2vec/ 2\ncode.google.com/p/word2vec/source/browse/trunk/demo-train-big-model-v1.sh\nfor each benchmark we report results only for word pairs consisting of words that are represented in ImageNet. A model word pair score is the cosine similarity between the vectors learned for its words.\nParameter Tuning We jointly optimized parameters together with the decision of which modeling motif to select at each layer, if at all. For PCA, CCA and R-CCA we iterated over dimensionality values from 50 onward in steps of 50, till the minimum dimensionality of the input sets. For LI, we iterated over α ∈ {0, 0.1, . . . , 1}. Among the best performing configurations we selected the one with the lowest dimension output vectors.\nNote that there is no agreed upon split of our benchmarks, except from MEN, to development and test portions. Therefore, to facilitate future comparison with our work, the main results we report are with the best configuration for each benchmark, as tuned on the entire benchmark. We also show that our results generalize well across evaluation sets, including MEN’s dev/test split.\nAlternative Models We compare our results to strong alternatives: MSKIP-A and MMSKIP-B ((Lazaridou et al., 2015), joint models), the best performing model of Bruni et al. (2014) and Kiela and Bottou (2014) (sequential models).3 While our results are not directly comparable to these models due to different training sources and parameter tuning strategies,4 this comparison puts them in context."
    }, {
      "heading" : "4 Results",
      "text" : "Table 1 presents the results. Best outperforms the unimodal models (Skip and CNN) and the alternative models. The gains (in ρ points) are: MEN: 4, WS: 1, SL: 9, SSim: 7 and VSim: 1. R-CCA is included in Best for MEN and SL, improving over the best configuration that does not include it by 2\n3To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al. (2014) report results on a different subset than ours, consisting of 252 word pairs. Kiela and Bottou (2014) report results on our subset of WS, and we copy their best result. Silberer and Lapata (2014) also report results for SSim and VSim but for the entire sets rather than for our subsets.\n4Section 5 of Lazaridou et al. (2015) provides the details of the alternative models, their training and parameter tuning.\nand 1 ρ points, respectively. Furthermore, R-CCA outperforms CCA on all five benchmarks (by 6-31 ρ points) and particularly on MEN, SSim and VSim.\nObservations The five Best configurations share meaningful patterns. (1) Best never includes concatenation (layer c.1); (2) LI is always included in Best and the weights assigned to the textual and visual modalities are mostly balanced. Particularly, the weight of the textual modality is 0.4-0.7 for MEN, SL, SSim and VSim; (3) In 3 out of 5 cases, Best (LI), that linearly interpolates the scores yielded by the input textual and visual vectors without PCA, CCA or R-CCA processing, outperforms the models from previous work and the unimodal models; (4) In all Best configurations, the reduced dimensionality is 50-250, which is encouraging as processing smaller vectors requires less resources.\nGeneralization We now show that our results generalize well across evaluation sets. First, for the portion of the MEN development set that overlaps with ImageNet, our Best MEN configuration is the thirdbest configuration, with ρ = 0.78. We also tested the Best configuration as tuned on each of the benchmarks, on the remaining four benchmarks. We observed that WS, MEN and SL serve as good development sets for each other. Best SL configuration (Best-SL): ρ = 0.7 on WS and ρ = 0.77 on MEN, Best-WS: ρ = 0.61 (SL) and ρ = 0.74 (MEN), and Best-MEN: ρ = 0.6 (SL) and ρ = 0.71 (WS). The performance of these models on SSim and VSim, however, is substantially lower than that of the Best models of these sets (e.g. ρ = 0.62 for Best-SL on SSim, ρ = 0.5 for Best-WS on VSim, compared to of ρ = 0.8 and ρ = 0.68, respectively). Likewise, SSim and VSim, that consist of the same word pairs scored along different dimensions, form good development sets for each other (ρ = 0.66 for Best-SSim on VSim, ρ = 0.78 for Best-VSim on SSim), but not for WS or SL. That is, each benchmark has other benchmarks that can serve as its dev. set."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We demonstrated the power of composition of common modeling motifs in multimodal VSM construction and presented the R-CCA method that exploits the residuals of the CCA signals. Our model yields\nstate-of-the-art results on 5 leading semantic benchmarks, for two of which R-CCA is part of the Best configuration. Moreover, R-CCA performs much better than CCA on all five benchmarks.\nOur results hence advocate two research directions. First, they encourage sequential modeling with systematic search in the configuration space for multimodal combination. Our future goal is making model composition a standard tool for this problem, by developing efficient inference algorithms for optimal configurations in possibly more complex search spaces than those we explored with an exhaustive grid search. Second, the encouraging results of R-CCA emphasize the potential of informed postprocessing of the CCA output. We intend to deeply delve into this issue in the immediate future."
    } ],
    "references" : [ {
      "title" : "Integrating experiential and distributional data to learn semantic representations",
      "author" : [ "Andrews et al.2009] Mark Andrews", "Gabriella Vigliocco", "David Vinson" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "Andrews et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Andrews et al\\.",
      "year" : 2009
    }, {
      "title" : "Grounding distributional semantics in the visual world",
      "author" : [ "Marco Baroni" ],
      "venue" : "Language and Linguistics Compass,",
      "citeRegEx" : "Baroni.,? \\Q2016\\E",
      "shortCiteRegEx" : "Baroni.",
      "year" : 2016
    }, {
      "title" : "Distributional semantics from text and images",
      "author" : [ "Bruni et al.2011] Elia Bruni", "Giang Binh Tran", "Marco Baroni" ],
      "venue" : "In Proc. of the GEMS workshop on geometrical models of natural language semantics,EMNLP,",
      "citeRegEx" : "Bruni et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bruni et al\\.",
      "year" : 2011
    }, {
      "title" : "Distributional semantics in technicolor",
      "author" : [ "Bruni et al.2012] Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran" ],
      "venue" : "In Proc. of ACL,",
      "citeRegEx" : "Bruni et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bruni et al\\.",
      "year" : 2012
    }, {
      "title" : "Multimodal distributional semantics",
      "author" : [ "Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR),",
      "citeRegEx" : "Bruni et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bruni et al\\.",
      "year" : 2014
    }, {
      "title" : "Vector space models of lexical meaning. Handbook of Contemporary Semantics, Wiley-Blackwell, à paraı̂tre",
      "author" : [ "Stephen Clark" ],
      "venue" : null,
      "citeRegEx" : "Clark.,? \\Q2012\\E",
      "shortCiteRegEx" : "Clark.",
      "year" : 2012
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Deng et al.2009] Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei" ],
      "venue" : "In Proc. of CVPR,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Visual information in semantic representation",
      "author" : [ "Feng", "Lapata2010] Yansong Feng", "Mirella Lapata" ],
      "venue" : "In Proc. of NAACL,",
      "citeRegEx" : "Feng et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2010
    }, {
      "title" : "Placing search in context: The concept revisited",
      "author" : [ "Solan", "Gadi Wolfman", "Eytan Ruppin." ],
      "venue" : "Proc. of WWW, pages 406–414.",
      "citeRegEx" : "Solan et al\\.,? 2001",
      "shortCiteRegEx" : "Solan et al\\.",
      "year" : 2001
    }, {
      "title" : "Model selection in compositional spaces",
      "author" : [ "Roger Baker Grosse" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Grosse.,? \\Q2014\\E",
      "shortCiteRegEx" : "Grosse.",
      "year" : 2014
    }, {
      "title" : "Canonical correlation analysis: An overview with application to learning methods",
      "author" : [ "Sandor Szedmak", "John Shawe-Taylor" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hardoon et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hardoon et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what i mean",
      "author" : [ "Hill", "Korhonen2014] Felix Hill", "Anna Korhonen" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "Hill et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-modal models for concrete and abstract concept meaning. Transactions of the Association for Computational Linguistics, 2:285–296",
      "author" : [ "Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen" ],
      "venue" : null,
      "citeRegEx" : "Hill et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2014
    }, {
      "title" : "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
      "author" : [ "Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen" ],
      "venue" : null,
      "citeRegEx" : "Hill et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "A model of grounded language acquisition: Sensorimotor features improve lexical and grammatical learning",
      "author" : [ "Damian Jankowicz", "Suzanna Becker" ],
      "venue" : "Journal of Memory and Language,",
      "citeRegEx" : "Howell et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Howell et al\\.",
      "year" : 2005
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Jia et al.2014] Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell" ],
      "venue" : "In Proc. of the ACM International Conference on Mul-",
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Principal component analysis",
      "author" : [ "Ian Jolliffe" ],
      "venue" : "Wiley Online Library",
      "citeRegEx" : "Jolliffe.,? \\Q2002\\E",
      "shortCiteRegEx" : "Jolliffe.",
      "year" : 2002
    }, {
      "title" : "Learning image embeddings using convolutional neural networks for improved multi-modal semantics",
      "author" : [ "Kiela", "Bottou2014] Douwe Kiela", "Léon Bottou" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "Kiela et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In Proc. of NIPS",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Combining language and vision with a multimodal skip-gram model",
      "author" : [ "Nghia The Pham", "Marco Baroni" ],
      "venue" : "In Proc. of NAACL",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Proc. of NIPS",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Improved lexical acquisition",
      "author" : [ "Reichart", "Korhonen2013] Roi Reichart", "Anna Korhonen" ],
      "venue" : null,
      "citeRegEx" : "Reichart et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Reichart et al\\.",
      "year" : 2013
    }, {
      "title" : "A multimodal lda model integrating textual, cognitive and visual modalities",
      "author" : [ "Roller", "Im Walde2013] Stephen Roller", "Sabine Schulte Im Walde" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "Roller et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2013
    }, {
      "title" : "Symmetric pattern based word embeddings for improved word similarity prediction",
      "author" : [ "Roi Reichart", "Ari Rappoport" ],
      "venue" : "In Proc. CoNLL",
      "citeRegEx" : "Schwartz et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2015
    }, {
      "title" : "Grounded models of semantic representation",
      "author" : [ "Silberer", "Lapata2012] Carina Silberer", "Mirella Lapata" ],
      "venue" : "In Proc. of EMNLP-CoNLL",
      "citeRegEx" : "Silberer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Silberer et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning grounded meaning representations with autoencoders",
      "author" : [ "Silberer", "Lapata2014] Carina Silberer", "Mirella Lapata" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Silberer et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Silberer et al\\.",
      "year" : 2014
    }, {
      "title" : "Models of semantic representation with visual attributes",
      "author" : [ "Vittorio Ferrari", "Mirella Lapata" ],
      "venue" : "In Proc. of ACL,",
      "citeRegEx" : "Silberer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Silberer et al\\.",
      "year" : 2013
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "Patrick Pantel" ],
      "venue" : "Journal of artificial intelligence research,",
      "citeRegEx" : "Turney and Pantel,? \\Q2010\\E",
      "shortCiteRegEx" : "Turney and Pantel",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "In recent years, vector space models (VSMs), deriving word meaning representations from word cooccurrence patterns in text, have become prominent in lexical semantics research (Turney et al., 2010; Clark, 2012).",
      "startOffset" : 176,
      "endOffset" : 210
    }, {
      "referenceID" : 1,
      "context" : "Recent work has demonstrated that when other modalities, particularly the visual, are exploited together with text, the resulting multimodal representations outperform strong textual models on a variety of tasks (Baroni, 2016).",
      "startOffset" : 212,
      "endOffset" : 226
    }, {
      "referenceID" : 2,
      "context" : "Sequential models first separately construct visual and textual representations and then merge them using a variety of techniques: concatenation (Bruni et al., 2011; Silberer et al., 2013; Kiela and Bottou, 2014), linear weighted combination of vectors (Bruni et al.",
      "startOffset" : 145,
      "endOffset" : 212
    }, {
      "referenceID" : 26,
      "context" : "Sequential models first separately construct visual and textual representations and then merge them using a variety of techniques: concatenation (Bruni et al., 2011; Silberer et al., 2013; Kiela and Bottou, 2014), linear weighted combination of vectors (Bruni et al.",
      "startOffset" : 145,
      "endOffset" : 212
    }, {
      "referenceID" : 3,
      "context" : ", 2013; Kiela and Bottou, 2014), linear weighted combination of vectors (Bruni et al., 2012; Bruni et al., 2014) or linear interpolation of model scores (Bruni et al.",
      "startOffset" : 72,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : ", 2013; Kiela and Bottou, 2014), linear weighted combination of vectors (Bruni et al., 2012; Bruni et al., 2014) or linear interpolation of model scores (Bruni et al.",
      "startOffset" : 72,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : ", 2014) or linear interpolation of model scores (Bruni et al., 2014), Canonical Correlation Analysis and its kernalized version (CCA, (Hill et al.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : ", 2014), Canonical Correlation Analysis and its kernalized version (CCA, (Hill et al., 2014; Silberer and Lapata, 2012; Silberer et al., 2013)), Singular Value Decomposition (SVD, (Bruni et al.",
      "startOffset" : 73,
      "endOffset" : 142
    }, {
      "referenceID" : 26,
      "context" : ", 2014), Canonical Correlation Analysis and its kernalized version (CCA, (Hill et al., 2014; Silberer and Lapata, 2012; Silberer et al., 2013)), Singular Value Decomposition (SVD, (Bruni et al.",
      "startOffset" : 73,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : ", 2013)), Singular Value Decomposition (SVD, (Bruni et al., 2014)) and Weighted Gram Matrix Combination (Reichart and Korhonen, 2013; Hill et al.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : ", 2014)) and Weighted Gram Matrix Combination (Reichart and Korhonen, 2013; Hill et al., 2014)).",
      "startOffset" : 46,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "Joint models directly learn a joint representation from textual and visual resources using Bayesian modeling (Andrews et al., 2009; Feng and Lapata, 2010; Roller and Im Walde, 2013) and various neural network (NN) techniques: autoencoders (Silberer and Lapata, 2014), extensions of word2vec skip-gram (Hill and Korhonen, 2014; Lazaridou et al.",
      "startOffset" : 109,
      "endOffset" : 181
    }, {
      "referenceID" : 19,
      "context" : ", 2009; Feng and Lapata, 2010; Roller and Im Walde, 2013) and various neural network (NN) techniques: autoencoders (Silberer and Lapata, 2014), extensions of word2vec skip-gram (Hill and Korhonen, 2014; Lazaridou et al., 2015) and others (e.",
      "startOffset" : 177,
      "endOffset" : 226
    }, {
      "referenceID" : 14,
      "context" : "(Howell et al., 2005)).",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "First, we advocate the sequential approach for text and vision combination and show that when a systematic search in the space of configurations of composition of common modeling motifs (Grosse, 2014) is employed, this approach outperforms recent joint models as well as sequential models that do not thoroughly search the space of configurations.",
      "startOffset" : 186,
      "endOffset" : 200
    }, {
      "referenceID" : 16,
      "context" : "Analysis ((PCA, (Jolliffe, 2002)), multimodal fusion with Canonical Correlation Analysis (CCA, (Hardoon et al.",
      "startOffset" : 16,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "Analysis ((PCA, (Jolliffe, 2002)), multimodal fusion with Canonical Correlation Analysis (CCA, (Hardoon et al., 2004)) and model score combination with linear interpolation (LI, (Bruni et al.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : ", 2004)) and model score combination with linear interpolation (LI, (Bruni et al., 2014)).",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "The composed models outperform strong alternatives on semantic benchmarks for word pair similarity and association: MEN (Bruni et al., 2014), WordSim353 (WS, (Finkelstein et al.",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 13,
      "context" : ", 2001)), SimLex999 (SL (Hill et al., 2015)), SemSim and VisSim (SSim, VSim, (Silberer and Lapata, 2014)).",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 16,
      "context" : "We hence do not describe its details here and refer the interested reader to (Jolliffe, 2002).",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 19,
      "context" : "MMSKIP-A and MMSKIP-B are the (joint) models of Lazaridou et al. (2015), BR-EA-14 is the best performing model of Bruni",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : "Input Vectors Our textual VSM is word2vec skipgram (Mikolov et al., 2013), 1 trained on the 8G words corpus generated by the word2vec script.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : "2 We followed the hyperparameter setting of (Schwartz et al., 2015) and, particularly, set vector dimensionality to 500.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "(2015), extracted with a pre-trained Convolutional Neural Network (CNN, (Krizhevsky et al., 2012)) and the Caffe toolkit (Jia et al.",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : ", 2012)) and the Caffe toolkit (Jia et al., 2014) from 100 pictures sampled for each word from its ImageNet (Deng et al.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : ", 2014) from 100 pictures sampled for each word from its ImageNet (Deng et al., 2009) entry.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "For the visual modality, we used the 5100 4096-dimensional vectors of Lazaridou et al. (2015), extracted with a pre-trained Convolutional Neural Network (CNN, (Krizhevsky et al.",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "Hence, following Lazaridou et al. (2015),",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : "Alternative Models We compare our results to strong alternatives: MSKIP-A and MMSKIP-B ((Lazaridou et al., 2015), joint models), the best performing model of Bruni et al.",
      "startOffset" : 88,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : ", 2015), joint models), the best performing model of Bruni et al. (2014) and Kiela and Bottou (2014) (sequential models).",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : ", 2015), joint models), the best performing model of Bruni et al. (2014) and Kiela and Bottou (2014) (sequential models).",
      "startOffset" : 53,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al.",
      "startOffset" : 74,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS.",
      "startOffset" : 74,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al.",
      "startOffset" : 74,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al. (2014) report results on a different subset than ours, consisting of 252 word pairs.",
      "startOffset" : 74,
      "endOffset" : 250
    }, {
      "referenceID" : 2,
      "context" : "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al. (2014) report results on a different subset than ours, consisting of 252 word pairs. Kiela and Bottou (2014) report results on our subset of WS, and we copy their best result.",
      "startOffset" : 74,
      "endOffset" : 352
    }, {
      "referenceID" : 2,
      "context" : "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al. (2014) report results on a different subset than ours, consisting of 252 word pairs. Kiela and Bottou (2014) report results on our subset of WS, and we copy their best result. Silberer and Lapata (2014) also report results for SSim and VSim but for the entire sets rather than for our subsets.",
      "startOffset" : 74,
      "endOffset" : 446
    }, {
      "referenceID" : 2,
      "context" : "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al. (2014) report results on a different subset than ours, consisting of 252 word pairs. Kiela and Bottou (2014) report results on our subset of WS, and we copy their best result. Silberer and Lapata (2014) also report results for SSim and VSim but for the entire sets rather than for our subsets. Section 5 of Lazaridou et al. (2015) provides the details of the alternative models, their training and parameter tuning.",
      "startOffset" : 74,
      "endOffset" : 574
    } ],
    "year" : 2016,
    "abstractText" : "We address the problem of integrating textual and visual information in vector space models for word meaning representation. We first present the Residual CCA (R-CCA) method, that complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared, max correlation, space. We then show that constructing visual and textual representations and then post-processing them through composition of common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (a.k.a sequential modeling) yields high quality models. On five standard semantic benchmarks our sequential models outperform recent multimodal representation learning alternatives, including ones that rely on joint representation learning. For two of these benchmarks our R-CCA method is part of the Best configuration our algorithm yields.",
    "creator" : "LaTeX with hyperref package"
  }
}