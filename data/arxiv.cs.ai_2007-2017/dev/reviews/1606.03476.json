{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Generative Adversarial Imitation Learning", "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.", "histories": [["v1", "Fri, 10 Jun 2016 20:51:29 GMT  (179kb,D)", "http://arxiv.org/abs/1606.03476v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jonathan ho", "stefano ermon"], "accepted": true, "id": "1606.03476"}
