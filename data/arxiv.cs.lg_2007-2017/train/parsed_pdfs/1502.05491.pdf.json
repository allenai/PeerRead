{
  "name" : "1502.05491.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimizing Text Quantifiers for Multivariate Loss Functions",
    "authors" : [ "ANDREA ESULI" ],
    "emails" : [ "andrea.esuli@isti.cnr.it.", "fsebastiani@qf.org.qa.", "permissions@acm.org." ],
    "sections" : [ {
      "heading" : null,
      "text" : "AAAA:1\nAuthors’ address: Andrea Esuli, Istituto di Scienza e Tecnologie dell’Informazione, Consiglio Nazionale delle Ricerche, Via Giuseppe Moruzzi 1, 56124 Pisa, Italy. E-mail: andrea.esuli@isti.cnr.it. Fabrizio Sebastiani, Qatar Computing Research Institute, PO Box 5825, Doha, Qatar. E-mail: fsebastiani@qf.org.qa. Fabrizio Sebastiani is on leave from Consiglio Nazionale delle Ricerche. The order in which the authors are listed is purely alphabetical; each author has given an equally important contribution to this work. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c© YYYY ACM 1556-4681/YYYY/-ARTAA $15.00 DOI:http://dx.doi.org/10.1145/0000000.0000000\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nar X\niv :1\n50 2.\n05 49\n1v 2\n[ cs\n.L G\n] 1\n5 A\npr 2\n01 5\nOptimizing Text Quantifiers for Multivariate Loss Functions\nANDREA ESULI, Consiglio Nazionale delle Ricerche FABRIZIO SEBASTIANI, Qatar Computing Research Institute\nWe address the problem of quantification, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or prevalence) of the class in a dataset of unlabelled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product, or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabelled items which have been assigned the class, and tuning the obtained counts according to some heuristics. In this paper we depart from the tradition of using general-purpose classifiers, and use instead a supervised learning model for structured prediction, capable of generating classifiers directly optimized for the (multivariate and non-linear) function used for evaluating quantification accuracy. The experiments that we have run on 5500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing, state-of-the-art quantification methods.\nCategories and Subject Descriptors: I.5.2 [Pattern Recognition]: Design Methodology—Classifier design and evaluation; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—Information filtering; Search process; I.2.7 [Artificial Intelligence]: Natural Language Processing—Text analysis\nGeneral Terms: Algorithm, Design, Experimentation, Measurements\nAdditional Key Words and Phrases: Quantification, Prevalence estimation, Prior estimation, Supervised learning, Text classification, Loss functions, Kullback-Leibler divergence\nACM Reference Format: Andrea Esuli and Fabrizio Sebastiani, YYYY. Optimizing Text Quantifiers for Multivariate Loss Functions. ACM Trans. Knowl. Discov. Data. VV, NN, Article AA ( YYYY), 26 pages. DOI:http://dx.doi.org/10.1145/0000000.0000000"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "In recent years it has been pointed out that, in a number of applications involving classification, the final goal is not determining which class (or classes) individual unlabelled data items belong to, but determining the prevalence (or “relative frequency”) of each class in the unlabelled data. The latter task is known as quantification [Forman 2005; 2006a; 2008; Forman et al. 2006].\nAlthough what we are going to discuss here applies to any type of data, we are mostly interested in text quantification, i.e., quantification when the data items are textual documents. To see the importance of text quantification, let us examine the task of classifying textual answers returned to open-ended questions in questionnaires [Esuli and Sebastiani 2010a; Gamon 2004; Giorgetti and Sebastiani 2003], and let us discuss two important such scenarios.\nIn the first scenario, a telecommunications company asks its current customers the question “How satisfied are you with our mobile phone services?”, and wants to classify the resulting textual answers according to whether they belong to the class MayDefectToCompetition. The company is likely interested in accurately classifying each individual customer, since it may want to call each customer that is assigned the class and offer her improved conditions.\nIn the second scenario, a market research agency asks respondents the question “What do you think of the recent ad campaign for product X?”, and wants to classify the resulting textual answers according to whether they belong to the class LovedTheCampaign. Here, the agency is likely not interested in whether a specific individual\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nbelongs to the class LovedTheCampaign, but is likely interested in knowing how many respondents belong to it, i.e., in knowing the prevalence of the class.\nIn sum, while in the first scenario classification is the goal, in the second scenario the real goal is quantification, i.e., evaluating the results of classification at the aggregate level rather than at the individual level. Other scenarios in which quantification is the goal may be, e.g., predicting election results by estimating the prevalence of blog posts (or tweets) supporting a given candidate or party [Hopkins and King 2010], or planning the amount of human resources to allocate to different types of issues in a customer support center by estimating the prevalence of customer calls related to a given issue [Forman 2005], or supporting epidemiological research by estimating the prevalence of medical reports in which a specific pathology is diagnosed [Baccianella et al. 2013].\nThe obvious method for dealing with the latter type of scenarios is aggregative quantification, i.e., classifying each unlabelled document and estimating class prevalence by counting the documents that have been attributed the class. However, there are two reasons why this strategy is suboptimal. The first reason is that a good classifier may not be a good quantifier, and vice versa. To see this, one only needs to look at the definition of F1, the standard evaluation function for binary classification, defined as\nF1 = 2 · TP\n2 · TP + FP + FN (1)\nwhere TP , FP and FN indicate the numbers of true positives, false positives, and false negatives, respectively. According to F1, a binary classifier Φ̂1 for which FP = 20 and FN = 20 is worse than a classifier Φ̂2 for which, on the same test set, FP = 0 and FN = 10. However, Φ̂1 is intuitively a better binary quantifier than Φ̂2; indeed, Φ̂1 is a perfect quantifier, since FP and FN are equal and thus compensate each other, so that the distribution of the test items across the class and its complement is estimated perfectly.\nA second reason is that standard supervised learning algorithms are based on the assumption that the training set is drawn from the same distribution as the unlabelled data the classifier is supposed to classify. But in real-world settings this assumption is often violated, a phenomenon usually referred to as concept drift [Sammut and Harries 2011]. For instance, in a backlog of newswire stories from year 2001, the prevalence of class Terrorism in August data will likely not be the same as in September data; training on August data and testing on September data might well yield low quantification accuracy. Violations of this assumption may occur “for reasons ranging from the bias introduced by experimental design, to the irreproducibility of the testing conditions at training time” [Quiñonero-Candela et al. 2009]. Concept drift usually comes in one of three forms [Kelly et al. 1999]: (a) the class priors p(ci) may change, i.e., the one in the test set may significantly differ from the one in the training set; (b) the classconditional distributions p(x|ci) may change; (c) the posterior distribution p(ci|x) may change. It is the first of these three cases that poses a problem for quantification.\nThe previous arguments indicate that text quantification should not be considered a mere byproduct of text classification, and should be studied as a task of its own. To date, proposed methods explicitly addressed to quantification (see e.g., [Bella et al. 2010; Forman 2005; 2006a; 2008; Forman et al. 2006; Hopkins and King 2010; Xue and Weiss 2009]) employ general-purpose supervised learning methods, i.e., address quantification by elaborating on the results returned by a general-purpose standard classifier. In this paper we take a sharply different, structured prediction approach, based upon the use of classifiers explicitly optimized for the non-linear, multivariate\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nevaluation function that we will use for assessing quantification accuracy. This idea was first proposed, but not implemented, in [Esuli and Sebastiani 2010b].\nThe rest of the paper is organized as follows. In Section 2, after setting the stage we describe the evaluation function we will adopt (§2.1) and sketch a number of quantification methods previously proposed in the literature (§2.2). In Section 3 we introduce our novel method based on explicitly minimizing, via a structured prediction model, the evaluation measure we have chosen. Section 4 presents experiments in which we test the method we propose on two large batches of binary, high-dimensional, publicly available datasets (the two batches consist of 5148 and 352 datasets, respectively), using all the methods introduced in §2.2 as baselines. Section 5 discusses related work, while Section 6 concludes."
    }, {
      "heading" : "2. PRELIMINARIES",
      "text" : "In this paper we will focus on quantification at the binary level. That is, given a domain of documents D and a class c, we assume the existence of an unknown target function (or ground truth) Φ : D → {−1,+1} that specifies which members of D belong to c; as usual, +1 and −1 represent membership and non-membership in c, respectively. The approaches we will focus on are based on aggregative quantification, i.e., they rely on the generation of a classifier Φ̂ : D → {−1,+1} via supervised learning from a training set Tr. We will indicated with Te the test set on which quantification effectiveness is going to be tested.\nWe define the prevalence (or relative frequency) λTe(c) of class c in a set of documents Te as the fraction of members of Te that belong to c, i.e., as\nλTe(c) = |{dj ∈ Te|Φ(dj) = +1}|\n|Te| (2)\nGiven a set Te of unlabelled documents and a class c, quantification is defined as the task of estimating λTe(c), i.e., of computing an estimate λ̂Te(c) such that λTe(c) and λ̂Te(c) are as close as possible1. What “as close as possible” exactly means will be formalized by an appropriate evaluation measure (see §2.1).\nThe reasons why we focus on binary quantification are two-fold:\n— Many quantification problems are binary in nature. For instance, estimating the prevalence of positive and negative reviews in a dataset of reviews of a given product is such a task. Another such task is estimating from blog posts the prevalence of support for either of two candidates in the second round of a two-round (“run-off”) election. — A multi-class multi-label problem (also known as an n-of-m problem, i.e., a problem where zero, one, or several among m classes can be attributed to the same document) can be reduced to m independent binary problems of type (cj vs. cj), where C = {c1, ..., cj , ..., cm} is the set of classes and where cj denotes the complement of cj . Binary quantification methods can thus also be applied to solving quantification in multi-class multi-label contexts.\nWe instead leave the discussion of quantification in single-label multi-class (i.e., 1-ofm) contexts to future work."
    }, {
      "heading" : "2.1. Evaluation measures for quantification",
      "text" : "Different measures have been used in the literature for measuring binary quantification accuracy.\n1Consistently with most mathematical literature we use the caret symbol (ˆ) to indicate estimation.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nThe simplest such measure is bias (B), defined as B(λTe, λ̂Te) = λ̂Te(c)− λTe(c) and used in [Forman 2005; 2006a; Tang et al. 2010]; positive bias indicates a tendency to overestimate the prevalence of c, while negative bias indicates a tendency to underestimate it.\nAbsolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; González-Castro et al. 2013; Sánchez et al. 2008; Tang et al. 2010]), defined as AE(λTe, λ̂Te) = |λ̂Te(c)− λTe(c)|, is an alternative, equally simplistic measure that accounts for the fact that positive and negative bias are (in the absence of specific application-dependent constraints) equally undesirable.\nRelative absolute error (RAE), defined as\nRAE(λTe, λ̂Te) = |λ̂Te(c)− λTe(c)|\nλTe(c) (3)\nis a refinement of AE meant to account for the fact that the same value of absolute error is a more serious mistake when the true class prevalence is small. For instance, predicting λ̂Te(c) = 0.10 when λTe(c) = 0.01 and predicting λ̂Te(c) = 0.50 when λTe(c) = 0.41 are equivalent errors according to B and AE, but the former is intuitively a more serious error than the latter.\nThe most convincing among the evaluation measures proposed so far is certainly Forman’s [2005], who uses normalized cross-entropy, better known as Kullback-Leibler Divergence (KLD – see e.g., [Cover and Thomas 1991]). KLD, defined as\nKLD(λTe, λ̂Te) = ∑ c∈C λTe(c) log λTe(c) λ̂Te(c) (4)\nand also used in [Esuli and Sebastiani 2010b; Forman 2006a; 2008; Tang et al. 2010], is a measure of the error made in estimating a true distribution λTe over a set C of classes by means of a distribution λ̂Te; this means that KLD is in principle suitable for evaluating quantification, since quantifying exactly means predicting how the test items are distributed across the classes. KLD ranges between 0 (perfect coincidence of λTe and λ̂Te) and +∞ (total divergence of λTe and λ̂Te). In the binary case in which C = {c, c}, KLD becomes\nKLD(λTe, λ̂Te) = λTe(c) log λTe(c)\nλ̂Te(c) + λTe(c) log\nλTe(c) λ̂Te(c) (5)\nContinuity arguments indicate that we should consider 0 log 0q = 0 and p log p 0 = +∞ (see [Cover and Thomas 1991, p. 18]). Note that, as from Equation 4, KLD is undefined when the predicted distribution λ̂Te is zero for at least one class (a problem that also affects RAE). As a result, we smooth the fractions λTe(c)/λ̂Te(c) and λTe(c)/λ̂Te(c) in Equation 4 by adding a small quantity to both the numerator and the denominator. The smoothed KLD function is always defined and still returns a value of zero when λTe and λ̂Te coincide.\nKLD offers several advantages with respect to RAE (and, a fortiori, to B and AE). One advantage is that, as evident from Equation 5, it is symmetric with respect to the complement of a class, i.e., switching the role of c and c does not change the result. This means that, e.g., predicting λ̂Te(c) = 0.10 when λTe(c) = 0.11 and predicting λ̂Te(c) = 0.90 when λTe(c) = 0.89, are equivalent errors (which seems intuitive), while RAE considers the former a much more serious error than the latter. This is especially useful in binary quantification tasks in which it is not clear which of the two classes should\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nplay the role of the positive class c, as in e.g., Employed vs. Unemployed. A second advantage is that KLD is not defined only on the binary (and multi-label multi-class) case, but is also defined on the single-label multi-class case; this allows evaluating different types of quantification tasks with the same measure. Last but not least, one benefit of using KLD is that it is a very well-known measure, having been the subject of intense study within information theory [Csiszár and Shields 2004] and, although from a more applicative angle, within the language modelling approach to information retrieval [Zhai 2008]."
    }, {
      "heading" : "2.2. Existing quantification methods",
      "text" : "A number of methods have been proposed in the (still brief) literature on quantification; below we list the main ones, which we will use as baselines in the experiments discussed in Section 4.\nClassify and Count (CC). An obvious method for quantification consists of generating a classifier from Tr, classifying the documents in Te, and estimating λTe by simply counting the fraction of documents in Te that are predicted positive, i.e.,\nλ̂CCTe (c) = |{dj ∈ Te|Φ̂(dj) = +1}|\n|Te| (6)\nForman [2008] calls this the classify and count (CC) method. Probabilistic Classify and Count (PCC). A variant of the above consists in generating a classifier from Tr, classifying the documents in Te, and computing λTe as the expected fraction of documents predicted positive, i.e.,\nλ̂PCCTe (c) = 1 |Te| ∑ dj∈Te p(c|dj) (7)\nwhere p(c|dj) is the probability of membership in c of test document dj returned by the classifier. If the classifier only returns confidence scores that are not probabilities (as is the case, e.g., when AdaBoost.MH is the learner [Schapire and Singer 2000]), the confidence scores must be converted into probabilities, e.g., by applying a logistic function. The PCC method is dismissed as unsuitable in [Forman 2005; 2008], but is shown to perform better than CC in [Bella et al. 2010] (where it is called “Probability Average”) and in [Tang et al. 2010].\nAdjusted Classify and Count (ACC). Forman [2005; 2008] uses a further method which he calls “Adjusted Count”, and which we will call Adjusted Classify and Count (ACC) so as to make its relation with CC more explicit. The underlying idea is that CC would be optimal, were it not for the fact that the classifier may generate different numbers of false positives and false negatives, and that this difference would lead to imperfect quantification. If we knew the “true positive rate” (tpr = TPTP+FN , a.k.a. recall) and “false positive rate” (fpr = FPFP+TN , a.k.a. fallout) that the classifier has obtained on Te, it is easy to check that perfect quantification would be obtained by adjusting λ̂CCTe (c) as follows:\nλ̂ACCTe (c) = λ̂CCTe (c)− fprTe(c) tprTe(c)− fprTe(c)\n(8)\nSince we cannot know the true values of tprTe(c) and fprTe(c), the ACC method consists of estimating them on Tr via k-fold cross-validation and using the resulting estimates in Equation 8.\nHowever, one problem with ACC is that it is not guaranteed to return a value in [0,1], due to the fact that the estimates of tprTe(c) and fprTe(c) may be imperfect. This\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nlead Forman [2008] to “clip” the results of the estimation (i.e., equate to 1 every value higher than 1 and to 0 every value lower than 0) in order for the final results to be in [0,1].\nProbabilistic Adjusted Classify and Count (PACC). The PACC method (proposed in [Bella et al. 2010], where it is called “Scaled Probability Average”) is a probabilistic variant of ACC, i.e., it stands to ACC like PCC stands to CC. Its underlying idea is to replace, in Equation 8, λ̂CCTe (c), tprTe(c) and fprTe(c) with their expected values, with probability of membership in c replacing binary predictions. Equation 8 is thus transformed into\nλ̂PACCTe (c) = λ̂PCCTe (c)− E[fprTe(c)] E[tprTe(c)]− E[fprTe(c)]\n(9)\nwhere E[tprTe(c)] and E[fprTe(c)] (expected tprTe(c) and expected fprTe(c), respectively) are defined as\nE[tprTe(c)] = 1 |Tec| ∑\ndj∈Tec\np(c|dj) (10)\nE[fprTe(c)] = 1 |Tec| ∑\ndj∈Tec\np(c|dj) (11)\nand Tec (resp., Tec) indicates the set of documents in Te that belong (resp., do not belong) to class c. Again, since we cannot know the true E[tprTe(c)] and E[fprTe(c)] (given that we do not know Tec and Tec), we estimate them on Tr via k-fold crossvalidation and use the resulting estimates in Equation 9.\nThreshold@0.50 (T50), Method X (X), and Method Max (MAX). Forman [2008] points out that the ACC method is very sensitive to the decision threshold of the classifier, which may yield unreliable values of λACCTe (c) (or lead to λACCTe (c) being undefined when tprTe = fprTe). In order to reduce this sensitivity, [Forman 2008] recommends to heuristically set the decision threshold in such a way that tprTr (as obtained via k-fold cross-validation) is equal to .50 before computing Equation 8. This method is dubbed Threshold@0.50 (T50). Alternative heuristics that [Forman 2008] discusses are to set the decision threshold in such a way that fprTr = 1− tprTr (this is dubbed Method X) or such that (tprTr − fprTr) is maximized (this is dubbed Method Max).\nMedian Sweep (MS). Alternatively, [Forman 2008] recommends to compute λACCTe (c) for every decision threshold that gives rise (in k-fold cross-validation) to different tprTr or fprTr values, and take the median of all the resulting estimates of λACCTe (c). This method is dubbed Median Sweep (MS).\nMixture Model (MM). The MM method (proposed in [Forman 2005]) consists of assuming that the distribution DTe of the scores that the classifier assigns to the test examples is a mixture\nDTe = λTe(c) ·DTec + (1− λTe(c)) ·DTec (12) where DTec and DTec are the distributions of the scores that the classifier assigns to the positive and the negative test examples, respectively, and where λTe(c) and λTe(c) are the parameters of this mixture. The MM method consists of estimating DTec and DTec via k-fold cross-validation, and picking as value of λTe(c) the one that generates the best fit between the observed DTe and the mixture. Two variants of this method, called the Kolmogorov-Smirnov Mixture Model (MM(KS)) and the PP-Area Mixture Model (MM(PP)), are actually defined in [Forman 2005], which differ in terms of how the goodness of fit between the left- and the right-hand side of Equation 12 is estimated. See [Forman 2005] for more details.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY."
    }, {
      "heading" : "3. OPTIMIZING QUANTIFICATION ACCURACY",
      "text" : "A problem with the methods discussed in §2.2 is that most of them are fairly heuristic in nature. For instance, the fact that methods such as ACC (and all the others based on it, such as T50, MS, X, and MAX) require “clipping” is scarcely reassuring. More in general, methods such as T50 or MS have hardly any theoretical foundation, and choosing them over CC only rests on our knowledge that they have performed better in previously reported experiments.\nA further problem is that some of these methods rest on assumptions that seem problematic. For instance, one problem with the MM method is that it seems to implicitly rely on the hypothesis that estimating DTec and DTec via k-fold cross-validation on Tr can be done reliably. However, since the very motivation of doing quantification is that the training set and the test set may have quite different characteristics, this hypothesis seems adventurous. A similar argument casts some doubt on ACC: how reliable are the estimates of tprTe and fprTe that can be generated via k-fold crossvalidation on Tr, given the different characteristics that training set and test set may have in the application contexts where quantification is required?2 In sum, the very same arguments that are used to deem the CC method unsuitable for quantification seem to undermine the previously mentioned attempts at improving on CC.\nIn this paper we propose a new, theoretically well-founded quantification method that radically differs from the ones discussed in §2.2. Note that all of the methods discussed in §2.2 employ general-purpose supervised learning methods, i.e., address quantification by post-processing the results returned by a standard classifier (where the decision threshold has possibly been tuned according to some heuristics). In particular, all the supervised learning methods adopted in the literature on quantification optimize Hamming distance or variants thereof, and not a quantification-specific evaluation function. When the dataset is imbalanced (typically: when the positives are by far outnumbered by the negatives), as is frequently the case in text classification, this is suboptimal, since a supervised learning method that minimizes Hamming distance will generate classifiers with a tendency to make negative predictions. This means that FN will be much higher than FP , to the detriment of quantification accuracy3.\nWe take a sharply different approach, based upon the use of classifiers explicitly optimized for the evaluation function that we will use for assessing quantification accuracy. Given such a classifier, we will simply use a “classify and count” approach, with no heuristic threshold tuning (à la T50 / X / MAX) and no a posteriori adjustment (à la ACC).\nThe idea of using learning algorithms capable of directly optimizing the measure (a.k.a. “loss”) used for evaluating effectiveness is well-established in supervised learning. However, in our case following this route is non-trivial, because the evaluation measure that we want to use (KLD) is non-linear, i.e., is such that the error on the test set may not be formulated as a linear combination of the error incurred by each test example. An evaluation measure for quantification is inherently non-linear, because how the error on an individual test item impacts on the overall quantification error depends on how the other test items have been classified. For instance, if in the other test items there are more false positives than false negatives, an additional false negative is actually beneficial to overall quantification error, because of the mutual compensation effect between FP and FN mentioned in Section 1. As a result, a measure of\n2In Appendix A we thoroughly analyse (also by means of concrete experiments) the issue of how (un)reliable the k-fold cross-validation estimates of tprTe and fprTe are in practice. 3To witness, in the experiments we report in Section 4 our 5148 test sets exhibit, when classified by the classifiers generated by the linear SVM used for implementing the CC method, an average FP/FN ratio of 0.109; by contrast, for an optimal quantifier this ratio is always 1.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nquantification accuracy is inherently non-linear, and should thus be multivariate, i.e., take in consideration all test items at once.\nAs discussed in [Joachims 2005], the assumption that the error on the test set may be formulated as a linear combination of the error incurred by each test example (as indeed happens for many common error measures – e.g., Hamming distance) underlies most existing discriminative learners, which are thus suboptimal for tackling quantification. In order to sidestep this problem, we adopt the SVM for Multivariate Performance Measures (SVMperf ) learning algorithm proposed by Joachims [2005]4. SVMperf is a learning algorithm of the Support Vector Machine family that can generate classifiers optimized for any non-linear, multivariate loss function that can be computed from a contingency table (as KLD is).\nSVMperf is a specialization to the problem of binary classification of the structural SVM (SVMstruct) learning algorithm [Joachims et al. 2009a; Joachims et al. 2009b; Tsochantaridis et al. 2004] for “structured prediction”, i.e., an algorithm designed for predicting multivariate, structured objects (e.g., trees, sequences, sets). SVMperf is fundamentally different from conventional algorithms for learning classifiers: while these latter learn univariate classifiers (i.e., functions of type Φ̂ : D → {−1,+1} that classify individual instances one at a time), SVMperf learns multivariate classifiers (i.e., functions of type Φ̂ : D|S| → {−1,+1}|S| that classify entire sets S of instances in one shot). By doing so, SVMperf can optimize properties of entire sets of instances, properties (such as KLD) that cannot be expressed as linear functions of the properties of the individual instances.\nAs discussed in [Joachims et al. 2009b], SVMstruct can be adapted to a specific task by defining four components:\n(1) A joint feature map Ψ(x,y). This function computes a vector of features (describing the match between the input vectors in x and the relative outputs, true or predicted, in y) from all the input-output pairs at the same time. In this way the number of features, and thus the number of parameters of the model, can be kept constant regardless of the size of the sample set. The Ψ function allows to generalise not only on inputs (x) but also on outputs (y), thus allowing to produce predictions not seen in the training data. In SVMperf Ψ is defined5 as\nΨ(x,y) = 1\nn n∑ i=1 yixi (13)\n(2) A loss function ∆(y, ŷ). SVMperf works with loss functions ∆(TP, FP, FN, TN) in which the four values are those from the contingency table resulting from comparing the true labels y with the predicted labels ŷ. In our work we take the loss\n4In [Joachims 2005] SVMperf is actually called SVM∆multi, but the author has released its implementation under the name SVMperf . We will use this latter name because it uniquely identifies the algorithm on the Web, while searching for “SVM multi” often returns the SVMmulticlass package, which addresses a different problem. 5For this formulation of Ψ, and when error rate is the chosen loss function, Joachims [2005] shows that SVMperf coincides with the traditional univariate SVM model (called SVMorg in [Joachims 2005]).\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nfunction to be KLD, i.e.,6\n∆KLD(TP, FP, FN, TN) = KLD(λ, λ̂) (14)\nwhere λ(c) = TP + FN\nTP + FP + FN + TN and λ̂Te(c) =\nTP + FP\nTP + FP + FN + TN (3) An algorithm for the efficient computation of a hypothesis\nΦ̂(x) = argmaxŷ∈Y{w ·Ψ(x, ŷ)} (15)\nwhere w is a vector of parameters. In SVMperf this simply corresponds to computing\nΦ̂(x) = (sign(w · x1), . . . , sign(w · xn)) (16) (4) An algorithm for the efficient computation of the loss-augmented hypothesis\nΦ̂∆(x) = argmaxŷ∈Y{∆(y, ŷ) + w ·Ψ(x, ŷ)} (17)\nwhich in SVMperf is computed via an algorithm [Joachims 2005, Algorithm 2] with O(n2) worst-case complexity.\nWe have used the implementation of SVMperf made available by Joachims7, which we have extended by implementing the module that takes care of the ∆KLD loss function. In the rest of the paper our method will be dubbed SVM(KLD)."
    }, {
      "heading" : "4. EXPERIMENTS",
      "text" : "We now present the results of experiments aimed at assessing whether the approach we have proposed in Section 3 delivers better quantification accuracy than state-ofthe-art quantification methods. In order to do this, we have run all our experiments by using as baselines for our SVM(KLD) method all the methods described in §2.2. For the CC, ACC, T50, X, MAX, MS, MM(KS), MM(PP) methods we have used the original implementation that we have obtained from the author (this guarantees that the baselines perform at their full potential). We have instead implemented PCC and PACC ourselves. At the heart of the implementation of all the baselines is a standard linear SVM with the parameters set at their default values; where quantities (such as e.g., fprTe and tprTe – see Equation 8) had to be estimated from the training set, we have used 50-fold cross-validation, as done and recommended in [Forman 2008]. In order to guarantee a fair comparison with the baselines we have used the default values for the parameters also for SVMperf , which lies at the basis of our SVM(KLD) method8.\n6In Equation 14 KLD is written as a function of TP , FP , FN , TN for the simple fact that in [Joachims 2005] (where SVMperf was originally described) the loss function ∆ is specified as a function of the four cells of the contingency table. However, it should be clear that λ(c) does not depend on the predicted labels: even if in Equation 14 we have written it out as λ(c) = TP+FN\nTP+FP+FN+TN , this latter is equivalent to writing\nλ(c) = GP GP+GN , where GP (the “gold positives”) is TP + FN and GN (the “gold negatives”) is FP + TN . Seen under this light, there is no trace of predicted labels in λ(c) = GP\nGP+GN , and λ(c) is just a function of\nthe gold standard and not of the prediction. Analogously, it should be clear that λ̂(c) is just a function of the prediction and not of the gold standard. 7SVMperf is available from http://www.cs.cornell.edu/People/tj/svm\\%5Flight/svm perf.html. Our module that extends it to deal with KLD is available at http://hlt.isti.cnr.it/quantification/. 8An additional reason why we have left the parameters at their default values is that, in a context in which the characteristics of Tr and Te may substantially differ, it is not clear that the parameter values which are found optimal on Tr via k-fold cross-validation will also prove optimal (or at least will perform reasonably) on Te.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nIn order to generate the vectorial representations for our documents, the classic “bag-of-words” approach has been adopted. In particular, punctuation has been removed, all letters have been converted to lowercase, numbers have been removed, stop words have been removed using the stop list provided in [Lewis 1992, pages 117–118], and stemming has been performed by means of the version of Porter’s stemmer available from http://snowball.tartarus.org/. All the remaining stemmed words (“terms”) that occur at least once in Tr have thus been used as features of our vectorial representations of documents; no feature selection has been performed. Feature weights have been obtained via the “ltc” variant [Salton and Buckley 1988] of the well-known tfidf class of weighting functions, i.e.,\ntfidf(tk, di) = tf(tk, di) · log |Tr|\n#Tr(tk) (18)\nwhere di is a document, #Tr(tk) denotes the number of documents in Tr in which feature tk occurs at least once and\ntf(tk, di) = { 1 + log #(tk, di) if #(tk, di) > 0 0 otherwise (19)\nwhere #(tk, di) denotes the number of times tk occurs in di. Weights obtained by Equation 18 are normalized through cosine normalization, i.e.,\nwki = tfidf(tk, di)√∑|T | s=1 tfidf(ts, di) 2\n(20)\nwhere T denotes the total number of features. Following [Forman 2008], we set the constant for smoothing KLD to the value = 12 |Te|."
    }, {
      "heading" : "4.1. Datasets",
      "text" : "The datasets we use for our experiments have been extracted from two important text classification test collections, REUTERS CORPUS VOLUME 1 version 2 (RCV1-V2) and OHSUMED-S.\nRCV1-V2 is a standard, publicly available benchmark for text classification consisting of 804,414 news stories produced by Reuters from 20 Aug 1996 to 19 Aug 19979. RCV1-V2 ranks as one of the largest corpora currently used in text classification research and, as pointed out in [Forman 2006b], suffers from extensive “drift”, i.e., from substantial variability between the training set and the test set, which makes it a challenging dataset for quantification. In our experiments we have used the 12,807 news stories of the 1st week (20 to 26 Aug 1996) for training, and the 791,607 news stories of the other 52 weeks for testing10. We have further partitioned these latter into 52 test sets each consisting of one week’s worth of data11. RCV1-V2 is multi-label, i.e., a document may belong to several classes at the same time. Of the 103 classes of which its “Topic” hierarchy consists, in our experiments we have restricted our attention to the 99 classes with at least one positive training example. Consistently with the evaluation presented in [Lewis et al. 2004], also classes placed at internal nodes in the hierarchically organized classification scheme are considered in the evaluation; as\n9http://trec.nist.gov/data/reuters/reuters.html 10This is the standard “LYRL2004” split between training and test data, originally defined in [Lewis et al. 2004]. 11More precisely, since the period covered by RCV1-V2 consists of 365 days, i.e., 52 full weeks + 1 day, the 52nd test set consists of 1 day’s worth of data only.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\npositive examples of these classes we use the union of the positive examples of their subordinate nodes, plus their “own” positive examples.\nThe OHSUMED-S dataset [Esuli and Sebastiani 2013] is a subset of the wellknown OHSUMED test collection [Hersh et al. 1994]. OHSUMED-S consists of a set of 15,643 MEDLINE records spanning the years from 1987 to 1991, where each record is classified under one or more of the 97 MeSH index terms that belong to the Heart Disease (HD) subtree of the well-known MeSH tree of index terms12. Each entry consists of summary information relative to a paper published on one of 270 medical journals; the available fields are title, abstract, author, source, publication type, and MeSH index terms. As the training set we have used, consistently with [Esuli and Sebastiani 2013], the 2,510 documents belonging to year 1987; 9 MeSH index terms out of the 97 in the HD subtree are never assigned to any training document, so the number of classes we actually use is 88. We partition the four remaining years’ worth of data into four bins (1988, 1989, 1990, 1991), each containing the documents generated within the corresponding calendar year. The reason why we do not use the entire OHSUMED dataset is that roughly 93% of OHSUMED entries have no class assigned from the HD subtree, which means that the classes in the HD subtree have very low prevalence (λTr = 0.003 on average); we thus prefer to use OHSUMED-S, which presents a wider range of prevalence values.\nThis experimental setting thus generates 52 × 99 = 5148 binary quantification test sets for RCV1-V2 (containing an average of 15,223 documents each), and 4 × 88 = 352 test sets for OHSUMED-S (containing an average of 3,283 documents each). This large number of test sets will give us an opportunity to study quantification across different dimensions (e.g., across classes characterized by different prevalence, across classes characterized by different amounts of drift, across time)13. More detailed figures about our datasets are given in Table I. Note that both RCV1-V2 and OHSUMED-S classes are characterized by severe imbalance, as can be noticed by the two “Avg prevalence of the positive class” rows of Table I, where both values are very far away from the value of 0.5, which represents perfect balance. On a side note, it is well-known that the “bag-of-words” extraction process outlined a few paragraphs above gives rise to very high-dimensional (albeit sparse) vectors; our case is no exception, and the dimensionality of our vectors is 53,204 (RCV1-V2) and 11,286 (OHSUMED-S), respectively.\nNote that the experimental protocol we adopt is different from the one adopted by Forman. In [Forman 2005; 2006a; 2008] he proposes a protocol in which, given a training set Tr and a test set Te, several controlled experiments are run by artificially altering class prevalences (i.e., by randomly removing predefined percentages of the positives or of the negatives) either on Tr or on Te. This protocol is meant to test the robustness of the methods with respect to different “distribution drifts” (i.e., differences between λTr(c) and λTe(c) of different magnitude) and different class prevalence values. We prefer to opt for a different protocol, one in which “natural” training and test sets are used, without artificial alterations. The reason is that artificial alterations may generate class prevalence values and/or distribution drifts that are simply not realistic (e.g., a situation in which λTr(c) = .40 and λTe(c) = .01); conversely, focusing on naturally occurring datasets forces us to come to terms with realistic levels of class prevalence and/or distribution drift. We will thus adopt the latter protocol in all the\n12https://www.nlm.nih.gov/mesh/ 13In order to guarantee perfect reproducibility of our results, we make available at http://hlt.isti.cnr.it/ quantification/ the feature vectors of the RCV1-V2 and OHSUMED-S documents as extracted from our preprocessing module, and already split respectively into the 53 and 5 sets described above.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nexperiments discussed in this paper, “compensating” for the absence of artificial alterations by also studying (see §4.2.3) the behaviour of our methods separately on test sets characterized by different (and natural) levels of distribution drift."
    }, {
      "heading" : "4.2. Testing quantification accuracy",
      "text" : "We have run our experiments by learning quantifiers for each class c on the respective training set and testing the quantifiers separately on each of the test sets, using KLD as the evaluation measure. We have done this for all the 99 classes × 52 weeks in RCV1-V2 and for all the 88 classes × 4 years in OHSUMED-S, and for all the 10 baseline methods discussed in §2.2 plus our SVM(KLD) method. 4.2.1. Analysing the results along the class dimension. We first discuss the results according to the class dimension, i.e., by averaging the results for each RCV1-V2 class across the 52 test weeks and for each OHSUMED-S class across the 4 years14. Since this would leave no less than 99 RCV1-V2 classes to discuss, we further average the results across all the RCV1-V2 classes characterized by a training class prevalence λTr(c) that falls into a certain interval (same for OHSUMED-S and its 88 classes). This allows us to separately check the behaviour of our quantification methods on groups of classes that are homogeneous by level of imbalance. We have also run statistical significance tests in order to check whether the improvement obtained by the best performing method over the 2nd best performer on the group is statistically significant15.\nThe results are reported in Table II, where four levels of imbalance have been singled out: very low prevalence (VLP, which accounts for all the classes c such that λTr(c) < 0.01; there are 48 RCV1-V2 and 51 OHSUMED-S such classes), low prevalence (LP\n14Wherever in this paper we speak of averaging accuracy results across different test sets, what we mean is actually macroaveraging, i.e., taking the accuracy results on the individual test sets and computing their arithmetic mean. This is sharply different from microaveraging, i.e., merging the test sets and computing a single accuracy figure on the merged set. Quite obviously, in a quantification setting microaveraging does not make any sense at all, since false positives from one set and false negatives from another set would compensate each other, thus generating misleadingly high accuracy values. 15All the statistical significance tests discussed in this paper are based on a two-tailed paired t-test and the use of a 0.001 significance level.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\n– 0.01 ≤ λTr(c) < 0.05; 34 RCV1-V2 and 28 OHSUMED-S classes), high prevalence (HP – 0.05 ≤ λTr(c) < 0.10; 10 RCV1-V2 and 4 OHSUMED-S classes), and very high prevalence (VHP – 0.10 ≤ λTr(c); 7 RCV1-V2 and 5 OHSUMED-S classes).\nThe first observation that can be made by looking at the RCV1-V2 results in this table is that, when evaluated across all our 5148 test sets (Column 6), SVM(KLD) outperforms all the other baseline methods in a statistically significant way, scoring a KLD value of 1.32E-03 against the 1.74E-03 value (a -24.2% error reduction) obtained by the best-performing baseline (the PACC method). This is largely a result of a much better balance between false positives and false negatives obtained by the base classifiers: while (as already observed in Footnote 3) the average FP/FN ratio across the 5148 test sets is 0.109 for CC, it is 0.684 for SVM(KLD) (and it is 1 for the perfect quantifier). The OHSUMED-S results essentially confirm the insights obtained from the RCV1-V2 results, with SVM(KLD) again the best of the 11 methods and PACC again the 2nd best; the difference between them is now even higher, with an error reduction of -56.8%.\nA second observation that the RCV1-V2 results allow is to make is that SVM(KLD) scores well on all the four groups of classes identified; while it is not always the best method (e.g., it is outperformed by other methods in the HP and VHP groups), it consistently performs well on all four groups. In particular, SVM(KLD) seems to excel at classes characterized by drastic imbalance, as witnessed by the VLP group, where SVM(KLD) is the best performer, and by the LP group, where SVM(KLD) is the best performer in a statistically significant way. In fact, this group of classes seems largely responsible for the excellent overall performance (Column 6) displayed by SVM(KLD),\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nsince on the LP group the margin between it and the other methods is large (4.92E-04 against 1.98E-03 of the 2nd best method), and since the VLP and LP groups altogether account for no less than 82 out of the total 99 classes. This latter fact characterizes most naturally occurring datasets, whose class distribution usually exhibits a power law, with very few highly frequent classes and very many highly infrequent classes. The OHSUMED-S results essentially confirm the RCV1-V2 results, with SVM(KLD) again the best performer on the VLP and LP classes, and still performing well, although not being the best, on HP and VHP.\nThe stability of SVM(KLD) is also confirmed by Table III, which reports, for the same groups of test sets identified by Table II, the variance in KLD across the members of the group. For example, on RCV1-V2, Column 3 reports the variance in KLD across all the 34 classes such that .01 ≤ λTr(c) ≤ 0.05 and across the 52 test weeks, for a total of 34 × 52 = 1, 768 test sets. What we can observe from this table is that, when averaged across all the 99 × 52 = 5148 test sets (Column 6), the variance of SVM(KLD) is lower than the variance of all other methods in a statistically significant way. The variance of SVM(KLD) is fairly low in all the four subsets of classes, and particularly so in the subsets of the most imbalanced classes (VLP and LP), in which SVM(KLD) is the best performer in a statistically significant way. The OHSUMED-S results essentially confirm the RCV1-V2 results, with SVM(KLD) the best performer on VLP and LP, and still behaving well on HP and VHP.\nConcerning the baselines, our results seem to disconfirm the ones reported in [Forman 2008] according to which the MS method is the best of the lot, and according to which the ACC method “can estimate the class distribution well in many situations, but its performance degrades severely when the training class distribution is highly imbalanced”. In our experiments, instead, MS is substantially outperformed by several baseline methods; ACC is instead a strong contender, and (contrary to the statement above) especially shines on the subsets of the most imbalanced classes. The results of both Tables II and III clearly show that, on both RCV1-V2 and OHSUMED-S, PACC is the best of the baseline methods presented in §2.2.\n4.2.2. Analysing the results along the temporal dimension. We now analyse the results along the temporal dimension. In order to do this for the RCV1-V2 dataset (resp., OHSUMED-S dataset), for each of the 52 test weeks (resp., 4 test years) we average the 99 (resp., 88) accuracy results corresponding to the individual classes, and check the temporal accuracy trend resulting from these averages. This trend is displayed in Figure 1, where the results of SVM(KLD) are plotted together with the results of the three best-performing baseline methods. The plots unequivocally show that SVM(KLD) is the best method across the entire temporal spectrum for both RCV1-V2 and OHSUMED-S.\nNote that quantification accuracy remains fairly stable across time, i.e., we are not witnessing any substantial decrease in quantification accuracy with time. Intuition might instead suggest that quantification accuracy should decrease with time, due to the combined effects of true concept drift and distribution drift. This may indicate that (at least in the context of broadcast news that RCV1-V2 represents, and in the context of medical scientific articles that OHSUMED-S represents) the chosen timeframe (one year for RCV1-V2, four years for OHSUMED-S) is not sufficient enough a timeframe to observe a significant such drift.\n4.2.3. Analysing the results along the distribution drift dimension. The last angle according to which we analyse the results is distribution “drift”. That is, we conduct our analysis in terms of how much the prevalence λTei(c) in a given test set Tei “drifts away” from\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nthe prevalence λTr(c) in the training set. Specifically, for all our 5148 RCV1-V2 test sets (resp., 352 OHSUMED-S test sets) Tei we compute KLD(λTei , λTr), we rank all the test sets according to the KLD value they have obtained, and we subdivide the resulting ranking into four equally-sized segments (quartiles) of 5148/4=1287 RCV1V2 test sets (resp., 352/4=88 OHSUMED-S test sets) each. As a result, each resulting quartile contains test sets that are homogeneous according to the divergence of their distributions from the corresponding distributions in the training set (different test sets pertaining to the same class c may thus end up in different quartiles). This allows us to investigate how well the different methods behave when distribution drift is low (indicated by low KLD values) and when distribution drift is high (high KLD values). We have also run statistical significance tests in order to check whether the improvement obtained by the best performing method over the 2nd best performer on the test sets belonging to a certain quartile is statistically significant.\nThe results of this analysis are displayed in Table IV. The most important observation here is that SVM(KLD) is the best performer in a statistically significant way, both overall and on three out of four quartiles (on the “very high drift” quartile SVM(KLD) is outperformed by PACC). Additionally, it is worth observing that its performance is consistently high on each quartile. Table V reports variance figures, showing again the stability of SVM(KLD), which is the most stable method overall and on three out of four quartiles (on the VHD quartile the most stable method is PACC).\n4.2.4. Evaluating the results according to RAE. It may be interesting to analyse the results of the previous experiments according to an evaluation measure different from KLD, such as the RAE measure introduced in §2.1. As discussed in §2.1, RAE is the most (and only) reasonable alternative to KLD proposed so far. Given the simplicity of its mathematical form, it is also a measure everyone can relate to.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nSimilarly to Table IV, Table VI reports the results of our experiments broken down into quartiles of test sets homogeneous by distribution drift; the difference with Table IV is that RAE is now used as the evaluation measure in place of KLD. The RCV1-V2 results of Table IV confirm the superiority of SVM(KLD) over the baselines, notwithstanding the discrepancy between evaluation measure (RAE) and loss function optimized (KLD). As an average across the 5148 test sets, SVM(KLD) obtains an average RAE value of 0.465, which improves in a statistically significant way over the 0.674 result obtained by the second best performer, ACC; the next best-performing methods, CC and PACC, obtain dramatically worse results (1.087 and 1.466, respectively). SVM(KLD) is also the best performer, in a statistically significant way, in each of the four quartiles.\nIn this case OHSUMED-S results are substantially different from the RCV1-V2 results, since here the best performer is MM(KS) (a weak contender in all the experiments that we have reported so far), while SVM(KLD) performs much less well. The overall performance of SVM(KLD) is penalized by a bad performance on the VHD quartile, since it is the best performer (and in a statistically significant way) on the other three quartiles. While there is some discrepancy between the outcomes of the RCV1-V2 experiments and those of the OHSUMED-S experiments, we observe that the former might be considered somehow more trustworthy than the latter ones, since\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nRCV1-V2 is much bigger than OHSUMED-S (approximately 60 times more test documents).\nAdditionally, and more importantly, let us recall that SVM(KLD) is optimized for KLD, and not for RAE. This means that, in keeping with our plan to use classifiers explicitly optimized for the evaluation function used for assessing quantification accuracy, should we really deem RAE to be the “right” such function we would implement\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nand use SVM(RAE)!, and not SVM(KLD). In other words, in the RCV1-V2 results of Table VI SVM(KLD) turns out to be the best performer despite the fact that we here use RAE as an evaluation measure."
    }, {
      "heading" : "4.3. Testing classification accuracy",
      "text" : "As discussed in Section 1, quantification accuracy is related to the classifier’s ability to balance false positives and false negatives, but is not related to its ability to keep their total number low, which is instead a key requirement in standard classification. However, it is fairly natural to expect that a user will trust a quantification method only inasmuch as its good quantification performance results from reasonable classification performance. In other words, a user is unlikely to accept a classifier with good quantification accuracy but bad classification accuracy.\nFor this reason we have compared, in terms of classification accuracy, SVM(KLD) against the traditional classification-oriented SVMs (i.e., SVMorg – see Section 3), with the goal of ascertaining if the former has also a reasonable classification accuracy. Default parameters have been used for both, for the reasons already explained in the first paragraph of Section 4. We have compared the two systems by using the same training set as used in the quantification experiments, and as the test set the union of the 52 test sets used for quantification (i.e., a single test set of 791,607 documents across 99 classes). Evaluation is based on the F1 measure, both in its micro-averaged (Fµ1 ) and macro-averaged (FM1 ) versions.\nOn the RCV1-V2 dataset, in terms of Fµ1 SVM(KLD) performs slightly worse than SVMorg (.755 instead of .777, with a relative decrease of −2.83%), while in terms of FM1 SVM(KLD) performs slightly better (.440 instead of .433, a relative increase of +1.61%), which indicates that, on highly imbalanced classes, SVM(KLD) is not only a better quantifier, but also a better classifier. To witness, on the 48 classes for which λTr ≤ 0.01 (i.e., on the most imbalanced classes) we obtain Fµ1 = .294 for SVMorg while SVM(KLD) obtains Fµ1 = .350 (an increase of +19.09%). The trends on the\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nOHSUMED-S dataset are similar, though with smaller margins. In terms of Fµ1 SVM(KLD) performs slightly worse than SVMorg (Fµ1 = .713 instead of .722, with a relative decrease of −1.24%), while in terms of FM1 SVM(KLD) performs slightly better (.405 instead of .398, a relative increase of +1.76%), confirming the insights obtained from RCV1-V2. On the 51 classes for which λTr ≤ 0.01 (i.e., on the most imbalanced classes) we obtain Fµ1 = .443 for SVMorg while SVM(KLD) obtains F µ 1 = .456 (an increase of +2.93%). All in all, these results show that classifiers trained via SVM(KLD), aside from delivering top-notch quantification accuracy, are also characterized by very good classification accuracy. This makes the quantifiers generated via SVM(KLD) not only accurate, but also trustworthy."
    }, {
      "heading" : "4.4. Testing efficiency",
      "text" : "SVM(KLD) has also good properties in terms of sheer efficiency.\nConcerning training, Joachims [2005] proves that training a classifier with SVMperf is O(n2), with n the number of training examples, for any loss function that can be computed from a contingency table (such as KLD indeed is). This is certainly more expensive than training a classifier by means of a standard, linear SVM (which is at the heart of Forman’s implementation of all the quantification methods of §2.2), since this latter is well-known to be O(sn) (with s the average number of non-zero features in the training objects) [Joachims 2006].\nHowever, note that, while SVM(KLD) only requires training a classifier by means of SVMperf , setting up a quantifier with any of the methods of §2.2 (with the only exception of the simple CC method) requires more than simply training a classifier. For instance, ACC (together with the methods derived from it, such as T50, X, MAX, MS, PACC) also requires estimating tprTe and fprTe on the training set via k-fold cross validation, which may be expensive; analogously, both MM(KS) and MM(PP) require estimatingDTec andDTec via k-fold cross-validation, and the same considerations apply.\nIn practice, using SVMperf turns out to be affordable. On RCV1-V2, training the 99 binary classifiers described in the previous sections via SVMperf required on average about 4.7 seconds each16. By contrast, training the analogous classifiers via a standard linear SVM required on average only 2.1 seconds each. However, this means that, if k-fold cross-validation is used for the estimation of parameters with a value of k ≥ 2 (meaning that, for each class, additional k classifiers need to be trained), the computational advantage of using a linear SVM instead of the more expensive SVMperf is completely lost. Forman [2008] recommends choosing k = 50 in order to obtain more accurate estimates of tprTe and fprTe for use in ACC and derived methods; this means making the training phase roughly (2.1 · 51)/4.7 ≈ 22 times slower than the training phase of SVM(KLD).\nConcerning the computational costs involved at classification / quantification time, SVM(KLD) and all the baseline methods discussed in this paper are equivalent, since (a) they all generate linear classifiers of equal efficiency, and (b) in ACC and derived methods the cost of the post-processing involved in computing class prevalences from the classification decisions is negligible."
    }, {
      "heading" : "5. RELATED WORK",
      "text" : "An early mention of quantification can be found in [Lewis 1995, Section 7], where this task is simply called counting; however, the author does not propose any specific solu-\n16All times reported in this section were measured on a commodity machine equipped with an Intel Centrino Duo 2×2Ghz processor and 2GB RAM.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\ntion for this problem. Forman [2005; 2006; 2006a; 2008] is to be credited for bringing the problem of quantification to the attention of the data mining and machine learning research communities, and for proposing several solutions for performing quantification and for evaluating it."
    }, {
      "heading" : "5.1. Applications of quantification",
      "text" : "Chan and Ng [2005; 2006] apply quantification (which they call “class prior estimation”) to determining the prevalence of different senses of a word in a text corpus, with the goal of improving the accuracy of word sense disambiguation algorithms as applied on that corpus. Forman [2008] uses quantification in order to establish the prevalence of various support-related issues in incoming telephone calls received at customer support desks. Esuli and Sebastiani [2010a] apply quantification methods for estimating the prevalence of various response classes in open-ended answers obtained in the context of market research surveys (they do not use the term “quantification”, and rather speak of “measuring classification accuracy at the aggregate level”). Hopkins and King [2010] classify blog posts with the aim of estimating the prevalence of different political candidates in bloggers’ preferences. Gonzalez-Castro et al. [2013] and Sanchez et al. [2008] use quantification for establishing the prevalence of damaged sperm cells in a given sample for veterinary applications. Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.e., attempt to estimate class prevalence in the test set in order to generate a classifier that better copes with differences in the class distributions of the training set and the test set.\nMany other works use quantification “without knowingly doing so”; that is, unaware of the existence of methods specifically optimized for quantification, they use classification with the only goal of estimating class prevalences. In other words, these works use plain “classify and count”. Among them, Mandel et al. [2012] use tweet quantification in order to estimate, from a quantitative point of view, the emotional responses of the population (segmented by location and gender) to a natural disaster; O’Connor et al. [2010] analyse the correlation between public opinion as measured via tweet sentiment quantification and via traditional opinion polls; Dodds et al. [2011] use tweet sentiment quantification in order to infer spatio-temporal happiness patterns; and Weiss et al. [2013] use quantification in order to measure the prevalence of different types of pets’ activity as detected by wearable devices."
    }, {
      "heading" : "5.2. Quantification methods",
      "text" : "Bella et al. [2010] compare many of the methods discussed in §2.2, and find that CC ≺ PCC ≺ ACC ≺ PACC (where ≺ means “underperforms”). Also Tang et al. [2010] experimentally compare several among the methods discussed in §2.2, and find that CC ≺ PCC ≺ ACC ≺ PACC ≺ MS. They also propose a method (specific to linked data) that does not require the classification of individual items, but they find that it underperforms a robust classification-based quantification method such as MS. However, the experimental comparisons of [Bella et al. 2010] and [Tang et al. 2010] are both framed in terms of absolute error, which seems a sub-standard evaluation measure for this task (see §2.1); additionally, the datasets they test on do not exhibit the severe imbalance typical of many binary text classification tasks, so it is not surprising that their results concerning MS are not confirmed by our experiments.\nThe idea of using a learner that directly optimizes a loss function specific to quantification was first proposed, although not implemented, in [Esuli and Sebastiani 2010b],\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nwhich indeed proposes using SVMperf to directly optimize KLD; the present paper is thus the direct realization of that proposal. The first published work that implements and tests the idea of directly optimizing a quantification-specific loss function is [Milli et al. 2013], whose authors propose variants of decision trees and decision forests that directly optimize a loss combining classification accuracy and quantification accuracy. At the time of going to print we have become aware of a related paper [Barranquero et al. 2015] whose authors, following [Esuli and Sebastiani 2010b], use SVMperf to perform quantification; differently from the present paper, and similarly to [Milli et al. 2013], they use an evaluation function that combines classification accuracy and quantification accuracy."
    }, {
      "heading" : "5.3. Other related work",
      "text" : "Bella et al. [2014] address the problem of performing quantification for the case in which the output variable to be predicted for a given individual is a real value, instead of a class as in the case we analyse; that is, they analyse quantification as a counterpart of regression, rather than of classification as we do.\nQuantification as defined in this paper bears some relation with density estimation [Silverman 1986], which can be defined as the task of estimating, based on observed data, the unknown probability density function of a given random variable. If the random variable is discrete, this means estimating, based on observed data, the unknown distribution across the discrete set of events, i.e., across the classes. An example density estimation problem is estimating the percentage of white balls in a very large urn containing white balls and black balls. However, there are two essential differences between quantification and density estimation, i.e., that (a) in density estimation the class to which a data item belongs can be established with certainty (e.g., if a ball is picked, it can be decided with certainty if it is black or white), while in quantification this is not true; and (b) in density estimation the population of data items is usually so large as to make it infeasible to check the class to which each data item belongs (e.g., only a limited sample of balls is picked), while this is not true in quantification, where it is assumed that all the items can be checked for the purpose of estimating the class distribution.\nA research area that might seem related to quantification is collective classification (CoC) [Sen et al. 2008]. Similarly to quantification, in CoC the classification of instances is not viewed in isolation. However, CoC is radically different from quantification in that its focus is on improving the accuracy of classification by exploiting relationships between the objects to classify (e.g., hypertextual documents that link to each other). Differently from quantification, CoC (a) assumes the existence of explicit relationships between the objects to classify, which quantification does not, and (b) is evaluated at the individual level, rather than at the aggregate level as quantification.\nQuantification bears strong relations with prevalence estimation from screening tests, an important task in epidemiology (see [Levy and Kass 1970; Lew and Levy 1989; Küchenhoff et al. 2012; Rahme and Joseph 1998; Zhou et al. 2002]). A screening test is a test that a patient undergoes in order to check if s/he has a given pathology. Tests are often imperfect, i.e., they may give rise to false positives (the patient is incorrectly diagnosed with the pathology) and false negatives (the test wrongly diagnoses the patient to be free from the pathology). Therefore, testing a patient is akin to classifying a document, and using these tests for estimating the prevalence of the pathology in a given population is akin to performing aggregative quantification. The main difference between this task and quantification is that a screening test typically has known and fairly constant recall (that epidemiologists call “sensitivity”) and fallout (whose complement epidemiologists call “specificity”), while the same usually does not happen for a classifier.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY."
    }, {
      "heading" : "6. CONCLUSIONS",
      "text" : "We have presented SVM(KLD), a new method for performing quantification, an important (if scarcely investigated) task in supervised learning, where estimating class prevalence, rather that classifying individual items, is the goal. The method is sharply different from most other methods presented in the literature. While most such methods adopt a general-purpose classifier (where the decision threshold has possibly been tuned according to some heuristics) and adjust the outcome of the “classify and count” phase, we adopt a straightforward “classify and count” approach (with no threshold tuning and/or a posteriori adjustment) but generate a classifier that is directly optimized for the evaluation measure used for estimating quantification accuracy. This is not straightforward, since an evaluation measure for quantification is inherently non-linear and multivariate, and thus does not lend itself to optimization via standard supervised learning algorithms. We circumvent this problem by adopting a supervised learning method for structured prediction that allows the optimization of non-linear, multivariate loss functions, and extend it to optimize KLD, the standard evaluation measure of the quantification literature.\nExperimental results that we have obtained by comparing SVM(KLD) with ten different state-of-the-art baselines show that SVM(KLD) (i) is more accurate (in a statistically significant way) than the competition, (ii) is more stable than the tested baselines, since it systematically shows very good performance irrespectively of class prevalence (i.e., level of imbalance) and distribution drift (i.e., discrepancy between the class distributions in the training and in the test set), (iii) is also accurate at the classification (aside from the quantification) level, and (iv) is 20 times faster to train than the competition. These experiments have been run, against 10 state-of-the-art baseline methods, on a batch of 5,148 binary, high-dimensional datasets (averaging more than 15,200 documents each and characterized by more than 50,000 features) and on a further batch of 352 binary, high-dimensional datasets (averaging more than 3,200 documents each and characterized by more than 10,000 features), all characterized by varying levels of class imbalance and distribution drift. These figures qualify the present experimentation as a very large one."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We are indebted to George Forman for letting us have the code for all the quantification methods he introduced in [Forman 2005; 2006a; 2008], which we have used as baselines. We are also very grateful to Thorsten Joachims for making his SVMperf package available, and for useful discussions about its use in quantification.\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\nA. APPENDIX: ON THE PRESUMED INVARIANCE OF TPR AND FPR ACROSS TEST SETS Most methods described in §2.2 (specifically: ACC, PACC, T50, X, MAX, MS) rely, among other things, on the assumption that tprTe and fprTe can reliably be estimated via k-fold cross-validation on the training set; in other words, they rely on the assumption that tpr and fpr do not change when the classifier is applied to different test sets.\nIndeed, Forman explicitly assumes that the application domains he confronts are of a type that [Fawcett and Flach 2005] call “y → x domains”, in which tpr and fpr are in fact invariant with respect to the test set the classifier is applied to. The notation y → x means that the value of the y variable (the output label) probabilistically determines the values of the x variables (the input features), i.e., x causally depends on y; in other words, the class-conditional probabilities p(x|y) are invariant across different test sets. For instance, our classification problem may consist in predicting whether a patient suffers or not from a given pathology (a fact represented by a binary variable y) given a vector x of observed symptoms. This is indeed a “y → x domain”, since the causality relation is from y to x, i.e., it is the presence of the pathology that determines the presence of the symptoms, and not vice versa. In other words, the class conditional probabilities p(x|y) do not change across test sets: if more people suffer from the pathology, more people will exhibit its symptoms. Important quantification problems within y → x domains do exist. One of them is when epidemiologists attempt to estimate the prevalence of various causes of death (y) from “verbal autopsies”, i.e., binary vectors of symptoms (x) as extracted from oral accounts obtained from relatives of the deceased [King and Lu 2008].\nHowever, many application domains are instead of the type that [Fawcett and Flach 2005] call “x → y domains”, where it is y that causally depends on x, and not vice versa. For instance, our classification problem may consist in predicting whether the home football team is going to win tomorrow’s match or not (y) given a number of stats (x) about its recent performance (e.g., number of goals scored in the last k matches, number of goals conceded, etc.). This is indeed a “x → y domain”, since the (assumed) causality relation is from x to y, i.e., the recent performance of the team is an indicator of its state of form, which may probabilistically determine the outcome of the game; it is certainly not the case that the outcome of the game determines the past performance of the team! And in x → y domains the class-conditional probabilities p(x|y) are not guaranteed to be constant.\nOne may wonder whether text classification contexts are y → x or x → y domains. Given the wide array of uses text classification has been put to, we think it is difficult to make general statements about this. We prefer to follow the advice in [Fawcett and Flach 2005], who recommend “that researchers test their assumptions in practice”. We have thus compared, for each of our 5,148 RCV1-V2 test sets and 352 OHSUMED-S test sets, (a) the tprTe and fprTe values that the standard linear SVM classifier (the one at the heart of all our baseline methods) has obtained, with (b) the corresponding tprTr and fprTr values that we have computed on the training set via k-fold cross validation. If tpr and fpr were invariant across different sets, then (a) and (b) should be approximately the same. The results, which are displayed in Table VII, show that in our domain tpr and fpr are far from being invariant across different sets. For instance, on RCV1-V2 the average tprTr as computed via k-fold cross-validation is 0.443, while the analogous average on the 5,148 test sets is 0.357, a -19.29% decrease; interestingly enough, on OHSUMED-S we witness an analogous trend but with opposite sign, with\nACM Transactions on Knowledge Discovery from Data, Vol. VV, No. NN, Article AA, Publication date: YYYY.\na +59.48% variation (from 0.196 to 0.313) in going from training to test sets17. A similar although less marked pattern can be observed for fpr.\nIn conclusion, tpr and fpr turn out to be far from being invariant across different sets, at least in the application contexts from which our datasets are drawn. It is thus evident that, at the very least in text quantification contexts, assuming that tpr and fpr are indeed invariant, and adopting methods that rely on this assumption, is risky, and definitely suboptimal in some cases. This is yet another reason to prefer methods, such as SVM(KLD), which do not rely on any such assumption."
    } ],
    "references" : [ {
      "title" : "Class and subclass probability re-estimation to adapt a classifier in the presence of concept drift",
      "author" : [ "Rocı́o Alaı́z-Rodrı́guez", "Alicia Guerrero-Curieses", "Jesús Cid-Sueiro" ],
      "venue" : "Neurocomputing 74,",
      "citeRegEx" : "Alaı́z.Rodrı́guez et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Alaı́z.Rodrı́guez et al\\.",
      "year" : 2011
    }, {
      "title" : "Variable-Constraint Classification and Quantification of Radiology Reports under the ACR Index",
      "author" : [ "Stefano Baccianella", "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "Expert Systems and Applications 40, 9 (2013), 3441–3449.",
      "citeRegEx" : "Baccianella et al\\.,? 2013",
      "shortCiteRegEx" : "Baccianella et al\\.",
      "year" : 2013
    }, {
      "title" : "Quantification-oriented learning based on reliable classifiers",
      "author" : [ "Jose Barranquero", "Jorge Dı́ez", "Juan José del Coz" ],
      "venue" : "Pattern Recognition 48,",
      "citeRegEx" : "Barranquero et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Barranquero et al\\.",
      "year" : 2015
    }, {
      "title" : "On the study of nearest neighbor algorithms for prevalence estimation in binary problems",
      "author" : [ "Jose Barranquero", "Pablo González", "Jorge Dı́ez", "Juan José del Coz" ],
      "venue" : "Pattern Recognition 46,",
      "citeRegEx" : "Barranquero et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Barranquero et al\\.",
      "year" : 2013
    }, {
      "title" : "Quantification via Probability Estimators",
      "author" : [ "Antonio Bella", "Cèsar Ferri", "José Hernández-Orallo", "Marı́a José Ramı́rez-Quintana" ],
      "venue" : "In Proceedings of the 11th IEEE International Conference on Data Mining (ICDM 2010). Sydney,",
      "citeRegEx" : "Bella et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bella et al\\.",
      "year" : 2010
    }, {
      "title" : "Word Sense Disambiguation with Distribution Estimation",
      "author" : [ "Yee Seng Chan", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI 2005). Edinburgh, UK, 1010–1015.",
      "citeRegEx" : "Chan and Ng.,? 2005",
      "shortCiteRegEx" : "Chan and Ng.",
      "year" : 2005
    }, {
      "title" : "Estimating Class Priors in Domain Adaptation for Word Sense Disambiguation",
      "author" : [ "Yee Seng Chan", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL 2006). Sydney, AU, 89–96.",
      "citeRegEx" : "Chan and Ng.,? 2006",
      "shortCiteRegEx" : "Chan and Ng.",
      "year" : 2006
    }, {
      "title" : "Elements of information theory",
      "author" : [ "Thomas M. Cover", "Joy A. Thomas." ],
      "venue" : "John Wiley & Sons, New York, US.",
      "citeRegEx" : "Cover and Thomas.,? 1991",
      "shortCiteRegEx" : "Cover and Thomas.",
      "year" : 1991
    }, {
      "title" : "Information Theory and Statistics: A Tutorial",
      "author" : [ "Imre Csiszár", "Paul C. Shields." ],
      "venue" : "Foundations and Trends in Communications and Information Theory 1, 4 (2004), 417–528.",
      "citeRegEx" : "Csiszár and Shields.,? 2004",
      "shortCiteRegEx" : "Csiszár and Shields.",
      "year" : 2004
    }, {
      "title" : "Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter",
      "author" : [ "Peter Sheridan Dodds", "Kameron Decker Harris", "Isabel M. Kloumann", "Catherine A. Bliss", "Christopher M. Danforth." ],
      "venue" : "PLoS ONE 6, 12 (2011).",
      "citeRegEx" : "Dodds et al\\.,? 2011",
      "shortCiteRegEx" : "Dodds et al\\.",
      "year" : 2011
    }, {
      "title" : "Machines that Learn how to Code Open-Ended Survey Data",
      "author" : [ "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "International Journal of Market Research 52, 6 (2010), 775–800.",
      "citeRegEx" : "Esuli and Sebastiani.,? 2010a",
      "shortCiteRegEx" : "Esuli and Sebastiani.",
      "year" : 2010
    }, {
      "title" : "Sentiment quantification",
      "author" : [ "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "IEEE Intelligent Systems 25, 4 (2010), 72–75.",
      "citeRegEx" : "Esuli and Sebastiani.,? 2010b",
      "shortCiteRegEx" : "Esuli and Sebastiani.",
      "year" : 2010
    }, {
      "title" : "Training Data Cleaning for Text Classification",
      "author" : [ "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "ACM Transactions on Information Systems 31, 4 (2013).",
      "citeRegEx" : "Esuli and Sebastiani.,? 2013",
      "shortCiteRegEx" : "Esuli and Sebastiani.",
      "year" : 2013
    }, {
      "title" : "A response to Webb and Ting’s ‘On the application of ROC analysis to predict classification performance under varying class distributions",
      "author" : [ "Tom Fawcett", "Peter Flach." ],
      "venue" : "Machine Learning 58, 1 (2005), 33–38.",
      "citeRegEx" : "Fawcett and Flach.,? 2005",
      "shortCiteRegEx" : "Fawcett and Flach.",
      "year" : 2005
    }, {
      "title" : "Counting Positives Accurately Despite Inaccurate Classification",
      "author" : [ "George Forman." ],
      "venue" : "Proceedings of the 16th European Conference on Machine Learning (ECML 2005). Porto, PT, 564–575.",
      "citeRegEx" : "Forman.,? 2005",
      "shortCiteRegEx" : "Forman.",
      "year" : 2005
    }, {
      "title" : "Quantifying trends accurately despite classifier error and class imbalance",
      "author" : [ "George Forman." ],
      "venue" : "Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (KDD 2006). Philadelphia, US, 157–166.",
      "citeRegEx" : "Forman.,? 2006a",
      "shortCiteRegEx" : "Forman.",
      "year" : 2006
    }, {
      "title" : "Tackling concept drift by temporal inductive transfer",
      "author" : [ "George Forman." ],
      "venue" : "Proceedings of the 29th ACM International Conference on Research and Development in Information Retrieval (SIGIR 2006). Seattle, US, 252–259.",
      "citeRegEx" : "Forman.,? 2006b",
      "shortCiteRegEx" : "Forman.",
      "year" : 2006
    }, {
      "title" : "Quantifying counts and costs via classification",
      "author" : [ "George Forman." ],
      "venue" : "Data Mining and Knowledge Discovery 17, 2 (2008), 164–206.",
      "citeRegEx" : "Forman.,? 2008",
      "shortCiteRegEx" : "Forman.",
      "year" : 2008
    }, {
      "title" : "Pragmatic text mining: Minimizing human effort to quantify many issues in call logs",
      "author" : [ "George Forman", "Evan Kirshenbaum", "Jaap Suermondt." ],
      "venue" : "Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (KDD 2006). Philadelphia, US, 852–861.",
      "citeRegEx" : "Forman et al\\.,? 2006",
      "shortCiteRegEx" : "Forman et al\\.",
      "year" : 2006
    }, {
      "title" : "Sentiment classification on customer feedback data: Noisy data, large feature vectors, and the role of linguistic analysis",
      "author" : [ "Michael Gamon." ],
      "venue" : "Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004). Geneva, CH, 841–847.",
      "citeRegEx" : "Gamon.,? 2004",
      "shortCiteRegEx" : "Gamon.",
      "year" : 2004
    }, {
      "title" : "Automating Survey Coding by Multiclass Text Categorization Techniques",
      "author" : [ "Daniela Giorgetti", "Fabrizio Sebastiani." ],
      "venue" : "Journal of the American Society for Information Science and Technology 54, 14 (2003), 1269–1277.",
      "citeRegEx" : "Giorgetti and Sebastiani.,? 2003",
      "shortCiteRegEx" : "Giorgetti and Sebastiani.",
      "year" : 2003
    }, {
      "title" : "Class distribution estimation based on the Hellinger distance",
      "author" : [ "Vı́ctor González-Castro", "Rocı́o Alaiz-Rodrı́guez", "Enrique Alegre" ],
      "venue" : "Information Sciences",
      "citeRegEx" : "González.Castro et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "González.Castro et al\\.",
      "year" : 2013
    }, {
      "title" : "OHSUMED: An interactive retrieval evaluation and new large text collection for research",
      "author" : [ "William Hersh", "Christopher Buckley", "T.J. Leone", "David Hickman." ],
      "venue" : "Proceedings of the 17th ACM International Conference on Research and Development in Information Retrieval (SIGIR 1994). Dublin, IE, 192–201.",
      "citeRegEx" : "Hersh et al\\.,? 1994",
      "shortCiteRegEx" : "Hersh et al\\.",
      "year" : 1994
    }, {
      "title" : "A Method of Automated Nonparametric Content Analysis for Social Science",
      "author" : [ "Daniel J. Hopkins", "Gary King." ],
      "venue" : "American Journal of Political Science 54, 1 (2010), 229–247.",
      "citeRegEx" : "Hopkins and King.,? 2010",
      "shortCiteRegEx" : "Hopkins and King.",
      "year" : 2010
    }, {
      "title" : "A support vector method for multivariate performance measures",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "Proceedings of the 22nd International Conference on Machine Learning (ICML 2005). Bonn, DE, 377–384. DOI:http://dx.doi.org/10.1145/1102351.1102399",
      "citeRegEx" : "Joachims.,? 2005",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2005
    }, {
      "title" : "Training Linear SVMs in Linear Time",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "Proceedings of the 12th ACM International Conference on Knowledge Discovery and Data Mining (KDD 2006). Philadelphia, US, 217–226.",
      "citeRegEx" : "Joachims.,? 2006",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2006
    }, {
      "title" : "Cutting-plane training of structural SVMs",
      "author" : [ "Thorsten Joachims", "Thomas Finley", "Chun-Nam Yu." ],
      "venue" : "Machine Learning 77, 1 (2009), 27–59.",
      "citeRegEx" : "Joachims et al\\.,? 2009a",
      "shortCiteRegEx" : "Joachims et al\\.",
      "year" : 2009
    }, {
      "title" : "Predicting Structured Objects with Support Vector Machines",
      "author" : [ "Thorsten Joachims", "Thomas Hofmann", "Yisong Yue", "Chun-Nam Yu." ],
      "venue" : "Commun. ACM 52, 11 (2009), 97–104.",
      "citeRegEx" : "Joachims et al\\.,? 2009b",
      "shortCiteRegEx" : "Joachims et al\\.",
      "year" : 2009
    }, {
      "title" : "The Impact of Changing Populations on Classifier Performance",
      "author" : [ "Mark G. Kelly", "David J. Hand", "Niall M. Adams." ],
      "venue" : "Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 1999). San Diego, US, 367–371.",
      "citeRegEx" : "Kelly et al\\.,? 1999",
      "shortCiteRegEx" : "Kelly et al\\.",
      "year" : 1999
    }, {
      "title" : "Verbal Autopsy Methods with Multiple Causes of Death",
      "author" : [ "Gary King", "Ying Lu." ],
      "venue" : "Statist. Sci. 23, 1 (2008), 78–91.",
      "citeRegEx" : "King and Lu.,? 2008",
      "shortCiteRegEx" : "King and Lu.",
      "year" : 2008
    }, {
      "title" : "Partially identified prevalence estimation under misclassification using the kappa coefficient",
      "author" : [ "Helmut Küchenhoff", "Thomas Augustin", "Anne Kunz." ],
      "venue" : "International Journal of Approximate Reasoning 53, 8 (2012), 1168–1182.",
      "citeRegEx" : "Küchenhoff et al\\.,? 2012",
      "shortCiteRegEx" : "Küchenhoff et al\\.",
      "year" : 2012
    }, {
      "title" : "A three-population model for sequential screening for bacteriuria",
      "author" : [ "Paul S. Levy", "E.H. Kass." ],
      "venue" : "American Journal of Epidemiology 91, 2 (1970), 148–154.",
      "citeRegEx" : "Levy and Kass.,? 1970",
      "shortCiteRegEx" : "Levy and Kass.",
      "year" : 1970
    }, {
      "title" : "Estimation of prevalence on the basis of screening tests",
      "author" : [ "Robert A. Lew", "Paul S. Levy." ],
      "venue" : "Statistics in Medicine 8, 10 (1989), 1225–1230.",
      "citeRegEx" : "Lew and Levy.,? 1989",
      "shortCiteRegEx" : "Lew and Levy.",
      "year" : 1989
    }, {
      "title" : "Representation and learning in information retrieval",
      "author" : [ "David D. Lewis." ],
      "venue" : "Ph.D. Dissertation. Department of Computer Science, University of Massachusetts, Amherst, US. http://www.research.att.com/∼lewis/ papers/lewis91d.ps",
      "citeRegEx" : "Lewis.,? 1992",
      "shortCiteRegEx" : "Lewis.",
      "year" : 1992
    }, {
      "title" : "Evaluating and optimizing autonomous text classification systems",
      "author" : [ "David D. Lewis." ],
      "venue" : "Proceedings of the 18th ACM International Conference on Research and Development in Information Retrieval (SIGIR 1995). Seattle, US, 246–254.",
      "citeRegEx" : "Lewis.,? 1995",
      "shortCiteRegEx" : "Lewis.",
      "year" : 1995
    }, {
      "title" : "RCV1: A New Benchmark Collection for Text Categorization Research",
      "author" : [ "David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li." ],
      "venue" : "Journal of Machine Learning Research 5 (2004), 361–397.",
      "citeRegEx" : "Lewis et al\\.,? 2004",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2004
    }, {
      "title" : "Handling Concept Drift via Ensemble and Class Distribution Estimation Technique",
      "author" : [ "Nachai Limsetto", "Kitsana Waiyamai." ],
      "venue" : "Proceedings of the 7th International Conference on Advanced Data Mining (ADMA 2011). Bejing, CN, 13–26.",
      "citeRegEx" : "Limsetto and Waiyamai.,? 2011",
      "shortCiteRegEx" : "Limsetto and Waiyamai.",
      "year" : 2011
    }, {
      "title" : "A Demographic Analysis of Online Sentiment during Hurricane Irene",
      "author" : [ "Benjamin Mandel", "Aron Culotta", "John Boulahanis", "Danielle Stark", "Bonnie Lewis", "Jeremy Rodrigue." ],
      "venue" : "Proceedings of the NAACL/HLT Workshop on Language in Social Media. Montreal, CA, 27–36.",
      "citeRegEx" : "Mandel et al\\.,? 2012",
      "shortCiteRegEx" : "Mandel et al\\.",
      "year" : 2012
    }, {
      "title" : "Quantification Trees",
      "author" : [ "Letizia Milli", "Anna Monreale", "Giulio Rossetti", "Fosca Giannotti", "Dino Pedreschi", "Fabrizio Sebastiani." ],
      "venue" : "Proceedings of the 13th IEEE International Conference on Data Mining (ICDM 2013). Dallas, US, 528–536.",
      "citeRegEx" : "Milli et al\\.,? 2013",
      "shortCiteRegEx" : "Milli et al\\.",
      "year" : 2013
    }, {
      "title" : "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series",
      "author" : [ "Brendan O’Connor", "Ramnath Balasubramanyan", "Bryan R. Routledge", "Noah A. Smith" ],
      "venue" : "In Proceedings of the 4th AAAI Conference on Weblogs and Social Media (ICWSM",
      "citeRegEx" : "O.Connor et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "O.Connor et al\\.",
      "year" : 2010
    }, {
      "title" : "Estimating the prevalence of a rare disease: Adjusted maximum likelihood",
      "author" : [ "Elham Rahme", "Lawrence Joseph." ],
      "venue" : "The Statistician 47 (1998), 149–158.",
      "citeRegEx" : "Rahme and Joseph.,? 1998",
      "shortCiteRegEx" : "Rahme and Joseph.",
      "year" : 1998
    }, {
      "title" : "Term-weighting approaches in automatic text retrieval",
      "author" : [ "Gerard Salton", "Christopher Buckley." ],
      "venue" : "Information Processing and Management 24, 5 (1988), 513–523.",
      "citeRegEx" : "Salton and Buckley.,? 1988",
      "shortCiteRegEx" : "Salton and Buckley.",
      "year" : 1988
    }, {
      "title" : "Concept drift",
      "author" : [ "Claude Sammut", "Michael Harries." ],
      "venue" : "Encyclopedia of Machine Learning, Claude Sammut and Geoffrey I. Webb (Eds.). Springer, Heidelberg, DE, 202–205.",
      "citeRegEx" : "Sammut and Harries.,? 2011",
      "shortCiteRegEx" : "Sammut and Harries.",
      "year" : 2011
    }, {
      "title" : "Classification and Quantification Based on Image Analysis for Sperm Samples with Uncertain Damaged/Intact Cell Proportions",
      "author" : [ "Lidia Sánchez", "Vı́ctor González", "Enrique Alegre", "Rocı́o Alaiz" ],
      "venue" : "In Proceedings of the 5th International Conference on Image Analysis and Recognition (ICIAR 2008). Póvoa de Varzim,",
      "citeRegEx" : "Sánchez et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sánchez et al\\.",
      "year" : 2008
    }, {
      "title" : "BoosTexter: A boosting-based system for text categorization",
      "author" : [ "Robert E. Schapire", "Yoram Singer." ],
      "venue" : "Machine Learning 39, 2/3 (2000), 135–168.",
      "citeRegEx" : "Schapire and Singer.,? 2000",
      "shortCiteRegEx" : "Schapire and Singer.",
      "year" : 2000
    }, {
      "title" : "Collective Classification in Network Data",
      "author" : [ "Prithviraj Sen", "Galileo Namata", "Mustafa Bilgic", "Lise Getoor", "Brian Gallagher", "Tina Eliassi-Rad." ],
      "venue" : "AI Magazine 29, 3 (2008), 93–106.",
      "citeRegEx" : "Sen et al\\.,? 2008",
      "shortCiteRegEx" : "Sen et al\\.",
      "year" : 2008
    }, {
      "title" : "Density estimation for statistics and data analysis",
      "author" : [ "Bernard W. Silverman." ],
      "venue" : "Chapman and Hall, London, UK.",
      "citeRegEx" : "Silverman.,? 1986",
      "shortCiteRegEx" : "Silverman.",
      "year" : 1986
    }, {
      "title" : "Network Quantification Despite Biased Labels",
      "author" : [ "Lei Tang", "Huiji Gao", "Huan Liu." ],
      "venue" : "Proceedings of the 8th Workshop on Mining and Learning with Graphs (MLG 2010). Washington, US, 147–154.",
      "citeRegEx" : "Tang et al\\.,? 2010",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2010
    }, {
      "title" : "Support Vector Machine Learning for Interdependent and Structured Output Spaces",
      "author" : [ "Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun." ],
      "venue" : "Proceedings of the 21 st International Conference on Machine Learning (ICML 2004). Banff, CA.",
      "citeRegEx" : "Tsochantaridis et al\\.,? 2004",
      "shortCiteRegEx" : "Tsochantaridis et al\\.",
      "year" : 2004
    }, {
      "title" : "WagTag: A Dog Collar Accessory for Monitoring Canine Activity Levels",
      "author" : [ "Gary M. Weiss", "Ashwin Nathan", "J.B. Kropp", "Jeffrey W. Lockhart." ],
      "venue" : "Proceedings of the 2013 ACM Conference on Pervasive and Ubiquitous Computing (UBICOMP 2013). Zurich, CH, 405–414.",
      "citeRegEx" : "Weiss et al\\.,? 2013",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2013
    }, {
      "title" : "Quantification and semi-supervised classification methods for handling changes in class distribution",
      "author" : [ "Jack Chongjie Xue", "Gary M. Weiss." ],
      "venue" : "Proceedings of the 15th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD 2009). Paris, FR, 897–906.",
      "citeRegEx" : "Xue and Weiss.,? 2009",
      "shortCiteRegEx" : "Xue and Weiss.",
      "year" : 2009
    }, {
      "title" : "Statistical Language Models for Information Retrieval: A Critical Review",
      "author" : [ "ChengXiang Zhai." ],
      "venue" : "Foundations and Trends in Information Retrieval 2, 3 (2008), 137–213.",
      "citeRegEx" : "Zhai.,? 2008",
      "shortCiteRegEx" : "Zhai.",
      "year" : 2008
    }, {
      "title" : "Transfer estimation of evolving class priors in data stream classification",
      "author" : [ "Zhihao Zhang", "Jie Zhou." ],
      "venue" : "Pattern Recognition 43, 9 (2010), 3151–3161.",
      "citeRegEx" : "Zhang and Zhou.,? 2010",
      "shortCiteRegEx" : "Zhang and Zhou.",
      "year" : 2010
    }, {
      "title" : "Statistical Methods in Diagnostic Medicine",
      "author" : [ "Xiao-Hua Zhou", "Donna K. McClish", "Nancy A. Obuchowski." ],
      "venue" : "Wiley, New York, US.",
      "citeRegEx" : "Zhou et al\\.,? 2002",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "The latter task is known as quantification [Forman 2005; 2006a; 2008; Forman et al. 2006].",
      "startOffset" : 43,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : ", predicting election results by estimating the prevalence of blog posts (or tweets) supporting a given candidate or party [Hopkins and King 2010], or planning the amount of human resources to allocate to different types of issues in a customer support center by estimating the prevalence of customer calls related to a given issue [Forman 2005], or supporting epidemiological research by estimating the prevalence of medical reports in which a specific pathology is diagnosed [Baccianella et al. 2013].",
      "startOffset" : 477,
      "endOffset" : 502
    }, {
      "referenceID" : 28,
      "context" : "Concept drift usually comes in one of three forms [Kelly et al. 1999]: (a) the class priors p(ci) may change, i.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : ", [Bella et al. 2010; Forman 2005; 2006a; 2008; Forman et al. 2006; Hopkins and King 2010; Xue and Weiss 2009]) employ general-purpose supervised learning methods, i.",
      "startOffset" : 2,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : ", [Bella et al. 2010; Forman 2005; 2006a; 2008; Forman et al. 2006; Hopkins and King 2010; Xue and Weiss 2009]) employ general-purpose supervised learning methods, i.",
      "startOffset" : 2,
      "endOffset" : 110
    }, {
      "referenceID" : 47,
      "context" : "The simplest such measure is bias (B), defined as B(λTe, λ̂Te) = λ̂Te(c)− λTe(c) and used in [Forman 2005; 2006a; Tang et al. 2010]; positive bias indicates a tendency to overestimate the prevalence of c, while negative bias indicates a tendency to underestimate it.",
      "startOffset" : 93,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; González-Castro et al. 2013; Sánchez et al. 2008; Tang et al. 2010]), de-",
      "startOffset" : 114,
      "endOffset" : 246
    }, {
      "referenceID" : 4,
      "context" : "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; González-Castro et al. 2013; Sánchez et al. 2008; Tang et al. 2010]), de-",
      "startOffset" : 114,
      "endOffset" : 246
    }, {
      "referenceID" : 21,
      "context" : "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; González-Castro et al. 2013; Sánchez et al. 2008; Tang et al. 2010]), de-",
      "startOffset" : 114,
      "endOffset" : 246
    }, {
      "referenceID" : 43,
      "context" : "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; González-Castro et al. 2013; Sánchez et al. 2008; Tang et al. 2010]), de-",
      "startOffset" : 114,
      "endOffset" : 246
    }, {
      "referenceID" : 47,
      "context" : "Absolute Error (AE - also used in [Esuli and Sebastiani 2010b], where it is called percentage discrepancy, and in [Barranquero et al. 2013; Bella et al. 2010; Forman 2005; 2006a; González-Castro et al. 2013; Sánchez et al. 2008; Tang et al. 2010]), de-",
      "startOffset" : 114,
      "endOffset" : 246
    }, {
      "referenceID" : 13,
      "context" : "The most convincing among the evaluation measures proposed so far is certainly Forman’s [2005], who uses normalized cross-entropy, better known as Kullback-Leibler Divergence (KLD – see e.",
      "startOffset" : 79,
      "endOffset" : 95
    }, {
      "referenceID" : 47,
      "context" : "and also used in [Esuli and Sebastiani 2010b; Forman 2006a; 2008; Tang et al. 2010], is a measure of the error made in estimating a true distribution λTe over a set C of classes by means of a distribution λ̂Te; this means that KLD is in principle suitable for evaluating quantification, since quantifying exactly means predicting how the test items are distributed across the classes.",
      "startOffset" : 17,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "The PCC method is dismissed as unsuitable in [Forman 2005; 2008], but is shown to perform better than CC in [Bella et al. 2010] (where it is called “Probability Average”) and in [Tang et al.",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 47,
      "context" : "2010] (where it is called “Probability Average”) and in [Tang et al. 2010].",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "The PACC method (proposed in [Bella et al. 2010], where it is called “Scaled Probability Average”) is a probabilistic variant of ACC, i.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "lead Forman [2008] to “clip” the results of the estimation (i.",
      "startOffset" : 5,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "Forman [2008] points out that the ACC method is very sensitive to the decision threshold of the classifier, which may yield unreliable values of λ Te (c) (or lead to λ Te (c) being undefined when tprTe = fprTe).",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 26,
      "context" : "SVMperf is a specialization to the problem of binary classification of the structural SVM (SVM) learning algorithm [Joachims et al. 2009a; Joachims et al. 2009b; Tsochantaridis et al. 2004] for “structured prediction”, i.",
      "startOffset" : 115,
      "endOffset" : 189
    }, {
      "referenceID" : 27,
      "context" : "SVMperf is a specialization to the problem of binary classification of the structural SVM (SVM) learning algorithm [Joachims et al. 2009a; Joachims et al. 2009b; Tsochantaridis et al. 2004] for “structured prediction”, i.",
      "startOffset" : 115,
      "endOffset" : 189
    }, {
      "referenceID" : 48,
      "context" : "SVMperf is a specialization to the problem of binary classification of the structural SVM (SVM) learning algorithm [Joachims et al. 2009a; Joachims et al. 2009b; Tsochantaridis et al. 2004] for “structured prediction”, i.",
      "startOffset" : 115,
      "endOffset" : 189
    }, {
      "referenceID" : 14,
      "context" : "In order to sidestep this problem, we adopt the SVM for Multivariate Performance Measures (SVMperf ) learning algorithm proposed by Joachims [2005]4.",
      "startOffset" : 72,
      "endOffset" : 148
    }, {
      "referenceID" : 27,
      "context" : "As discussed in [Joachims et al. 2009b], SVM can be adapted to a specific task by defining four components:",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 24,
      "context" : "4In [Joachims 2005] SVMperf is actually called SVM∆multi, but the author has released its implementation under the name SVMperf . We will use this latter name because it uniquely identifies the algorithm on the Web, while searching for “SVM multi” often returns the SVMmulticlass package, which addresses a different problem. 5For this formulation of Ψ, and when error rate is the chosen loss function, Joachims [2005] shows that SVMperf coincides with the traditional univariate SVM model (called SVMorg in [Joachims 2005]).",
      "startOffset" : 5,
      "endOffset" : 419
    }, {
      "referenceID" : 35,
      "context" : "Consistently with the evaluation presented in [Lewis et al. 2004], also classes placed at internal nodes in the hierarchically organized classification scheme are considered in the evaluation; as",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 35,
      "context" : "html 10This is the standard “LYRL2004” split between training and test data, originally defined in [Lewis et al. 2004].",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "The OHSUMED-S dataset [Esuli and Sebastiani 2013] is a subset of the wellknown OHSUMED test collection [Hersh et al. 1994].",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : "Concerning training, Joachims [2005] proves that training a classifier with SVMperf is O(n), with n the number of training examples, for any loss function that can be computed from a contingency table (such as KLD indeed is).",
      "startOffset" : 21,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "This is certainly more expensive than training a classifier by means of a standard, linear SVM (which is at the heart of Forman’s implementation of all the quantification methods of §2.2), since this latter is well-known to be O(sn) (with s the average number of non-zero features in the training objects) [Joachims 2006]. However, note that, while SVM(KLD) only requires training a classifier by means of SVMperf , setting up a quantifier with any of the methods of §2.2 (with the only exception of the simple CC method) requires more than simply training a classifier. For instance, ACC (together with the methods derived from it, such as T50, X, MAX, MS, PACC) also requires estimating tprTe and fprTe on the training set via k-fold cross validation, which may be expensive; analogously, both MM(KS) and MM(PP) require estimatingD c andD c via k-fold cross-validation, and the same considerations apply. In practice, using SVMperf turns out to be affordable. On RCV1-V2, training the 99 binary classifiers described in the previous sections via SVMperf required on average about 4.7 seconds each16. By contrast, training the analogous classifiers via a standard linear SVM required on average only 2.1 seconds each. However, this means that, if k-fold cross-validation is used for the estimation of parameters with a value of k ≥ 2 (meaning that, for each class, additional k classifiers need to be trained), the computational advantage of using a linear SVM instead of the more expensive SVMperf is completely lost. Forman [2008] recommends choosing k = 50 in order to obtain more accurate estimates of tprTe and fprTe for use in ACC and derived methods; this means making the training phase roughly (2.",
      "startOffset" : 121,
      "endOffset" : 1534
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.",
      "startOffset" : 0,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.",
      "startOffset" : 0,
      "endOffset" : 319
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.",
      "startOffset" : 0,
      "endOffset" : 349
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.",
      "startOffset" : 0,
      "endOffset" : 371
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.",
      "startOffset" : 0,
      "endOffset" : 398
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.e., attempt to estimate class prevalence in the test set in order to generate a classifier that better copes with differences in the class distributions of the training set and the test set. Many other works use quantification “without knowingly doing so”; that is, unaware of the existence of methods specifically optimized for quantification, they use classification with the only goal of estimating class prevalences. In other words, these works use plain “classify and count”. Among them, Mandel et al. [2012] use tweet quantification in order to estimate, from a quantitative point of view, the emotional responses of the population (segmented by location and gender) to a natural disaster; O’Connor et al.",
      "startOffset" : 0,
      "endOffset" : 969
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.e., attempt to estimate class prevalence in the test set in order to generate a classifier that better copes with differences in the class distributions of the training set and the test set. Many other works use quantification “without knowingly doing so”; that is, unaware of the existence of methods specifically optimized for quantification, they use classification with the only goal of estimating class prevalences. In other words, these works use plain “classify and count”. Among them, Mandel et al. [2012] use tweet quantification in order to estimate, from a quantitative point of view, the emotional responses of the population (segmented by location and gender) to a natural disaster; O’Connor et al. [2010] analyse the correlation between public opinion as measured via tweet sentiment quantification and via traditional opinion polls; Dodds et al.",
      "startOffset" : 0,
      "endOffset" : 1174
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.e., attempt to estimate class prevalence in the test set in order to generate a classifier that better copes with differences in the class distributions of the training set and the test set. Many other works use quantification “without knowingly doing so”; that is, unaware of the existence of methods specifically optimized for quantification, they use classification with the only goal of estimating class prevalences. In other words, these works use plain “classify and count”. Among them, Mandel et al. [2012] use tweet quantification in order to estimate, from a quantitative point of view, the emotional responses of the population (segmented by location and gender) to a natural disaster; O’Connor et al. [2010] analyse the correlation between public opinion as measured via tweet sentiment quantification and via traditional opinion polls; Dodds et al. [2011] use tweet sentiment quantification in order to infer spatio-temporal happiness patterns; and Weiss et al.",
      "startOffset" : 0,
      "endOffset" : 1323
    }, {
      "referenceID" : 1,
      "context" : "Baccianella et al. [2013] classify radiology reports with the aim of estimating the prevalence of different pathologies. Tang et al. [2010] focus on network quantification problems, i.e., problems in which the goal is to estimate class prevalence among a population of nodes in a network. Alaiz-Rodriguez et al. [2011], Limsetto and Waiyamai [2011], Xue and Weiss [2009], and Zhang and Zhou [2010] use quantification in order to improve classification, i.e., attempt to estimate class prevalence in the test set in order to generate a classifier that better copes with differences in the class distributions of the training set and the test set. Many other works use quantification “without knowingly doing so”; that is, unaware of the existence of methods specifically optimized for quantification, they use classification with the only goal of estimating class prevalences. In other words, these works use plain “classify and count”. Among them, Mandel et al. [2012] use tweet quantification in order to estimate, from a quantitative point of view, the emotional responses of the population (segmented by location and gender) to a natural disaster; O’Connor et al. [2010] analyse the correlation between public opinion as measured via tweet sentiment quantification and via traditional opinion polls; Dodds et al. [2011] use tweet sentiment quantification in order to infer spatio-temporal happiness patterns; and Weiss et al. [2013] use quantification in order to measure the prevalence of different types of pets’ activity as detected by wearable devices.",
      "startOffset" : 0,
      "endOffset" : 1436
    }, {
      "referenceID" : 4,
      "context" : "However, the experimental comparisons of [Bella et al. 2010] and [Tang et al.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 47,
      "context" : "2010] and [Tang et al. 2010] are both framed in terms of absolute error, which seems a sub-standard evaluation measure for this task (see §2.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 38,
      "context" : "The first published work that implements and tests the idea of directly optimizing a quantification-specific loss function is [Milli et al. 2013], whose authors propose variants of decision trees and decision forests that directly optimize a loss combining classification accuracy and quantification accuracy.",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "At the time of going to print we have become aware of a related paper [Barranquero et al. 2015] whose authors, following [Esuli and Sebastiani 2010b], use SVMperf to perform quantification; differently from the present paper, and similarly to [Milli et al.",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 38,
      "context" : "2015] whose authors, following [Esuli and Sebastiani 2010b], use SVMperf to perform quantification; differently from the present paper, and similarly to [Milli et al. 2013], they use an evaluation function that combines classification accuracy and quantification accuracy.",
      "startOffset" : 153,
      "endOffset" : 172
    }, {
      "referenceID" : 45,
      "context" : "A research area that might seem related to quantification is collective classification (CoC) [Sen et al. 2008].",
      "startOffset" : 93,
      "endOffset" : 110
    }, {
      "referenceID" : 30,
      "context" : "Quantification bears strong relations with prevalence estimation from screening tests, an important task in epidemiology (see [Levy and Kass 1970; Lew and Levy 1989; Küchenhoff et al. 2012; Rahme and Joseph 1998; Zhou et al. 2002]).",
      "startOffset" : 126,
      "endOffset" : 230
    }, {
      "referenceID" : 53,
      "context" : "Quantification bears strong relations with prevalence estimation from screening tests, an important task in epidemiology (see [Levy and Kass 1970; Lew and Levy 1989; Küchenhoff et al. 2012; Rahme and Joseph 1998; Zhou et al. 2002]).",
      "startOffset" : 126,
      "endOffset" : 230
    } ],
    "year" : 2015,
    "abstractText" : "Authors’ address: Andrea Esuli, Istituto di Scienza e Tecnologie dell’Informazione, Consiglio Nazionale delle Ricerche, Via Giuseppe Moruzzi 1, 56124 Pisa, Italy. E-mail: andrea.esuli@isti.cnr.it. Fabrizio Sebastiani, Qatar Computing Research Institute, PO Box 5825, Doha, Qatar. E-mail: fsebastiani@qf.org.qa. Fabrizio Sebastiani is on leave from Consiglio Nazionale delle Ricerche. The order in which the authors are listed is purely alphabetical; each author has given an equally important contribution to this work. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c © YYYY ACM 1556-4681/YYYY/-ARTAA $15.00 DOI:http://dx.doi.org/10.1145/0000000.0000000",
    "creator" : "TeX"
  }
}