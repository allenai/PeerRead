{
  "name" : "1704.07511.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scalable Planning with Tensorflow for Hybrid Nonlinear Domains",
    "authors" : [ ],
    "emails" : [ "wuga@mie.utoronto.ca", "bsay@mie.utoronto.ca", "ssanner@mie.utoronto.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n07 51\n1v 2\n[ cs\n.L G\n] 2\n9 A\npr 2\n01 7"
    }, {
      "heading" : "Introduction",
      "text" : "Many real-world hybrid (mixed discrete continuous) planning problems such as Reservoir Control [Yeh, 1985], Heating, Ventilation and Air Conditioning (HVAC) [Erickson et al., 2009; Agarwal et al., 2010], and Navigation [Faulwasser and Findeisen, 2009] have highly nonlinear transition and (possibly nonlinear) reward functions to optimize. Unfortunately, existing state-of-the-art hybrid planners [Ivankovic et al., 2014; Löhr et al., 2012; Coles et al., 2013; Piotrowski et al., 2016] are not compatible with arbitrary nonlinear transition and reward models. Monte Carlo Tree Search (MCTS) methods [Coulom, 2006; Kocsis and Szepesvári, 2006; Keller and Helmert, 2013]\nincluding AlphaGo [Silver et al., 2016] that can use any (nonlinear) black box model of transition dynamics do not inherently work with continuous action spaces due to the infinite branching factor. While MCTS with continuous action extensions such as HOOT [Weinstein and Littman, 2012] have been proposed, their continuous partitioningmethods do not scale to high-dimensional continuous action spaces (e.g., 100’s or 1,000’s of dimensions as used in this paper). Finally, offline model-free reinforcement learning (e.g., Q-learning) with function approximation [Sutton and Barto, 1998; Szepesvári, 2010] and deep extensions [Mnih et al., 2013] do not require any knowledge of the (nonlinear) transition model or reward, but they also do not directly apply to domains with high-dimensional continuous action spaces. I.e., offline learning methods like Q-learning require action maximization for every update, but in high-dimensional continuous action spaces such nonlinear function maximization is non-convex and computationally intractable at the scale of millions or billions of updates.\nTo address the above scalability and expressivity limitations of existing methods, we turn to Tensorflow [Abadi et al., 2015], which is a symbolic computation platform used in the machine learning community for deep\nlearning due to its compilation of complex layered symbolic functions into a representation amenable to fast GPU-based reverse-mode automatic differentiation [Linnainmaa, 1970] for gradient-based optimization. Given recent results in gradient descent optimization with deep learning that demonstrate the ability to effectively optimize high-dimensional non-convex functions, we ask whether Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces?\nOur results answer this question affirmatively, where we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent [Tieleman and Hinton, 2012] is surprisingly effective at planning in complex hybrid nonlinear domains. As evidence, we reference figure 1, where we show Tensorflow with RMSProp optimizing a path in a 2d nonlinear Navigation domain. In general, Tensorflow with RMSProp planning results are competitive with optimal MILPbased optimization on piecewise linear planning domains and directly extend to nonlinear domains, where they substantially outperform interior point methods for nonlinear function optimization. Furthermore, we remark that Tensorflow converges to a strong policy on a large-scale concurrent domain with 576,000 continuous actions spread over a horizon of 96 time steps in 4 minutes.\nTo explain such excellent results, we note that gradient descent algorithms such as RMSProp are highly effective for the non-convex function optimization that occurs in deep learning. Further, we provide an analysis of many transition functions in planning domains that suggest gradient descent on these domains will not suffer from either the vanishing or exploding gradient problems and hence provide a strong signal for optimization over long horizons. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with the Tensorflow toolkit."
    }, {
      "heading" : "Hybrid Nonlinear Planning via Tensorflow",
      "text" : ""
    }, {
      "heading" : "Hybrid Planning",
      "text" : "A hybrid planning problem is a tuple 〈S,A, T ,R, C〉 with S denoting the set of states, A the set of actions bounded by action constraints C, R : S × A → R the reward function and T : S × A → S the transition function. There is also an initial state s0 and the planning objective is to maximize the cumulative reward over a decision horizon of H time steps. Before proceeding, we outline necessary notation:\n• st: mixed discrete, continuous state vector at time t. • at: mixed discrete, continuous action vector at time t. • sitj : the jth dimension of state vector of problem i at time t. • aitj : the jth dimension of action vector of problem i at time t. • R(st, at): a non-positive reward function — higher absolute values indicate higher cost. • T (st, at): a (nonlinear) transition function.\n• V : cumulative value of reward to maximize:\nV = H ∑\nt=1\nrt = H−1 ∑\nt=0\nR(st, at)."
    }, {
      "heading" : "Planning through Backpropagation",
      "text" : "Backpropagation[Rumelhart et al., ] is a standard method for optimizing parameters of large multilayer neural networks via gradient descent. Via the chain rule of derivatives, backpropagation passes the derivative of the output error of a neural network back to each of its parameters in a single linear time pass in the size of the network using what is now simply known as reverse-mode automatic differentiation [Linnainmaa, 1970]. Despite its relative efficiency, backpropagation in large-scale (deep) neural networks is still computationally expensive and it is only with the advent of recent GPU-based symbolic toolkits like Tensorflow [Abadi et al., 2015] that recent advances in training very large deep neural networks have become possible. In this paper, we reverse the idea of training parameters of the network given fixed inputs to instead optimizing the inputs (i.e., actions) subject to fixed parameters (effectively the transition and reward parameterization assumed a priori known in planning). That is, given transition T (st, at) and reward functionR(st, at) whose parameters are fixed, we want to optimize the input at for all t to minimize the overall costbased reward. Specifically, we want to optimize all actions\na = a− η ∂L\n∂a , (1)\nwhere η is the optimization rate and the partial derivatives comprising the gradient based optimization are computed as\n∂L\n∂aitj =\n∂L\n∂Li ∂Li ∂aitj\n= ∂L\n∂Li ∂Li ∂sit+1 ∂sit+1 ∂aitj\n= ∂L\n∂Li ∂sit+1 ∂aitj\nT ∑\nτ=t+2\n[ ∂Li ∂riτ ∂riτ ∂siτ\nt+2 ∏\nκ=τ\n∂siκ siκ−1 ].\n(2)\nTools such as Tensorflow typically assume that a loss function is being minimized. To connect our planning objective to a standard Tensorflow loss function, we choose Mean Squared Error (MSE), which given two continuous vectorsY andY∗ is defined as\nMSE(Y,Y∗) = 1\n2 ‖Y∗ −Y‖2. (3)\nWe adopt MSE(0,V) to minimize our cost-based cumulative reward objective; here MSE takes as its input a constant vector 0 and objective value vectorV = (. . . , Vi, . . .) where Vi is the value of the ith problem instance. We will further explain the use of MSE in a moment, but first we digress to explain why we need to solve multiple problem instances i. Since both transition and reward functions are not assumed to be convex, optimization on such a domain could result in a local minimum. To mitigate this problem, we use randomly\ninitialized actions in a batch optimization: we optimize multiple mutually independent planning problems i simultaneously since the GPU can exploit their parallel computation, and then select the best-performing action sequence among the independent simultaneously solved problems. MSE then has dual effects of optimizing each problem instance i independently and providing fast convergence. We remark that simply defining the objective V and the definition of all state variables in terms of predecessor state and action variables via the transition dynamics (back to the known initial state constants) is enough for Tensorflow to build the symbolic directed acyclic graph (DAG) representing the objective and take its gradient w.r.t. to all free action parameters as shown in (2) using reverse-mode automatic differentiation."
    }, {
      "heading" : "Long Horizon Planning",
      "text" : "Given that the transition dynamics of planning problems are typically non-stationary, the Tensorflow compilation of a nonlinear planning problem reflects the same structure as recurrent neural network (RNN) deep nets commonly used in deep learning. The connection here is not superficial since a longstanding difficulty with training RNNs lies in the vanishing gradient problem, i.e., multiplying long sequences of gradients in the chain rule usually renders them extremely small and irrelevant for weight updates, especially when using nonlinear transfer functions such as a sigmoid. However in hybrid planning problems, continuous state updates often take the form si(t+1)j = sitj +∆ for some∆ function of the state and action at time t. Critically we note that the transfer function here is linear in sitj which is the largest determiner of si(t+1)j , hence avoiding vanishing gradients. In addition, a gradient can explode with the chain rule through backpropagation if the elements of the Jacobian matrix of state transitions are too large:\n∂s′ ∂s =\n\n    \n∂s′ 1 ∂s1 ∂s′ 1 ∂s2 . . . ∂s′ 1\n∂sn ∂s′\n2 ∂s1\n∂s′ 2 ∂s2 . . . ∂s′ 2\n∂sn ... ... ...\n... ∂s′n ∂s1 ∂s′n ∂s2 . . . ∂s′n ∂sn\n\n    \n(4)\nIn this case, if the planning horizon is large enough, a simple Stochastic Gradient Descent (SGD) optimizer would suffer from overshooting the optimum and never converge. RMSProp optimization algorithm has a significant advantage on backpropagation planning because of its gradient normalization ability. Specifically, instead of naively updating action aitj through\naitj = aitj − η ∂L\n∂aitj , (5)\nRMSProp maintains a decaying root mean squared gradients value G for each variable, which averages over squared gradients of previous epochs\nG′aitj = 0.9Gaitj + 0.1( ∂L\n∂aitj )2, (6)\nand updates each action variable through\naitj = aitj − η √\nGaitj + ǫ\n∂L\n∂aitj . (7)\nHere, the gradient is relatively small and consistent over iterations. Although Adagrad and Adadelta optimization algorithm have similar mechanisms, their learning rate could quickly reduce to an extremely small value when encountering large gradients. We compare all methods in experiments."
    }, {
      "heading" : "Experiments",
      "text" : "In this section, we introduce our three benchmark domains and then validate Tensorflow planning performance in the following steps. (1) We evaluate the optimality of the Tensorflow backpropagation planning on linear and bilinear domains through comparison with the optimal solution given by Mixture Integer Linear Programming (MILP). (2) We evaluate the performance of Tensorflow backpropagation planning on nonlinear domains (that MILPs cannot handle) through comparison with the Matlab-based interior point nonlinear solver FMINCON. (4) We investigate the impact of several popular gradient descent optimizers on planning performance. (5) We evaluate optimization of the learning rate."
    }, {
      "heading" : "Domain Descriptions",
      "text" : "Navigation: The Navigation domain is designed to test the ability of optimization of Tensorflow in a relatively small environment that supports different complexity transitions. Navigation has a 2D state of the agent location s and a 2D action a. Both of the states and action spaces are continuous and constrained by their maximum and minimum boundaries separately.\nThe goal of the problem is for an agent move to the target state as soon as possible (cf. figure 1). Therefore, we compute the reward based on the Manhattan distance from the agent to the target state at each time step as\nR(st, at) = −‖st − g‖1, (8)\nwhere g is the goal state.\nWe designed three different transition functions: nonlinear, bilinear and linear. The nonlinear transition has a radius deceleration zone in the center of the field. The agent’s movement distance is reduced based on its Euclidean distance to the center of deceleration zone. The following equation shows the transition function:\ndt = ‖st − z‖\nλ = 2\n1 + exp(−2dt) − 0.99\np = st + λat\nT (st, at) = max(u,min(l,p)),\n(9)\nwhere dt is the distance from the deceleration zone z, p is the proposed next state and u,l are upper and lower boundaries of domain respectively.\nThe bilinear domain is designed to compare with MILP where domain discretization is possible. In this setting, we evaluate the efficacy of approximately discretizing bilinear planning problems into MILPs. The following equation\nshows the transition function:\ndt =\n2 ∑\nj=1\n|stj − zj |\nλ =\n{\ndt 4 , dt < 4 1, dt ≥ 4\np = st + λat\nT (st, at) = max(u,min(l,p))\n(10)\nThe linear domain is the discretized version of the bilinear domain that MILP optimizes on. We also test Tensorflow on this domain to see the optimality of the Tensorflow solution. The following equation shows the transition function:\ndt = ‖st − z‖1\nλ =\n\n      \n       \n0.8, 3.6 ≤ dt < 4 0.6, 2.4 ≤ dt < 3.6 0.4, 1.6 ≤ dt < 2.4 0.2, 0.8 ≤ dt < 1.6 0.05, dt < 0.8 1, dt ≥ 4\np = st + λat\nT (st, at) = max(u,min(l,p))\n(11)\nReservoir Control: Reservoir Control [Yeh, 1985] is a system to control multiple connected reservoirs. Each of the reservoirs in the system has a single state sj ∈ R that denotes the water level of the reservoir j and a corresponding action to permit a flow aj ∈ [0, sj ] from the reservoir to the next downstream reservoir. The goal of this problem is to maintain the target water level of each reservoir in a safe range and as close to half of its capacity as possible. Therefore, we compute the reward based on the following equation:\ncj =\n\n\n 0, Lj ≤ sj ≤ Uj −5, sj < Lj −100, sj > Uj\nR(st, at) = −‖c− 0.1 ∗ | (u− l)\n2 − st|‖1,\n(12)\nwhere cj is the cost value of Reservoir j that penalizes water levels outside a safe range.\nWe introduce two settings: Nonlinear and Linear. For the nonlinear domain, nonlinearity due to the water loss ej for each reservoir j includes water usage and evaporation. The transition function is\net = 0.5 ∗ st ⊙ sin( st\nm )\nT (st, at) = st + rt − et − at + atΣ, (13)\nwhere⊙ represents an elementwise product, r is a rain quantity parameter,m is the maximum capacity of the largest tank, and Σ is a lower triangular adjacency matrix that indicates connections to upstream reservoirs.\nFor the linear domain, we only replace the nonlinear function of water loss by a linear function:\net = 0.1 ∗ st\nT (st, at) = st + rt − et − at + atΣ, (14)\nUnlike Navigation, We do not limit the state dimension of the whole system into 2D. In the experiments, we use domain setting of a network with 20 reservoirs.\nHVAC: Heating, Ventilation, and Air Conditioning [Erickson et al., 2009; Agarwal et al., 2010] is a centralized control problem, with concurrent controls of multiple rooms and even multiple connected buildings. For each room j there is a state variable sj denoting the temperature and an action aj for sending the specified volume of heated air to each room j via vent actuation. The goal of this problem is to maintain the temperature of each room in a comfortable range and consume as little energy as possible in doing so. Therefore, we compute the reward based on the following equation:\ndt = | (u− l)\n2 − st|\net = at ∗ C\nR(st, at) = −‖et + dt‖1,\n(15)\nwhere C is the unit electricity cost.\nSince thermal models for HVAC are inherently nonlinear, we only present one domain version with a nonlinear transition function:\nθt = at ⊙ (F vent − st)\nφt = (stQ− st ⊙\nJ ∑\nj=1\nqj)/wq\nϑt = (F out t − st)⊙ o/wo φt = (F hall t − st)⊙ h/wh\nT (st, at) = st + α ∗ (θt + φt + ϑt + φt),\n(16)\nwhere F vent, F outt and F hall t are temperatures of the room vent, outside and hallway, respectively, Q. o and h are respectively the adjacency matrix of rooms, adjacency vector of outside areas, and the adjacency vector of hallways. wq ,\nwo and wh are thermal resistances with a room and the hallway and outside walls, respectively. In the experiments, we work with a building layout with five floors and 12 rooms on each floor for a total of 60 rooms. For scalability testing, we apply batched backpropagation on 100 instances of such domain simultaneously, of which, there are 576,000 actions needed to plan concurrently over all time steps."
    }, {
      "heading" : "Planning Performance",
      "text" : "In this section, we investigate the performance of Tensorflow optimization through comparison with the MILP on linear domains and with Matlab’s fmincon nonlinear interior point solver on nonlinear domains. We ran our experiments on Ubuntu Linux system with one E5-1620 v4 CPU, 16GB RAM, and one GTX1080 GPU. The Tensorflow version is beta 0.12.1, the Matlab version is R2016b, and the MILP version is IBM ILOG CPLEX 12.6.3."
    }, {
      "heading" : "Performance in Linear Domains",
      "text" : "In figure 2, we show Tensorflow backpropagation planning results in lower cost plans than domain-specific heuristic policies, and the overall cost is relatively close to the optimal reward given by the MILP. While Tensorflow backpropagation planning always shows good performance, when comparing the performance of Tensorflow on bilinear and linear domains of Navigation to the MILP solution (recall that the linear domain was discretized from the bilinear case), we notice that Tensorflow does much better relative to the MILP on the bilinear domain than the discretized linear domain. The reason for this is quite simple: gradient optimization of smooth bilinear functions is actually much easier for Tensorflow than the piecewise linear discretized version which has large piecewise steps that make it hard for RMSProp to get a consistent and smooth gradient signal."
    }, {
      "heading" : "Performance in Nonlinear Domains",
      "text" : "In figure 3, we show Tensorflow backpropagation planning always achieves the best performance among all of the three methods. For relatively simple domains like Navigation, we see the fmincon nonlinear solver provides a very competitive\nsolution, while, for the complex domain HVAC with a large concurrent action space, the fmincon solver shows a complete failure at solving the problem in the given time period. In figure 4(a), Tensorflow backpropagation planning shows 16 times faster optimization in the first 15s, which is close to the result given by fmincon at 4mins. In figure 4(b), the optimization speed of it shows it to be hundreds of times faster than fmincon nonlinear solver to achieve the same value (if fmincon does ever reach it). These remarkable results demonstrate the power of fast parallel GPU computation of the Tensorflow framework."
    }, {
      "heading" : "Scalability",
      "text" : "In table 1, we show the scalability of Tensorflow backpropagation planning via the running times required to converge to for different domains. The results demonstrate the extreme efficiency with which Tensorflow can converge on exceptionally large nonlinear hybrid planning domains."
    }, {
      "heading" : "Optimization Methods",
      "text" : "In this experiment, we investigate the effects of different backpropagation optimizers. In figure 5, we show that the RMSProp optimizer provides exceptionally fast convergence\namong the five standard optimizers of Tensorflow. This observation reflects the previous analysis and discussion concerning equation (7) that RMSProp manages to avoid exploding gradients. As mentioned, although Adagrad and Adadelta have similar mechanisms, their normalization methods may cause vanishing gradients after several epochs, which corresponds to our observation of nearly flat curves for these methods. This is a strong indicator that exploding gradients are a significant concern for hybrid planning with gradient descent and that RMSProp performs well despite this well-known potential problem for long horizon gradients."
    }, {
      "heading" : "Optimization Rate",
      "text" : "In figure 6, we show the best learning optimization rate for the HVAC domain is 0.01 since this rate converges to nearoptimal extremely fast. The overall trend is smaller optimization rates have a better opportunity to reach a better final optimization solution, but can be extremely slow as shown for optimization rate 0.001. Hence, while larger optimization rates may cause overshooting, rates that are too small may simply converge too slowly for practical use. This suggests a critical need to tune the optimization rate per planning domain."
    }, {
      "heading" : "Conclusion",
      "text" : "We investigated the practical feasibility of using the Tensorflow toolbox to do fast, large-scale planning in hybrid nonlinear domains. We worked with a direct symbolic (nonlinear) planning domain compilation to Tensorflow for which we optimized planning actions directly through gradient-based backpropagation. We then investigated long horizon planning and suggested that RMSProp avoids both the vanishing and\nexploding gradient problems and showed experiments to corroborate this finding. Our key empirical results demonstrated that Tensorflow with RMSProp is competitive with MILPs on linear domains (where the optimal solution is known — indicating near optimality of Tensorflow and RMSProb for these non-convex functions) and strongly outperforms Matlab’s state-of-the-art interior point optimizer on nonlinear domains, optimizing up to 576,000 actions in under 4 minutes. These results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent such as RMSProp with highly optmized toolkits like Tensorflow."
    }, {
      "heading" : "Network Structure Clarification",
      "text" : "Forward propagation represents execution process of a plan given actions are filled into the network sequentially. A intermediate state of the planning problem is computed through the RNN like network structure as a hidden states. Cumulative reward function is\nL(r1, r2, r3 · · · ) =\nH ∑\ni\nγiri\nBack-propagation of the negative cumulative reward with respect to action sequence means updating actions to optimize cumulative reward, which is formally defined in equation 1 and 2.\nAdditional Visualization on HVAC problem\nFigure 8 shows that even though our model and Heuristic method show same temperature control performance, our model saves more energy.\n1Github Repository:https://github.com/wuga214/TOOLBOX-Learning-and-Planning-through-Backpropagation"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning",
      "author" : [ "Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : "on heterogeneous systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Occupancy-driven energy management for smart building automation",
      "author" : [ "Agarwal et al", "2010] Yuvraj Agarwal", "Bharathan Balaji", "Rajesh Gupta", "Jacob Lyles", "Michael Wei", "Thomas Weng" ],
      "venue" : "In Proceedings of the 2nd ACM Workshop on Embedded Sensing Systems",
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "JAIR)",
      "author" : [ "Amanda Jane Coles", "Andrew Coles", "Maria Fox", "Derek Long. A hybrid LP-RPG heuristic for modelling numeric resource flows in planning. J. Artif. Intell. Res" ],
      "venue" : "46:343–412,",
      "citeRegEx" : "Coles et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "pages 72–83",
      "author" : [ "Rémi Coulom. Efficient selectivity", "backup operators in monte-carlo tree search. In International Conference on Computers", "Games" ],
      "venue" : "Springer Berlin Heidelberg,",
      "citeRegEx" : "Coulom. 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Energy efficient building environment control strategies using realtime occupancy measurements",
      "author" : [ "Erickson et al", "2009] Varick L. Erickson", "Yiqing Lin", "Ankur Kamthe", "Rohini Brahme", "Alberto E. Cerpa", "Michael D. Sohn", "Satish Narayanan" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2009
    }, {
      "title" : "Nonlinear Model Predictive PathFollowing Control. In Nonlinear Model Predictive Control - Towards New Challenging Applications, Lecture Notes in Control and Information",
      "author" : [ "Faulwasser", "Findeisen", "2009] Timm Faulwasser", "Rolf Findeisen" ],
      "venue" : null,
      "citeRegEx" : "Faulwasser et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Faulwasser et al\\.",
      "year" : 2009
    }, {
      "title" : "Optimal planning with global numerical state constraints",
      "author" : [ "Ivankovic et al", "2014] Franc Ivankovic", "Patrik Haslum", "Sylvie Thiebaux", "Vikas Shivashankar", "Dana Nau" ],
      "venue" : "In International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "In Proceedings of the 23rd International Conference on Automated Planning and Scheduling",
      "author" : [ "Thomas Keller", "Malte Helmert. Trial-based heuristic tree search for finite horizon mdps" ],
      "venue" : "ICAPS 2013, Rome, Italy, June 10-14, 2013,",
      "citeRegEx" : "Keller and Helmert. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "In Proceedings of the 17th European Conference on Machine Learning (ECML-06)",
      "author" : [ "Levente Kocsis", "Csaba Szepesvári. Bandit based Monte-Carlo planning" ],
      "venue" : "pages 282–293,",
      "citeRegEx" : "Kocsis and Szepesvári. 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors",
      "author" : [ "S. Linnainmaa" ],
      "venue" : "Master’s thesis, Univ. Helsinki",
      "citeRegEx" : "Linnainmaa. 1970",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "A planning based framework for controlling hybrid systems",
      "author" : [ "Löhr et al", "2012] Johannes Löhr", "Patrick Eyerich", "Thomas Keller", "Bernhard Nebel" ],
      "venue" : "In Proceedings of the Twenty-Second International Conference on Automated Planning and Scheduling,",
      "citeRegEx" : "al. et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2012
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "NIPS Deep Learning Workshop.",
      "citeRegEx" : "Mnih et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Heuristic planning for hybrid systems",
      "author" : [ "Piotrowski et al", "2016] Wiktor Mateusz Piotrowski", "Maria Fox", "Derek Long", "Daniele Magazzeni", "Fabio Mercorio" ],
      "venue" : "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17,",
      "citeRegEx" : "al. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2016
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2016
    }, {
      "title" : "USA",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press", "MA Cambridge" ],
      "venue" : "1st edition,",
      "citeRegEx" : "Sutton and Barto. 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Algorithms for Reinforcement Learning",
      "author" : [ "Csaba Szepesvári" ],
      "venue" : "Morgan & Claypool,",
      "citeRegEx" : "Szepesvári. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Lecture 6.5—RmsProp: Divide the gradient by a running",
      "author" : [ "Tieleman", "Hinton", "2012] T. Tieleman", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Tieleman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman et al\\.",
      "year" : 2012
    }, {
      "title" : "Bandit-based planning and learning in continuous-action markov decision processes",
      "author" : [ "Weinstein", "Littman", "2012] Ari Weinstein", "Michael L. Littman" ],
      "venue" : "In Proceedings of the Twenty-Second International Conference on Automated Planning and Scheduling,",
      "citeRegEx" : "Weinstein et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Weinstein et al\\.",
      "year" : 2012
    }, {
      "title" : "Reservoir management and operations models: A state-of-the-art review",
      "author" : [ "William G Yeh" ],
      "venue" : "Water Resources research, 21,12:17971818,",
      "citeRegEx" : "Yeh. 1985",
      "shortCiteRegEx" : null,
      "year" : 1985
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Introduction Many real-world hybrid (mixed discrete continuous) planning problems such as Reservoir Control [Yeh, 1985], Heating, Ventilation and Air Conditioning (HVAC) [Erickson et al.",
      "startOffset" : 108,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "Unfortunately, existing state-of-the-art hybrid planners [Ivankovic et al., 2014; Löhr et al., 2012; Coles et al., 2013; Piotrowski et al., 2016] are not compatible with arbitrary nonlinear transition and reward models.",
      "startOffset" : 57,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "Monte Carlo Tree Search (MCTS) methods [Coulom, 2006; Kocsis and Szepesvári, 2006; Keller and Helmert, 2013] 0.",
      "startOffset" : 39,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "Monte Carlo Tree Search (MCTS) methods [Coulom, 2006; Kocsis and Szepesvári, 2006; Keller and Helmert, 2013] 0.",
      "startOffset" : 39,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "Monte Carlo Tree Search (MCTS) methods [Coulom, 2006; Kocsis and Szepesvári, 2006; Keller and Helmert, 2013] 0.",
      "startOffset" : 39,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : ", Q-learning) with function approximation [Sutton and Barto, 1998; Szepesvári, 2010] and deep extensions [Mnih et al.",
      "startOffset" : 42,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : ", Q-learning) with function approximation [Sutton and Barto, 1998; Szepesvári, 2010] and deep extensions [Mnih et al.",
      "startOffset" : 42,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : ", Q-learning) with function approximation [Sutton and Barto, 1998; Szepesvári, 2010] and deep extensions [Mnih et al., 2013] do not require any knowledge of the (nonlinear) transition model or reward, but they also do not directly apply to domains with high-dimensional continuous action spaces.",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "learning due to its compilation of complex layered symbolic functions into a representation amenable to fast GPU-based reverse-mode automatic differentiation [Linnainmaa, 1970] for gradient-based optimization.",
      "startOffset" : 158,
      "endOffset" : 176
    }, {
      "referenceID" : 9,
      "context" : "Via the chain rule of derivatives, backpropagation passes the derivative of the output error of a neural network back to each of its parameters in a single linear time pass in the size of the network using what is now simply known as reverse-mode automatic differentiation [Linnainmaa, 1970].",
      "startOffset" : 273,
      "endOffset" : 291
    }, {
      "referenceID" : 18,
      "context" : "Reservoir Control: Reservoir Control [Yeh, 1985] is a system to control multiple connected reservoirs.",
      "startOffset" : 37,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "Given recent deep learning results that demonstrate the ability to effectively optimize high-dimensional non-convex functions with gradient descent optimization on GPUs, we ask in this paper whether symbolic gradient optimization tools such as Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces? To this end, we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent is competitive with mixed integer linear program (MILP) based optimization on piecewise linear planning domains (where we can compute optimal solutions) and substantially outperforms state-of-the-art interior point methods for nonlinear planning domains. Furthermore, we remark that Tensorflow is highly scalable, converging to a strong policy on a largescale concurrent domain with a total of 576,000 continuous actions over a horizon of 96 time steps in only 4 minutes. We provide a number of insights that clarify such strong performance including observations that despite long horizons, RMSProp avoids both the vanishing and exploding gradients problem. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with highly optmized toolkits like Tensorflow. Introduction Many real-world hybrid (mixed discrete continuous) planning problems such as Reservoir Control [Yeh, 1985], Heating, Ventilation and Air Conditioning (HVAC) [Erickson et al., 2009; Agarwal et al., 2010], and Navigation [Faulwasser and Findeisen, 2009] have highly nonlinear transition and (possibly nonlinear) reward functions to optimize. Unfortunately, existing state-of-the-art hybrid planners [Ivankovic et al., 2014; Löhr et al., 2012; Coles et al., 2013; Piotrowski et al., 2016] are not compatible with arbitrary nonlinear transition and reward models. Monte Carlo Tree Search (MCTS) methods [Coulom, 2006; Kocsis and Szepesvári, 2006; Keller and Helmert, 2013] 0.600 0.750 .900 0.600 0.750 .900 0.600 0.750 0.9 00 0.600 0.750 .900 0.600 0.750 .900 0.600 0.750 .900 0.600 0.750 0.9 00 Epochs:10 Epochs:20 Epochs:40 Epochs:80 Epochs:160 Epochs:320 Figure 1: The evolution of RMSProp gradient descent based Tensorflow planning in a 2D Navigation domain with nested central rectangles indicating nonlinearly increasing resistance to robot movement. (top) In initial RMSProp epochs, the plan evolves directly towards the goal shown as a star. (bottom) As later epochs of RMSProp descend the objective cost surface, the fastest path evolves to avoid the central obstacle entirely. including AlphaGo [Silver et al., 2016] that can use any (nonlinear) black box model of transition dynamics do not inherently work with continuous action spaces due to the infinite branching factor. While MCTS with continuous action extensions such as HOOT [Weinstein and Littman, 2012] have been proposed, their continuous partitioningmethods do not scale to high-dimensional continuous action spaces (e.g., 100’s or 1,000’s of dimensions as used in this paper). Finally, offline model-free reinforcement learning (e.g., Q-learning) with function approximation [Sutton and Barto, 1998; Szepesvári, 2010] and deep extensions [Mnih et al., 2013] do not require any knowledge of the (nonlinear) transition model or reward, but they also do not directly apply to domains with high-dimensional continuous action spaces. I.e., offline learning methods like Q-learning require action maximization for every update, but in high-dimensional continuous action spaces such nonlinear function maximization is non-convex and computationally intractable at the scale of millions or billions of updates. To address the above scalability and expressivity limitations of existing methods, we turn to Tensorflow [Abadi et al., 2015], which is a symbolic computation platform used in the machine learning community for deep learning due to its compilation of complex layered symbolic functions into a representation amenable to fast GPU-based reverse-mode automatic differentiation [Linnainmaa, 1970] for gradient-based optimization. Given recent results in gradient descent optimization with deep learning that demonstrate the ability to effectively optimize high-dimensional non-convex functions, we ask whether Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces? Our results answer this question affirmatively, where we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent [Tieleman and Hinton, 2012] is surprisingly effective at planning in complex hybrid nonlinear domains. As evidence, we reference figure 1, where we show Tensorflow with RMSProp optimizing a path in a 2d nonlinear Navigation domain. In general, Tensorflow with RMSProp planning results are competitive with optimal MILPbased optimization on piecewise linear planning domains and directly extend to nonlinear domains, where they substantially outperform interior point methods for nonlinear function optimization. Furthermore, we remark that Tensorflow converges to a strong policy on a large-scale concurrent domain with 576,000 continuous actions spread over a horizon of 96 time steps in 4 minutes. To explain such excellent results, we note that gradient descent algorithms such as RMSProp are highly effective for the non-convex function optimization that occurs in deep learning. Further, we provide an analysis of many transition functions in planning domains that suggest gradient descent on these domains will not suffer from either the vanishing or exploding gradient problems and hence provide a strong signal for optimization over long horizons. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with the Tensorflow toolkit. Hybrid Nonlinear Planning via Tensorflow",
    "creator" : "LaTeX with hyperref package"
  }
}