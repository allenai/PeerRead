{
  "name" : "1205.2601.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Most Relevant Explanation: Properties, Algorithms, and Evaluations",
    "authors" : [ "Changhe Yuan", "Xiaolu Liu", "Tsai-Ching Lu" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Most Relevant Explanation (MRE) is a method for nding multivariate explanations for given evidence in Bayesian networks [12]. This paper studies the theoretical properties of MRE and develops an algorithm for nding multiple top MRE solutions. Our study shows that MRE relies on an implicit soft relevance measure in automatically identifying the most relevant target variables and pruning less relevant variables from an explanation. The soft measure also enables MRE to capture the intuitive phenomenon of explaining away encoded in Bayesian networks. Furthermore, our study shows that the solution space of MRE has a special lattice structure which yields interesting dominance relations among the solutions. A K-MRE algorithm based on these dominance relations is developed for generating a set of top solutions that are more representative. Our empirical results show that MRE methods are promising approaches for explanation in Bayesian networks."
    }, {
      "heading" : "1 Introduction",
      "text" : "Bayesian networks offer compact and intuitive graphical representations of uncertain relations among the random variables of a domain and provide a foundation for many diagnostic expert systems. However, these systems typically focus on disambiguating single-fault diagnostic hypotheses because it is hard to generate just right multiplefault hypotheses that contain only the most relevant faults. Maximum a Posteriori (MAP) assignment and Most Probable Explanation (MPE) are two explanation methods for Bayesian networks that nd a complete assignment to a set of target variables as the best explanation for given evidence and can be applied to generate multiple-fault hypotheses. A priori, the set of target variables is often large and can be in tens or even hundreds for a real-world diag-\nnostic system. Given that so many variables are involved, even the best solution by MAP or MPE may have an extremely low probability, say in the order of 10−6. It is hard to make any decision based on such hypotheses. In real-world problems, it is observed that usually only a few target variables are most relevant in explaining any given evidence. For example, there are many possible diseases in a medical domain, but a patient can have at most a few diseases at one time, as long as he or she does not delay treatments for too long. It is desirable to nd diagnostic hypotheses containing only those relevant diseases. Other diseases should be excluded from further tests or treatments. In a recent work, Yuan and Lu [12] propose an approach called Most Relevant Explanation (MRE) to generate explanations containing only the most relevant target variables for given evidence in Bayesian networks. Its main idea is to traverse a trans-dimensional space containing all the partial instantiations of the target variables and nd one instantiation that maximizes a relevance measure called generalized Bayes factor [3]. The approach was shown in [12] to be able to nd precise and concise explanations. This paper provides a study of the theoretical properties of MRE and offers further evidence for its validity. The study shows that MRE relies on an implicit soft relevance measure that enables the automatic identi cation of the most relevant target variables and pruning of less relevant variables from an explanation. Furthermore, the solution space of MRE has a special lattice structure that allows two interesting dominance relations among the solutions to be de ned. These dominance relations are used to design and develop a K-MRE algorithm for nding a set of top explanations that are more representative. Our empirical results show that MRE methods are promising approaches for explanation in Bayesian networks. The remainder of the paper is structured as follows. We rst review methods for explanation in Bayesian networks, including Most Relevant Explanation. Then we introduce several theoretical properties of Most Relevant Explanation. We also develop a K-MRE algorithm for generating multiple top explanations and evaluate it empirically."
    }, {
      "heading" : "2 A Running Example",
      "text" : "Let us rst introduce a running example used throughout this paper. Consider the circuit in Figure 1(a) adapted from [9, 12]. Gates A,B, C and D are defective if they are closed. The prior probabilities that the gates close independently are 0.016, 0.1, 0.15 and 0.1 respectively. Connections between the gates may not work properly with certain small probabilities. The circuit can be modeled with a diagnostic Bayesian network as shown in Figure 1(b). Nodes A,B, C and D correspond to the gates in the circuit and each has two states: defective and ok . Others are input or output nodes and have two states: current or noCurr . Uncertainty is introduced to the model such that an output node is in state current with a certain probability less than 1.0 if its parent gate, when exists, is defective and any of its other parents is in state current . Otherwise, it is in noCurr state with probability 1.0. For example, node output of B takes state current with probability 0.99 if parent gate B is in state defective and parent Input is in state current . Suppose we observe that current ows through the circuit, which means that nodes Input and Total Output in the Bayesian network are both in the state current . The task is to diagnose the system and nd the best fault hypotheses. Based on our knowledge of the domain, we know there are three basic scenarios that most likely lead to the observation: (1) A is defective; (2) B and C are defective; and (3) B and D are defective."
    }, {
      "heading" : "3 Related Work",
      "text" : "Many methods exist for explaining evidence in Bayesian networks. However, they often fail to nd just-right explanations containing the most relevant target variables. Many existing methods make simplifying assumptions and focus on singleton explanations [5, 7]. However, singleton explanations may be underspeci ed and are unable to fully explain given evidence. For the running example, the posterior probabilities of A,B, C, and D failing independently are 0.391, 0.649, 0.446, and 0.301 respectively. Therefore, (¬B) is the best singleton explanation1. However, B alone does not fully explain the evidence. C or D has to be involved. Actually, if we are not focusing on faulty states, (D) (0.699) is the best singleton explanation. It is clearly not an adequate explanation for the evidence. For a domain in which target variables are interdependent, multivariate explanations are often more natural for explaining given evidence. However, existing methods often produce hypotheses that are overspeci ed. MAP nds a con guration of a set of target variables that maximize the joint posterior probability given partial evidence on the other variables. For the running example, if we set A,B, C and D as the target variables, MAP will nd (A∧¬B∧¬C∧D) as the best explanation. However, given that B and C are faulty, A and D are somewhat redundant for explaining the evidence. MPE nds an explanation with even more variables. Several other approaches use the dependence relations encoded in Bayesian networks to prune independent variables [10, 11]. They will nd the same explanation as MAP because all of the target variables are dependent on the evidence. Yet several other methods measure the quality of an explanation using the likelihood of the evidence [1]. Unfortunately they will over t and choose (¬A ∧ ¬B ∧ ¬C ∧ ¬D) as the explanation, because the likelihood of the evidence given that all the target variables fail is almost 1.0. There have been efforts trying to generate more appropriate explanations. Henrion and Druzdzel [6] assume that a system has a set of pre-de ned scenarios as potential explanations and nd the scenario with the highest posterior probability. Flores et al. [4] propose to grow an explanation tree incrementally by branching the most informative variable at each step while maintaining the probability of each explanation above certain threshold. Nielsen et al. [8] use a different measure called causal information ow to grow the explanation trees. Because the explanations in the trees have to branch on the same variable(s), they may still contain redundant variables. Finding more concise hypotheses also have been studied in model-based diagnosis [2]. The approach focus on truth-based systems and cannot be easily generalized to deal with Bayesian networks.\n1We use a variable and its negation to stand for its ok and defective states respectively"
    }, {
      "heading" : "4 Most Relevant Explanation",
      "text" : "There are two most essential properties for a good explanation. First, the explanation should be precise, meaning it should explain the presence of the evidence well. Second, the explanation should be concise and only contain the most relevant variables. The above discussions show that existing approaches for explaining evidence in Bayesian networks often generate explanations that are either underspeci ed (imprecise) or overspeci ed (inconcise). To address the limitations, Yuan and Lu [12] propose a method called Most Relevant Explanation (MRE) to automatically identify the most relevant target variables for given evidence in Bayesian networks. First, explanation in Bayesian networks is formally de ned as follows. De nition 1. Given a set of target variables X in a Bayesian network and evidence e on the remaining variables, an explanation for the evidence is a partial instantiation x1:k of X, i.e., X1:k ⊆ X and X1:k 6= ∅. MRE is then de ned as follows [12]. De nition 2. Let X be a set of target variables, and e be the evidence on the remaining variables in a Bayesian network. Most Relevant Explanation is the problem of nding an explanation x1:k that has the maximum Generalized Bayes Factor score GBF (x1:k; e), i.e.,\nMRE(X, e) ≡ arg maxx1:k,X1:k⊆X,X1:k 6=∅GBF (x1:k; e) , (1) where GBF is de ned as\nGBF (x1:k1; e) ≡ P (e|x1:k1) P (e|x1:k1) . (2)\nTherefore, MRE traverses the trans-dimensional space containing all the partial assignments of X and nds an assignment that maximizes the GBF score. Potentially, MRE can use any measure that provides a common ground for comparing the partial instantiations of the target variables. GBF is chosen because it is shown to provide a plausible measure for representing the degree of evidential support in recent studies in Bayesian con rmation theory [3]. MRE was shown to be able to generate precise and concise explanations for the running example [12]. The best explanation according to MRE is:\nGBF (¬B,¬C; e) = 42.62 . (3)\nFor simplicity we often omit e and write GBF (¬B,¬C). (¬B,¬C) is a better explanation than both (¬A) (39.44) and (¬B,¬D) (35.88), because its prior and posterior probabilities are both relatively high; The posterior probabilities of the explanations are 0.394, 0.391, and 0.266 respectively. Therefore, MRE seems able to automatically identify the most relevant target variables and states as the explanations for given evidence."
    }, {
      "heading" : "5 A Theoretical Study",
      "text" : ""
    }, {
      "heading" : "5.1 Theoretical properties of MRE",
      "text" : "We now discuss several theoretical properties of MRE. Since MRE relies heavily on the GBF measure in generating its explanations, it is not surprising that these properties are mostly originated from GBF . The proofs of these properties can be found in the appendix. First, we note that GBF can be expressed in a different way using the belief update ratio. De nition 3. The belief update ratio of x1:k1 given e, r(x1:k1; e), is de ned as\nr(x1:k; e) ≡ P (x1:k|e) P (x1:k) . (4)\nGBF can then be expressed as the ratio between the belief update ratios of x1:k1 and alternative explanations x1:k1 given e, i.e.,\nGBF (x1:k1; e) = r(x1:k1; e) r(x1:k1; e) . (5)\nThe most important property of MRE is that it is able to weigh the relative importance of multiple variables and only include the most relevant variables in explaining the given evidence. The degree of relevance is evaluated using a measure called conditional Bayes factor (CBF) implicitly encoded in the GBF measure and de ned as follows. De nition 4. The conditional Bayes factor of hypothesis y1:m for given evidence e conditional on x1:k is de ned as\nCBF (y1:m; e|x1:k) ≡ P (e|y1:m,x1:k) P (e|y1:m,x1:k) . (6)\nThen, we have the following theorem. Theorem 1. Let the conditional Bayes factor of y1:m given x1:k be less than or equal to inverse of the belief update ratio of the alternative explanations x1:k, i.e.,\nCBF (y1:m; e|x1:k) ≤ 1 r(x1:k; e) , (7)\nthe following holds\nGBF (x1:k ∪ y1:m; e) ≤ GBF (x1:k; e). (8)\nTherefore, CBF (y1:m, e|x1:k) provides a soft measure on the relevance of a new set of variable states with regard to an existing explanation and can be used to decide whether or not to include them in an existing explanation. GBF also encodes a decision boundary, the inverse belief update\nratio of alternative explanations x1:k given e, which provides a threshold on how important the remaining variables should be in order to be included in the current explanation. If CBF (y1:m; e|x1:k) is greater than or equal to 1r(x1:k;e) , y1:m is regarded as relevant and will be included. Otherwise, y1:m will be excluded from the explanation. Theorem 1 has several intuitive and desirable corollaries. First, the following corollary shows that, for any explanation x1:k with belief update ratio greater than or equal to 1.0, adding any independent variable to the explanation will decrease its GBF score [12]. Corollary 1. Let x1:k be an explanation with r(x1:k; e) ≥ 1.0, and y be a state of variable Y independent from variables in x1:k and e. Then\nGBF (x1:k ∪ {y}; e) ≤ GBF (x1:k; e). (9)\nTherefore, adding an irrelevant variable dilutes the explanative power of an existing explanation. MRE is able to automatically prune such variables. This is clearly a desirable property. Note that we focus on the explanations with belief update ratio greater than or equal to 1.0. We believe that an explanation whose probability decreases given the evidence is unlikely to be a good explanation for the evidence. Corollary 1 requires the additional variable Y to be independent from both X1:k and E. The assumption is rather strong. The following corollary relaxes it to be that Y is conditionally independent from E given X1:k and shows the same result still holds. Corollary 2. Let x1:k be an explanation with r(x1:k; e) ≥ 1.0, and y be a state of a variable Y conditionally independent from variables in e given x1:k. Then\nGBF (x1:k ∪ {y}; e) ≤ GBF (x1:k; e). (10)\nCorollary 2 is a more general result than corollary 1 and captures the intuition that conditionally independent variables add no additional information to an explanation in explaining given evidence, even though the variable may be marginally dependent on the evidence. Also note that these properties are all relative to an existing explanation. It is possible that a variable is independent from the evidence given one explanation, but becomes dependent on the evidence given another explanation. In other words, GBF score is not monotonic. Looking at variables one by one does not guarantee to nd the optimal solution. The above results can be further relaxed to accommodate cases where the posterior probability of y given e is smaller than its prior, i.e.,\nCorollary 3. Let x1:k be an explanation with r(x1:k; e) ≥ 1.0, and y be a state of a variable Y such that P (y|x1:k, e) ≤ P (y|x1:k). Then\nGBF (x1:k ∪ {y}; e) ≤ GBF (x1:k; e). (11)\nThis is again an intuitive result; a variable state whose posterior probability decreases for given evidence should not be part of an explanation for the evidence. The above theoretical results can be veri ed using the running example. For example,\nGBF (¬B,¬C) > GBF (¬B,¬C, A) & GBF (¬B,¬C,D) > GBF (¬B,¬C, A,D) .\nThe results suggest that GBF has the intrinsic capability to penalize higher-dimensional explanations and prune less relevant variables."
    }, {
      "heading" : "5.2 Explaining away",
      "text" : "One unique property of Bayesian networks is that they can model the so called explaining away phenomenon using the V structure, i.e., a single variable with two or more parents. This structure intuitively captures the situation where an effect has multiple causes. Observing the presence of the effect and one of the causes reduces the likelihood of the presence of the other causes. It is desirable to capture this phenomenon when generating explanations. MRE seems able to capture the explaining away effect using CBF. CBF provides a measure on how relevant a new variable is to an existing explanation. In an explainingaway situation, if one of the causes is already present in the current explanation, other causes typically do not receive high CBF scores. Again for the running example, (¬B,¬C) and (¬A) are both good explanations for the evidence by themselves. The CBF of ¬A given only e (the effect) is equal to its GBF (39.44), which is rather high. However, when (¬B,¬C) (one of the causes) is also observed, CBF (¬A; e|¬B,¬C) becomes rather low and is only equal to 1.03. Clearly, CBF is able to capture the explaining away phenomenon in this example."
    }, {
      "heading" : "5.3 Dominance relations",
      "text" : "MRE has a solution space with an interesting lattice structure similar to the graph in Figure 2 for three binary target variables. The graph contains all the partial assignments of the target variables. Two explanations are linked together if they only have a local difference, meaning they either have the same set of variables with one variable in different states, or one explanation has one fewer variable than the other explanation with all the other variables being in the same states. There are two dominance relations among these potential solutions that are implied by Figure 2. The rst concept is strong dominance. De nition 5. An explanation x1:k strongly dominates another explanation y1:m if and only if x1:k ⊂ y1:m and GBF (x1:k) ≥ GBF (y1:m).\nIf x1:k strongly dominates y1:m, x1:k is clearly a better explanation than y1:m, because it not only has a no-worse explanative score but also is more concise. We only need to consider x1:k when nding multiple top MRE explanations. The second concept is weak dominance. De nition 6. An explanation x1:k weakly dominates another explanation y1:m if and only if x1:k ⊃ y1:m and GBF (x1:k) > GBF (y1:m).\nIn this case, x1:k has a strictly larger GBF score than y1:m, but the latter is more concise. It is possible that we can include them both and let the decision makers to decide whether they prefer higher score or conciseness. However, we believe that we only need to include x1:k, because its higher GBF score indicates that the extra variable states are relevant to explain given evidence and should be included in the explanation. Based on the two kinds of dominance relations, we de ne the concept minimal. De nition 7. An explanation is minimal if it is neither strongly nor weakly dominated by any other explanation.\nIn case we want to nd multiple top explanations, we only need to consider the minimal explanations, because they are the most representative ones."
    }, {
      "heading" : "6 K-MRE Algorithm",
      "text" : "In many decision problems, outputting the single top solution may not be the best practice. Decision makers typically would like multiple competing options to choose from. This is especially important when there are multiple solutions that are almost equally good. For the circuit example, all three basic explanations will lead to the same observation. However, we can only recover one explanation if we are satis ed with one top solution. It is better to\noutput all the top solutions rather than selecting any one of the solutions. The dominance relations de ned in the last section allow us to develop a K-MRE algorithm to nd a set of top solutions that are more representative. Let us look at the running example again to illustrate the idea. The explanations in Table 1 have the highest GBF scores. If we simply select top three explanations solely based on GBF, we will obtain these rather similar explanations: (¬B,¬C), (A,¬B,¬C), and (¬B,¬C,D), which are rather similar. Since (A,¬B,¬C), (¬B,¬C,D), and (A,¬B,¬C, D) are strongly dominated by (¬B,¬C), we should only consider (¬B,¬C) out of those four explanations. Similarly, (¬A,B) and (¬A,C) are strongly dominated by (¬A). These dominated explanations should be excluded from the top solution set. In the end, we get the set of top explanations shown in boldface in Table 1, which is clearly more diverse and representative than the original set. MAP and MPE clearly do not have this nice property. Therefore, our proposed K-MRE algorithm works as follows. Whenever we generate a new explanation, we check its score against the best solution pool. If it is lower than the worst score in the pool, reject the new explanation. If there are fewer than K best solutions or if the score of the new explanation is higher than the worst score in the pool, we consider adding the new explanation to the top pool. We rst check whether the new solution is strongly or weakly dominated by any of the top explanations. If so, reject the new explanation. Otherwise, we add the new explanation to the top pool. However, we then need to check whether there are existing top explanations that are dominated by the newly added explanation. If yes, these existing explanations should be excluded. Otherwise we delete the top explanation with the least score."
    }, {
      "heading" : "7 Empirical Results",
      "text" : ""
    }, {
      "heading" : "7.1 Experimental design",
      "text" : "We tested the K-MRE algorithm on a set of benchmark models, including Alarm, Circuit, Hepar, Munin, and SmallHepar. We chose these several models because we have the diagnostic versions of these networks, whose variables have been annotated into three categories: target, observation, and auxiliary. For generating the test cases, we used the networks as generative models and sampled with-\nout replacement from their prior probability distributions. We only kept those test cases with at least one abnormal observation and used the abnormal observations as evidence. Since Circuit and SmallHepar have 4 and 3 target variables respectively, we collected as many test cases as possible. Munin also has 4 target variables but each with many more states. Hepar and Alarm have 9 and 12 target variables respectively. We collected 50 test cases for the last three networks. We also extracted from them the test cases which contain at least two faulty target variables for separate experiments on multiple-fault test cases. Our experiments compared MRE with MAP given their similarities. We tested two versions of the MAP algorithm, one focusing on all the target variables (F-MAP) and the other only on the target variables selected by MRE (PMAP). In addition, we compared with the Marginal algorithm, which neglects the interdependence among the target variables and uses the marginal posterior probabilities to determine the most likely states of the target variables. We plot the accuracy statistics, including precision (the per-\ncentage of faulty states correctly identi ed among all faulty explanation variables) and recall (the percentage of faulty states correctly identi ed among all faulty variables in test cases) of these algorithms in Figure 3. We also include sample results on F-Score, which is de ned as\nF-Score = 2× (precision× recall) (precision + recall) . (12)"
    }, {
      "heading" : "7.2 Results and analysis",
      "text" : "We make the following observations from these results. First, MRE is able to achieve higher precision and/or recall rates in identifying the faulty target variables than the other algorithms on all the networks except Munin. An outstanding example is the SmallHepar network. Marginal, F-MAP and P-MAP all failed badly on this model in identifying the faulty variables, while MRE was able to achieve reasonable performance. It is clearly desirable given that one major goal of diagnosis or explanation is to identify problems, e.g. faulty states. We investigated the results of Munin\nnetwork further and found that all target variables of these test cases are in faulty states. Marginal and F-MAP have exactly the same statistics, which suggests that the target variables may have weak correlations with each other. This puts MRE in disadvantage because MRE takes into account such weak correlations and generate concise explanations with fewer target variables. On average, the explanations of MRE identi es 4.3 variables out of 12 target variables for Alarm, 1.7/4 for Circuit, 4/9 for Hepar, 2.5/4 for Munin, and 2.3/3 for SmallHepar. For networks with strong correlations among the target variables, e.g. Circuit and Hepar, MRE has much higher precision/recall rates. The sample F-score results in the case of K1F1 further con rmed the observation. Second, by comparing rows K1F1 vs. K3F1 and K1F2 vs. K3F2, we found that using multiple top solutions helps MRE signi cantly in improving the precision/recall rates than the other algorithms. With multiple solutions, we kept the results with the maximum precision rates. The results seem to support our claim that K-MRE was able to generate solutions that are more representative. It is somewhat surprising that the precision/recall rates of F-MAP were not improved at all on the networks, but those of P-MAP were improved. Our hypothesis is that, since the explanations by F-MAP are more grained because more variables are involved, its top explanations tend to agree with each other on the faulty variables and differ mostly in the less important non-faulty variables. Generating multiple top solutions could not really help F-MAP much in improving its accuracy statistics. Third, although P-MAP gets the target variables identi ed by MRE as input, it still failed badly on the SmallHepar network in identifying faulty states of the target variables. It did not show any signi cant advantage over F-MAP on other networks either. The results suggest that relying on posterior probabilities may not work well in certain diagnostic systems. Fourth, although multiple-fault cases are believed to be more dif cult because of their low likelihood, the algorithms in our experiments seem able to maintain the same level of accuracy rates in face of multiple-fault test cases (rows K1F2 and K3F2 ). We hope to apply the proposed methods to real-world systems and test cases to gain more insights. Last but not least, the Marginal algorithm is ef cient and sometimes can achieve similar accuracy rates with other methods. However, since it does not take into account the dependence among the target variables, its results can be arbitrarily bad if the dependence are strong. It is evident on the Circuit network for which the accuracy rates of Marginal algorithm are much lower than other methods. The results suggest that we have to be cautious about the use of the Marginal algorithm in certain systems."
    }, {
      "heading" : "8 Concluding Remarks",
      "text" : "In this paper, we discuss several theoretical properties of Most Relevant Explanation (MRE) and develop an algorithm for nding multiple top MRE solutions. Our study shows that MRE relies on an implicit soft relevance measure in automatically identifying the most relevant target variables and pruning less relevant variables from an explanation. The soft measure also enables MRE to capture the intuitive phenomenon of explaining away encoded in Bayesian networks. Furthermore, we de ne two dominance relations among the explanations that are implied by the structure of the solution space of MRE. These relations allow us to design and develop a K-MRE algorithm for nding top MRE solutions that are much more representative. Our empirical results agree quite well with the theoretical understanding of MRE. The results show that MRE is effective in identifying the most relevant target variables, especially the true faulty target variables. Furthermore, KMRE seems able to generate more representative top explanations than K-MAP methods. We believe that MRE is especially suitable for systems in which target variables are strong correlated with each other and can generate more precise and concise explanations for these systems. This research has many future works. It is desirable to understand the theoretical complexity of MRE. It has a solution space even larger than MAP and is believed to be at least as hard. Currently we rely on an exhaustive search algorithm for solving MRE and K-MRE. More ef cient methods for solving MRE need be developed to make it applicable to large real-world problems.\nAcknowledgement This research was supported by the National Science Foundation grant IIS-0842480. All experimental data have been obtained using SMILE, a Bayesian inference engine developed at the Decision Systems Laboratory at University of Pittsburgh and available at http://genie.sis.pitt.edu."
    } ],
    "references" : [ {
      "title" : "De ning explanation in probabilistic systems",
      "author" : [ "U. Chajewska", "J.Y. Halpern" ],
      "venue" : "In Proceedings of the Thirteenth Annual Conference on Uncertainty in Arti cial Intelligence",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1997
    }, {
      "title" : "Diagnosis with behavioral modes",
      "author" : [ "J. de Kleer", "B.C. Willams" ],
      "venue" : "In Proceedings of IJCAI-89,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1989
    }, {
      "title" : "Likelihoodism, Bayesianism, and relational con rmation",
      "author" : [ "B. Fitelson" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Abductive inference in Bayesian networks: nding a partition of the explanation space",
      "author" : [ "J. Flores", "J.A. Gamez", "S. Moral" ],
      "venue" : "In Eighth European Conference on Symbolic and  YUAN ET AL",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Toward normative expert systems: Part I the path nder project",
      "author" : [ "D.E. Heckerman", "E.J. Horvitz", "B.N. Nathwani" ],
      "venue" : "Methods of Information in Medicine,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1992
    }, {
      "title" : "Qualitative propagation and scenario-based schemes for explaining probabilistic reasoning",
      "author" : [ "M. Henrion", "M.J. Druzdzel" ],
      "venue" : "Uncertainty in Arti cial Intelligence",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1991
    }, {
      "title" : "A comparison of decision analysis and expert rules for sequential diagnosis",
      "author" : [ "J. Kalagnanam", "M. Henrion" ],
      "venue" : "In Proceedings of the 4th Annual Conference on Uncertainty in Arti cial Intelligence",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1988
    }, {
      "title" : "Explanation trees for causal Bayesian networks",
      "author" : [ "U. Nielsen", "J.-P. Pellet", "A. Elisseeff" ],
      "venue" : "In Proceedings of the 24th Annual Conference on Uncertainty in Arti cial Intelligence",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "What is the most likely diagnosis",
      "author" : [ "D. Poole", "G.M. Provan" ],
      "venue" : "Uncertainty in Arti cial Intelligence",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1991
    }, {
      "title" : "The role of relevance in explanation I: Irrelevance as statistical independence",
      "author" : [ "S.E. Shimony" ],
      "venue" : "International Journal of Approximate Reasoning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1993
    }, {
      "title" : "Ef cient multiple-disorder diagnosis by strategic focusing, pages 187204",
      "author" : [ "L. van der Gaag", "M. Wessels" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1995
    }, {
      "title" : "A general framework for generating multivariate explanations in Bayesian networks",
      "author" : [ "C. Yuan", "T.-C. Lu" ],
      "venue" : "In Proceedings of the Twenty-Third National Conference on Arti cial Intelligence",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Most Relevant Explanation (MRE) is a method for nding multivariate explanations for given evidence in Bayesian networks [12].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "In a recent work, Yuan and Lu [12] propose an approach called Most Relevant Explanation (MRE) to generate explanations containing only the most relevant target variables for given evidence in Bayesian networks.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "Its main idea is to traverse a trans-dimensional space containing all the partial instantiations of the target variables and nd one instantiation that maximizes a relevance measure called generalized Bayes factor [3].",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 11,
      "context" : "The approach was shown in [12] to be able to nd precise and concise explanations.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "Consider the circuit in Figure 1(a) adapted from [9, 12].",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 11,
      "context" : "Consider the circuit in Figure 1(a) adapted from [9, 12].",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Many existing methods make simplifying assumptions and focus on singleton explanations [5, 7].",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "Many existing methods make simplifying assumptions and focus on singleton explanations [5, 7].",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "Several other approaches use the dependence relations encoded in Bayesian networks to prune independent variables [10, 11].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "Several other approaches use the dependence relations encoded in Bayesian networks to prune independent variables [10, 11].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "Yet several other methods measure the quality of an explanation using the likelihood of the evidence [1].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "Henrion and Druzdzel [6] assume that a system has a set of pre-de ned scenarios as potential explanations and nd the scenario with the highest posterior probability.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "[4] propose to grow an explanation tree incrementally by branching the most informative variable at each step while maintaining the probability of each explanation above certain threshold.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] use a different measure called causal information ow to grow the explanation trees.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Finding more concise hypotheses also have been studied in model-based diagnosis [2].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "To address the limitations, Yuan and Lu [12] propose a method called Most Relevant Explanation (MRE) to automatically identify the most relevant target variables for given evidence in Bayesian networks.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : "MRE is then de ned as follows [12].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "GBF is chosen because it is shown to provide a plausible measure for representing the degree of evidential support in recent studies in Bayesian con rmation theory [3].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "MRE was shown to be able to generate precise and concise explanations for the running example [12].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "0, adding any independent variable to the explanation will decrease its GBF score [12].",
      "startOffset" : 82,
      "endOffset" : 86
    } ],
    "year" : 2009,
    "abstractText" : "Most Relevant Explanation (MRE) is a method for nding multivariate explanations for given evidence in Bayesian networks [12]. This paper studies the theoretical properties of MRE and develops an algorithm for nding multiple top MRE solutions. Our study shows that MRE relies on an implicit soft relevance measure in automatically identifying the most relevant target variables and pruning less relevant variables from an explanation. The soft measure also enables MRE to capture the intuitive phenomenon of explaining away encoded in Bayesian networks. Furthermore, our study shows that the solution space of MRE has a special lattice structure which yields interesting dominance relations among the solutions. A K-MRE algorithm based on these dominance relations is developed for generating a set of top solutions that are more representative. Our empirical results show that MRE methods are promising approaches for explanation in Bayesian networks.",
    "creator" : " TeX output 2009.05.27:0938"
  }
}