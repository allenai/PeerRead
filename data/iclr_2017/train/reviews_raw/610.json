{"conference": "ICLR 2017 conference submission", "title": "Evaluation of Defensive Methods for DNNs against Multiple Adversarial Evasion Models", "abstract": "Due to deep cascades of nonlinear units, deep neural networks (DNNs) can automatically learn non-local generalization priors from data and have achieved high performance in various applications. However, such properties have also opened a door for adversaries to generate the so-called adversarial examples to fool DNNs. Specifically, adversaries can inject small perturbations to the input data and therefore decrease the performance of deep neural networks significantly. Even worse, these adversarial examples have the transferability to attack a black-box model based on finite queries without knowledge of the target model.  Therefore, we aim to empirically compare different defensive strategies against various adversary models and analyze the cross-model efficiency for these robust learners. We conclude that the adversarial retraining framework also has the transferability, which can defend adversarial examples without requiring prior knowledge of the adversary models. We compare the general adversarial retraining framework with the state-of-the-art robust deep neural networks, such as distillation, autoencoder stacked with classifier (AEC), and our improved version, IAEC, to evaluate their robustness as well as the vulnerability in terms of the distortion required to mislead the learner. Our experimental results show that the adversarial retraining framework can defend most of the adversarial examples notably and consistently without adding additional vulnerabilities or performance penalty to the original model.", "reviews": [{"is_meta_review": true, "comments": "I reviewed the manuscript as of December 6th.\n\nThe authors perform a systematic investigation of various retraining methods for making a classification network robust to adversarial examples. The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples. This method suggests a promising direction for building a defense for adversarial examples.\n\nMajor Comments:\nI find the paper to not be lacking in exposition and clarity. The paper has a laundry list of related results (page 2) but no clear message. I *think* one larger point is the superior performance of their retraining techniques but it is not clear how well these techniques perform compared to other retraining techniques, nor are the details of the retraining techniques clear. The paper requires more discussion and a clear exposition about the methods the authors introduced (i.e. RAD, IAEC). What follow are some more detailed comments along this theme of improving the exposition and clarity:\n\n- The authors should provide more details about how they constructed the auto-encoder in the IAEC method (diagram?). The same needs to be said for the RAD method. The authors point to a previous workshop submission (", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The paper investigates several retraining approached based upon adversarial data. While the experimental evaluation looks reasonable, the actual contribution of this paper is quite small. The approaches being evaluated, for the most part, are already proposed in the literature, with the one exception being the \"improved autoencoder stacked with classifier\" (IAEC), which is really just a minor modification to the existing AEC approach with an additional regularization term. The results are fairly thorough, and seem to suggest that the IAEC method performs best in some cases, but this is definitely not a novel enough contribution to warrant publication at ICLR.\n \n Pros:\n + Nice empirical evaluation of several adversarial retraining methods\n \n Cons:\n - Extremely minor algorithmic advances\n - Not clear what is the significant contribution of the paper", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Paper revision", "OTHER_KEYS": "Xinyun Chen", "comments": "We thank all reviewers and readers for the helpful comments! We have updated the paper with the following changes:\n\n1) Provide more details about the experimental settings in section 4.1.\n2) Add a section \"Related Work\" (section 2) and move the detailed discussion of related works in Introduction to this section, so that the Introduction focuses more on our contributions.\n3) Update the Introduction section to highlight the differences between our work and previous works, and emphasize our contributions. Also, we provide more explanation of the term \"cross-model\" in Introduction section (at the top of page 2) to avoid confusion.\n4) Add the discussion of following related works: Huang et al. (", "IS_META_REVIEW": false, "DATE": "16 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Systematic experimental setting, but lack of clarity and originality", "is_meta_review": false, "comments": "This paper performs a series of experiments to systematically evaluate the robustness of several defense methods, including RAD, AEC and its improved version etc.. It provides interesting observations. Overall, RAD and distillation have the best performances, but none of the methods can really resist the 'additional' attack from cg or adam. Since it is an experimental paper, my main concern is about its clarity. See the comments below for details.\n\nPros:\n1. This paper provides a good comparison of the performances for the selected methods.\n2. Section 3.3 (the 'additional' attack) is a interesting investigation. Although the final result about the defense methods is negative, its results are still inspiring. \n3. Overall, this paper provides interesting and inspiring experimental results about the selected methods.\n\nCons:\n1. There are several other methods in the literature that are missing from the paper. For example the defense methods and the attack methods in the papers [1,2]. \n2. Although a long list of experimental results are provided in the paper, many details are skipped. For example, details of the experiments that generate the results in Table 5. \n3. Without further explanations and analyses about the experimental results, the contribution of the paper seems limited. \n4. This paper proposed an improved version of the AEC algorithm. But its experimental results seems not promising. \n\nMinor comments:\nPage 3: Equation (3) is also non-convex. So the non-convexity of Equation (2) should not be the motivation of Equation (3).\n\n[1] ", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Interesting comparisons, not very original.", "is_meta_review": false, "comments": "The paper compares several defense mechanisms against adversarial attacks: retraining, two kinds of autoencoders and distillation with the conclusion that the retraining methodology proposed by Li et al. works best of those approaches.\n\nThe paper documents a series of experiments on making models robust against adversarial examples. The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al's \"Explaining and harnessing adversarial examples\", stacked autoencoders were proposed by Szegedy et al's \"Intriguing Properties of Neural Networks\". The most original part of the paper is the improved version of autoencoders proposed in this paper.\n\nThe paper establishes experimental evidence that the RAD framework provides the best defense mechanism against adversarial attacks which makes the introduction of the improved autoencoder mechanism less appealing.\n\nAlthough the paper establishes interesting measurement points and therefore it has the potential for being cited as a reference, its relative lack of originality decreases its significance.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Official review of submission", "is_meta_review": false, "comments": "I reviewed the manuscript as of December 6th.\n\nThe authors perform a systematic investigation of various retraining methods for making a classification network robust to adversarial examples. The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples. This method suggests a promising direction for building a defense for adversarial examples.\n\nMajor Comments:\nI find the paper to not be lacking in exposition and clarity. The paper has a laundry list of related results (page 2) but no clear message. I *think* one larger point is the superior performance of their retraining techniques but it is not clear how well these techniques perform compared to other retraining techniques, nor are the details of the retraining techniques clear. The paper requires more discussion and a clear exposition about the methods the authors introduced (i.e. RAD, IAEC). What follow are some more detailed comments along this theme of improving the exposition and clarity:\n\n- The authors should provide more details about how they constructed the auto-encoder in the IAEC method (diagram?). The same needs to be said for the RAD method. The authors point to a previous workshop submission (", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "13 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "The definition of distortion", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "About the experiments of cg and adam", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Explored how each defense responds to transferability ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "is_meta_review": false}], "SCORE": 4, "authors": "Xinyun Chen, Bo Li, Yevgeniy Vorobeychik", "KEYWORDS": "robust adversarial retraining", "accepted": false, "id": ""}
