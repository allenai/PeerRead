{
  "name" : "1301.6717.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Representing and Combining Partially Specified CPTs",
    "authors" : [ "Suzanne M. Mahoney" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1. INTRODUCTION A Bayesian network represents the probabilistic dependencies among a set of random variables by a directed acyclic graph (Pearl, 1988). Each node in the network represents a random variable (a set of mutually exclusive and exhaustive possible values for some hypothesis). A node in a Bayesian network is conditionally independent of all non-descendant nodes given its direct parents. Each node in the netwo�k stores its probability distribution given its parents. Th1s information is sufficient to represent implicitly the full joint probability distribution over all nodes in the network. Efficient inference algorithms have been developed for computing the conditional distributio�s for each node given observed values of some nodes m the network.\nThe graphical structure of Bayesian networks makes explicit certain dependencies and independencies among the variables in the network. The standard representation for the conditional distribution of a node given its parents is a conditional probability table (CPT), which specifies a probability distribution for values of the node for each unique combination of parent states. Frequently, there are other structural relationships among the distributions in the conditional probability tables (CPTs). These relationships are represented only implicitly in the standard representation. A number of authors have suggested\nstructured representations that capture additional semantic information about the conditioning distributions. These include independence of causal influence models such as the noisy-OR (Pearl, 1988; Srini vas, 1993 ), parametric models such as the sigmoid function (Jaakkola and Jordan, 1996a; Neal, 1992), and models for asymmetric independence such as multinets (Geiger and Heckerman, 1991) or de�ision u:ees (Boutilier, et al. 1996). Such structural mformauon, when available, can greatly reduce the burden of knowledge elicitation and can often be exploited to increase the efficiency of inference.\nIn this paper, we propose a general representation for asymmetric independence relationships. Asymmetric independence occurs when a variable is independent of another variable for some configurations of its parent variable(s) but not others. When there is asymmetric independence, the standard CPT representation fails to capture structural knowledge implicitly encoded as equality relationships among distributions in the CPT. One common type of asymmetric independence is subset independence (Heckerman, 1990; Geiger and Heckerman, 1991), which occurs when the distribution of the child is independent of one parent given a subset of the values of another parent. The subset for which the distributions are the same may represent possible causes of a common effect. Subset independence relationships can be represented by specifying probability distributions not for each unique combination of parent states, but for each element of a panition of the cross product of the parents' state spaces.\nA number of belief network software packages now support partitions. In his local expression language for probabilistic dependencies, D'Ambrosio ( !995) defines rectangular partitions that are the cross-product of partitions of parent variables. Boutilier et a! (19?6) defined context-specific independence (CSI), m wh1ch independence relationships exist in some contexts (combinations of states of parent variables) but not others. They used trees to represent the conditioning structure of a CPT and showed how to exploit context specific independence for efficient inference. Poole ( 1998) detined minimal parent contexts that expressed\n392 Mahoney and Laskey\nthe conditioning for a given distribution by a minimum set of parent values. Shimony (1993) defined disjunctive assignments for nodes when the probability distributions of the descendent nodes were statistically independent of which value in the disjunction was conditioned on. Friedman and Goldszmidt (1996) use a decision tree to represent local structure in a conditional probability table to facilitate and improve learning of the Bayesian networks.\nIn this paper we will define asymmetry networks, an alternative representation for CPTs that is more expressive than the tree representation or the rectangular partition. Asymmetry networks can be used to represent subset independence, context-specific independence, or any independence relationship involving equality constraints among distributions in the CPT. Asymmetry networks are a natural way to specify the partial conditional distributions for network fragments (Laskey and Mahoney, 1997). Network fragments are an object-oriented representation designed to support construction of situation-specific networks (Mahoney and Laskey, 1998). A network fragment holds a partial representation of the distribution of its resident variables. Network fragments factor an asymmetry network in a way that provides an explicit representation for subset independence relationships and reveals context-specific independence. In this paper we define asymmetry networks and demonstrate their utility as a parsimonious representation for asymmetric independence. We describe how context-specific independence can be exploited to decompose an asymmetry network into subnetworks associated with hypothesis-conditioned network fragments. We describe operators for consistently combining the asymmetry subnetworks associated with hypothesis specific network fragments during construction of a situation-specific network.\n2. BACKGROUND 2.1 NETWORK FRAGMENTS\nNetwork fragments (Laskey and Mahoney, 1997) provide an object-oriented representation for probabilistic knowledge. A probability model is represented implicitly as a knowledge base of Bayesian network fragments. Each fragment consists of a set of random variables connected by a fragment graph, together with information used to construct local distributions for variables. Variables in a fragment are classified as resident, input, or hypothesis variables. Hypothesis variables represent the context in which the independence relationships in the fragment hold. Input variables condition the distributions of the resident variables, but have their own distributions defined in other fragments. The fragment represents information needed to construct the distributions of its resident variables. Context-specific independence relationships can be specified by identifying a set of hypothesis\nvariables. partitioning their state spaces into a set of contexts in which independence relationships hold, and defining a hypothesis-conditioned fragment for the remaining variables given each context.\nBoth fragments and random variables are objects organized in a type hierarchy. with associated structure and methods. Each random variable and fragment has a set of identifying attributes which, when specified, create a unique identifier for an instance of the random variable or fragment. There is a mapping from identifying attributes of the fragment to identifying attributes of its random variables. Identifying attributes play the role of variables in a logic programming language (to avoid confusion, we reserve the term variable for random variables).\nWhen a variable's probability distribution has local structure. it is often convenient to specify its distribution in several different fragments to be combined at run time into a full distribution. For example, the \"Disease\" node in a medical diagnosis system may be specified as a noisy-OR distribution with perhaps hundreds of input diseases. These input distributions could be organized into groups of related diseases, each represented as a separate fragment. At run-time there may be information available that rules out entire categories of diseases, requiring only a small percentage of the groups to be included in the final constructed model.\nWe use influence combination to represent local structure (Laskey and Mahoney, 1997). Random variables have an influence type with associated influence combination method. Enabling conditions for a given influence type specify restrictions on the influencing variables. For example, a noisy-OR variable must take only binary input variables. Each individual fragment uses an influence function to represent parameters of the influence combination method. For example, the trigger probability for a candidate cause in a noisy-OR distribution is returned by the influence function in the fragment where the cause-effect link is defined. In this paper we focus on local structure arising from asymmetric independence relationships.\n2.2 NETWORK CONSTRUCTION AND FRAGMENT COMBINATION\nIn complex domains it is infeasible to construct a complete Bayesian network encompassing all the situations one might encounter in problem solving. Knowledge-Based Model Construction (KBMC) is the process of constructing a model for a problem instance from a knowledge base representing generic domain entities and their interrelationships. A KBMC system includes a knowledge base, search operators for retrieving problem-relevant knowledge base elements, network construction operators, network evaluation operators, and model construction control mechanisms. Objectives for a KBMC system are to minimize costs of\nRepresenting and Combining Partially Specified CPTs 393\nrepresentation, retrieval, construction and evaluation, while providing accurate responses to queries.\nIn previous work (Laskey and Mahoney, 1998), we proposed a model construction control strategy for producing situation-specific networks from a knowledge base of network fragments. A situation specific network is a minimal network sufficient to respond to a query for which the knowledge base is query complete. During network construction, creation of random variable instances triggers the retrieval of network fragments in which the corresponding random variables are resident. The identifying attributes of the random variable in a retrieved fragment are unified with the values of the identifying attributes of the instance that triggered its retrieval. These identifying attribute values are also unified with the same identifying attributes appearing in other random variables in the fragment. This process creates new random variable instances, which triggers the retrieval of fragments in which they are resident.\nAfter a retrieved fragment is unified with its identifying attributes, variables in the retrieved fragment are combined with matching variables in the situation specific network. When fragments are combined, local distributions for the combined fragment are computed from the fragment influence functions using the node's influence combination method. The influence function for a variable in a fragment in which it is resident must provide the inputs needed by that variable's influence combination method. Thus, influence combination and influence functions must be designed to work together. We cataloged (Laskey and Mahoney, 1997) a number of generic influence combination methods: Simple Combination. independence of causal intluence (ICI). and Parameterized-Combination. We also defined the requirements for a combination method that combines hypothesis-conditioned fragments representing asymmetric independence relationships.\n3. ASYMMETRY NETWORKS 3.1 DEFINITION\nAn asymmetry network represents asymmetric independence relationships for a conditional probability table in a Bayesian network. An asymmetry network has three components: I) a random variable for which a distribution is being defined; 2) a partition of the state space defined by the parents of the random variable; and 3) a function from the partition elements of the parents • state space to distributions for the dependent variable. Whereas the tree representation is limited to rectangular partitions. an asymmetry network can represent an arbitrary partition of the state space defined by the parent variables.\nDefinition 1 Let n be a set of random variables with state space cr0. Let R be a partition of cr0. Let X be a random variable such that X li!' n. Let H be a set of distributions for X. Let F be a function from R to H.\nLet Pi be a partition element of R. If F( Pi ) is defined for all partition elements Pi e R, then F is known as a asymmetry network function for X and R. If 1t is empty and F is defined for the null set, then F is simply an asymmetry network function for X.\nThe partition R defines a partitioning of the conditioning events for the random variable into subsets for which the distribution is the same. The asymmetry network function maps each partition element into a probability distribution for X that applies for all parent variable configurations in the partition element. In our experience, the same partition R can be used as the domain of the asymmetry function for several different child variables. Because F maps a partition element to any positive number, an asymmetry network function may be defined in terms of ratios and frequencies, which convey semantic meaning, as well as probabilities. In our network fragment representation, we have specified an influence type called asymmetry_ variable, for which the influence function is an asymmetry function.\nDefinition 2 Let n be a set of random variables. Let R be a partition of the state space cr0. Let X be a random variable such that X li!' n. Let F be an asymmetry network function for X and R. Then N = (R. X, F) is an asymmetry network for X, with conditioning partition R and asymmetry function F.\nAn asymmetry network represents a conditional probability distribution for a dependent variable. It is often convenient to decompose an asymmetry network into a set of asymmetry subnetworks. An asymmetry subnetwork maps a subset of the associated asymmetry network's conditioning partition onto distributions for the dependent variable. The distributions of the asymmetry subnetwork must be consistent with those of the associated asymmetry network.\nDefinition 3 Let N = (R, X, F) be an asymmetry network. Let Pk be a partition element of R. Then Ni = (Rj. X. Fj) is an asymmetry subnetwork of N if Ri � R, Ri is the domain ofF_;, and F,{pk) = F( Pk) for all Pk e Ri. R; is called the conditioning partition for the subnetwork and F; is called the asymmetry function.\n3.2 ASYMMETRY NETWORKS EXAMPLE\nConsider the following example that illustrates how an asymmetry network encodes equality constraints among the conditional probability distributions of a variable. The task is to model the activity of military unit given a set of environmental conditions. Background information is as follows: (I) The unit does not want to be detected by airborne photographers while it is moving. (2) Nor does it wish to be on the road when driving may be hazardous. Figure I presents a graphical representation of the asymmetry network for the \"Activity\" of the unit. The shaded boxes represent the partition elements of an asymmetry partition for \"Activity\" and its parents. \"Wind\", \"Rain\" and \"Time\n394 Mahoney and Laskey\npl ...................... .\nBest to travel on a calm, dry night:good driving conditions and low visibility.\np2.\np3... .......... .\nPoor driving conditions in windy, wet weatber,but low visibility.\np4... ............. .\nMost visible in dry daylight.\n··- p(allpl) > p(aljp2)\np(a2ip2) + p(a2jpl) <p(a2jp3)\np(aljp3) > 3 xp(aljp4)\nconstraints among the distributions of the dependent variable. The numbers in the distributions reflect other constraints among the distributions, which can be captured as annotations to the distributions. While constraints other than equality ones are difficult to represent graphically, mathematical expressions represent some of these relationships rather nicely. The example illustrates only a few of the relationships that may exist among distributions. Druzdzel and van der Gaag (1995)\nP (Activity I Wind, Precipitation, TimeojDay) catalog a set of constraints that apply to the probabilities of a joint probability distribution. Wind: wl=Windy, w2=Calm\nRain: pl=Wet, p2=Dry TimeofDay: tl=Night, t2=Day Activity: al=Move, a2=Hide\nWellman (1990) catalogs relationships among variables in a qualitative probabilistic network. We hypothesize that many of the constraints tallied in these catalogs can be\nof Day\". Each partition element is a set of elements from the state space of the parent variables. The white boxes to the right are the associated conditional probability distributions. A statement summarizing the rationale used to determine the elements that appear in the partition element is below each partition element distribution pair. The key for the variables' states is at the lower right hand side of the picture.\nIn addition to the equality constraints encoded in the asymmetry network, there often exist semantic relationships between distributions for different partitions. In Figure I, annotated arrows link the partition element - distribution pairs. These annotations represent constraints that hold among the linked partition element-distribution pairs. The annotated arrow between the top two partition element distribution pairs represents the knowledge: the probability of the unit moving during a calm dry night is greater than the probability of the unit moving during wet weather or windy, dry nights. The annotation on the merged arrows from the top two pairs to the third pair represents the knowledge: the probability of hiding during windy wet weather is greater than the sum of the probability of hiding during a calm, dry night and the probability of hiding during calm wet weather or windy dry night.\nAs the example illustrates, asymmetry networks with their paired conditional probability distributions represent structural knowledge about the domain. The partition elements explicitly represent equality\nclaim that partttlon element-distribution pairs are an ideal foundation for a representation that expresses these constraints.\n3.3 REPRESENTING ASYMMETRY SUBNETWORKS IN NETWORK FRAGMENTS\nWe use asymmetry subnetworks to represent partial influence relationships among variables in a network fragment. We have the following objectives for the decomposition of an asymmetry network into asymmetry subnetworks: I) minimize the amount of information that must be represented in a subnetwork; 2) support combination of asymmetry subnetworks; 3) maximize reuse of the representation. Asymmetry networks are associated with network fragments in which their associated random variables are resident.\nBoth asymmetry networks and asymmetry subnetworks have three parts: a partition of the state space of a set of conditioning variables, a dependent variable and a function from the partition elements of the partition to probability distributions over the states of the dependent variable. The objective is to define an asymmetry subnetwork so that independence relationships hold within the subnetwork that may not hold globally. For example, a variable Y may be a parent of the variable X, but may be independent of X and all other variables in a fragment, given the context of the fragment. In such a case, Y need not be represented in the fragment.\nRepresenting and Combining Partially Specified CPTs 395\n3.4 AN EXAMPLE\nFigure 2 graphically illustrates how our representation can be used to represent the structural information associated with equality constraints among distributions in a CPT. It shows both the elements of the conditioning structure and the object classes that represent them.\nThe figure shows two elements of an asymmetry network for the dependent variable W. W has parents X, Y and Z. In this example, parent variable Z has two contexts, Z=z1 and Z=z2• (More generally, these contexts could be subsets of the state space of Z rather than single values.) An asymmetry subnetwork is defined for each of these contexts. The first subnetwork has the context variable Z set to z,. It has a single conditioning partition, marked XY-1. The conditioning partition for this subnetwork consists of two partition elements, represented graphically by the white and gray regions in the rectangle below the designation Z=z1• The first of these partition elements is represented in the figure, and is labeled XY -I. I. The elements of the partition element are (x1, y,), (Xz, y4), and (x3, y4). Because the context Z=z1 is understood, we do not explicitly represent it in the conditioning partition. In the second asymmetry subnetwork, Z is set to z2• Its partition is marked XY-2. Again, the asymmetry subnetwork consists of two partition elements, the first of which is represented in the figure and is marked XY -2.1. Again there is a graphical\nrepresentation of the represented elements: (x�o Yz), (x�o y4), (xz, y,), and (x3, y,).\nIn the illustration, the names of the classes being represented are in the right hand column. The symbol ® stands for cross-product. The symbol E9 indicates collect. At the bottom of the figure we have a row of simple assignments. Simple assignments are the atomic values for a variable and are identified by unique names. Note that each of these is a singleton element. The next higher row of rectangles represents simple partition elements. A simple partition element is collection of possible values for a single variable. An example of such a collection is [x2 E9 x3]. Conditioning assignments, the ellipses in the third row from the bottom, represent the cross-product of partition elements. The conditioning assignment on the left, XY · I, represents (xi, yl). The next conditioning assignment, XY-2, represents (x2, y4) and (x3, y4). These are simply the elements of the cross product of the simple partition elements for the variables X and Y.\nConditioning partitions and their conditioning partition elements represent the structure of partitions of the state space for a set of conditioning variables. A conditioning partition may have multiple conditioning partition elements. Associated with each element is a distribution for the dependent variable. The black boxes in Figure 2 make this association and so represent the asymmetry network function for the dependent variable W. We call the function an asvmmetry mapping. The key advantage\n396 Mahoney and Laskey\nor even just some of the same conditioning partition elements, the conditioning partition elements can be readily reused by simply having a different set of asymmetry mappings point to them.\nConsider Figure 2 again. Note that a given simple assignment may contribute to many simple partition elements and that a simple partition element may be part of many conditioning assignments. In turn each conditioning assignment may be referenced by many conditioning partition elements and conditioning partition elements may participate in many conditioning partitions. In addition, our experience is that several different local distributions will make use of the same conditioning partition. For this reason, it makes sense to have an explicit representation for partitions, partition elements and assignments. When creating a local distribution for a resident variable, we can simplify knowledge base construction by reusing or modifying existing structures. When deleting a distribution, we may not automatically delete the associated partitions if they have semantic meaning and may be reused.\nIn Figure 2, we illustrated portions of two asymmetry subnetworks. One was for the case Z=z1 and the other for Z=z2• In neither case do we explicitly represent the value of Z in the partition. Instead, the value of Z is held fixed as a context value. When constructing a\nsituation-specific network, it may be the case that both values of the context variable Z are relevant. Then, we combine the asymmetry subnetworks, making the context variable explicit. Figure 3 illustrates a portion of the combined asymmetry network. In the combined asymmetry network we retain elements of the old asymmetry networks. To bring the context for Z into the local distribution, we simply construct a conditioning partition element for all three variables. For example, the ellipse XYZ-1 represents a conditioning assignment for the cross product of the conditioning partition element XY -1.1 and the context value z1• The rectangle XYZ-1 is the corresponding conditioning partition element. Note that the same distribution, Distribution I, is now associated through an asymmetry mapping with the rectangle XYZ-1 instead of XY -I. I.\nThis representation for asymmetry networks has the following advantages. First of all it only represents what actually needs to be explicit for the asymmetry network. In the examples of Figure 2, the context value for Z does not have to be explicitly represented in the asymmetry network because it is pegged to either z 1 or z2 for all of the assignments being represented. Only when we combine the representations is the value of Z represented explicitly in the conditioning partitions.\nThe combined representation for asymmetry networks is parsimonious in that it represents only what is necessary to represent, and facilitates reuse of the partition elements of the contributing asymmetry networks. Note that both the new asymmetry and partition simply elements mappings conditional elements point of to\nthe contributing asymmetry networks. Therefore, constructing the combined asymmetry network effectively reuses representation from the contributing\nasymmetry networks. This permits the efficient maintenance of both the combined and uncombined asymmetry networks in a common knowledge base.\nContext-specific\nRepresenting and Combining Partially Specified CPTs 397\nindependence is represented rather nicely in a conditioning partition. In our above example, the dependent variable only depends upon conditioning values of X, Y and Z. These three variables may not be the only parents of the dependent variable W in the global network. The variable W may have another parent V which is applicable in some other context, say Ze { z3, z4}. In this case, the variable V need not be represented in the network fragments for contexts z1 and z2, but is represented in the fragment for the context { z3, z4}. When this context is combined with contexts z1 and z2, the variable V would appear as a parent to W. It is implicitly represented in the z1 and z2 fragments by its whole panition element, a partition element containing all of the possible values for V. In a tree representation (Boutilier, et al, 1996; Friedman and Goldzsmidt, 1996) a branch is simply the cross-product of its variables' values. Simple partition elements represent partitions of single variables (Geiger and Heckerman, 1991; D'Ambrosio, 1995). D'Ambrosio's rectangular partitions are cross-products of Simple Partitions.\n4. CONTEXT ALGEBRA This section describes an algebra for 'factoring' and combining the elements of partial conditional probability tables. It is called context algebra because it facilitates reasoning about the context elements in network fragments. Like any algebra it has elements and operators. Context algebra's elements represent the classes introduced in Figure 2. Its operators are collect, e. cross, ®, and Amap. Cross is used to form cross products and collect to aggregate elements of the same type. The Amap operator is the 'black box' that associates distributions with conditioning partition elements.\nFirst, context algebra represents both asymmetry subnetworks and the local distributions in hypothesis specific network fragments. Second, context algebra provides a sound method for combining asymmetry subnetworks and the local distributions in hypothesis specific network fragments.\n4.1 VARIABLE AND DISTRIBUTION TYPES\nBefore we define the elements and operators of the algebra we define variable types and distribution types. The types are vital to the sound functioning of the algebra. We not only have to distinguish random variable X from random variable Y, we also have to be able to distinguish among instances of random variables. For example, we may want to reason about the Activity of entity E at time t2 given the Activity of entity E at time t I. These would be represented as two different instances of the Activity variable with different values for the identifying attribute time.\nWe uniquely identify fragment variables by a combination of their random variable and the variable value pairs for their identifying attributes. We use a\nvariable-value pair in which the value is null to represent an identifying attribute whose value is unspecified. A variable-value is the concatenation the name of a random variable with the name of a state from the state space of the specified variable. All instances of a specified random variable that have the same set of variable-value pairs for their identifying attributes are of the same type. A variable-type is the name of a random variable concatenated with a set of variable values for each of its identifying attributes. A typed variable is usually denoted by a capital letter: for example X. Y, Z. If two typed variables have the same random variable and different values for their identifying attributes, they carry distinguishing subscripts in parentheses: for example if fragment variable X is instantiated for times t l and t2, we write �,1 > X< a>· We represent the possible values of a typed variable by subscripted small letters: for example X�o Yk· or z1,1)k where k denotes the k\nth possible value of the random variable.\nWe also type the distributions. A distribution-type is the variable type of the resident variable with which it is associated. We denote a distribution with a bold italic D whose first subscript specifies the typed variable. For example, Dxj denotes the t distribution for the random variable X. If necessary, we may include identifying attributes in parentheses following the variable name: e.g. Dxrau·\n4.2 ELEMENTS AND OPERATORS\nWe begin with a general definition for the elements and operators of our algebra.\nDefmition 4 An element is an object. It has one or more operators that take elements as parameters to construct new elements. An element has an element-type. Simple elements cannot be subdivided into simpler elements. Complex elements are composed of other elements.\nThe term brace represents subsets of a state space. In a Bayesian network. the parents of a dependent variable form a state space. A brace is associated with a distribution for the dependent variable. A set of braces and their associated distributions represents the local distribution for the dependent variable. Simple braces represent Simple_Assignments. A Simple_Assignment is the atomic representation for the states of a variable.\nDefinition 5 A simple brace is a simple element. It contains a variable-value pair and an element-type. Its element-type is the variable-type for the variable in the variable-value pair.\nUsually we denote a simple brace with a subscripted lower case character. For example, Yk is the kth partition element in a state space for fragment variable Y. If the identifying attributes, :!1!. of Y need to be made explicit, then the notation is Y<lll>k· We also have simple elements for carrying distributions and their types.\n398 Mahoney and Laskey\nDefinition 6 A typed distribution is a simple element. It contains a Distribution and a distribution-type.\nCombination operators construct complex elements from pairs of elements. Map operators pair elements with typed distributions.\nDefmition 7 A combination operator constructs a complex element from a pair of elements. The combination operator also establishes the element-type for the complex element.\nA complex brace is simply a composition of braces. Obviously, a complex brace can be recursively decomposed to reveal a set of simple braces.\nDefinition 8 A complex brace is a complex element obtained by combining a pair of braces with a combination operator. A type method returns the element-type for the brace.\nConsider the example shown in Figure 2. The Simple_Panition_Element [x2 E9 x3] is a complex brace. Its element-type is the same as that for the variable X. The complex brace, ([x2 E9 x3] ® y4), consists of a complex brace, [x2 $ x3], and a simple brace, y4. joined by the combination operator ®. Its element-type is a set of types, one for variable X and the second for variable Y. Conditioning_Assignments are usually braces like this one. Obviously, a complex brace can be recursively decomposed to reveal a set of simple braces.\nMappings are elements that contain distributions. Map operators join other elements with distributions. Note that the map operator works only with a single brace and a single distribution.\nDefinition 9 A map operator constructs an asymmetry mapping from a brace and a typed distribution. It also establishes the element-type for the asymmetry mapping.\nDefinition 10 An asymmetry mapping is a complex element. It contains a brace, a typed distribution, their map operator, and an element-type. The element-type is a pair consisting of the element-type for the brace and distribution-type for the typed distribution.\nSee Figure 2. The 'black boxes' represent asymmetry mappings. For example, the Conditioning_Panition_Element XY-I.I is represented by the brace, [([xz E9 x3] ® y4) E9 ([xd ® Yt)], and is mapped to Distribution I to form the asymmetry mapping { [([xz E9 x3] ® Y4) E9 ([xd ® Yt)l A Distribution I } . Assuming that the distributions are for variable V, the asymmetry mapping type is a pair: ({Tx,Tv).Tv) where Tk is the variable-type for variable k.\nSimple braces, complex braces and asymmetry mappings are the elements in our representation. The combination operators that operate on these elements are collect and cross. We use these operators to 'factor' a CPT.\nDefinition 11 Let S-r be a set of elements of type T. Collect, $, is a combination operator that takes two elements of the same element-type, T, to construct a complex element of the same element-type, T, such that:\n(I) ST is closed under collect. For any elements x and y in Sr. x E9 y is in ST.\n(2) The atomic elements comprising x E9 y consist of the union of the atomic elements comprising x and y respectively.\n(3) Collect is associative. For any elements x, y and z in Sr. x E9 (y $ z) = (x $ y) E9 z.\n(4) Collect is commutative. For any elements x and y in Sr. x E9 y = y E9 x.\n(5) Collect is idempotent. For any x in Sr. x E9 x = X.\n(6) The zero identity for collect is the null set. For any X in S-r, X $ 0 = X.\nA complex element formed with the collect operator is called a collection.\nA simple partition element is a collection of simple braces. Suppose that X has two possible values, xi and xko that make up a simple partition element. Then we represent that simple partition element by the complex brace [xi E9 xJ. This brace has the same element-type as X does. On the other hand, cross produces elements with a different type. The element-types for elements being crossed must be disjoint. The element-type for a crossed pair is the set of element-types for the contributing elements. However, cross is not defined for certain pairs of elements. To properly represent conditional probability tables, a complex element's type must be a concatenation of types for the parents with the type for a single dependent variable. An asymmetry mapping already has the type for the dependent variable. Crossing two asymmetry mappings or distributions for different dependent variables is meaningless. Therefore, an asymmetry mapping may not be crossed with another asymmetry mapping and a typed distribution may not be crossed with another typed distribution ..\nDefinition 12 Let STt be a set of elements of type T !. Let Sn be a set of elements of type T2. Cross, ®, is a combination operator that pairs two elements of disjoint element-types to produce a complex element of a third element-type such that:\n(I) For any x that is a typed distribution and any y, x ® y and y® x are not defined.\n(2) For any x and y that are both asymmetry mappings, x ® y and is not defined.\n(3) For any elements x in Sn and y in Sn and TI 11 T2 * 0, x ® y is not defined.\nRepresenting and Combining Partially Specified CPTs 399\n(4) For any elements x in S-r1 and y in Sn. and Till T2 = 0, x ® y is in S-r12 where Tl2 is the concatenation ofT I and T2.\n(5) Cross is associative. For any elements x in Sn, y in Sn and z in Sn. and T l ll T21l T3 = 0, x ® (y®z) = (x®y) ® z.\n(6) Cross is commutative. For any elements x in ST1 and y in Sn. and Till T2 = 0, x ® y = y ® x.\n(7) The zero identity for cross is the null set. For any x in ST�o x ® 0 = x.\n(8) Cross is distributive through collect. For any x in ST�o Yi and Yk in Sn. and T l ll T2 = 0, X ® [yi E9 Yk] = [(x ® Yi) E9 (x ® Yk)].\nObviously, any element in a state space for a set of parent variables can be represented by a cross of their simple partition elements. One example of ® is a Conditioning_Assignment. Suppose that X and Y are parents of Z with the following simple partition elements, the complex brace [xi E9 xJ for X, and the simple brace [yd for Y. The conditioning assignment is represented by the complex brace ([xi E9 xJ ® [y;]). Because cross is distributive through collect, any complex brace can be reduced to a set of atomic elements from the state space. For example, ([xi E9 xJ ® [y;]) =[(xi ® y,) E9 (xk ® y,)].\nAs illustrated in Figure 3, a conditioning partition element for the conditioning assignment could simply be the conditioning assignment itself or a collection of conditioning assignments.\nDefinition 13 AMap, 8, is a map operator that pairs a brace with a distribution to construct an asymmetry mapping. The type for the distribution may not be a member of the set of types for the brace.\n(I) For any elements x in S-r1 and Dv in Sn and T 11lT 2 ;1: 0, x t. Dv is not defined.\n(2) For any brace x in S-r1 and distribution Dv in Sn, and T 11lT 2 = 0, x t. Dv is in the set S-r1.n, all of whose elements are asymmetry mappings with type pair ( T\" T2}.\n(3) Map is distributive through collect. For any elements X; and xi in Sn. Dv in Sn. and\nT11lT2= 0, (X; E9 Xj) t. Dy = (X; t. Dy) E9 (Xj t. Dy ).\n( 4) Map is associative with cross. For any elements xi in Sn. Yk in Sn and Dz in S-r3, and\nT,llTzllT3 = 0, ( xi ® yd t. Dz = xi ® (yk t. Dz).\n4.3 FACTORED CPTS AND COMBINATION\nA factored CPT is a representation for a portion of a conditional probability table. It is a collection of asymmetry mappings. Each asymmetry mapping must represent the contribution of all of the parents for a\ndependent variable. Furthermore, the subset of the parent variables' state space represented by each asymmetry mapping must be disjoint from the subsets represented by the other asymmetry mappings in the factored CPT. This restricts an element of the parent state space to exactly one distribution over the values of the dependent variable. So the elements composing a factored CPT are mutually exclusive. However, they are not required to be exhaustive. This permits a factored CPT to represent the partially specified conditional probability tables of network fragments while maintaining consistency among the distributions assigned to elements of the state space for the parent variables.\nDefinition 14 Let X be a random variable. Let nx be the parents of X. A factored CPT for X is a collection of asymmetry mappings in which the braces for the elements of the asymmetry mapping are disjoint and whose element-type pair is the set of variable-types for Ox and the variable-type for X.\nWe (Mahoney, 1999) have shown that any asymmetry subnetwork and any hypothesis conditioned network fragment may be represented by a factored CPT and any factored CPT may be represented by an asymmetry subnetwork or a hypothesis conditioned network fragment. We have also shown that permitted combinations of factored CPTs produce valid factored CPTs. Furthermore. any subset of a conditional probability table can be represented by the algebra.\n5. CONCLUSION AND FUTURE RESEARCH We have presented an alternative representation for partially specified CPTs. The representation is first a parsimonious one for CPTs. As such, it supports the efficient elicitation and maintenance of the parameters of a Bayesian network.\nIt also represents the local distributions in hypothesis specific network fragments. We have also presented an algebra that supports the independence relationships and reveals context-specific independence. We described how context-specific independence can be exploited to decompose an asymmetry network into subnetworks associated with hypothesis-conditioned network fragments. We described operators for consistently combining the asymmetry subnetworks associated with hypothesis-specific network fragments during construction of a situation-specific network.\nThis representation for partially specified CPTs makes it easy to extract variables and values that serve as context to other variables. This is not only nice for minimizing the representation of local distributions in network fragments and for making elicitation in complex domains manageable, but it is also an important tool that facilitates reasoning about context during automated network construction. This is an area that we plan to explore in a future paper. We also look\n400 Mahoney and Laskey\ntowards using the algebra to extend D'Ambrosio's ( 1995) local expression language.\nAcknowledgements\nThe research reported in this paper was sponsored by DARPA and the U.S. Air Force Wright Laboratory under contract DACA76-97-0005 to Information Extraction and Transport, Inc. We also thank the anonymous reviewers for their helpful comments.\nBibliography\nBoutilier, C., N. Friedman, M. Goldszmidt, and D. Koller (1996) Context-Specific Independence in Bayesian Networks, in Recognition in Uncenainty in Anificial Intelligence: Proceedings of the Twelfth Conference, Morgan Kaufmann Publishers, San Francisco, CA. pp. 115-123.\nD'Ambrosio, B. (1995) Local Expression Languages for Probabilistic Dependence. In International Journal of Approximate Reasoning, P. P. Bonissone (ed), North Holland, New York. pp.61-81.\nDruzdzel, Marek J. and Linda C. van der Gaag (1995) Elicitation of Probabilities for Belief Networks. In Uncenainty in Anijicial Intelligence, Proceedings of the Eleventh Conference. Morgan Kaufmann, San Francisco, CA. pp. 141-148.\nFriedman, N. M. Goldszmidt ( 1996) Learning Bayesian Networks with Local Structure. In Uncenainty in Anificial Intelligence: Proceedings of the Founeenth Conference, E. Horvitz and F. Jensen (eds.), San Francisco, CA: Morgan Kaufmann. pp. 252-262.\nGeiger, D. and Heckerman, D. (1991) Advances in Probabilistic Reasoning, in Uncenainty in Anificial Intelligence: Proceedings of the Seventh Conference (1991) Morgan Kaufmann Publishers, San Mateo, CA. pp. 118-126.\nJaakkola, T. and Jordan, M. (1996b) Computing Upper and Lower Bounds on Likelihoods in Intractable Networks. In Horvitz, E. and Jensen, F. (eds)Uncenainty in Anificiallntelligence: Proceedings of the Twelfth Conference, San Francisco, CA: Morgan Kaufmann.\nLaskey, K. B. and S. M. Mahoney (1997) Network Fragments: Representing Knowledge for Constructing Probabilistic Models. In Geiger, D. and Shenoy, P. (eds)Uncenainty in Anificiallntelligence: Proceedings of the Thineenth Conference, San Francisco, CA: Morgan Kaufmann. pp. 334-341.\nMahoney, S. M. (1999) Network Fragments. Dissertation, School of Information Technology and Engineering, George Mason University, Fairfax, VA.\nMahoney, S.M. and K.B. Laskey (1996) Network Engineering for Complex Belief Networks. In Uncenainty in Anijicial Intelligence: Proceedings of\nthe Founeenth Conference, E. Horvitz and F. Jensen (eds.), San Francisco, CA: Morgan Kaufmann. pp. 370-378.\nMahoney, S.M. and K.B. Laskey (1998) Constructing Situation-Specific Belief Networks. In Uncenainty in Artificial Intelligence: Proceedings of the Fourteenth Conference, G. Cooper and S. Moral (eds.), San Francisco, CA: Morgan Kaufmann. pp. 370-378.\nMorgan, M. G, M. Henri on and M. Small ( 1990) Uncenainty: A Guide to Dealing with Uncenainty in Quantitative Risk and Policy Analysis, Cambridge University Press, New York.\nNeal, R. ( 1992) Connectionist Learning of Belief Networks. Anificiallntelligence 56, pp. 71-113.\nPearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA.\nPoole, D. (1998) Context-Specific Approximation in Probabilistic Inference. In Uncenainty in Anijicial Intelligence: Proceedings of the Founeenth Conference, G. Cooper and S. Moral (eds.), San Francisco, CA. pp. 447-454.\nShimony, S. E.(l993) Relevant Explanations: Allowing Disjunctive Assignments. In Uncenainty in Anijicial Intelligence: Proceedings of the Ninth Conference, Morgan Kaufmann Publishers, San Mateo, CA. pp. 200-207.\nSrinivas. Sampath (1993) A Generalization of the Noisy-Or Model. In Uncenainty in Anijicial Intelligence: Proceedings of the Ninth Conference. Morgan Kaufmann Publishers, San Mateo, CA. pp. 208-215.\nWellman, M. P. (1990) Fundamental Concepts of Qualitative Probabilistic Networks. Anificial Intelligence, 44(3):257-303, August."
    } ],
    "references" : [ {
      "title" : "Context-Specific Independence in Bayesian Networks, in Recognition in Uncenainty in Anificial Intelligence",
      "author" : [ "C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller" ],
      "venue" : "Proceedings of the Twelfth Conference,",
      "citeRegEx" : "Boutilier et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Boutilier et al\\.",
      "year" : 1996
    }, {
      "title" : "Local Expression Languages for Probabilistic Dependence",
      "author" : [ "B. D'Ambrosio" ],
      "venue" : "In International Journal of Approximate Reasoning,",
      "citeRegEx" : "D.Ambrosio,? \\Q1995\\E",
      "shortCiteRegEx" : "D.Ambrosio",
      "year" : 1995
    }, {
      "title" : "Elicitation of Probabilities for Belief Networks",
      "author" : [ "Druzdzel", "Marek J", "Linda C. van der Gaag" ],
      "venue" : "In Uncenainty in Anijicial Intelligence, Proceedings of the Eleventh Conference",
      "citeRegEx" : "Druzdzel et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Druzdzel et al\\.",
      "year" : 1995
    }, {
      "title" : "Learning Bayesian Networks with Local Structure",
      "author" : [ "Friedman", "N.M. Goldszmidt" ],
      "venue" : "In Uncenainty in Anificial Intelligence: Proceedings of the Founeenth Conference,",
      "citeRegEx" : "Friedman and Goldszmidt,? \\Q1996\\E",
      "shortCiteRegEx" : "Friedman and Goldszmidt",
      "year" : 1996
    }, {
      "title" : "Advances in Probabilistic Reasoning, in Uncenainty in Anificial Intelligence",
      "author" : [ "D. Geiger", "D. Heckerman" ],
      "venue" : "Proceedings of the Seventh Conference",
      "citeRegEx" : "Geiger and Heckerman,? \\Q1991\\E",
      "shortCiteRegEx" : "Geiger and Heckerman",
      "year" : 1991
    }, {
      "title" : "Computing Upper and Lower Bounds on Likelihoods in Intractable Networks",
      "author" : [ "T. Jaakkola", "M. Jordan" ],
      "venue" : "in Anificiallntelligence: Proceedings of the Twelfth Conference,",
      "citeRegEx" : "Jaakkola and Jordan,? \\Q1996\\E",
      "shortCiteRegEx" : "Jaakkola and Jordan",
      "year" : 1996
    }, {
      "title" : "Network Fragments: Representing Knowledge for Constructing Probabilistic Models",
      "author" : [ "K.B. Laskey", "S.M. Mahoney" ],
      "venue" : "in Anificiallntelligence: Proceedings of the Thineenth Conference,",
      "citeRegEx" : "Laskey and Mahoney,? \\Q1997\\E",
      "shortCiteRegEx" : "Laskey and Mahoney",
      "year" : 1997
    }, {
      "title" : "Network Engineering for Complex Belief Networks",
      "author" : [ "S.M. Mahoney", "K.B. Laskey" ],
      "venue" : "In Uncenainty in Anijicial Intelligence: Proceedings",
      "citeRegEx" : "Mahoney and Laskey,? \\Q1996\\E",
      "shortCiteRegEx" : "Mahoney and Laskey",
      "year" : 1996
    }, {
      "title" : "Constructing Situation-Specific Belief Networks",
      "author" : [ "S.M. Mahoney", "K.B" ],
      "venue" : "In Uncenainty in Artificial Intelligence: Proceedings of the Fourteenth Conference,",
      "citeRegEx" : "Mahoney and K.B.,? \\Q1998\\E",
      "shortCiteRegEx" : "Mahoney and K.B.",
      "year" : 1998
    }, {
      "title" : "Uncenainty: A Guide to Dealing with Uncenainty in Quantitative Risk and Policy Analysis",
      "author" : [ "Morgan", "M. G", "M.M. Henri on" ],
      "venue" : null,
      "citeRegEx" : "Morgan et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Morgan et al\\.",
      "year" : 1990
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Context-Specific Approximation in Probabilistic Inference",
      "author" : [ "D. Poole" ],
      "venue" : "In Uncenainty in Anijicial Intelligence: Proceedings of the Founeenth Conference,",
      "citeRegEx" : "Poole,? \\Q1998\\E",
      "shortCiteRegEx" : "Poole",
      "year" : 1998
    }, {
      "title" : "A Generalization of the Noisy-Or Model. In Uncenainty in Anijicial Intelligence: Proceedings of the Ninth Conference",
      "author" : [ "Srinivas. Sampath" ],
      "venue" : null,
      "citeRegEx" : "Sampath,? \\Q1993\\E",
      "shortCiteRegEx" : "Sampath",
      "year" : 1993
    }, {
      "title" : "Fundamental Concepts of Qualitative Probabilistic Networks",
      "author" : [ "M.P. Wellman" ],
      "venue" : "Anificial Intelligence,",
      "citeRegEx" : "Wellman,? \\Q1990\\E",
      "shortCiteRegEx" : "Wellman",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "A Bayesian network represents the probabilistic dependencies among a set of random variables by a directed acyclic graph (Pearl, 1988).",
      "startOffset" : 121,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "These include independence of causal influence models such as the noisy-OR (Pearl, 1988; Srini vas, 1993 ), parametric models such as the sigmoid function (Jaakkola and Jordan, 1996a; Neal, 1992), and models for asymmetric independence such as multinets (Geiger and Heckerman, 1991) or de�ision u:ees (Boutilier, et al.",
      "startOffset" : 254,
      "endOffset" : 282
    }, {
      "referenceID" : 4,
      "context" : "One common type of asymmetric independence is subset independence (Heckerman, 1990; Geiger and Heckerman, 1991), which occurs when the distribution of the child is independent of one parent given a subset of the values of another parent.",
      "startOffset" : 66,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Friedman and Goldszmidt (1996) use a decision tree to represent local structure in a conditional probability table to facilitate and improve learning of the Bayesian networks.",
      "startOffset" : 0,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "fragments (Laskey and Mahoney, 1997).",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Network fragments (Laskey and Mahoney, 1997) provide an object-oriented representation for probabilistic knowledge.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "We use influence combination to represent local structure (Laskey and Mahoney, 1997).",
      "startOffset" : 58,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "We cataloged (Laskey and Mahoney, 1997) a number of generic influence combination methods: Simple­ Combination.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "Activity: al=Move, a2=Hide Wellman (1990) catalogs relationships among variables in a qualitative probabilistic network.",
      "startOffset" : 27,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "Simple partition elements represent partitions of single variables (Geiger and Heckerman, 1991; D'Ambrosio, 1995).",
      "startOffset" : 67,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "Simple partition elements represent partitions of single variables (Geiger and Heckerman, 1991; D'Ambrosio, 1995).",
      "startOffset" : 67,
      "endOffset" : 113
    } ],
    "year" : 2011,
    "abstractText" : "This paper extends previous work with network fragments and situation-specific network construction. We formally define the asymmetry network, an alternative representation for a conditional probability table. We also present an object-oriented representation for partially specified asymmetry networks. We show that the representation is parsimonious. We define an algebra for the elements of the representation that allows us to 'factor' any CPT and to soundly combine the partially specified asymmetry networks.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}