{
  "name" : "1510.03528.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Yuchen Zhang", "Jason D. Lee", "Michael I. Jordan" ],
    "emails" : [ "yuczhang@eecs.berkeley.edu", "jasondlee@eecs.berkeley.edu", "jordan@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 0.\n03 52\n8v 1\n[ cs\n.L G\n] 1\n3 O"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural networks have been successfully applied in many areas of artificial intelligence, such as image classification, face recognition, speech recognition and natural language processing. Practical successes have been driven by the rapid growth in the size of data sets and the increasing availability of large-scale parallel and distributed computing platforms. Examples of recent work in this area include [16, 15, 22, 7, 9, 11].\nThe theoretical understanding of learning in neural networks has lagged the practical successes. It is known that any smooth function can be approximated by a network with just one hidden layer [4], but training such a network is NP-hard [6]. In practice, people use optimization algorithms such as stochastic gradient descent (SGD) to train neural networks. Although strong theoretical results are available for SGD in the setting of convex objective functions, there are few such results in the nonconvex setting of neural networks. While it is possible to transform the neural network training problem to a convex optimization problem involving an infinite number of variables [5], the infinitude of variables means that there is no longer a guarantee that the learning algorithm will terminate in polynomial time.\nSeveral recent papers have risen to the challenge of establishing polynomial-time learnability results for neural networks. These papers necessarily (given that the problem is NP-hard) introduce additional assumptions or relaxations. For instance, one may assume that the data is in fact generated by the neural network. Under this assumption, Arora et al. [2] study the recovery of denoising auto-encoders which are represented by multi-layer neural networks. They assume that the toplayer values of the network are randomly generated and all network weights are randomly drawn\nfrom {−1, 1}. As a consequence, the bottom layer generates a sequence of random observations using which the algorithm can recover the network weights. The algorithm has polynomial-time complexity and is capable of learning random networks that are drawn from a specific distribution. However, in practice people want to learn deterministic networks that encode data-dependent representations.\nSedghi and Anandkumar [19] study the supervised learning of neural networks under the assumption that the data distribution has a score function that is known in advance. They show that if the input dimension is large enough and the network is sparse enough, then the first network layer can be learned by a polynomial-time algorithm. Learning the deeper layers remains as an open problem. In addition, their method assumes that the network weights are randomly drawn from a Bernoulli-Gaussian distribution. More recently, Janzamin et al. [12] propose another algorithm based on the score function that removes the restrictions of Sedghi and Anandkumar [19]. The assumption in this case is that the network weights satisfy a non-degeneracy condition; moreover, the algorithm is only capable of learning neural networks with one hidden layer.\nAnother approach to the problem is via the improper learning framework. The goal in this case is to find a predictor that is not a neural network, but performs as well as the best possible neural network in terms of the generalization error. Livni et al. [18] consider changing the activation function and over-specifying the network to make it easier to train. They show that polynomial networks (e.g., networks whose activation function is quadratic) with sufficient width and depth are as expressive as the sigmoid-activated neural networks. Although a deep polynomial network is still hard to train, they propose training in a superclass—the class of all polynomial functions with bounded degree. As a consequence, there is an improper learning algorithm which achieves a generalization error at most ǫ worse than that of the best neural network. The time complexity is polynomial in the input dimension d and quasi-polynomial in 1/ǫ. Since the dependence on d has a large power, the algorithm is not practical unless d is quite small. Livni et al. [18] further show, however, that there is a practical algorithm to directly train the polynomial network if it has one or two hidden layers.\nA recent line of work has focused on understanding the energy landscape of a neural network. After several simplifying assumptions, a neural network can be shown to be a Gaussian field whose critical points can be analyzed using the Kac-Rice formula and properties of the Gaussian Orthogonal Ensemble [3, 10, 8]. The conclusion of these papers is that all critical points with nonnegative eigenvalues tend to have objective value near the global minimum. Thus in such networks if we could find such a point, it would have small objective value and thus small training error. This combined with generalization error bounds would imply finding a neural network with low excess risk. However, there is no provably efficient algorithm for finding a critical point with nonnegative eigenvalues."
    }, {
      "heading" : "1.1 Our contribution",
      "text" : "In this paper, we propose a practical algorithm called the recursive kernel method for learning multi-layer neural networks, under the framework of improper learning. Our method is inspired by the work of Shalev-Shwartz et al. [20], which shows that for binary classification with the sigmoidal loss, there is a kernel-based method that achieves the same generalization error as the best linear classifier. We extend this method to deeper networks. In particular, we assume that the neural network to be learned takes d-dimensional input. It has k hidden layers and the ℓ1norm of the incoming weights of any neuron is bounded by L. Under these assumptions, the\nalgorithm learns a kernel-based predictor whose generalization error is at most ǫ worse than that of the best neural network. The sample and the time complexity of the algorithm are polynomial in (d, 1/ǫ, log(1/δ), F (k, L)), where F (k, L) is a function depending on (k, L) and on the activation function, independent of the input dimension or the number of neurons. The theoretical result holds for any data distribution.\nAs concrete examples, we demonstrate that if the activation function is a quadratic function, then F (k, L) is a polynomial function of L. Thus, the algorithm recovers the theoretical guarantee of Livni et al. [18]. We also demonstrate two activation functions, one that approximates the sigmoid function and the other that approximates the ReLU function, under which F (k, L) is finite. Thus, the algorithm also learns neural networks activated by sigmoid-like or ReLU-like functions. For these latter examples, the dependence on L is no longer polynomial. This nonpolynomial dependence is in fact inevitable: Under a hardness assumption in cryptographics and assuming sigmoid-like or ReLU-like activation, we prove that no algorithm running in poly(L) time can improperly learn the neural network.\nThe paper is organized as follow. In Section 2, we formalize the problem and clarify the assumptions that we make for the theoretical analysis. In Section 3, the algorithm and the associated theoretical results are presented. We discuss concrete examples to demonstrate the application of the theory. In Section 4, we present hardness results for the improper learning of neural networks. In Section 5, we report experiments on the MNIST dataset and its variations, demonstrating that in addition to its role in our theoretical analysis the proposed algorithm is comparable in practice with baseline neural network learning methods."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "We consider a fully-connected neural network N that maps a vector x ∈ Rd to a real number N (x) via k hidden layers. Let d(p) represent the number of neurons in the p-th layer. Let y\n(p) i represent\nthe output of the i-th neuron in the p-th layer. We define the zero-th layer to be the input vector so that d(0) = d and y(0) = x. The transformation performed by the neural network is defined as follows:\ny (p) i := σ\n( d(p−1)∑\nj=1\nw (p−1) i,j y (p−1) j\n) and N (x) := d(k)∑\nj=1\nw (k) 1,j y (k) j ,\nwhere w (p−1) i,j is the weight of the edge that connects the neuron j on the (p − 1)-th layer to the neuron i on the p-th layer. The activation function σ : R → R is a one-dimensional nonlinear function. We will discuss the choice of function σ later in this section.\nWe assume that the input vector has bounded ℓ2-norm and the edge weights have bounded ℓ1 or ℓ2 norms. The assumptions are formalized as follows.\nAssumption A. The input vector x satisfies ‖x‖2 ≤ 1. The neuron edge weights satisfy d∑\nj=1\n(w (0) i,j ) 2 ≤ L2 for all i ∈ {1, . . . , d}.\nd(p)∑\nj=1\n|w(p)i,j | ≤ L for all (p, i) ∈ {1, . . . , k} × {1, . . . , d(p+1)}.\nLet Nk,L,σ be the set of k-layer neural networks with activation function σ that satisfy the edge weight constraints.\nAssumption A implies that for all neurons on the first hidden layer, the ℓ2-norm of their incoming weights is bounded by L. For other neurons, the ℓ1-norm of their incoming weights is bounded by L. The ℓ1-regularization imposes sparsity on the neural network. It is observed in practice that sparse neural networks are capable of learning meaningful representations. For example, the convolution neural network has sparse edges. It has been argued that sparse connectivity is a natural constraint which can lead to improved performance in practice [21].\nIn a prediction task, there is a convex function ℓ : R × R → R that measures the loss of the prediction. For a feature-label pair (x, y) ∈ X × R, its prediction loss is measured by ℓ(N (x), y). We assume that (x, y) is sampled from an underlying distribution D. The prediction risk of the neural network is defined by E[ℓ(N (x), y)]. Our goal is to learn a predictor f : X → R, which is not necessarily a neural network, such that\nE[ℓ(f(x), y)] ≤ arg min N∈Nk,L,σ E[ℓ(N (x), y)] + ǫ. (1)\nIn other words, we want to learn a predictor whose generalization loss is at most ǫ worse than that of the best neural network in Nk,L,σ.\nIn practice, both the sigmoid function σ(x) = (1 + e−βx)−1 and the ReLU function σ(x) = max(0, x) are widely used as activation functions for neural networks. We define two classes of activation functions that includes the sigmoid and ReLU respectively.\nDefinition 1 (sigmoid-like activation). A function σ is called sigmoid-like if it is non-decreasing on (−∞,+∞) and\nlim x→−∞ xcσ(x) = 0 and lim x→∞ xc(1− σ(x)) = 0\nfor some positive constant c.\nDefinition 2 (ReLU-like activation). A function σ is called ReLU-like if σ(x)−σ(x−1) a sigmoidlike function.\nIntuitively, a sigmoid-like function is a non-decreasing function on [0, 1]. When x → −∞ or x → ∞, the function value approaches 0 or 1 at a polynomial rate (or faster) in x. A ReLU-like function is a convex function on [0,∞). When x → ∞, it approaches a linear function with unit slope."
    }, {
      "heading" : "3 Algorithm and Theoretical Result",
      "text" : "In this section, we present a kernel method which learns a predictor performing as well as the neural network. We begin by recursively defining a sequence of kernels. Let K : RN × RN → R be a function defined by\nK(x, y) := 1\n2− 〈x, y〉 ,\nwhere both ‖x‖2 and ‖y‖2 are assumed to be bounded by one. The function K is a kernel function because we can find a mapping ψ : RN → RN such that K(x, y) = 〈ψ(x), ψ(y)〉. The function ψ maps an infinite-dimensional vector to an infinite-dimensional vector. We use xi to represent the\nAlgorithm 1: Recursive Kernel Method for Learning Neural Network\nInput: Feature-label pairs {(xi, yi)}ni=1; Loss function ℓ : R× R → R; Number of hidden layers k; Regularization coefficient B. Solve the following convex optimization problem:\nα̂ = arg min α∈Rn\n1\nn\nn∑\nj=1\nℓ\n( n∑\ni=1\nαiK (k)(xi, xj), yi ) s.t. n∑\ni,j=1\nαiαjK (k)(xi, xj) ≤ B2\nwhere K(k) is defined in Eq. (4). Output: Predictor f̂n(x) = ∑n i=1 α̂iK (k)(xi, x).\ni-th coordinate of an infinite-dimensional vector x. The (k1, . . . , kj)-th coordinate of ψ(x), where j ∈ N and k1, . . . , kj ∈ N, is defined as 2− j+1 2 xk1 . . . xkj . By this definition, we have\n〈ψ(x), ψ(y)〉 = ∞∑\nj=0\n2−(j+1) ∑\n(k1,...,kj)∈Nj\nxk1 . . . xkjyk1 . . . ykj . (2)\nThe inner term on the right-hand side of Eq. (2) can be simplified to\n∑\n(k1,...,kj)∈Nj\nxk1 . . . xkjyk1 . . . ykj = (〈x, y〉)j . (3)\nCombining Eqs. (2) and (3) and using the fact that 〈x, y〉 ≤ 1, we have\n〈ψ(x), ψ(y)〉 = ∞∑\nj=0\n2−(j+1)(〈x, y〉)j = 1 2− 〈x, y〉 = K(x, y),\nwhich verifies that K is a kernel function and ψ is the associated mapping. Since ψ maps from RN to RN and ‖x‖2 ≤ 1 implies ‖ψ(x)‖2 = K(ψ(x), ψ(x)) ≤ 1, we can recursively define a sequence of mappings\nψ(0)(x) = x and ψ(p)(x) = ψ(ψ(p−1)(x)).\nUsing the relation between K and ψ, it is easy to verify that the associated kernels are\nK(0)(x, y) = 〈x, y〉 and K(p)(x, y) = 1 2−K(p−1)(x, y) , (4)\nwhich satisfy 〈ψ(p)(x), ψ(p)(y)〉 = K(p)(x, y). Thus, the kernel function K(k)(x, y) can be easily computed from the inner product of x and y."
    }, {
      "heading" : "3.1 Algorithm",
      "text" : "We are now ready to specify the algorithm to learn the neural network. Suppose that the neural network has k hidden layers. Let Fk represent the Reproducing Kernel Hilbert Space (RKHS)\ninduced by the kernel K(k) and let Fk,B ⊂ Fk be the set of RKHS elements whose norm are bounded by B. Given training examples {(xi, yi)}ni=1, define the predictor\nf̂n := arg min f∈Fk,B\n1\nn\nn∑\ni=1\nℓ(f(xi), yi).\nAccording to the representer theorem, we can represent f̂n by\nf̂n(x) =\nn∑\ni=1\nαiK (k)(xi, x) where\nn∑\ni,j=1\nαiαjK (k)(xi, xj) ≤ B2, (5)\nComputing the vector α is a convex optimization problem in Rn and therefore can be solved in time poly(n, d) using standard optimization tools. We call this algorithm the recursive kernel method and summarize it in Algorithm 1. It is an improper learning algorithm since the learned predictor f̂n cannot be represented by a neural network."
    }, {
      "heading" : "3.2 Main Result",
      "text" : "Applying classical results from learning theory, we can upper bound the Rademacher complexity of Fk,B by √ 2B2/n (see, e.g., [13]). Thus, with probability at least 1− δ, we can upper bound the generalization loss of predictor f̂n(x) by\nE[ℓ(f̂n(x), y)] ≤ arg min f∈Fk,B E[ℓ(f(x), y)] + ǫ,\nwhen the sample size n = Ω(B2 log(1/δ)/ǫ2). See [20, Theorem 2.2] for the proof of this claim. In order to establish the bound (1), it suffices to show that Nk,L,σ ⊂ Fk,B where B is a constant that only depends on k and L. The following lemma establishes the claim. See Appendix A for the proof. Lemma 1. Assume that the function σ(x) has a polynomial expansion σ(x) = ∑∞\nj=0 βjx j. Let\nH(λ) := L · √∑∞\nj=0 2 j+1β2j λ 2j and define H(k)(x) be the degree-k composition of function H, then\nNk,L,σ ⊂ Fk,H(k)(L).\nUsing Lemma 1 and the above analyses, we obtain the main result of this paper.\nTheorem 1. Let Assumption A be true and define F (k, L) := H(k)(L) where H(k)(L) is specified in Lemma 1. If F (k, L) is finite, then with probability at least 1 − δ, the predictor defined in Algorithm 1 achieves\nE[ℓ(f̂n(x), y)] ≤ arg min N∈Nk,L,σ E[ℓ(N (x), y)] + ǫ.\nThe sample complexity is bounded by poly(1/ǫ, log(1/δ), F (k, L)); the time complexity is bounded by poly(d, 1/ǫ, log(1/δ), F (k, L))."
    }, {
      "heading" : "3.3 Examples",
      "text" : "We study several concrete examples where F (k, L) is finite. Our first example is the quadratic activation function:\nσsq(x) = x 2.\nThis activation function has been studied by Livni et al. [18], who refer to a neural network activated by this function as a polynomial network. In Theorem 1, if the quadratic activation function is employed, we have H(λ) = 2Lλ2. As a consequence, we have F (1, L) = 2L2 and more generally F (k, L) ≤ (2L)2k+1−1 by induction. Thus, the sample and the time complexity of Algorithm 1 is a polynomial function of (d, 1/ǫ, log(1/δ), L) for any constant k.\nNext, we study sigmoid-like or ReLU-like activation functions. We consider a shifted erf function defined as:\nσerf(x) = 1\n2 (1 + erf(\n√ πx)),\nand a smoothed hinge loss function defined as:\nσsh(x) =\n∫ x\n−∞\nσerf(t)dt = σerf(x) · x+ e−πx\n2\n2π .\nIn Figure 1, we compare σerf and σsh with the sigmoid function and the ReLU function. It is seen that σerf is similar to the sigmoid function and σsh is a smoothed version of ReLU. It is also easy to verify that σerf is sigmoid-like and σsh is ReLU-like. The following proposition shows that if either σerf or σsh is used as the activation function, the quantity F (k, L) is finite. See Appendix B for the proof.\nProposition 1. For the σerf function, we have\nH(λ) ≤ L · √ 1\n2 + 4λ2(1 + 3eπλ2e4πλ2) for any λ ≥ 3.\nFor the σsh function, we have\nH(λ) ≤ L · √ λ2 + 8λ4(1 + 3eπλ2e4πλ2) for any λ ≥ 3.\nThus, Theorem 1 implies that the neural network activated by σerf or σsh is learnable in polynomial time given any constant (k, L).\nFinally, we demonstrate how the conditions of Assumption A could be modified. Consider a sigmoid-activated network with k hidden layers which satisfies the following:\nd(p)∑\nj=1\n|w(p)i,j | ≤ L for all (p, i) ∈ {1, . . . , k} × {0, . . . , d(p+1)}.\nThis means that the ℓ1-norm of all layers is bounded by L. In addition, we assume that the input vector satisfies ‖x‖∞ ≤ 1. This is in contrast to the condition ‖x‖2 ≤ 1 in Assumption A. It was shown by Livni et al. [18, Theorem 4] that this sigmoid network can be approximated by a polynomial network with arbitrarily small approximation error ǫ. The associated polynomial network has O(k log(Lk+L log(1/ǫ))) hidden layers, whose ℓ1-norms are bounded by eO(L log(1/ǫ)). If we normalize the input vector x ∈ Rd by x ← x/ √ d and multiple all first-layer weights by √ d, the output of the network remains invariant and it satisfies Assumption A. Thus, combining our result for the polynomial network and the above analysis, the sigmoid network can be learned in\npoly ( d(Lk+L log(1/ǫ)) O(k) , log(1/δ) )\nsample and time complexity. This is a quasi-polynomial dependence on 1/ǫ for any constant (k, L). Notice that the dimension d comes into the expression."
    }, {
      "heading" : "4 Hardness Result",
      "text" : "In Section 3.3, we see that the dependence of the time complexity on L is at least exponential for σerf and σsh, but it is polynomial for the quadratic activation. It is thus natural to wonder if there is a sigmoid-like or ReLU-like activation function that makes the time complexity a polynomial function of L. In this section, we prove that this is impossible given standard hardness assumptions.\nOur proof relies on the hardness of standard (nonagnostic) PAC learning of intersection of halfspaces given in Klivans and Sherstov [14]. More precisely, let\nH = {x → sign(wTx− b− 1/2) : x ∈ {−1, 1}d, b ∈ N, w ∈ Nd, |b|+ ‖w‖1 ≤ poly(d)}\nbe the family of halfspace indicator functions mapping X = {−1, 1}d to {−1, 1}, and let HT be the set of functions taking the form:\nh(x) = { 1 if h1(x) = · · · = hT (x) = 1, −1 otherwise. where h1, . . . , hT ∈ H.\nThus, HT is the set of functions that indicates the intersection of T halfspaces. For any distribution on X , an algorithm A takes a sequence of (x, h∗(x)) as input where x is a sample from X and h∗ ∈ HT . The algorithm learns a function ĥ such that with probability at least 1− δ, one has\nP (ĥ(x) 6= h∗(x)) ≤ ǫ. (6)\nIf there is such an algorithm A whose sample complexity and time complexity scale as poly(d), then we say that HT is efficiently learnable. Klivans and Sherstov [14] show that HT is not efficiently learnable under a certain cryptographic assumption.\nTheorem 2 (Klivans and Sherstov [14]). If T = dρ for some constant ρ > 0, then under a certain cryptographic assumption, HT is not efficiently learnable.\nWe use this hardness result to prove the hardness of learning neural networks. In particular, we construct a neural network N such that if there is a learning algorithm computing a predictor f̂ such that E[ℓ(f̂(x), y)] ≤ E[ℓ(N (x), y)] + ǫ, then the error bound (6) is satisfied. Thus, the hardness of learning intersection of halfspaces implies the hardness of learning neural networks. See Appendix C for the proof.\nTheorem 3. Assume the cryptographic assumption of Theorem 2. Let σ be a sigmoid-like or ReLU-like function and let ℓ(f(x), y) = max(0, 1 − yf(x)) be the hinge loss. For fixed (δ, ǫ), there is no algorithm running in poly(L) time that learns a predictor f̂ satisfying\nE[ℓ(f̂(x), y)] ≤ arg min N∈N1,L,σ E[ℓ(N (x), y)] + ǫ with probability at least 1− δ. (7)\nThe hardness of learning sigmoid-activated and ReLU-activated neural networks has been proved by Livni et al. [18] when ℓ is the zero-one loss. Theorem 3 presents a more general result, showing that any activation function that is sigmoid-like or ReLU-like leads to the computational hardness, even if the loss function ℓ is convex."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we compare the proposed algorithm with several baseline algorithms on the MNIST digit recognition task. Since the basic MNIST digits are relatively easy to classify, we introduce three variations which make the problem more challenging.\nDatasets We use the MNIST handwritten digits dataset and three variations of it. See Figure 2 for the description of these datasets and several exemplary images. All the images are of size 28× 28. For all datasets, we use 10,000 images for training, 2,000 images for validation and 50,000 images for testing. This partitioning is recommended by the source of the data [1].\nAlgorithms For the recursive kernel method, we train one-vs-all SVM classifiers with Algorithm 1. The hyper-parameters are given by k ∈ {1, 4} and B = 100. All images are pre-processed by the following steps: deskewing, centering and normalization. The deskewing step computes the principal axis of the shape that is closest to the vertical, and shifts the lines so as to make it vertical. It is a common preprocessing step for the kernel method [17]. The centering and normalization steps center the feature vector and scale it to have the unit ℓ2-norm.\nWe compare with the following baseline models: multi-class logistic regression, multi-layer perceptron and convolution neural networks. The multi-layer perceptron is a fully connected neural network with a single hidden layer which contains 500 hidden neurons. It covers the networks that can be learned by the method of Janzamin et al. [12]. The convolution neural networks implement the LeNet5 architecture [17]. All baseline models are trained via stochastic gradient descent.\nResults The classification error rates are summarized in Table 1. As the table shows, the recursive kernel method is consistently more accurate than logistic regression and the multi-layer perceptron. On the Basic and the Rotation datasets, the proposed algorithm is comparable with LeNet5. On the other two datasets, LeNet5 wins over other methods by a relatively large margin. It is worth noting that when we choose a greater k, the performance of the proposed algorithm gets better. Recall that a greater k learns a deeper neural network, thus the empirical observation is intuitive.\nAlthough the recursive kernel method doesn’t outperform the LeNet5 model, the experiment demonstrates that it does learn better predictors than fully connected neural networks such as the multi-layer perceptron. The LeNet5 architecture encodes prior knowledge about digit recogniition via the convolution and pooling operations; thus its performance is better than the generic architectures."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we have presented an algorithm and a theoretical analysis for the improper learning of multi-layer neural networks. The proposed method, which is based on a recursively defined kernel, is guaranteed to learn the neural network if it has a constant depth and a constant ℓ1-norm. We also present hardness results showing that the time complexity cannot be polynomial in the ℓ1-norm bound. We compare the algorithm with several baseline methods on the MNIST dataset and its variations. The algorithm learns better predictors than the full-connected multi-layer perceptron but is outperformed by LeNet5. We view this line of work as a contribution to the ongoing effort to develop learning algorithms for neural networks that are both understandable in theory and useful in practice."
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "Consider an arbitrary neural network N ∈ Nk,L,σ. Let g(p)i := ∑d(p) j=1w (p) ji y (p) j represent the input of the neuron i at layer p+1. Note that g (p) i is a function of the input vector x. By this definition, it suffices to show that g (k) 1 ∈ Fk,H(k)(L).\nWe claim that g (p) i ∈ Fp,H(p)(L) for any p ∈ {0, 1, . . . , k} and prove the claim by induction. For\np = 0, we have\ng (0) i (x) =\nd∑\nj=1\nw (0) i,j xj = 〈w (0) i , ψ (0)(x)〉.\nThus, g (0) i belongs to the RKHS induced by the kernel K (0). Furthermore, we have ‖g(0)i ‖F0 = ‖w(0)i ‖2 ≤ L = H(0)(L), which implies g (0) i ∈ F0,H(0)(L).\nFor p > 0, we assume that the claim holds for p − 1 and we will prove it for p. The definition of g\n(p) i implies\ng (p) i (x) =\nd(p)∑\nj=1\nw (p) ji σ ( g (p−1) j (x) ) .\nUsing the inductive hypothesis, we have g (p−1) j ∈ Fp−1,H(p−1)(L), which implies that g (p−1) j (x) = 〈vj , ψ(p−1)(x)〉 for some vj ∈ RN, and ‖vj‖2 ≤ H(p−1)(L). This implies\ng (p) i (x) =\nd(p)∑\nj=1\nw (p) i,j σ(〈vj , ψ(p−1)(x)〉). (8)\nLet x(p−1) be a shorthand notation of ψ(p−1)(x). We define vector uj ∈ RN as follow: the (k1, . . . , kt)th coordinate of uj , where t ∈ N and k1, . . . , kt ∈ N+, is equal to 2 t+1 2 βtvj,k1 . . . vj,kt. By this definition, we have\nσ(〈vj , x(p−1)〉) = ∞∑\nt=0\nβt(〈vj , x(p−1)〉)t\n= ∞∑\nt=0\nβt ∑\n(k1,...,kt)∈Nt\nvj,k1 . . . vj,ktx (p−1) k1 . . . x (p−1) kt\n= 〈uj , ψ(x(p−1))〉, (9)\nwhere the first equation holds since σ(x) has a polynomial expansion σ(x) = ∑∞\nt=0 βtx t, the second\nby expanding the inner product, and the third by definition of ψ(x) . Combining Eq. (8) and Eq. (9), we have\ng (p) i (x) =\nd(p)∑\nj=1\nw (p) i,j 〈uj , ψ(ψ(p−1)(x))〉 =\n〈 d(p)∑\nj=1\nw (p) ji uj, ψ\n(p)(x) 〉 .\nThis implies that g (p) i belongs to the RKHS induced by the kernel K (p).\nFinally, we upper bound the norm of g (p) i . Notice that\n‖g(p)i ‖Fp = ∥∥∥ d(p)∑\nj=1\nw (p) i,j uj ∥∥∥ 2 ≤ d(p)∑\ni=1\n|w(p)i,j | · ‖uj‖2 ≤ L · max j∈[d(p)] {‖uj‖2}. (10)\nUsing the definition of uj and the inductive hypothesis, we have\n‖uj‖22 = ∞∑\nt=0\n2t+1β2t ∑\n(k1,...,kt)∈Nt\nv2j,k1v 2 j,k2 · · · v2j,kt\n=\n∞∑\nt=0\n2t+1β2t ‖vj‖2t2 ≤ ∞∑\nt=0\n2t+1β2t (H (p−1)(L))2t. (11)\nCombining inequality (10) and (11), we have ‖g(p)i ‖Fp ≤ H(p)(L), which verifies that g (p) i ∈ Fp,H(p)(L)."
    }, {
      "heading" : "B Proof of Proposition 1",
      "text" : "For the σerf function, the polynomial expansion is\nσerf(x) = 1\n2 + 1√ π\n∞∑\nj=0\n(−1)j(√πx)2j+1 j!(2j + 1) .\nTherefore, we have\nH(λ) = L · √√√√1 2 + 2 π ∞∑\nj=0\n(2πλ2)2j+1\n(j!)2(2j + 1)2 . (12)\nShalev-Shwartz et al. [20, Corollary C] provide an upper bound on the right-hand side of Eq. (12). In particular, they prove that\n2\nπ\n∞∑\nj=0\n(2πλ2)2j+1\n(j!)2(2j + 1)2 ≤ 4λ2(1 + 3eπλ2e4πλ2) for any λ ≥ 3. (13)\nPlugging this upper bound to Eq. (12) completes the proof.\nFor the σsh function, since it is the integral of the σerf function, its polynomial expansion is\nσsh(x) = x\n2 + 1√ π\n∞∑\nj=0\n(−1)j(√πx)2j+1x j!(2j + 1)(2j + 2) ,\nand consequently,\nH(λ) = L · √√√√λ2 + 2 π ∞∑\nj=0\n(2πλ2)2j+1(2λ2)\n(j!)2(2j + 1)2(2j + 2)2 . (14)\nWe upper bound the right-hand side of Eq. (14) by\n2\nπ\n∞∑\nj=0\n(2πλ2)2j+1(2λ2)\n(j!)2(2j + 1)2(2j + 2)2 ≤ 4λ\n2\nπ\n∞∑\nj=0\n(2πλ2)2j+1\n(j!)2(2j + 1)2\n≤ 8λ4(1 + 3eπλ2e4πλ2) for any λ ≥ 3,\nwhere the final inequality holds because of Eq. (13). Plugging this upper bound into Eq. (14) completes the proof."
    }, {
      "heading" : "C Proof of Theorem 3",
      "text" : "We construct a one-hidden-layer neural network that encodes the intersection of T halfspaces. Suppose that the t-th halfspace is characterized by gt(x) = w T t x− bt− 1/2. Since both x, wt and bt are composed of integers, we have gt(x) ≥ 1/2 when ht(x) = 1, and gt(x) ≤ −1/2 when ht(x) = −1. We extend x to be (x, 1), then extend wt to be (wt, bt), and define\ng̃t(x) = 〈w̃t, x̃〉 where x̃ := 1√ d+ 1\n(x, 1) and w̃ := 2λ √ d+ 1(wt, bt),\nwhere λ is a scalar to be specified. According to this definition, we have ‖x̃‖2 = 1 and ‖w̃‖2 = poly(d). In addition, we have g̃t(x) ≥ λ when ht(x) = 1, and g̃t(x) ≤ −λ when ht(x) = −1.\nSigmoid-like Activation If σ a is sigmoid-like function, there is a constant c such that\nlim x→−∞ xcσ(x) = lim x→∞ xc(1− σ(x)) = 0.\nThus, there is a sufficiently large constant C such that σ(x) ≤ x−c for all x ≤ −C and σ(x) ≥ 1−x−c for all x ≥ C. Note that the number T of intersecting halfspaces is a polynomial function of dimension d. As a consequence, there is a sufficiently large constant λ ∼ poly(d) such that\nσ(x) ≥ 1− 1 4T for all x > λ and σ(x) ≤ 1 4T for all x ≤ −λ.\nThus, we have σ(g̃t(x)) ≥ 1− 14T if ht(x) = 1 and σ(g̃t(x)) ≤ 14T if ht(x) = −1. We define the neural network N to be\nN (x) = T∑\nt=1\n4 σ(g̃t(x))− (4T − 2). (15)\nIt is easy to verify that N ∈ N1,L,σ for some L ∼ poly(d). If h∗(x) = 1, then x belongs to the intersection of halfspaces. It implies that σ(g̃t(x)) ≥ 1 − 14T for all t ∈ [T ]. Combining with Eq. (15), we obtain N (x) ≥ 1. On the other hand, if h∗(x) = −1, then there is some t such that σ(g̃t(x)) ≤ 14T . Thus, Eq. (15) implies N (x) ≤ −1. In summary, we have h∗(x)N (x) ≥ 1 for any x ∈ X . As a consequence, we have ℓ(N (x), h∗(x)) ≡ 0 where ℓ is the hinge loss.\nAssume that there is a predictor f̂ satisfying the error bound (7). Let ĥ(x) = sign(f̂(x)) be a classifier that judges the intersection of hyperplanes. Since the hinge loss is an upper bound on the zero-one loss, we have\nP (ĥ(x) 6= h∗(x)) = E[I(ĥ(x) 6= h∗(x))] = E[I(sign(f̂(x)) 6= h∗(x))] ≤ E[ℓ(f̂(x), h∗(x))] ≤ E[ℓ(N (x), h∗(x))] + ǫ = ǫ,\nwhere the final inequality follows from inequality (7). The last equation holds since ℓ(N (x), h∗(x)) ≡ 0. This implies that the associated classifier ĥ satisfies the error bound (6). Since ĥ cannot be computed in poly(d) time, we conclude that f̂ cannot be computed in poly(L) time.\nReLU-like Activation If σ is a ReLU-like function, then by definition, we have σ′(x) := σ(x)− σ(x − 1) is a sigmoid-like function. Following the argument for the sigmoid-like activation, if we treat σ′ as the activation function, then the remaining part of the proof will go through without any further modification. This completes the proof for the ReLU-like activation."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "<lb>We study the improper learning of multi-layer neural networks. Suppose that the neural network<lb>to be learned has k hidden layers and that the l1-norm of the incoming weights of any neuron is<lb>bounded by L. We present a kernel-based method, such that with probability at least 1− δ, it<lb>learns a predictor whose generalization error is at most ǫ worse than that of the neural network.<lb>The sample complexity and the time complexity of the presented method are polynomial in the<lb>input dimension and in (1/ǫ, log(1/δ), F (k, L)), where F (k, L) is a function depending on (k, L)<lb>and on the activation function, independent of the number of neurons. The algorithm applies to<lb>both sigmoid-like activation functions and ReLU-like activation functions. It implies that any<lb>sufficiently sparse neural network is learnable in polynomial time.",
    "creator" : "LaTeX with hyperref package"
  }
}