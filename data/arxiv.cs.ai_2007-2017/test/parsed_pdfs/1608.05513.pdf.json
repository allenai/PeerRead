{
  "name" : "1608.05513.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Data Centroid Based Multi-Level Fuzzy Min-Max Neural Network",
    "authors" : [ "Shraddha Deshmukh", "Sagar Gandhi", "Pratap Sanap", "Vivek Kulkarni" ],
    "emails" : [ "shraddhadeshmukh2@gmail.com;", "gandhi@persistent.co.in;", "sanap@persistent.com;", "kulkarni@persistent.com)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Data Centroid Based Multi-Level Fuzzy Min-Max Neural Network\nShraddha Deshmukh*, Sagar Gandhi*, Pratap Sanap and Vivek Kulkarni\nAbstract—Recently, a multi-level fuzzy min max neural network (MLF) was proposed, which improves the classification accuracy by handling an overlapped region (area of confusion) with the help of a tree structure. In this brief, an extension of MLF is proposed which defines a new boundary region, where the previously proposed methods mark decisions with less confidence and hence misclassification is more frequent. A methodology to classify patterns more accurately is presented. Our work enhances the testing procedure by means of data centroids. We exhibit an illustrative example, clearly highlighting the advantage of our approach. Results on standard datasets are also presented to evidentially prove a consistent improvement in the classification rate.\nIndex Terms—Hyperbox, Fuzzy min-max, Data Centroids, Neural Networks, Neurofuzzy, Classification, Machine Learning.\nI. INTRODUCTION\nFUZZY set theory was proposed with the intendeduse to the fields of pattern classification and information processing [1]. Indeed, it has attracted many researchers, and their applications to real-life problems are of a great significance. Simpson [2] presented the fuzzy min max neural network (FMM), which makes the soft decisions to organize hyperboxes by its degree of belongingness to a particular class, which is known as a membership function. Hyperbox is a convex box, completely represented by min and max points. FMM classification results are completely characterized with the help of a membership function. Along with this elegant proposal, [2] also presented the characteristics for a good classifier, among which, nonlinear separability, overlapping classes and tuning parameters have proved to be of a great interest to a research community.\nSimpson also presented a clustering approach using FMM in [3]. But many problems in real-life require\n* : These authors contributed equally. The authors are with Persistent Systems Ltd., India (e-mail Ids: shraddhadeshmukh2@gmail.com; sagar gandhi@persistent.co.in; pratap sanap@persistent.com; vivek kulkarni@persistent.com)\nPersistent System Ltd. (www.persistent.com) is a publicly listed company. Persistent LABS is the R&D group of Persistent Systems Ltd., which focuses on R&D activities involving Machine Learning, Video Analytics, Internet of Things (IoT) in addition to many other areas.\nPersistent LABS is Recognized and Approved R&D Center by Department of Scientific and Industrial Research (DSIR), Govt. of India.\nboth classification and clustering. To address this issue, GFMM [4] brought this generality. Besides generality, the more significant contribution has proved to be modification to the membership function. The presented membership function computes the belongingness to the hyperbox so that the membership value decreases uniformly as we move away from the hyperbox. Another weakness of FMM was the patterns belonging to overlapped region, where the rate of misclassification is considerably high. The tuning parameter, theta (θ), which controls the size of a hyperbox, has a great impact on this overlapped region. Smaller theta values produce less overlaps producing high training accuracy, but the efficacy of the network gets compromised, and for larger theta values, accuracy gets decreased.\nMultiple approaches were presented to tackle this problem. Earlier, the process of contraction [1][4] was employed, which used to eliminate all the overlapping regions. This method had the intrinsic problem of representing patterns not belonging to any of the hyperbox, in turn lessening the accuracy. Exclusion/Inclusion Fuzzy Classification (HEFC) network was introduced in [5], which further reduced the number of hyperboxes and increased the accuracy. Inclusion hyperboxes were used to represent patterns belonging to the same class, while exclusion hyperboxes were used to denote the overlapped region, treated as if it is a hyperbox. This notion is used as it is in almost all the newly introduced models [6][7][8][9]. Fuzzy Min-Max Neural Network Classifier with Compensatory Neurons (FMCN) was acquainted in [7]. Authors categorized the overlap into three parts, namely, full containment, partial overlap and no overlap, and then a new membership function to accommodate belongingness based on the compensation value. Authors also analyzed that neatly taking care of overlapped region automatically brings the insensitivity to the hyperbox size parameter, θ.\nData Core based Fuzzy Min-Max Neural Network (DCFMN) [8] further improved upon FMCN. Authors eliminated the need of overlap categorization. They also suggest a new membership function based on noise, geometric center and data cores of the hyperbox. Wherein DCFMN improved the accuracy in few cases, there are some serious drawbacks.\nar X\niv :1\n60 8.\n05 51\n3v 1\n[ cs\n.A I]\n1 9\nA ug\n2 01\n6\n2 1) DCFMN introduces two new user controlled variables, and λ. is used to suppress the influence of the noise and λ is used to control the descending speed of the membership function. These two variables greatly impact the performance of the model and naturally, defining their values is a tedious job. 2) There exists an underlying assumption that noise within all the hyperboxes is similar, which may not be true. Moreover, the sequence of the training exemplars plays a role as well. 3) MLF conveys that this membership function is not always preferred, in that, it does not work well for high percentage of samples belonging to overlapped area.\nMulti-level fuzzy min max neural network (MLF) [9] addresses the problem of overlapped region with an elegant approach. It uses separate levels for overlapping regions, and monotonically decreases the hyperbox size (θ). For most cases, MLF produces 100% training accuracy.\nThough MLF achieves a significant milestone, entertaining testing accuracy is rather more important than training accuracy, as it greatly sways the usage of the algorithm in practical scenarios. In this brief, we identify and define a new boundary region, where misclassification rate is substantial. To the best of our knowledge, this kind of approach is presented for the first time, at least we did not come across any similar published work. Hence we propose a method, based on data centroids, to evidentially prove that handling this newly introduced area of confusion between hyperboxes of different classes significantly increases the testing accuracy.\nThe paper is organized as follows. MLF is reviewed in Section II. We introduced D-MLF algorithm in Section III. An illustrative example and comparative results of DMLF with MLF model are presented in Section IV and V, respectively. Finally, conclusion is given in Section VI."
    }, {
      "heading" : "II. MULTI-LEVEL FUZZY MIN-MAX NEURAL NETWORK",
      "text" : "Multi-level fuzzy min max neural network (MLF) is a classifier which efficiently caters misclassification of patterns belonging to overlapped region by maintaining a tree structure, which is a homogeneous tree [9].\nIn MLF training phase, exemplars are continuously recurred to form the hyperboxes and overlaps, each recursion resulting in one level. This recursive procedure is carried till the predefined maximum depth or till overlap exists. Hyperbox expansion, based on hyperbox size controlling parameter (θ), is validated using equation (1) and expansion is carried out by equation (2).\n∀i = 1...D(max(wib, ai)−min(vib, ai) ≤ θ (1)\nvib ← min(vib, ai); 1 ≤ i ≤ D wib ← min(wib, ai); 1 ≤ i ≤ D\n(2)\nWhere, vib and w i b are min point and max point of hyperbox B respectively, ai is the ith dimension of pattern A and D is the number of dimensions. Also, prior to each recursion, θ is updated using equation (3)\nθn = µθp (3)\nWhere, θn and θp thetas for next level and previous level, respectively and µ, being the value between 0 and 1, ensures that size of hyperbox in overlapped region is less than its previous level.\nIn the testing phase, overlap regions are first traversed recursively, to discover appropriate subnet to which a test pattern belongs to. Thence, in that level, a class of hyperbox having highest membership value with the hyperboxes in the discovered subnet, is selected as a predicted class.\nMLF is able to achieve higher accuracy rates than previous FMM methods. This is due to an elegant treatment to the boundary region a confusion area. But, after training, there exists a room for yet another boundary. The region where membership function generates very close by values, it becomes difficult to assign a class with high degree of assurance. As per our experiments, MLF, and all the previous classifiers, do not perform well in this area. Hence, a definition of this new region, and a methodology to solve it is proposed."
    }, {
      "heading" : "III. DATA CENTROID BASED MULTI-LEVEL FUZZY MIN-MAX NEURAL NETWORK",
      "text" : "Fig. 1. D-MLF Subnet Tree Structure\nIn this section, we give details about a newly proposed algorithm, specifically, we define a new boundary region generated due to trained network and propose a solution to correctly classify test patterns belonging to it.\nFigure 1 describes the D-MLF structure, each node in S net contains two segments, Hyperboxes Segment (HBS) and Overlapped Segment (OLS). HBS represents hyperboxes generated in that level, whereas OLS\n3 represents overlaps in that level. Along with hyperbox information, Data Centroid (DC)."
    }, {
      "heading" : "A. D-MLF Training Phase",
      "text" : "Similar to the MLF learning procedure, D-MLF maintains S nets using HBS and OLS structures. First, all the patterns are passed through, resulting in creation and expansion of hyperboxes using equations (1) and (2). Then each hyperbox is checked with the rest of hyperboxes to detect the overlap using equation (4).\n∀i = 1...D( min( w1b , w2b )−max( v1b , v2b ) > 0 ) (4)\nWhere w1b and w 2 b are the max points and v 1 b and v2b are the min points of the two hyperboxes, among which overlap is tested. Moreover, D-MLF adds a new step at the learning phase, known as data centroid (DC) computation, where DC of all input patterns belonging to each hyperbox is maintained in the HBS. DC is computed as follows:\nDCh = 1\nn n∑ i=1 Phi (5)\nWhere DCh is the data centroid of the hth hyperbox, n is number of patterns belonging to hth hyperbox and Phi is the ith pattern in hth hyperbox.\nIf there exists an overlap, patterns belonging to the overlapped region are again sent to training procedure, where HBS and OLS creation takes place for the next level. This process of recursion is followed afterward to train all the patterns. Due to computation of OLS\nand process of finding patterns belonging to OLS, DMLF and MLF are not single pass algorithms. In general, given the n overlaps in the first level, entire training data has to be traversed n times. Thereafter, in the subsequent stages, data belonging to overlapped region is traversed in order of magnitude of number of overlaps in that region. This is a novel finding, and contradictory to what MLF authors have mentioned [9].\nNote that, the patterns belonging to overlapped region are not part of the DC computation. This step makes sure that training patterns balloting for more than one class are omitted in the final decision making.\nAlgorithm 1 D-MLF Training Pseudo code Net = D-MLF-train(net, θ) {\n1: if (stop condition) then 2: for all (h as hyperbox Net) do 3: DCh = h.centroid / h.membercount 4: end for 5: return null 6: end if 7: for all (sample data) do 8: if ((sample l hyperboxes) q (∃ hyperbox h: (h + sample < θ)) then 9: h.centroid += sample;\n10: h.membercount += 1; 11: else 12: create new hyperbox h; 13: h.centroid = sample; 14: h.membercount = 1; 15: end if 16: end for 17: for all (i as an overlapped area) do 18: sdata = samples which inhabit in i region; 19: for all (s sdata) do 20: if (s hyperbox Hi) then 21: Hi.centroid -= s; 22: Hi.membercount -= 1; 23: end if 24: create an overlap-box as OLi and add to OLS 25: Sneti = D-MLF-train (sdata, µθ ); 26: link OLi to Sneti with link ei; 27: end for 28: end for }"
    }, {
      "heading" : "B. D-MLF Testing Phase",
      "text" : "The original MLF used a decision making based on the subnets decision. The selected subnet need not be a leaf node in the tree. We do not alter this model, rather enhance the process of how subnet marks the choice.\n4 Membership function mentioned in the equation (11) is used against overlapped boxes. After recursively traversing the OLS an appropriate subnet is discovered, to which test pattern belongs to. A membership function explained in the equation (6) is used, this time, to compute the membership with the hyperboxes within the selected subnet.\nbj(Ah) = min( min([1− f(aih − w j i , γi)]\n[1− f(vji − a i h, γi)]))\nf(x, γ) =  1 if xγ > 1\nx if 0 ≤ xγ ≤ 1 0 if xγ < 0\n(6)\nWhere bj represents belongingness of sample Ah with jth hyperbox. x is a difference between min and max point with sample Ah and γ is a tuning parameter to control fuzziness.\nWithin these membership values, hyperboxes with highest two values are selected to define a boundary. Medial region of these hyperboxes, controlled by ψ, is treated as a boundary region. ψ is a user controlled variable, mentioned in the percentage value. At this point, it is necessary to check if test pattern belongs to the boundary region. We define α1 and α2 as incident angles between test pattern and two hyperboxes, respectively. Inclusion value is evaluated as follows:\nInclusion V alue =\n{ 1, (α1 < 90) and (α2 < 90)\n0, Otherwise (7)\nFurther, Based on the inclusion value, output class is chosen. If pattern exists in the area outside of the defined boundary, we simply follow a path of MLF, and classify the pattern based on the maximum membership value, which is already computed. If the pattern belongs to the boundary region, Euclidean Distance [10] between test pattern and the data centroids of the selected hyperboxes is computed. Hence, centered on the inclusion value, the output of the network is denoted as either the class of maximum gs among all the hyperboxes, or as a minimum of the distances of the topmost two hyperboxes\nOutput =\n{ gis, if inclusive value = 0\nf is, if inclusive value = 1 (8)\nWhere gis is given by;\ngis =\n{ cis, if esos = 1\n0, Otherwise (9)\nWhere cis is the i th class membership for the test sample in s subnet, es is edge between subnet s and the corresponding overlap box that enables the subnet if\ntest sample is in this overlap box. And os is the output of OLS, which is given by equation (10)\nos = max 1 ≤ j ≤ m lj (10)\nWhere m is number of overlap boxes in OLS and lj is membership function of the jth overlap box for test sample Ah, given by equation (11)\nlj =\n{ 1, if Ah is inside OLj\n0, Otherwise (11)\nAnd f is is given by equation(12)\nf is = min(D 1 s , D 2 s) (12)\nWhere Dis is the Euclidean Distance computed amongst sample s and the data centroid of the topmost ith hyperbox using equation (13)\nEuclidean Distance = √√√√ n∑ i=1 (Xi − Yi)2 (13)\nAlgorithm 2 D-MLF Testing Pseudo code Out = D-MLF-test(net, sample) {\n1: for all (oli as OL box in OLS) do 2: if (sample is in oli) then 3: out = D-MLF-test (Sneti, sample); 4: return null; 5: end if 6: end for 7: MV = []; 8: for all (Bi as hyperbox in HBS) do 9: MV += membership (sample, Bi);\n10: end for 11: [h1, h2] = [max(MV), max(MV (MV < max(MV)))]\n12: if (Inclusion value == 1) then 13: d = EuDistance(sample, h1.DC, h2.DC); 14: out = min(d).class; 15: else 16: out = max (MV).class; 17: end if }\nIV. ILLUSTRATIVE EXAMPLE\nIn this illustration, we describe the effectiveness of the proposed model, clearly pointing out the identification and handling of the stated area of confusion.\nFigure 3 illustrates the 2-diamentional data space. We consider 14 data samples for training and 6 data samples for testing. Hyperbox size parameter (θ) is fixed at 0.3 and a boundary parameter (ψ) is fixed at 5%. Both\n5\nFig. 3. Illustrative example signifying advantage of D-MLF\nMLF and D-MLF create two hyperboxes at 0th layer. D-MLF also computes data centroids (DC) for each hyperbox, h1 and h2. Here, data centroids of h1 and h2 are DC1 = (0.19, 0.21) and DC2 = (0.70, 0.71), respectively. Patterns which do not belong to boundary region are classified correctly by MLF. But when it comes to boundary region, it fails to correctly classify the patterns. Whereas the proposed D-MLF works better in the boundary region as well, as its decision making is not completely based on the membership value, but it also considers data centroids.\nIt can be noted that the patterns in the above example are not uniformly spread out. Which is a very common scenario in real-world examples. It occurs because of the dominance of the parameters such as outliers, temporal nature of the variables, etc. Due to them, most of the times, the patterns within the overall data, and in case of Fuzzy Min Max hierarchy, within hyperboxes, will not be steadily spread across all the dimensions. As demonstrated above, our proposed method treats them elegantly, without many of the modifications to the state of the art."
    }, {
      "heading" : "V. EXPERIMENTAL RESULTS",
      "text" : "Performance of proposed method (D-MLF) is studied on the basis of the classification rate. Various experiments were carried out to test D-MLF on different standard datasets.\nStandard datasets such as Iris, Glass, Wine, Wisconsin Breast Cancer (WBC), Wisconsin Diagnostic Breast Cancer (WDBC) and Ionosphere were used. These datasets were obtained from the UCI repository of machine learning databases [11]. In these experiments, hyperbox size parameter (θ) was chosen as 0.2, 0.5 and 0.9. This was to perform the measurements across the spectrum. As we increase the size of the hyperbox, the number of overlaps increase, and so does the misclassification rate. We split the data evenly for training and testing. The average results are shown over 100\nexperiments. For each iteration, training and testing data is chosen randomly.\nTable 1 shows results, we compare our results to MLF method, as it has been already proven to perform better than the previously proposed FMM methods [9]."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "In this brief, we introduced a new boundary region and distance based MLF classification method to handle patterns belonging to that boundary region. A data centroid based method, D-MLF, minimizes significance of outliers and similar errors in decision making. It has been evidentially proven that the proposal outperforms all the previously proposed FMM methods. More importantly, we have proposed a model suited for data in the real world, extending the state of the art. D-MLF will help humongous application areas such as Security, Natural Language Processing, Biomedical Reasoning, etc."
    } ],
    "references" : [ {
      "title" : "Fuzzy sets",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Information and Control, vol. 8, no. 3, pp. 338-353",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Fuzzy min-max neural networks. Part 1. Classification",
      "author" : [ "P.K. Simpson" ],
      "venue" : "IEEE Trans. Neural Network,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1992
    }, {
      "title" : "Fuzzy Min-Max Neural Networks - Part 2: Clustering",
      "author" : [ "P.K. Simpson" ],
      "venue" : "IEEE Trans Fuzzy Systems",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1993
    }, {
      "title" : "General fuzzy min-max neural network for clustering and classification",
      "author" : [ "B. Gabrys", "A. Bargiela" ],
      "venue" : "IEEE Trans. Neural Networks, vol. 11, pp. 769783",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "An inclusion/exclusion fuzzy hyperbox classifier",
      "author" : [ "Bargiela", "W. Pedrycz", "M. Tanaka" ],
      "venue" : "Int. J. Knowl. Based Intell. Eng. Syst.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "A fuzzy min-max neural network classifier with compensatory neuron architecture",
      "author" : [ "A.V. Nandedkar", "P.K. Biswas" ],
      "venue" : "IEEE Trans. Neural Netw.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Mansourizade, “Multi-level fuzzy min-max neural network classifier",
      "author" : [ "R. Davtalab", "M.M.H. Dezfoulian" ],
      "venue" : "IEEE Trans. Neural Netw. Learn. Syst.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Results of an analysis and recognition of vowels by computer using zero-crossing data",
      "author" : [ "W. Bezdel", "H.J. Chandler" ],
      "venue" : "Proc. Inst. Elec. Eng.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1965
    }, {
      "title" : "Lichman., UCI machine learning repository",
      "author" : [ "M.K. Bache" ],
      "venue" : "School Inf. Comput. Sci., Univ. California,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "FUZZY set theory was proposed with the intended use to the fields of pattern classification and information processing [1].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "Simpson [2] presented the fuzzy min max neural network (FMM), which makes the soft decisions to organize hyperboxes by its degree of belongingness to a particular class, which is known as a membership function.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "Along with this elegant proposal, [2] also presented the characteristics for a good classifier, among which, nonlinear separability, overlapping classes and tuning parameters have proved to be of a great interest to a research community.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "Simpson also presented a clustering approach using FMM in [3].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "To address this issue, GFMM [4] brought this generality.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Earlier, the process of contraction [1][4] was employed, which used to eliminate all the overlapping regions.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "Earlier, the process of contraction [1][4] was employed, which used to eliminate all the overlapping regions.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "Exclusion/Inclusion Fuzzy Classification (HEFC) network was introduced in [5], which further reduced the number of hyperboxes and increased the accuracy.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "This notion is used as it is in almost all the newly introduced models [6][7][8][9].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "This notion is used as it is in almost all the newly introduced models [6][7][8][9].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "Fuzzy Min-Max Neural Network Classifier with Compensatory Neurons (FMCN) was acquainted in [7].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "Multi-level fuzzy min max neural network (MLF) [9] addresses the problem of overlapped region with an elegant approach.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "patterns belonging to overlapped region by maintaining a tree structure, which is a homogeneous tree [9].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "This is a novel finding, and contradictory to what MLF authors have mentioned [9].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "If the pattern belongs to the boundary region, Euclidean Distance [10] between test pattern and the data centroids of the selected hyperboxes is computed.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "These datasets were obtained from the UCI repository of machine learning databases [11].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "method, as it has been already proven to perform better than the previously proposed FMM methods [9].",
      "startOffset" : 97,
      "endOffset" : 100
    } ],
    "year" : 2017,
    "abstractText" : "Recently, a multi-level fuzzy min max neural network (MLF) was proposed, which improves the classification accuracy by handling an overlapped region (area of confusion) with the help of a tree structure. In this brief, an extension of MLF is proposed which defines a new boundary region, where the previously proposed methods mark decisions with less confidence and hence misclassification is more frequent. A methodology to classify patterns more accurately is presented. Our work enhances the testing procedure by means of data centroids. We exhibit an illustrative example, clearly highlighting the advantage of our approach. Results on standard datasets are also presented to evidentially prove a consistent improvement in the classification rate.",
    "creator" : "LaTeX with hyperref package"
  }
}