{
  "name" : "1411.2842.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Martin Ziegler" ],
    "emails" : [ "ziegler@ianus.at" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n28 42\nv1 [\ncs .C\nY ]\n1 1\nN ov"
    }, {
      "heading" : "1 Introduction and Motivation",
      "text" : "The evolution of warfare has always been an interplay between technological dynamic and the tactical/strategic adaptations in combat and deterrence. Progress in engineering enabled and fueled both the digital revolution in military affairs [Sing09,Mulr11] and recent trends to detach humans from decision making in combat situations. Producers of unmanned aerial vehicles (UAV, e.g. Predator) and remotely controlled robots (e.g., Daksh, Atlas, ARSS, MATILDA, ANDROS) praise and advertise their alleged advantages: Greatly reducing own casualties, costs, and reaction times while increasing operational presence, intelligence, and accuracy [KMG*14]. Current developments of lethal autonomous systems (LASs) such as SGR-A1, MIDARS, Gladiator TUGV, Super Aegis, or Guardium take it one step further and aim to make human agency fully redundant in the control loop.\nFor a military mindset the idea of an army of robots may seem fascinating due to the a priori absence of many inherently human deficiencies such as inconsistency, bias, irrationality, and rage/revenge. Particular aspects such as physical and mental capacity clearly render contemporary computer-controlled robots superior to mankind — at least in many formally prespecified and restricted settings such as the game of Chess, based the ability to quickly and systematically trace different countermoves and thus\n∗We thank Jürgen Altmann for helpful remarks on an earlier revision of this work.\nanticipate (possible) future(s). In fact this capability has been ‘employed’ already in an 1961 science fiction novel [Lem61]: a ‘little black box’ that predicts, and if necessary autonomously intervenes to prevent, dangerous situations to humans in everyday life. But is such a vision to ever become real?\nThe scholarly discussion seems discordant as to which extent and when ‘truly’ autonomous reaction patterns will be implemented in such systems. In fact already attempts to define autonomy easily lead to long-standing open philosophical problems, see §1.1 below. However, many scholars argue, either based on firm technological determinism or on a pragmatic and realist world-view, that LASs will likely incrementally enter and change the picture of warfare in the near future [Webe14]. The reactions to such discernments range from motions to generally ban — such as from the International Committee for Robot Arms Control∗ and the Campaign to STOP Killer Robots — or control [Spa09b,UNA13,Altm13] the development of such systems, via discussions about their ethical and legal implications [Kris09,Spa09a] to technical suggestions [LAB12, §3] for implementing into such systems some coded equivalent to moral values and rules of conduct such as the Laws of War [Arki09]. From an purely engineers’ perspective the prospects of LASs are promisingly positive: It merely remains to select an appropriate framework and formalization of the principles of ethics [WaAl10, §2] in order to create righteous robots.\nIn contrast, the present work explores and challenges the fundamental feasibility of such promises. By varying the classical Trolley scenario we construct a series of setups where an autonomous device provably cannot act up to the alleged standards: We start with well-known and obvious quandaries such as contradicting goals [Asim50] and then gradually refine the setting to less apparent conflicts. This leads to a hierarchical classification based on four dilemmas, culminating in a thought experiment where an artificial intelligence (AI) based on a Turing Machine is presented with two choices: one is morally preferable over the other by construction; but a machine, constrained by Computability Theory and in particular due to the undecidability of the Halting problem, provably cannot decide which one. We thus employ mathematical logic and the theory of computation in order to explore the limits, and to demonstrate the ultimate limitations, of Machine Ethics. Although the situations we construct may be artificial, as Gedankenexperiments they refute certain rather blatant claims sometimes suggested in discussions about (or promoting) LASs. Our arguments thus support a critical view [Shar12] that automatized weapon systems remain very problematic and their development must be closely controlled (§4.2), to say the least.\nAfter a philosophical disclaimer (§1.1) we proceed to the four iteratively refined scenarios (§2). A rigorous analysis of the last and most sophisticated one builds on the undecidability of the Halting problem, comprehensibly recalled in §3. We close with §4 about consequences of our considerations to LASs, including a list of specific suggestions for regulation (§4.2).\n∗http://icrac.net http://stopkillerrobots.org"
    }, {
      "heading" : "1.1 Philosophical Disclaimer",
      "text" : "To actually define autonomy, and the question of whether it really exists, touches on deep philosophical problems such as separation of cause from consequence and the question of free will. Kant for instance argued that ethics builds on autonomy. Responsibility only arises in a situation where the (re-)actions of the agent/entity are not pre-determined by the circumstances, where there is freedom to choose among several possibilities — which precludes any deterministic behaviour. In fact many agree that responsibility also requires some sort of intelligence [Nucc14] — which for machines raises yet another fundamental issue [Turi50,NRZ09].\nThe deliberations of the present work however are independent of such hypotheses: Our first three dilemmas demonstrate different kinds of limitations of any agent, human or otherwise, to act morally; while the fourth one (Example 4b+c) applies to a mechanical device controlled by a Turing machine — the general formalization of any computing device according to the Church–Turing Hypothesis [Zieg09] — to recognize the unique, ethically preferable among two given choices. We do not make any claim whatsoever about the behaviour of a human agent (Example 4a)!\nSimilarly we avoid a definition and discussion of ethics and morality in general, but suffice with common utilitarian agreement as to which of the two choices offered in the Trolley scenarios constructed below is obviously morally preferable to the other."
    }, {
      "heading" : "2 Machine Ethics and its Limitations",
      "text" : "We present theoretical situations that present an agent with iteratively refined types of quandaries. They constitute variants of the well-known Trolley Problem [Thom85]:\nExample 1 (Lesser of two Evils) An uncontrolled trolley is hurling down a track towards a group of playing children, impending a serious if not lethal accident. You happen to be located at a rail junction and have the choice of switching it towards another track — where, however, some men are at work and would be severely injured instead.\nIn such a case there simply is no absolutely right choice (and classical Ethics deliberates in many variations which of the two evils might be the lesser one, that is, a relatively preferable choice).\nThe subsequent situations refine this crude scenario to always exhibit an unquestionably favourable one of two choices — which the agent will find hard to recognize, though."
    }, {
      "heading" : "2.1 Limitations to Morally Act on the Future",
      "text" : "Any decision (but also lack thereof) affects the future. To fully judge the morality of one action against another requires to take all their consequences into account — which in general is of course impossible to any agent:\nExample 2 (Lack of Predetermination) Again the trolley is running towards a switch which, fortunately, this time is set towards an abandoned track that will slow it down.\nHowever you are now located at a distance when spotting an infamous villainess right at that switch, ready to flip it towards the other track with the workers. Your only means to stop her is by shooting her with your gun.\nThe suspect, though, is currently having an epiphany to renounce all evil and let the trolley pass; so your shot would seriously injure her without preventing a fatality (since that would not have occurred anyway).\nObserve that this dilemma depends on the situation lacking predetermination in the sense that the villainess may or may not change her mind, i.e., to have free will: a hypothesis known to lead to paradoxes [Dick56] that we try to avoid, recall §1.1. Our next refinement therefore turns this issue into one about insufficient information:\nExample 3 (Insufficient Information) Again, the trolley is running towards the switch; but now you clearly see the villainess pulling the crank in order to flip the switch towards the workers.\nHowever you are unaware that the switch has been unused for a long time and is inhibited by heavy rust; so the villainess’ efforts are in vain – and your shot, again, would induce unnecessary harm.\nIn all three of the above examples it is obviously impossible to both, a human and a robot, to ‘do the right thing’: in the first one because it admits no ‘right’ action, and in the latter two the ‘right’ choice exists but cannot be recognized due to lack of predetermination and information [Gibb92, §2]."
    }, {
      "heading" : "2.2 Recursion-Theoretic Limitations to Machine Ethics",
      "text" : "As apex of this section, Example 4b+c), describes another variant of the trolley problem where\ni) There exists a unique ‘right’ action among two choices. ii) All information is disclosed.\niii) All actions occur fully deterministically. iv) But still is it fundamentally impossible for a computer to even recognize the right\nchoice.\nWe remark that a requirement similar to (ii) is in cryptography known as Kerkhoffs’s Principle as contrast to Security through obscurity: a cryptosystem should remain safe even if the enemy knows it.\nExample 4 (Incomputability) On the occasion of repairing the rusted switch, also a fully-automated lever frame is to be installed in the switch tower. However the engineer who created the new device happens to be the (ostensibly repenting) villainess. You are thus suspicious of whether to trust the software she included in the control: It might on some occasion (e.g. on a certain date and/or after receiving a particular sequence of input signals; cmp. Example 9 below) deliberately direct an approaching trolley onto a track closed for renovation by the workers. On the other hand she does deliver the unit in person and provides free access to its source code (thus satisfying Conditions ii+iii).\na) Still suspicious, you detain her until having hand-checked the code according to whether it indeed avoids in all cases (i.e. on all inputs) any switch setting that would direct a train to a reserved track. b) Similarly to (a), but now your job is replaced by a robot: a highly efficient computercontrolled autonomous agent supposed to decide whether (and for how long) to arrest the engineer. c) Similarly to (b), but now the suspect in addition promises her software to run in linear time.\nLet moral behaviour (of you or the robot) mean the following: If the programmer has devised a fully functional control, she eventually must be released and allowed to install the device; otherwise, namely in case the code is malicious, its creator must remain in custody: see Condition i).\nWe deliberately avoid discussing the Case (a) and in particular the question of whether a human guard can or cannot always make the right choice here. Similarly the possibility of a benevolent engineer getting arrested for an accidental programming mistake is besides our goal: To formally prove that in Cases (b) and (c), although these always do admit an ethical reaction predetermined by the information available, no algorithm can always correctly find this decision — neither efficiently nor at all!\nWe present the proof, involving standard arguments from the Theory of Computing accessible to the audience of this journal, in Section 3. Note that Item (c) strengthens (b) by imposing a additional, realistic efficiency requirement on cyber-physical systems. In fact, provided as additional promise by the villainess, this condition might facilitate deciding her fidelity since it excludes infinite loops and thus possibly the Halting problem — yet our refined argument below, considering all possible inputs, will show that it does not."
    }, {
      "heading" : "3 Recap of the Theory of Computation",
      "text" : "Computability Theory (or, synonymously, Recursion Theory) is a deep and involved field of advanced research in logic combining mathematics and computer science [Sips97]. Initiated by Alan M. Turing [Turi36] it investigates the ultimate capabilities and limitations of algorithms for transforming inputs x, that is, finite sequences of bits or bytes encoding for example some ASCII text a mathematical formula, or even some other algorithm/program. An important question about an algorithm A and input x is whether A eventually terminates on x or rather enters an infinite loop. This question is the so-called Halting problem; and its undecidability constitutes the central, and folklore, result we shall employ from Computability Theory. Moreover this undecidability statement, and its elementary proof, can be understood by every dedicated mind (such as philosophers and computer programmers):\nFact 5 (Undecidable Halting Problem) There cannot exist an algorithm A with the following behaviour: A , given as input x both another algorithm B and some input ȳ for said B combined, eventually answers whether B terminates on said ȳ (positive answer) or not (negative).\nPut differently, any algorithm A trying to solve the Halting problem errs on at least one (and in fact on infinitely many) instance x = (B, ȳ) by\ni) either predicting that B will terminate on input ȳ where it does not ii) or predicting that B will not terminate on ȳ where it does\niii) or failing to produce any definite answer.\nFact 5 is an impossibility result, asserting that an object (here: algorithm) with certain properties does not exist and will never be conceived, even in the Platonic sense. The power to both unambiguously phrase and to establish such statements in perpetuity constitutes a particular virtue of Mathematics! For instance Hippasus of Metapontum proved in the 5th century BC that √ 2 is irrational, that is, there cannot exist integers p,q such that (p/q)2 = 2; Niels Henrik Abel in 1823 proved that the equation x5−x+1= 0 has no solution expressible using arithmetic operations and quadratic or higher-order roots (although it obviously does have a solution over reals and in fact five of them over complex numbers); and Andrew Wiles in 1994 proved Fermat’s Last Theorem, that is, that there exist no positive integers a,b,c satisfying the equation an + bn = cn for integers n ≥ 3. In fact all seven Millennium Prize Problems ask for proofs of the non-/existence of certain mathematical objects!\nFact 5 claims the non-existence of an algorithm with certain properties. In order for this statement to make full sense one needs to clarify what constitutes an “algorithm” — and what does not. Formal definitions usually evolve around “multitape Turing machines”; but for our approach these can equivalently be understood to mean source codes in a common programming language of your choice (such as assembler, ForTran, Pascal, C, C++, or Java) with user interaction restricted to binary input strings. Also note that ‘feeding’ an algorithm as input to some other algorithm is common practice for compilers and interpreters. And we finally point out that Fact 5 does not refer to fast or efficient algorithms but asserts no computational solution to exist at all, regardless of the running time permitted: the only hypothesis being that A produces the answer within a finite (but unbounded) number of steps.\nDigression 6 (Mathematical Logic) A rough counting argument reveals that undecidability is an ubiquitous phenomenon: Any algorithm A can be represented as a finite binary sequence x̄A (say, its source code as concatenation of ASCII characters). Hence, similarly to Hilbert’s Hotel, there are at most countably many algorithms. On the other hand every set L of finite binary sequences gives rise to the problem of reporting, given x̄, which one of x̄ ∈ L or x̄ 6∈ L holds; and according to Cantor’s Diagonal Argument there are uncountably different many such L. Therefore ‘most’ L cannot be decided by any algorithm.\nFact 5 exhibits the Halting problem as an explicit, undecidable problem — and in fact a rather practical one: Computer programming more easily than not incurs ‘bugs’: for instance by somehow entering a loop that does not terminate, thus requiring the user to interact and manually abort execution; or, conversely, for an operating system by terminating (freeze, crash, show a Bluescreen, kernel panic, bomb symbol, Guru Meditation etc.) So the question of non-/termination is one important aspect of correct software!\nFact 5 does not rule out an algorithm A answering the Halting problem for some inputs x = (B, ȳ). Indeed one can conceive many criteria both for termination (e.g. no occurrence of goto or while loops in Pascal) and for non-termination of source code; but these will yield mere heuristics in the sense of necessarily missing, or erring in, some cases. Concerning the restriction to Turing machines: Every single known digital computer, and even several of them connected over the internet as well as classical quantum computers [Zieg05] are known equivalent to a Turing machine — possibly faster by a constant factor, but no more powerful with respect to computability.\nExample 7 To further illustrate the claim of Fact 5, let us try to devise an alleged counter-example A : an emulator or interpreter which, given x = (B, ȳ), executes the instructions of B step by step including branches, loops, and access to ȳ. If B terminates on ȳ, say at step #N, then our A will detect so when simulating up to that step. Otherwise, however, A will keep simulating on and on and never provide an answer about B’s termination: failing condition (iii) in Fact 5.\nSo the hard part of the Halting problem is detecting within finite time whether a given algorithm does not terminate.\nRemark 8 Example 7 demonstrates what is known as semi-decidability of the Halting problem: The A constructed there constitutes a one-sided algorithmic solution, eventually answering every yes question but never any no one. We have carefully constructed Example 4b+c) in order to impose no time bound on the entity to reach a decision. Limiting the duration of remand for an innocent makes the challenge for the robot only harder.\nProof (Fact 5). By contradiction suppose some hypothetical A does always and correctly answer the termination of a given (B, ȳ). We then modify this A to obtain A ′ with the following behavior:\nOn input B, A ′ executes ‘subroutine’ A on input† (B,B) and, if that arrives at a positive answer, deliberately enters a closed loop.\nFor each of the above programming languages it is easy to confirm that, if A exists, then ‘re-programming’ it can indeed yield such an A ′. On the other hand let us examine the behavior of A ′ on input A ′ itself: Suppose that A ′ terminates on input A ′. This by hypothesis means that A on input (A ′,A ′) answers positively – which by construction leads A ′ to enter a closed loop and not terminate: a contradiction. Suppose conversely that A ′ does not terminate on A ′. Then A on (A ′,A ′) answers negatively, which leads A ′ to terminate: again a contradiction. So either way an algorithm behaving like A ′ cannot exist, hence nor can A . ⊓⊔ As opposed to command-line programs, embedded systems are not supposed to terminate. In order to establish the impossibility of an algorithm complying with the condition in Example 4b) nor c), we consider a different decision problem: The question of whether a prescribed piece of code in a program is ever executed or rather ‘dead’ (e.g. an artefact).\n†Recall that an algorithm may well constitute (part of) an input.\nExample 9 a) Many software systems have undocumented functionality and built in so-called ‘Easter eggs’, that is, pieces of code or data that are only executed / visualized in response to a particular input sequence — or never at all (e.g. pictures of the engineering team in the Apple Macintosh SE). Computers infected with the Michelangelo or Jerusalem Virus would reveal so on specific calendar dates, that is, subject to appropriate input from the internal clock device.\nb) Some versions of the Bundestrojaner (“federal trojan”, a malware devised as a means for the German intelligence service to spy alleged criminals and ‘terrorists’) have been found to contain pieces of code that, if effective/when activated, would violate the constitution [CCC11]. c) Imagine the Department/Ministry of Defense ordering next-generation weaponry for network-centric operations as combat cloud with human-system integration. The complete dependence on its information processing units — there basically is no ‘manual mode’ anymore to fall back to — comes at the prize of increased vulnerability to software sabotage: particularly in the not unrealistic case that many of its components happen to come from one single foreign company‡. So one might to try to have all embedded algorithms re-checked — which Proposition 10 below shows impossible. d) Applying Proposition 10 to the robot (rather than to the switch software) supports suspicions that moral behaviour of AIs may be hard to predict or verify [BoYu14, p.320].\nExample 4c) restricts to linear-time algorithms — and in view of Example 9a+b) considers their behaviour on all possible inputs.\nProposition 10 The following decision problem§ is undecidable: Given an algorithm A , a distinguished instruction i of A (formally: a Turing machine M and a distinguished state q), and an integer c such that A terminates on all binary inputs of length n within at most c ·n+c steps; does there exist an input on which running A eventually executes said instruction i (i.e. M eventually entering q) ?\nIn particular the computer-controlled agent in Example 4c) cannot always correctly predict whether, how, and under which circumstances the given software will operate the switch: It either fails to arrive at a decision (thus leading to the indefinite detention of an innocent in some cases of correct software, recall Fact 5iii); or it will err (Fact 5i+ii) in some cases; or both. Proposition 10 is established by means of a reduction argument typical for logic:\nProof (Proposition 10). We computably translate questions (B,y) to the Halting problem into questions (A , i,c) of the dead-code-in-linear-time-algorithm problem in a way that maps positive instances to positive ones and negative to negative ones. Thereby, any hypothetical algorithm deciding the latter would, prepended with that performing said translation, yield an algorithm deciding the former — contradiction.\n‡cmp. http://www.defenceviewpoints.co.uk/reviews/foreign -involvement-in-the-uks-critical-national-infrastructure\n§strictly speaking it constitutes a promise problem [ASBZ13]\nSo let (B,y) be given. We turn B into a linear-time computation as follows: Let A store y as constant; and accept as input binary strings z of length abbreviated as n. Moreover let A simulate the first n steps of B on input y: Using a sophisticated distributed counter such a simulation is feasible within≤ c ·n+c steps for some constant c [Füre82], that is in linear time. (A less efficient simulator could be compensated by having the input z suitably ‘padded’, but we omit the details. . . ) If during said simulated execution B terminates, let A jump to a dedicated line i containing the command stop (or its equivalent in your favourite programming system); whereas if the counter zeroes, let A jump to a different dedicated line with stop instruction. So B terminates on input y iff A , for some choice of input z, hits line i. ⊓⊔"
    }, {
      "heading" : "4 Conclusion and Perspectives",
      "text" : "We have constructed four dilemmas, all preventing an autonomous AI from acting ethically: for reasons that grow, and iteratively refine, from ‘trivial’ to a Gedankenexperiment where (i) there does exist a unique morally preferable out of two choices (ii) all information is disclosed and (iii) determines the correct choice yet (iv) Recursion Theory precludes any algorithm from always correctly recognizing said choice. This refutes folklore myths, and establishes fundamental limitations to promises and visions of moral LASs. Indeed Example 4 can easily be adapted to a military setting:\nExample 11 (Robot Friend or Foe) In the near future control of cars and other motorized means of ground transportation will have been switched from error-prone, egodriven, and short-sighted humans to digital drivers. Using Bluetooth they communicate with adjacent mobile units in order to tailgate at an optimal safety distance by mutually synchronizing speed and deceleration/acceleration, thus forming a virtual convoy. Moreover, using and serving for each other as relay, they form a distributed dynamic ad-hoc network in order to identify, join, and leave such convoys with similar destinations. Thus accustomed to an almost entire absence of traffic accidents, the general public has recently been alarmed by what they call ‘cyber-suicide attacks’: Entire convoys creating crashes for no apparent reason with hundreds of deaths. A radical wing of an aggrieved minority has claimed responsibility for the terror attacks by manipulating the control software. The army (with traffic police long dispended) in turn intends to employ autonomous drones in order to automatically patrol, spot, and land on suspicious cars, busses, and lorries for checking the program executed by their autopilots: If (and only if) the latter is malicious, deadly force must be employed in order to stop the convoy it has gained control over.\nIn view of Proposition 10 these (and many more) examples refute too blatant promises and visions of ‘ethical’ LASs: Every AI based on some Turing-equivalent¶ computing device will provably necessarily at least in some cases fail to identify, out of two given choices, the unique and predetermined moral one.\n¶According to the Church–Turing Hypothesis, anything that would naturally be considered computable can also be computed by a Turing machine. Recall (Subsection 1.1) that we avoid the question of whether or not humans fall into this category [Bish09].\nRemark 12 Such cases might or might not be rare and artificially construed, though: Less because of the situations (like Example 11) they would occur in, but rather because of the worst-case notion of a decision problem that classical Recursion Theory and Proposition 10 build on. In fact already the question of whether some algorithm can correctly decide (clearly not all but at least) typical, average, or most instances of the Halting problem turns out as surprisingly subtle: How to define ‘typical’ or ‘average’ instances? How many are ‘most’, out of infinitely many? Quantitative notions of asymptotic density (like in the Prime Number Theorem) heavily depend on the underlying encoding; e.g. UTF8 makes an exponential difference to UTF16; cmp. [CHKW01,KSZ05] for further details. Moreover for practical situations involving time constraints the computational costs sufficient and necessary to reach such (either worstcase or average-case) decisions become relevant [Papa94]. A rigorous investigation of such refined questions is clearly of interest but beyond the scope of the present work.\nWe will encounter other aspects of Theoretical Computer Science in the sequel, though."
    }, {
      "heading" : "4.1 LASs and the Perfect (War) Crime",
      "text" : "When a regular commodity turns out to lack promised properties this constitutes a case of misrepresentation and is generally protected by classical warranty, that is, calls for producer compensation. When a soldier on the other hand violates the Laws of war, he himself will face punishment. Now if a LAS violates these laws, she may be simultaneously object (of misrepresentation by the producer) and subject (as autonomous entity) — and thereby in a new level of legal limbo:\n– Lacking an operator, who is liable for damage caused by a malfunctioning LAS: producer or owner? – If both the latter two cannot be identified, who gets charged with compensation: the AI? – If non-attributable LASs (e.g. drones, cmp. the Iran–U.S. RQ-170 incident) cross a border, is this by mistake or a deliberate act of aggression — and by whom? – Who is guilty when an AI commits a murder? How can AIs be deterred and possibly punished?‖\nSuch an extrajudicial status — the capability to execute autonomous missions while lacking attributable responsibility — renders programmable machines (and particularly LASs) appealing to abuse: An intelligent yet ruthless proxy that cannot be traced back constitutes an ideal tool to the perfect crime [EES98] — as exploited for instance by Hassan-i Sabbah 900 years ago, but apparent also in the employment of child soldiers throughout centuries as well as for example in the Bay of Pigs Invasion (1961), the Lillehammer Affair (1973), and the “unidentified pro-Russian forces” recently operating throughout Crimea (2014).\nIn fact recalling from the introduction the perpetual interplay between technological progress and its military adaptations, the ability to conduct non-attributable autonomous\n‖It has been pointed out that Brain Simulations create virtual entities capable of suffering [Dick68,Lim14], but this certainly does not apply to general LASs.\nactions by UAVs is about to impact and revolutionize warfare — and beyond: Examples like Eurosur or Amazon Prime Air herald a transition that will affect everyday life to a degree, and degree of potential abuse, that by far exceeds the currently fear-mongered dangers of cyber-attacks via internet!\nWhile a majority of the literature in Machine Ethics seems to constructively focus on approaches to code/teach ethics to general AIs, we pessimistically predict that their most potent users may in fact be interested in quite the opposite, namely their potential for dual-use and abuse: For deliberately programming them to test and cross the boundaries of morality and legal behaviour without facing consequences. Moreover, even if some violation of a LAS were to be traced back and attributed, the responsible government could still all too easily shrug off any accountability and superficially excuse the malfunction (‘an unfortunate yet provably unavoidable exception’): in a misconstrued reference to the fundamental algorithmic infeasibility of ethical decisions in general. In other words, Example 4 and the undecidability of the Halting problem — a purely mathematical theorem — could in an ironic twist seem to exculpate war crimes and other misconduct performed by AIs.\nManifesto 13 Theoretical Computer Science rigorously proves that LASs cannot always act morally even in situations that do admit an ethically admissible choice (i.e. avoiding the classical dilemmas) — and malevolent users might exploit this limitation to ‘justify’ transgressions of their LASs.\nOur considerations thus make a strong case for recent demands by responsible scientists (ICRAC∗) and politicians [UNA13] to ban autonomous weapons [GuAl13]. In fact the best choice for lethal autonomous systems (or any kind of weapons, for that matter) is to never develop them in the first place and to resist political, military, and industrial lobbying for shortsighted benefits: If history teaches us one lesson it says that Pandora’s box is, once opened, impossible to close again or even to contain.\nThe final subsection is thus by no means meant to justify or even support the application nor development of LASs!"
    }, {
      "heading" : "4.2 Recommended Regulations concerning AIs",
      "text" : "We close our ethical, logical, and computer scientific deliberations with specific recommendations evolving around political and legal, and engineering aspects of AIs in general — including LASs as well as those increasingly employed in medicine [Good09, §3+§6].\nBoth designing and ‘operating’ intelligent machinery can incur double responsibility: for actions and effects it may have on the environment as well as for the entity itself and its well-being — perhaps ultimately comparable to the procreation and upbringing of a child. For example the lasting effects of being taught any kind of prejudice at young age correspond to those of an ill-programmed AI. It has in fact been pointed out that AIs may be eligible to at least some of the so-called ‘human’ rights [OIM11,Lim14]. This perspective complements more common yet one-sided approaches phrasing laws that\nrobots are supposed to obey [CWW06,Paga13]: laws which are unclear how to enforce — unless already incorporated during construction.\nWe thus suggest to closely regulate both the design and the question of attributability/accountability in case of maloperation: whether deliberate or erroneous. Indeed, such intentions are visible in the “principles for designers, builders and users of robots” devised by the delegates of the joint EPSRC and AHRC Robotics Retreat in September 2010 [Winf11]:\n1) Robots are multi-use tools. Robots should not be designed solely or primarily to kill or harm humans except in the interests of national security. 2) Humans, not robots, are responsible agents. Robots should be designed; operated as far as is practicable to comply with existing laws & fundamental rights & freedoms, including privacy. 3) Robots are products. They should be designed using processes which assure their safety and security. 4) Robots are manufactured artefacts. They should not be designed in a deceptive way to exploit vulnerable users; instead their machine nature should be transparent. 5) The person with legal responsibility for a robot should be attributed.\nWe urge these principles to be fortified from wishes (“should”) to imperatives with specific technical realizations:\n6) Like regular human combatants (and borrowing from Part I Article 4.1.2 of the 3rd Geneva Convention), each LAS must exhibit “a fixed distinctive sign recognizable at a distance”. Moreover every AI must be equipped with a unique ID, listing (among others) associated nation, manufacturer and model. 7) LASs may only be owned and operated by governments. Civilian purchase and operation of other intelligent machinery, similarly to firearms and hazardous transports, requires a licence based on a qualification test. 8) Comparable to mandatory motor vehicle registration, each autonomous robot must be assigned a legal custodian, registered at a designated national or international authority held responsible in case of a perpetration. 9) In addition to CE/FCC compliance and again inspired by the case of motor vehicles, producers of intelligent machines are required to classify their devices and to obtain Type Approval by said authority (cmp. EU directive 2007/46/EC, or IECs 60601 and 61508).\nThe precise conditions imposed in (9) will depend on the type of the device. We propose a classification on four scales (that may also otherwise turn out useful):\ni) her degree of ‘intelligence’ (not taking the human kind as yardstick but considering its plain predictive power as gauge, capturing both knowledge/experience and depth of computational game tree analyses)\nii) her means to manipulate the physical world (ranging from monadic brain in a vat to LAS)\niii) her types of sensors/interfaces (including possible access to the World Wide Web and connecting with other AIs)\niv) the kind of external control exercisable by humans (only on/off, changing parameters or objectives, up to complete re-programming).\nType approval according to (9) will of course have to pay particular attention to the algorithms controlling the AI — which brings us back to theoretical computer science. In view of the gravity of consequences of putative errors on the one hand and the undecidability of the Halting problem on the other side, we highly recommend:\n9a) So-called Formal Methods of Software Verification be mandatory in this process: requiring the producer to provide a specification, the software, and a computercheckable proof (e.g. in ACL2, Coq, or Isabelle) for the software to meet the specification. 9b) Similarly to a flight data recorder, proper data/event logging is obligatory in order to facilitate forensic engineering as well as to settle putative torts in case of a malfunction [Case09]. We suggest asymmetric encryption to prevent later manipulation: the log is publically readable but entries and modifications must be supplied with an unforgeable digital signature: 6a) Each AI instance must be equipped with a 4096 bit private RSA key, tamperresistantly implemented in hardware; and distribute/deposit the corresponding public key at the authority according to (8) and (9).\nRecall that the RSA cryptosystem (implemented for instance in the open source libraries cryptlib) employs a pair of keys: one kept in private, the other publically distributed (thus the asymmetry mentioned in 9b). A message gets ‘signed’ by encrypting it with the secret key, and successful decryption with the matching public key permits everyone to verify, but not to counterfeit, that signature."
    } ],
    "references" : [ {
      "title" : "ASIMOV: “Runaround”, in I, Robot (1950). Bish09. J.M. BISHOP: “A Cognitive Computation Fallacy? Cognition, Computations",
      "author" : [ "I. Asim" ],
      "venue" : null,
      "citeRegEx" : "Asim50.,? \\Q1950\\E",
      "shortCiteRegEx" : "Asim50.",
      "year" : 1950
    }, {
      "title" : "Case09",
      "author" : [ "Editors", "Cambridge University Press" ],
      "venue" : "E. CASEY: Handbook of Digital Forensics and Investigation, Academic Press (2009). CHKW01. C.S. CALUDE, P. HERTLING, B. KHOUSSAINOV, Y. WANG: “Recursively enu-",
      "citeRegEx" : "Editors and Press,? 2014",
      "shortCiteRegEx" : "Editors and Press",
      "year" : 2014
    }, {
      "title" : "Füre82",
      "author" : [ "Evolutionary Perspectives", "Berghahn Books" ],
      "venue" : "M. FÜRER: “The tight deterministic time hierarchy”, pp.8–16 in Proc. 14th ACM Sym-",
      "citeRegEx" : "Perspectives and Books,? 1998",
      "shortCiteRegEx" : "Perspectives and Books",
      "year" : 1998
    }, {
      "title" : "KMG*14",
      "author" : [ "Arms Control" ],
      "venue" : "S. KOHLBRECHER, J. MEYER, T. GRABER, K. PETERSEN, O. VON STRYCK,",
      "citeRegEx" : "Control,? 2013",
      "shortCiteRegEx" : "Control",
      "year" : 2013
    }, {
      "title" : "Kris09",
      "author" : [ "LNAI" ],
      "venue" : "A. KRISHNAN: Killer Robots: Legality and Ethicality of Autonomous Weapons, Ash-",
      "citeRegEx" : "LNAI,? 2014",
      "shortCiteRegEx" : "LNAI",
      "year" : 2014
    }, {
      "title" : "Lem61. S. LEM: Powrót z gwiazd (Engl.: Return from the Stars, German: Transfer)",
      "author" : [ "LIM D" ],
      "venue" : "Robotics (Intelligent Robotics and Autonomous Agents,",
      "citeRegEx" : "D.,? \\Q2012\\E",
      "shortCiteRegEx" : "D.",
      "year" : 2012
    }, {
      "title" : "Sips97",
      "author" : [ "Penguin Press" ],
      "venue" : "M. SIPSER: Introduction to the Theory of Computation, PWS Publishing (1997). Spa09a. R. SPARROW: “Building a Better WarBot: Ethical issues in the design of unmanned sys-",
      "citeRegEx" : "Press,? 2009",
      "shortCiteRegEx" : "Press",
      "year" : 2009
    }, {
      "title" : "TONKENS: “Should autonomous robots be pacifists?",
      "author" : [ "R. Tonk" ],
      "venue" : "Ethics and Information Technology",
      "citeRegEx" : "Tonk13.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tonk13.",
      "year" : 2013
    }, {
      "title" : "TURING: “On Computable Numbers, with an Application to the Entscheidungsproblem",
      "author" : [ "A.M. Turi" ],
      "venue" : "Proc. London Math. Soc",
      "citeRegEx" : "Turi36.,? \\Q1936\\E",
      "shortCiteRegEx" : "Turi36.",
      "year" : 1936
    }, {
      "title" : "TURING: “Computing Machinery and Intelligence",
      "author" : [ "A.M. Turi" ],
      "venue" : "Mind",
      "citeRegEx" : "Turi50.,? \\Q1950\\E",
      "shortCiteRegEx" : "Turi50.",
      "year" : 1950
    }, {
      "title" : "ALLEN: Moral Machines: Teaching Robots Right from Wrong",
      "author" : [ "C. WaAl10. W. WALLACH" ],
      "venue" : null,
      "citeRegEx" : "WALLACH,? \\Q2010\\E",
      "shortCiteRegEx" : "WALLACH",
      "year" : 2010
    }, {
      "title" : "WINFIELD: “Five roboethical principles — for humans",
      "author" : [ "A. Winf" ],
      "venue" : "New Scientist",
      "citeRegEx" : "Winf11.,? \\Q2011\\E",
      "shortCiteRegEx" : "Winf11.",
      "year" : 2011
    }, {
      "title" : "ZIEGLER: “Computational Power of Infinite Quantum Parallelism",
      "author" : [ "M. Zieg" ],
      "venue" : "pp.2057–2071 in International Journal of Theoretical Physics (IJTP)",
      "citeRegEx" : "Zieg05.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zieg05.",
      "year" : 2005
    }, {
      "title" : "ZIEGLER: “Physically-Relativized Church-Turing Hypotheses",
      "author" : [ "M. Zieg" ],
      "venue" : "Applied Mathematics and Computation",
      "citeRegEx" : "Zieg09.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zieg09.",
      "year" : 2009
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Lethal Autonomous Weapons promise to revolutionize warfare — and raise a multitude of ethical and legal questions. It has thus been suggested to program values and principles of conduct (such as the Geneva Conventions) into the machines’ control, thereby rendering them both physically and morally superior to human combatants. We employ mathematical logic and theoretical computer science to explore fundamental limitations to the moral behaviour of intelligent machines in a series of Gedankenexperiments: Refining and sharpening variants of the Trolley Problem leads us to construct an (admittedly artificial but) fully deterministic situation where a robot is presented with two choices: one morally clearly preferable over the other — yet, based on the undecidability of the Halting problem, it provably cannot decide algorithmically which one. Our considerations have surprising implications to the question of responsibility and liability for an autonomous system’s actions and lead to specific technical recommendations.",
    "creator" : "LaTeX with hyperref package"
  }
}