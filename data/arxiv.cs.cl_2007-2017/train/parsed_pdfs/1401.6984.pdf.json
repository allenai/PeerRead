{
  "name" : "1401.6984.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN",
    "authors" : [ "Yajie Miao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN\nYajie Miao\nLanguage Technologies Institute, School of Computer Science, Carnegie Mellon University\nThe Kaldi 1 toolkit is becoming popular for constructing automated speech recognition (ASR) systems. Meanwhile, in recent years, deep neural networks (DNNs) have shown state-of-the-art performance on various ASR tasks. This document describes our recipes to implement fully-fledged DNN acoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning toolkit developed under the Theano environment. Using these recipes, we can build up multiple systems including DNN hybrid systems, convolutional neural network (CNN) systems and bottleneck feature systems. These recipes are directly based on the Kaldi Switchboard 110-hour setup. However, adapting them to new datasets is easy to achieve.\n1. Introduction Deep neural networks (DNNs) have shown superior performance over the traditional state-of-the-art GMMHMM on ASR tasks [1, 2]. When applied in ASR, DNNs have multiple hidden layers and model context-dependent states directly. In this paper, we present our Kaldi recipes to build DNN acoustic models using PDNN2, a lightweight deep learning toolkit developed on top of Theano3. The rough idea behind these recipes is to build the initial GMM models with Kaldi, train DNNs with PDNN and finally load the DNN models back to Kaldi for further decoding or system building. More details about the pipeline can be found in Section 2. We elaborate on PDNN and individual recipes in Section 3 and 4. The recipes have the following key features:\nA. Consistent with Kaldi\nThe recipes are written in the Kaldi style and thus can be integrated with any existing recipes seamlessly.\nB. Diverse Systems\nMultiple systems with different architectures; potentially useful for system combination.\nC. Easy to Run and Modify 1 http://kaldi.sourceforge.net/ 2 http://www.cs.cmu.edu/~ymiao/pdnntk.html 3 http://deeplearning.net/software/theano\nThe recipes are run in the “one-button” fashion, producing word error rates (WERs) at the end without any user intervention. All the DNN configurations (e.g., network structure, learning rate, output format, etc.) are visible when PDNN commands are called.\nD. Convenient for Research\nThe PDNN toolkit is open source and written in python. Experiments with new research ideas become easy and fast.\nE. Open License\nAll the recipes, including the PDNN toolkit, are licensed under Apache 2.0, one of the least restrictive licenses.\n2. Overall Pipeline\nThe current release of Kaldi+PDNN consists of 4 DNN recipes. Fig. 1 shows the overall pipeline for these recipes. More information about individual recipes will be presented in Section 4.\nThe recipes start with initial GMM models which can be obtained after we run the existing Kaldi recipes, e.g., egs/swbd/s5b/run.sh for the Switchboard setup. Training and validation data for DNNs are generated from the GMM models by performing forced alignment. By default, our recipes use the speaker adaptive training (SAT) model.\nThen, various networks are trained with PDNN and saved in the Kaldi-supported format. Some recipes require multiple networks to be trained. For instance, in the BNF+DNN recipe, we need to train the bottleneck feature network and the DNN on top of bottleneck features. Note that for DNN training, Fig. 1 is not showing other optional steps such as pre-training.\nFinally, we load the saved networks back to Kaldi. If we are building hybrid systems, the trained networks can be used directly for decoding. Alternatively, if the networks have the bottleneck structure, then GMM systems, which are also referred to as tandem systems, can be further constructed over the bottleneck feature representations."
    }, {
      "heading" : "3. PDNN: Yet Another Python Toolkit for Deep Neural Networks",
      "text" : "PDNN is a lightweight deep learning toolkit developed under the Theano environment. It takes advantage of two Theano’s features: GPU utilization and gradient computation. Therefore, Theano needs to be installed properly on your computing machine, where a GPU card should be available to speed up DNN training. Check the PDNN website http://www.cs.cmu.edu/~ymiao/pdnntk for details about installation, configuration and usage.\nCurrently, the input data into PDNN have to be in the format of “PFile”, the ICSI feature file archive format. Future releases will support more data formats. Table 1 demonstrates the PFile format and lists 3 example lines. Each row starts with the utterance and frame index, followed by the feature vector. The last field represents “class label” which in speech recognition means contextdependent state label for each frame. Note that frame index is based on each utterance, instead of being sequenced globally. You can specify the following arguments for PFile reading.\nUtterance Index Frame Index\nFeature Vector Class Label\n0 0 [0.2, 0.3, 0.5, 1.4, 1.8, 2.5] 10 0 1 [1.3, 2.1, 0.3, 0.1, 1.4, 0.9] 179 1 0 [0.3, 0.5, 0.5, 1.4, 0.8, 1.4] 32\nrun_SdA.py. This command trains stacked denoising autoencoders (SdAs) [3], mostly used from DNN pretraining. A denoising autoencoder (DA) has the same structure as the traditional autoencoder, with the only difference of corrupting the input by adding some form of noise. SDAs are trained in a greedy layer-wise manner. Training of each DA involves reconstructing the clean input from the corrupted version of it.\nrun_RBM.py. This command performs unsupervised training of the generative restricted Boltzmann machines (RBMs) model. A RBM is an undirected graphical model with a set of nodes representing visible units and a set of nodes representing hidden units. RBM training involves maximizing the likelihood of the observations with the contrastive divergence algorithm [4]. Similarly with SdAs, a stack of RBMs can be trained in a greedy layerwise manner and used to initialize the parameters of DNNs. The first layer of a DNN corresponds to a Gaussian-Bernoulli RBM and each of the other hidden layers corresponds to a Bernoulli-Bernoulli RBM. Interested readers can refer to [5] for details on RBM.\nrun_DNN.py. This command implements fine-tuning of DNNs based on stochastic gradient descent (SGD). Two special techniques can be adopted to improve DNN training.\nDropout: Dropout has been proven to be effective in curbing overfitting and enhancing DNN training. In principal, on each presentation of a training example, dropout omits outputs from each hidden layer randomly via a binomial distribution. This distribution is governed by a pre-specified probability referred to as dropout factor in [6]. Dropout is applied only during training (fine-tuning). For testing (recognition), network parameters need to be scaled properly according to the value of the dropout factor [6]. In PDNN, dropout is triggered by the argument “--dropout-factor” to specify the value of the dropout factor. For the hidden layers, 0.2 has been found to be the optimal value for the dropout factor.\nMaxout: Previous works [7, 8, 9] have proposed to apply maxout networks to speech recognition. In maxout networks, the units at each hidden layer are divided into non-overlapping groups, each of which generates a single activation with max-pooling. Compared with standard DNNs, maxout networks result in superior performance on both hybrid systems and bottleneck-feature tandem systems. The advantages of maxout networks are twofold. First, maxout networks can shrink the size of network parameters significantly, due to the reduced hidden activations. Second, maxout networks do not fix the shape of the activation function for hidden outputs. By tuning weight vectors of the subsumed hidden units, each maxout unit is capable of approximating any convex functions and thus can be optimized towards specific datasets in hand. Figure 2 illustrates the maxout layer in which the unit group size is 3.\nrun_CNN.py. This command implements SGD-based fine-tuning of CNNs. CNNs can outperform DNNs on LVCSR tasks [10]. Instead of using the fully-connected parameter matrices, CNNs are characterized by parameter sharing and local feature filtering. On top of the convolution layer, a max-pooling layer is usually added to normalize spectral variations and reduce output dimensionality. Figure 3 shows our CNN architecture which works slightly different from the existing proposals [10]. In the convolution layer, we only consider filters along the frequency, assuming that the time variability can be modeled by HMM. Interested readers can refer to [11] for more details.\n4. Recipes\nCurrently, Kaldi+PDNN contains four recipes. By default, they have the following configurations. In our descriptions, target_number means the number of classification targets at the final layer.\nA. DNN Hybrid (run-dnn.sh) The classic context-dependent DNN hybrid system Input: spliced fMLLR features further projected by\nLDA to 250 dimensions Network: 250:1024:1024:1024:1024:1024:target_ number\n5. References [1] G. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent pre-trained deep neural networks for large vocabulary speech recognition,” IEEE Transactions on Speech and Audio Processing, Special Issue on Deep Learning for Speech and Lang Processing, 2012. [2] F. Seide, G. Li, X. Chen, and D. Yu, “Feature engineering in context-dependent deep neural networks for conversational speech transcription,” in Proc. ASRU, pp. 24–29, 2011. [3] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol, “Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion,” Journal of Machine Learning Research, vol. 11, pp. 3371-3408, 2010. [4] M. A. Carreira-Perpinan, and G. E. Hinton, “On contrastive divergence learning,” Artificial Intelligence and Statistics , 2005. [5] G. E. Hinton, “A practical guide to training restricted Boltzmann machines,” UTML TR., Deparment of Computer Science, University of Toronto, 2010. [6] Y. Miao, and F. Metze, “Improving low-resource CD-DNNHMM using dropout and multilingual DNN training,” in Proc. Interspeech, pp. 2237-2241, 2013. [7] Y. Miao, F. Metze, and S. Rawat, “Deep maxout networks for low-resource speech recognition,” in Proc. ASRU, 2013. [8] M. Cai, Y. Shi, and J. Liu, “Deep maxout neural networks for speech recognition,” in Proc. ASRU, 2013. [9] X. Zhang, J. Trmal, D. Povey, and S. Khudanpur, “Improving deep neural network acoustic models using generalized maxout networks,” submitted to ICASSP 2014. [10] T. N. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran, “Deep convolutional neural networks for LVCSR,” in Proc. ICASSP, pp. 8614-8618, 2013. [11] Y. Miao, and F. Metze, “Convolutional Neural Networks for Language-Universal Feature Extraction and Cross-language Hybrid Systems,” submitted to ICASSP 2014. [12] J. Gehring, Y. Miao, F. Metze, and A. Waibel, “Extracting deep bottleneck features using stacked auto-encoders,” in Proc. ICASSP, pp. 3377-3381, 2013. [13] J. Gehring, W. Lee, K. Kilgour, I. Lane, Y. Miao, and A. Waibel, “Modular Combination of Deep Neural Networks for Acoustic Modeling,” in Proc. Interspeech, pp. 94-98, 2013."
    } ],
    "references" : [ {
      "title" : "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition",
      "author" : [ "G. Dahl", "D. Yu", "L. Deng", "A. Acero" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing, Special Issue on Deep Learning for Speech and Lang Processing, 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Feature engineering in context-dependent deep neural networks for conversational speech transcription",
      "author" : [ "F. Seide", "G. Li", "X. Chen", "D. Yu" ],
      "venue" : "Proc. ASRU, pp. 24–29, 2011.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol" ],
      "venue" : "Journal of Machine Learning Research, vol. 11, pp. 3371-3408, 2010.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On contrastive divergence learning",
      "author" : [ "M.A. Carreira-Perpinan", "G.E. Hinton" ],
      "venue" : "Artificial Intelligence and Statistics , 2005.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A practical guide to training restricted Boltzmann machines",
      "author" : [ "G.E. Hinton" ],
      "venue" : "UTML TR., Deparment of Computer Science, University of Toronto, 2010.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Improving low-resource CD-DNN- HMM using dropout and multilingual DNN training",
      "author" : [ "Y. Miao", "F. Metze" ],
      "venue" : "Proc. Interspeech, pp. 2237-2241, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep maxout networks for low-resource speech recognition",
      "author" : [ "Y. Miao", "F. Metze", "S. Rawat" ],
      "venue" : "Proc. ASRU, 2013.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep maxout neural networks for speech recognition",
      "author" : [ "M. Cai", "Y. Shi", "J. Liu" ],
      "venue" : "Proc. ASRU, 2013.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Improving deep neural network acoustic models using generalized maxout networks",
      "author" : [ "X. Zhang", "J. Trmal", "D. Povey", "S. Khudanpur" ],
      "venue" : "submitted to ICASSP 2014.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep convolutional neural networks for LVCSR",
      "author" : [ "T.N. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran" ],
      "venue" : "Proc. ICASSP, pp. 8614-8618, 2013.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convolutional Neural Networks for Language-Universal Feature Extraction and Cross-language Hybrid Systems",
      "author" : [ "Y. Miao", "F. Metze" ],
      "venue" : "submitted to ICASSP 2014.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Extracting deep bottleneck features using stacked auto-encoders",
      "author" : [ "J. Gehring", "Y. Miao", "F. Metze", "A. Waibel" ],
      "venue" : "Proc. ICASSP, pp. 3377-3381, 2013.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Modular Combination of Deep Neural Networks for Acoustic Modeling",
      "author" : [ "J. Gehring", "W. Lee", "K. Kilgour", "I. Lane", "Y. Miao", "A. Waibel" ],
      "venue" : "Proc. Interspeech, pp. 94-98, 2013.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep neural networks (DNNs) have shown superior performance over the traditional state-of-the-art GMMHMM on ASR tasks [1, 2].",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "Deep neural networks (DNNs) have shown superior performance over the traditional state-of-the-art GMMHMM on ASR tasks [1, 2].",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "This command trains stacked denoising autoencoders (SdAs) [3], mostly used from DNN pretraining.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "RBM training involves maximizing the likelihood of the observations with the contrastive divergence algorithm [4].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "Interested readers can refer to [5] for details on RBM.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "This distribution is governed by a pre-specified probability referred to as dropout factor in [6].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "For testing (recognition), network parameters need to be scaled properly according to the value of the dropout factor [6].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "Maxout: Previous works [7, 8, 9] have proposed to apply maxout networks to speech recognition.",
      "startOffset" : 23,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "Maxout: Previous works [7, 8, 9] have proposed to apply maxout networks to speech recognition.",
      "startOffset" : 23,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "Maxout: Previous works [7, 8, 9] have proposed to apply maxout networks to speech recognition.",
      "startOffset" : 23,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "CNNs can outperform DNNs on LVCSR tasks [10].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "Figure 3 shows our CNN architecture which works slightly different from the existing proposals [10].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "Interested readers can refer to [11] for more details.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "4 shows the Deep Bottleneck Feature (DBNF) architecture [12] which our recipes are using to extract bottleneck front-end.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "Hybrid system over spliced bottleneck features [13]",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "Architecture for the Deep Bottleneck Feature (DBNF) network [12].",
      "startOffset" : 60,
      "endOffset" : 64
    } ],
    "year" : 2014,
    "abstractText" : "The Kaldi 1 toolkit is becoming popular for constructing automated speech recognition (ASR) systems. Meanwhile, in recent years, deep neural networks (DNNs) have shown state-of-the-art performance on various ASR tasks. This document describes our recipes to implement fully-fledged DNN acoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning toolkit developed under the Theano environment. Using these recipes, we can build up multiple systems including DNN hybrid systems, convolutional neural network (CNN) systems and bottleneck feature systems. These recipes are directly based on the Kaldi Switchboard 110-hour setup. However, adapting them to new datasets is easy to achieve.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}