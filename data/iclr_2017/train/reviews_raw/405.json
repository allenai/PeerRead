{"conference": "ICLR 2017 conference submission", "title": "Recurrent Environment Simulators", "abstract": "Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.", "reviews": [{"is_meta_review": true, "comments": "The authors propose a recurrent neural network architecture that is able to output more accurate long-term predictions of several game environments than the current state-of-the-art.\nThe original network architecture was inspired by inability of previous methods to accurately predict many time-steps into the future,\nand their inability to jump directly to a future prediction without iterating through all intermediate states.\nThe authors have provided an extensive experimental evaluation on several benchmarks with promising results.\nIn general the paper is well written and quite clear in its explanations.\nDemonstrating that this kind of future state prediction is useful for 3D maze exploration is a plus.\n\n# Minor comments:\n`jumpy predictions have been developed in low-dimensional observation spaces' - cite relevant work in the paper.\n\n# Typos\nSection 3.1 - `this configuration is all experiments'", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "Quality, Clarity: \n  The paper is well written. Further revisions have been made upon the original.\n \n Originality, Significance:\n  The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. This is done using a mix of (a) architectural modifications; (b) jumpy predictions; and (c) particular training schemes. The experimental validation is extensive, now including additional comparisons suggested by reviewers.\n There is not complete consensus about the significance of the contributions, with one reviewer seeking additional technical novelty. Overall, the paper appears to provide interesting and very soundly-evaluated results, which likely promises to be the new standard for this type of prediction problem.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Reply to AnonReviewer4", "OTHER_KEYS": "Silvia Chiappa", "comments": "We have thought more deeply about the fact that the results in Bengio et al. appear to be in contradiction with ours, in the sense that the Always Sampling training scheme (corresponding to our 100% Pred. Frames training scheme) seems to perform worse than mixed schemes. We also had a chat with one of the authors of the Bengio et al. paper to get a better understanding of the methods and experiments. \nWe have reached the conclusion that the difference might be due to the fact that Bengio et al. focus on discrete problems and to the fact that we were mostly reasoning about long-term prediction, whilst Bengio et al. focus on shorter-term prediction (e.g., in Image Captioning Section, the average prediction length is 11 (this is not mentioned in the paper, but has been pointed out to us by one of the authors)) -- if we look at our results for short-term prediction only, they do not look so much different anymore.\n\nIn Figures 12-16 of the latest version of our paper, in addition to the prediction error at time-step 100, we also plot the prediction error at time-steps 5 and 10 for most games. We can notice that the prediction error with the 100% and the 0%-100% Pred. Frames training schemes (called here Schemes I and II) (red and dark green lines) is almost never lower than the prediction error with the other mixed schemes for time-steps 5 and 10, and often higher (see for example Fishing Derby in Fig. 13). The situation is different at time-step 100, where Schemes I and II are always almost preferable to the other mixed schemes. Therefore, by looking only at the error up to time-step 10, we would also prefer other schemes to Schemes I and II. The error is higher at lower time-steps with Schemes I and II as the objects are less sharply represented in the frames. In other words, these two schemes capture the global dynamics (as this enables better long-term prediction) at the expense of not representing the details very accurately, as lack of the details at earlier predictions do not harm subsequent predictions. In Bengio et al., where the problem is discrete, one error at the beginning of the sequence might lead to drastic errors at subsequent time-steps.\nWe thank the reviewer for this question, as he made us thinking more carefully about this point. We need to modify the paper to include as summary of this discussion.\n\nWe also thank the reviewer for pointing out that in \u201cNitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016\u201d, the unconditional model can perform jumpy prediction. We briefly mention this paper in the introduction at the moment, but we need to add a discussion. Notice that, in this paper, prediction is shorter-term than in our case, namely 10-13 time-steps ahead. We are not sure whether, at training time, the real or the generated frames are fed in the conditioned model. From the sentence \u201cAt test time we feed in the generated frame from the previous step without adding any noise. At training time we feed in the ground truth.\u201d in Section 2.3, it would look like the model is trained with feeding in the real frames, i.e. with the 0% Pred. Frames training scheme. This approach seems to work better than the unconditional model -- this is surprising to us, as it is in contradiction with our results.\n", "IS_META_REVIEW": false, "DATE": "20 Jan 2017", "is_meta_review": false}, {"TITLE": "Reply to Review from AnonReviewer3", "OTHER_KEYS": "Silvia Chiappa", "comments": "We tried to address the reviewer's concerns by adding a substantial amount of new results and discussion, mainly to the Appendix (notice that some experiments are still running, thus many figures will need to be updated). We highlighted the modifications in red. We did not finish making all the modifications: we mostly still need to improve the main text. However, we decided to send a first update to give the reviewers the time to digest the new material, and to make the remaining changes in the next few days.\n\nBelow we describe in some detail how we addressed the reviewer's comments.\n\n1. Modification to model architecture.\n\nTo address the reviewer's concern about lack of comparison of different ways of incorporating the action, we added a section in the Appendix (Section B.1.3, Figures 19 and 20) that compares the architecture used in the main text (combining the action a_t-1 only with h_t-1 in a multiplicative way), with 8 alternatives, namely combining a_t-1 with the frame before encoding (ConvA), combining a_t-1 only with the encoded frame s_t-1, combining a_t-1 with both h_t-1 and s_t-1 in 4 possible ways, having a multiplicative/additive interaction of a_t-1 with h_t-1, and considering the action as an additional input. \nFrom the figures and videos it looks like combining the action with the frame (dark green lines) is a bad idea, and that there is no much difference in performance among the remaining approaches, although combining a_t-1 with both h_t-1 and s_t-1 (W^h h_{t-1}xW^s s_{t-1}x W^{a} a_{t-1)) seems to be overall a bit better (notice that this architecture results in a quite different core than the traditional LSTM core).\nThe experiments are still running -- the experiments with considering the action as an additional input (light green lines) are behind the other experiments.\n\nWe also modified the section in the Appendix describing the difference between our direct action influence and the indirect action influence as in Oh et al. (now at the end of Section B.1.3). We changed Fig. 12 in the previous version with Figure 21 to show the difference in the other games in addition to Seaquest (we need to add Riverraid for which we want to use 2 subsequences of length T=15). \nDirect action influence performs similarly to indirect action influence in games with lower prediction error, such as Bowling, Freeway and Pong, but better in games with higher prediction error. The most striking difference is with Seaquest, which is most evident when looking at the videos (is is difficult to judge the difference in performance by looking at the prediction error only, that's why we added randomly selected videos from which the types of errors in the prediction can be visually understood). These results are computed using the best training scheme for Seaquest. The videos show that, with indirect action influence, the simulator models the fish quite poorly, even though using a training scheme that more strongly encourages long-term accuracy enables the simulator to learn the appearance of the fish better than with the training scheme of Oh et al. Therefore, these videos clearly show that the training scheme alone is not sufficient, our architecture plays a crucial role in substantially improving the prediction. Having indirect action influence also makes it more difficult to correctly update the score in some games such as Seaquest and Fishing Derby. Therefore, whilst the training scheme is the major responsible for improving long-term prediction, it needs to be combined with an appropriate architecture in order to work. \n\nAll together, the experiments in Section B.1.3 suggest that having a direct (as opposed to Oh at al.) and global (as opposed to ConvA) action influence on the state dynamics is much preferable. On the other hand, performance is less sensitive to different ways of imposing direct action influence. We need to improve the discussion in the paper about this point.\n\n\n2. Exploring the idea of jumpy predictions.\n\nTo address the reviewer's concerns about lack of a detailed analysis, we added a new section in the Appendix (Section B.2, Figures 25 and 26), and modified Figure 5(b) to include all games (except Breakout for which the prediction error is uninformative -- see the discussion about this game in Section B.1.1). Section B.2 compares different architectures, sequence lengths and number of subsequences (we plan to add a few more experiments in the next few weeks). \nIt is clear from the results that the jumpy simulator is much more sensitive to changes of structure, and therefore fragile, than the single-step simulator. However, we show that with the right training structure, the jumpy simulator can be accurate in many games. Having accurate and fast to use simulators is essential for model-based reinforcement learning. Thus, whilst the novelty might seems marginal, it is of extreme interest for the community to advance in this type of approaches. Furthermore, accurate jumpy approaches are very difficult to obtain -- we believe that we are the first ones showing some success for long-term prediction in large scale domains.\n\nWe do not understand the sentence 'However, it is again not clear whether the modification in the current model leads to this effect or it could be achieved by previous models like Oh et al.'. The model of Oh et al. cannot be modified to perform jumpy predictions as, when omitting intermediate frames, the corresponding actions do not have any influence (as they influence the hidden states indirectly through the frames). This is actually an important reason why a direct action influence is preferable.\n\n\n3. Exploring different training schemes.\n\nWe would like to comment on the sentence \u201cWhile this observation is interesting, this effect has been previously explored in detail in other works like schedule sampling (Bengio et al.) and to some extent in Oh et al.?\u201d\nThe schedule sampling work of Bengio et al. focuses on different applications, with discrete data and on shorter-term prediction (for example, the length of sequences in the Image Captioning example (Section 4.1) is 11 on average). In this applications, the 100% Pred. Frames training scheme (called Always Sampling) actually performs poorly. Thus, based on the results of Bengio et al., we would never use the 100% Pred. Frames. We show that, in simpler games, this is actually the scheme that performs best, and that, in more complex games, a scheme that is as close as possible to such a scheme is preferable. Our analysis (especially in the new material) shows in detail the characteristics of each scheme on both short-term and long-term prediction. Thus, we make a significant contribution in advancing the understanding of different training schemes in frame prediction.\nOh et al. used a mixed scheme without any explanation. Thus, it is totally unclear from their paper the importance of using a mixed scheme and what are the differences of different types of mixing. That's actually the reason that motivatesd us to perform a detailed analysis. \n\nTo improve the understanding of the results, we have added a section in the Appendix (Section B.1.1) that discusses the characteristics and effects of different training schemes on each game.\n\nIn conclusion, we addressed the reviewer's concerns on the analysis by adding more results and discussion. Regarding the concern on little novelty, we strongly disagree that this should be a concern. We greatly improved on the performance of previous methods (it is enough to compare the videos in Oh at al. with ours to realize that there is an enormous difference). Even more importantly, we improved the understanding of these methods with a detailed analysis. We believe that papers like our paper are as important for the community as papers that provide completely novel architectures without much analysis. \n\nFinally, the reviewer should understand that providing a detailed analysis and understanding is extremely challenging and time consuming, due to both the large scale and the complexity of the problem. Ours, whilst not fully comprehensive (as this is not possible), represents an enormous effort in trying to provide an in depth analysis and advance the understanding on the problem of high dimensional frame prediction.", "IS_META_REVIEW": false, "DATE": "16 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Review", "is_meta_review": false, "comments": "[UPDATE]\nAfter going through the response from the author and the revision, I increased my review score for two reasons.\n1. I thank the reviewers for further investigating the difference between yours and the other work (Scheduled sampling, Unsupervised learning using LSTM) and providing some insights about it.\nThis paper at least shows empirically that 100%-Pred scheme is better for high-dimensional video and for long-term predictions.\nIt would be good if the authors briefly discuss this in the final revision (either in the appendix or in the main text).\n\n2. The revised paper contains more comprehensive results than before.\nThe presented result and discussion in this paper will be quite useful to the research community as high-dimensional video prediction involves large-scale experiments that are computationally expensive.\n\n- Summary\nThis paper presents a new RNN architecture for action-conditional future prediction. The proposed architecture combines actions into the recurrent connection of the LSTM core, which performs better than the previous state-of-the-art architecture [Oh et al.]. The paper also explores and compares different architectures such as frame-dependent/independent mode and observation/prediction-dependent architectures. The experimental result shows that the proposed architecture with fully prediction-dependent training scheme achieves the state-of-the-art performance on several complex visual domains. It is also shown that the proposed prediction architecture can be used to improve exploration in a 3D environment.\n\n- Novelty\nThe novelty of the proposed architecture is not strong. The difference between [Oh et al.] and this work is that actions are combined into the LSTM in this paper, while actions are combined after LSTM in [Oh et al.]. The jumpy prediction was already introduced by [Srivastava et al.] in the deep learning area. \n\n- Experiment\nThe experiments are well-designed and thorough. Specifically, the paper evaluates different training schemes and compares different architectures using several rich domains (Atari, 3D worlds). Besides, the proposed method achieves the state-of-the-art results on many domains and presents an application for model-based exploration.\n\n- Clarity\nThe paper is well-written and easy to follow.\n\n- Overall \nAlthough the proposed architecture is not much novel, it achieves promising results on Atari games and 3D environments. In addition, the systematic evaluation of different architectures presented in the paper would be useful to the community.\n\n[Reference]\nNitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Some interesting experimental observations, but significance and novelty of proposed architecture needs better justification", "is_meta_review": false, "comments": "The paper presents an action-conditional recurrent network that can predict frames in video games hundreds of steps in the future. The paper claims three main contributions: \n1. modification to model architecture (used in Oh et al.) by using action at time t-1 to directly predict hidden state at t\n2. exploring the idea of jumpy predictions (predictions multiple frames in future without using intermediate frames)\n3. exploring different training schemes (trade-off between observation and prediction frames for training LSTM)\n\n1. modification to model architecture\n+ The motivation seems good that in past work (Oh et al.) the action at t-1 influences x_t, but not the state h_t of the LSTM. This could be fixed by making the LSTM state h_t dependent on a_{t-1}\n- However, this is of minor technical novelty. Also, as pointed in reviewer questions, a similar effect could be achieved by adding a_t-1 as an input to the LSTM at time t. This could be done without modifying the LSTM architecture as stated in the paper. While the authors claim that combining a_t-1 with h_t-1 and s_t-1 performs worse than the current method which combines a_t-1 only with h_t-1, I would have liked to see the empirical difference in combining a_t-1 only with s_t-1 or only with h_t-1. Also, a stronger motivation is required to support the current formulation.\n- Further, the benefits of this change in architecture is not well analyzed in experiments. Fig. 5(a) provides the difference between Oh et al. (with traditional LSTM) and current method. However, the performance difference is composed of 2 components (difference in training scheme and architecture). This contribution of the architecture to the performance is not clear from this experiment. The authors did claim in the pre-review phase that Fig. 12 (a) shows the difference in performance only due to architecture for \"Seaquest\". However, from this plot it appears that the gain at 100-steps (~15)  is only a small fraction of the overall gain in Fig. 5 (a) (~90). It is difficult to judge the significance of the architecture modification from this result for one game.\n\n2. Exploring the idea of jumpy predictions:\n+ As stated by the authors, omitting the intermediate frames while predicting future frames could significantly sppedup simulations.\n+ The results in Fig. 5(b) present some interesting observations that omitting intermediate frames does not lead to significant error-increase for at least a few games.\n- However, it is again not clear whether the modification in the current model leads to this effect or it could be achieved by previous models like Oh et al.\n- While, the observations themselves are interesting, it would have been better to provide a more detailed analysis for more games. Also, the novelty in dropping intermediate frames for speedup is marginal.\n\n3. Exploring different training schemes\n+ This is perhaps the most interesting observation presented in the paper. The authors present the difference in performance for different training schemes in Fig. 2(a). The training schemes are varied based on the fraction of training phase which only uses observation frames and the fraction that uses only prediction frames.\n+ The results show that this change in training can significantly affect prediction results and is the biggest contributor to performance improvement compared to Oh et al.\n- While this observation is interesting, this effect has been previously explored in detail in other works like schedule sampling (Bengio et al.) and to some extent in Oh et al.\n\nClarity of presentation:\n- The exact experimental setup is not clearly stated for some of the results. For instance, the paper does not say that Fig. 2(a) uses the same architecture as Oh et al. However, this is stated in the response to reviewer questions.\n- Fig. 4 is difficult to interpret. The qualitative difference between Oh et al. and current method could be highlighted explicitly. \n- Minor: The qualitative analysis section requires the reader to navigate to various video-links in order to understand the section. This leads to a discontinuity in reading and is particularly difficult while reading a printed-copy.\n\nOverall, the paper presents some interesting experimental observations. However, the technical novelty and contribution of the proposed architecture and training scheme is not clear.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Review", "is_meta_review": false, "comments": "The authors propose a recurrent neural network architecture that is able to output more accurate long-term predictions of several game environments than the current state-of-the-art.\nThe original network architecture was inspired by inability of previous methods to accurately predict many time-steps into the future,\nand their inability to jump directly to a future prediction without iterating through all intermediate states.\nThe authors have provided an extensive experimental evaluation on several benchmarks with promising results.\nIn general the paper is well written and quite clear in its explanations.\nDemonstrating that this kind of future state prediction is useful for 3D maze exploration is a plus.\n\n# Minor comments:\n`jumpy predictions have been developed in low-dimensional observation spaces' - cite relevant work in the paper.\n\n# Typos\nSection 3.1 - `this configuration is all experiments'", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Few questions about motivation of formulation", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "07 Dec 2016", "is_meta_review": false}, {"TITLE": "Some questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Jumpy predictions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "29 Nov 2016", "is_meta_review": false}], "SCORE": 8, "authors": "Silvia Chiappa, S\u00e9bastien Racaniere, Daan Wierstra, Shakir Mohamed", "accepted": true, "id": ""}
