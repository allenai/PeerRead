{
  "name" : "1611.08998.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DeepSetNet: Predicting Sets with Deep Neural Networks",
    "authors" : [ "Seyed Hamid Rezatofighi", "Vijay Kumar", "B G Anton", "Milan Ehsan Abbasnejad", "Anthony Dick", "Ian Reid" ],
    "emails" : [ "hamid.rezatofighi@adelaide.edu.au" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20]. However, traditional convolutional architectures require a problem to be formulated in a certain way: in particular, they are designed to predict a vector (or a matrix, or a tensor in a more general sense), that is either of a fixed length or whose size depends on the input.\nFor example, consider the task of scene classification where the goal is to predict the label (or category) of a given image. Modern approaches typically address this by a series of convolutional layers, followed by a number of fully connected layers, which are finally mapped to predict a fixedsized vector [17, 30, 33]. The length of the predicted vector corresponds to the number of candidate categories, e.g. 1,000 for the ImageNet challenge [28]. Each element is a score or probability of a particular category, and the final prediction is a probability distribution over all categories. This strategy is perfectly admissible if one expects to find exactly one or at least the same number of categories across all images. However, natural images typically show multiple entities (e.g. table, pizza, person, etc.), and what is\nperhaps more important, this number differs from image to image. During evaluation, this property is not taken into account. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) only counts an error if the “true” label is not among the top-5 candidates. Another strategy to account for multiple classes is to fix the number to a certain value for all test instances, and report precision and recall by counting false positive and false negative predictions, as was done in [11, 38]. Arguably, both methods are suboptimal because in a real-world scenario, where the correct labelling is unknown, the prediction should in fact not only rank all labels according to their likelihood of being present, but also to report how many objects (or labels) are actually present in one particular image. Deciding how many objects are actually present in an image is a crucial part of human scene understanding that is missing from our current evaluation of automated image understanding methods.\nAs a second example, let us turn to object detection, and in particular pedestrian detection. The parallel to scene classification that we motivated above is that, once again, in real scenarios, the number of people in a particular scene is not known beforehand. The most common approach is to assign a confidence score to a number of region candidates [4, 8, 10, 27], which are typically selected heuristically by thresholding and non-maxima suppression. We ar-\n1\nar X\niv :1\n61 1.\n08 99\n8v 1\n[ cs\n.C V\n] 2\n8 N\nov 2\n01 6\ngue that it is important not to simply discard the information about the actual number of objects at test time, but to exploit it while selecting the subset of region proposals.\nThe examples above motivate and underline the importance of set prediction in certain applications. It is important to note that, in contrast to vectors, a set is a collection of elements which is invariant under permutation. One naive way of extracting a set from a distribution is to apply thresholding. One single global threshold, however, cannot produce the desired effect because it is highly dependent on each data sample and should therefore be adjusted for each instance. However, an adaptive threshold per training instance cannot be learned in a straightforward manner as it is not directly observable from the training data. Instead, we use a principled definition of a set as the union of cardinality distribution and family of joint distributions over each cardinality value.\nIn summary, our main contributions include:\n• Starting from the mathematical definition of a set distribution, we derive a loss that enables us to employ existing machine learning methodology to learn this distribution from data.\n• We integrate our loss into a deep learning framework to exploit the power of a multi-layer architecture.\n• We present state-of-the-art results on standard datasets on the tasks of multi-label image classification and pedestrian detection."
    }, {
      "heading" : "2. Related Work",
      "text" : "A sudden success in multiple applications including voice recognition [13], machine translation [32] and image classification [17], has sparked the deployment of deep learning methods throughout numerous research areas. Deep convolutional (CNN) and recurrent (RNN) neural networks now outperform traditional approaches in tasks like semantic segmentation [3], image captioning [16] or object detection [20]. Here, we will briefly review some of the recent approaches to image classification and object detections and point out their limitations.\nImage or scene classification is a fundamental task of understanding photographs. The goal here is to predict a scene label for a given image. Early datasets, such as Caltech-101 [7], mostly contained one single object and could easily be described by one category. Consequently, a large body of literature focused on single-class prediction [17, 29, 40, 23]. However, real-world photographs typically contain a collection of multiple objects and should therefore be captioned with multiple tags.\nSurprisingly, there exists rather little work on multi-class image classification that makes use of deep architectures. Gong et al. [12] combine deep CNNs with a top-k approximate ranking loss to predict multiple labels. Wei et al. [39]\npropose a Hypotheses-Pooling architecture that is specifically designed to handle multi-label output. While both methods open a promising direction, their underlying architectures largely ignore the correlation between multiple labels. To address this limitation, recently, Wang et al. [38] proposed a model that combines CNNs and RNNs (convolutional and recurrent networks) to predict a number classes in a sequential manner. RNNs, however, are not suitable for set prediction mainly for two reasons. First, the output represents a sequence and is thus highly dependent on the prediction order, as was shown recently by Vinyals et al. [34]. Second, the final prediction may not result in a feasible solution (e.g. it may contain the same element multiple times), such that post-processing or heuristics such as beam search must be employed [35, 38]. Here we show that our approach not only guarantees to always predict a valid set, but also outperforms previous methods.\nPedestrian detection can also be viewed as a classification problem. Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person. More recent methods, such as Fast R-CNN [10] or the single-shot multi-box detector (SSD) [20] learn the relevant image features rather than manually engineering them, but retain the sliding window approach.\nAll the above approaches require some form of postprocessing to suppress spurious detection responses that originate from the same person. This is typically addressed by non-maximum suppression (NMS), a greedy optimization strategy with a fixed overlap threshold. Recently, several alternatives have been proposed to replace the greedy NMS procedure. Russel et al. [31] perform end-to-end head detection by predicting the bounding boxes sequentially using an LSTM [14]. Their approach, however, processes only a small image region at a time, thus limiting the applicability on crowded scenarios with tens or hundreds of people. Pham et al. [26] and Lee et al. [18] formulate NMS as a global optimisation problem while Hosang et al. [15] propose to learn the NMS algorithm end-to-end using CNNs. Both methods, however, do not consider the number of objects while selecting the final set of boxes. Contrary to existing pedestrian detection approaches, we incorporate the cardinality into the NMS algorithm itself. This leads to an improvement over the state of the art, validated on two benchmarks."
    }, {
      "heading" : "3. Random Vectors vs. Random Finite Sets",
      "text" : "To explain our approach, we first review some mathematical background and introduce the notation used throughout the paper.\nIn statistics, a continuous random variable y is a variable that can take an infinite number of possible values.\nA continuous random vector can be defined by stacking several continuous random variables into a vector, Y = (y1, · · · , ym). The mathematical function describing the possible values of a continuous random vector, and their associated joint probabilities, is known as a probability density function (PDF) p(Y ) such that ∫ p(Y )dY = 1.\nA random finite set (RFS) Y is a finite-set valued random variable Y = {y1, · · · , ym}. The main difference between an RFS and a random vector is that for the former, the number of constituent variables is random and the variables themselves are random and unordered.\nA statistical function describing a finite-set variable Y is a combinatorial probability density function p(Y), which consists of a discrete probability distribution, the so-called cardinality distribution, and a family of joint probability densities on both the number and the value of the constituent variables.\nSimilar to the definition of a PDF for a random variable, the PDF of an RFS must sum to unity over all possible cardinality values and all possible element values and their permutations, i.e.∫\np(Y)δY , p(∅)+ ∞∑ m=1 1 m! ∫ p({y1, · · · , ym})dy1 · · · dym = 1,\n(1) where f(∅) is the probability of the empty set. The PDF of anm-dimensional random vector can be defined in terms of an RFS as:\np(y1, · · · , ym) , 1\nm! p({y1, · · · , ym}). (2)\nThe factor m! = ∏m k=1 k appears because the probability density for a set {y1, · · · , ym} must be equally distributed among all the m! possible permutations of the vector [21].\nConventional machine learning approaches, such as Bayesian learning and convolutional neural networks, have been proposed to learn the optimal parameters θ∗ of the distribution p(Y |θ∗,x) which maps the input vector x to the output vector Y . In this paper, we instead propose an approach that can learn a set of parameters (θ∗,w∗) for a set distribution that allow one to map the input vector x into the output set Y , i.e. p(Y|θ∗,w∗,x). The additional parameters w define a PDF over the set cardinality, as we explain in the next section."
    }, {
      "heading" : "4. Deep Set Network",
      "text" : "Let us begin by defining a training set D = {Yi,xi}, where each training sample i = 1, . . . , n is a pair consisting of an input feature xi ∈ Rl and an output (or label) set Yi = {y1,y2, . . . ,ym},yk ∈ Rd. In the following we will drop the instance index i for better readability. Note that\nm := |Y| denotes the cardinality of set Y . The probability of a set Y is defined as:\np(Y|θ,w,x) =p(m|w,x)× m!× p(y1,y2, · · · ,ym|θ,x), (3)\nwhere θ denotes the parameters of yk that estimates the distribution of set element values for a fixed cardinality1, while w represents the collection of parameters which estimate the cardinality distribution of the set elements. For example, if the outputs (or labels) in the set are independent and identically distributed (i.i.d) and their cardinality follows a Poisson distribution, we can write the likelihood as\np(Y|θ,w,x) = ∫ p(m|λ)p(λ|x,w)dλ×\nm!× ( m∏ k=1 p(yk|θ,x) ) .\n(4)"
    }, {
      "heading" : "4.1. Posterior distribution",
      "text" : "To learn the parameters θ and w, we first define the posterior distribution over them as\np(θ,w|D) ∝ p(D|θ,w)p(θ)p(w)\n∝ n∏ i=1 [∫ p(mi|λ)p(λ|xi,w)dλ×\nmi!× ( mi∏ k=1 p(yk|θ,xi) )] p(xi)p(θ)p(w).\n(5) 1This is also known as the spatial distribution of points in point process\nstatistics.\nA closed form solution for the integral in Eq. (20) can be obtained by using conjugate priors:\nm ∼ P(m;λ) λ ∼ G(λ;α(x,w), β(x,w))\nα(x,w), β(x,w) > 0 ∀x,w θ ∼ N (θ; 0, σ21I) w ∼ N (w; 0, σ22I),\nwhere P(·, λ), G(·;α, β), and N (·; 0, σ2I) represent respectively a Poisson distribution with parameters λ, a Gamma distribution with parameters (α, β) and a zero mean normal distribution with covariance equal to σ2I.\nWe assume that the cardinality follows a Poisson distribution whose mean, λ, follows a Gamma distribution, with parameters which can be estimated from the input data x. Consequently, the integrals in p(θ,w|D) are simplified and form a negative binomial distribution,\nNB (m; a, b) = Γ(m+ a)\nΓ(m+ 1)Γ(a) .(1− b)abm, (6)\nwhere Γ is the Gamma function. Finally, the full posterior distribution can be written as\np(θ,w|D) ∝ n∏ i=1 [ NB ( mi;α(xi,w),\n1\n1 + β(xi,w)\n)\n×mi!× ( mi∏ k=1 p(yk|θ,xi) )] p(θ)p(w).\n(7)"
    }, {
      "heading" : "4.2. Learning",
      "text" : "For simplicity, we use a point estimate for the posterior p(θ,w|D), i.e. p(θ,w|D) = δ(θ = θ∗,w = w∗|D), where (θ∗,w∗) are computed using the following MAP estimator:\n(θ∗,w∗) = arg max θ,w log (p (θ,w|D)) . (8)\nThe optimisation problem in Eq. (24) can be decomposed w.r.t. the parameters θ and w. Therefore, we can learn them independently as\nθ∗ = arg max θ γ1‖θ‖+ n∑ i=1 mi∑ k=1 log (p(yk|θ,xi)) (9)\nand\nw∗ = arg max w n∑ i=1 [ log ( Γ(mi + α(xi,w)) Γ(mi + 1)Γ(α(xi,w)) ) +log ( β(xi,w) α(xi,w)\n(1 + β(xi,w)α(xi,w)+mi)\n)] + γ2‖w‖,\n(10)\nwhere γ1 and γ2 are the regularisation parameters, proportional to the predefined covariance parameters σ1 and σ2. These parameters are also known as weight decay parameters and commonly used in training neural networks.\nThe learned parameters θ∗ in Eq. (9) are used to map an input feature vector x into an output vector Y . For example, in image classification, θ∗ is used to predict the distribution Y over all categories, given the input image x. Note that θ∗ can generally be learned using a number of existing machine learning techniques. In this paper we rely on deep CNNs to perform this task.\nTo learn the highly complex function between the input feature x and the parameters (α, β), which are used for estimating the output cardinality distribution, we train a second deep neural network. Using neural networks to predict a discrete value may seem surprising, because these methods at their core rely on the backpropagation algorithm, which assumes a differentiable loss. Note that we achieve this by describing the discrete distribution by continuous parameters α, β (Negative binomial NB(·, α, 11+β )), and can then easily draw discrete samples from that distribution. More formally, to estimate w∗, we compute the partial derivatives of the objective function in Eq. (27) w.r.t. α(·, ·) and β(·, ·) and use standard backpropagation to learn the parameters of the deep neural network.\nWe refer the reader to the supplementary material for the complete derivation of the partial derivatives, a more detailed derivation of the posterior in Eqs. (20)-(23) and the proof for decomposition of the MAP estimation in Eq. (24)."
    }, {
      "heading" : "4.3. Inference",
      "text" : "Having the learned parameters of the network (w∗,θ∗), for a test feature x∗, we use a MAP estimate to generate a set output as\nY∗ = arg max Y p(Y∗|D,x∗), (11)\nwhere p(Y∗|D,x∗) = ∫ p(Y∗|θ,w,x∗)p(θ,w|D)dθdw\nand p(θ,w|D) = δ(θ = θ∗,w = w∗|D). To calculate the mode of the set distribution p(Y∗|D,x∗), we first need to calculate the mode m∗ of the cardinality distribution\nm∗ = arg max m p(m|w∗,x∗), (12)\nwhere p(m|w∗,x∗) = NB ( m;α(w∗,x∗), 11+β(w∗,x∗) ) . Then, we calculate the mode of the joint distribution for the given cardinality m∗ as\nY∗ = arg max Ym∗ p({y1, · · · , ym∗}|θ∗,x∗). (13)\nIt can be seen from Eq. (15) that the mode of p({y1, · · · , ym∗}|θ∗,x∗) and p(y1, · · · , ym∗ |θ∗,x∗) needs to be identical w.r.t. a fixed permutation. Since the output of the first CNN with the parameters θ∗ is already the mode of p(y1, · · · , yM |θ∗,x∗) and the samples are i.i.d., we use the m∗ highest values from the output of the first CNN to generate the final set output Y∗."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "To evaluate the performance of our proposed deep set network, we perform experiments on two separate and relevant applications: multi-label image classification and pedestrian detection."
    }, {
      "heading" : "5.1. Multi-label Image Classification",
      "text" : "In this section, we evaluate our approach on the task of multi-label image classification. As opposed to the more common and more studied problem of (single-label) image classification, the task here is rather to label a photograph with an arbitrary, a-priori unknown number of tags. We perform experiments on two standard benchmarks, the PASCAL VOC 2007 dataset [6] and the Microsoft Common Objects in Context (MS COCO) dataset [19].\nImplementation details. In this experiment, similar to [38], we build on the 16-layers VGG network [30], pretrained on the 2012 ImageNet dataset. We adapt VGG for our purpose by modifying the last fully connected prediction layer to predict 20 classes for PASCAL VOC, and 80 classes for MS COCO. We then fine-tune the entire network for each of these datasets using two commonly used losses for multi-label classification, softmax and binary cross-entropy (BCE)2 [11, 38]. To learn both classifiers, we set the weight decay to 5 ·10−4, with a momentum of 0.9 and a dropout rate of 0.5. The learning rate is adjusted to gradually decrease after each epoch, starting from 0.01 for softmax and from 0.001 for binary cross-entropy. The learned parameters of these classifiers correspond to θ∗ for our proposed deep set network (cf . Eq. (9) and Fig. 2). To learn the cardinality distribution, we use the same VGG-16 network as above and modify the final fully connected layer to predict 2 values for α and β. It is important to note, that the predicted values must be positive to describe a valid Gamma distribution. We therefore also append two weighted sigmoid transfer functions with weights αM , βM to ensure that the values predicted for α and β are in a valid range. Our model is not sensitive to these parameters and we set their values to be large enough (αM = 160 and βM = 20) to guarantee that the mode of the distribution can accommodate the largest cardinality existing in the\n2Weighted Approximate Ranking (WARP) objective is another commonly used loss for multi-label classification. However, it does not perform as well as softmax and binary cross-entropy for the used datasets [38].\ndataset. We then fine-tune the network on cardinality distribution using the objective loss defined in Eq. (27). To train the cardinality CNN, we set a constant learning rate 0.001, weight decay 5·10−12, momentum rate 0.9 and dropout 0.5.\nEvaluation protocol. To evaluate the performance of the classifiers and our deep set network, we employ the commonly used evaluation metrics for multi-label image classification [11, 38]: precision and recall of the generated labels per-class (C-P and C-R) and overall (O-P and O-R). Precision is defined as the ratio of correctly predicted labels and total predicted labels, while recall is the ratio of correctly predicted labels and ground-truth labels. In case no predictions (or ground truth) labels exist, i.e. the denominator becomes zero, precision (or recall) is defined as %100. To generate the predicted labels for a particular image , we perform a forward pass of the CNN and choose top-k labels according to their scores similar to [11, 38]. Since the classifier always predicts a fixed-sized prediction for all categories, we sweep k from 0 to the maximum number of classes to generate a precision/recall curve. However, for our proposed DeepSet Network, the number of labels per instance is predicted from the cardinality network. Therefore, prediction/recall is not dependent on value k and one single precision/recall value can be computed.\nTo calculate the per class and overall precision and recall, their average values over all classes and all examples are respectively computed. In addition, we also report the F1 score (the harmonic mean of precision and recall) averaged over all classes (C-F1) and all instances and classes (O-F1).\nPASCAL VOC 2007. The Pascal Visual Object Classes (VOC) [6] benchmark is one of the most widely used datasets for detection and classification. It consists of 9963 images with a 50/50 split for training and test, where objects from 20 pre-defined categories have been annotated by bounding boxes. Each image may contain between 1 and 7 unique objects.\nWe compare our results with a state-of-the-art classifier as described above. The resulting precision/recall plots are shown in Fig. 3(a) together with our proposed approach using the estimated cardinality. Note that by enforcing the correct cardinality for each image, we are able to clearly outperform the baseline w.r.t. both measures. Note also that our prediction (+) can nearly replicate the oracle (∗), where the ground truth cardinality is known. The mean absolute cardinality error of our prediction on PASCAL VOC is 0.32± 0.52.\nMicrosoft COCO. Another popular benchmark for image captioning, recognition, and segmentation is the recent Microsoft Common Objects in Context (MS-COCO) [19]. The dataset consists of 123 thousand images, each labelled\nwith per instance segmentation masks of 80 classes. The number of unique objects for each image can vary between 0 and 18. Around 700 images in the training dataset do not contain any of the 80 classes and there are only a handful of images that have more than 10 tags. The majority of the images contain between one and three labels. We use 82783 images as training and validation split (%90-10%), and the remaining 40504 images as test data. We predict the cardinality of objects in the scene with a mean absolute error of 0.74 and a standard deviation of 0.86.\nFig. 3(b) shows a significant improvement of precision and recall and consequently the F1 score using our deep set network compared to the softmax and binary cross-entropy classifiers for all ranking values k. We also outperform the state-of-the art multi-label classifier CNN-RNN [38], for the reported value of k = 3 in Table 1. Our results show around 10 percentage points improvement for the F1 score on top of the baseline classifiers and about 3 percentage points improvement compared to the state of the art on this dataset. Examples of perfect label prediction using our proposed approach are shown in Fig. 8. The deep set network can properly recognise images with no labels at all, as well as images with many tags. We also investigated failure cases where either the cardinality CNN or the classifier fails to make a correct prediction. We showcase some of these cases in Fig 9. We argue here that some of the failure cases are simply due to a missed ground truth annotation, such as the left-most example, but some are actually semantically correct w.r.t. the cardinality prediction, but are penalized during evaluation because a particular object category is not available in the dataset. This is best illustrated in the second example in Fig. 9. Here, our network cor-\nrectly predicts the number of objects in the scene, which is two, however, the can does not belong to any of the 80 categories in the dataset and is thus not annotated. Similar situations also appear in other images further to the right."
    }, {
      "heading" : "5.2. Pedestrian Detection",
      "text" : "To demonstrate the generality of our approach, we test it on an entirely different setting of pedestrian detection. We perform experiments on two widely used datasets, Caltech Pedestrians [5] and MOT16, the 2016 edition of the MOTChallenge benchmark [22]. To push the state-of-the art performance on pedestrian detection, we chose the leading detector on Caltech Pedestrians, which is the recent the multi-scale deep CNN approach (MS-CNN) [2]. Note that we do not retrain the detector, but are still able to improve its performance by predicting the number of pedestrians in each frame.\nTo learn the cardinality distribution, we use a gray-scale image as the network input, constructed by superimposing all region proposals and their scores generated by MS-CNN detector (before non-maximum suppression approach). We found, that this input provides a stronger signal than the raw RGB images, yielding better results. We build on top of the well-known AlexNet [17] architecture, and replace the first convolutional layer with a single channel filter and last fully connected layer with 2 layers output followed by two weighted sigmoid activation function, similar to the case above (cf . Sec. 5.1). The weights are initialised randomly and the network is trained for 100 epochs using the objective loss proposed in Eq. (27). We set a constant learning rate 10−3, weight decay 5 · 10−4, momentum rate 0.9 and dropout rate 0.5. The checkpoint with the lowest objective on the validation set is chosen for prediction.\nNon-maximum suppression. To generate the final detection outputs, most detectors often rely on non-maximum suppression, which greedily picks the boxes with highest scores and suppresses any boxes that overlap more than a pre-defined threshold TO. In fact, choosing the right threshold is crucial and can have a large impact on the overall performance, as shown e.g. in [31]. However, there may not exist a single value TO, rather it highly depends on the setting. We argue that it should therefore be adjusted for each frame separately. To that end, we use the prediction on the number of people (m) in the scene to choose an adap-\ntive NMS threshold for each image. In particular, we start from the default value of TO, and adjust it step-wise until the number of boxes reaches m. In the case if the number of final boxes is larger than m, we pick m boxes with the highest scores. For a fair comparison, we also find the best (global) value for TO for our baseline, the MS-CNN detector.\nEvaluation metrics. To quantify the detection performance, we adapt the same evaluation metrics and follow the protocols used on the Caltech detection benchmark [5]. The evaluation metrics used here are log-average miss rate (MR) over false positive per image. Additionally, we compute the F1 score (the harmonic mean of precision recall). The F1 score is computed from all detections predicted from our DeepSet network and is compared with the highest F1 score along the MS-CNN precision-recall curve. To calculate MR, we concatenate all boxes resulted from our adaptive NMS approach and change the threshold over all scores from our predicted sets.\nCaltech Pedestrians [5] is a de-facto standard benchmark for pedestrian detection. The dataset contains sequences captured from a vehicle driving through regular traffic in an urban environment and provides bounding box annotations of nearly 350, 000 pedestrians. The annotations also includes detailed occlusion labels. The number of pedestrians per image varies between 0 and 14. How-\never, more than 55% of the images contain no people at all and around 30% of the data includes one or two persons. We use the MS-CNN [2] network model and its parameters learned on the Caltech training set as θ∗ in Eq. (9). To learn the cardinality, we use 4250 images provided as a training set, splitting it into training and validation (80% − 20%), reaching a mean absolute error of 0.54 (cf . Tab. 2). Quantitative detection results are shown in Tab. 3. We achieve state-of-the art performance on this benchmark, improving on the currently best detector by applying our DeepSet network to estimate the number of persons in each image. The improvement, however, is not as significant as in the case of multi-class image classification. We believe that this is mainly due to very limited variation of the number of pedestrians in the scene. For more than 85% of the images containing 0 to 2 pedestrians. A single global threshold for a state-of-the art detection output appears to work almost as well as introducing the knowledge about the number of persons in the scene. Another reason for the relatively low improvement maybe the already remarkable performance of modern detectors on this dataset (cf . [5]).\nMOTCallenge 2016. Additionally, we evaluate the MSCNN detector and DeepSet in significantly more crowded and more challenging sequences from MOT16 dataset [22]. This benchmark targeted at multi-object tracking and is not yet commonly used for evaluating the pedestrian detection. However the variation in the number of pedestrians across\nthe frames is relatively large (between 0 and 32) and is also distributed more uniformly, which makes correct cardinality estimation more important. Since the labels for the test set are not available, we use the provided training set of this benchmark consisting of 5316 images from 7 different sequences, and divide it into training, validation and test set with split ratios 60%, 15% and 25%, respectively. We only learn the cardinality network w∗ on training set and we use the MS-CNN network model and its parameters learned on the KITTI dataset [9] as θ∗ in Eq. (9). The results are summarised in Tab. 3 and shows a more significant improvement over MS-CNN on this dataset compared to Caltech. We believe that these results can be improved further by formulating a more principled way of choosing the m predicted targets in the scene, as opposed to relying on the greedy NMS heuristic."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We proposed a deep learning approach for predicting sets. To achieve this goal, we derived a loss for predicting a discrete distribution over the set cardinality. This allowed us to use standard backpropagation for training a deep network for set prediction. We have demonstrated the effectiveness of this approach on multi-class image classification\nand pedestrian detection, achieving state-of-the-art results in both applications. As our network is trained independently, it can be trivially applied to any existing classifier or detector, to further improve performance.\nIn future, we plan to extend our model to multi-class cardinality estimation, extending its application to general object detectors. Another potential avenue could be to exploit the Bayesian nature of the model to include uncertainty as opposed to only relying on the MAP estimation. Finally, we have only considered set outputs. We believe a promising direction for a follow up work is to extend our idea to also handle set inputs, which can then be applied to problems like graph matching."
    }, {
      "heading" : "A. Background on Finite Set Statistics",
      "text" : "Finite Set Statistics provides powerful and practical mathematical tools for dealing with random finite sets, based on the notion of integration and density that is consistent with the point process theory [21]. In this section, we review some basic mathematical background about this subject of statistics.\nIn the conventional statistics theory, a continuous random variable y is a variable that can take an infinite number of possible values. A continuous random vector can be defined by stacking several continuous random variables into a fixed length vector, Y = (y1, · · · , ym). The mathematical function describing the possible values of a continuous random vector, and their associated joint probabilities, is known as a probability density function (PDF) p(Y ) such that ∫ p(Y )dY = 1.\nA random finite set (RFS) Y is a finite-set valued random variable Y = {y1, · · · , ym}. The main difference between an RFS and a random vector is that for the former, the number of constituent variables is random and the variables themselves are random and unordered, while the latter is of a fixed size with a predefined order.\nA statistical function describing a finite-set variable Y is a combinatorial probability density function p(Y), which consists of a discrete probability distribution, the so-called cardinality distribution, and a family of joint probability densities on the values of the constituent variables for each cardinality. Similar to the definition of a PDF for a random variable, the PDF of an RFS must sum to unity over all possible cardinality values and all possible element values and their permutations, i.e.∫\np(Y)δY , p(∅) + ∞∑ m=1 1 m! ∫ p({y1, · · · , ym})dy1 · · · dym = 1, (14)\nwhere f(∅) is the probability of the empty set. The PDF of an m-dimensional random vector can be defined in terms of an RFS as:\np(y1, · · · , ym) , 1\nm! p({y1, · · · , ym}). (15)\nThe factor m! = ∏m k=1 k appears because the probability density for a set {y1, · · · , ym} must be equally distributed among all the m! possible permutations of the vector [21]. The cardinality distribution p(m) over the number of elements in the random finite set Y is attained by\np(m) = ∫ |Y|=m p(Y)δY , 1 m! ∫ p({y1, · · · , ym})dy1 · · · dym. (16)\nSimilar to the conventional statistics for random variables, the expectation of an RFS has been defined above. The first statistical moment, or the expected value, of an RFS is known as intensity density or probability hypothesis density (PHD) and is calculated by definition as\nv(y) , ∫ δY(y)p(Y)δY, (17)\nwhere δY(y) = ∑ x∈Y δx(y) and δx(y) denotes the Dirac delta function concentrated at x. The PHD function v(y) is interpreted as the instantaneous expected number of the variables that exist at that point y. Moreover, the integral of the PHD over a region gives the expected number of elements in that region and the peaks of the PHD indicate highest local concentrations of the expected number of elements.\nHaving an RFS distribtuion p(Y), the samples can be drawn from this distribution as shown in Algorithm 4. The mode of this distribution is defined as the mode of the cardinality distribution p(m) and the mode of the joint distribution for m∗ elements, i.e. p({y1, · · · , ym∗}) ."
    }, {
      "heading" : "B. Deep Set Network",
      "text" : "Let us begin by defining a training set D = {Yi,xi}, where each training sample i = 1, . . . , n is a pair consisting of an input feature xi ∈ Rl and an output (or label) set Yi = {y1,y2, . . . ,ym},yk ∈ Rd. In the following we will drop the\nAlgorithm 4: Drawing samples from a set distribution.\nSampling an RFS Probability Distribution • Initialize Y ← ∅ • Sample cardinality m ∼ p(m) • Sample m points from an m-dimensional joint distribution\nY ∼ p({y1, y2, · · · .ym})← m!× p(y1, y2, · · · .ym) In the case of i.i.d. samples:\nfor i← {1, . . . ,m} sample yi ∼ p(y) set Y ← Y ∪ yi end\ninstance index i for better readability. Note that m := |Y| denotes the cardinality of set Y . The probability of a set Y with an unknown cardinality is defined as:\np(Y|θ,w,x) =p(m|w,x)× p({y1,y2, · · · ,ym}|θ,x) =p(m|w,x)×m!× p(y1,y2, · · · ,ym|θ,x),\n(18)\nwhere θ denotes the parameters of yk that estimates the distribution of set element values for a fixed cardinality, while w represents the collection of parameters which estimate the cardinality distribution of the set elements. For example, if the outputs (or labels) in the set are independent and identically distributed (i.i.d) and their cardinality follows a Poisson distribution, we can write the likelihood as\np(Y|θ,w,x) = ∫ p(m|λ)p(λ|x,w)dλ×m!× ( m∏ k=1 p(yk|θ,x) ) . (19)\nB.1. Posterior distribution\nTo learn the parameters θ and w, it is valid to assume that the training samples are independent from each other and the distribution over the input data p(x) is independent from the output and the parameters. Therefore, the posterior distribution over the parameters can be derived as\np(θ,w|D) = 1 Z p(D|θ,w)p(θ)p(w)\n= 1\nZ p({Yi,xi}∀i|θ,w)p(θ)p(w)\n= 1\nZ n∏ i=1 [ p(Yi|θ,w,xi)p(xi) ] p(θ)p(w)\n= 1\nZ n∏ i=1\n[∫ p(mi|λ)p(λ|xi,w)dλ×mi!× ( mi∏ k=1 p(yk|θ,xi) ) p(xi) ] p(θ)p(w),\n(20)\nwhere Z is a normalizer defined as\nZ = ∫ ∫ n∏ i=1 [∫ p(mi|λ)p(λ|xi,w)dλ×mi!× ( mi∏ k=1 p(yk|θ,xi) ) p(xi) ] p(θ)p(w) dθdw.\nThe probability p(xi) can be eliminated as it appears in both the numerator and the denominator. Therefore,\np(θ,w|D) = 1 Z̃ n∏ i=1\n[∫ p(mi|λ)p(λ|xi,w)dλ×mi!× ( mi∏ k=1 p(yk|θ,xi) )] p(θ)p(w), (21)\nwhere\nZ̃ = ∫ ∫ n∏ i=1 [∫ p(mi|λ)p(λ|xi,w)dλ×mi!× ( mi∏ k=1 p(yk|θ,xi) )] p(θ)p(w) dθdw.\nA closed form solution for the integral in Eq. (21) can be obtained by using conjugate priors:\nm ∼ P(m;λ) λ ∼ G(λ;α(x,w), β(x,w))\nα(x,w), β(x,w) > 0 ∀x,w θ ∼ N (θ; 0, σ21I) w ∼ N (w; 0, σ22I),\nwhere P(·, λ), G(·;α, β), and N (·; 0, σ2I) represent respectively a Poisson distribution with parameters λ, a Gamma distribution with parameters (α, β) and a zero mean normal distribution with covariance equal to σ2I.\nWe assume that the cardinality follows a Poisson distribution whose mean, λ, follows a Gamma distribution, with parameters which can be estimated from the input data x. Consequently, the integrals in p(θ,w|D) are simplified and form a negative binomial distribution,\nNB (m; a, b) = ( m+ a− 1\nm\n) .(1− b)abm\n= Γ(m+ a)\nΓ(m+ 1)Γ(a) .(1− b)abm,\n(22)\nwhere Γ is the Gamma function. Finally, the full posterior distribution can be written as\np(θ,w|D) = 1 Z̃ n∏ i=1 [ NB ( mi;α(xi,w),\n1\n1 + β(xi,w)\n) ×mi!× ( mi∏ k=1 p(yk|θ,xi) )] p(θ)p(w). (23)\nB.2. Learning\nFor simplicity, we use a point estimate for the posterior p(θ,w|D), i.e. p(θ,w|D) = δ(θ = θ∗,w = w∗|D), where (θ∗,w∗) are computed using the following MAP estimator:\n(θ∗,w∗) = arg max θ,w log (p (θ,w|D)) . (24)\nSince the solution to the above problem is independent from the normalisation constant Z̃, we have\n(θ∗,w∗) = arg max θ,w log (p(θ)) + n∑ i=1\n[ log (mi!) +\nmi∑ k=1 log (p(yk|θ,xi))\n+ log ( NB ( mi;α(xi,w1),\n1\n1 + β(xi,w2)\n))] + log (p(w))\n= arg max θ,w f1(θ) + f2(w).\n(25)\nTherefore, the optimisation problem in Eq. (25) can be decomposed w.r.t. the parameters θ and w. Therefore, we can learn them independently in two separate problems\nθ∗ = arg max θ f1(θ)\n= arg max θ γ1‖θ‖+ n∑ i=1\n[ log (mi!) +\nmi∑ k=1 log (p(yk|θ,xi))\n]\n≡ arg max θ γ1‖θ‖+ n∑ i=1 mi∑ k=1 log (p(yk|θ,xi))\n(26)\nand w∗ = arg max\nw f2(w)\n= arg max w n∑ i=1 [ log ( Γ(mi + α(xi,w)) Γ(mi + 1)Γ(α(xi,w)) ) + log ( β(xi,w) α(xi,w)\n(1 + β(xi,w)α(xi,w)+mi)\n)] + γ2‖w‖,\n(27)\nwhere γ1 and γ2 are the regularisation parameters, proportional to the predefined covariance parameters σ1 and σ2. These parameters are also known as weight decay parameters and commonly used in training neural networks.\nThe learned parameters θ∗ in Eq. (26) are used to map an input feature vector x to an output vector Y . For example, in image classification, θ∗ is used to predict the distribution Y over all categories, given the input image x.\nTo estimate w∗, we compute the partial derivatives of the objective function f2(·) w.r.t. the outputs of the deep neural network, i.e. α and β, and use standard backpropagation to learn the parameters of the deep neural network.\n∂f2(w)\n∂w =\n∂f2(w) ∂α(x,w) . ∂α(x,w) ∂w + ∂f2(w) ∂β(x,w) . ∂β(x,w) ∂w + 2γ2w, (28)\nwhere ∂f2(w)\n∂α(x,w) = n∑ i=1 [ Ψ ( mi + α(xi,w) ) −Ψ ( α(xi,w) ) + log ( β(xi,w) 1 + β(xi,w) )] , (29)\nand ∂f2(w)\n∂β(x,w) = n∑ i=1 [ α(xi,w)−mi.β(xi,w) β(xi,w). ( 1 + β(xi,w) )], (30)\nwhere Ψ(·) is the digamma function defined as\nΨ(α) = d\ndα log (Γ(α)) =\nΓ′(α) Γ(α) . (31)\nB.3. Inference\nHaving learned the parameters of the network (w∗,θ∗), for a test feature x∗, we use a MAP estimate to generate a set output as\nY∗ = arg max Y p(Y∗|D,x∗), (32)\nwhere\np(Y∗|D,x∗) = ∫ p(Y∗|θ,w,x∗)p(θ,w|D)dθdw\nand p(θ,w|D) = δ(θ = θ∗,w = w∗|D) as above. To calculate the mode of the set distribution p(Y∗|D,x∗), we first need to calculate the mode m∗ of the cardinality distribution\nm∗ = arg max m p(m|w∗,x∗), (33)\nwhere\np(m|w∗,x∗) = NB ( m;α(w∗,x∗),\n1\n1 + β(w∗,x∗)\n) . (34)\nThen, we calculate the mode of the joint distribution for the given cardinality m∗ as\nY∗ = arg max Ym∗ p({y1, · · · , ym∗}|θ∗,x∗). (35)\nIt can be seen from the equantions above that the mode of p({y1, · · · , ym∗}|θ∗,x∗) and p(y1, · · · , ym∗ |θ∗,x∗) needs to be identical w.r.t. a fixed permutation. Since the output of the first CNN with the parameters θ∗ is already the mode of p(y1, · · · , yM |θ∗,x∗) and the samples are i.i.d., we use the m∗ highest values from the output of the first CNN to generate the final set output Y∗."
    }, {
      "heading" : "C. Further Experimental Results",
      "text" : "Here, we show additional evaluation plots and qualitative results that could not be included in the main paper.\nMulti-class image classification. Figure 8 shows more results for successful image tagging. Figure 9 points to some interesting failures and erroneous predictions.\nPedestrian detection. ROC curves on two detection datasets are shown in Fig. 7. Qualitative results of pedestrian detection are shown in Figure 9."
    } ],
    "references" : [ {
      "title" : "Pedestrian detection at 100 frames per second",
      "author" : [ "R. Benenson", "M. Mathias", "R. Timofte", "L.V. Gool" ],
      "venue" : "CVPR",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A unified multi-scale deep convolutional neural network for fast object detection",
      "author" : [ "Z. Cai", "Q. Fan", "R. Feris", "N. Vasconcelos" ],
      "venue" : "ECCV",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected CRFs",
      "author" : [ "L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille" ],
      "venue" : "ICLR,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Histograms of oriented gradients for human detection",
      "author" : [ "N. Dalal", "B. Triggs" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Pedestrian detection: An evaluation of the state of the art",
      "author" : [ "P. Dollár", "C. Wojek", "B. Schiele", "P. Perona" ],
      "venue" : "IEEE T. Pattern Anal. Mach. Intell., 34,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The PASCAL Visual Object Classes Challenge",
      "author" : [ "M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "One-shot learning of object categories",
      "author" : [ "L. Fei-Fei", "R. Fergus", "P. Perona" ],
      "venue" : "IEEE T. Pattern Anal. Mach. Intell., 28(4):594–611, Apr.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Object detection with discriminatively trained part based models",
      "author" : [ "P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan" ],
      "venue" : "IEEE T. Pattern Anal. Mach. Intell., 32(9):1627–1645,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Are we ready for autonomous driving? The KITTI Vision Benchmark Suite",
      "author" : [ "A. Geiger", "P. Lenz", "R. Urtasun" ],
      "venue" : "CVPR",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Fast R-CNN",
      "author" : [ "R. Girshick" ],
      "venue" : "ICCV",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep convolutional ranking for multilabel image annotation",
      "author" : [ "Y. Gong", "Y. Jia", "T. Leung", "A. Toshev", "S. Ioffe" ],
      "venue" : "arXiv preprint arXiv:1312.4894,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep convolutional ranking for multilabel image annotation",
      "author" : [ "Y. Gong", "Y. Jia", "T. Leung", "A. Toshev", "S. Ioffe" ],
      "venue" : "CoRR, abs/1312.4894,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "A. Graves", "A.-r. Mohamed", "G.E. Hinton" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Comput., 9(8):17351780, Nov.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A convnet for nonmaximum suppression",
      "author" : [ "J. Hosang", "R. Benenson", "B. Schiele" ],
      "venue" : "B. Rosenhahn and B. Andres, editors, gcpr-2016, volume 9796 of Lecture Notes in Computer Science, pages 192–204, Hannover, Germany,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Densecap: Fully convolutional localization networks for dense captioning",
      "author" : [ "J. Johnson", "A. Karpathy", "L. Fei-Fei" ],
      "venue" : "CVPR",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS*2012,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Individualness and determinantal point processes for pedestrian detection",
      "author" : [ "D. Lee", "G. Cha", "M.-H. Yang", "S. Oh" ],
      "venue" : "ECCV",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Microsoft COCO: Common objects in context",
      "author" : [ "T.-Y. Lin", "M. Maire", "S. Belongie", "L. Bourdev", "R. Girshick", "J. Hays", "P. Perona", "D. Ramanan", "C.L. Zitnick", "P. Dollár" ],
      "venue" : "arXiv:1405.0312 [cs], May",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "SSD: Single shot multibox detector",
      "author" : [ "W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C.- Y. Fu", "A.C. Berg" ],
      "venue" : "In ECCV 2016, Lecture Notes in Computer Science,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Statistical multisource-multitarget information fusion, volume 685",
      "author" : [ "R.P. Mahler" ],
      "venue" : "Artech House Boston,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Mot16: A benchmark for multi-object tracking",
      "author" : [ "A. Milan", "L. Leal-Taixé", "I. Reid", "S. Roth", "K. Schindler" ],
      "venue" : "arXiv:1603.00831 [cs], Mar.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep decision network for multi-class image classification",
      "author" : [ "V.N. Murthy", "V. Singh", "T. Chen", "R. Manmatha", "D. Comaniciu" ],
      "venue" : "CVPR 2016, June",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning multi-domain convolutional neural networks for visual tracking",
      "author" : [ "H. Nam", "B. Han" ],
      "venue" : "CVPR",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Weakly- and semi-supervised learning of a deep convolutional network for semantic image segmentation",
      "author" : [ "G. Papandreou", "L.-C. Chen", "K.P. Murphy", "A.L. Yuille" ],
      "venue" : "ICCV 2015, Dec.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient point process inference for large-scale object detection",
      "author" : [ "T.T. Pham", "S. Hamid Rezatofighi", "I. Reid", "T.-J. Chin" ],
      "venue" : "CVPR",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "S. Ren", "K. He", "R. Girshick", "J. Sun" ],
      "venue" : "In NIPS*2015",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei" ],
      "venue" : "Int. J. Comput. Vision, 115(3):211–252,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "OverFeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "CoRR, abs/1312.6229,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "CoRR, abs/1409.1556,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "End-to-end people detection in crowded scenes",
      "author" : [ "R. Stewart", "M. Andriluka", "A.Y. Ng" ],
      "venue" : "CVPR",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "In NIPS*2014,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "CoRR, abs/1409.4842,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Order matters: Sequence to sequence for sets",
      "author" : [ "O. Vinyals", "S. Bengio", "M. Kudlur" ],
      "venue" : "arXiv:1511.06391 [cs, stat], Nov.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Robust real-time face detection",
      "author" : [ "P. Viola", "M.J. Jones" ],
      "venue" : "Int. J. Comput. Vision, 57(2):137–154, May",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "New features and insights for pedestrian detection",
      "author" : [ "S. Walk", "N. Majer", "K. Schindler", "B. Schiele" ],
      "venue" : "CVPR",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "CNN-RNN: A unified framework for multi-label image classification",
      "author" : [ "J. Wang", "Y. Yang", "J. Mao", "Z. Huang", "C. Huang", "W. Xu" ],
      "venue" : "CVPR, June",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "CNN: Single-label to multi-label",
      "author" : [ "Y. Wei", "W. Xia", "J. Huang", "B. Ni", "J. Dong", "Y. Zhao", "S. Yan" ],
      "venue" : "CoRR, abs/1406.5726,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "M.D. Zeiler", "R. Fergus" ],
      "venue" : "ECCV",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 23,
      "context" : "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 19,
      "context" : "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].",
      "startOffset" : 221,
      "endOffset" : 225
    }, {
      "referenceID" : 16,
      "context" : "Modern approaches typically address this by a series of convolutional layers, followed by a number of fully connected layers, which are finally mapped to predict a fixedsized vector [17, 30, 33].",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 29,
      "context" : "Modern approaches typically address this by a series of convolutional layers, followed by a number of fully connected layers, which are finally mapped to predict a fixedsized vector [17, 30, 33].",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 32,
      "context" : "Modern approaches typically address this by a series of convolutional layers, followed by a number of fully connected layers, which are finally mapped to predict a fixedsized vector [17, 30, 33].",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 27,
      "context" : "1,000 for the ImageNet challenge [28].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "), and what is (a) Proposals (b) MS-CNN [2] (c) Our result",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "Another strategy to account for multiple classes is to fix the number to a certain value for all test instances, and report precision and recall by counting false positive and false negative predictions, as was done in [11, 38].",
      "startOffset" : 219,
      "endOffset" : 227
    }, {
      "referenceID" : 36,
      "context" : "Another strategy to account for multiple classes is to fix the number to a certain value for all test instances, and report precision and recall by counting false positive and false negative predictions, as was done in [11, 38].",
      "startOffset" : 219,
      "endOffset" : 227
    }, {
      "referenceID" : 3,
      "context" : "The most common approach is to assign a confidence score to a number of region candidates [4, 8, 10, 27], which are typically selected heuristically by thresholding and non-maxima suppression.",
      "startOffset" : 90,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "The most common approach is to assign a confidence score to a number of region candidates [4, 8, 10, 27], which are typically selected heuristically by thresholding and non-maxima suppression.",
      "startOffset" : 90,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "The most common approach is to assign a confidence score to a number of region candidates [4, 8, 10, 27], which are typically selected heuristically by thresholding and non-maxima suppression.",
      "startOffset" : 90,
      "endOffset" : 104
    }, {
      "referenceID" : 26,
      "context" : "The most common approach is to assign a confidence score to a number of region candidates [4, 8, 10, 27], which are typically selected heuristically by thresholding and non-maxima suppression.",
      "startOffset" : 90,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "A sudden success in multiple applications including voice recognition [13], machine translation [32] and image classification [17], has sparked the deployment of deep learning methods throughout numerous research areas.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 31,
      "context" : "A sudden success in multiple applications including voice recognition [13], machine translation [32] and image classification [17], has sparked the deployment of deep learning methods throughout numerous research areas.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "A sudden success in multiple applications including voice recognition [13], machine translation [32] and image classification [17], has sparked the deployment of deep learning methods throughout numerous research areas.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Deep convolutional (CNN) and recurrent (RNN) neural networks now outperform traditional approaches in tasks like semantic segmentation [3], image captioning [16] or object detection [20].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "Deep convolutional (CNN) and recurrent (RNN) neural networks now outperform traditional approaches in tasks like semantic segmentation [3], image captioning [16] or object detection [20].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 19,
      "context" : "Deep convolutional (CNN) and recurrent (RNN) neural networks now outperform traditional approaches in tasks like semantic segmentation [3], image captioning [16] or object detection [20].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 6,
      "context" : "Early datasets, such as Caltech-101 [7], mostly contained one single object and could easily be described by one category.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "Consequently, a large body of literature focused on single-class prediction [17, 29, 40, 23].",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 28,
      "context" : "Consequently, a large body of literature focused on single-class prediction [17, 29, 40, 23].",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 38,
      "context" : "Consequently, a large body of literature focused on single-class prediction [17, 29, 40, 23].",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 22,
      "context" : "Consequently, a large body of literature focused on single-class prediction [17, 29, 40, 23].",
      "startOffset" : 76,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "[12] combine deep CNNs with a top-k approximate ranking loss to predict multiple labels.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 37,
      "context" : "[39] propose a Hypotheses-Pooling architecture that is specifically designed to handle multi-label output.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[38] proposed a model that combines CNNs and RNNs (convolutional and recurrent networks) to predict a number classes in a sequential manner.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[34].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "it may contain the same element multiple times), such that post-processing or heuristics such as beam search must be employed [35, 38].",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 34,
      "context" : "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 35,
      "context" : "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "More recent methods, such as Fast R-CNN [10] or the single-shot multi-box detector (SSD) [20] learn the relevant image features rather than manually engineering them, but retain the sliding window approach.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "More recent methods, such as Fast R-CNN [10] or the single-shot multi-box detector (SSD) [20] learn the relevant image features rather than manually engineering them, but retain the sliding window approach.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 30,
      "context" : "[31] perform end-to-end head detection by predicting the bounding boxes sequentially using an LSTM [14].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[31] perform end-to-end head detection by predicting the bounding boxes sequentially using an LSTM [14].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 25,
      "context" : "[26] and Lee et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] formulate NMS as a global optimisation problem while Hosang et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] propose to learn the NMS algorithm end-to-end using CNNs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "The factor m! = ∏m k=1 k appears because the probability density for a set {y1, · · · , ym} must be equally distributed among all the m! possible permutations of the vector [21].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "We perform experiments on two standard benchmarks, the PASCAL VOC 2007 dataset [6] and the Microsoft Common Objects in Context (MS COCO) dataset [19].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : "We perform experiments on two standard benchmarks, the PASCAL VOC 2007 dataset [6] and the Microsoft Common Objects in Context (MS COCO) dataset [19].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 36,
      "context" : "In this experiment, similar to [38], we build on the 16-layers VGG network [30], pretrained on the 2012 ImageNet dataset.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 29,
      "context" : "In this experiment, similar to [38], we build on the 16-layers VGG network [30], pretrained on the 2012 ImageNet dataset.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "We then fine-tune the entire network for each of these datasets using two commonly used losses for multi-label classification, softmax and binary cross-entropy (BCE)2 [11, 38].",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 36,
      "context" : "We then fine-tune the entire network for each of these datasets using two commonly used losses for multi-label classification, softmax and binary cross-entropy (BCE)2 [11, 38].",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 36,
      "context" : "However, it does not perform as well as softmax and binary cross-entropy for the used datasets [38].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "To evaluate the performance of the classifiers and our deep set network, we employ the commonly used evaluation metrics for multi-label image classification [11, 38]: precision and recall of the generated labels per-class (C-P and C-R) and overall (O-P and O-R).",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 36,
      "context" : "To evaluate the performance of the classifiers and our deep set network, we employ the commonly used evaluation metrics for multi-label image classification [11, 38]: precision and recall of the generated labels per-class (C-P and C-R) and overall (O-P and O-R).",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "To generate the predicted labels for a particular image , we perform a forward pass of the CNN and choose top-k labels according to their scores similar to [11, 38].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 36,
      "context" : "To generate the predicted labels for a particular image , we perform a forward pass of the CNN and choose top-k labels according to their scores similar to [11, 38].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "The Pascal Visual Object Classes (VOC) [6] benchmark is one of the most widely used datasets for detection and classification.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "Another popular benchmark for image captioning, recognition, and segmentation is the recent Microsoft Common Objects in Context (MS-COCO) [19].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 36,
      "context" : "We also outperform the state-of-the art multi-label classifier CNN-RNN [38], for the reported value of k = 3 in Table 1.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "We perform experiments on two widely used datasets, Caltech Pedestrians [5] and MOT16, the 2016 edition of the MOTChallenge benchmark [22].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "We perform experiments on two widely used datasets, Caltech Pedestrians [5] and MOT16, the 2016 edition of the MOTChallenge benchmark [22].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 1,
      "context" : "To push the state-of-the art performance on pedestrian detection, we chose the leading detector on Caltech Pedestrians, which is the recent the multi-scale deep CNN approach (MS-CNN) [2].",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 16,
      "context" : "We build on top of the well-known AlexNet [17] architecture, and replace the first convolutional layer with a single channel filter and last fully connected layer with 2 layers output followed by two weighted sigmoid activation function, similar to the case above (cf .",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 30,
      "context" : "in [31].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 36,
      "context" : "9 CNN-RNN [38] k=3 66.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "To quantify the detection performance, we adapt the same evaluation metrics and follow the protocols used on the Caltech detection benchmark [5].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "Caltech Pedestrians [5] is a de-facto standard benchmark for pedestrian detection.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "We use the MS-CNN [2] network model and its parameters learned on the Caltech training set as θ∗ in Eq.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "[5]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 21,
      "context" : "Additionally, we evaluate the MSCNN detector and DeepSet in significantly more crowded and more challenging sequences from MOT16 dataset [22].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "We only learn the cardinality network w∗ on training set and we use the MS-CNN network model and its parameters learned on the KITTI dataset [9] as θ∗ in Eq.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "MOT16 MS-CNN [2] 51.",
      "startOffset" : 13,
      "endOffset" : 16
    } ],
    "year" : 2016,
    "abstractText" : "This paper addresses the task of set prediction using deep learning. This is important because the output of many computer vision tasks, including image tagging and object detection, are naturally expressed as sets of entities rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a deep neural network. We also derive a loss for predicting a discrete distribution corresponding to set cardinality. Set prediction is demonstrated on the problems of multi-class image classification and pedestrian detection. Our approach yields state-of-theart results in both cases on standard datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}