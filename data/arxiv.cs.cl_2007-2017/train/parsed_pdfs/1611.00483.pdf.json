{
  "name" : "1611.00483.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Detecting Context Dependent Messages in a Conversational Environment",
    "authors" : [ "Chaozhuo Li", "Yu Wu", "Wei Wu", "Chen Xing", "Zhoujun Li", "Ming Zhou" ],
    "emails" : [ "lichaozhuo@buaa.edu.cn", "wuyu@buaa.edu.cn", "lizj@buaa.edu.cn", "wuwei@microsoft.com", "v-chxing@microsoft.com", "mingzhou@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n00 48\n3v 1\n[ cs\n.C L\n] 2\nN ov\n2 01\n6"
    }, {
      "heading" : "1 Introduction",
      "text" : "Together with the rapid growth of social media such as Twitter and Weibo, the amount of conversation data on the web has tremendously increased. This makes building open domain chatbot systems with data-driven approaches possible. To carry on reasonable conversations with humans, a chatbot system needs to generate proper response with regard to users’ messages. Recently, with the large amount of conversation data available, learning a response generator from data has drawn a lot of attention (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015).\nA key step to coherent response generation is determining when to consider linguistic context of messages. Existing work on response generation, however, has overlooked this step. They either totally ignores linguistic context (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015) or simply considers context for every message (Sordoni et al., 2015b; Serban et al., 2015). The former case is easy to lead to irrelevant responses when users’ input messages rely on the context information in previous conversation turns, while the latter case is costly (e.g., on memory and responding time) for building a real chatbot system and has the risk of bringing in noise to response generation especially when users want to end the current conversation topic and start a new one. According to our observation, there are two types of messages in a conversational environment. The first type is context dependent message, which means to reply to the message, one must consider previous utterances in the dialogue1, while the second type is context independent message, which means even without the previous utterances, the message itself can still lead to a reasonable response. Table 1 compares the two types of messages using examples. In Case 1, “why do you think so” is a context dependent message. In order to reply to the message, one cannot ignore its linguistic context “I think it will rain tomorrow”. On the other hand, in Case 2, “Well, what time is it now” is a context independent message, as one can give a reasonable response without looking at the previous turns. Distinguishing context dependent messages from context independent messages is important for building a good response generator. Missing linguistic context for context dependent\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/.\n1Broadly speaking, context may not be limited to linguistic context. For example, a user’s interest could also be a kind of context. As the first step, in this work, we only focus on “linguistic context”.\nmessages will lead to nonsense response. For example, “because I love you” could also be a response for the message “why do you think so” if we only look at the message itself, but it is nonsense appearing in the dialogue of Case 1. Incorporating context information into context independent messages will increase the workload of a generation system and has the risk of bringing in noise to the generation process. For example, if we consider the context “NBA” for the message “Well, what time is it now”, the chatbot will probably say something about “NBA” rather than answer the question with a time answer. Although detecting context dependent messages is crucial for building chatbot systems, there is limited understanding about it.\nIn this paper, we study this important but less explored problem. Instead of answering how to incorporate context information, we try to understand when we need the information. Therefore, our effort is complementary to the existing work on response generation. It can keep the existing generation algorithms context-aware and improve their efficiency and robustness to noise. The task is challenging, as messages in a conversational environment are usually short and informal, and evidence that can indicate a message is context dependent is scarce. For example, on 3 million post-response pairs crawled from Weibo, the average length of messages is 4.65. On such short texts, classic NLP tools such as POS Tagger and Parser suffer from bad performance (Derczynski et al., 2013; Foster et al., 2011) and it is difficult to explicitly extract features that are discriminative on the two types of messages. More seriously, there are no large scale annotations available for building a supervised learning procedure.\nWe consider leveraging the large amount of human-human conversation data available on the web to learn a message classifier. Our intuition is that a context dependent message has different linguistic context in different conversation sessions, therefore its responses could be more diverse on content than responses of a context independent message. To verify this idea, we study the distributions of responses of messages using conversation data crawled from social media and find that the length distribution of responses and the word distribution of responses are quite discriminative on the two types of messages. Based on this observation, for each message in the crawled data, we estimate the average length of responses, the entropy of the word distribution of responses, and the maximum mass of the word distribution of responses, and take these characteristics as weak supervision signals to learn a classifier. The classifier takes a message as input and can make prediction for any messages in a real conversation environment, even though the messages do not appear in the crawled data and characteristics like entropy are not available for them. We propose using a Long Short Term Memory (LSTM) architecture to learn the classifier. Our model represents message texts in a continuous vector space using a one-layer LSTM network. The text vectors are then provided as input to a two-layer feed-forward neural network to perform classification. The neural network architecture carries out feature learning and model learning in a unified framework, and thus can avoid explicit feature extraction which is difficult on short conversational messages. Our method leverages large scale weak supervision signals extracted from responses in social conversation data and can reach a satisfactory accuracy with only a few human annotations.\nWe conduct experiments on large scale English and Chinese conversation data mined from Twitter and Weibo respectively, and test the performance of our method on thousands of messages annotated by human labelers. Experimental results show that our method can significantly outperform baseline methods on accuracy of message classification on both of the two data sets.\nWe make the following contributions in this paper: 1) proposal of detecting context dependent messages in a conversational environment; 2) proposal of learning weak supervision signals from responses of messages using large scale conversation data; 3) proposal of using an LSTM architecture to learn a message classifier; 4) empirical verification of the proposed method on human annotated data."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work lies in the path of building chatbot systems with data-driven approaches. Differing from traditional dialogue systems (cf., (Young et al., 2013)) which rely on hand-crafted features and rules to generate reply sentences for specific applications such as voice dialling (Williams, 2008) and appointment scheduling (Janarthanam et al., 2011) etc., recent effort focuses on exploiting an end-toend approach to learn a response generator from social conversation data for open domain dialogue (Koshinda et al., 2015; Higashinaka et al., 2016). For example, Ritter et al. (Ritter et al., 2011) employed a phrase-based machine translation model for response generation. In (Shang et al., 2015; Vinyals and Le, 2015), neural network architectures were proposed to learning response generators from one-round conversation data. Based on these work, Sordoni et al. (Sordoni et al., 2015b) incorporated linguistic context into the learning of response generator. Serban et al. (Serban et al., 2015) proposed a hierarchical neural network architecture to building context-aware response generation. In this paper, instead of studying how to incorporate context into response generation, we consider the problem that when we need context in the process. Our work can keep the existing generation algorithms context-aware and at the same time improve their efficiency and robustness.\nWe employ a Recurrent Neural Network (RNN) architecture to learn a message classifier. RNN models (Elman, 1990), due to their capability of modeling sequences with arbitrary length, have been widely used in many natural language processing tasks such as language modeling (Mikolov et al., 2010) and tagging (Xu et al., 2015) etc. Recently, it is reported that Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as two special RNN models which can capture long term dependencies in sequences outperform state of the art methods on tasks like machine translation (Sutskever et al., 2014) and response generation (Shang et al., 2015). In this paper, we apply the LSTM architecture to the task of context dependent message detection. We append LSTM with a two-layer feed-forward neural network, thus feature learning and model learning can be carried out simultaneously.\nOur work belongs to the scope of short text classification (Song et al., 2014). Existing applications of short text classification include query classification (Kang and Kim, 2003), tweet classification (Sriram et al., 2010), and question classification (Zhang and Lee, 2003). We study a new problem in short text classification: distinguishing context dependent messages from context independent messages in a conversational environment. The task is important for building open domain chatbot systems and has its unique challenges (e.g., new data structure). We tackle the challenges by leveraging the responses of messages and utilizing an LSTM network to conduct feature learning and model learning simultaneously."
    }, {
      "heading" : "3 Learning to Detect Context Dependent Messages",
      "text" : "Suppose that we have a data set D = {(mi, yi)}Ni=1 where mi is a message composed of a sequence of words (wmi,1, . . . , wmi,ni) and yi is an indicator whose value reflects whether mi is context dependent or not. Our goal is to learn a function g(·) ∈ {−1, 1} using D, thus for any new message m, g(·) predicts m a context dependent message if g(m) = 1. To this end, we need to answer two questions: 1) how to construct D; 2) how to perform learning using D.\nFor the first question, we can crawl conversation data from social media like Twitter and ask human labelers to annotate the messages in the data. The problem is that human annotation is expensive and time consuming and therefore we cannot obtain a large scale data set for learning. To solve the problem, we automatically learn some weak supervision signals using responses of messages in social conversation data, and take the signals as {yi} in D. For the second question, one straightforward way is first extracting shallow features such as bag-of-words and syntax from messages and then employing off-the-shelf machine learning tools to learn a model. The problem is that shallow features are not effective enough on representing semantics in short conversation messages, which will be seen in our experiments. We propose using a Long Short Term Memory (LSTM) architecture to learn a model from D. The advantage of our approach is that it can avoid explicit feature extraction and large scale human annotations, and carry out feature learning and model learning in a unified framework."
    }, {
      "heading" : "3.1 Learning Weak Supervision Using Responses",
      "text" : "Instead of requiring human annotations, we consider creating signals that are discriminative on the two types of messages from large scale social conversation data available on the web. Our intuition is that a context dependent message has different linguistic context in different conversation sessions, therefore, its responses could be more diverse on content than responses of a context independent message (one message may appear multiple times, and therefore it may correspond to multiple responses). Table 2 illustrates our idea with some examples from Twitter. The last column of the table represents the frequency of the message or the frequency of the response under the message. For each message, we show the top 5 most frequent responses. From the examples, we can see that a context dependent message tends to have divergent and uniformly distributed responses corresponding to different linguistic context, while the responses of a context independent message share relatively similar content and some content dominates the distribution.\nThe examples inspire us to investigate some statistical characteristics that can reflect the diversity of responses. These characteristics could be good indicators of context dependent messages, and we can construct {yi} in D using the characteristics. We estimate the following statistical characteristics for each message using its responses, and examine how the characteristics are discriminative on the two types of messages using 1000 labeled messages from Twitter and Weibo respectively. The details of the labeled data will be described in our experiments.\nEntropy: the first characteristic we investigate is the entropy of the word distribution of responses, which is a common measure for diversity. Given a word distribution P = (p1, p2, . . . , pn), the entropy of the distribution is defined as\nE(P ) = n ∑\ni=1\n−pi log2(pi). (1)\nThe maximum of the entropy is log2(n) which is reached when the distribution is uniform. Then, a large entropy means a word distribution covers many words (i.e., n is big) and is close to a uniform distribution. Therefore, a context dependent message should have a larger entropy on responses than a context independent message (see the comparison in Table 2). We normalize the entropy to [0, 1] by E(P )−min(E) max(E)−min(E) , where max(E) and min(E) represent the maximum entropy and the minimum entropy in the data set. Figure 1(a) shows the comparison of the two types of messages on normalized entropy using the Twitter labeled data. In the figure, each value on the x-axis represents an interval with a fixed length 0.05. For example, 0.50 means an interval [0.5, 0.55). Each value on the y-axis represents the percentage of messages in a specific interval. For example, among messages falling in the interval [0.95, 1), nearly 80% are labeled as context dependent and only about 20% are labeled as context independent. From the figure, we can see that entropy is discriminative on the two types of messages: context dependent messages distributes on large entropy areas, while context independent messages tend to have smaller entropy.\nM(P): in addition to entropy, another characteristic that might reflect the diversity of responses could be the maximum mass of the word distribution of responses, as in diverse responses, words should be uniformly distributed (stopwords are removed), while in less diverse responses, there may exit dominant words (e.g., “night” in Table 2). Given a word distribution P = (p1, p2, . . . , pn), we define a characteristic as\nM(P ) = 1− max 16i6n pi (2)\nFigure 1(b) compares the two types of messages on M(P ) using the Twitter labeled data, in which values\non the x-axis and y-axis have the same meaning as those in Figure 1(a). From the figure, we can see that similar to entropy, M(P ) is useful on distinguishing the two types of messages. Context dependent messages have larger M(P ) than context independent messages.\nAverage length of responses: finally, we consider the length distribution of responses. Since responses of context dependent messages are more diverse on content, they might be longer than responses of context independent messages. We calculate the average length of responses for each message and normalize it to [0, 1] in the same way as entropy. Figure 1(c) compares the two types of messages on average length of responses using the Twitter labeled data, where values on the x-axis represent intervals with a length 0.1. The result supports our claim and clearly indicates that average length is discriminative on the two types of messages.\nWe combine the three characteristics using a linear SVM classifier learned with the 1000 labeled messages and take the output of the SVM (a real value) as {yi} in D. By this means, we can create a large scale training data set with only a little human labeling effort. Here, as a reference, we also report the classification accuracy of the three characteristics and the SVM classifier on the 1000 labeled data. Each characteristic corresponds to a threshold tuned on the 1000 labeled data with 5-fold cross validation. If a value of a characteristic of a message is larger than the threshold, then the message will be predicted as context dependent. Table 3 shows the classification accuracy of 5-fold cross validation (average of 5 results), where SVM (com) refers to the SVM classifier. Details of experiment setting will be described in Section 4. From Table 3, we can see that the numbers are consistent with Figure 1(a), 1(b), and 1(c)."
    }, {
      "heading" : "3.2 Model Learning",
      "text" : "We head for learning g(·) using D constructed in Section 3.1. Note that g(·) only takes a message m as input, and thus can make prediction for any messages in a real chatbot system even though the messages are not in D and their entropy, M(P), and average length of responses are not available. Our idea is that we first learn a regression model by fitting {yi} in D through minimizing the sum of squared residuals and then construct g(·) by comparing the output of the regression model with a threshold. We can obtain the threshold by tuning it on a few labeled data (e.g., the 1000 labeled data). The key is how to learn the regression model. We propose using a Recurrent Neural Network (RNN) architecture to embed messages into a continuous vector space and learning a regression model with the embedding of messages using a feed-forward neural network. The RNN model, which is capable of embedding sequences with arbitrary\nlength, can encode the order of words and the semantics of a message into a vector representation which has been recently proven effective on capturing similarity of short texts (Sordoni et al., 2015a). We take the output vector given by RNN as a feature representation of a message and feed it to a feed-forward neural work. By this means, we can conduct feature learning and model learning in a unified framework and jointly optimize the two components.\nGiven a message m which consists of n words, the RNN model reads the words one by one, and updates a recurrent state ht for the t-th word wt by\nht = f(ht−1, xt), h0 = 0, (3)\nwhere ht ∈ Rdh , xt ∈ Rdw is the vector representation of wt, and f is non-linear transformation. ht acts as an encoding of the semantics of the word sequence up to position t, and the final output hn is a representation of message m. Both xt and ht are learned in the optimization of the RNN model. We select the Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) as f , since it can model long term dependencies in sequences with affordable complexity. LSTM controls the learning of the representation of a sequence by gates. Specifically, at position t, LSTM controls the information that should be kept from previous states by an input gate it, and the information that should be forgotten by a forget gate ft. After memorizing and forgetting, the information is stored in a memory cell ct. ct generates the recurrent state ht through an output gate ot. The specific parameterization of LSTM is given by\nit = σ(W (i) xt + U (i) ht−1 + b (i)) ft = σ(W (f) xt + U (f) ht−1 + b (f)) ot = σ(W (o) xt + U (o) ht−1 + b (o)) ut = tanh(W (u) xt + U (u) ht−1 + b (u)) ct = it ⊗ ut + ft ⊗ c(t−1) ht = ot ⊗ tanh(ct),\nwhere σ(·) is a sigmoid function and tanh(·) is a hyperbolic tangent function. W (i), W (f), W (o), W (u) ∈ Rdh×dw , U (i), U (f), U (o), U (u) ∈ Rdh×dh , and b(i), b(f), b(o), b(u) ∈ Rdh×1 are parameters. ⊗ means element-wise multiplication. After we get the final state hn, we feed it to a two-layer feed-forward neural network to get an output s which is defined by\ns = b2 +W2 (tanh(b1 +W1hn)) , (4)\nwhere b1 ∈ Rds×1, W1 ∈ Rds×dh , W2 ∈ R1×ds , and b2 ∈ R are parameters. Figure 2 illustrates the architecture of our method.\nFor each mi in D, we calculate an si using Equation (4) as an estimation of yi. We then learn the parameters of the LSTM network and the feed-forward network by minimizing the sum of the squared residuals. Formally, our learning approach can be formulated as\nargmin s\nN ∑\ni=1\n(yi − si) 2 . (5)\nAfter we obtain the parameters, we can calculate an sm for any message m using Equation (4). We then tune a threshold T with a few labeled messages. The classifier g(·) is given by\ng(m) =\n{\n1 if sm > T −1 otherwise (6)\nThe gradients of the objective function (5) are computed using the back-propagation through time (BPTT) algorithm (Williams and Peng, 1990). We share the code for model learning at https://github.com/whatsname1991/coling2016."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experiment Setup",
      "text" : "We constructed the conversation data for experiments from Weibo and Twitter. In each of the two social media, two persons can communicate by replying to each other under a post. We crawled sequences of reply with posts and extracted triples like “(context, message, response)” as experimental data. In a triple, “message” is a reply, “context” is the sentence in the previous turn of the message (a reply or a post), and “response” is the sentence in the next turn (reply to the message). Note that in this work, we restrict the context of a message to a single sentence. This is a simplification of context in conversation. In real conversation, context could be more complicated and we leave the discussion of it as future work.\nWe crawled 5.9 million English triples from Twitter, and 3.1 million Chinese triples from Weibo. The numbers of distinct messages in the Twitter data and in the Weibo data are 92, 755 and 112, 175 respectively. On average, each Twitter message has 63.26 responses (some messages like “hello” can have many different responses) and each Weibo message has 27.52 responses. The average word length of Twitter message is 3.39 and the word average length of Weibo message is 4.65. English sentences were stemmed and stop words were removed, and Chinese sentences were segmented.\nWe constructed D = {(mi, yi)}Ni=1 in Section 3.1 in the following way: we first calculated entropy, M(P ), and average length of responses for each message using the 5.9 million English triples and 3.1 million Chinese triples. Then from these data, we randomly sampled 1000 English triples and 1000 Chinese triples as validation sets. For each triple in the validation data, we hid the response and recruited human judges to label if the message is context dependent or not. Note that we hid responses when labeling messages because this is more close to the real case. In a real chatbot system, one has to determine if a message is context dependent or not before generating a response. Each judge labeled a message with 1 if it is context dependent, otherwise the judge labeled the message with −1. Each message got three labels and the majority of the labels was taken as the final decision for the message. In the Weibo data, there are 412 positive examples and 588 negative examples. In the Twitter data, the two numbers are 440 and 560, respectively. With the two validation data sets, we learned two SVM classifiers in order to combine the three characteristics as described in Section 3.1. Parameters of SVMs were tuned by 5-fold cross validation. Finally, we assigned a yi to each mi in the 112, 175 Twitter messages and 92, 755 Weibo messages by the output of the SVM classifiers, and formed D for both English data and Chinese data. We trained LSTM models using D.\nTo evaluate the performance of different models, we crawled another 3000 Chinese context-message pairs and 1000 English context-message pairs from Weibo and Twitter respectively, and followed the same way as the validation data to judge if the messages are context dependent or not. We used these data to simulate real context-message pairs in chatbot systems. In the Weibo data, there are 2715 unique messages and 1983 messages are not in D. The numbers of positive examples and negative examples are 1472 and 1528 respectively. In the Twitter data, the number of unique messages is 875 and 366 messages are not included by D. The numbers of positive and negative examples are 464 and 536 respectively. Note that for messages that are not included by D, their characteristics (i.e., entropy, M(P ), and average length of responses) are not available, and we can only use classifiers whose features are extracted from messages (like our LSTM models) to make prediction. This is close to a real situation in chatbots, and we took the two data sets as test sets.\nWe considered the following methods as baselines:\nLength: intuitively, short messages tend to be context dependent (e.g., “why” in Table 2). Therefore, we employed length of a message as a baseline. A message shorter than a threshold will be predicted as a context dependent message.\nMDF: given a word, we estimated the number of messages that contain the word and named it “document frequency” (DF). We constructed a list of words associated with DF using D. For a new message, we calculated the minimal DF of words in the message using the list. A context dependent message like “why do you think so” may consist of common words, and thus correspond to a high minimal DF. We considered minimal DF as a baseline. A message with a minimal DF larger than a threshold will be predicted as a context dependent message.\nSVM (Length+MDF): we linearly combined Length and MDF by learning an SVM classifier on the validation data.\nSVM (classification): we extracted unigrams, bigrams, and frequencies of POS tags as features from a message, and learned a linear SVM classifier on the validation data with these features. POS tags for Chinese data were obtained using Stanford Parser (http://nlp.stanford.edu/software/lex-parser.shtml) and POS tags for English data were obtained using TweetNLP (http://www.cs.cmu.edu/˜ark/TweetNLP/).\nSVM (regression): instead of learning a classifier from annotations in the validation data, we fitted {yi} in D by learning an SVM regression model using the same features as SVM (classification) and made predictions on new messages by a threshold.\nAll SVM models were learned using SVM-Light (http://svmlight.joachims.org/). We employed classification accuracy as an evaluation metric."
    }, {
      "heading" : "4.2 Parameter Tuning",
      "text" : "For Length and MDF, the only parameter is a threshold. We tuned the thresholds on the validation data. For all SVM models, we selected the trade-off parameter in SVM from {0.01, 0.1, 1, 10, 100} by 5-fold cross validation on the validation data. SVM (regression) also needs a threshold. We tuned it on the validation data. The parameters of LSTM include the dimension of word vectors dw, the dimension of hidden states dh, and the dimension of the first layer of the feed-forward network ds. We set dw = dh = 256, and ds = 100. Besides these parameters, we also set a dropout rate 0.1 in the learning of the feed-forward network as regularization.\nTable 5: Comparison between LSTM, SVM (classification), and SVM (regression)\nExample context : Have you heard Taylor Swift’s new song? message: Yep, I have heard it on Saturday night. Label context dependent SVM (regression) context independent SVM (classification) context independent LSTM context dependent"
    }, {
      "heading" : "4.3 Quantitative Evaluation",
      "text" : "Table 4 reports quantitative evaluation results on the test data. From the results, we can see that our methods outperform baseline methods. The improvement over the best performing baseline methods (i.e., SVM (classification) on Webio and SVM(regression) on Twitter) is statistically significant (sign test, p-value < 0.01).\nLength and MDF are characteristics of messages. The results tell us that these characteristics are not so discriminative on the two types of messages. The reason is easy to understand: we may think that context dependent messages tend to be short and consist of common words, but the fact is that short messages composed of common words could be context independent (e.g., “Good night” in Table 2) while long messages like “Yep, I have heard it on Saturday night” (see the example in Table 5) could be context\ndependent. Both SVM (classification) and SVM (regression) perform worse than our LSTM model, indicating that shallow features are not effective enough to represent the semantics in short conversation messages. Our method outperforms the baseline methods on both data sets. The results verified our idea on leveraging responses for context dependent message detection, and demonstrates the power of big data and the advantage of LSTM on capturing semantics in short messages."
    }, {
      "heading" : "4.4 Qualitative Evaluation",
      "text" : "We use an example to further explain why our method is effective on distinguishing the two types of messages. Table 5 compares LSTM with SVM (classification) and SVM (regression). Both SVM (classification) and SVM (regression) rely on shallow features such as bag of words and pos tags to perform learning. These features, however, are not effective on representing the semantics of short messages. The representation is easily to be biased by some specific words like “Saturday night” in the example. Therefore, both SVM (classification) and SVM (regression) failed on this case. On the other hand, LSTM models term dependencies in sequences with a memorizing-forgetting mechanism. It can capture the semantics in the message “Yep, I have heard it on Saturday night.” and identify that it is similar to messages like “Yes, I did” and “Yes, I have”. For example, the cosine of the vector of “Yep, I have heard it on Saturday night.” and the vector of “Yes, I have” given by LSTM is 0.63. Since messages like “Yes, I did” and “Yes, I have” are common context dependent messages, LSTM can successfully recognize that the message in the example is also context dependent."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We propose learning a LSTM network with weak supervision signals estimated from responses of messages to detecting context dependent messages in a conversational environment. Evaluation results show that the proposed method can significantly outperform baseline methods on distinguishing the two types of messages."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was supported by Beijing Advanced Innovation Center for Imaging Technology (No.BAICIT2016001), the National Natural Science Foundation of China (Grand Nos. 61370126, 61672081), National High Technology Research and Development Program of China (No.2015AA016004),the Fund of the State Key Laboratory of Software Development Environment (No.SKLSDE-2015ZX-16)."
    } ],
    "references" : [ {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine",
      "author" : [ "Cho et al.2014] Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Twitter part-ofspeech tagging for all: Overcoming sparse and noisy data",
      "author" : [ "Alan Ritter", "Sam Clark", "Kalina Bontcheva" ],
      "venue" : "In RANLP,",
      "citeRegEx" : "Derczynski et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Derczynski et al\\.",
      "year" : 2013
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "Elman.,? \\Q1990\\E",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "hardtoparse: Pos tagging and parsing the twitterverse",
      "author" : [ "Özlem Çetinoglu", "Joachim Wagner", "Joseph Le Roux", "Stephen Hogan", "Joakim Nivre", "Deirdre Hogan", "Josef Van Genabith" ],
      "venue" : "In AAAI 2011 Workshop on Analyzing Microtext,",
      "citeRegEx" : "Foster et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Foster et al\\.",
      "year" : 2011
    }, {
      "title" : "Syntactic filtering and content-based retrieval of twitter sentences for the generation of system utterances in dialogue systems",
      "author" : [ "Nozomi Kobayashi", "Toru Hirano", "Chiaki Miyazaki", "Toyomi Meguro", "Toshiro Makino", "Yoshihiro Matsuo" ],
      "venue" : "In Situated Dialog in Speech-Based Human-Computer Interaction,",
      "citeRegEx" : "Higashinaka et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Higashinaka et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "The day after the day after tomorrow?: a machine learning approach to adaptive temporal expression generation: training and evaluation with real users",
      "author" : [ "Helen Hastie", "Oliver Lemon", "Xingkun Liu" ],
      "venue" : "In Proceedings of the SIGDIAL 2011 Conference,",
      "citeRegEx" : "Janarthanam et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Janarthanam et al\\.",
      "year" : 2011
    }, {
      "title" : "Query type classification for web document retrieval",
      "author" : [ "Kang", "Kim2003] In-Ho Kang", "GilChang Kim" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "Kang et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2003
    }, {
      "title" : "Machine-learned ranking based non-task-oriented dialogue agent using twitter data",
      "author" : [ "Michimasa Inaba", "Kenichi Takahashi" ],
      "venue" : "In 2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),",
      "citeRegEx" : "Koshinda et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Koshinda et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur" ],
      "venue" : "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Data-driven response generation in social media",
      "author" : [ "Ritter et al.2011] Alan Ritter", "Colin Cherry", "William B Dolan" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Ritter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2011
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models. arXiv preprint arXiv:1507.04808",
      "author" : [ "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau" ],
      "venue" : null,
      "citeRegEx" : "Serban et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li" ],
      "venue" : "arXiv preprint arXiv:1503.02364",
      "citeRegEx" : "Shang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "Short text classification: A survey",
      "author" : [ "Song et al.2014] Ge Song", "Yunming Ye", "Xiaolin Du", "Xiaohui Huang", "Shifu Bie" ],
      "venue" : "Journal of Multimedia,",
      "citeRegEx" : "Song et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2014
    }, {
      "title" : "2015a. A hierarchical recurrent encoder-decoder for generative context-aware query suggestion",
      "author" : [ "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "Jian-Yun Nie" ],
      "venue" : "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,",
      "citeRegEx" : "Sordoni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "2015b. A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "arXiv preprint arXiv:1506.06714",
      "citeRegEx" : "Sordoni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Short text classification in twitter to improve information filtering",
      "author" : [ "Dave Fuhry", "Engin Demir", "Hakan Ferhatosmanoglu", "Murat Demirbas" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "Sriram et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sriram et al\\.",
      "year" : 2010
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112",
      "author" : [ "Oriol Vinyals", "Quoc VV Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "A neural conversational model. arXiv preprint arXiv:1506.05869",
      "author" : [ "Vinyals", "Le2015] Oriol Vinyals", "Quoc Le" ],
      "venue" : null,
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "An efficient gradient-based algorithm for online training of recurrent network trajectories",
      "author" : [ "Williams", "Peng1990] Ronald J Williams", "Jing Peng" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Williams et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 1990
    }, {
      "title" : "Demonstration of a pomdp voice dialer",
      "author" : [ "Jason Williams" ],
      "venue" : "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Demo Session,",
      "citeRegEx" : "Williams.,? \\Q2008\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 2008
    }, {
      "title" : "Ccg supertagging with a recurrent neural network",
      "author" : [ "Xu et al.2015] Wenduan Xu", "Michael Auli", "Stephen Clark" ],
      "venue" : "In ACL’15,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Pomdp-based statistical spoken dialog systems: A review",
      "author" : [ "Milica Gasic", "Blaise Thomson", "John D Williams" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "Young et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2013
    }, {
      "title" : "Question classification using support vector machines",
      "author" : [ "Zhang", "Lee2003] Dell Zhang", "Wee Sun Lee" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Recently, with the large amount of conversation data available, learning a response generator from data has drawn a lot of attention (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015).",
      "startOffset" : 133,
      "endOffset" : 196
    }, {
      "referenceID" : 12,
      "context" : "Recently, with the large amount of conversation data available, learning a response generator from data has drawn a lot of attention (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015).",
      "startOffset" : 133,
      "endOffset" : 196
    }, {
      "referenceID" : 10,
      "context" : "They either totally ignores linguistic context (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015) or simply considers context for every message (Sordoni et al.",
      "startOffset" : 47,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "They either totally ignores linguistic context (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015) or simply considers context for every message (Sordoni et al.",
      "startOffset" : 47,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : ", 2015; Vinyals and Le, 2015) or simply considers context for every message (Sordoni et al., 2015b; Serban et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "On such short texts, classic NLP tools such as POS Tagger and Parser suffer from bad performance (Derczynski et al., 2013; Foster et al., 2011) and it is difficult to explicitly extract features that are discriminative on the two types of messages.",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "On such short texts, classic NLP tools such as POS Tagger and Parser suffer from bad performance (Derczynski et al., 2013; Foster et al., 2011) and it is difficult to explicitly extract features that are discriminative on the two types of messages.",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 22,
      "context" : ", (Young et al., 2013)) which rely on hand-crafted features and rules to generate reply sentences for specific applications such as voice dialling (Williams, 2008) and appointment scheduling (Janarthanam et al.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 20,
      "context" : ", 2013)) which rely on hand-crafted features and rules to generate reply sentences for specific applications such as voice dialling (Williams, 2008) and appointment scheduling (Janarthanam et al.",
      "startOffset" : 132,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : ", 2013)) which rely on hand-crafted features and rules to generate reply sentences for specific applications such as voice dialling (Williams, 2008) and appointment scheduling (Janarthanam et al., 2011) etc.",
      "startOffset" : 176,
      "endOffset" : 202
    }, {
      "referenceID" : 8,
      "context" : ", recent effort focuses on exploiting an end-toend approach to learn a response generator from social conversation data for open domain dialogue (Koshinda et al., 2015; Higashinaka et al., 2016).",
      "startOffset" : 145,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : ", recent effort focuses on exploiting an end-toend approach to learn a response generator from social conversation data for open domain dialogue (Koshinda et al., 2015; Higashinaka et al., 2016).",
      "startOffset" : 145,
      "endOffset" : 194
    }, {
      "referenceID" : 10,
      "context" : "(Ritter et al., 2011) employed a phrase-based machine translation model for response generation.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "In (Shang et al., 2015; Vinyals and Le, 2015), neural network architectures were proposed to learning response generators from one-round conversation data.",
      "startOffset" : 3,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "(Serban et al., 2015) proposed a hierarchical neural network architecture to building context-aware response generation.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "RNN models (Elman, 1990), due to their capability of modeling sequences with arbitrary length, have been widely used in many natural language processing tasks such as language modeling (Mikolov et al.",
      "startOffset" : 11,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "RNN models (Elman, 1990), due to their capability of modeling sequences with arbitrary length, have been widely used in many natural language processing tasks such as language modeling (Mikolov et al., 2010) and tagging (Xu et al.",
      "startOffset" : 185,
      "endOffset" : 207
    }, {
      "referenceID" : 21,
      "context" : ", 2010) and tagging (Xu et al., 2015) etc.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Recently, it is reported that Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) as two special RNN models which can capture long term dependencies in sequences outperform state of the art methods on tasks like machine translation (Sutskever et al.",
      "startOffset" : 126,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : ", 2014) as two special RNN models which can capture long term dependencies in sequences outperform state of the art methods on tasks like machine translation (Sutskever et al., 2014) and response generation (Shang et al.",
      "startOffset" : 158,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : ", 2014) and response generation (Shang et al., 2015).",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "Our work belongs to the scope of short text classification (Song et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "Existing applications of short text classification include query classification (Kang and Kim, 2003), tweet classification (Sriram et al., 2010), and question classification (Zhang and Lee, 2003).",
      "startOffset" : 123,
      "endOffset" : 144
    } ],
    "year" : 2016,
    "abstractText" : "While automatic response generation for building chatbot systems has drawn a lot of attention recently, there is limited understanding on when we need to consider the linguistic context of an input text in the generation process. The task is challenging, as messages in a conversational environment are short and informal, and evidence that can indicate a message is context dependent is scarce. After a study of social conversation data crawled from the web, we observed that some characteristics estimated from the responses of messages are discriminative for identifying context dependent messages. With the characteristics as weak supervision, we propose using a Long Short Term Memory (LSTM) network to learn a classifier. Our method carries out text representation and classifier learning in a unified framework. Experimental results show that the proposed method can significantly outperform baseline methods on accuracy of classification.",
    "creator" : "LaTeX with hyperref package"
  }
}