{
  "name" : "1303.5718.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Advances in Probabilistic Reasoning",
    "authors" : [ "Dan Geiger", "David Beckerman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper discuses multiple Bayesian net works representation paradigms for encod ing asymmetric independence assertions. We offer three contributions: (1) an inference mechanism that makes explicit use of asym metric independence to speed up computa tions, (2) a simplified definition of similarity networks and extensions of their theory, and (3) a generalized representation scheme that encodes more types of asymmetric indepen dence assertions than do similarity networks.\n1 Introduction\nTraditional probabilistic approaches to diagnosis, clas sification, and pattern recognition face a critical choice: either specify precise relationships between all interacting variables or make uniform independence assumptions throughout. The first choice is computa tionally infeasible except in very small domains, while the second, which is rarely justified, often yields inad equate conclusions.\nBayesian networks offer a compromise between the two extremes by encoding independence when possible and dependence when necessary. They allow a wide spec trum of independence assertions to be considered by the model builder so that a practical balance can be es tablished between computational needs and adequacy of conclusions.\nAlthough Bayesian networks considerably extend tra ditional approaches, they are still not expressive enough to encode every piece of information that might reduce computations. The most obvious omissions are asymmetric independence assertions stating that vari ables are independent for some but not necessarily for all of their values. Such asymmetric assertions can not be represented naturally in a Bayesian network. Several researchers observed this limitation, however, until recently no effort was made to remove it.\nSimilarity network paradigm IS the first ma jor effort towards the representation of asym metric independence [Heckerman, 1990]. Contin gent influence diagrams is an alternative approach [Fung and Shachter, 1991]. Both schemes employ asymmetric independence to ease the elicitation and improve the quality of probabilistic models.\nThis article offers three contributions: ( 1) an inference mechanism that makes explicit use of asymmetric in dependence to speed up computations, (2) a simplified definition of similarity networks and extensions of their theory, and (3) a generalized representation scheme that encodes more types of asymmetric independence assertions than do similarity networks.\nThese contributions address problems of knowledge representation, inference, and knowledge acquisition. In particular, Section 2 describes Bayesian multinets and how to use them for inference, Section 3 describes knowledge acquisition using similarity networks and how to convert them to Bayesian multinets, Section 4 extends these representation schemes to the case where hypotheses are not mutually exclusive and section 5 summarizes the results. We assume the reader is famil iar with the definition and usage of Bayesian networks. For details consult [Pearl, 1988].\n2 Representation and Inference\n2.1 Bayesian Multinets\nThe following example demonstrates the problem of representing asymmetric independence by Bayesian networks:\nA guard of a secured building expects three types of persons to approach the building's entrance: workers in the building, approved visitors, and spies. As a person approaches the building, the guard notes its gender and whether or not the person wears a badge. Spies are mostly men. Spies always wear badges in order to fool the guard. Visitors\ndon't wear badges because they don't have one. Female-workers tend to wear badges more often than do male-workers. The task of the guard is to identify the type of person approaching the building.\nA Bayesian network that represents this story is shown in Figure 1. Variable h in the figure represents the cor rect identification. It has three values w, v, and s re spectively denoting worker, visitor, and spy. Variables g and b are binary variables representing, respectively, the person's gender and whether or not the person wears a badge. The links from h to g and from h to b reflect the fact that both gender and badge-wearing are clues for correct identification, and the link from g to b encodes the relationship between gender and badge-wearing.\nUnfortunately, the topology of this network hides the fact that, independent of gender, spies always wear badges and visitors never do. The network does not show that gender and badge-wearing are conditionally independent given the person is a spy or a visitor. A link between g and b is drawn merely because gender and badge-wearing are related variables when the per son is a worker.\nFigure 1: A Bayesian network for the secured-building example.\nWe can more adequately represent this story using two Bayesian networks shown in Figure 2. The first net work represents the cases where the person approach ing the entrance is either a spy or a visitor. In these cases, badge-wearing depends merely on the type of person approaching, not on its gender. Consequently, nodes b and g are shown to be conditionally indepen dent (node h blocks the path between them) . The links from h to b and from h to g in this network re flect the fact that badges and gender are relevant clues for distinguishing between spies and visitors. The sec ond network represents the hypothesis that the person is a worker, in which case gender and badge-wearing are related as shown.\nFigure 2 is a better representation than Figure 1 be cause it shows the dependence of badge-wearing on gender only in context in which such a relationship exists, namely, for workers. Moreover, the former rep resentation requires 11 parameters while the represen tation of Figure 2 requires only 9. This gain, due to asymmetric independence, could be substantially larger for real-sized problems because the number of\nparameters needed grows exponentially in the num ber of variables, whereas the overhead of representing multiple networks grows only linearly.\nWe call the representation scheme of figure 2, a Bayesian multinet.\nDefinition Let { u1 • . . Un} be a finite set of variables each having a finite set of values, P be a probabil ity distribution having the Cartesian product of these sets of values as its sample space, and h be a dis tinguished variable among the ui's that represents a mutually-exclusive and exhaustive set of hypotheses. Let A1, ... , Ak be a partition of the values of h. A di rected acyclic graph Di is called a local network of P (associated with Ai) if it is a Bayesian network of P given that one of the hypotheses in A; holds, i.e., D; is a Bayesian network of P (u1 ... un[Ai)· The set of k local networks is called a Bayesian multinet of P.1\nIn the secured-building example of Figure 2, { {spy, visitor}, {worker}} is a partition of the values of the hypothesis node h, one local net work is a Bayesian network of P(h, b, gj worker) and the other local network is a Bayesian network of P(h, b, g[ {spy, visitor}). 2 The fundamental idea of multinets is that of condition ing; each local network represents a distinct situation conditioned that hypotheses are restricted to a speci fied subset. Savings in computations and space occur because, as a result of conditioning, asymmetric inde pendence assertions are encoded in the topology of the local networks. In the example above, conditional in dependence between gender and badge-wearing is en coded as a result of conditioning on h.\nNotably, conditioning may also destroy independence relationships rather then create them [Pearl, 1988]. However, if the distinguished variable is a root node (i.e., a node with no incoming links) , conditioning on\n1 A Bayesian multinet roughly corresponds to an hypothesis-specific similarity network as defined in Hecker man's dissertation (1990, page 76).\n2The conditioning set {spy, visitor} is a short hand nota tion for saying that h draws its values from this set, namely, either h = spy or h = visitor.\n120 Geiger and Heckerman\nits values never decreases and often increases the num ber of independence relationships, resulting in a more expressive graphical representation. Other situations are addressed below where the hypothesis variable is not a root node or where more than one node repre sents hypotheses.\n2.2 Representational and Computational Advantages\nThe vanishing dependence between gender and badge wearing is an example of an hypothesis-specific inde pendence because it is manifest only when condition ing on specific hypotheses, that is, for spies and visi tors, but not for workers. The following variation of the secured-building example demonstrates an addi tional type of asymmetric independence that can be represented by Bayesian multinets as well.\nThe guard of the secured building now ex pects four types of persons to approach the building's entrance: executives, regu lar workers, approved visitors, and spies. The guard notes gender, badge-wearing, and whether or not the person arrives in a limou sine (l). We assume that only executives arrive in limousines and that male and fe male executives wear badges just as do regu lar workers (to serve as role models).\nThis story is represented by the two local networks shown in Figure 3. One network represents a situation where either a spy or a visitor approaches the building, and the other network represents a situation where ei ther a worker or an executive approaches the building. The link from h to l in the latter network reflects the fact that arriving in limousines is a relevant clue for distinguishing between workers and executives. The absence of this link in the former network reflects the fact that it is not relevant for distinguishing between spies and visitors.\nThe vanishing dependence between gender and the hy pothesis variable h when h is restricted to a subset of hypotheses {worker, executive} is an example of subset independence. Similarly, badge-wearing is independent of h when restricted to {worker, executive}, and arriv ing in limousines is independent of h when restricted to {spy, visitor}. 3\nSubset independence is a source of considerable com putational savings. For example, in lymph-node pathology less than 20% of the potential morpholog ical findings are relevant for distinguishing any given pair of disease hypotheses (among over 60 diseases) [Heckerman, 1990].\n3Heckerman coined the terms subset independence and hypothesis-specific independence in his dissertation.\nBelow we demonstrate these computational savings us ing the simple secured-building example; more sav ings are obtained in real domains such as lymph-node pathology.\nSuppose the guard sees a male (g) wearing a badge (b) approaches the building and suppose the guard doesn't notice whether or not the person arrives in a limousine. A computation of the posterior probability of each possible identification (executive, worker, vis itor, spy) based on the Bayesian network of Figure 1 simply yields the chaining rule:\nP(h[g, b) = K · P(h) · P(g[h) · P(b[g, h). (1) where K is the normalizing constant.\nUsing the representation of Figure 3, however, the fol lowing more efficient computations are done instead:\nP(spy[g, b) = K · P(spy) · P(g[spy) · P(b[spy) (2) P(visitor[g, b)= K ·?(visitor) · P(g[visitor)· P(b[ visitor) (3) P(worker[g, b)= K ·?(worker) · P(g[worker)·\nP(b[g, worker) ( 4) P(g, b[executive) = P(g, b[worker). (5)\nEquations 2 and 3 take advantage of an hypothesis specific independence assertion, namely, that g and b are conditionally independent given, respectively, that h = spy and h = visitor. Equation 5 uses a subset independence assertion, namely, that b and g are in dependent of h restricted to {worker, executive}.\nMore generally, calculating the posterior probability of each hypothesis based on a set of observations e1, ... , em is done in two steps. First, for each hypothe sis h;, the probability P(e1, ... , em[hi) is computed via standard algorithms such as Spiegelhalter and Lau ritzen's (88) or Pearl's (88). Second, these results are combined via Bayes' rule:\n(6)\nNotably, the computation of P( e1 . .. ek [h;) in the first step uses the local networks as done in Eqs. (2) through\n(5) and does not use a single Bayesian network as done in Eq. (1). Consequently, when the values of h are properly partitioned, the extra independence relation ships encoded in each local network could considerably reduce computations.\nThe parameters needed to perform the above compu tations consist, as we shall see next, of the prior of each hypothesis hi and the parameters encoded in the local networks:\nTheorem 1 Let { u1 . . . u,} be a finite set of variables each having a finite set of values, P be a probability distribution having the Cartesian product of these sets of values as its sample space, h be a distinguished vari able among the uis, and M be a Bayesian multinet of P. Then, the posterior probability of every hypothe sis given any value combination for the variables in { u1 . . . u,} can be computed from the prior probability of h 's values and from the parameters encoded in M.\nAccording to Eq. 6 above, the only parameters needed for computing the posterior probability of each hy pothesis hi, aside of the priors, are p( vz ... v, /hi) where Vz . . . v, are arbitrary values of Uz . . . Un (assum ing without loss of generality that h = u! ) . Let D; de note a local network in M, Ai be the hypotheses asso ciated with Di, and hi be an hypothesis in A;. Clearly, p(vz . . . vn/hi) is equal to p(vz ... vn/hi,Ai) because hi logically implies the disjunction over all hypothe ses in Ai . The latter probability is computable from the local network Di by any standard algorithm (e.g., [Pearl, 1988]), thus, the former is also computable as needed. 0\nFor example, P(g/ worker, {worker, executive}) is equal to the probability P(g/worker) because worker logically implies the disjunction worker V executive. In fact, P(g/ worker, {worker, executive}) is also equal to P(g/ {worker, executive}) because g and worker are independent given {worker, executive} as shown in Figure 3. In this example, the needed prob ability P(g/ worker) is equal to the given one P(g/ {worker, executive}), however in general, the needed probabilities are computed via standard infer ence algorithms.\n2.3 Overcoming some Limitations\nThe multinet approach described thus far is especially beneficial when the hypothesis variable can be mod eled as a root node because, then, no dependencies are ever introduced by conditioning on the different hypotheses. However, the hypothesis node cannot al ways be modeled as a root node. For example, in the secured-building story, suppose there are two indepen dent reports indicating possible spying, say, for mili tary and economical reasons respectively. Such a priori factors for correct identification are modeled as parent\nAdvances in Probabilistic Reasoning 121\nnodes of h, called, say, economics and military having no link between them to show their mutual indepen dence. The resulting network in this case is simply economics -+ h <- military. However when h assumes the value spy, an induced link is introduced between its parents economics and military; one explanation for seeing a spy changes the plausibility of the other explanation, thus making the two variables economics and military be not indepen dent conditioned on h = spy. Consequently, an in duced link must be drawn between the economics and military nodes in the local network for spies vs. visi tors to account for the above dependency. This link would not appear in the full Bayesian network be cause economics and military are marginally indepen dent (they become dependent only when conditioning on h = spy). Such induced links are often hard to quantify and therefore, constructing a single local net work is sometimes harder than constructing the full network, as is the case in the above example.\nOne approach to handle this situation is to first con struct a Bayesian network that represents only a priori factors that influence the hypotheses, ignoring any ev idential variables (such as gender, badge-wearing, and limousines). In our example, this network would be economics -+ h <- military. Then, use this network to revise the a priori probabilities of the different hy potheses. Finally, construct local networks ignoring a priori factors (as done in Figure 2) and use the result ing multinet with the revised priors of h to compute the posterior probability of h as determined by the evidential clues. This decomposition technique works best if a priori factors are independent of all clues con ditioned on the different hypotheses. That is, in situ ations that can be modeled with Bayesian networks of the form shown in Figure 4 where all paths between a priori factors r;'s and evidential clues f; 's pass through h.\nWhen a network of this form cannot serve as a justi fiable model, another approach can be used instead; compose a Bayesian multinet ignoring a priori fac tors, construct a Bayesian network from the local net works by taking the union of all their links (e.g., the union of all links in Figure 2 yields the Bayesian net work of Figure 1). Finally, add a priori factors to the resulting network. This approach was proposed in [Heckerman, 1990]. The disadvantage of this method is that in the pro cess of generating a Bayesian network from a multinet, one encodes asymmetric independence in the parame ters rather than in the topology of the Bayesian net work. Consequently, these asymmetric assertions are not available to standard inference algorithm to speed up their computations.\nNevertheless, this approach is still the best alternative for decomposing the construction of large Bayesian\n122 Geiger and Heckerman\nFigure 4: A Bayesian network where all paths be tween a priori factors r;'s and evidential clues f; 's pass through h.\nnetworks having topologies more complex than that of Figure 4. Such decomposition techniques are cru cially needed due to the overwhelming details of real life problems. Additional issues of knowledge acquisi tion are discussed below.\n3 Knowledge Acquisition/ Representation\n3.1 Similarity Networks\nRecall the guard that must distinguish between work ers, executives, visitors and spies. In this story, some variables do not help distinguish between certain hy potheses. For example, gender and badges do not help distinguish between workers and executives, and limousines do not help distinguish between spies and visitors. In richer domains, large numbers of variables are often not relevant for distinguishing between cer tain hypotheses.\nUnfortunately, the Bayesian multinet approach re quires full specification of all variables in each local network even when they are not relevant to distin guish between the hypotheses associated with that lo cal network. For example the relationship between b and g is encoded in the local network for spies vs. vis itors although these variables do not help distinguish between this pair of hypotheses (Figure 3). Assess ing such relationships, in contexts where they are not relevant, poses insurmountable burden on the expert consulted as is demonstrated by the following quote [Heckerman, 1990):\n\"When the expert pathologist was asked questions of the form\nGiven any disease, does observing feature x change your belief that you will observe feature y ?\nthe expert sometimes would reply\nI've never thought about these two features at the same time before. Feature x is relevant to only one set of diseases, while feature y is only relevant to another set of diseases. These sets of diseases do not over lap, and I never confuse the first set of diseases with the second.\"\nThe solution is to simply include in each local network only those variables that are relevant for distinguishing between the hypothesis covered by that local network.\nHowever, by doing so, valuable information for correct identification might be lost. For example, the rela tionships between badge-wearing and gender in Fig ure 3 would be lost. To compensate for such losses of information, additional local networks must be con structed.\nFor example, the secured-building can be represented with three local networks shown in Figure 5 rather than two as in Figure 3. One network is used to dis tinguish between spies and visitors, another between visitors and workers, and a third between workers and executives. In each local network we include only those variables relevant to distinguishing the hypotheses cov ered by that local network. In particular, the relation ship between badge-wearing and gender is not included in the local network for workers vs. executives as in Figure 3. This relationship, however, is included in the local networks for visitors vs. workers because it helps distinguish between these two hypotheses. The reason for not loosing needed information is that the three local networks are based on a connected cover of hypotheses (rather than a partition) .\nDefinition A cover of a set A IS a collection {A1, ... , Ak} of non-empty subsets of A whose union is A. Each cover is a hypergraph, called the similar ity hypergraph, where the A;'s are edges and elements of A are nodes. A cover is connected if the similarity hypergraph is connected.\nIn Figure 5, {spy, visitor}, {visitor, worker}, {worker, executive} is a cover of the hypotheses set. This cover is connected because it is simply a four-nodes chain spy-visitor-worker-executive which, by definition, is a connected hypergraph. The set { {spy, visitor}, {worker, executive}} is also a cover but it is not con nected. The set { {worker, executive, visitor}, {visitor, spy}} is an example of a connected cover that is a hy pergraph which is not a graph.\nDefinition Let U = { u1 ... un} be a finite set of variables each having a finite set of values, P be a\nSpy /Visitor Visitor /Worker\nWorker /Executive\nFigure 5: A similarity network representation of the secured-building story.\nprobability distribution having the cross product of these sets of values as its sample space, and h be a distinguished variable among the u;'s that represents a mutually-exclusive and exhaustive set of hypothe ses. Let At, . . . , Ak be a connected cover of the values of h. A directed acyclic graph D; is called a compre hensive local network of P (associated with A;) if it is a Bayesian network of P assuming one of the hy potheses in A; holds, i.e., D; is a Bayesian network of P(u1 . . . un[A;). The network obtained from D; by removing nodes that are not relevant to distinguishing between hypotheses in A; is called an ordinary local network. The set of k ordinary local networks is called an (ordinary) similarity network of P.\nFor example, the local networks of Figure 5 are or dinary, and together form an ordinary similarity net work. Notably, hypotheses covered by each local net work are often similar (e.g., spies and visitors) , 4 a choice that maximizes the number of asymmetric in dependence relationships encoded.\nHeckerman (1990) shows that under several assump tions, if a cover is connected, one can always remove from each local network variables that do not help dis tinguish between hypotheses covered by that local net work and yet not loose the information necessary for representing the full joint distribution. These assump tions consist of 1) the hypothesis variable is a root node, 2) the cover is a graph and not a hypergraph, 3) the local networks are constrained by the same par tial order, and 4) the distribution is strictly positive.\n4Hence the name: similarity network.\nAdvances in Probabilistic Reasoning 123\nTheses assumptions are relaxed below.\nTheorem 2 Let { u1 • • • un} be a finite set of variables each having a finite set of values, P be a probability distribution having the Cartesian product of these sets of values as its sample space, h be a d�\"stinguished van· able among the u;s, and S be a similarity network of P. Then, the posterior probability of every hypothe sis given any value combination for the variables in { Ut . • • un} can be computed from the parameters en coded in S provided p(h;) f' 0 for every value h; of h.\nTo prove the above theorem, it suffices to consider the case where h is a root node in all the local networks of S because, otherwise, arc-reversal transformations [Shachter 1986[ can be applied until h becomes one. Also note that since the similarity hypergraph is con nected, it imposes n- 1 independent equations among the following n: p(h;) = p(h;[Ai) · 2:h,EA, p(hi), i = 1. .. n. In addition, 2:� p(hi) = 1. The values for p(hi) are the unique solution of these linear equa tions provided p( hi) f' 0 for i = 1 ... n. Aside of the priors, the only remaining parameters needed for computing the posterior probability of each hypothesis h;, are p( v2 . .. Vn [hi) where v2 . . . Vn are ar bitrary values of u2 • • • !l.n (assuming without loss of generality that h = ut ) . Due to the chaining rule, p(v2 .. . vn[hi) can be factored as follows:\np(v2 . . . vn[h;) = P(v2[h;) · P(v3[v2 hi) . . . p(vn[Vt . . . Vn-1 h;).\nThus, it suffices to show that for each variable UJ, p(vi[v2 ... Vj-1 hi) can be computed from the param eters encoded in S.\nLet D; denote a local network in S, A; be the hy potheses associated with D;, and h; be an hypothesis in A;. There are two cases; either ui is depicted in D; or it is not. Let A;, A;+l . . . Am be a path in the similarity hypergraph where Am is the only edge on this path associated with a local network that depicts Uj as a node. If UJ is depicted in D;, then the path consists of one edge A; which is equal to Am. If UJ is not depicted in any local network, then Uj does not alter the posterior probability of any hypothesis and is therefore omitted from the computations.\nLet Dk be the local netowrk associated with Ak for k = i + 1 .. . m and let hi+ I, hi+2 . . . hm be a sequence of hypotheses such that hk E Ak-l n Ak. Due to the definition of similarity networks, since ui i� not depicted in Dk where k < m, the following equality must hold:\np(vj[V2 · · · Vj-1 hk-tl = p(vJh . . . Vj-1 hk)· Since this equation holds for every k between i + 1 and m, we obtain,\np(vih · . . Vj-1 h;) = p(vJ[v2 . . . v1-1 hm)·\n124 Geiger and Heckerman\nMoreover,\nwhere u� ... u� are the variables depicted in Dm (a subset of { u2 • . • Uj- t}) because, due to the definition of similarity network, the variables deleted are condi tionally independent of Vj, given the other variables; they are disconnected from all the other variables in Dm· 5\nFinally,\np(vj[v� ... v� hm) = p(vj[v� ... v� hm, Am), because hm logically implies the disjunction over all hypotheses in Am.\nThe latter probability is computable from the lo cal network Dm by any standard algorithm (e.g., [Pearl, 1988]), thus, due the three equalities above, p(vifv2 ... Vj-l h,) is also computable as needed. D\nFor example, to compute P(g, b, lfspy) we use the fol lowing two equalities implied by Figure 5: From the first local network, P(g, b, lfspy) = P(gfspy) · P(b[spy) · P (l [ spy) and from the absence of l in the first and second local networks, P(lfspy) = P(l[worker). Thus, P(g, b, lfspy) = P(gfspy) · P(bfspy) · P(l[worker), where all the needed probabilities are encoded in the similar ity network. In fact, the proof of Theorem 2 provides a general way of factoring any desired probability, thus, the full joint distribution P(g, b, l, h) is encoded in the ordinary similarity network of Figure 5.\nSimilarity networks have another important advantage not mentioned so far : protecting the model builder from omitting relevant clues. For example, suppose workers and executives often arrive with a smile to work (because the secured buHding is such a great place to be in) while spies and visitors arrive seriously. Such a clue, smile, is likely to be forgotten when con structing the local networks for spies vs. visitors and for visitors vs. executives because it does not help dis tinguish between these pairs of hypotheses. However, when constructing the similarity network of Figure 5, which includes a local network for distinguishing vis itors from workers, smile is more likely to be recalled because the distinctions between visitors and workers are explicitly in focus.\n3.2 Redundancy\nBasing the construction of local networks on covers of hypotheses raises the problem of redundancy, namely, that some parameters are specified in more than one local network. For example, in Figure 5, the parameter P (g [ visitor) should, in principle, be specified both in the first and in the second local network. This problem\n\"Geiger and Beckerman (1990) discuss weaker defini tions of being irrelevant other than being disconnected.\nis particularly crucial because local networks are actu ally constructed from expert's judgments rather than from a coherent probability distribution as implied by the definition of similarity networks.\nOne way to remove redundancy is to automatically translate a similarity network as it is being constructed to a Bayesian multinet which is never redundant. For example, instead of storing Figure 5, we can actually store Figure 3 which contains no redundant informa tion.\nThe translation is done by the following algorithm.\nConversion Algorithm\nInput: A similarity network S of a probability distri bution P.\nOutput: A Bayesian multinet of P.\n1. For each ordinary local network L in S :\n• Add a node for each variable not represented in L.\n• For each added node x, set the parents of x in L to be the union of all parents of x in all other local networks where x originally appeared, excluding variables that were orig inally in L.\n2. Remove enough local networks from S and enough hypotheses from the remaining local networks un til a Bayesian multinet is obtained.\n(A finer version of this algorithm is forthcoming). Notably, the user of a similarity network need not know about the conversion to a Bayesian multinet which can be thought of as an internal representation. The user benefits from both the advantages of simi larity network for knowledge acquisition, and from an inference algorithm (Section 2) that uses the Bayesian multinet produced by the conversion algorithm.\n4 Generalized Similarity Networks\nPrevious sections assume all hypotheses are mutually exclusive and are, therefore, represented as values of a single hypothesis variable denoted h. Here this as sumption is relaxed. We allow several variables to rep resent hypotheses, as needed by the following example:\nConsider the guard of Section 2 who has to distinguish between workers, visitors, and spies. A pair of people approach the building and the guard tries to classify them as they approach. Assume that only workers con verse (c) and that workers often arrive with other workers (because they must car-pool to conserve energy).\nA Bayesian network representing this situation is shown in Figure 6 where nodes h1 and h2 stand for the respective identity of the two persons. (The direc tion of the link between h1 and h2 is arbitrary.)\nFigure 6: A Bayesian network with two hypothesis nodes h1 and h2.\nAlternatively, we can represent this example using a generalized similarity network, or a generalized Bayesian multinet.\nDefinition Let { u1 ... un} be a finite set of variables each having a finite set of values, P be a probability distribution having the cross product of these sets of values as its sample space, and H be a subset of dis tinguished variables among the u;'s each representing a set of hypotheses. Denote the Cartesian product of the sets of values of the distinguished variables by domain(H ). Let A1, . . , Ak be a connected cover of domain(H ). A directed acyclic graph D; is called a comprehensive local network of P if it is a Bayesian network of P(u1 ... un]Ai). The network obtained from D; by removing nodes that are not relevant to distinguishing between hypotheses in A; is called an ordinary local network. The set of k local networks is called a generalized similarity network of P. When A1, ... , Ak is a partition of domain(H), then the set of k comprehensive local networks is called a generalized Bayesian multinet.\nFor example, the secured-building story is repre sented in the generalized similarity network of Fig ure 7. Note, H = {h1,h2} and domain(H} con sists of nine elements (x, y) where both x and y are drawn from the set { w, v, s }. A con nected cover of domain(H) upon which Figure 7 is based consists of: { (s, s) (v, s) (s, v) (v, v)}, { (v, v) (w, v) (v, w) (w, w)} , and {(s, s) (s, w) (w, s)}. This cover is connected.\nMost asymmetric independence assertions encoded in Figure 7 were either explained in previous sections or are obvious from the verbal description of the story.\nThe absence of a link between h1 and h2 in the top network encodes the fact that if the guard knew that one person is a spy, this knowledge would not help him/her decide whether the other person is a spy or a visitor. The existence of a link between h1 and h2 in the middle network encodes the fact that workers come in pairs more often than do visitors. Hence the\nAdvances in Probabilistic Reasoning 125\nknowledge that one person is a worker is a clue for classifying the other person.\nThe vanishing dependence between hypothesis vari ables h1 and h2 in case of spies vs. visitors is an exam ple of inter-hypothesis independence. Such asymmet ric assertions cannot be encoded in ordinary similarity networks.\n5 Summary\nThis paper proposes an efficient format for encoding and using asymmetric independence assertions for in ference. The model builder is asked to express knowl edge about independence by constructing multiple lo cal networks using informal guidelines of causation and time ordering. Like any Bayesian network, local net works possess precise semantics in terms of indepen dence assertions and these can be used to verify 1) whether the network faithfully represents the domain and 2) whether the input is consistent.\n126 Geiger and Heckerman\nMultiple local networks have several advantages com pared to a single Bayesian network. The elicitation of several small networks is easier than eliciting a sin gle full-scale Bayesian network because the expert can focus his/her attention to particular subdomains, and hence, provide more reliable judgments. Multiple net works represent a domain better because more knowl edge about independence is qualitatively encoded. Al gorithms for finding the most likely hypothesis run faster when using multiple networks. And finally, the overall storage requirement of multiple networks is of ten smaller than that of a single Bayesian network because as independence assertions become more de tailed, less numeric parameters are needed for describ ing a domain.\nNotably, when independence assertions in the domain are symmetric, a single Bayesian network is preferable.\nThe challenges remain to 1) devise additional graphical representation schemes of salient patterns of indepen dence assertions, (2) provide computer-aided elicita tion procedures for constructing these representations, and (3) devise efficient inference procedures that make use of the encoded assertions.\nReferences\n[Geiger and Heckerman, 1990] Geiger D., and H _ e?k\neman, D. (1990). Separable and transitive graphoids. Sixth Conference on Uncertainty in Ar tificial Intelligence.\n[Heckerman, 1990] Heckerman, D. (1990). Probabilis t�·c s�·milarity Networks. PhD thesis, Program in Medical Information Sciences, Stanford University, Stanford, CA.\n[Fung and Shachter, 1991] Contingent Influence Dia grams. Submitted for publication.\n[Lauritzen and Spiegelhalter, 1988] Lauritzen, S.L.; and Spiegelhalter, D.J. 1988. Lo cal Computations with Probabilities on Graphical Structures and Their Application to Expert Sys tems (with discussion) . Journal Royal Statistical Society, B, 50(2):157-224.\n[Pearl, 1988] Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible In ference. Morgan Kaufmann, San Mateo, CA.\n[Shachter, 1986] Shachter, R. (1986). Evaluating In fluence Diagrams. Operations Research 34:871-882.\n[Verma and Pearl, 1988] Verma, T. and Pearl, J. (1988). Causal networks: Semantics and expres siveness. In Proceedings of Fourth Workshop on Uncertainty in Artificial Intelligence, Minneapolis, MN, pages 352-359. Association for Uncertainty in Artificial Intelligence, Mountain View, CA."
    } ],
    "references" : [ {
      "title" : "Lo­ cal Computations with Probabilities on Graphical Structures and Their Application to Expert Sys­ tems (with discussion",
      "author" : [ "Lauritzen", "Spiegelhalter", "1988] Lauritzen", "S.L", "D.J. Spiegelhalter" ],
      "venue" : null,
      "citeRegEx" : "Lauritzen et al\\.,? \\Q1988\\E",
      "shortCiteRegEx" : "Lauritzen et al\\.",
      "year" : 1988
    }, {
      "title" : "Causal networks: Semantics and expres­",
      "author" : [ "T. 1988] Verma", "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Verma and Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Verma and Pearl",
      "year" : 1988
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "This paper discuses multiple Bayesian net­ works representation paradigms for encod­ ing asymmetric independence assertions. We offer three contributions: (1) an inference mechanism that makes explicit use of asym­ metric independence to speed up computa­ tions, (2) a simplified definition of similarity networks and extensions of their theory, and (3) a generalized representation scheme that encodes more types of asymmetric indepen­ dence assertions than do similarity networks.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}