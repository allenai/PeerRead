{
  "name" : "1605.04731.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CNN based texture synthesize with Semantic segment",
    "authors" : [ "Xianye Liang", "Bocheng Zhuo", "Peijie Li", "Liangju He" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "has been applied to solve problems in the subarea of Image-generating, which has been widely applied in areas\nsuch as photo editing, image design, computer animation, real-time rendering for large scale of scenes and for\nvisual effects in movies. However in the texture synthesize procedure. The state-of-art CNN can not capture the\nspatial location of texture in image, lead to significant distortion after texture synthesize, we propose a new way to\ngenerating-image by adding the semantic segment step with deep learning algorithm as Pre-Processing and analyze\nthe outcome. Key words: convolutional neural network; semantic segment; texture synthesize"
    }, {
      "heading" : "1. Introduction",
      "text" : "Convolutional Neural Networks was first applied in document recognition and Le-net 5 achieve much higher recognition precision than other recognition algorithm [1]. The development of deep learning is quite exciting in recent years, particular in computer vision area, Over the past three years DCNNs have pushed the performance of computer vision systems to soaring heights on a broad array of high-level problems, including image classification(Krizhevsky[2] et al., 2013; Sermanet[3] et al., 2013; Simonyan & Zisserman[4], 2014; Szegedy[5] et al., 2014; Papandreou et al., 2014), fine-grained categorization (Zhanget[6] al., 2014), among others.( Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs)\nThe way CNN model processing in image detection and recognition is simulating human being.in recent research, The features generate within CNN model is quite suitable for imagesynthesis.Image-generating is mainly aim to make a new graph combine by content of an origin image and style(texture) of a target image, in general, there are two ways to texture synthesis.\na.Procedural texture synthesis. This method utilized biologically motivated method to generate texture. Greg Turk [7] texture an image by simulating a reaction-diffusion process on it. Walter and Fournier [8] synthesis mammalian coat patterns based on cell division and cell-to-cell interactions. Although these method performs well the parameter of model is hard to optimized into ideal texture.\nb.Texture synthesis from samples. In this method the texture is produce by one or some of target sample image, Zhu et. al. [9] model texture as a Markov Random Field and use Gibbs sampling for synthesis. Portilla和 Simoncelli [10] propose a model combine all sort of wavelet\nfeatures and their coefficients, it was considered to be the state of art model in texture synthesis. Efros [11] convert an input image into a specific stylization, through extract patterns as texture patches from existing artworks and proved to be a success in synthesizing in painting. Recently, generative models based on deep neural networks have shown exciting new perspectives for image synthesis [12,13,14,15]. Deep architectures capture appearance variations in object classes beyond the abilities of pixel-level approaches.\nIn general image synthesize procedure, the whole target image will be synthesized by one texture image, it will cause severe distortion in the objects of the target image. In this paper we\npropose to add semantic segmentation step before synthesize.\n2.Image segment\nIn general image synthesize procedure, the whole target image will be synthesized by one texture image, it will cause severe distortion in the objects of the target image. In this paper we propose to add semantic segmentation step before synthesize. Once we get the origin image, we extract the foreground from the background (person, dog, cat, etc.), attribute to the development of deep learning in image semantic segmentation, which has been one of the most active topics in the field of image understanding and computer vision for a long time. we can get the foreground image directly in pixel level.\nsome of the recently proposed approaches to this task are based on the fully convolutional\nnetwork(FCN) [16] trained end-to-end, pixels-to-pixels which is efficient and at the same time has achieved the state-of-the-art performance. By reusing the computed feature maps for an image, FCN avoids redundant re-computation for classifying individual pixels in the image. FCN becomes the standard approach to dense prediction and methods were proposed to further improve this framework, e.g., the DeepLab [17], deconvNet[18]and CRF-RNN[8]. One key reason for the success of these methods is that they are based on rich features learned from the very large ImageNet [19] dataset, often in the form of a 16-layer VGGNet [20]. However, currently, there exist much improved models for image classification, e.g., the ResNet [21, 22].\nIn this paper we compare four models in semantic segmentation in VOC 2012 test in Tab 1.\nWe found that the CRF-RNN model has the best performance in accuracy, so we finally choose the CRF-RNN as our segment tool.\nThe key idea of CRF inference for semantic labelling is to formulate the label assignment\nproblem as a probabilistic inference problem that incorporates assumptions such as the label agreement between similar pixels. CRF inference is able to refine weak and coarse pixel-level label predictions to produce sharp boundaries and fine-grained segmentations. Therefore, intuitively, CRFs can be used to overcome the drawbacks in utilizing CNNs for pixel-level labeling tasks. Importantly, with our formulation, the whole deep network, which comprises a traditional CNN and an RNN for CRF inference, can be trained end-to-end utilizing the usual back-propagation algorithm\nThe model comprises a fully convolutional network stage (used the FCN-8s architecture of\n[15], which provides unary potentials to the CRF. This network is based on the VGG-16 network), which predicts pixel-level labels without considering structure, followed by a CRF-RNN stage, which performs CRF-based probabilistic graphical modelling for structured prediction. The complete system, therefore, unifies strengths of both CNNs and CRFs and is trainable end-to-end using the back-propagation algorithm. and the Stochastic Gradient Descent (SGD) procedure.\nDuring training, a whole image (or many of them) can be used as the mini-batch and the error at each pixel output of the network can be computed using an appropriate loss function such as the softmax loss with respect to the ground truth.\n3.Style matching\nafter extract the forward and backward image, we use the method in [13] to match the texture from style set. We compare AlexNet [2], GoogLeNet [5], andVGG-19, with AlexNet performing similarly to GoogLeNet and VGG-16 similarly to VGG-19. VGG networks perform much better at style transfer due to their architecture. For example, AlexNet and GoogLeNet strongly compress the input at the first convolutional layer using large kernels and stride (11 × 11 with stride 4 and 7 × 7 with stride 2 respectively) and thus a lot of fine detail is lost. VGG networks use 3 × 3 kernels with stride 1 at all convolutional layers and thus capture much more information. We have therefore used the VGG-19 network for our model.\nTo characterise a given vectorised texture x in our model, we first pass x through the\nconvolutional neural network and compute the activations for each layer l in the network. Since each layer in the network can be understood as a non-linear filter bank, its activations in response to an image form a set of filtered images (so-called feature maps). A layer with Nl distinct filters has Nl feature maps each of size Ml when vectorised. These feature maps can be stored in a matrix Fl∈RNl×Ml, whereFjkl is the activation of the jth filter at position k in layer l. Textures are per definition stationary, so a texture model needs to be agnostic to spatial information. A summary statistic that discards the spatial information in the feature maps is given by the correlations between the responses of Textures are per definition stationary, so a texture model needs to be agnostic to spatial information. A summary statistic that discards the spatial information in the feature maps is given by the correlations between the responses of different features. These feature\ncorrelations are, up to a constant of proportionality, given by the Gram matrix l l N NlG R   ,\nwhere l\nijG is the inner product between feature map i and j in layer l:\n(1)l l lij ik ik k G F F\nA set of Gram matrices {G1, G2, ..., GL} from some layers 1,2 . . , L in the network in response to a given texture provides a stationary description of the texture.\nTo generate a new texture on the basis of a given image, we use gradient descent from a\nwhite noise image to find another image that matches the Gram-matrix representation of the original image. This optimisation is done by minimised the mean-squared distance between the entries of the Gram matrix of the original image and the Gram matrix of the image being generated (Fig. 1B).\nLet x and y be the original image and the image that is generated, and Gl and Ql their respective Gram-matrix representations in layer l (Eq. 1). The contribution of layer l to the total loss is then\n  2\n2 2 ,\n1\n4\nl l\nl ij ij\ni jl l\nE G Q N M   (2)\nand the total loss is\n0\n( , ) L\nll l Loss x y w E   (3)\nwhere wl are weighting factors of the contribution of each layer to the total loss. The derivative of El with respect to the activations in layer l can be computed analytically:\n   2 2 1\n0\n0 0\nT l l l l\nij ij ij l jil ll lij\nij\nF G Q if FE N M F if F            \n(4)\nThe gradients of El, and thus the gradient of Loss(x, y), with respect to the pixels y can be readily computed using standard error back-propagation [23]. The entire procedure relies mainly on the standard forward-backward pass that is used to train the convolutional network. Therefore, in spite of the large complexity of the model, texture generation can be done in reasonable time using GPUs and performance-optimised toolboxes for training deep neural networks [24].\nin order to preserve the shape of forward image，we chose the forward image instead of\nwhite noise. we show textures generated by our model from different source images.as show in Fig3-B, the detail content been loss due to the spatial irrelevant in the feature map on synthesize procedure, by apply the segment procedure in target texture image Picasso, the detail content has been reserve in Fig3-C. The final synthesize result can be seen in Fig 4.\n4.Conclusion\nThis paper presents a new way to texture synthesize which use two CNN(VGG-16 and VGG-19 respectively). This method reduced the texture distortion among different region in one image. In future research, establish an integrated texture database and intelligent texture matching algorithm will further realize the AI technology in painting."
    } ],
    "references" : [ {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "In Proc. IEEE,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1998
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Untangling local and global deformations in deep convolutional networks for image classification and sliding window detection",
      "author" : [ "G. Papandreou", "I. Kokkinos", "Savalle", "P.-A" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Clonal mosaic model for the synthesis of mammalian coat patterns",
      "author" : [ "M Walter", "A Fournier" ],
      "venue" : "Proc of Graphics Interface. Vancouver: BC Press,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "Filters, random fields and maximum entropy (frame)",
      "author" : [ "S.C. Zhu", "Y. Wu", "D. Mumford" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "A parametric texture modelbased on joint statistics of comple x wavelet coefficients",
      "author" : [ "J. PORTILLA", "E.P. SIMONCELLI" ],
      "venue" : "InternationalJournal of Computer Vision 40,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2000
    }, {
      "title" : "Image quilting for texture synthesis and transfer",
      "author" : [ "Efros", "Alexei A", "Freeman", "William T" ],
      "venue" : "Conference on Computer Graphics & Interactive",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "In: NIPS. pp",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "A neural algorithm of artistic style",
      "author" : [ "L.A. Gatys", "A.S. Ecker", "M. Bethge" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "DRAW: A recurrent neural network for image generation",
      "author" : [ "K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks[J",
      "author" : [ "Alex J. Champandard" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "J. Long", "E. Shelhamer", "T. Darrell" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Semantic image segmentation with deep convolutional nets and fully connected CRFs",
      "author" : [ "L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A. Yuille" ],
      "venue" : "Proc. Int. Conf. Learn. Representations, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Han.Learning Deconvolution Network for Semantic Segmentation",
      "author" : [ "Hyeonwoo Noh", "Seunghoon Hong", "Bohyung" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Conditional Random Fields as Recurrent Neural Networks",
      "author" : [ "Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip H.S. Torr" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "ImageNet Large Scale Visual Rcognition Challenge 2012 (ILSVRC 2012)",
      "author" : [ "J. Deng", "A. Berg", "S. Satheesh", "H. Su", "A. Khosla", "L. Fei-Fei" ],
      "venue" : "2012.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Proc.IEEE Conf. Comp. Vis. Patt. Recogn.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Directional Texture Transfer",
      "author" : [ "H. Lee", "S. Seo", "S. Ryoo", "K. Yoon" ],
      "venue" : "In Proc. 8th International Symposium on Non-Photorealistic Animation and Rendering,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Separating style and content with bilinear models",
      "author" : [ "J.B. Tenenbaum", "W.T. Freeman" ],
      "venue" : "Neural computation",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Introduction Convolutional Neural Networks was first applied in document recognition and Le-net 5 achieve much higher recognition precision than other recognition algorithm [1].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "The development of deep learning is quite exciting in recent years, particular in computer vision area, Over the past three years DCNNs have pushed the performance of computer vision systems to soaring heights on a broad array of high-level problems, including image classification(Krizhevsky[2] et al.",
      "startOffset" : 292,
      "endOffset" : 295
    }, {
      "referenceID" : 2,
      "context" : ", 2013; Sermanet[3] et al.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : ", 2013; Simonyan & Zisserman[4], 2014; Szegedy[5] et al.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : ", 2014), fine-grained categorization (Zhanget[6] al.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "Walter and Fournier [8] synthesis mammalian coat patterns based on cell division and cell-to-cell interactions.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "[9] model texture as a Markov Random Field and use Gibbs sampling for synthesis.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "Portilla和 Simoncelli [10] propose a model combine all sort of wavelet features and their coefficients, it was considered to be the state of art model in texture synthesis.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "Efros [11] convert an input image into a specific stylization, through extract patterns as texture patches from existing artworks and proved to be a success in synthesizing in painting.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 9,
      "context" : "Recently, generative models based on deep neural networks have shown exciting new perspectives for image synthesis [12,13,14,15].",
      "startOffset" : 115,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Recently, generative models based on deep neural networks have shown exciting new perspectives for image synthesis [12,13,14,15].",
      "startOffset" : 115,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "Recently, generative models based on deep neural networks have shown exciting new perspectives for image synthesis [12,13,14,15].",
      "startOffset" : 115,
      "endOffset" : 128
    }, {
      "referenceID" : 12,
      "context" : "Recently, generative models based on deep neural networks have shown exciting new perspectives for image synthesis [12,13,14,15].",
      "startOffset" : 115,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "some of the recently proposed approaches to this task are based on the fully convolutional network(FCN) [16] trained end-to-end, pixels-to-pixels which is efficient and at the same time has achieved the state-of-the-art performance.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : ", the DeepLab [17], deconvNet[18]and CRF-RNN[8].",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : ", the DeepLab [17], deconvNet[18]and CRF-RNN[8].",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : ", the DeepLab [17], deconvNet[18]and CRF-RNN[8].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "One key reason for the success of these methods is that they are based on rich features learned from the very large ImageNet [19] dataset, often in the form of a 16-layer VGGNet [20].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "One key reason for the success of these methods is that they are based on rich features learned from the very large ImageNet [19] dataset, often in the form of a 16-layer VGGNet [20].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 18,
      "context" : ", the ResNet [21, 22].",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "The model comprises a fully convolutional network stage (used the FCN-8s architecture of [15], which provides unary potentials to the CRF.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "after extract the forward and backward image, we use the method in [13] to match the texture from style set.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "We compare AlexNet [2], GoogLeNet [5], andVGG-19, with AlexNet performing similarly to GoogLeNet and VGG-16 similarly to VGG-19.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "The gradients of El, and thus the gradient of Loss(x, y), with respect to the pixels y can be readily computed using standard error back-propagation [23].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "Therefore, in spite of the large complexity of the model, texture generation can be done in reasonable time using GPUs and performance-optimised toolboxes for training deep neural networks [24].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "Reference [1]LeCun, Y.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "[2]Krizhevsky, A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3]Sermanet, P.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4]Simonyan, K.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6]Papandreou, G.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[8]M Walter, A Fournier.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[9]S.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[10]PORTILLA, J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[11]Efros , Alexei A, Freeman, William T.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "Conference on Computer Graphics & Interactive Techniques2001:341-346 [12]Goodfellow, I.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "[13]Gatys, L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "06576,2015 [14]Gregor, K.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "04623,2015 [15] Alex J.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 13,
      "context" : "[16]Long, J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[17] L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[18] Hyeonwoo Noh, Seunghoon Hong, Bohyung Han.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[19]Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "03240,2015 [20] J.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 18,
      "context" : "[21] K.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[23] Lee, H.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[24] Tenenbaum, J.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2016,
    "abstractText" : "Deep learning algorithm display powerful ability in Computer Vision area, in recent year, the CNN has been applied to solve problems in the subarea of Image-generating, which has been widely applied in areas such as photo editing, image design, computer animation, real-time rendering for large scale of scenes and for visual effects in movies. However in the texture synthesize procedure. The state-of-art CNN can not capture the spatial location of texture in image, lead to significant distortion after texture synthesize, we propose a new way to generating-image by adding the semantic segment step with deep learning algorithm as Pre-Processing and analyze the outcome.",
    "creator" : "Microsoft® Word 2016"
  }
}