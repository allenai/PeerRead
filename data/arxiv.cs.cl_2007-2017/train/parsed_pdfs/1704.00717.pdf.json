{
  "name" : "1704.00717.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "It Takes Two to Tango: Towards Theory of AI’s Mind",
    "authors" : [ "Arjun Chandrasekaran", "Deshraj Yadav", "Prithvijit Chattopadhyay", "Viraj Prabhu", "Devi Parikh" ],
    "emails" : [ "parikh}@gatech.edu,", "virajp}@vt.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI’s mind – get to know its strengths, weaknesses, beliefs, and quirks. We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples (50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. Surprisingly, we find that having access to the model’s internal states – its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image – do not help people better predict its behavior."
    }, {
      "heading" : "1. Introduction",
      "text" : "Background. Internal states of an agent are often inaccessible to other agents. Some primates have overcome this limitation by acquiring, through natural selection, the ability to make sense of, and (to an extent) successfully predict the behavior of other agents [38]. Indeed, our ability to estimate the beliefs, feelings, and actions of other agents in novel situations forms the basis of human social cognition [46]. The capacity to attribute mental states1 to other\n∗ Denotes equal contribution. 1including beliefs, intents, desires, knowledge, etc.\nagents that are different from one’s own, is called Theory of Mind [53]2.\nHumans frequently attribute mental states to fellow humans (conspecifics) and can make reasonable inferences about their behavior. However, upon encountering a novel agent, for instance, an artificially intelligent (AI) agent, can humans estimate its behavior? Research suggests that to understand new entities such as a robot, humans project existing preconceptions and social constructs upon them [26]. However, as recent research has shown [17, 28], the behavior of an AI agent is often quite different from that of a human – sometimes in ways that are surprising. Thus, inferences based on existing social constructs or preconceptions may fail while estimating the behavior of AI agents. In this work, we consider the novel problem of improving a person’s estimate of the behavior of a complex AI agent by\n2Inferences about behavior by attributing mental states to an agent is a theory because a) mental states are not directly observable, and b) the system can be used to predict the behaviors of other agents.\nar X\niv :1\n70 4.\n00 71\n7v 1\n[ cs\n.C V\n] 3\nA pr\n2 01\nfamiliarizing the person with the agent. Further, we explore whether representations of the agent’s internal states aid in this process. Motivation. As AI progresses, we find ourselves working with AI agents increasingly often. Intelligent virtual assistants like Siri, Cortana, Google Assistant, and Alexa make our lives more convenient. Doctors collaborate with IBM’s Watson [24, 59], dividing work based on their expertise to make better informed diagnoses [35]. Visually-impaired users are starting to rely on computer vision algorithms to interpret the world around them [73, 43, 11]. In-vehicle AI in autonomous cars leverage humans’ experience to make decisions in unpredictable situations [70]. There is also an increased interest in using computer vision algorithms for medical imaging [64].\nClearly, in each of these cases it is critical for the human to have a sense for what the AI is good at (vs not), or when the AI might fail and should not be trusted. The human-AI team will be more effective if the human collaborating with the AI agent has a deeper understanding of the AI agent’s behavior. However, AI research has traditionally placed much of the burden on the AI to play its part in the team: to be more accurate [32, 56, 65, 57, 39], more human-like [4, 36, 48, 23, 3, 54, 50, 10, 13], understand our intentions [49, 67, 68], beliefs [22], tendencies [18], contexts [55], and mental states [20, 19]. In this work, we argue that for human-AI teams to be effective, Theory of Mind must go both ways. Humans must also understand the AI’s beliefs, knowledge, and quirks. See Fig. 1. Effective teams. Effective communication involves considering a teammate’s background knowledge, abilities, preferences and modifying one’s interactions accordingly [31]. Indeed, recent studies [21, 72] conclude that the most effective teams are those with members who, among other traits, demonstrated good Theory of Mind abilities. In this work, we consider two tasks that we believe demonstrate varying degrees to which a human understands an AI team member. The first task, Failure Prediction (FP), involves estimating whether an agent in a specific situation will succeed or fail. This is especially relevant for human-AI teams since AI agents often fail in ways that are different from humans [28, 78, 7]. Accurately estimating the success or failure of an agent in a specific situation is one way to measure whether a person understands the strengths and weaknesses of an agent. The second task, Knowledge Prediction (KP), involves estimating the exact response of an agent in a given situation. Making an accurate estimation of the response of the agent requires a deeper understanding of its behavior. Our setup. Effective communication in human teams is task-oriented, with grounding in a common knowledge base [16]. In our work, we consider an AI agent that is capable of grounded natural language communication with humans. Specifically, we consider an agent trained to per-\nform the multi-modal task of Visual Question Answering (VQA) [5, 42, 66, 11, 27]. Given an image and a freeform open-ended natural language question about the image, the AI agent’s task is to answer the question accurately. Call this agent – a VQA model – Vicki. VQA is applicable to scenarios where humans (e.g., visually impaired users, surveillance analysts, etc.) actively elicit information from visual data. It naturally lends itself to humanmachine teams. The human teammates in our experiments are from Amazon Mechanical Turk (AMT). In Failure Prediction (FP), we show AMT subjects an image and a question about the image, and ask them to estimate if Vicki will correctly answer the question. In Knowledge Prediction (KP), subjects are asked to estimate Vicki’s exact response.\nWe study the extent to which humans can accurately estimate the behavior of Vicki. Then, we explicitly aid humans in developing a theory of Vicki’s mind by (1) familiarizing them with Vicki’s actual behavior during a training phase and (2) exposing them to Vicki’s internal states via several existing ‘explanation’ modalities. We evaluate if these explanation modalities aid humans in accurately estimating Vicki’s behavior (FP and KP). Applicability of Theory of Mind to AI. Humans have an innate tendency to anthropomorphize [34]. We often attribute human traits to non-human entities. For instance, Human-Robot Interaction (HRI) research finds that people attribute a ‘personality’ and ‘mind’ to non-human entities like robots. Interestingly, their perception can also vary based on surface features like the robot’s appearance and behavior [12, 26]. Thus, it is evident that humans attempt to understand and reason about actions of non-human entities like robots by attributing mental states to them.\nTheory of Mind involves attributing mental states to other agents. For human agents, these mental states encompass a large range of states, such as desires, beliefs, and knowledge. Some of these notions, like beliefs and knowledge, are clearly applicable to AI. While a Theory of a Human Mind might appear to involve many complex mental states, in practice, it is measured by a fairly simple test – “reading the mind in the eyes” [9]3. In a similar vein, we propose the two tasks of FP and KP as simple yet effective techniques to measure a person’s understanding and estimation of an AI agent’s behavior, i.e., their Theory of an AI’s mind (ToAIM)4. Contributions. The contributions of this work are: 1. We advocate a line of research to study the extent to which humans can build a Theory of AI’s Mind (ToAIM) and develop approaches to aid the process.\n3This test involves looking at a photo of a human’s eyes and choosing one of two adjectives that better describes the person’s mental state.\n4On the subject of whether AI can have a mind at all, a number of philosophers suggest that it can. For instance, in ‘society of mind’ [45], Minsky says that a mind simply emerges as a result of complex interactions between many smaller non-intelligent entities which he calls agents.\n2. As a specific instantiation of this, we consider the problem of VQA where the AI’s task is to answer a free-form natural language question about an image. 3. We conduct large-scale human studies to measure the effectiveness of training, and of different explanation modalities, in helping humans accurately predict the successes, failures,and output responses of a VQA model on question– image pairs. To the best of our knowledge, this is the first evaluation that measures whether interpretability mechanisms do, in fact, allow humans to build a model of AI. 4. We will make the interfaces we used for our human studies publicly available, to enable others to both evaluate a person’s Theory of AI’s Mind and further investigate the effectiveness of training and of explanation modalities in improving the same. 5. Our key findings are that (1) humans are indeed capable of predicting successes, failures, and outputs of the VQA model better than chance. (2) explicitly training humans to familiarize themselves with the model by using just a few examples improves their performance (3) existing explanation modalities do not enhance human abilities at predicting the model’s behavior. While most prior work on interpretabilty has focused on qualitatively demonstrating the role of such explanation modalities in improving human trust, our findings indicate that they do not yet help humans build more accurate mental models of AI. We believe that as computer vision and AI technology matures, developing improved modalities that can aid humans in this process will be critical towards developing successful human-AI teams."
    }, {
      "heading" : "2. Related Work",
      "text" : "AI with a theory of (human) mind. A number of works in AI attempt to develop agents with an understanding of human characteristics and behavior. AI agents employing computer vision have been trained to predict the motivations [67], intentions [49], actions [68], tendencies [18], contexts [55], etc., of humans. In addition, Scassellati [60] examines theories that explain the development of Theory of Mind in children and their applicability to building robots with similar capabilities. More recently, in the domain of abstract scenes, Eyesenbach et al. [22] address the problem of identifying incorrect beliefs in people. The ability to identify false beliefs [71] in other agents is considered an important milestone in the development of Theory of Mind in an agent [8]. Unlike these works where AI agents “understand” humans, our work addresses the converse problem – to have humans understand AI agents, their quirks, weaknesses, and beliefs. Explainable AI. Recently, there has been a thrust in the direction of “explainable” AI agents in vision-related tasks. Introspection vs Justification: Generating explanations for classification decisions has attracted considerable interest. Several works propose introspective explanations based\non internal states of a decision process [77, 61, 30, 79], while others generate justifications consistent with model outputs [58, 33, 51]. Riberio et al. [58] explain the predictions of a classifier by learning an interpretable model locally around the prediction. Hendricks et al. [33] develop a justification system that produces explanations consistent with visual recognition decisions. Natural language vs Visual explanations: Prior art has assessed the usefulness of natural language explanations of model decisions in improving model trust [33]. MacLeod et al. [41] investigate the role of phrasing of a model’s confidence in blind and visually impaired persons’ trust in image captioning models. Park et al. [51] propose a pointing and justification model for VQA that can both justify predictions in natural language and also point to visual evidence. Explicit vs Implicit attention: There is a line of work in designing models that explicitly attend to relevant parts of their input for vision tasks such as object recognition [6, 47], image captioning [75, 15], and VQA [40, 76, 74]. In contrast, recent work by Zhou et al. [80] and Selvaraju et al. [61] expose implicit attention for predictions from CNN-based models as visual explanations. Across these works, the focus is on making AI agents more transparent and capable of explaining their decisions in order to build trust. In our work, we explore the role explanation modalities play in improving a human’s model of the AI, as measured by the human’s accuracy at predicting the AI’s success, failure, and output responses. Failure Prediction. There exists prior art that deals with building models that predict failure modes of systems [7, 78]. Whereas these works employ statistical models to predict failure modes of a base system, we evaluate the role a training phase as well as explanation modalities play when humans perform the same task. In addition to predicting the success or failure of AI agents, we also train humans to more accurately predict the “knowledge”, i.e., the actual output of an AI agent. Humans adapting to technology. A few works [69, 52] observe human strategies while adapting to the limited capabilities of an AI agent in interactive language games. For instance, in a human-AI game of charades, humans modify strategies such as word selection, turn length, and prosody, to adapt to the robot’s limited perceptive abilities. While both these works observe that humans dynamically adapt their behavior while interacting with an AI on a particular task, in our work we explicitly measure to what extent humans have formed an accurate model of the AI. We also evaluate the role that explanation/interpretability modalities play in helping humans build a more accurate model."
    }, {
      "heading" : "3. Meet Vicki",
      "text" : "We instantiate the idea of humans building a Theory of AI’s Mind in the VQA task. Our AI agent (that we call\nVicki) is a VQA model trained to answer a free-form natural language question about an image. Concretely, we use the VQA model by Lu et al. [40]. It is a hierarchical coattention model that models the question at multiple levels of granularity (words, phrases, entire question) and at each level, has explicit attention mechanisms on the image (where to look) as well as the question (which words and phrases to listen to). Among the different variants introduced in [40], we use the alternating co-attention model trained with VGG19 [63] as the CNN to derive image-representations.\nVicki was trained on the VQA dataset [5] train split containing 248349 QI pairs, and outputs one of a 1000 possible answers (most frequent in the train split). Its accuracy on the VQA dataset (test-standard) is 62.2%5 (human accuracy is 83.3%), which was the state-of-the-art at publication [40] and is still competitive today. Despite being 4.7% less accurate than the current state-of-the-art VQA model [37], Vicki’s image and question attention maps provide access to its ‘internal states’ while making a prediction. These maps highlight the regions of the image and words of the question that Vicki attends to. This presents an opportunity to assess the role such explanation modalities can play in aiding humans better predict Vicki’s behavior. Among the various settings explored in [40], we use the question-level image and question attention maps in our experiments. Vicki is Quirky. There are several factors that contribute to Vicki being quirky, in a predictable fashion. Some of these quirks are well-known in VQA literature [2]. Vision is not perfect: Vicki, like most other vision models, has a limited capability to understand the image. Observing Vicki’s behavior during its failures demonstrates its quirks. For instance, when the question asks the color of a small object in the scene, say a soda can, Vicki may simply respond with the most dominant color in the scene. This is clearly evident when we observe the distribution of Vicki’s responses across a diverse set of images [2]. Language is not perfect: Vicki has a limited capability to understand free-form natural language. Vicki seems to converge on a predicted answer after listening to just half the question 49% of the time [2]. So in many cases, it answers questions based only on the first few words of the question alone [2]. Vicki cannot reason: Vicki has no mechanism to leverage external knowledge and reason about common sense. Vicki is poor at compositionality – it is unable to disentangle and recompose concepts seen in training to generalize to unseen test concepts [2]. Vicki does not have an explicit counting mechanism [14]. So it often defaults to the popular answer “2” for “How many” questions. Vicki cannot say much: Since Vicki is a 1000-way classifier, it only has a fixed set of utterances. Vicki answers every question: Vicki was trained only on questions that were relevant to the image. Thus, Vicki does not know how to say “That doesn’t make\n5http://www.visualqa.org/roe.html\nsense.” or “There is no woman in this image.” when asked “What color is the woman’s shirt?” on an image that does not contain a woman. Thus, when posed with a question that is irrelevant to the image, Vicki is forced to provide an answer from its limited vocabulary. Interestingly, because Vicki is a deterministic function of the question and image, observing its response across QI-pairs often gives us a sense for what it might be basing its responses on. Vicki may ignore the image: Vicki picks up on the language priors that are inherent in the world which are easier to leverage than complicated visual signals. For example, when the question “What color is the banana?” is asked, Vicki often ignores the image and answers “yellow”. Vicki is biased: Vicki is very likely to answer “yes” to a yes/no question, and answer “white’ to a “what color” question due to biases inherent in the VQA dataset that it was trained on [29].\nTo get a sense for this, see Fig. 2. The patterns are clear. In top-left, even when there is no grass, Vicki tends to latch on to one of the dominant colors in the image. For top-right, even when there are no people in the image, Vicki seems to respond with what people could plausibly do in the scene if they were present. A priori, one (especially lay people) may not expect this. But when exposed to several examples of Vicki’s responses, it is conceivable that subjects may begin to have an understanding of Vicki’s behavior and consequently form a theory of its Mind."
    }, {
      "heading" : "4. Meet the tasks",
      "text" : "We present two tasks that can measure a human’s understanding of the capabilities of an AI agent such as Vicki. These tasks are especially relevant to human-AI teams since they are analogous to measuring if a human teammate’s trust in an AI teammate is well-calibrated, and if a human can estimate the behavior of an AI in a specific scenario.\nFailure Prediction (FP). In this task, we study the ability of a human to predict the success or failure of Vicki. That is, given an image and a question about the image, we measure how accurately a person can predict if Vicki will successfully answer the question. A person can presumably predict the failure modes of Vicki reasonably well if they have a good sense of Vicki’s strengths and weaknesses. A collaborator who performs well on this task can accurately determine whether they should trust Vicki’s response to a question about an image. Please see a snapshot of the FP interface in Fig. 3a. Note that we do not show the human what Vicki’s predicted answer is.6\nKnowledge Prediction (KP). In this task, we measure the capability of a human to develop a deeper understanding of Vicki’s behavior. Given an image and a question, a person guesses Vicki’s exact response (answer) from a set of its output labels (vocabulary). Recall that Vicki can only say one of a 1000 things in response to a question about an image. Please see a snapshot of the KP interface in Fig. 3b.\n6Otherwise, given an image from the COCO dataset (everyday consumer images) and a question from the VQA dataset (mostly mundane questions about everyday objects and scenes), it would be trivial for the human to verify if Vicki’s predicted answer is right or wrong. In general, one might wonder why a human would need Vicki to answer questions if they are already looking at the image. This may be true for the VQA dataset, but outside of that there are scenarios where the human either does not know the answer to a question of interest (e.g., the species of a bird), or the amount of visual data is so large (e.g., long surveillance videos) that it would be prohibitively cumbersome for them to sift through it. Note that even in this scenario where the human does not know the answer to the question, a human who understands Vicki’s failure modes from past experience would know when to trust its decision (e.g., if the bird is occluded, or the scene is cluttered, or the lighting is bad, or the bird pose is odd, Vicki will fail). Moreover, the idea of humans predicting the AI’s failure (and ToAIM in general) also applies to other scenarios where the human may not be looking at the image, and hence needs to work with Vicki (e.g., blind user, or a human working with a tele-operated robot). In these cases too, it would be useful for the human to have a sense for the contexts and environments and/or kinds of questions for which Vicki can be trusted. In this work, as a first step, we focus on the first scenario where the human is looking at the image and a question while predicting Vicki’s failures and responses.\nWe provide subjects a convenient dropdown interface with autocomplete to choose an answer from Vicki’s vocabulary of 1000 answers.\nIn FP, a good understanding of Vicki’s strengths and weaknesses might lead to good human performance. However, KP requires a deeper understanding of Vicki’s behavior, deeply rooted in its quirks and beliefs. In addition to reasoning about Vicki’s failure modes, one has to guess its exact response for a given question about an image. Note that KP measures subjects’ ability to take reality (the image the subject sees) and translate it to what Vicki might say. High performance at KP is likely to correlate to high performance at the reverse task – take what Vicki says and translate it to what the image really contains. This can be very helpful when the visual content (image) is not directly available to the user. Explicitly measuring this is part of future work. A person who performs well at KP has likely successfully modeled a more fine-grained behavior of Vicki than just modes of success or failure. In contrast to typical efforts where the goal is for AI to approximate human abilities, KP involves measuring a human’s ability to approximate a neural network’s behavior!"
    }, {
      "heading" : "5. Perception of AI",
      "text" : "Before introducing people to Vicki and gauging their expectations from a modern VQA system, we attempt to assess their general impressions of present-day AI. We then observe correlations in their expectations from Vicki with their familiarity with AI, their estimates of AI’s capabilities, and their demographic and socio-economic background. We ask each subject to fill out a survey with questions aimed to collect three types of information: 1. Background information. We collect basic demographic information such as age, gender, educational qualifications, type of residential area, and profession. We also collect socio-economic background information such as employment status and income group.\n2. Familiarity with computers and AI. We ask subjects if their jobs involve computers, if they know how to program, how much time they spend in front of a computer or smartphone, and their familiarity with popular AI assistants such as Siri, Alexa and Google Assistant. We also ask if they are aware of recent advances in AI, especially those trending in popular media, such as Watson [25], AlphaGo [62], machine learning, and deep learning. 3. Estimates of AI’s capabilities. We ask subjects their duration and source of exposure to AI and gather their impressions on the capabilities of modern-day AI systems on a range of tasks. We also ask them about their understanding, trust and sentiment towards modern AI systems, as well as their expectations and predictions for AI in the future. Please find the full list of survey questions and distributions of responses in the appendix. As an interesting tidbit: Fig. 4 shows what % of subjects think certain tasks are “solved”. 80-90% of the subjects think AI today can recognize faces and infer your mood from social media posts. 65-70% of subjects think AI today can recognize handwriting, be creative (write, compose, draw), or drive a car. They are more split on whether AI can describe an image in a sentence. However, most (96%) agree that AI today cannot read our minds! Interestingly, 62% of subjects think that AI can become smarter than the smartest human."
    }, {
      "heading" : "6. Perception of VQA",
      "text" : "To set the baseline for our specific task, we measure people’s current estimates about VQA models. To this end, we briefly introduce Vicki to subjects as an “AI trained to answer questions about images”. We then ask subjects to use their current understanding and expectation of what AI agents can do, to estimate the behavior of Vicki. Subjects fill out the survey described above before doing this task.\nWe study the ability of humans to estimate Vicki’s behavior via the Failure Prediction and Knowledge Prediction tasks. For both tasks, we randomly sample questions from\nthe set of ∼1,400 most frequent questions in the validation set of the VQA dataset [5]. A description of our experimental setup for each task follows."
    }, {
      "heading" : "6.1. Failure Prediction (FP)",
      "text" : "In this task we show a question, an image on which this question was asked in the VQA dataset, and ask subjects if they think Vicki’s response would be correct or wrong. To get ground truth, similar to the VQA accuracy metric [5], we check if Vicki’s response matched at least 3 of 10 human-provided answers in the VQA dataset. Overall, a total of 88 unique subjects participated in our study, providing responses on 1000 QI–pairs. On average, subjects accurately guessed whether Vicki would answer the question correctly (success) or not (failure) 59.88% of the time. The accuracy of always guessing success is 61.52%. While subjects’ performance seems lower than this, normalizing for the prior of each class (success vs. failure), always guessing success drops to 50% but humans are at 54.24%. This shows that even without prior exposure to Vicki, human subjects can predict its failure better than chance.\nWe further measure people’s optimism about Vicki’s abilities. Fig. 5 shows the percentage of QI-pairs that subjects predicted Vicki would answer correctly for different answer types. We find that subjects expect Vicki to answer questions whose answers are numbers (e.g., counting questions starting with “how many”) correctly quite often. Interestingly, today’s VQA models are in fact quite ineffective at counting. The VQA leaderboard shows significant improvements in performance on “other” questions over time, but improvements on “number” questions has stalled [1].\nOverall, subjects demonstrated an average optimism – as measured by % of “correctly” (success) predictions – of 75.46%. Interestingly, subjects who first heard about AI only in the past 6 months and those who thought that it could read minds are amongst the most optimistic (mean optimism >90%). Those who use a Personal Assistant (PA) on their smartphones frequently (PA usage >3 times a day), as well as older or retired populations (age>60 years) are amongst the least optimistic (mean optimism <57%)!"
    }, {
      "heading" : "6.2. Knowledge Prediction (KP)",
      "text" : "In the KP task, we ask subjects what they think Vicki would say in response to a question about an image. Note that the VQA dataset only contains questions about an image that are relevant to the image. In the VQA dataset collection protocol, annotators were looking at the image while asking questions. So a question “What color is the man’s shirt?” would only be asked for an image that contains a man wearing a shirt.\nAs an interesting twist intended to elicit Vicki’s quirky behavior described in Sec. 3, we also paired images with random (and likely irrelevant [54]) questions (e.g., “What are the people doing?” on an image that may not contain people). Recall that Vicki is forced to respond with a limited vocabulary (one of 1000 answers). These samples are useful to measure a person’s understanding of an agent’s responses to any given stimulus – including those that come from a distribution under which the agent has not been trained. Note that FP cannot be evaluated on irrelevant images. The notion of a “correct” answer is ill-defined if a question is not relevant to an image.\nWe performed the KP task7 on 1000 QI-pairs (700 relevant and 300 irrelevant). We collected 25 responses to each pair. A total of 173 unique subjects participated in our study. The accuracy achieved by predicting Vicki’s most popular answer(‘yes’) is 15.79%. Subjects were able to accurately predict Vicki’s response 24.81% of the time. Interestingly, younger subjects (age <20 years) performed the best (mean accuracy of 35.12%), while those who had heard of AI very recently (in the past month), and those who spent less than 1 hour daily on devices, were amongst the worst performers (mean accuracy <20%)."
    }, {
      "heading" : "7. Familiarizing people with Vicki",
      "text" : "In this section we describe our experimental setup to familiarize subjects with Vicki’s behavior. We approach this in two ways – by providing instant feedback about Vicki’s actual behavior on each QI pair once the subject responds, and by exposing subjects to various explanation modalities that reveal Vicki’s internal states. Challenges. Collecting data for this setup is challenging for a couple of reasons: (1) Each subject has to go through a training phase to become familiar with Vicki before we can test them. This results in each task on AMT being unusually long and expensive. It also reduces the subject pool down to those willing to participate in long tasks. (2) Once a subject does one task for us, they cannot do another task because the training / exposure to Vicki would leak over.\n7We ensured that subjects who perform a KP task are not allowed to perform an FP task since subjects who have performed a KP task are familiar with the set of answers that Vicki is capable of producing which influences their expectation of what Vicki can or cannot do.\nThis means we need as many subjects as tasks. This makes data collection quite slow. In light of these challenges, to systematically evaluate the roles of training and exposure to Vicki’s internal states, we focus on a small set of questions. Data. We identify a subset of questions in the VQA [5] validation split that occur more than 100 times. We select 7 diverse questions from this subset that are representative of the different types of questions (counting, yes/no, color, scene layout, activity, etc.) in the dataset8. For each of the 7 questions, we then sample a set of 100 images. For FP, the 100 images per question are random samples from the set of images on which the question was asked in the VQA validation split (VQA-val). For the KP task, these 100 images are random images from VQA-val. Ray et al. [54] found that randomly pairing an image with a question in the VQA dataset results in about 79% of pairs being irrelevant. Recall that this combination of relevant and irrelevant question-image pairs allows us to test subjects’ ability to develop a robust understanding of Vicki’s behavior across a wide variety of inputs. Task setup. Each human study is comprised of 100 QIpairs where a single question is asked across 100 images. The motivation behind keeping the question constant is to make it easier for the subject to pick up trends in Vicki’s responses across images. The annotation task is broken down into a train phase where the person is shown 50 QI-pairs, and a test phase where we evaluate subject’s performance on the remaining 50 QI-pairs."
    }, {
      "heading" : "7.1. Does feedback help?",
      "text" : "To familiarize subjects with Vicki, we provide them with instant feedback during the train phase. Immediately after a subject responds to a QI–pair, we show them whether Vicki actually answered the question correctly or not (in FP) or what Vicki’s response was (in KP). In the train phase, subjects are also shown a live score of how well they are doing and are allowed to scroll through feedback for previous images (of course, they are not allowed to change their answers to previous images). Once training is complete, no further feedback (including running score) is provided and subjects are asked to draw from the intuition they have built in training to best answer all questions in the test phase. Subjects are also paid a bonus if they do particularly well.\nTo evaluate the role of instant feedback, we have 2 subjects do our study with and without instant feedback each, for each of the questions (7) and each task (FP and KP). This results in a total of 28 human studies (with 28 unique human subjects). Even without feedback, subjects still go through all 100 images.\n8What kind of animal is this? What time is it? What are the people doing? Is it raining? What room is this? How many people are there? What color is the umbrella?\nIn FP, always answering correctly would result in an accuracy of 58.29%. We find that subjects do slightly better and achieve 62.66% accuracy on FP, even without prior familiarity with Vicki (No Train). Thus, subjects are already slightly better calibrated with an AI’s capabilities than unbridled optimism (or pessimism). Further, we find that subjects that receive training as instant feedback (IF) achieve 13.09% (absolute) higher mean accuracies than those who do not (see Fig 6); IF vs No Train (No IF) for FP (green).\nIn KP, answering each question with Vicki’s most popular answer overall (‘no’) would lead to an accuracy of 13.4%. Additionally, answering each question with Vicki’s most popular answer for that question9 leads to an accuracy of 31.43%. Interestingly, subjects who are unfamiliar with Vicki (No Train) achieve 21.27% accuracy – better than the most popular answer overall prior, but worse than the question-specific prior over Vicki’s answers. The latter is understandable as subjects unfamiliar with Vicki do not know which of its 1000 possible answers are more likely a priori for each question.\nWe find that mean absolute performance in KP with IF is 51.11%, 29.84% higher than KP without IF (see Fig 6; IF vs No Train (No IF) for KP (blue)). Subjects thus considerably outperform both the ‘most popular answer’ and ‘most popular answer per question’ priors. It is apparent that just from a few (50) training examples, subjects learn to generalize beyond Vicki’s favorites among it’s vocabulary of 1000 answers. Additionally, the 29.84% improvement over No Train for KP is significantly larger than that for FP (13.09%). This is understandable because a priori (No Train setting), KP is a much harder task as compared to\n9Vicki’s most frequent answer (in the train set) to each question is as follows: What kind of animal is this? (Dog) What time is it? (Daytime). What are the people doing? (Standing) Is it raining? (No) What room is this? (Kitchen) How many people are there? (1) What color is the umbrella? (Black)\nFP, due to the increased space of possible subject responses given a QI-pair, and the combination of relevant and irrelevant QI-pairs in the test phase.\nQuestions such as ‘Is it raining?’ have strong language priors – to these Vicki often defaults to the most popular answer (‘no’), irrespective of image. We observe that on such questions, subjects perform considerably better in KP once they develop a sense for Vicki’s inherent bias via instant feedback. For open-ended questions like ‘What time is it?’, feedback helps subjects (1) narrow down the 1000 potential options to the subset that Vicki typically answers with – in this case time periods such as ‘daytime’ rather than actual clock times and (2) identify correlations between visual patterns and Vicki’s answer (as seen in Fig. 2). In other cases like ‘How many people are in the image?’ the space of possible answers is clear a priori, but after IF subjects realize Vicki is not good at detailed counting but does base its count predictions on coarse signals of the scene layout.\nIn Sec. 3, we described how montages (refer to Fig.2) help highlight Vicki’s quirks. In order to test the effectiveness of such montages as a teaching tool, we also experimented with a modification of the KP + IF setting (two unique subjects per question participated in this setting, resulting in an additional 14 human studies). In the train phase of this new setting, instead of individual images, subjects are shown a series of montages, each containing 4 to 16 images across which Vicki gave the same answer to the question. The objective remains the same – to guess what that answer was (with IF provided after each guess). The test phase is kept identical to the KP + IF test phase, with a single image per question and no IF. We find that subjects achieve 41.6% mean accuracy in the test phase of this setting, which is lower than the mean accuracy in the test phase of the KP + IF setting (51.1%). Interestingly, mean accuracy in the train phase of the montage setting is 68.7%, significantly higher than the mean accuracy in the train phase of the KP + IF setting (49.3%). This seems to indicate that while montages make it much easier to guess Vicki’s response correctly by picking out patterns (as seen in Fig. 8 and Fig. 9), the focus on identifying commonalities between groups of images interferes with the ability to pick up on image-level patterns. As a result, subjects do not generalize well to individual images at test time, resulting in worse performance. Keeping the train and test tasks identical (individual images in both cases) is more effective. VQA Researchers. Just as an anecdotal point of reference, we also conducted experiments across experts with varying degree of familiarity with agents like Vicki. We observed that a VQA researcher had an accuracy of 80% versus a computer vision (but not VQA) researcher who had 60% in a shorter version of the FP task without instant feedback. Clearly, familiarity with Vicki plays a critical role in how well a human can predict its oncoming failures. Our studies\nexamine the extent to which lay people can be made familiar with Vicki to better predict its behavior."
    }, {
      "heading" : "7.2. Do explanation modalities help?",
      "text" : "In this section, we briefly describe the different explanation modalities that we utilize to expose Vicki’s internal states to the human subject. In addition to an image and question about the image, we also show the subject one of the three explanation modalities described below. Subjects are asked to use these as hints to perform the task (FP or KP) more accurately. Subjects can leverage the training phase (with instant feedback and a running score) to learn how best they would like to leverage these hints.\nWe experiment with 3 qualitatively different explanation modalities (see Fig 7): Confidence of top-5 predictions. We show subjects Vicki’s confidence in its top-5 answer predictions from its vocabulary as a bar plot10. If Vicki is relatively more confident in its top-1 prediction, it is more likely to be right. If Vicki is confused about the top-5 predictions, it is more likely to be wrong. Attention maps. Recall that Vicki is the co-attention VQA model proposed by Lu et al [40] which jointly reasons about image and question attention (Sec 3). Thus, along with the image we show subjects the spatial attention map over the image that indicates the regions that Vicki is looking at and an attention map over each word of the question highlighting the relative importance of words in the question for Vicki, while producing an answer. We show subjects a legend to interpret what the colors in each attention map indicate. Grad-CAM. In contrast to explicit attention maps described above, we experiment with an implicit attention map. We use the CNN visualization technique by Selvaraju et al. [61], using the attention maps corresponding to Vicki’s most confident answer.\n10Of course, we don’t show the actual top-5 predictions, just the confidence in the predictions.\nWe have 2 subjects perform each of our tasks (2) for each of the explanation modalities (3) for each question (7) resulting in a total of 84 tasks (and unique subjects). Across all studies (including those described in earlier sections), we have collected over 65k responses from 415 unique subjects. Conducting studies in-house in controlled environments at this scale would be prohibitive.\nTo put human FP accuracies (using explanation modalities) in perspective, we experiment with a few automatic approaches to detect Vicki’s failure from its internal states. We find that a decision stump on Vicki’s confidence in its top answer or the entropy of its 1000-way softmax output results in FP accuracy of 60% on our test set. We train a Multilayer Perceptron (MLP) neural network on Vicki’s output softmax and predict success vs failure. This achieves an FP accuracy of 81%11. Training an MLP which takes as input question features (average word2vec embeddings [44] of words in the question) concatenated with image features (fc7 from VGG-19) to predict success vs failure (which we call ALERT following [78]) achieves an FP accuracy of 65%. Note that these methods are trained on about 66% of the VQA–val set (∼81k examples, rest used for validation). Human subjects are trained on only 50 examples.\nAccuracies of subjects in the test phase of both tasks (FP and KP) for different settings of the explanation modalities are summarized in Fig. 6. Recall, all studies that include an explanation modality also include instant feedback 12 (IF) and a running score during training. For reference, we also show performance of subjects with no explanation modality both with and without IF. We observe that on both tasks, subjects shown explanation modalities along with IF show no statistically significant improvement in performance over those shown just IF. In fact, in some cases performance is worse. While piloting these tasks ourselves, we found that it was easy to “overfit” to the explanation modalities and hallucinate patterns when none may exist. While the works introducing some of these modalities assessed their interpretability qualitatively or measured their role in improving human trust, our preliminary hypothesis is that these modalities may not yet help human-AI teams be more accurate in a goal-driven collaborative setting because they do not yet help humans predict the AI’s behavior more accurately.\n11Showing a visualization of this score to a human may make for a good “explanation modality” for FP! Exploring this is part of future work.\n12In real–world settings, we consider familiarizing via instant–feedback, followed by showing explanation modalities, as the natural progression for acquainting subjects with Vicki. Hence, we evaluate the role explanation modalities play on top of instant feedback. Nevertheless, for sake of completeness, studying the effect of showing explanation modalities on subject performance, independent of instant feedback, is part of future work."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We posit that as computer vision (and AI in general) makes progress, human-AI teams are imperative. We argue that for these teams to be effective, it is not only important for the AI to be capable of modeling the intentions, beliefs, strengths and weaknesses of the human, but also for the human to build a Theory of the AI’s Mind (ToAIM). Takehome message #1: We should pursue research directions to help humans build models of the strengths, weaknesses, quirks, and tendencies of AI. This is especially relevant in computer vision where input signals are high dimensional and the models we train are becoming ever more complex. We instantiate these ideas in the domain of Visual Question Answering (VQA). We propose two tasks that help measure the extent to which a human “understands” a VQA model (we call Vicki) – Failure Prediction (FP) and Knowledge Prediction (KP) – where given an input instance (question– image pair) the human has to predict whether Vicki will answer the question correctly or not, and what Vicki’s exact answer will be. We evaluate the roles that familiarity with Vicki and explanation modalities that expose the internal states of Vicki play. Take-home message #2: Lay people indeed get better at predicting Vicki’s behavior using just a few (50) “training” examples. Take-home message #3: Surprisingly, existing explanation modalities that are popular in computer vision do not help make Vicki’s failures more predictable. In fact, humans seem to overfit to the additional information provided and perform slightly worse at KP in the presence of explanation modalities. Take-home message #4: Clearly, much work remains to be done in developing improved explanation modalities that do in fact help make AI more predictable to a human.\nThis work just scratches the surface, and numerous avenues of further exploration remain. Studying other vision models (AI agents in general) at varying points on the interpretability vs performance spectrum for other tasks, evaluating other existing explanation modalities, and conducting human studies at an even larger scale are natural extensions. Relevant to the increased interest in building interpretable models, this work presents novel opportunities to evaluate explanation modalities grounded in specific tasks (FP and KP). Finally, it would be exciting to close the loop and evaluate the extent to which improved human performance at FP and KP translates to improved success of human-AI teams at accomplishing a shared goal. Co-operative humanAI games may be a natural fit for such evaluation. Acknowledgements. We would like to acknowledge the countless hours of effort provided by the workers on Amazon Mechanical Turk. We thank Satwik Kottur for his help with data analysis, and for many fruitful discussions. This work was funded in part by an NSF CAREER award, ONR YIP award, Sloan Fellowship, ARO YIP award, Allen Distinguished Investigator award from the Paul G. Allen Fam-\nily Foundation, Google Faculty Research Award, Amazon Academic Research Award to DP. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor."
    }, {
      "heading" : "3. FP + IF + Explanation Modalities",
      "text" : "• “Even though Vicki is looking at the right spot doesn’t\nalways mean she will guess correctly. To me there was no rhyme or reason to guessing correctly. Thank you.”\n• “I think she can accurately know a small number of people but cannot know a huge grouping yet.”\n• “I would be more interested to find out how Vickis metrics work. What I was assuming is just color phase and distance might not be accurate.”\n4. KP • “Time questions are tricky because all Vicki can do is\nround to the nearest number.”\n• “there were a few that seemed like it was missing obvious answers - like bus and bus stop but not bus station. Also words like lobby seemed to be missing.”\n5. KP + IF • “Interesting, though it seems Vicki has a lot more\nlearning to do. Thank you!”\n• “This HIT was interesting, but a bit hard. Thank you for the opportunity to work this.”"
    }, {
      "heading" : "6. KP + IF + Explanation Modalities",
      "text" : "• “You need to eliminate the nuances of night time and\ndaytime from the computer and choose one phrasing ”night” or ”day” Vicki understands. The nuance keeps me and I’m sure others obtaining a higher score here on this task.”\n• “I felt that Vickie was mistaken as to what some colors were for the first test which probably carried over and I tried my best to recreate her responses.”"
    }, {
      "heading" : "7. KP + IF + Montages",
      "text" : "• “I am not sure that I ever completely understood how\nVicki thought. It seemed it had more to do with what was in the pictures instead of the time of day it looked in the pictures. If there was food, she chose noon or morning, even though at times it was clearly breakfast food and she labeled it noon.”\n• “It doesn’t seem very accurate as I made sure to count and took my time assessing the pictures.”\n• “it is hard to figure out what they are looking for since there isn’t many umbrellas in the pictures”\nOn a high-level reading through all comments, we found that subjects felt that Vicki’s response often revolves around the most salient object in the image, that Vicki is bad at counting, and that Vicki often responds with the most dominant color in the image when asked a color question. In Fig. 10 we show a word cloud of all the comments left by the subjects after completing the tasks. From the comments, we observed that subjects were very enthusiastic to familiarize themselves with Vicki, and found the process engaging. Many thought that the scenarios presented to them were interesting and fun, despite being hard. We used some basic elements of gamification, such as performance–based reward and narrative, to make our tasks more engaging; we think the positive response indicates the possibility of making such human–familiarization with AI engaging even in real–world settings."
    }, {
      "heading" : "C. Survey Questions",
      "text" : "In this section we describe the survey that subjects were asked to answer before carrying out the FP and KP tasks described in Sections 6.1 and 6.2 of the main paper. These questions attempt to assess the subjects’ general impressions of present-day AI, and can be broken down into 3 categories – Population Demographics, Exposure to AI, and Perception of AI.\nAs part of the survey, subjects were asked a few subjective questions about their opinions on present–day AI’s capabilities. Specifically, they were asked to list tasks that they thought AI is capable of performing today (see Fig. 11), will be capable of in the next 3 years (see Fig. 12), and will be capable of in the next 10 years (see Fig. 13).\nWe also asked how they think AI works (see Fig. 14). In Fig. 11, 12 and 13, we show word clouds corresponding to what subjects thought about the capabilities of AI. We also share some of those responses below. 1. Name three things that you think AI today can do. Predict sports games; Detect specific types of cancer in images; Control house temp based on outside weather; translate; calculate probabilities; Predictive Analysis; AI can predict future events that happen like potential car accidents; lip reading; code; Facial recognition; Drive cars; Play Go; predict the weather; Hold a conversation; Be a personal assistant; Speech recognition; search the web quicker. 2. Name three things that you think AI today can’t yet do but will be able to do in 3 years. Fly planes; Judge emotion in voices; Predict what I want for dinner; perform surgery; drive cars; manage larger amounts of information at a faster rate; think independently totally; play baseball; drive semi trucks; Be a caregiver; anticipate a person’s lying ability; read minds; Diagnose patients; improve robots to walk straight; Run websites; solve complex problems like climate change issues; program other ai; guess ages; form conclusions based on evidence; act on more complex commands; create art. 3. Name three things that you think AI today can’t yet do and will take a while (> 10 years) before it can do it. Imitate humans; be indistinguishable from humans; read minds; Have emotions; Develop feelings; make robots act like humans; truly learn and think; Replace humans; impersonate people; teach; be a human; full AI with personalities; Run governments; be able to match a human entirely; take over the world; Pass a turing test; be a human like friend; intimacy; Recognize things like sarcasm and humor.\nInterestingly, we observe a steady progression in subjects’ expectations of AI’s capabilities, as the time span increases. On a high-level reading through the responses, we notice that subjects believe that AI today can successfully\n(see Fig. 13). A major proportion of subjects believe that AI will gain the ability to understand and emulate human beings, teach human beings, develop feelings and emotions and pass the Turing test.\nWe also observe how subjects think AI works (see Fig. 14). Mostly, subjects believe that an AI agent today is a system with high computational capabilities that has been programmed to simulate intelligence and perform certain tasks by exposing it to huge amounts of information, or, as one of subjects phrased it – broadly AI recognizes patterns and creates optimal actions based on those patterns towards some predefined goals. In summary, it appears that subjects have high expectations from AI, given enough time. While it is uncertain at this stage how many, or how soon, these feats will actually be achieved, we believe that building Theory of AI’s mind skills will help humans generally become more active and effective collaborators in human–AI teams.\nWe now provide a full list of all questions in the survey. In Fig. 15, 16 and 17, we also break down the 321 subjects that completed the survey by their response to each question. 1. How old are you?\n(a) Less than 20 years (b) Between 20 and 40 years (c) Between 40 and 60 years (d) Greater than 60 years\n2. What is your gender? (a) Male (b) Female (c) Other 3. Where do you live? (a) Rural (b) Suburban (c) Urban 4. Are you? (a) A student (b) Employed (c) Self-employed\n(d) Unemployed (e) Retired (f) Other\n5. To which income group do you belong? (a) Less than 5000$ per year (b) 5,000-10,000$ per year (c) 10,000-25,000$ per year (d) 25,000-60,000$ per year (e) 60,000-120,000$ per year (f) More than 120,000$ per year 6. What is your highest level of education? (a) No formal education (b) Middle School (c) High School (d) College (Bachelors) (e) Advanced Degree 7. What was your major? (a) Computer Science / Computer Engineering (b) Engineering but not Computer Science (c) Mathematics / Physics (d) Philosophy (e) Biology / Physiology / Neurosciences (f) Psychology / Cognitive Sciences (g) Other Sciences (h) Liberal Arts (i) Other (j) None 8. Do you know how to program/code? (a) Yes (b) No 9. Does your full-time job involve: (a) No computers (b) Working with computers but no programming/coding (c) Programming/Coding 10. How many hours a day do you spend on your computer/laptop/smartphone?\n(a) Less than 1 hour (b) 1-5 hours (c) 5-10 hours (d) Above 10 hours\n11. Do you know what Watson is in the context of Jeopardy?\n(a) Yes (b) No\n12. Have you ever used Siri, Alexa, or Google Now/Google Assistant?\n(a) Yes (b) No\n13. How often do you use Siri, Alexa, Google Now, Google Assistant, or something equivalent?\n(a) About once every few months (b) About once a month (c) About once a week\n(d) About 1-3 times a day (e) More than 3 times a day\n14. Have you heard of AlphaGo? (a) Yes (b) No 15. Have you heard of Machine Learning? (a) Yes (b) No 16. Have you heard of Deep Learning? (a) Yes (b) No 17. When did you first hear of Artificial Intelligence (AI)? (a) I have not heard of AI (b) More than 10 years ago (c) 5-10 years ago (d) 3-5 years ago (e) 1-3 years ago (f) In the last six months (g) Last month 18. How did you learn about AI? (a) School / College (b) Conversation with people (c) Movies (d) Newspapers (e) Social media (f) Internet (g) TV (h) Other 19. Do you think AI today can drive cars fully autonomously?\n(a) Yes (b) No\n20. Do you think AI today can automatically recognize faces in a photo?\n(a) Yes (b) No\n21. Do you think AI today can read your mind? (a) Yes (b) No 22. Do you think AI today can automatically read your handwriting?\n(a) Yes (b) No\n23. Do you think AI today can write poems, compose music, make paintings?\n(a) Yes (b) No\n24. Do you think AI today can read your Tweets, Facebook posts, etc. and figure out if you are having a good day or not?\n(a) Yes (b) No\n25. Do you think AI today can take a photo and automatically describe it in a sentence?\n(a) Yes (b) No\n26. Other than those mentioned above, name three things that you think AI today can do. 27. Other than those mentioned above, name three things that you think AI today can’t yet do but will be able to do in 3 years. 28. Other than those mentioned above, name three things that you think AI today can’t yet do and will take a while (> 10 years) before it can do it. 29. Do you have a sense of how AI works?\n(a) Yes (b) No (c) If yes, describe in a sentence or two how AI works.\n30. Would you trust an AI’s decisions today? (a) Yes (b) No 31. Do you think AI can ever become smarter than the smartest human?\n(a) Yes (b) No\n32. If yes, in how many years? (a) Within the next 10 years (b) Within the next 25 years (c) Within the next 50 years (d) Within the next 100 years (e) In more than 100 years 33. Are you scared about the consequences of AI? (a) Yes (b) No (c) Other (d) If other, explain."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Theory of Mind is the ability to attribute mental states (beliefs, intents, knowledge, perspectives, etc.) to others and recognize that these mental states may differ from one’s own. Theory of Mind is critical to effective communication and to teams demonstrating higher collective performance. To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. Traditionally, there has been much emphasis on research to make AI more accurate, and (to a lesser extent) on having it better understand human intentions, tendencies, beliefs, and contexts. The latter involves making AI more human-like and having it develop a theory of our minds. In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI’s mind – get to know its strengths, weaknesses, beliefs, and quirks. We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples (50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. Surprisingly, we find that having access to the model’s internal states – its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image – do not help people better predict its behavior.",
    "creator" : "LaTeX with hyperref package"
  }
}