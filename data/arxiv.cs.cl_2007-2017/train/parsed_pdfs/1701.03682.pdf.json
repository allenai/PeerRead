{
  "name" : "1701.03682.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LIDE: Language Identification from Text Documents",
    "authors" : [ "Priyank Mathur", "Arkajyoti Misra", "Emrah Budur" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION Automatic language detection is the first step toward achieving a variety of tasks like detecting the source language for machine translation, improving the search relevancy by personalizing the search results according to the query language [1], [2], providing uniform search box for a multilingual dictionary [3], tagging data stream from Twitter with appropriate language etc. While classifying languages belonging to disjoint groups is not hard, disambiguation of languages originating from the same source and dialects still pose a considerable challenge in the area of natural language processing. Regular classifiers based on word frequency only are inadequate in making a correct prediction for such similar languages and utilization of state of the art machine learning tools to capture the structure of the language has become necessary to boost the classifier performance. In this work we took advantage of recent advancement of deep neural network based models showing stellar performance in many natural language processing tasks to build a state of the art language classifier.\nWe benchmarked our solution with the industry leaders and achieved near perfect score in the DSL test dataset."
    }, {
      "heading" : "II. PREVIOUS WORK",
      "text" : "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc. and the best accuracy achieved are still in the lower ninety percents.\nThe researchers have worked on various critical tasks challenging the dimensions of the topic, including but not limited to, supporting low resource languages, i.e. Nepali, Urdu, and Icelandic [10], [11] handling user-generated unstructured short\ntexts, i.e. microblogs [10], [9] building a domain agnostic engine [10], [7]. Existing benchmarking solutions approach the LID problem in different ways where LogR [10] adopts a discriminative approaches with regularized logistic regression, TextCat and Google CLD [12] recruits N-gram-based algorithm, langid.py [4] relies on a Naive Bayes classifier with a multinomial event model.\nThe outstanding results, of the time, suggested by Cavnar and Trenkle became de facto standard of LID even today [6]. The significant ingredient of their method is shown to use a rank order statistic called ”out of place” distance measure [13]. The problem in their approach is that they generated n-grams out of words that requires tokenization. However, many languages including Japanese and Chinese have no word boundaries. Considering that Japanese is the second most frequent language used in Twitter [11], there is a need for better approach to scale the solution to all languages. As a solution to their problem, Dunning came up with a better approach with incorporating byte level n-grams of the whole string instead of char level n-grams of the words [13].\nAfter a rigorous literature survey, we found no prior study that applied deep learning on language identification of text. On the other hand, there are a few number of studies that applied deep learning to identify the language of speech [14], [15], [16], [17]. We believe this study will be the first in the literature if published for LID in textual data by means of deep learning."
    }, {
      "heading" : "III. DATASET DESCRIPTION",
      "text" : "The data for this project work was obtained from ”Discriminating between Similar Language (DSL) Shared Task 2015” [18]. A set of 20000 instances per language (18000 training (train.txt) and 2000 evaluation (test.txt)) was provided for 13 different world languages. The dataset also consisted of a subset (devel.txt) of the overall training data which we utilized for hyper-parameter tuning. The languages are grouped as shown in Table I. The names of the groups will be frequently referred in the subsequent sections.\nEach entry in the dataset is a full sentence extracted from journalistic corpora and written in one of the languages and tagged with the language group and country of origin. A similar set of mixed language instance was also provided to\nar X\niv :1\n70 1.\n03 68\n2v 1\n[ cs\n.C L\n] 1\n3 Ja\nn 20\n17\nhttp://SeeYourLanguage.info\nadd noise to the data. A separate gold test data was provided for the final evaluation (test-gold.txt).\nWe applied t-SNE algorithm to visualize the instances in 3D euclidean space [19], [20]. For feature extraction, we vectorized each sentence over 1 to 5-grams of the tokens delimited by white space characters. Fig. 1 shows the resulting plot. As can be seen on the plot, the languages in the same group overlap a lot while the languages in different groups can be linearly separable. A 3 dimensional visualization of all the languages can be viewed at www.youtube.com/watch?v=mhRdfC26q78."
    }, {
      "heading" : "IV. METHODS",
      "text" : ""
    }, {
      "heading" : "A. Multinomial Naive Bayes",
      "text" : "We created a baseline result by training a Multinomial Naive Bayes model because it is quick to prototype, runs fast and known to provide decent results in the field of text processing. We have done no pre-processing of the text commonly done in the field like stemming or stop word removal because we believe that could potentially remove important signatures of a particular language, particularly when the same language is spoken by two geographically disconnected group of people (e.g Portuguese spoken in Portugal and Brazil). We experimented with both word and character n-grams. The character n-grams turned out to be particularly useful when\ndifferentiating between two languages using mostly distinct character sequences in their alphabet.\nThe character level n-gram behaves quite differently from that of word level n-grams as shown in Fig. 2. Single characters carry little information and therefore the performance for character n-gram improves quite sharply as the number of characters is increased before saturating at about n=8. We experimented with character n-grams both restricted at word boundaries and spanning across word boundaries. The latter has a marginal performance boost at the cost of longer training time and memory pressure.\nThe word n-gram model peaks at n=2 and drops beyond that. While higher order n-grams carry more structure of the language, they become increasingly infrequent too and therefore the models don’t always get a boost from it. Both the character level and word level n-gram models show similar performance where they really excel at certain languages (Czech, Slovak) and do poorly at other (Bosnian, Croatian, Serbian)."
    }, {
      "heading" : "B. Logistic Regression",
      "text" : "We next tried a regularized logistic regression and here too the character level n-gram performed a little better than the word n-grams. Fig. 3 shows that the model was able to completely fit the training set but the performance on the validation set plateaued close to 0.945. The best performance was obtained by a character 9-gram model that includes all n-grams up to n=9. These n-grams were truncated at the word boundaries, or in other words these n-grams did not capture two or more consecutive words. Relaxing this criterion significantly increases the size of the term frequency matrix and pushes the boundary of the computer memory but it does improve the performance by a fraction of a percent."
    }, {
      "heading" : "C. Recurrent Neural Network",
      "text" : "The MNB and LR approaches work really well in distinguishing two languages that have very little in common because the set of n-grams will have very little overlap between them. This approach does not work very well when two languages are close to each other and share a lot of words between them. Therefore, it becomes necessary to capture the\nstructure of a languages better to distinguish between similar languages. We explored Recurrent Neural Networks (RNN) for this purpose.\nRNNs are a special kind of neural networks which possess an internal state by virtue of a cycle in their hidden units. As such, RNNs are able to record temporal dependencies among the input sequence, as opposed to most other machine learning algorithms where the inputs are considered independent of each other. Hence, they are very well suited to natural language processing tasks and have been successfully used for applications like speech recognition, hand writing recognition etc.\nUntil recently, RNNs were considered very difficult to train because of the problem of exploding or vanishing gradients [22] which makes it very difficult for them to learn long sequences of input. Few methods like gradient clipping have been proposed to remedy this. Recent architectures like Long Short Term Memory (LSTM) [23] and Gated Recurrent Unit (GRU) [24] were also specifically designed to get around this problem. In our experiments, we used single hidden layer recurrent neural networks that used gated recurrent units.\nHyper-parameter tuning: In our single layer networks, we had three model hyper parameters to search over\n1) Epochs - the number of iterations over training data. We generally try to train until the network saturates.\n2) Hidden layer size - Number of hidden units in the hidden layer.\n3) Dropout - Deep neural networks with large number of parameters are very powerful machines but are extremely susceptible to overfitting. Dropout provides a simple way to remedy this problem by randomly dropping hidden units as each example propagates\nthrough the network and back [25].\nWe used a subset of our overall training data (devel.txt) for hyper parameter selection. This subset was further divided into 75% training data and 25% validation data. In the first step of the process, we varied a single parameter while keeping the other two constant. The plots below (fig. 4) show the performance of the resultant models on the validation dataset as each parameter was changed.\nAs we can see in the plot above in Fig.5, increasing the number of training epochs improves the model performance up to a certain stage, after which it plateaus. Hence, for the next stage of tuning, we fixed the number of training epochs to 20. Using the best values for the number of hidden units and dropout found above, we performed grid search over all combinations of these parameters. The result of the grid search is visualized in Fig. 6. The (number of hidden units, dropout) combinations (1280, 0.4) and (768, 0.45) gave us the best performance on the validation set. The final values chosen for further experimentation were 768 hidden units and 0.45 dropout so as to avoid overfitting.\nTraining procedure: Our final model is an ensemble of 5 RNNs, each built using a different feature set, namely, from character 2-grams to character 5-grams and word unigrams. To train our models, we divided our entire training data (train.txt) into 90% training set and 10% validation set. Once trained, we measured the performance of each model individually on the validation set and is reported in Table II.\nAs seen in Fig. 7, to construct the ensemble, instead of manually assigning weights to each model, we constructed a Logistic Regression model to get the final output. The features\nfor this LR model were the outputs from the 5 RNNs created earlier and it was tuned using 5 fold cross validation over the 10% validation dataset.\nFor training the RNNs, we used a Python library called passage, which is built on top of Theano. Although the library provides several tools for text pre-processing including tokenization, it lacked the ability to generate character n-gram level features. Therefore, we had to extend the library with custom character level feature generators. In addition, training neural networks on CPUs consumes a lot of time. Hence, for our experiments, we leveraged AWS GPU (g2.2xlarge) instances that provided a 10x boost in time required to train one model."
    }, {
      "heading" : "V. RESULTS",
      "text" : "Table II shows a comparison of the models we have experimented with. One surprising feature of the result is that individual RNN models were not able to beat the performance of the MNB and LR models, even though the latter models have minimal knowledge of a language structure. However, when we created an ensemble of RNN models, it turned out to be the best model and crossed the 95% threshold for the first time. It should be noted that for a particular n-gram model, MNB and LR models use all m-grams where 1 ≤ m ≤ n. However, due to the very nature of an RNN architecture, a combination of n-grams cannot be used because that will lead to an overlapping sequence of content to be fed to the network. Since any given n-gram captures only limited information about a language, it was natural to try an ensemble of n-gram RNN models with different values of n, so that structure of the language can be captured at multiple different levels.\nThe boost in performance due to ensemble can also be attributed to model combination, which aims to achieve at least as good of a performance as the worst model in the ensemble. This is because individual models can make mistakes on different examples, and therefore, by using an ensemble we are able to reduce this variance. While we tried other model combination strategies like median and manual weighting, building a Logistic Regression classifier on top of RNNs really helped us find the optimal weight that should be given to each individual model. We could not include RNN models beyond character 5-gram in the ensemble because of memory limitation and including the MNB or LR model in the ensemble did\nnot improve the performance of the model."
    }, {
      "heading" : "VI. DISCUSSION",
      "text" : "The final classification for each language group is captured in the confusion matrix in Fig. 8. It is quite evident from our results that the biggest challenge consistently posed to our classifiers is distinguishing the languages in South Western Slavic group (bs, hr, cr). The training set revealed that among all the words in bs, 48% are common to hr and 41% to sr. Since Fig. 3 clearly showed we didn’t underfit the training set, it made sense to augment the training data in these three language categories. We incorporated a significantly larger labeled data for two of these languages and also downloaded newspaper articles in bs, but the classification accuracy in this language group did not improve. Looking closer to some of these external datasets revealed that none of the new words could be uniquely associated to any of the three languages and therefore, the additional data probably added more noise than signal.\nTo understand the failure mechanism of the classifier for the South Western Slavic language group, we fed the LR classifier, which is the best of single models in validation set according to Table II, different fractions of a document it failed to classify correctly. For example, the following document is in Bosnian(bs) but the classifier predicts its language as Serbian(sr): Usto se osvrnuo na ekonomsku situaciju u kojoj je veliki broj novinara u potrazi za poslom, na mizerne plae i guranje etike strane profesije u zapeak. So we fed the classifier with ”Usto” and noted the prediction, then fed it with ”Usto se” and noted the prediction, and so on until the full sentence is fed. The classifier prediction at different stages of the sentence scan is plotted in Fig.9. The top left panel of Fig.9 shows that the classifier for the most part thinks the document to be actually bs, until it saw the last word of the sentence when it switched its prediction to sr. We think this is due to the fact that the last word associated very uniquely to sr in the training corpus. The bottom left panel of Fig.9 shows a similar scenario but in this case the classifier switched back and forth a couple of times. The ’confusion’ of the classifier is very high in the top right panel of the figure because the particular sentence was made of words and phrases that are common to all three languages. We believe that the correct classification of such documents needs creation of extra features based on deeper understanding of this language group. Another possible scenario where any classifier can struggle is when the body of the text contains a quotation of a different language. The bottom right panel of Fig.9 shows a scenario where a document in Serbian had a comment in Portuguese, though that was not the cause of the eventual classification failure. Removing quotes from a document is a potential option but it can also have adverse effect if the quote is in the same language as that of the main document.\nCOMPARISON WITH OTHER SYSTEMS\nWe assessed the performance of LIDE by comparing its result with the domain leaders in an unfair test described\nbelow. We queried the test file of dataset of DSL Shared Task 2015 and accepted the resulting predictions even if\n• the dialect of the language is not distinguished in IberoRomance language group due to lack of support, i.e. Google always predicts Portuguese for sentences both in Brazilian Portuguese and European Portuguese. • a certain language is not supported at all, i.e. Rosette doesn’t support Bosnian.\nTable IV shows the resulting accuracies. Although LIDE had lack of competitive advantage in this unfair test, it surpassed the industry leaders in terms accuracy.\nComparative test design: We queried the test file of dataset of DSL Shared Task 2015 and compared the resulting predictions with the labels in the test-gold file. We didn’t employ any training session with benchmarked solutions, since, these solutions were already trained and claimed to be ready for general purpose use.\nNone of these solutions were able to distinguish the language varieties. i.e.\n• They predicted simply Spanish for Ibero-Romance (Spanish) language group. • They predicted simply Portuguese for Ibero-Romance (Portuguese) language group.\nTherefore we accepted the prediction of the benchmarked solution if it predicted the main language correctly. Otherwise we considered it to be a misprediction.\nTable IV shows the accuracy and Figure 10 shows the confusion matrices of each solution. Below is a comparative analysis relative to our solution.\nGoogle Translate API 1\nGoogle provides a language detection service for 91 languages. LIDE surpassed in distinguishing South Western Slavic group. The accuracy of Google Translate API for this particular group was 38% lower than LIDE which formed the main difference of the overall accuracy between LIDE and Google Translate API.\nYandex Translator API 2\nYandex supports 63 languages. Similar to Google, Yandex failed to distinguish the languages in South Western Slavic group. Hence, it moved off one step in the overall accuracy\n1https://cloud.google.com/translate 2https://tech.yandex.com/translate\nrelative to LIDE. In addition, Yandex showed a very low accuracy of 62.75% in Astronesian group which moved off a second step in the overall accuracy relative to LIDE.\nRosette Language API 3\nRosette Language API supports 54 languages excluding Bosnian. Since Bosnian is excluded in the language inventory of Rosette, we discarded Bosnian sentences and queried the remaining languages. Apart from the Bosnian sentences, Rosette has showed highly similar accuracy characteristics with Yandex Translator API.\nlangid.py\nLangid.py is an off-the-shelf language identification tool and it is considered to be a cornerstone in the literature [4]. langid.py shared very similar accuracy characteristics with Yandex Translator API, with a subtle difference that langid.py came up with slightly higher accuracy both in Astronesian and South Western Slavic groups. It should be noted that langid.py is the software that is owned and used by one of the competitors of DSL 2014, namely UniMelb-NLP.\n3https://developer.rosette.com"
    }, {
      "heading" : "VII. CONCLUSION AND NEXT STEPS",
      "text" : "We have presented a deep neural network based language identification scheme that achieves near perfect accuracy in classifying dissimilar languages and about 90% accuracy on highly similar languages. Specifically, the languages in Western Slavic Slavic group posed the highest challenge. And expanding the corpus of these languages using external sources did not help much mainly because no n-grams of words that are unique to certain languages were ingested by the expanded part of the corpus. We have relied on the ensemble of RNN models to discover the structure unique to a specific language but we could not engineer any additional feature due to lack of knowledge in those specific languages. At this point, we think, further improvement can only be achieved by designing rule based features by talking to language experts or native speakers."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank David Jurgens from Department of Computer Science, Stanford University for helping with the initial idea, dataset and previous research, Junjie Qin from Department of Computational and Mathematical Engineering, Stanford University for his mentoring and insightful comments that polished the outcome of the study, AWS Educate Program for providing EC2 credits and computing resources and Microsoft Azure for Research Program for providing Azure credits and full featured computing resources, and lastly Google, Yandex and Basis Tech for providing free access to their language detection APIs for our benchmarking analysis."
    } ],
    "references" : [ {
      "title" : "Ambiguity of Queries and the Challenges for Query Language Detection",
      "author" : [ "J. Stiller", "M. Gäde", "V. Petras" ],
      "venue" : "CLEF 2010 Labs and Workshops Notebook Papers, 2010. [Online]. Available: http://clef2010. org/resources/proceedings/clef2010labs{ }submission{ }41.pdf",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Query language determination using query terms and interface language",
      "author" : [ "F. Datta", "Ruchira S. Lopiano" ],
      "venue" : "Patent US 11/408 245, 2006.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Word Level Language Identification in Online Multilingual Communication",
      "author" : [ "D. Nguyen", "a. S. Do" ],
      "venue" : "vol. 23, no. October, pp. 857–862, 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "langid. py: An off-the-shelf language identification tool",
      "author" : [ "M. Lui", "T. Baldwin" ],
      "venue" : "Proceedings of the ACL 2012 System Demonstrations, no. July, pp. 25–30, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Language identification of names with SVMs",
      "author" : [ "A. Bhargava", "G. Kondrak" ],
      "venue" : "Computational Linguistics, no. June, pp. 693–696, 2010.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "N-Gram-Based Text Categorization",
      "author" : [ "W.B. Cavnar", "J.M. Trenkle", "A.A. Mi" ],
      "venue" : "Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, 1994.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Graph-based N-gram language identification on short texts",
      "author" : [ "E. Tromp", "M. Pechenizkiy" ],
      "venue" : "”Proceedings of the 20th annual Belgian-Dutch Conference on Machine Learning”, pp. 27–34, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Native Language Identification with PPM",
      "author" : [ "V. Bobicev" ],
      "venue" : "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, 2013, pp. 180–187.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Microblog language identification: overcoming the limitations of short, unedited and idiomatic text",
      "author" : [ "S. Carter", "W. Weerkamp", "M. Tsagkias" ],
      "venue" : "Language Resources and Evaluation, vol. 47, no. 1, pp. 195–215, 2012. [Online]. Available: http://www.springerlink.com/ index/10.1007/s10579-012-9195-y",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Language Identification for Creating Language-Specific Twitter Collections",
      "author" : [ "S. Bergsma", "P. Mcnamee", "M. Bagdouri", "C. Fink", "T. Wilson" ],
      "venue" : "”Proceedings of the 2nd Workshop on Language in Social Media at NAACL-HLT’12”, no. Lsm 2012, pp. 65–74, 2012. [Online]. Available: http://www.aclweb.org/anthology-new/W/ W12/W12-2108.pdf",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Half of messages on Twitter are not in English Japanese is the second most used language",
      "author" : [ "Semicoast" ],
      "venue" : "Semiocast, p. 75005, 2010. [Online]. Available: http: //semiocast.com/downloads/Semiocast{ }Half{ }of{ }messages{ }on{ }Twitter{ }are{ }not{ }in{ }English{ }20100224.pdf",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Google Compact Language Detector 2",
      "author" : [ "D. Sites" ],
      "venue" : "2013. [Online]. Available: https://github.com/CLD2Owners/cld2",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Statistical Identification of Language",
      "author" : [ "T. Dunning" ],
      "venue" : "Computing, no. November, 1994.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Automatic language identification using deep neural networks",
      "author" : [ "I. Lopez-Moreno", "J. Gonzalez-Dominguez", "O. Plchot", "D. Martinez", "J. Gonzalez-Rodriguez", "P.J. Moreno" ],
      "venue" : "Icassp-2014, pp. 5337–5341, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep learning for spoken language identification",
      "author" : [ "G. Montavon" ],
      "venue" : "NIPS Workshop on Deep Learning for Speech Recognition and Related Applications, pp. 1–4, 2009.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Deep bottleneck features for spoken language identification",
      "author" : [ "B. Jiang", "Y. Song", "S. Wei", "J.H. Liu", "I.V. McLoughlin", "L.R. Dai" ],
      "venue" : "PLoS ONE, vol. 9, no. 7, pp. 3012–3016, 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automatic Language Identification using Long Short-Term Memory Recurrent Neural Networks",
      "author" : [ "J. Gonzalez-Dominguez", "I. Lopez-Moreno", "H. Sak", "J. Gonzalez- Rodriguez", "P.J. Moreno" ],
      "venue" : "Interspeech- 2014, 2014, pp. 2155–2159.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Accelerating t-SNE using tree-based algorithms",
      "author" : [ "L. Van Der Maaten" ],
      "venue" : "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 3221–3245, 1 2014. [Online]. Available: http://dl.acm.org/citation.cfm? id=2627435.2697068",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "R. Pascanu", "T. Mikolov", "Y. Bengio" ],
      "venue" : "International Conference on Machine Learning, no. 2, pp. 1310–1318, 2013. [Online]. Available: http://jmlr.org/proceedings/papers/v28/pascanu13.pdf",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S.H. Schmidhuber", "Jrgen" ],
      "venue" : "Neural Computation, vol. 9, no. 8, pp. 1735 – 1780, 1997.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning Phrase Representations using RNN Encoder- Decoder for Statistical Machine Translation",
      "author" : [ "K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "arXiv, pp. 1724–1734, 2014. [Online]. Available: http://arxiv.org/abs/1406.1078",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 1 2014. [Online]. Available: http://dl.acm.org/citation.cfm?id=2627435.2670313",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1929
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Automatic language detection is the first step toward achieving a variety of tasks like detecting the source language for machine translation, improving the search relevancy by personalizing the search results according to the query language [1], [2], providing uniform search box for a multilingual dictionary [3], tagging data stream from Twitter with appropriate language etc.",
      "startOffset" : 242,
      "endOffset" : 245
    }, {
      "referenceID" : 1,
      "context" : "Automatic language detection is the first step toward achieving a variety of tasks like detecting the source language for machine translation, improving the search relevancy by personalizing the search results according to the query language [1], [2], providing uniform search box for a multilingual dictionary [3], tagging data stream from Twitter with appropriate language etc.",
      "startOffset" : 247,
      "endOffset" : 250
    }, {
      "referenceID" : 2,
      "context" : "Automatic language detection is the first step toward achieving a variety of tasks like detecting the source language for machine translation, improving the search relevancy by personalizing the search results according to the query language [1], [2], providing uniform search box for a multilingual dictionary [3], tagging data stream from Twitter with appropriate language etc.",
      "startOffset" : 311,
      "endOffset" : 314
    }, {
      "referenceID" : 3,
      "context" : "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.",
      "startOffset" : 274,
      "endOffset" : 277
    }, {
      "referenceID" : 9,
      "context" : "Icelandic [10], [11] handling user-generated unstructured short texts, i.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "Icelandic [10], [11] handling user-generated unstructured short texts, i.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "microblogs [10], [9] building a domain agnostic engine [10], [7].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : "microblogs [10], [9] building a domain agnostic engine [10], [7].",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "microblogs [10], [9] building a domain agnostic engine [10], [7].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "microblogs [10], [9] building a domain agnostic engine [10], [7].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "Existing benchmarking solutions approach the LID problem in different ways where LogR [10] adopts a discriminative approaches with regularized logistic regression, TextCat and Google CLD [12] recruits N-gram-based algorithm, langid.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "Existing benchmarking solutions approach the LID problem in different ways where LogR [10] adopts a discriminative approaches with regularized logistic regression, TextCat and Google CLD [12] recruits N-gram-based algorithm, langid.",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 3,
      "context" : "py [4] relies on a Naive Bayes classifier with a multinomial event model.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "The outstanding results, of the time, suggested by Cavnar and Trenkle became de facto standard of LID even today [6].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "The significant ingredient of their method is shown to use a rank order statistic called ”out of place” distance measure [13].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "Considering that Japanese is the second most frequent language used in Twitter [11], there is a need for better approach to scale the solution to all languages.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "As a solution to their problem, Dunning came up with a better approach with incorporating byte level n-grams of the whole string instead of char level n-grams of the words [13].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, there are a few number of studies that applied deep learning to identify the language of speech [14], [15], [16], [17].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : "On the other hand, there are a few number of studies that applied deep learning to identify the language of speech [14], [15], [16], [17].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : "On the other hand, there are a few number of studies that applied deep learning to identify the language of speech [14], [15], [16], [17].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "On the other hand, there are a few number of studies that applied deep learning to identify the language of speech [14], [15], [16], [17].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 17,
      "context" : "We applied t-SNE algorithm to visualize the instances in 3D euclidean space [19], [20].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : "Until recently, RNNs were considered very difficult to train because of the problem of exploding or vanishing gradients [22] which makes it very difficult for them to learn long sequences of input.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "Recent architectures like Long Short Term Memory (LSTM) [23] and Gated Recurrent Unit (GRU) [24] were also specifically designed to get around this problem.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "Recent architectures like Long Short Term Memory (LSTM) [23] and Gated Recurrent Unit (GRU) [24] were also specifically designed to get around this problem.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "dropping hidden units as each example propagates through the network and back [25].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "py is an off-the-shelf language identification tool and it is considered to be a cornerstone in the literature [4].",
      "startOffset" : 111,
      "endOffset" : 114
    } ],
    "year" : 2017,
    "abstractText" : "The increase in the use of microblogging came along with the rapid growth on short linguistic data. On the other hand deep learning is considered to be the new frontier to extract meaningful information out of large amount of raw data in an automated manner. In this study, we engaged these two emerging fields to come up with a robust language identifier on demand, namely Language Identification Engine (LIDE). As a result, we achieved 95.12% accuracy in Discriminating between Similar Languages (DSL) Shared Task 2015 dataset, which is comparable to the maximum reported accuracy of 95.54% achieved so far.",
    "creator" : "LaTeX with hyperref package"
  }
}