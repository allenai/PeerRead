{
  "name" : "1511.05688.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DICTION INTERVAL ESTIMATION USING NOMINAL VARIABLES",
    "authors" : [ "Ameen Eetemadi", "Ilias Tagkopoulos" ],
    "emails" : [ "eetemadi@ucdavis.edu", "itagkopoulos@ucdavis.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Although most of the well-known regression methods, are designed to provide point prediction for a real valued variable, this is not always sufficient. There are many cases where the stakeholders are interested in knowing the precision of individual predictions. In the machine learning literature this is sometimes referred to as Conformal Prediction (CP) Papadopoulos & Haralambous (2011) and others simply refer to it as providing Prediction Interval (PI) Khosravi et al. (2011). In the general single-task learning (STL) problem, a regression CP method trains a predictive model. The predictive model takes an input feature vector x ∈ Rd, a desired confidence level α ∈ [0, 100] and provides a PI [yl, yu] for the output variable y ∈ R. It is conceivable to have a model which would predict multiple intervals, but we only study the case which providing a single interval is sufficient. Compared to point prediction, such precise information would be much more useful for human decision making in various applications such as clinical diagnosis, financial services and experimental design. PIs also enable getting meaningful predictions by stacking layers of - independently trained - predictors on top of each other. In this work, we provide a novel method called DAPIEN tailored to datasets with nominal input variables. Although DAPIEN can incorporate various regression techniques, we only use Artificial Neural Networks (ANN) in our experiments. This is in part due to the fact that, from a theoretical standpoint, ANNs can model any non-linear function and in the last ten years, there has been\nar X\niv :1\n51 1.\n05 68\n8v 2\n[ cs\n.L G\n] 2\n0 N\nov 2\nrecord breaking achievements through the application of ANNs particularly in speech recognition, computer vision, machine translation and predictive genomics in the context of deep learning Hinton et al. (2006); Seide et al. (2011); Lee et al. (2009); ?.\nSection 2 contains a discussion on existing methods for PI prediction with focus on ANNs. The detailed description DAPIEN is provided in section 3. Experimental results using synthetic data is provided in section 4. Conclusion and future work is briefly discussed in section 5."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "An extensive review for neural network-based prediction intervals is provided in Khosravi et al. (2011). Next we provide a concise summary of PI assessment measures and also discuss the latest developments in PI prediction methods, some of which have not been reviewed together before."
    }, {
      "heading" : "2.1 PI ASSESSMENT MEASURES",
      "text" : "When it comes to assessing the quality of PIs, it is common to solely focus on the overall coverage of the calculated intervals [ToDo: Add ref from Khosravi et al. (2011)]. Such an approach can lead into favoring methods that provide wide intervals and hence provide minimal specificity for the predicted outcome. The following are among the most informative PI quality measures.\n• Prediction Interval Coverage Probability(PICP), is calculated by measuring the fraction of target values covered by the PIs\nPICP = |{yi|yi ∈ Targets, yi ∈ [ŷil, ŷiu]|\n|Targets| (1)\n• Mean PI Width(MPIW), quantifies the overall width of the PIs\nMPIW = ∑NTargets i=1 (ŷiu − ŷil) |NTargets|\n(2)\n• Normalized MPIW (NMPIW), enables comparison of PIs using different datasets\nNMPIW = MPIW\nRange(y) (3)\n• Coverage Width-based Criterion (CWC) combines PICP and NMPIW and serves as a single quality measure to enable draw comparisons for multiple methods\nCWC = NMPIW (1 + γ(PICP )e−η(PICP−µ)) (4)\nγ(PICP ) = { 0, P ICP ≥ µ 1, otherwize\n(5)\nA smaller CWC is preferred over larger ones. The µ parameter, gives the user control over the acceptable PICP. By tuning the η parameter, user would be able to define the relative importance of PICP in the overall CWC measure when PICP is bellow the acceptable threshold.\nIn this study, we only report the PICP and MPIW measures as they are easier to interpret and describe well what we aim to achieve. Other PI assessment measures can be useful particularly in studies where either large number of various datasets are involved or the PI prediction method involves a feedback loop to use a PI measure such as CWC to tune the prediction process."
    }, {
      "heading" : "2.2 NEURAL NETWORK-BASED METHODS FOR PROVIDING PREDICTION INTERVALS",
      "text" : "Before introducing the new DAPIEN method, it is important to understand existing methods to see why a method tailored for categorical dataset is necessary."
    }, {
      "heading" : "2.2.1 BOOTSTRAP",
      "text" : "The most common technique for construction of PIs is the Bootstrap method Heskes (1997). It consists of two phases: first, estimate the portion of error which comes from inaccurate modeling and then estimate the error which comes from noise in the observed output itself. In the first phase, the Bootstrap resampling is used to generate B datasets, which are then used to train the prediction method. Although what we describe is applicable in many regressors and classifiers, for the rest of the paper we will focus on artificial neural networks (ANN).\nWhen provided with a new sample for prediction, all trained ANNs will be used to provide with B predictions. The variance of the predictions represents the modeling error. In the second phase, a separate ANN is trained for predicting the output error. This is done by generating a new dataset in which each sample consists of input features and the residual errors. For each sample, the residual error is generated by subtracting the modeling error from the prediction error in the first phase. Given a new sample, the overall error is estimated by adding errors from both phases. The following PI would is provided:\nµ− cconfidenceσ ≤ ŷ ≤ µ+ cconfidenceσ (6)\nThe Cconfidence value is calculated using the t-table, the provided confidence level and B degrees of freedom. The µ is the mean of the predicted value as calculated in the first phase. The value of σ is the estimated by adding the estimated error from both phases."
    }, {
      "heading" : "2.2.2 BAYESIAN AND DELTA",
      "text" : "Although the Bayesian Bishop et al. (1995) and Delta Hwang & Ding (1997) are very different from each other, they have similar properties. Similar to the Bootstrap, they both distinguish between the modeling error and data error. But in contrast to Bootstrap they assume normal distribution for the error (with Delta having stronger assumptions), regardless of the output values. They also include the costly calculation of the Hessian matrix. They both provide high quality PIs when the assumptions for distribution of error are valid. The Bayesian method for training ANN, has particularly strong mathematical foundation and good generalization error Khosravi et al. (2011)."
    }, {
      "heading" : "2.2.3 MVE",
      "text" : "In Mean and Variance Estimation method Nix & Weigend (1994), it is also assumed that the errors are normally distributed around the mean of the target. Although in the Bayesian and Delta methods assume fixed variance, MVE assumes that the variance is also a function of the input. Therefore, it trains two distinct neural networks for estimating the mean and variance of the output. One drawback for this method, is ignoring the modeling error Khosravi et al. (2011) which affects the quality of the predicted intervals."
    }, {
      "heading" : "2.2.4 BACK-PROPAGATION OF PSEUDO-ERRORS",
      "text" : "In Ding & He (2003), the authors consider a case in which the distribution of error is not necessarily normal and can be skewed. The idea is to integrate the box-cox transformation Sakia (1992) during training to model non-Gaussian distribution of noise. It will then use the Bootstrap method for providing PIs."
    }, {
      "heading" : "2.2.5 COMBINED PIS",
      "text" : "Khosravi et al. (2011) proposes an ensemble method using Bootstrap, Bayesian, Delta and MVE methods. First the dataset is separated to two parts D1, D2. D1 is used to train all methods. Then an ensemble approach is taken by providing new PI which is a linear combination of the intervals provided by all four approaches. The parameters of the linear combination is estimated in an optimization procedure to minimize the overall CWC measure over D2. Due to the form of CWC function, gradient descent cannot be used and instead a Genetic Algorithm approach is taken. Authors show that the Combined PI approach consistently outperforms the individual methods when compares based on the CWC measure."
    }, {
      "heading" : "3 METHOD",
      "text" : "All PI prediction methods described in 2.2, deal with the case where the input vector is real valued. Hence they lack the ability to measure and model the variance of output within a group of samples with identical nominal inputs. In other words, although all nominal input vectors can be represented with real valued vectors, but by throwing away the information within each group of samples (e.g. mean, variance, skewness of the output variable) the quality of prediction intervals can be undermined. Here we discuss the problem of estimating confidence interval for an output variable y ∈ R, given the input binary vector x = (x1, x2, ..., xd) where xi ∈ {0, 1} representing nominal features. Assume that the output variable y ∈ R, is a single scalar value. However, the same method can be generalized to multi-task learning for predicting multiple output variables. Also assume that the distribution of y, follows the same form of distribution function regardless of the input parameters, however the parameters of the distribution function can vary depending on the input vector. We show in section 3.2, one way to use this method when distributions do not follow the same function."
    }, {
      "heading" : "3.1 GENERAL PROCEDURE",
      "text" : "The following describes the method outline:\n1. Given a distribution function fθ(y), where ∫ +∞ −∞ fθ(y)dy = 1 and the training set D t =\n{(xi, yi)}, i = 1 . . . N total: (a) Identify the unique input training vectors xu, u = 1 . . . p. For each xu, there is a\ncorresponding yu which consists of all yi records which share the same xi (b) Construct p datasets, each containing samples that have the same input vectors, repre-\nsented by Du = {xu, yu}, u = 1 . . . p, such that xu ∈ Rd, yu ∈ RNu where Nu is the number of samples with input vector x = xu\n(c) Using each dataset Du, estimate θu, representing the parameters of distributions fθ1(y|x1) . . . fθp(y|xp). Note that θu ∈ Rk, where k is the number of parameters in the distribution function f\n(d) Construct new training set Ddist = {(xu, θu)}, u = 1 . . . p 2. Using the new data set Ddist, train a model for predicting θ\n3. Given a new input vector x, the trained model in step 2, provides θ̂ representing distribution fθ̂(y|x)\n4. Given a confidence level α and fθ̂(y|x), a prediction interval will be calculated.\nNext we describe this method for Gaussian and Gamma distributions. The same procedure can be extended to other types of distribution functions."
    }, {
      "heading" : "3.2 GAUSSIAN DISTRIBUTION PROCEDURE",
      "text" : "Consider the case where a single output variable, follows a Gaussian distribution while the parameters of the distribution may vary given input vector x. For providing prediction intervals, the general procedure as in section 3.2, will be followed. In step 1c, for each unique xu the Gaussian distribution parameters θu = (µu, σ2u) would be calculated from each dataset Du. After constructing Ddist using the estimated values of θ, a predictor would be trained to predict θ given a new input. Finally (as in step 4) in order to provide the prediction interval, the Student’s t-distribution will be used. Given a confidence level α and degrees of freedom (nDF ), the cconfidence will be calculated from the t-table. The nDF will be the average number of samples in Dus. Hence the following PI will be provided using the estimated values of µ and σ and the cconfidence taken from Student’s t-distribution:\nµ− cconfidenceσ ≤ yt ≤ µ+ cconfidenceσ (7)\n[missing citation below]"
    }, {
      "heading" : "3.3 GAMMA DISTRIBUTION PROCEDURE",
      "text" : "Although, Gaussian distributions are widely used, they are not applicable to many datasets. For example the steady-state probability distribution of protein number per cell is known to follow a gamma distribution Cai et al. (2006). For a Gamma probability distribution, the general procedure in section 3.2, will be followed. In step 1c, for each unique xu the Gamma distribution parameters θu = (αu, βu, µu) will be estimated (α is the shape parameter, β is the scale parameter, and µ is the location parameter). Then similar to section 3.2, Ddist will be constructed using the xu and estimated values of θ. Then a model will be trained using Ddist as described in 3.2. For providing the prediction interval (as in step 4), the inverse gamma distribution function F−1 would be used (see equation (8)). That is, given a new x, first θ̂ = (α̂, β̂, µ̂), will be predicted. Then using the requested confidence level cconfidence (e.g. 0.95), (α̂, β̂, µ̂) and F−1 the prediction interval will be provided as in equation (9).\npdf(t;α, β, µ) = βα\nΓ(α) (t− µ)e−β(t−µ)\nF−1(p, α, β, µ) = z s.t. ∫ z 0 pdf(t;α, β, µ) dt = p\n(8)\nF−1( 1− cconfidence\n2 , α̂, β̂, µ̂) ≤ yt ≤ F−1( 1 + cconfidence 2 , α̂, β̂, µ̂) (9)"
    }, {
      "heading" : "4 RESULTS",
      "text" : "As an early step in methodical assessment of the performance, synthetic data has been used. This allows us to (i) introduce controlled error to the dataset, (ii) ensure, there is an underlying pattern in the data-set to be discovered by training ANN and (iii) examine and debug a given method in a smaller scale before scaling up.\nTherefore two datasets with distinct characteristics are generated to evaluate the performance of the proposed method against the common Bootstrap method in the case where input variable solely consists of categorical values. Table 1 shows the overall results where the proposed DAPIEN method provides higher PICP levels while maintaining an appropriate MPIW level. The following sections provide more detail about the data generation procedure as well as the figurative illustration of the performance for of each method.\n4.1 datasetA: SUM OF INPUT BITS, WITH ADDED CONDITIONAL WHITE NOISE\nIn this experiment the synthetic generated dataset models a case where the distribution of the added noise, varies depending on the input. Initially, all unique binary vectors of length d = 10 are generated. For each unique x, a vector y is then generated using (13). The number of elements in vector y for each unique x in this experiment is 20. Note that the vector y may consist of equal elements, or they would vary, based on (12).\nx ∈ {0, 1}d (10) f(x) = xT · 1 (11)\nerr(x) = { 0, if (f(x) mod 2 = 0) N (0, 0.2), otherwize (12)\ny = f(x) + err(x) (13)\nHence the generated dataset has total 210 ∗ 20 = 20480 number of records. The dataset is then separated into 20% test and 80% training subsets while ensuring that there is no overlap between the subsets with regards to the values of x."
    }, {
      "heading" : "4.1.1 RESULTS",
      "text" : "Following the procedure mentioned in 3.2, 3.1, two separate feed forward networks with no hidden layer are trained. For the FNN which predicts σ, an exponential activation function is added in order to ensure predictions are always positive. For optimizing the neural network the back-propagation Hecht-Nielsen (1989) is used. For finding the optimal weights, initially both Stochastic Gradient Descent (SGD) and Conjugate Gradient(CG) were tried. However for our dataset, CG with full batch, provided a much faster convergence rate. This is in accordance with optimization guidelines provided in LeCun et al. (2012) for small networks with small datasets. In order to avoid over-fitting, stratified 5-fold cross validation was used to select the model with lowest generalization error. Figure 1 provides qualitative comparison amongst both the Bootstrap and DAPIEN methods with respect to the provided PIs for the test data set using confidence level 95%.\n4.2 datasetB : SUM OF INPUT BITS, WITH ADDED WHITE NOISE SCALED BY TARGET VALUE\nFor datasetB , the records are generated similar to datasetA except for the added error as described in (14).\nerr(x) = f(x) · N (0, 0.1) (14)"
    }, {
      "heading" : "4.2.1 RESULTS",
      "text" : "Same FNN architecture and procedure described in 4.1.1 is used for building the DAPIEN predictors for datasetB . As the results in figure 2 shows, DAPIEN adjusts the PI width as the noise increases for higher target values while the Bootstrap method is providing similar PI width regardless of the target value.\n4.3 datasetC : SUM OF INPUT BITS, WITH ADDED GAMMA NOISE SCALED BY TARGET VALUE\nFor datasetC , the records are generated similar to datasetB except for the added error follows a gamma distribution as in equation (15).\nerr(x) = f(x) · gamma(α = 1, β = 1, µ = 0) (15)"
    }, {
      "heading" : "4.3.1 RESULTS",
      "text" : "Following the procedure in 3.3, three single layer FNN predictors were trained to predict the gamma distribution parameters α, β and µ. Same optimization techniques described in 4.1.1 were used. Figure 3 shows the comparison of this technique with the bootstrap method using the confidence level of 95%. The DAPIEN method is able to recover the original function and provide appropriate prediction intervals. Although the prediction intervals provided using the bootstrap method cover 99% of the dataset, but they are too wide."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "The presented results on synthetic data, suggests that the DAPIEN, can provide accurate prediction intervals for datasets with categorical input variables. The performance of this method is particularly dependent on the appropriate selection of the distribution function. It can model a system in which, the parameters of the target distribution can change depending on the input, while the distribution function remains the same. This method can play a key role in designing more accurate predictors when input variables are solely categorical. In subsequent work, we intend to apply this technique to provide prediction intervals for gene expression levels given genetic and environmental conditions."
    } ],
    "references" : [ {
      "title" : "Neural networks for pattern recognition",
      "author" : [ "Bishop", "Christopher M" ],
      "venue" : null,
      "citeRegEx" : "Bishop and M,? \\Q1995\\E",
      "shortCiteRegEx" : "Bishop and M",
      "year" : 1995
    }, {
      "title" : "Stochastic protein expression in individual cells at the single molecule",
      "author" : [ "Cai", "Long", "Friedman", "Nir", "Xie", "X Sunney" ],
      "venue" : "level. Nature,",
      "citeRegEx" : "Cai et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2006
    }, {
      "title" : "Backpropagation of pseudo-errors: neural networks that are adaptive to heterogeneous noise",
      "author" : [ "Ding", "Aidong Adam", "He", "Xiali" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Ding et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2003
    }, {
      "title" : "Theory of the backpropagation neural network",
      "author" : [ "Hecht-Nielsen", "Robert" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "Hecht.Nielsen and Robert.,? \\Q1989\\E",
      "shortCiteRegEx" : "Hecht.Nielsen and Robert.",
      "year" : 1989
    }, {
      "title" : "Practical confidence and prediction intervals",
      "author" : [ "Heskes", "Tom" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Heskes and Tom.,? \\Q1997\\E",
      "shortCiteRegEx" : "Heskes and Tom.",
      "year" : 1997
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Hinton", "Geoffrey", "Osindero", "Simon", "Teh", "Yee-Whye" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Prediction intervals for artificial neural networks",
      "author" : [ "Hwang", "JT Gene", "Ding", "A Adam" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hwang et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hwang et al\\.",
      "year" : 1997
    }, {
      "title" : "Comprehensive review of neural network-based prediction intervals and new advances",
      "author" : [ "Khosravi", "Abbas", "Nahavandi", "Saeid", "Creighton", "Doug", "Atiya", "Amir F" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Khosravi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Khosravi et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient backprop",
      "author" : [ "LeCun", "Yann A", "Bottou", "Léon", "Orr", "Genevieve B", "Müller", "Klaus-Robert" ],
      "venue" : "In Neural networks: Tricks of the trade,",
      "citeRegEx" : "LeCun et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2012
    }, {
      "title" : "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
      "author" : [ "Lee", "Honglak", "Grosse", "Roger", "Ranganath", "Rajesh", "Ng", "Andrew Y" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Lee et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2009
    }, {
      "title" : "Estimating the mean and variance of the target probability distribution",
      "author" : [ "Nix", "David A", "Weigend", "Andreas S" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "Nix et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Nix et al\\.",
      "year" : 1994
    }, {
      "title" : "Reliable prediction intervals with regression neural networks",
      "author" : [ "Papadopoulos", "Harris", "Haralambous", "Haris" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Papadopoulos et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Papadopoulos et al\\.",
      "year" : 2011
    }, {
      "title" : "The box-cox transformation technique: a review",
      "author" : [ "Sakia", "RM" ],
      "venue" : "The statistician, pp",
      "citeRegEx" : "Sakia and RM.,? \\Q1992\\E",
      "shortCiteRegEx" : "Sakia and RM.",
      "year" : 1992
    }, {
      "title" : "Conversational speech transcription using context-dependent deep neural networks",
      "author" : [ "Seide", "Frank", "Li", "Gang", "Yu", "Dong" ],
      "venue" : "In Interspeech, pp",
      "citeRegEx" : "Seide et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Seide et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "In the machine learning literature this is sometimes referred to as Conformal Prediction (CP) Papadopoulos & Haralambous (2011) and others simply refer to it as providing Prediction Interval (PI) Khosravi et al. (2011). In the general single-task learning (STL) problem, a regression CP method trains a predictive model.",
      "startOffset" : 196,
      "endOffset" : 219
    }, {
      "referenceID" : 5,
      "context" : "record breaking achievements through the application of ANNs particularly in speech recognition, computer vision, machine translation and predictive genomics in the context of deep learning Hinton et al. (2006); Seide et al.",
      "startOffset" : 190,
      "endOffset" : 211
    }, {
      "referenceID" : 5,
      "context" : "record breaking achievements through the application of ANNs particularly in speech recognition, computer vision, machine translation and predictive genomics in the context of deep learning Hinton et al. (2006); Seide et al. (2011); Lee et al.",
      "startOffset" : 190,
      "endOffset" : 232
    }, {
      "referenceID" : 5,
      "context" : "record breaking achievements through the application of ANNs particularly in speech recognition, computer vision, machine translation and predictive genomics in the context of deep learning Hinton et al. (2006); Seide et al. (2011); Lee et al. (2009); ?.",
      "startOffset" : 190,
      "endOffset" : 251
    }, {
      "referenceID" : 7,
      "context" : "An extensive review for neural network-based prediction intervals is provided in Khosravi et al. (2011). Next we provide a concise summary of PI assessment measures and also discuss the latest developments in PI prediction methods, some of which have not been reviewed together before.",
      "startOffset" : 81,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "1 PI ASSESSMENT MEASURES When it comes to assessing the quality of PIs, it is common to solely focus on the overall coverage of the calculated intervals [ToDo: Add ref from Khosravi et al. (2011)].",
      "startOffset" : 173,
      "endOffset" : 196
    }, {
      "referenceID" : 7,
      "context" : "The Bayesian method for training ANN, has particularly strong mathematical foundation and good generalization error Khosravi et al. (2011).",
      "startOffset" : 116,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "One drawback for this method, is ignoring the modeling error Khosravi et al. (2011) which affects the quality of the predicted intervals.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "5 COMBINED PIS Khosravi et al. (2011) proposes an ensemble method using Bootstrap, Bayesian, Delta and MVE methods.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "For example the steady-state probability distribution of protein number per cell is known to follow a gamma distribution Cai et al. (2006). For a Gamma probability distribution, the general procedure in section 3.",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : "This is in accordance with optimization guidelines provided in LeCun et al. (2012) for small networks with small datasets.",
      "startOffset" : 63,
      "endOffset" : 83
    } ],
    "year" : 2015,
    "abstractText" : "Proposed methods for prediction interval estimation so far focus on cases where input variables are numerical. In datasets with solely nominal input variables, we observe records with the exact same input x, but different real valued outputs due to the inherent noise in the system. Existing prediction interval estimation methods do not use representations that can accurately model such inherent noise in the case of nominal inputs. We propose a new prediction interval estimation method tailored for this type of data, which is prevalent in biology and medicine. We call this method Distribution Adaptive Prediction Interval Estimation given Nominal inputs (DAPIEN) and has four main phases. First, we select a distribution function that can best represent the inherent noise of the system for all unique inputs. Then we infer the parameters θi (e.g. θi = [meani, variancei]) of the selected distribution function for all unique input vectors xi and generate a new corresponding training set using pairs of xi , θi. III). Then, we train a model to predict θ given a new xu. Finally, we calculate the prediction interval for a new sample using the inverse of the cumulative distribution function once the parameters θ is predicted by the trained model. We compared DAPIEN to the commonly used Bootstrap method on three synthetic datasets. Our results show that DAPIEN provides tighter prediction intervals while preserving the requested coverage when compared to Bootstrap. This work can facilitate broader usage of regression methods in medicine and biology where it is necessary to provide tight prediction intervals while preserving coverage when input variables are nominal.",
    "creator" : "LaTeX with hyperref package"
  }
}