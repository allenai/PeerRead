{
  "name" : "1702.06856.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Mahdieh Abbasi", "Christian Gagné" ],
    "emails" : [ "mahdieh.abbasi.1@ulaval.ca,", "christian.gagne@gel.ulaval.ca" ],
    "sections" : [ {
      "heading" : "ROBUSTNESS TO ADVERSARIAL EXAMPLES THROUGH",
      "text" : ""
    }, {
      "heading" : "AN ENSEMBLE OF SPECIALISTS",
      "text" : "Mahdieh Abbasi & Christian Gagné Computer Vision and Systems Laboratory, Electrical and Computer Engineering Department Université Laval, Québec (Québec), Canada mahdieh.abbasi.1@ulaval.ca, christian.gagne@gel.ulaval.ca"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications. However, CNNs have a high risk of failing due to adversarial examples, which fool them consistently with the addition of small perturbations to natural images, undetectable by the human eyes.\nTo mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015). Although such trained CNNs are robust to specific types of adversaries, they are not necessarily protected from all possible types. To increase the robustness of CNNs, it has been proposed to train them on a diverse set of adversaries, generating adversarial examples for any single images with various algorithms (Rozsa et al., 2016). However, it is still possible to produce other types of adversaries, uncovered by the current set, impacting significantly the reliability of CNNs. Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016).\nWe are rather considering recognition of adversarial examples as an open set recognition problem, where unknown samples should be detected and rejected by the underlying models. Bendale & Boult (2016) have adapted CNNs by adding an extra layer designed to recognize the unknown samples, which can be either from unknown classes or fooling adversarial instances from Nguyen et al. (2015). However, as mentioned by the authors, the method fails to detect hard adversaries where the target class and the true class of an adversary are close together, like those generated by Fast Gradient Sign (FGS) (Goodfellow et al., 2014) and DeepFool (DF) (Moosavi-Dezfooli et al., 2016).\nWe are proposing to use an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. Indeed, we observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm this interpretation that a rejection mechanism can provide a means of rendering the system more robust to adversarial examples, rather than trying to classify them properly at any cost."
    }, {
      "heading" : "2 SPECIALISTS+1 ENSEMBLE",
      "text" : "Ensemble construction The confusion matrices of FGS adversaries (Fig. 1) reveals that samples from each class have a high tendency of being fooled toward a limited number of classes. From these confusion matrices of training adversaries we define subsets of classes, similarly to Hinton et al. (2015) on training clean samples. Considering a classification problem of K classes (C = {c1, c2, . . . , cK}), each row of the confusion matrix is used to identify two subsets of classes: 1) the confusing target subset for class ci (subset Ui), which is built by adding classes sequentially in decreasing ci-related confusion values order until at least 80 % of confusions are covered, and 2) the remaining classes with lower confusion, formed as subset Ui+K = C \\Ui. Duplicate subsets should be ignored, although we encountered none of them for MNIST and CIFAR-10.\nar X\niv :1\n70 2.\n06 85\n6v 3\n[ cs\n.N E\n] 1\n0 M\nar 2\n01 7\nAlgorithm 1 Voting Mechanism\nInput: EnsembleH = {h1, . . . , hM} with hj ∈ RK , label subsets U = {U1, . . . , UM}, input x Output: Final prediction h̄(x) ∈ RK\n1: vk(x)← ∑M j=1 I[ck = argmaxKi=1 h j i (x)], k = 1, . . . ,K 2: k∗ ← argmaxKk=1 vk(x) 3: if vk∗(x) = K + 1 4: S ← {hi ∈ H | ck∗ ∈ Ui} 5: h̄(x)← 1K+1 ∑ hi∈S h\ni(x) 6: else 7: h̄(x)← 1M ∑ hi∈H h i(x) 8: return h̄(x)\nFor each of these class subsets, a specialist CNN is trained on samples from the associated classes, instances from the other classes being ignored. The ensemble also includes a generalist CNN trained on the complete labels (subset U2K+1), hence the name “specialists+1 ensemble”.\nVoting mechanism Using the generalist to activate the related specialists is not possible as it is usually being fooled by adversaries. In Algorithm 1, we propose a voting mechanism to compute the final prediction. As each class ci appears K + 1 times in M subsets of classes, the maximum expected number of votes to class ci is K + 1. Also, we define vi(x) as the actual number of votes to class ci for a given input image x (the equation in line 1 of the algorithm 1). If only one class has its actual number of votes equal to its maximum expected number of votes, i.e., vi(x) = K + 1, it means that all K related specialists and the generalist agree to vote to the winner class. Then only those CNNs voting for the winner class should be activated in order to compute the final prediction. Otherwise, if none of the classes obtain their maximum expected number of votes, it means that at least one of the individuals was fooled. So, some votes are incorrectly distributed between different classes. In the presence of such entropy, where there is no winner class, all of the individuals should be activated to compute the final prediction."
    }, {
      "heading" : "3 EMPIRICAL EVALUATION",
      "text" : "Networks and datasets Similarly to Hinton et al. (2012), we used CNNs with three convolutional layers having 32, 32, and 64 filters respectively, and a fully-connected layer followed by a softmax. The networks are trained on usual training and testing sets of MNIST and CIFAR-10, without any data augmentation. See the Appendix section for full details on the network architecture and hyperparameters used for each dataset. Note that all CNNs presented in the experiments have an identical architecture.\nExperiments We compared our proposed specialists+1 ensemble with a pure ensemble, which consists of 5 generalist CNNs with different random initializations, and a naive CNN*, whose weights initialization is different from GA-CNN (i.e., the CNN used to generate the adversaries). Using this GA-CNN, three types of adversaries, namely FGS, DF, and Szegedy et al. (2013) adversaries, are generated for correctly classified clean test samples.\nNaive CNN*, pure ensemble, and specialists+1 ensemble are compared according to their distributions of confidence on correctly classified clean test samples and their corresponding adversaries for MNIST and CIFAR-10 in Fig. 2 and 3, respectively. According to these observations for MNIST, specialists+1 successfully provides significantly lower confidence for most of the misclassified adversaries, regardless of their types, than naive CNN* and the pure ensemble. Also, as it can be seen from Fig. 2(a), 2(b), and 2(c) that the distribution of confidence on MNIST correctly classified clean test samples by these three frameworks are roughly similar. For CIFAR-10, we observed the same behavior as MNIST for adversaries (Fig. 3). But for correctly classified clean test samples, specialists+1 shifts some of these samples to lower confidence (the green curve in Fig. 3(c)).\nAlthough the specialists+1 is not trained from any adversaries, it appears able to automatically reduce the confidence of predictions for most of the misclassified adversaries, regardless of their types, while preserving up to some point the confidence on clean samples. However, it reduces the confidence of a few clean test samples. Therefore, developing a learning model that is not confident about unknown samples but yet is confident about known samples can be a used to identify and reject adversaries. We depicted the effect of rejecting low confidence adversaries on the error rates of different types of adversaries in Fig. 5, in Appendix due to space consideration.\nConclusion In brief, without training from adversaries and by leveraging diversity in ensembles by a specialization over the labels, the specialists+1 ensemble approach is able to better discriminate between legitimate samples and adversarial instances. The approach is better at rejecting adversaries while accepting clean samples based on confidence, compared to the pure ensemble and Naive CNN*. That is an important matter in order to increase robustness of CNNs to carefully crafted attacks, preferring to refuse processing suspicious instances rather than being fooled by carefully crafted attacks. As future work, we will compare our approach with CNN explicitly trained to being robust to specific types of adversaries."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work was made possible through funding from NSERC-Canada, MITACS, and E Machine Learning Inc. Computational resources were provided by Compute Canada / Calcul Québec and by a GPU grant from NVIDIA. The authors are also grateful to Annette Schwerdtfeger for proofreading this manuscript."
    }, {
      "heading" : "A APPENDIX",
      "text" : ""
    }, {
      "heading" : "A.1 EXPERIMENTAL PROCEDURES",
      "text" : "We consider a CNN with three convolutional layers and one fully connected layer, where each convolutional layer is interlaced with ReLU, local contrast normalization, and a pooling layer. For regularization, dropout is used at the last layer, i.e., fully-connected layer, with p = 0.5. All of the hyper parameters such as initial learning rate, training schedule, and so on are set according to Hinton et al. (2012).\nMNIST This dataset contains grayscale images of size 28x28, where each image holds a handwritten digit. The training and test sets have 60,000 and 10,000 samples, respectively. All of the images are scaled to [0, 1]. 150 epochs for training with batch size 128 and the initial learning rate 0.1 with momentum 0.9 are exploited. The learning rate is decayed by factor 10 twice during training, at epochs 50 and 100.\nCIFAR-10 This dataset consists of 50,000 RGB images of size 32x32 as training set and 10,000 32x32 RGB images as test set. Each image contains one object from one of 10 classes. All images, either from train or from test sets, are scaled to [0, 1], then normalized by mean subtraction, where the mean is computed over the training set. Like MNIST, 150 epochs with batch size 128 and the initial learning rate is 0.01 with momentum of 0.9. The learning rate decays twice by factor 10 shortly before terminating training, at epochs 120 and 130."
    }, {
      "heading" : "A.2 GENERATING ADVERSARIES",
      "text" : "Fast Gradient Sign (FGS) (Goodfellow et al., 2014), DeepFool (DF) (Moosavi-Dezfooli et al., 2016), and the algorithm proposed by (Szegedy et al., 2013) are used for adversarial example generation. The latter algorithm finds minimum required perturbations at a high computational cost, while FGS and DF generates adversaries significantly faster, i.e., less than 3 iterations.\nUsing the GA-CNN (the baseline CNN for generating adversaries), correctly classified clean samples are identified then used for generating adversarial examples. Therefore, 9943 and 8152 adversaries are generated from MNIST and CIFAR-10 test sets, respectively. The optimal values for hyper parameters of FGS and Szegedy et al. (2013) are obtained for each dataset such that GA-CNN misclassifies 100% of the correctly classified clean samples after adding perturbations.\nIn Fig. 4, the average distortions (perturbations) generated by each algorithm for MNIST and CIFAR-10 are depicted. As well, their average misclassification confidences are written in blue. Distortion is measured by √∑D\ni=1(xi−x′i)2 D for each pair of clean sample (x ∈ R\nD) and its corresponding adversary (x′)."
    }, {
      "heading" : "A.3 EXTRA EXPERIMENTAL RESULTS",
      "text" : "Let h(x) = [h1(x), . . . , hK(x)] be a multi-classification system (e.g., single classifier h(x) or ensemble h̄(x)) trained on clean training samples. Like Bendale & Boult (2016), we consider a threshold (τ ) for rejecting instances with low confidence, assigning them to a reject class cK+1. The following classical decision function is used:\nr(x) =\n{ argmax\nci∈C hi(x) if max ci∈C hi(x) ≥ τ\ncK+1 otherwise . (1)\nTwo types of errors should be considered: error ED on the clean set D = {xi, yi}Ni=1, and error EA on the adversaries set A = {x′i, y′i}N ′ i=1 (y ′ i is the true label of x ′ i). Error ED takes into account both clean samples that are misclassified and correctly classified rejected clean samples:\nED = 1\nN N∑ i=1\n( I[r(xi) 6= yi] + I[r(xi) = cK+1 ∧ argmax\ncj∈C hj(xi) = yi]\n) . (2)\nError EA considers misclassified adversarial instances that are not rejected:\nEA = 1\nN ′ N ′∑ i=1 I[r(x′i) 6= y′i ∧ r(x′i) 6= cK+1]. (3)\nFig. 5 presents error rates ED (Eq. 2) and EA (Eq. 3) on the MNIST and CIFAR-10 datasets with clean samples and three types of adversaries. For naive CNN* and the pure ensemble, error rates of different types of adversaries (EA) decrease monotonically as the threshold increases. However, the error rates of adversaries by specialists+1 ensemble are not monotonically decreased. As confidences for most of the misclassified adversaries by specialists+1 ensemble is lower than 0.5, rejection of low confidence predictions at this threshold results in a significant reduction of adversaries error EA in comparison to naive CNN* and the pure ensemble using this threshold. Accordingly, it can be confirmed that specialists+1 can shift most of the misclassified adversaries to low confidence, thus they are being rejected.\nSome clean samples that can be correctly classified with high confidence by a CNN are rejected by the specialists+1 ensemble due to their low confidence, thus increasing slightly ED at a threshold 0.5. Note that increasing the threshold to a higher value causes rejection of a vast majority of adversaries as well as more clean samples. This thus requires a trade-off between keeping rejection rate of clean test samples low vs rejecting adversaries that would otherwise fool the networks.\nMoreover, from the error rates of FGS and DF adversaries (EA) at threshold zero (Fig. 5), it can be seen that FGS and DF adversaries can severely fool the new models since they are transferable, i.e. their cross-model generalization property, while adversaries by Szegedy et al. (2013) are less generalized across different models. So, a remarkable number of Szegedy adversaries can be correctly and confidently classified by the models that are different from GA-CNN (the adversaries generative model). Notice that EA of Szegedy adversaries by specialists+1 does not change considerably after threshold 0.5. Since most of the high confidence predictions for this type of adversaries (shown by the cyan pick at the high confidence in Fig. 6(e) and Fig. 7(e)) are correctly and confidently classified by specialists+1 ensemble.\nFor a better insight, the rejection rate as a function of the threshold on confidence for MNIST and CIFAR-10 are shown in association with their distributions of confidence in Fig. 6 and 7, respectively. According to these observations, specialists+1 successfully provides significantly lower confidence for most of the adversaries, regardless of their types, than naive CNN* and the pure ensemble. Therefore, its rejection rate curves of adversaries are increasing at lower confidence and reach to some picks very fast at a threshold of 0.5. However, the rejection rates of adversaries by the pure ensemble are monotonically increasing by increasing the threshold, and reach to their picks at a higher threshold, when a mix of both clean and adversaries samples are being rejected. Also, as it can be seen from Fig. 6(b), 6(d), and 6(f) that rejection rate curves for MNIST correctly classified clean test samples by these three frameworks are mostly similar."
    } ],
    "references" : [ {
      "title" : "Towards open set deep networks",
      "author" : [ "Abhijit Bendale", "Terrance E Boult" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Bendale and Boult.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bendale and Boult.",
      "year" : 2016
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean" ],
      "venue" : "arXiv preprint arXiv:1503.02531,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Robust convolutional neural networks under adversarial noise",
      "author" : [ "Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello" ],
      "venue" : "arXiv preprint arXiv:1511.06306,",
      "citeRegEx" : "Jin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2015
    }, {
      "title" : "Large-scale video classification with convolutional neural networks",
      "author" : [ "Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li FeiFei" ],
      "venue" : "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Karpathy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2014
    }, {
      "title" : "Deepfool: a simple and accurate method to fool deep neural networks",
      "author" : [ "Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "Moosavi.Dezfooli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Moosavi.Dezfooli et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "Anh Nguyen", "Jason Yosinski", "Jeff Clune" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial diversity and hard positive generation",
      "author" : [ "Andras Rozsa", "Ethan M Rudd", "Terrance E Boult" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,",
      "citeRegEx" : "Rozsa et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rozsa et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial manipulation of deep representations",
      "author" : [ "Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J Fleet" ],
      "venue" : "arXiv preprint arXiv:1511.05122,",
      "citeRegEx" : "Sabour et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sabour et al\\.",
      "year" : 2015
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2013
    }, {
      "title" : "Deepface: Closing the gap to human-level performance in face verification",
      "author" : [ "Yaniv Taigman", "Ming Yang", "Marc’Aurelio Ranzato", "Lior Wolf" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Taigman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Taigman et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "1 INTRODUCTION Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications.",
      "startOffset" : 130,
      "endOffset" : 192
    }, {
      "referenceID" : 12,
      "context" : "1 INTRODUCTION Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications.",
      "startOffset" : 130,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : "1 INTRODUCTION Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications.",
      "startOffset" : 130,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015).",
      "startOffset" : 167,
      "endOffset" : 266
    }, {
      "referenceID" : 11,
      "context" : "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015).",
      "startOffset" : 167,
      "endOffset" : 266
    }, {
      "referenceID" : 7,
      "context" : "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015).",
      "startOffset" : 167,
      "endOffset" : 266
    }, {
      "referenceID" : 10,
      "context" : "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015).",
      "startOffset" : 167,
      "endOffset" : 266
    }, {
      "referenceID" : 9,
      "context" : "To increase the robustness of CNNs, it has been proposed to train them on a diverse set of adversaries, generating adversarial examples for any single images with various algorithms (Rozsa et al., 2016).",
      "startOffset" : 182,
      "endOffset" : 202
    }, {
      "referenceID" : 5,
      "context" : "Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 165
    }, {
      "referenceID" : 1,
      "context" : "However, as mentioned by the authors, the method fails to detect hard adversaries where the target class and the true class of an adversary are close together, like those generated by Fast Gradient Sign (FGS) (Goodfellow et al., 2014) and DeepFool (DF) (Moosavi-Dezfooli et al.",
      "startOffset" : 209,
      "endOffset" : 234
    }, {
      "referenceID" : 7,
      "context" : ", 2014) and DeepFool (DF) (Moosavi-Dezfooli et al., 2016).",
      "startOffset" : 26,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015). Although such trained CNNs are robust to specific types of adversaries, they are not necessarily protected from all possible types. To increase the robustness of CNNs, it has been proposed to train them on a diverse set of adversaries, generating adversarial examples for any single images with various algorithms (Rozsa et al., 2016). However, it is still possible to produce other types of adversaries, uncovered by the current set, impacting significantly the reliability of CNNs. Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016). We are rather considering recognition of adversarial examples as an open set recognition problem, where unknown samples should be detected and rejected by the underlying models. Bendale & Boult (2016) have adapted CNNs by adding an extra layer designed to recognize the unknown samples, which can be either from unknown classes or fooling adversarial instances from Nguyen et al.",
      "startOffset" : 168,
      "endOffset" : 1120
    }, {
      "referenceID" : 1,
      "context" : "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015). Although such trained CNNs are robust to specific types of adversaries, they are not necessarily protected from all possible types. To increase the robustness of CNNs, it has been proposed to train them on a diverse set of adversaries, generating adversarial examples for any single images with various algorithms (Rozsa et al., 2016). However, it is still possible to produce other types of adversaries, uncovered by the current set, impacting significantly the reliability of CNNs. Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016). We are rather considering recognition of adversarial examples as an open set recognition problem, where unknown samples should be detected and rejected by the underlying models. Bendale & Boult (2016) have adapted CNNs by adding an extra layer designed to recognize the unknown samples, which can be either from unknown classes or fooling adversarial instances from Nguyen et al. (2015). However, as mentioned by the authors, the method fails to detect hard adversaries where the target class and the true class of an adversary are close together, like those generated by Fast Gradient Sign (FGS) (Goodfellow et al.",
      "startOffset" : 168,
      "endOffset" : 1306
    }, {
      "referenceID" : 1,
      "context" : "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015). Although such trained CNNs are robust to specific types of adversaries, they are not necessarily protected from all possible types. To increase the robustness of CNNs, it has been proposed to train them on a diverse set of adversaries, generating adversarial examples for any single images with various algorithms (Rozsa et al., 2016). However, it is still possible to produce other types of adversaries, uncovered by the current set, impacting significantly the reliability of CNNs. Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016). We are rather considering recognition of adversarial examples as an open set recognition problem, where unknown samples should be detected and rejected by the underlying models. Bendale & Boult (2016) have adapted CNNs by adding an extra layer designed to recognize the unknown samples, which can be either from unknown classes or fooling adversarial instances from Nguyen et al. (2015). However, as mentioned by the authors, the method fails to detect hard adversaries where the target class and the true class of an adversary are close together, like those generated by Fast Gradient Sign (FGS) (Goodfellow et al., 2014) and DeepFool (DF) (Moosavi-Dezfooli et al., 2016). We are proposing to use an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. Indeed, we observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm this interpretation that a rejection mechanism can provide a means of rendering the system more robust to adversarial examples, rather than trying to classify them properly at any cost. 2 SPECIALISTS+1 ENSEMBLE Ensemble construction The confusion matrices of FGS adversaries (Fig. 1) reveals that samples from each class have a high tendency of being fooled toward a limited number of classes. From these confusion matrices of training adversaries we define subsets of classes, similarly to Hinton et al. (2015) on training clean samples.",
      "startOffset" : 168,
      "endOffset" : 2626
    }, {
      "referenceID" : 3,
      "context" : "3 EMPIRICAL EVALUATION Networks and datasets Similarly to Hinton et al. (2012), we used CNNs with three convolutional layers having 32, 32, and 64 filters respectively, and a fully-connected layer followed by a softmax.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Using this GA-CNN, three types of adversaries, namely FGS, DF, and Szegedy et al. (2013) adversaries, are generated for correctly classified clean test samples.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "2 GENERATING ADVERSARIES Fast Gradient Sign (FGS) (Goodfellow et al., 2014), DeepFool (DF) (Moosavi-Dezfooli et al.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : ", 2014), DeepFool (DF) (Moosavi-Dezfooli et al., 2016), and the algorithm proposed by (Szegedy et al.",
      "startOffset" : 23,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : ", 2016), and the algorithm proposed by (Szegedy et al., 2013) are used for adversarial example generation.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "All of the hyper parameters such as initial learning rate, training schedule, and so on are set according to Hinton et al. (2012). MNIST This dataset contains grayscale images of size 28x28, where each image holds a handwritten digit.",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "2 GENERATING ADVERSARIES Fast Gradient Sign (FGS) (Goodfellow et al., 2014), DeepFool (DF) (Moosavi-Dezfooli et al., 2016), and the algorithm proposed by (Szegedy et al., 2013) are used for adversarial example generation. The latter algorithm finds minimum required perturbations at a high computational cost, while FGS and DF generates adversaries significantly faster, i.e., less than 3 iterations. Using the GA-CNN (the baseline CNN for generating adversaries), correctly classified clean samples are identified then used for generating adversarial examples. Therefore, 9943 and 8152 adversaries are generated from MNIST and CIFAR-10 test sets, respectively. The optimal values for hyper parameters of FGS and Szegedy et al. (2013) are obtained for each dataset such that GA-CNN misclassifies 100% of the correctly classified clean samples after adding perturbations.",
      "startOffset" : 51,
      "endOffset" : 735
    }, {
      "referenceID" : 11,
      "context" : "Figure 4: Average distortion to MNIST and CIFAR-10 samples by FGS, DF, and Szegedy et al. (2013). The average misclassification confidences are shown by blue text.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "their cross-model generalization property, while adversaries by Szegedy et al. (2013) are less generalized across different models.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "(d) EA(%) on MNIST adversaries by Szegedy et al. (2013)",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 11,
      "context" : "(h) EA(%) on CIFAR-10 adversaries by Szegedy et al. (2013) Figure 5: Error rates ED on clean test samples, and error rates EA on their corresponding adversaries, as a function of threshold (τ ), for the MNIST and CIFAR-10 datasets.",
      "startOffset" : 37,
      "endOffset" : 59
    } ],
    "year" : 2017,
    "abstractText" : "Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications. However, CNNs have a high risk of failing due to adversarial examples, which fool them consistently with the addition of small perturbations to natural images, undetectable by the human eyes.",
    "creator" : "LaTeX with hyperref package"
  }
}