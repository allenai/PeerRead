{
  "name" : "1606.06406.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Incremental Parsing with Minimal Features Using Bi-Directional LSTM",
    "authors" : [ "James Cross", "Liang Huang" ],
    "emails" : [ "crossj@oregonstate.edu", "liang.huang@oregonstate.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition, and potentially omitting important global context. To further reduce feature engineering to the bare minimum, we use bi-directional LSTM sentence representations to model a parser state with only three sentence positions, which automatically identifies important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese."
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, neural network-based parsers have become popular, with the promise of reducing the burden of manual feature engineering. For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use 48 atomic features from Zhang and Nivre (2011), including select thirdorder dependencies. More importantly, this approach inevitably leaves out some nonlocal information which could be useful. In particular,\nthough such a model can exploit similarities between words and other embedded categories, and learn interactions among those atomic features, it cannot exploit any other details of the text.\nWe aim to reduce the need for manual induction of atomic features to the bare minimum, by using bi-directional recurrent neural networks to automatically learn context-sensitive representations for each word in the sentence. This approach allows the model to learn arbitrary patterns from the entire sentence, effectively extending the generalization power of embedding individual words to longer sequences. Since such a feature representation is less dependent on earlier parser decisions, it is also more resilient to local mistakes.\nWith just three positional features we can build a greedy shift-reduce dependency parser that is on par with the most accurate parser in the published literature for English Treebank. This effort is similar in motivation to the stack-LSTM of Dyer et al. (2015), but uses a much simpler architecture.\nWe also extend this model to predict phrasestructure trees with a novel shift-promote-adjoin system tailored to greedy constituency parsing, and with just two more positional features (defining tree span) and nonterminal label embeddings we achieve the most accurate greedy constituency parser for both English and Chinese.\n2 LSTM Position Features\nar X\niv :1\n60 6.\n06 40\n6v 1\n[ cs\n.C L\n] 2\n1 Ju\nn 20\n16\nThe central idea behind this approach is exploiting the power of recurrent neural networks to let the model decide what apsects of sentence context are important to making parsing decisions, rather than relying on fallible linguistic information (which moreover requires leaving out information which could be useful). In particular, we model an input sentence using Long Short-Term Memory networks (LSTM), which have made a recent resurgence after being initially formulated by Hochreiter and Schmidhuber (1997).\nThe input at each time step is simply a vector representing the word, in this case an embedding for the word form and one for the part-of-speech tag. These embeddings are learned from random initialization together with other network parameters in this work. In our initial experiments, we used one LSTM layer in each direction (forward and backward), and then concatenate the output at each time step to represent that sentence position: that word in the entire context of the sentence. This network is illustrated in Figure 1.\nIt is also common to stack multiple such LSTM layers, where the output of the forward and backward networks at one layer are concatenated to form the input to the next. We found that parsing performance could be improved by using two bidirectional LSTM layers in this manner, and concatenating the output of both layers as the positional feature representation, which becomes the input to the fully-connected layer. This architec-\nture is shown in Figure 2. Intuitively, this represents the sentence position by the word in the context of the sentence up to that point and the sentence after that point in the first layer, as well as modeling the “higher-order” interactions between parts of the sentence in the second layer. In Section 5 we report results using only one LSTM layer (“Bi-LSTM”) as well as with two layers where output from each layer is used as part of the positional feature (“2-Layer BiLSTM”)."
    }, {
      "heading" : "3 Shift-Reduce Dependency Parsing",
      "text" : "We use the arc-standard system for dependency parsing (see Figure 4). By exploiting the LSTM architecture to encode context, we found that we were able to achieve competitive results using only three sentence-position features to model parser state: the head word of each of the top two trees on the stack (s0 and s1), and the next word on the queue (q0); see Table 1.\nThe usefulness of the head words on the stack is clear enough, since those are the two words that are linked by a dependency when taking a reduce action. The next incoming word on the queue is also important because the top tree on the stack should not be reduced if it still has children which have not yet been shifted. That feature thus allows\nthe model to learn to delay a right-reduce until the top tree on the stack is fully formed, shifting instead."
    }, {
      "heading" : "3.1 Hierarchical Classification",
      "text" : "The structure of our network model after computing positional features is fairly straightforward and similar to previous neural-network parsing approaches such as Chen and Manning (2014) and Weiss et al. (2015). It consists of a multilayer perceptron using a single ReLU hidden layer followed by a linear classifier over the action space, with the training objective being negative log softmax.\nWe found that performance could be improved, however, by factoring out the decision over structural actions (i.e., shift, left-reduce, or rightreduce) and the decision of which arc label to assign upon a reduce. We therefore use separate classifiers for those decisions, each with its own fully-connected hidden and output layers but sharing the underlying recurrent architecture. This structure was used for the results reported in Section 5, and it is referred to as “Hierarchical Actions” when compared against a single action classifier in Table 3."
    }, {
      "heading" : "4 Shift-Promote-Adjoin Constituency Parsing",
      "text" : "To further demonstrate the advantage of our idea of minimal features with bidirectional sentence representations, we extend our work from dependency parsing to constituency parsing. However, the latter is significantly more challenging than the former under the shift-reduce paradigm because:\n• we also need to predict the nonterminal labels\n• the tree is not binarized (with many unary rules and more than binary branching rules)\nWhile most previous work binarizes the constituency tree in a preprocessing step (Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015), we propose a novel “Shift-PromoteAdjoin” paradigm which does not require any binariziation or transformation of constituency trees (see Figure 5). Note in particular that, in our case only the Promote action produces a new tree node (with a non-terminal label), while the Adjoin action is the linguistically-motivated “sisteradjunction” operation, i.e., attachment (Chiang, 2000; Henderson, 2003). By comparison, in previous work, both Unary-X and Reduce-L/R-X actions produce new labeled nodes (some of which are auxiliary nodes due to binarization). Thus our paradigm has two advantages:\n• it dramatically reduces the number of possible actions, from 3X + 1 or more in previous work to 3 + X , where X is the number of nonterminal labels, which we argue would simplify learning;\n• it does not require binarization (Zhu et al., 2013; Wang and Xue, 2014) or compression of unary chains (Mi and Huang, 2015)\nThere is, however, a more closely-related “shiftproject-attach” paradigm by Henderson (2003). For the example in Figure 5 he would use the following actions:\nshift(I), project(NP), project(S), shift(like), project(VP), shift(sports), project(NP), attach, attach.\nThe differences are twofold: first, our Promote action is head-driven, which means we only promote the head child (e.g., VP to S) whereas his Project action promotes the first child (e.g., NP to S); and secondly, as a result, his Attach action is always right-attach whereas our Adjoin action could be either left or right. The advantage of our method is its close resemblance to shift-reduce dependency parsing, which means that our constituency parser is jointly performing both tasks and can produce both kinds of trees. This also means that we use head rules to determine the correct order of gold actions.\nWe found that in this setting, we did need slightly more input features. As mentioned, node labels are necessary to distinguish whether a tree has been sufficiently promoted, and are helpful in any case. We used 8 labels: the current and immediate predecessor label of each of the top two stacks on the tree, as well as the label of the leftand rightmost adjoined child for each tree. We also found it helped to add positional features for the leftmost word in the span for each of those trees, bringing the total number of positional features to five. See Table 1 for details."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "We report both dependency and constituency parsing results on both English and Chinese.\nAll experiments were conducted with minimal hyperparameter tuning. The settings used for the reported results are summarized in Table 6. Networks parameters were updated using gradient backpropagation, including backpropagation through time for the recurrent components, using ADADELTA for learning rate scheduling (Zeiler, 2012). We also applied dropout (Hinton et al., 2012) (with p = 0.5) to the output of each LSTM layer (separately for each connection in the case of the two-layer network).\nWe tested both types of parser on the Penn Treebank (PTB) and Penn Chinese Treebank (CTB-5), with the standard splits for each of training, development, and test sets. Automatically predicted part of speech tags with 10-way jackknifing were used as inputs for all tasks except for Chinese dependency parsing, where we used gold tags, following the traditions in literature."
    }, {
      "heading" : "5.1 Dependency Parsing: English & Chinese",
      "text" : "Table 2 shows results for English Penn Treebank using Stanford dependencies. Despite the minimally designed feature representation, relatively few training iterations, and lack of precomputed embeddings, the parser performed on par with state-of-the-art incremental dependency parsers, and slightly outperformed the state-ofthe-art greedy parser.\nThe ablation experiments shown in the Table 3 indicate that both forward and backward contexts for each word are very important to obtain strong results. Using only word forms and no part-ofspeech input similarly degraded performance.\nFigure 6 compares our parser with that of Chen and Manning (2014) in terms of arc recall for various arc lengths. While the two parsers perform similarly on short arcs, ours significantly outpeforms theirs on longer arcs, and more interestingly our accuracy does not degrade much after length 6. This confirms the benefit of having a global sentence repesentation in our model.\nTable 4 summarizes the Chinese dependency parsing results. Again, our work is competitive with the state-of-the-art greedy parsers."
    }, {
      "heading" : "5.2 Constituency Parsing: English & Chinese",
      "text" : "Table 5 compares our constituency parsing results with state-of-the-art incremental parsers. Although our work are definitely less accurate than those beam-search parsers, we achieve the highest accuracy among greedy parsers, for both English and Chinese.1,2\n1 The greedy accuracies for Mi and Huang (2015) are from Haitao Mi, and greedy results for Zhu et al. (2013) come from duplicating experiments with code provided by those authors.\n2 The parser of Vinyals et al. (2015) does not use an explicit transition system, but is similar in spirit since generating a right bracket can be viewed as a reduce action."
    }, {
      "heading" : "6 Related Work",
      "text" : "Because recurrent networks are such a natural fit for modeling languages (given the sequential nature of the latter), bi-directional LSTM networks are becoming increasingly common in all sorts of linguistic tasks, for example event detection in Ghaeini et al. (2016). In fact, we discovered after submission that Kiperwasser and Goldberg (2016) have concurrently developed an extremely similar approach to our dependency parser. Instead of extending it to constituency parsing, they also apply the same idea to graph-based dependency parsing."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have presented a simple bi-directional LSTM sentence representation model for minimal features in both incremental dependency and incremental constituency parsing, the latter using a novel shift-promote-adjoin algorithm. Experiments show that our method are competitive with the state-of-the-art greedy parsers on both parsing tasks and on both English and Chinese."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for comments. We also thank Taro Watanabe, Muhua Zhu, and Yue Zhang for sharing their code, Haitao Mi for producing greedy results from his parser, and Ashish Vaswani and Yoav Goldberg for discussions. The authors were supported in part by DARPA FA8750-13-2-0041 (DEFT), NSF IIS1449278, and a Google Faculty Research Award."
    } ],
    "references" : [ {
      "title" : "A fast and accurate dependency parser using neural networks. In Empirical Methods in Natural Language Processing (EMNLP)",
      "author" : [ "Chen", "Manning2014] Danqi Chen", "Christopher D Manning" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistical parsing with an automatically-extracted tree-adjoining grammar",
      "author" : [ "David Chiang" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Chiang.,? \\Q2000\\E",
      "shortCiteRegEx" : "Chiang.",
      "year" : 2000
    }, {
      "title" : "Transition-based dependency parsing with stack long short-term memory. arXiv preprint arXiv:1505.08075",
      "author" : [ "Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith" ],
      "venue" : null,
      "citeRegEx" : "Dyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Event nugget detection with forward-backward recurrent neural networks",
      "author" : [ "Ghaeini et al.2016] Reza Ghaeini", "Xiaoli Z. Fern", "Liang Huang", "Prasad Tadepalli" ],
      "venue" : "In Proc. of ACL",
      "citeRegEx" : "Ghaeini et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ghaeini et al\\.",
      "year" : 2016
    }, {
      "title" : "Inducing history representations for broad coverage statistical parsing",
      "author" : [ "James Henderson" ],
      "venue" : "In Proceedings of NAACL",
      "citeRegEx" : "Henderson.,? \\Q2003\\E",
      "shortCiteRegEx" : "Henderson.",
      "year" : 2003
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580",
      "author" : [ "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Simple and accurate dependency parsing using bidirectional LSTM feature representations. CoRR, abs/1603.04351",
      "author" : [ "Kiperwasser", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Kiperwasser et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kiperwasser et al\\.",
      "year" : 2016
    }, {
      "title" : "Shift-reduce constituency parsing with dynamic programming and pos tag lattice",
      "author" : [ "Mi", "Huang2015] Haitao Mi", "Liang Huang" ],
      "venue" : "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Mi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mi et al\\.",
      "year" : 2015
    }, {
      "title" : "Maltparser: A data-driven parsergenerator for dependency parsing",
      "author" : [ "Nivre et al.2006] Joakim Nivre", "Johan Hall", "Jens Nilsson" ],
      "venue" : "In Proc. of LREC",
      "citeRegEx" : "Nivre et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2006
    }, {
      "title" : "Algorithms for deterministic incremental dependency parsing",
      "author" : [ "Joakim Nivre" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Nivre.,? \\Q2008\\E",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2008
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Joint pos tagging and transition-based constituent parsing in chinese with non-local features",
      "author" : [ "Wang", "Xue2014] Zhiguo Wang", "Nianwen Xue" ],
      "venue" : "In Proceedings of ACL",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Structured training for neural network transition-based parsing",
      "author" : [ "Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov" ],
      "venue" : "In Proceedings of ACL",
      "citeRegEx" : "Weiss et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2015
    }, {
      "title" : "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701",
      "author" : [ "Matthew D. Zeiler" ],
      "venue" : null,
      "citeRegEx" : "Zeiler.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Transition-based dependency parsing with rich non-local features",
      "author" : [ "Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2011
    }, {
      "title" : "Fast and accurate shift-reduce constituent parsing",
      "author" : [ "Zhu et al.2013] Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu" ],
      "venue" : "In Proceedings of ACL",
      "citeRegEx" : "Zhu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features.",
      "startOffset" : 142,
      "endOffset" : 185
    }, {
      "referenceID" : 8,
      "context" : "For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al.",
      "startOffset" : 143,
      "endOffset" : 401
    }, {
      "referenceID" : 8,
      "context" : "For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use 48 atomic features from Zhang and Nivre (2011), including select thirdorder dependencies.",
      "startOffset" : 143,
      "endOffset" : 449
    }, {
      "referenceID" : 8,
      "context" : "For example, Chen and Manning (2014) and subsequent work replace the huge amount of manual feature combinations in non-neural network efforts (Nivre et al., 2006; Zhang and Nivre, 2011) by vector embeddings of the atomic features. However, this approach has two related limitations. First, it still depends on a large number of carefully designed atomic features. For example, Chen and Manning (2014) and subsequent work such as Weiss et al. (2015) use 48 atomic features from Zhang and Nivre (2011), including select thirdorder dependencies.",
      "startOffset" : 143,
      "endOffset" : 500
    }, {
      "referenceID" : 2,
      "context" : "This effort is similar in motivation to the stack-LSTM of Dyer et al. (2015), but uses a much simpler architecture.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "Figure 3: The arc-standard dependency parsing system (Nivre, 2008) (rey omitted).",
      "startOffset" : 53,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "The structure of our network model after computing positional features is fairly straightforward and similar to previous neural-network parsing approaches such as Chen and Manning (2014) and Weiss et al. (2015). It consists of a multilayer perceptron using a single ReLU hidden layer followed by a linear classifier over the action space, with the training objective being negative log softmax.",
      "startOffset" : 191,
      "endOffset" : 211
    }, {
      "referenceID" : 16,
      "context" : "While most previous work binarizes the constituency tree in a preprocessing step (Zhu et al., 2013; Wang and Xue, 2014; Mi and Huang, 2015), we propose a novel “Shift-PromoteAdjoin” paradigm which does not require any binariziation or transformation of constituency trees (see Figure 5).",
      "startOffset" : 81,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : ", attachment (Chiang, 2000; Henderson, 2003).",
      "startOffset" : 13,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : ", attachment (Chiang, 2000; Henderson, 2003).",
      "startOffset" : 13,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "• it does not require binarization (Zhu et al., 2013; Wang and Xue, 2014) or compression of unary chains (Mi and Huang, 2015)",
      "startOffset" : 35,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "There is, however, a more closely-related “shiftproject-attach” paradigm by Henderson (2003). For the example in Figure 5 he would use the following actions:",
      "startOffset" : 76,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "Networks parameters were updated using gradient backpropagation, including backpropagation through time for the recurrent components, using ADADELTA for learning rate scheduling (Zeiler, 2012).",
      "startOffset" : 178,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "We also applied dropout (Hinton et al., 2012) (with p = 0.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 15,
      "context" : "1 The greedy accuracies for Mi and Huang (2015) are from Haitao Mi, and greedy results for Zhu et al. (2013) come from duplicating experiments with code provided by those authors.",
      "startOffset" : 91,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : "2 The parser of Vinyals et al. (2015) does not use an explicit transition system, but is similar in spirit since generating a right bracket can be viewed as a reduce action.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Because recurrent networks are such a natural fit for modeling languages (given the sequential nature of the latter), bi-directional LSTM networks are becoming increasingly common in all sorts of linguistic tasks, for example event detection in Ghaeini et al. (2016). In fact, we discovered after submission that Kiperwasser and Goldberg (2016) have concurrently developed an extremely similar approach to our dependency parser.",
      "startOffset" : 245,
      "endOffset" : 267
    }, {
      "referenceID" : 3,
      "context" : "Because recurrent networks are such a natural fit for modeling languages (given the sequential nature of the latter), bi-directional LSTM networks are becoming increasingly common in all sorts of linguistic tasks, for example event detection in Ghaeini et al. (2016). In fact, we discovered after submission that Kiperwasser and Goldberg (2016) have concurrently developed an extremely similar approach to our dependency parser.",
      "startOffset" : 245,
      "endOffset" : 345
    } ],
    "year" : 2016,
    "abstractText" : "Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition, and potentially omitting important global context. To further reduce feature engineering to the bare minimum, we use bi-directional LSTM sentence representations to model a parser state with only three sentence positions, which automatically identifies important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese.",
    "creator" : "LaTeX with hyperref package"
  }
}