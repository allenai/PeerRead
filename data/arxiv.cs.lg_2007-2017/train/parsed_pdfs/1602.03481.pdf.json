{
  "name" : "1602.03481.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reliable Crowdsourcing under the Generalized Dawid-Skene Model",
    "authors" : [ "Ashish Khetan", "Sewoong Oh" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n03 48\n1v 1\n[ cs\n.L G\n] 1\n0 Fe\nb 20\n16"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent advances from the machine learning community has made practical crowdsourcing systems more efficient. Several algorithms have been proposed that achieve the desired accuracy from smaller number of questions. This translates into savings in the budget for crowdsourcing requesters. However, the algorithmic and theoretical advances has focused on the Dawid-Skene model to describe how workers respond to the tasks, which does not capture how some tasks are inherently more difficult than the other. In fact, on real-world crowdsourced classification tasks, general models tailored to capture the inherent difficulty of the tasks achieve significantly better performances. We bridge this gap by studying a generalized Dawid-Skene model and investigating the fundamental limit on what the best algorithm can achieve and providing a matching linear-time algorithm.\nSetup. We consider a crowdsourcing system with m tasks to be classified and n workers. We model the worker responses using a recent generalization of the Dawid-Skene model introduced in Zhou et al. (2015), that models how different tasks can have different difficulties. Each task is parametrized by qi ∈ [0, 1] representing how likely the task is to be a positive task. When a worker j is assigned a task i, the task is perceived as a positive task with probability qi. Hence, if qi is close to a half then it is confusing and difficult to correctly classify, and easy if close to one or zero. The qi’s capture the heterogeneous difficulty of the tasks. We want to infer the true labels defined as\nti = I{qi>(1/2)} − I{qi<(1/2)} , (1)\nfrom noisy answers from the crowd. Each worker is parametrized by pj ∈ [0, 1] representing how likely the worker is to respond as he perceives. Presented with a task that he perceives as positive, a worker labels it as positive with probability pj , and otherwise makes a mistake and gives the opposite label. Hence, if pj is close to one then he tells the truth (in his opinion) and if it is close to half he gives random answers. Precisely, for each assigned task i, the worker j gives a noisy answer Aij ∈ {+1,−1} such that\nAij =\n{\n1, w.p. qipj + q̄ip̄j , −1, w.p. q̄ipj + qip̄j . , (2)\n∗Department of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana-Champaign, email: khetan2@illinois.edu\n†Department of Industrial and Enterprise Systems Engineering, University of Illinois at Urbana-Champaign, email: swoh@illinois.edu\nwhere q̄i = 1 − qi and p̄j = 1 − pj . Note that we focus on only binary tasks with two types of classes, and also the workers are symmetric, i.e. the error probability does not depend on the perceived label of the task. The original Dawid-Skene model introduced in Dawid & Skene (1979) and analyzed in Karger et al. (2014a) is a special case, when all tasks are equally easy, i.e. qi’s are either one or zero. All tasks are perceived their true class and the only source of error is in worker’s nosy response as per pj’s.\nLet p and q represent the vector of all the pj’s and qi’s respectively. Our analysis holds for any deterministic p and q, but for notational convenience we assume pj’s and qi’s are drawn i.i.d. according to some distribution, and we do not impose any condition on the distributions.\nGiven this mixture of easy and hard tasks, and also reliable and noisy workers, we want to correctly classify the tasks from the collected responses conveniently represented by a matrix A ∈ {0,±1}m×n. We let Tj denote the set of tasks assigned to worker j and Wi denote the set of workers assigned to task i. In practical crowdsourcing systems, the task requester has some control over the task assignment, as long as it is done in an online-fashion where the task assignments are made before any responses are collected. We propose using random (ℓ, r)-regular bipartite graphs for task assignment, where ℓ workers are assigned to each task and r tasks are assigned to each worker. The number of workers recruited, n, should be chosen according to the choice of ℓ and r, such that the total number of edges is consistent, i.e. n = mℓ/r. We use G(U, V,E) to denote the resulting task assignment graph, where U is the set of tasks, V is the set of workers, and E is the edge of assignment. Our main results support this choice of random graphs. We show that random graphs with the proposed inference algorithm is near-optimal, by comparing to a fundamental lower bound on what can be achieved using the best possible task assignment together with the best possible inference algorithm.\nRelated work. A naive approach to classify tasks is majority voting which is used widely in practice. Majority voting simply follows what the majority of workers agree on. This is the optimal ML estimator when all workers have the same error probability. However, its performance degrades when the quality of the workers is diverse. In a general scenario, when the crowd is of diverse quality, we can iteratively infer reliability of each worker and update our beliefs on the tasks. This idea was first introduced by Dawid and Skene (Dawid & Skene, 1979) who proposed an Expectation Maximization (EM) approach. Following this work, several approaches have been proposed to solve the inference problem under the Dawid-Skene model Smyth et al. (1995); Sheng et al. (2008); Karger et al. (2011); Liu et al. (2012a); Zhou et al. (2012); Li & Yu (2014); Zhang et al. (2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem. However, in this dense regime, all tasks are assigned a growing number of workers as the problem size increases and eventually all tasks are labelled correctly with high probability.\nIn this paper, we are interested in a more challenging setting where each task is assigned only a small number of workers. A spectral algorithm has been analyzed under the original Dawid-Skene model in Dalvi et al. (2013), and tighter analysis showed that the spectral approach is near-optimal in Karger et al. (2011). A weighted majority voting algorithm has been analyzed in Li et al. (2013), showing the dependence on the choice of the weights.\nThe original Dawid-Skene model, when all qi’s are either one or zero, several practical inference algorithms have been developed with strong performance guarantees. However, one of the main weakness of the model is that it does not capture how some tasks are more difficult than the others. To overcome this challenge, several practical models have been proposed recently Jin & Ghahramani (2003); Whitehill et al. (2009); Welinder et al. (2010); Zhou et al. (2015). Although inference algorithms based on these more complex models significantly improve over those based on Dawid-Skene model on real-world datasets, there is no analysis on their performance.\nContributions. We bridge this gap between simple models with strong guarantees and complex models that represent real-world data. To the best of our knowledge, we provide the first analysis on a crowdsourcing model that captures the difficulties of the tasks, namely the generalized Dawid-Skene model introduced in Zhou et al. (2015). We propose an efficient iterative algorithm based on spectral methods, which does not require any hyper-parameters, only takes the crowdsourced data as input, and runs in time linear in the number of received answers up to a logarithmic factor. We prove that this iterative algorithm is near-optimal by providing a sharp upper bound on the error rate and comparing it to a fundamental limit on what any algorithm can achieve using the best possible task assignment (with the same number of edges) and the best possible inference algorithm. We present numerical results on both synthetic and real datasets that supports our theoretical results."
    }, {
      "heading" : "2 Main Results",
      "text" : "We present out iterative algorithm based on a spectral method, and provide an upper bound on the error rate achieved by this algorithm in Theorem 2.2. Comparing it to a fundamental lower bound in Lemma 2.3, this establishes the near-optimality of our approach."
    }, {
      "heading" : "2.1 Algorithm",
      "text" : "We propose using a state-of-the-art spectral method based on non-backtracking operators, first introduced for inference in Karger et al. (2011). A similar approach has been later applied to other inference problems, e.g. Krzakala et al. (2013); Bordenave et al. (2015). We compare the proposed iterative algorithm with other standard methods in Section 4, and show that typical approaches such as the EM or the belief propagation are computationally intractable.\nThe proposed algorithm is a message passing algorithm that operates on two sets of messages: the task messages {xi→j}(i,j)∈E capturing how likely the task is to be a positive task and the worker messages {yj→i}(i,j)∈E capturing how reliable the worker is. In each round, all messages are updated as\nxi→j = ∑\nj′∈Wi\\j\nAij′yj′→i , and (3)\nyj→i = ∑\ni′∈Tj\\i\nAi′jxi′→j . (4)\nThe first is taking the weighted majority according to how reliable each worker is, and the second is updating the reliability according to how many times the worker agreed with what we believe. The precise description is given in Algorithm 1.\nAlgorithm 1 Message-Passing Algorithm for Inference Input: E, {Aij}(i,j)∈E , kmax Output: Estimate {t̂(k)i = sign(xi)}i∈[m]\n1: for all (i, j) ∈ E do 2: Initialize y(0)j→i with random Zj→i ∼ N (1, 1) 3: end for 4: for all k = 1, 2, · · · , kmax do 5: for all (i, j) ∈ E do 6: x\n(k) i→j ← ∑ j′∈Wi\\j Aij′y k−1 j′→i\n7: end for 8: for all (i, j) ∈ E do 9: y\n(k) j→i ← ∑ i′∈Tj\\i Ai′jx k i′→j\n10: end for 11: end for 12: for all i ∈ [m] do 13: xi ← ∑\nj∈Wi Aijy kmax−1 j→i\n14: end for\nThis message-passing algorithm is also a spectral method. Precisely, we are computing the top eigenvector of a matrix known as weighted non-backtracking operator, via standard power method. Note that the above mapping is a linear mapping from the messages to the messages. This mapping, if formed into a 2mℓ× 2mℓ dimensional matrix B is known as the non-backtracking operator. Precisely, for (i, j), (i′, j′) ∈ E,\nB(i→j),(j′→i′) =\n\n\n\nAi′j′ if j = j′ and i 6= i′ , Ai′j′ if j 6= j′ and i = i′ ,\n0 otherwise ,\nThe spectrum, which is the set of eigenvalues of this square but non-symmetric matrix B illustrates when and why spectral method might work. First consider decomposing the data matrix as A = E[A] + (A − E[A]). Simple\nanalysis shows that E[A] is a rank one matrix with spectral norm ‖E[A]‖ = √ (ℓ − 1)(r − 1)αβ, where\nα ≡ E[(2qi − 1)2], and β ≡ E[(2pj − 1)2] .\nAlso, typical random matrix analyses, such as those in Keshavan et al. (2009); Karger et al. (2013), shows that the spectral norm of the noise (A − E[A]) is bounded by C((ℓ − 1)(r − 1))1/4 with some constant C. Hence, when ‖E[A]‖ > ‖(A − E[A])‖, the top eigenvector of this matrix A corresponds to the true underlying signal, and we can hope to estimate the true labels from this top eigenvector. On the other hand, if ‖E[A]‖ < ‖(A − E[A])‖, one cannot hope to recover any signal from the top eigenvector of A. This is known as the spectral barrier.\nThis phenomenon is more prominent in the matrix B. Similar spectral analysis can be applied to show that when we are above the spectral barrier, the top eigenvalue is real-valued and λ1(B) = √\n(ℓ− 1)(r − 1)αβ and the mode of the rest of the complex valued eigenvalues are bounded within a circle of radius: |λi(B)| ≤ ((ℓ − 1)(r − 1))1/4. Hence, the spectral barrier is exactly when (ℓ−1)(r−1)α2β2 = 1, and this will play a crucial role in the performance guarantee in Theorem 2.2. Figure 1 illustrates two sides of the spectral barrier. The one on the left shows the scatter plot of the complex valued eigen values of B. Notice a pair of top eigen values at √\n0.3× (1.4/3)× 14× 14 ≃ 5.24 and −5.24 as predicted by the analysis. They always appear in pairs, due to the bipartite nature of the graph involved. The rest of the spurious eigenvalues are constrained within a circle of radius (14 × 14)1/4 ≃ 3.74 as predicted. The figure on the right is when we are below the spectral barrier, since the eigenvalue corresponding to the signal is √\n0.3× (1.4/3)× 4× 4 ≃ 1.5 which is smaller than (4 × 4)1/4 ≃ 2. The relevant eigenvalue is buried under other spurious eigenvalues and does not show."
    }, {
      "heading" : "2.2 Performance Guarantee",
      "text" : "We make the above intuitions precise in this section. We identify when the proposed spectral method works, and prove a sharp bound on the achievable error rate. To simplify the notation, let ℓ̂ ≡ ℓ−1, r̂ ≡ r−1, and define µ ≡ E[2pj−1]. When ℓ and r are increasing with the problem size, the messages converge to a Gaussian distribution due to the central limit theorem. By tracking how the mean and variance evolves, one can precisely describe the probability of error achieved by the proposed iterative algorithm.\nTheorem 2.1. Suppose for ℓ = Θ( √ logm/ log logm) and r = Θ( √\nlogm/ log logm), tasks are assigned according to (ℓ, r)-regular random graphs. If µ > 0, then for any t ∈ {±1}m, the estimate t̂i after k iterations of Algorithm 1 achieves for each i\nlim m→∞\nP [ ti 6= t̂(k)i ∣ ∣qi ]\n= Q (µ\n(k) qi ρ (k) qi ) , (5)\nwhere Q(x) is the cumulative distribution function of a standard Gaussian, and\nµ(k)q = (2q − 1)µℓ(ℓ̂r̂αβ)k , and (6)\n( ρ(k)q )2 = ℓ(2− µ2α)(ℓ̂r̂)k−1 + µ2ℓ(ℓ̂r̂αβ)2(k−1)× (\nα− (2q − 1)2 + (1− αβ)(1 + r̂αβ)αℓ̂(1 − (ℓ̂r̂α 2β2)−(k−1)\n(ℓ̂r̂α2β2)2 − 1\n)\n.\nComputing the mean of the messages conditioned on qi gives the expression of µ (k) q , and the variance gives\n(ρ (k) q )2. The theorem follows from the central limit theorem and a proof is provided in Section 5.\nThe above theorem exactly shows the two regimes of the spectral barrier. There is a threshold at ℓ̂r̂α2β2 = 1, above which the ratio µ(k)q /ρ (k) q decreases with the number of iterations achieving smaller error, until eventually it converges to a value that depends on the problem parameters. Since one step of the iterative algorithm is the same as majority voting, this implies that the performance of the iterative algorithm can significantly improve over majority voting. On the other hand, below the threshold, the ratio µ(k)q /ρ (k) q increases indefinitely. We are below the spectral barrier and the top eigenvector does not give us a accurate estimate of the underlying signal. Hence, iterating our algorithm gives worse performance than the simple majority voting.\nThe error for large k decays as e−ℓα(2qi−1) 2\n. Large ℓ means we have more data, large α means we have a reliable pool of crowds, and large (2qi − 1)2 means this task is an easy task. An interesting phenomenon is that the overall accuracy mainly depends on the distributions on qi and pj via only one parameter: β. This implies that, as long as we are above the spectral barrier, it does not matter how hard the other tasks are. We can achieve the same accuracy that we want for a task with difficulty qi, whether other tasks are easy or difficult.\nThis gives a very precise description of the accuracy, but holds only in the asymptotic regime where the problem size grows to infinity and also when the degrees grow with the problem size. In practice, we are most interested in constant degrees, since only a few workers are assigned to each task, and also in non-asymptotic results. We establish an upper bound on the error in this full generality, in the following theorem. The price we pay for removing the assumptions is in the slightly increased effective variance in (33) compared to the exact variance in Theorem 2.1.\nDefine σ2k to be the effective variance in the sub-Gaussian tail of the estimates after k iterations of the inference algorithm:\nσ2k ≡ 2β\nµ2 ( ℓ̂r̂(αβ)2 )k−1\n+ 3 ( 1 + 1\nr̂αβ\n)1− 1/ ( ℓ̂r̂(αβ)2 )k−1\n1− 1/ ( ℓ̂r̂(αβ)2 ) . (7)\nWith this, we prove the following upper bound on the probability of error for each task conditioned on its difficulty level qi.\nTheorem 2.2. For any ℓ > 1 and r > 1, suppose m tasks are assigned according to a random (ℓ, r)-regular graph drawn from the configuration model. If µ > 0, ℓ̂r̂α2β2 > 1, and r̂α > 1, then for any t ∈ {±1}m, the estimate t̂i after k iterations of Algorithm 1 achieves\nP [ ti 6= t̂(k)i ∣ ∣qi ] ≤ e−ℓβ(2qi−1)2/(2σ2k) + 3ℓr m (ℓ̂r̂)2k−2. (8)\nTherefore, we have,\n1\nm\nm ∑\ni=1\nP[ti 6= t̂(k)i ] ≤ Eq [ e −ℓβ 2σ2 k (2q−1)2 ] + 3ℓr\nm (ℓ̂r̂)2k−2. (9)\nFurther, when we run our algorithm for large enough numbers of iterations, σ2k converges linearly to a finite limit σ2∞ ≡ limk→∞ σ2k such that\nσ2∞ = 3\n(\n1 + 1\nr̂αβ\n)\n(ℓ̂r̂αβ)2\n(ℓ̂r̂αβ)2 − 1 . (10)\nWe provide a proof of the theorem in Section 5. Note that the error decays mainly as e−α(2qi−1) 2ℓ.\nAn interesting observation is that the error exponent depends on (2qi − 1)2. For easy tasks, one can obtain accurate estimates, whereas for difficult tasks the accuracy significantly degrades. Therefore, the overall average\naccuracy will be dominated by those difficult tasks in (9). This is illustrate in Figure 2. The error decays exponentially in ℓ and α as predicted, but the rate of decay crucially hinges on the difficulty level. We run synthetic experiments with m = n = 1000 and the crowds are generated from the spammer-hammer model where pj = 1 with probability β and 1/2 otherwise Karger et al. (2014b). We fix β = 0.3 and vary ℓ in the left figure and fix ℓ = 30 and vary β in the right figure. We let qi’s take values in {0.6, 0.8, 1} with equal probability such that α = 1.4/3. The error rate of those tasks with the same qi’s are plotted in the dashed lines. This illustrates the predicted error rate of e−Ω(ℓβ(2qi−1)\n2), and also shows that the overall average error rate plotted in solid lines are dominated by those difficult tasks.\nTo confirm the scaling of the error with respect to (2qi−1), we rescale the logarithmic error rate in Figure 3 (left), where we plot the error achieved by subset of tasks with the same level of difficulty. We used finer quantization for the levels of difficulty. The iterative algorithm matches the theoretical prediction of scaling as 2qi, whereas majority voting has significantly smaller error exponent, and hence larger error. This is compared against an oracle estimator that knows all the pj’s and performs the optimal estimate.\nThe figure on the right panel of Figure 3 illustrates that the error rate of the iterative algorithm converges to a mother curve when appropriately rescaled by ℓβ. For two very different choices of β, 0.01 and 0.25, we observe that both oracle estimator and the iterative algorithm converge to the same mother curve, where as MV significantly degrades for small β. This is because the MV is not able to identify those few workers who are reliable.\nWe have set ℓ = r and α = 1.4/3 in this experiment. The theoretical spectral barrier is at ℓ̂β = 3/1.4. We see that in the limit where we fix β = 1/ℓ̂ and grow ℓ̂, the curve for MV and the iterative algorithm cross at ℓβ = 2, which is very close to the spectral barrier.\n√"
    }, {
      "heading" : "2.3 Fundamental Limit",
      "text" : "It follows from Theorem 2.2 that in order to achieve an error smaller than ε for a task i, it suffices to assign ℓ ≥ (c/(β(2qi − 1)2)) log(1/ε), under a broad range of parameters. We show in the following theorem that this scaling is also necessary. Hence, our iterative algorithm is near-optimal in terms of the average number of workers assigned to each task, in a minimax scenario where we consider the best task assignment scheme with the best inference algorithm, and the nature chooses the worst distribution of worker pj’s among the set of distributions with the same β. We provide a proof of the lemma in Section B in the supplementary material.\nLemma 2.3. There exists a positive constant C′ s.t. when (2qi − 1) < 1, if the number of workers assigned to task i by any non-adaptive task assignment scheme is less than (C′/(β(2qi − 1)2)) log(1/ǫ), then no algorithm can achieve conditional probability of error on task i, conditioned on qi, less than ǫ for any m and r."
    }, {
      "heading" : "3 Experimental Results",
      "text" : "In Figure 4, we compare our algorithm with alternating minimization and majority voting on simulated data and real data. The first plot is generated under the same settings as the first plot of Figure 3 except that here we use n = m = 300 and β = 0.2. It shows that our algorithm and alternating minimization performs almost same after the phase transition while alternating minimization performs better before the phase transition. In the second plot, we compare all the three algorithms on real data collected from Amazon Mechanical Turk. We designed an experiment as follows. We created tasks for comparing colors; we showed three colors on each task and asked the worker to indicate “whether the first color is more similar to the second color or the third color.” We generated 50 of such color comparison tasks and recruited 28 workers to answer all the tasks. We take the ground truth according to which color is closer to the first color in pairwise distances in the Lab color space. The second plot shows probability of error of the three algorithms when number of queries per task ℓ is varied. We generated responses for different values of ℓ by uniformly sub-sampling."
    }, {
      "heading" : "4 Comparisons to other algorithms",
      "text" : "Expectation Maximization. When the crowd is of diverse quality, to estimate tasks optimally, we need to first estimate workers abilities pj’s, usually via a maximum a posteriori estimator, p̂ = argmax logP[p|A,Fp] = argmax log ∫\nP[p,q|A,Fp]dq, where Fp is the prior distribution on latent parameters pj’s. If qi’s are discrete this is commonly solved using an EM algorithm treating them as latent variables. However, since qi’s are continuous in our setting, minimizing the lower bound ∫ P[q|p0,A] log ( P[p,q|A,Fp]/P[q|p0,A] )\ndq is computationally challenging. This is because the posterior distribution P[q|p0,A] for a given initial p0, involves product of likelihood of each response.\nGraphical Model. The joint posterior distribution of the task difficulties qi’s and workers abilities pj’s conditional on the observed responses Aij’s and prior distributions Fq and Fp on qi’s and pj’s respectively is\nP[q,p|A,Fq,Fp] ∝ ∏\ni∈[m]\n( P[qi|Fq] )\n∏\nj∈[n]\n(\nP[pj|Fp] ∏\ni∈Wj\nP[Aij |pi, qj ] ) . (11)\nThe optimal estimator of ti’s to minimize the bit-wise error rate is given by\nt̂i = argmax ti\nP[ti|A,Fq,Fp] where\nP[ti = 1|A,Fq,Fp]\n=\n∫\nqi′ :i ′∈[m]\\i\n∫ 1\n1/2\n∫\np\nP[q,p|A,Fq ,Fp]dp dqi dqi′ . (12)\nFor ti = −1, the first integral would be from 0 to 0.5. If q andp are both discrete the marginals on ti’s can be computed using belief propagation or mean field method. Only when pj’s are continuous and qi’s are discrete Liu et al. (2012b) propose algorithms to compute the marginals using modified belief propagation and mean field method both. However, in our setting where qi’s are also continuous, standard belief propagation approaches are challenging to implement.\nAlternating minimization. Since none of the above methods are immediately tractable, we propose to minimize the joint posterior distribution (11). However, this is not jointly concave in q and p. Therefore, we perform alternating minimization. Define a function g : {±1} × [0, 1]× [0, 1] → [−∞, 0] such that\ng(Aij , qi, pj) =\n{\nlog(qipj + q̄ip̄j) if Aij = 1 log(q̄ipj + qj p̄i) if Aij = −1 (13)\nThe logarithm of joint posterior distribution (11) is\nL(q,p|A,Fq ,Fp) = ∑\ni∈[m]\n∑\nj∈Wi\ng(Aij , qi, pj)\n+ ∑\ni∈[m]\nlog(P[qi|Fq]) + ∑\nj∈[n]\nlog(P[pj |Fp]) . (14)\nWith properly chosen prior distributions Fq and Fp, in particular Beta priors, it is easy to see that given q the joint log likelihood is a concave function ofp and vice-versa. We start with qi = |W+i |/(|W+i |+|W−i |) and perform alternating minimization on (14) with respect to q and p iteratively until convergence, where W+i = {j ∈ Wi : Aij = 1} and W−i = {j ∈ Wi : Aij = −1}. The performance of this algorithm is compared in Figure 4."
    }, {
      "heading" : "5 Proof of Theorems 2.1 and 2.2",
      "text" : "We will prove the main results for a randomly chosen node I, and all the analyses naturally holds for a specific i, when conditioned on qi. Let t̂ (k) i denote the resulting estimate of task i after running the iterative inference algorithm for k iterations. We want to compute the conditional probability of error of a task I selected uniformly at random in [m], conditioned on its difficulty level, i.e.,\nP [ tI 6= t̂(k)I ∣ ∣qI ] .\nIn the following, we assume qI ≥ (1/2), i.e. the true label is ti = 1. Analysis for qI ≤ (1/2) would be similar and result in the same bounds. In our algorithm, we perform task assignment on a random bipartite graph G([m]∪ [n], E) constructed according to the configuration model. Let Gi,k denote a subgraph of G([m] ∪ [n], E) that includes all the nodes that are within k distance from the the “root” i. If we run our inference algorithm for one run to estimate t̂i, we only use the responses provided by the workers who were assigned to task i. That is we are running inference algorithm only on the local neighborhood graphGi,1. Similarly, when we run our algorithm for k iterations to estimate t̂i, we perform inference only on the local subgraph Gi,2k−1. Since we update both task and worker messages at each iteration, the local subgraph grows by distance two at each iteration. We use a result from Karger et al. (2014b) to show that the local neighborhood of a randomly chosen task node I is a tree with high probability. Therefore, assuming that the graph is locally tree like with high probability, we can apply a technique known as density evolution to estimate the probability of error. When the graph is not locally tree like, we can assume that our algorithm makes an error. Therefore, we have,\nP [ tI 6= t̂(k)I ∣ ∣qI ] ≤ P [ tI 6= t̂(k)I ∣ ∣GI,2k−1 is a tree, qI ]\n+P [ GI,2k−1 is not a tree ] . (15)\nThe next lemma from Karger et al. (2014b) shows that the local subgraph is a tree with high probability as m grows.\nLemma 5.1 (Lemma 5 from Karger et al. (2014b)). For a random (ℓ, r)-regular bipartite graph generated according to the configuration model,\nP [ GI,2k−1 is not a tree ] ≤ ( (ℓ− 1)(r − 1) )2k−2 3ℓr\nm . (16)\nTo provide an upper bound on the first term in (15), let x(k)i denote the decision variable for task i after k iterations\nof the algorithm such that t̂(k)i = sign(x (k) i ). Then as per our assumption that ti = 1, we have,\nP [ tI 6= t̂(k)I |GI,2k−1is a tree, qI ] ≤ P [\nx (k) I ≤ 0|GI,2k−1is a tree, qI ] . (17)\nNext, we apply “density evolution” Mezard & Montanari (2009) and provide a sharp upper bound on the probability of the decision variable x(k)I being negative in a locally tree like graph given qI ≥ (1/2). The proof technique is similar to the one introduced in Karger et al. (2014b). Precisely, we show,\nP [ x (k) I ≤ 0|GI,2k−1 is a tree , qI ] = P [ x̂(k)q ≤ 0 ] , (18)\nwhere x̂(k)q is defined in Equations (21)-(23) using density evolution. We will prove in the following that when ℓ̂r̂(αβ)2 > 1 and r̂α > 1,\nP [ x̂(k)q ≤ 0 ] ≤ e−ℓβ(2qI−1)2/(2σ2k). (19) Theorem 2.2 follows by combining Equations (15),(16),(17) and (18).\nDensity Evolution. Let {x(k)i→j}(i,j)∈E and {y (k) j→i}(i,j)∈E denote the messages at the k-th iteration of the algorithm. For an edge (i, j) chosen uniformly at random, let x(k)q denote the random variable corresponding to the message x(k)i→j conditioned on the i-th task’s difficulty being q. Similarly, let y (k) p denote the random variable corresponding to the message y(k)j→i conditioned on the j-th worker’s quality being p.\nAt the first iteration, the task messages are updated according to x(1)i→j = ∑ j′∈∂i\\j Aij′y (0) j′→i. Since we initialize the worker messages {y(0)j→i}(i,j)∈E with independent Gaussian random variables with mean and variance both one, if we know the distribution of Aij′ ’s, then we have the distribution of x (1) i→j . Since, we are assuming that the local subgraph is tree-like, all x(1)i→j for i ∈ GI,2k−1 for any randomly chosen node I are independent. Further, because of the symmetry in the construction of the random graph G all messages x(1)i→j ’s are identically distributed. Precisely, x (1) i→j are distributed according to x (1) q defined in Equation (21). In the following, we recursively define x (k) q and y (k) p in Equations (21) and (22). For brevity, here and after, we drop the superscript k-iteration number whenever it is clear from the context. Let xq,a’s and yp,b’s be independent random variables distributed according to xq and yp respectively. We use a and b as indices for independent random variables with the same distribution. Also, let zp,q,a’s and zp,q,b’s be independent random variables distributed according to zp,q , where\nzp,q =\n{\n+1 w.p. pq + (1− p)(1− q) , −1 w.p. p(1− q) + (1− p)q . (20)\nThis represents the response given by a worker conditioned on the task having difficulty q and the worker having ability p. Let F1 and F2 over [0, 1] be the distributions of the tasks’ difficulty level and workers’ quality respectively. Let q ∼ F1 and p ∼ F2. Then qa’s and pb’s are independent random variables distributed according to q and p respectively. Further, zp,qa,a’s and xqa,a’s are conditionally independent conditioned on qa; and zpb,q,b’s and ypb,b’s are conditionally independent conditioned on pb.\nLet d = denote equality in distribution. Then for k ∈ {1, 2, · · · }, the task messages (conditioned on the latent task difficulty level q) are distributed as the sum of ℓ − 1 incoming messages that are i.i.d. according to y(k−1)p and weighted by i.i.d. responses:\nx(k)q d =\n∑\nb∈[ℓ−1]\nzpb,q,by (k−1) pb,b . (21)\nSimilarly, the worker messages (conditioned on the latent worker quality p) are distributed as the sum of r−1 incoming messages that are i.i.d. according to x(k)q and weighted by the i.i.d. responses:\ny(k)p d =\n∑\na∈[r−1]\nzp,qa,ax (k) qa,a. (22)\nFor the decision variable x(k)I on a task I chosen uniformly at random, we have\nx̂(k)q d =\n∑\na∈[ℓ]\nzpa,q,ay (k−1) pa,a . (23)\nSince the messages take continuous values, we cannot hope to compute the distribution of messages in (21) and (22). We prove using recursion that the messages are sub-Gaussian and hence obtain the exponential bound on probability of error in (19).\nMean and Variance Computation. First, to understand how the messages behave, we analyze the evolution of the mean and variance of the task and worker messages. Define m(k)q ≡ E[x(k)q |q] and m̂(k)p ≡ E[y(k)p |p], ν(k)q ≡ Var(x (k) q |q) and ν̂(k)p ≡ Var(y(k)p |p). Recall the notations µ ≡ E[2p − 1], α ≡ E[(2q − 1)2], β ≡ E[(2p − 1)2], ℓ̂ = ℓ− 1, and r̂ = r − 1. Then from (21) and (22) and using E[zp,q ] = (2p− 1)(2q − 1) we get the following:\nm(k)q = ℓ̂(2q− 1)Ep [ (2p− 1)m̂(k−1)p ] , (24) m̂(k)p = r̂(2p− 1)Eq [ (2q− 1)m(k)q ] , (25) ν(k)q = ℓ̂ { Ep [ ν̂(k−1)p + (m̂ (k−1) p ) 2 ] − (m(k)q /ℓ̂)2 } , (26)\nν̂(k)p = r̂ { Eq [ ν(k)q + (m (k) q ) 2 ] − (m̂(k)p /r̂)2 } . (27)\nDefine m(k) ≡ Eq[(2q − 1)m(k)q ] and ν(k) ≡ Eq[ν(k)q ]. From (24) and (25), we have the following recursion on the first moment of the random variable x(k)q :\nm(k)q = ℓ̂r̂(2q− 1)βm(k−1),m(k) = ℓ̂r̂αβm(k−1) . (28)\nFrom (26) and (27), and using Eq[(m (k) q )2] = (m(k))2/α (from (28)), and Ep[(m̂ (k) p )2] = r̂2β(m(k))2 (from (25)) , we get the following recursion on the second moment:\nν(k)q = ℓ̂r̂ν (k−1) + ℓ̂r̂(m(k−1))2\n(\n(1− αβ)(1 + r̂αβ) +r̂α(β)2(α− (2q− 1)2) ) /α , (29)\nν(k) = ℓ̂r̂ν(k−1) + ℓ̂r̂(m(k−1))2(1− αβ)(1 + r̂αβ)/α. (30)\nSince m̂(0)p = 1 as per our assumption, we have m (1) q = ℓ̂µ(2q − 1) and m(1) = ℓ̂µα. Therefore from (28), we have m(k) = ℓ̂µα(ℓ̂r̂αβ)k−1 and m(k)q = ℓ̂µ(2q − 1)(ℓ̂r̂αβ)k−1. Further, since ν̂(0)p = 1 as per our assumption, we have ν (1) q = ℓ̂(2 − µ2(2q − 1)2) and ν(1) = ℓ̂(2 − µ2α). This implies that ν(k) = aν(k−1) + bck−2, with a = ℓ̂r̂, b = µ2αℓ̂3r̂(1−αβ)(1+ r̂αβ) and c = (ℓ̂r̂αβ)2. After some algebra, we have that ν(k) = ν(1)ak−1+bck−2 ∑k−2ℓ=0 (a/c)ℓ. For ℓ̂r̂(αβ)2 > 1, we have a/c < 1 and\nν(k)q = ℓ̂(2− µ2α)(ℓ̂r̂)k−1 + µ2ℓ̂(ℓ̂r̂αβ)2k−2(α− (2q− 1)2)\n+\n( 1− 1/(ℓ̂r̂(αβ)2)k−1 (ℓ̂r̂(αβ)2)2 − 1 )\n(1− αβ)(1 + r̂αβ)µ2αℓ̂2(ℓ̂r̂αβ)2k−2. (31)\nBy a similar analysis, mean and variance of the decision variable x̂(k)q in (23) can also be computed. In particular, we show that x̂(k) is sub-Gaussian with some appropriate parameter and then apply the Chernoff bound. A random\nvariable x with mean µ is said to be sub-Gaussian with parameter σ if for all λ ∈ R the following bound holds for its moment generating function:\nE[eλx] ≤ eµλ+(1/2)σ2λ2 . (32)\nDefine,\nσ̃2k ≡ 3ℓ̂3r̂µ2α(r̂αβ + 1)(ℓ̂r̂αβ)2k−4 (1− 1/(ℓ̂r̂(αβ)2)k−1\n1− 1/(ℓ̂r̂αβ) )\n+ 2ℓ̂(ℓ̂r̂)k−1 , (33)\nmk ≡ µℓ̂(ℓ̂r̂αβ)k−1, and mk,q ≡ (2q − 1)mk for k ∈ Z, where q ∼ F1. We will show that, x(k)q is sub-Gaussian with mean mk,q and parameter σ̃2k for |λ| ≤ 1/(2mk−1r̂α), i.e.,\nE[eλx (k) q |q] ≤ emk,qλ+(1/2)σ̃2kλ2 . (34)"
    }, {
      "heading" : "6 Discussion",
      "text" : "We study a generalized Dawid-Skene model that captures the difficulties of the tasks. We propose a spectral approach that runs in time linear in the problem parameters. We show this approach achieves near-optimal performance by providing a sharp bound on the error rate, and comparing it to a fundamental lower bound. One of the most interesting finding is that when tasks are heterogeneous, uniformly assigning the same number of workers to each task is bad. Overall average error rate is dominated by the most difficult tasks. It matches our intuition that one should adaptively assign more workers to those tasks that are inherently more difficult. Systematically investigating rigorous adaptive task assignment schemes is an important open question."
    }, {
      "heading" : "A Proof of Theorems 2.1 and 2.2 continued",
      "text" : "Analyzing the Density. Notice that the parameter σ̃2k does not depend upon the random variable q. By definition of x̂ (k) q , (23), we have E[eλx̂ (k) q |q] = E[eλx(k)q |q](ℓ/ℓ̂). Therefore, it follows that E[eλx̂(k)q |q] ≤ e(ℓ/ℓ̂)mk,qλ+(ℓ/2ℓ̂)σ̃2kλ2 . Using the Chernoff bound with λ = −mk,q/(σ̃2k), we have\nP[x̂(k)q ≤ 0 | q] ≤ E[eλx̂ (k) q |q] ≤ e−ℓm2k,q/(2ℓ̂σ̃2k) . (35)\nNote that, with the assumption that q ≥ (1/2), mk,q is non-negative. Since\nmk,qmk−1,q σ̃2k ≤ (2q− 1) 2µ2ℓ̂2(ℓ̂r̂αβ)2k−3 3µ2β(α)2 ℓ̂3r̂2(ℓ̂r̂αβ)2k−4 = (2q− 1)2 3r̂α ,\nit follows that |λ| ≤ 1/(2mk−1r̂α). The desired bound in (19) follows. Now, we are left to prove Equation (34). From (21) and (22), we have the following recursive formula for the evolution of the moment generating functions of xq and yp:\nE[eλx (k) q |q] = (\nEp\n[ (pq+ p̄q̄)E[eλy (k−1) p |p]\n+(pq̄+ p̄q)E[e−λy (k−1) p |p] ])ℓ̂ , (36)\nE[eλy (k) p |p] = (\nEq\n[ (pq+ p̄q̄)E[eλx (k) q |q]\n+(pq̄+ p̄q)E[e−λx (k) q |q] ])r̂ , (37)\nwhere p̄ = 1 − p and q̄ = 1 − q. We apply induction to prove that the messages are sub-Gaussian. First, for k = 1, we show that x(1)q is sub-Gaussian with mean m1,q = (2q − 1)µℓ̂ and parameter σ̃21 = 2ℓ̂. Since, yp is initialized as a Gaussian random variable with mean and variance both one, we have E[eλy (0) p ] = eλ+(1/2)λ 2\n. Substituting this into Equation (36), we get for any λ,\nE[eλx (1) q |q] = (( E[p]q + E[p̄]q̄ ) eλ + ( E[p]q̄ + E[p̄]q ) e−λ )ℓ̂\ne(1/2)λ 2 ℓ̂ (38)\n≤ e(2q−1)µℓ̂λ+(1/2)(2ℓ̂)λ2 , (39)\nwhere the inequality follows from the fact that aez + (1 − a)e−z ≤ e(2a−1)z+(1/2)z2 for any z ∈ R and a ∈ [0, 1] (Lemma A.1.5 from Alon & Spencer (2004)). Next, assuming E[eλx (k) q |q] ≤ emk,qλ+(1/2)σ̃2kλ2 for |λ| ≤ 1/(2mk−1r̂α), we show that E[eλx (k+1) q |q] ≤ emk+1,qλ+(1/2)σ̃2k+1λ2 for |λ| ≤ 1/(2mkr̂α), and compute appropriate mk+1,q and σ̃2k+1.\nSubstituting the bound E[eλx (k) q |q] ≤ emk,qλ+(1/2)σ̃2kλ2 in (37), we have\nE[eλy (k) p |p]\n≤ (\nEq\n[ (pq+ p̄q̄)emk,qλ + (pq̄+ p̄q)e−mk,qλ ])r̂\ne(1/2)σ̃ 2 kλ 2 r̂\n≤ (\nEq\n[ e(2q−1)(2p−1)mk,qλ+(1/2)(mk,qλ) 2])r̂ e(1/2)σ̃ 2 kλ 2 r̂ (40)\n= (\nEq\n[ e(2p−1)(2q−1) 2mkλ+(1/2)(2q−1) 2(mkλ) 2])r̂ e0.5σ̃ 2 kλ 2 r̂ (41)\nwhere (40) uses the inequality aez + (1 − a)e−z ≤ e(2a−1)z+(1/2)z2 and (41) follows from the definition of mk,q ≡ (2q− 1)mk. To bound the term in (41), we use the following lemma.\nLemma A.1. For any random variable s ∈ [0, 1], |z| ≤ 1/2 and |t| < 1, we have\nE [ estz+(1/2)sz 2] ≤ exp ( E[s]tz + (3/2)E[s]z2 ) . (42)\nFor |λ| ≤ 1/(2mkr̂α), using the assumption that r̂α > 1, we have mkλ ≤ (1/2). Applying Lemma A.1 on the term in (41), with s = (2q− 1)2, z = mkλ and t = (2p− 1), we get\nE[eλy (k) p |p] ≤ eα(2p−1)r̂mkλ+(1/2)\n(\n3αm2k+σ̃ 2 k\n)\nλ2 r̂ . (43)\nSubstituting the bound in (43) in Equation (36), we get\nE[eλx (k+1) q |q]\n≤ (\nEp\n[\n(pq+ p̄q̄)eα(2p−1)mkλr̂ +\n(pq̄+ p̄q)e−α(2p−1)mkλr̂ ])ℓ̂ e(1/2)(3αm 2 k+σ̃ 2 k)λ 2 ℓ̂r̂\n≤ (\nEp\n[ e(2q−1)(2p−1) 2αmkλr̂+(1/2)(2p−1) 2(αmkλr̂) 2])ℓ̂\ne(1/2)(3αm 2 k+σ̃ 2 k)λ 2ℓ̂r̂ (44)\n≤ eℓ̂r̂αβmk,qλ+(1/2)ℓ̂r̂ ( σ̃2k+3αm 2 k(1+r̂αβ) ) λ2 , (45)\nwhere (44) uses the inequality aez + (1 − a)e−z ≤ e(2a−1)z+(1/2)z2 . Equation (45) follows from the application of Lemma A.1, with s = (2p− 1)2, z = αmkλr̂ and t = (2q− 1). For |λ| ≤ 1/(2mkr̂α), |z| < (1/2).\nIn the regime where ℓ̂r̂(αβ)2 > 1, as per our assumption, mk is non-decreasing in k. At iteration k, the above recursion holds for |λ| ≤ 1/(2r̂α)min{1/m1, · · · , 1/mk−1} = 1/(2mk−1r̂α). Hence, we get the following recursion for mk,q and σ̃2k such that (34) holds for |λ| ≤ 1/(2mk−1r̂α):\nmk,q = ℓ̂r̂αβmk−1,q,\nσ̃2k = ℓ̂r̂σ̃ 2 k−1 + 3ℓ̂r̂(1 + r̂αβ)αm 2 k−1 . (46)\nWith the initialization m1,q = (2q− 1)µℓ̂ and σ̃21 = 2ℓ̂, we have mk,q = µ(2q − 1)ℓ̂(αβℓ̂r̂)k−1 for k ∈ {1, 2, · · · } and σ̃2k = aσ̃ 2 k−1 + bc\nk−2 for k ∈ {2, 3 · · · }, with a = ℓ̂r̂, b = 3ℓ̂3r̂µ2α(1 + αβr̂), and c = (αβℓ̂r̂)2. After some algebra, we have σ̃2k = σ̃ 2 1a k−1 + bck−2 ∑k−2 ℓ=0 (a/c) ℓ. For ℓ̂r̂(αβ)2 6= 1, we have a/c 6= 1, whence σ̃2k = σ̃ 2 1a k−1 + bck−2(1− (a/c)k−1)/(1− a/c). This finishes the proof of (34).\nA.1 Proof of Lemma A.1\nUsing the fact that ea ≤ 1 + a+ 0.63a2 for |a| ≤ 5/8,\nE [ estz+(1/2)sz 2]\n≤ E [ 1 + stz + (1/2)sz2 + 0.63 ( stz + (1/2)sz2 )2]\n≤ E [ 1 + stz + (1/2)sz2 + 0.63 ( (5/4)z √ s )2]\n≤ 1 + E[s]tz + (3/2)E[s]z2 ≤ exp ( E[s]tz + (3/2)E[s]z2 ) ."
    }, {
      "heading" : "B Proof of lower bound lemma 2.3",
      "text" : "Let F denote a distribution on the worker quality pj such that pj ∼ F . Let Fβ be a collection of all distributions F such that:\nFβ = { F | EF [(2pj − 1)2] = β } .\nDefine the minimax rate on the probability of error of a task i, conditioned on its difficulty level qi, as\nmin τ∈Tℓi ,t̂ max ti∈{±},F∈Fβ\nP[ti 6= t̂i | qi] , (47)\nwhere Tℓi is the set of all nonadaptive task assignment schemes that assign ℓi workers to task i, and t̂ ranges over the set of all estimators of ti. Since the minimax rate is the maximum over all the distributions F ∈ Fβ , we consider a\nparticular worker quality distribution to get a lower bound on it. In particular, we assume the pj’s are drawn from a spammer-hammer model with perfect hammers:\npj =\n{\n1/2 with probability 1− β, 1 otherwise.\nObserve that the chosen spammer-hammer models belongs to Fβ , i.e. E[(2pj−1)2] = β. To get the optimal estimator, we consider an oracle estimator that knows all the pj’s and hence makes an optimal estimation. It estimates t̂i using majority voting on hammers and ignores the answers of hammers. If there are no hammers then it flips a fair coin and estimates t̂i correctly with half probability. It does the same in case of tie among the hammers. Concretely,\nt̂i = sign\n(\n∑\nj∈Wi\nI{j ∈ H})Aij ) ,\nwhere Wi denotes the neighborhood of node i in the graph and H is the set of hammers. Note that this is the optimal estimation for the spammer-hammer model. We want to compute a lower bound on P[ti 6= t̂i|qi]. Let ℓ̃i be the number of hammers answering task i, i.e.,ℓ̃i = |Wi ∩H|. Since pj’s are drawn from spammer-hammer model, ℓ̃i is a binomial random variable Binom(ℓi, β). We first compute probability of error conditioned on ℓ̃i, i.e. P[ti 6= t̂i|ℓ̃i, qi]. For this, we use the following lemma from Karger et al. (2014b).\nLemma B.1 (Lemma 2 from Karger et al. (2014b)). For any C < 1, there exists a positive constant C′ such that when (2qi − 1) ≤ C, the error achieved by majority voting is at least\nmin τ∈T\nℓ̃\nmax ti∈{±}\nP[ti 6= t̂i|ℓ̃i, qi] ≥ e−C ′(ℓ̃i(2qi−1) 2+1). (48)\nTaking expectation with respect to random variable ℓ̃i and applying Jensen’s inequality on the term in right side, we get a lower bound on the minimiax probability of error in (47)\nmin τ∈T\nℓ̃ ,t̂ max F∈Fβ ti∈{±}\nP[ti 6= t̂i|qi] ≥ e−C ′(ℓiβ(2qi−1) 2+1) . (49)"
    } ],
    "references" : [ {
      "title" : "The probabilistic method",
      "author" : [ "Alon", "Noga", "Spencer", "Joel H" ],
      "venue" : null,
      "citeRegEx" : "Alon et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2004
    }, {
      "title" : "Non-backtracking spectrum of random graphs: community detection and non-regular ramanujan graphs",
      "author" : [ "Bordenave", "Charles", "Lelarge", "Marc", "Massoulié", "Laurent" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Bordenave et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bordenave et al\\.",
      "year" : 2015
    }, {
      "title" : "Aggregating crowdsourced binary ratings",
      "author" : [ "N. Dalvi", "A. Dasgupta", "R. Kumar", "V. Rastogi" ],
      "venue" : "In Proceedings of the 22nd international conference on World Wide Web,",
      "citeRegEx" : "Dalvi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2013
    }, {
      "title" : "Maximum likelihood estimation of observer error-rates using the em algorithm",
      "author" : [ "A.P. Dawid", "A.M. Skene" ],
      "venue" : "Applied statistics,",
      "citeRegEx" : "Dawid and Skene,? \\Q1979\\E",
      "shortCiteRegEx" : "Dawid and Skene",
      "year" : 1979
    }, {
      "title" : "Minimax optimal convergence rates for estimating ground truth from crowdsourced labels",
      "author" : [ "C. Gao", "D. Zhou" ],
      "venue" : "arXiv preprint arXiv:1310.5764,",
      "citeRegEx" : "Gao and Zhou,? \\Q2013\\E",
      "shortCiteRegEx" : "Gao and Zhou",
      "year" : 2013
    }, {
      "title" : "Who moderates the moderators?: crowdsourcing abuse detection in user-generated content",
      "author" : [ "Ghosh", "Arpita", "Kale", "Satyen", "McAfee", "Preston" ],
      "venue" : "In Proceedings of the 12th ACM conference on Electronic commerce,",
      "citeRegEx" : "Ghosh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning with multiple labels",
      "author" : [ "R. Jin", "Z. Ghahramani" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Jin and Ghahramani,? \\Q2003\\E",
      "shortCiteRegEx" : "Jin and Ghahramani",
      "year" : 2003
    }, {
      "title" : "Iterative learning for reliable crowdsourcing systems. In Advances in neural information processing",
      "author" : [ "D.R. Karger", "S. Oh", "D. Shah" ],
      "venue" : null,
      "citeRegEx" : "Karger et al\\.,? \\Q1953\\E",
      "shortCiteRegEx" : "Karger et al\\.",
      "year" : 1953
    }, {
      "title" : "Efficient crowdsourcing for multi-class labeling",
      "author" : [ "D.R. Karger", "S. Oh", "D. Shah" ],
      "venue" : "In Proceedings of the ACM SIGMETRICS/international conference on Measurement and modeling of computer systems,",
      "citeRegEx" : "Karger et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Karger et al\\.",
      "year" : 2013
    }, {
      "title" : "Budget-optimal task allocation for reliable crowdsourcing systems",
      "author" : [ "D.R. Karger", "S. Oh", "D. Shah" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Karger et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Karger et al\\.",
      "year" : 2014
    }, {
      "title" : "Budget-optimal task allocation for reliable crowdsourcing systems",
      "author" : [ "Karger", "David R", "Oh", "Sewoong", "Shah", "Devavrat" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Karger et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Karger et al\\.",
      "year" : 2014
    }, {
      "title" : "Matrix completion from a few entries",
      "author" : [ "Keshavan", "Raghunandan H", "Oh", "Sewoong", "Montanari", "Andrea" ],
      "venue" : "In Information Theory,",
      "citeRegEx" : "Keshavan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Keshavan et al\\.",
      "year" : 2009
    }, {
      "title" : "Spectral redemption in clustering sparse networks",
      "author" : [ "Krzakala", "Florent", "Moore", "Cristopher", "Mossel", "Elchanan", "Neeman", "Joe", "Sly", "Allan", "Zdeborová", "Lenka", "Zhang", "Pan" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Krzakala et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Krzakala et al\\.",
      "year" : 2013
    }, {
      "title" : "Error rate bounds and iterative weighted majority voting for crowdsourcing",
      "author" : [ "H. Li", "B. Yu" ],
      "venue" : "arXiv preprint arXiv:1411.4086,",
      "citeRegEx" : "Li and Yu,? \\Q2014\\E",
      "shortCiteRegEx" : "Li and Yu",
      "year" : 2014
    }, {
      "title" : "Error rate analysis of labeling by crowdsourcing",
      "author" : [ "H. Li", "B. Yu", "D. Zhou" ],
      "venue" : "In Proc. Machine Learning meets Crowdsourcing, Workshop at the Int?l Conference on Machine Learning (ICML-2013),",
      "citeRegEx" : "Li et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Variational inference for crowdsourcing",
      "author" : [ "Liu", "Qiang", "Peng", "Jian", "Ihler", "Alex T" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Liu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2012
    }, {
      "title" : "Information, physics, and computation",
      "author" : [ "Mezard", "Marc", "Montanari", "Andrea" ],
      "venue" : null,
      "citeRegEx" : "Mezard et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mezard et al\\.",
      "year" : 2009
    }, {
      "title" : "Get another label? improving data quality and data mining using multiple, noisy labelers",
      "author" : [ "Sheng", "Victor S", "Provost", "Foster", "Ipeirotis", "Panagiotis G" ],
      "venue" : "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Sheng et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sheng et al\\.",
      "year" : 2008
    }, {
      "title" : "Inferring ground truth from subjective labelling of venus images",
      "author" : [ "P. Smyth", "U. Fayyad", "M. Burl", "P. Perona", "P. Baldi" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Smyth et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Smyth et al\\.",
      "year" : 1995
    }, {
      "title" : "The multidimensional wisdom of crowds",
      "author" : [ "P. Welinder", "S. Branson", "S. Belongie", "P. Perona" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Welinder et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Welinder et al\\.",
      "year" : 2010
    }, {
      "title" : "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise",
      "author" : [ "J. Whitehill", "P. Ruvolo", "T. Wu", "J. Bergsma", "J. Movellan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Whitehill et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Whitehill et al\\.",
      "year" : 2009
    }, {
      "title" : "Spectral methods meet em: A provably optimal algorithm for crowdsourcing",
      "author" : [ "Y. Zhang", "X. Chen", "D. Zhou", "M.I. Jordan" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning from the wisdom of crowds by minimax entropy",
      "author" : [ "D. Zhou", "J. Platt", "S. Basu", "Y. Mao" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zhou et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2012
    }, {
      "title" : "Regularized minimax conditional entropy for crowdsourcing",
      "author" : [ "D. Zhou", "Q. Liu", "J.C. Platt", "C. Meek", "N.B. Shah" ],
      "venue" : "arXiv preprint arXiv:1503.07240,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "We model the worker responses using a recent generalization of the Dawid-Skene model introduced in Zhou et al. (2015), that models how different tasks can have different difficulties.",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "The original Dawid-Skene model introduced in Dawid & Skene (1979) and analyzed in Karger et al. (2014a) is a special case, when all tasks are equally easy, i.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "This idea was first introduced by Dawid and Skene (Dawid & Skene, 1979) who proposed an Expectation Maximization (EM) approach. Following this work, several approaches have been proposed to solve the inference problem under the Dawid-Skene model Smyth et al. (1995); Sheng et al.",
      "startOffset" : 34,
      "endOffset" : 266
    }, {
      "referenceID" : 2,
      "context" : "This idea was first introduced by Dawid and Skene (Dawid & Skene, 1979) who proposed an Expectation Maximization (EM) approach. Following this work, several approaches have been proposed to solve the inference problem under the Dawid-Skene model Smyth et al. (1995); Sheng et al. (2008); Karger et al.",
      "startOffset" : 34,
      "endOffset" : 287
    }, {
      "referenceID" : 2,
      "context" : "This idea was first introduced by Dawid and Skene (Dawid & Skene, 1979) who proposed an Expectation Maximization (EM) approach. Following this work, several approaches have been proposed to solve the inference problem under the Dawid-Skene model Smyth et al. (1995); Sheng et al. (2008); Karger et al. (2011); Liu et al.",
      "startOffset" : 34,
      "endOffset" : 309
    }, {
      "referenceID" : 2,
      "context" : "This idea was first introduced by Dawid and Skene (Dawid & Skene, 1979) who proposed an Expectation Maximization (EM) approach. Following this work, several approaches have been proposed to solve the inference problem under the Dawid-Skene model Smyth et al. (1995); Sheng et al. (2008); Karger et al. (2011); Liu et al. (2012a); Zhou et al.",
      "startOffset" : 34,
      "endOffset" : 329
    }, {
      "referenceID" : 2,
      "context" : "This idea was first introduced by Dawid and Skene (Dawid & Skene, 1979) who proposed an Expectation Maximization (EM) approach. Following this work, several approaches have been proposed to solve the inference problem under the Dawid-Skene model Smyth et al. (1995); Sheng et al. (2008); Karger et al. (2011); Liu et al. (2012a); Zhou et al. (2012); Li & Yu (2014); Zhang et al.",
      "startOffset" : 34,
      "endOffset" : 349
    }, {
      "referenceID" : 2,
      "context" : "This idea was first introduced by Dawid and Skene (Dawid & Skene, 1979) who proposed an Expectation Maximization (EM) approach. Following this work, several approaches have been proposed to solve the inference problem under the Dawid-Skene model Smyth et al. (1995); Sheng et al. (2008); Karger et al. (2011); Liu et al. (2012a); Zhou et al. (2012); Li & Yu (2014); Zhang et al.",
      "startOffset" : 34,
      "endOffset" : 365
    }, {
      "referenceID" : 2,
      "context" : "This idea was first introduced by Dawid and Skene (Dawid & Skene, 1979) who proposed an Expectation Maximization (EM) approach. Following this work, several approaches have been proposed to solve the inference problem under the Dawid-Skene model Smyth et al. (1995); Sheng et al. (2008); Karger et al. (2011); Liu et al. (2012a); Zhou et al. (2012); Li & Yu (2014); Zhang et al. (2014); Dalvi et al.",
      "startOffset" : 34,
      "endOffset" : 386
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al.",
      "startOffset" : 8,
      "endOffset" : 195
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance.",
      "startOffset" : 8,
      "endOffset" : 283
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem.",
      "startOffset" : 8,
      "endOffset" : 340
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem. However, in this dense regime, all tasks are assigned a growing number of workers as the problem size increases and eventually all tasks are labelled correctly with high probability. In this paper, we are interested in a more challenging setting where each task is assigned only a small number of workers. A spectral algorithm has been analyzed under the original Dawid-Skene model in Dalvi et al. (2013), and tighter analysis showed that the spectral approach is near-optimal in Karger et al.",
      "startOffset" : 8,
      "endOffset" : 866
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem. However, in this dense regime, all tasks are assigned a growing number of workers as the problem size increases and eventually all tasks are labelled correctly with high probability. In this paper, we are interested in a more challenging setting where each task is assigned only a small number of workers. A spectral algorithm has been analyzed under the original Dawid-Skene model in Dalvi et al. (2013), and tighter analysis showed that the spectral approach is near-optimal in Karger et al. (2011). A weighted majority voting algorithm has been analyzed in Li et al.",
      "startOffset" : 8,
      "endOffset" : 962
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem. However, in this dense regime, all tasks are assigned a growing number of workers as the problem size increases and eventually all tasks are labelled correctly with high probability. In this paper, we are interested in a more challenging setting where each task is assigned only a small number of workers. A spectral algorithm has been analyzed under the original Dawid-Skene model in Dalvi et al. (2013), and tighter analysis showed that the spectral approach is near-optimal in Karger et al. (2011). A weighted majority voting algorithm has been analyzed in Li et al. (2013), showing the dependence on the choice of the weights.",
      "startOffset" : 8,
      "endOffset" : 1038
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem. However, in this dense regime, all tasks are assigned a growing number of workers as the problem size increases and eventually all tasks are labelled correctly with high probability. In this paper, we are interested in a more challenging setting where each task is assigned only a small number of workers. A spectral algorithm has been analyzed under the original Dawid-Skene model in Dalvi et al. (2013), and tighter analysis showed that the spectral approach is near-optimal in Karger et al. (2011). A weighted majority voting algorithm has been analyzed in Li et al. (2013), showing the dependence on the choice of the weights. The original Dawid-Skene model, when all qi’s are either one or zero, several practical inference algorithms have been developed with strong performance guarantees. However, one of the main weakness of the model is that it does not capture how some tasks are more difficult than the others. To overcome this challenge, several practical models have been proposed recently Jin & Ghahramani (2003); Whitehill et al.",
      "startOffset" : 8,
      "endOffset" : 1488
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem. However, in this dense regime, all tasks are assigned a growing number of workers as the problem size increases and eventually all tasks are labelled correctly with high probability. In this paper, we are interested in a more challenging setting where each task is assigned only a small number of workers. A spectral algorithm has been analyzed under the original Dawid-Skene model in Dalvi et al. (2013), and tighter analysis showed that the spectral approach is near-optimal in Karger et al. (2011). A weighted majority voting algorithm has been analyzed in Li et al. (2013), showing the dependence on the choice of the weights. The original Dawid-Skene model, when all qi’s are either one or zero, several practical inference algorithms have been developed with strong performance guarantees. However, one of the main weakness of the model is that it does not capture how some tasks are more difficult than the others. To overcome this challenge, several practical models have been proposed recently Jin & Ghahramani (2003); Whitehill et al. (2009); Welinder et al.",
      "startOffset" : 8,
      "endOffset" : 1513
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem. However, in this dense regime, all tasks are assigned a growing number of workers as the problem size increases and eventually all tasks are labelled correctly with high probability. In this paper, we are interested in a more challenging setting where each task is assigned only a small number of workers. A spectral algorithm has been analyzed under the original Dawid-Skene model in Dalvi et al. (2013), and tighter analysis showed that the spectral approach is near-optimal in Karger et al. (2011). A weighted majority voting algorithm has been analyzed in Li et al. (2013), showing the dependence on the choice of the weights. The original Dawid-Skene model, when all qi’s are either one or zero, several practical inference algorithms have been developed with strong performance guarantees. However, one of the main weakness of the model is that it does not capture how some tasks are more difficult than the others. To overcome this challenge, several practical models have been proposed recently Jin & Ghahramani (2003); Whitehill et al. (2009); Welinder et al. (2010); Zhou et al.",
      "startOffset" : 8,
      "endOffset" : 1537
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem. However, in this dense regime, all tasks are assigned a growing number of workers as the problem size increases and eventually all tasks are labelled correctly with high probability. In this paper, we are interested in a more challenging setting where each task is assigned only a small number of workers. A spectral algorithm has been analyzed under the original Dawid-Skene model in Dalvi et al. (2013), and tighter analysis showed that the spectral approach is near-optimal in Karger et al. (2011). A weighted majority voting algorithm has been analyzed in Li et al. (2013), showing the dependence on the choice of the weights. The original Dawid-Skene model, when all qi’s are either one or zero, several practical inference algorithms have been developed with strong performance guarantees. However, one of the main weakness of the model is that it does not capture how some tasks are more difficult than the others. To overcome this challenge, several practical models have been proposed recently Jin & Ghahramani (2003); Whitehill et al. (2009); Welinder et al. (2010); Zhou et al. (2015). Although inference algorithms based on these more complex models significantly improve over those based on Dawid-Skene model on real-world datasets, there is no analysis on their performance.",
      "startOffset" : 8,
      "endOffset" : 1557
    }, {
      "referenceID" : 2,
      "context" : "(2014); Dalvi et al. (2013). In the theoretical side, the dense regime has been studied first, where all workers are assigned all tasks. Spectral method was first analyzed in Ghosh et al. (2011) and an EM approach followed by spectral initial step is analyzed in Zhang et al. (2014) to achieve a near-optimal performance. Gao & Zhou (2013) analyzed the MAP estimator, which is computationally intractable, and identified the minimax error rate of this problem. However, in this dense regime, all tasks are assigned a growing number of workers as the problem size increases and eventually all tasks are labelled correctly with high probability. In this paper, we are interested in a more challenging setting where each task is assigned only a small number of workers. A spectral algorithm has been analyzed under the original Dawid-Skene model in Dalvi et al. (2013), and tighter analysis showed that the spectral approach is near-optimal in Karger et al. (2011). A weighted majority voting algorithm has been analyzed in Li et al. (2013), showing the dependence on the choice of the weights. The original Dawid-Skene model, when all qi’s are either one or zero, several practical inference algorithms have been developed with strong performance guarantees. However, one of the main weakness of the model is that it does not capture how some tasks are more difficult than the others. To overcome this challenge, several practical models have been proposed recently Jin & Ghahramani (2003); Whitehill et al. (2009); Welinder et al. (2010); Zhou et al. (2015). Although inference algorithms based on these more complex models significantly improve over those based on Dawid-Skene model on real-world datasets, there is no analysis on their performance. Contributions. We bridge this gap between simple models with strong guarantees and complex models that represent real-world data. To the best of our knowledge, we provide the first analysis on a crowdsourcing model that captures the difficulties of the tasks, namely the generalized Dawid-Skene model introduced in Zhou et al. (2015). We propose an efficient iterative algorithm based on spectral methods, which does not require any hyper-parameters, only takes the crowdsourced data as input, and runs in time linear in the number of received answers up to a logarithmic factor.",
      "startOffset" : 8,
      "endOffset" : 2084
    }, {
      "referenceID" : 6,
      "context" : "1 Algorithm We propose using a state-of-the-art spectral method based on non-backtracking operators, first introduced for inference in Karger et al. (2011). A similar approach has been later applied to other inference problems, e.",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "1 Algorithm We propose using a state-of-the-art spectral method based on non-backtracking operators, first introduced for inference in Karger et al. (2011). A similar approach has been later applied to other inference problems, e.g. Krzakala et al. (2013); Bordenave et al.",
      "startOffset" : 135,
      "endOffset" : 256
    }, {
      "referenceID" : 1,
      "context" : "(2013); Bordenave et al. (2015). We compare the proposed iterative algorithm with other standard methods in Section 4, and show that typical approaches such as the EM or the belief propagation are computationally intractable.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "Also, typical random matrix analyses, such as those in Keshavan et al. (2009); Karger et al.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "(2009); Karger et al. (2013), shows that the spectral norm of the noise (A − E[A]) is bounded by C((l − 1)(r − 1)) with some constant C.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "We run synthetic experiments with m = n = 1000 and the crowds are generated from the spammer-hammer model where pj = 1 with probability β and 1/2 otherwise Karger et al. (2014b). We fix β = 0.",
      "startOffset" : 156,
      "endOffset" : 178
    }, {
      "referenceID" : 15,
      "context" : "Only when pj’s are continuous and qi’s are discrete Liu et al. (2012b) propose algorithms to compute the marginals using modified belief propagation and mean field method both.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "We use a result from Karger et al. (2014b) to show that the local neighborhood of a randomly chosen task node I is a tree with high probability.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "(15) The next lemma from Karger et al. (2014b) shows that the local subgraph is a tree with high probability as m grows.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "1 (Lemma 5 from Karger et al. (2014b)).",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "The proof technique is similar to the one introduced in Karger et al. (2014b). Precisely, we show,",
      "startOffset" : 56,
      "endOffset" : 78
    } ],
    "year" : 2016,
    "abstractText" : "Crowdsourcing systems provide scalable and cost-effective human-powered solutions at marginal cost, for classification tasks where humans are significantly better than the machines. Although traditional approaches in aggregating crowdsourced labels have relied on the Dawid-Skene model, this fails to capture how some tasks are inherently more difficult than the others. Several generalizations have been proposed, but inference becomes intractable and typical solutions resort to heuristics. To bridge this gap, we study a recently proposed generalize Dawid-Skene model, and propose a linear-time algorithm based on spectral methods. We show near-optimality of the proposed approach, by providing an upper bound on the error and comparing it to a fundamental limit. We provide numerical experiments on synthetic data matching our analyses, and also on real datasets demonstrating that the spectral method significantly improves over simple majority voting and is comparable to other methods.",
    "creator" : "gnuplot 5.0 patchlevel 1"
  }
}