{
  "name" : "1202.6583.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Lexical Analysis Tool with Ambiguity Support",
    "authors" : [ "Luis Quesada" ],
    "emails" : [ "lquesada@decsai.ugr.es,", "fberzal@decsai.ugr.es,", "cb@decsai.ugr.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 2.\n65 83\nv1 [\ncs .C\nL ]\n2 9\nFe b\n20 12"
    }, {
      "heading" : "A Lexical Analysis Tool with Ambiguity Support",
      "text" : "Luis Quesada, Fernando Berzal, and Francisco J. Cortijo\nDepartment of Computer Science and Artificial Intelligence, CITIC, University of Granada, Granada 18071, Spain\nlquesada@decsai.ugr.es, fberzal@decsai.ugr.es, cb@decsai.ugr.es\nLexical ambiguities naturally arise in languages. We present Lamb, a lexical analyzer that produces a lexical analysis graph describing all the possible sequences of tokens that can be found within the input string. Parsers can process such lexical analysis graphs and discard any sequence of tokens that does not produce a valid syntactic sentence, therefore performing, together with Lamb, a context-sensitive lexical analysis in lexically-ambiguous language specifications.\nI. INTRODUCTION\nA lexical analyzer, also called lexer or scanner, is a piece of software that processes an input string conforming to a language specification and produces a sequence of the tokens or terminal symbols found in it. The obtained sequence of tokens is then usually fed to a parser or syntactic analyzer as the next step of a data translation, compilation or interpretation procedure. Sometimes, lexical ambiguities may show up in a language specification. Lexical ambiguities occur when an input string simultaneously corresponds to several token sequences [9]. The traditional way of choosing a sequence amongst potential alternatives [6] involves assigning an unique priority to each token. This causes that, when the regular expressions associated to two different tokens match the same fragment of the input string, only the one with the greater priority will be considered. However, the language developer may want similar substrings to be recognized as different sequences of tokens depending on their context. This cannot be achieved with the unique priority approximation. Statistical lexical analyzers also exist [7]. Although statistical models may perform well in context-sensitive scenarios, they require intensive training and, as token types are actually guessed, they do not formally guarantee that the obtained token sequence will be what the developer intended. When it comes to programming languages, data specification languages, or limited natural languages scenarios, the syntactic rules are clear as to what should be accepted. The usage of statistical models introduces an unpredictable possibility of error during token recognition that would render scanning and parsing theoretically and pragmatically unfeasible. Our proposal, Lamb (standing for Lexical AMBiguity), performs a lexical analysis that efficiently captures all the possible sequences of tokens and generates a lexical analysis graph that describes them all. A subsequent parsing process discards any sequence of tokens that does not provide a valid syntactic sentence conforming to the syntactic rule set of the language specification. This solves the lexical ambiguity problem with formal correctness.\nTherefore, Lamb allows language developers to specify more complex languages than traditional techniques. Token priorities are still supported but their usage is optional. Several tokens may be set to share the same priority if the developer wants ambiguities involving them to be considered. As research in lexical analyzers sets the basis for the application of parsers, it inherits their application fields: the compilation or interpretation of source code written in programming languages [1], the interpretation and integration of data in data mining applications [4], and natural language processing [5]."
    }, {
      "heading" : "II. BACKGROUND",
      "text" : "The IEEE POSIX P1003.2 standard describes the requirements of the lex and yacc tools [6], which are a traditional lexical analyzer generator and a traditional syntactic analyzer generator, respectively. Implementations of these tools are typically used in conjunction:\n• Lex generates a lexer that takes as input a set of token types, associated regular expressions [12], and the string to be scanned; and produces the sequence of tokens found in the string.\n• Yacc generates a parser that takes as input the sequence of tokens and a syntactic rule set; and produces a parse tree.\nRegarding ambiguities, lex enforces the assignment of unique priorities to each token. Indeed, tokens are tried and matched in the very same order they have been specified. The order of efficiency of a lex -generated lexical analyzer is O(n), being n the input string length. The example lex specification in Figure 1 shows an example of implicitly reserved words, as the words “true”, “false”, “if”, or “while” will not be considered identifiers, because they will match BOOLEAN, IF, or WHILE tokens before reaching the regular expression for IDENTIFIER. Therefore, it is not possible for lex to consider these words as identifiers in some contexts, even if syntactic rules make clear whether occurrences of these words should be considered as identifiers or not.\n2 Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings. These models need intensive corpus-based training and they produce results with associated implicit probabilities. It should be noted that, even though they can perform well in natural language processing, their training requirement is impractical for programming or data representation languages, especially when the syntactic rules provide all the needed context information to unequivocally identify tokens. Furthermore, the results are prone to interpretation errors that would render the analysis unusable. The semi-syntactic lexical analyzer proposed in [11] brings some of the context information found in the syntactic rule set to the deterministic finite automaton that perform the lexical analysis. Although this technique considers context information found in syntactic rules, it is not able to capture syntactic ambiguities for its further consideration, since the minimal automaton needed to do this is a non-deterministic finite automaton, which would have increased complexity of the algorithm. Therefore, if the lexical ambiguities may cause syntactic ambiguities or, in other words, there are several syntactic interpretations of the input string due to lexical ambiguities, a Shyu-like lexer would be unable to find them."
    }, {
      "heading" : "III. LAMB",
      "text" : "In contrast to the aforementioned techniques, Lamb is able to recognize and capture lexical ambiguities. Our proposed algorithm takes as input the string to be scanned and a list of tokens associated to their corresponding regular expressions. It produces, as output, a lexical analysis graph, in which each token is connected to its following and preceding tokens in the input sequence. Our algorithm consists of two steps: the scanning step, which recognizes all the possible tokens in the input string; and the graph generation step, which computes the sets of preceding and following tokens for each token and builds the resulting lexical analysis graph."
    }, {
      "heading" : "A. The Scanning Step",
      "text" : "The pseudocode for the scanning step is shown in Figure 2. Our algorithm receives an input string called input and a list of matchers called matcherlist. Each matcher consists of a regular expression and its corresponding match method, a priority value, and a next value. The match method tries to perform a match given the input string and a starting position in it. The priority value specifies the matcher priority. The\nvalue 0 is reserved for ignored patterns, which are patterns that represent text that does not correspond to tokens. Then, priority values for relevant token start at 1, being the lower the value, the higher the priority. If two tokens share the same priority value, the lexer will capture both of them if they overlap due to lexical ambiguities. If two tokens have distinct priority values and they start at the same position in the input string, only the greater priority token will be considered. The next value specifies the position before the next string position a matcher will be tried at. It defaults to -1, so every matcher will be tried at the 0 position. The prio variable represents the last priority that has been matched in the current input position. Its value is -1 if no match has been made, 0 if an ignored element match has been made, and a higher value if any token of\n3 for i in tokenlist.size()-1..0:\nt = tokenlist[i] for j in i+1..tokenlist.size()-1:\ntc = tokenlist[j] if (tc.start > t.end &&\n(tc.prevstart==tc.start ||\n(tc.prevstart<tc.start &&\ntc.prevstart<t.end))):\nt.addfollowing(tc) tc.addpreceding(t) tc.prevstart = min(t.start,\ntc.prevstart)\nFigure 3 Pseudocode of the graph generation step in our lexical analysis algorithm.\nthat specific priority has been identified. Themin variable is computed in order to determine the next position the current matcher will be tried at, and its value is the minimum of either the ending position of the found token or the ending position of any tokens that end before it. This algorithm step has a theoretical order of efficiency of O(n2 · l), being n the input string length and l the number of matchers in the lexer."
    }, {
      "heading" : "B. The Graph Generation Step",
      "text" : "The algorithm pictured in Figure 3 goes through the identified token list in reverse order and efficiently computes the sets of preceding and following tokens for every token. The sets of preceding and following tokens of the token x are defined in Equation 1, being a, b, c tokens and xstart and xend the starting and ending positions of the token x in the input string.\nb ∈FOLLOWING(a), a ∈ PRECEDING(b) iif\naend < bstart & ∄c, cstart > aend, cend < bstart (1)\nThe prevstart variable in the pseudocode avoids the need of iterating through the token list to find out if there is any token between two particular tokens, because it represent the starting position values of preceding tokens, given a certain token. After the following and preceding sets have been computed for every token, any token whose preceding set is empty is added to the start token set of the lexical analysis graph. The graph generation has a theoretical order of efficiency of O(t2), being t the number of tokens found. As t ≤ n · l, the theoretical order of efficiency of this step is O(n2 · l2). Both scanning and graph generation steps together have an order of efficiency of O(n2 · l2).\ndo:\nflag = false for each rule r in rules:\nfor each token t in tokenlist:\nmatches = r.match(t) if matches.size() != 0:\nfor each match m in matches:\nif !tokenlist.contains(m):\ntokenlist.add(m) if m is start symbol\nstart.add(m)\nflag = true\nwhile flag = true\nFigure 4 Pseudocode of the proof of concept parser supporting ambiguities."
    }, {
      "heading" : "IV. COMPARISON",
      "text" : "In order to perform a formal comparison of traditional techniques and Lamb, we have implemented a simple (and inefficient) proof of concept parser that supports ambiguities and allows a lexical analysis guided by a syntactic rule set. This parser returns as many parse trees as they can be obtained by applying a set of syntactic rules to a lexical analysis graph. Its pseudocode is shown in Figure 4. It iteratively tries to match every rule starting from every existing token and following any possible token path, and it adds the newly found tokens to the list until no new tokens have been found in an iteration. Given a language specification that describes the tokens listed in Figure 10, the input string “&5.2& /25.20/” can correspond to the four different lexical analysis alternatives enumerated in Figure 11, depending on whether the sequences of digits separated by points are considered real numbers or integer numbers separated by points. The syntactic rules shown in Figure 12 illustrate a scenario of lexical ambiguity sensitivity. Depending on the surrounding tokens, which may be either Ampersand tokens or Slash tokens, the sequences of digits separated by points should be considered either Real tokens or Integer Point Integer token sequences. The expected results of analyzing the input string “&5.2& /25.20/” is shown in Figure 5. In order to resolve the ambiguities when using a lex - alike lexer, the developer can assign the Integer token a greater priority than the Real token. In that case, the only valid interpretation would be the one shown in Figure 6. The developer can also assign the Real token a greater priority than the Integer token. In that case, the only valid interpretation would be the one shown in Figure 7. Therefore, lex -alike lexers cannot produce the token sequence that is needed to parse strings that belong to our language with lexical ambiguities. Nonetheless, as Lamb is able to capture all the possible token sequences in the form of a lexical analysis graph, as shown in Figure 8, the later application of a parser\n5 (-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.[0-9]+ Real \\. Point \\/ Slash \\& Ampersand"
    }, {
      "heading" : "E ::= A B",
      "text" : ""
    }, {
      "heading" : "A ::= Ampersand Real Ampersand",
      "text" : ""
    }, {
      "heading" : "B ::= Slash Integer Point Integer Slash",
      "text" : "Figure 12 Context-sensitive syntactic rules to resolve lexical ambiguities.\nsupporting lexical ambiguities will produce the only possible valid sentence, which, in turn, is based on the only valid lexical analysis possible. Both of them are shown in Figure 9. Even though statistical models as Hidden Markov Models may produce correct results in similar situations, they cannot be used for this kind of language specifications, where the specification states how each token is to be recognized. Besides, their results may not be always accurate, which difficults formally proving their correctness in a well-defined setting."
    }, {
      "heading" : "V. CONCLUSIONS",
      "text" : "We have presented a lexical analyzer, Lamb, that supports lexical ambiguities. It performs a lexical analysis that efficiently captures all the possible sequences of tokens for lexically-ambiguous languages and it generates a lexical analysis graph that describes them all. Lamb supports assigning priorities to tokens as traditional techniques do but, in contrast to them, it does not enforce\nthese priorities to be set and it allows for priority values to be shared. Tokens with shared priorities are considered valid alternatives instead of mutually-exclusive options. The lexical graph can be then fed as input to a parser, which will discard any sequence of tokens that does not produce a valid syntactic analysis. In summary, our proposal performs a context-sensitive lexical analysis guided by syntactic rules and supports lexically-ambiguous language specifications."
    } ],
    "references" : [ {
      "title" : "Compilers: Principles, Techniques, and Tools",
      "author" : [ "Alfred V. Aho", "Monica S. Lam", "Ravi Sethi", "Jeffrey D. Ullman" ],
      "venue" : "Addison Wesley,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Hidden markov processes",
      "author" : [ "Yariv Ephraim", "Neri Merhav" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "The hierarchical hidden markov model: Analysis and applications",
      "author" : [ "Shai Fine", "Yoram Singer", "Naftali Tishby" ],
      "venue" : "Machile Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "Data Mining: Concepts and Techniques. The Morgan Kaufmann Series in Data Management Systems",
      "author" : [ "Jiawei Han", "Micheline Kamber", "Jian Pei" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition",
      "author" : [ "Daniel Jurafsky", "James H. Martin" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Extension of the limit theorems of probability theory to a sum of variables connected in a chain",
      "author" : [ "Andrey Andreyevich Markov" ],
      "venue" : "Dynamic Probabilistic Systems",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1971
    }, {
      "title" : "Maximum entropy markov models for information extraction and segmentation",
      "author" : [ "Andrew McCallum", "Dayne Freitag", "Fernando Pereira" ],
      "venue" : "In Proc. of the 17th International Conference on Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Conflict detection and resolution in a lexical analyzer generator",
      "author" : [ "J.R. Nawrocki" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1991
    }, {
      "title" : "A tutorial on hidden markov models and selected applications in speech recognition",
      "author" : [ "Lawrence R. Rabiner" ],
      "venue" : "In Proceedings of the IEEE,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1989
    }, {
      "title" : "From semi-syntactic lexical analyzer to a new compiler model",
      "author" : [ "Yuh-Huei Shyu" ],
      "venue" : "ACM SIGPLAN Notices,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1986
    }, {
      "title" : "Introduction to the Theory of Computation",
      "author" : [ "Michael Sipser" ],
      "venue" : "Course Technology,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Lexical ambiguities occur when an input string simultaneously corresponds to several token sequences [9].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "Statistical lexical analyzers also exist [7].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "As research in lexical analyzers sets the basis for the application of parsers, it inherits their application fields: the compilation or interpretation of source code written in programming languages [1], the interpretation and integration of data in data mining applications [4], and natural language processing [5].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "As research in lexical analyzers sets the basis for the application of parsers, it inherits their application fields: the compilation or interpretation of source code written in programming languages [1], the interpretation and integration of data in data mining applications [4], and natural language processing [5].",
      "startOffset" : 276,
      "endOffset" : 279
    }, {
      "referenceID" : 4,
      "context" : "As research in lexical analyzers sets the basis for the application of parsers, it inherits their application fields: the compilation or interpretation of source code written in programming languages [1], the interpretation and integration of data in data mining applications [4], and natural language processing [5].",
      "startOffset" : 313,
      "endOffset" : 316
    }, {
      "referenceID" : 10,
      "context" : "• Lex generates a lexer that takes as input a set of token types, associated regular expressions [12], and the string to be scanned; and produces the sequence of tokens found in the string.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.",
      "startOffset" : 43,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.",
      "startOffset" : 43,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.",
      "startOffset" : 43,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "The semi-syntactic lexical analyzer proposed in [11] brings some of the context information found in the syntactic rule set to the deterministic finite automaton that perform the lexical analysis.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 1,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 2,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 5,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 6,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 7,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 7,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.",
      "startOffset" : 29,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "[0-9]+ Real",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 1,
      "context" : "[0-9]+ Real",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 2,
      "context" : "[0-9]+ Real",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 3,
      "context" : "[0-9]+ Real",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 4,
      "context" : "[0-9]+ Real",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 5,
      "context" : "[0-9]+ Real",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 6,
      "context" : "[0-9]+ Real",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 7,
      "context" : "[0-9]+ Real",
      "startOffset" : 0,
      "endOffset" : 5
    } ],
    "year" : 2012,
    "abstractText" : "Lexical ambiguities naturally arise in languages. We present Lamb, a lexical analyzer that produces a lexical analysis graph describing all the possible sequences of tokens that can be found within the input string. Parsers can process such lexical analysis graphs and discard any sequence of tokens that does not produce a valid syntactic sentence, therefore performing, together with Lamb, a context-sensitive lexical analysis in lexically-ambiguous language specifications.",
    "creator" : "LaTeX with hyperref package"
  }
}