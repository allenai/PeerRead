{
  "name" : "1512.04455.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Memory-based control with recurrent neural networks",
    "authors" : [ "Nicolas Heess", "Jonathan J Hunt", "Timothy P Lillicrap", "David Silver" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Partially observed control problems are a challenging aspect of reinforcement learning. We extend two related, model-free algorithms for continuous control – deterministic policy gradient and stochastic value gradient – to solve partially observed domains using recurrent neural networks trained with backpropagation through time. We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements. These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps. We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task. Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels. We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies."
    }, {
      "heading" : "1 Introduction",
      "text" : "The use of neural networks for solving continuous control problems has a long tradition. Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12]. However, all of this work assumes fully observed state.\nMany real world control problems are partially observed. Partial observability can arise from different sources including the need to remember information that is only temporarily available such as a way sign in a navigation task, sensor limitations or noise, unobserved variations of the plant under control (system identification), or state-aliasing due to function approximation. Partial observability also arises naturally in many tasks that involve control from vision: a static image of a dynamic scene provides no information about velocities, occlusions occur as a consequence of the three-dimensional nature of the world, and most vision sensors are bandwidth-limited and only have a restricted field-of-view.\nResolution of partial observability is non-trivial. Existing methods can roughly be divided into two broad classes:\nOn the one hand there are approaches that explicitly maintain a belief state that corresponds to the distribution over the world state given the observations so far. This approach has two major disadvantages: The first is the need for a model, and the second is the computational cost that is typically associated with the update of the belief state [8, 23].\nar X\niv :1\n51 2.\n04 45\n5v 1\n[ cs\n.L G\n] 1\n4 D\nec 2\nOn the other hand there are model free approaches that learn to form memories based on interactions with the world. This is challenging since it is a priori unknown which features of the observations will be relevant later, and associations may have to be formed over many steps. For this reason, most model free approaches tend to assume the fully-observed case. In practice, partial observability is often solved by hand-crafting a solution such as providing multiple-frames at each timestep to allow velocity estimation [16, 14].\nIn this work we investigate a natural extension of two recent, closely related policy gradient algorithms for learning continuous-action policies to handle partially observed problems. We primarily consider the Deterministic Policy Gradient algorithm (DPG) [24], which is an off-policy policy gradient algorithm that has recently produced promising results on a broad range of difficult, highdimensional continuous control problems, including direct control from pixels [14]. DPG is an actor-critic algorithm that uses a learned approximation of the action-value (Q) function to obtain approximate action-value gradients. These are then used to update a deterministic policy via the chain-rule. We also consider DPG’s stochastic counterpart, SVG(0) ([6]; SVG stands for “Stochastic Value Gradients”) which similarly updates the policy via backpropagation of action-value gradients from an action-value critic but learns a stochastic policy.\nWe modify both algorithms to use recurrent networks trained with backpropagation through time. We demonstrate that the resulting algorithms, Recurrent DPG (RDPG) and Recurrent SVG(0) (RSVG(0)), can be applied to a number of partially observed physical control problems with diverse memory requirements. These problems include: short-term integration of sensor information to estimate the system state (pendulum and cartpole swing-up tasks without velocity information); system identification (cart pole swing-up with variable and unknown pole-length); long-term memory (a robot arm that needs to reach out and grab a payload to move it to the position the arm started from); as well as a simplified version of the water maze task which requires the agent to learn an exploration strategy to find a hidden platform and then remember the platform’s position in order to return to it subsequently. We also demonstrate successful control directly from pixels.\nOur results suggest that actor-critic algorithms that rely on bootstrapping for estimating the value function can be a viable option for learning control policies in partially observed domains. We further find that, at least in the setup considered here, there is little performance difference between stochastic and deterministic policies, despite the former being typically presumed to be preferable in partially observed domains."
    }, {
      "heading" : "2 Background",
      "text" : "We model our environment as discrete-time, partially-observed Markov Decision process (POMDP). A POMDP is described a set of environment states S and a set of actions A, an initial state distribution p0(s0), a transition function p(st+1|st, at) and reward function r(st, at). This underlying MDP is partially observed when the agent is unable to observe the state st directly and instead receives observations from the set O which are conditioned on the underlying state p(ot|st). The agent only indirectly observes the underlying state of the MDP through the observations. An optimal agent may, in principle, require access to the entire history ht = (o1, a1, o2, a2, ...at−1, ot).\nThe goal of the agent is thus to learn a policy π(ht) which maps from the history to a distribution over actions P (A) which maximizes the expected discounted reward (below we consider both stochastic and deterministic policies). For stochastic policies we want to maximise\nJ = Eτ [ ∞∑ t=1 γt−1r(st, at) ] , (1)\nwhere the trajectories τ = (s1, o1, a1, s2, . . . ) are drawn from the trajectory distribution induced by the policy π: p(s1)p(o1|s1)π(a1|h1)p(s2|s1, a1)p(o2|s2)π(a2|h2) . . . and where ht is defined as above. For deterministic policies we replace π with a deterministic function µ which maps directly from states S to actions A and we replace at ∼ π(·|ht) with at = µ(ht). In the algorithms below we make use of the action-value function Qπ . For a fully observed MDP, when we have access to s, the action-value function is defined as the expected future discounted reward when in state st the agent takes action at and thereafter follows policy π. Since we are\ninterested in the partially observed case where the agent does not have access to s we instead define Qπ in terms of h:\nQπ(ht, at) = Est|ht [rt(st, at)] + Eτ>t|ht,at [ ∞∑ i=1 γir(st+i, at+i) ] (2)\nwhere τ>t = (st+1, ot+1, at+1 . . . ) is the future trajectory and the two expectations are taken with respect to the conditionals p(st|ht) and p(τ>t|ht, at) of the trajectory distribution associated with π. Note that this equivalent to defining Qπ in terms of the belief state since h is a sufficient statistic.\nObviously, for most POMDPs of interest, it is not tractable to condition on the entire sequence of observations. A central challenge is to learn how to summarize the past in a scalable way."
    }, {
      "heading" : "3 Algorithms",
      "text" : ""
    }, {
      "heading" : "3.1 Recurrent DPG",
      "text" : "We extend the Deterministic Policy Gradient (DPG) algorithm for MDPs introduced in [24] to deal with partially observed domains and pixels. The core idea of the DPG algorithm for the fully observed case is that for a deterministic policy µθ with parameters θ, and given access to the true action-value function associated with the current policy Qµ, the policy can be updated by backpropagation:\n∂J(θ)\n∂θ = Es∼ρµ\n[ ∂Qµ(s, a)\n∂a\n∣∣∣∣ a=µθ(s) ∂µθ(s) ∂θ ] , (3)\nwhere the expectation is taken with respect to the (discounted) state visitation distribution ρµ induced by the current policy µθ [24]. Similar ideas had previously been exploited in NFQCA [4] and in the ADP [13] community. In practice the exact action-value function Qµ is replaced by an approximate (critic) Qω with parameters ω that is differentiable in a and which can be learned e.g. with Qlearning.\nIn order to ensure the applicability of our approach to large observation spaces (e.g. from pixels), we use neural networks for all function approximators. These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16]. [14] proposed modifications to DPG necessary in order to learn effectively with deep neural networks which we make use of here (cf. sections 3.1.1, 3.1.2).\nUnder partial observability the optimal policy and the associated action-value function are both functions of the entire preceding observation-action history ht. The primary change we introduce is the use of recurrent neural networks, rather than feedforward networks, in order to allow the network to learn to preserve (limited) information about the past which is needed in order to solve the POMDP. Thus, writing µ(h) and Q(h, a) rather than µ(s) and Q(s, a) we obtain the following policy update:\n∂J(θ)\n∂θ = Eτ [∑ t γt−1 ∂Qµ(ht, a) ∂a ∣∣∣∣ a=µθ(ht) ∂µθ(ht) ∂θ ] , (4)\nwhere we have written the expectation now explicitly over entire trajectories τ = (s1, o1, a1, s2, o2, a2, . . . ) which are drawn from the trajectory distribution induced by the current policy and ht = (o1, a1, . . . , ot−1, at−1, ot) is the observation-action trajectory prefix at time step t, both as introduced above1. In practice, as in the fully observed case, we replace Qµ by learned approximation Qω (which is also a recurrent network with parameters ω). Thus, rather than directly conditioning on the entire observation history, we effectively train recurrent neural networks to summarize this history in their recurrent state using backpropagation through time (BPTT). For\n1 A discount factor γt appears implicitly in the update which is absorbed in the discounted state-visitation distribution in eq. 3. In practice we ignore this term as is often done in policy gradient implementations in practice (e.g. [26]).\nlong episodes or continuing tasks it is possible to use truncated BPTT, although we do not use this here.\nThe full algorithm is given below (Algorithm 1).\nRDPG is an algorithm for learning deterministic policies. As discussed in the literature [25, 20] it is possible to construct examples where deterministic policies perform poorly under partial observability. In RDPG the policy is conditioned on the entire history but since we are using function approximation state aliasing may still occur, especially early in learning. We therefore also investigate a recurrent version of the stochastic counterpart to DPG: SVG(0) [6] (DPG can be seen as the deterministic limit of SVG(0)). In addition to learning stochastic policies SVG(0) also admits on-policy learning whereas DPG is inherently off policy (see below).\nSimilar to DPG, SVG(0) updates the policy by backpropagation ∂Q/∂a from the action-value function, but does so for stochastic policies. This is enabled through a “re-parameterization” (e.g. [10, 19]) of the stochastic policy: The stochastic policy is represented in terms of a fixed, independent noise source and a parameterized deterministic function that transforms a draw from that noise source, i.e., in our case, a = πθ(h, ν) with ν ∼ β(·) where β is some fixed distribution. For instance, a Gaussian policy πθ(a|h) = N(a|µθ(h), σ2) can be re-parameterized as follows: a = πθ(h, ν) = µθ(h) + σν where ν ∼ N(·|0, 1). See [6] for more details. The stochastic policy is updated as follows:\n∂J(θ)\n∂θ = Eτ,ν ∑ t γt−1 ∂Qπ θ (ht, a) ∂a ∣∣∣∣∣ a=πθ(ht,νt) ∂πθ(ht, νt) ∂θ  , (5) with τ drawn from the trajectory distribution which is conditioned on IID draws of νt from β at each time step. The full algorithm is provided in the supplementary (Algorithm 2)."
    }, {
      "heading" : "3.1.1 Off-policy learning and experience replay",
      "text" : "DPG is typically used in an off-policy setting due to the fact that the policy is deterministic but exploration is needed in order to learn the gradient of Q with respect to the actions. Furthermore, in practice, data efficiency and stability can also be greatly improved by using experience replay (e.g. [4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2). Thus, during learning we store experienced trajectories in a database and then replace the expectation in eq. (4) with trajectories sampled from the database.\nOne consequence of this is a bias in the state distribution in eqs. (3, 5) which no longer corresponds to the state distribution induced by the current policy . With function approximation this can lead to a bias in the learned policy, although this typically ignored in practice. RDPG and RSVG(0) may similarly be affected; in fact since policies (and Q) are not just a function of the state but of an entire action-observation history (eq. 4) the bias might be more severe.\nOne potential advantage of (R)SVG(0) in this context is that it allows on-policy learning although we do not explore this possibility here. We found that off-policy learning with experience replay remained effective in the partially observed case."
    }, {
      "heading" : "3.1.2 Target networks",
      "text" : "A second algorithmic feature that has been found to greatly improve the stability of neural-network based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Q and of the policy π each, with parameters θ and θ′, and ω and ω′ respectively. θ and ω are the parameters that are being updated by the algorithm; θ′ and ω′ track them with some delay and are used to compute the “targets values” for the Q function update. Different authors have explored different approaches to updating θ′ and ω′. In this work we use “soft updates” as in [14] (see Algorithms 1 and 2 below).\nAlgorithm 1 RDPG algorithm\nInitialize critic network Qω(at, ht) and actor µθ(ht) with parameters ω and θ. Initialize target networks Qω ′ and µθ ′ with weights ω′ ← ω, θ′ ← θ. Initialize replay buffer R. for episodes = 1, M do\ninitialize empty history h0 for t = 1, T do\nreceive observation ot ht ← ht−1, at−1, ot (append observation and previous action to history) select action at = µθ(ht) + (with : exploration noise)\nend for Store the sequence (o1, a1, r1...oT , aT , rT ) in R Sample a minibatch of N episodes (oi1, a i 1, r i 1, ...o i T , a i T , r i T )i=1,...,N from R Construct histories hit = (o i 1, a i 1, . . . a i t−1, o i t) Compute target values for each sample episode (yi1, ...y i T ) using the recurrent target networks\nyit = r i t + γQ ω′(hit+1, µ θ′(hit+1))\nCompute critic update (using BPTT)\n∆ω = 1\nNT ∑ i ∑ t ( yit −Qω(hit, ait) ) ∂Qω(hit, ait) ∂ω\nCompute actor update (using BPTT)\n∆θ = 1\nNT ∑ i ∑ t ∂Qω(hit, µ θ(hit)) ∂a ∂µθ(hit) ∂θ\nUpdate actor and critic using Adam [9] Update the target networks\nω′ ← τω + (1− τ)ω′\nθ′ ← τθ + (1− τ)θ′\nend for"
    }, {
      "heading" : "4 Results",
      "text" : "We tested our algorithms on a variety of partial-observed environments, covering different types of memory problems. Videos of the learned policies for all the domains are included in our supplementary videos2, we encourage viewing them as these may provide a better intuition for the environments. All physical control problems except the simulated water maze (section 4.3) were simulated in MuJoCo [28]. We tested both standard recurrent networks as well as LSTM networks."
    }, {
      "heading" : "4.1 Sensor integration and system identification",
      "text" : "Physical control problems with noisy sensors are one of the paradigm examples of partially-observed environments. A large amount of research has focused on how to efficiently integrate noisy sensory information over multiple timesteps in order to derive accurate estimates of the system state, or to estimate derivatives of important properties of the system [27].\nHere, we consider two simple, standard control problems often used in reinforcement learning, the under-actuated pendulum and cartpole swing up. We modify these standard benchmarks tasks such that in both cases the agent receives no direct information of the velocity of any of the components, i.e. for the pendulum swing-up task the observation comprises only the angle of the pendulum, and\n2Video of all the learned policies is available at https://youtu.be/V4_vb1D5NNQ\nfor cartpole swing-up it is limited to the angle of the pole and the position of the cart. Velocity is crucial for solving the task and thus it must be estimated from the history of the system. Figure 1a shows the learning curves for pendulum swing-up. Both RDPG and RSVG0 were tested on the pendulum task, and are able to learn good solutions which bring the pole to upright.\nFor the cartpole swing-up task, in addition to not providing the agent with velocity information, we also varied the length of the pole from episode to episode. The pole length is invisible to the agent and needs to be inferred from the response of the system. In this task the sensor integration problem is thus paired with the need for system identification. As can be seen in figure 1b, the RDPG agent with an LSTM network reliably solves this task every time while a simple feedforward agent (DDPG) fails entirely. RDPG with a simple RNN performs considerably less well than the LSTM agent, presumably due to relatively long episodes (T=350 steps) and the failure to backpropagate gradients effectively through the plain RNN. We found that a feedforward agent that does receive velocity information can solve the variable-length swing-up task partly but does so less reliably than the recurrent agent as it is unable to identify the relevant system parameters (not shown)."
    }, {
      "heading" : "4.2 Memory tasks",
      "text" : "Another type of partially-observed task, which has been less studied in the context of reinforcement learning, involves the need to remember explicit information over a number of steps. We constructed two tasks like this. One was a 3-joint reacher which must reach for a randomly positioned target, but the position of the target is only provided to the agent in the initial observation (the entire episode is 80 timesteps). As a harder variant of this task, we constructed a 5 joint gripper which must reach for a (fully-observed) payload from a randomized initial configuration and then return the payload to the initial position of its ”hand” (T=100). Note that this is a challenging control problem even in the fully observed case. The results for both tasks are shown in figure 2, RDPG agents with LSTM networks solve both tasks reliably whereas purely feedforward agents fail on the memory components of the task as can be seen in the supplemental video."
    }, {
      "heading" : "4.3 Water maze",
      "text" : "The Morris water maze has been used extensively in rodents for the study of memory [3]. We tested our algorithms on a simplified version of the task. The agent moves in a 2-dimensional circular space where a small region of the space is an invisible “platform” where the agent receives a positive reward. At the beginning of the episode the agent and platform are randomly positioned in the tank. The platform position is not visible to the agent but it “sees” when it is on platform. The agent needs to search for and stay on the platform to receive reward by controlling its acceleration. After 5 steps on the platform the agent is reset randomly to a new position in the tank but the platform stays in place for the rest of the episode (T=200). The agent needs to remember the position of the platform to return to it quickly.\nIt is sometimes presumed that a stochastic policy is required in order to solve problems like this, which require learning a search strategy. Although there is some variability in the results, we found that both RDPG and RSVG(0) were able to find similarly good solutions (figure 3a), indicating RDPG is able to learn reasonable, deterministic search strategies. Both solutions were able to make use of memory to return to the platform more quickly after discovering it during the initial search (figure 3b). A non-recurrent agent (DDPG) is able to learn a limited search strategy but fails to exploit memory to return the platform after having been reset to a random position in the tank."
    }, {
      "heading" : "4.4 High-dimensional observations",
      "text" : "We also tested our agents, with convolutional networks, on solving tasks directly from highdimensional pixel spaces. We tested on the pendulum task (but now the agent is given only a static rendering of the pendulum at each timestep), and a two-choice reaching task, where the target disappears after 5 frames (and the agent is not allowed to move during the first 5 frames to prevent it from encoding the target position in its initial trajectory).\nWe found that RDPG was able to learn effective policies from high-dimensional observations which integrate information from multiple timesteps to estimate velocity and remember the visually queued target for the full length of the episode (in the reacher task). Figure 4 shows the results."
    }, {
      "heading" : "5 Discussion",
      "text" : ""
    }, {
      "heading" : "5.1 Variants",
      "text" : "In the experiments presented here, the actor and critic networks are entirely disjoint. However, particularly when learning deep, convolutional networks the filters required in the early layers may be similar between the policy and the actor. Sharing these early layers could improve computational efficiency and learning speed. Similar arguments apply to the recurrent part of the network, which could be shared between the actor and the critic. Such sharing, however, can also result in instabilities as updates to one network may unknowingly damage or shift the other network. For this reason, we have not used any sharing here, although it is a potential topic for further investigation."
    }, {
      "heading" : "5.2 Related work",
      "text" : "There is a large body of literature on solving partially observed control problems. We focus on the most closely related work that aims to solve such problems with learned memory.\nSeveral groups [15, 1, 5] have studied the use of model-free algorithms with recurrent networks to solve POMDPs with discrete action spaces. [1] focused on relatively long-horizon (”deep”) memory problems in small state-action spaces. In contrast, [5] modified the Atari DQN architecture [16] (i.e. they perform control from high-dimensional pixel inputs) and demonstrated that recurrent Q learning [15] can perform the required information integration to resolve short-term partial observability (e.g. to estimate velocities) that is achieved via stacks of frames in the original DQN architecture.\nContinuous action problems with relatively low-dimensional observation spaces have been considered e.g. in [30, 31, 29, 32]. [30] trained LSTM-based stochastic policies using Reinforce; [31, 29, 32] used actor-critic architectures. The algorithm of [31] can be seen as a special case of DPG where the deterministic policy produces the parameters of an action distribution from which the actions are then sampled. This requires suitable exploration at the level of distribution parameters (e.g. exploring in terms of means and variances of a Gaussian distribution); in contrast, SVG(0) also learns stochastic policies but allows exploration at the action level only.\nAll works mentioned above, except for [32], consider the memory to be internal to the policy and learn the RNN parameters using BPTT, back-propagating either TD errors or policy gradients. [32] instead take the view of [17] and consider memory as extra state dimensions that can can be read and set by the policy. They optimize the policy using guided policy search [12] which performs explicit trajectory optimization along reference trajectories and, unlike our approach, requires a well defined full latent state and access to this latent state during training."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have demonstrated that two related model-free approaches can be extended to learn effectively with recurrent neural networks on a variety of partially-observed problems, including directly from pixel observations. Since these algorithms learn using standard backpropagation through time, we\nare able to benefit from innovations in supervised recurrent neural networks, such as long-short term memory networks [7], to solve challenging memory problems such as the Morris water maze."
    }, {
      "heading" : "7 Supplementary",
      "text" : "Algorithm 2 RSVG(0) algorithm\nInitialize critic network Qω(at, ht) and actor πθ(ht) with parameters ω and θ. Initialize target networks Qω ′ and πθ ′ with weights ω′ ← ω, θ′ ← θ. Initialize replay buffer R. for episodes = 1, M do\ninitialize empty history h0 for t = 1, T do\nreceive observation ot ht ← ht−1, at−1, ot (append observation and previous action to history) select action at = πθ(ht, ν) with ν ∼ β)\nend for Store the sequence (o1, a1, r1...oT , aT , rT ) in R Sample a minibatch of N episodes (oi1, a i 1, r i 1, ...o i T , a i T , r i T )i=1,...,N from R Construct histories hit = (o i 1, a i 1, . . . a i t−1, o i t) Compute target values for each sample episode (yi1, ...y i T ) using the recurrent target networks\nyit = r i t + γQ ω′(hit+1, π θ′(hit+1, ν)) with ν ∼ β\nCompute critic update (using BPTT)\n∆ω = 1\nNT ∑ i ∑ t (yit −Qω(hit, ait) ∂Qω(hit, a i t) ∂ω\nCompute actor update (using BPTT)\n∆θ = 1\nNT ∑ i ∑ t ∂Qω(hit, π θ(hit, ν)) ∂a ∂πθ(hit, ν) ∂θ with ν ∼ β\nUpdate actor and critic using Adam [9] Update the target networks\nω′ ← τω + (1− τ)ω′\nθ′ ← τθ + (1− τ)θ′\nend for"
    } ],
    "references" : [ {
      "title" : "Reinforcement learning with long short-term memory",
      "author" : [ "B. Bakker" ],
      "venue" : "NIPS",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Compatible value gradients for reinforcement learning of continuous deep policies",
      "author" : [ "D. Balduzzi", "M. Ghifary" ],
      "venue" : "arXiv preprint arXiv:1509.03005",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Applications of the morris water maze in the study of learning and memory",
      "author" : [ "R. DHooge", "P.P. De Deyn" ],
      "venue" : "Brain research reviews, 36(1):60–90",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Reinforcement learning in feedback control",
      "author" : [ "R. Hafner", "M. Riedmiller" ],
      "venue" : "Machine learning, 84(1-2):137–169",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Deep recurrent q-learning for partially observable mdps",
      "author" : [ "M. Hausknecht", "P. Stone" ],
      "venue" : "arXiv preprint arXiv:1507.06527",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning continuous control policies by stochastic value gradients",
      "author" : [ "N. Heess", "G. Wayne", "D. Silver", "T. Lillicrap", "T. Erez", "Y. Tassa" ],
      "venue" : "NIPS",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, 9(8):1735– 1780",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Planning and acting in partially observable stochastic domains",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra" ],
      "venue" : "Artificial intelligence, 101(1):99–134",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "CoRR, abs/1312.6114",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "S. Levine", "C. Finn", "T. Darrell", "P. Abbeel" ],
      "venue" : "arXiv preprint arXiv:1504.00702",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement learning and adaptive dynamic programming for feedback control",
      "author" : [ "F.L. Lewis", "D. Vrabie" ],
      "venue" : "Circuits and Systems Magazine, IEEE, 9(3):32–50",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra" ],
      "venue" : "arXiv preprint arXiv:1509.02971",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement learning with hidden states",
      "author" : [ "L.-J. Lin", "T.M. Mitchell" ],
      "venue" : "J.-A. Meyer, H. L. Roitblat, and S. W. Wilson, editors, From animals to animats 2, pages 271–280. MIT Press, Cambridge, MA, USA",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "et al",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski" ],
      "venue" : "Human-level control through deep reinforcement learning. Nature, 518(7540):529–533",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning policies with external memory",
      "author" : [ "L. Peshkin", "N. Meuleau", "L.P. Kaelbling" ],
      "venue" : "ICML",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Cnn features off-the-shelf: an astounding baseline for recognition",
      "author" : [ "A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson" ],
      "venue" : "Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on, pages 512–519. IEEE",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "D.J. Rezende", "S. Mohamed", "D. Wierstra" ],
      "venue" : "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 1278–1286",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Reinforcement learning for factored markov decision processes",
      "author" : [ "B. Sallans" ],
      "venue" : "PhD thesis, Citeseer",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz" ],
      "venue" : "ICML",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "High-dimensional continuous control using generalized advantage estimation",
      "author" : [ "J. Schulman", "P. Moritz", "S. Levine", "M.I. Jordan", "P. Abbeel" ],
      "venue" : "CoRR, abs/1506.02438",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A survey of point-based pomdp solvers",
      "author" : [ "G. Shani", "J. Pineau", "R. Kaplow" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems, 27(1):1–51",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deterministic policy gradient algorithms",
      "author" : [ "D. Silver", "G. Lever", "N. Heess", "T. Degris", "D. Wierstra", "M. Riedmiller" ],
      "venue" : "ICML",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning without state-estimation in partially observable markovian decision processes",
      "author" : [ "S.P. Singh" ],
      "venue" : "ICML",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Bias in natural actor-critic algorithms",
      "author" : [ "P. Thomas" ],
      "venue" : "Proceedings of The 31st International Conference on Machine Learning, pages 441–448",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Probabilistic robotics",
      "author" : [ "S. Thrun", "W. Burgard", "D. Fox" ],
      "venue" : "MIT press",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Mujoco: A physics engine for model-based control",
      "author" : [ "E. Todorov", "T. Erez", "Y. Tassa" ],
      "venue" : "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026–5033. IEEE",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Contextual behaviors and internal representations acquired by reinforcement learning with a recurrent neural network in a continuous state and action space task",
      "author" : [ "H. Utsunomiya", "K. Shibata" ],
      "venue" : "M. Kppen, N. Kasabov, and G. Coghill, editors, Advances in Neuro-Information Processing, volume 5507 of Lecture Notes in Computer Science, pages 970–978. Springer Berlin Heidelberg",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Solving deep memory pomdps with recurrent policy gradients",
      "author" : [ "D. Wierstra", "A. Förster", "J. Peters", "J. Schmidhuber" ],
      "venue" : "ICANN",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Policy gradient critics",
      "author" : [ "D. Wierstra", "J. Schmidhuber" ],
      "venue" : "ECML",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Policy learning with continuous memory states for partially observed robotic control",
      "author" : [ "M. Zhang", "S. Levine", "Z. McCarthy", "C. Finn", "P. Abbeel" ],
      "venue" : "CoRR, abs/1507.01273",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].",
      "startOffset" : 206,
      "endOffset" : 228
    }, {
      "referenceID" : 5,
      "context" : "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].",
      "startOffset" : 206,
      "endOffset" : 228
    }, {
      "referenceID" : 13,
      "context" : "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].",
      "startOffset" : 206,
      "endOffset" : 228
    }, {
      "referenceID" : 20,
      "context" : "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].",
      "startOffset" : 206,
      "endOffset" : 228
    }, {
      "referenceID" : 21,
      "context" : "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].",
      "startOffset" : 206,
      "endOffset" : 228
    }, {
      "referenceID" : 11,
      "context" : "Several recent papers successfully apply model-free, direct policy search methods to the problem of learning neural network control policies for challenging continuous domains with many degrees of freedoms [2, 6, 14, 21, 22, 12].",
      "startOffset" : 206,
      "endOffset" : 228
    }, {
      "referenceID" : 7,
      "context" : "This approach has two major disadvantages: The first is the need for a model, and the second is the computational cost that is typically associated with the update of the belief state [8, 23].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 22,
      "context" : "This approach has two major disadvantages: The first is the need for a model, and the second is the computational cost that is typically associated with the update of the belief state [8, 23].",
      "startOffset" : 184,
      "endOffset" : 191
    }, {
      "referenceID" : 15,
      "context" : "In practice, partial observability is often solved by hand-crafting a solution such as providing multiple-frames at each timestep to allow velocity estimation [16, 14].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : "In practice, partial observability is often solved by hand-crafting a solution such as providing multiple-frames at each timestep to allow velocity estimation [16, 14].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 23,
      "context" : "We primarily consider the Deterministic Policy Gradient algorithm (DPG) [24], which is an off-policy policy gradient algorithm that has recently produced promising results on a broad range of difficult, highdimensional continuous control problems, including direct control from pixels [14].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "We primarily consider the Deterministic Policy Gradient algorithm (DPG) [24], which is an off-policy policy gradient algorithm that has recently produced promising results on a broad range of difficult, highdimensional continuous control problems, including direct control from pixels [14].",
      "startOffset" : 285,
      "endOffset" : 289
    }, {
      "referenceID" : 5,
      "context" : "We also consider DPG’s stochastic counterpart, SVG(0) ([6]; SVG stands for “Stochastic Value Gradients”) which similarly updates the policy via backpropagation of action-value gradients from an action-value critic but learns a stochastic policy.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : "We extend the Deterministic Policy Gradient (DPG) algorithm for MDPs introduced in [24] to deal with partially observed domains and pixels.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "where the expectation is taken with respect to the (discounted) state visitation distribution ρ induced by the current policy μ [24].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "Similar ideas had previously been exploited in NFQCA [4] and in the ADP [13] community.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "Similar ideas had previously been exploited in NFQCA [4] and in the ADP [13] community.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 13,
      "context" : "These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16].",
      "startOffset" : 202,
      "endOffset" : 210
    }, {
      "referenceID" : 15,
      "context" : "These networks, with convolutional layers have proven effective at many sensory processing tasks [11, 18], and been demonstrated to be effective for scaling reinforcement learning to large state spaces [14, 16].",
      "startOffset" : 202,
      "endOffset" : 210
    }, {
      "referenceID" : 13,
      "context" : "[14] proposed modifications to DPG necessary in order to learn effectively with deep neural networks which we make use of here (cf.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "As discussed in the literature [25, 20] it is possible to construct examples where deterministic policies perform poorly under partial observability.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "As discussed in the literature [25, 20] it is possible to construct examples where deterministic policies perform poorly under partial observability.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "We therefore also investigate a recurrent version of the stochastic counterpart to DPG: SVG(0) [6] (DPG can be seen as the deterministic limit of SVG(0)).",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "[10, 19]) of the stochastic policy: The stochastic policy is represented in terms of a fixed, independent noise source and a parameterized deterministic function that transforms a draw from that noise source, i.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 18,
      "context" : "[10, 19]) of the stochastic policy: The stochastic policy is represented in terms of a fixed, independent noise source and a parameterized deterministic function that transforms a draw from that noise source, i.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "See [6] for more details.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 13,
      "context" : "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 15,
      "context" : "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "[4, 5, 14, 16, 6]) and we use the same approach here (see Algorithms 1, 2).",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "A second algorithmic feature that has been found to greatly improve the stability of neural-network based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Q and of the policy π each, with parameters θ and θ′, and ω and ω′ respectively.",
      "startOffset" : 226,
      "endOffset" : 240
    }, {
      "referenceID" : 13,
      "context" : "A second algorithmic feature that has been found to greatly improve the stability of neural-network based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Q and of the policy π each, with parameters θ and θ′, and ω and ω′ respectively.",
      "startOffset" : 226,
      "endOffset" : 240
    }, {
      "referenceID" : 15,
      "context" : "A second algorithmic feature that has been found to greatly improve the stability of neural-network based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Q and of the policy π each, with parameters θ and θ′, and ω and ω′ respectively.",
      "startOffset" : 226,
      "endOffset" : 240
    }, {
      "referenceID" : 5,
      "context" : "A second algorithmic feature that has been found to greatly improve the stability of neural-network based reinforcement learning algorithms that rely on bootstrapping for learning value functions is the use of target networks [4, 14, 16, 6]: The algorithm maintains two copies of the value function Q and of the policy π each, with parameters θ and θ′, and ω and ω′ respectively.",
      "startOffset" : 226,
      "endOffset" : 240
    }, {
      "referenceID" : 13,
      "context" : "In this work we use “soft updates” as in [14] (see Algorithms 1 and 2 below).",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "Update actor and critic using Adam [9] Update the target networks ω′ ← τω + (1− τ)ω′ θ′ ← τθ + (1− τ)θ′ end for",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : "3) were simulated in MuJoCo [28].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "A large amount of research has focused on how to efficiently integrate noisy sensory information over multiple timesteps in order to derive accurate estimates of the system state, or to estimate derivatives of important properties of the system [27].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 2,
      "context" : "The Morris water maze has been used extensively in rodents for the study of memory [3].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "Several groups [15, 1, 5] have studied the use of model-free algorithms with recurrent networks to solve POMDPs with discrete action spaces.",
      "startOffset" : 15,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "Several groups [15, 1, 5] have studied the use of model-free algorithms with recurrent networks to solve POMDPs with discrete action spaces.",
      "startOffset" : 15,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "Several groups [15, 1, 5] have studied the use of model-free algorithms with recurrent networks to solve POMDPs with discrete action spaces.",
      "startOffset" : 15,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "[1] focused on relatively long-horizon (”deep”) memory problems in small state-action spaces.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "In contrast, [5] modified the Atari DQN architecture [16] (i.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "In contrast, [5] modified the Atari DQN architecture [16] (i.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "they perform control from high-dimensional pixel inputs) and demonstrated that recurrent Q learning [15] can perform the required information integration to resolve short-term partial observability (e.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "in [30, 31, 29, 32].",
      "startOffset" : 3,
      "endOffset" : 19
    }, {
      "referenceID" : 30,
      "context" : "in [30, 31, 29, 32].",
      "startOffset" : 3,
      "endOffset" : 19
    }, {
      "referenceID" : 28,
      "context" : "in [30, 31, 29, 32].",
      "startOffset" : 3,
      "endOffset" : 19
    }, {
      "referenceID" : 31,
      "context" : "in [30, 31, 29, 32].",
      "startOffset" : 3,
      "endOffset" : 19
    }, {
      "referenceID" : 29,
      "context" : "[30] trained LSTM-based stochastic policies using Reinforce; [31, 29, 32] used actor-critic architectures.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[30] trained LSTM-based stochastic policies using Reinforce; [31, 29, 32] used actor-critic architectures.",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 28,
      "context" : "[30] trained LSTM-based stochastic policies using Reinforce; [31, 29, 32] used actor-critic architectures.",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 31,
      "context" : "[30] trained LSTM-based stochastic policies using Reinforce; [31, 29, 32] used actor-critic architectures.",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 30,
      "context" : "The algorithm of [31] can be seen as a special case of DPG where the deterministic policy produces the parameters of an action distribution from which the actions are then sampled.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 31,
      "context" : "All works mentioned above, except for [32], consider the memory to be internal to the policy and learn the RNN parameters using BPTT, back-propagating either TD errors or policy gradients.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 31,
      "context" : "[32] instead take the view of [17] and consider memory as extra state dimensions that can can be read and set by the policy.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[32] instead take the view of [17] and consider memory as extra state dimensions that can can be read and set by the policy.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "They optimize the policy using guided policy search [12] which performs explicit trajectory optimization along reference trajectories and, unlike our approach, requires a well defined full latent state and access to this latent state during training.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : "are able to benefit from innovations in supervised recurrent neural networks, such as long-short term memory networks [7], to solve challenging memory problems such as the Morris water maze.",
      "startOffset" : 118,
      "endOffset" : 121
    } ],
    "year" : 2015,
    "abstractText" : "Partially observed control problems are a challenging aspect of reinforcement learning. We extend two related, model-free algorithms for continuous control – deterministic policy gradient and stochastic value gradient – to solve partially observed domains using recurrent neural networks trained with backpropagation through time. We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements. These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps. We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task. Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels. We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies.",
    "creator" : "LaTeX with hyperref package"
  }
}