{
  "name" : "1512.00818.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos",
    "authors" : [ "Mohamed Elhoseiny", "Jingen Liu", "Hui Cheng", "Harpreet Sawhney", "Ahmed Elgammal" ],
    "emails" : [ "elgammal@cs.rutgers.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Every minute, hundreds of hours of video are uploaded to video archival site such as YouTube (Google2014 ). Developing methods to automatically understand the events captured in this large volume of videos is necessary and meanwhile challenging. One of the important tasks in this direction is event detection in videos. The main objective of this task is to determine the relevance of a video to an event based on the video content (e.g., feeding an animal, birthday party; see Fig. 1). The cues of an event in a video could include visual objects, scene, actions, detected speech (by Automated Speech Recognition(ASR)), detected text (by Optical Character Recognition (OCR)), and audio concepts (e.g. music and water concepts).\nSearch and retrieval of videos for arbitrary events using only free-style text and unseen text in particular has been a dream in computational video and multi-media understanding. This is referred as “zero-shot event detection”, because there is no positive exemplar videos to train a detector. Due to the proliferation of videos, especially consumer-generated\nCopyright c© 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nvideos (e.g., YouTube), zero-shot search and retrieval of videos has become an increasingly important problem.\nSeveral research works have been proposed to facilitate performing the zero-shot learning task by establishing an intermediate semantic layer between events or generally categories (i.e., concepts or attributes) and the low-level representation of a multimedia content from the visual perspective. (Lampert, Nickisch, and Harmeling 2009) and (Farhadi et al. 2009) were the two first to use attribute learning representation for the zero-shot setting for object recognition in still images. Attributes were similarly adopted for recognizing human actions (Liu, Kuipers, and Savarese 2011); attributes are generalized and denoted by concepts in this context. Later, (Liu et al. 2013) proposed Concept Based Event Retrieval (CBER) for videos InTheWild. Even though these methods facilitate zero-shot event detection, they only capture the visual modality and more importantly they assume that the relevant concepts for a query event are manually defined. This manual definition of concepts, also known as semantic query editing, is a tedious task and may be biased due to the limitation of human knowledge. Instead, we aim at automatically generating relevant concepts by leveraging information from distributional semantics.\nRecently, several systems were proposed for zero-shot event detection methods (Wu et al. 2014; Jiang et al. 2014b; 2014a; Chen et al. 2014; Habibian, Mensink, and Snoek 2014). These approaches rely on the whole text description of an event where relevant concepts are specified; see example event descriptions used in these approaches in the Supplementary Materials (SM)1 ( explicitly define the event ex-\n1Supplementary Materials (SM) could be found here https:// sites.google.com/site/mhelhoseiny/EDiSE_supp.zip\nar X\niv :1\n51 2.\n00 81\n8v 1\n[ cs\n.C V\n] 2\nD ec\n2 01\n5\nplication, scene, objects, activities, and audio). In practice, however, we think that typical use of event queries under this setting should be similar to text-search, which is based on few words instead that we model their connection to the multimodal content in videos.\nThe main question addressed in this paper is how to use an event text query (i.e. just the event title like “birthday party” or “feeding an animal”) to retrieve a ranked list of videos based on their content. In contrast to (Lampert, Nickisch, and Harmeling 2009; Liu et al. 2013), we do not manually assign relevant concepts for a given event query. Instead, we leverage information from a distributional semantic space (Mikolov et al. 2013b) trained on a large text corpus to embed event queries and videos to the same space, where similarity between both could be directly estimated. Furthermore, we only assume that query comes in the form of an “unstructured” few-keyword query (in contrast to (Wu et al. 2014; Jiang et al. 2014b; 2014a)). We abbreviate our method as EDiSE (Event-detection by multimodal Distributional Semantic Embedding of videos).\nContributions. The contributions of this paper can be listed as follows: (1) Studying how to use few-keyword unstructured-text query to detect/retrieve videos based on their multimedia content, which is novel in this setting. We show how relevant concepts to that event query could be automatically retrieved through a distributional semantic space and got assigned a weight associated with the relevance; see Fig. 1 “Birthday” and “Grooming an Animal” example events. (2) To the best of our knowledge, our work is the first attempt to model the connection between few keywords and multimodal information in videos by distributional semantics . We study and propose different similarity metrics in the distributional semantic space to enable event retrieval based on (a) concepts, (b) ASR, and (c) OCR in videos. Our unified framework is capable of embedding all of them into the same space; see Fig. 2. (3) Our method is also very fast, which makes it applicable to both large number of videos and concepts (i.e. 26.67 times faster than the state of the art (Jiang et al. 2014a))."
    }, {
      "heading" : "Related Work",
      "text" : "Attribute methods for zero-shot learning are based on manually specifying the attributes for each category (e.g., (Lampert, Nickisch, and Harmeling 2009; Parikh and Grauman 2011)). Other methods focused on attribute discovery (Rohrbach, Stark, and Schiele 2011; Rohrbach, Ebert, and Schiele 2013) and then apply the same mechanism. Recently, several methods were proposed to perform zero shot recognition by representing unstructured text in document terms (e.g. (Elhoseiny, Saleh, and Elgammal 2013; Mensink, Gavves, and Snoek 2014)) One drawback of the TFIDF (Salton and Buckley 1988) in (Elhoseiny, Saleh, and Elgammal 2013) and hardly matching tag terms in (Mensink, Gavves, and Snoek 2014; Rohrbach et al. 2010) is that they do not capture semantically related terms that our model can relate in noisy videos instead of still images. Also, WordNet (Miller 1995), adopted in (Rohrbach et al. 2010), does not connect objects with actions (e.g., person blowing candle), making it hard to apply in our setting and\nheavily depending on predefined information like WordNet. There has been a recent interest especially in the computational linguistics’ community in word-vector representation ( e.g., (Bengio et al. 2006)), which captures word semantics based on context. While word-vector representation is not new, recent algorithms (e.g. (Mikolov et al. 2013b; 2013a)) enabled learning these vectors from billions of words, which makes them much more semantically accurate. As a result, these models got recently adopted in several tasks including translation (Mikolov, Le, and Sutskever 2013) and web search (Shen et al. 2014). Several computer vision researchers explored using these word-vector representation to perform Zero-Shot learning in the object recognition (e.g. (Frome et al. 2013; Socher et al. 2013; Norouzi et al. 2014)). They embed the object class name into the wordvector semantic space learnt by models like (Mikolov et al. 2013b). It is worth mentioning that these zero-shot learning approaches (Frome et al. 2013; Socher et al. 2013) and also the aforementioned work (Elhoseiny, Saleh, and Elgammal 2013) assume that during training, there is a set of training classes and test classes. Hence, they learn a transformation to correlate the information between both domains (textual and visual). In contrast, zero-shot setting of event retrieval rely mainly on the event information without seeing any training events, as assumed in recent zero-shot event retrieval methods (e.g., (Dalton, Allan, and Mirajkar 2013; Jiang et al. 2014b; Wu et al. 2014; Liu et al. 2013)). Hence, there does not exist seen events to learn such transformation from. Differently, we also model multimodal connection from free text query to video information.\nIn the context of videos, (Wu et al. 2014) proposed a method for zero-shot event detection by using the salient words in the whole structured event description, where relevant concept are already defined in the event structured text description; also see Eq. 1 in (Wu et al. 2014). Similarly, (Dalton, Allan, and Mirajkar 2013) adopted a MarkovRandom-Field language model proposed by (Metzler and Croft 2005). One drawback of this model is that it performs an intensive processing for each new concept. This is since it determines the relevance of the concept to a query event by creating a text document to represent each concept. This document is created by web-querying the concept name and some of its keywords and merging the top retrieved pages. In contrast, our model does not require this step to determine relevance of an event to a query. Once the language model is trained, any concept can be instantly added and captured in our multimodal semantic embedding of videos.\nIn contrast to both (Wu et al. 2014) and (Dalton, Allan, and Mirajkar 2013), we focus on retrieving videos only with the event title (i.e., few-words query) and without semantic editing. The key difference is in modeling and embedding concepts to allow zero-shot event retrieval. In (Wu et al. 2014) and (Dalton, Allan, and Mirajkar 2013), the semantic space is a vector whose dimensionality is the number of the concepts. Our idea is to embed concepts, video information, and the event query into a distributional semantic space whose dimensionality is independent of the number of concepts. This property, together with the semantic properties captured by distributional semantics, feature our approach\nwith two advantages (a) scalability to any concept size. Having new concepts does not affect the representation dimensionality (i.e., in all our experiments concepts, videos, event queries are embedded to M dimensional space; M is few hundreds in our experiments). (b) facilitating automatic determination of relevant concepts given an unstructured short event query: For example, being able to automatically determine that “blowing a candle” concept is a relevant concept to “birthday party” event. (Wu et al. 2014) and (Dalton, Allan, and Mirajkar 2013) used the complete text description of an event for retrieval that explicitly specifies relevant concepts.\nThere is a class of models that improve zero-shot Event Detection performance by reranking. Jiang et al. proposed multimodal pseudo relevance feedback (Jiang et al. 2014b) and self-paced reranking (Jiang et al. 2014a) algorithms. The main assumption behind these models is that all unlabeled test examples are available and the top few examples by a given initial ranking have high top K precision (K ∼ 10). This means that reranking algorithms can not update confidence of a video for an event without knowing the confidences of the remaining videos to perform reranking. In contrast, our goal is different which is to directly model the probability of a few-keyword event-query given an arbitrary video. Hence, our work does not require an initial ranking and can compute the conditional probability of a video without any information about other videos. Our method is also 26.67 times faster, as detailed in our experiments."
    }, {
      "heading" : "Method",
      "text" : ""
    }, {
      "heading" : "Problem Definition",
      "text" : "Given an arbitrary event query e and a video v, our objective is to model p(e|v). We start by defining the representation of event query e, the concept set c, the video v in our setting.\nEvent-Query representation e: We use the unstructured event title to represent an event query for concept based retrieval. Our framework also allows additional terms specifically for ASR or OCR based retrieval. While we show retrieval on different modalities, concept based retrieval is our main focus in this work. The few-keyword event query for concept based retrieval is denoted by ec, while query keywords for OCR and ASR are denoted by eo and ea, respectively. Hence, under our setting e = {ec, eo, ea}.\nConcept Set c: We denote the whole concept set in our setting as c, which include visual concepts cv and audio concepts cd, i.e., c = {cv, cd}. The visual concepts include object, scene and action concepts. The audio concepts include acoustic related concepts like water sound. We performed an experiment on a set of audio concepts trained on MFCC audio features (Davis and Mermelstein 1980; Logan and others 2000). However, we found their performance≈ 1% MAP, and hence we excluded them from our final experiments. Accordingly, our final performance mainly relies on the visual concepts for concept based retrieval; i.e., cd = ∅. We denote each member ci ∈ c as the definition of the ith concept in c. ci is defined by the ith concept’s name and optionally some related keywords; see examples in SM. Hence, c = {c1, · · · , cN} is the the set of concept definitions, where N is the number concepts.\nVideo Representation: For our zero-shot purpose, a\nvideo v is defined by three pieces of information, which are video OCR denoted by vo, video ASR denoted by va, and video concept representation denoted by vc. vo and va are the detected text in OCR and ASR, respectively. We used (Myers et al. 2005) to extract vo and (van Hout et al. 2013) to extract va. In this paper, we mainly focus on the visual video content, which is the most challenging. The video concept based representation vc is defined as vc = [p(c1|v), p(c2|v), · · · , p(cN |v)] (1) where p(ci|v) is a conditional probability of concept ci given video v, detailed later. We denote p(ci|v) by vic.\nIn zero-shot event detection setting, we aim at recognizing events in videos without training examples based on its multimedia content including still-image concepts like objects and scenes, action concepts, OCR, and ASR2. Given a video v = {vc, vo, va}, our goal is to compute p(e|v) by embedding both the event query e and information of video v of different modalities (vc, vo, and vo) into a distributional semantic space, where relevance of v to e could be directly computed; see Fig. 2. Specifically, our approach is to model p(e|v) as a function F of θ(e), ψ(vc), θ(vo), and θ(va), which are the distributional semantic embedding of e, vc, vo, and va, respectively\np(e|v) ∝F ( θ(e), ψ(vc), θ(vo), θ(va) ) (2)\nWe remove the stop words from e, vo, va before applying the embedding θ(·). The rest of this section is organized as follows. First, we present the distributional semantic manifold and the embedding function θ(·) which is applied to e, va, vo, and the concept definitions c in our framework. Then, we show how to determine automatically relevant concepts to an event title query and assign a relevance weight to them, as illustrated in Fig. 1. We present this concept relevance weighting in a separate section since it might be generally useful for other applications. Finally, we present the details of p(e|v) where we derive vc embedding (i.e. ψ(vc)), which is based on the proposed concept relevance weighting."
    }, {
      "heading" : "Distributional Semantic Model & θ(·) Embedding",
      "text" : "We start by the distributional semantic model by (Mikolov et al. 2013b; 2013a) to train our semantic manifold. We denote the trained semantic manifold byMs, and the vectorization function that maps a word toMs space as vec(·). We denote the dimensionality of the real vector returned from vec(·) by M . These models learn a vector for each word wn, such that p(wn|(wi−L, wi−L+1, · · · , wi+L−1, wi+L) is maximized over the training corpus; 2×L is the context window size. Hence similarity between vec(wi) and vec(wj) is high if they co-occurred a lot in context of size 2 × L in the training text-corpus (i.e., semantically similar words share similar context). Based on the trained Ms space, we define how to embed the event query e, and c. Each of ec, ea, and eo is set of one or more words. Each of these words can be directly embedded intoMs manifold by vec(·) function. Accordingly, we represent these sets of word vectors for each of ec, ea, and eo as θ(ec), θ(ea), and θ(eo). We denote {θ(ec), θ(ea), θ(eo)} by θ(e). Regarding embedding of\n2Note that OCR and ASR are not concepts. They are rather detected text in video frames and speech\nc, each concept c∗ ∈ c is defined by its name and optionally some related keywords. Hence, the corresponding word vectors are then used to define θ(c∗) inMs space. Relevance of Concepts to Event Query Let us define a similarity function between θ(c∗) and θ(ec) as s(θ(ec), θ(c∗)). We propose two functions to measure the similarity between θ(ec) and θ(c∗). The first one is inspired by an example by (Mikolov et al. 2013b) to show the quality of their language model, where they indicated that vec(“king”) + vec(“woman”) − vec(“man”) is closest to vec(“queen”). Accordingly, we define a version of s(X,Y ), where the sets X and Y are firstly pooled by the sum operation; we denote the sum pooling operation on a set by an overline. For instance, X = ∑ i xi and Y = ∑ i yj , where xi and yj are the word vectors of the ith element in X and the jth element in Y , respectively. Then, cosine similarity between X and Y is computed. We denote this version as sp(·, ·); see Eq. 3. Fig. 3 shows how sp(·, ·) could be used to retrieve the top 20 concepts relevant to θ(“Grooming An Animal”) in Ms space. The figure also shows embedding of the query and the relevant concept sets in 3D PCA visualization. θ(ec =“Grooming An Animal”) and each of θ(ci) for the most relevant 20 concepts are represented by their corresponding pooled vectors (θ(ec) and θ(ci))∀i), normalized to unit length under L2 norm. Another idea is to define s(X,Y ) as a similarity function between the X and Y sets. For robustness (Torki and Elgammal 2010), we used percentile-based Hausdorff point set metric, where similarity between each pair of points is computed by the cosine similarity. We denote this version by st(X,Y ); see Eq. 3. We used l = 50% (i.e., median).\nsp(X,Y ) = ( ∑ i xi) T( ∑ j yj)\n‖ ∑ i xi‖‖ ∑ j yj‖ = X T Y ‖X‖‖Y ‖ (3)\nst(X,Y ) = min{ l%\nmin j m i ax xTi yj ‖xi‖‖‖yj‖ , l% min i m j ax xTi yj ‖xi‖‖yj‖ }\nEvent Detection p(e|v) In practice, we decomposed p(e|v) into p(ec|v), p(eo|v), p(ea|v), which makes the problem reduces to deriving p(ec|v) (concept based retrieval), p(eo|v) (OCR based re-\ntrieval), and p(ea|v) (ASR based retrieval) under Ms. We start by p(ec|v) then we will how later in this section how p(eo|v), and p(ea|v) could be estimated.\nEstimating p(ec|v) : In our work, concepts are linguistic meanings that have corresponding detection functions given the video v. From Fig. 3, Ms space could be viewed as a space of meanings captured by a training text-corpus, where only sparse points in that space has a corresponding visual detection functions given v, which are the concepts c (e.g., “blowing a candle”). For zero shot event detection, we aim at exploiting these sparse points by the information captured by s(θ(ec), θ(ci ∈ c)) inMs space. We derive p(ec|v) from probabilistic perspective starting from marginalizing p(ec|v) over the concept set c\np(ec|v) ∝ ∑ ci p(ec|ci)p(ci|v) ∝ ∑ ci s(θ(ec), θ(ci))v i c (4) where p(e|ci)∀i are assumed to be proportional to s(θ(ec), θ(ci)) in our framework. From semantic embedding perspective, each video v is embedded into Ms by the set ψ(vc) = {θv(ci) = vicθ(ci),∀ci ∈ c}, where vicθ(ci) is a set of the same points in θ(ci) scaled with vic; ψ(vc) could be then directly compared with θ(ec); see Eq. 5\np(ec|v) ∝ ∑ ci s(θ(ec), θ(ci))v i c\n∝ s′(θ(ec), ψ(vc) = {θv(ci), ∀ci ∈ c}) (5)\nwhere s′(θ(ec), ψ(p(c|v)) = ∑ i s(θ(ec), θv(ci)) and s(·, ·) could be replaced by sp(·, ·), st(·, ·), or any other measure in Ms space. An interesting observation is that when sp(·, ·) is chosen, p(ec|v) ∝ θ(ec) T\n‖θec‖ (∑ i θ(ci) ‖θci‖v i c ) which is a direct\nsimilarity between θ(ec) representing the query and the embedding of ψ(vc) as ∑ i θ(ci) ‖θci‖v i c; see proof in Appendix A. sp(·, ·) performs consistently better than st(·, ·) in our experiments. In practice, we only include θv(ci) in ψ(vc) such that ci is among the top R concepts with highest p(ec|ci). This is assuming that the remaining concepts are assigned p(ec|ci) = 0 which makes those items vanish; we used R=5. Hence, only a few concept detectors needs to be computed for on v which is a computational advantage.\nEstimating p(eo|v) and p(ea|v) : Both vo and va can be\ndirectly embedded into Ms since they are sets of words. Hence, we can model p(eo|v) and p(ea|v) as follows p(eo|v) ∝ sd(θ(eo), θ(vo)), p(ea|v) ∝ sd(θ(ea) , θ(va)) (6)\nwhere sd(X,Y ) = ∑ ij x T i yj . We found this similarity function more appropriate for ASR/OCR text since they normally contains more text compared to concept definition. We also exploited an interesting property in Ms that nearest words to an arbitrary point can be retrieved. Hence, we automatically augment ea and eo with the nearest words to the event title inMs using cosine similarity before retrieval. We found this trick effective in practice since it automatically retrieve relevant words that might appear in vo or va.\nFusion: We fuse p(ec|v), p(eo|v), and p(ea|v) by weighted geometric mean with focus on visual concepts, i.e. p(e|v) = w+1 √ p(ec|v)w √ p(eo|v)p(ea|v)); w = 6. p(ec|v), p(ec|v), and p(ec|v) involves the similarity between θ(e) and each of ψ(vc), θ(vo), and θ(va), leading to Eq. 2 view.\nVisual Concept Detection functions (p(c|v)) We leverage the information from three types of visual concepts in cv: object concepts co, action concepts ca, and scene concepts cs. Hence, c = cv = {co ∪ ca ∪ cs}; the list of concepts are attached in SM. We define object and scene concept probabilities per video frame, and action concepts per video chunks. The rest of this section summarizes the concept detection for objects and scenes per frame f , and action concepts per video chunk u. Then, we show how they can be reduced to video level probabilities. Fig. 4 shows example high confidence concepts in a “Birthday Party” video.\nObject Concepts p(oi|f), oi ∈ co: We involve 1000 Overfeat (Sermanet et al. 2014) object concept detectors which maps to 1000-ImageNet categories. We also adopt the concept detectors of face, car and person from a publicity available detector (i.e., (Felzenszwalb, McAllester, and Ramanan 2008)) Scene Concepts p(si|f), si ∈ cs: We represented scene concepts (p(si|f)) as bag of word representation on static features (i.e., SIFT (Lowe 2004) and HOG (Dalal and Triggs 2005)) with 10000 codebooks. We used TRECVID 500 SIN concepts concepts, including scene categories like “city” and “hall” way; these concepts are provided by provided by TRECVID2011 SIN track. Action Concepts p(ai|u), ai ∈ ca: We use both manually annotated (i.e. strongly supervised) and automatically annotated (i.e. weekly supervised) concepts; detailed in SM. We have ∼500 action concepts; please refer to (Liu et al. 2013) for the action concept detection method that we adopt. Video level concept probabilities p(c|v) We represent probabilities of the cv set given a video v by a pooling operation over the the chunks or the frames of the videos\nsimilar to (Liu et al. 2013). In our experiments, we evaluated both max and average pooling. Specifically, p(oi|v) = ρ({p(oi|fk), fk ∈ v}), p(sl|v) = ρ({p(si|fk), fk ∈ v}), p(ak|v) = ρ({p(ai|uk), uk ∈ v}, where p(oi|v) and p(sl|v) are the video level probabilities of for the ith object and the lth scene concepts respectively, pooled over frames fk ∈ v. {fk ∈ v} are selected every M frames in v (M= 250). p(ak|v) is the video level probability of the kth action concept, pooled over a set of video chunks {uk ∈ v}. The chunk size is set to the mean chunk length of all concept training chunks. Finally, ρ is the pooling function. We denote average and max pooling as ρa(·) and ρm(·), respectively."
    }, {
      "heading" : "EDiSE Computational Performance Benefits",
      "text" : "Here we discuss the computational complexity of concept based EDISE, and ASR/OCR based EDiSE. The fusion part is negligible since it is constant time."
    }, {
      "heading" : "Concept based EDiSE",
      "text" : "The computational complexity for computing p(ec|v) is mainly linear in the number of videos, denoted by |V |. We here detail why computational complexity of p(ec|v) is almost constant and hence video retrieval is almost O(|V |).\nFrom Eq. 5, p(ec|v) has a computational complexity of O(N ·Q) for on e video, whereQ is the computational complexity of computing s(·, ·) andN is the number of concepts. We detail next the computational complexity of sp(·, ·) and st(·, ·) for the whole set of videos |V |. Complexity of p(ec|v) for sp(·, ·) Let’s assume that there θ(ec) set has |ec| terms and θ(ci) has |ci| terms. Then, the computational complexity of sp(θ(ec), θ(ci)) isO(M(|ec|+ |ci|). |ci| and and |ec| are usually few terms in our case (< 10). Hence the computational complexity of sp(θ(ec), θ(ci)) is O(M), where M is the dimensionality of the word vectors. In our experiments M = 300. Given the complexity of sp(θ(ec), θ(ci)), the computational complexity of p(ec|v) will be O(N · M), where N is the number of concepts. Hence, the computational complexity for computing p(ec|v) for |V | videos isO(|V | ·N ·M). However, for a given event, only few concepts are relevant, which are computed based on sp(θ(ec), θ(ci)) and only few concepts 5 in our case are sufficient for event zero shot retrieval, retrieved by Nearest Neighbor search of ci ∈ c that is close the ec. Hence the computational complexity reduced toO(|V | ·M),M = 300 for the GoogleNews word2vec model that we used. Hence, the computational complexity for |V | videos is basically linear O(|V |), given M is a constant and M << |V |. Complexity of p(ec|v) for st(·, ·) The previous argument applies here in all elements except the complexity of the similarity function st(θ(ec), θ(ci)), which is O(M(|ec| · |ci|). Assuming that |ec| · |ci| is bounded by a constant, then the complexity of |V | videos is also O(|V | ·M), but with a bigger constant compared to sp(·, ·) (linear in |V | for constant M << |V |)."
    }, {
      "heading" : "ASR/OCR based EDiSE",
      "text" : "The computational complexity of sd(θ(eo), θ(vo)) and sd(θ(ea), θ(va)) are O(|eo| · |vo| ·M) and O(|ea| · |va| ·M),\nTable 1: MED2013 MAP performance on four concept sets (event title query)\nOurs-Gnews Ours-Wiki (Dalton etal, 2013) TRECVID MED 2013 ρm(·) ρa(·) ρm(·) ρa(·)\nsp(·, ·) st(·, ·) sp(·, ·) st(·, ·) sp(·, ·) st(·, ·) sp(·, ·) st(·, ·) Concepts G1 (152 concepts) 4.29 3.94% 2.39% 2.38% 3.14% 2.13% 1.85% 1.70% 2.57% Concepts G2 (101 concepts) 1.74 1.20 1.56% 1.20% 1.09% 0.96% 0.66% 0.60% 1.17% Concepts G3 (60 concepts) 1.72 1.33% 1.28% 1.16% 1.21% 0.88% 0.88% 0.74% 1.54% Concepts G4 (56 concepts) 1.22 0.95 0.84% 0.69% 0.87% 0.76% 0.67% 0.56% 0.83%\nTable 2: MED2013 full concept set MAP Performance (auto-weighted versus manually-weighted concepts)\nOurs (auto-weighted )) (Dalton etal,13)(auto-weighted) (Dalton etal,13) (manually-weighted) Overfeat 8.36% 3.40% 7.4% 2.43%\nSUN Object Rank Classeme CDDT WSCD−SIFTY ouTube 0.48% 0.77% 0.84% 2.28% 3.48%\nrespectively. There is no concepts for ASR/OCR based retrieval. Hence, the computational complexity of p(eo, v) and p(ea|v) areO(|V | · |eo| · |vo| ·M) andO(|V | · |ea| · |va| ·M), respectively. Since |eo| |V |, |vo| |V |, |ea| |V |, |va| |V |, and M |V |, the dominating factor in the complexity for both p(eo, v) and p(ea|v) will be |V |."
    }, {
      "heading" : "Experiments",
      "text" : "We evaluated our method on the large TRECVID MED (Felzenszwalb, McAllester, and Ramanan 2013). We show the MAP (Mean Average Precision) and ROC AUC performance of the designated MEDTest set (Felzenszwalb, McAllester, and Ramanan 2013), containing more than 25,000 videos. Unless otherwise mentioned, our results are on TRECVID MED2013. There are two distributional semantic models in our experiments, trained on Wikipedia and GoogleNews using (Mikolov et al. 2013b). The Wikipedia model got trained on 1 billion words resulting in a vocabulary of size of≈120,000 words and word vectors of 250 dimensions. The GoogleNews model got trained on 100 billion words resulting in a vocabulary of size 3 million words and word vectors of 300 dimensions. The objective of having two models is to compare how well our EDiSE method performs depending on the size of the training corpus, used to train the language model. In the rest of this section, we present Concepts, OCR, ASR, and fusion results."
    }, {
      "heading" : "Concept based Retrieval",
      "text" : "All the results in this section were generated by automatically retrieved concepts using only the event title. We start by comparing different settings of our method against (Dalton, Allan, and Mirajkar 2013). We used the language model in (Dalton, Allan, and Mirajkar 2013) for concept based retrieval to rank the concepts. This indicates that p(e|ci) in Eq. 4 is computed by the language model in (Metzler and Croft 2005) as adopted in (Dalton, Allan, and Mirajkar 2013), that we compare with under exactly the same setting. For our model, we evaluated the two pooling operations ρm(·) and ρp(·) and also the two different similarity measures onMs space sp(·, ·) and st(·, ·). Furthermore, we evaluated the methods on both Wikipedia and GNews language models. In order to have conclusive experiments on these eight settings of our model compared to (Dalton, Allan, and Mirajkar 2013), we performed all of them on the four different sets of concepts (i.e. each has the same concept detectors; completely consistent comparison); see Table 1. Details about these concept sets are attached in SM.\nThere are a number of observations. (1) using GNews (the bigger text corpus) language model is consistently better than using the Wikipedia language model. This indicates when the word embedding model is trained with a bigger text corpus, it captures more semantics and hence more accurate in our setting. (2) max pooling ρm(·) behaves consistently better than average pooling ρa(·). (3) sp(·, ·) sim-\nilarity measure is consistently better than st(·, ·), which we see very interesting since this indicates that our hypothesis of using the vector operations onMs manifold better represent p(e|ci). Hence, we recommend finally to use the model trained on the larger corpus, ρm(·) for concept pooling, and use sp(·, ·) to measure the performance on Ms manifold. (4) our model’s final setting is consistently better than (Dalton, Allan, and Mirajkar 2013). The final MED13 ROC AUC performance is 0.834. MAP for MED13 Events 31 to 40 (E31:40) is 5.97%. Detailed figures are attached in SM.\nOur next experiment shows the final MAP performance using the recommended setting for our framework on the whole set of concepts, detailed earlier and in SM. Table 2 shows our final performance compared with (Dalton, Allan, and Mirajkar 2013) on the same concept detectors. It is not hard to see that our method performs more than double the MAP performance of (Dalton, Allan, and Mirajkar 2013) under the same concept set. Even when manual semantic editing is applied to (Dalton, Allan, and Mirajkar 2013), our performance is still better without semantic editing. We also show the performance on the same events of different concepts (i.e. SUN (Patterson and Hays 2012), Object Rank (Li et al. 2010), Classeme (Torresani, Szummer, and Fitzgibbon 2010)), and the best performing concepts in (Wu et al. 2014) (i.e., CDDT , WSCD−SIFTY ouTube ). These numbers are as reported in (Wu et al. 2014). The results indicate the value of our concepts and approach compared to (Wu et al. 2014) and their concepts. We also report our performance using Overfeat concepts only to retrieve videos for the same events. This shows the value of involving action and scene concepts compared to only still image concepts like Overfeat for zero-shot event detection. We highlight that the results in (Wu et al. 2014) uses the whole event description which explicitly includes names of relevant concepts."
    }, {
      "heading" : "ASR and OCR based Retrieval",
      "text" : "First, we compared our OCR and ASR retrieval trained on both Wikipedia and GoogleNews language model. Table 3 shows that the GoogleNews model MED13 MAP is better than the Wikipedia Model MAP in both ASR and OCR, which is consistent with our concept retrieval results. Figure 5 shows the GoogleNews MED13 AP per event for both OCR and ASR. We further show our AP performance on MED14 events 31 to 40 in Fig. 5.\nIn order to show the value our semantic modeling, we\ncomputed the performance of string matching method as a baseline, which basically increment the score for every exact match in the the detected text to words in the query. While, both our model and the matching model use the same query words and ASR/OCR detection, semantic properties captured byMs boosts the performance compared to string matching; see table 3. This is since semantically relevant terms to the query have a high cosine similarity inMs (i.e., high vec(wi)Tvec(wj) if wi is semantically related to wj). On the other hand, hard matching basically assumes that vec(wi)Tvec(wj) = 1 ifwi = wj , 0 otherwise. We also computed the ROC AUC metric for our method and the hard matching method on ASR and OCR; see Fig. 6. For ASR, average AUC is 0.623 for ours and 0.567 for Matching (9.9% gain). For OCR, average AUC is 0.621 for ours and 0.53 for Matching (17.1% gain). We report our GNews model results compared with (Wu et al. 2014) to indicate that, we achieve state-of-the-art MED13 MAP performance or even better for ASR/OCR; see table 4. The table also shows our ASR&OCR MED14 (E31:40) MAP."
    }, {
      "heading" : "Fusion Experiments and Related Systems",
      "text" : "In table 5, we start by presenting a summary of our earlier ASR/OCR results on MED13 Test. Comparing OCR and ASR performances to Concepts performance, it is not hard to see that OCR/ASR have much lower average AUC zeroshot performance compared to concepts which are visual in our work. This indicates that OCR/ASR produces much higher false negatives compared to visual concepts. When we fused our all OCR and ASR confidences, we achieved 10.7% MAP performance, however, the average AUC performance is as low as 0.67. We achieved lower MAP for our concepts 8.36% MAP but the average AUC performance is as high as 0.834. This indicates that measuring retrieval performance on MAP performance only is not informative, so one approach might achieve a high MAP but lower average AUC and vice versa. We further achieved the best performance of our system by fusing all Concepts, OCR, and ASR to achieve 13.1% MAP and 0.830 average AUC. We found our system achieves better than the state of the art system (Wu et al. 2014) 4.0% gain in MAP, but significantly in average AUC; see 13.6% gain to (Wu et al. 2014) in table 5.\nWe also discuss CPRF (Yang and Hanjalic 2010), MMPRF (Jiang et al. 2014b), and SPaR (Jiang et al. 2014a) reranking systems in contrast to our system that does not involve reranking. The initial retrieval performance is 3.9%\nMAP without reranking. Interestingly, we achieved a performance of 13.1% MAP also without reranking. The reranking methods assumes high top 5-10 precision of the initial ranking and that all test videos are available. Without any of these assumptions, our system without reranking performs 6.7%, 3.0%, and 0.2% better than CPRF (Yang and Hanjalic 2010), MMPRF (Jiang et al. 2014b), and SPaR (Jiang et al. 2014a) re-ranking systems; see table 5. Unfortunately, ROC AUC performances are not available for these method to compare with. Regarding efficiency, given vc representation of videos, our concept retrieval experiment on our whole concept set it takes ≈270 seconds on a 16 cores Intel Xeon processor (64GB RAM) to the retrieval task on 20 events altogether. This is more than the time that SPaR (Jiang et al. 2014a) takes to rerank one event on an Intel Xeon processor(16GB RAM); see (Jiang et al. 2014a). Since, we detect the MED13 events in≈270 given vc representation of videos and as reported in (Jiang et al. 2014a), their average detection time per event for MED13 is ≈ 5 minutes assuming feature representation of videos (i.e., 360 seconds per event = 7200 seconds per 20 events). This indicates that our system is 26.67X faster than (Jiang et al. 2014a) in MED13 detection. Finally, when we applied SPaR on our output as an initial ranking, we found that it improves MAP (from 13.1% to 13.5%) but hurts ROC AUC (from 0.83 to 0.79). This indicates that reranking has a limited/harmful effect on the performance of our method. We think is since our method already achieve a high performance without re-ranking; see SM for details about the features in this experiment."
    }, {
      "heading" : "Conclusion",
      "text" : "We proposed a method for zero shot event detection by distributional semantic embedding of video modalities and with only event title query. By fusing all modalities, our method outperformed the state of the art on the challenging TRECVID MED benchmark. Based on this notion, we also showed how to automatically determine relevance of concepts to an event based on the distributional semantic space.\nAcknowledgements. This work has been supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11-PC20066. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either ex-\npressed or implied, of IARPA, DOI/NBC, or the U.S. Government. This work is also partially funded by NSF-USA award # 1409683."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We propose a new zero-shot Event Detection method by Multi-modal Distributional Semantic embedding of videos. Our model embeds object and action concepts as well as other available modalities from videos into a distributional semantic space. To our knowledge, this is the first Zero-Shot event detection model that is built on top of distributional semantics and extends it in the following directions: (a) semantic embedding of multimodal information in videos (with focus on the visual modalities), (b) automatically determining relevance of concepts/attributes to a free text query, which could be useful for other applications, and (c) retrieving videos by free text event query (e.g., ”changing a vehicle tire”) based on their content. We embed videos into a distributional semantic space and then measure the similarity between videos and the event query in a free text form. We validated our method on the large TRECVID MED (Multimedia Event Detection) challenge. Using only the event title as a query, our method outperformed the state-of-the-art that uses big descriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC metric. It is also an order of magnitude faster.",
    "creator" : "TeX"
  }
}