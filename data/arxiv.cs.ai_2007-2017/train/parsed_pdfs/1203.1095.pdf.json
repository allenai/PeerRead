{
  "name" : "1203.1095.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "tom.schrijvers@ugent.be", "pieter.wuille@ugent.be", "guido.tack@monash.edu", "pieter.wuille@cs.kuleuven.be", "samulowitz@us.ibm.com", "pjs@cs.mu.oz.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This article introduces search combinators, a lightweight and solver-independent method that bridges the gap between a conceptually simple modeling language for search (highlevel, functional and naturally compositional) and an efficient implementation (low-level, imperative and highly non-modular). By allowing the user to define application-tailored search strategies from a small set of primitives, search combinators effectively provide a rich domain-specific language (DSL) for modeling search to the user. Remarkably, this DSL comes at a low implementation cost to the developer of a constraint solver.\nThe article discusses two modular implementation approaches and shows, by empirical evaluation, that search combinators can be implemented without overhead compared to a native, direct implementation in a constraint solver.\nTom Schrijvers · Pieter Wuille Universiteit Gent, Belgium E-mail: {tom.schrijvers,pieter.wuille}@ugent.be\nGuido Tack Monash University, Victoria, Australia E-mail: guido.tack@monash.edu\nPieter Wuille Katholieke Universiteit Leuven, Belgium E-mail: pieter.wuille@cs.kuleuven.be\nHorst Samulowitz IBM Research, New York, USA E-mail: samulowitz@us.ibm.com\nPeter J. Stuckey National ICT Australia (NICTA) and University of Melbourne, Victoria, Australia E-mail: pjs@cs.mu.oz.au\nar X\niv :1\n20 3.\n10 95\nv1 [\ncs .A\nI] 6\nM ar\n2 01\n2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Search heuristics often make all the difference between effectively solving a combinatorial problem and utter failure. Heuristics make a search algorithm efficient for a variety of reasons, e.g., incorporation of domain knowledge, or randomization to avoid heavy-tailed runtimes. Hence, the ability to swiftly design search heuristics that are tailored towards a problem domain is essential for performance. This article introduces search combinators, a versatile, modular, and efficiently implementable language for expressing search heuristics.\n1.1 Status Quo\nIn CP, much attention has been devoted to facilitating the modeling of combinatorial problems. A range of high-level modeling languages, such as Zinc [13], OPL [29] or Comet [27], enable quick development and exploration of problem models. However, we see very little support on the side of formulating accompanying search heuristics. Most languages and systems, e.g. MiniZinc [14], Comet [27], Gecode [23], or ECLiPSe [7] provide a small set of predefined heuristics “off the shelf”. Some systems also support user-defined search based on a general-purpose programming language (e.g., all of the above systems except MiniZinc). The former is clearly too confining, while the latter leaves to be desired in terms of productivity, since implementing a search heuristic quickly becomes a non-negligible effort. This also explains why the set of predefined heuristics is typically small: it takes a lot of time for CP system developers to implement heuristics, too – time they would much rather spend otherwise improving their system.\n1.2 Contributions\nIn this article we show how to resolve this stand-off between solver developers and users, by introducing a domain-specific modular search language based on combinators, as well as a modular, extensible implementation architecture.\nFor the user, we provide a modeling language for expressing complex search heuristics based on an (extensible) set of primitive combinators. Even if the users are only provided with a small set of combinators, they can already express a vast range of combinations. Moreover, using combinators to program application-tailored search is vastly more productive than resorting to a general-purpose language. For the system developer, we show how to design and implement modular combinators. The modularity of the language thus carries over directly to modularity of the implementation. Developers do not have to cater explicitly for all possible combinator combinations. Small implementation efforts result in providing the user with a lot of expressive power. Moreover, the cost of adding one more combinator is small, yet the return in terms of additional expressiveness can be quite large.\nThe technical challenge is to bridge the gap between a conceptually simple search language and an efficient implementation, which is typically low-level, imperative and highly non-modular. This is where existing approaches fail; they restrict the expressiveness of their search specification language to face up to implementation limitations, or they raise errors when the user strays out of the implemented subset.\nThe contribution is therefore the novel design of an expressive, high-level, compositional search language with an equally modular, extensible, and efficient implementation architecture.\n1.3 Approach\nWe overcome the modularity challenge by implementing the primitives of our search language as mixin components [4]. As in Aspect-Oriented Programming [10], mixin components neatly encapsulate the cross-cutting behavior of primitive search concepts, which are highly entangled in conventional approaches. Cross-cutting means that a mixin component can interfere with the behavior of its sub-components (in this case, sub-searches). The combination of encapsulation and cross-cutting behavior is essential for systematic reuse of search combinators. Without this degree of modularity, minor modifications require rewriting from scratch.\nAn added advantage of mixin components is extensibility. We can add new features to the language by adding more mixin components. The cost of adding such a new component is small, because it does not require changes to the existing ones. Moreover, experimental evaluation bears out that this modular approach has no significant overhead compared to the traditional monolithic approach. Finally, our approach is solver-independent and therefore makes search combinators a potential standard for designing search.\n1.4 Plan of the Article\nThe rest of the article is structured as follows. The next section defines the high-level search language in terms of basic heuristics and combinators. Sect. 3 shows how the modular language is mapped to a modular design of the combinator implementations. Sect. 4 presents two concrete implementation approaches for combinators and gives an overview of how we integrate search combinators into the MiniZinc toolchain. Sect. 5 verifies that combinators can be implemented with low overhead. Finally, Sect. 6 discusses related approaches, and Sect. 7 concludes the article.\n1.5 Note to Reviewers\nThis article is an extended version of a paper [21] that appeared in the proceedings of the 17th International Conference on Principles and Practice of Constraint Programming (CP) 2011. That paper further developed the ideas laid out in our earlier paper [19], which was presented at ModRef 2010.\nCompared to the CP’11 conference version, this article features a completely rewritten and more detailed introduction of the combinator language, both from the high-level (Sect. 2) and the implementation-level (Sect. 3) point of view. We have completed the integration into MiniZinc and present a full toolchain in Sect. 4.3. In addition, the discussion of related work (Sect. 6) has been extended with much more detail."
    }, {
      "heading" : "2 High-Level Search Language",
      "text" : "This section introduces the syntax of our high-level search language and illustrates its expressive power and modularity by means of examples. The rest of the article then presents an architecture that maps the modularity of the language down to the implementation level.\nThe search language is used to define a search heuristic, which a search engine applies to each node of the search tree. For each node, the heuristic determines whether to continue search by creating child nodes, or to prune the tree at that node. The queuing strategy, i.e., the strategy by which new nodes are selected for further search (such as depth-first traversal), is determined separately by the search engine, it is thus orthogonal to the search language. The search language features a number of primitives, listed in the catalog of Fig. 1. These are the building blocks in terms of which more complex heuristics can be defined, and they can be grouped into basic heuristics (base_search and prune), combinators (ifthenelse, and, or, portfolio, and restart), and state management (let, assign, post). This section introduces the three groups of primitives in turn.\nWe emphasize that this catalog is open-ended; we will see that the language implementation explicitly supports adding new primitives.\nThe concrete syntax we chose for presentation uses simple nested terms, which makes it compatible with the annotation language of MiniZinc [14]. Sect. 4.3 discusses our implementation of MiniZinc with combinator support. However, other concrete syntax forms are easily supported (e.g., we support C++ and Haskell).\n2.1 Basic Heuristics\nLet us first discuss the two basic primitives, base_search and prune.\nbase_search. The most widely used method for specifying a basic heuristic for a constraint problem is to define it in terms of a variable selection strategy which picks the next variable to constrain, and a domain splitting strategy which splits the set of possible values of the selected variable into two (or more) disjoint sets. Common variable selection strategies are:\n– firstfail: select the variable with the smallest current domain, – smallest: select the variable which can take the smallest possible value, – domwdeg [2]: select the variable with smallest ratio of size of current domain and num-\nber of failures the variable has been involved in, and – impact [18]: select the variable that will (based on past experience) reduce the raw search\nspace of the problem the most.\nCommon domain splitting strategies are:\n– min: set the variable to its minimum value or greater than its minimum, – max: set the variable to its maximum value or less than its maximum, – median: set the variable to its median value, or not equal to this value, and – split: constrain the variable to the lower half of its range of possible values, or its upper\nhalf.\nThe CP community has spent a considerable amount of work on defining and exploring the above and many other variable selection and domain splitting heuristics. The provision of a flexible language for defining new basic searches is an interesting problem in its own right, but in this article we concentrate on search combinators that combine and modify basic searches.\nTo this end, our search language provides the primitive base_search(vars, var-select, domain-split), which specifies a systematic search. If any of the variables vars are still not fixed at the current node, it creates child nodes according to var-select and domain-split as variable selection and domain splitting strategies respectively.\nNote that base_search is a CP-specific primitive; other kinds of solvers provide their own search primitives. The rest of the search language is essentially solver-independent. While the solver provides few basic heuristics, the search language adds great expressive power by allowing these to be combined arbitrarily using combinators.\nprune. The second basic primitive, prune, simply cuts the search tree below the current node. Obviously, this primitive is useless on its own, but we will see shortly how prune can be used together with combinators.\n2.2 Combinators\nThe expressive power of the search language relies on combinators, which combine search heuristics (which can be basic or themselves constructed using combinators) into more complex heuristics.\nand/or. Probably the most widely used combination of heuristics is sequential composition. For instance, it is often useful to first label one set of problem variables before starting to label a second set. The following heuristic uses the and combinator to first label all the xs variables using a first-fail strategy, followed by the ys variables with a different strategy:\nand([base_search(xs,firstfail,min), base_search(ys,smallest,max)])\nAs you can see in Fig. 1, the and combinator accepts a list of searches s1, . . . ,sn, and performs their and-sequential composition. And-sequential means, intuitively, that solutions are found by performing all the sub-searches sequentially down one branch of the search tree, as illustrated in Fig. 2.1.\nThe dual combinator, or([s1, . . . ,sn]), performs a disjunctive combination of its subsearches – a solution is found using any of the sub-searches (Fig. 2.2), trying them in the given order.\nStatistics and ifthenelse. The ifthenelse combinator is centered around a conditional expression c. As long as c is true for the current node, the sub-search s1 is used. Once c is false, s2 is used for the complete subtree below the current node (see Fig. 2.3).\nWe do not specify the expression language for conditions in detail, we simply assume that it comprises the typical arithmetic and comparison operators and literals that require no further explanation. It is notable though that the language can refer to the constraint variables and parameters of the underlying model. Additionally, a condition may refer to one or more statistics variables. Such statistics are collected for the duration of a subsearch until the condition is met. For instance ifthenelse(depth < 10,s1,s2) maintains the search depth statistic during subsearch s1. At depth 10, the ifthenelse combinator switches to subsearch s2.\nWe distinguish two forms of statistics: Local statistics such as depth and discrepancies express properties of individual nodes. Global statistics such as number of explored nodes, encountered failures, solution, and time are computed for entire search trees.\nIt is worthwhile to mention that developers (and advanced users) can also define their own statistics, just like combinators, to complement any predefined ones. In fact, Sect. 3 will show that statistics can be implemented as a subtype of combinators that can be queried for the statistic’s value.\nAbstraction. Our search language draws its expressive power from the combination of primitive heuristics using combinators. An important aspect of the search language is abstraction: the ability to create new combinators by effectively defining macros in terms of existing combinators.\nFor example, we can define the limiting combinator limit(c,s) to perform s while condition c is satisfied, and otherwise cut the search tree using prune:\nlimit(c,s)≡ ifthenelse(c,s,prune) The well-known once(s) combinator is a special case of the limiting combinator where the number of solutions is less than one. This is simply achieved by maintaining and accessing the solutions statistic:\nonce(s)≡ limit(solutions < 1,s)\nExhaustiveness and portfolio/restart. The behavior of the final two combinators, portfolio and restart, depends on whether their sub-search was exhaustive. Exhaustiveness simply means that the search has explored the entire subtree without ever invoking the prune primitive.\nThe portfolio([s1, . . . ,sn]) combinator performs s1 until it has explored the whole subtree. If s1 was exhaustive, i.e., if it did not call prune during the exploration of the subtree, the search is finished. Otherwise, it continues with portfolio([s2, . . . ,sn]). This is illustrated in\nFig. 2.4, where the subtree of s1 represents a non-exhaustive search, s2 is exhaustive and therefore s3 is never invoked.\nAn example for the use of portfolio is the hotstart(c,s1,c2) combinator. It performs search heuristic s1 while condition c holds to initialize global parameters for a second search s2. This heuristic can for example be used to initialize the widely applied Impact heuristic [18]. Note that we assume here that the parameters to be initialized are maintained by the underlying solver, so we omit an explicit reference to them.\nhotstart(c,s1,s2)≡ portfolio([limit(c,s1),s2])\nThe restart(c,s) combinator repeatedly runs s in full. If s was not exhaustive, it is restarted, until condition c no longer holds. Fig. 2.5 shows the two cases, on the left terminating with an exhaustive search s, on the right terminating because c is no longer true.\nThe following implements random restarts, where search is stopped after 1000 failures and restarted with a random strategy:\nrestart(true, limit(failures < 1000,base_search(xs, randomvar, randomval)))\nClearly, this strategy has a flaw: If it takes more than 1000 failures to find the solution, the search will never finish. We will shortly see how to fix this by introducing user-defined search variables.\nThe prune primitive is the only source of non-exhaustiveness. Combinators propagate exhaustiveness in the obvious way:\n– and([s1, . . . ,sn]) is exhaustive if all si are – or([s1, . . . ,sn]) is exhaustive if all si are – portfolio([s1, . . . ,sn]) is exhaustive if one si is – restart(c,s) is exhaustive if the last iteration is exhaustive – ifthenelse(c,s1,s2) is exhaustive if s1 and s2 are\n2.3 State Access and Manipulation\nThe remaining three primitives, let, assign, and post, are used to access and manipulate the state of the search:\n– let(v,e,s) introduces a new search variable v with initial value of the expression e and visible in the search s, then continues with s. Note that search variables are distinct from the decision variables of the model. – assign(v,e): assigns the value of the expression e to search variable v and succeeds. – post(c,s): provides access to the underlying constraint solver, posting a constraint c at\nevery node during s. If s is omitted, it posts the constraint and immediately succeeds.\nThese primitives add a great deal of expressivity to the language, as the following examples demonstrate.\nRandom restarts: Let us reconsider the example using random restarts from the previous section, which suffered from incompleteness because it only ever explored 1000 failures. A standard way to make this strategy complete is to increase the limit geometrically with each iteration:\ngeom_restart(s)≡ let(maxfails,100, restart(true,portfolio([limit(failures < maxfails,s),\nassign(maxfails,maxfails∗1.5), prune]))\nThe search initializes the search variable maxfails to 100, and then calls search s with maxfails as the limit. If the search is exhaustive, both the portfolio and the restart combinators are finished. If the search is not exhaustive, the limit is multiplied by 1.5, and the search starts over. Note that assign succeeds, so we need to call prune afterwards in order to propagate the non-exhaustiveness of s to the restart combinator.\nBranch-and-bound: A slightly more advanced example is the branch-and-bound optimization strategy:\nbab(obj,s)≡ let(best,∞, post(obj < best,and([s,assign(best,obj)]))) It introduces a variable best that initially takes value ∞ (for minimization). In every node, it posts a constraint to bound the objective variable by best. Whenever a new solution is found, the bound is updated accordingly using assign.\nThe bab example demonstrates how search variables (like best) and model variables (like obj) can be mixed in expressions. This makes it possible to remember the state of the search between invocations of a heuristic. All of the following combinators make use of this feature.\nRestarting branch-and-bound: This is a twist on regular branch-and-bound that restarts whenever a solution is found.\nrestart_bab(obj,s)≡ let(best,∞, restart(true, and([post(obj < best),once(s), assign(best,obj)])))\nRadiotherapy treatment planning: The following search heuristic can be used to solve radiotherapy treatment planning problems [1]. The heuristic minimizes a variable k using branch-and-bound (bab), first searching the variables N, and then verifying the solution by partitioning the problem along the rowi variables for each row i one at a time (expressed as a MiniZinc array comprehension). Failure on one row must be caused by the search on the variables in N, and consequently search never backtracks into other rows.\nThis behavior is similar to the once combinator defined above. However, when a single solution is found, the search should be considered exhaustive. We therefore need an exhaustive variant of once, which can be implemented by replacing prune with post(false):\nexh_once(s)≡ ifthenelse(solutions < 1,s,post(false)) This allows us to express the entire search strategy for radiotherapy treatment planning:\nbab(k, and([base_search(N, . . .)]++ [exh_once(base_search(rowi, . . .)) | i in 1..n]))\nFor: The for loop construct (v ∈ [l,u]) can be defined as: for(v, l,u,s)≡ let(v, l, restart(v≤ u,\nportfolio([s,and([assign(v,v+1),prune])])))\nIt simply runs u− l + 1 times the search s, which of course is only sensible if s makes use of side effects or the loop variable v. As in the geom_restart combinator above, prune propagates the non-exhaustiveness of s to the restart combinator.\nLimited discrepancy search [9] with an upper limit of l discrepancies for an underlying search s.\nlds(l,s)≡ for(n, 0, l, limit(discrepancies≤ n,s)) The for construct iterates the maximum number of discrepancies n from 0 to l, while limit executes s as long as the number of discrepancies is smaller than n. The search makes use of the discrepancies statistic that is maintained by the search infrastructure. The original LDS [9] visits the nodes in a specific order. The search described here visits the same nodes in the same order of discrepancies, but possibly in a different individual order – as this is determined by the global queuing strategy.\nThe following is a combination of branch-and-bound and limited discrepancy search for solving job shop scheduling problems, as described in [9]. The heuristic searches the Boolean variables prec, which determine the order of all pairs of tasks on the same machine. As the order completely determines the schedule, we then fix the start times using exh_once.\nbab(makespan, lds(∞,and([base_search(prec, . . .), exh_once(base_search(start, . . .))])))\nFully expanded, this heuristic consists of 17 combinators and is 11 combinators deep.\nIterative deepening [11] for an underlying search s is a particular instance of the more general pattern of restarting with an updated bound, which we have already seen in the geom_restart example. Here, we generalize this idea:\nid(s)≡ ir(depth,0,+,1,∞,s) ir(p, l,⊕, i,u,s)≡ let(n, l, restart(n≤ u,and([assign(n,n⊕ i),\nlimit(p≤ n,s)]))) With let, bound n is initialized to l. Search s is pruned when statistic p exceeds n, but iteratively restarted by restart with n updated to n⊕ i. The repetition stops when n exceeds u or when s has been fully explored. The bound increases geometrically, if we supply ∗ for ⊕, as in the restart_flip heuristic:\nrestart_flip(p, l, i,u,s1,s2)≡let(flip,1, ir(p, l,∗, i,u,and([assign(flip,1−flip), ifthenelse(flip = 1,s1,s2)])))\nThe restart_flip search alternates between two search heuristics s1 and s2. Using this as its default strategy in the free search category, the lazy clause generation solver Chuffed scored most points in the 2010 and 2011 MiniZinc Challenges.1\nProbe search: Try out two searches s1 and s2 to a limited extent defined by condition c. Then, for the remainder, use the search that resulted in the best solution so far.\nprobe(c,obj,s1,s2)≡ let(best1,∞, let(best2,∞, portfolio([ limit(c,and([s1,assign(best1,obj)]))\nlimit(c,and([s2,assign(best2,obj)])) ifthenelse(best1 ≤ best2,s1,s2)])))\nDichotomic search [24] solves an optimization problem by repeatedly partitioning the interval in which the possible optimal solution can lie. It can be implemented by restarting as long the lower bound has not met the upper bound (line 2), computing the middle (line 3), and then using an or combinator to try the lower half (line 5). If it succeeds, obj− 1 is the new upper bound, otherwise, the lower bound is increased (line 6).\ndicho(s,obj, lb,ub)≡let(l, lb, let(u,ub, restart(l < u,\nlet(h, l + d(u− l)/2e, once(or([\nand([post(l ≤ obj≤ h),s,assign(u,obj−1)]), and([assign(l,h+1),prune])]))\n))))"
    }, {
      "heading" : "3 Modular Combinator Design",
      "text" : "The previous section caters for the user’s needs, presenting a high-level modular syntax for our combinator-based search language. To cater for the system developer’s needs, this section goes beyond modularity of syntax, introducing modularity of design.\nModularity of design is the one property that makes our approach practical. Each combinator corresponds to a separate module that has a meaning and an implementation independent of the other combinators. This enables us to actually realize the search specifications defined by modular syntax.\nModularity of design also enables growing a system from a small set of combinators (e.g., those listed in Fig. 1), gradually adding more as the need arises. Advanced users can complement the system’s generic combinators with a few application-specific ones.\nSolver independence is another notable property of our approach. While a few combinators access solver-specific functionality (e.g., base_search and post), the approach as such and most combinators listed in Fig. 1 are in fact generic (solver- and even CP-independent); their design and implementation is reusable.\nThe solver-independence of our approach is reflected in the minimal interface that solvers must implement. This interface consists of an abstract type State which represents a state\n1 http://www.g12.csse.unimelb.edu.au/minizinc/challenge2011/\nof the solver (e.g., the variable domains and accumulated constraint propagators) which supports copying. Truly no more is needed for the approach or all of the primitive combinators in Fig. 1, except for base_search and post which require CP-aware operations for querying variable domains, solver status and posting constraints, and possibly interacting with statistics maintained by the solver. Note that there need not be a 1-to-1 correspondence between an implementation of the abstract State type and the solver’s actual state representation; e.g., for solvers based on trailing, recomputation techniques [15] can be used. We have implementations of the interface based on both copying and trailing.\nIn the following we explain our design in detail by means of code implementations of most of the primitive combinators we have covered in the previous section.\n3.1 The Message Protocol\nTo obtain a modular design of search combinators we step away from the idea that the behavior of a search combinator, like the and combinator, forms an indivisible whole; this leaves no room for interaction. The key insight here is that we must identify finer-grained steps, defining how different combinators interact at each node in the search tree. Interleaving these finer-grained steps of different combinators in an appropriate manner yields the composite behavior of the overall search heuristic, where each combinator is able to crosscut the others’ behavior.\nConsidering the diversity of combinators and the fact that not all units of behavior are explicitly present in all of them, designing this protocol of interaction is non-trivial. It requires studying the intended behavior and interaction of combinators to isolate the fine-grained units of behavior and the manner of interaction. The contribution of this section is an elegant and conceptually uniform design that is powerful enough to express all the combinators presented in this article.\nWe present this design in the form of a message protocol. The protocol specifies a set of messages (i.e., an interface with one procedure for each fine-grained step) that have to be\nimplemented by all combinators. In pseudo-code, this protocol for combinators consists of four different messages:\nprotocol combinator start(rootNode); enter(currentNode); exit(currentNode,status); init(parentNode,childNode);\nAll of the message signatures specify one or two search tree nodes as parameters. Each such node keeps track of a solver State and the information associated by combinators to that State. We observe three different access patterns of nodes:\n1. In keeping with the solver independence stipulated above, we will see that most combinators only query and update their associated information and do not access the underlying solver State at all. 2. Restarting-based combinators, like restart and portfolio, copy nodes. This means copying the underlying solver State and all associated information. 3. Finally, selected solver-specific combinators like base_search do perform solverspecific operations on the underlying State, like querying variable domains and posting constraints.\nIn addition to the message signatures, the protocol also stipulates in what order the messages are sent among the combinators (see Fig. 3). While in general a combinator composition is tree-shaped, the processing of any single search tree node p only involves a stack of combinators. For example, given or([and([s1,s2]),and([s3,s4])]), either s1,s2 or s3,s4 are active for p. The picture shows this stack of active combinators on the left. Every combinator in the stack has both a super-combinator above and a sub-combinator below, except for the top and the bottom combinators. The bottom is always a basic heuristic (base_search, prune, assign, or post). The important aspect to take away from the picture is the direction of the four different messages, either top-down or bottom-up.\n3.2 Basic Setup\nBefore we delve into the interesting search combinators, we first present an example implementation of the basic setup consisting of a base search (base_search) and a search engine (dfs). This allows us to express overall search specifications of the form: dfs(base_search(vars,var-select,domain-split)).\nBase Search. We do not provide full details on a base_search combinator, as it is not the focus of this article. However, we will point out the aspects relevant to our protocol.\nIn the enter message, the node’s solver state is propagated. Subsequently, the condition isLeaf(c,vars) checks whether the solver state is unsatisfiable or there are no more variables to assign. If either is the case, the exit status (respectively failure or success) is sent to the parent combinator. For now, the parent combinator is just the search engine, but later we will see how how other combinators can be inserted between the search engine and the base search.\nIf neither is the case, the search branches depending on the variable selection and domain splitting strategies. This involves creating a child node for each branch, determining the variable and value for that child and posting the assignment to the child’s state. Then, the\ntop combinator (i.e., the engine) is asked to initialize the child node. Finally the child node is pushed onto the search queue.\ncombinator base_search(vars,var-select,domain-select) enter(c):\nc.propagate if isLeaf(c,vars)\nparent.exit(leafstatus(c)) pos = ... // from vars based on var-select for each child: // based on domain-select\nval = ... // from values of var based on domain-select child.post(vars[pos]=val) top.init(c,child) queue.push(child)\nNote that, as the base_search combinator is a base combinator, its exit message is immaterial (there is no child heuristic of base_search that could ever call it). The start and init messages are empty. Many variants on and generalizations of the above implementation are possible.\nDepth-first search engine. The engine dfs serves as a pseudo-combinator at the top of a combinator expression heuristic and serves as the heuristic’s immediate parent as well. It maintains the queue of nodes, a stack in this case. The search starts from a given root node by starting the heuristic with that node and then entering it. Each time a node has been processed, new nodes may have been pushed onto the queue. These are popped and entered successively.\ncombinator dfs(heuristic) start(root):\ntop=this heuristic.parent=this queue=new stack() heuristic.start(root) heuristic.enter(root) while not queue.empty\nheuristic.enter(queue.pop())\ninit(n,c): heuristic.init(n,c)\nThe engine’s exit message is empty, the enter message is never called and the init message delegates initialization to the heuristic.\nOther engines may be formulated with different queuing strategies.\n3.3 Combinator Composition\nThe idea of search combinators is to augment a base_search. We illustrate this with a very simple print combinator that prints out every solution as it is found. For simplicity we assume\na solution is just a set of constraint variables vars that is supplied as a parameter. Hence, we obtain the basic search setup with solution printing with:\ndfs(print(vars,base_search(vars,strategy)))\nPrint. The print combinator is parametrized by a set of variables vars and a search combinator child. Implicitly, in a composition, that child’s parent is set to the print instance. The same holds for all following search combinators with one or more children.\nThe only message of interest for print is exit. When the exit status is success, the combinator prints the variables and propagates the message to its parent.\ncombinator print (vars,child) exit(c,status):\nif status==success print c.vars\nparent.exit(c,status)\nThe other messages are omitted. Their behavior is default: they all propagate to the child. The same holds for the omitted messages of following unary combinators.\n3.4 Binary Combinators\nBinary combinators are one step up from unary ones. They combine two complete search heuristics into a composite one. The most basic binary combinator is the binary version of and. For instance, if we need to label two sets of variables, we can do so with\nand(base_search(vars1,...),base_search(vars2,...))\nThe principle shown here easily generalizes to n-ary combinators.\nAnd. The (binary) and combinator has two children, left and right. In order to keep track of what child combinator is handling a particular node, the and combinator associates with every node an inLeft Boolean variable. The local keyword indicates that every node has its own instance of that variable. We denote the instance of the inLeft variable associated with node c as c.inLeft.\nWhen entering a node, it is delegated to the left or right combinator based on inLeft. At the start, the root node is delegated to the left combinator, so its inLeft variable is set to true. The value of inLeft is inherited in init from the current node to its children. Upon a successful exit for left, the leaf node becomes the root of a new subtree that is further handled by the right combinator.\ncombinator and(left,right) { local bool inLeft\nstart(root): root.inLeft=true left.start(root)\nenter(c): if c.inLeft\nleft.enter(c) else\nright.enter(c)\nexit(c,status): if c.inLeft and status==success\nc.inLeft=false right.start(c) right.enter(c)\nelse parent.exit(c,status)\ninit(p,c): c.inLeft=p.inLeft if c.inLeft\nleft.init(p,c) else\nright.init(p,c)\nNote that the right combinator is started repeatedly, once for each leaf node of left. In general, each combinator can be managing multiple subtrees of the search.\nMultiple and combinators may be handling a search node at the same time. For instance in a heuristic of the form and(and(s1,s2),s3), two and combinators are active at the same time. The scoping of the associated variables works in such a way that each and has its own instance of inLeft for each node.\n3.5 Reusable Combinators\nNow we show how a monolithic combinator can be decomposed into more primitive combinators that can be reused for other purposes.\nMonolithic Combinator. We start from the following limitsolutions combinator that prunes the search after cutoff solutions have been found. One new concept is the notion of a global variable associated with a (sub)tree: all descendants of root (implicitly) share the same instance of count. Hence, any update of count by one node is seen by all other nodes in the (sub)tree.\ncombinator limitsolutions(cutoff,child) global int count\nstart(root): root.count = 0 child.start(root)\nenter(c): if count == cutoff\nparent.exit(abort) else\nchild.enter(c)\nexit(c,status): if status==success\nc.count++ parent.exit(c,status)\nDecomposition. We can split up the above limitsolutions combinator into three different combinators: ifthenelse, solutionslimit and prune. They form a directed acyclic graph as depicted in Figure 4 or denoted as an expression with sharing below:\nlimitsolutions(cutoff,s) = ifthenelse(s’,s’,prune)\nwhere\ns’ = solutionslimit(cutoff,s)\nHere, solutionslimit monitors the number of solutions produced by s. If this number reaches the cutoff, then ifthenelse switches to prune, which discards the remaining nodes in the tree.\nWe now elaborate on each of these combinators individually.\nPrune. The prune combinator is a minimal base combinator that immediately exits every node with the abort status. The start message is empty, and the exit and init messages are never called.\ncombinator prune () enter(c):\nparent.exit(c,abort)\nSolutions Count. The solutionslimit combinator below illustrates how statistics gathering combinators are implemented. It implements a sub-protocol of combinator with an extra message eval that queries the current Boolean value:\nprotocol condition extends combinator eval(currentNode);\nIn the case of solutionslimit, the returned Boolean value is whether a particular number (cutoff) of solutions has not yet been reached by its child. For this purpose it maintains the number of solutions found so far in a global variable.\ncondition solutionslimit(cutoff,child) global int count\nstart(root): root.count = 0 child.start(root)\nexit(c,status): if status==success\nc.count++ parent.exit(c,status)\neval(c): return c.count <= cutoff\nIfthenelse. The ifthenelse combinator is parametrized by one condition and two child combinators. It associates with every node whether it is handled by the left child (inLeft); this is the case for the root node. Whenever a node c is entered that is inLeft, the condition is checked. If the condition fails, c becomes the root of a subtree that is further handled by right.\ncombinator ifthenelse(cond,left,right) local bool inLeft\nstart(root): root.inLeft=true left.start(root)\nenter(c): if not c.inLeft\nright.enter(c) else if cond.eval()\nleft.enter(c) else\nc.inLeft=false right.start(c) right.enter(c)\ninit(p,c): c.inLeft=p.inLeft if c.inLeft\nleft.init(p,c) else\nright.init(p,c)\n3.6 Restarting Combinators\nRestarting the search is common to several combinators; the mechanic is illustrated below in the portfolio combinator.\nPortfolio. Like the ifthenelse and and combinators, the portfolio combinator switches between child combinators. Only the logic for switching is more complex. In order to simplify presentation, we again restrict the code to the binary case; the n-ary variant is a straightforward generalization.\nFirstly, portfolio keeps track of a global “reference” count ref of unprocessed nodes to be handled by the s1 child. This count is incremented whenever a new child node is initialized, and decremented whenever a node is entered for actual processing.\nWhen the last node of s1 exits (witnessed by the reference count being 0) and the search was not exhaustive, the search starts over from the root, but now with the s2 child. In order to decide about exhaustiveness, the portfolio combinator registers whether any exit with status abort occurred. At the same time it converts an abort inside s1 into a failure, because the s2 combinator may still perform an exhaustive search and avoid overall nonexhaustiveness. In order to restart from the root, a copy of the root node is made at the start.\nUpon a successful exit, the leaf node becomes the root of a new subtree that is further handled by the s2 combinator.\ncombinator portfolio(s1,s2) global node copy global bool inLeft global bool exhaustive global int ref\nstart(root): copy=root.copy() root.inLeft=true root.exhaustive=true root.ref=1 s1.start(root)\nenter(c): if c.inLeft\nref-s1.enter(c)\nelse s2.enter(c)\nexit(c,status): if not c.inLeft\nparent.exit(c,status) else\nif status==abort status=failure c.exhaustive=false\nif c.ref==0 if c.exhaustive\nparent.exit(c,status) else\ncopy.inLeft=false s2.start(copy) self.enter(copy)\nelse parent.exit(c,status)\ninit(p,c): ref++; if c.inLeft\ns1.init(p,c) else\ns2.init(p,c)"
    }, {
      "heading" : "4 Modular Combinator Implementation",
      "text" : "The message-based combinator approach lends itself well to different implementation strategies. In the following we briefly discuss two diametrically opposed approaches we have explored: dynamic composition (interpretation) and static composition (compilation). Using these different approaches, combinators can be adapted to the implementation choices of existing solvers. Sect. 5 shows that both implementation approaches have competitive performance.\n4.1 Dynamic Composition\nTo support dynamic composition, we have implemented our combinators as C++ classes whose objects can be allocated and composed into a search specification at runtime. The protocol events correspond to virtual method calls between these objects. For the delegation mechanism from one object to another, we explicitly encode a form of dynamic inheritance called open recursion or mixin inheritance [4]. In contrast to the OOP inheritance built into C++ and Java, this mixin inheritance provides two essential abilities: 1) to determine the inheritance graph at runtime and 2) to use multiple copies of the same combinator class at different points in the inheritance graph. In contrast, C++’s built-in static inheritance provides neither.\nThe C++ library currently builds on top of the Gecode constraint solver [23]. However, the solver is accessed through a layer of abstraction that is easily adapted to other solvers (e.g., we have a prototype interface to the Gurobi MIP solver). The complete library weighs in at around 2500 lines of code, which is even less than Gecode’s native search and branching components.\n4.2 Static Composition\nIn a second approach, also on top of Gecode, we statically compile a search specification to a tight C++ loop. Again, every combinator is a separate module independent of other combinator modules. A combinator module now does not directly implement the combinator’s behavior. Instead it implements a code generator (in Haskell), which in turn produces the C++ code with the expected behavior.\nHence, our search language compiler parses a search specification, and composes (in mixin-style) the corresponding code generators. Then it runs the composite code generator according to the message protocol. The code generators produce appropriate C++ code fragments for the different messages, which are combined according to the protocol into the monolithic C++ loop. This C++ code is further post-processed by the C++ compiler to yield a highly optimized executable.\nAs for dynamic composition, the mixin approach is crucial, allowing us to add more combinators without touching the existing ones. At the same time we obtain with the press of a button several 1000 lines of custom low-level code for the composition of just a few combinators. In contrast, the development cost of hand crafted code is prohibitive.\nA compromise between the above two approaches, itself static, is to employ the built-in mixin mechanism (also called traits) available in object-oriented languages such as Scala [6] to compose combinators. A dynamic alternative is to generate the combinator implementations using dynamic compilation techniques, for instance using the LLVM (Low Level Virtual Machine) framework. These options remain to be explored.\n4.3 MiniZinc with Combinators\nAs a proof of concept and platform for experiments, we have integrated search combinators into a complete MiniZinc toolchain, comprising a pre-compiler and a FlatZinc interpreter.\nThe pre-compiler is necessary to support arbitrary expressions in annotations, such as the condition expressions for an ifthenelse. The expressions are translated into standard MiniZinc annotations that are understood by the FlatZinc interpreter. User-defined variables have type-inst svar int and can be introduced using the standard MiniZinc let construct. The annotation construct of MiniZinc has been extended to support simple function definitions. The following example shows a MiniZinc version of the restart-based branchand-bound heuristic from Sect. 2.3:\nannotation limit(var bool: c, ann: s) = ifthenelse(c,s,prune);\nannotation once(ann: s) = limit(solutions < 1, s);\nannotation rbab(var int: obj, ann: s) = let { svar int: best = MAXINT } in restart(true, and([\npost(obj < best), once(s), assign(best,obj)]));\nsolve ::rbab(x,int_search(y,input_order,assign_lb)) satisfy;\nThe pre-compiler translates this code as follows:\nsolve :: sh_let(sh_letvar(\"best\"), sh_int(MAXINT), sh_restart(sh_cond_true, sh_and([\nsh_post_succeed(sh_cond_lt(sh_intvar(objective), sh_letvar(\"best\"))),\nsh_let(sh_letvar(\"solutioncount\"), 0, sh_ifthenelse(sh_cond_lt(sh_letvar(\"solutioncount\"),\nsh_int(1)), sh_solutioncount(sh_letvar(\"solutioncount\"),\nsh_int_search(x, sh_var_input_order, sh_val_assign_lb)),\nsh_prune)), sh_assign(sh_letvar(\"best\"), sh_intvar(objective))]))) satisfy;\nAll literals are quoted (e.g. sh_int(1)), user-defined search variables are turned into quoted strings (lv(\"best\")), expressions like obj < best are translated into annotation terms (sh_cond_lt . . . ), and statistics are made explicit, introducing search variables and special combinators (sh_solutioncount). The result of the pre-compilation is valid, well-typed MiniZinc, which is then passed through the standard mzn2fzn translator to produce FlatZinc ready for solving. We intend to incorporate the translations done by the precompiler into the standard mzn2fzn in the future.\nWe extended the Gecode FlatZinc interpreter to parse the search combinator annotation and construct the corresponding heuristic using the Dynamic Composition approach described above. The three tools, pre-compiler, mzn2fzn, and the modified FlatZinc interpreter thus form a complete toolchain for solving MiniZinc models using search combinators. The source code including examples can be downloaded from http://www.gecode.org/flatzinc.html."
    }, {
      "heading" : "5 Experiments",
      "text" : "This section evaluates the performance of our two implementations. It establishes that a search heuristic specified using combinators is competitive with a custom implementation of the same heuristic, exploring exactly the same tree.\nSect. 3.1 introduced a message protocol that defines the communication between the different combinators for one node of the search tree. Any overhead of a combinator-based implementation must therefore come from the processing of each node using this protocol. All combinators discussed earlier process each message of the protocol in constant time (except for the base_search combinators, of course). Hence, we expect at most a constant overhead per node compared to a native implementation of the heuristic.\nIn the following, two sets of experiments confirm this expectation. The first set consists of artificial benchmarks designed to expose the overhead per node. The second set consists of realistic combinatorial problems with complex search strategies.\nThe experiments were run on a 2.26 GHz Intel Core 2 Duo running Mac OS X. The results are the averages of 10 runs, with a coefficient of deviation less than 1.5%.\nStress Test. The first set of experiments measures the overhead of calling a single combinator during search. We ran a complete search of a tree generated by 7 variables with domain\n{0, . . . ,6} and no constraints (1 647 085 nodes). To measure the overhead, we constructed a basic search heuristic s and a stack of n combinators:\nportfolio([portfolio([. . .portfolio([s,prune]) . . . ,prune]),prune]) where n ranges from 0 to 20 (realistic combinator stacks, such as those from the examples in this article, are usually not deeper than 10). The numbers in the following table report the runtime with respect to using the plain heuristic s, for both the static and the dynamic approach:\nn 1 2 5 10 20 static % 106.6 107.7 112.0 148.3 157.5 dynamic % 107.3 117.6 145.2 192.6 260.9\nA single combinator generates an overhead of around 7%, and 10 combinators add 50% for the static and 90% for the dynamic approach. In absolute runtime, however, this translates to an overhead of around 17 ms (70 ms) per million nodes and combinator for the static (dynamic) approach. Note that this is a worst-case experiment, since there is no constraint propagation and almost all the time is spent in the combinators.\nBenchmarks. The second set of experiments shows that in practice, this overhead is dwarfed by the cost of constraint propagation and backtracking. Note that the experiments are not supposed to demonstrate the best possible search heuristics for the given problems, but that a search heuristic implemented using combinators is just as efficient as a native implementation.\nFig. 5 compares Gecode’s optimization search engines with branch-and-bound implemented using combinators. On the well-known Golomb Rulers problem, both dynamic combinators and native Gecode are slightly slower than static combinators. Native Gecode uses dynamically combined search heuristics, but is much less expressive. That is why the static approach with its specialization yields better results.\nOn the radiotherapy problem (see Sect. 2.3), the dynamic combinators show an overhead of 6–11%. For native Gecode, exh_once must be implemented as a nested search, which performs similarly to the dynamic combinators. However, in instances 5 and 6, the compiled combinators lose their advantage over native Gecode. This is due to the processing of exh_once: As soon as it is finished, the combinator approach processes all nodes of the exh_once tree that are still in the search queue, which are now pruned by exh_once. The native Gecode implementation simply discards the tree. We will investigate how to incorporate this optimization into the combinator approach.\nThe job shop scheduling examples, using the combination of branch-and-bound and discrepancy limit discussed in Sect. 2.3, show similar behavior. In ABZ1-5 and mt10, the interpreted combinators show much less overhead than in the short-running instances. This is due to more expensive propagation and backtracking in these instances, reducing the relative overhead of executing the combinators.\nIn summary, the experiments show that the expressiveness and flexibility of a rich combinator-based search language can be achieved without any runtime overhead in the case of the static approach, and little overhead for the dynamic version."
    }, {
      "heading" : "6 Related Work",
      "text" : "This section explores and discusses previous work that is closely related to search combinators as presented in this article."
    }, {
      "heading" : "6.1 MCP",
      "text" : "This work directly extends our earlier work on Monadic Constraint Programming (MCP) [20]. MCP introduces stackable search transformers, which are a simple form of search combinators, but only provide a much more limited and low level form of search control. In trying to overcome its limitations we arrived at search combinators.\n6.2 Constraint Logic Programming\nConstraint logic programming languages such as ECLiPSe [7] and SICStus Prolog [25] provide programmable search via the built-in search of the paradigm, allowing the user to define goals in terms of conjunctive or disjunctive sub-goals.\nProlog’s limitation is that it does not permit cross-cutting between goals. For instance, disjunctions inside goals are too well encapsulated to observe them or interfere with them from outside that goal. Hence, combinators that inject additional behavior in disjunctions, i.e. to observe and/or prune the number of branches, cannot be expressed in a modular way. In contrast, cross-cutting is a crucial feature of our combinator approach, where a combinator higher up in the stack can interfere with a sub-combinator, while remaining fully compositional. In summary, apart from conjunction and disjunction, Prolog’s goalbased heuristics cannot be combined arbitrarily.\nECLiPSe copes with this limitation by combining a limited number of search heuristics into a monolithic search/6 predicate. With various parameters the user controls which of the heuristics is enabled (e.g., depth-bounded, node-bounded or limited discrepancy search). A fixed number of compositions are supported, such as changing strategy when the depth bound finishes. The labeling itself is user programmable. If a user is not happy with the set of supported heuristics in search/6, he has to program his own from scratch.\nIBM ILOG CP Optimizer [5] supports Prolog-style goals in C++ [16], and like Prolog goals, these do not support cross-cutting.\n6.3 The Comet Language\nThe Comet [27] system features fully programmable search [28], built upon the basic concept of continuations, which make it easy to capture the state of the solver and write nondeterministic code.\nThe Comet library provides abstractions like the non-deterministic primitives try and tryall that split the search specification in two (orthogonal) parts: 1) the specification of the search tree which corresponds to our to our base_search heuristics, and 2) the exploration of that search tree by means of a search controller. In terms of our approach, the search controller determines both the queueing strategy and the behavior of the search heuristic (minus the base search) within a single entity. In other words, it defines what to do when starting or ending a search, failing, or adding a new choice.\nComplex heuristics can be constructed as custom controllers, either by inheriting from existing controllers or implementing them from scratch.\nAlbeit at a different level of abstraction (e.g., compare the Comet definition of depthbounded search in Figure 6 to the combinator definition dbs(n,s) ≡ limit(depth ≤ n,s)), search controllers are quite similar to combinators as presented in this article. However, there is one essential difference. Our combinators are meant to be compositional, whereas search controllers are not. This difference in spirit is clearly reflected in 1) the design of the interface and its associated protocol, and 2) the instances:\n1. The design of search controllers is simpler than that of search combinators because it does not take compositionality into account. While many of the messages in the two approaches are similar in spirit, the search combinator approach also stipulates the flow of messages within a search combinator composition. Notably, while most of the messages propagate top-down through a combinator stack, it is vital to compositionality that the exit message proceeds in a bottom-up manner. For instance, this bottom-up flow enables the inner and combinator in the composition and(and(s1,s2),s3) to intercept leaf nodes of s1 and start s2 before its parent starts s3. The other way around would clearly exhibit an undesirable semantics. In Comet, this compositional protocol is entirely absent. All messages are directed at the single search controller. 2. In terms of instantiation, because of their compositional nature, we promote many “small” combinator instances that each capture a single primitive feature. This approach provides us with a high-level modeling language for search, as the primitive combinators are conveniently assembled into many different search heuristics. In contrast, all Comet search controller instances we are aware of2 are essentially monolithic implementations of a particular search heuristic; none of them takes other search controllers as arguments. Through a common abstract base class the instances share some basic infrastructure, but to implement a new search controller one basically starts from scratch.\nThe fact that search controllers have not been designed with compositionality in mind obviously does not mean that compositionality cannot be achieved in Comet. On the contrary, we believe that it is most easily achieved by integrating search controllers with the compositional design of our search combinators. In fact, because of Comet’s powerful primitives for non-determinism, this would lead to a particularly elegant implementation.\n2 i.e., those published in papers and shipped with the Comet library.\n6.4 Other Systems\nThe Salsa [12] language is an imperative domain-specific language for implementing search algorithms on top of constraint solvers. Its center of focus is a node in the search process. Programmers can write custom Choice strategies for generating next nodes from the current one; Salsa provides a regular-expression-like language for combining these Choices into more complex ones. In addition, Salsa can run custom procedures at the exits of each node, right after visiting it. We believe that Salsa’s Choice construct is orthogonal to our approach and could be incorporated. Custom exit procedures show similarity to combinators, but no support is provided for arbitrary composition.\nOz [26] was the first language to truly separate the definition of the constraint model from the exploration strategy [22]. Computation spaces capture the solver state and the possible choices. Strategies such as DFS, BFS, LDS, Branch and Bound and Best First Search are implemented by a combination of copying and recomputation of computation spaces. The strategies are monolithic, there is no notion of search combinators.\nZinc/MiniZinc [13,14] lets the user specify search in its annotation language. There is a proposal for a more expressive search language for MiniZinc [19], but it is limited to basic variable ordering and domain splitting strategies. For Zinc, a language extension is available for implementing variable selection and domain splitting [17] but again it does not address more than basic search.\n6.5 Autonomous Search\nAutonomous search (AS) [8] addresses the challenge of providing complex applicationtailored search heuristics in a different way. Rather than leaving the specification and tuning of search heuristics to the programmer, AS promotes systems that autonomously self-tune their performance while solving problems. Hence, while search combinators make writing\nsearch heuristics easier, AS takes it out of the hands of the programmer altogether. Wellknown instances of this approach are Impact Based Search [18] or the weighted degree heuristic [3].\nAS has advantages for 1) smaller problems where it produces a decent heuristic without programmer investment, and for 2) novice users who don’t know how to obtain a decent heuristic. However, loss of programmer control is a liability for hard problems where AS can be ineffective and often only expert knowledge makes the difference."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have shown how combinators provide a powerful high-level language for modeling complex search heuristics. To make this approach useful in practice, we devised an architecture in which the modularity of the language is matched by the modularity of the implementation. This relieves system developers from a high implementation cost and yet, as our experiments show, imposes no runtime penalty.\nFor future work, parallel search on multi-core hardware fits perfectly in our combinator framework. We have already performed a number of preliminary experiments and will further explore the benefits of search combinators in a parallel setting. We will also explore potential optimizations (such as the short-circuit of exh_once from Sect. 5) and different compilation strategies (e.g., combining the static and dynamic approaches from Sect. 4).\nIn addition we consider to apply search combinators in other problem domains like Mixed Integer Programming (MIP) and A∗ where search strategies have a major impact on performance and no dominant default search exists. Finally, combinators need not necessarily be heuristics that control the search. They may also monitor search, e.g., by gathering statistics or visualizing the search tree.\nAcknowledgements NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council. This work was partially supported by Asian Office of Aerospace Research and Development grant 10-4123."
    } ],
    "references" : [ {
      "title" : "CP and IP approaches to cancer radiotherapy delivery optimization",
      "author" : [ "D. Baatar", "N. Boland", "S. Brand", "P.J. Stuckey" ],
      "venue" : "Constraints 16(2), 173–194",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Boosting systematic search by weighting constraints",
      "author" : [ "F. Boussemart", "F. Hemery", "C. Lecoutre", "L. Sais" ],
      "venue" : "ECAI’04, pp. 146–150",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Boosting systematic search by weighting constraints",
      "author" : [ "F. Boussemart", "F. Hemery", "C. Lecoutre", "L. Sais" ],
      "venue" : "R.L. de Mántaras, L. Saitta (eds.) ECAI, pp. 146–150. IOS Press",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A denotational semantics of inheritance",
      "author" : [ "W.R. Cook" ],
      "venue" : "Ph.D. thesis, Brown University",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A core calculus for Scala type checking",
      "author" : [ "V. Cremet", "F. Garillot", "S. Lenglet", "M. Odersky" ],
      "venue" : "R. Kralovic, P. Urzyczyn (eds.) MFCS, LNCS, vol. 4162, pp. 1–23. Springer, Heidelberg",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Autonomous Search",
      "author" : [ "Y. Hamadi", "E. Monfroy", "Saubion", "F. (eds." ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Limited discrepancy search",
      "author" : [ "W.D. Harvey", "M.L. Ginsberg" ],
      "venue" : "IJCAI, pp. 607–613",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Aspect-oriented programming",
      "author" : [ "G. Kiczales", "J. Lamping", "A. Menhdhekar", "C. Maeda", "C. Lopes", "J. Loingtier", "J. Irwin" ],
      "venue" : "ECOOP’97, pp. 220–242",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Depth-first iterative-deepening: an optimal admissible tree search",
      "author" : [ "R.E. Korf" ],
      "venue" : "Artif. Intell. 27, 97–109",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "SALSA: A language for search algorithms",
      "author" : [ "F. Laburthe", "Y. Caseau" ],
      "venue" : "Constraints 7(3), 255–288",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The design of the Zinc modelling language",
      "author" : [ "K. Marriott", "N. Nethercote", "R. Rafeh", "P. Stuckey", "M. Garcia de la Banda", "M. Wallace" ],
      "venue" : "Constraints 13(3), 229–267",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "MiniZinc: Towards a standard CP modelling language",
      "author" : [ "N. Nethercote", "P.J. Stuckey", "R. Becket", "S. Brand", "G.J. Duck", "G. Tack" ],
      "venue" : "C. Bessiere (ed.) CP, LNCS, vol. 4741, pp. 529–543. Springer, Heidelberg",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Search procedures and parallelism in constraint programming",
      "author" : [ "L. Perron" ],
      "venue" : "J. Jaffar (ed.) CP, LNCS, vol. 1713, pp. 346–360. Springer, Heidelberg",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A C++ implementation of CLP",
      "author" : [ "J.F. Puget" ],
      "venue" : "Proceedings of the Second Singapore International Conference on Intelligent Systems (SPICIS), pp. B256–B261",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Adding search to zinc",
      "author" : [ "R. Rafeh", "K. Marriott", "M.G. de la Banda", "N. Nethercote", "M. Wallace" ],
      "venue" : "P.J. Stuckey (ed.) CP, LNCS, vol. 5202, pp. 624–629. Springer",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Impact-based search strategies for constraint programming",
      "author" : [ "P. Refalo" ],
      "venue" : "M. Wallace (ed.) CP, LNCS, vol. 3258, pp. 557–571. Springer, Heidelberg",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Towards a lightweight standard search language",
      "author" : [ "H. Samulowitz", "G. Tack", "J. Fischer", "M. Wallace", "P. Stuckey" ],
      "venue" : "ModRef",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Monadic constraint programming",
      "author" : [ "T. Schrijvers", "P.J. Stuckey", "P. Wadler" ],
      "venue" : "Journal of Functional Programming 19(6), 663–697",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Search combinators",
      "author" : [ "T. Schrijvers", "G. Tack", "P. Wuille", "H. Samulowitz", "P.J. Stuckey" ],
      "venue" : "J.H. Lee (ed.) CP’11, LNCS, vol. 6876, pp. 774–788. Springer",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Programming constraint inference engines",
      "author" : [ "C. Schulte" ],
      "venue" : "G. Smolka (ed.) CP, LNCS, vol. 1330, pp. 519–533. Springer, Heidelberg",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Gecode, the generic constraint development environment",
      "author" : [ "C Schulte" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Dichotomic search protocols for constrained optimization",
      "author" : [ "M. Sellmann", "S. Kadioglu" ],
      "venue" : "P.J. Stuckey (ed.) CP, LNCS, vol. 5202, pp. 251–265. Springer, Heidelberg",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The Oz programming model",
      "author" : [ "G. Smolka" ],
      "venue" : "J. van Leeuwen (ed.) Computer Science Today, LNCS, vol. 1000, pp. 324–343. Springer, Heidelberg",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Constraint-Based Local Search",
      "author" : [ "P. Van Hentenryck", "L. Michel" ],
      "venue" : "MIT Press",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Nondeterministic control for hybrid search",
      "author" : [ "P. Van Hentenryck", "L. Michel" ],
      "venue" : "Constraints 11(4), 353–373",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Search and strategies in OPL",
      "author" : [ "P. Van Hentenryck", "L. Perron", "J.F. Puget" ],
      "venue" : "ACM TOCL 1(2), 285–315",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "A range of high-level modeling languages, such as Zinc [13], OPL [29] or Comet [27], enable quick development and exploration of problem models.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : "A range of high-level modeling languages, such as Zinc [13], OPL [29] or Comet [27], enable quick development and exploration of problem models.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 23,
      "context" : "A range of high-level modeling languages, such as Zinc [13], OPL [29] or Comet [27], enable quick development and exploration of problem models.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "MiniZinc [14], Comet [27], Gecode [23], or ECLiPSe [7] provide a small set of predefined heuristics “off the shelf”.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 23,
      "context" : "MiniZinc [14], Comet [27], Gecode [23], or ECLiPSe [7] provide a small set of predefined heuristics “off the shelf”.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "MiniZinc [14], Comet [27], Gecode [23], or ECLiPSe [7] provide a small set of predefined heuristics “off the shelf”.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "We overcome the modularity challenge by implementing the primitives of our search language as mixin components [4].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "As in Aspect-Oriented Programming [10], mixin components neatly encapsulate the cross-cutting behavior of primitive search concepts, which are highly entangled in conventional approaches.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "This article is an extended version of a paper [21] that appeared in the proceedings of the 17th International Conference on Principles and Practice of Constraint Programming (CP) 2011.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : "That paper further developed the ideas laid out in our earlier paper [19], which was presented at ModRef 2010.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : "The concrete syntax we chose for presentation uses simple nested terms, which makes it compatible with the annotation language of MiniZinc [14].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "– firstfail: select the variable with the smallest current domain, – smallest: select the variable which can take the smallest possible value, – domwdeg [2]: select the variable with smallest ratio of size of current domain and number of failures the variable has been involved in, and – impact [18]: select the variable that will (based on past experience) reduce the raw search space of the problem the most.",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "– firstfail: select the variable with the smallest current domain, – smallest: select the variable which can take the smallest possible value, – domwdeg [2]: select the variable with smallest ratio of size of current domain and number of failures the variable has been involved in, and – impact [18]: select the variable that will (based on past experience) reduce the raw search space of the problem the most.",
      "startOffset" : 295,
      "endOffset" : 299
    }, {
      "referenceID" : 15,
      "context" : "This heuristic can for example be used to initialize the widely applied Impact heuristic [18].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "Radiotherapy treatment planning: The following search heuristic can be used to solve radiotherapy treatment planning problems [1].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "Limited discrepancy search [9] with an upper limit of l discrepancies for an underlying search s.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "The original LDS [9] visits the nodes in a specific order.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "The following is a combination of branch-and-bound and limited discrepancy search for solving job shop scheduling problems, as described in [9].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : "Iterative deepening [11] for an underlying search s is a particular instance of the more general pattern of restarting with an updated bound, which we have already seen in the geom_restart example.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 21,
      "context" : "Dichotomic search [24] solves an optimization problem by repeatedly partitioning the interval in which the possible optimal solution can lie.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : ", for solvers based on trailing, recomputation techniques [15] can be used.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "For the delegation mechanism from one object to another, we explicitly encode a form of dynamic inheritance called open recursion or mixin inheritance [4].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "The C++ library currently builds on top of the Gecode constraint solver [23].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "A compromise between the above two approaches, itself static, is to employ the built-in mixin mechanism (also called traits) available in object-oriented languages such as Scala [6] to compose combinators.",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "This work directly extends our earlier work on Monadic Constraint Programming (MCP) [20].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "IBM ILOG CP Optimizer [5] supports Prolog-style goals in C++ [16], and like Prolog goals, these do not support cross-cutting.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "The Comet [27] system features fully programmable search [28], built upon the basic concept of continuations, which make it easy to capture the state of the solver and write nondeterministic code.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "The Comet [27] system features fully programmable search [28], built upon the basic concept of continuations, which make it easy to capture the state of the solver and write nondeterministic code.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "The Salsa [12] language is an imperative domain-specific language for implementing search algorithms on top of constraint solvers.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 22,
      "context" : "Oz [26] was the first language to truly separate the definition of the constraint model from the exploration strategy [22].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "Oz [26] was the first language to truly separate the definition of the constraint model from the exploration strategy [22].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "Zinc/MiniZinc [13,14] lets the user specify search in its annotation language.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "Zinc/MiniZinc [13,14] lets the user specify search in its annotation language.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : "There is a proposal for a more expressive search language for MiniZinc [19], but it is limited to basic variable ordering and domain splitting strategies.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "For Zinc, a language extension is available for implementing variable selection and domain splitting [17] but again it does not address more than basic search.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "Autonomous search (AS) [8] addresses the challenge of providing complex applicationtailored search heuristics in a different way.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "Wellknown instances of this approach are Impact Based Search [18] or the weighted degree heuristic [3].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "Wellknown instances of this approach are Impact Based Search [18] or the weighted degree heuristic [3].",
      "startOffset" : 99,
      "endOffset" : 102
    } ],
    "year" : 2012,
    "abstractText" : "The ability to model search in a constraint solver can be an essential asset for solving combinatorial problems. However, existing infrastructure for defining search heuristics is often inadequate. Either modeling capabilities are extremely limited or users are faced with a general-purpose programming language whose features are not tailored towards writing search heuristics. As a result, major improvements in performance may remain unexplored. This article introduces search combinators, a lightweight and solver-independent method that bridges the gap between a conceptually simple modeling language for search (highlevel, functional and naturally compositional) and an efficient implementation (low-level, imperative and highly non-modular). By allowing the user to define application-tailored search strategies from a small set of primitives, search combinators effectively provide a rich domain-specific language (DSL) for modeling search to the user. Remarkably, this DSL comes at a low implementation cost to the developer of a constraint solver. The article discusses two modular implementation approaches and shows, by empirical evaluation, that search combinators can be implemented without overhead compared to a native, direct implementation in a constraint solver. Tom Schrijvers · Pieter Wuille Universiteit Gent, Belgium E-mail: {tom.schrijvers,pieter.wuille}@ugent.be Guido Tack Monash University, Victoria, Australia E-mail: guido.tack@monash.edu Pieter Wuille Katholieke Universiteit Leuven, Belgium E-mail: pieter.wuille@cs.kuleuven.be Horst Samulowitz IBM Research, New York, USA E-mail: samulowitz@us.ibm.com Peter J. Stuckey National ICT Australia (NICTA) and University of Melbourne, Victoria, Australia E-mail: pjs@cs.mu.oz.au ar X iv :1 20 3. 10 95 v1 [ cs .A I] 6 M ar 2 01 2 2 Tom Schrijvers et al.",
    "creator" : "LaTeX with hyperref package"
  }
}