{
  "name" : "1512.01525.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning the Semantics of Manipulation Action",
    "authors" : [ "Yezhou Yang", "Yiannis Aloimonos", "Cornelia Fermüller", "Eren Erdal Aksoy" ],
    "emails" : [ "fer}@umiacs.umd.edu", "eren.aksoy@kit.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Autonomous robots will need to learn the actions that humans perform. They will need to recognize these actions when they see them and they will need to perform these actions themselves. This requires a formal system to represent the action semantics. This representation needs to store the semantic information about the actions, be encoded in a machine readable language, and inherently be in a programmable fashion in order to enable reasoning beyond observation. A formal representation of this kind has a variety of other applications such as intelligent manufacturing, human\nrobot collaboration, action planning and policy design, etc.\nIn this paper, we are concerned with manipulation actions, that is actions performed by agents (humans or robots) on objects, resulting in some physical change of the object. However most of the current AI systems require manually defined semantic rules. In this work, we propose a computational linguistics framework, which is based on probabilistic semantic parsing with Combinatory Categorial Grammar (CCG), to learn manipulation action semantics (lexicon entries) from annotations. We later show that this learned lexicon is able to make our system reason about manipulation action goals beyond just observation. Thus the intelligent system can not only imitate human movements, but also imitate action goals.\nUnderstanding actions by observation and executing them are generally considered as dual problems for intelligent agents. The sensori-motor bridge connecting the two tasks is essential, and a great amount of attention in AI, Robotics as well as Neurophysiology has been devoted to investigating it. Experiments conducted on primates have discovered that certain neurons, the so-called mirror neurons, fire during both observation and execution of identical manipulation tasks (Rizzolatti et al., 2001; Gazzola et al., 2007). This suggests that the same process is involved in both the observation and execution of actions. From a functionalist point of view, such a process should be able to first build up a semantic structure from observations, and then the decomposition of that same structure should occur when the intelligent agent executes commands.\nAdditionally, studies in linguistics (Steedman, 2002) suggest that the language faculty develops in humans as a direct adaptation of a more primitive apparatus for planning goal-directed action in the world by composing affordances of tools and consequences of actions. It is this more primitive\nar X\niv :1\n51 2.\n01 52\n5v 1\n[ cs\n.R O\n] 4\nD ec\n2 01\n5\napparatus that is our major interest in this paper. Such an apparatus is composed of a “syntax part” and a “semantic part”. In the syntax part, every linguistic element is categorized as either a function or a basic type, and is associated with a syntactic category which either identifies it as a function or a basic type. In the semantic part, a semantic translation is attached following the syntactic category explicitly.\nCombinatory Categorial Grammar (CCG) introduced by (Steedman, 2000) is a theory that can be used to represent such structures with a small set of combinators such as functional application and type-raising. What do we gain though from such a formal description of action? This is similar to asking what one gains from a formal description of language as a generative system. Chomskys contribution to language research was exactly this: the formal description of language through the formulation of the Generative and Transformational Grammar (Chomsky, 1957). It revolutionized language research opening up new roads for the computational analysis of language, providing researchers with common, generative language structures and syntactic operations, on which language analysis tools were built. A grammar for action would contribute to providing a common framework of the syntax and semantics of action, so that basic tools for action understanding can be built, tools that researchers can use when developing action interpretation systems, without having to start development from scratch. The same tools can be used by robots to execute actions.\nIn this paper, we propose an approach for learning the semantic meaning of manipulation action through a probabilistic semantic parsing framework based on CCG theory. For example, we want to learn from an annotated training action corpus that the action “Cut” is a function which has two arguments: a subject and a patient. Also, the action consequence of “Cut” is a separation of the patient. Using formal logic representation, our system will learn the semantic representations of “Cut”:\nCut :=(AP\\NP )/NP : λx.λy.cut(x, y) → divided(y)\nHere cut(x, y) is a primitive function. We will further introduce the representation in Sec. 3. Since our action representation is in a common calculus form, it enables naturally further logical reasoning beyond visual observation.\nThe advantage of our approach is twofold: 1) Learning semantic representations from annotations helps an intelligent agent to enrich automatically its own knowledge about actions; 2) The formal logic representation of the action could be used to infer the object-wise consequence after a certain manipulation, and can also be used to plan a set of actions to reach a certain action goal. We further validate our approach on a large publicly available manipulation action dataset (MANIAC) from (Aksoy et al., 2014), achieving promising experimental results. Moreover, we believe that our work, even though it only considers the domain of manipulation actions, is also a promising example of a more closely intertwined computer vision and computational linguistics system. The diagram in Fig.1 depicts the framework of the system."
    }, {
      "heading" : "2 Related Works",
      "text" : "Reasoning beyond appearance: The very small number of works in computer vision, which aim to reason beyond appearance models, are also related to this paper. (Xie et al., 2013) proposed that beyond state-of-the-art computer vision techniques, we could possibly infer implicit information (such as functional objects) from video, and they call them “Dark Matter” and “Dark Energy”. (Yang et al., 2013) used stochastic tracking and graphcut based segmentation to infer manipulation consequences beyond appearance. (Joo et al., 2014) used a ranking SVM to predict the persuasive motivation (or the intention) of the photographer who captured an image. More recently, (Pirsiavash et al., 2014) seeks to infer the motivation of the person in the image by mining knowledge stored in\na large corpus using natural language processing techniques. Different from these fairly general investigations about reasoning beyond appearance, our paper seeks to learn manipulation actions semantics in logic forms through CCG, and further infer hidden action consequences beyond appearance through reasoning.\nAction Recognition and Understanding: Human activity recognition and understanding has been studied heavily in Computer Vision recently, and there is a large range of applications for this work in areas like human-computer interactions, biometrics, and video surveillance. Both visual recognition methods, and the non-visual description methods using motion capture systems have been used. A few good surveys of the former can be found in (Moeslund et al., 2006) and (Turaga et al., 2008). Most of the focus has been on recognizing single human actions like walking, jumping, or running etc. (Ben-Arie et al., 2002; Yilmaz and Shah, 2005). Approaches to more complex actions have employed parametric approaches, such as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual frames e.g. (Saisan et al., 2001; Chaudhry et al., 2009). More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation.\nThere also have been many syntactic approaches to human activity recognition which used the concept of context-free grammars, because such grammars provide a sound theoretical basis for modeling structured processes. Tracing back to the middle 90’s, (Brand, 1996) used a grammar to recognize disassembly tasks that contain hand manipulations. (Ryoo and Aggarwal, 2006) used the context-free grammar formalism to recognize composite human activities and multi-person interactions. It is a two level hierarchical approach where the lower-levels are composed of HMMs and Bayesian Networks while the higher-level interactions are modeled by CFGs. To deal with errors from low-level processes such as tracking, stochastic grammars such as stochastic CFGs were also used (Ivanov and Bobick, 2000; Moore and Essa, 2002). More recently, (Kuehne et al., 2014) proposed to model goal-directed human activities using Hidden Markov Models and treat subactions just like words in speech. These works\nproved that grammar based approaches are practical in activity recognition systems, and shed insight onto human manipulation action understanding. However, as mentioned, thinking about manipulation actions solely from the viewpoint of recognition has obvious limitations. In this work we adopt principles from CFG based activity recognition systems, with extensions to a CCG grammar that accommodates not only the hierarchical structure of human activity but also action semantics representations. It enables the system to serve as the core parsing engine for both manipulation action recognition and execution.\nManipulation Action Grammar: As mentioned before, (Chomsky, 1993) suggested that a minimalist generative grammar, similar to the one of human language, also exists for action understanding and execution. The works closest related to this paper are (Pastra and Aloimonos, 2012; Summers-Stay et al., 2013; Guha et al., 2013). (Pastra and Aloimonos, 2012) first discussed a Chomskyan grammar for understanding complex actions as a theoretical concept, and (SummersStay et al., 2013) provided an implementation of such a grammar using as perceptual input only objects. More recently, (Yang et al., 2014) proposed a set of context-free grammar rules for manipulation action understanding, and (Yang et al., 2015) applied it on unconstrained instructional videos. However, these approaches only consider the syntactic structure of manipulation actions without coupling semantic rules using λ expressions, which limits the capability of doing reasoning and prediction.\nCombinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). (Mooney, 2008) presented a framework of grounded language acquisition: the interpretation of language entities into semantically informed structures in the context of perception and actuation. The concept has been applied successfully in tasks such as robot navigation (Matuszek et al., 2011), forklift operation (Tellex et al., 2014) and of human-robot interaction (Matuszek et al., 2014). In this work, instead of grounding natural language sentences directly, we ground information obtained from visual perception into seman-\ntically informed structures, specifically in the domain of manipulation actions."
    }, {
      "heading" : "3 A CCG Framework for Manipulation Actions",
      "text" : "Before we dive into the semantic parsing of manipulation actions, a brief introduction to the Combinatory Categorial Grammar framework in Linguistics is necessary. We will only introduce related concepts and formalisms. For a complete background reading, we would like to refer readers to (Steedman, 2000). We will first give a brief introduction to CCG and then introduce a fundamental combinator, i.e., functional application. The introduction is followed by examples to show how the combinator is applied to parse actions."
    }, {
      "heading" : "3.1 Manipulation Action Semantics",
      "text" : "The semantic expression in our representation of manipulation actions uses a typed λ-calculus language. The formal system has two basic types: entities and functions. Entities in manipulation actions are Objects or Hands, and functions are the Actions. Our lambda-calculus expressions are formed from the following items:\nConstants: Constants can be either entities or functions. For example, Knife is an entity (i.e., it is of type N) and Cucumber is an entity too (i.e., it is of type N). Cut is an action function that maps entities to entities. When the event Knife Cut Cucumber happened, the expression cut(Knife, Cucumber) returns an entity of type AP, aka. Action Phrase. Constants like divided are status functions that map entities to truth values. The expression divided(cucumber) returns a true value after the event (Knife Cut Cucumber) happened.\nLogical connectors: The λ-calculus expression has logical connectors like conjunction (∧), disjunction (∨), negation(¬) and implication(→).\nFor example, the expression\nconnected(tomato, cucumber)∧ divided(tomato) ∧ divided(cucumber)\nrepresents the joint status that the sliced tomato merged with the sliced cucumber. It can be regarded as a simplified goal status for “making a cucumber tomato salad”. The expression ¬connected(spoon, bowl) represents the status after the spoon finished stirring the bowl.\nλx.cut(x, cucumber) → divided(cucumber)\nrepresents that if the cucumber is cut by x, then the status of the cucumber is divided. λ expressions: lambda expressions represent functions with unknown arguments. For example, λx.cut(knife, x) is a function from entities to entities, which is of type NP after any entities of type N that is cut by knife."
    }, {
      "heading" : "3.2 Combinatory Categorial Grammar",
      "text" : "The semantic parsing formalism underlying our framework for manipulation actions is that of combinatory categorial grammar (CCG) (Steedman, 2000). A CCG specifies one or more logical forms for each element or combination of elements for manipulation actions. In our formalism, an element of Action is associated with a syntactic “category” which identifies it as functions, and specifies the type and directionality of their arguments and the type of their result. For example, action “Cut” is a function from patient object phrase (NP) on the right into predicates, and into functions from subject object phrase (NP) on the left into a sub action phrase (AP):\nCut := (AP\\NP )/NP. (1)\nAs a matter of fact, the pure categorial grammar is a conext-free grammar presented in the accepting, rather than the producing direction. The expression (1) is just an accepting form for Action “Cut” following the context-free grammar. While it is now convenient to write derivations as follows, they are equivalent to conventional tree structure derivations in Figure. 3.2.\nThe semantic type is encoded in these categories, and their translation can be made explicit\nin an expanded notation. Basically a λ-calculus expression is attached with the syntactic category. A colon operator is used to separate syntactical and semantic expressions, and the right side of the colon is assumed to have lower precedence than the left side of the colon. Which is intuitive as any explanation of manipulation actions should first obey syntactical rules, then semantic rules. Now the basic element, Action “Cut”, can be further represented by:\nCut :=(AP\\NP )/NP : λx.λy.cut(x, y) → divided(y).\n(AP\\NP )/NP denotes a phrase of type AP , which requires an element of type NP to specify what object was cut, and requires another element of type NP to further complement what effector initiates the cut action. λx.λy.cut(x, y) is the λcalculus representation for this function. Since the functions are closely related to the state update, → divided(y) further points out the status expression after the action was performed.\nA CCG system has a set of combinatory rules which describe how adjacent syntatic categories in a string can be recursively combined. In the setting of manipulation actions, we want to point out that similar combinatory rules are also applicable. Especially the functional application rules are essential in our system."
    }, {
      "heading" : "3.3 Functional application",
      "text" : "The functional application rules with semantics can be expressed in the following form:\nA/B : f B : g => A : f(g) (2)\nB : g A\\B : f => A : f(g) (3)\nRule. (2) says that a string with type A/B can be combined with a right-adjacent string of type B to form a new string of type A. At the same time, it also specifies how the semantics of the category A can be compositionally built out of the semantics for A/B and B. Rule. (3) is a symmetric form of Rule. (2).\nIn the domain of manipulation actions, following derivation is an example CCG parse. This parse shows how the system can parse an observation (“Knife Cut Cucumber”) into a semantic representation (cut(knife, cucumber) → divided(cucumber)) using the functional application rules.\nKnife Cut Cucumber\nN N\nNP (AP\\NP)/NP NP knife λx .λy .cut(x , y) cucumber knife → divided(y) cucumber\n> AP\\NP\nλx .cut(x , cucumber) → divided(cucumber)\n< AP\ncut(knife, cucumber) → divided(cucumber)"
    }, {
      "heading" : "4 Learning Model and Semantic Parsing",
      "text" : "After having defined the formalism and application rule, instead of manually writing down all the possible CCG representations for each entity, we would like to apply a learning technique to derive them from the paired training corpus. Here we adopt the learning model of (Zettlemoyer and Collins, 2005), and use it to assign weights to the semantic representation of actions. Since an action may have multiple possible syntactic and semantic representations assigned to it, we use the probabilistic model to assign weights to these representations."
    }, {
      "heading" : "4.1 Learning Approach",
      "text" : "First we assume that complete syntactic parses of the observed action are available, and in fact a manipulation action can have several different parses. The parsing uses a probabilistic combinatorial categorial grammar framework similar to the one given by (Zettlemoyer and Collins, 2007). We assume a probabilistic categorial grammar (PCCG) based on a log linear model. M denotes a manipulation task, L denotes the semantic representation of the task, and T denotes its parse tree. The probability of a particular syntactic and semantic parse is given as:\nP (L, T |M ; Θ) = e f(L,T,M)·Θ∑\n(L,T ) e f(L,T,M)·Θ (4)\nwhere f is a mapping of the triple (L, T,M ) to feature vectors ∈ Rd, and the Θ ∈ Rd represents the weights to be learned. Here we use only lexical features, where each feature counts the number of times a lexical entry is used in T . Parsing a manipulation task under PCCG equates to finding L such that P (L|M ; Θ) is maximized:\nargmaxLP (L|M ; Θ) = argmaxL ∑ T P (L, T |M ; Θ). (5)\nWe use dynamic programming techniques to calculate the most probable parse for the manipulation task. In this paper, the implementation from (Baral et al., 2011) is adopted, where an inverse-λ technique is used to generalize new semantic representations. The generalization of lexicon rules are essential for our system to deal with unknown actions presented during the testing phase."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Manipulation Action (MANIAC) Dataset",
      "text" : "(Aksoy et al., 2014) provides a manipulation action dataset with 8 different manipulation actions (cutting, chopping, stirring, putting, taking, hiding, uncovering, and pushing), each of which consists of 15 different versions performed by 5 different human actors1. There are in total 30 different objects manipulated in all demonstrations. All manipulations were recorded with the Microsoft Kinect sensor and serve as training data here.\nThe MANIAC data set contains another 20 long and complex chained manipulation sequences (e.g. “making a sandwich”) which consist of a total of 103 different versions of these 8 manipulation tasks performed in different orders with novel objects under different circumstances. These serve as testing data for our experiments.\n(Aksoy et al., 2014; Aksoy and Wörgötter, 2015) developed a semantic event chain based model free decomposition approach. It is an unsupervised probabilistic method that measures the frequency of the changes in the spatial relations embedded in event chains, in order to extract the subject and patient visual segments. It also decomposes the long chained complex testing actions into their primitive action components according to the spatio-temporal relations of the manipulator. Since the visual recognition is not the core of this work, we omit the details here and refer the interested reader to (Aksoy et al., 2014; Aksoy and Wörgötter, 2015). All these features make the MANIAC dataset a great testing bed for both the theoretical framework and the implemented system presented in this work."
    }, {
      "heading" : "5.2 Training Corpus",
      "text" : "We first created a training corpus by annotating the 120 training clips from the MANIAC dataset,\n1Dataset available for download at https: //fortknox.physik3.gwdg.de/cns/index. php?page=maniac-dataset.\nin the format of observed triplets (subject action patient) and a corresponding semantic representation of the action as well as its consequence. The semantic representations in λ-calculus format are given by human annotators after watching each action clip. A set of sample training pairs are given in Table.1 (one from each action category in the training set). Since every training clip contains one single full execution of each manipulation action considered, the training corpus thus has a total of 120 paired training samples.\nWe also assume the system knows that every “object” involved in the corpus is an entity of its own type, for example:\nKnife := N : knife\nBowl := N : bowl\n......\nAdditionally, we assume the syntactic form of each “action” has a main type (AP\\NP )/NP (see Sec. 3.2). These two sets of rules form the initial seed lexicon for learning."
    }, {
      "heading" : "5.3 Learned Lexicon",
      "text" : "We applied the learning technique mentioned in Sec. 4, and we used the NL2KR implementation from (Baral et al., 2011). The system learns and generalizes a set of lexicon entries (syntactic and semantic) for each action categories from the training corpus accompanied with a set of weights.\nWe list the one with the largest weight for each action here respectively:\nChopping :=(AP\\NP )/NP : λx.λy.chopping(x, y) → divided(y)\nCutting :=(AP\\NP )/NP : λx.λy.cutting(x, y) → divided(y)\nStirring :=(AP\\NP )/NP : λx.λy.stirring(x, y) Take down :=(AP\\NP )/NP : λx.λy.take down(x, y) → ¬connected(x, y) ∧moved(x) Put on top :=(AP\\NP )/NP : λx.λy.put on top(x, y)\n→ on top(x, y) ∧moved(x) Hiding :=(AP\\NP )/NP : λx.λy.hiding(x, y)\n→ contained(x, y) ∧moved(x) Pushing :=(AP\\NP )/NP : λx.λy.pushing(x, y) → moved(y) Uncover :=(AP\\NP )/NP : λx.λy.uncover(x, y)\n→ appear(y) ∧moved(x).\nThe set of seed lexicon and the learned lexicon entries are further used to probabilistically parse the detected triplet sequences from the 20 long manipulation activities in the testing set."
    }, {
      "heading" : "5.4 Deducing Semantics",
      "text" : "Using the decomposition technique from (Aksoy et al., 2014; Aksoy and Wörgötter, 2015), the reported system is able to detect a sequence of action triplets in the form of (Subject Action Patient) from each of the testing sequence in MANIAC dataset. Briefly speaking, the event chain representation (Aksoy et al., 2011) of the observed long manipulation activity is first scanned to estimate the main manipulator, i.e. the hand, and manipulated objects, e.g. knife, in the scene without employing any visual feature-based object recognition method. Solely based on the interactions between the hand and manipulated objects in the scene, the event chain is partitioned into chunks. These chunks are further fragmented into subunits to detect parallel action streams. Each parsed Semantic Event Chain (SEC) chunk is then compared with the model SECs in the library to decide whether the current SEC sample belongs to one of the known manipulation models or represents a novel manipulation. SEC models, stored in the library, are learned in an on-line unsupervised fashion using the semantics of manipulations derived from a given set of training data in order to create a large vocabulary of single atomic manipulations.\nFor the different testing sequence, the number of triplets detected ranges from two to seven. In total, we are able to collect 90 testing detections and\nthey serve as the testing corpus. However, since many of the objects used in the testing data are not present in the training set, an object model-free approach is adopted and thus “subject” and “patient” fields are filled with segment IDs instead of a specific object name. Fig. 3 and 4 show several examples of the detected triplets accompanied with a set of key frames from the testing sequences. Nevertheless, the method we used here can 1) generalize the unknown segments into the category of object entities and 2) generalize the unknown actions (those that do not exist in the training corpus) into the category of action function. This is done by automatically generalizing the following two types of lexicon entries using the inverse-λ technique from (Baral et al., 2011):\nObject [ID] :=N : object [ID]\nUnknown :=(AP\\NP )/NP : λx.λy.unknown(x, y)\nAmong the 90 detected triplets, using the learned lexicon we are able to parse all of them into semantic representations. Here we pick the representation with the highest probability after parsing as the individual action semantic representation. The “parsed semantics” rows of Fig. 3 and 4 show several example action semantics on testing sequences. Taking the fourth sub-action from Fig. 4 as an example, the visually detected triplets based on segmentation and spatial decomposition is (Object 014, Chopping,Object 011). After semantic parsing, the system predicts that divided(Object 011). The complete training corpus and parsed results of the testing set will be made publicly available for future research."
    }, {
      "heading" : "5.5 Reasoning Beyond Observations",
      "text" : "As mentioned before, because of the use of λcalculus for representing action semantics, the obtained data can naturally be used to do logical reasoning beyond observations. This by itself is a very interesting research topic and it is beyond this paper’s scope. However by applying a couple of common sense Axioms on the testing data, we can provide some flavor of this idea.\nCase study one: See the “final action consequence and reasoning” row of Fig. 3 for case one. Using propositional logic and axiom schema, we can represent the common sense statement (“if an object x is contained in object y, and object z is on top of object y, then object z is on top of object x”) as follows:\n.\n.\nAxiom (1): ∃x, y, z, contained(y, x) ∧ on top(z, y)→ on top(z, x).\nThen it is trivial to deduce an additional final action consequence in this scenario that (on top(object 007, object 009)). This matches the fact: the yellow box which is put on top of the red bucket is also on top of the black ball.\nCase study two: See the “final action consequence and reasoning” row of Fig. 4 for a more complicated case. Using propositional logic and axiom schema, we can represent three common sense statements:\n1) “if an object y is contained in object x, and object z is contained in object y, then object z is contained in object x”;\n2) “if an object x is contained in object y, and object y is divided, then object x is divided”;\n3) “if an object x is contained in object y, and object y is on top of object z, then object x is on top of object z” as follows:\nAxiom (2): ∃x, y, z, contained(y, x) ∧ contained(z, y)→ contained(z, x).\nAxiom (3): ∃x, y, contained(y, x) ∧ divided(y)→ divided(x).\nAxiom (4): ∃x, y, z, contained(y, x) ∧ on top(y, z)→ on top(x, z).\nWith these common sense Axioms, the system is able to deduce several additional final action consequences in this scenario:\ndivided(object 005) ∧ divided(object 010) ∧ on top(object 005, object 012) ∧ on top(object 010, object 012).\nFrom Fig. 4, we can see that these additional consequences indeed match the facts: 1) the bread and cheese which are covered by ham are also divided, even though from observation the system only detected the ham being cut; 2) the divided bread and cheese are also on top of the plate, even though from observation the system only detected the ham being put on top of the plate.\nWe applied the four Axioms on the 20 testing action sequences and deduced the “hidden” consequences from observation. To evaluate our system performance quantitatively, we first annotated all the final action consequences (both obvious and “hidden” ones) from the 20 testing sequences as ground-truth facts. In total there are 122 consequences annotated. Using perception only (Aksoy and Wörgötter, 2015), due to the decomposition errors (such as the red font ones in Fig. 4) the system can detect 91 consequences correctly, yielding a 74% correct rate. After applying the four Axioms and reasoning, our system is able to detect 105 consequences correctly, yielding a 86% correct rate. Overall, this is a 15.4% of improvement.\nHere we want to mention a caveat: there are definitely other common sense Axioms that we are not able to address in the current implementation. However, from the case studies presented, we can see that using the presented formal framework, our system is able to reason about manipulation action goals instead of just observing what is happening visually. This capability is essential for intelligent agents to imitate action goals from observation."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this paper we presented a formal computational framework for modeling manipulation actions based on a Combinatory Categorial Grammar. An empirical study on a large manipulation action dataset validates that 1) with the introduced formalism, a learning system can be devised to deduce the semantic meaning of manipulation actions in λ-schema; 2) with the learned schema and several common sense Axioms, our system is able to reason beyond just observation and deduce “hidden” action consequences, yielding a decent performance improvement.\nDue to the limitation of current testing scenarios, we conducted experiments only considering a relatively small set of seed lexicon rules and logical expressions. Nevertheless, we want to mention that the presented CCG framework can also be extended to learn the formal logic representation of more complex manipulation action semantics. For example, the temporal order of manipulation actions can be modeled by considering a seed rule such as AP\\AP : λf.λg.before(f(·), g(·)), where before(·, ·) is a temporal predicate. For actions in this paper we consider seed main type (AP\\NP )/NP . For more general manipulation\nscenarios, based on whether the action is transitive or intransitive, the main types of action can be extended to include AP\\NP .\nMoreover, the logical expressions can also be extended to include universal quantification ∀ and existential quantification ∃. Thus, manipulation action such as “knife cut every tomato” can be parsed into a representation as ∀x.tomato(x) ∧ cut(knife, x) → divided(x) (the parse is given in the following chart). Here, the concept “every” has a main type of NP\\NP and semantic meaning of ∀x.f(x). The same framework can also extended to have other combinatory rules such as composition and type-raising (Steedman, 2002). These are parts of the future work along the line of the presented work. Knife Cut every Tomato\nN N\nNP (AP\\NP)/NP NP\\NP NP knife λx .λy .cut(x , y) ∀x .f (x ) tomato knife → divided(y) ∀x .f (x ) tomato\n> NP ∀x .tomato(x ) >\nAP\\NP ∀y .λx .tomato(y) ∧ cut(x , y) → divided(y)\n< AP\n∀y .tomato(y) ∧ cut(knife, y) → divided(y)\nThe presented computational linguistic framework enables an intelligent agent to predict and reason action goals from observation, and thus has many potential applications such as human intention prediction, robot action policy planning, human robot collaboration etc. We believe that our formalism of manipulation actions bridges computational linguistics, vision and robotics, and opens further research in Artificial Intelligence and Robotics. As the robotics industry is moving towards robots that function safely, effectively and autonomously to perform tasks in real-world unstructured environments, they will need to be able to understand the meaning of actions and acquire human-like common-sense reasoning capabilities."
    }, {
      "heading" : "7 Acknowledgements",
      "text" : "This research was funded in part by the support of the European Union under the Cognitive Systems program (project POETICON++), the National Science Foundation under INSPIRE grant SMA 1248056, and by DARPA through U.S. Army grant W911NF-14-1-0384 under the Project: Shared Perception, Cognition and Reasoning for Autonomy."
    } ],
    "references" : [ {
      "title" : "Semantic decomposition and recognition of long and complex manipulation action sequences",
      "author" : [ "E E. Aksoy", "F. Wörgötter." ],
      "venue" : "International Journal of Computer Vision, page Under Review.",
      "citeRegEx" : "Aksoy and Wörgötter.,? 2015",
      "shortCiteRegEx" : "Aksoy and Wörgötter.",
      "year" : 2015
    }, {
      "title" : "Learning the semantics of object–action relations by observation",
      "author" : [ "E.E. Aksoy", "A. Abramov", "J. Dörr", "K. Ning", "B. Dellen", "F. Wörgötter." ],
      "venue" : "The International Journal of Robotics Research, 30(10):1229– 1249.",
      "citeRegEx" : "Aksoy et al\\.,? 2011",
      "shortCiteRegEx" : "Aksoy et al\\.",
      "year" : 2011
    }, {
      "title" : "Model-free incremental learning of the semantics of manipulation actions",
      "author" : [ "E E. Aksoy", "M. Tamosiunaite", "F. Wörgötter." ],
      "venue" : "Robotics and Autonomous Systems, pages 1–42.",
      "citeRegEx" : "Aksoy et al\\.,? 2014",
      "shortCiteRegEx" : "Aksoy et al\\.",
      "year" : 2014
    }, {
      "title" : "Using inverse λ and generalization to translate english to formal languages",
      "author" : [ "Chitta Baral", "Juraj Dzifcak", "Marcos Alvarez Gonzalez", "Jiayu Zhou." ],
      "venue" : "Proceedings of the Ninth International Conference on Computational Semantics, pages 35–44. Associ-",
      "citeRegEx" : "Baral et al\\.,? 2011",
      "shortCiteRegEx" : "Baral et al\\.",
      "year" : 2011
    }, {
      "title" : "Human activity recognition using multidimensional indexing",
      "author" : [ "Jezekiel Ben-Arie", "Zhiqian Wang", "Purvin Pandit", "Shyamsundar Rajaram." ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(8):1091–1104.",
      "citeRegEx" : "Ben.Arie et al\\.,? 2002",
      "shortCiteRegEx" : "Ben.Arie et al\\.",
      "year" : 2002
    }, {
      "title" : "Understanding manipulation in video",
      "author" : [ "Matthew Brand." ],
      "venue" : "Proceedings of the Second International Conference on Automatic Face and Gesture Recognition, pages 94–99, Killington,VT. IEEE.",
      "citeRegEx" : "Brand.,? 1996",
      "shortCiteRegEx" : "Brand.",
      "year" : 1996
    }, {
      "title" : "Histograms of oriented optical flow and binetcauchy kernels on nonlinear dynamical systems for the recognition of human actions",
      "author" : [ "R. Chaudhry", "A. Ravichandran", "G. Hager", "R. Vidal." ],
      "venue" : "Proceedings of the 2009 IEEE Intenational Conference on Com-",
      "citeRegEx" : "Chaudhry et al\\.,? 2009",
      "shortCiteRegEx" : "Chaudhry et al\\.",
      "year" : 2009
    }, {
      "title" : "Syntactic Structures",
      "author" : [ "N. Chomsky." ],
      "venue" : "Mouton de Gruyter.",
      "citeRegEx" : "Chomsky.,? 1957",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1957
    }, {
      "title" : "Lectures on government and binding: The Pisa lectures",
      "author" : [ "Noam Chomsky." ],
      "venue" : "Walter de Gruyter.",
      "citeRegEx" : "Chomsky.,? 1993",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1993
    }, {
      "title" : "The anthropomorphic brain: the mirror neuron system responds to human and robotic actions",
      "author" : [ "V Gazzola", "G Rizzolatti", "B Wicker", "C Keysers." ],
      "venue" : "Neuroimage, 35(4):1674–1684.",
      "citeRegEx" : "Gazzola et al\\.,? 2007",
      "shortCiteRegEx" : "Gazzola et al\\.",
      "year" : 2007
    }, {
      "title" : "Minimalist plans for interpreting manipulation actions",
      "author" : [ "Anupam Guha", "Yezhou Yang", "Cornelia Fermüller", "Yiannis Aloimonos." ],
      "venue" : "Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5908–5914.",
      "citeRegEx" : "Guha et al\\.,? 2013",
      "shortCiteRegEx" : "Guha et al\\.",
      "year" : 2013
    }, {
      "title" : "Recognition of visual activities and interactions by stochastic parsing",
      "author" : [ "Yuri A. Ivanov", "Aaron F. Bobick." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):852–872.",
      "citeRegEx" : "Ivanov and Bobick.,? 2000",
      "shortCiteRegEx" : "Ivanov and Bobick.",
      "year" : 2000
    }, {
      "title" : "Visual persuasion: Inferring communicative intents of images",
      "author" : [ "Jungseock Joo", "Weixin Li", "Francis F Steen", "SongChun Zhu." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 216–223. IEEE.",
      "citeRegEx" : "Joo et al\\.,? 2014",
      "shortCiteRegEx" : "Joo et al\\.",
      "year" : 2014
    }, {
      "title" : "Identification of humans using gait",
      "author" : [ "A. Kale", "A. Sundaresan", "AN Rajagopalan", "N.P. Cuntoor", "A.K. Roy-Chowdhury", "V. Kruger", "R. Chellappa." ],
      "venue" : "IEEE Transactions on Image Processing, 13(9):1163–1173.",
      "citeRegEx" : "Kale et al\\.,? 2004",
      "shortCiteRegEx" : "Kale et al\\.",
      "year" : 2004
    }, {
      "title" : "The language of actions: Recovering the syntax and semantics of goal-directed human activities",
      "author" : [ "Hilde Kuehne", "Ali Arslan", "Thomas Serre." ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 780–787. IEEE.",
      "citeRegEx" : "Kuehne et al\\.,? 2014",
      "shortCiteRegEx" : "Kuehne et al\\.",
      "year" : 2014
    }, {
      "title" : "A joint model of language and perception for grounded attribute learning",
      "author" : [ "Cynthia Matuszek", "Nicholas FitzGerald", "Luke Zettlemoyer", "Liefeng Bo", "Dieter Fox." ],
      "venue" : "International Conference on Machine learning (ICML).",
      "citeRegEx" : "Matuszek et al\\.,? 2011",
      "shortCiteRegEx" : "Matuszek et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning from unscripted deictic gesture and language for human-robot interactions",
      "author" : [ "Cynthia Matuszek", "Liefeng Bo", "Luke Zettlemoyer", "Dieter Fox." ],
      "venue" : "Twenty-Eighth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Matuszek et al\\.,? 2014",
      "shortCiteRegEx" : "Matuszek et al\\.",
      "year" : 2014
    }, {
      "title" : "A survey of advances in vision-based human motion capture and analysis",
      "author" : [ "T.B. Moeslund", "A. Hilton", "V. Krüger." ],
      "venue" : "Computer vision and image understanding, 104(2):90–126.",
      "citeRegEx" : "Moeslund et al\\.,? 2006",
      "shortCiteRegEx" : "Moeslund et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning to connect language and perception",
      "author" : [ "Raymond J Mooney." ],
      "venue" : "AAAI, pages 1598–1601.",
      "citeRegEx" : "Mooney.,? 2008",
      "shortCiteRegEx" : "Mooney.",
      "year" : 2008
    }, {
      "title" : "Recognizing multitasked activities from video using stochastic context-free grammar",
      "author" : [ "Darnell Moore", "Irfan Essa." ],
      "venue" : "Proceedings of the National Conference on Artificial Intelligence, pages 770–776, Menlo Park, CA. AAAI.",
      "citeRegEx" : "Moore and Essa.,? 2002",
      "shortCiteRegEx" : "Moore and Essa.",
      "year" : 2002
    }, {
      "title" : "The minimalist grammar of action",
      "author" : [ "K. Pastra", "Y. Aloimonos." ],
      "venue" : "Philosophical Transactions of the Royal Society B: Biological Sciences, 367(1585):103–117.",
      "citeRegEx" : "Pastra and Aloimonos.,? 2012",
      "shortCiteRegEx" : "Pastra and Aloimonos.",
      "year" : 2012
    }, {
      "title" : "Inferring the why in images",
      "author" : [ "Hamed Pirsiavash", "Carl Vondrick", "Antonio Torralba." ],
      "venue" : "arXiv preprint arXiv:1406.5472.",
      "citeRegEx" : "Pirsiavash et al\\.,? 2014",
      "shortCiteRegEx" : "Pirsiavash et al\\.",
      "year" : 2014
    }, {
      "title" : "Neurophysiological mechanisms underlying the understanding and imitation of action",
      "author" : [ "Giacomo Rizzolatti", "Leonardo Fogassi", "Vittorio Gallese." ],
      "venue" : "Nature Reviews Neuroscience, 2(9):661–670.",
      "citeRegEx" : "Rizzolatti et al\\.,? 2001",
      "shortCiteRegEx" : "Rizzolatti et al\\.",
      "year" : 2001
    }, {
      "title" : "Recognition of composite human activities through contextfree grammar based representation",
      "author" : [ "Michael S Ryoo", "Jake K Aggarwal." ],
      "venue" : "Proceedings of the 2006 IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 1709–",
      "citeRegEx" : "Ryoo and Aggarwal.,? 2006",
      "shortCiteRegEx" : "Ryoo and Aggarwal.",
      "year" : 2006
    }, {
      "title" : "Dynamic texture recognition",
      "author" : [ "P. Saisan", "G. Doretto", "Y.N. Wu", "S. Soatto." ],
      "venue" : "Proceedings of the 2001 IEEE Intenational Conference on Computer Vision and Pattern Recognition, volume 2, pages 58–63, Kauai, HI. IEEE.",
      "citeRegEx" : "Saisan et al\\.,? 2001",
      "shortCiteRegEx" : "Saisan et al\\.",
      "year" : 2001
    }, {
      "title" : "The syntactic process, volume 35",
      "author" : [ "Mark Steedman." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Steedman.,? 2000",
      "shortCiteRegEx" : "Steedman.",
      "year" : 2000
    }, {
      "title" : "Plans, affordances, and combinatory grammar",
      "author" : [ "Mark Steedman." ],
      "venue" : "Linguistics and Philosophy, 25(56):723–753.",
      "citeRegEx" : "Steedman.,? 2002",
      "shortCiteRegEx" : "Steedman.",
      "year" : 2002
    }, {
      "title" : "Using a minimal action grammar for activity understanding in the real world",
      "author" : [ "D. Summers-Stay", "C.L. Teo", "Y. Yang", "C. Fermüller", "Y. Aloimonos." ],
      "venue" : "Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Sys-",
      "citeRegEx" : "Summers.Stay et al\\.,? 2013",
      "shortCiteRegEx" : "Summers.Stay et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning perceptually grounded word meanings from unaligned parallel data",
      "author" : [ "Stefanie Tellex", "Pratiksha Thaker", "Joshua Joseph", "Nicholas Roy." ],
      "venue" : "Machine Learning, 94(2):151–167.",
      "citeRegEx" : "Tellex et al\\.,? 2014",
      "shortCiteRegEx" : "Tellex et al\\.",
      "year" : 2014
    }, {
      "title" : "Machine recognition of human activities: A survey",
      "author" : [ "P. Turaga", "R. Chellappa", "V.S. Subrahmanian", "O. Udrea." ],
      "venue" : "IEEE Transactions on Circuits and Systems for Video Technology, 18(11):1473– 1488.",
      "citeRegEx" : "Turaga et al\\.,? 2008",
      "shortCiteRegEx" : "Turaga et al\\.",
      "year" : 2008
    }, {
      "title" : "Inferring “dark matter” and “dark energy” from videos",
      "author" : [ "Dan Xie", "Sinisa Todorovic", "Song-Chun Zhu." ],
      "venue" : "Computer Vision (ICCV), 2013 IEEE International Conference on, pages 2224–2231. IEEE.",
      "citeRegEx" : "Xie et al\\.,? 2013",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2013
    }, {
      "title" : "Detection of manipulation action consequences (MAC)",
      "author" : [ "Yezhou Yang", "Cornelia Fermüller", "Yiannis Aloimonos." ],
      "venue" : "Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, pages 2563–2570, Portland, OR. IEEE.",
      "citeRegEx" : "Yang et al\\.,? 2013",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2013
    }, {
      "title" : "A cognitive system for understanding human manipulation actions",
      "author" : [ "Y. Yang", "A. Guha", "C. Fermuller", "Y. Aloimonos." ],
      "venue" : "Advances in Cognitive Sysytems, 3:67–86.",
      "citeRegEx" : "Yang et al\\.,? 2014",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2014
    }, {
      "title" : "Robot learning manipulation action plans by “watching” unconstrained videos from the world wide web",
      "author" : [ "Yezhou Yang", "Yi Li", "Cornelia Fermuller", "Yiannis Aloimonos." ],
      "venue" : "The Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15).",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Actions sketch: A novel action representation",
      "author" : [ "A. Yilmaz", "M. Shah." ],
      "venue" : "Proceedings of the 2005 IEEE Intenational Conference on Computer Vision and Pattern Recognition, volume 1, pages 984–989, San Diego, CA. IEEE.",
      "citeRegEx" : "Yilmaz and Shah.,? 2005",
      "shortCiteRegEx" : "Yilmaz and Shah.",
      "year" : 2005
    }, {
      "title" : "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
      "author" : [ "Luke S Zettlemoyer", "Michael Collins." ],
      "venue" : "UAI.",
      "citeRegEx" : "Zettlemoyer and Collins.,? 2005",
      "shortCiteRegEx" : "Zettlemoyer and Collins.",
      "year" : 2005
    }, {
      "title" : "Online learning of relaxed ccg grammars for parsing to logical form",
      "author" : [ "Luke S Zettlemoyer", "Michael Collins." ],
      "venue" : "EMNLP-CoNLL, pages 678–687.",
      "citeRegEx" : "Zettlemoyer and Collins.,? 2007",
      "shortCiteRegEx" : "Zettlemoyer and Collins.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Experiments conducted on primates have discovered that certain neurons, the so-called mirror neurons, fire during both observation and execution of identical manipulation tasks (Rizzolatti et al., 2001; Gazzola et al., 2007).",
      "startOffset" : 177,
      "endOffset" : 224
    }, {
      "referenceID" : 9,
      "context" : "Experiments conducted on primates have discovered that certain neurons, the so-called mirror neurons, fire during both observation and execution of identical manipulation tasks (Rizzolatti et al., 2001; Gazzola et al., 2007).",
      "startOffset" : 177,
      "endOffset" : 224
    }, {
      "referenceID" : 26,
      "context" : "Additionally, studies in linguistics (Steedman, 2002) suggest that the language faculty develops in humans as a direct adaptation of a more primitive apparatus for planning goal-directed action in the world by composing affordances of tools and consequences of actions.",
      "startOffset" : 37,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "Combinatory Categorial Grammar (CCG) introduced by (Steedman, 2000) is a theory that can be used to represent such structures with a small set of combinators such as functional application and type-raising.",
      "startOffset" : 51,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "Chomskys contribution to language research was exactly this: the formal description of language through the formulation of the Generative and Transformational Grammar (Chomsky, 1957).",
      "startOffset" : 167,
      "endOffset" : 182
    }, {
      "referenceID" : 2,
      "context" : "We further validate our approach on a large publicly available manipulation action dataset (MANIAC) from (Aksoy et al., 2014), achieving promising experimental results.",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 30,
      "context" : "(Xie et al., 2013) proposed that beyond state-of-the-art computer vision techniques, we could possibly infer implicit information (such as functional objects) from video, and they call them “Dark Matter” and “Dark Energy”.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 31,
      "context" : "(Yang et al., 2013) used stochastic tracking and graphcut based segmentation to infer manipulation consequences beyond appearance.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "(Joo et al., 2014) used a ranking SVM to predict the persuasive motivation (or the intention) of the photographer who captured an image.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 21,
      "context" : "More recently, (Pirsiavash et al., 2014) seeks to infer the motivation of the person in the image by mining knowledge stored in",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 17,
      "context" : "A few good surveys of the former can be found in (Moeslund et al., 2006) and (Turaga et al.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 29,
      "context" : ", 2006) and (Turaga et al., 2008).",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "(Ben-Arie et al., 2002; Yilmaz and Shah, 2005).",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 34,
      "context" : "(Ben-Arie et al., 2002; Yilmaz and Shah, 2005).",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : "Approaches to more complex actions have employed parametric approaches, such as HMMs (Kale et al., 2004) to learn the transition between feature representations in individual frames e.",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : "(Saisan et al., 2001; Chaudhry et al., 2009).",
      "startOffset" : 0,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "(Saisan et al., 2001; Chaudhry et al., 2009).",
      "startOffset" : 0,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation.",
      "startOffset" : 15,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "More recently, (Aksoy et al., 2011; Aksoy et al., 2014) proposed a semantic event chain (SEC) representation to model and learn the semantic segment-wise relationship transition from spatial-temporal video segmentation.",
      "startOffset" : 15,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Tracing back to the middle 90’s, (Brand, 1996) used a grammar to recognize disassembly tasks that contain hand manipulations.",
      "startOffset" : 33,
      "endOffset" : 46
    }, {
      "referenceID" : 23,
      "context" : "(Ryoo and Aggarwal, 2006) used the context-free grammar formalism to recognize composite human activities and multi-person interactions.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "To deal with errors from low-level processes such as tracking, stochastic grammars such as stochastic CFGs were also used (Ivanov and Bobick, 2000; Moore and Essa, 2002).",
      "startOffset" : 122,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "To deal with errors from low-level processes such as tracking, stochastic grammars such as stochastic CFGs were also used (Ivanov and Bobick, 2000; Moore and Essa, 2002).",
      "startOffset" : 122,
      "endOffset" : 169
    }, {
      "referenceID" : 14,
      "context" : "More recently, (Kuehne et al., 2014) proposed to model goal-directed human activities using Hidden Markov Models and treat subactions just like words in speech.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "Manipulation Action Grammar: As mentioned before, (Chomsky, 1993) suggested that a minimalist generative grammar, similar to the one of human language, also exists for action understanding and execution.",
      "startOffset" : 50,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "The works closest related to this paper are (Pastra and Aloimonos, 2012; Summers-Stay et al., 2013; Guha et al., 2013).",
      "startOffset" : 44,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "The works closest related to this paper are (Pastra and Aloimonos, 2012; Summers-Stay et al., 2013; Guha et al., 2013).",
      "startOffset" : 44,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "The works closest related to this paper are (Pastra and Aloimonos, 2012; Summers-Stay et al., 2013; Guha et al., 2013).",
      "startOffset" : 44,
      "endOffset" : 118
    }, {
      "referenceID" : 20,
      "context" : "(Pastra and Aloimonos, 2012) first discussed a Chomskyan grammar for understanding complex actions as a theoretical concept, and (SummersStay et al.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 32,
      "context" : "More recently, (Yang et al., 2014) proposed a set of context-free grammar rules for manipulation action understanding, and (Yang et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 33,
      "context" : ", 2014) proposed a set of context-free grammar rules for manipulation action understanding, and (Yang et al., 2015) applied it on unconstrained instructional videos.",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 35,
      "context" : "Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007).",
      "startOffset" : 212,
      "endOffset" : 274
    }, {
      "referenceID" : 36,
      "context" : "Combinatory Categorial Grammar and Semantic Parsing: CCG based semantic parsing originally was used mainly to translate natural language sentences to their desired semantic representations as λ-calculus formulas (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007).",
      "startOffset" : 212,
      "endOffset" : 274
    }, {
      "referenceID" : 18,
      "context" : "(Mooney, 2008) presented a framework of grounded language acquisition: the interpretation of language entities into semantically informed structures in the context of perception and actuation.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "The concept has been applied successfully in tasks such as robot navigation (Matuszek et al., 2011), forklift operation (Tellex et al.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : ", 2011), forklift operation (Tellex et al., 2014) and of human-robot interaction (Matuszek et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : ", 2014) and of human-robot interaction (Matuszek et al., 2014).",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "For a complete background reading, we would like to refer readers to (Steedman, 2000).",
      "startOffset" : 69,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : "The semantic parsing formalism underlying our framework for manipulation actions is that of combinatory categorial grammar (CCG) (Steedman, 2000).",
      "startOffset" : 129,
      "endOffset" : 145
    }, {
      "referenceID" : 35,
      "context" : "Here we adopt the learning model of (Zettlemoyer and Collins, 2005), and use it to assign weights to the semantic representation of actions.",
      "startOffset" : 36,
      "endOffset" : 67
    }, {
      "referenceID" : 36,
      "context" : "The parsing uses a probabilistic combinatorial categorial grammar framework similar to the one given by (Zettlemoyer and Collins, 2007).",
      "startOffset" : 104,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "In this paper, the implementation from (Baral et al., 2011) is adopted, where an inverse-λ technique is used to generalize new semantic representations.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "(Aksoy et al., 2014) provides a manipulation action dataset with 8 different manipulation actions (cutting, chopping, stirring, putting, taking, hiding, uncovering, and pushing), each of which consists of 15 different versions performed by 5 different human actors1.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "(Aksoy et al., 2014; Aksoy and Wörgötter, 2015) developed a semantic event chain based model free decomposition approach.",
      "startOffset" : 0,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "(Aksoy et al., 2014; Aksoy and Wörgötter, 2015) developed a semantic event chain based model free decomposition approach.",
      "startOffset" : 0,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "Since the visual recognition is not the core of this work, we omit the details here and refer the interested reader to (Aksoy et al., 2014; Aksoy and Wörgötter, 2015).",
      "startOffset" : 119,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "Since the visual recognition is not the core of this work, we omit the details here and refer the interested reader to (Aksoy et al., 2014; Aksoy and Wörgötter, 2015).",
      "startOffset" : 119,
      "endOffset" : 166
    }, {
      "referenceID" : 3,
      "context" : "4, and we used the NL2KR implementation from (Baral et al., 2011).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "Using the decomposition technique from (Aksoy et al., 2014; Aksoy and Wörgötter, 2015), the reported system is able to detect a sequence of action triplets in the form of (Subject Action Patient) from each of the testing sequence in MANIAC dataset.",
      "startOffset" : 39,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "Using the decomposition technique from (Aksoy et al., 2014; Aksoy and Wörgötter, 2015), the reported system is able to detect a sequence of action triplets in the form of (Subject Action Patient) from each of the testing sequence in MANIAC dataset.",
      "startOffset" : 39,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "Briefly speaking, the event chain representation (Aksoy et al., 2011) of the observed long manipulation activity is first scanned to estimate the main manipulator, i.",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "This is done by automatically generalizing the following two types of lexicon entries using the inverse-λ technique from (Baral et al., 2011):",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "The segmentation output and detected triplets are from (Aksoy and Wörgötter, 2015) .",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "The segmentation output and detected triplets are from (Aksoy and Wörgötter, 2015) .",
      "startOffset" : 55,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "Using perception only (Aksoy and Wörgötter, 2015), due to the decomposition errors (such as the red font ones in Fig.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 26,
      "context" : "The same framework can also extended to have other combinatory rules such as composition and type-raising (Steedman, 2002).",
      "startOffset" : 106,
      "endOffset" : 122
    } ],
    "year" : 2015,
    "abstractText" : "In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs λ-calculus; (2) enable a probabilistic semantic parsing schema to learn the λ-calculus representation of manipulation action from an annotated action corpus of videos; (3) use (1) and (2) to develop a system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation.",
    "creator" : "TeX"
  }
}