{
  "name" : "1702.05639.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Stochastic Configuration Networks: Universal Approximation and Learning Representation",
    "authors" : [ "Dianhui Wang", "Ming Li" ],
    "emails" : [ "dh.wang@latrobe.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n05 63\n9v 1\n[ cs\n.L G\n] 1\n8 Fe\nb 20"
    }, {
      "heading" : "1 Introduction",
      "text" : "In the past decade, deep neural networks (DNNs) have received extensively considerable attention due to its great potential for dealing with computer vision, pattern recognition, natural language processing, etc. [21, 30]. The success of deep learning is attributed to its representation capability for visual data [4, 18]. In view of some empirical evidence, DNNs are becoming increasingly popular because of a hypothesis that a deep learner model can be exponentiallymore efficient at representing some functions than a shallow one [15]. It seems that DNNs have a good potential in learning higher-level abstractions (or multiple levels of features), which sounds difficult for models with shallow architecture. Formal analyses of the representation power and learning complexity of DNNs can be found in [11, 25]. In short, the community has reached a common sense that DNNs are much more expressive than the shallow ones.\nAlthough deep learning schemes draw tremendous attention for their overwhelming high performance for some complex data modelling tasks [21], two key issues should be concerned seriously in model design: the architecture determination for DNNs and the training strategies for deep architectures. As usually done, the number of layers and of the nodes at each layer of a DNN are set manually, which poses great threats on the effectiveness of the resulting model. This is because DNNs with moderate depth but fewer number of hidden nodes at each layer cannot exhibit sufficient learning ability (i.e., under fitting), while the large sized architecture (i.e. with quite a few layers and hidden nodes) may cause over-fitting that degrades the generalization capability. Also, it leads\n∗Corresponding author.\nto unnecessary time cost or even unsatisfactory deep models, if end-users determine the architecture by trial and error approaches. In [1], Alvarez and Salzmann introduced an approach to automatically determine the number of nodes at each layer of the DNN by using group sparsity regularizers. Their method can result in a compact width setting for each layer, but with the prerequisite that the network’s depth is determined in advance. Except for the architecture setting problem, fast training algorithms for DNNs are always welcome. Indeed, training standard multilayer perceptrons by the back-propagation algorithm comes up with a complex non-convex optimization problem, and the simultaneous learning of all parameters leads to a high memory and computation cost for very deep models. Empirically, this gradient-based learning method is less effective for representing highly non-linear and highly-varying functions, since the learning curve easily gets stuck in local minima or lost in plateaus. These obstacles also lie in the process of fine-tuning a pre-trainedmodel obtained by the greedy layer-wise unsupervised learning strategy proposed in [17], as the parameters within all layers need to be optimized according to to an objective cost function. To the best of our knowledge on data sciences, deep learning techniques are bravely and blindly generating DNNs with bulk of data, but with less art on architecture design and fast implementation. Motivated by constructive approaches for building shallow neural networks and randomized methods for fast learning, we make efforts to develop randomized learner model with deep architecture.\nThis paper is built on recent work reported in [32] where the way used to construct shallow neural networks with randomness (termed as stochastic configuration networks, SCNs) is original, innovative and effective. Extension of SCNs to deep version is not trivial, because it needs to prove that DeepSCNs share the universal approximation property, and also define some proper and meaningful criteria on node adding, layer growth, and construction termination throughout the learning process. Briefly, a DeepSCN starts with a small sized network (e.g, one hidden layer with one hidden node), and stochastically configures its nodes for the current layer until the hidden output matrix fails the full-rank condition, then continues to add the next hidden layer by repeating procedures as done in the last layer, keeps proceeding to deeper layers until an acceptable error tolerance is achieved. Specifically, as the constructive process proceeds, the hidden parameters are randomly assigned under a supervisory mechanism, while the read-out weights linking the hidden nodes to the output nodes are calculated by the least squares method. The success of SCNs can guarantee the convergence of error sequence approaching to certain accuracy, if a moderate number of hidden nodes are generated [32]. Once starting to add a new hidden layer, the last error function acts as a new target function to be approximated. An immediate benefit from DeepSCNs lies in the links between all hidden nodes and the output nodes, which allow end-users to manipulate layer-wise approximations, in other words, one can make options to use these nodes freely after off-line building an accurate DeepSCN model.\nOur work focuses on framework development for randomized DNNs rather than addresses any specific applications, aiming to offer an advanced solution for the aforementioned challenges. Technical contributions are summarized as follows:\n• Establish the universal approximation theory for DeepSCNs;\n• Determine the architecture of DeepSCNs automatically without human interventions, and use full-rank based criterion to guide the deployment of DeepSCNs;\n• Implement fast learning by stochastically assigning the hidden parameters and evaluating the read-out weights by least squares method.\nThe remainder of this paper is organized as follows: Section 2 reviews some related work concerning randomness in DNNs and optimization-based learning techniques. Section 3 details DeepSCNs framework with theoretical fundamentals on the universal approximation property, learning representation and algorithmic implementation. Section 4 reports some simulation results with comparisons against SCNs. Section 5 concludes this paper and provides some further research directions."
    }, {
      "heading" : "2 Related Work",
      "text" : "Some researchers anticipated the usefulness of randomization in the development of neural networks [29]. In [20], it was found that a random filter in convolutional neural networks can perform slightly worse than a well-designed filter, which usually needs pre-training and discriminative\nfine-tuning. Saxe et al. addressed this issue in [27] and showed that the results obtained from the randomized learner are comparable to that after regular pre-training and fine-tuning processes. These experimental results imply that the involvement of randomization in deep architectures has a good potential in enhancing the computational efficiency of the state-of-the-art systems without a significant deterioration in performance. Coates and Ng [8] tried to use random weights in unsupervised learning, and their experimental results suggested that randomization can be helpful to build large sized models very rapidly, with much ease in training and encoding than sparse coding techniques. Randomness was also concerned in the selection of local receptive fields [7], and the pooling operations of deep convolutional neural networks [35]. Arora et al. investigated the learning of autoencoders with random weights and demonstrated that it is possible to train them in polynomial time under some restrictions on the network depth [2]. To speed the training process of back-propagation (BP) in DNNs, authors in [22] proposed a random feedbackmechanism by multiplying error signals with random weights, which contributes to fast extracting useful information from signals sent through the connections. Motivated by a series of works reported in [9, 24], where they empirically showed some successful learning techniques based on randomization, Giryes et al. [14] theoretically proved that DNNs with random Gaussian weights can perform a stable embedding of the original data, permitting a stable recovery of the data from the learning representation. In [33], the authors stated that the capacity of multilayer perceptronsmainly depends on the number of nodes in the last hidden layer and associated output weights. Indeed, their method came up with the same philosophy of random projection for dimensionality reduction [5]. In [16], some investigations on the functionality of randomized DNNs for deep visualization were empirically conducted. Their findings demonstrate good potential for convolutional neural networks with random weights on visualization.\nIn [19], a constructive algorithm was proposed for building cascaded networks, where all hidden nodes are directly linked to the output nodes. At each layer, the nodes are successively added by optimizing an objective function (equivalent to searching for the hidden parameters). This suffers from all troubles occurring in gradient-based methods. The used criterion on layer growth is heuristic and lacks association with model’s generalization. In [34], the BP-like algorithm was developed for training this type of cascaded networks with specified architecture. However, their approaches are not efficient for building DNNs and require more human intervention in terms of learning parameter setting."
    }, {
      "heading" : "3 Deep Stochastic Configuration Networks",
      "text" : "This section provides some fundamentals of DeepSCNs, including a theoretical result on the universal approximation property, interpretation on learning representation, algorithm description and some technical remarks."
    }, {
      "heading" : "3.1 Universal Approximation Property",
      "text" : "Let L2(D) denote the space of all Lebesgue-measurable vector-valued functions F = [f1, f2, . . . , fm] : R d → Rm on a compact set D ⊂ Rd, with the L2 norm defined as\n‖F‖ :=\n( m∑\nq=1\n∫\nD\n|fq(x)| 2dx\n)1/2\n< ∞, (1)\nand inner product defined as\n〈F ,G〉 :=\nm∑\nq=1\n〈fq, gq〉 =\nm∑\nq=1\n∫\nD\nfq(x)gq(x)dx, (2)\nwhere G = [g1, g2, . . . , gm] : R d → Rm.\nGiven a target function F : Rd → Rm, suppose a DeepSCN with n hidden layers and each layer has Lk hidden nodes (k = 1, 2, . . . , n), has been constructed, that is,\nF (n) Ln (x) =\nn∑\nk=1\nLk∑\nj=1\nβ (k) j φk,j\n(\nx(k−1);w (k−1) j , b (k−1) j\n)\n, (3)\nwhere w (k−1) j and b (k−1) j stand for the hidden parameters within the k-th hidden layer, φk,j is the activation function used in the k-th hidden layer, x (0) = x and x(k) = Φ ( x(k−1);W (k−1), B(k−1) )\n= [φk,1, φk,2, . . . , φk,Lk ], with W (k−1) =\n[w (k−1) 1 , w (k−1) 2 , . . . , w (k−1) Lk ] and B(k−1) = [b (k−1) 1 , b (k−1) 2 , . . . , b (k−1) Lk ], k = 1, 2, . . . , n. Then, the residual error function is defined by E (n) Ln = F − F (n) Ln = [E (n) Ln,1 , E (n) Ln,2 , . . . , E (n) Ln,m ]. Specifically, E (1) 0 = F and E (n+1) 0 = E (n) Ln , n = 1, 2, . . . .\nThe universal approximation property of DeepSCNs is stated in the following Theorem 1.\nTheorem 1. Suppose that span(Γ) is dense in L2 space and ∀φ ∈ Γ, 0 < ‖φ‖ < c for some c ∈ R+. Given 0 < r < 1 and a nonnegative decreasing sequence {µl} with liml→+∞ µl = 0 and µl < (1− r). For n = 1, 2 . . ., and j = 1, 2, . . . , Ln + 1, denoted by\nδ (n) j = (1 − r − µj)‖E (n) j−1‖ 2. (4)\nStochastically configuring the j-th hidden node φn,j (within the n-th hidden layer) to satisfy the following inequality:\nm∑\nq=1\n〈E (n) j,q , φn,j〉 2 ≥ c2δ (n) j , j = 1, 2, . . . , Ln + 1. (5)\nIf the random basis functionsφn,1, φn,2, . . . , φn,Ln+1 are linearly dependent on a compact setDn ⊂ Rd, i.e., for some real numbers c1, c2, . . . , cLn+1 not all zero, the following holds\nLn+1∑\nj=1\ncjφn,j(t) = 0, t ∈ Dn, (6)\nfix Ln and start to add the first hidden node φn+1,1 in the (n + 1)-th hidden layer according to the following inequality\nm∑\nq=1\n〈E (n) Ln,q , φn+1,1〉 2 ≥ c2δ (n) Ln . (7)\nKeep adding new hidden nodes within the (n + 1)-th hidden layer based on (5), followed by generating the first hidden node in a new hidden layer via (7) (as (6) holds). After adding one hidden node (either in the present hidden layer or starting a new hidden layer), the read-out weights are evaluated by the least squares method, that is,\nβ∗ = argmin β ‖F −\nn∑\nk=1\nLk∑\nj=1\nβ (k) j φk,j‖ 2. (8)\nThen, we have limn→+∞ ‖F − F (n) Ln ‖ = 0, where F (n) Ln is defined by (3).\nProof. Based on Theorem 7 in [32], we can easily obtain that\n‖E (n) Ln ‖2 ≤ (r + µLn)‖E (n) Ln−1 ‖2. (9)\nThe error after adding one hidden node in the (n+ 1)-hidden layer can be expressed as\n‖E (n+1) 1 ‖ 2 = ‖E (n) Ln − β (n+1) 1 φn+1,1‖ 2, (10)\nwhere φn+1,1 and β (n+1) 1 are obtained by using (7) and (8) with , respectively. To identify the relationship between E (n+1) 1 and E (n) Ln , we denote β̄ (n+1) 1 = [β̄ (n+1) 1,1 , . . . , β̄ (n+1) 1,m ] T, where\nβ̄ (n+1) 1,q =\n〈E (n) Ln,q , φn+1,1〉\n‖φn+1,1‖2 , q = 1, 2, . . . ,m, (11)\nReplacing β (n+1) 1 by β̄ (n+1) 1 , with β̄ (n+1) 1,q evaluated by (11), we have\n‖E (n+1) 1 ‖ 2 ≤ ‖E (n) Ln − β̄ (n+1) 1 φn+1,1‖ 2\n= ‖E (n) Ln ‖2 −\n∑m q=1〈E (n) Ln,q , φn+1,1〉 2\n‖φn+1,1‖2\n≤ ‖E (n) Ln ‖2. (12)\nWith the help of (9), we can obtain\n‖E (n+1) Ln+1 ‖2 ≤ ‖E (n+1) 1 ‖ 2 2 ≤ ‖E (n) Ln ‖2\n≤ n∏\nk=1\n(r + µLk)‖E (n) 1 ‖ 2\n≤ (r + µ1) ∑ n k=1 Lk‖E (1) 1 ‖ 2. (13)\nLet n → +∞ for both sides of (13), we have limn→+∞ ‖E (n+1) Ln+1 ‖ = 0. That completes the proof."
    }, {
      "heading" : "3.2 Learning Representation",
      "text" : "The idea of learning internal representation can be traced back to 80’s [26]. In the past decades, many exciting researches and progresses have been reported in literature [12, 23], where attempts are made to meet the following three properties: fidelity, sparsity and interpretability. Learner models can appear in different forms but essentially they must share the universal approximation property so that the fidelity can be achieved. Theorem 1 above ensures that DeepSCNs fit the essential prerequisite well for learning representation. Due to the special architecture of DeepSCNs, learning representation becomes quite straightforward and understandable. The outcomes from the first hidden layer contain two parts: a set of random basis functions with the original inputs, and associated read-out weights between the first hidden layer and the output layer. Apparently, one can define a learning representation from the first hidden layer (indeed, a SCN model), which characterizes the main feature of the target function. Next, the outputs from the first hidden layer, as a new set of inputs, can be fed into the next hidden layer to constructively generate a set of random basis functions with the cascaded inputs, and associated read-out weights between the second hidden layer and the output layer. Again, one can obtain a refined learning representation from the second hidden layer. This procedure keeps going on until some termination criterion is satisfied. Figure 1 shows a typical DeepSCN with n = 2, L1 = 3, and L2 = 2, d = 2,m = 1.\nIt should be highlighted that DeepSCNs advocate fully connections between each hidden layer and the output layer, which supports the diversity in learning representation for the target function by\nusing an explicit span of the basis functions stochastically configured (corresponds to φk,j in (3)), instead of using only the compositional expression from the last hidden layer to conduct signal representation. Note that the set of random basis functions generated by the proposed DeepSCNs are data dependent and with cascaded inputs. According to (3), DeepSCNs can be regarded as a producer to output learning representations (14). Indeed, such a learning representation offers end-users more flexibility, for instance, some nodes at certain layer can be discarded according to some criteria, leading to a new learning representation with sparsity.\nF ≃ ( φ1,1 . . . φ1,L1 · · · φn,1 . . . φn,Ln β (1) 1 . . . β (1) L1 · · · β (n) 1 . . . β (n) Ln )\n︸ ︷︷ ︸ Hidden Layer 1 ︸ ︷︷ ︸ • • • ︸ ︷︷ ︸ Hidden Layer n\n(14)"
    }, {
      "heading" : "3.3 Algorithm Description",
      "text" : "Given a training dataset with inputs X = {x1, x2, . . . , xN}, xi = [xi,1, . . . , xi,d] T ∈ Rd and outputs T = {t1, t2, . . . , tN}, where ti = [ti,1, . . . , ti,m] T ∈ Rm, i = 1, . . . , N . Denote E (n) Ln−1 = E (n) Ln−1 (X) = [E (n) Ln−1,1 (X), . . . , E (n) Ln−1,m (X)]T as the corresponding residual error vector before the Ln-th new hidden node of the n-th hidden layer is added, where E (n) Ln−1,q (X) = [E (n) Ln−1,q (x1), . . . , E (n) Ln−1,q (xN )] ∈ R N , q = 1, 2, . . . ,m. After adding the Ln-th hidden node in the n-th hidden layer, we get\nh (n) Ln := h (n) Ln (X) = [φn,Ln(x (n−1) 1 ), . . . , φn,Ln(x (n−1) N )] T, (15)\nwhere φn,Ln(x (n−1) i ) is used to simplify φn,Ln(x (n−1) i ;w (n−1) j , b (n−1) j ), and x (0) i = xi = [xi,1, . . . , xi,d] T, x (n−1) i = Φ(x (n−2);W (n−1), B(n−1)) for n ≥ 2. Let H (n) Ln = [h (n) 1 , h (n) 2 , . . . , h (n) Ln ] represent the hidden layer output matrix and denote a temporary variable θ (n) Ln to make it convenient in the followed algorithm description.\nθ (n) Ln\n= m∑\nq=1\n〈E (n) Ln−1,q , h (n) Ln 〉2l2\n〈h (n) Ln , h (n) Ln 〉l2 −\nm∑\nq=1\n(1 − r − µL)〈E (n) Ln−1,q E (n) Ln−1,q 〉l2 , (16)\nwhere 〈·, ·〉l2 denotes the dot product and we omit the argumentX in E (n) Ln−1,q and h (n) Ln ."
    }, {
      "heading" : "3.4 Further Remark",
      "text" : "In essence, the universal approximation property ensures the capacity of DeepSCNs for both data modelling and signal representation. Thus, DeepSCNs can be employed as either predictive models or feature extractors in domain applications. For regression problems, one may be more interested in the predictability of a learner model rather than its learning capability. Unfortunately, it is almost impossible to directly establish some certain correlation between these two performances. That is, a better learning performance does not always imply a sound generalization. In this regard, consistency concept becomes a meaningful metric to assess the goodness of learning machines. From our hands-on experience, DeepSCNs demonstrate very good consistency performance, and it is believed that this merit is associated with both the proposed full-rank terminal criterion and the supervisory mechanism for assigning the random hidden parameters. For classification problems, DeepSCNs can be used directly to put a class label for a testing data by using linear discriminate analysis. Alternatively, we can use DeepSCNs as feature extractors to generate a collection of samples in the DeepSCN feature space, followed by using variety of feature-classifiers to implement classification. In a nutshell, as results of deep learning for DeepSCNs, a set of data dependent random basis functions and (14) are generated for signal representation with piece-wise ‘resolutions’."
    }, {
      "heading" : "4 Empirical Demonstration",
      "text" : "This section reports some simulation results to illustrate advantages of DeepSCNs compared against SCNs in terms of the approximation accuracy, quality of learning representation and model’s\nDeepSCN Algorithm (DSCN)\nGiven inputs X = {x1, x2, . . . , xN}, xi ∈ R d and outputs T = {t1, t2, . . . , tN}, ti ∈ R m; SetM as the maximum number of hidden layers, L (n) max as the maximum number of hidden nodes within the n-th hidden layer, 1 ≤ n ≤ M , ǫ as the expected error tolerance, Tmax as the maximum times of random configuration; Choose a set of scalars Υ= {λmin :∆λ :λmax}.\n1. Initialize E (1) 0 := [t1, t2, . . . , tN ] T, 0 < r < 1, three empty sets H,Ω,W := ∅; 2. While n ≤ M AND ‖E (1) 0 ‖F > ǫ, Do 3. While Ln ≤ L (n) max AND ‖E (1) 0 ‖F > ǫ, Do 4. Forλ ∈ Υ, Do 5. For k = 1, 2 . . . , Tmax, Do 6. Randomly assign ω (n−1) Ln and b (n−1) Ln from [−λ, λ]d and [−λ, λ], respectively; 7. Calculate h (n) Ln , θ (n) Ln based on Eq. (15) and (16), set µLn = (1− r)/(Ln + 1); 8. If θ (n) Ln ≥ 0 9. Save w (n−1) Ln and b (n−1) Ln inW , θ (n) Ln\nin Ω; 10. Else go back to Step 5 11. End If 12. End For (corresponds to Step 5) 13. If W is not empty 14. Find w (n−1)∗ Ln , b (n−1)∗ Ln maximizing θ (n) Ln in Ω, set H (n) Ln = [h (n)∗ 1 , . . . , h (n)∗ Ln\n]; 15. Break (go to Step 19); 16. Else Randomly take τ ∈ (0, 1− r), renew r := r + τ , return to Step 5; 17. End If 18. End For (corresponds to Step 4) 19. If H (n) Ln is full rank 20. SetH := [H, H (n) Ln ], β∗ = H†T , E (n) Ln := Hβ∗ − T , E (n) = E (n) Ln ; 21. Else Set H̃(n) := H (n) Ln (:, 1 : Ln − 1),H := [H, H̃ (n)]; 22. Calculate β∗ = H†T , E (n) = Hβ∗ − T , Ln := Ln − 1; 23. Break (Go to Step 27) 24. End If 25. Renew E (1) 0 := E (n) Ln\n, and Ln := Ln + 1; 26. End While (corresponds to Step 3) 27. Set E (n+1) 0 = E\n(n), Ω,W = ∅; 28. End While (corresponds to Step 2) 29. Return β∗ = [β (1)∗ 1 , . . . , β (n)∗ Ln ], ω∗ = [w (0)∗ 1 , . . . , w (n−1)∗ Ln ], b∗ = [b (0)∗ 1 , . . . , b (n−1)∗ Ln ].\ncapacity. Here, we also provide a robustness analysis on DSCN algorithm with respect to the learning parameter setting. The following real-valued function is employed in the simulation study:\nf(x) = 0.2e−(10x−4) 2 + 0.5e−(80x−40) 2 + 0.3e−(80x−20) 2 , x ∈ [0, 1].\nIn this study, we take 1000 points (randomly generated) as the training data and another 1000 points (randomly generated) as the test data in our experiments. The sigmoidal activation function g(x) = 1/(1 + exp(−x)) is used here for all hidden nodes."
    }, {
      "heading" : "4.1 Effectiveness and Consistency",
      "text" : "Figure 2 depicts the training and test performances, respectively, where L = 200 for SCN, M = 4 and L (n) max = 50 for each layer in DeepSCN. It is clear that DeepSCN approximates the target function within a given tolerance more faster than SCN. Also, the consistency relationship between learning and generalization can be observed. As a matter of fact, both shallow and deep SCNs share this nice consistency property. It should be clearly pointed out that our statement on the consistency comes from a large number of experimental observations, and theoretical justifications are being expected. Figure 2 shows three independent trials, which indicate that DeepSCN outperforms SCN in terms of effectiveness. In this test, we set rk = 1 − 10\n−k, k = 1, 2, ..., 7 in Step 16 of DSCN algorithm. With different settings, the performance on both effectiveness and consistency could be further verified by the results from Figure 5."
    }, {
      "heading" : "4.2 Signal Representation",
      "text" : "From mathematical perspective, both SCN and DeepSCN can produce a set of random basis functions. The difference between them lies in the way how to generate the random basis functions, which relate to the input variables and random parameters of learner models. It is difficult to quantify the quality of learning representations. In this paper, we favourably consider a learning representation if the distribution of the read-out weights in (14) has sound statistical characteristics, for instance, lower expectation and standard deviation. In Figure 3, two normalized distributions of the read-out weights (converted into [-1,1] on the basis of 200 coefficients from both SCN and DeepSCN) are plotted, and the approximation results for the test dataset are displayed beside the distributions, respectively. As can be seen that DeepSCN has a set of concentrated read-out weights and performs much better in generalization than SCN, which indeed has a quite poor distribution of the read-out weights with a few of peaking values. In this scenario, DeepSCN has four hidden layers and L (n) max = 25 is applied for each layer (without use of the full-rank terminal condition)."
    }, {
      "heading" : "4.3 Model Capacity versus Rank Deficiency",
      "text" : "In common sense, a learner model’s capacity can be measured by using a trade-off metric over the learning and generalization performance. In addition, we may evaluate the capacity of a learner model by looking at the interpretability. In fact, it is always desirable to have a learner model with balanced performance among learning, generalization and model complexity. For DeepSCN framework, due to the specific configuration in model building, the model capacity is not only associated with the number of nodes for each hidden layer, but also closely related to some algebraic properties of the hidden output matrix at each hidden layer. Technically, it is very interesting and useful to look into the relationship between the rank-deficiency property of the joint hidden output matrix and the model’s generalization capability. To do so, the degree of rank-deficiency is computed by p = rank(H)/ ∑j\nn=1 Ln, where H represents the hidden output matrix composed of\nall existing hidden nodes, ∑j\nn=1 Ln represents the (current) number of hidden nodes after adding j(j = 1, 2, 3, 4) hidden layers and Lj hidden nodes 1 ≤ Lj ≤ 25. The values of p for both SCN and DeepSCN are plotted in Figure 4 (a), in which it can be observed that H of DeepSCN is with higher rank even when the number of hidden nodes is approaching to 100, however, the rank-deficiency level of SCN becomes much more higher as L is larger. Note that the results from DeepSCN reported in Figure (a) were obtained with a fixed architecture (i.e., 4 hidden layers with 25 nodes for each) and without use of the full-rank terminal criterion. Figure 4 (b) depicts six curves of rank-deficiency for SCN and DeepSCN, whereM = 4 and L (n) max = 50 for each hidden layer are used in DSCN algorithm. By referring to the previous results in comparison with SCN, we infer that the DeepSCN model capacity has stronger correlation to the rank of the joint hidden output matrix."
    }, {
      "heading" : "4.4 Robustness Analysis",
      "text" : "In general, robustness refers to the ability of a system to maintain its performancewhilst subjected to noise in external inputs, changes to internal structure and/or shifts in parameter setting. Obviously, the quality or performance of DeepSCNs depends upon the parameter setting. In order to investigate the robustness of DSCN algorithm, in this paper we limit our study on the learning parameter r with 11 values only. A similar robustness analysis on another set of key learning parameters Υ= {λmin : ∆λ : λmax} is not reported here. Figure 5 shows the training and test performances for both SCN and DeepSCN with three different settings of r, that is, randomly taking 10 real numbers from the open interval (0.9, 0.99) and arranging them in increasing order to form a set r1, and set r = {r1, 1 − 10\n−6}. As can be seen that both SCN and DeepSCN perform robustly against this learning parameter setting (i.e., not sensitive to the uncertain sequence r). In this case, we fixed the architecture of DeepSCN (i.e., with 4 hidden layers and 25 nodes for each) and kept the full-rank terminal criterion free. A remarkable drop-down can be observed after completing the construction of the first layer, which clearly exhibits the advantage of DeepSCN over SCN."
    }, {
      "heading" : "5 Conclusions",
      "text" : "Although many empirical proofs demonstrate great potential of deep neural networks for learning representation, it is still blind for end-users to deploy the network architecture so that the resulting deep learner model has sufficient capacity to approximately represent signals in some ways. Except for concerns on the architecture, one cares much about fast learning algorithms. In this paper, our proposed DeepSCNs, as a class of randomized deep neural networks, offer a fast and feasible pathway for problem solving. Before ending up this paper, we would like to highlight and summarize again our technical contributions to the working field as follows:\n• The supervisory mechanism characterised by the inequalities (5) and (7) is the key for developing DeepSCN framework, which was originally proposed in [32] and could be used as a unique feature to distinguish our SCN framework from other randomized learner models.\n• The scopes of random parameters at each layer of DeepSCNs could be updated adaptively. Indeed, this must be done to make the set of basis functions rich and functional.\n• The termination criterion, i.e., the full-rank condition, for keep adding hidden nodes at the same layer is original and insightful. It is strongly believed that this criterion is an appropriate option although there is no theoretical justification in place at the moment.\nFurther researches on this topic move into two main clusters: theoretical aspects, including further explorations on various supervisory mechanisms, scientific justifications on the proposed full-rank criterion, studies on the existence of the global random basis functions (GRBFs) for a collection of functions, and looking into more complicated supervisory mechanisms to generate GRBFs, development of learning representation with sparsity and interpretability; practical aspects, comparing with other deep neural networks for gaining more understandings on merits and limits of DeepSCNs, and applying DeepSCNs for real world applications."
    } ],
    "references" : [ {
      "title" : "Learning the number of neurons in deep networks",
      "author" : [ "J. Alvarez", "M. Salzmann" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Provable bounds for learning some deep representations",
      "author" : [ "S. Arora", "A. Bhaskara", "R. Ge", "T. Ma" ],
      "venue" : "Proceedings of the 31th International Conference on Machine Learning, pages 584–592",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Random search for hyper-parameter optimization",
      "author" : [ "J. Bergstra", "Y. Bengio" ],
      "venue" : "Journal of Machine Learning Research, 13:281–305",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A. Courville", "P. Vincent" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Random projection in dimensionality reduction: applications to image and text data",
      "author" : [ "E. Bingham", "H. Mannila" ],
      "venue" : "Proceedings of the 7th International Conference on Knowledge Discovery and Data Mining, pages 245–250",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "A. Choromanska", "M. Henaff", "M. Mathieu", "G. Arous", "Y. LeCun" ],
      "venue" : "Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, pages 192–204",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Selecting receptive fields in deep networks",
      "author" : [ "A. Coates", "A. Ng" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2528–2536",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The importance of encoding versus training with sparse coding and vector quantization",
      "author" : [ "A. Coates", "A. Ng" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning, pages 921–928",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Beyond simple features: A large-scale feature search approach to unconstrained face recognition",
      "author" : [ "D. Cox", "N. Pinto" ],
      "venue" : "Proceedings of 2011 IEEE International Conference on Automatic Face & Gesture Recognition and Workshops, pages 8–15",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2933–2941",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The power of depth for feedforward neural networks",
      "author" : [ "R. Eldan", "O. Shamir" ],
      "venue" : "arXiv preprint arXiv:1512.03965",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Compressed sensing: theory and applications",
      "author" : [ "Y.C. Eldar", "G. Kutyniok" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Texture synthesis using convolutional neural networks",
      "author" : [ "L. Gatys", "A. Ecker", "M. Bethge" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 262–270",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep neural networks with random gaussian weights: A universal classification strategy? IEEE Transactions on Signal Processing",
      "author" : [ "R. Giryes", "G. Sapiro", "A. Bronstein" ],
      "venue" : "64(13):3444–3457",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep Learning",
      "author" : [ "I. Goodfellow", "Y. Bengio", "A. Courville" ],
      "venue" : "MIT Press",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A powerful generative model using random weights for the deep image representation",
      "author" : [ "K. He", "Y. Wang", "J. Hopcroft" ],
      "venue" : "Advances In Neural Information Processing Systems, pages 631–639",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G. Hinton", "S. Osindero", "Y. Teh" ],
      "venue" : "Neural Computation, 18(7):1527–1554",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "G. Hinton", "R. Salakhutdinov" ],
      "venue" : "Science, 313(5786):504–507",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A new constructive algorithm for architectural and functional adaptation of artificial neural networks",
      "author" : [ "M. Islam", "M. Sattar", "M. Amin", "X. Yao", "K. Murase" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(6):1590–1605",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "What is the best multi-stage architecture for object recognition? In Proceedings of the 12th IEEE International Conference on Computer Vision",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. Lecun" ],
      "venue" : "pages 2146–2153",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. LeCun", "Y. Bengio", "G. Hinton" ],
      "venue" : "Nature, 521(7553):436–444",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Random feedback weights support learning in deep neural networks",
      "author" : [ "T. Lillicrap", "D. Cownden", "D. Tweed", "C. Akerman" ],
      "venue" : "arXiv preprint arXiv:1411.0247",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Statistical Learning with Sparsity: The Lasso and Generalizations",
      "author" : [ "C.M. O’Brien" ],
      "venue" : "Wiley Online Library,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "A high-throughput screening approach to discovering good forms of biologically inspired visual representation",
      "author" : [ "N. Pinto", "D. Doukhan", "J. DiCarlo", "D. Cox" ],
      "venue" : "PLoS Computational Biology, 5(11):e1000579",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Representational power of restricted Boltzmann machines and deep belief networks",
      "author" : [ "N. Roux", "Y. Bengio" ],
      "venue" : "Neural Computation, 20(6):1631–1649",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning internal representations by backpropagating errors",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "Nature 323 ",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "On random weights and unsupervised feature learning",
      "author" : [ "A. Saxe", "P. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A. Ng" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning, pages 1089–1096",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "A. Saxe", "J. McClelland", "S. Ganguli" ],
      "venue" : "Proceedings of International Conference on Learning Representations",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Randomness in neural networks: An overview",
      "author" : [ "S. Scardapane", "D. Wang" ],
      "venue" : "WIREs Data Mining and Knowledge Discovery",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Networks, 61:85–117",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "Proceedings of International Conference on Learning Representations",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stochastic configuration networks: Fundamentals and algorithms",
      "author" : [ "D. Wang", "M. Li" ],
      "venue" : "arXiv preprint arXiv:1702.03180",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "The no-prop algorithm: A new learning algorithm for multilayer neural networks",
      "author" : [ "B. Widrow", "A. Greenblatt", "Y. Kim", "D. Park" ],
      "venue" : "Neural Networks, 37:182–188",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Neural network architectures and learning algorithms",
      "author" : [ "B.M. Wilamowski" ],
      "venue" : "IEEE Industrial Electronics Magazine, 3(4):56–63",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Stochastic pooling for regularization of deep convolutional neural networks",
      "author" : [ "M. Zeiler", "R. Fergus" ],
      "venue" : "arXiv preprint arXiv:1301.3557",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "[21, 30].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 29,
      "context" : "[21, 30].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 3,
      "context" : "The success of deep learning is attributed to its representation capability for visual data [4, 18].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "The success of deep learning is attributed to its representation capability for visual data [4, 18].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "In view of some empirical evidence, DNNs are becoming increasingly popular because of a hypothesis that a deep learner model can be exponentiallymore efficient at representing some functions than a shallow one [15].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 10,
      "context" : "Formal analyses of the representation power and learning complexity of DNNs can be found in [11, 25].",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 24,
      "context" : "Formal analyses of the representation power and learning complexity of DNNs can be found in [11, 25].",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "Although deep learning schemes draw tremendous attention for their overwhelming high performance for some complex data modelling tasks [21], two key issues should be concerned seriously in model design: the architecture determination for DNNs and the training strategies for deep architectures.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "In [1], Alvarez and Salzmann introduced an approach to automatically determine the number of nodes at each layer of the DNN by using group sparsity regularizers.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 16,
      "context" : "These obstacles also lie in the process of fine-tuning a pre-trainedmodel obtained by the greedy layer-wise unsupervised learning strategy proposed in [17], as the parameters within all layers need to be optimized according to to an objective cost function.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 31,
      "context" : "This paper is built on recent work reported in [32] where the way used to construct shallow neural networks with randomness (termed as stochastic configuration networks, SCNs) is original, innovative and effective.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 31,
      "context" : "The success of SCNs can guarantee the convergence of error sequence approaching to certain accuracy, if a moderate number of hidden nodes are generated [32].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 28,
      "context" : "Some researchers anticipated the usefulness of randomization in the development of neural networks [29].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "In [20], it was found that a random filter in convolutional neural networks can perform slightly worse than a well-designed filter, which usually needs pre-training and discriminative",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "addressed this issue in [27] and showed that the results obtained from the randomized learner are comparable to that after regular pre-training and fine-tuning processes.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "Coates and Ng [8] tried to use random weights in unsupervised learning, and their experimental results suggested that randomization can be helpful to build large sized models very rapidly, with much ease in training and encoding than sparse coding techniques.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "Randomness was also concerned in the selection of local receptive fields [7], and the pooling operations of deep convolutional neural networks [35].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 34,
      "context" : "Randomness was also concerned in the selection of local receptive fields [7], and the pooling operations of deep convolutional neural networks [35].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "investigated the learning of autoencoders with random weights and demonstrated that it is possible to train them in polynomial time under some restrictions on the network depth [2].",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 21,
      "context" : "To speed the training process of back-propagation (BP) in DNNs, authors in [22] proposed a random feedbackmechanism by multiplying error signals with random weights, which contributes to fast extracting useful information from signals sent through the connections.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "Motivated by a series of works reported in [9, 24], where they empirically showed some successful learning techniques based on randomization, Giryes et al.",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 23,
      "context" : "Motivated by a series of works reported in [9, 24], where they empirically showed some successful learning techniques based on randomization, Giryes et al.",
      "startOffset" : 43,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "[14] theoretically proved that DNNs with random Gaussian weights can perform a stable embedding of the original data, permitting a stable recovery of the data from the learning representation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "In [33], the authors stated that the capacity of multilayer perceptronsmainly depends on the number of nodes in the last hidden layer and associated output weights.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : "Indeed, their method came up with the same philosophy of random projection for dimensionality reduction [5].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "In [16], some investigations on the functionality of randomized DNNs for deep visualization were empirically conducted.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "In [19], a constructive algorithm was proposed for building cascaded networks, where all hidden nodes are directly linked to the output nodes.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 33,
      "context" : "In [34], the BP-like algorithm was developed for training this type of cascaded networks with specified architecture.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 31,
      "context" : "Based on Theorem 7 in [32], we can easily obtain that",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 25,
      "context" : "The idea of learning internal representation can be traced back to 80’s [26].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "In the past decades, many exciting researches and progresses have been reported in literature [12, 23], where attempts are made to meet the following three properties: fidelity, sparsity and interpretability.",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : "In the past decades, many exciting researches and progresses have been reported in literature [12, 23], where attempts are made to meet the following three properties: fidelity, sparsity and interpretability.",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "3e 2 , x ∈ [0, 1].",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "In Figure 3, two normalized distributions of the read-out weights (converted into [-1,1] on the basis of 200 coefficients from both SCN and DeepSCN) are plotted, and the approximation results for the test dataset are displayed beside the distributions, respectively.",
      "startOffset" : 82,
      "endOffset" : 88
    }, {
      "referenceID" : 31,
      "context" : "• The supervisory mechanism characterised by the inequalities (5) and (7) is the key for developing DeepSCN framework, which was originally proposed in [32] and could be used as a unique feature to distinguish our SCN framework from other randomized learner models.",
      "startOffset" : 152,
      "endOffset" : 156
    } ],
    "year" : 2017,
    "abstractText" : "This paper focuses on the development of randomized approaches for building deep neural networks. A supervisory mechanism is proposed to constrain the random assignment of the hidden parameters (i.e., all biases and weights within the hidden layers). Full-rank oriented criterion is suggested and utilized as a termination condition to determine the number of nodes for each hidden layer, and a pre-defined error tolerance is used as a global indicator to decide the depth of the learner model. The read-out weights attached with all direct links from each hidden layer to the output layer are incrementally evaluated by the least squares method. Such a class of randomized leaner models with deep architecture is termed as deep stochastic configuration networks (DeepSCNs), of which the universal approximation property is verified with rigorous proof. Given abundant samples from a continuous distribution, DeepSCNs can speedily produce a learning representation, that is, a collection of random basis functions with the cascaded inputs together with the read-out weights. Simulation results with comparisons on function approximation align with the theoretical findings.",
    "creator" : "LaTeX with hyperref package"
  }
}