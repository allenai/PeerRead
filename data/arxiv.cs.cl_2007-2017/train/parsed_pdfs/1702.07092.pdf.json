{
  "name" : "1702.07092.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Neural Attention Model for Categorizing Patient Safety Events",
    "authors" : [ "Arman Cohan", "Allan Fong", "Nazli Goharian", "Raj Ratwani" ],
    "emails" : [ "arman@ir.cs.georgetown.edu", "nazli@ir.cs.georgetown.edu", "allan.fong@medicalhfe.org", "raj.ratwani@medicalhfe.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Natural Language Processing, Text Categorization, Medical Text Processing, Deep Learning"
    }, {
      "heading" : "1 Introduction",
      "text" : "There is an increasing demand for use of textual electronic health records and clinical notes to promote healthcare, and as such, In recent years NLP/IR have become increasingly important in understanding, searching, and analyzing medical information [22]. Human or system errors do occur frequently in the health centers, many of which can lead to serious harm to individuals. There are in fact an alarming number of annual death incidents (up to 200K) being reported due to medical errors [1]; medical errors are shown to be the third leading cause of death in the US [14]. Many healthcare centers have deployed patient safety event reporting systems to better identify, mitigate, and prevent errors [4]. Patient safety event reports are narratives describing a safety event and they belong to different safety categories such as “medication”, “diagnosis”, “treatment”, “lab”, etc. Recently, due to the importance of patient safety reports, more healthcare centers are enforcing patient safety reporting, resulting in an overwhelming number of daily produced reports. Manual processing of all these reports to identify important cases, trends, or system issues is extremely difficult, inefficient, and expensive. The first step in understanding and analyzing these events is to identify their general categories. This task is challenging because the ar X iv :1\n70 2.\n07 09\n2v 1\n[ cs\n.C L\n] 2\n3 Fe\nevent descriptions can be very complex; the frontline staff usually focus more on taking care of the patient at the moment than to think through the classification schema when they later write a safety report. For example, an event where a patient fell after being given an incorrect medication might have been classified as “fall” however, the fall could be due to a mis-medication and therefore belong to the “medication” safety event. Without the ability to correctly identify the medication category, such problems will not be addressed. Therefore, classifying the patient safety reports not only helps in further search and analytic tasks, but also it contributes to reducing the human reporting errors.\nIn this paper, we present a method for categorizing the Patient Safety Reports as the first step towards understanding adverse events and the way to prevent them. Traditional approaches of text categorization rely on sparse feature extraction from clinical narratives and then classifying the types of events based on these feature representations. In these conventional methods, complex lexical relations and long-term dependencies of the narratives are not captured. We propose a neural attention architecture for classifying safety events, which performs the feature extraction, and type classification jointly; our proposed architecture is based on a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) with soft attention mechanism. We evaluate our method on two large scale datasets obtained from two large healthcare providers. We demonstrate that our proposed method significantly improves over several traditional baselines, as well as more recent neural network based methods."
    }, {
      "heading" : "2 The proposed Neural Attention Architecture",
      "text" : "Our proposed model for classifying patient safety reports is a neural architecture based on Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) utilizing a soft attention mechanism. Our architecture is partially similar to models by [12,11] in convolutional layers, to [19] in recurrent layer, and to [21] in the document modeling. Our point of departure is that unlike these works which are mainly targeted for sentence and short documents, we utilize a soft neural attention mechanism coupled with CNN and RNN to capture the more salient local features in longer sequences. Below we present the building blocks of our proposed architecture from bottom to the top.\nEmbedding layer. Given a sequence of words S = 〈w1;w2; ...;wn〉 where wi are words in the sequence and “:” is the concatenation operation, the embedding layer represents S as an input vector x ∈ R(m,d) where d is the embedding dimension size and m is the maximum sequence length. xi’s can be either initialized randomly or by pre-trained word embeddings, and then they can be jointly trained with the model.\nCNN. CNNs are feed-forward networks which include two main operations: convolution and pooling. Convolution is an operation on two functions (input and kernel) of real valued arguments [13]. In our context, in layer ` in the network, convolution operates on sliding windows of width k` on the input x`−1 and yields a feature map F`:\nF (i) ` = g(W` . x (i,k`) `−1 + b`) (1)\nwhere W` and b` are the shared wights and biases in layer `, g is an activation function, and x(i,k`) = 〈xi− (k`−1) 2 ; ...;xi+ (k`−1)\n2 〉 shows the sliding window of size k` centered at position i on the input. We use ReLU [5] for the activation function (In our experiments ReLU showed the best results among other activation functions). For pooling, we use “max-pooling” operation whose role is to downsample the feature map and capture significant local features. Similar to [12], we use filters of sizes from 2 to 6 to capture local features of different granularities. The convolution layer allows the model to learn the salient features that are needed for identifying the type of the safety events.\nRNN. Unlike CNNs which are local feature encoders, RNNs can encode large windows of local features and capture long temporal dependencies. Given an input sequence h = (x1, ..., xT ) where each xt ∈ Rd is an input word vector of dimension d at time step t, an RNN computes the hidden states h = (h1, ..., hT ) and outputs y = (y1, ..., yT ) according to the following equations [8]:\nht =g(W (hh)ht−1 + W (xh)xt + bh) yt =W (hy)ht + by (2)\nwhere W shows the weight matrices for the corresponding input, b denotes the biases, and g is the activation function. RNNs in theory, can capture temporal dependencies of any length. However, training RNNs in their basic form is problematic due to the vanishing gradient problem [16]. Long Short-Term Memory (LSTM) [10] is a type of RNN that has several gates controlling the flow of information to be preserved or forgotten, and mitigates the vanishing gradient problem. We use the LSTM formulation as in [9]. We aslo employ bidirectional LSTM to capture both forward and backward temporal dependencies. Using this layer, we capture the dependencies between local features along long sequences.\nNeural attention. The trouble with RNNs for classification is that they encode the entire sequence into the vector at the last temporal step. While the application of RNNs have been successful in encoding sentences or short documents, in longer documents this can result in loss of information [3], and putting more focus on the recent temporal entries [18]. Bidirectional RNNs try to alleviate this problem by considering both the forward and backward context vectors. However, they suffer from the same problem in long sequences.\nInspired by work in machine-translation, to address this problem, we utilize the soft attention mechanism [7]. Neural attention allows the model to decide which parts of the sequence are more important instead of directly considering the context vector output by the RNN. Specifically, instead of considering the final cell state of LSTM for the classification, we allow the model to attend to the important timesteps and build a context vector c as follows:\nc = ∑T\nt=1 αtht (3)\nwhere αt are weights computed at each timestep t for the state ht and are computed as follows:\nαt = exp(e>t z)∑T k=1 exp(e > k z)\n(4) et = fatt(ht) (5)\nwhere fatt is a function whose role is to capture the importance of hti and z is a context vector that is learned jointly during training. We use a feed-forward network with “tanh” activation function for fatt. The context vector c is then fed to a fully-connected and then a softmax layer to perform final classification."
    }, {
      "heading" : "3 Experiments",
      "text" : "Setup. We evaluate the effectiveness of our model on two large scale patient safety data obtained from a large healthcare providers in mid-Atlantic US and the Institute for Safe Medication Practices (ISMP). ISMP serves as a safe harbor for all PSE reports from hospitals in Pennsylvania, US. The dataset that was analyzed contains all categories of safety reports (fall, medication, surgery, etc.) and is not limited to medication reports. This study was approved by the MedStar Health Research Institute Institutional Review Board (protocol 2014-101). The characteristics of the data and the categories are shown in tables 1 and 2. We split the data with stratified random sampling into 3 sets: train, validation, and test. We tune the parameters of the neural models on the validation set and the test set remains unseen to the models. We compare our results with conventional text classification models (bag of words feature representation with different types of classifiers), as well as related work on neural architectures (CNNs, RNNs and Bidirectional RNNs and their combinations). For space limitation, we do not explain the details of the baselines and refer the reader to the corresponding citations in Table 3. We report accuracy and average F1-score results for the categories which are standard evaluation metrics for this task.\nImplementation. We used Keras and TensorFlow for the implementation. We empirically made the following design choices: We used Word2Vec [15] for training the embeddings on both general (Wikipedia) and domain specific corpora (PubMed), similar to [17]. We used dropout rates of 0.25 for the recurrent and 0.5 for the convolutional layers. Training was done using Adam optimizer with categorical cross entropy loss; we also applied early stopping for the training. Number of epochs for the larger dataset was 2 and for small dataset 6.\nV\nResults. Table 3 demonstrates our main results. As illustrated, our method (last row) significantly outperforms all other methods in virtually all the datasets. This shows the general effectiveness of our model in comparison with the prior work. We observe that our method’s performance improvement is slightly larger in the second (larger) dataset. This is expected since our model can better learn the parameters when trained on larger data. Improvement over RNN and CNNBi-RNN baselines shows the effectiveness of the neural soft attention in capturing salient parts of the sequence in comparison with the models without attention.\nError Analysis. While our method effectively outperforms the prior work, we conducted error analysis to better understand the cases that our method fails to correctly perform categorization. In particular, we observed that for both datasets, many wrongly classified samples in the categories were misclassified as the “miscellaneous” category. This pattern was more common for the categories with smaller training samples. This shows that the model learns a broader set of texts for the “miscellaneous” category, which is expected, given the broad nature of this category. We also observed some misclassified samples in the categories that are closely related together. For example in dataset 1, 32% of the misclassified samples in the “blood-bank” category were classified as “lab/specimen”. A similar pattern was observed for the “diagnosis” and “medication” safety events. These closely related categories usually have\noverlaps in terms of training data and this makes it hard for the model to differentiate the edge cases. We furthermore observe that the performance on each category correlates with the number of samples in that category. Figure 1 shows this correlation. We observe that generally, our method performs better with the categories of larger relative size. While the correlation is stronger for dataset 1, both datasets show similar trends. This shows that having more training samples helps our model in better learning the characteristics of that particular category and results in higher performance."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We presented a neural network model based on a soft attention mechanism for categorizing patient safety event reports. We demonstrated the effectiveness of our model on two large-scale real-world datasets and we obtained significant improvements over existing methods. The impact of our method and results is substantial on the patient safety and healthcare, as better categorization of events results in better patient management and prevention of harm to the individuals."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the 3 anonymous reviewers for their helpful comments. This project was funded under contract/grant number Grant R01 HS023701-02 from the Agency for Healthcare Research and Quality (AHRQ), U.S. Department of Health and Human Services. The opinions expressed in this document are those of the authors and do not necessarily reflect the official position of AHRQ or the U.S. Department of Health and Human Services."
    } ],
    "references" : [ {
      "title" : "Xgboost: A scalable tree boosting system",
      "author" : [ "T. Chen", "C. Guestrin" ],
      "venue" : "Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "On the Properties of Neural Machine Translation: EncoderDecoder Approaches",
      "author" : [ "K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio" ],
      "venue" : "Proceedings of SSST- 8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation pp",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "How a system for reporting medical errors can and cannot improve patient safety",
      "author" : [ "J.R. Clarke" ],
      "venue" : "The American surgeon 72(11),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Improving deep neural networks for lvcsr using rectified linear units and dropout",
      "author" : [ "G.E. Dahl", "T.N. Sainath", "G.E. Hinton" ],
      "venue" : "In: Acoustics, Speech and Signal Pro- cessing (ICASSP),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Q.V.: Semi-supervised Sequence Learning",
      "author" : [ "A.M. Dai", "Le" ],
      "venue" : "Neural Information Processing Systems (NIPS). pp",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Neural Machine Translation By Jointly Learning To Align and Translate",
      "author" : [ "Dzmitry Bahdana", "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "Iclr 2015 pp",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Finding structure in time",
      "author" : [ "J.L. Elman" ],
      "venue" : "Cognitive Science",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1990
    }, {
      "title" : "Towards End-To-End Speech Recognition with Recurrent Neural Networks",
      "author" : [ "A. Graves", "N. Jaitly" ],
      "venue" : "JMLR Workshop and Conference Proceedings",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation 9(8),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1997
    }, {
      "title" : "A Convolutional Neural Network for Modelling Sentences",
      "author" : [ "N. Kalchbrenner", "E. Grefenstette", "P. Blunsom" ],
      "venue" : "Association for Computational Linguistics",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Convolutional Neural Networks for Sentence Classification",
      "author" : [ "Y. Kim" ],
      "venue" : "Proceed- ings of the 2014 Conference on Empirical Methods in Natural Language Process- ing (EMNLP",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE 86(11),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Medical error the third leading cause of death in the US",
      "author" : [ "M.A. Makary", "M. Daniel" ],
      "venue" : "Bmj 2139(May),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Distributed Representations of Words and Phrases and their Compositionality",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "Neural Information Processing Sys- tems (NIPS). pp",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "On the Difficulties of Training",
      "author" : [ "R. Pacanu", "T. Mikolov", "Y. Bengio" ],
      "venue" : "Recurrent Neural Networks. International Conference on Machine Learning (ICML)",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Learning to Rank for Consumer Health Search: a Se- mantic Approach. In: ECIR",
      "author" : [ "L. Soldaini", "N. Goharian" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2017
    }, {
      "title" : "Q.V.: Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Le" ],
      "venue" : "Neural Information Processing Systems (NIPS). pp",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification",
      "author" : [ "D. Tang", "B. Qin", "T. Liu" ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification",
      "author" : [ "S. Wang", "C. Manning" ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Hierarchical Attention Networks for Document Classification",
      "author" : [ "Z. Yang", "D. Yang", "C. Dyer", "X. He", "A. Smola", "E. Hovy" ],
      "venue" : "Proceedings of NAACL-HLT",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Extracting Adverse Drug Reactions from Social Media",
      "author" : [ "A. Yates", "N. Goharian", "O. Frieder" ],
      "venue" : "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Self-Adaptive Hierarchical Sentence Model",
      "author" : [ "H. Zhao", "Z. Lu", "P. Poupart" ],
      "venue" : "IJ- CAI. pp. 4069–4076",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "There is an increasing demand for use of textual electronic health records and clinical notes to promote healthcare, and as such, In recent years NLP/IR have become increasingly important in understanding, searching, and analyzing medical information [22].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 12,
      "context" : "There are in fact an alarming number of annual death incidents (up to 200K) being reported due to medical errors [1]; medical errors are shown to be the third leading cause of death in the US [14].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Many healthcare centers have deployed patient safety event reporting systems to better identify, mitigate, and prevent errors [4].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "Our architecture is partially similar to models by [12,11] in convolutional layers, to [19] in recurrent layer, and to [21] in the document modeling.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "Our architecture is partially similar to models by [12,11] in convolutional layers, to [19] in recurrent layer, and to [21] in the document modeling.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "Our architecture is partially similar to models by [12,11] in convolutional layers, to [19] in recurrent layer, and to [21] in the document modeling.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "Our architecture is partially similar to models by [12,11] in convolutional layers, to [19] in recurrent layer, and to [21] in the document modeling.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 11,
      "context" : "Convolution is an operation on two functions (input and kernel) of real valued arguments [13].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "We use ReLU [5] for the activation function (In our experiments ReLU showed the best results among other activation functions).",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 10,
      "context" : "Similar to [12], we use filters of sizes from 2 to 6 to capture local features of different granularities.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 6,
      "context" : ", yT ) according to the following equations [8]:",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "However, training RNNs in their basic form is problematic due to the vanishing gradient problem [16].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : "Long Short-Term Memory (LSTM) [10] is a type of RNN that has several gates controlling the flow of information to be preserved or forgotten, and mitigates the vanishing gradient problem.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "We use the LSTM formulation as in [9].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "While the application of RNNs have been successful in encoding sentences or short documents, in longer documents this can result in loss of information [3], and putting more focus on the recent temporal entries [18].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "While the application of RNNs have been successful in encoding sentences or short documents, in longer documents this can result in loss of information [3], and putting more focus on the recent temporal entries [18].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 5,
      "context" : "Inspired by work in machine-translation, to address this problem, we utilize the soft attention mechanism [7].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "We empirically made the following design choices: We used Word2Vec [15] for training the embeddings on both general (Wikipedia) and domain specific corpora (PubMed), similar to [17].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "We empirically made the following design choices: We used Word2Vec [15] for training the embeddings on both general (Wikipedia) and domain specific corpora (PubMed), similar to [17].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "SVM [20] 70.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 18,
      "context" : "9 MNB [20] 71.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "6 XGB [2] 71.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : "cBoW [23] 67.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : "1 Adaptive cBoW [23] 69.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "CNN [12] 73.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 4,
      "context" : "5 RNN [6] 76.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 4,
      "context" : "2 Bi-RNN [6] 76.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 17,
      "context" : "5 CNN-BiRNN [19] 77.",
      "startOffset" : 12,
      "endOffset" : 16
    } ],
    "year" : 2017,
    "abstractText" : "Medical errors are leading causes of death in the US and as such, prevention of these errors is paramount to promoting healthcare. Patient Safety Event reports are narratives describing potential adverse events to the patients and are important in identifying, and preventing medical errors. We present a neural network architecture for identifying the type of safety events which is the first step in understanding these narratives. Our proposed model is based on a soft neural attention model to improve the effectiveness of encoding long sequences. Empirical results on two large-scale real-world datasets of patient safety reports demonstrate the effectiveness of our method with significant improvements over existing methods.",
    "creator" : "LaTeX with hyperref package"
  }
}