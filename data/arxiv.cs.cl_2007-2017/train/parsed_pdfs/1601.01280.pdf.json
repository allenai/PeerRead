{
  "name" : "1601.01280.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Language to Logical Form with Neural Attention",
    "authors" : [ "Li Dong" ],
    "emails" : [ "li.dong@ed.ac.uk", "mlap@inf.ed.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Semantic parsing is the task of translating text to a formal meaning representation such as logical forms or structured queries. There has recently been a surge of interest in developing machine learning methods for semantic parsing (see the references in Section 2), due in part to the existence of corpora containing utterances annotated with formal meaning representations. Figure 1 shows an example of a question (left hand-side) and its annotated logical form (right hand-side), taken from JOBS (Tang and Mooney, 2001), a well-known semantic parsing benchmark. In order to predict the correct logical form for a given input utterance, most previous systems rely on predefined templates and manually designed features, which often render parsers domain- or representation-specific. In this work, we aim to use a simple yet effective method to bridge\nthe gap between natural language and logical form with minimal domain knowledge.\nSequence transduction architectures based on recurrent neural networks have been successfully applied to a variety of NLP tasks ranging from syntactic parsing (Vinyals et al., 2015a), to machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), and image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b). An attention mechanism is often used to improve performance for long sequences (Bahdanau et al., 2015; Vinyals et al., 2015a). We adapt the general sequence-to-sequence modeling paradigm to the semantic parsing task. As shown in Figure 1, our model learns from natural language descriptions paired with meaning representations; it encodes sentences and decodes logical forms using recurrent neural networks with long short-term memory (LSTM) units. We also introduce an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015b) which allows the model to learn soft alignments between natural language and logical forms. Rare words present a problem to sequence transduction models, and semantic parsing is no exception. In order to handle rare words, we mask entities and numbers with their types and recover them in a postar X iv :1 60 1. 01 28\n0v 1\n[ cs\n.C L\n] 6\nJ an\n2 01\n6\nprocessing step. Evaluation results demonstrate that our model achieves similar or better performance when compared to previous methods across different domains and meaning representations, despite using no hand-engineered features, templates, domain or linguistic knowledge.\nThe contributions of our work are three-fold: (1) we present a general-purpose sequence-tosequence model for semantic parsing; (2) propose a method to address the rare word problem for our neural model; and (3) conduct extensive experiments on several benchmark datasets providing detailed analysis on what our model learns."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks.\nThe problem of learning semantic parsers has received significant attention, dating back to Woods (1973). A large number of approaches learn from sentences paired with logical forms following different modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logical-from annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014).\nOur model learns from natural language descriptions paired with meaning representations. Most\nprevious systems rely on high-quality lexicons, manually-built templates, and features which are either domain- or representation-specific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks such as syntactic parsing (Vinyals et al., 2015a), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b), question answering (Hermann et al., 2015), and summarization (Rush et al., 2015).\nMei et al. (2016) use a sequence-to-sequence model to map navigational instructions to actions. Our model works on more well-defined meaning representations (such as Prolog and lambda calculus) and is conceptually simpler; it does not employ bidirectionality or multi-level alignments. Grefenstette et al. (2014) propose a different architecture for semantic parsing based on the combination of two neural network models. The first model learns shared representations from pairs of questions and their translations into knowledge base queries, whereas the second model generates the queries conditioned on the learned representations. However, they do not report empirical evaluation results."
    }, {
      "heading" : "3 Model",
      "text" : "Our goal is to learn a model which maps natural language input q = ( x1, · · · , x|q| ) to a logical form\nrepresentation of its meaning a = ( y1, · · · , y|a| ) . We regard both input q and output a as sequences and wish to estimate the conditional probability p (a|q) which is decomposed as follows:\np (a|q) = |a|∏ t=1 p (yt|y<t, q) (1)\nwhere y<t = (y1, · · · , yt−1). In the following we describe the details of how p (a|q) is computed."
    }, {
      "heading" : "3.1 Sequence-to-Sequence Model",
      "text" : "Our model consists of an encoder which encodes natural language input q into a vector representation and a decoder which learns to generate y1, · · · , y|a| conditioned on the encoding vector.\nAs shown in Figure 2, the encoder and decoder are two different L-layer recurrent neural networks with Long Short-Term Memory (LSTM) units which recursively process tokens one by one. The first |q| time steps belong to the encoder, while the following |a| time steps belong to the decoder. Let hlt ∈ Rn denote an n-dimensional hidden vector in time step t and layer l. hlt is computed by:\nhlt = f ( hlt−1,h l−1 t ) (2)\nwhere h0t is the word vector of the current token for the encoder or the previously predicted word for the decoder. Specifically, we follow the definition of the LSTM unit used in Zaremba et al. (2015) to recursively compute the hidden vectors:\ni f o g\n = \nsigm sigm sigm tanh\nW l (hl−1thlt−1 )\nmlt = f mlt−1 + i g hlt = o tanh ( mlt\n) (3)\nwhere tanh and sigm are applied element-wise, is element-wise multiplication, and W l ∈ R4n×2n is a parameter matrix for the l-th layer. Notice that the encoder and decoder have different parameters.\nOnce the tokens of the input sequence x1, · · · , x|q| are encoded into vectors, they are used to initialize the hidden states of the first time step in the decoder. Next, the hidden vector of the topmost LSTM hL|q|+t is used to predict the\nt-th output token as: p (yt|y<t, q) = softmax ( Wph L |q|+t )ᵀ e (yt) (4)\nwhere Wp ∈ R|Va|×n is a parameter matrix, and e (yt) ∈ {0, 1}|Va| is a one-hot vector used to obtain yt’s probability from the predicted distribution.\nWe augment every sequence with a “start-ofsequence” <s> and “end-of-sequence” <e> token. The generation process terminates once <e> is predicted. Then, the conditional probability of generating the whole sequence p (a|q) is computed using Equation (1)."
    }, {
      "heading" : "3.2 Attention Mechanism",
      "text" : "As shown in Equation (4), the hidden vectors of the input sequence are not directly used in the decoding process. However, it makes intuitively sense to consider relevant information from the input to better predict the current token. Following this idea, various techniques have been proposed to integrate encoder-side information (in the form of a context vector) into each time step of the decoder (Bahdanau et al., 2015; Luong et al., 2015b; Xu et al., 2015; Hermann et al., 2015). We use the global attention architecture described in Luong et al. (2015b).\nAs shown in Figure 3, in order to find relevant encoder-side context for the current hidden state hL|q|+t, we compute its attention score with the k-th hidden state in the encoder as:\ns |q|+t k = exp{(hLk ) ᵀ hL|q|+t}∑|q|\nj=1 exp{(hLj ) ᵀ hL|q|+t}\n(5)\nwhere only the top-layer hidden vectors are utilized. Then, the context vector is the weighted sum of the hidden vectors in the encoder:\nc|q|+t = |q|∑ k=1 s |q|+t k h L k (6)\nIn lieu of Equation (4), we further use this context vector which acts as a summary of the encoder to compute the probability of generating yt as:\nhatt|q|+t = tanh ( W1h L |q|+t +W2c |q|+t )\n(7)\np (yt|y<t, q) = softmax ( Wph att |q|+t )ᵀ e (yt) (8)\nwhere Wp ∈ R|Va|×n and W1,W2 ∈ Rn×n are three parameter matrices, and e (yt) is a one-hot vector used to obtain yt’s probability."
    }, {
      "heading" : "3.3 Model Training",
      "text" : "Our goal is to maximize the likelihood of generated logical forms given natural language utterances as input. So the objective function is defined as:\nmin− ∑\n(q,a)∈D\nlog p (a|q) (9)\nwhere D is the set of all natural language-logical form training pairs, and p (a|q) is computed as shown in Equation (1). The RMSProp algorithm (Tieleman and Hinton, 2012) is employed to solve this non-convex optimization problem. Moreover, dropout is used for regularizing the model as suggested in Zaremba et al. (2015). Specifically, dropout operators are used between different LSTM layers and for the hidden layers before the softmax classifiers. This technique can substantially reduce overfitting, especially on datasets of small size."
    }, {
      "heading" : "3.4 Inference",
      "text" : "At test time, the model predicts the logical form for an input utterance q maximizing the conditional probability:\nâ = argmax a′\np ( a′|q )\n(10)\nwhere a′ represents a candidate logical form sequence. However, it is not practical to iterate over\nall possible sequences to obtain the optimal prediction. According to Equation (1), we decompose the probability p (a|q) so that we can use beam search or greedy search to predict tokens one by one. We add special tokens <s> and <e> to the beginning and end of every sequence. The generation process terminates once token <e> is predicted."
    }, {
      "heading" : "3.5 Handling Rare Words",
      "text" : "Some entities and numbers are rare or even do not appear in the training set at all, especially on smallscale datasets. Conventional sequence-to-sequence models replace rare words with the unknown word symbol (Luong et al., 2015a; Jean et al., 2015). This strategy, however, would adversely affect our model’s accuracy, because a prediction result is correct only if all tokens are parsed correctly.\nOur solution is to link entities and numbers in utterances to their corresponding logical constants, and replace them with their type names and unique IDs. For instance, we pre-process the training example “jobs with a salary of 40000” and its logical form “job(ANS), salary greater than(ANS, 40000, year)” as “jobs with a salary of num0” and “job(ANS), salary greater than(ANS, num0, year)”. We use the pre-processed examples as training data. At inference time, we also mask entities and numbers with their types and IDs. Once we obtain the decoding result, a post-processing step recovers all the markers typei to their corresponding logical constants."
    }, {
      "heading" : "4 Experiments",
      "text" : "We compare our method against multiple previous systems on four datasets. We describe these datasets below, and present our experimental settings and results. Finally, we conduct model analysis in order to understand what the model learns."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Our model was trained on the following datasets, covering different domains and using different meaning representations. Examples for each domain are shown in Table 1.\nJOBS This benchmark dataset contains 640 queries to a database of job listings. Specifically, questions are paired with Prolog-style queries. We used the same training-test split as Zettlemoyer\nand Collins (2005) which contains 500 training and 140 test instances. Values for the variables company, degree, language, platform, location, job area, and number are considered as rare words.\nGEO This is a standard semantic parsing benchmark which contains 880 queries to a database of U.S. geography. GEO has 880 sentence logicalform pairs split into a training set of 680 training examples and 200 test examples (Zettlemoyer and Collins, 2005). We used the same meaning representation based on lambda-calculus as Kwiatkowski et al. (2011). Values for the varibles city, state, country, river, and number are handled as rare words.\nATIS This dataset has 5, 410 queries to a flight booking system. The standard split (Kwiatkowski et al., 2011) has 4, 480 training instances, 480 development instances, and 450 test instances. Sentences are paired with lambda-calculus expressions. Values for the variables date, time, city, aircraft code, airport, airline, and number are treated as rare words.\nIFTTT Quirk et al. (2015) created this dataset by extracting a large number of if-this-then-that recipes from the IFTTT website1. Recipes are simple programs with exactly one trigger and one action which users specify on the site. Whenever the conditions of the trigger are satisfied, the action is performed. Actions typically revolve around home security (e.g., “turn on my lights when I arrive home”), automation (e.g., “text me if the door opens”), wellbeing (e.g., “remind me to drink water if I’ve been at a bar for more than two hours”), and so on. Triggers and actions are selected from different channels (160\n1http://www.ifttt.com\nin total) representing various types of services, devices (e.g., Android), and knowledge sources (such as ESPN or Gmail). In the dataset, there are 552 trigger functions from 128 channels, and 229 action functions from 99 channels. We used Quirk et al.’s (2015) original split which contains 77, 495 training, 5, 171 development, and 4, 294 test examples. The IFTTT programs are represented as abstract syntax trees and are paired with natural language descriptions provided by users (see Table 1). Here, numbers and URLs are handled as rare words."
    }, {
      "heading" : "4.2 Settings",
      "text" : "We converted natural language sentences to lowercase and corrected misspelled words using a dictionary based on the Wikipedia list of common misspellings. To reduce sparsity, words were stemmed using the Snowball Stemmer within NLTK (Bird et al., 2009). For IFTTT, we filtered tokens, channels and functions which appeared less than five times in the training set. For the other datasets, we filtered input words which did not occur at least two times in the training set, but kept all tokens in the logical forms. Plain string matching was employed to identify entities for handling rare words as described in Section 3.5. More sophisticated approaches could be used, however we leave this future work.\nModel hyper-parameters were cross-validated on the training set for JOBS and GEO. We used the standard development sets for ATIS and IFTTT. Our model was trained with mini-batch stochastic gradient descent using the RMSProp algorithm (with batch size set to 20). The smoothing constant of RMSProp was set to 0.95. The base learning rate was 0.002 for IFTTT and 0.01 for the other do-\nmains. After 5 epochs, the learning rate was decreased by a factor of 0.95 at every epoch as suggested in Karpathy et al. (2015). Gradients were clipped at 5 to alleviate the exploding gradient problem (Pascanu et al., 2013). Parameters were initialized by randomly sampling values from a uniform distribution U (−0.08, 0.08). A two-layer LSTM was used for IFTTT, while a one-layer LSTM was employed for the other domains. We also used dropout regularization; the dropout rate was selected from {0.2, 0.3, 0.4, 0.5}. Dimensions of the hidden vector and the word embedding were the same, and selected from {150, 200, 250}. Early stopping was employed to determine the number of epochs. The input sentences were reversed before feeding into the sequence encoder as suggested by Sutskever et al. (2014). Greedy search was used to generate logical forms during inference. Notice that two decoders with shared word embeddings were used to predict triggers and actions for IFTTT. All experiments were conducted on a single GTX 980 GPU device."
    }, {
      "heading" : "4.3 Results",
      "text" : "We first discuss the performance of our model on JOBS, GEO, and ATIS, and then examine our results on IFTTT. Tables 2–4 compare our results against a variety of systems previously described in the literature. In addition to our model (seq2seq), we present two ablation variants, namely without using an attention mechanism (−attention) and without handing rare words (−rare). We report accuracy which is defined as the proportion of input sentences that are correctly parsed to their gold standard logical forms. Notice that DCS+L, KCAZ13 and GUSP output answers directly, so accuracy in this setting is defined as the percentage of correct answers.\nWe find that adding attention substantially im-\nproves performance on all three datasets. This underlines the importance of utilizing soft alignments between inputs and outputs. We further analyze what the attention layer learns in Section 4.4. Moreover, the results show that handling rare words is critical for small-scale datasets. For example, about 92% of city names appear less than 4 times in the GEO training set, so it is difficult to learn reliable parameters for these words. In relation to previous models, the seq2seq framework achieves comparable or better performance without relying on manually defined templates or language/domain specific features. Importantly, we use the same model across datasets and meaning representations (Prolog-style logical forms in JOBS and lambda calculus in the\nother two datasets), without modification. Despite this relatively simple approach, we observe that seq2seq ranks third on JOBS, and second on ATIS.\nFor IFTTT, we follow the same evaluation protocol introduced in Quirk et al. (2015). The dataset is extremely noisy and measuring accuracy is problematic since predicted abstract syntax trees (ASTs) almost never exactly match the gold standard. Quirk et al. view an AST as a set of productions and compute balanced F1 instead which we also adopt. The first column in Table 5 shows the percentage of channels selected correctly for both triggers and actions. The second column measures accuracy for both channels and functions. The last column shows balanced F1-measure against the gold tree over all produc-\ntions in the proposed derivation. We compare our model against posclass, the method introduced in Quirk et al. and several of their baselines. posclass is reminiscent of KRISP (Kate and Mooney, 2006), it learns distributions over productions given input sentences represented as a bag of linguistic features. The retrieval baseline searches for the closest description in the training data based on character string-edit-distance and returns the recipe for that training program. The phrasal method uses phrasebased machine translation to generate the recipe, whereas sync extracts synchronous grammar rules from the data, essentially recreating WASP (Wong and Mooney, 2006). Finally, the classifier approach uses a binary classifier to predict whether a production should be present in the derivation tree corresponding to the description.\nQuirk et al. (2015) report results on the full test data and smaller subsets after noise filtering, e.g., when non-English and unintelligible descriptions are removed (Tables 5a and 5b). They also ran their system on a high-quality subset of descriptionprogram pairs which were found in the gold standard and at least three humans managed to independently reproduce (Table 5c). As can be seen, across all subsets seq2seq outperforms posclass and related baselines. Compared to the previous datasets, the attention mechanism and the method used to handle rare words yield less of an improvement here. This may be due to the size of Quirk et al. (2015) and the way it was created – user curated descriptions are often of low quality, and thus align very loosely to their corresponding ASTs."
    }, {
      "heading" : "4.4 Model Analysis",
      "text" : "Figure 4 illustrates examples of alignments produced by our model. Matrices of attention scores are computed using Equation (5) and are represented in grayscale. Aligned input words and logical form predicates are enclosed in (same color) rectangles whose overlapping areas contain the attention scores. Input sentences are reversed (Sutskever et al., 2014) for better performance. Also notice that attention scores are computed by LSTM hidden vectors which encode context information rather than just the words in their current positions. The examples demonstrate that the attention mechanism learns soft alignments between input sentences and output logical forms, performing surprisingly well, even in cases of reordering (Figure 4b, 4c and 4d) and one-to-many alignments (Figure 4c).\nWe also explore whether the length of natural language descriptions and logical forms influences performance. Figure 5 shows how accuracy varies with different length ranges on the ATIS development set. Overall, we observe that accuracy drops as sequence length becomes larger, because longer sequences tend to have more complex meanings and be more compositional. We also find that the attention mechanism consistently boosts performance\nacross all ranges of length. Moreover, adding attention allows the system to better cope with longer sequences compared to the vanilla seq2seq method."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper we presented a sequence-to-sequence model for semantic parsing. Given natural language descriptions as input our model predicts meaning representations as output. Both input sentences and output logical forms are regarded as sequences, and are encoded and decoded by different recurrent neural networks. Our experimental results show that enhancing the model with an attention mechanism improves performance across the board, especially for long sequences. We also show that our method of handling rare words (via masking and post-processing) is effective and boosts performance. Extensive comparisons with previous approaches demonstrate that our model performs competitively, without recourse to domain- or representation-specific features. Directions for future work are many and varied. For example, it would be interesting to learn a model from questionanswer pairs without access to target logical forms. We would also like to use the model for question answering over a knowledge base, e.g., by ranking\nthe predicate paths between entities (figuring in the queries) and candidate answers."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Semantic parsing aims at mapping natural<lb>language to machine interpretable meaning<lb>representations. Traditional approaches rely<lb>on high-quality lexicons, manually-built tem-<lb>plates, and linguistic features which are either<lb>domainor representation-specific. In this pa-<lb>per, we present a general method based on<lb>an attention-enhanced sequence-to-sequence<lb>model. We encode input sentences into vec-<lb>tor representations using recurrent neural net-<lb>works, and generate their logical forms by<lb>conditioning the output on the encoding vec-<lb>tors. The model is trained in an end-to-end<lb>fashion to maximize the likelihood of target<lb>logical forms given the natural language in-<lb>puts. Experimental results on four datasets<lb>show that our approach performs competi-<lb>tively without using hand-engineered features<lb>and is easy to adapt across domains and mean-<lb>ing representations.",
    "creator" : "TeX"
  }
}