{
  "name" : "1702.06925.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TRANSFERRING FACE VERIFICATION NETS TO PAIN AND EXPRESSION REGRESSION",
    "authors" : [ "Feng Wang", "Xiang Xiang", "Chang Liu", "Trac D. Tran", "Austin Reiter", "Gregory D. Hager", "Harry Quon", "Jian Cheng", "Alan L. Yuille" ],
    "emails" : [ "xxiang@cs.jhu.edu)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms— CNN, regression, expression intensity"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Obtaining accurate patient-reported pain assessments is important to effectively manage pain. Hence, it is highly desired to develop an automated approach. Firstly, it simplifies the pain reporting process and reduce the strain on manual efforts. Secondly, it standardizes the feedback mechanism by ensuring a single automated metric that performs all assessments and thus reduces bias. Thirdly, it enables giving continuousvalued pain levels. There exist efforts to measure pain using the observational or behavioral effect caused by pain such as physiological data. Medasense©has developed medical devices for objective pain monitoring. Their basic premise is that pain may cause the vital signs such as blood pressure, pulse rate, respiration rate, SpO2 from EMG, ECG or EEG, alone or in combination to change and often to increase. However, it takes much more efforts to obtain physiological data than face images and videos from unobtrusive cameras.\n* Corresponding author (e-mail: xxiang@cs.jhu.edu).\nComputer vision and supervised learning has come a long way in recent years, redefining the state-of-the-art using deep Convolutional Neural Networks (CNNs). However, the ability to train deep CNNs for automated pain assessment is limited by small datasets associated with labels of patientreported pain levels, i.e., annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3]. In particular we perform experiments on the Shoulder-Pain dataset which is designed for visual analysis only. The dataset contains 200 videos of 25 patients who repeatedly raise their arms (feeling pain) and then put them down (pain released). All frames per video are labeled with discrete-valued ground-truth pain levels, as illustrated in Fig. 1. Note that 91.35% of all images are labeled as pain level 0 which may induce trivial solutions.\nTwo pieces of recent works make progress in estimating pain intensity visually using the Shoulder-Pain dataset only: Ordinal Support Vector Regression (OSVR) [4] and Recurrent Convolutional Regression (RCR) [5]. Notably, RCR\nar X\niv :1\n70 2.\n06 92\n5v 1\n[ cs\n.C V\n] 2\n2 Fe\nb 20\n[5] is trained end-to-end achieving sub-optimal performance. Please see reference therein for other existing works.\nAlthough the limited labeled data prevents us from directly training a deep pain intensity regressor, we show that fine-tuning from a data-extensive pre-trained domain such as face verification can alleviate this problem, While our work is not the first attempt of this transferring idea, to our knowledge we are the first to apply it in pain intensity estimation.\nTo summarize, the main contributions of this work are: •We address limited data with expression intensity labels by transferring face verification models to new tasks. •We push the bound of the performance of the expression intensity estimation by a large margin. •We propose to add center loss regularization to make the predicted values more close to discrete labels. •A novel evaluation metric is proposed to fairly judge the performance on imbalanced dataset, such as the video-based Shoulder-Pain [1] where mostly painless expression occurs."
    }, {
      "heading" : "2. RELATED WORKS",
      "text" : "For facial expression recognition in general, there is a tradeoff between method simplicity and performance, i.e., imagebased [6, 7] vs. video-based [8, 9, 10] methods. Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14]. Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17]. Similar tradeoff also lies in linear model vs. non-linear models. Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21]. By introducing more information, one approach is 3D models [22] while another is multi-modal models [23].\nAs regards transfer learning with deep networks, there exist recent works that regularize deep face recognition nets for expression classification - FaceNet2ExpNet [6]. During pretraining, they train convolutional layers of the expression net, regularized by the deep face recognition net. In the refining stage, they append fully-connected layers to the pre-trained convolutional layers and train the whole network jointly."
    }, {
      "heading" : "3. TRANSFERRED DEEP REGRESSIOR",
      "text" : "A face f is visually generated by confounding factors the primary of which are the identity i, the facial expression e and the head pose p. Normally, given a set of rich training examples (f , i), deep face verification algorithms [24] seek a function g : F → I where F is the input space spanned by all possible face appearances and I is the output space formed by all possible identities. Now, given g and limited training samples (f , e), the problem that we are addressing is learning a function h : F→ E where E is the output space formed by all possible face expressions. A natural question is if we are able to transfer g to h. In this section, we investigate how g and h can be related, since they are mappings from the same\ninput space to different output space. Namely, given g, we design a network to learn h by refining g with a few additional training examples of (f , e).\nOur network is transferred from a state-of-the-art face feature learning and verification network [24]1 which is trained using the CASIA-WebFace dataset contaning 0.5 million face images with identity labels. Now, we transfer the network to learn features for pain intensity regression with limited face images with pain labels. In detail, we remove all the fullyconnected (FC) layers in the original network and then add two new FC layers. Since over-fitting is a severe problem when training with limited data, the number of neurons in our hidden FC layer is relatively smaller than those in the original network (50 vs 512), and Dropout[25] operation is applied before the two FC layers. We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 51+e−x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].\nThe architecture of the new added layers is shown in Fig. 2, details of which will be explained in the following sections."
    }, {
      "heading" : "3.1. Regression Loss",
      "text" : "At the end of the network, we leverage both `1 and `2 norm distance to do the regression task.\nLR = ‖y − ỹ‖pp (1)\nwhere I is the input image; y ∈ R1 is the output of the activated output of the last fully connected layer; ỹ is the ground truth label and p denotes which norm should be used.\nIn practice, whether to choose `1-norm or `2-norm depends on the evaluation metrics, e.g. `2-norm performs better on Mean Square Error (MSE) and `1-norm performs better on Mean Abosolute Error (MAE). However, we found that\n1Model available at https://github.com/ydwen/caffe-face\ndirectly applying the `2-norm distance layer would cause the gradient exploding problem because of the big gradient magnitude in initial iterations. This phenomenon is also described in [26]. To solve this problem, we follow [26] to use a smooth `1 loss instead of `2 loss, to make the gradients smaller when the error |y − ỹ| is very large. The smooth `1 is defined as\nLR = {\n0.5|y − ỹ|2 if |y − ỹ| < t |y − ỹ| − t+ 0.5t2 otherwise (2)\nwhere x is the output of the last layer and t is the turning point between `2 distance and `1 distance. When t = 1, it works similar with `2-norm loss since the final error is usually below 1. When t = 0, it just equals with `1-norm loss. In this way, we can implement our regression loss in one layer."
    }, {
      "heading" : "3.2. Regularizing by Reducing Inter-Class Variance",
      "text" : "Since the pain intensity level is discretely labeled specifically in the Shoulder-Pain dataset, it is natural to add some kinds classification signals on the loss function to make the regressed values to be more ‘discrete’. In this work, we try two kinds of classification loss functions, one is the Softmax,\nLS = −log WTỹ x+ bỹ∑n i=1 W T i x+ bi , (3)\nanother is center loss [24],\nLC = −‖x− cỹ‖pp, (4)\nwhere x is the output of the second last layer, ỹ is the corresponding label, cỹ represent the center for class ỹ and p denote which norm should be selected. The classification losses are added after the first fully connected layer, acting on training\nthe neural network simultaneously with the regression loss. Different from the description of center loss in [24], we jointly learn the centers and minimize the distances within classes by gradient descending, while [24]’s centers are learned in a moving-average way.\nThe main difference between the two kinds of classification loss is that the Softmax loss optimizes both distances between and within classes, while the center loss only concerns about distances within class (Fig. 3). From the Figure, it can be inferred that there is some degree of conflict between Softmax loss and the regression loss. We hope that the calibration is determined by the regression loss only to get the minimal error, but Softmax still tries to enlarge the distances between classes, which makes the calibrated scale on the coordinate axis invalid. The center loss, which tries to shrink the distances within classes only, would have far less influence on the calibration of the regression loss."
    }, {
      "heading" : "4. EXPERIMENTS",
      "text" : "In this section, we present implementations and experiments. The project page2 has been set up with programs and data."
    }, {
      "heading" : "4.1. Dataset and Training Details",
      "text" : "We test our network on the UNBC-McMaster Shoulder-Pain dataset [1] that is widely used for benchmarking intensity estimations of the pain expression in particular and facial action units in general. The dataset comes with four types of labels. The three annotated online during the video collection are the sensory scale (SEN), affective scale (AFF) and visual analog scale (VAS) ranging from 0 (no pain) to 15 (severe pain). Additionally, observers rated pain intensity (OPI) offline from recorded videos ranging from 0 (no pain) to 5 (severe pain). In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].\nThe face verification network [24] is trained on CASIAWebface dataset [28], which contains 494,414 training images from 10,575 identities. To be consistent with face verification, we perform the same pre-processing on the images of Shoulder-Pain dataset. To be specific, we leverage MTCNN model [29] to detect faces and facial landmarks. Then the faces are aligned according to the detected landmarks.\nThe learning rate is set to 0.0001 to avoid huge modification on the convolution layers. The network is trained over 5,000 iterations, which is reasonable for the networks to converge observed in a few cross validation folds. We set the weight of the regression loss to be 1 and the weights of Softmax loss and center loss to be 1 and 0.01 respectively."
    }, {
      "heading" : "4.2. Pain Intensity Regression",
      "text" : "We run leave-one-out cross validation 25 times on the Shoulder-Pain dataset. Each time, the videos of one patient\n2https://github.com/happynear/PainRegression.\nare reserved for testing. All the other videos are used to train the deep regression network. The performance is summarized in Table 1. It can be concluded from the table that our algorithm performs best or equally best on various evaluation metrics, especially the combination of smooth `1 loss and `1 center loss. Even though the MAE of using Softmax loss as regularization is also competitive, we find that it just learns to predict more zeros by observing the predicted curve."
    }, {
      "heading" : "4.3. Class Imbalance Problem",
      "text" : "In Table 1, we provide the performance of predicting all zeros as a baseline. Interestingly, on the metrics MAE and MSE, zero prediction performs much better than several state-ofthe-art algorithms. This is because the Shoulder-Pain dataset is highly imbalanced. 91.35% frames are labeled as pain level 0. Thus, for the regression algorithms, it is relatively safe to predict the pain level to be zero.\nTo fairly evaluate the performance, we propose the weighted version of evaluation metrics i.e. weighted MAE (wMAE) and MSE (wMSE) to address the dataset imbalance issue. For example, the wMAE is simply the mean of MAE on each pain level. In this way, the MAE is weighted by the population of each pain level.\nWe apply two techniques to sample the training data to make our training set more consistent with the new metrics. First, we eliminate the redundant frames on the sequences following [4]. If the intensity level remains the same for more than 5 consecutive frames, we choose the first one as representative frame. Second, during training, we uniformly sample images from the 6 classes to feed into the network. In this way, what the neural network ‘see’ is a total balanced dataset.\nUsing the new proposed metrics, the performance is summarized in Table 2. It can be drawn that the uniform class sampling strategy performs much better on the new evalu-\nation metrics. Simply predicting all zeros no longer yields good performance. We have not compared with other works using the proposed new metrics because we find it challenging to obtain the predicted values of other works. However, we will provide the evaluation program in our project page and encourage future works to report their performance with the new evaluation metrics."
    }, {
      "heading" : "4.4. Facial Action Unit Recognition in General",
      "text" : "Replacing the regression layer with a Softmax layer, we apply our proposed method on the EmotionNet Challenge3, a facial expression recognition in the wild challenge organized by the CBCSL of Ohio State University. The competition has two tracks. The first one requires the detection of 11 action units (AUs), and the second one is an emotion recognition task. 26,117 and 2,477 labeled images are provided by the organizer for training the two tracks respectively. The predictions generated by our program are submitted to the organizer and they further evaluate our program’s output according to the metrics they previously defined. For the AU detection track, we got second place with averaged metrics of 0.7101, while the best is 0.7290. Our individual F-scores are all the top (F1 is 0.6405, F2 is 0.6354 and F.5 is 0.6380). For the emotion recognition task, we also got the second place. Our final score is 0.4799 while the top is 0.5968. It is noteworthy that the winning team comes from an industrial face verification company, who is capable of additional training data, and we only use the data provided by the organizer."
    }, {
      "heading" : "5. SUMMARY",
      "text" : "Given the restriction of labeled data which prevents us to directly train a deep pain intensity regressor, fine-tuning from a data-extensive pre-trained domain such as identities can alleviate the problem. In this paper, we transfer a face verification network for pain intensity regression. The fine-tuned transferred network with a regression layer is tested on the UNBC-McMaster Shoulder-Pain dataset and achieves stateof-the-art performance on pain intensity estimation.\n3For challenge details and results, please see http://cbcsl.ece. ohio-state.edu/EmotionNetChallenge/."
    }, {
      "heading" : "6. REFERENCES",
      "text" : "[1] Patrick Lucey, Jeffrey F Cohn, Kenneth M Prkachin, Patricia E Solomon, and Iain Matthews, “Painful data: The unbcmcmaster shoulder pain expression archive database,” in IEEE International Conference on Automatic Face & Gesture Recognition, 2011, pp. 57–64.\n[2] Min Aung, Sebastian Kaltwang, Bernardino Romera-Paredes, Brais Martinez, Aneesha Singh, Matteo Cella, Michel Valstar, Hongying Meng, Andrew Kemp, Aaron Elkins, et al., “The automatic detection of chronic pain-related expression: requirements, challenges and a multimodal dataset,” IEEE Trans. Affective Computing.\n[3] Philipp Werner, Ayoub Al-Hamadi, Robert Niese, Steffen Walter, Sascha Gruss, and Harald C Traue, “Towards pain monitoring: Facial expression, head pose, a new database, an automatic system and remaining challenges,” in BMVC, 2013.\n[4] Rui Zhao, Quan Gan, Shangfei Wang, and Qiang Ji, “Facial expression intensity estimation using ordinal information,” in CVPR, 2016.\n[5] Jing Zhou, Xiaopeng Hong, Fei Su, and Guoying Zhao, “Recurrent convolutional neural network regression for continuous pain intensity estimation in video,” CVPR workshops, 2016.\n[6] Hui Ding, Shaohua Kevin Zhou, and Rama Chellappa, “Facenet2expnet: Regularizing a deep face recognition net for expression recognition,” in FG, 2017.\n[7] C Fabian Benitez-Quiroz, Ramprakash Srinivasan, and Aleix M Martinez, “Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild,” in CVPR, 2016, pp. 5562–5570.\n[8] Kaili Zhao, Wen-Sheng Chu, Fernando De la Torre, Jeffrey F. Cohn, and Honggang Zhang, “Joint patch and multi-label learning for facial action unit detection,” in CVPR, 2015.\n[9] Ping Liu, Joey Tianyi Zhou, Ivor Wai-Hung Tsang, Zibo Meng, Shizhong Han, and Yan Tong, “Feature disentangling machinea novel approach of feature selection and disentangling in facial expression analysis,” in ECCV, 2014, pp. 151–166.\n[10] Salah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent, and Mehdi Mirza, “Disentangling factors of variation for facial expression recognition,” in ECCV, 2012, pp. 808–822.\n[11] Arnaud Dapogny, Kevin Bailly, and Severine Dubuisson, “Pairwise conditional random forests for facial expression recognition,” in ICCV, 2015.\n[12] Mengyi Liu, Shiguang Shan, Ruiping Wang, and Xilin Chen, “Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition,” in CVPR, 2014, pp. 1749–1756.\n[13] Ziheng Wang, Shangfei Wang, and Qiang Ji, “Capturing complex spatio-temporal relations among facial muscles for facial expression recognition,” in CVPR, 2013, pp. 3422–3429.\n[14] Yimo Guo, Guoying Zhao, and Matti Pietikäinen, “Dynamic facial expression recognition using longitudinal facial expression atlases,” in ECCV, pp. 631–644. 2012.\n[15] Ognjen Rudovic, Vladimir Pavlovic, and Maja Pantic, “Multioutput laplacian dynamic ordinal regression for facial expression recognition and intensity estimation,” in CVPR, 2012, pp. 2634–2641.\n[16] Minyoung Kim and Vladimir Pavlovic, “Structured output ordinal regression for dynamic facial emotion intensity prediction,” in ECCV, 2010, pp. 649–662.\n[17] Peng Yang, Qingshan Liu, and Dimitris N Metaxas, “Exploring facial expressions with compositional features,” in CVPR, 2010, pp. 2638–2644.\n[18] C. Fabian Benitez-Quiroz, Ramprakash Srinivasan, and Aleix M. Martinez, “Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild,” in CVPR, 2016.\n[19] Xiangyun Zhao, Xiaodan Liang, Luoqi Liu, Teng Li, Yugang Han, Nuno Vasconcelos, and Shuicheng Yan, “Peak-piloted deep network for facial expression recognition,” in ECCV, 2016, pp. 425–442.\n[20] Heechul Jung, Sihaeng Lee, Junho Yim, Sunjeong Park, and Junmo Kim, “Joint fine-tuning in deep neural networks for facial expression recognition,” in ICCV, 2015.\n[21] Ping Liu, Shizhong Han, Zibo Meng, and Yan Tong, “Facial expression recognition via a boosted deep belief network,” in CVPR, 2014, pp. 1805–1812.\n[22] Hui Chen, Jiangdong Li, Fengjun Zhang, Yang Li, and Hongan Wang, “3d model-based continuous emotion recognition,” in CVPR, 2015.\n[23] Zheng Zhang, Jeff M. Girard, Yue Wu, Xing Zhang, Peng Liu, Umur Ciftci, Shaun Canavan, Michael Reale, Andy Horowitz, Huiyuan Yang, Jeffrey F. Cohn, Qiang Ji, and Lijun Yin, “Multimodal spontaneous emotion corpus for human behavior analysis,” in CVPR, 2016.\n[24] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao, “A discriminative feature learning approach for deep face recognition,” in ECCV, 2016, pp. 499–515.\n[25] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, “Dropout: a simple way to prevent neural networks from overfitting.,” Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\n[26] Ross Girshick, “Fast r-cnn,” in ICCV, 2015.\n[27] Ognjen Rudovic, Vladimir Pavlovic, and Maja Pantic, “Automatic pain intensity estimation with heteroscedastic conditional ordinal random fields,” in International Symposium on Visual Computing, 2013, pp. 234–243.\n[28] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li, “Learning face representation from scratch,” arXiv preprint arXiv:1411.7923, 2014.\n[29] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and alignment using multitask cascaded convolutional networks,” IEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499–1503, 2016."
    } ],
    "references" : [ {
      "title" : "Painful data: The unbcmcmaster shoulder pain expression archive database",
      "author" : [ "Patrick Lucey", "Jeffrey F Cohn", "Kenneth M Prkachin", "Patricia E Solomon", "Iain Matthews" ],
      "venue" : "IEEE International Conference on Automatic Face & Gesture Recognition, 2011, pp. 57–64.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The automatic detection of chronic pain-related expression: requirements, challenges and a multimodal dataset",
      "author" : [ "Min Aung", "Sebastian Kaltwang", "Bernardino Romera-Paredes", "Brais Martinez", "Aneesha Singh", "Matteo Cella", "Michel Valstar", "Hongying Meng", "Andrew Kemp", "Aaron Elkins" ],
      "venue" : "IEEE Trans. Affective Computing.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Towards pain monitoring: Facial expression, head pose, a new database, an automatic system and remaining challenges",
      "author" : [ "Philipp Werner", "Ayoub Al-Hamadi", "Robert Niese", "Steffen Walter", "Sascha Gruss", "Harald C Traue" ],
      "venue" : "BMVC, 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Facial expression intensity estimation using ordinal information",
      "author" : [ "Rui Zhao", "Quan Gan", "Shangfei Wang", "Qiang Ji" ],
      "venue" : "CVPR, 2016.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Recurrent convolutional neural network regression for continuous pain intensity estimation in video",
      "author" : [ "Jing Zhou", "Xiaopeng Hong", "Fei Su", "Guoying Zhao" ],
      "venue" : "CVPR workshops, 2016.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Facenet2expnet: Regularizing a deep face recognition net for expression recognition",
      "author" : [ "Hui Ding", "Shaohua Kevin Zhou", "Rama Chellappa" ],
      "venue" : "FG, 2017.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "author" : [ "C Fabian Benitez-Quiroz", "Ramprakash Srinivasan", "Aleix M Martinez" ],
      "venue" : "CVPR, 2016, pp. 5562–5570.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Joint patch and multi-label learning for facial action unit detection",
      "author" : [ "Kaili Zhao", "Wen-Sheng Chu", "Fernando De la Torre", "Jeffrey F. Cohn", "Honggang Zhang" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Feature disentangling machinea novel approach of feature selection and disentangling in facial expression analysis",
      "author" : [ "Ping Liu", "Joey Tianyi Zhou", "Ivor Wai-Hung Tsang", "Zibo Meng", "Shizhong Han", "Yan Tong" ],
      "venue" : "ECCV, 2014, pp. 151–166.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Disentangling factors of variation for facial expression recognition",
      "author" : [ "Salah Rifai", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent", "Mehdi Mirza" ],
      "venue" : "ECCV, 2012, pp. 808–822.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Pairwise conditional random forests for facial expression recognition",
      "author" : [ "Arnaud Dapogny", "Kevin Bailly", "Severine Dubuisson" ],
      "venue" : "ICCV, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition",
      "author" : [ "Mengyi Liu", "Shiguang Shan", "Ruiping Wang", "Xilin Chen" ],
      "venue" : "CVPR, 2014, pp. 1749–1756.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Capturing complex spatio-temporal relations among facial muscles for facial expression recognition",
      "author" : [ "Ziheng Wang", "Shangfei Wang", "Qiang Ji" ],
      "venue" : "CVPR, 2013, pp. 3422–3429.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Dynamic facial expression recognition using longitudinal facial expression atlases",
      "author" : [ "Yimo Guo", "Guoying Zhao", "Matti Pietikäinen" ],
      "venue" : "ECCV, pp. 631–644. 2012.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multioutput laplacian dynamic ordinal regression for facial expression recognition and intensity estimation",
      "author" : [ "Ognjen Rudovic", "Vladimir Pavlovic", "Maja Pantic" ],
      "venue" : "CVPR, 2012, pp. 2634–2641.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Structured output ordinal regression for dynamic facial emotion intensity prediction",
      "author" : [ "Minyoung Kim", "Vladimir Pavlovic" ],
      "venue" : "ECCV, 2010, pp. 649–662.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exploring facial expressions with compositional features",
      "author" : [ "Peng Yang", "Qingshan Liu", "Dimitris N Metaxas" ],
      "venue" : "CVPR, 2010, pp. 2638–2644.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "author" : [ "C. Fabian Benitez-Quiroz", "Ramprakash Srinivasan", "Aleix M. Martinez" ],
      "venue" : "CVPR, 2016.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Peak-piloted deep network for facial expression recognition",
      "author" : [ "Xiangyun Zhao", "Xiaodan Liang", "Luoqi Liu", "Teng Li", "Yugang Han", "Nuno Vasconcelos", "Shuicheng Yan" ],
      "venue" : "ECCV, 2016, pp. 425–442.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Joint fine-tuning in deep neural networks for facial expression recognition",
      "author" : [ "Heechul Jung", "Sihaeng Lee", "Junho Yim", "Sunjeong Park", "Junmo Kim" ],
      "venue" : "ICCV, 2015.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Facial expression recognition via a boosted deep belief network",
      "author" : [ "Ping Liu", "Shizhong Han", "Zibo Meng", "Yan Tong" ],
      "venue" : "CVPR, 2014, pp. 1805–1812.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "3d model-based continuous emotion recognition",
      "author" : [ "Hui Chen", "Jiangdong Li", "Fengjun Zhang", "Yang Li", "Hongan Wang" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Multimodal spontaneous emotion corpus for human behavior analysis",
      "author" : [ "Zheng Zhang", "Jeff M. Girard", "Yue Wu", "Xing Zhang", "Peng Liu", "Umur Ciftci", "Shaun Canavan", "Michael Reale", "Andy Horowitz", "Huiyuan Yang", "Jeffrey F. Cohn", "Qiang Ji", "Lijun Yin" ],
      "venue" : "CVPR, 2016.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A discriminative feature learning approach for deep face recognition",
      "author" : [ "Yandong Wen", "Kaipeng Zhang", "Zhifeng Li", "Yu Qiao" ],
      "venue" : "ECCV, 2016, pp. 499–515.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1929
    }, {
      "title" : "Fast r-cnn",
      "author" : [ "Ross Girshick" ],
      "venue" : "ICCV, 2015.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Automatic pain intensity estimation with heteroscedastic conditional ordinal random fields",
      "author" : [ "Ognjen Rudovic", "Vladimir Pavlovic", "Maja Pantic" ],
      "venue" : "International Symposium on Visual Computing, 2013, pp. 234–243.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning face representation from scratch",
      "author" : [ "Dong Yi", "Zhen Lei", "Shengcai Liao", "Stan Z Li" ],
      "venue" : "arXiv preprint arXiv:1411.7923, 2014.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Joint face detection and alignment using multitask cascaded convolutional networks",
      "author" : [ "K. Zhang", "Z. Zhang", "Z. Li", "Y. Qiao" ],
      "venue" : "IEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499–1503, 2016.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Face Samples are also provided to give an intuitive feeling about the Shoulder-Pain dataset[1].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : ", annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : ", annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : ", annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "Two pieces of recent works make progress in estimating pain intensity visually using the Shoulder-Pain dataset only: Ordinal Support Vector Regression (OSVR) [4] and Recurrent Convolutional Regression (RCR) [5].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : "Two pieces of recent works make progress in estimating pain intensity visually using the Shoulder-Pain dataset only: Ordinal Support Vector Regression (OSVR) [4] and Recurrent Convolutional Regression (RCR) [5].",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 4,
      "context" : "[5] is trained end-to-end achieving sub-optimal performance.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "•A novel evaluation metric is proposed to fairly judge the performance on imbalanced dataset, such as the video-based Shoulder-Pain [1] where mostly painless expression occurs.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : ", imagebased [6, 7] vs.",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : ", imagebased [6, 7] vs.",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "video-based [8, 9, 10] methods.",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "video-based [8, 9, 10] methods.",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 9,
      "context" : "video-based [8, 9, 10] methods.",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].",
      "startOffset" : 170,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].",
      "startOffset" : 170,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].",
      "startOffset" : 170,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 18,
      "context" : "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "By introducing more information, one approach is 3D models [22] while another is multi-modal models [23].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 22,
      "context" : "By introducing more information, one approach is 3D models [22] while another is multi-modal models [23].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "As regards transfer learning with deep networks, there exist recent works that regularize deep face recognition nets for expression classification - FaceNet2ExpNet [6].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 23,
      "context" : "Normally, given a set of rich training examples (f , i), deep face verification algorithms [24] seek a function g : F → I where F is the input space spanned by all possible face appearances and I is the output space formed by all possible identities.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "Our network is transferred from a state-of-the-art face feature learning and verification network [24]1 which is trained using the CASIA-WebFace dataset contaning 0.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "Since over-fitting is a severe problem when training with limited data, the number of neurons in our hidden FC layer is relatively smaller than those in the original network (50 vs 512), and Dropout[25] operation is applied before the two FC layers.",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 5 1+e−x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].",
      "startOffset" : 65,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 5 1+e−x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 5 1+e−x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].",
      "startOffset" : 212,
      "endOffset" : 218
    }, {
      "referenceID" : 25,
      "context" : "This phenomenon is also described in [26].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 25,
      "context" : "To solve this problem, we follow [26] to use a smooth `1 loss instead of `2 loss, to make the gradients smaller when the error |y − ỹ| is very large.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 23,
      "context" : "another is center loss [24],",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 23,
      "context" : "Different from the description of center loss in [24], we jointly learn the centers and minimize the distances within classes by gradient descending, while [24]’s centers are learned in a moving-average way.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "Different from the description of center loss in [24], we jointly learn the centers and minimize the distances within classes by gradient descending, while [24]’s centers are learned in a moving-average way.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "We test our network on the UNBC-McMaster Shoulder-Pain dataset [1] that is widely used for benchmarking intensity estimations of the pain expression in particular and facial action units in general.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].",
      "startOffset" : 34,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].",
      "startOffset" : 34,
      "endOffset" : 44
    }, {
      "referenceID" : 26,
      "context" : "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].",
      "startOffset" : 34,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].",
      "startOffset" : 152,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "The face verification network [24] is trained on CASIAWebface dataset [28], which contains 494,414 training images from 10,575 identities.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 27,
      "context" : "The face verification network [24] is trained on CASIAWebface dataset [28], which contains 494,414 training images from 10,575 identities.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : "To be specific, we leverage MTCNN model [29] to detect faces and facial landmarks.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "OSVR-L1 (CVPR16) [4] 1.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "OSVR-L2 (CVPR16) [4] 0.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "601 RCNN (CVPR16w) [5] N/A 1.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "First, we eliminate the redundant frames on the sequences following [4].",
      "startOffset" : 68,
      "endOffset" : 71
    } ],
    "year" : 2017,
    "abstractText" : "Limited annotated data is available for the research of estimating facial expression intensities, which makes the training of deep networks for automated expression assessment very challenging. Fortunately, fine-tuning from a data-extensive pre-trained domain such as face verification can alleviate the problem. In this paper, we propose a transferred network that fine-tunes a state-of-the-art face verification network using expression-intensity labeled data with a regression layer. In this way, the expression regression task can benefit from the rich feature representations trained on a huge amount of data for face verification. The proposed transferred deep regressor is applied in estimating the intensity of facial action units (2017 EmotionNet Challenge) and in particular pain intensity estimation (UNBS-McMaster Shoulder-Pain dataset). It wins the second place in the challenge and achieves the stateof-the-art performance on Shoulder-Pain dataset. Particularly for Shoulder-Pain with the imbalance issue of different pain levels, a novel weighted evaluation metric is proposed.",
    "creator" : "LaTeX with hyperref package"
  }
}