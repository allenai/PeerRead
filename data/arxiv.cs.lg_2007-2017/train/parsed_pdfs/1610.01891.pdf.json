{
  "name" : "1610.01891.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A New Data Representation Based on Training Data Characteristics to Extract Drug Named-Entity in Medical Text",
    "authors" : [ "Sadikin Mujiono", "Mohamad Ivan Fanany", "Chan Basaruddin" ],
    "emails" : [ "mujiono.sadikin@mercubuana.ac.id" ],
    "sections" : [ {
      "heading" : null,
      "text" : "One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645.\nKeywords— drug name entity, word embedding, MLP, DBN, SAE, LSTM\nIntroduction\nThe rapid growth of information technology provides rich text data resources in all areas, including the medical field. An abundant amount of medical text data can be used to obtain valuable information for the benefit of many purposes. The understanding of drug interactions, for example, is an important aspect of manufacturing new medicines or controlling drug distribution in the market. The process to produce a medicinal product is an expensive and complex task. In many recent cases, however, many drugs are withdrawn from the market when it was discovered that the interaction between the drugs is hazardous to health [27].\n1/25\nar X\niv :1\n61 0.\n01 89\n1v 1\n[ cs\n.C L\n] 6\nO ct\nInformation, or objects extraction, from an unstructured text document, is one of the most challenging studies in the text mining area. The difficulties of text information extraction keeps increasing due to the increasing size of corpora, continuous growth of human’s natural language, and the unstructured formatted data [33]. Among such valuable information are medical entities such as drug name, compound, and brand; disease name, and their relations, such as drug - drug interaction and drug - compound relation. We need a suitable method to extract such information. To embed those abundant data resources; however, many problems have to be tackled. For example, large data size, unstructured format, choosing the right NLP, and the limitation of annotated datasets.\nA more specific and valuable information contained in medical text data is a drug entity (drug name). Drug name recognition is a primary task of medical text data extraction since the drug finding is the essential element in solving other information extraction problems [18,37]. Among derivative work of drug name extractions are drug-drug interaction [12], drug adverse reaction [29], or other applications (information retrieval, decision support system, drug development or drug discovery) [32].\nCompared to other NER (Named-Entity Recognition) tasks, such as PERSON, LOCATION, EVENT, or TIME, drug name entity recognition faces more challenges. First, the drug name entities are usually unstructured texts [17] where the number of new entities is quickly growing over time. Thus, it is hard to create a dictionary which always includes the entire lexicon and is up to date [25]. Second, the naming of the drug also widely varies. The abbreviation and acronym increase the difficulties in determining the concepts referred by the terms. Third, many drug names contain a combination of non-word and word symbols [20].Fourth, the other problem in drug name extraction is that a single drug name might be represented by multiple tokens [10]. Due to the complexity in extracting multiple tokens for drugs, some researchers such as [2] even ignores that case in the MedLine & DrugBank training with the reason that the multiple tokens drug is only 18% of all drug names. It is different with another domain, i.e., entity names in the biomedical field are usually longer. Fifth, in some cases, the drug name is a combination of medical and general terms. Sixth, the lack of the labeled dataset is another problem; it has yet to be solved by extracting the drug name entities.\nThis paper presents three data representation techniques to extract drug name entities contained in the sentences of medical texts. For the first and the second techniques, we created an instance of the dataset as a tuple, which is formed from 5 vectors of words. In the first technique, the tuple was constructed from all sentences treated as a sequence, whereas in the second technique the tuple is made from each sentence treated as a sequence. The first and second techniques were evaluated with the standard MLP-NN model which is perfomed in the first experiment. In the second experiment, we use the second data representation technique which is also applied to the other NN model i.e. DBN and SAE. The third data representation, which assumes the text as sequential entities, was assessed with the recurrent NN model, LSTM. Those three data representation techniques are based on the word2vec value characteristics, i.e., their cosine and the Euclidean distance between the vectors of words.\nIn the first and second techniques, we apply three different scenarios to select the most possible words which represent the drug name. The scenarios are based on the characteristics of training data, i.e., drug words distribution that is usually assumed has a smaller frequency of appearance in the dataset sentences. The drug name candidate selections are as follows. In the first case, all test dataset is taken. In the second case, 2/3 of all test dataset is selected. In the third case, x/y(x < y) of the test dataset (where x and y are arbitrary integer numbers) are selected after clustering the test dataset into y clusters.\nIn the third experiment, based on the characteristics of the resulting word vectors of\n2/25\nthe trained word embedding, we formulate a sequence data representation which applied to RNN-LSTM. We used the Euclidian distance of the current input to the previous input as an additional feature besides its vector of words. In this study, the vector of words is provided by word embedding methods proposed by Mikolov [21].\nOur main important contributions in this study are:\n1. The new data representation techniques which do not require any external knowledge nor hand-crafted features.\n2. The drug extraction techniques based on the words distribution contained in the training data.\nOur proposed method is evaluated on DrugBank and MedLine medical open dataset obtained from SemEval 2013 Competition task 9.1, see http://www.cs.york.ac.uk/semeval-2013/task9.html, which is also used by [1, 2, 10]. The format of both medical texts is in English where some sentences contain drug name entities. In extracting drug entity names from the dataset, our data representation techniques give the best performance with F-score values 0.687 for MLP, 0.6700 for DBN, and 0.682 for SAE, whereas the third technique with LSTM gives the best F-score, i.e., 0.9430. The average F-score of the third technique is 0.8645, i.e., the best performance compared to the other previous methods.\nBy applying the data representation techniques, our proposed approach provides at least three advantages:\n1. The capability to identify multiple tokens as a single name entity.\n2. The ability to deal with the absence of any external knowledge in certain languages.\n3. No need to construct any additional features, such as characters type identification, orthography feature (lowercase or uppercase identification), or token position.\nThe rest sections of this paper are organized as follow: Section 1 explain some previous works dealing with name entity (and drug name as well) extraction from medical text sources. The framework, approach, and methodology to overcome the challenges of drug name extraction is presented in the Section 2. The section also describes dataset materials and experiment scenarios. Section 3 discusses the experiment results and its analysis while section 4 explains the achievement, the shortcoming, and the prospects of this study. The section also describes several potential explorations for future research."
    }, {
      "heading" : "1 Related Works",
      "text" : "The entity recognition in a biomedical text is an active research, and many methods have been proposed. For example, Gurinder et al. [25] summarizes their survey on various entity recognition approaches. The approaches can be categorized into three models: dictionary based, rule based, and learning based methods [17, 33]. A dictionary based approach uses a list of terms (term collection) to assist in predicting which targeted entity will be included in the predicted group. Although their overall precision is more accurate, their recall is poor since they anticipate less new terms. The rule-based approach defines a certain rule which describes such pattern formation surrounding the targeted entity. This rule can be a syntactic term or lexical term. Finally, the learning approach is usually based on statistical data characteristics to\n3/25\nbuild a model using machine learning techniques. The model is capable of automatically learn based on positive, neutral, and negative training data.\nDrug name extraction and their classification are one of the challenges in the Semantic Evaluation Task (SemEval 2013). The best-reported performance for this challenge was 71.5% in F-score [31]. Until now the studies to extract drug names still continue and many approaches have been proposed. CRF-based learning is the most common method utilized in the clinical text information extraction. CRF is used by one of the best [10] participants in SemEval challenges in the clinical text (https://www.cs.york.ac.uk/semeval-2013). As for the use of external knowledge aimed to increase the performance, the author [10] uses ChEBI (Chemical Entities of Biological Interest), i.e., a dictionary of small molecular entities. The best-achieved performance is 0.57 in F-score (for the overall dataset).\nA hybrid approach model, which combines statistical learning and dictionary based, is proposed by [30]. In their study, the author utilizes word2vec representation, CRF learning model and DINTO, a drug ontology. With this word2vec representation, targeted drug is treated as a current token in a context windows which consists of three tokens on the left and three tokens in the right. Additional features are included in the data representation such as pos tags, lemma in the windows context, an orthography feature as uppercase, lowercase, and mixed cap. The author also used Wikipedia text as an additional resource to perform word2vec representation training. The best F-score value in extracting the drug name provided by the method is 0.72.\nThe result of a CRF based active learning, which is applied to NER BIO (Beginning, Inside, Output) annotation token for extracting named entity in the clinical text, is presented in [6]. The framework of this active learning approach is a sequential process: initial model generation, querying, training, and iteration. The CRF Algorithm BIO approach was also studied by A. Ben et al. [1]. The features for the CRF algorithm is formulated based on token and linguistics feature and semantic feature. The best F-score achieved by this proposed method is 0.72.\nKorkontzelos et al. studied a combination of aggregated classifier, maximum entropy-multinomial classifier, and handcrafted feature to extract drug entity. [18].They classified drug and non-drug based on the token features formulation such as tokens windows, the current token, and 8 other hand-crafted features.\nAnother approach for discovering valuable information from clinical text data that adopts event-location extraction model was examined by J, Bjornoe et al. [2]. They use an SVM classifier to predict drug or non-drug entity which is applied to DrugBank dataset. The best performance achieved by their method is a 0.6 in F-score. The drawback of their approach is that it only deals with a single token drug name.\nTo overcome the ambiguity problem in NER mined from a medical corpus, a segment representation method has also been proposed by S. Keretna et al. [17]. Their approach treats each word as belonging to three classes, i.e., NE, not NE and an ambiguous class. The ambiguity of the class member is determined by identifying whether the word appears in more than one context or not. If so, this word falls into the ambiguous class. After three class segments are found, it is then applied to the classifier learning. Related to their approach, in our previous work, we propose a pattern learning that utilizes the regular expression surrounding drug names and their compounds [28]. The performance of our method is quite good with the average F-score being 0.81 but has a limitation in dealing with more unstructured text data.\nIn summarizing the related previous works on drug name entity extraction, we noted some drawbacks which need to be addressed. In general, almost all state of the art methods works based on ad-hoc external knowledge which is not always available. The requirement of the handcrafted feature is another difficult constraint since not all datasets contain such feature. An additional challenge that remained unsolved by the\n4/25\nprevious works is the problem of multiple tokens representation for a single drug name. This study proposes a new data representation technique to handle those challenges.\nOur proposed method is based only on the data distribution pattern and vector of words characteristics, so there is no need for external knowledge nor additional handcrafted features. To overcome the multiple tokens problem, we propose a new technique which treats a target entity as a set of tokens (a tuple) at once rather than treating the target entity as a single token surrounded by other tokens such as used by [30] or [4]. By addressing a set of the tokens as a single sample, our proposed method can predict whether a set of tokens is a drug name or not. In our first experiment, we evaluate the first and second data representation techniques and apply MLP learning model. In our second scenario, we choose the the second technique which gave the best result with MLP, and apply it to two different machine learning methods: DBN, and SAE. In our third experiment, we examined the third data representation technique which utilizes the Euclidian distance between successive words in a certain sentence of medical text. The third data representation is then fed into an LSTM model. Based on the resulted F-score value, the second experiment gives the best performance."
    }, {
      "heading" : "2 Method & Material",
      "text" : ""
    }, {
      "heading" : "2.1 Framework",
      "text" : "In this study, using the word2vec value characteristics, we conducted three experiments based on different data representation techniques. The first and second experiment examine conventional tuple data representation, whereas the third experiment examines sequence data representation. We describe the organization of these three experiments in this Section. In general, the proposed method to extract drug name entities in this study consists of two main phases. The first phase is a data representation to formulate the feature representation. In the second phase, model training, testing, and their evaluation is then conducted to evaluate the performance of the proposed method.\nThe proposed method of the first experiment consists of 4 steps (see Figure 1). The first step is a data representation formulation. The output of the first step are the tuples of training and testing dataset. The second step is dataset labeling which is\n5/25\napplied to both testing and training data. The step provides the label of each tuple. The third step is the candidate selection which is performed to minimize the noises since the actual drug target quantity is a far less compared to nondrug name. In the last step, we performed the experiment with MLP-NN model and its result evaluation. The detailed explanation of each step is explained in the Subsection 2.4, 2.6, and 2.9, whereas section 2.2 and 2.3 describe training data analysis as the foundation of this proposed method. As a part of the first experiment, we also evaluate the impact of the usage of the Euclidean distance average as the model’s regularization. This regularization term is described in the Subsection 2.7.1.\nThe framework of the second experiment which involves DBN and SAE learning model to the second data representation technique is illustrated in Figure 2. In general, the steps of the second experiment is similar to the first one, with its differences are the data representation used and the learning model involved. In the second experiment, it is used the second technique only with DBN and SAE as the learning model.\nThe framework of the third experiment using the LSTM is illustrated in Figure 3. There are tree steps in the third experiment. The first step is sequence data representation formulation which provides both sequence training data and testing data. The second step is data labeling which generates the label of training and testing data. LSTM experiment and its result evaluation are performed in the third step. The detail description of these tree step are presented in Subsection 2.4, 2.4.3, and Subsection 2.9 as well."
    }, {
      "heading" : "2.2 Training Data Analysis",
      "text" : "Each of the sentences in the dataset contains four data types i.e. drug, group, brand, and drug-n. If the sentence contains none of those four types, the type value is null. In the study, we extracted drug and drug-n. Overall in both DrugBank and MedLine datasets, the quantity of drug name target is far less compared to the non-drug target. Segura et al. [31] present the first basic statistics of the dataset. A more detailed exploration regarding token distribution in the training dataset is described in this section. The MedLine sentences training dataset contains 25.783 single token, which consists of 4.003 unique tokens. Those tokens distributions are not uniform, but are dominated by a small part of some unique tokens. If all of the unique tokens are\n6/25\narranged and ranked based on the most frequent appearances in the sentences, the quartile distribution will have the following result presented in Figure 4. Q1 represents the token number 1 to 1001 which their total of frequency is 20.688. Q2 represents the token number 1002 to 2002 which their total of frequency is 2.849. Q3 represents the token number 2003 to 3002 which their total of frequency is 1.264, and Q4 represents the token number 3003 to 4003 which their total of frequency is 1.000. The Figure shows that the majority appearances are dominated by only a small amount of the total tokens.\nFurther analysis of the dataset tokens shows that most of the drug names of the targeted token rarely appear in the dataset. When we divide those token collections into three partitions based on their sum of frequency, as presented in Table 1, it is shown that all of the drug name entity targeted are contained in 2/3 part with less frequent appearances of each token (a unique token in the same sum of frequency). The similar pattern of training data token distribution also emerged in the DrugBank dataset as illustrated in Figure 5 and Table 2. When we look into specific token distributions, the position of most of drug name target are in the third part. Since the most frequently appeared words in the first and the second parts are the most common words such as: stop words (”of”, ”the”, ”a”, ”end”, ”to”, ”where”, ”as”, ”from”, and such kind of words) and common words in medical domain such as ”administrator”, ”patient”, ”effect”, ”dose”, etc.\n8/25"
    }, {
      "heading" : "2.3 Word Embedding Analysis",
      "text" : "To represent the dataset we utilized the word embedding model proposed by Mikolov et al. [21]. We treated all of the sentences as a corpus after the training dataset and testing dataset were combined. The used word2vec training model was the CBOW (Continuous Bag Of Words) model with context window length 5, and the vector dimension 100. The result of the word2vec training is the representation of word in 100 dimension row vectors. Base on the row vector, it can be estimated the similarities or dissimilarities between words. The description below is the analysis summary of word2vec representation result which is used as a base reference for the data representation technique and the experiment scenarios. By taking some sample of drug targets and non-drug vector representation, it is shown that drug word has more similarities (cosine distance) to another drug than to non-drug and the vice versa. Some of those samples are illustrated in Table 3. We also computed the Euclidean distance between all of the words. Table 4 shows the average of Euclidean distance and cosine distance between drug-drug, drug - non-drug, and non-drug - non-drug. These values of the average distance show us that, intuitively, it is feasible to group the collection of the word into drug group and non-drug group based on their vector representations value."
    }, {
      "heading" : "2.4 Feature Representation, Data Formatting, & Data Labelling",
      "text" : "Based on the training data and word embedding analysis, we formulate the feature representation and its data formatting. In the first and second techniques, we try to overcome the multiple tokens drawback that left unsolved in [2] by formatting a single input data as an N - gram model with N=5 (one tuple data consist 5 tokens) to accommodate the maximum token which acts as a single drug entity target name. The tuples were provided from the sentences of both training and testing data. Thus, we\n9/25\nhave a set of tuples of training data and a set of tuples of testing data. Each tuple was treated as a single input.\nResult: Labelled dataset Input:array of tuple, array of drug ; output: array of label {Array of drug contains list of drug and drug-n only} ; label[]<=1 Initialization; for each t in tuple do\nfor each d in drug do if length (d) = 1 then\nif t[1] = d[1] then //match 1 token drug; label <= 2, break, exit from for each d in drug; else\nend\nelse if length (d) = 2 then\nif t[1] = d[1] and t[2] = d[2] then //match 2 tokens drug; label <= 3, break, exit from for each d in drug; else\nend\nelse if length (d) = 3 then\nif t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else\nend\nelse if length (d) = 4 then\nif t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else\nend\nelse if length (d) = 5 then\nif t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then\nlabel <= 6, break, exit from for each d in drug; else\nend\nelse\nend\nend\nend\nend\nend\nend\nend\nAlgorithm 1: Dataset Labelling\nTo identify a single input, whether it is a non-drug or drug target, we use a multi-classification approach which classifies the single input into one of six classes. class 1 represents non-drug whereas the other classes represent drug target which also identified how many tokens (word) that perform the drug target. To identify which class a certain tuple belongs to is determined as follows: The drug tuple is the tuple which its first token (token-1) is the drug type. If the token-1 is not a drug, regardless of whatever the rest of the 4 tokens are, then the tuple is classified as no drug. This kind of tuple is identified as class 1. If the token-1 is a drug, and token-2 is not a drug, regardless of the last 3 tokens, the tuple will be identified as class 2, and so on.\n10/25\nSince we only extracted the drug entity, we ignored the other token types, whether it is a group, brand, or another common token. To provide the label of each tuple, we only use the drug and drug-n types as the tuple reference list. In general, if the sequence of token in each tuple in dataset contains the sequence which is exactly same with one of tuple reference list members, then the tuple in dataset is identified as drug entity. The detail of the algorithm used to provide the label of each tuple in both training data and testing data is described in Algorithm 1:\nWe proposed two techniques in constructing the tuple set of the sentences. The first technique treats all sentences as one sequence, whereas in the second technique, each sentence is processed as one sequence. The first and the second techniques are evaluated with MLP, DBN, and SAE model. The third technique treats the sentences of dataset as a sequence where the occurrence of the current token is influenced by the previous one. By treating the sentence as a sequence not only in the data representation but also in the classification and recognition process, the most suitable model to be used is RNN. We applied RNN-LSTM to the third technique."
    }, {
      "heading" : "2.4.1 First Technique",
      "text" : "The first dataset formatting (one sequence for all sentences) is performed as follows. In the first step, all sentences in the dataset are formatted as a token sequence. Let the token sequence is:\nt1t2t3t4t5t6t7t8...tn\nwith n is number of token in the sequences, then the dataset format will be:\nt1t2t3t4t5; t2t3t4t5t6; . . . ..tn−4tn−3tn−2tn−1tn;\nA sample of sentences and their drug name are presented in Table 5.Taken from DrugBank training data Table 5 is the raw data of 3 samples with three relevant fields, i.e., sentences, character drug position, and the drug name. Table 6 illustrates a portion of the dataset and its label as the result of the raw data in Table 5. Refer to the drug-n name field in the dataset, dataset number 6 is identified as a drug, whereas the others are classified as a non-drug entity. The complete label illustration of the dataset provided by the first technique is presented in Table 7. As described in the Section 2.4, the value of vector dimension for each token is 100. Therefore, for a single data, it is represented as 100*5 = 500 lengths of a one-dimensional vector."
    }, {
      "heading" : "2.4.2 Second Technique",
      "text" : "The second technique is used for treating one sequence that comes from each sentence of the dataset. With this treatment, we added special characters ∗, as a padding, to the last part of the token when its dataset length is less than 5. By applying the second technique the first sentence of the sample provided a dataset as illustrated in Table 8."
    }, {
      "heading" : "2.4.3 Third Technique",
      "text" : "Naturally, the NLP sentence is a sequence in which the occurrence of the current word is conditioned by the previous one. Based on the word2vec value analysis, it is shown that intuitively we can separate the drug word and non-drug word by their Euclidean\n12/25\ndistance. Therefore, we used the Euclidean distance between the current words with the previous one to represent the influence. Thus, each current input xi is represented by [xvixdi] which is the concatenation of word2vec value xvi and its Euclidian distance to the previous one, xdi. Each x is the row vector with the dimension length is 200, the first 100 values are its word2vector, and the rest of all 100 values are the Euclidian distance to the previous. For the first word all value of xdi is 0. With the LSTM model, the task to extract the drug name from the medical data text is the binary classification that applied to each word of the sentence. We formulate the word sequence and its class as described in Table 9. In this experiment, each word that represents the drug name is identified as class 1, such as ’plenaxis’, ’cytochrome’, and ’p-450,’ whereas the other words are identified by class 0."
    }, {
      "heading" : "2.5 Wiki Sources",
      "text" : "In this study we also utilize Wikipedia as the additional text sources in word2vec training as used by [30]. The Wiki text addition is used to evaluate the impact of the training data volume in improving the quality of word’s vector."
    }, {
      "heading" : "2.6 Candidates Selection",
      "text" : "The tokens as the drug entities target are only a tiny part of the total tokens. In MedLine dataset, 171 of 2.000 token (less than ten %) are drugs, whereas in DrugBank, the number of drug tokens are 180 of 5.252 [31]. So the major part of these tokens are non-drug and other noises such as a stop word, and special or numerical characters. Based on this fact, we propose a candidate selection step to eliminate those noises. We examine two mechanisms in the candidate selection. The first is based on token distribution. The second is formed by selecting x/y part of the clustering result of data test. In the first scenario, we only used 2/3 of the token, which appears in the lower 2/3 part of the total token. This is presented in Table 1 and Table 2. Whereas, in the second mechanism we selected x/y(x < y) which is a part of total token after the tokens are clustered into y clusters."
    }, {
      "heading" : "2.7 Overview of NN Model",
      "text" : ""
    }, {
      "heading" : "2.7.1 MLP",
      "text" : "In the first experiment, we used multi-layer perceptron NN to train the model and evaluate the performance [19]. Given a training set of m examples, then the overall cost function can be defined as:\nJ(W, b) =\n[ 1\nm m∑ i=1 J(W, b;xi, yi)\n] + λ\n2 nl−1∑ l=1 sl∑ i=1 sl−1∑ j=1 ( W (l) ji )2 (1)\nJ(W, b) =\n[ 1\nm m∑ i=1\n( 1\n2 ∣∣∣∣∣ ∣∣∣∣∣hwb(x(i))− yi ∣∣∣∣∣ ∣∣∣∣∣ 2)] + λ 2 nl−1∑ l=1 sl∑ i=1 sl−1∑ j=1 ( W (l) ji )2 (2)\n13/25\nIn the definition of J(W, b), the first term is an average sum-of-squares error term, whereas the second term is a regularization term which is also called a weight decay term. In this experiment we use three kinds of regularization: #0, L0 with λ = 0, #1, L1 with λ = 1, and #2 with λ = the average of Euclidean distance. We computed the L2’s λ based on the word embedding vector analysis that drug target and non-drug can be distinguished by looking at their Euclidean distance. Thus, for L2 regularization, the parameter is calculated as:\nλ = 1\nn ∗ (n− 1) n−1∑ i=1 n∑ j=i+1 dist(xi, xj) (3)\nwhere dist(xi,xj) is the Euclidean distance of xi and xj . The model training and testing are implemented by modifying the code from [26] which can be downloaded at https://github.com/rasmusbergpalm/DeepLearnToolbox."
    }, {
      "heading" : "2.7.2 DBN",
      "text" : "DBN is a learning model composed of two or more stacked RBM [8,14]. An RBM is an undirected graph learning model which associates with a Markov Random Fields (MRF). In the DBN, the RBM acts as feature extractor where the pre-training process provides initial weights values to be fine-tuned in the discriminative process in the last layer. The last layer may be formed by logistic regression or any standard discriminative classifiers [8]. RBM was originally developed for binary data observation [7, 35]. It is a popular type of unsupervised model for binary data [13,34]. Some derivative of RBM models are also proposed to tackle a continuous/real values suggested in [5, 36]."
    }, {
      "heading" : "2.7.3 SAE",
      "text" : "An autoencoder (AE) neural network is one of the unsupervised learning algorithms. The NN tries to learn a function h(w, x) ≈ x. The autoencoder NN architecture also consists of input, hidden, and output layers. The particular characteristic of the autoencoder is that the target output is similar to the input. The interesting structure of the data is estimated by applying a certain constraint to the network, which limits the number of hidden units. However, when the number of hidden units has to be larger, it can be imposed with sparsity constraints on the hidden units [22]. The sparsity constraint is used to enforce the average value of hidden unit activation constrained to a certain value. As used in the DBN model, after we trained the SAE, the trained weight was used to initialize the weight of NN for the classification."
    }, {
      "heading" : "2.7.4 RNN-LSTM",
      "text" : "RNN (Recurrent Neural Network) is an NN, which considers the previous input in determining the output of the current input. RNN is powerful when it is applied to the dataset with a sequential pattern or when the current state input depends on the previous one, such as the time series data, sentences of NLP. An LSTM network is special kind of RNN which also consists of 3 layers, i.e., an input layer, a single recurrent hidden layer, and an output layer [16]. The main innovation of LSTM is that its hidden layer consists of one or more memory blocks. Each block includes one or more memory cells. In the standard form, the inputs are connected to all of the cells and gates, whereas the cells are connected to the outputs. The gates are connected to other gates and cells in the hidden layer. The single standard LSTM is a hidden layer with input, memory cell, and output gates [11,23].\n14/25"
    }, {
      "heading" : "2.8 Dataset",
      "text" : "To validate the proposed approach, we utilized DrugBank and MedLine open dataset, which have also been used by previous researchers. Additionally, we used drug label documents from various drug producers and regulator Internet sites located in Indonesia:\n1. http://www.kalbemed.com/\n2. http://www.dechacare.com/\n3. http://infoobatindonesia.com/obat/, and\n4. http://www.pom.go.id/webreg/index.php/home/produk/01.\nThe drug labels are written in Bahasa Indonesia, and their common contents are drug name, drug components, indication, contra indication, dosage, and warning."
    }, {
      "heading" : "2.9 Evaluation",
      "text" : "To evaluate the performance of the proposed method, we use common measured parameters in data mining i.e., precision, recall, and F-score. The computation formula of these parameters is as follows. Let C = {C1, C2, C3...Cn} is a set of the extracted drug-name of this method, and K = {K1,K2,K3, ...Kl} is set of actual drug-name in the document set D. Adopted from [28], the parameter computations formula are:\nPrecision(Ki, Cj) = (TruePositive)\n(TruePositive+ FalsePositive) = (||Ki ∩ Cj ||) (||Cj ||)\n(4)\nRecall(Ki, Cj) = (TruePositive)\n(TruePositive+ FalseNegative) = (||Ki ∩ Cj ||) (||Ki||)\n(5)\nwhere ‖Ki‖, ‖Cj‖, and ‖Ki ∩ Cj‖ denote the number of drug-name in K, in C, and in both K and C respectively. The F-score value is computed by the below formula:\nF − score(Ki, Cj) = (2 ∗ Precision(Ki, Cj) ∗Recall(Ki, Cj))\n(TruePositive+ FalsePositive) (6)"
    }, {
      "heading" : "3 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "3.1 MLP Learning Performance",
      "text" : "The following experiments are the part of the first experiment. These experiments are performed to evaluate the contribution of the three regularization settings as described in the Subsection 2.7.1 . By arranging the sentence in training dataset as 5-gram of words, the quantity of generated sample is presented in Table 10. We do training and testing of the MLP-NN learning model for all those test data compositions. The result of model performances on both datasets, i.e., MedLine & DrugBank, in learning phase is shown Figure 6 and Figure 7. The NN Learning parameters that are used for all experiments are: 500 input nodes, two hidden layers where each layer has 100 nodes with sigmoid activation, and 6 output nodes with softmax function; the learning rate = 1, momentum = 0.5; and epochs = 100. We used mini-batch scenario in the training with the batch size is 100. The presented errors in Figure 6 and Figure 7 are the errors for full batch, i.e., the mean errors of all mini batches.\nThe learning model performance shows different patterns between MedLine and DrugBank datasets. For both datasets, L1 regularization tends to stabilize in the lower iteration and its training error performance is always less than L0 or L2. The L0 and L2\n15/25\ntraining error performance pattern, however, shows a slight different behavior between MedLine and DrugBank. For the MedLine dataset, L0 and L2 produce different results for some of iterations. Whereas, the training error performance of L0 and L2 for DrugBank are almost the same in every iteration. Different pattern results are probably due to the variation in the quantity of training data. As illustrated in Table 10, the volume of DrugBank training data is almost four times the volume of the MedLine dataset. It can be concluded that, for larger dataset, the contribution of L2 regularization setting is not too significant in achieving better performance. For smaller dataset (MedLine), however, the performance is better even after only few iterations."
    }, {
      "heading" : "3.2 Open Dataset Performance",
      "text" : "In the Table 11, 12, 13, and 14, the numbering (1), (2) and (3) in the most left column indicate the candidate selection technique with:\n1. (1) : all data test are selected\n2. (2) : 2/3 part of data test are selected, and\n3. (3) : 2/3 part of 3 clusters for MedLine or 3/4 part of 4 cluster for DrugBank"
    }, {
      "heading" : "3.2.1 MLP-NN Performance",
      "text" : "In this first experiment, for two data representation techniques and three candidate selection scenarios, we have six experiment scenarios. The result of the experiment which applies the first data representation technique and three candidate selection\n16/25\nscenarios is presented in Table 11. In computing the F-score, we only select the predicted target which is provided by the lowest error (the minimum one). For MedLine dataset, the best performance is shown by L2 regularization setting where the error is 0.041818, in third candidate selection scenario with F-score 0.439516, whereas the DrugBank is achieved together by L0 and L1 regularization setting, with an error test of 0.0802, in second candidate selection scenario, the F-score was 0.641745. Overall, it can be concluded that DrugBank experiments give the best F-score performance. The candidate selection scenarios also contributed to improving the performance, as we found for both of MedLine and DrugBank, the best achievement is provided by the second and third scenarios respectively.\nThe next experimental scenario in the first experiment is performed to evaluate the impact of the data representation technique and the addition of Wiki source in word2vec training. The results are presented in Table 12 and Table 13. According to the obtained results presented in Table 11, the L0 regularization gives the best F-score. Hence, accordingly we only used the L0 regularization for the next experimental scenario. The table 12 presents the impact of the data representation technique. Looking at the F-score, the second technique gives better results for both datasets, i.e., the MedLine and DrugBank.\nTable 13 shows the result of adding the Wiki source into word2vec training in\n17/25\nproviding the vector of word representation. These results confirm that the addition of training data will improve the performance. It might be due to the fact that most of the targeted token such as drug name, are uncommon words, whereas the words that are used in Wiki’s sentence are commonly used words. Hence, the addition of commonly used words will make the difference between drug token and the non-drug token (the commonly used token) becomes greater. For the MLP-NN experimental results, the 4th scenario, i.e the second data representation with 2/3 partition data selection in Drugbank dataset, provides the best performance with 0.684646757 in F-score."
    }, {
      "heading" : "3.2.2 DBN & SAE Performance",
      "text" : "In the second experiment, which involves DBN and SAE learning model, we only use the experiment scenario that gives the best results in the first experiment. The best experiment scenario uses the second data representation technique with Wiki text as an additional source in the word2vec training step.\nIn the DBN experiment, we use two stacked RBMs with 500 nodes of visible unit, 100 nodes of the hidden layer for the first and also the second RBMs. The used learning parameters are as follows: momentum = 0; and alpha= 1. We used mini-batch scenario in the training, with the batch size of 100. As for RBM constraints, the range of input data value is restricted to [0..1] - as the original RBM, which is developed for binary data type -, whereas the range of vector of word value is [−1..1]. So we normalize the data value into [0..1] range before performing the RBM training. In the last layer of DBN, we use one layer of MLP with 100 hidden nodes and 6 output nodes with softmax output function as classifier.\nThe used SAE architecture is two stacked AEs with the following nodes configuration. The first AE has 500 units of visible unit, 100 hidden layers, 500 output layer. The\n18/25\nsecond AE has 100 nodes of visible unit, 100 nodes hidden unit, and 100 nodes output unit. The used learning parameters for first SAE and the second SAE respectively are as follows: activation function = sigmoid and tanh; learning rate = 1 and 2; momentum = 0.5 and 0.5; sparsity target = 0.05 and 0.05. The batch size of 100 is set for both of AEs. In the SAE experiment, we use the same discriminative layer as DBN i.e. one layer MLP with 100 hidden node and 6 output nodes with softmax activation function.\nThe experiments results are presented in Table 14. There is a difference in performances when using the MedLine and the DrugBank datasets when feeding them into MLP, DBN, and SAE models. The best results for the MedLine dataset is obtained when using the SAE. For the DrugBank, the MLP gives the best results. The DBN gives lower average performance for both datasets. The lower performance is probably due to the normalization on the word vector value to [0...1], whereas their original value range is in fact between [−1..1]. The best performance for all experiments 1 and 2, is given by SAE, with the second scenario of candidate selection as described in the Subsection 2.6. Its F-score is 0.686192469."
    }, {
      "heading" : "3.2.3 LSTM Performance",
      "text" : "The global LSTM network used is presented in Figure 8. Each single LSTM block consists of two stacked hidden layers, one input node with each input dimension is 200 as described in Subsection 2.4.3. All hidden layers are fully connected. We used sigmoid as an output activation function, which is the most suitable for binary classification. We implemented a peepholes connection LSTM variant where its gate layers look at the cell\n19/25\nstate [9]. In addition to implementing the peepholes connection, we also use a coupled of forget and input gates. The detail single LSTM architecture and its each gate formula computation can be referred in [23].\nThe LSTM experiments were implemented with several different parameter settings. Their results presented in this section are the best among all our experiments. Each input data consists of two components, its word vector value and its Euclidian distance to the previous input data. In treating both input data components, we adapt the Adding Problem Experiment as presented in [15]. We use the Jannlab tools [24] with some modifications in the part of entry to conform with our data settings.\nThe best achieved performance is obtained with LSTM block architecture of one node input layer, two nodes hidden layer, and one node output layer. The used parameters are: learning rate = 0.001, momentum = 0.9, and epoch = 30, input dimension = 200, and with the time sequence frame set to 2. The complete treatment of drug sentence as a sequence both in representation and recognition, to extract the drug name entities, is the best technique, as shown by F-score performance in Table 15.\nAs described in previous work section, there are many approaches related to drug extraction have been proposed. Most of them utilize certain external knowledge to achieve the extraction objective. The Table 16 summarizes their F-score performance. Among the state of the arts, our third data representation technique applied to the LSTM model is outperforming. Also, our proposed method does not require any external knowledge."
    }, {
      "heading" : "3.3 Drug Label Dataset Performance",
      "text" : "As additional experiment, we also use Indonesian language drug label corpus to evaluate the method’s performance. Regarding the Indonesian drug label, we could not found any certain external knowledge that can be used to assist the extraction of the drug name contained in the drug label. In the presence of this hinderance, we found our proposed method is more suitable than any other previous approaches. As the drug\n20/25\nlabel texts are collected from various sites of drug distributors, producers, and government regulators; it does not clearly contain training data and testing data as in DrugBanks or Medline datasets. The other characteristics of these texts are the more structured sentences contained in the data. Although the texts are coming from various sources, all of them are similar kind of document (the drug label that might be generated by machine). After the data cleaning step (HTML tag removal, etc.), we annotated the dataset manually. The total quantity of dataset after performing the data representation step, as described in Subsection 2.4, is 1.046.200. In this experiment, we perform 10 times cross-validation scenario by randomly selecting 80% data for the training data and 20% data for testing.\nThe experimental result for drug label dataset shows that all of the candidate selection scenarios provide excellent F-score (above 0.9). The excellent F-score performance is probably due to the more structured sentences in those texts. The best result of those ten experiments are presented in Table 17."
    }, {
      "heading" : "3.4 Choosing The Best Scenario",
      "text" : "In the first and second experiments, we studied various experiment scenarios, which involves three investigated parameters: additional Wiki source, data representation techniques, and drug target candidate selection. In general, the Wiki addition contributes in improving the F-score performance. The additional source in word2vec training enhances the quality of the resulted word2vec. Through the addition of common words, from Wiki, the difference between the common words and the uncommon words i.e drug name becomes greater (better distinguishing power).\nOne problem in mining drug name entity from medical text is the imbalanced quantity between drug token and other tokens [31]. Also, the targeted drug entities are only a small part of the total tokens. Thus, majority of tokens are noise. In dealing with this problem, the second and third candidate selection scenarios show their contribution to reduce the quantity of noise. Since the possibility to extract the noises is reduced then the recall value and F-score value increase as well, as shown in the first and second experiments results.\nThe third experiment which uses LSTM model does not applied the candidate selection scenario because the input dataset is treated as sentence sequence. So the input dataset can not be randomly divide (selected) as the tuple treatment in the first and second experiments."
    }, {
      "heading" : "4 Conclusion & Future Works",
      "text" : "This study proposes a new approach in the data representation and classification to extract drug name entities contained in the sentences of medical text documents. The suggested approach solves the problem of multiple tokens for a single entity that remained unsolved in previous studies. This study also introduces some techniques to tackle the absence of specific external knowledge. Naturally, the words contained in the sentence follow a certain sequence patterns, i.e., the current word is conditioned by other previous words. Based on the sequence notion, the treatment of medical text sentences which apply the sequence NN model, gives better results. In this study, we presented three data representation techniques. The first and second techniques treat the sentence as a non-sequence pattern which is evaluated with the non-sequential NN classifier (MLP, DBN, SAE), whereas the third technique treats the sentences as a sequence to provide data that is used as the input of the sequential NN classifier i.e. LSTM. The performance of the application of LSTM models for the sequence data representation, with the average F-score being 0.8645, rendered the best result compared to the state of the art.\nSome opportunities to improve the performance of the proposed technique are still widely opened. The first step improvement can be the incorporation of additional handcrafted features - such as the words position, the use of capital case at the beginning of the word, the type of character - as also used in the previous studies [3, 30]. As presented in the MLP experiments for drug label document, the proposed methods achieved excellent performance when applied to the more structured text. Thus, the effort to make the sentence of the dataset, i.e., DrugBank and MedLine, to be more structured can also be elaborated. Regarding the LSTM model and the sequence data representation for the sentences of medical text, our future study will tackle the multiple entity extractions such as drug group, drug brand, and drug compounds. Another task that is potential to be solved with the LSTM model is the drug – drug interaction extraction. Our experiments also utilizes the Euclidean distance measure in addition to the word2vec features. Such addition gives a good F-score performance. The significance of embedding the Euclidean distance features, however, needs to be explored further."
    }, {
      "heading" : "5 Acknowledgment",
      "text" : "This work is supported by Higher Education Science and Technology Development Grant funded Indonesia Ministry of Research and Higher Education Contract No. 1004/UN2.R12/HKP.05.00/2016"
    }, {
      "heading" : "6 Conflict of Interests",
      "text" : "The authors declare that there is no conflict of interest regarding the publication of this paper."
    } ],
    "references" : [ {
      "title" : "Text mining for pharmacovigilance : Using machine learning for drug name recognition and drug – drug interaction extraction and classification",
      "author" : [ "A. Ben", "F. Mahbub", "A. Karanasiou", "Y. Mrabet", "A. Lavelli", "P. Zweigenbaum" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "UTurku : Drug Named Entity Recognition and Drug-Drug Interaction Extraction Using SVM Classification and 22/25  Domain Knowledge",
      "author" : [ "J. Björne", "S. Kaewphan", "T. Salakoski" ],
      "venue" : "In Second Joint Conferernce on Lexical and Computational Semantic,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Using Natural Language Processing to Identify Pharmacokinetic Drug- Drug Interactions Described in Drug Package Inserts",
      "author" : [ "R. Boyce", "G. Gardner" ],
      "venue" : "In the 2012 Workshop on Biomedical Natural Language Processing (BioNLP",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "A novel feature-based approach to extract drug-drug interactions from biomedical",
      "author" : [ "Q.C. Bui", "P.M.A. Sloot", "E.M. Van Mulligen", "J.A. Kors" ],
      "venue" : "text. Bioinformatics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Continuous Restricted Boltzman Machine with an Implementation Training Algorithm",
      "author" : [ "H. Chen", "A. Murray" ],
      "venue" : "imag Signal Processing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "A study of active learning methods for named entity recognition in clinical text",
      "author" : [ "Y. Chen", "T.A. Lasko", "Q. Mei", "J.C. Denny", "H. Xu" ],
      "venue" : "Journal of biomedical informatics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Training restricted boltzmann machines on word observations",
      "author" : [ "G. Dahl", "R. Adams", "H. Larochelle" ],
      "venue" : "arXiv preprint arXiv:1202.5695,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: 17th Iberoamerican Congress, CIARP 2012, Buenos Aires, Argentina",
      "author" : [ "A. Fischer", "C. Igel" ],
      "venue" : "September 3-6,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Recurrent nets that time and count",
      "author" : [ "F.A. Gers", "J. Schmihuber" ],
      "venue" : "In Proceedings of the IEEE-INNS-ENNS International Joint Conference on,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "LASIGE : using Conditional Random Fields and ChEBI ontology",
      "author" : [ "T. Grego", "F.M. Couto" ],
      "venue" : "In 7th International Workshop on Semantic Evaluation (SemEval 2013).,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Named entity recognition with long short-term memory",
      "author" : [ "J. Hammerton" ],
      "venue" : "Proceedings of CoNLL,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "The DDI corpus : An annotated corpus with pharmacological substances and drug – drug interactions",
      "author" : [ "M. Herrero-zazo", "I. Segura-bedmar", "P. Mart́ınez", "T. Declerck" ],
      "venue" : "Journal of biomedical informatics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "A Practical Guide to Training Restricted Boltzmann Machines A Practical Guide to Training Restricted Boltzmann Machines",
      "author" : [ "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.-W. Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Lstm Can Solve Hard. In Advances in neural information processing",
      "author" : [ "S. Hochreiter" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1996
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1997
    }, {
      "title" : "Enhancing medical named entity recognition with an extended segment representation technique",
      "author" : [ "S. Keretna", "C. Peng", "D. Creighton", "K. Bashir" ],
      "venue" : "Computer Methods and Programs in Bimoedicine,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Artificial Intelligence in Medicine Boosting drug named entity recognition using an aggregate classifier",
      "author" : [ "I. Korkontzelos", "D. Piliouras", "A.W. Dowsey", "S. Ananiadou" ],
      "venue" : "Artificial Intelligence in Medicine,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Drug name recognition: Approaches and resources",
      "author" : [ "S. Liu", "B. Tang", "Q. Chen", "X. Wang" ],
      "venue" : "Information (Switzerland),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "T. Mikolov", "G. Corrado", "K. Chen", "J. Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Jannlab neural network framework for java",
      "author" : [ "S. Otte", "D. Krechel", "M. Liwicki" ],
      "venue" : "In Poster Proceedings Conference MLDM",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "A Survey of Biological Entity Recognition Approaches",
      "author" : [ "G. Pal", "S. Gosal" ],
      "venue" : "International Journal on Recent and Innovation Trends in Computing and Communication,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Prediction as a candidate for learning deep hierarchical models of data",
      "author" : [ "R.B. Palm" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Translation and Classification Algorithm of FDA-Drugs to DOEN2011 Class Therapy to Estimate Drug - Drug Interaction",
      "author" : [ "M. Sadikin", "I. Wasito" ],
      "venue" : "In The 2nd International Conference on Information Systems for Business Competitiveness",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Toward Object Interaction Mining By Starting With Object Extraction Based on Pattern Learning Method",
      "author" : [ "M. Sadikin", "I. Wasito" ],
      "venue" : "In 2014 Asia-Pacific Materials Science and Information Technology Conference (APMSIT",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "Mining Adverse Drug Reactions from online healthcare forums using Hidden Markov Model",
      "author" : [ "H. Sampathkumar", "X.-w. Chen", "B. Luo" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "Exploring Word Embedding for Drug Name Recognition",
      "author" : [ "I. Segura-bedmar", "P. Mart" ],
      "venue" : "In The Sixth International Workshop on Health Text Mining and Information Analysis, number September,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013)",
      "author" : [ "I. Segura-Bedmar", "P. Martinez", "M. Herrero-Zazo. Semeval-2013 task 9" ],
      "venue" : "In Proceedings of the Seventh International Workshop on Semantic Evaluation",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Drug name recognition and classification in biomedical texts A case study outlining approaches underpinning automated systems",
      "author" : [ "I. Segura-bedmar", "P. Martı" ],
      "venue" : "Drug Discovery Today,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2008
    }, {
      "title" : "Data normalization in the learning of restricted Boltzmann machines",
      "author" : [ "Y. Tang", "I. Sutskever" ],
      "venue" : "Department of Computer science, Toronto university,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2011
    }, {
      "title" : "Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient",
      "author" : [ "T. Tieleman" ],
      "venue" : "Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2008
    }, {
      "title" : "Exponential Family Harmoniums with an Application to Information Retrieval",
      "author" : [ "M. Welling", "M. Rosen-zvi", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2005
    }, {
      "title" : "Unsupervised biomedical named entity recognition : Experiments with clinical and biological texts",
      "author" : [ "S. Zhang", "N. Elhadad" ],
      "venue" : "Journal of biomedical informatics,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "In many recent cases, however, many drugs are withdrawn from the market when it was discovered that the interaction between the drugs is hazardous to health [27].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 17,
      "context" : "Drug name recognition is a primary task of medical text data extraction since the drug finding is the essential element in solving other information extraction problems [18,37].",
      "startOffset" : 169,
      "endOffset" : 176
    }, {
      "referenceID" : 32,
      "context" : "Drug name recognition is a primary task of medical text data extraction since the drug finding is the essential element in solving other information extraction problems [18,37].",
      "startOffset" : 169,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "Among derivative work of drug name extractions are drug-drug interaction [12], drug adverse reaction [29], or other applications (information retrieval, decision support system, drug development or drug discovery) [32].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : "Among derivative work of drug name extractions are drug-drug interaction [12], drug adverse reaction [29], or other applications (information retrieval, decision support system, drug development or drug discovery) [32].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 28,
      "context" : "Among derivative work of drug name extractions are drug-drug interaction [12], drug adverse reaction [29], or other applications (information retrieval, decision support system, drug development or drug discovery) [32].",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 16,
      "context" : "First, the drug name entities are usually unstructured texts [17] where the number of new entities is quickly growing over time.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "Thus, it is hard to create a dictionary which always includes the entire lexicon and is up to date [25].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "Third, many drug names contain a combination of non-word and word symbols [20].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "Fourth, the other problem in drug name extraction is that a single drug name might be represented by multiple tokens [10].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "Due to the complexity in extracting multiple tokens for drugs, some researchers such as [2] even ignores that case in the MedLine & DrugBank training with the reason that the multiple tokens drug is only 18% of all drug names.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "In this study, the vector of words is provided by word embedding methods proposed by Mikolov [21].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "html, which is also used by [1, 2, 10].",
      "startOffset" : 28,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "html, which is also used by [1, 2, 10].",
      "startOffset" : 28,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "html, which is also used by [1, 2, 10].",
      "startOffset" : 28,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "[25] summarizes their survey on various entity recognition approaches.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "The approaches can be categorized into three models: dictionary based, rule based, and learning based methods [17, 33].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "5% in F-score [31].",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "CRF is used by one of the best [10] participants in SemEval challenges in the clinical text (https://www.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "As for the use of external knowledge aimed to increase the performance, the author [10] uses ChEBI (Chemical Entities of Biological Interest), i.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 26,
      "context" : "A hybrid approach model, which combines statistical learning and dictionary based, is proposed by [30].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "The result of a CRF based active learning, which is applied to NER BIO (Beginning, Inside, Output) annotation token for extracting named entity in the clinical text, is presented in [6].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "[1].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "[18].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[2].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "[17].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "Related to their approach, in our previous work, we propose a pattern learning that utilizes the regular expression surrounding drug names and their compounds [28].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 26,
      "context" : "To overcome the multiple tokens problem, we propose a new technique which treats a target entity as a set of tokens (a tuple) at once rather than treating the target entity as a single token surrounded by other tokens such as used by [30] or [4].",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 3,
      "context" : "To overcome the multiple tokens problem, we propose a new technique which treats a target entity as a set of tokens (a tuple) at once rather than treating the target entity as a single token surrounded by other tokens such as used by [30] or [4].",
      "startOffset" : 242,
      "endOffset" : 245
    }, {
      "referenceID" : 27,
      "context" : "[31] present the first basic statistics of the dataset.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "In the first and second techniques, we try to overcome the multiple tokens drawback that left unsolved in [2] by formatting a single input data as an N - gram model with N=5 (one tuple data consist 5 tokens) to accommodate the maximum token which acts as a single drug entity target name.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "Result: Labelled dataset Input:array of tuple, array of drug ; output: array of label {Array of drug contains list of drug and drug-n only} ; label[]<=1 Initialization; for each t in tuple do for each d in drug do if length (d) = 1 then if t[1] = d[1] then //match 1 token drug; label <= 2, break, exit from for each d in drug; else",
      "startOffset" : 241,
      "endOffset" : 244
    }, {
      "referenceID" : 0,
      "context" : "Result: Labelled dataset Input:array of tuple, array of drug ; output: array of label {Array of drug contains list of drug and drug-n only} ; label[]<=1 Initialization; for each t in tuple do for each d in drug do if length (d) = 1 then if t[1] = d[1] then //match 1 token drug; label <= 2, break, exit from for each d in drug; else",
      "startOffset" : 248,
      "endOffset" : 251
    }, {
      "referenceID" : 0,
      "context" : "end else if length (d) = 2 then if t[1] = d[1] and t[2] = d[2] then //match 2 tokens drug; label <= 3, break, exit from for each d in drug; else",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "end else if length (d) = 2 then if t[1] = d[1] and t[2] = d[2] then //match 2 tokens drug; label <= 3, break, exit from for each d in drug; else",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "end else if length (d) = 2 then if t[1] = d[1] and t[2] = d[2] then //match 2 tokens drug; label <= 3, break, exit from for each d in drug; else",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "end else if length (d) = 2 then if t[1] = d[1] and t[2] = d[2] then //match 2 tokens drug; label <= 3, break, exit from for each d in drug; else",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 26,
      "context" : "In this study we also utilize Wikipedia as the additional text sources in word2vec training as used by [30].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 27,
      "context" : "252 [31].",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "The model training and testing are implemented by modifying the code from [26] which can be downloaded at https://github.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "DBN is a learning model composed of two or more stacked RBM [8,14].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "DBN is a learning model composed of two or more stacked RBM [8,14].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "The last layer may be formed by logistic regression or any standard discriminative classifiers [8].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "RBM was originally developed for binary data observation [7, 35].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 30,
      "context" : "RBM was originally developed for binary data observation [7, 35].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "It is a popular type of unsupervised model for binary data [13,34].",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 29,
      "context" : "It is a popular type of unsupervised model for binary data [13,34].",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Some derivative of RBM models are also proposed to tackle a continuous/real values suggested in [5, 36].",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "Some derivative of RBM models are also proposed to tackle a continuous/real values suggested in [5, 36].",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : ", an input layer, a single recurrent hidden layer, and an output layer [16].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "The single standard LSTM is a hidden layer with input, memory cell, and output gates [11,23].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "Adopted from [28], the parameter computations formula are:",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "state [9].",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 14,
      "context" : "In treating both input data components, we adapt the Adding Problem Experiment as presented in [15].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "We use the Jannlab tools [24] with some modifications in the part of entry to conform with our data settings.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "The Best of SemEval 2013 [31] 0.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "7150 [10] 0.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 26,
      "context" : "5700 With external knowledge, ChEBI [30]+Wiki 0.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "7200 With external knowledge, DINTO [1] 0.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "7200 Additional feature, BIO [2] 0.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 27,
      "context" : "One problem in mining drug name entity from medical text is the imbalanced quantity between drug token and other tokens [31].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "The first step improvement can be the incorporation of additional handcrafted features - such as the words position, the use of capital case at the beginning of the word, the type of character - as also used in the previous studies [3, 30].",
      "startOffset" : 232,
      "endOffset" : 239
    }, {
      "referenceID" : 26,
      "context" : "The first step improvement can be the incorporation of additional handcrafted features - such as the words position, the use of capital case at the beginning of the word, the type of character - as also used in the previous studies [3, 30].",
      "startOffset" : 232,
      "endOffset" : 239
    } ],
    "year" : 2016,
    "abstractText" : "One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645. Keywords— drug name entity, word embedding, MLP, DBN, SAE, LSTM",
    "creator" : "LaTeX with hyperref package"
  }
}