{
  "name" : "1510.03753.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improved Deep Learning Baselines for Ubuntu Corpus Dialogs",
    "authors" : [ "Rudolf Kadlec", "Martin Schmid", "Jan Kleindienst" ],
    "emails" : [ "jankle}@cz.ibm.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The Ubuntu Dialogue Corpus is the largest freely available multi-turn based dialog corpus [1]1. It was constructed from the Ubuntu chat logs2 — a collection of logs from Ubuntu-related chat rooms on the Freenode IRC network. Although multiple users can talk at the same time in the chat room, the logs were preprocessed using heuristics to create two-person conversations. The resulting corpus consists of almost one million two-person conversations, where a user seeks help with his/her Ubuntu-related problems (the average length of a dialog is 8 turns, with a minimum of 3 turns). Because of its size, the corpus is well-suited for explorations of deep learning techniques in the context of dialogue systems. In this paper, we introduce our preliminary research and experiments with this corpus, and report state-of-the-art results.\nThe rest of the paper continues as follows: 1. we introduce the setup — the data as well as the evaluation of the task; 2. we briefly describe the previously evaluated models; 3. we introduce three different models (one of them being the same as in the previous work); 4. we evaluate these models and experiment with different amount of training data; 5. we conclude and discuss our plans for future works"
    }, {
      "heading" : "2 Data",
      "text" : "In this section we briefly describe the data and evaluation metrics used in [1]. First, all the collected data was preprocessed by replacing named entities with corresponding tags (name, location, organization, url, path). This is analogical to the prepossessing of [2] (note that the IT helpdesk dataset used there is not publicly available). Second, these data are further processed to create tuples of (context, response, flag). The flag is a Boolean variable indicating whether the response is correct or incorrect.\n1http://cs.mcgill.ca/˜jpineau/datasets/ubuntu-corpus-1.0/ 2http://irclogs.ubuntu.com/\nar X\niv :1\n51 0.\n03 75\n3v 1\n[ cs\n.C L\n] 1\n3 O\nct 2\nTo form the training set, each utterance (starting from the third one) is considered as a potential response, while the previous utterances form its context. So a dialogue of length n yields (n − 2) training examples (context, response, 1) and (n − 2) training examples (context, response′, 0). The negative response response′ is a randomly sampled utterance from the entire corpus. Finally, the training examples are shuffled."
    }, {
      "heading" : "2.1 Evaluation Metric",
      "text" : "Randomly-selected 2% of the conversations are used to create a test set. The proposed task is that of the best response selection. The system is presented with n response candidates, and it is asked to rank them. To vary the task’s difficulty (and to remedy that some of the sampled candidates flagged as incorrect can very well be correct), the system’s ranking is considered correct if the correct response is among the first k candidates (Recall@k). The baselines were reported with (n, k) of (2, 1), (10, 1), (10, 2) and (10, 5)."
    }, {
      "heading" : "3 Approaches",
      "text" : "This task can naturally be formulated as a ranking problem which is often tackled by three techniques [3]: (i) pointwise; (ii) pairwise and (iii) listwise ranking.\nWhile pairwise and listwise approaches are empirically superior to pointwise approach, our preliminary experiments use pointwise ranking. Note that pointwise method was also used in the original baselines [1]."
    }, {
      "heading" : "3.1 Pointwise Ranking",
      "text" : "In pointwise ranking, only the context and the response are directly used to compute the probability of the pair. All the pairs are then sorted by their probabilities. We denote the function that outputs the probability of the pair as g(context, response). In our settings, the function g is represented by a neural network (learned using the training data). We describe the details of the network architectures used in the following sections."
    }, {
      "heading" : "4 Previous Work",
      "text" : "The pointwise architectures reported in [1] included (i) TF-IDF, (ii) RNN and (iii) LSTM. In this section, we briefly describe these models."
    }, {
      "heading" : "4.1 TF-IDF",
      "text" : "The motivation here is that the correct response tends to share more words with the context than the incorrect ones. First, the TF-IDF vectors are calculated for the context and each of the candidate responses. Next, the cosine similarity between the context vector and each response vector is used to rank the responses. Note that there is no learning, thus the training set is simply ignored.\ntfidfcontext(w) = tf(w, context)× idf(w,D) (1) tfidfdocument(w) = tf(w, document)× idf(w,D) (2)\ng(context, response) = tfidfcontext · tfidfcontext (3) tfidfcontext and tfidfresponse are the resulting TF-IDF vectors for context and response respectively. D stands for the corpus and w is a word. The dimension of the resulting vectors is thus equal to the dictionary size."
    }, {
      "heading" : "4.2 Neural Network Embeddings",
      "text" : "A neural network is used to create an embedding of both the context and the candidate response. These embeddings, denoted as c and r, are then multiplied using a matrix M and the result is fed into the sigmoid function to score the response.\nc = f(context) (4) r = f(response) (5)\ng(context, response) = σ(c>Mr + b) (6) c and r are the resulting embeddings of the context and response, computed using a neural network. We present some different architectures to compute these embeddings.\nFigure 1 illustrates the approach. Note that matrixM , bias b and parameters of the function f (which is a neural network) are all learned using the training data.\nOne can think of this approach as a predictive one — given the context, we predict the embedding of the response as r′ = c>M , and measure the similarity of the predicted response r′ to the actual response r using the dot product (or vice-versa, predicting the context from the response as c′ =Mr)\nThe authors experimented with vanilla RNN and LSTM [4] as the underlying networks producing the embeddings. LSTM significantly outperformed RNN in the author’s experiments."
    }, {
      "heading" : "5 Our Architectures",
      "text" : "All our architectures fall within the neural network embedding based approach. We implemented three different architectures (i) CNN [5] (ii) LSTM and (iii) Bi-Directional [6] LSTM. We also report an ensemble of our models.\nAll of our architectures share the same design where the words from the input sequence (context or response) are projected into the words’ embeddings vectors. Thus, if the input sequence consist of 42 words, we project these words into a matrix E which has a dimension e × 42, where e is the dimension of the word’s embedding."
    }, {
      "heading" : "5.1 CNN",
      "text" : "While originated from computer vision [7], CNN models have recently been very successfully applied in NLP problems [5].\nAt the very heart of the CNN model, the convolving filters are sequentially applied over the input sequence. The width of the filters might vary, and in NLP typically range from 1 to 5 (the filters can be thought of here as a form of n-grams). These filters are followed by a max-pooling layer to get a fixed-length input. In our architecture, the output of the max-pooling operation forms the context/response embedding. Thus, the resulting embedding has a dimension equal to the number of filters. Figure 2a displays this architecture with two filters.\n5.2 LSTM\nLong short-term memory (LSTM) is a recurrent neural network (RNN) architecture designed to remedy the vanishing gradient problem of vanilla RNN [4]. Thus, LSTM networks are well-suited for working with (very) long sequences. We use the same model as the authors’ LSTM network. LSTM iterates over the sequence embeddings, and the resulting embedding is the last state of the LSTM’s cells. Figure 2b illustrates this architecture."
    }, {
      "heading" : "5.3 Bi-Directional LSTM",
      "text" : "Although the LSTM are tailor-made to keep context over large sequences, empirically it can be problematic for the network to capture the meaning of the entire sequence as it gets longer. If the important parts of the sequence are found at the beginning of a long sequence, the LSTM might struggle to get well-performing embedding. We decided to experiment with Bi-LSTMs to see whether this is the case in our settings. Bi-directional [6] LSTMSs feed the sequence into two recurrent networks — one reads the sequence as it is, the second reads the sequence from the end to the beginning. To avoid forming cycles, only the outputs of the recurrent networks (not the state-to-state connections) lead to same units in the next layers. Figure 2c illustrates this architecture."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Method",
      "text" : "To match the original setup of [1] we use the same training data3. We use one million training examples and we use the same word vectors pre-trained by GloVe [8]. All our models were implemented using Theano [9] and Blocks [10]. For training we use ADAM learning rule [11] and binary negative log-likelihood as training objective. We stop the training once Recall@1 starts increasing on\n3The dataset in binary format is available at http://cs.mcgill.ca/˜jpineau/datasets/ ubuntu-corpus-1.0/ubuntu_blobs.tgz [accessed 25.9.2015]\na validation set. The experiments were executed on Nvidia K40 GPUs. The best meta-parameters were found by simple grid search.\nIn all architectures we tried both: (i) learning separate parameters for the network encoding context and response and (ii) learning shared parameters for both networks. Here we report only the results for the architectures with shared parameters, since they consistently achieved higher accuracy.\nAside from learning single models, we also experimented with model ensembles. We found that averaging predictions of multiple models further improves performance, which is common in many machine learning tasks [12, 13]. Our best classifier is an ensemble of 11 LSTMs, 7 Bi-LSTMs and 10 CNNs trained with different meta-parameters."
    }, {
      "heading" : "6.2 Results",
      "text" : "Table 1 shows performance of the models with the best metaparameters in each category.\nThe performance was evaluated after every epoch of training. Most of the models achieved the best cost on validation data after single epoch of training. However, the best Recall metrics were usually recorded in the second epoch of training."
    }, {
      "heading" : "6.3 Discussion",
      "text" : "Our ensemble of classifiers sets a new state-of-the art performance on the Ubuntu Dialog Corpus — the largest, publicly available multi-turn dialog corpus. Interestingly LSTMs and Bi-LSTMs achieve almost the same accuracy. We hypothesise that: (i) either utterances that appear at the beginning of the context are less important than the later utterances or, (ii) LSTMs successfully capture all of the important parts of the sequence. When we inspect accuracy of individual models we see that recurrent models are superior to CNNs. However, CNNs proved to significantly improve performance of the ensemble. An ensemble without the 10 CNNs had Recall@1 accuracy of only 66.8 compared to 68.3 of the larger ensemble. This shows that CNNs learned representations that are complementary to the recurrent models. We believe that our results are important, since they can be used as baselines for more complicated models (see the Future Work)."
    }, {
      "heading" : "6.4 Varying Train Data Size",
      "text" : "We also experimented with different training data sizes in order to see how this affects the resulting models. We trained all networks on a training data size ranging from 100, 000 to the full 1, 000, 000 examples. The graph in Figure 3 shows the Recall@1 for all the three models (reported on the test data). There are two main observations here: (i) CNNs outperform recurrent models if the training dataset is small. We believe that this is mostly due to the ’max’ operation performed on top of the feature maps. Thanks to the simplicity of this operation, the model does not over-fit the data and generalizes better when learned on small training datasets. On the other hand, the simplicity of the operation does not allow the model to properly handle more complicated dependencies (such as the order in which the n-grams occur in the text), thus recurrent models perform better given enough data; (ii) the recurrent models have not made its peak yet, suggesting that adding more training data would improve the model’s accuracy. This agrees with Figure 3 of the previous evaluation [1]."
    }, {
      "heading" : "7 Future Work",
      "text" : "In our future work, we plan to investigate applicability of neural networks architectures extended with memory (e.g., [14, 15, 16]) on this task. It is an appealing idea to bootstrap the system with external source of information (e.g., user manual or man pages) to help the system pick the right answer. For successful application of this paradigm in the domain of reinforcement learning, see [17].\nAn alternative direction for future research might be to extend the model with attention [18] over sentences in the dialog context. This would allow the model to explain which facts in the context were the most important for its prediction. Therefore, the prediction could be better interpreted by a human.\nAdditional accuracy improvements might be also achieved by different text pre-processing pipelines. For instance, in the current dataset all named entities were replaced with generic tags, which could possibly harm the performance."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this work we achieved a new state-of-the-art results on the next utterance ranking problem recently introduced in [1]. The best performing system is an ensemble of multiple diverse neural networks. In the future, we plan to use our system as a base for more complicated models going beyond the standard neural network paradigm."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "This paper presents results of our experiments using the Ubuntu Dialog Corpus –<lb>the largest publicly available multi-turn dialog corpus. First, we use an in-house<lb>implementation of previously reported models to do an independent evaluation<lb>using the same data. Second, we evaluate the performances of various LSTMs,<lb>Bi-LSTMs and CNNs on the dataset. Third, we create an ensemble by averaging<lb>predictions of multiple models. The ensemble further improves the performance<lb>and it achieves a state-of-the-art result for this dataset. Finally, we discuss our<lb>future plans using this corpus.",
    "creator" : "LaTeX with hyperref package"
  }
}