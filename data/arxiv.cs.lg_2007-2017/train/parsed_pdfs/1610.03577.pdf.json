{
  "name" : "1610.03577.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Minimax Filter: Learning to Preserve Privacy from Inference Attacks",
    "authors" : [ "Jihun Hamm" ],
    "emails" : [ "hammj@cse.ohio-state.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: inference attack, empirical risk minimization, minimax optimization, differential privacy, k-anonymity"
    }, {
      "heading" : "1. Introduction",
      "text" : "Privacy is an important issue when data collected from or related to individuals are analyzed and released to a third party. In response to growing privacy concerns, various methods for privacy-preserving data publishing have been proposed (see Fung et al. (2010) for a review.)\nSyntactic anonymization methods, such as k-anonymity (Sweeney, 2002) and l-diversity (Machanavajjhala et al., 2007) focus on anonymization of quasi-identifiers and protection of sensitive attributes in static databases. However, it is known that syntactic anonymization is susceptible to several types of attacks such as the DeFinetti attack (Kifer, 2009). An adversary may be able to accurately infer sensitive attributes of individuals from insensitive, sanitized attributes. High-dimensional data also poses a challenge for syntactic anonymization methods. For example, k-anonymity is known to be ineffective for high-dimensional sparse databases (Narayanan and Shmatikov, 2008). In addition, syntactic anonymization methods are designed for discrete attributes. Discretizing continuous attributes by binning\nar X\niv :1\n61 0.\n03 57\n7v 1\n[ cs\n.L G\n] 1\n2 O\nct 2\nor clustering is unnatural for certain data types such as video, image, audio, and biometric data. Besides, conventional categorization of attributes as {identifiers, quasi-identifiers, sensitive informations} becomes ambiguous with multimedia-type data. For example, image can an identifier if it contains a face or fingerprints. Otherwise, it may be sensitive or insensitive depending on the content. Audio can also be an identifier if it contains identifiable voice or if there is a mention of identifying information such as names, or it can also be either sensitive or insensitive.\nDifferential privacy (Dwork and Nissim, 2004; Dwork et al., 2006; Dwork, 2006) was proposed to address many weaknesses of syntactic methods (see the discussion by Clifton and Tassa (2013).) Differential privacy has a more formal privacy guarantee than that of syntactic methods, and is applicable to many problems beyond database release (Dwork et al., 2014). In particular, differential privacy can be defined for continuous and/or highdimensional attributes and for functions (Hall et al., 2013). However, similar to syntactic anonymization, differential privacy is not immune to inference attacks either (Cormode, 2011), as differentially privacy only prevents an adversary from gaining additional knowledge by inclusion/exclusion of a participant (Dwork et al., 2014), and not from gaining knowledge from released data itself. Therefore, an adversary can still be able to guess sensitive attributes of participants from differentially-private attributes with some confidence.\nTo preserve privacy of continuous high dimensional data from inference attacks, this paper proposes a learning approach which differs significantly from previous syntactic or differentially-private approaches. Suppose each participant uses a privacy mechanisms to sanitize her data before releasing them to third parities. The sanitization mechanism considered in the paper is non-invertible transformation–called filter–of original raw features. An adversary’s goal is to infer sensitive or identifying attributes of participants from the released filtered data using machine learning algorithms. The problem can be viewed as a minimax game between participants and an adversary. Participants need a filter that minimizes the maximum accuracy any adversary may achieve in predicting sensitive or identifying attributes. In this context, privacy of filtered data is measured by expected risk of adversarial algorithms on specific inference tasks. However, if privacy is the only goal, near-perfect privacy is achievable with a simple mechanism that sends no or garbage data, which has no utility for data analysts at all. To avoid trivial solutions, utility of filtered data needs to be considered as the second goal, which is measured by expected risk of analysts’ algorithms on target tasks. Finding an optimal tradeoff between utility and privacy is a central question in privacy research (see Related work.) The paper proposes to minimize the difference of two risks which is essentially a minimax problem, and the corresponding optimal solution will be referred to as minimax filter. Since the training procedure can only access empirical risks, the performance of the filter on test data is given in the form of expectation/probability. The paper presents generalization error analysis for empirical minimax optimizers in analogy with the analysis of empirical risk minimizers (ERM). Lastly, finding a minimax filter involves solving a continuous min-diff-max problem The paper discusses several optimization methods including a numerical optimizer by (Kiwiel, 1987) and a new alternating optimization method, based on the classic theorem of Danskin (1967).\nThe goal of minimax filter is to prevent inference attacks, and its privacy guarantee is quite different from other privacy mechanisms. It is task-dependent and is given in probability/expectation rather than given absolutely. This may be considered weaker than\nsome others such as differential privacy. Since minimax filter and differential privacy have almost independent goals, it is natural to ask if the two methods can be combined to take advantages of both methods. Consequently, this paper presents an extension of minimax filter called noisy minimax filter, which combines the filter with additive noise mechanism to satisfy the differential privacy criterion. Two methods of combination–preprocessing and postprocessing–are proposed (see Fig. 2.) In the preprocessing approach, minimax filter is applied before perturbation to reduce the sensitivity of transformed data, so that the same level of differential privacy is achieved with less noise. However, it requires a trusted curator who trains the minimax filter. In the postprocessing approach, a minimax filter is applied after perturbation. Since postprocessing cannot worsen differential privacy (Dwork et al., 2014), this approach has the advantages of not requiring a trusted curator and no leak of information through the released filter.\nMinimax filter and its extensions are evaluated with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion. Experiments show that publicly available continuous and high-dimensional data sets are surprisingly susceptible to subject identification attacks, and that minimax filters can reduce the privacy risks to near chance levels without sacrificing utility too much. Experiments with noisy minimax filter also yield intuitive results. Differential privacy and resilience to inference attack are indeed different goals, such that using differentially private mechanism alone to achieve the latter requires a large amount of noise that destroys utility of data. In contrast, minimax filters can suppress inference attack with little loss of utility with or without perturbation. Therefore, adding a small amount of noise to the minimax filter can provide a formal differential privacy to a degree and also high on-average taskdependent utility and privacy against inference attacks.\nTo summarize, the paper has the following contributions.\n• The paper proposes a novel filtering approach which preserves privacy of continuous and high-dimensional attributes against inference attacks. This mechanism is different from previous mechanisms in many ways; in particular, it is a learning-based approach and is task-dependent. • The paper measures utility and privacy by expected risks, and formulates the utilityprivacy tradeoff as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization performance of empirical optimizers. • The paper presents a practical algorithm which can find minimax filters for a broad family of filters and losses/classifiers. The proposed optimization algorithm and supporting classes are made public on an open-source repository1. • The paper proposes preprocessing and postprocessing approaches to combine minimax filter with noisy mechanisms. The resulting combination can achieve resilience to inference attacks and differential privacy at the same time. • The paper evaluates proposed algorithms on real-world tasks and compare them with representative algorithms from the literature.\nThe rest of the paper is organized as follows. Sec. 2 compares the proposed approach further with previous approaches. Sec. 3 presents minimax filters and analyzes its general-\n1. https://github.com/jihunhamm/MinimaxFilter\nization performance on test data. Sec. 4 explains the difficulty of solving general minimax problems, and present a simple alternating optimization algorithm. Sec. 5 presents noisy minimax filters and two methods of perturbation by additive noise. Sec. 6 evaluates minimax filters with three data sets compared to non-minimax approaches and also evaluates noisy minimax filters under varying levels of privacy. Sec. 7 concludes the paper with discussions."
    }, {
      "heading" : "2. Related work",
      "text" : "Optimal utility-privacy tradeoff is one of the main questions in privacy research. Utilityprivacy tradeoff has particularly been well-studied under differential privacy assumptions (Dwork and Nissim, 2004; Dwork et al., 2006; Dwork, 2006), in the context of the statistical estimation (Smith, 2011; Alvim et al., 2012; Duchi et al., 2013) and learnability (Kasiviswanathan et al., 2011).\nOther measures of privacy and utility were also proposed. Information-theoretic quantities were proposed by Sankar et al. (2010); Rebollo-Monedero et al. (2010); du Pin Calmon and Fawaz (2012) who analyzed privacy in terms of the rate-distortion theory in communication. One problem with using mutual information or related quantity is that it is difficult to estimate mutual information of high-dimensional and continuous variables in practice without assuming a simple distribution. In contrast, this paper proposes to use classification or regression risks to measure privacy and utility, which is directly computable from data without making assumptions on the distribution. Regarding the use of risks in this paper, classification error-based quantities have been suggested in the literature (Iyengar, 2002; Brickell and Shmatikov, 2008; Li and Li, 2009). However, privacy in these works is measured either by syntactic anonymity or probabilistic divergence which are main appropriate for discrete attributes. In this paper, privacy and utility are both defined with risks and are therefore directly comparable when defining the tradeoff of the two. Furthermore, the proposed method explicitly preserves privacy against inference attacks, which both syntactic and differentially-private methods are known to be susceptible to (Cormode, 2011).\nMost of the aforementioned works focused on the analyses of utility-privacy tradeoff using different measures and assumptions. Few studied efficient algorithms to actively find optimal tradeoff which this paper aims to do. For discrete variables, Krause and Horvitz (2008) studied the NP-hardness of optimal utility-privacy tradeoff in discrete attribute selection, and demonstrated near-optimality of greedy selection. In particular, they used a weighted difference of utility and privacy cost as the joint cost similar to this work. Ghosh et al. (2009) proposed geometric mechanism and linear programming to achieve near-optimal utility for unknown users. However, optimization problems for discrete distributions are quite different from the problems involving high-dimensional and/or continuous distributions.\nAlgorithms for preserving privacy of high-dimensional face images has been proposed previously. Newton et al. (2005) applies k-anonymity to images; Enev et al. (2012) learns a linear filter using Partial Least Squares to reduce the covariance between filtered data and private labels; Whitehill and Movellan (2012) also learns a linear filter using the logratio of the Fisher’s Linear Discriminant Analysis metrics. This paper differs from these in\nseveral aspects: it is not limited to linear filters and is applicable to arbitrary differentiable nonlinear filters such as multilayer neural networks; it directly optimizes the utility-privacy risk instead of optimizing heuristic criteria such as covariance differences or LDA log-ratios.\nLastly, the alternating optimization algorithm (Alg. 1) presented in this paper is related to the algorithm proposed by Goodfellow et al. (2014), which solves a different problem. The algorithm in this paper solves a min-diff-max problem to find an optimal utility-privacy tradeoff, while Goodfellow et al. (2014) solve a minimax problem to learn generative models.\nParts of this paper have appeared in conference proceedings (Hamm, 2015, 2016). New materials in this paper include reformulations of concepts and terms, ERM-like analysis of generalization error, new closed-form examples for minimax optimization, and a new alternating optimization algorithm to solve min-diff-max problems. All results in this paper are produced using the new algorithm."
    }, {
      "heading" : "3. Minimax Filter",
      "text" : "In this section, minimax filter is formulated and discussed in detail, and its generalization performance is analyzed."
    }, {
      "heading" : "3.1 Formulation",
      "text" : "Minimax filter is a non-invertible transformation of raw features/attributes such that the transformed data has optimal utility-privacy tradeoff. Non-invertibility is assumed so that original features are not always recoverable from the filtered data. Let’s assume the filter is deterministic; randomize filters will be discussed in Sec. 5. Let X ⊂ RD be the space of features/attributes as real-valued vectors. Note that discrete attributes can also be represented by real vectors, e.g., by one-hot vector. Let the filter be a map\ng(x;u) ∈ G : X × U → Rd (1)\nwhich continuous in x and is continuously differentiable w.r.t. the parameter u ∈ U . Given a filtered output g(x), an adversary can make a prediction hp(g(x); v) of a private variable y which can be an identifier (e.g., subject number) or a sensitive attribute (e.g., religious beliefs). The prediction function hp(g(x); v) parameterized by v ∈ V is assumed to be continuous in v and continuously differentiable w.r.t. the input g(x). The paper proposes to use the expected risk to measure the privacy of filtered output against adversarial inference:\nfpriv(u, v) , E[lp(hp(g(x;u); v), y)], (2)\nwhere lp(·) is a continuously differentiable loss function. From the assumptions above, fpriv is continuously differentiable w.r.t. the filter parameter u.\nTrivial solutions to maximize privacy already exist, which are the filters that output random or constant numbers independent of actual data. However, such filters have no utility whatsoever for data analysts. To avoid trivial solutions, it is necessary to consider the secondary goal of maximizing utility. Let z be a target variable such as medical diagnosis that is of interest to the participants/analysts. An analyst can make a prediction hu(g(x);w) parameterized by w ∈ W, which is assumed to be continuous in w and continuously differentiable w.r.t. the input g(x). The ‘disutility’ of the filtered output for the\nanalyst is also measured by the expected risk\nfutil(u,w) , E[lu(hu(g(x;u);w), z)], (3)\nwhere lu(·) is a continuously differentiable loss function, such that futil is continuously differentiable w.r.t. the filter parameter u. To facilitate analysis, the paper assumes that the constraint sets W, V, and U are compact and convex subsets of Euclidean spaces such as a ball with a large but finite radius. Along with the assumption that the filter g and the risks fpriv and futil are all continuous, min and max values are bounded and attainable. In addition, the solutions to min or max problems are assumed to be in the interior of W, V, and U , enforced by adding appropriate regularization (e.g, λ‖w‖2) to the optimization problems if necessary. For this reason, min or max problems that appear in the paper will be treated as unconstrained and the notations u ∈ U , v ∈ V, and w ∈ W will be omitted.\nHaving defined the privacy and utility measure, the goal of a filter designer is to find a filter that achieves two objectives. The first objective is to maximize privacy\nmax u min v fpriv(u, v) (or equivalently, min u max v −fpriv(u, v)) (4)\nwhere minv fpriv(u, v) represents the risk of the worst (i.e., most capable) adversary: the smaller the risk, the more accurately can she infer private variables. As mentioned above, this problem alone has a trivial solution such as a constant filter that outputs zeros. The second objective is to minimize disutility\nmin u min w futil(u,w) (or equivalently, min u −max w −futil(u,w)) (5)\nwhere minw futil(u,w) represents the risk of the best analyst: the smaller the risk, the more accurately can the analyst predict variables of interest. To achieve the two opposing goals, one solves the joint problem which is to minimize a weighted sum\nmin u [ max v −fpriv(u, v) + ρ min w futil(u,w) ] , (6)\nor equivalently a weighted difference\nmin u [ max v −fpriv(u, v)− ρmax w −futil(u,w) ] . (7)\nThe constant ρ > 0 determines the relative importance of utility versus privacy. For a small ρ 1, the problem is close to a trivial privacy-only task, and for a large ρ 1, the problem is close to a utility-only task. The solution to (6) or (7) will be referred to as Minimax filter 2 and is by definition an optimal filter for utility-privacy tradeoff in terms of expected risks given the family of filters and the family of losses/classifiers. Note that the choice of filter and loss/classifier families is very flexible, with the assumption of differentiability only. In practice, almost-everywhere differentiability suffices to use the algorithm in the paper. Fig. 1 shows an example filter/classifier from the class of multilayer neural networks. As an aside, the joint problem may be formulated as minimization of disutility with a hard constraint on privacy risk. When using interior-point methods, the procedure is roughly equivalent to solving (7) iteratively with an increasing ρ, which is more demanding than minimizing the weighted sum once as the paper proposes.\n2. To be precise, the joint task (6) is a min-diff-max problem and the privacy-only task (4) is a minimax problem. However, both will be referred to as minimax when it is not important to distinguish the two."
    }, {
      "heading" : "3.2 Notes on private and utility tasks",
      "text" : "The private variable y can be any attribute which is considered sensitive or identifying. For example, let y be any number or string unique to a person in the data set. Such identifiers are bijective with {1, ..., S}, where S is the total number of participants, so assume y ∈ {1, ..., S}. The private task for an adversary is then to predict the identity y from the filtered data g(x), whose inaccuracy is measured by expected risk. That is, the higher the risk, the more anonymous the filtered output is. The identity variable can also be group identifiers, e.g., y is a demographic grouping based on age, sex, ethnicity, etc. Another example of private tasks is to single-out a particular subject among the rest, in which case y is binary: y = −1 means ‘not the target subject’ and y = 1 means ‘target subject’. To summarize, anonymity of filtered data in this paper means resilience to inference attacks on any identity variable y that we choose, and is no different from attacks on other non-identity type variables. This unifying approach is convenient since we need not determine whether an attribute is an identifier, a quasi-identifier or a sensitive attribute as in syntactic anonymization. Any information hidden in the continuous high-dimensional features which are relevant to the private variable y–whatever it may be–will be filtered out by construction.\nSimilarly, the target variable z of interest can be any attribute that is not the same as the private variable y. When they are the same (z = y), the objective (7) becomes\nmin u\n[ (1− ρ) max\nv −f(u, v)\n] (8)\nwhich is either a trivial privacy-only problem when 0 ≤ ρ < 1, or a utility-only problem when ρ > 1. In general, z and y will be correlated to certain degrees, and the minimax filter\nwill find the best compromise of utility and privacy risks. Also, private and target tasks need not be classification tasks only. Regression tasks can also be used as a target task, as well as unsupervised tasks that do not require label z. Unsupervised tasks are useful when the participants/curator do not know beforehand what tasks analysts will perform at a later time. Without such knowledge, one can use least-squares autoencoding of original features as the target task\nfutil(u,w) = E[‖h(g(x;u);w)− x‖2], (9)\nwhere g(x;u) and h(·;w) are a encoder and a decoder respectively, such as a nonlinear autoencoder constructed from multilayer neural networks."
    }, {
      "heading" : "3.3 Multiple tasks",
      "text" : "Extension to multiple private and target tasks is straightforward. Suppose there are Np private tasks f1priv(u, v1), ..., f Np priv(u, vNp) associated with private random variables y 1, ..., yNp. Note that f ipriv(u, vi) = E[l i p(y i(g(x;u); vi), y i)]. Similarly, suppose there are Nu target tasks f1util(u,w1), ..., f Nu util(u,wNu) associated with target random variables z1, ..., zNu. If κ1, ..., κNp are the coefficients representing relative importance of private tasks, and ρ1, ..., ρNu are the coefficients for utility tasks, then the joint goal is to solve the following problem\nmin u  Np∑ i=1 κi max vi −f ipriv(u, vi) + Nu∑ i=1 ρi min wi f iutil(u,wi)  . (10) Since this can be rewritten as\nmin u  max v1,...,vNp − Np∑ i=1 κif i priv(u, vi) + minw1,...,wNu Nu∑ i=1 ρif i util(u,wi)  , (11) a multiple task problem is nearly identical to the original single task problem (6), with the new utility and privacy tasks defined as\nf̂priv(u, v=(v1, ..., vNp)) , ∑ i κif i priv(u, vi), and (12)\nf̂util(u,w=(w1, ..., wNu)) , ∑ i ρif i util(u,wi), (13)\nwith ρ = 1. Using this, it is straightforward to extend the analysis and algorithms developed for single tasks to those for multiple tasks."
    }, {
      "heading" : "3.4 Generalization performance of minimax filter",
      "text" : "The proposed privacy mechanism is a learning approach. An optimal filter is one that solves the expected risk optimization (6). However, in reality, an optimal filter has to be estimated from finite training samples, and we need a guarantee on the performance of the learned filter on unseen test samples. This section derives generalization bounds for empirical minimax filter, similarly to the bounds for empirical risk minimization (ERM).\nThe joint problem for expected risks was\nmin u [ max v −fpriv(u, v) + ρmin w futil(u,w) ] = min u [ max v −E[lp(u, v)] + ρmin w E[lu(u,w)] ] .\n(14) A joint loss lJ is introduced for convenience:\nlJ(u, v, w) , −lp(u, v) + ρ lu(u,w). (15)\nLet (u∗, v∗, w∗) be a solution to the expected risk optimization problem:\nED[lJ(u ∗, v∗, w∗)] = min\nu [ max v ED[−lp(u, v)] + ρmin w ED[lu(u,w)] ] , (16)\nwhere ED[·] is the expected value w.r.t. the unknown data distribution P (x, y). Similarly, Let (û, v̂, ŵ) be a solution to the empirical risk minimax problem:\nES [lJ(û, v̂, ŵ)] = min u [ max v ES [−lp(u, v)] + ρmin w ES [lu(u,w)] ] , (17)\nwhere the empirical mean ES [·] for S = {(x1, y1), · · · , (xN , yN )} is\nES [l(x, y)] , 1 |S| ∑\n(x,y)∈S\nl(x, y). (18)\nThe goal in this analysis is to show that the expected and the empirical optimizers perform equally well in expectation/probability given enough training samples:\nED[lJ(û, v̂, ŵ)] ' ED[lJ(u∗, v∗, w∗)], as |S| → ∞. (19)\nThe main result is the Theorem 4 which is proved in the remainder of this section. Let’s define optimal parameters v(u) and w(u) given u as\nv∗(u) , arg max v ED[−lp(u, v)], v̂(u) , arg max v ES [−lp(u, v)], (20)\nw∗(u) , arg min w ED[lu(u,w)], ŵ(u) , arg min w ES [lu(u,w)]. (21)\nOne can then write\nED[lJ(u ∗, v∗, w∗)] = min u [ED[−lp(u, v∗(u))] + ρED[lu(u,w∗(u))]] (22)\n= min u ED[lj(u, v\n∗(u), w∗(u))], (23)\nand similarly\nES [lJ(û, v̂, ŵ)] = min u [ES [−lp(u, v̂(u))] + ρES [lu(u, ŵ(u))]] = min u ES [lj(u, v̂(u), ŵ(u))].\n(24) From these definitions we have for all u,\nED[lJ(u ∗, v∗, w∗)] ≤ ED[lJ(u, v∗(u), w∗(u))], (25)\nES [lJ(û, v̂, ŵ)] ≤ ES [lJ(u, v̂(u), ŵ(u))]. (26)\nAlso from definition, for all (u, v, w),\nED[lJ(u, v, w ∗(u))] ≤ ED[lJ(u, v, w)] ≤ ED[lJ(u, v∗(u), w)], (27)\nES [lJ(u, v, ŵ(u))] ≤ ES [lJ(u, v, w)] ≤ ES [lJ(u, v̂(u), w)]. (28)\nThese observations imply the following theorem.\nTheorem 1 The risk difference of expected and empirical optimizers is at most twice of the largest difference of expected and empirical risks of any set of parameters:\n|ED[lJ(û, v̂, ŵ)]− ED[lJ(u∗, v∗, w∗)]| ≤ 2 sup u,v,w |ED[lJ(u, v, w)]− ES [lJ(u, v, w)]| . (29)\nProof The expected risk of empirical risk optimizers (û, v̂, ŵ) is upper-bounded by the risk of expected risk optimizers (u∗, v∗, w∗) as follows:\nED[lJ(û, v̂, ŵ)]− ED[lJ(u∗, v∗, w∗)] = ED[lJ(û, v̂, ŵ)]− ES [lJ(û, v̂, ŵ)]− (ED[lJ(u∗, v∗, w∗)]− ES [lJ(û, v̂, ŵ)]) ≤ ED[lJ(û, v̂, ŵ)]− ES [lJ(û, v̂, ŵ)]− (ED[lJ(u∗, v∗, w∗)]− ES [lJ(u∗, v̂(u∗), ŵ(u∗))]) (from (26)) ≤ ED[lJ(û, v̂, ŵ)]− ES [lJ(û, v̂, ŵ)]− (ED[lJ(u∗, v̂(u∗), w∗)]− ES [lJ(u∗, v̂(u∗), ŵ(u∗))]) (from (27)) ≤ ED[lJ(û, v̂, ŵ)]− ES [lJ(û, v̂, ŵ)]− (ED[lJ(u∗, v̂(u∗), w∗)]− ES [lJ(u∗, v̂(u∗), w∗)]) (from (28)) ≤ 2 sup\nu,v,w |ED[lJ(u, v, w)]− ES [lJ(u, v, w)]| .\nThe difference can also be lower-bounded as follows:\nED[lJ(u ∗, v∗, w∗)]− ED[lJ(û, v̂, ŵ)]\n= ED[lJ(u ∗, v∗, w∗)]− ES [lJ(û, v̂, ŵ)]− (ED[lJ(û, v̂, ŵ)]− ES [lJ(û, v̂, ŵ)]) ≤ ED[lJ(û, v∗(û), w∗(û))]− ES [lJ(û, v̂, ŵ)]− (ED[lJ(û, v̂, ŵ)]− ES [lJ(û, v̂, ŵ)]) (from (25)) ≤ ED[lJ(û, v∗(û), w∗(û))]− ES [lJ(û, v∗(û), ŵ)]− (ED[lJ(û, v̂, ŵ)]− ES [lJ(û, v̂, ŵ)]) (from (28)) ≤ ED[lJ(û, v∗(û), ŵ)]− ES [lJ(û, v∗(û), ŵ)]− (ED[lJ(û, v̂, ŵ)]− ES [lJ(û, v̂, ŵ)]) (from (27)) ≤ 2 sup\nu,v,w |ED[lJ(u, v, w)]− ES [lJ(u, v, w)]| .\nTo bound the RHS of (29), one can use the Rademacher complexity theory (e.g., Lemma 26.2 of Shalev-Shwartz and Ben-David (2014).)\nLemma 2 Let F be a class of real-valued functions, and let S be a set of N samples S = {(x1, y1), ..., (xN , yN )}. Then,\nES∼DN [ sup f∈F |ED[f ]− ES [f ]| ] ≤ 2ES∼DN [R(F ◦ S)], (30)\nwhere R(F ◦ S) is the empirical Rademacher complexity\nR(f ◦ S) , 1 N Eσ∼{−1,+1}N [ sup f∈F N∑ i=1 σif(xi, yi) ] (31)\nfor the class of real-valued functions {(x, y) 7→ f(x, y) : ∀f ∈ F}.\nConsider the class of real-valued functions defined from the joint loss (15):\nlJ ◦HJ ◦ S , {(x, y, z) 7→ lJ(x, y, z;u, v, w) : u ∈ U , v ∈ V, w ∈ W} (32) = {(x, y, z) 7→ −lp(hv(gu(x)), y) + ρ lu(hw(gu(x)), z) : u ∈ U , v ∈ V, w ∈ W}.\nLet R(lJ ◦ HJ ◦ S) denote the empirical Rademacher complexity of the joint loss class. Furthermore, the Rademacher complexity of sum of functions can be upper-bounded by the sum of complexities:\nLemma 3 The empirical Rademacher complexity of the joint privacy-utility loss is upperbounded as\nR(lJ ◦HJ ◦ S) ≤ R(lp ◦Hp ◦G ◦ S) + ρ R(lu ◦Hu ◦G ◦ S), (ρ > 0) (33)\nwhere\nlp ◦Hp ◦G ◦ S , {(x, y, z) 7→ lp(hp(gu(x)), y) : u ∈ U , v ∈ V}, (34) lu ◦Hu ◦G ◦ S , {(x, y, z) 7→ lu(hu(gu(x)), z) : u ∈ U , w ∈ W}. (35)\nProof\nR(lJ ◦HJ ◦ S) = 1\nN Eσ [ sup u,v,w ∣∣∣∣∣ N∑ i=1 σilJ(xi, yi, zi;u, v, w) ∣∣∣∣∣ ]\n(36)\n= 1\nN Eσ [ sup u,v,w ∣∣∣∣∣ N∑ i=1 σi(−lp(xi, yi;u, v) + ρ lu(xi, zi;u,w)) ∣∣∣∣∣ ]\n(37)\n≤ 1 N Eσ [ sup u,v ∣∣∣∣∣ N∑ i=1 σilp(xi, yi;u, v) ∣∣∣∣∣+ ρ supu,w ∣∣∣∣∣ N∑ i=1 σilu(xi, zi;u,w) ∣∣∣∣∣ ] (38) = R(lp ◦Hp ◦G ◦ S) + ρ R(lu ◦Hu ◦G ◦ S). (39)\nFrom Theorem 1 and Lemmas 2 and 3, we get the following generalization bounds in terms of the Rademacher complexity.\nTheorem 4\nES∼Dm [|ED[lJ(u∗, v∗, w∗)]− ED[lJ(û, v̂, ŵ)]|] ≤ 4ES∼Dm [R(lp ◦Hp ◦G ◦ S) + ρ R(lu ◦Hu ◦G ◦ S)] . (40)\nA probabilistic bound instead of expected value can also be obtained by applying McDiarmid’s inequality, which is omitted.\nThe Rademacher complexity of privacy and utility losses depends on our choice of loss functions, hypothesis classes, and filter classes. For the simple case of linear filters and linear classifiers, one can compute the complexity using the following lemmas (26.9 and 26.10 from Shalev-Shwartz and Ben-David (2014)):\nLemma 5 Suppose φ : R → R is α-Lipschitz, i.e., |φ(a) − φ(b)| ≤ α|a − b|, ∀a, b ∈ R. Then,\nR(φ ◦ F ) = αR(F ). (41)\nLemma 6 For the class of linear classifiers H = {x 7→ wTx : ‖w‖2 ≤ 1},\nR(H ◦ S) ≤ 1√ N sup x∈S ‖x‖2. (42)\n.\nFrom these lemmas and Theorem 4, we have a corollary for a simple case of linear filters and classifiers.\nCorollary 7 Let the loss functions lu and lp be α-Lipschitz (e.g., α = 1 for logistic regression.) Suppose U is a d × D real matrix with a bounded norm ‖U‖2 ≤ 1, and w and v are vectors with bounded norms (‖w‖2 ≤ 1 and ‖v‖2 ≤ 1). If the feature domain X is also bounded with a radius r = maxx∈X ‖x‖2, then we have\n|ED[lJ(u∗, v∗, w∗)]− ED[lJ(û, v̂, ŵ)]| ≤ 4(1 + ρ) α r√\nN . (43)\nAlternatively, one can use the VC dimension to specify the bound. In any case, the generalization bounds in this section justify the claim that minimax filter can preserve utilityprivacy of not only the current data but also the unseen data in expectation/probability generated from the same unknown distribution."
    }, {
      "heading" : "4. Minimax Optimization",
      "text" : "This section presents theoretical and numerical solutions of the joint problem (7), which is a variant of unconstrained continuous minimax problems (see Rustem and Howe (2009) for a review.) The problem (7) can be written in an equivalent form\nmin u Φ(u) = min u\n[Φpriv(u)− ρ Φutil(u)] (44)\n= min u [max v −fpriv(u, v)− ρ max w −futil(u,w)] (45)\nThe optimization above is a min-diff-max problem and can be considered as simultaneously solving two subproblems minu[maxv −fpriv(u, v)] and minu[−maxw−futil(u,w)], but is evidently not the same as summing individual solutions\nmin u Φ(u) 6= min u [max v −fpriv(u, v)] + min u [−ρmax w −futil(u,w)]. (46)\nSince the second subproblem minu[−maxw−futil(u,w)] = minu,w futil(u,w) is a standard minimization problem, let’s focus only on the first subproblem minu[maxv −fpriv(u, v)] which is a continuous minimax problem. Continuous minimax problems are in general more challenging to solve than standard minimization problems, as the inner optimization Φpriv(u) = maxv −fpriv does not usually have a closed-form solution; when it does, the whole problem can be treated as a standard minimization problem. Furthermore, there can be more than one solution to Φpriv(u) = maxv −fpriv. To better understand minimax problems, we look at several examples starting from a simple case where Φpriv and Φutil have closed-form solutions."
    }, {
      "heading" : "4.1 Simple case: eigenvalue problem",
      "text" : "Consider finding a minimax filer for the following problem. The filter class is a linear dimensionality reduction (g(x;u) = UTx) parameterized by the matrix U ∈ RD×d, and the private and target tasks are least-squares regressions parameterized by the matrices V and W :\nfpriv(U, V ) = 1\nN ∑ i ‖V TUTxi − yi‖2, and (47)\nfutil(U,W ) = 1\nN ∑ i ‖W TUTxi − zi‖2. (48)\nIn this case, Φpriv(U) = maxV −fp(U, V ) and Φutil(U) = maxW −fu(U,W ) are both concave problems with closed-form solutions\nV̂ = arg min V\nfpriv = (U TCxxU) −1UTCxy and (49)\nŴ = arg min W\nfutil = (U TCxxU) −1UTCxz, (50)\nwhere\nCxy = 1\nN ∑ i xiy T i , Cxz = 1 N ∑ i xiz T i , and Cxx = 1 N ∑ i xix T i . (51)\nThe corresponding min values are Φpriv(U) = −fpriv(U, V̂ ) = Tr [ (UTCxxU) −1UTCxyC T xyU ] + const, and (52)\nΦutil(U) = −futil(U, Ŵ ) = Tr [ (UTCxxU) −1UTCxzC T xzU ] + const. (53)\nThe outer minimization over u is then\nmin U Φ(U) = min U\n[Φpriv(U)− ρΦutil(U)] (54)\n= min U\n[ −fpriv(U, V̂ ) + ρfutil(U, Ŵ ) ] = Tr [ (UTCxxU) −1 UTCxyzU ] , (55)\nwhere Cxyz = CxyC T xy − ρCxzCTxz. (56)\nThis special case problem is quite similar to the objective of Enev et al. (2012):\nmax u\n[−λuTCTxyCxyu+ uTCTxzCxzu], s. t. uTu = 1,\nThe problem (55) can be reformulated as a generalized eigenvalue problem. Let Q = C 1/2 xx U be a D × d full-rank matrix. The problem can be rewritten as\nmin U\nTr [ (UTCxxU) −1 UTCxyzU ]\n= min Q\nTr [ (QTQ)−1QTC−1/2xx CxyzC −1/2 xx Q ] . (57)\nFurthermore, note that min value (55) is invariant to the right multiplication of U by any d× d nonsingular matrix R. So chose R so that QTQ = RTUTCxxUR = Id without loss of\ngenerality. Let A = (C −1/2 xx )TCxyzC −1/2 xx , and the minimax problem becomes the following eigenvalue problem: min U Φ(U) = min {Q | QTQ=Id} Tr QTAQ, (58)\nwhich is the sum of the d smallest eigenvalues of A. Note that A may not be positive definite.\nThe paper also proposes a variant of the above eigenvalue problem, called Privacy LDS which is an analogue of linear discriminant analysis (LDS) for privacy-utility optimization problem. Define the symmetric positive semidefinite matrix Cu as\nCu = K∑ k=1 Nk(µk − µ)(µk − µ)T , (59)\nwhere\nz ∈ {1, ...,K}, Nk = N∑ i=1 I[zi = k], µk = 1 Nk N∑ i=1 xiI[zi = k], and µ = 1 N N∑ i=1 xi, (60)\nDefine Cp similarly with\ny ∈ {1, ...,K ′}, N ′k = N ′∑ i=1 I[yi = k], and µ ′ k = 1 N ′k N ′∑ i=1 xiI[yi = k]. (61)\nThe proposed Privacy LDS is a linear filter g(x;W ) = W Tx, where W = [v1, ..., vd] is a matrix of top eigenvectors vi’s from the following generalized eigenvalue problem:\nmax ‖v‖=1\nvT (Cu + λI)v vT (Cp + λI)v . (62)\nThis paper uses Privacy LDS as a heuristic to find the initial linear filter before fine-tuning the parameter u using a general optimization methods presented in the following sections. Note that this initialization applicable only to linear filters."
    }, {
      "heading" : "4.2 Saddle-point problem",
      "text" : "Continuous minimax problems in general cannot be solved in closed form and require numerical solvers. There is a subclass of continuous minimax problems which are easier to solve than others. Saddle point problems are minimax problems for which f(u, v) is convex in u and concave in v, such as\nmin u max v f(u, v) = min u max v\n[u2 − v2]. (63)\nAnalogous to convex problems, f(u, v) has a global optimum (u∗, v∗) which satisfies\nf(u∗, v) ≤ f(u∗, v∗) ≤ f(u, v∗). (64)\nThe presence of global optima in saddle-point problems makes numerical optimization practical. Convergence rate of a simple subgradient-descent method for saddle-point problems\nwas previously analyzed by Nedić and Ozdaglar (2009). Unfortunately, the minimax problem minu maxv −fpriv considered in this paper is not a saddle-point problem even for a relative simple case. Suppose one chooses linear filters, convex differentiable losses (e.g., least-squares, logistic, or exponential losses) and linear classifiers for the problem. Then\n− fpriv(u, v) = −E[l(g(u); v)] = −E[l(yvT g(UTx))] (65)\nis the negative expected value of the composition of a convex l(·) and a linear UTx, which is concave in u as well as concave in v, which cannot a saddle-point problem."
    }, {
      "heading" : "4.3 General problem",
      "text" : "A general numerical solution to the optimization (44) is described in this section. Let f(u, v) be a real-valued function f : Rd × V → R, where V is a compact subset of the Euclidean space. Suppose f is jointly continuous and has a continuous partial derivative ∇uf w.r.t. the first variable u. The maximum over v\nΦ(u) = max v∈V f(u, v) (66)\nhas a property that Φ(u) is in general not differentiable in u even if f(u, v) is. The derivatives of Φ(u) was studied by Danskin (1967). Suppose V (u) is the set of maximizers of f given u:\nV (u) = {v̂ ∈ V | f(u, v̂) = max v∈V f(u, v)}. (67)\nDanskin proved that the directional derivative DyΦ(u) in any direction y ∈ Rd can be written as the maximum directional derivatives of f(u, v) over all v̂ ∈ V (u):\nDyΦ(u) = max v̂∈V (u) Dyf(u, v̂), (68)\nwhere Dyf(u, v) is the directional derivative of f w.r.t. u. Furthermore, in the case where V (u) is a singleton {v̂(u)} for each u, we have\nDyΦ(u) = Dyf(u, v̂(u)). (69)\nThere are several classic minimax optimization algorithms using this property. Suppose f(u, v) is also continuously differentiable w.r.t. v, and ∇uf is continuously differentiable w.r.t. v. A first-order method for minimax problems was proposed by Panin (1981) and was later refined by Kiwiel (1987). The latter uses a linear approximation of f at a fixed ū along the direction q\nf l(q, v) = f(ū, v) + 〈∇uf(ū, v), q〉, (70)\nand uses it to compute the approximate max value\nΦl(q) = max v f l(q, v). (71)\nUsing this approximation, a line search can be performed along the descent direction q that minimizes the max function Φ(ū+ αq). In particular, with additional assumptions of Lipschitz continuity of ∇uf and compactness of U and V, Kiwiel’s algorithm monotonically\ndecreases f for each iteration and converges to a stationary point u∗, i.e., a point u for which maxv〈∇uf(u∗, v), q〉 ≥ 0 for all directions q. Previously, Hamm (2015) used Kiwiel’s algorithm to solve the optimization problems (44). However, one disadvantage of the method was that its slow speed in practice, due to the auxiliary routine of finding the descent direction q at each iteration (described in the supplementary material of Hamm (2015).)\nInstead, this paper proposes a simple alternating algorithm (Alg. 1) for solving min-diffmax problem based directly on Danskin’s theorem. The algorithm only assumes fpriv(u, v) and futil(u,w) to be jointly continuous and have continuous partial derivatives ∇ufpriv and ∇ufutil. Additionally, if fpriv(u, v) and futil(u,w) are convex in v and w respectively, then the global minima\nvt = arg min v fpriv(ut, v) and wt = arg min w futil(ut, w) (72)\ncan be found easily, either approximately or accurately. Furthermore, if futil and fpriv are strongly convex (e.g., due to regularization), the solutions are unique. Consequently, the descent direction qt in Alg. 1 is truly the (negative) gradient of Φ(u) (44) as desired:\nqt = ∇ufpriv(u, vt)− ρ∇ufutil(u,wt) = −∇uΦpriv(u) + ρ ∇uΦutil(u) = −∇uΦ(u). (73)\nNote that it is still a heuristic for non-convex futil and fpriv such as when using neural networks for the filter and/or the classifiers. A related heuristic for minimax problems was proposed by Goodfellow et al. (2014) for solving an unrelated problem of learning generative models.\nAlgorithm 1 Alternating algorithm for min-diff-max Input: data {(xi, yi, zi)}, filter g, loss l, classifier h, coefficient ρ, max iteration T Output: optimal filter parameter u Begin:\nInitialize u1 for t = 1, ..., T do\nSolve (approximately)\nvt = arg min v fpriv(ut, v) and wt = arg min w futil(ut, w), where (74)\nfpriv(u, v) = 1\nN N∑ i=1 lp(hp(g(xi;u); v), yi) and futil(u,w) = 1 N N∑ i=1 lu(hu(g(xi;u);w), zi).\n(75) Compute the descent direction by\nqt = ∇ufpriv(u, vt)− ρ∇ufutil(u,wt) (76)\nPerform line search along qt and update ut+1 = ut + αt · qt Exit if solution converged\nend for\nThe proposed optimization algorithm and supporting classes are implemented in Python and are made available on an open-source repository3."
    }, {
      "heading" : "5. Noisy Minimax Filter",
      "text" : "The privacy guarantee that minimax filters provide is very different from that of differentiallyprivate mechanisms. As the filter is learned from data, its privacy guarantee is given only in expectation/probability. Besides, it is a deterministic mechanism which cannot provide differential privacy. This section presents noisy minimax filter that combines minimax filter with additive noise mechanism to satisfy differential privacy criteria. Two methods of combination–preprocessing and postprocessing–are proposed and compared. For completeness, a brief introduction to differential privacy is given."
    }, {
      "heading" : "5.1 Differential privacy",
      "text" : "A randomized algorithm that takes dataD as input and outputs f̃(D) is called -differentially private if\nPr(f̃(D) ∈ S) ≤ e Pr(f̃(D′) ∈ S) (77)\nfor all measurable S ⊂ T of the output range and for all data sets D and D′ differing in a single item, denoted by D ∼ D′. That is, even if an adversary knows the whole data set D except for a single item, she cannot infer much more about the unknown item from the output of the algorithm. A well-known mechanism for turning a non-private function f into a private function f̃ is the perturbation by additive noise. When an algorithm outputs a real-valued vector f(D) ∈ RD, its global sensitivity (Dwork et al., 2006) is defined as\nS(f) = max D∼D′\n‖f(D)− f(D′)‖ (78)\nwhere ‖ · ‖ is a norm such as the Euclidean norm. An important result from Dwork et al. (2006) is that the perturbation by additive noise\nf̃(D) = f(D) + ξ, (79)\nwhere ξ has the Laplace-like probability density whose scale parameter is proportional to S(f)\nP (ξ) ∝ e− S(f) ‖ξ‖\n(80)\nis -differentially private. This paper considers local differential privacy (Duchi et al., 2013) of filtered output g(x), that is, perturbation is applied by each participant before g(x) is released to a third party. In addition, per-sample privacy is of interest, in which the neighboring data sets are D = {x} and D′ = {x′}, where x and x′ are any two samples from the common feature space (x, x′ ⊂ X ) of all participants. Consequently, a randomized filter g̃(·) is -differentially private if for all x, x′ ∈ X and all measurable S ⊂ T of the output range,\nPr(g̃(x) ∈ S) ≤ e Pr(g̃(x′) ∈ S). (81)\n3. https://github.com/jihunhamm/MinimaxFilter\nTo use additivity noise mechanism (79), the sensitivity (78) of the non-random g(·) needs to be determined:\nS(g) = sup x,x′∈X\n‖g(x)− g(x′)‖ (82)\nwhich is finite if X is compact and g(·) is continuous. Care must be taken when computing the sensitivity S(g), since the filter g(·) is learned from training data and is dependent on them. This breaks the differential privacy guarantee if one simply uses a trained g(·) to compute the sensitivity. To avoid this problem without making more assumptions on X or g(·), the paper proposes to directly bound the diameter of g(X ) by bounding functions. Let h = g(x) be the filtered value of any sample x ∈ X . The bounding functions b : RD → RD are one of\n1. Hard-bound by clipping: b(h) = min{1, 1/‖h‖}h, 2. Soft-bound by squashing: b(h) = tanh(a‖h‖), or 3. Normalization: b(h) = h/‖h‖.\nNote that these functions enforce the sensitivity S(b(g)) to be at most 2, regardless of X or g(·)."
    }, {
      "heading" : "5.2 Preprocessing vs postprocessing",
      "text" : "Minimax filters can be made locally differentially private by additive noise mechanism (79) in the signal chain of filtering. The paper proposes two approaches: filtering is performed before perturbation (called preprocessing) and after perturbation (called postprocessing)4. Fig. 2 show the signal chains of the two approaches. In preprocessing, the original feature x is first filtered by g(x), and then made -differential private by bounding and perturbing b(g(x)) + ξ. In postprocessing, the original feature x is first made -differentially private by\n4. Preprocessing and postprocessing are similar to but not exactly the same as output perturbation and input perturbation (Sarwate and Chaudhuri, 2013)).\nbounding and perturbing b(x) + ξ followed by filtering g(b(x) + ξ). By adding an appropriate amount of noise, both approaches are locally differentially private regardless of data distribution. However, when the noisy mechanism is used in conjunction with a minimax filter which is dependent on data distribution P (x, y, z), preprocessing and postprocessing approaches have different effects depending on the distribution.\nA scenario when preprocessing is preferable to postprocessing is as follows. For convenience of explanation, let’s assume the private task is subject identification. Let y(x) be the subject identity label of sample x and let z(x) be the target label of sample x. Define between-subject diameter as the max distance of two samples x, x′ from different subjects that have the same target label:\nSb , max x,x′∈X\n‖x− x′‖ s.t. y(x) 6= y(x′), z(x) = z(x′). (83)\nSimilarly, define within-subject diameter as the max distance of two samples x, x′ from the same subject that have different target labels:\nSw , max x,x′X\n‖x− x′‖ s.t. y(x) = y(x′), z(x) 6= z(x′). (84)\nFor a given data set X , if the between-subject diameter is larger than the within-subject diameter (Sb > Sw) in the original feature space (Fig. 3a), then the sensitivity after minimax filtering S(g) = maxx,x′ ‖g(x)− g(x′)‖ can potentially be reduced a lot. This translates to less amount of noise to achieve the same -privacy than the amount of noise required before filtering, as the data diameter has shrunk. This results in better utility of the preprocessing approach over the postprocessing approach where noise added before filtering. From the same reasoning, if the opposite is true (Sw > Sb) (Fig. 3b), then the sensitivity after minimax filtering S(g) will not change much, and the preprocessing approach will not offer much advantage over the postprocessing. However, there are other aspects to consider as well. In the preprocessing approach, training of minimax filter by solving (6) is not itself a differentially private procedure and requires trusted collector/curator. In addition, the learned filters contain information about private data analogous to learned PCA components revealing information about data (Chaudhuri et al., 2012), and require further sanitization if the filters are released to a third party. In contrast, the postprocessing approach makes the whole process much simpler. Any postprocessing–including the training of minimax filter–does not worsen differential privacy guarantees (Dwork et al., 2014), and there is no need for trusted collector/curator once original data are perturbed early in the signal chain. In the experiment, utility and privacy of two approaches are further compared using real data."
    }, {
      "heading" : "6. Experiments",
      "text" : "In this section, algorithms proposed in the paper are evaluated using three real-world data sets: face data for gender/expression classification speech data for emotion classification, and motion data for activity classification. Firstly, minimax filters are compared with nonminimax methods in terms of privacy breach vs utility as measured by accuracy of private and target tasks classifiers on test data. Secondly, noisy minimax filters are tested under various conditions using the same data sets."
    }, {
      "heading" : "6.1 Methods",
      "text" : "Filters. The following minimax and non-minimax filters are compared:\n• Rand: random subspace projection with g(x;U) = UTx, where U is a random full rank D × d matrix. • PCA: principal component analysis with g(x;U) = UTx, where U is the eigenvectors\ncorresponding to d largest eigenvalues of Cov(x). • PPLS: private partial least squares, using Algorithm 1 from Enev et al. (2012). • DDD: discriminately decreasing discriminability (DDD) from Whitehill and Movellan\n(2012) with a mask-type filter from the code5. • Minimax 1: linear filter g(x;U) = UTx where U is computed from Alg. (1). • Minimax 2: nonlinear filter g(x) from a two-layer sigmoid neural network with of\nhidden nodes of 20 and 10, computed from Alg. (1).\nRemarks. DDD requires analytical solutions to eigenvalue problems which are unavailable for multiclass problems, and is used only in the binary problem with the face database. Also, DDD code uses a mask-type filter, and the dimension d is same as the image size. The dimension d is also irrelevant to nonlinear Minimax filter 2 since it does not use linear dimensionality reduction. The nonlinear filter is pretrained as a stacked denoising autoencoders (Vincent et al., 2008) followed by supervised backpropagation with the target task. Classifier/loss. For all experiments, binary or multinomial logistic regression is used a classifier for both utility and privacy risks, where the loss l(h(g(x;u); v), y) is the negative log-likelihood:\nl = −v(y)T g(x;u) + log( K∑ k=1 ev(k) T g(x;u)) + λ 2 K∑ k=1 ‖v(k)‖2 (85)\nwhere K is the number of classes. The regularization coefficient was λ = 10−6) and the utility-privacy tradeoff coefficient was ρ = 10. The main iteration in Alg. 1 was stopped manually when the progress was slow, which was between T = 20− 200. 5. http://mplab.ucsd.edu/~jake"
    }, {
      "heading" : "6.2 Data sets",
      "text" : "Gender/expression classification from face: The GENKI database (Whitehill and Movellan, 2012) consists of face images with varying poses and facial expressions. The original data set is used unchanged, which have N = 1740 training images (50% male and 50% female; 50% smile and 50% non-smile). The test set has 100 images (50 males and 50 females; 50 smiling and 50 non-smiling) not overlapping with the training set. The dimensionality of the original data is D = 256, and the filters are tested with d = 10, 20, 50, 100. The data set has gender and expression labels but no subject label. Consequently, gender classification is used as the private task and expression classification is used as the target task.\nEmotion classification from speech: The ENTERFACE database (Martin et al., 2006) is an audiovisual emotion database of 43 speakers from 14 nations reading predefined English sentences in six induced emotions. From the raw speech signals sampled in 48 KHz, MFCC coefficients are computed using 20 ms windows with 50% overlap and 13 Mel-frequency bands. The mean, max, min, and standard deviation of the MFCC coefficients over the duration of each sentence are computed, resulting in N = 427 samples of D = 52 dimensional feature vectors from S = 43 subjects. Each subject’s samples are randomly split to generate training (80%) and test (20%) sets. Average test accuracy over 10 such trials is reported. Filters are tested with d = 10, 20, 30, 40. The target task is binary classification of ‘happy’ and ‘non-happy’ emotions from speech, and the privacy risks is multiclass (S = 43) subject classification.\nActivity classification from motion: The UCI Human Activity Recognition (HAR) data set (Anguita et al., 2012) is a collection of motion sensor data on a smartphone by 30 subjects performing six activities (walking, walking upstairs, walking downstairs, sitting, standing, laying). Various time and frequency domain variables are extracted from the signal, resulting in N = 10299 samples of D = 561 dimensional features from 30 subjects which are used unchanged. Out of 30 subjects, 15 subjects are chosen randomly. For each domain, each subject’s samples are randomly split to generate training (50%) and test (50%) sets. At each trial, the subjects and the training/test sets are randomized, and the average test accuracy over 10 such trials is reported. Filters with dimensions d = 10, 20, 50, 100 are used. The target task is multiclass (C = 6) classification of activity, and the privacy risks is multiclass (S = 15) subject classification risk."
    }, {
      "heading" : "6.3 Result 1: Minimax filters",
      "text" : "Fig. 4 shows the test accuracy with GENKI. The dotted lines are level sets of utility-privacy tradeoff (i.e., target task accuracy - private task accuracy) shown for reference. Minimax 2 achieves the best utility (i.e., most accurate expression classification) and Minimax 1 (linear) achieves the best privacy (i.e., least accurate gender classification). For all dimensions d, Minimax 1 achieves the best utility-privacy compromise (i.e., closest to the top-left corner of the plot), with Minimax 2 and DDD performing very closely. In terms of privacy risk, Minimax 1 achieves almost the chance level accuracy (0.5), which implies a strong privacy preservation. DDD comes close to Minimax 1, while another private method PPLS is not very successful in reducing the privacy risk. As expected, non-private methods Rand and PCA do not reduce the privacy risk. As dimension d increases from 10 to 100, the accuracy\nof both the target and the private tasks increase (toward the top-right corner of the plot) for PPLS, PCA and Rand, but the value of utility-privacy tradeoff (i.e., target task accuracy - private task accuracy) remains relatively similar even though d changes. Note that d is irrelevant to Minimax 2 and DDD.\nFig. 5 shows the test accuracy of ENTERFACE. Minimax 2 achieves the best utility (i.e., most accurate emotion classification) and the best privacy (i.e., least accurate subject classification) at the same time. PPLS performs well in this task; its private and target task accuracy is close to those of Minimax 2. The private task accuracy of Minimax 2 is near the chance level (1/S = 0.02) compared to 0.4 − 0.5 of non-private methods, suggesting that seemingly harmless statistics (mean, max, min, s.d. of MFCC) are quite susceptible to identification attacks if no privacy mechanism is used. Similar to GENKI, the accuracy of both the target and the private tasks increases with the dimension d for PCA and Rand, and the value of utility-privacy tradeoff remains similar regardless of d.\nFig. 6 shows the test accuracy of HAR. Minimax 1 achieves the best utility (i.e., most accurate activity recognition) and the best privacy (i.e., least accurate subject classification), while Minimax 2 and PPLS performs similarly well. The private task accuracy of Minimax\n1 is lower than others close to the chance level (1/S = 0.067). The figure also shows that motion data are susceptible (0.2−0.7) to identification attacks when no privacy mechanism is used. For all dimensions d, Minimax 1 achieves the best compromise of all methods similar to previous experiments. Also the accuracy of both the target and the private tasks roughly increases with d for PCA and Rand, but the value of utility-privacy tradeoff remains similar."
    }, {
      "heading" : "6.4 Result 2: Noisy minimax filters",
      "text" : "The same data sets from the previous section are used to demonstrate the effect of noisy mechanism on minimax filters. Four types of noisy filters are compared: PCA-pre, PCApost, Minimax-pre, and Minimax-post. PCA is chosen as a non-minimax reference filter which preserves the original signal in the least mean-squared-error sense. PCA-pre/post means that PCA is applied before/after the perturbation similarly to Minimax-pre/post from Fig. 2. For Minimax-pre/post, a linear filter of the same dimension d as PCA-pre/post is used. Tests are performed for d = 20. Optimization of 6 is done similarly to the previous section. All tests are repeated 10 times for different noise samples of (80), for each of 10 random training/test splits.\nFig. 7 shows the following results. Firstly, within each plot, increasing the privacy level from left ( −1=0) to right ( −1=10) lowers the accuracy of both target and private tasks for all filter types and data sets, which is intuitively correct. Secondly, target task accuracy (top row) shows that the four filters are equally accurate with no noise ( −1=0), with Minimaxpre/post slightly more accurate than PCA-pre/post. This observation is consistent with the results in Sec. 6.3. In GENKI and HAR, preprocessing is better than postprocessing for both PCA and Minimax, and Minimax-pre performs the best. In ENTERFACE, preprocessing and postprocessing approaches perform similarly, and all four filters is perform similarly on the target task. This result may be ascribed to the discussion of different data distribution in Sec. 5.2. Thirdly, and most importantly, private task accuracy (bottom row) is quite different between Minimax-pre/post and non-minimax PCA-pre/post. For both Minimaxpre and Minimax-post, the private task accuracy is almost as low as the chance accuracy of each data set (0.5, 0.03, 0.07) regardless of the noise level . This demonstrates that minimax\nfilter can prevent inference attacks with little help of noise. In contrast, the non-minimax filters (PCA-pre/post) allow an adversary to infer private variables quite accurately (0.8, 0.5, 0.3) when no noise is used. Preventing such attacks for non-minimax filters requires a significant amount of additive noise (e.g., −1≥0.1) which destroys the utility of data. These results show that differentially privacy is indeed different from privacy against inference attacks and the combination of two methods is beneficial."
    }, {
      "heading" : "7. Conclusion",
      "text" : "This work presents a new privacy-preserving mechanism for preventing inference attacks on continuous and high-dimensional data. In this mechanism, a filter transforms continuous and high-dimensional raw features to dimensionality-reduced representation of data. After filtering, information on target tasks remains but information on identifying/sensitive attributes is removed which makes it difficult for an adversary to accurately infer such attributes from the released filtered output. Minimax filters are designed to achieve the optimal utility-privacy tradeoff in terms of expected risks. The paper proves that a filter\nlearned from empirical risks is not far from an ideal filter as the number of samples increases. This property and its dependency on the task make this mechanism quite different from previous mechanisms, including syntactic anonymization and differential privacy. Algorithms for finding minimax filters are presented and evaluated on real-world data sets to show its practical usages. Experiments show that publicly available multisubject data sets are surprisingly susceptible to subject identification attacks, and that even simple linear minimax filters can reduce the privacy risks close to chance level without sacrificing target task accuracy by much.\nThis work also presents preprocessing and postprocessing approaches to combine minimax privacy and differential privacy. While differential privacy has become a popular definition of privacy, it is not without limitations, in particular against inference attacks as empirically demonstrated in the paper. This leaves room for development of new mechanisms such as the noisy minimax filter presented in the paper, which aims to achieve high on-average utility and protection against inference attacks, and a formal privacy guarantee in the worst case. The results from experiments encourage further research on potential benefits of combining different notions and mechanisms of privacy, which is left as future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : ""
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differential privacy, which provides a more formal definition of privacy, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform the optimization. In addition, the paper proposes an extension that combines minimax filter and differentially-private noisy mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or better target task accuracy and lower inference accuracy, often significantly lower than previous methods.",
    "creator" : "LaTeX with hyperref package"
  }
}