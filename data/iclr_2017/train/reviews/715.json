{"conference": "ICLR 2017 conference submission", "title": "Coarse Pruning of Convolutional Neural Networks with Random Masks", "abstract": "The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns further increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning changes the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple strategy to choose the least adversarial pruning masks. The proposed approach is generic and can select good pruning masks for feature map, kernel and intra-kernel pruning. The pruning masks are generated randomly, and the best performing one is selected using the evaluation set. The sufficient number of random pruning masks to try depends on the pruning ratio, and is around 100 when 40% complexity reduction is needed. The pruned network is retrained to compensate for the loss in accuracy. We have extensively evaluated the proposed approach with the CIFAR-10, SVHN and MNIST datasets. Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be induced in the convolution layers with less than 1% increase in the misclassification rate of the baseline network.", "reviews": [{"is_meta_review": true, "comments": "Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "Unfortunately this paper is not competitive enough for ICLR. The paper is focusing on efficiency where for the results to be credible it is of utmost importance to present experiments on large scale data and state-of-the-art models.\n I am afraid the reviewers were not able to take into account the latest drafts of the paper that were submitted in January, but those submissions came very late.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Some related works", "OTHER_KEYS": "(anonymous)", "comments": "1) Wen, Wei, et al. \"Learning structured sparsity in deep neural networks.\" Advances in Neural Information Processing Systems. 2016.\n2) Lebedev, Vadim, and Victor Lempitsky. \"Fast convnets using group-wise brain damage.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n3) Alvarez, Jose M., and Mathieu Salzmann. \"Learning the Number of Neurons in Deep Networks.\" Advances in Neural Information Processing Systems. 2016.", "IS_META_REVIEW": false, "DATE": "02 Feb 2017", "is_meta_review": false}, {"TITLE": "Reactions to author responses", "OTHER_KEYS": "ICLR 2017 conference", "comments": "Dear reviewers,\n\ncan you please take a look at the responses by the authors and add a comment indicating that you have taken them into consideration?\n\nThanks!\n", "IS_META_REVIEW": false, "DATE": "23 Jan 2017 (modified: 26 Jan 2017)", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Paper review", "is_meta_review": false, "comments": "This paper proposes a simple randomized algorithm for selecting which weights in a ConvNet to prune in order to reduce theoretical FLOPs when evaluating a deep neural network. The paper provides a nice taxonomy or pruning granularity from coarse (layer-wise) to fine (intra-kernel). The pruning strategy is empirically driven and uses a validation set to select the best model from N randomly pruned models. Makes claims in the intro about this being \"one shot\" and \"near optimal\" that cannot be supported: it is \"N-shot\" in the sense that N networks are generated and tested and there is no evidence or theory that the found solution is \"near optimal.\"\n\nPros:\n- Nice taxonomy of pruning levels\n- Comparison to the recent weight-sum pruning method\n\nCons:\n- Experimental evaluation does not touch upon recent models (ResNets) and large scale datasets (ImageNet)\n- Paper is somewhat hard to follow\n- Feature map pruning can obviously accelerate computation without specialized sparse implementations of convolution, but this is not the case for finer grained sparsity; since this paper considers fine-grained sparsity it should provide some evidence that introducing that sparsity can yield performance improvements\n\nAnother experimental downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much direct interest in the tasks of MNIST, CIFAR10, or even ImageNet. Instead, a main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect pruning to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.\n\nIn summary, the proposed method is simple, which is good, but the experimental evaluation is somewhat incomplete and does not cover recent models and larger scale datasets.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Useful topic, but not very solid techinique", "is_meta_review": false, "comments": "This paper proposes two pruning methods to reduce the computation of deep neural network. In particular, whole feature maps and the kernel connections can be removed with not much decrease of classification accuracy. \nHowever, this paper also has the following problems. \n1)\tThe method is somehow trivial, since the pruning masks are mainly chosen by simple random sampling. The novelty and scalability are both limited. \n2)\tExperiment results are mainly focused on the classification rate and the ideal complexity. As a paper on improving computation efficiency, it should include results on practical time consumption. It is very common that reducing numbers of operations may not lead to reduced computational time on a highly parallel platform (e.g., GPU). \n3)\tIt is more important to improve the computational efficiency on large-scale models (e.g., ImageNet classification network) than on small models (e.g., MNIST, CIFAR network). However, results on large-scale network is missing.\n4)\t(*Logical validity of the proposed method*) For feature map pruning, what if just to train reduced-size network is trained from scratch without transfer any knowledge from the pretrained large network? Is it possible to get the same accuracy? If so, it will simply indicate the hyper-parameter is not optimal for the original network. Experimental results are necessary to clarify the necessity of feature map pruning. \nNote that I agree with that a smaller network may be more generalizable than a larger network. \n\n----------------------------------------------\n\nComments to the authors's response:\n\nThanks for replying to my comments. \n\n1) I still believe that the proposed methods are trivial.\n2) It is nice to show GPU implementation. Compared to existing toolboxes (e.g., Torch, Caffe, TensorFlow), is the implementation of convolution efficient enough?\n3) Experiments on Cifar-100 are helpful (better than cifar-10), but it is not really large-scale, where speed-up is not so critical. ImageNet and Places datasets are examples of large-scale datasets.\n4) The author did not reply to the question wrt the validity of the proposed methods. This question is critical.   \n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "15 Dec 2016 (modified: 21 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "N random trails to get best pruning?", "is_meta_review": false, "comments": "Summary: There are many different pruning techniques to reduce memory footprint of CNN models, and those techniques have different granularities (layer, maps, kernel or intra kernel), pruning ratio and sparsity of representation. The work proposes a method to choose the best pruning masks out to many trials. Tested on CIFAR-10, SVHN and MNIST.\n\nPros:\nProposes a method to choose pruning mask out of N trials. \nAnalysis on different pruning methods.\n\nCons & Questions:\n\u201cThe proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select pruning mask in one shot and is simpler than the multi-step technique.\u201d How can one get the best pruning mask in one shot if you ran N random pruning trials? (answered)\nMissing tests of the approach with bigger CNN: like AlexNet, VGG, GoogLeNet or ResNet. (extended to VGG ok)\nSince reducing model size for embedded systems is the final goal, then showing how much memory space in MB is saved with the proposed technique compared with other approaches like Han et al. (2015) would be good.\n\nMisc:\nTypo in figure 6 a) caption: \u201cFeatuer\u201d (corrected)\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "15 Dec 2016 (modified: 23 Jan 2017)", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Experiments", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Do you have number for ImageNet?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "", "IS_META_REVIEW": false, "DATE": "29 Nov 2016", "is_meta_review": false}, {"TITLE": "What does MCR mean?", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "28 Nov 2016", "is_meta_review": false}], "SCORE": 6, "authors": "Sajid Anwar, Wonyong Sung", "KEYWORDS": "This work has proposed a new pruning strategy for CNN. Further, feature map and kernel pruning granularities are proposed for good pruning ratios and simple sparse representation.", "accepted": false, "id": ""}
