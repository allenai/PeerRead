{
  "name" : "0910.2279.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Positive Semidefinite Metric Learning with Boosting",
    "authors" : [ "Chunhua Shen", "Junae Kim", "Lei Wang", "Anton van den Hengel" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5]. Such distance metrics are essential to the effectiveness of many critical algorithms such as k-nearest neighbor (kNN), k-means clustering, and kernel regression, for example. We show in this work how a Mahalanobis metric is learned from proximity comparisons among triples of training data. Mahalanobis distance, a.k.a. Gaussian quadratic distance, is parameterized by a positive semidefinite (p.s.d.) matrix. Therefore, typically methods for learning a Mahalanobis distance result in constrained semidefinite programs. We discuss the problem setting as well as the difficulties for learning such a p.s.d. matrix. If we let ai, i = 1, 2 · · · , represent a set of points in RD, the training data consist of a set of constraints upon the relative distances between these points, S = {(ai,aj ,ak)|distij < distik}, where distij measures the distance between ai and aj . We are interested in the case that dist computes the Mahalanobis distance. The Mahalanobis distance between two vectors takes the form: ‖ai − aj‖X = √ (ai − aj)>X(ai − aj),with X < 0, a p.s.d. matrix. It is equivalent to learn a projection matrix L and X = LL>. Constraints such as those above often arise when it is known that ai and aj belong to the same class of data points while ai,ak belong to different classes. In some cases, these comparison constraints are much easier to obtain than either the class labels or distances between data elements. For example, in video content retrieval, faces extracted from successive frames at close locations can be safely assumed to belong to the same person, without requiring the individual to be identified. In web search, the results returned by a search engine are ranked according to the relevance, an ordering which allows a natural conversion into a set of constraints.\n∗NICTA is funded through the Australian Government’s Backing Australia’s Ability initiative, in part through the Australian Research Council.\nar X\niv :0\n91 0.\n22 79\nv1 [\ncs .C\nV ]\n1 3\nO ct\n2 00\nThe requirement of X being p.s.d. has led to the development of a number of methods for learning a Mahalanobis distance which rely upon constrained semidefinite programing. This approach has a number of limitations, however, which we now discuss with reference to the problem of learning a p.s.d. matrix from a set of constraints upon pairwise-distance comparisons. Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.\nXing et al [4] firstly proposed to learn a Mahalanobis metric for clustering using convex optimization. The inputs are two sets: a similarity set and a dis-similarity set. The algorithm maximizes the distance between points in the dis-similarity set under the constraint that the distance between points in the similarity set is upper-bounded. Neighborhood component analysis (NCA) [6] and large margin nearest neighbor (LMNN) [7] learn a metric by maintaining consistency in data’s neighborhood and keeping a large margin at the boundaries of different classes. It has been shown in [7] that LMNN delivers the state-of-the-art performance among most distance metric learning algorithms.\nThe work of LMNN [7] and PSDBoost [9] has directly inspired our work. Instead of using hinge loss in LMNN and PSDBoost, we use the exponential loss function in order to derive an AdaBoostlike optimization procedure. Hence, despite similar purposes, our algorithm differs essentially in the optimization. While the formulation of LMNN looks more similar to support vector machines (SVM’s) and PSDBoost to LPBoost, our algorithm, termed BOOSTMETRIC, largely draws upon AdaBoost [10].\nIn many cases, it is difficult to find a global optimum in the projection matrix L [6]. Reformulationlinearization is a typical technique in convex optimization to relax and convexify the problem [11]. In metric learning, much existing work instead learns X = LL> for seeking a global optimum, e.g., [4, 7, 12, 8]. The price is heavy computation and poor scalability: it is not trivial to preserve the semidefiniteness of X during the course of learning. Standard approaches like interior point Newton methods require the Hessian, which usually requires O(D4) resources (where D is the input dimension). It could be prohibitive for many real-world problems. Alternative projected (sub-)gradient is adopted in [7, 4, 8]. The disadvantages of this algorithm are: (1) not easy to implement; (2) many parameters involved; (3) slow convergence. PSDBoost [9] converts the particular semidefinite program in metric learning into a sequence of linear programs (LP’s). At each iteration of PSDBoost, an LP needs to be solved as in LPBoost, which scales around O(J3.5) with J the number of iterations (and therefore variables). As J increases, the scale of the LP becomes larger. Another problem is that PSDBoost needs to store all the weak learners (the rank-one matrices) during the optimization. When the input dimension D is large, the memory required is proportional to JD2, which can be prohibitively huge at a late iteration J . Our proposed algorithm solves both of these problems.\nBased on the observation from [9] that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices, we propose BOOSTMETRIC for learning a p.s.d. matrix. The weak learner of BOOSTMETRIC is a rank-one p.s.d. matrix as in PSDBoost. The proposed BOOSTMETRIC algorithm has the following desirable properties: (1) BOOSTMETRIC is efficient and scalable. Unlike most existing methods, no semidefinite programming is required. At each iteration, only the largest eigenvalue and its corresponding eigenvector are needed. (2) BOOSTMETRIC can accommodate various types of constraints. We demonstrate learning a Mahalanobis metric by proximity comparison constraints. (3) Like AdaBoost, BOOSTMETRIC does not have any parameter to tune. The user only needs to know when to stop. In contrast, both LMNN and PSDBoost have parameters to cross validate. Also like AdaBoost it is easy to implement. No sophisticated optimization techniques such as LP solvers are involved. Unlike PSDBoost, we do not need to store all the weak learners. The efficacy and efficiency of the proposed BOOSTMETRIC is demonstrated on various datasets.\nThroughout this paper, a matrix is denoted by a bold upper-case letter (X); a column vector is denoted by a bold lower-case letter (x). The ith row of X is denoted by Xi: and the ith column X:i. Tr(·) is the trace of a symmetric matrix and 〈X,Z〉 = Tr(XZ>) = ∑ ij XijZij calculates the inner product of two matrices. An element-wise inequality between two vectors like u ≤ v means ui ≤ vi for all i. We use X < 0 to indicate that matrix X is positive semidefinite. For a matrix X ∈ SD, the following statements are equivalent: (1) X < 0 (X ∈ SD+ ); (2) All eigenvalues of X are nonnegative (λi(X) ≥ 0, i = 1, · · · , D); and (3) ∀u ∈ RD, u>Xu ≥ 0."
    }, {
      "heading" : "2 Algorithms",
      "text" : "In this section, we define the mathematical problems ((P0), (P1)) we want to solve. In order to derive an efficient optimization strategy, we investigate the dual problem ((D1)) as well from a convex optimization viewpoint."
    }, {
      "heading" : "2.1 Distance Metric Learning",
      "text" : "As discussed, the Mahalanobis metric is equivalent to linearly transform the data by a projection matrix L ∈ RD×d (usually D ≥ d) before calculating the standard Euclidean distance:\ndist2ij = ‖L>ai − L>aj‖22 = (ai − aj)>LL>(ai − aj) = (ai − aj)>X(ai − aj). (1)\nAlthough one can learn L directly as many conventional approaches do, in this setting, non-convex constraints are involved, which make the problem difficult to solve. As we will show, in order to convexify these conditions, a new variable X = LL> is introduced instead. This technique has been used widely in convex optimization and machine learning such as [12]. If X = I, it reduces to the Euclidean distance. If X is diagonal, the problem corresponds to learning a metric in which the different features are given different weights, a.k.a. feature weighting.\nIn the framework of large-margin learning, we want to maximize the distance between distij and distik. That is, we wish to make dist2ij−dist 2 ik = (ai−ak)>X(ai−ak)−(ai−aj)>X(ai−aj) as large as possible under some regularization. To simplify notation, we rewrite the distance between dist2ij and dist 2 ik as dist 2 ij − dist 2 ik = 〈Ar,X〉,\nAr = (ai − ak)(ai − ak)> − (ai − aj)(ai − aj)>, (2)\nr = 1, · · · , |S |. |S | is the size of the set S ."
    }, {
      "heading" : "2.2 Learning with Exponential Loss",
      "text" : "We derive a general algorithm for p.s.d. matrix learning with exponential loss. Assume that we want to find a p.s.d. matrix X < 0 such that a bunch of constraints\n〈Ar,X〉 > 0, r = 1, 2, · · · ,\nare satisfied as well as possible. These constraints need not be all strictly satisfied. We can define the margin ρr = 〈Ar,X〉, ∀r. By employing exponential loss, we want to optimize\nmin log (∑|S| r=1 exp−ρr ) + vTr(X) s.t. ρr = 〈Ar,X〉, r = 1, · · · , |S |, X < 0. (P0)\nNote that: (1) We have worked on the logarithmic version of the sum of exponential loss. This transform does not change the original optimization problem of sum of exponential loss because the logarithmic function is strictly monotonically decreasing. (2) A regularization term Tr(X) has been applied. Without this regularization, one can always multiply an arbitrarily large factor to X to make the exponential loss approach zero in the case of all constraints being satisfied. This tracenorm regularization may also lead to low-rank solutions. (3) An auxiliary variable ρr, r = 1, . . . must be introduced for deriving a meaningful dual problem, as we show later.\nWe can decompose X into: X = ∑J\nj=1wjZj , with wj ≥ 0, rank(Zj) = 1 and Tr(Zj) = 1, ∀j. So\nρr = 〈Ar,X〉 = 〈 Ar, ∑J j=1wjZj 〉 = ∑J j=1wj〈Ar,Zj〉 = ∑J j=1wjHrj = Hr:w,∀r. (3)\nHere Hrj is a shorthand for Hrj = 〈Ar,Zj〉. Clearly Tr(X) = ∑J j=1wj Tr(Zj) = 1 >w."
    }, {
      "heading" : "2.3 The Lagrange Dual Problem",
      "text" : "We now derive the Lagrange dual of the problem we are interested in. The original problem (P0) now becomes\nmin log (∑|S| r=1 exp−ρr ) + v1>w, s.t. ρr = Hr:w, r = 1, · · · , |S |, w ≥ 0. (P1)\nIn order to derive its dual, we write its Lagrangian\nL(w,ρ,u,p) = log (∑|S| r=1 exp−ρr ) + v1>w + ∑|S| r=1ur(ρr −Hr:w)− p >w, (4)\nwith p ≥ 0. Here u and p are Lagrange multipliers. The dual problem is obtained by finding the saddle point of L; i.e., supu,p infw,ρ L.\ninf w,ρ L = inf ρ\nL1︷ ︸︸ ︷ log (∑|S| r=1 exp−ρr )\n+ u>ρ+ inf w\nL2︷ ︸︸ ︷ (v1> − ∑|S| r=1urHr: − p >)w = − ∑|S| r=1ur log ur.\nThe infimum of L1 is found by setting its first derivative to zero and we have:\ninf ρ L1 =\n{ − ∑\nrur log ur if u ≥ 0,1 >u = 1,\n−∞ otherwise.\nThe infimum is Shannon entropy. L2 is linear in w, hence L2 must be 0. It leads to∑|S| r=1urHr: ≤ v1 >. (5)\nThe Lagrange dual problem of (P1) is an entropy maximization problem, which writes\nmax u − ∑|S| r=1ur log ur, s.t. u ≥ 0,1 >u = 1, and (5). (D1)\nWeak and strong duality hold under mild conditions [11]. That means, one can usually solve one problem from the other. The KKT conditions link the optimal between these two problems. In our case, it is\nu?r = exp−ρ?r∑|S|\nk=1 exp−ρ?k ,∀r. (6)\nWhile it is possible to devise a totally-corrective column generation based optimization procedure for solving our problem as the case of LPBoost [13], we are more interested in considering one-ata-time coordinate-wise descent algorithms, as the case of AdaBoost [10], which has the advantages: (1) computationally efficient and (2) parameter free. Let us start from some basic knowledge of column generation because our coordinate descent strategy is inspired by column generation.\nIf we knew all the bases Zj(j = 1 . . . J) and hence the entire matrix H is known, then either the primal (P1) or the dual (D1) could be trivially solved (at least in theory) because both are convex optimization problems. We can solve them in polynomial time. Especially the primal problem is convex minimization with simple nonnegativeness constraints. Off-the-shelf software like LBFGSB [14] can be used for this purpose. Unfortunately, in practice, we do not access all the bases: the number of possible Z’s is infinite. In convex optimization, column generation is a technique that is designed for solving this difficulty.\nInstead of directly solving the primal problem (P1), we find the most violated constraint in the dual (D1) iteratively for the current solution and add this constraint to the optimization problem. For this purpose, we need to solve\nẐ = argmaxZ {∑|S| r=1ur 〈 Ar,Z 〉 , s.t. Z ∈ Ω1 } . (7)\nHere Ω1 is the set of trace-one rank-one matrices. We discuss how to efficiently solve (7) later. Now we move on to derive a coordinate descent optimization procedure."
    }, {
      "heading" : "2.4 Coordinate Descent Optimization",
      "text" : "We show how an AdaBoost-like optimization procedure can be derived for our metric learning problem. As in AdaBoost, we need to solve for the primal variables wj given all the weak learners up to iteration j.\nAlgorithm 1 Bisection search for wj .\nInput: An interval [wl, wu] known to contain the optimal value of wj and convergence tolerance ε > 0. repeat1 · wj = 0.5(wl + wu);2 · if l.h.s. of (8) > 0 then3 wl = wj ;4\nelse5 wu = wj .6\nuntil wu − wl < ε ;7 Output: wj .\nOptimizing for wj Since we are interested in the one-at-a-time coordinate-wise optimization, we keep w1, w2, . . . , wj−1 fixed when solving for wj . The cost function of the primal problem is (in the following derivation, we drop those terms irrelevant to the variable wj)\nCp(wj) = log [∑|S| r=1 exp(−ρ j−1 r ) · exp(−Hrjwj) ] + vwj .\nClearly, Cp is convex in wj and hence there is only one minimum that is also globally optimal. The first derivative of Cp w.r.t. wj vanishes at optimality, which results in∑|S|\nr=1(Hrj − v)u j−1 r exp(−wjHrj) = 0. (8)\nIf Hrj is discrete, such as {+1,−1} in standard AdaBoost, we can obtain a close-form solution similar to AdaBoost. Unfortunately in our case, Hrj can be any real value. We instead use bisection to search for the optimal wj . The bisection method is one of the root-finding algorithms. It repeatedly divides an interval in half and then selects the subinterval in which a root exists. Bisection is a simple and robust, although it is not the fastest algorithm for root-finding. Newton-type algorithms are also applicable here. Algorithm 1 gives the bisection procedure. We have utilized the fact that the l.h.s. of (8) must be positive at wl. Otherwise no solution can be found. When wj = 0, clearly the l.h.s. of (8) is positive.\nUpdating u The rule for updating the dual variable u can be easily obtained from (6). At iteration j, we have\nujr ∝ exp−ρjr ∝ uj−1r exp(−Hrjwj), and ∑|S| r=1u j r = 1,\nderived from (6). So once wj is calculated, we can update u as\nujr = uj−1r exp(−Hrjwj)\nz , r = 1, . . . , |S |, (9)\nwhere z is a normalization factor so that ∑|S|\nr=1u j r = 1. This is exactly the same as AdaBoost."
    }, {
      "heading" : "2.5 Base Learning Algorithm",
      "text" : "In this section, we show that the optimization problem (7) can be exactly and efficiently solved using eigenvalue-decomposition (EVD). From Z < 0 and rank(Z) = 1, we know that Z has the format: Z = ξξ>, ξ ∈ RD; and Tr(Z) = 1 means ‖ξ‖2 = 1. We have∑|S|\nr=1ur 〈 Ar,Z 〉 = 〈∑|S| r=1urAr,Z 〉 = ξ> (∑|S| r=1urAr ) ξ.\nBy denoting Â = ∑|S| r=1urAr, (10) the base learning optimization equals: maxξ ξ>Âξ, s.t. ‖ξ‖2 = 1. It is clear that the largest eigenvalue of Â, λmax(Â), and its corresponding eigenvector ξ1 gives the solution to the above problem. Note that Â is symmetric. Also see [9] for details.\nλmax(Â) is also used as one of the stopping criteria of the algorithm. Form the condition (5), λmax(Â) < v means that we are not able to find a new base matrix Ẑ that violates (5)—the algorithm converges. We summarize our main algorithmic results in Algorithm 2.\nAlgorithm 2 Positive semidefinite matrix learning with boosting. Input:\n• Training set triplets (ai,aj ,ak) ∈ S ; Compute Ar, r = 1, 2, · · · , using (2). • J : maximum number of iterations; • (optional) regularization parameter v; We may simply set v to a very small value, e.g., 10−7."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we present experiments on data visualization, classification and image retrieval tasks."
    }, {
      "heading" : "3.1 An Illustrative Example",
      "text" : "We demonstrate a data visualization problem on an artificial toy dataset (concentric circles) in Fig. 1. The dataset has four classes. The first two dimensions follow concentric circles while the left eight dimensions are all random Gaussian noise. In this experiment, 9000 triplets are generated for training. When the scale of the noise is large, PCA fails to find the first two informative dimensions. LDA fails too because clearly each class does not follow a Gaussian distraction and their centers overlap at the same point. The proposed BOOSTMETRIC algorithm find the informative features. The eigenvalues of X learned by BOOSTMETRIC are {0.542, 0.414, 0.007, 0, · · · , 0}, which indicates that BOOSTMETRIC successfully reveals the data’s underlying structure."
    }, {
      "heading" : "3.2 Classification on Benchmark Datasets",
      "text" : "We evaluate BOOSTMETRIC on 15 datasets of different sizes. Some of the datasets have very high dimensional inputs. We use PCA to decrease the dimensionality before training on these datasets (datasets 2-6). PCA pre-processing helps to eliminate noises and speed up computation. Table 1 summarizes the datasets in detail. We have used USPS and MNIST handwritten digits, ORL face\nrecognition datasets, Columbia University Image Library (COIL20)1, and UCI machine learning datasets2 (datasets 7-13), Twin Peaks and Helix. The last two are artificial datasets3.\nExperimental results are obtained by averaging over 10 runs (except USPS-1). We randomly split the datasets for each run. We have used the same mechanism to generate training triplets as described in [7]. Briefly, for each training point ai, k nearest neighbors that have same labels as yi (targets), as well as k nearest neighbors that have different labels from yi (imposers) are found. We then construct triplets from ai and its corresponding targets and imposers. For all the datasets, we have set k = 3 except that k = 1 for datasets USPS-1, ORLFace-1 and ORLFace-2 due to their large size. We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7]. LMNN is one of the state-of-the-art according to recent studies such as [15]. Also in Table 2, “Euclidean” is the baseline algorithm that uses the standard Euclidean distance. The codes for these compared algorithms are downloaded from the corresponding authors’ websites. We have released our codes for BOOSTMETRIC at [16]. Experiment setting for LMNN follows [7]. For BOOSTMETRIC, we have set v = 10−7, the maximum number of iterations J = 500. As we can see from Table 2, we can conclude: (1) BOOSTMETRIC consistently improves kNN classification using Euclidean distance on most datasets. So learning a Mahalanobis metric based upon the large margin concept does lead to improvements in kNN classification. (2) BOOSTMETRIC outperforms other algorithms in most cases (on 11 out of 15 datasets). LMNN is the second best algorithm on these 15 datasets statistically. LMNN’s results are consistent with those given in [7]. (3) Xing et al [4] and NCA can only handle a few small datasets. In general they do not perform very well. A good initialization is important for NCA because NCA’s cost function is non-convex and can only find a local optimum.\nInfluence of v Previously, we claim that our algorithm is parameter-free like AdaBoost. However, we do have a parameter v in BOOSTMETRIC. Actually, AdaBoost simply set v = 0. The coordinatewise gradient descent optimization strategy of AdaBoost leads to an `1-norm regularized maximum margin classifier [17]. It is shown that AdaBoost minimizes its loss criterion with an `1 constraint on the coefficient vector. Given the similarity of the optimization of BOOSTMETRIC with AdaBoost, we conjecture that BOOSTMETRIC has the same property. Here we empirically prove that as long as v is sufficiently small, the final performance is not affected by the value of v. We have set v from 10−8 to 10−4 and run BOOSTMETRIC on 3 UCI datasets. Table 3 reports the final 3NN classification error with different v. The results are nearly identical.\nComputational time As we discussed, one major issue in learning a Mahalanobis distance is heavy computational cost because of the semidefiniteness constraint.\n1http://www1.cs.columbia.edu/CAVE/software/softlib/coil-20.php 2http://archive.ics.uci.edu/ml/ 3http://boosting.googlecode.com/files/dataset1.tar.bz2\nWe have shown the running time of the proposed algorithm in Table 1 for the classification tasks4. Our algorithm is generally fast. It involves matrix operations and an EVD for finding its largest eigenvalue and its corresponding eigenvector. The time complexity of this EVD isO(D2) withD the input dimensions. We compare our algorithm’s running time with LMNN in Fig. 2 on the artificial dataset (concentric circles). We vary the input dimensions from 50 to 1000 and keep the number of triplets fixed to 250. LMNN does not use standard interior-point SDP solvers, which do not scale well. Instead LMNN heuristically combines sub-gradient descent in both the matrices L and X. Instead of using standard interior-point SDP solvers that do not scale well, LMNN heuristically combines sub-gradient descent in both the matrices L and X. At each iteration, X is projected back onto the p.s.d. cone using EVD. So a full EVD with time complexity O(D3) is needed. Note that LMNN is much faster than SDP solvers like CSDP [18]. As seen from Fig. 2, when the input dimensions are low, BOOSTMETRIC is comparable to LMNN. As expected, when the input dimensions become high, BOOSTMETRIC is significantly faster than LMNN. Note that our implementation is in Matlab. Improvements are expected if implemented in C/C++.\n4We have run all the experiments on a desktop with an Intel CoreTM2 Duo CPU, 4G RAM and Matlab 7.7 (64-bit version).\n0 200 400 600 800 1000 0\n100\n200\n300\n400\n500\n600\n700\n800\ninput dimensions\nC PU\nti m\ne pe\nr ru\nn (s\nec on\nds )\nBoostMetric LMNN\nFigure 2: Computation time of the proposed BOOSTMETRIC and the LMNN method versus the input data’s dimensions on an artificial dataset. BOOSTMETRIC is faster than LMNN with large input dimensions because at each iteration BOOSTMETRIC only needs to calculate the largest eigenvector and LMNN needs a full eigen-decomposition."
    }, {
      "heading" : "3.3 Visual Object Categorization and Detection",
      "text" : "The proposed BOOSTMETRIC and the LMNN are further compared on four classes of the Caltech101 object recognition database [19], including Motorbikes (798 images), Airplanes (800), Faces (435), and Background-Google (520). For each image, a number of interest regions are identified by the Harris-affine detector [20] and the visual content in each region is characterized by the SIFT descriptor [21]. The total number of local descriptors extracted from the images of the four classes are about 134, 000, 84, 000, 57, 000, and 293, 000, respectively. This experiment includes both object categorization (Motorbikes vs. Airplanes) and object detection (Faces vs. Background-Google) problems. To accumulate statistics, the images of two involved object classes are randomly split as 10 pairs of training/test subsets. Restricted to the images in a training subset (those in a test subset are only used for test), their local descriptors are clustered to form visual words by using k-means clustering. Each image is then represented by a histogram containing the number of occurrences of each visual word.\nMotorbikes vs. Airplanes This experiment discriminates the images of a motorbike from those of an airplane. In each of the 10 pairs of training/test subsets, there are 959 training images and 639 test images. Two visual codebooks of size 100 and 200 are used, respectively. With the resulting histograms, the proposed BOOSTMETRIC and the LMNN are learned on a training subset and evaluated on the corresponding test subset. Their averaged classification error rates are compared in Fig. 3 (left). For both visual codebooks, the proposed BOOSTMETRIC achieves lower error rates than the LMNN and the Euclidean distance, demonstrating its superior performance. We also apply a linear SVM classifier with its regularization parameter carefully tuned by 5-fold cross-validation. Its error rates are 3.87%± 0.69% and 3.00%± 0.72% on the two visual codebooks, respectively. In contrast, a 3NN with BOOSTMETRIC has error rates 3.63% ± 0.68% and 2.96% ± 0.59%. Hence, the performance of the proposed BOOSTMETRIC is comparable to or even slightly better than the SVM classifier. Also, Fig. 3 (right) plots the test error of the BOOSTMETRIC against the number of triplets for training. The general trend is that more triplets lead to smaller errors.\nFaces vs. Background-Google This experiment uses the two object classes as a retrieval problem. The target of retrieval is the face images. The images in the class of Background-Google are randomly collected from the Internet and they are used to represent the non-target class. BOOSTMETRIC is first learned from a training subset and retrieval is conducted on the corresponding test subset. In each of the 10 training/test subsets, there are 573 training images and 382 test images. Again, two visual codebooks of size 100 and 200 are used. Each face image in a test subset is used as a query, and its distances from other test images are calculated by BOOSTMETRIC, LMNN and the Euclidean distance. For each metric, the precision of the retrieved top 5, 10, 15 and 20 images are computed. The retrieval precision for each query are averaged on this test subset and then averaged over the whole 10 test subsets. We report the retrieval precision in Fig. 4 (with a codebook size 100). As shown, BOOSTMETRIC consistently attains the highest values, which again verifies its advantages over LMNN and the Euclidean distance. With a codebook size 200, very similar results are obtained."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We have presented a new algorithm, BOOSTMETRIC, to learn a positive semidefinite metric using boosting techniques. We have generalized AdaBoost in the sense that the weak learner of BOOSTMETRIC is a matrix, rather than a classifier. Our algorithm is simple and efficient. Experiments show its better performance over a few state-of-the-art existing metric learning methods. We are currently combining the idea of on-line learning into BOOSTMETRIC to make it handle even larger datasets."
    } ],
    "references" : [ {
      "title" : "Discriminant adaptive nearest neighbor classification",
      "author" : [ "T. Hastie", "R. Tibshirani" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "Distance learning for similarity estimation",
      "author" : [ "J. Yu", "J. Amores", "N. Sebe", "P. Radeva", "Q. Tian" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Metric learning using Iwasawa decomposition",
      "author" : [ "B. Jian", "B.C. Vemuri" ],
      "venue" : "In Proc. IEEE Int. Conf. Comp. Vis.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Distance metric learning, with application to clustering with side-information",
      "author" : [ "E. Xing", "A. Ng", "M. Jordan", "S. Russell" ],
      "venue" : "In Proc. Adv. Neural Inf. Process. Syst. MIT Press,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "Learning a Mahalanobis metric from equivalence constraints",
      "author" : [ "A. Bar-Hillel", "T. Hertz", "N. Shental", "D. Weinshall" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Neighbourhood component analysis",
      "author" : [ "J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov" ],
      "venue" : "In Proc. Adv. Neural Inf. Process. Syst. MIT Press,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K.Q. Weinberger", "J. Blitzer", "L.K. Saul" ],
      "venue" : "In Proc. Adv. Neural Inf. Process. Syst.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Metric learning by collapsing classes",
      "author" : [ "A. Globerson", "S. Roweis" ],
      "venue" : "In Proc. Adv. Neural Inf. Process. Syst.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "PSDBoost: Matrix-generation linear programming for positive semidefinite matrices learning",
      "author" : [ "C. Shen", "A. Welsh", "L. Wang" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Theoretical views of boosting and applications",
      "author" : [ "R.E. Schapire" ],
      "venue" : "In Proc. Int. Conf. Algorithmic Learn. Theory,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "Unsupervised learning of image manifolds by semidefinite programming",
      "author" : [ "K.Q. Weinberger", "L.K. Saul" ],
      "venue" : "Int. J. Comp. Vis.,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Linear programming boosting via column generation",
      "author" : [ "A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor" ],
      "venue" : "Mach. Learn.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "L-BFGS-B: Algorithm 778: L-BFGS-B, FORTRAN routines for large scale bound constrained optimization",
      "author" : [ "C. Zhu", "R.H. Byrd", "J. Nocedal" ],
      "venue" : "ACM Trans. Math. Softw.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1997
    }, {
      "title" : "A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval",
      "author" : [ "L. Yang", "R. Jin", "L. Mummert", "R. Sukthankar", "A. Goode", "B. Zheng", "S. Hoi", "M. Satyanarayanan" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell. IEEE computer Society Digital Library,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Boosting as a regularized path to a maximum margin classifier",
      "author" : [ "S. Rosset", "J. Zhu", "T. Hastie" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "CSDP, a C library for semidefinite programming",
      "author" : [ "B. Borchers" ],
      "venue" : "Optim. Methods and Softw.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1999
    }, {
      "title" : "One-shot learning of object categories",
      "author" : [ "L. Fei-Fei", "R. Fergus", "P. Perona" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Scale & affine invariant interest point detectors",
      "author" : [ "K. Mikolajczyk", "C. Schmid" ],
      "venue" : "Int. J. Comp. Vis.,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Distinctive image features from scale-invariant keypoints",
      "author" : [ "D.G. Lowe" ],
      "venue" : "Int. J. Comp. Vis.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].",
      "startOffset" : 170,
      "endOffset" : 185
    }, {
      "referenceID" : 1,
      "context" : "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].",
      "startOffset" : 170,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].",
      "startOffset" : 170,
      "endOffset" : 185
    }, {
      "referenceID" : 3,
      "context" : "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].",
      "startOffset" : 170,
      "endOffset" : 185
    }, {
      "referenceID" : 4,
      "context" : "It has been an extensively sought-after goal to learn an appropriate distance metric in image classification and retrieval problems using simple and efficient algorithms [1, 2, 3, 4, 5].",
      "startOffset" : 170,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Relevant work on this topic includes [3, 4, 5, 6, 7, 8] amongst others.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "Xing et al [4] firstly proposed to learn a Mahalanobis metric for clustering using convex optimization.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "Neighborhood component analysis (NCA) [6] and large margin nearest neighbor (LMNN) [7] learn a metric by maintaining consistency in data’s neighborhood and keeping a large margin at the boundaries of different classes.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 6,
      "context" : "Neighborhood component analysis (NCA) [6] and large margin nearest neighbor (LMNN) [7] learn a metric by maintaining consistency in data’s neighborhood and keeping a large margin at the boundaries of different classes.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "It has been shown in [7] that LMNN delivers the state-of-the-art performance among most distance metric learning algorithms.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "The work of LMNN [7] and PSDBoost [9] has directly inspired our work.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "The work of LMNN [7] and PSDBoost [9] has directly inspired our work.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "While the formulation of LMNN looks more similar to support vector machines (SVM’s) and PSDBoost to LPBoost, our algorithm, termed BOOSTMETRIC, largely draws upon AdaBoost [10].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : "In many cases, it is difficult to find a global optimum in the projection matrix L [6].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "Reformulationlinearization is a typical technique in convex optimization to relax and convexify the problem [11].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : ", [4, 7, 12, 8].",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 6,
      "context" : ", [4, 7, 12, 8].",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 11,
      "context" : ", [4, 7, 12, 8].",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : ", [4, 7, 12, 8].",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 6,
      "context" : "Alternative projected (sub-)gradient is adopted in [7, 4, 8].",
      "startOffset" : 51,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "Alternative projected (sub-)gradient is adopted in [7, 4, 8].",
      "startOffset" : 51,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Alternative projected (sub-)gradient is adopted in [7, 4, 8].",
      "startOffset" : 51,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "PSDBoost [9] converts the particular semidefinite program in metric learning into a sequence of linear programs (LP’s).",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "Based on the observation from [9] that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices, we propose BOOSTMETRIC for learning a p.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "This technique has been used widely in convex optimization and machine learning such as [12].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "Weak and strong duality hold under mild conditions [11].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "While it is possible to devise a totally-corrective column generation based optimization procedure for solving our problem as the case of LPBoost [13], we are more interested in considering one-ata-time coordinate-wise descent algorithms, as the case of AdaBoost [10], which has the advantages: (1) computationally efficient and (2) parameter free.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 9,
      "context" : "While it is possible to devise a totally-corrective column generation based optimization procedure for solving our problem as the case of LPBoost [13], we are more interested in considering one-ata-time coordinate-wise descent algorithms, as the case of AdaBoost [10], which has the advantages: (1) computationally efficient and (2) parameter free.",
      "startOffset" : 263,
      "endOffset" : 267
    }, {
      "referenceID" : 13,
      "context" : "Off-the-shelf software like LBFGSB [14] can be used for this purpose.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "Also see [9] for details.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 6,
      "context" : "We have used the same mechanism to generate training triplets as described in [7].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "We have compared our method against a few methods: Xing et al [4], RCA [5], NCA [6] and LMNN [7].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "LMNN is one of the state-of-the-art according to recent studies such as [15].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "Experiment setting for LMNN follows [7].",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "LMNN’s results are consistent with those given in [7].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "(3) Xing et al [4] and NCA can only handle a few small datasets.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "The coordinatewise gradient descent optimization strategy of AdaBoost leads to an `1-norm regularized maximum margin classifier [17].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "Results of NCA and Xing et al [4] on large datasets are not available either because the algorithm does not converge or due to the out-of-memory problem.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "dataset Euclidean Xing et al [4] RCA NCA LMNN BOOSTMETRIC 1 USPS-1 5.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : "Note that LMNN is much faster than SDP solvers like CSDP [18].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "The proposed BOOSTMETRIC and the LMNN are further compared on four classes of the Caltech101 object recognition database [19], including Motorbikes (798 images), Airplanes (800), Faces (435), and Background-Google (520).",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "For each image, a number of interest regions are identified by the Harris-affine detector [20] and the visual content in each region is characterized by the SIFT descriptor [21].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 19,
      "context" : "For each image, a number of interest regions are identified by the Harris-affine detector [20] and the visual content in each region is characterized by the SIFT descriptor [21].",
      "startOffset" : 173,
      "endOffset" : 177
    } ],
    "year" : 2009,
    "abstractText" : "The learning of appropriate distance metrics is a critical problem in image classification and retrieval. In this work, we propose a boosting-based technique, termed BOOSTMETRIC, for learning a Mahalanobis distance metric. One of the primary difficulties in learning such a metric is to ensure that the Mahalanobis matrix remains positive semidefinite. Semidefinite programming is sometimes used to enforce this constraint, but does not scale well. BOOSTMETRIC is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices. BOOSTMETRIC thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints. Experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time.",
    "creator" : "TeX"
  }
}