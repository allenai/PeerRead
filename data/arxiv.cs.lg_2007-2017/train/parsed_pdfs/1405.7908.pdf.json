{
  "name" : "1405.7908.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semantic Composition and Decomposition: From Recognition to Generation",
    "authors" : [ "Peter D. Turney" ],
    "emails" : [ "peter.turney@nrc-cnrc.gc.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n79 08\nv1 [\ncs .C\nL ]\n3 0\nM ay\n2 01"
    }, {
      "heading" : "1. Introduction",
      "text" : "Distributional semantics is based on the hypothesis that words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957). This hypothesis leads naturally to vector space models, in which words are represented by context vectors (Turney & Pantel, 2010). Much recent work has been concerned with the problem of extending vector space models beyond words, to phrases and sentences (Erk, 2013). The approach that has worked with words does not scale up to phrases and sentences, due to data sparsity and growth in model size (Turney, 2012).\nIn this paper, we focus on noun-modifier bigrams as a relatively simple testbed for exploring the problem of modeling phrases. A noun-modifier bigram (milk sugar) consists of a head noun (sugar) and a noun or adjective (milk) that modifies the meaning of the head noun. Given context vectors for the noun and the modifier, we would like to model the meaning of the noun-modifier bigram. A natural test of a model is whether it can recognize when a noun unigram (lactose) is synonymous with a bigram (milk sugar). Given a target\nc©2014 National Research Council Canada.\nbigram and a list of candidate unigrams, can the model recognize the synonymous unigram among the candidates (Turney, 2012)? Given a target unigram and a list of candidate bigrams, can the model recognize the synonymous bigram among the candidates (Kartsaklis, Sadrzadeh, & Pulman, 2012)?\nMany noun unigrams have synonymous noun-modifier bigrams. For example, the WordNet synonym set for contrabass is bass fiddle, bass viol, bull fiddle, double bass, contrabass, and string bass.1 Each of the bigrams may be viewed as a decomposition of the unigram contrabass into parts (factors, aspects, constituents). For example, the bigram bass fiddle expresses that a contrabass is a type of fiddle that covers the bass range; the head noun fiddle expresses the type aspect of contrabass and the modifier bass expresses the range aspect of contrabass. Decomposition (unigram → bigram) maps a word (contrabass) to a (miniature) definition (bass fiddle), whereas composition (bigram → unigram) maps a definition to a word (by composing the components of the definition into a whole).\nPast work has achieved promising results on recognizing noun-modifier compositions and decompositions, given relatively short lists of candidates. For example, Turney (2013) achieves 75.9% accuracy on noun-modifier composition recognition, given seven candidate unigrams for each target bigram, and Kartsaklis et al. (2012) achieve 24% accuracy on noun decomposition recognition, given seventy-two choices. However, the degree of challenge in the recognition task depends on the nature of the lists of candidates. If the distractors are very different from the correct choice, then the task is easy. The generation task avoids this criticism, because there is no predetermined list of choices. The choices are only limited by the vocabulary.\nIn this work, our main resources are the WordNet lexicon and the Waterloo corpus. The corpus consists of web pages gathered from university websites with a webcrawler, by Charles Clarke at the University of Waterloo. The corpus contains about 5 × 1010 words. Stored as plain text, without HTML markup, the corpus size is 280 gigabytes.\nFrom the terms in WordNet, we extracted approximately 114,000 n-grams that occur at least 100 times in the Waterloo corpus. The n-grams include about 73,000 unigrams, 36,000 bigrams, and 5,000 n-grams with n > 2. The majority of the unigrams are nouns and the majority of the bigrams are noun-modifier phrases.2\nFor composition (bigram → unigram), given a target bigram (bass fiddle), we use a fast unsupervised algorithm, Comp, to score the 73,000 unigrams from WordNet, and we output the top 2000 highest scoring candidates. We then use a slower supervised algorithm, Super, to rescore the top 2000 candidates from Comp. Our aim is to have a synonymous unigram (contrabass) somewhere in the top 100 rescored candidates from Super. We achieve this for 77.8% of the 351 target bigrams in our test dataset.\nFor decomposition (unigram → bigram), given a target unigram (contrabass), we use a fast unsupervised algorithm, Decomp, to score the 73,000 unigrams from WordNet. First the unigrams are scored as candidate modifiers and then they are scored as candidate heads, resulting in a list of the top 1000 potential modifiers and another list of the top 1000 potential heads. Combining these two lists by concatenation of every top modifier\n1. WordNet is available at http://wordnet.princeton.edu/. Our experiments use WordNet 3.0 for Linux. 2. We do not attempt to filter out the unigrams that are not nouns and the bigrams that are not noun-\nmodifiers, although it would be easy to do so, using the information in WordNet. The intent is that the algorithms should be robust enough to handle them automatically, using only corpus-based information.\nwith every top head, we form a list of 1,000,000 candidate bigrams. These bigrams are then scored and we output the top 2000 highest scoring candidates. We then use the slower supervised algorithm, Super, to rescore the top 2000 candidates from Decomp. 50.7% of the time, a synonymous bigram (bass fiddle) is among the top 100 rescored candidates for the 355 target unigrams in our test dataset.\nComp and Decomp are variations on the unsupervised learning algorithm of Turney (2012). Super is based on the supervised algorithm of Turney (2013). These algorithms were originally designed for the recognition task. The main contribution of this paper is to show that, working together, the algorithms can scale up from recognition to generation.\nIn Section 2, we review related work on recognizing compositions and decompositions. The datasets we use for evaluating the algorithms are presented in Section 3. The three algorithms (Comp, Decomp, and Super) share a set of features, which we describe in Section 4. The algorithms are presented in Section 5. Experiments with composition are given in Section 6 and Section 7 covers the experiments with decomposition. We summarize the results in Section 8. Section 9 discusses limitations of this work and we conclude in Section 10."
    }, {
      "heading" : "2. Related Work",
      "text" : "We first examine related work on paraphrase in general and then look at work on composition and decomposition."
    }, {
      "heading" : "2.1 Paraphrasing",
      "text" : "Mapping between nouns and noun-modifiers is a form of paraphrasing. Madnani and Dorr (2010) and Androutsopoulos and Malakasiotis (2010) present thorough surveys of datadriven approaches to paraphrasing. Paraphrases can also be generated using knowledgebased techniques, but our focus here is corpus-based methods, since they generally require much less human effort than knowledge-based techniques.\nIn general, corpus-based approaches to paraphrase have extended the distributional hypothesis from words to phrases. The extended distributional hypothesis is that phrases that occur in similar contexts tend to have similar meanings (Lin & Pantel, 2001). For example, consider the following fragments of text (Pasca & Dienes, 2005):\n• 1989, when Soviet troops withdrew from Afghanistan\n• 1989, when Soviet troops pulled out of Afghanistan\nFrom the shared context, we can infer a degree of semantic similarity between the phrases withdrew from and pulled out of. We call this the holistic (non-compositional) approach to paraphrase (Turney, 2012), because the phrases are treated as opaque wholes. The holistic approach does not model the individual words in the phrases.\nThe creative power of language comes from combining words to create new meanings. With a vocabulary of N unigrams, there are N2 possible bigrams and N3 possible trigrams. We give meaning to n-grams (n > 1) by composing the meanings of their component words. The holistic approach lacks the ability to compose meanings and cannot scale up to phrases and sentences. Holistic approaches to paraphrase do not address the creative power of language (Chomsky, 1975; Fodor & Lepore, 2002).\nThe holistic approach often achieves excellent results. It is especially suited to idiomatic phrases (kick the bucket), but, eventually, as we consider longer and more diverse phrases, we encounter problems with data sparsity and model size. In this paper, we concentrate on compositional models, but we include holistic models as baselines in our experiments."
    }, {
      "heading" : "2.2 Composition",
      "text" : "Let ab be a noun-modifier phrase, and assume that we have context vectors a and b that represent the component words a and b. One of the earliest proposals for semantic composition is to represent the bigram ab by the vector sum a+ b (Landauer & Dumais, 1997). To measure the similarity of a noun-modifier phrase, ab, and a noun, c, we calculate the cosine of the angle between a+ b and the context vector c for c.\nThis simple proposal actually works relatively well (Mitchell & Lapata, 2008, 2010), although it lacks order sensitivity. Since a+b = b+ a, animal farm and farm animal have the same representation, although one is a type of farm and the other is a type of animal. Landauer (2002) estimates that 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order, thus vector addition misses at least 20% of the meaning of a bigram.\nKintsch (2001) and Utsumi (2009) propose variations of additive composition in which ab is represented by a+b+ ∑\ni ni, the sum of a, b, and selected neighbours ni of a and b. The neighbours are context vectors for other words in the given vocabulary. Mitchell and Lapata (2010) found that a simple additive model peformed better than an additive model that included neighbours.\nMitchell and Lapata (2008, 2010) suggest element-wise multiplication as a composition operation, c = a⊙ b, where ci = ai · bi. Since a⊙ b = b⊙ a, element-wise multiplication is not sensitive to word order. However, in an experimental evaluation of seven compositional models and two noncompositional models, element-wise multiplication had the best performance (Mitchell & Lapata, 2010).\nIn the holistic approach, ab is treated as if it were an individual word. A context vector for ab is constructed from a corpus in the same manner as it would be constructed for a unigram. This approach does not scale up, but it does work well for a predetermined small set of high frequency n-grams (Turney, 2012). Guevara (2010) and Baroni and Zamparelli (2010) point out that a small set of bigrams with holistic context vectors can be used to train a regression model. For example, a regression model can be trained to map the context vectors a and b to the holistic context vector for ab (Guevara, 2010). Given a new bigram, cd, with context vectors c and d, the regression model can use c and d to predict the holistic context vector for cd.\nMany other ideas have been proposed for extending distributional semantics to phrases and sentences. Recently there have been several overviews of this topic (Mitchell & Lapata, 2010; Turney, 2012; Erk, 2013). Most of the proposed extensions to distributional semantics involve operations from linear algebra, such as tensor products (Clark & Pulman, 2007; Widdows, 2008; Clark, Coecke, & Sadrzadeh, 2008; Grefenstette & Sadrzadeh, 2011). Another proposal is to operate on similarities instead of (or in addition to) working directly with context vectors (Socher, Huang, Pennington, Ng, & Manning, 2011; Turney, 2012, 2013).\nFor example, let sims be a measure of semantic similarity for words, phrases, and sentences. Let simv be a measure of similarity (or, conversely, distance) for vectors, matrices, and tensors, such as cosine, Euclidean distance, inner product, or Frobenius norm. Given a bigram ab, a unigram c, and context vectors, a, b, and c, suppose that we are seeking a good measure of the semantic similarity between ab and c, sims(ab, c). This semantic similarity must somehow compose a and b in order to recognize the similarity between ab and c.\nOne approach to capturing sims(ab, c) is to search for a function, f , that composes vectors (or matrices or tensors), as follows:\nsims(ab, c) = simv(f(a,b), c) (1)\nElement-wise multiplication (Mitchell & Lapata, 2008, 2010) is an instance of this approach, where f(a,b) = a⊙ b, as follows:\nsims(ab, c) = cos(a⊙ b, c) (2)\nMuch work focuses on finding the right f for various types of semantic composition (Clark & Pulman, 2007; Widdows, 2008; Mitchell & Lapata, 2008, 2010; Guevara, 2010; Grefenstette & Sadrzadeh, 2011). We call this general approach context composition, due to the arguments of the function f .\nAnother approach attempts to capture sims(ab, c) by searching for a function, f , that composes similarities, as follows:\nsims(ab, c) = f(simv(a, c), simv(b, c)) (3)\nFor instance, f could be the geometric mean (see Equation 10 in Section 5.1), as follows:\nsims(ab, c) = √ cos(a, c) · cos(b, c) (4)\nTurney (2012) took the second approach to recognizing compositions, using hand-built functions for f . Turney (2013) used supervised learning to find f . We call this general approach similarity composition, based on the arguments of f .\nSocher et al. (2011) combined context composition and similarity composition for sentence paraphrase recognition. Unsupervised recursive autoencoders were used to compose context vectors and then a supervised softmax classifier was used to compose a similarity matrix.\nTurney (2012) introduced a dataset of 2180 semantic composition questions, split into 680 questions for training and 1500 for testing.3 Table 1 shows one of the questions. The stem is the target noun-modifier bigram and there are seven candidate unigrams. These questions were generated automatically from WordNet. The stem and the solution always belong to the same WordNet synonym set. The intention is to evaluate a proposed similarity measure, sims(ab, c), by its accuracy on the 1500 testing questions.\nUsing an unsupervised learning algorithm and a hand-built function for f , Turney (2012) achieved an accuracy of 58.3% on the 1500 testing questions. Vector addition reached 50.1% and element-wise multiplication attained 57.5%. Turney (2013) used supervised learning to\n3. The dataset is available at http://jair.org/papers/paper3640.html.\noptimize f on the training questions, yielding an accuracy of 75.9% on the testing questions. The holistic approach gets 81.6% correct, but suffers from scaling problems.\nDinu, Pham, and Baroni (2013) created a dataset of 620 adjective-noun phrases, each paired with a noun unigram from the same WordNet synonym set. Table 2 shows the first four phrases in their dataset. Given a vocabulary of 21,000 noun unigrams, the task of each distributional model was to rank the nouns by their semantic similarity to each adjective-noun phrase.\nDinu et al. (2013) evaluated seven different models on their adjective-noun dataset. The performance of the models was measured by the medians of the ranks of the solution nouns in the ranked lists of 21,000 candidates. Our experiments (Sections 6 and 7) use the same general approach to evaluation of models.\nIn a noun-modifier phrase, the modifier may be either a noun or an adjective; therefore adjective-noun phrases are a subset of noun-modifier phrases. Dinu et al. (2013) hypothesize that adjectives are functions that map nouns onto modified nouns (Baroni & Zamparelli, 2010), thus they believe that noun-noun phrases and adjective-noun phrases should have different kinds of models. The models we present here (Section 5) treat all noun-modifiers the same way, hence our datasets contain both noun-noun phrases and adjective-noun phrases. For comparison, we will also evaluate our models on Dinu et al.’s (2013) adjective-noun dataset (Section 6.4)."
    }, {
      "heading" : "2.3 Decomposition",
      "text" : "Collins-Thompson and Callan (2007) describe the definition production task, in which human subjects are asked to generate a short definition of a given target word, to evaluate their understanding of the word. They propose an algorithm for automatically scoring the\nanswers of the human subjects. Their algorithm measures the semantic similarity between a gold standard reference definition, given by a human expert, and the human subject’s definition. The semantic similarity is calculated using a Markov chain.\nKartsaklis et al. (2012) created a dataset with 72 target noun unigrams and 40 target verb unigrams.4 Each target unigram has three gold standard definitions, where the definitions contain 2.4 words on average (the majority are bigrams). Table 3 shows the first four target terms and their corresponding definitions.\nKartsaklis et al. (2012) treat each target term (unigram) as a class label and evaluate each model by its accuracy on classifying the definitions. For the noun unigrams, there are 216 (72 × 3) definitions, and each definition must be assigned to one of 72 classes (target nouns). For the verb unigrams, there are 120 (40 × 3) definitions and 40 classes. Their model, based on matrix multiplication combined with element-wise multiplication, achieves an accuracy of 24% on the nouns and 28% on the verbs. Element-wise multiplication on its own attains an accuracy of 22% on the nouns and 30% on the verbs.\nRecently Dinu and Baroni (2014) partially addressed the task of decomposing a noun unigram into an adjective-noun bigram. For a given noun, they generated a ranked list of candidate adjectives and a ranked list of candidate nouns, but they did not attempt to rank the combined adjective-noun bigrams."
    }, {
      "heading" : "3. Datasets",
      "text" : "In this section, we introduce the four datasets that will be used in the experiments. The two standard datasets are based on WordNet synonym sets. The two holistic datasets are based on the holistic approach to bigrams. All four datasets were created with the WordNet::QueryData Perl package.5\nThe two standard datasets are derived from the 2180 seven-choice noun-modifier questions used in previous work (Turney, 2012) to test composition recognition (see Table 1 in Section 2.2). The original dataset included both compositional bigrams (magnetic force) and idiomatic (non-compositional) bigrams (stool pigeon). Given the difficulty of the generation problem, relative to the recognition problem, we decided to make the dataset easier by avoiding idiomatic bigrams. We used WordNet glosses as heuristic clues for finding compositional bigrams. A bigram was considered to be highly compositional if there was at least one gloss that contained the head noun and at least one gloss (not necessarily the same gloss; possibly a gloss for another sense of the bigram) that contained the modifier.\n4. The dataset is available at http://www.cs.ox.ac.uk/activities/compdistmeaning/index.html. 5. The four datasets are available on request from the author.\nTo allow for derivationally related word forms, a gloss was deemed to contain a word if the first five characters of the word matched the first five characters of any word in the gloss.\nFor example, stool pigeon has the glosses someone acting as an informer or decoy for the police and a dummy pigeon used to decoy others. The head noun pigeon occurs in the second gloss, but the modifier stool appears in neither gloss. Therefore stool pigeon fails the test; it is not considered to be highly compositional. On the other hand, magnetic force has the gloss attraction for iron; associated with electric currents as well as magnets; characterized by fields of force. The head noun force occurs in the gloss and the first five characters of the modifier magnetic match the first five characters of the word magnets in the gloss. Therefore magnetic force is deemed to be highly compositional.\nThe standard composition dataset (bigram → unigram) was generated by extracting the stem bigram from each of the 2180 seven-choice noun-modifier questions and checking the glosses of each bigram to see whether the bigram is highly compositional. Bigrams that passed the test were selected as target bigrams for the standard composition dataset. For each target bigram, all of the unigrams that are in the same WordNet synonym set as the bigram were selected as solutions for that bigram. The standard composition dataset was divided into training and testing subsets, based on whether the target bigram came from the training or testing subset of the original 2180 seven-choice noun-modifier questions.\nThe standard decomposition dataset (unigram → bigram) was generated by extracting the solution unigram from each of the 2180 seven-choice noun-modifier questions. For each unigram, all of the bigrams that are in the same WordNet synonym set as the unigram were selected as solutions for that unigram. The solution bigrams were then checked to see whether at least one of the solution bigrams was highly compositional. If the unigram had one highly compositional solution, then the unigram was selected as a target unigram for the standard decomposition dataset, and all of the bigrams in the same WordNet synonym set as the unigram were selected as solutions for that unigram. The standard decomposition dataset was divided into training and testing subsets, based on whether the target unigram came from the training or testing subset of the original 2180 seven-choice noun-modifier questions.\nThe two standard datasets exploit the human expertise and effort that went into the construction of WordNet synonym sets. The idea of the holistic datasets is to reduce the need for human expertise, by using holistic vectors instead of synonym sets. This idea was inspired by Guevara (2010) and Baroni and Zamparelli (2010), who used holistic vectors to train regression models.\nLet’s use a b to represent a bigram that we will represent with a holistic vector. That is, although a b is a bigram, we will pretend it is a unigram and we will construct a context vector for it in the same manner as we would for any unigram. We call a b a pseudounigram. The holistic composition dataset consists of true bigram targets and pseudounigram solutions. The holistic decomposition dataset consists of pseudo-unigram targets and true bigram solutions.\nThe idea is that mapping red salmon to sockeye (standard composition) is analogous to mapping red salmon to red salmon (holistic composition), and mapping sockeye to red salmon (standard decomposition) is analogous to mapping red salmon to red salmon (holistic decomposition). Although the mapping between red salmon and red salmon is trivial\nwhen we have access to their written forms, it is not trivial when we have only the three corresponding context vectors (for red, salmon, and red salmon).\nThe two holistic datasets are based on bigrams selected from the 36,000 bigrams in WordNet. We calculated the frequency of each bigram in our corpus and used the most frequent bigrams to construct targets and solutions. The sizes of the holistic datasets were matched to the sizes of the corresponding standard datasets.\nFor each of the four datasets, Table 4 shows the first four targets and their solutions. Table 5 gives the sizes of datasets. Each holistic target has only one solution but the standard targets may have more than one solution. When there are several possible solutions, an algorithm is considered successful if a guess matches any of the possible soutions."
    }, {
      "heading" : "4. Features",
      "text" : "Comp, Decomp, and Super use five types of features for ranking candidates. The features are functions that take unigrams or pseudo-unigrams as arguments and return real values. Tables 6 and 7 list the five functions and their arguments. The features DS and FS were introduced in Turney (2012). These two features were supplemented with LUF and PPMI in Turney (2013). The feature LBF is a new addition to the group."
    }, {
      "heading" : "4.1 LUF: Log Unigram Frequency",
      "text" : "Let UF(a) be the frequency of the unigram a in the Waterloo corpus. We define LUF(a) as log(UF(a) + 1). We add one to the frequency because log(0) is undefined. If a is not in the Waterloo corpus, UF(a) is zero, and thus LUF(a) is also zero. For a pseudo-unigram, a b, UF(a b) is the frequency of the bigram ab in the Waterloo corpus.\nWe precompute UF(a) for the 73,000 unigrams and 36,000 bigrams in WordNet, hence LUF(a) covers 73,000 unigrams and 36,000 pseudo-unigrams. In our experiments, we use a hash table to quickly obtain LUF(a).6"
    }, {
      "heading" : "4.2 LBF: Log Bigram Frequency",
      "text" : "For LBF(a, b), we use the Google Web 1T 5-gram (Web1T5) dataset (Brants & Franz, 2006).7 Let BF(a, b) be the frequency of the bigram ab in the Web1T5 dataset. We define LBF(a, b) as log(BF(a, b)+1). For a pseudo-unigram, a b, BF(a b, c) is the frequency of the trigram abc in the Web1T5 dataset and BF(c, a b) is the frequency of the trigram cab. In our experiments, we never need to compute BF when both arguments are pseudo-unigrams, BF(a b, c d), so we do not need to work with 4-grams.\n6. The frequencies of all WordNet n-grams in the Waterloo corpus are available on request from the author. 7. The Web1T5 dataset is available at http://catalog.ldc.upenn.edu/LDC2006T13.\nWe precompute BF(a, b) for all bigrams and trigrams such that the component unigrams are in WordNet and the bigrams and trigrams are in the Web1T5 dataset. This is too much data to fit in RAM, so we store the data in a Berkeley DB database, in order to rapidly find LBF(a, b).8"
    }, {
      "heading" : "4.3 PPMI: Positive Pointwise Mutual Information",
      "text" : "Pointwise mutual information (PMI) is a measure of the strength of the association between two words in a corpus (Church & Hanks, 1989). Let p(a) and p(b) be the probabilities of observing the words a and b in a given corpus. Let p(a, b) be the probability of observing a and b together. The PMI of a and b is defined as follows:\npmi(a, b) = log\n(\np(a, b)\np(a)p(b)\n)\n(5)\nPMI ranges from negative infinity to positive infinity. Positive values indicate association and negative values indicate a lack of association. Research suggests that negative PMI values are not useful in vector space models of semantics (Bullinaria & Levy, 2007), so it is common to use positive pointwise mutual information (PPMI), defined as follows:\nppmi(a, b) =\n{\npmi(a, b) if pmi(a, b) > 0 0 otherwise\n(6)\nPPMI ranges from zero to positive infinity.\nIn experiments with our training datasets, we found that accuracy improved when PPMI was forced to range between zero and one, by applying a logistic function. The logistic function is a sigmoid function (an S-shaped range squashing function) with the following equation:\nf(x) = 1\n1 + e−x (7)\nWhen x is zero, f(x) is 0.5. Positive infinity maps to one and negative infinity maps to zero. We applied a linear rescaling to the logistic function, as follows:\ng(x) = 2\n1 + e−x − 1 (8)\nThe function g(x) maps zero to zero and positive infinity to one. When we apply g to PPMI and use the natural logarithm (base e), the e−x in Equation 8 and the log in Equation 5 cancel out, yielding the following result:\ng(ppmi(a, b)) =\n\n \n \n2\n1 + p(a)p(b) p(a,b)\n− 1 if p(a, b) > p(a)p(b)\n0 otherwise\n(9)\nWe use this normalized form of PPMI, g(ppmi(a, b)), in our experiments. (Another approach to normalizing PPMI is given in Bouma (2009).)\n8. We use the BerkeleyDB Perl package.\nThe positive pointwise mutual information features are stored in a sparse matrix. The general procedure for creating a PPMI matrix is described in detail in Turney and Pantel (2010). In the following experiments, we use the PPMI matrix from Turney, Neuman, Assaf, and Cohen (2011).9 It is a word-context matrix in which the rows correspond to n-grams in WordNet and the columns correspond to unigrams from WordNet, marked left or right. There are approximately 114,000 rows in the matrix and 140,000 columns. The matrix has a density of about 1.2%.\nLet a be an n-gram in WordNet, b be a unigram in WordNet, and h be either left or right. Suppose that a corresponds to the i-th row in the matrix and b, marked with the handedness h, corresponds to the j-th column in the matrix. We define PPMI(a, b, h) as the value in the i-th row and j-th column of the matrix. This value is the normalized positive pointwise mutual information of observing b on the h side of a in the Waterloo corpus, where b is either immediately adjacent to a or separated from a by one or more stop words.10 Any word that is not in WordNet is treated as a stop word. If a does not correspond to a row in the matrix or b (marked h) does not correspond to column, then PPMI(a, b, h) is assigned the value zero.\nTurney et al. (2011) estimated PPMI(a, b, h) by sampling the Waterloo corpus for phrases containing a and then looking for b on the h side of a in the sampled phrases. Due to this sampling process, PPMI(a, b, left) does not necessarily equal PPMI(b, a, right). Suppose a is a rare word and b is common. Given PPMI(a, b, left), when we sample phrases containing a, we are relatively likely to find b in some of these phrases. Given PPMI(b, a, right), when we sample phrases containing b, we are less likely to find any phrases containing a. Although, in theory, PPMI(a, b, left) should equal PPMI(b, a, right), they are likely to be unequal given a limited sample.\nGiven a pseudo-unigram, a b, PPMI(a b, c, h) can be found by looking for the row that corresponds to the bigram ab and the column that corresponds to the unigram c, marked h. However, PPMI(c, a b, h) is a problem, because the columns of the PPMI matrix are unigrams, so the bigram ab will never correspond to a column in the matrix. In this case, we approximate PPMI(c, a b, h) by PPMI(a b, c, h′), where h′ is the opposite handedness of h. We never need to compute PPMI when both arguments are pseudo-unigrams, PPMI(a b, c d, h)."
    }, {
      "heading" : "4.4 DS: Domain Similarity",
      "text" : "Domain similarity (DS) was designed to capture the topic (the field, area, or domain) of a word. The following experiments use the domain matrix from Turney (2012).11 To make the domain matrix, Turney (2012) first constructed a frequency matrix, in which the rows correspond to n-grams in WordNet and the columns correspond to nouns that were observed near the row n-grams in the Waterloo corpus. The hypothesis was that the nouns near a term characterize the topics associated with the term. Given an n-gram, a, the Waterloo corpus was sampled for phrases containing a and the phrases were processed with a partof-speech tagger, to identify nouns. If the noun b was the closest noun to the left or right\n9. The PPMI matrix is available from the author on request. 10. Turney et al. (2011) did not normalize PPMI in their experiments. They used Equation 6, whereas we\nuse Equation 9 here. 11. The domain matrix is available from the author on request.\nof a, where a corresponds to the i-th row of the frequency matrix and b corresponds to the j-th column of the matrix, then the frequency count for the i-th row and j-th column was incremented.\nThe word-context frequency matrix for domain space has about 114,000 rows (WordNet terms) and 50,000 columns (noun contexts, topics), with a density of 2.6%. The frequency matrix was converted to a PPMI matrix and then processed with singular value decomposition (SVD).12 The SVD yields three matrices, U, Σ, and V. A term in domain space is represented by a row vector in UkΣ p\nk. The parameter k specifies the number of singular values in the truncated singular value decomposition; that is, k is the number of latent factors in the low-dimensional representation of the term (Landauer & Dumais, 1997). We generate Uk and Σk by deleting the columns in U and Σ corresponding to the smallest singular values. The parameter p raises the singular values in Σk to the power p (Caron, 2001). When p is zero, all of the k factors have equal weight. When p is one, each factor is weighted by its corresponding singular value in Σ. Decreasing p has the effect of making the similarity measure more discriminating (Turney, 2012).\nThe similarity of two words in domain space, DS(a, b, k, p), is computed by extracting the row vectors in UkΣ p k that correspond to the n-grams a and b, and then calculating their cosine. Optimal performance requires tuning the parameters k and p for the task (Bullinaria & Levy, 2012; Turney, 2012). For Comp and Decomp, we use the parameter settings given by Turney (2012). For Super, we generate features with a wide range of parameter settings and let the supervised learning algorithm decide how to use the features.\nIf either a or b does not correspond to a row in UkΣ p\nk, then DS(a, b, k, p) is set to zero. Since the rows in the domain matrix correspond to n-grams in WordNet, pseudo-unigrams do not present a problem. Given a pseudo-unigram, a b, we compute DS(a b, c, k, p) by extracting the row vectors in UkΣ p k that correspond to the bigram ab and the unigram c and then calculating the cosine of the vectors."
    }, {
      "heading" : "4.5 FS: Function Similarity",
      "text" : "Function similarity (FS) was designed to capture the function of a word (its usage, role, or relationship). The following experiments use the function matrix from Turney (2012).13 It is similar to the domain matrix, except the context is based on verbal patterns, instead of nearby nouns. The hypothesis was that the functional role of a word is characterized by the patterns that relate the word to nearby verbs.\nThe word-context frequency matrix for function space has about 114,000 rows (WordNet terms) and 50,000 columns (verb pattern contexts, functional roles), with a density of 1.2%. The frequency matrix was converted to a PPMI matrix and smoothed with SVD.14 The similarity of two words in function space, FS(a, b, k, p), is computed by extracting the row vectors in UkΣ p k that correspond to the n-grams a and b, and then calculating their cosine.\n12. The PPMI matrix was based on Equation 6, not Equation 9. 13. The function matrix is available from the author on request. 14. The PPMI matrix was based on Equation 6, not Equation 9."
    }, {
      "heading" : "5. Algorithms",
      "text" : "This section presents the algorithms, Comp, Decomp, and Super. Comp and Decomp use fast unsupervised algorithms to generate initial lists of candidates (Turney, 2012). Super uses a slower supervised algorithm to refine the initial lists from Comp or Decomp (Turney, 2013). Super handles both composition and decomposition using the same algorithm, but it requires different training datasets and it builds different models for the two different tasks.\nFor composition, Comp takes a bigram ab as input and generates a ranked list of maxu unigrams as output. Then Super takes the bigram ab and the list of maxu unigrams as input and generates a new ranking of the list as output.\nFor decomposition, Decomp takes a unigram a as input and generates a ranked list of maxb bigrams as output. Then Super takes the unigram a and the list of maxb bigrams as input and generates a new ranking of the list as output."
    }, {
      "heading" : "5.1 Comp",
      "text" : "• Input: a bigram, ab.\n• Output: a ranked list of maxu unigrams.\nLet ab be a bigram and let c be a unigram, such that a, b, and c are any of the 73,000 WordNet unigrams. Comp uses a scoring function, scoreu(ab, c), to estimate the quality of the unigram c considered as a semantic composition of the bigram ab. The score is based on the geometric mean of DS (domain similarity) and FS (function similarity). The geometric mean is only suitable for positive numbers, but DS and FS can be negative. We suppress these negative values for Comp and Decomp, but we allow them for Super, since the supervised learning algorithm should be able to make use of them. Let nn(x) (nn for nonnegative) be x when x > 0 and let it be zero otherwise. We define scoreu(ab, c) as follows:\nscoreu(ab, c) = √ nn(DS(a, c, kd, pd)) · nn(FS(b, c, kf , pf)) (10)\nThis heuristic score is based on the idea that, when ab is a noun-modifier bigram, the noun c should be in the same general domain as the modifier, a, and have the same general function as the head noun, b. For example, suppose ab is red salmon and c is sockeye. It seems reasonable to say that sockeye is in the domain of things that are red and sockeye has the same functional role as salmon.\nFor a given bigram, ab, Comp calculates scoreu(ab, c) for every unigram, c, in the 73,000 WordNet unigrams. The candidate unigrams are then sorted in order of decreasing score and the top maxu highest scoring candidates are the output of Comp.\nWe use the Perl Data Langague (PDL) package to rapidly calculate scoreu(ab, c). The scores can be calculated quickly by processing all of the candidate unigrams together in a matrix, instead of working with each individual candidate as a single vector.\nTable 8 shows the parameter settings we use for Comp in the following experiments. The values of the first four parameters were copied from Turney (2012). The last parameter, maxu, was set to 2000, based on a small number of trials using the standard composition training dataset. Larger values for maxu tend to yield improved accuracy, after applying Super, but this comes at the cost of increased execution time for Super."
    }, {
      "heading" : "5.2 Decomp",
      "text" : "• Input: a unigram, a.\n• Output: a ranked list of maxb bigrams.\nLet a be a unigram and let bc be a bigram, such that a, b, and c are any of the 73,000 Wordnet unigrams. Decomp uses a scoring function, scorem(a, b), to estimate the quality of the unigram b considered as a modifier in the bigram bc, where bc is a semantic decomposition of a. We define scorem(a, b) as follows:\nscorem(a, b) = nn(DS(a, b, kd, pd)) · (PPMI(a, b, left) + PPMI(a, b, right)) (11)\nDecomp also uses a scoring function, scoreh(a, c), to estimate the quality of the unigram c considered as a head noun in the bigram bc. We define scoreh(a, c) as follows:\nscoreh(a, c) = nn(FS(a, c, kf , pf)) · (PPMI(a, c, left) + PPMI(a, c, right )) (12)\nLike Comp, Decomp uses DS (domain similarity) for modifiers and FS (function similarity) for heads. The PPMI terms in Equations 11 and 12 are designed to give more weight to candidate modifiers and heads that are observed in the corpus near the target unigram, a. For example, suppose a is sockeye, b is red, and c is salmon. In a large corpus, if we sample phrases that contain sockeye, we would expect a few of these phrases to contain either red (the sockeye was a deep red) or salmon (their favourite salmon was sockeye). The scoring functions, scorem(a, b) and scoreh(a, c), have no preference about whether b and c appear to the right or left of a. If they appear on both sides of a, that is better than appearing on only one side.\nDecomp scores every unigram, b, in the 73,000 Wordnet unigrams with scorem(a, b). The unigrams are then sorted in order of decreasing score and the top maxm highest scoring unigrams are considered as candidate modifiers. Every unigram, c, is scored with scoreh(a, c) and the top maxh highest scoring unigrams are taken as candidate heads. Decomp then generates maxm · maxh bigrams, bc, by combining each candidate modifier, b, with each candidate head, c. Finally, these maxm ·maxh bigrams are scored with scoreb(a, bc), defined as follows:\nscoreb(a, bc) = scorem(a, b) · scoreh(a, c) · LBF(b, c) · PPMI(b, c, right ) (13)\nThe scored bigrams are sorted in order of decreasing score and the top maxb highest scoring candidates are the output of Decomp. The LBF term in Equation 13 is designed to give\nmore weight to candidate bigrams that occur more frequently. The PPMI term increases the score when c is often found to the right of b in the corpus.\nTable 9 shows the parameter settings we use for Decomp in the following experiments. The first four parameters are set the same as for Comp in Table 8. The remaining three, maxm, maxh, and maxb, were set based on a small number of trials using the standard decomposition training dataset. Larger values tend to improve accuracy at the cost of increased execution time for Super.\nThe equations for Decomp are more complex than the equation for Comp, because Decomp is exploring a larger set of candidates. Given a target bigram, ab, Comp considers 73,000 possible unigram compositions. Given a target unigram, a, Decomp considers 5,300,000,000 (73,000 squared) candidate bigram decompositions. The two algorithms share the core functions, DS and FS, but Decomp requires the additional functions, PPMI and LBF, in order to handle the larger set.\nWith maxm and maxh set to 1000, maxm · maxh is 1,000,000. Thus scoreb(a, bc) is applied to one million candidate bigrams for each input target unigram, a. This may seem to be a large set of candidates, but it is considerably smaller than the 5,300,000,000 possible bigrams, and it is also smaller than the 314,843,401 bigrams in the Google Web 1T 5-gram (Web1T5) dataset (Brants & Franz, 2006). This is why Decomp splits the task into two steps: First select subsets of the candidate modifiers and heads independently, then evaluate bigrams formed from these relatively small subsets."
    }, {
      "heading" : "5.3 Super",
      "text" : "• Input: a list of triples of the form 〈a, b, c〉.\n• Output: a ranking of the input list.\nSuper uses a supervised learning algorithm to refine the lists that it gets from Comp or Decomp. Comp takes a target bigram, ab, as input and generates maxu unigrams, c1, c2, . . . as output. Super views this as maxu triples of the form 〈a, b, ci〉, where i ranges from one to maxu. Decomp takes a target unigram, a, as input and generates maxb bigrams, b1c1, b2c2, . . . as output. Super views this as maxb triples of the form 〈a, bi, ci〉, where i ranges from one to maxb. In both cases, the task of Super is to learn to rank triples. Super uses the same feature vectors to represent these triples, regardless of whether the triples come from Comp or Decomp.\nThe first step for Super is to represent each triple with a feature vector. Let 〈a, b, c〉 be a triple, where a, b, and c are unigrams. Super generates three LUF features, one for each unigram, a, b, and c. There are six LBF features, one for each possible pair of unigrams, 〈a, b〉, 〈a, c〉, 〈b, a〉, 〈b, c〉, 〈c, a〉, and 〈c, b〉. For PPMI, there are six possible pairs of unigrams and two possible values of h (left and right), hence there are twelve features. For DS and FS, the order of the pairs does not matter, because cosine is symmetric (cos(a,b) = cos(b,a)), so there are only three pairs to consider, 〈a, b〉, 〈a, c〉, and 〈b, c〉. However, we explore ten values of k and eleven values of p, so there are 3 × 10 × 11 = 330 features. This gives a total of 681 features, as summarized in Table 10. These are the same features as in Turney (2013), except for the six new LBF features.\nSince Super is supervised, it requires a training dataset. For example, the standard composition training dataset contains 154 target bigrams (see Table 5). Given that maxu is 2000, after running Comp on this training dataset, we have 154× 2000 = 308,000 triples, and thus 308,000 training feature vectors.\nWe do not use all of these feature vectors to train the model. First, we drop any target bigrams for which there are no solutions. For example, we see in Table 4 that the target bigram foot lever has the solutions pedal and treadle. If neither of these unigrams appear among the 2000 candidates in the output of Comp, then we remove all of the 2000 triples containing foot lever from the training dataset. For the standard composition training dataset, this step leaves us with 135 target bigrams, for a total of 135 × 2000 = 270,000 training triples.\nSecond, we adjust the class ratio. Table 5 shows that there are 1.5 possible solutions per target bigram in the standard composition training dataset, but not all of these solutions appear in the 2000 candidates per target that are generated by Comp. For the 135 target bigrams that survive the first filtering step above, there are 1.33 solutions per target bigram that actually occur in the 2000 candidates per target from Comp. This gives us 135 × 1.33 = 180 triples that we label as class 1 (correct solutions). The remaining 270,000 − 180 = 269,820 triples are labeled as class 0 (incorrect candidates). This class imbalance makes learning difficult (Japkowicz & Stephen, 2002). Therefore, for each of the 135 target bigrams, we select all triples from class 1 and we randomly sample triples from class 0, until we have ratio01 triples in class 0 for every triple in class 1. In the following experiments, ratio01 is 30, so we have 180 triples in class 1 and 180 × 30 = 5,400 triples in class 0, for a total of 5,400 + 180 = 5,580 triples.\nThe above numbers use the standard composition training dataset as an example, but the same basic method is applied to all four datasets: First, drop targets that lack solutions.\nSecond, adjust the class ratio. These steps only apply when training. No modifications are made to the testing datasets.\nSuper uses the sequential minimal optimization (SMO) support vector machine (SVM) as implemented in Weka (Platt, 1998; Witten, Frank, & Hall, 2011).15 The kernel is a normalized third-order polynomial. Weka provides probability estimates for the classes by fitting the outputs of the SVM with logistic regression models. After training, we apply Super to the testing dataset. For each testing target, we rank the 2000 candidates in descending order of the probability that they belong in class 1, as estimated by the SVM.\nTable 11 shows the parameter settings that we use for Super in the following experiments. The Super parameter values are copied from Turney (2013), except for ratio01, which was set based on a small number of trials using the standard composition training dataset. The same parameter settings are used for all four datasets.\nSupervised learning is able to construct more accurate scoring functions than Equations 10 to 13, but Super requires training data and it executes more slowly. Super uses 681 features, whereas Comp uses only two features and Decomp uses only eight features. (The features in Comp and Decomp are included among the 681 features of Super. See Equations 10 to 13.) Most of the computation time of Super is spent calculating the extra features. By combining Comp and Decomp with Super, we are able to get the speed advantage of the unsupervised heuristics and the accuracy advantage of supervised learning."
    }, {
      "heading" : "6. Experiments with Composition",
      "text" : "This section describes the experiments with Comp and Super on the standard and holistic composition datasets."
    }, {
      "heading" : "6.1 Evaluation with the Standard Composition Dataset",
      "text" : "Table 12 uses the target bigram red salmon to illustrate the process of Comp and Super. (This is the first target in the standard composition testing dataset; see Table 4.) The solution unigram, sockeye, is marked with an asterisk. Among the top 2000 candidates in the output of Comp, sockeye is ranked 445th, with a score of 0.167, according to Equation 10. When the 2000 candidates from Comp are rescored by Super, sockeye rises up to rank third, with a score of 0.5, according to the SVM model.\nTable 13 summarizes the performance of Comp and Super on the standard composition testing dataset, containing 351 target bigrams. The row labeled mean rank in 2000 candidates is the mean rank of first correct answer, if any, in the 2000 guesses. The row labeled\n15. Weka is available at http://www.cs.waikato.ac.nz/ml/weka/.\nmedian rank in 2000 candidates is the median rank of first correct answer, if any, in the 2000 guesses. The rows labeled percent in top N give the percentage of the targets for which a correct solution is one of the top N guesses. If there are two or more solutions in the top N guesses, we use the rank of the first solution. The row percent in 2000 candidates is the percentage of the target bigrams for which there is a correct answer anywhere in the maxu (2000) candidates generated by Comp. The final row, number of candidates considered, shows the number of possibilities considered by Comp and Super.\nWhen calculating the mean and median rank, we only include the targets for which a correct answer is among the 2000 guesses, since the rank is not defined for Super when the correct answer is not among the 2000 guesses. Thus the mean and median only cover the 84.0% of the 351 targets. This makes the mean and median slightly misleading, because an algorithm could improve its mean and median ranks by refusing to guess unless it is very confident. The other four evaluation metrics cover 100% of the 351 targets. We include all six metrics in order to provide a broad perspective on the performance of the algorithms. Our preferred evaluation metric is percent in top 100, the percentage of targets for which a correct answer is one of top 100 guesses. By this metric, Comp and Super, working together, achieve a score of 77.8%."
    }, {
      "heading" : "6.2 Comparison with Baselines",
      "text" : "We compare Comp and Super with three baselines, vector addition, element-wise multiplication, and the holistic approach. Let ab be a bigram and let c be a unigram. Let a, b, and c be the corresponding context vectors. With vector addition, we score the triple 〈a, b, c〉 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a⊙ b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c).\nFor vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1000 and p set to −0.1.\nFor element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets × 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune.\nFor the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012). The mono matrix was formed by merging the domain and function matrices. See Turney (2012) for details. We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1300 and p set to −0.5.\nSince Turney (2012) showed that the geometric mean of domain similarity and function similarity performed better than vector addition and element-wise multiplication on the noun-modifier composition recognition task, we decided to apply addition and multiplication to the output of Comp, instead of applying them to the full set of 73,000 unigrams. The intention was to give vector addition and element-wise multiplication the benefit of preprocessing by Comp. On the other hand, Turney (2012) found that the holistic approach was more accurate than all other approaches; therefore we applied the holistic approach to the whole set of 73,000 unigrams, with no preproessing by Comp.\nTable 14 shows the results. When we look at percent in top 1 and percent in top 10 in Table 14, addition and multiplication seem to impair the ranking done by Comp. For percent in top 100, there seems to be some benefit to addition and multiplication. However, Super (77.8%) is significantly better than addition (66.1%) and multiplication (60.4%), according to Fisher’s Exact Test at the 95% confidence level. The difference between Super (77.8%) and the holistic approach (78.9%) is not signficant. For percent in top 10, the difference between Super (50.4%) and the holistic approach (54.4%) is also not signficant. For percent in top 1, the holistic approach (25.1%) is significantly better than Super (17.7%).\nNote that the holistic approach can only handle target bigrams included in the 36,000 bigrams in WordNet, whereas Super can handle any target bigram that contains unigrams from the 73,000 unigrams in WordNet (that is, 5,300,000,000 bigrams). Because we chose to\nbase our vocabulary on WordNet, the target bigrams in the standard composition dataset are restricted to bigrams that appear in WordNet. Since our datasets (Section 3) and our features (Section 4) are based on the vocabulary of WordNet, the holistic approach seems more useful than it would appear if the testing dataset were not derived from WordNet.\nThe holistic approach cannot not scale up to 5,300,000,000 bigrams. Comp and Super scale up and attain accuracy near the level of the holistic approach, although the testing dataset favours the holistic approach."
    }, {
      "heading" : "6.3 Evaluation with the Holistic Composition Dataset",
      "text" : "Given the standard composition dataset, Super learns from 5,580 triples (see Section 5.3), derived from 154 WordNet synonym sets (see Table 5). Super learns from the expert knowledge that is embedded in WordNet. We would like to be able to train Super without using this kind of expertise.\nPast work with adjective-noun bigrams has shown that we can use holistic bigram vectors to train a supervised regression model (Guevara, 2010; Baroni & Zamparelli, 2010). Turney (2013) adapted this approach for supervised classification applied to composition recognition. The purpose of the two holistic datasets (see Section 3) is to investigate whether we can apply holistic training to composition and decomposition generation. The holistic datasets were generated without using WordNet synomym sets. To construct the holistic datasets, we used WordNet as a source of bigrams, but we ignored the rich information that WordNet provides about these bigrams, such as their synonyms, hypernyms, hyponyms, meronyms, and glosses.\nTable 15 shows the performance of Comp and Super on the holistic compostion dataset. For ease of comparison, the table also shows the performance on the standard composition dataset. The training and testing subsets of the holistic composition dataset have the same sizes as the corresponding training and testing subsets of the standard composition dataset (154 training targets and 351 testing targets). However, we see that Comp and Super achieve much better results on the holistic dataset than on the standard dataset. This suggests that the holistic dataset is not nearly as challenging as the dataset based on WordNet synonym sets.\nTable 16 looks at the performance with various combinations of testing and training datasets. Although Comp and Super work together, the table only shows the results for Super, in order to make the table easier to read. The two columns that are labeled both\nuse Daumé III’s (2007) domain adaptation algorithm for training. This algorithm allows us to train Super with both the standard training dataset and the holistic training dataset. Daumé III’s (2007) algorithm is a general strategy for merging datasets that have somewhat different statistical distributions of class labels, such as our standard and holistic datasets.\nTable 16 shows that the model learned from standard training (with WordNet synonym sets) carries over well to holistic testing. Focusing on percent in top 100, standard training achieves 92.3% on holistic testing, which is not significantly different from the 93.7% obtained with holistic training (Fisher’s Exact Test at the 95% confidence level). On the other hand, the model learned from holistic training (without WordNet synonym sets) does not carry over well to standard training. Holistic training only achieves 64.4% on standard testing, whereas standard training achieves 77.8%, which is significantly higher.\nTable 16 suggests that there is not much benefit to merging the holistic and standard training datasets with Daumé III’s (2007) domain adaptation algorithm. Training on the standard training dataset alone achieves results that are as good as merging the datasets."
    }, {
      "heading" : "6.4 Evaluation with the Adjective-Noun Dataset",
      "text" : "Here we experiment with the 620 adjective-noun phrases of Dinu et al. (2013) (see Table 2 in Section 2.2).16 Table 17 shows the performance of Comp and Super on the adjective-noun\n16. Georgiana Dinu, Nghia The Pham, and Marco Baroni gave us a copy of their 620 adjective-noun phrases, the corresponding noun solutions for each noun phrase, and their vocabulary of 21,000 nouns.\ndataset. For training, we merged the standard training and testing composition datasets (154 + 351 = 505 targets) and removed any target bigrams that appeared in the adjectivenoun dataset (505−115 = 390 targets). For testing, we used all 620 adjective-noun phrases.\nThere is a slight decrease in the performance with the adjective-noun phrases (Table 17), compared to the noun-modifier dataset (Table 13). We hypothesize that this is because we deliberately sought highly compositional phrases for the noun-modifier dataset, using WordNet glosses as heuristic clues (see Section 3). It seems that the phrases in the adjectivenoun dataset are less compositional than the phrases in the noun-modifier dataset.\nThe 620 adjective-noun phrases were introduced by Dinu et al. (2013), but they were used more recently, and with better results, by Li, Baroni, and Dinu (2014). Table 18 is a copy of their results (this is Table 3 in their paper). They evaluated seven different models on the adjective-noun dataset. The reduction column indicates the algorithm used to reduce the dimensionality of the matrix, nonnegative matrix factorization (NMF) or singular value decomposition (SVD). The dim column indicates the number of factors in the dimensionality reductions. The rank column gives the median rank of the solution in the ranked list of candidates.\nIn Table 18, Add is vector addition with weights (Mitchell & Lapata, 2008). Dil is the dilation model introduced by Mitchell and Lapata (2008). Mult is element-wise multiplication (Mitchell & Lapata, 2008) with powers as weights (Dinu et al., 2013). Fulladd multiplies each vector by a weight matrix and then adds the resulting weighted vectors (Guevara, 2010). Lexfunc represents adjectives by matrices and nouns by vectors (Baroni & Zamparelli, 2010); the adjective matrix is a kind of lexical function that modifies the noun vector. Fulllex represents every component unigram, whether noun or adjective, with both\na vector and a matrix (Socher, Huval, Manning, & Ng, 2012). Enetlex is a lexical function model, like Lexfunc, where the model is trained using elastic-net regression (Li et al., 2014).\nAll of the models in Table 18 have weights or parameters that are tuned or learned on training data. The training takes a holistic approach. Given a bigram, ab, the models are trained to predict the vector of the pseudo-unigram, a b.\nIn order to compare our results on this dataset with the results of Li et al. (2014), we need to make two adjustments to our experimental setup. First, we consider a vocabulary of 73,000 unigrams, but Li et al. (2014) consider 21,000 nouns. For a fair comparison, we restrict Comp to the same 21,000 nouns. FilterComp is a modified version of Comp that filters the output of Comp to remove any unigrams that are not among the 21,000 nouns of Li et al. (2014). Second, we only calculate the mean and median rank for the targets that have solutions among the top 2000 candidates, because Super does not rank the other candidates. Since Li et al. (2014) use a single pass approach, they rank all of the candidates. To make our median comparable to their median, we give a rank to all candidates. If a target does not have a solution among the top 2000 candidates, we assume that the rank of the solution is the worst possible rank; that is, the rank of the last candidate, 21,098.\nTable 19 shows the performance of FilterComp and Super with these two adjustments. Filtering makes the task easier by removing irrelevant candidates, which tends to improve the results, but our adjustment to the mean and median calculation has a negative impact on these evaluation metrics. Fortunately the median is a robust statistic that is only slightly affected by the adjustment. The mean, on the other hand, is greatly changed, for the worse.\nWith medians of 52 and 13, FilterComp and Super are performing much better than the best model in Table 18, EnetLex, with a median rank of 108.5. Recall the discussion in Section 2.2 about context composition and similarity composition. All of the models in Table 18 involve context composition, whereas FilterComp and Super use similarity composition. We believe that this is the main reason for the better performance of FilterComp and Super.\nSuper is attempting to learn a function, f , that maps feature vectors to scalar probabilities. The majority of the features are various kinds of similarities (see Table 10). The models in Table 18 are attempting to learn a function, f , that maps to vectors. The models are trained with pseudo-unigram context vectors. We believe that pseudo-unigram context vectors are only an approximation, a surrogate for what we really want to learn. We really want to learn that red salmon and sockeye are synonymous, but instead we learn that red\nsalmon and red salmon are synonymous. The results in Section 6.3 suggest that red salmon is a reasonable substitute for sockeye, but it is not quite as good as the real thing.\nFuthermore, Lexfunc and EnetLex must learn a separate model (a unique matrix) for each adjective in the vocabulary. There are 411 different adjectives in the 620 adjectivenoun phrases; thus Lexfunc and EnetLex require at least 411 different trained models in order to find solution nouns for the 620 targets. FilterComp and Super use one model for all noun-modifier expressions, both adjective-noun and noun-noun. They can readily handle adjectives that never appeared in their training data, unlike Lexfunc and EnetLex."
    }, {
      "heading" : "7. Experiments with Decomposition",
      "text" : "This section describes the experiments with Decomp and Super on the standard and holistic decomposition datasets."
    }, {
      "heading" : "7.1 Evaluation with the Standard Decomposition Dataset",
      "text" : "Decomp first builds a list of maxm modifiers using scorem and a list of maxh heads using scoreh. Table 20 illustrates this first step, using the target unigram sockeye as an example. Table 4 shows that sockeye has several possible solution bigrams: blueback salmon, red salmon, sockeye salmon, and Oncorhynchus nerka. Looking ahead, the solution that will eventually be found is sockeye salmon. In Table 20, sockeye is ranked first in the list of candidate modifiers and salmon is ranked sixth in the list of candidate heads. (They are marked with asterisks in the table.)\nNext, Decomp builds a list of maxb bigrams using scoreb to rank the combination of each of the maxm modifiers with each of the maxh heads. Super then rescores the list of bigrams. Table 21 continues the sockeye example. Decomp ranks sockeye salmon first. Unfortunately, Super moves the rank of sockeye salmon down to the twenty-first choice. (Asterisks mark sockeye salmon in the table.)\nThe second guess of Super in Table 21 is pink salmon, which seems like a good decomposition for sockeye, but it is not a member of the WordNet synonym set for sockeye. Pink salmon (Oncorhynchus gorbuscha) and red salmon (Oncorhynchus nerka, sockeye) are distinct species. Coho salmon (silver salmon, Oncorhynchus kisutch), chinook salmon\n(Oncorhynchus tshawytscha), and chum salmon (Oncorhynchus keta) are other species of salmon.\nTable 22 summarizes the performance of Decomp and Super on the standard decomposition testing dataset, containing 355 target unigrams. According to our preferred evaluation metric, percent in top 100, Decomp and Super, working together, achieve a score of 50.7% on this dataset. This is below the 77.8% that Comp and Super reach on the standard composition testing dataset, but Comp and Super only consider 73,000 candidates, whereas Decomp and Super consider 5.3 × 109. Decomposition is a more difficult task than composition."
    }, {
      "heading" : "7.2 Comparison with Baselines",
      "text" : "As with composition (Section 6.2), we compare Decomp and Super with three baselines, vector addition, element-wise multiplication, and the holistic approach. Addition and multiplication take the output of Decomp and rescore it. The holistic approach scores all bigrams in WordNet, treating the bigrams as pseudo-unigrams. Table 23 shows the results. Addition and multiplication perform significantly worse than Decomp and Super according to percent in top 1, top 10, and top 100 (Fisher’s Exact Test, 95% confidence level). The holistic approach performs significantly better than Decomp and Super according to percent in top 1, top 10, and top 100 (Fisher’s Exact Test, 95% confidence level).\nNote that the holistic approach can only decompose a unigram into a bigram if the bigram is one of the 36,000 bigrams in WordNet, whereas Decomp and Super can decompose a unigram into any bigram that contains unigrams from the 73,000 unigrams in WordNet (5,300,000,000 bigrams). Although the holistic approach performs better in Table 23, Decomp and Super are considering a much larger set of candidates.\nFor a more fair comparison, we can restrict Decomp to the same 36,000 bigrams as the holistic approach. FilterDecomp is a modified version of Decomp that simply filters the output of Decomp to remove any bigrams that are not in WordNet. Table 24 compares FilterDecomp and Super to the holistic approach. Now FilterDecomp and Super approach the performance of the holistic approach. The difference in percent in top 1, 23.9% for Super versus 34.4% for the holistic approach, is statistically significant, but the differences in top 10 (58.3% versus 63.4%) and top 100 (78.6% versus 83.1%) are not significant (Fisher’s Exact Test, 95% confidence level). Also, the performance of Super on the decomposition dataset (78.6% in Table 24) is now near its performance on the composition dataset (77.8% in Table 13). This suggests that the main difference between the composition and decomposition tasks is simply the number of candidates that must be considered.\nThe point of Table 24 is only to show that Decomp and Super are competitive with the performance of the holistic approach, contrary to appearances in Table 23. We would not actually want to use FilterDecomp in a realistic application. One problem is that 36,000 bigrams are not enough bigrams to serve as miniature definitions for 73,000 unigrams."
    }, {
      "heading" : "7.3 Evaluation with the Holistic Decomposition Dataset",
      "text" : "Table 25 shows the performance of Decomp and Super on the holistic decomposition dataset. The previous results for the standard decomposition dataset (Table 22) are copied here to make comparison easier. As with the composition task (see Table 15), the results show that the holistic dataset is not nearly as challenging as the dataset based on WordNet synonym sets.\nTable 26 gives results for various combinations of testing and training datasets. Here we see some benefit to merging the two training datasets, using Daumé III’s (2007) domain adaptation algorithm (see the columns labeled both). However, if we focus on percent in top 100, the differences between training with the standard dataset and the merged dataset (80.3% versus 82.3% with holistic testing and 50.7% versus 52.7% with standard testing) are not significant. Training on the standard training dataset alone achieves results that are as good as merging the datasets. We still do not see a significant benefit from using the holistic dataset."
    }, {
      "heading" : "8. Discussion and Analysis",
      "text" : "In Section 6.1, we evaluted Comp and Super by comparing their output to WordNet synonym sets. For the bigrams in the standard compositon dataset, the top 100 most highly ranked unigrams included a WordNet synonym 77.8% of the time (Table 13). In Section 7.1,\nwe evaluated Decomp and Super with WordNet. For the standard decomposition dataset, the top 100 bigrams included a WordNet synonym 50.7% of the time (Table 22). Together, Comp and Super explore 73,000 candidate unigram compositions for a bigram. Decomp and Super explore 5,300,000,000 candidate bigram decompositions for a unigram. Given that Decomp and Super explore a much larger space than Comp and Super, it is encouraging that Decomp and Super are able to achieve 50.7%.\nIn Sections 6.2 and 7.2, we compared Comp, Decomp, and Super to three baselines, vector addition, element-wise multiplication, and the holistic approach. Given the output of Comp, addition and multiplication are able to improve the percentage of solutions found in the top 100 candidates, but Super is able to achieve a significantly larger improvement (Table 14). Given the output of Decomp, addition and multiplication impair the ranking of Decomp, whereas Super improves the ranking of Decomp (Table 23). However, the holistic approach seems to achieve better results than Comp, Decomp, and Super.\nComp and Super, working together, achieve 50.4% in the top 10 and 77.8% in the top 100, whereas the holistic approach achieves 54.4% and 78.9% (Table 14), but the differences in these scores are not statistically significant. Decomp and Super at first seem to be performing substantially below the holistic approach (Table 23), but this does not take into account that Decomp and Super explore 5,300,000,000 candidate bigram decompositions, whereas the holistic approach, due to its scaling problems, can only explore 36,000 decompositions. Once we correct for this (Table 24), the differences in the top 10 (58.3% for FilterDecomp and Super versus 63.4% for the holistic approach) and the top 100 (78.6% versus 83.1%) are not statistically significant.\nGiven the good performance of the holistic baseline, it is natural to consider using holistic datasets as a relatively inexpensive way to train a supervised system (Guevara, 2010; Baroni & Zamparelli, 2010; Turney, 2013). The strength of Super is that it uses a compositional approach, so it can scale up, but its weakness is that it requires training. The strength of the holistic approach is that it works well without training, but its weakness is that it does not scale up. The hope is that training Super with holistic datasets will give us both of the strengths: a compositional approach with relatively inexpensive holistic training.\nWe explored holistic training in Sections 6.3 and 7.3. In the experiments where the training and testing datasets are the same (both are standard or both are holistic), we observed that the holistic datasets are much easier to master than the standard WordNetbased datasets (Tables 15 and 25). Understanding the experiments where the training and testing datasets are different (Tables 16 and 26) takes more effort. Here we present a model that may give some insight into the cross-domain results (that is, the results when training on one kind of domain, standard or holistic, and testing on another). We are particularly interested in training on a holistic dataset (because it is relatively inexpensive) and then testing on a standard dataset (because it is more representative of potential real-world applications).\nSuppose that Super is given a target t and generates a list of the top N candidate compositions or decompositions for t. Let Phh(t) be the probability that there is a solution for t among theN candidates generated by Super, where the subscript hh on Phh(t) indicates holistic training and holistic testing. Let Phs(t) be the probability that there is a solution for the target t among the top N candidates of Super, where the subscript hs indicates\nholistic training and standard testing. Let Qss(t) be the probability that there is a solution for the target t among the top N candidates of the holistic approach, where ss indicates standard training and standard testing. The holistic approach does not require training in the usual sense, but we do use the standard training set to tune the parameters, k and p. We model Phs(t) as Phh(t) · Qss(t). That is, the probability that Super can transfer what was learned in the holistic domain to targets sampled from the standard domain, Phs(t), can be estimated by multiplying the probability that Super can successfully emulate the holistic approach, Phh(t), with the probability that the holistic approach can successfully handle the standard domain, Qss(t).\nWe can view the results with holistic training and holistic testing (Tables 15 and 25) as providing us with estimates of the probability that Super can successfully emulate the holistic approach, Phh(t). Likewise, we can view the results with the holistic approach, given standard training and testing (Tables 14 and 23), as providing us with estimates of the probability that the holistic approach can find solutions to the WordNet-based targets, Qss(t). If Super is trained on the holistic datasets and then evaluated on the standard datasets, then we can estimate its probability of success, Phs(t), by Phh(t) ·Qss(t).\nTable 27 compares the cross-domain model with the actual observed performance of Comp and Super on the composition datasets:\nA. The values in column A show the performance of Super when trained and tested using the holistic composition dataset. We interpret the percentages in this column as estimated probabilities for Phh(t) with N = {1, 10, 100, 2000}. B. This column gives the performance of the holistic approach when trained and tested using the standard composition dataset. We interpret the percentages in this column as estimated probabilities for Qss(t) with varying values of N . C. The numbers in column C are the product of the corresponding values in columns A and B, based on interpreting the percentages as probabilities. For example, for N = 1, we have 0.268 × 0.251 = 0.067. We interpret the percentages in this column as the predictions of our model, Phh(t) ·Qss(t), with varying values of N . D. This column shows the performance of Super when trained on the holistic composition dataset and tested on the standard composition dataset. This column gives the observed values of Phs(t), which we may compare to the calculated values in column C.\nThe predictions of the model (column C) do not exactly match the observed values (column D), but the model seems reasonable as a first-order approximation.\nWe cannot apply this model immediately to the performance of Decomp and Super on the decomposition datasets, because the performance of the holistic approach in Table 23 is based on partial search. The holistic approach only considers 36,000 possible candidates, but we need to base Qss(t) on full search, considering 5.3×10\n9 candidates. This is a problem, because the holistic approach will not scale up to this size. We would need to build a termcontext matrix with 5.3 × 109 rows, which is considerably beyond the current state of the art. Therefore we approximate the expected performance of the holistic approach with full search by extrapolating from our results with partial and full search for Decomp and Super.\nTable 28 gives our estimated probabilities for the holistic approach, Qss(t), if it were able to scale up to full search:\nComposition A B C D Evaluation metric Super Holistic Super Super Percent in top 1 26.8 25.1 6.7 7.1 Percent in top 10 86.6 54.4 47.1 34.8 Percent in top 100 93.7 78.9 73.9 64.4 Percent in 2000 candidates 94.6 92.9 87.9 84.0 Training dataset holistic standard holistic holistic Testing dataset holistic standard standard standard Source of percentages Table 15 Table 14 A × B Table 16 Observed or calculated? observed observed calculated observed\nWe cannot actually observe the values in column E, given the scaling problems of the holistic approach, but they seem to be reasonable estimates.\nNow that we have estimates for Qss(t) with full search, we can apply the cross-domain model to the performance of Decomp and Super on the decomposition datasets. Table 29 gives the results:\nA. The values show the performance of Super when trained and tested using the holistic decomposition dataset. We interpret the percentages as estimates for Phh(t). B. This column gives the expected performance of the holistic approach, if it could extended to full search. We interpret the percentages as estimates for Qss(t). C. The numbers in column C are the product of the corresponding values in columns A and B. We interpret the percentages in this column as the predictions of our model, Phh(t) ·Qss(t). D. This column shows the performance of Super when trained on the holistic decomposition dataset and tested on the standard decomposition dataset. This column gives the observed values of Phs(t), which we may compare to the calculated values in column C.\nThe predictions of the model (column C) do not exactly match the observed values (column D), but the model seems to be a reasonable approximation.\nThe holistic approach is unsupervised. Although training data is useful for parameter tuning, a larger training dataset is unlikely to have much impact on Qss(t). More training data may improve Phh(t), but the best we can hope for is Phh(t) = 1. Thus our model predicts that Phs(t) cannot be greater than Qss(t), and we are not likely to see big improvements in Qss(t). On the other hand, training Super with WordNet, Pss(t), is already achieving performance near Qss(t). However, our model does not take into account what might be possible with domain adaptation (Daumé III, 2007).\nIn summary, we can gain some insight into the cross-domain results in Sections 6.3 and 7.3 (Tables 16 and 26) by viewing the observed performance of Super, Phs(t), as being the product of two things: the ability of Super to emulate the holistic approach, Phh(t), and the ability of the holistic approach to handle targets and solutions based on WordNet, Qss(t)."
    }, {
      "heading" : "9. Limitations and Future Work",
      "text" : "The main limitation of the work described here is that we have focused our attention on noun-modifier bigrams and noun unigrams. A natural next step would be to consider subject-verb bigrams, verb-object bigrams, or subject-verb-object trigrams.\nWe plan to extend this approach to recognizing when one sentence is a paraphrase of another. To achieve this goal, we propose to adapt the dynamic pooling approach introduced by Socher et al. (2011). Eventually, we hope to be able to move beyond sentence paraphrase recognition to sentence paraphrase generation.\nTurney (2013) has demonstrated that the features we have used here (Section 4) work well for recognizing semantic relations; for example, we can recognize that the semantic relation between mason and stone is analogous to the semantic relation between carpenter and wood. Analogy generation is another possibility for further work.\nWe believe that improved performance can be obtained by adding more features. Features based on third-order tensors might be useful for recognizing and generating paraphrases (Baroni & Lenci, 2010). With the right features, it should be possible to surpass the accuracy of the holistic approach.\nComp and Decomp rely on hand-crafted score functions (Section 5). The advantage of these hand-crafted functions is that they use only eight features, which makes them much faster than using the 681 features of Super. One way to avoid hand-crafting would be to use aggressive feature selection with Super. Instead of using Comp and Decomp for the first pass through the candidates, we could use a highly pruned version of Super. We expect that the pruning will reduce accuracy, so there will still be a benefit from a second pass of Super using the full feature set.\nAnother area for future work is more experimentation with domain adaptation and holistic training. Although our model (Section 8) suggests that there are limits to the value of holistic training, it may be possible to overcome these limits with domain adaptation (Daumé III, 2007)."
    }, {
      "heading" : "10. Conclusions",
      "text" : "The main weakness of the recognition task is that the degree of challenge in the task depends on the given list of candidates. A critic can always claim that the given list was too easy; the distractors were too unlike the solution. The generation task avoids this criticism. Furthermore, the ability to generate solutions opens up opportunities for new applications, beyond the applications that are possible with the ability to recognize solutions.\nFollowing the example of Dinu et al. (2013) and Li et al. (2014), we have extended distributional models of composition from the task of recognition to the task of generation. Our results on their adjective-noun dataset suggest that similarity composition is a better approach to generating compositions than context composition (Section 6.4). The experiments with holistic training support the hypothesis that the models of Dinu et al. (2013) and Li et al. (2014) are limited by their reliance on holistic pseudo-unigram training (Section 6.3).\nWe have also extended distributional models to the task of decomposition generation. Our results indicate that decomposition is considerably more difficult than composition,\ndue to the larger search space, which increases the difficulty of finding the solution among the candidates (Section 7.2).\nBy the stringent criterion that allows only one guess (percent in top 1), we achieved an accuracy of 17.7% on composition generation (Table 13) and 5.6% on decomposition generation (Table 22). By the more relaxed criterion that allows 100 guesses, the accuracy on composition is 77.8% and the accuracy on decomposition is 50.7%. We do not know what accuracy an average human would have on these tasks, but generating a solution that agrees with WordNet is difficult. Many of the words in WordNet are unfamiliar and also the developers of WordNet are likely to have missed many reasonable possibilities for the WordNet synonym sets.\nWe introduced a simple cross-domain model of holistic training and WordNet-based testing (Section 8), which gives some insight into the limitations of holistic training. With standard training, our models are achieving performance near the level of the holistic approach (Tables 14 and 24). With new features (beyond the features in Section 4), we expect that Super will be able to surpass the performance of the holistic approach. (Super already surpasses the holistic approach in that Super can scale up in a way that is not possible for any non-compositional approach.) However, the simple cross-domain model may suggest ways to improve holistic training. The results with Daumé III’s (2007) domain adaptation algorithm (Section 7.3) hint that holistic training may be able to serve as a supplement to standard training.\nOur main contribution is to extend the similarity composition approach (Turney, 2012, 2013) beyond recognition, to generation of composition and decomposition. By combining an unsupervised first pass with a supervised second pass, we can scale up to the generation task."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Thanks to Charles Clarke and Egidio Terra for sharing the Waterloo corpus. Thanks to my colleagues at NRC, who provided many helpful comments during a presentation of this research. Thanks to Georgiana Dinu, Nghia The Pham, and Marco Baroni for sharing their noun-adjective dataset and their vocabulary of 21,000 nouns."
    } ],
    "references" : [ {
      "title" : "A survey of paraphrasing and textual entailment methods",
      "author" : [ "I. Androutsopoulos", "P. Malakasiotis" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Androutsopoulos and Malakasiotis,? \\Q2010\\E",
      "shortCiteRegEx" : "Androutsopoulos and Malakasiotis",
      "year" : 2010
    }, {
      "title" : "Distributional memory: A general framework for corpusbased semantics",
      "author" : [ "M. Baroni", "A. Lenci" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Baroni and Lenci,? \\Q2010\\E",
      "shortCiteRegEx" : "Baroni and Lenci",
      "year" : 2010
    }, {
      "title" : "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
      "author" : [ "M. Baroni", "R. Zamparelli" ],
      "venue" : "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP",
      "citeRegEx" : "Baroni and Zamparelli,? \\Q2010\\E",
      "shortCiteRegEx" : "Baroni and Zamparelli",
      "year" : 2010
    }, {
      "title" : "Normalized (pointwise) mutual information in collocation extraction",
      "author" : [ "G. Bouma" ],
      "venue" : "Proceedings of the Biennial Conference of the German Society for Computational Linguistics and Language Technology (GSCL 2009), pp. 31–40.",
      "citeRegEx" : "Bouma,? 2009",
      "shortCiteRegEx" : "Bouma",
      "year" : 2009
    }, {
      "title" : "Web 1T 5-gram Version 1. Linguistic Data Consortium, Philadelphia",
      "author" : [ "T. Brants", "A. Franz" ],
      "venue" : null,
      "citeRegEx" : "Brants and Franz,? \\Q2006\\E",
      "shortCiteRegEx" : "Brants and Franz",
      "year" : 2006
    }, {
      "title" : "Extracting semantic representations from word cooccurrence statistics: A computational study",
      "author" : [ "J. Bullinaria", "J. Levy" ],
      "venue" : "Behavior Research Methods,",
      "citeRegEx" : "Bullinaria and Levy,? \\Q2007\\E",
      "shortCiteRegEx" : "Bullinaria and Levy",
      "year" : 2007
    }, {
      "title" : "Extracting semantic representations from word cooccurrence statistics: Stop-lists, stemming, and SVD",
      "author" : [ "J. Bullinaria", "J. Levy" ],
      "venue" : "Behavior Research Methods,",
      "citeRegEx" : "Bullinaria and Levy,? \\Q2012\\E",
      "shortCiteRegEx" : "Bullinaria and Levy",
      "year" : 2012
    }, {
      "title" : "Experiments with LSA scoring: Optimal rank and basis",
      "author" : [ "J. Caron" ],
      "venue" : "In Proceedings of the SIAM Computational Information Retrieval Workshop,",
      "citeRegEx" : "Caron,? \\Q2001\\E",
      "shortCiteRegEx" : "Caron",
      "year" : 2001
    }, {
      "title" : "The Logical Structure of Linguistic Theory",
      "author" : [ "N. Chomsky" ],
      "venue" : "Plenum Press.",
      "citeRegEx" : "Chomsky,? 1975",
      "shortCiteRegEx" : "Chomsky",
      "year" : 1975
    }, {
      "title" : "Word association norms, mutual information, and lexicography",
      "author" : [ "K. Church", "P. Hanks" ],
      "venue" : "In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics,",
      "citeRegEx" : "Church and Hanks,? \\Q1989\\E",
      "shortCiteRegEx" : "Church and Hanks",
      "year" : 1989
    }, {
      "title" : "A compositional distributional model of meaning",
      "author" : [ "S. Clark", "B. Coecke", "M. Sadrzadeh" ],
      "venue" : "In Proceedings of the 2nd Symposium on Quantum Interaction,",
      "citeRegEx" : "Clark et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2008
    }, {
      "title" : "Combining symbolic and distributional models of meaning",
      "author" : [ "S. Clark", "S. Pulman" ],
      "venue" : "In Proceedings of the AAAI Spring Symposium on Quantum Interaction,",
      "citeRegEx" : "Clark and Pulman,? \\Q2007\\E",
      "shortCiteRegEx" : "Clark and Pulman",
      "year" : 2007
    }, {
      "title" : "Automatic and human scoring of word definition responses",
      "author" : [ "K. Collins-Thompson", "J. Callan" ],
      "venue" : "In Proceedings of the HLT-NAACL",
      "citeRegEx" : "Collins.Thompson and Callan,? \\Q2007\\E",
      "shortCiteRegEx" : "Collins.Thompson and Callan",
      "year" : 2007
    }, {
      "title" : "Frustratingly easy domain adaptation",
      "author" : [ "III H. Daumé" ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pp. 256–263, Prague, Czech Republic.",
      "citeRegEx" : "Daumé,? 2007",
      "shortCiteRegEx" : "Daumé",
      "year" : 2007
    }, {
      "title" : "How to make words with vectors: Phrase generation in distributional semantics",
      "author" : [ "G. Dinu", "M. Baroni" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL",
      "citeRegEx" : "Dinu and Baroni,? \\Q2014\\E",
      "shortCiteRegEx" : "Dinu and Baroni",
      "year" : 2014
    }, {
      "title" : "General estimation and evaluation of compositional distributional semantic models",
      "author" : [ "G. Dinu", "N.T. Pham", "M. Baroni" ],
      "venue" : "In Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality (CVSC",
      "citeRegEx" : "Dinu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dinu et al\\.",
      "year" : 2013
    }, {
      "title" : "Towards a semantics for distributional representations",
      "author" : [ "K. Erk" ],
      "venue" : "Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013), Potsdam, Germany.",
      "citeRegEx" : "Erk,? 2013",
      "shortCiteRegEx" : "Erk",
      "year" : 2013
    }, {
      "title" : "A synopsis of linguistic theory 1930–1955",
      "author" : [ "J.R. Firth" ],
      "venue" : "Studies in Linguistic Analysis, pp. 1–32. Blackwell, Oxford.",
      "citeRegEx" : "Firth,? 1957",
      "shortCiteRegEx" : "Firth",
      "year" : 1957
    }, {
      "title" : "The Compositionality Papers",
      "author" : [ "J. Fodor", "E. Lepore" ],
      "venue" : null,
      "citeRegEx" : "Fodor and Lepore,? \\Q2002\\E",
      "shortCiteRegEx" : "Fodor and Lepore",
      "year" : 2002
    }, {
      "title" : "Experimenting with transitive verbs in a DisCoCat",
      "author" : [ "E. Grefenstette", "M. Sadrzadeh" ],
      "venue" : "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics",
      "citeRegEx" : "Grefenstette and Sadrzadeh,? \\Q2011\\E",
      "shortCiteRegEx" : "Grefenstette and Sadrzadeh",
      "year" : 2011
    }, {
      "title" : "A regression model of adjective-noun compositionality in distributional semantics",
      "author" : [ "E. Guevara" ],
      "venue" : "Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics (GEMS 2010), pp. 33–37.",
      "citeRegEx" : "Guevara,? 2010",
      "shortCiteRegEx" : "Guevara",
      "year" : 2010
    }, {
      "title" : "Distributional structure",
      "author" : [ "Z. Harris" ],
      "venue" : "Word, 10 (23), 146–162.",
      "citeRegEx" : "Harris,? 1954",
      "shortCiteRegEx" : "Harris",
      "year" : 1954
    }, {
      "title" : "The class imbalance problem: A systematic study",
      "author" : [ "N. Japkowicz", "S. Stephen" ],
      "venue" : "Intelligent Data Analysis,",
      "citeRegEx" : "Japkowicz and Stephen,? \\Q2002\\E",
      "shortCiteRegEx" : "Japkowicz and Stephen",
      "year" : 2002
    }, {
      "title" : "A unified sentence space for categorical distributional-compositional semantics: Theory and experiments",
      "author" : [ "D. Kartsaklis", "M. Sadrzadeh", "S. Pulman" ],
      "venue" : "In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012): Posters,",
      "citeRegEx" : "Kartsaklis et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kartsaklis et al\\.",
      "year" : 2012
    }, {
      "title" : "Predication",
      "author" : [ "W. Kintsch" ],
      "venue" : "Cognitive Science, 25 (2), 173–202.",
      "citeRegEx" : "Kintsch,? 2001",
      "shortCiteRegEx" : "Kintsch",
      "year" : 2001
    }, {
      "title" : "On the computational basis of learning and cognition: Arguments from LSA",
      "author" : [ "T.K. Landauer" ],
      "venue" : "Ross, B. H. (Ed.), The Psychology of Learning and Motivation: Advances in Research and Theory, Vol. 41, pp. 43–84. Academic Press.",
      "citeRegEx" : "Landauer,? 2002",
      "shortCiteRegEx" : "Landauer",
      "year" : 2002
    }, {
      "title" : "A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge",
      "author" : [ "T.K. Landauer", "S.T. Dumais" ],
      "venue" : "Psychological Review,",
      "citeRegEx" : "Landauer and Dumais,? \\Q1997\\E",
      "shortCiteRegEx" : "Landauer and Dumais",
      "year" : 1997
    }, {
      "title" : "Improving the lexical function composition model with pathwise optimized elastic-net regression",
      "author" : [ "J. Li", "M. Baroni", "G. Dinu" ],
      "venue" : "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "DIRT – discovery of inference rules from text",
      "author" : [ "D. Lin", "P. Pantel" ],
      "venue" : "In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "citeRegEx" : "Lin and Pantel,? \\Q2001\\E",
      "shortCiteRegEx" : "Lin and Pantel",
      "year" : 2001
    }, {
      "title" : "Generating phrasal and sentential paraphrases: A survey of data-driven methods",
      "author" : [ "N. Madnani", "B. Dorr" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Madnani and Dorr,? \\Q2010\\E",
      "shortCiteRegEx" : "Madnani and Dorr",
      "year" : 2010
    }, {
      "title" : "Vector-based models of semantic composition",
      "author" : [ "J. Mitchell", "M. Lapata" ],
      "venue" : "In Proceedings of ACL-08: HLT,",
      "citeRegEx" : "Mitchell and Lapata,? \\Q2008\\E",
      "shortCiteRegEx" : "Mitchell and Lapata",
      "year" : 2008
    }, {
      "title" : "Composition in distributional models of semantics",
      "author" : [ "J. Mitchell", "M. Lapata" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "Mitchell and Lapata,? \\Q2010\\E",
      "shortCiteRegEx" : "Mitchell and Lapata",
      "year" : 2010
    }, {
      "title" : "Aligning needles in a haystack: Paraphrase acquisition across the web",
      "author" : [ "M. Pasca", "P. Dienes" ],
      "venue" : "In Proceedings of the 2005 International Joint Conference on Natural Language Processing (IJCNLP",
      "citeRegEx" : "Pasca and Dienes,? \\Q2005\\E",
      "shortCiteRegEx" : "Pasca and Dienes",
      "year" : 2005
    }, {
      "title" : "Fast training of support vector machines using sequential minimal optimization",
      "author" : [ "J.C. Platt" ],
      "venue" : "Advances in Kernel Methods: Support Vector Learning, pp. 185–208, Cambridge, MA. MIT Press.",
      "citeRegEx" : "Platt,? 1998",
      "shortCiteRegEx" : "Platt",
      "year" : 1998
    }, {
      "title" : "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
      "author" : [ "R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS",
      "citeRegEx" : "Socher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "R. Socher", "B. Huval", "C. Manning", "A. Ng" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL",
      "citeRegEx" : "Socher et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Domain and function: A dual-space model of semantic relations and compositions",
      "author" : [ "P.D. Turney" ],
      "venue" : "Journal of Artificial Intelligence Research, 44, 533–585.",
      "citeRegEx" : "Turney,? 2012",
      "shortCiteRegEx" : "Turney",
      "year" : 2012
    }, {
      "title" : "Distributional semantics beyond words: Supervised learning of analogy and paraphrase",
      "author" : [ "P.D. Turney" ],
      "venue" : "Transactions of the Association for Computational Linguistics, 1, 353–366.",
      "citeRegEx" : "Turney,? 2013",
      "shortCiteRegEx" : "Turney",
      "year" : 2013
    }, {
      "title" : "Literal and metaphorical sense identification through concrete and abstract context",
      "author" : [ "P.D. Turney", "Y. Neuman", "D. Assaf", "Y. Cohen" ],
      "venue" : "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Turney et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Turney et al\\.",
      "year" : 2011
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "P.D. Turney", "P. Pantel" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Turney and Pantel,? \\Q2010\\E",
      "shortCiteRegEx" : "Turney and Pantel",
      "year" : 2010
    }, {
      "title" : "Computational semantics of noun compounds in a semantic space model",
      "author" : [ "A. Utsumi" ],
      "venue" : "Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI-09), pp. 1568–1573.",
      "citeRegEx" : "Utsumi,? 2009",
      "shortCiteRegEx" : "Utsumi",
      "year" : 2009
    }, {
      "title" : "Semantic vector products: Some initial investigations",
      "author" : [ "D. Widdows" ],
      "venue" : "Proceedings of the 2nd Symposium on Quantum Interaction, Oxford, UK.",
      "citeRegEx" : "Widdows,? 2008",
      "shortCiteRegEx" : "Widdows",
      "year" : 2008
    }, {
      "title" : "Data Mining: Practical Machine Learning Tools and Techniques, Third Edition",
      "author" : [ "I.H. Witten", "E. Frank", "M.A. Hall" ],
      "venue" : null,
      "citeRegEx" : "Witten et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Witten et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Distributional semantics is based on the hypothesis that words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957).",
      "startOffset" : 124,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : "Distributional semantics is based on the hypothesis that words that occur in similar contexts tend to have similar meanings (Harris, 1954; Firth, 1957).",
      "startOffset" : 124,
      "endOffset" : 151
    }, {
      "referenceID" : 16,
      "context" : "Much recent work has been concerned with the problem of extending vector space models beyond words, to phrases and sentences (Erk, 2013).",
      "startOffset" : 125,
      "endOffset" : 136
    }, {
      "referenceID" : 36,
      "context" : "The approach that has worked with words does not scale up to phrases and sentences, due to data sparsity and growth in model size (Turney, 2012).",
      "startOffset" : 130,
      "endOffset" : 144
    }, {
      "referenceID" : 36,
      "context" : "bigram and a list of candidate unigrams, can the model recognize the synonymous unigram among the candidates (Turney, 2012)? Given a target unigram and a list of candidate bigrams, can the model recognize the synonymous bigram among the candidates (Kartsaklis, Sadrzadeh, & Pulman, 2012)? Many noun unigrams have synonymous noun-modifier bigrams.",
      "startOffset" : 109,
      "endOffset" : 123
    }, {
      "referenceID" : 35,
      "context" : "bigram and a list of candidate unigrams, can the model recognize the synonymous unigram among the candidates (Turney, 2012)? Given a target unigram and a list of candidate bigrams, can the model recognize the synonymous bigram among the candidates (Kartsaklis, Sadrzadeh, & Pulman, 2012)? Many noun unigrams have synonymous noun-modifier bigrams. For example, the WordNet synonym set for contrabass is bass fiddle, bass viol, bull fiddle, double bass, contrabass, and string bass.1 Each of the bigrams may be viewed as a decomposition of the unigram contrabass into parts (factors, aspects, constituents). For example, the bigram bass fiddle expresses that a contrabass is a type of fiddle that covers the bass range; the head noun fiddle expresses the type aspect of contrabass and the modifier bass expresses the range aspect of contrabass. Decomposition (unigram → bigram) maps a word (contrabass) to a (miniature) definition (bass fiddle), whereas composition (bigram → unigram) maps a definition to a word (by composing the components of the definition into a whole). Past work has achieved promising results on recognizing noun-modifier compositions and decompositions, given relatively short lists of candidates. For example, Turney (2013) achieves 75.",
      "startOffset" : 110,
      "endOffset" : 1247
    }, {
      "referenceID" : 23,
      "context" : "9% accuracy on noun-modifier composition recognition, given seven candidate unigrams for each target bigram, and Kartsaklis et al. (2012) achieve 24% accuracy on noun decomposition recognition, given seventy-two choices.",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 36,
      "context" : "Comp and Decomp are variations on the unsupervised learning algorithm of Turney (2012). Super is based on the supervised algorithm of Turney (2013).",
      "startOffset" : 73,
      "endOffset" : 87
    }, {
      "referenceID" : 36,
      "context" : "Comp and Decomp are variations on the unsupervised learning algorithm of Turney (2012). Super is based on the supervised algorithm of Turney (2013). These algorithms were originally designed for the recognition task.",
      "startOffset" : 73,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : "Madnani and Dorr (2010) and Androutsopoulos and Malakasiotis (2010) present thorough surveys of datadriven approaches to paraphrasing.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Madnani and Dorr (2010) and Androutsopoulos and Malakasiotis (2010) present thorough surveys of datadriven approaches to paraphrasing.",
      "startOffset" : 28,
      "endOffset" : 68
    }, {
      "referenceID" : 36,
      "context" : "We call this the holistic (non-compositional) approach to paraphrase (Turney, 2012), because the phrases are treated as opaque wholes.",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "Holistic approaches to paraphrase do not address the creative power of language (Chomsky, 1975; Fodor & Lepore, 2002).",
      "startOffset" : 80,
      "endOffset" : 117
    }, {
      "referenceID" : 24,
      "context" : "One of the earliest proposals for semantic composition is to represent the bigram ab by the vector sum a+ b (Landauer & Dumais, 1997). To measure the similarity of a noun-modifier phrase, ab, and a noun, c, we calculate the cosine of the angle between a+ b and the context vector c for c. This simple proposal actually works relatively well (Mitchell & Lapata, 2008, 2010), although it lacks order sensitivity. Since a+b = b+ a, animal farm and farm animal have the same representation, although one is a type of farm and the other is a type of animal. Landauer (2002) estimates that 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order, thus vector addition misses at least 20% of the meaning of a bigram.",
      "startOffset" : 109,
      "endOffset" : 569
    }, {
      "referenceID" : 24,
      "context" : "Kintsch (2001) and Utsumi (2009) propose variations of additive composition in which ab is represented by a+b+ ∑",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 24,
      "context" : "Kintsch (2001) and Utsumi (2009) propose variations of additive composition in which ab is represented by a+b+ ∑",
      "startOffset" : 0,
      "endOffset" : 33
    }, {
      "referenceID" : 36,
      "context" : "This approach does not scale up, but it does work well for a predetermined small set of high frequency n-grams (Turney, 2012).",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "For example, a regression model can be trained to map the context vectors a and b to the holistic context vector for ab (Guevara, 2010).",
      "startOffset" : 120,
      "endOffset" : 135
    }, {
      "referenceID" : 36,
      "context" : "Recently there have been several overviews of this topic (Mitchell & Lapata, 2010; Turney, 2012; Erk, 2013).",
      "startOffset" : 57,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "Recently there have been several overviews of this topic (Mitchell & Lapata, 2010; Turney, 2012; Erk, 2013).",
      "startOffset" : 57,
      "endOffset" : 107
    }, {
      "referenceID" : 41,
      "context" : "Most of the proposed extensions to distributional semantics involve operations from linear algebra, such as tensor products (Clark & Pulman, 2007; Widdows, 2008; Clark, Coecke, & Sadrzadeh, 2008; Grefenstette & Sadrzadeh, 2011).",
      "startOffset" : 124,
      "endOffset" : 227
    }, {
      "referenceID" : 27,
      "context" : "Mitchell and Lapata (2010) found that a simple additive model peformed better than an additive model that included neighbours.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : "Guevara (2010) and Baroni and Zamparelli (2010) point out that a small set of bigrams with holistic context vectors can be used to train a regression model.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "Guevara (2010) and Baroni and Zamparelli (2010) point out that a small set of bigrams with holistic context vectors can be used to train a regression model.",
      "startOffset" : 19,
      "endOffset" : 48
    }, {
      "referenceID" : 41,
      "context" : "Much work focuses on finding the right f for various types of semantic composition (Clark & Pulman, 2007; Widdows, 2008; Mitchell & Lapata, 2008, 2010; Guevara, 2010; Grefenstette & Sadrzadeh, 2011).",
      "startOffset" : 83,
      "endOffset" : 198
    }, {
      "referenceID" : 20,
      "context" : "Much work focuses on finding the right f for various types of semantic composition (Clark & Pulman, 2007; Widdows, 2008; Mitchell & Lapata, 2008, 2010; Guevara, 2010; Grefenstette & Sadrzadeh, 2011).",
      "startOffset" : 83,
      "endOffset" : 198
    }, {
      "referenceID" : 34,
      "context" : "Socher et al. (2011) combined context composition and similarity composition for sentence paraphrase recognition.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 34,
      "context" : "Socher et al. (2011) combined context composition and similarity composition for sentence paraphrase recognition. Unsupervised recursive autoencoders were used to compose context vectors and then a supervised softmax classifier was used to compose a similarity matrix. Turney (2012) introduced a dataset of 2180 semantic composition questions, split into 680 questions for training and 1500 for testing.",
      "startOffset" : 0,
      "endOffset" : 283
    }, {
      "referenceID" : 34,
      "context" : "Socher et al. (2011) combined context composition and similarity composition for sentence paraphrase recognition. Unsupervised recursive autoencoders were used to compose context vectors and then a supervised softmax classifier was used to compose a similarity matrix. Turney (2012) introduced a dataset of 2180 semantic composition questions, split into 680 questions for training and 1500 for testing.3 Table 1 shows one of the questions. The stem is the target noun-modifier bigram and there are seven candidate unigrams. These questions were generated automatically from WordNet. The stem and the solution always belong to the same WordNet synonym set. The intention is to evaluate a proposed similarity measure, sims(ab, c), by its accuracy on the 1500 testing questions. Using an unsupervised learning algorithm and a hand-built function for f , Turney (2012) achieved an accuracy of 58.",
      "startOffset" : 0,
      "endOffset" : 866
    }, {
      "referenceID" : 34,
      "context" : "Socher et al. (2011) combined context composition and similarity composition for sentence paraphrase recognition. Unsupervised recursive autoencoders were used to compose context vectors and then a supervised softmax classifier was used to compose a similarity matrix. Turney (2012) introduced a dataset of 2180 semantic composition questions, split into 680 questions for training and 1500 for testing.3 Table 1 shows one of the questions. The stem is the target noun-modifier bigram and there are seven candidate unigrams. These questions were generated automatically from WordNet. The stem and the solution always belong to the same WordNet synonym set. The intention is to evaluate a proposed similarity measure, sims(ab, c), by its accuracy on the 1500 testing questions. Using an unsupervised learning algorithm and a hand-built function for f , Turney (2012) achieved an accuracy of 58.3% on the 1500 testing questions. Vector addition reached 50.1% and element-wise multiplication attained 57.5%. Turney (2013) used supervised learning to",
      "startOffset" : 0,
      "endOffset" : 1019
    }, {
      "referenceID" : 36,
      "context" : "Table 1: A seven-choice noun-modifier question based on WordNet (Turney, 2012).",
      "startOffset" : 64,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "Table 2: Adjective-noun composition problems based on WordNet (Dinu et al., 2013).",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "Dinu et al. (2013) evaluated seven different models on their adjective-noun dataset.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : "Dinu et al. (2013) evaluated seven different models on their adjective-noun dataset. The performance of the models was measured by the medians of the ranks of the solution nouns in the ranked lists of 21,000 candidates. Our experiments (Sections 6 and 7) use the same general approach to evaluation of models. In a noun-modifier phrase, the modifier may be either a noun or an adjective; therefore adjective-noun phrases are a subset of noun-modifier phrases. Dinu et al. (2013) hypothesize that adjectives are functions that map nouns onto modified nouns (Baroni & Zamparelli, 2010), thus they believe that noun-noun phrases and adjective-noun phrases should have different kinds of models.",
      "startOffset" : 0,
      "endOffset" : 479
    }, {
      "referenceID" : 15,
      "context" : "Dinu et al. (2013) evaluated seven different models on their adjective-noun dataset. The performance of the models was measured by the medians of the ranks of the solution nouns in the ranked lists of 21,000 candidates. Our experiments (Sections 6 and 7) use the same general approach to evaluation of models. In a noun-modifier phrase, the modifier may be either a noun or an adjective; therefore adjective-noun phrases are a subset of noun-modifier phrases. Dinu et al. (2013) hypothesize that adjectives are functions that map nouns onto modified nouns (Baroni & Zamparelli, 2010), thus they believe that noun-noun phrases and adjective-noun phrases should have different kinds of models. The models we present here (Section 5) treat all noun-modifiers the same way, hence our datasets contain both noun-noun phrases and adjective-noun phrases. For comparison, we will also evaluate our models on Dinu et al.’s (2013) adjective-noun dataset (Section 6.",
      "startOffset" : 0,
      "endOffset" : 921
    }, {
      "referenceID" : 23,
      "context" : "Table 3: A sample of the dataset for the term-definition task (Kartsaklis et al., 2012).",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 22,
      "context" : "Kartsaklis et al. (2012) created a dataset with 72 target noun unigrams and 40 target verb unigrams.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 22,
      "context" : "Kartsaklis et al. (2012) created a dataset with 72 target noun unigrams and 40 target verb unigrams.4 Each target unigram has three gold standard definitions, where the definitions contain 2.4 words on average (the majority are bigrams). Table 3 shows the first four target terms and their corresponding definitions. Kartsaklis et al. (2012) treat each target term (unigram) as a class label and evaluate each model by its accuracy on classifying the definitions.",
      "startOffset" : 0,
      "endOffset" : 342
    }, {
      "referenceID" : 14,
      "context" : "Recently Dinu and Baroni (2014) partially addressed the task of decomposing a noun unigram into an adjective-noun bigram.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 36,
      "context" : "5 The two standard datasets are derived from the 2180 seven-choice noun-modifier questions used in previous work (Turney, 2012) to test composition recognition (see Table 1 in Section 2.",
      "startOffset" : 113,
      "endOffset" : 127
    }, {
      "referenceID" : 19,
      "context" : "This idea was inspired by Guevara (2010) and Baroni and Zamparelli (2010), who used holistic vectors to train regression models.",
      "startOffset" : 26,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "This idea was inspired by Guevara (2010) and Baroni and Zamparelli (2010), who used holistic vectors to train regression models.",
      "startOffset" : 45,
      "endOffset" : 74
    }, {
      "referenceID" : 36,
      "context" : "The features DS and FS were introduced in Turney (2012). These two features were supplemented with LUF and PPMI in Turney (2013).",
      "startOffset" : 42,
      "endOffset" : 56
    }, {
      "referenceID" : 36,
      "context" : "The features DS and FS were introduced in Turney (2012). These two features were supplemented with LUF and PPMI in Turney (2013). The feature LBF is a new addition to the group.",
      "startOffset" : 42,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "(Another approach to normalizing PPMI is given in Bouma (2009).)",
      "startOffset" : 50,
      "endOffset" : 63
    }, {
      "referenceID" : 36,
      "context" : "The general procedure for creating a PPMI matrix is described in detail in Turney and Pantel (2010). In the following experiments, we use the PPMI matrix from Turney, Neuman, Assaf, and Cohen (2011).",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 36,
      "context" : "The general procedure for creating a PPMI matrix is described in detail in Turney and Pantel (2010). In the following experiments, we use the PPMI matrix from Turney, Neuman, Assaf, and Cohen (2011).9 It is a word-context matrix in which the rows correspond to n-grams in WordNet and the columns correspond to unigrams from WordNet, marked left or right.",
      "startOffset" : 75,
      "endOffset" : 199
    }, {
      "referenceID" : 36,
      "context" : "The general procedure for creating a PPMI matrix is described in detail in Turney and Pantel (2010). In the following experiments, we use the PPMI matrix from Turney, Neuman, Assaf, and Cohen (2011).9 It is a word-context matrix in which the rows correspond to n-grams in WordNet and the columns correspond to unigrams from WordNet, marked left or right. There are approximately 114,000 rows in the matrix and 140,000 columns. The matrix has a density of about 1.2%. Let a be an n-gram in WordNet, b be a unigram in WordNet, and h be either left or right. Suppose that a corresponds to the i-th row in the matrix and b, marked with the handedness h, corresponds to the j-th column in the matrix. We define PPMI(a, b, h) as the value in the i-th row and j-th column of the matrix. This value is the normalized positive pointwise mutual information of observing b on the h side of a in the Waterloo corpus, where b is either immediately adjacent to a or separated from a by one or more stop words.10 Any word that is not in WordNet is treated as a stop word. If a does not correspond to a row in the matrix or b (marked h) does not correspond to column, then PPMI(a, b, h) is assigned the value zero. Turney et al. (2011) estimated PPMI(a, b, h) by sampling the Waterloo corpus for phrases containing a and then looking for b on the h side of a in the sampled phrases.",
      "startOffset" : 75,
      "endOffset" : 1220
    }, {
      "referenceID" : 36,
      "context" : "The following experiments use the domain matrix from Turney (2012).11 To make the domain matrix, Turney (2012) first constructed a frequency matrix, in which the rows correspond to n-grams in WordNet and the columns correspond to nouns that were observed near the row n-grams in the Waterloo corpus.",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 36,
      "context" : "The following experiments use the domain matrix from Turney (2012).11 To make the domain matrix, Turney (2012) first constructed a frequency matrix, in which the rows correspond to n-grams in WordNet and the columns correspond to nouns that were observed near the row n-grams in the Waterloo corpus.",
      "startOffset" : 53,
      "endOffset" : 111
    }, {
      "referenceID" : 36,
      "context" : "Turney et al. (2011) did not normalize PPMI in their experiments.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "The parameter p raises the singular values in Σk to the power p (Caron, 2001).",
      "startOffset" : 64,
      "endOffset" : 77
    }, {
      "referenceID" : 36,
      "context" : "Decreasing p has the effect of making the similarity measure more discriminating (Turney, 2012).",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 36,
      "context" : "Optimal performance requires tuning the parameters k and p for the task (Bullinaria & Levy, 2012; Turney, 2012).",
      "startOffset" : 72,
      "endOffset" : 111
    }, {
      "referenceID" : 36,
      "context" : "Optimal performance requires tuning the parameters k and p for the task (Bullinaria & Levy, 2012; Turney, 2012). For Comp and Decomp, we use the parameter settings given by Turney (2012). For Super, we generate features with a wide range of parameter settings and let the supervised learning algorithm decide how to use the features.",
      "startOffset" : 98,
      "endOffset" : 187
    }, {
      "referenceID" : 36,
      "context" : "The following experiments use the function matrix from Turney (2012).13 It is similar to the domain matrix, except the context is based on verbal patterns, instead of nearby nouns.",
      "startOffset" : 55,
      "endOffset" : 69
    }, {
      "referenceID" : 36,
      "context" : "Comp and Decomp use fast unsupervised algorithms to generate initial lists of candidates (Turney, 2012).",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 37,
      "context" : "Super uses a slower supervised algorithm to refine the initial lists from Comp or Decomp (Turney, 2013).",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 36,
      "context" : "The values of the first four parameters were copied from Turney (2012). The last parameter, maxu, was set to 2000, based on a small number of trials using the standard composition training dataset.",
      "startOffset" : 57,
      "endOffset" : 71
    }, {
      "referenceID" : 36,
      "context" : "These are the same features as in Turney (2013), except for the six new LBF features.",
      "startOffset" : 34,
      "endOffset" : 48
    }, {
      "referenceID" : 33,
      "context" : "Super uses the sequential minimal optimization (SMO) support vector machine (SVM) as implemented in Weka (Platt, 1998; Witten, Frank, & Hall, 2011).",
      "startOffset" : 105,
      "endOffset" : 147
    }, {
      "referenceID" : 33,
      "context" : "Super uses the sequential minimal optimization (SMO) support vector machine (SVM) as implemented in Weka (Platt, 1998; Witten, Frank, & Hall, 2011).15 The kernel is a normalized third-order polynomial. Weka provides probability estimates for the classes by fitting the outputs of the SVM with logistic regression models. After training, we apply Super to the testing dataset. For each testing target, we rank the 2000 candidates in descending order of the probability that they belong in class 1, as estimated by the SVM. Table 11 shows the parameter settings that we use for Super in the following experiments. The Super parameter values are copied from Turney (2013), except for ratio01, which was set based on a small number of trials using the standard composition training dataset.",
      "startOffset" : 106,
      "endOffset" : 669
    }, {
      "referenceID" : 25,
      "context" : "With vector addition, we score the triple 〈a, b, c〉 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a⊙ b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k.",
      "startOffset" : 114,
      "endOffset" : 538
    }, {
      "referenceID" : 25,
      "context" : "With vector addition, we score the triple 〈a, b, c〉 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a⊙ b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1000 and p set to −0.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements.",
      "startOffset" : 114,
      "endOffset" : 839
    }, {
      "referenceID" : 25,
      "context" : "With vector addition, we score the triple 〈a, b, c〉 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a⊙ b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1000 and p set to −0.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets × 2000 candidates per target = 702,000 testing dataset vectors).",
      "startOffset" : 114,
      "endOffset" : 1059
    }, {
      "referenceID" : 25,
      "context" : "With vector addition, we score the triple 〈a, b, c〉 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a⊙ b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1000 and p set to −0.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets × 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012).",
      "startOffset" : 114,
      "endOffset" : 1437
    }, {
      "referenceID" : 25,
      "context" : "With vector addition, we score the triple 〈a, b, c〉 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a⊙ b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1000 and p set to −0.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets × 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012). The mono matrix was formed by merging the domain and function matrices.",
      "startOffset" : 114,
      "endOffset" : 1525
    }, {
      "referenceID" : 25,
      "context" : "With vector addition, we score the triple 〈a, b, c〉 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a⊙ b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1000 and p set to −0.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets × 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012). The mono matrix was formed by merging the domain and function matrices. See Turney (2012) for details.",
      "startOffset" : 114,
      "endOffset" : 1616
    }, {
      "referenceID" : 25,
      "context" : "With vector addition, we score the triple 〈a, b, c〉 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a⊙ b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1000 and p set to −0.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets × 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012). The mono matrix was formed by merging the domain and function matrices. See Turney (2012) for details. We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1300 and p set to −0.5. Since Turney (2012) showed that the geometric mean of domain similarity and function similarity performed better than vector addition and element-wise multiplication on the noun-modifier composition recognition task, we decided to apply addition and multiplication to the output of Comp, instead of applying them to the full set of 73,000 unigrams.",
      "startOffset" : 114,
      "endOffset" : 1815
    }, {
      "referenceID" : 25,
      "context" : "With vector addition, we score the triple 〈a, b, c〉 using cos(a+b, c), the cosine of the angle between a+b and c (Landauer & Dumais, 1997). With element-wise multiplication, the triple is scored by cos(a⊙ b, c) (Mitchell & Lapata, 2008, 2010). With the holistic approach, we treat ab as a pseudo-unigram. Let d be the context vector for the pseudo-unigram a b. We score the triple using cos(d, c). For vector addition, we used the domain matrix (see Section 4.4), since this matrix had the best performance with addition in Turney (2012). We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1000 and p set to −0.1. For element-wise multiplication, we used the PPMI matrix (see Section 4.3), since this matrix has no negative elements. Turney (2012) pointed out that element-wise multiplication is not suitable when the vectors contain negative elements. The DS and FS matrices contain negative elements, due to the truncated singular value decomposition. Turney (2012) suggested a modified form of element-wise multiplication, to address this issue, but we found that it did not scale up to the number of vectors we have in our testing dataset (351 targets × 2000 candidates per target = 702,000 testing dataset vectors). With the PPMI matrix, there are no parameters to tune. For the holistic approach, we used the mono matrix from Turney (2012), since this matrix had the best performance with the holistic approach in Turney (2012). The mono matrix was formed by merging the domain and function matrices. See Turney (2012) for details. We used the training dataset to optimize the parameters k and p of the smoothed matrix, UkΣ p k. The best results were obtained with k set to 1300 and p set to −0.5. Since Turney (2012) showed that the geometric mean of domain similarity and function similarity performed better than vector addition and element-wise multiplication on the noun-modifier composition recognition task, we decided to apply addition and multiplication to the output of Comp, instead of applying them to the full set of 73,000 unigrams. The intention was to give vector addition and element-wise multiplication the benefit of preprocessing by Comp. On the other hand, Turney (2012) found that the holistic approach was more accurate than all other approaches; therefore we applied the holistic approach to the whole set of 73,000 unigrams, with no preproessing by Comp.",
      "startOffset" : 114,
      "endOffset" : 2289
    }, {
      "referenceID" : 20,
      "context" : "Past work with adjective-noun bigrams has shown that we can use holistic bigram vectors to train a supervised regression model (Guevara, 2010; Baroni & Zamparelli, 2010).",
      "startOffset" : 127,
      "endOffset" : 169
    }, {
      "referenceID" : 20,
      "context" : "Past work with adjective-noun bigrams has shown that we can use holistic bigram vectors to train a supervised regression model (Guevara, 2010; Baroni & Zamparelli, 2010). Turney (2013) adapted this approach for supervised classification applied to composition recognition.",
      "startOffset" : 128,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "use Daumé III’s (2007) domain adaptation algorithm for training.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "use Daumé III’s (2007) domain adaptation algorithm for training. This algorithm allows us to train Super with both the standard training dataset and the holistic training dataset. Daumé III’s (2007) algorithm is a general strategy for merging datasets that have somewhat different statistical distributions of class labels, such as our standard and holistic datasets.",
      "startOffset" : 4,
      "endOffset" : 199
    }, {
      "referenceID" : 13,
      "context" : "use Daumé III’s (2007) domain adaptation algorithm for training. This algorithm allows us to train Super with both the standard training dataset and the holistic training dataset. Daumé III’s (2007) algorithm is a general strategy for merging datasets that have somewhat different statistical distributions of class labels, such as our standard and holistic datasets. Table 16 shows that the model learned from standard training (with WordNet synonym sets) carries over well to holistic testing. Focusing on percent in top 100, standard training achieves 92.3% on holistic testing, which is not significantly different from the 93.7% obtained with holistic training (Fisher’s Exact Test at the 95% confidence level). On the other hand, the model learned from holistic training (without WordNet synonym sets) does not carry over well to standard training. Holistic training only achieves 64.4% on standard testing, whereas standard training achieves 77.8%, which is significantly higher. Table 16 suggests that there is not much benefit to merging the holistic and standard training datasets with Daumé III’s (2007) domain adaptation algorithm.",
      "startOffset" : 4,
      "endOffset" : 1115
    }, {
      "referenceID" : 15,
      "context" : "Here we experiment with the 620 adjective-noun phrases of Dinu et al. (2013) (see Table 2 in Section 2.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : "Table 18: Performance of models on the adjective-noun dataset, from Li et al. (2014).",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "Mult is element-wise multiplication (Mitchell & Lapata, 2008) with powers as weights (Dinu et al., 2013).",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : "Fulladd multiplies each vector by a weight matrix and then adds the resulting weighted vectors (Guevara, 2010).",
      "startOffset" : 95,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "The 620 adjective-noun phrases were introduced by Dinu et al. (2013), but they were used more recently, and with better results, by Li, Baroni, and Dinu (2014).",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "The 620 adjective-noun phrases were introduced by Dinu et al. (2013), but they were used more recently, and with better results, by Li, Baroni, and Dinu (2014). Table 18 is a copy of their results (this is Table 3 in their paper).",
      "startOffset" : 50,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "The 620 adjective-noun phrases were introduced by Dinu et al. (2013), but they were used more recently, and with better results, by Li, Baroni, and Dinu (2014). Table 18 is a copy of their results (this is Table 3 in their paper). They evaluated seven different models on the adjective-noun dataset. The reduction column indicates the algorithm used to reduce the dimensionality of the matrix, nonnegative matrix factorization (NMF) or singular value decomposition (SVD). The dim column indicates the number of factors in the dimensionality reductions. The rank column gives the median rank of the solution in the ranked list of candidates. In Table 18, Add is vector addition with weights (Mitchell & Lapata, 2008). Dil is the dilation model introduced by Mitchell and Lapata (2008). Mult is element-wise multiplication (Mitchell & Lapata, 2008) with powers as weights (Dinu et al.",
      "startOffset" : 50,
      "endOffset" : 784
    }, {
      "referenceID" : 27,
      "context" : "Enetlex is a lexical function model, like Lexfunc, where the model is trained using elastic-net regression (Li et al., 2014).",
      "startOffset" : 107,
      "endOffset" : 124
    }, {
      "referenceID" : 27,
      "context" : "In order to compare our results on this dataset with the results of Li et al. (2014), we need to make two adjustments to our experimental setup.",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 27,
      "context" : "In order to compare our results on this dataset with the results of Li et al. (2014), we need to make two adjustments to our experimental setup. First, we consider a vocabulary of 73,000 unigrams, but Li et al. (2014) consider 21,000 nouns.",
      "startOffset" : 68,
      "endOffset" : 218
    }, {
      "referenceID" : 27,
      "context" : "In order to compare our results on this dataset with the results of Li et al. (2014), we need to make two adjustments to our experimental setup. First, we consider a vocabulary of 73,000 unigrams, but Li et al. (2014) consider 21,000 nouns. For a fair comparison, we restrict Comp to the same 21,000 nouns. FilterComp is a modified version of Comp that filters the output of Comp to remove any unigrams that are not among the 21,000 nouns of Li et al. (2014). Second, we only calculate the mean and median rank for the targets that have solutions among the top 2000 candidates, because Super does not rank the other candidates.",
      "startOffset" : 68,
      "endOffset" : 459
    }, {
      "referenceID" : 27,
      "context" : "In order to compare our results on this dataset with the results of Li et al. (2014), we need to make two adjustments to our experimental setup. First, we consider a vocabulary of 73,000 unigrams, but Li et al. (2014) consider 21,000 nouns. For a fair comparison, we restrict Comp to the same 21,000 nouns. FilterComp is a modified version of Comp that filters the output of Comp to remove any unigrams that are not among the 21,000 nouns of Li et al. (2014). Second, we only calculate the mean and median rank for the targets that have solutions among the top 2000 candidates, because Super does not rank the other candidates. Since Li et al. (2014) use a single pass approach, they rank all of the candidates.",
      "startOffset" : 68,
      "endOffset" : 651
    }, {
      "referenceID" : 13,
      "context" : "Here we see some benefit to merging the two training datasets, using Daumé III’s (2007) domain adaptation algorithm (see the columns labeled both).",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : "Given the good performance of the holistic baseline, it is natural to consider using holistic datasets as a relatively inexpensive way to train a supervised system (Guevara, 2010; Baroni & Zamparelli, 2010; Turney, 2013).",
      "startOffset" : 164,
      "endOffset" : 220
    }, {
      "referenceID" : 37,
      "context" : "Given the good performance of the holistic baseline, it is natural to consider using holistic datasets as a relatively inexpensive way to train a supervised system (Guevara, 2010; Baroni & Zamparelli, 2010; Turney, 2013).",
      "startOffset" : 164,
      "endOffset" : 220
    }, {
      "referenceID" : 33,
      "context" : "To achieve this goal, we propose to adapt the dynamic pooling approach introduced by Socher et al. (2011). Eventually, we hope to be able to move beyond sentence paraphrase recognition to sentence paraphrase generation.",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 33,
      "context" : "To achieve this goal, we propose to adapt the dynamic pooling approach introduced by Socher et al. (2011). Eventually, we hope to be able to move beyond sentence paraphrase recognition to sentence paraphrase generation. Turney (2013) has demonstrated that the features we have used here (Section 4) work well for recognizing semantic relations; for example, we can recognize that the semantic relation between mason and stone is analogous to the semantic relation between carpenter and wood.",
      "startOffset" : 85,
      "endOffset" : 234
    }, {
      "referenceID" : 15,
      "context" : "Following the example of Dinu et al. (2013) and Li et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "Following the example of Dinu et al. (2013) and Li et al. (2014), we have extended distributional models of composition from the task of recognition to the task of generation.",
      "startOffset" : 25,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "Following the example of Dinu et al. (2013) and Li et al. (2014), we have extended distributional models of composition from the task of recognition to the task of generation. Our results on their adjective-noun dataset suggest that similarity composition is a better approach to generating compositions than context composition (Section 6.4). The experiments with holistic training support the hypothesis that the models of Dinu et al. (2013) and Li et al.",
      "startOffset" : 25,
      "endOffset" : 444
    }, {
      "referenceID" : 15,
      "context" : "Following the example of Dinu et al. (2013) and Li et al. (2014), we have extended distributional models of composition from the task of recognition to the task of generation. Our results on their adjective-noun dataset suggest that similarity composition is a better approach to generating compositions than context composition (Section 6.4). The experiments with holistic training support the hypothesis that the models of Dinu et al. (2013) and Li et al. (2014) are limited by their reliance on holistic pseudo-unigram training (Section 6.",
      "startOffset" : 25,
      "endOffset" : 465
    }, {
      "referenceID" : 13,
      "context" : "The results with Daumé III’s (2007) domain adaptation algorithm (Section 7.",
      "startOffset" : 17,
      "endOffset" : 36
    } ],
    "year" : 2014,
    "abstractText" : "Semantic composition is the task of understanding the meaning of text by composing the meanings of the individual words in the text. Semantic decomposition is the task of understanding the meaning of an individual word by decomposing it into various aspects (factors, constituents, components) that are latent in the meaning of the word. We take a distributional approach to semantics, in which a word is represented by a context vector. Much recent work has considered the problem of recognizing compositions and decompositions, but we tackle the more difficult generation problem. For simplicity, we focus on noun-modifier bigrams and noun unigrams. A test for semantic composition is, given context vectors for the noun and modifier in a noun-modifier bigram (red salmon), generate a noun unigram that is synonymous with the given bigram (sockeye). A test for semantic decomposition is, given a context vector for a noun unigram (snifter), generate a noun-modifier bigram that is synonymous with the given unigram (brandy glass). With a vocabulary of about 73,000 unigrams from WordNet, there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a unigram. We generate ranked lists of potential solutions in two passes. A fast unsupervised learning algorithm generates an initial list of candidates and then a slower supervised learning algorithm refines the list. We evaluate the candidate solutions by comparing them to WordNet synonym sets. For decomposition (unigram to bigram), the top 100 most highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time. For composition (bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given bigram 77.8% of the time.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}