{
  "name" : "1509.04385.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "KANNADA NAMED", "(MNB) CLASSIFIER", "S Amarappa" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "DOI: 10.5121/ijnlc.2015.4404 39\ntext and classification of those nouns into certain predefined categories like person name, location, organization, date, and time etc. NERC in Kannada is an essential and challenging task. The aim of this work is to develop a novel model for NERC, based on Multinomial Naïve Bayes (MNB) Classifier. The Methodology adopted in this paper is based on feature extraction of training corpus, by using term frequency, inverse document frequency and fitting them to a tf-idf-vectorizer. The paper discusses the various issues in developing the proposed model. The details of implementation and performance evaluation are discussed. The experiments are conducted on a training corpus of size 95,170 tokens and test corpus of 5,000 tokens. It is observed that the model works with Precision, Recall and F1-measure of 83%, 79% and 81% respectively.\nKEYWORDS Named Entity Recognition, Named Entity Classification, Multinomial Naïve-Bayes Classifier, Information Extraction, Training-set, Development-set, Test-set"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "India is rich with more than 1,652 mother tongues, out of which 22 are Scheduled Languages included in the Constitution of India. Among the 22 Scheduled Languages, Kannada is one of the major Dravidian languages of India, spoken predominantly in the state of Karnataka. The Karnataka Official Language Act 1963 recognized Kannada as its official language. The native speakers of Kannada are roughly 40 million, making it the 33rd most spoken language in the world. The language uses forty-nine phonemic letters, and the character set is almost identical to that of other Indian languages. Kannada is highly agglutinating and inflected language. It is a free word order language with rich heritage and large grammar. Processing of Kannada language and extraction of named entities is challenging. This language is inflected with three genders (masculine, feminine, and neutral) and two numbers (singular and plural). The Noun is inflected by various factors such as case, number and gender.\nNatural Language Processing (NLP) has two major tasks: Natural Language Understanding (NLU) and Natural Language Generation (NLG) Liddy (2001) [1]. NLU deals with machine reading comprehension i.e., the level of understanding of a text or message. NLG is the task of generating natural language from a machine representation system such as a knowledge base.\nApart from NLG and NLU, the other tasks to be done in NLP include automatic summarization, Information Extraction (IE), Information Retrieval (IR), Named Entity Recognition (NER) etc. In NLP, the primary goal of IE and IR is to automatically extract structured information. NER is a typical subtask of IE James Allen (2006) [2]. NERC involves processing of structured and unstructured documents and identifying proper names that refer to persons, organizations locations (cities, countries, rivers, etc), date, time etc. The aim of NERC is to automatically extract proper names which is useful to address many problems such as machine translation, information extraction, information retrieval, question answering, and automatic text summarization etc., Kavi N M (2006) [3]. In this paper, Kannada NERC based on MNB approach is dealt. The results obtained from the proposed model are quite encouraging with an average accuracy of 83%. What follows are the details of the proposed research work. Section 2 discusses about the details of existing work and challenges in current work. Section 3 deals with Naïve Bayes classifier principles, the technique used for NERC. Proposed methodology is dealt in section 4. Section 5 discusses implementation details. Finally the results are evaluated and discussed in section 6."
    }, {
      "heading" : "2. EXISTING WORK AND CHALLENGES IS CURRENT WORK",
      "text" : "The NLP work was started way back in the 1940s. From 1940 to 1980, NLP systems were based on complex sets of hand-made rules. After 1980 NLP took new dimension, with machine learning algorithms. There is an enormous amount of data available for languages like English, but for Indian languages it is at the initial stage. Recent NLP algorithms are based on statistical machine learning. The term Named Entity was introduced in the sixth Message Understanding Conference (MUC-6), Gobinda Chowdhury (2003) [4]. The different techniques for addressing the NERC problem include: Maximum Entropy Models (Max-Ent) (Jaynes 1957), Hidden Markov Models (HMM) (Baum et al. 1966), Support Vector Machines (SVM) (Vapnik et al. 1992), Decision Trees (Sekine 1998) and Conditional Random Fields (CRF) (Lafferty et al. 2003) etc.\nA lot of NLP work has been done in English, most of the European languages, some of the Asian languages like Chinese, Japanese, Korean and other foreign languages like Spanish, Arabic etc. NLP research in Indian languages is at the initial stage, as annotated corpora and other lexical resources have started appearing recently. In Computational Linguistics, Kannada is lagging far behind when compared to other Indian languages. In the following sections we are mentioning, a brief survey of research on NERC in Indian languages including Kannada. This is not a comprehensive and thorough survey, but is an indication of today’s status in NERC research.\nBenajiba et al. (2009) [5] discussed about SVM based Language Independent and Language Specific Features to Enhance Arabic NER. Padmaja et al. (2010) [6] discussed on the first steps towards Assamese NER. Asif Ekbal et al. (2008) [7] developed an NER system in Bengali using CRF approach. Ekbal and Shivaji (2008) [8] reported about the development of a NER system for Bengali using SVM. Ekbal and Shivaji (2008) [9] discussed about Bengali Named Entity Tagged Corpus and its use in NER Systems.\nA Lot of work on NERC has been done in English language and here are quoted a few recent works. Maksim et al. (2012) [10] build a CRF based system that achieves 91.02% F1-measure on the CoNLL 2003 (Sang and Meulder, 2003) dataset and 81.4% F1-measure on the Onto Notes version 4 (Hovy et al., 2006) CNN dataset. Mansouri et al. (2008) [11] proposed a robust and novel Machine Learning based method called Fuzzy support Vector Machine (FSVM) for NER. Nadeau et al. (2006) [12] devised an unsupervised NER by Generating Gazetteers and Resolving\nAmbiguity. Monica et al. (2009) [13] discussed about the evaluation of Named Entity Extraction Systems. Sujan et al. (2008) [14] developed system for NER in Hindi using Max-Ent and Transliteration. In Li and McCallum (2004) [15] the authors have used CRF with feature induction to the Hindi NER task. Sujan et al. (2008) [16] developed a NER system for Hindi using Max-Ent. Sudha and Nusrat (2013) [17] experimented, NER using HMM on Hindi, Urdu and Marathi Languages. Deepti and Sudha (2013) [18] deviced algorithm for the Detection and Categorization of Named Entities in Indian Languages using HMM. Nusrat et al. (2012) [19] devised algorithm for NER in Indian Languages using Gazetteer method and HMM. S. Biswas et al. (2010) [20] developed a Two Stage Language Independent NER for Hindi, Oriya, Bengali and Telugu. Animesh et al. (2008) [21] talked about a new approach to recognize named entities for Indian languages. Sujan et al. (2008) [22] described a hybrid system that applies Max-Ent, language specific rules and gazetteers to the task of NER in Indian languages. Erik and Fien (2003) [23] gave Introduction to the CoNLL-2003 Shared Task a Language-Independent NER. Ekbal and Shivaji (2010) [24] reported about the development of a language independent NER system for Bengali and Hindi using SVM. Ekbal et al. (2008) [25] developed Language Independent NER system for South and South East Asian languages, particularly for Bengali, Hindi, Telugu, Oriya and Urdu as part of the IJCNLP-08 NER Shared Task1.\nKishorjit et al. (2011) [26] developed a model on CRF Based NER in Manipuri. Thoudam et al. (2009) [27] developed NER for Manipuri using SVM. Sitanath et al. (2010) [28] described a hybrid system that applies Max-Ent model with HMM and some linguistic rules to recognize Name Entities in Oriya language. Vishal and Gurpreet (2011) [29] explained about the NER System for Punjabi language text summarization using a Condition based approach. Pandian et al. (2008) [30] presented the construction of a hybrid, three stage NER for Tamil. Raju et al. (2008) [31] described a Max-Ent NER system for Telugu. Vijayanand and Seenivasan (2011) [32] devised NER and Transliteration for Telugu. Srikanth and KN Murthy (2008) [33] developed NER for Telugu using CRF based Noun Tagger. Praneeth et al. (2008) [34] conducted experiments in Telugu NER using CRF.\nShambhavi et al. (2012) [35] developed A Max-Ent model to Kannada Part Of Speech Tagging. Ramasamy et al. (2011) [36] proposed and developed a rule based Kannada Morphological Analyzer and Generator (MAG) using finite state transducer. Amarappa and Sathyanarayana [37] (2013) developed a HMM based system for NERC in Kannada Language.\nBased on the survey, it is observed that a lot of work on NERC has been done in English and other foreign languages. NERC work in Indian languages is still in its initial stage. As for as Indian languages are concerned, some works related to NERC are found in Hindi, Bengali, Telugu, Tamil, Oriya, Manipuri, Punjabi, Marathi and Assamese Languages. In Kannada Language, some works on Kannada Morphology are reported in [35] [56]. In our earlier work [37] we have carried out NERC work in Kannada using HMM on a limited corpus of 10,000 tokens, however the works on NERC in Kannada are yet to be investigated and implemented. This motivated us to take up NERC in Kannada as the proposed Research area."
    }, {
      "heading" : "2.1 Challenges and Issues specific to Kannada language",
      "text" : "Kannada language has no capitalization. It is Brahmi script with high phonetic characteristics that could be utilized by NERC system. There is non-availability of large gazetteer, lack of standardization and spelling. There are a number of frequently used words (common nouns),\nwhich can also be used as names. There is a lack of annotated data and it is highly agglutinating and inflected language."
    }, {
      "heading" : "3. MULTINOMIAL NAÏVE BAYES (MNB) CLASSIFIER",
      "text" : "The Naïve Bayes classifier is a Generative Model of supervised learning algorithms. It is a simple probabilistic classifier which is based on Bayes’ theorem with strong and naïve independence assumptions between every pair of features. It is one of the most basic classifier used for text classification. Moreover, the training time with Naïve Bayes is significantly smaller as opposed to alternative methods such as Support Vector Machine (SVM) and Maximum Entropy (Max-Ent) classifiers. Naïve Bayes classifier is superior in terms of CPU and memory consumption as shown by Huang, J. (2003). Its performance is very close to SVM and Max-Ent classifiers. The Multinomial Naïve Bayes classifier is suitable for classification with discrete features. The multinomial distribution normally requires integer feature counts; however, fractional counts such as Term Frequency and Inverse Document Frequency (tf-idf) will also work. Multinomial Naïve Bayes classifier is based on the Naïve Bayes algorithm. In order to find the probability for a label, this algorithm uses the Bayes rule to express P (label | features) in terms of P (label) and P (features | label). The Naïve Bayes classifier requires training data samples in the format: (xi, yi) where, xi includes the contextual information of the word/document (the sparse array) and yi, its class. Graphical representation of Naïve Bayes decoder is shown in Figure1. Here fi is ith feature of vocabulary (vi = xi) and P (fi |yj) = P (xi = vi | yj) is the maximum probability that the input xi belongs to the class yj.\nGiven a class variable y and a dependent feature vector x1 through xn, Bayes’ theorem states the following relationship of Joint Probability: ( , ) ( , )\n( ) ( | ) ( ) ( | )\nP X Y P Y X\nP X P Y X P Y P X Y\n\n   (1)\n( ) ( | ) ( | ) ( ) ( | )\n( )\nP Y P X Y P Y X P Y P X Y\nP X\n    (2)\n( ) ( | ) ( | ) ( ) ( | )\n( )\nP X P Y X P X Y P X P Y X\nP Y\n    (3)\nEquation (2) can be alternatively written as:\n1 2\n1 2\n1 2\n( ) ( , ,..., | ) ( | , ,..., )\n( , ,..., )\nj n j\nj n\nn\nP y P x x x y P y x x x\nP x x x\n  (4)\nUsing the Naïve independence assumption 1 1 1 ( | , ,...., , ,..., ) ( | )i j i i n i jP x y x x x x P x y   for all i, Equation (4) is simplified to:\n1 1 2\n1 2\n( ) ( | )\n( | , ,..., ) ( , ,..., )\nn\nj i j\ni j n\nn\nP y P x y\nP y x x x P x x x\n\n  \n(5)\nSince 1 2 ( , ,..., ) nP x x x is constant for given input, we can use the following classification rule:\n1 2\n1\n1\n( | , ,..., ) ( ) ( | )\narg max ( | )\nn\nj n j i j\ni\nn\ni j\nj i\nP y x x x P y P x y\ny P x y y Y\n\n\n \n\n \n\n\n(6)\nP (xi | yj ) is the relative frequency of class yj in the training set. y is the Maximum probability of generating instance xi for the given class yj."
    }, {
      "heading" : "4. PROPOSED WORK AND METHODOLOGY",
      "text" : "The main aim of this work is to develop a Supervised Statistical Machine Learning NERC system for Kannada Language based on MNB classifier. NERC involves identification of proper names in texts, and classification of those names into a set of pre-defined categories of interest such as: Person names (names of people), Organization names (companies, government organizations, committees, etc.), Location names (cities, countries etc.), and miscellaneous names (date, time, number, percentage, monetary expressions, number expressions and measurement expressions). The functional block diagram of the proposed system is as shown in Figure2. NERC in Kannada is important, because it gives solution to many applications of NLP such as web searching, scanning a set of documents written in a natural language and populating the database, building of useful dictionaries, constructing sophisticated word processors for Natural Languages, Information Extraction, Information Retrieval, Data mining, Publishing Books of Names, Places, Organizations etc. With the above context, the proposed system is designed.\nThe design of proposed the system and methodology goes as follows:\n1. Corpus creation and usage Kannada NERC is very hard without tagged Corpora and hence we manually tagged about 100K Kannada words. This Kannada Corpus is used to build the NERC Model. The manually tagged Corpus include: Part of EMILLE (Enabling Minority Language Engineering) corpus, a part of the corpus taken from web articles and part of the corpus self typed from Kannada books. The whole corpus is divided into two sets: Development-set and Test-set as shown in Figure2. First, select the Development-set and then subdivide it into Training-set and development test set (Dev-testset). The Training-set is used to train the model, and the Dev-test-set is used to perform error analysis. The Test-set serves for the final evaluation of the system. The machine learning used in the work is fully supervised MNB.\nTake Training data from the development-set: Input (training) data points : 1 2 3 MX = [X , X , X , ..., X ] ;\nOr : 11 21 12 22 1M 2MX= array ([[x , x ], [x , x ], ... [x , x ]])\nLabels (states) : th 1 2 3 N jY = [y , y , y , ..., y ] ; y = j label\n2. Pre-processing stage Here, the tagged text corpus is tokenized into words and tags (labels). The separated words are 1 2 3 MX = [w , w , w , ..., w ] and separated tags (labels) are 1 2 3 NY = [l , l , l , ..., l ] .\n3. Training stage for the model\n From the separated words (X) and tags (Y) in pre-processing stage, extract the vocabulary (feature set) and unique labels. Let vocabulary be 1 2 3 kV = [v , v , v , ..., v ] or 1 2 3 kF = [f , f , f , ..., f ] and unique labels be 1 2 3 j Y = [l , l , l , ..., l ] (here M reduces to k and N reduces to j)\n Find raw count of each vocabulary i.e., term frequencies (tf) according to their label.\n The term-frequency is a measure of how many times a particular term of V(t), is present in the documents d1,d2, . dM. The term-frequency is defined as a counting\nfunction: ( , ) ( , )\nx t tf t d fr x t   (8)\nwhere ( , )fr x t is a simple function, defined as:\n1, if x= t0, otherwise( , ) = fr x t\n(9)\nThe ( , )tf t d returns count of t in document d and it can be shown in matrix form:\n( )trainD XM M (10)\n Find inverse document frequency ( )idf t of training corpus defined by the function\n : ( | ) d t d p t d\nD\n \n, so idf is defined as:\n \n1 log ( | ) log log\n( | ) :\nD idf p t d\np t d d t d     \n \n1 ( ) ln 1\n1 :\nD idf t\nd t d\n    \n   \n(11)\nHere  :d t d is the number of documents where the term t appears; when the\nterm-frequency function satisfies ( , ) 0tf t d  . It should be noted that adding 1 into\nthe formula above avoids zero division.\n Now to find tf-idf use the following steps:\n( ) ( , ) ( )tf idf t tf t d idf t   (12)\n Find idf for each feature present in the feature matrix with the term frequency and idf weights can be represented by a vector as given by\n 1 2 3( ), ( ), ( ),...., ( )ktrainidf idf t idf t idf t idf t (13)\n tf-idf matrix of training set in un-normalized form:\nNow the tf matrix, trainD X M M   of equation (10) and the idf matrix\nidftrain idf M of equation (13) are multiplied to calculate the tf-idf weights. (Among M documents i number of documents are taken for training)  And then multiply \uD835\uDC40\uD835\uDC56\uD835\uDC51\uD835\uDC53 to the term frequency matrix, so the final result can be defined as:\n tf idf train idfi ki k k kM M M         \n(14)\n tf-idf matrix of Training-set in normalized form:\n2\ntf idf\ntf idf\ntf idf\nM M\nM\n\n\n\n\n(15)\n tf-idf vectors are the actual trained parameters of the MNB model (Scikit-learn version 0.14 documentation).\n4. Validation stage: Reserve a fold of the annotated training data for the Dev-test-set. Perform multiple evaluations on different Dev-test-sets and combine the scores from those evaluations. (http://www.nltk.org/book/ch06.html). Take a fold of the annotated training data as Dev-testset from the Development-set and perform the following computations:\n Pre-processing and tf-idf computation.\n Compute the probability: 1\narg max ( | ) ( | ) n\ni\nj i\nP X Y P x Y y Y\n\n  \nas shown in Figure1.\n5. Test(decoding) stage: Take test data from the corpus set and do the following computations:\n Pre-processing and tf-idf computation\n Compute the probability: 1\narg max ( | ) ( | ) n\ni\nj i\nP X Y P x Y y Y\n\n  "
    }, {
      "heading" : "5. IMPLEMENTATION",
      "text" : "The proposed system is designed based on MNB classification as discussed in section 4. The proposed model is as shown in Figure2. The system is programmed using Python 2.7 and Sklearn toolkit. The program is executed on windows platform using Intel Core2Duo CPU @3.00 GHz, a state of the art machine. The following are the various steps of implementation.\n1. Kannada Baraha editor is used to manually create named entity tagged Kannada corpus in UTF8 encoding format. 2. Twenty-two Named Entities (NEs) tabulated in Table1 are considered. A non-named entity is tagged as NONE. 3. From the tagged corpus, separate words, tags and store in separate lists. For the separated tags assign appropriate tag-labels. 4. Separated words and tag-labels are fed as Training-set to the MNB Model. In the training stage, MNB extracts features from the training corpus, such as vocabulary words, tf matrix, idf\nmatrix and tf-idf matrix.\n5. Test-set sequence is given as input to the model. 6. The features of the Test-set sequence are calculated and compared with the trained features.\nAccordingly each word is tagged with appropriate Named Entities (NEs).\n7. Model performance is evaluated by finding Precession, Recall, and F1-measure. 8. Evaluated parameters are tabulated and plotted for better comparison.\nThe Algorithm Gives The Implementation Procedure Of The MNB Model.\nAlgorithm: 1. Import tools for MNB implementation from python libraries. 2. Read the Development-set of tagged Kannada corpus and divide into N folds (N=10). 3. Reserve one fold of the tagged data as the Dev-test-set. 4. Take all other tagged corpus folds for Training, separate words, tags and store in two lists,\nx_train, y_train_tag_list. Assign label to each tag and store in another list, y_train.\n5. Feature extraction and training of MNB classifier Training data: x_train , Y_train from step5\n Define vectorizer by the statement\nvectorizer = TfidfVectorizer (min_df=1, ngram_range =(1,2), stop_words='english', strip_accents='unicode', norm='l2')\n Read system time t0\n Transform x_train to vectors by using X_train = vectorizer.fit_transform(x_train)\n Define MNB classifier using clf = MultinomialNB()\n Train MNB classifier using mnb_classifier = clf.fit (X_train, y_train)\n Read system time now and determine training time,\nTrain_-time = time () - t0\n Print Feature extraction and Training time of MNB classifier in seconds. 6. Take reserved fold of the Dev-test-set, separate words, tags and store in two lists, x_test,\nactual_tag_list. Assign tag labels and store in another list called y_test.\n7. validation of MNB model and Feature extraction of Dev-test-set using MNB classifier: Dev-test-set : x_test : y_test from step7\n Read system time t0\n Transform x_test to vectors by using X1_test = vectorizer.fit_transform(x_test)\n Predict labels of X_test by using the statement mnb_classifier.predict (X1_test)\n Read system time now and determine testing time,\nFold_Test_time = time () - t0 8. Attach predicted tags to the Dev-test-set words and print the resultant tagged sequence. 9. Find metrics and print evaluation metrics: Accuracy, Precision, Recall, and F1-measure. 10. Print classification report: individual Precision, Recall, F1_measure and average values. 11. Repeat steps 4 to 11 for all the N folds of Dev-test-sets, and then combine the scores from\nthose evaluations. This technique is known as cross-validation.\n12. Take Test-set, tokenize and store in a list called x_test, find tags manually and store in a list called actual_tag_list and assign tag labels and store in another list called y_test. 13. Testing of MNB model and Feature extraction of test data using MNB classifier Test data: x_test : y_test from step11\n Read system time t0\n Transform x_test to vectors by using X1_test = vectorizer.fit_transform(x_test)\n Predict labels of X_test by using the statement mnb_classifier.predict (X1_test) 14. Read system time now and determine testing time,\nTest-time = time () - t0 15. Attach predicted tags to the Test-set words and print the resultant tagged sequence. 16. Find metrics: Accuracy, Precision, Recall, and F1-measure. 17. Print classification report: individual P, R, F1_mes and average values. Plot graphs for the visualization of evaluated results."
    }, {
      "heading" : "6. RESULTS AND DISCUSSIONS",
      "text" : "The proposed system is designed and implemented as discussed in sections 4 and 5. The system is tested using several test cases, containing training corpus of size 95,170 tokens and test corpus of 5,000 tokens. It is to be noted that the system achieves an average accuracy of 83%. The details of the results obtained are as given below. The system’s performance is measured in terms of Precision (P), Recall (R) and F1-measure (F1). The details of the Corpus created in this work are given in section 4. The nature of input Test-set sequence and output tagged sequence are given in Table2 and Table3 respectively. The corpus size and program run time is tabulated in Table4. Table5 shows the results of 10 fold cross validation where validation fold is of size 9,517 tokens. Table 6 Indicates The Final Results Of Test-Set Corpus. Graphical Representation Of MNB Results On Test-Set Corpus Is Plotted As In Figure3.\n9 0.83 0.83 0.83 9517\n10 0.87 0.81 0.83 9517\nAverage / Total 78.6 % 77.2% 77.2% 95170"
    }, {
      "heading" : "7. CONCLUSION",
      "text" : "Natural Language Processing is an important research area containing challenging issues to be investigated. NERC is a class of NLP which is used for extracting named entities from unstructured data. In this context this paper focuses on NERC in Kannada Language as it is found that little work is done in this area. In this direction, we have conducted a vast survey in the related area of NLP and based on the survey we proposed a problem and the methodology that has been formulated. Various Modeling techniques are investigated, out of which a design of Supervised MNB is done. The results obtained are recorded and evaluated. We developed an efficient model and trained on a Corpus consisting of around 95170 words. From this training Corpus, some test samples are chosen and fed as input to the MNB. It is interesting to note that the model recognizes the named entities with an average F-measure of 81% and 10 fold cross validation F-measure of 77.2%."
    } ],
    "references" : [ {
      "title" : "Natural Language Processing",
      "author" : [ "Elizabeth D Liddy" ],
      "venue" : "In Encyclopedia of Library and Information Science. 2nd edition,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "Natural Language Understanding",
      "author" : [ "James Allen" ],
      "venue" : "Pearson Publication Inc.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "Natural Language Processing. Ess Ess Publications for Sarada Ranganathan Endowment for Library Science, Bangalore, INDIA",
      "author" : [ "Kavi Narayana Murthy" ],
      "venue" : "1st edition,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Natural language processing, annual review of information science and technology",
      "author" : [ "Gobinda G. Chowdhury" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Using language independent and language specific features to enhance Arabic named entity recognition",
      "author" : [ "Yassine Benajiba", "Mona T Diab", "Paolo Rosso" ],
      "venue" : "Int. Arab J. Inf. Technol.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "The first steps towards Assamese named entity recognition",
      "author" : [ "Padmaja Sharma", "Utpal Sharma", "Jugal Kalita" ],
      "venue" : "Brisbane Convention Center,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Named entity recognition in Bengali. A conditional random field approach",
      "author" : [ "Asif Ekbal Rejwanul Haqueand", "Sivaji Bandyopadhyay" ],
      "venue" : "Pages 589–594,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Bengali named entity recognition using support vector machine",
      "author" : [ "Asif Ekbal", "Sivaji Bandyopadhyay" ],
      "venue" : "In IJCNLP,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Development of Bengali named entity tagged corpus and its use in NER systems",
      "author" : [ "Asif Ekbal", "Sivaji Bandyopadhyay" ],
      "venue" : "In IJCNLP,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Named entity recognition: Exploring features",
      "author" : [ "Maksim Tkachenko", "Andrey Simanovsky", "St Petersburg" ],
      "venue" : "In Proceedings of KONVENS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Named entity recognition approaches",
      "author" : [ "Alireza Mansouri", "Lilly Suriani Affendey", "Ali Mamat" ],
      "venue" : "International Journal of Computer Science and Network Security,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Unsupervised named-entity recognition: Generating gazetteers and resolving ambiguity",
      "author" : [ "David Nadeau", "Peter Turney", "Stan Matwin" ],
      "venue" : "Published at the 19th Canadian Conference on Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Evaluation of named entity extraction systems",
      "author" : [ "Monica Marrero", "Sonia Sanchez-Cuadrado", "Jorge Morato Lara", "George Andreadakis" ],
      "venue" : "Advances in Computational Linguistics, Research in Computing Science,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Named entity recognition in Hindi using maximum entropy and transliteration",
      "author" : [ "Sujan Kumar Saha", "Partha Sarathi Ghosh", "Sudeshna Sarkar", "Pabitra Mitra" ],
      "venue" : "Research journal on Computer Science and Computer Engineering with Applications,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Rapid development of hindi named entity recognition using conditional random fields and feature induction (short paper)",
      "author" : [ "Li Wei", "McCallum Andrew" ],
      "venue" : "ACM Transactions on Computational Logic,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "A hybrid feature set based maximum entropy Hindi named entity recognition",
      "author" : [ "Sujan Kumar Saha", "Sudeshna Sarkar", "Pabitra Mitra" ],
      "venue" : "In IJCNLP,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Named entity recognition using hidden markov model (hmm): An experimental result on Hindi, urdu and marathi languages",
      "author" : [ "Sudha Morwal", "Nusrat Jahan" ],
      "venue" : "International Journal of Advanced Research in Computer Science and Software Engineering (IJARCSSE),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Detection and categorization of named entities in Indian languages using Hidden",
      "author" : [ "Deepti Chopra", "Sudha Morwal" ],
      "venue" : "Markov Model. International Journal of Computer Science,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Named entity recognition in Indian languages using gazetteer method and hidden markov model: A hybrid approach",
      "author" : [ "Nusrat Jahan", "Sudha Morwal", "Deepti Chopra" ],
      "venue" : "IJCSET, March,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "A two stage language independent named entity recognition for indian languages",
      "author" : [ "S Biswas", "MK Mishra", "S Acharya Sitanath", "S Mohanty" ],
      "venue" : "IJCSIT International Journal of Computer Science and Information Technologies,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Named entity recognition for Indian languages",
      "author" : [ "Animesh Nayan", "B Ravi Kiran Rao", "Pawandeep Singh", "Sudip Sanyal", "Ratna Sanyal" ],
      "venue" : "In IJCNLP,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "A hybrid approach for named entity recognition in Indian languages",
      "author" : [ "Sujan Kumar Saha", "Sanjay Chatterji", "Sandipan Dandapat", "Sudeshna Sarkar", "Pabitra Mitra" ],
      "venue" : "NER for South and South East Asian Languages,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F Tjong Kim Sang", "Fien De Meulder" ],
      "venue" : "In Proceedings of the seventh conference on",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "Named entity recognition using support vector machine: A language independent approach",
      "author" : [ "Asif Ekbal", "Sivaji Bandyopadhyay" ],
      "venue" : "International Journal of Electrical, Computer, and Systems Engineering,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Language independent named entity recognition in indian languages",
      "author" : [ "Asif Ekbal", "Rejwanul Haque", "Amitava Das", "Venkateswarlu Poka", "Sivaji Bandyopadhyay" ],
      "venue" : "In IJCNLP,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Crf based name entity recognition (ner) in manipuri: A highly agglutinative indian language",
      "author" : [ "Kishorjit Nongmeikapam", "Tontang Shangkhunem", "Ngariyanbam Mayekleima Chanu", "Laishram Newton Singh", "Bishworjit Salam", "Sivaji Bandyopadhyay" ],
      "venue" : "In Emerging Trends and Applications in Computer Science (NCETACS),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Named entity recognition for manipuri using support vector machine",
      "author" : [ "Thoudam Doren Singh", "Kishorjit Nongmeikapam", "Asif Ekbal", "Sivaji Bandyopadhyay" ],
      "venue" : "In PACLIC,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "A hybrid oriya named entity recognition system: harnessing the power of rule",
      "author" : [ "Sitanath Biswas", "SP Mishra", "S Acharya", "S Mohanty" ],
      "venue" : "International Journal of Artificial Intelligence and Expert Systems (IJAE),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "Named entity recognition for Punjabi language text summarization",
      "author" : [ "Vishal Gupta", "Gurpreet Singh Lehal" ],
      "venue" : "International Journal of Computer Applications,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    }, {
      "title" : "Hybrid three-stage named entity recognizer for Tamil",
      "author" : [ "S Pandian", "Krishnan Aravind Pavithra", "T Geetha" ],
      "venue" : "INFOS,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "Named entity recognition and transliteration for Telugu language. Parsing in Indian Languages, Special Volume: Problems of Parsing in Indian Languages: 64–70",
      "author" : [ "Kommaluri Vijayanand", "RP Seenivasan" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2011
    }, {
      "title" : "Named entity recognition for Telugu",
      "author" : [ "P Srikanth", "Kavi Narayana Murthy" ],
      "venue" : "In IJCNLP,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2008
    }, {
      "title" : "Experiments in Telugu NER: A conditional random field approach",
      "author" : [ "Praneeth Shishtla", "Karthik Gali", "Prasad Pingali", "Vasudeva Varma" ],
      "venue" : "In IJCNLP,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2008
    }, {
      "title" : "A maximum entropy approach to Kannada part of speech tagging",
      "author" : [ "BR Shambhavi", "Kumar P Ramakanth", "G Revanth" ],
      "venue" : "International Journal of Computer Applications,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2012
    }, {
      "title" : "A rule based Kannada morphological analyzer and generator using finite state transducer",
      "author" : [ "Ramasamy Veerappan", "PJ Antony", "S Saravanan", "KP Soman" ],
      "venue" : "Proceedings of International Journal of Computer Applications (0975-8887),",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2011
    }, {
      "title" : "Named entity recognition and classification in Kannada language",
      "author" : [ "S Amarappa", "SV Sathyanarayana" ],
      "venue" : "International Journal of Electronics and Computer Science Engineering,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Natural Language Processing (NLP) has two major tasks: Natural Language Understanding (NLU) and Natural Language Generation (NLG) Liddy (2001) [1].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "NER is a typical subtask of IE James Allen (2006) [2].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : ", Kavi N M (2006) [3].",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "The term Named Entity was introduced in the sixth Message Understanding Conference (MUC-6), Gobinda Chowdhury (2003) [4].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 4,
      "context" : "(2009) [5] discussed about SVM based Language Independent and Language Specific Features to Enhance Arabic NER.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "(2010) [6] discussed on the first steps towards Assamese NER.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 6,
      "context" : "(2008) [7] developed an NER system in Bengali using CRF approach.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 7,
      "context" : "Ekbal and Shivaji (2008) [8] reported about the development of a NER system for Bengali using SVM.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "Ekbal and Shivaji (2008) [9] discussed about Bengali Named Entity Tagged Corpus and its use in NER Systems.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "(2012) [10] build a CRF based system that achieves 91.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 10,
      "context" : "(2008) [11] proposed a robust and novel Machine Learning based method called Fuzzy support Vector Machine (FSVM) for NER.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "(2006) [12] devised an unsupervised NER by Generating Gazetteers and Resolving",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 12,
      "context" : "(2009) [13] discussed about the evaluation of Named Entity Extraction Systems.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 13,
      "context" : "(2008) [14] developed system for NER in Hindi using Max-Ent and Transliteration.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 14,
      "context" : "In Li and McCallum (2004) [15] the authors have used CRF with feature induction to the Hindi NER task.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "(2008) [16] developed a NER system for Hindi using Max-Ent.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 16,
      "context" : "Sudha and Nusrat (2013) [17] experimented, NER using HMM on Hindi, Urdu and Marathi Languages.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "Deepti and Sudha (2013) [18] deviced algorithm for the Detection and Categorization of Named Entities in Indian Languages using HMM.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "(2012) [19] devised algorithm for NER in Indian Languages using Gazetteer method and HMM.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 19,
      "context" : "(2010) [20] developed a Two Stage Language Independent NER for Hindi, Oriya, Bengali and Telugu.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 20,
      "context" : "(2008) [21] talked about a new approach to recognize named entities for Indian languages.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 21,
      "context" : "(2008) [22] described a hybrid system that applies Max-Ent, language specific rules and gazetteers to the task of NER in Indian languages.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 22,
      "context" : "Erik and Fien (2003) [23] gave Introduction to the CoNLL-2003 Shared Task a Language-Independent NER.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 23,
      "context" : "Ekbal and Shivaji (2010) [24] reported about the development of a language independent NER system for Bengali and Hindi using SVM.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 24,
      "context" : "(2008) [25] developed Language Independent NER system for South and South East Asian languages, particularly for Bengali, Hindi, Telugu, Oriya and Urdu as part of the IJCNLP-08 NER Shared Task1.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 25,
      "context" : "(2011) [26] developed a model on CRF Based NER in Manipuri.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 26,
      "context" : "(2009) [27] developed NER for Manipuri using SVM.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 27,
      "context" : "(2010) [28] described a hybrid system that applies Max-Ent model with HMM and some linguistic rules to recognize Name Entities in Oriya language.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 28,
      "context" : "Vishal and Gurpreet (2011) [29] explained about the NER System for Punjabi language text summarization using a Condition based approach.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 29,
      "context" : "(2008) [30] presented the construction of a hybrid, three stage NER for Tamil.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 30,
      "context" : "Vijayanand and Seenivasan (2011) [32] devised NER and Transliteration for Telugu.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 31,
      "context" : "Srikanth and KN Murthy (2008) [33] developed NER for Telugu using CRF based Noun Tagger.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 32,
      "context" : "(2008) [34] conducted experiments in Telugu NER using CRF.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 33,
      "context" : "(2012) [35] developed A Max-Ent model to Kannada Part Of Speech Tagging.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 34,
      "context" : "(2011) [36] proposed and developed a rule based Kannada Morphological Analyzer and Generator (MAG) using finite state transducer.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 35,
      "context" : "Amarappa and Sathyanarayana [37] (2013) developed a HMM based system for NERC in Kannada Language.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 33,
      "context" : "In Kannada Language, some works on Kannada Morphology are reported in [35] [56].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 35,
      "context" : "In our earlier work [37] we have carried out NERC work in Kannada using HMM on a limited corpus of 10,000 tokens, however the works on NERC in Kannada are yet to be investigated and implemented.",
      "startOffset" : 20,
      "endOffset" : 24
    } ],
    "year" : 2015,
    "abstractText" : "Named Entity Recognition and Classification (NERC) is a process of identification of proper nouns in the text and classification of those nouns into certain predefined categories like person name, location, organization, date, and time etc. NERC in Kannada is an essential and challenging task. The aim of this work is to develop a novel model for NERC, based on Multinomial Naïve Bayes (MNB) Classifier. The Methodology adopted in this paper is based on feature extraction of training corpus, by using term frequency, inverse document frequency and fitting them to a tf-idf-vectorizer. The paper discusses the various issues in developing the proposed model. The details of implementation and performance evaluation are discussed. The experiments are conducted on a training corpus of size 95,170 tokens and test corpus of 5,000 tokens. It is observed that the model works with Precision, Recall and F1-measure of 83%, 79% and 81% respectively.",
    "creator" : "Microsoft® Word 2013"
  }
}