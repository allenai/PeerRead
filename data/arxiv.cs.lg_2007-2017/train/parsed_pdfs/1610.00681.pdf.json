{
  "name" : "1610.00681.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Network Structures and Fast Distributed MMSE Estimation",
    "authors" : [ "Muhammed O. Sayina", "Suleyman S. Kozatb" ],
    "emails" : [ "sayin2@illinois.edu", "kozat@ee.bilkent.edu.tr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n00 68\n1v 1\n[ cs\n.S Y\n] 3\nO ct\n2 01\nWe construct optimal estimation algorithms over distributed networks for state estimation in the mean-square error (MSE) sense. Here, we have a distributed collection of agents with processing and cooperation capabilities. These agents continually observe a noisy version of a desired state of the nature through a linear model and seek to learn this state by interacting with each other. Although this problem has attracted significant attention and extensively been studied in several different fields including machine learning theory to signal processing, all the well-known strategies achieve suboptimal learning performance in the MSE sense. To this end, we provide algorithms that achieve distributed minimum MSE (MMSE) performance over an arbitrary network topology based on the aggregation of information at each agent. This approach differs from the diffusion of information across network, i.e., exchange of local estimates per time instance. Importantly, we show that exchange of local estimates is sufficient only over the certain network topologies. By inspecting these network structures, we also propose strategies that achieve the distributed MMSE performance also through the diffusion of information such that we can substantially reduce the communication load while achieving the best possible MSE performance. For practical implementations we provide approaches to reduce the complexity of the algorithms through the time-windowing of the observations. Finally, in the numerical examples, we demonstrate the superior performance of the introduced algorithms in the MSE sense due to optimal estimation.\nKeywords: Distributed networks, distributed Kalman filter, MMSE estimation, tree networks."
    }, {
      "heading" : "1. Introduction",
      "text" : "Over a distributed network of agents with measurement, processing and communication capabilities, we can have enhanced processing performance, e.g., fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3]. Mainly, distributed agents observe a true state of the system through noisy measurements from different viewpoints, process the observation data in order to estimate the state, and communicate with each other to alleviate the estimation process in a fully distributed manner. Notably, the agents\n∗Corresponding author Email addresses: sayin2@illinois.edu (Muhammed O. Sayin), kozat@ee.bilkent.edu.tr (Suleyman S.\nKozat)\nPreprint submitted to Elsevier October 4, 2016\ncan respond to streaming data in an online manner by disclosing information among each other at certain instances. This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7]. As an example, say that we have radar systems distributed over an area and seeking to locate hostile missiles, i.e., the location of the missile is the underlying state of the system. In that respect, distributed processing approach has vital importance in terms of detecting the missiles and reacting as fast as possible. In particular, even if the viewpoints of a few radar systems are blocked due to environmental obstacles, through the communication among the radar systems, each system can still locate the missiles. Additionally, since each radar system not only collects measurements but also process them to locate the missiles, the overall system can react against the missiles faster than a centralized approach in which measurements of all the radar systems are collected at a centralized unit and processed together.\nAlthough there is an extensive literature on this field, e.g., [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents. Prior art focuses on the computationally simple algorithms that aim to achieve certain performance criterion asymptotically, e.g., diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns. However, there is a trade-off in terms of computational complexity and estimation performance. Correspondingly, for certain network scenarios, as exemplified in the following, computationally demanding yet fast algorithms can be preferable to achieve desired performance. In particular, commonly, at each iteration of the algorithms, agents make a noisy measurement, process the data, and exchange information with each other. In this chain of steps, each step, i.e., measurement, processing, and communication steps, plays significant roles for the overall performance of the algorithms as follows:\nSpeed of reliable communication: The reliable communication rate among the agents, i.e., communication step, should be as fast as the processing step for fast estimation performance. Otherwise, the communication rate creates a burden for the overall speed of the algorithm. In chemical kinetics, this phenomena is called as the rate determining step (RDS) such that speed of a chain of reactions can be approximately determined by the speed of the slowest reaction [13]. This implies that we can employ fast algorithms in spite of high complexity in the network scenarios with relatively slow communication step due to time flexibility.\nCommunication power: The low complexity algorithms have relatively slow learning rate such that agents disclose similar information with the previously disclosed ones. However, communication between the agents also requires significant power resources. To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12]. In [14, 12], the authors study this problem by compressing the data before the transmission and extracting the compressed information adaptively. We point out that in a fast estimation algorithm, exchanged data carries relatively more, i.e., new, information. This implies a communication flexibility such that through certain protocols, we can reduce the number of information exchange due to the disclosure of fertile information, and correspondingly this can reduce the communication load of the overall system.\nNumber of samples: Finally, in certain network scenarios, the number of measurements can also be too low, e.g., due to small sampling frequency, that the low complexity approaches aiming to achieve the performance asymptotically cannot yield an acceptable result. Particularly, some applications can require the agents to process the measurements as much as they can, i.e., optimally with respect to certain performance measure, due to the limited number of measurements.\nAs noted above, computationally demanding yet fast algorithms have applications for certain network scenarios. Hence, formulating the optimal distributed estimation algorithms with respect to certain performance criteria is a significant and unexplored challenge. To this end, instead of computationally simple yet slow algorithms, we seek to formulate the optimal estimation algorithms over distributed networks in the mean-square-error (MSE) sense. We design which information to disclose and how to utilize them for the minimum MSE (MMSE) performance. In addition to the aforementioned scenarios, we also analyze the network topologies in which the optimal algorithms can also be computationally simple and require less communication load. In the single agent systems, the well-known Kalman filter is extensively used for the MMSE estimation. Due to its iterative nature and superior estimation performance, Kalman filter appeals significant attention in both control theory and signal processing literatures. Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18]. However, while extending Kalman filter for distributed networks, these approaches develop asymptotically optimal or sub-optimal approaches in the MSE sense due to practical concerns. Particularly, the cooperation among the agents results in complex MMSE algorithms for arbitrary network topologies. As explained in [19], once agent-1 transmits an information to agent-2, this information becomes common knowledge between the agents. This implies that agent-1 knows that agent-2 knows it, agent-2 knows that agent-1 knows it, agent-1 knows that agent-2 knows that agent-1 knows it, and so on. Therefore, in the design of the distributed MMSE estimator, we take these into consideration and utilize the exchanged information based on its content rather than a blind approach in which all exchanged information is handled irrespective of the content as in the diffusion or consensus based approaches.\nIn diffusion or consensus based approaches, the agents utilize the exchanged information generally through certain static combination rules, e.g., the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22]. However, if the statistical profile of the measurement data varies over the network, i.e., each agent observes diverse signal-to-noise ratios, by ignoring the variation in noise, these rules yield severe decline in the estimation performance [2]. In such cases the agents can even perform better without cooperation [2]. Hence, we also seek to formulate the optimal utilization of the exchanged information in the MSE sense.\nConsider distributed networks of the agents that observe a noisy version of an underlying state and can exchange information with only certain agents at each time instant. Note that information is transmitted over the hops since certain agents might not be connected directly. To this end, we introduce a comprehensive cost measure considering the transmission of information over the hops and the corresponding content. We formulate the MMSE estimator, called the optimal distributed online learning (ODOL) algorithm, using common knowledge about the statistical profiles and the network topology for the jointly Gaussian state and noise signals. We point out that the ODOL algorithm achieves the linear MMSE (LMMSE) performance for other statistical profiles. Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11]. The aggregation of information at each agent requires excessive communication load relative to the diffusion of information in general. In particular, the ODOL algorithm is not practical for real life applications due to the excessive communication load, however, we utilize this algorithm to analyze the performance of proposed approaches since the ODOL algorithm achieves the oracle performance, i.e., the distributed MMSE performance, for any network scenario.\nFurthermore, we modify the ODOL algorithm by exploiting the network topology to reduce the communication load such that agents disclose their local estimates only yet achieve the\nMMSE performance. Over tree networks excluding multiple transmission paths, the diffusion of local estimates is sufficient to achieve the distributed MMSE performance. We analytically show that for sufficiency of diffusion of estimates, tree networks could also involve cell structures that we define as the sub-networks in which all agents are connected with each other. Additionally, we formulate the optimal and efficient distributed online learning (OEDOL) algorithm, which is practical for real life applications and achieves the MMSE performance over the tree networks. Note that for an arbitrary network topology, we can construct such network connections by eliminating certain communication links. Through numerical examples, we examine the impact of constructing the spanning tree of a network on the estimation performance and observe that the impact is negligible. Finally, we propose the time windowing of the observation set in order to reduce the complexity of the algorithms. In these algorithms, agents combine the received information linearly with time-invariant combination weights.\nWe can list our main contributions as follows: 1) We introduce a thorough cost measure considering the transmission of information over hops across networks. 2) We derive the ODOL algorithm achieving the oracle performance over arbitrary networks through the aggregation of information at each agent. 3) For practical applications, we study the network structures in which we can achieve the MMSE performance through the disclosure of local estimates. 4) We propose the OEDOL algorithm achieving the MMSE performance over certain network topologies with tremendously reduced communication load. 5) We also formulate sub-optimal versions of the algorithms with reduced complexity. 6) We provide numerical examples demonstrating the significant gains due to the introduced algorithms.\nThe remainder of the paper is organized as follows. We introduce the distributed MMSE framework in Section II. We study the tree networks, exploit the network topology to formulate the OEDOL algorithm that reduces the communication load and introduce cell structures enhancing the learning rate further under communication constraints in Section III. We propose the sub-optimal versions of the ODOL and OEDOL algorithms for practical implementations in Section IV. In Section V, we provide numerical examples demonstrating significant gains due to the introduced algorithms compared to the conventional algorithms. We conclude the paper in Section VI with several remarks.\nNotation: Bold lower (or upper) case letters denote column vectors (or matrices). For a vector a (or matrix A), aT (or AT ) is its ordinary transpose. The terms of the vector 1 (and 0) are all 1s (and 0s) and the size of the vector will be understood from the context. We use calligraphic letters for random variables and underlined calligraphic letters for random vectors, e.g., X and X. Bold calligraphic letters, e.g., Z , denote a set of random variables. For a random variable X (or vector X), E[X] (or E[X]) represents its expectation. We work with real data for notational simplicity. The operator col{·} produces a column vector or a matrix in which the arguments of col{·} are stacked one under the other. For a matrix argument, diag{A} operator constructs a diagonal matrix with the diagonal entries of A. For a given set N, diag{N} creates a diagonal matrix whose diagonal block entries are elements of the set. The operator ⊗ denotes the Kronecker product."
    }, {
      "heading" : "2. Distributed-MMSE Estimation Framework",
      "text" : "Consider a distributed network of m agents with processing and communication capabilities. In Fig. 1, we illustrate this network through an undirected graph, where the vertices and the edges correspond to the agents and the communication links across the network, respectively.\nFor each agent i, we denote the set of agents whose information could be received at least after k hops, i.e., k-hop neighbors, by N(k)i defined as\nN(k)i △ =\n{\nj1, · · · , jπ(k)i\n}\nand π(k)i = ∣ ∣ ∣N(k)i ∣ ∣ ∣ is the cardinality of N(k)i (See Fig. 1b). We assume that N (0) i = {i} and N (k) i = ∅ for k < 0. Here, the agents observe a noisy version of a time-invariant and unknown state vector x ∈ Rp which is a realization of the Gaussian random vector process X with mean x̄ and autocovariance Σx. Each agent i observes the state as\nyi,t = Hix + ni,t,\nwhere Hi ∈ Rq×p is a known matrix, ni,t ∈ Rq is a realization of a zero-mean white Gaussian vector process N i,t with auto-covariance Σni and correspondingly the observation yi,t ∈ R\nq is a realization of the random process Y\ni,t = HiX +N i,t. The noise ni,t is also independent from the\nstate and the other noise parameters. We assume that the variance of the noise signals are known, which can also be readily estimated from the data [23]. At each instant, the agents observe the underlying state through yi,t, diffuse information to the neighboring agents and receive information from them as seen in Fig. 1a. Naturally, the agents can learn the true state, under certain regularity conditions [23], irrespective of cooperation among them, provided that their own measurements are not biased and sufficient to estimate the true state in time. However, through the cooperation of agents, we can increase the learning rate significantly [2]. To this end, we aim to find the optimal learning strategy over distributed networks in the MSE sense.\nAs a lower bound on the MSE performance over distributed networks, we first consider the case when there is no limitation on the amount of the disclosed information. Agents disclose their own measurements and transmit the received information from the neighboring agents (with time stamps, i.e., the information is ordered). Through this implementation, each agent can obtain the measurements of the other agents separately in a connected network. We point\nout that the information on the non-neighboring agents could only be received after certain hops due to the sparsely connected structure. As an example, the disclosed information of j ∈ N(2)i reaches to i by passing through two communication links as seen in Fig. 1b. In particular, this case assumes that each agent has access to full information from the other agents, albeit with certain hops, and corresponds to the direct aggregation of all information across the network at each agent.\nRemark 2.1: The aggregation of all information directly at each agent and processing them optimally yield excessive communication load in general. Since the communication load is crucial for the applicability of the distributed learning algorithms [12, 14], we also seek to reduce the communication load yet achieve the MMSE performance. In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8]. However, later in this paper we show that the disclosure of the local estimates is sufficient to achieve the MMSE performance only over certain network topologies.\nAt time t, all the information aggregated at ith agent is given by { {\nyi,τ }τ≤t , { y j,τ }τ≤t−1\nj∈N(1)i , · · · ,\n{ y j,τ }τ≤t−κi\nj∈N (κi ) i\n}\n,\nwhere the information from the furthest agent is received at least after κi hops1. Here, the collection set { y j,τ }τ≤t−k\nj∈N(k)i includes the observations from the k-hop neighbors N(k)i . Particularly, the\ncollection is explicitly defined as {\ny j,τ }τ≤t−k\nj∈N(k)i\n△ =\n{\ny j1,t−k, ..., y j1,0,\n..., y j π (k) i ,t−k, ..., y j π (k) i ,0\n}\n. (1)\nThen, we have the comprehensive cost measure in the distributed-MMSE framework as\nx̂i,t = minx E [ ∥ ∥ ∥Xt − x ∥ ∥ ∥ 2 ∣ ∣ ∣ ∣ { Y i,τ = yi,τ }τ≤t ,\n{\nY j,τ = y j,τ\n}τ≤t−1\nj∈N(1)i , · · · ,\n{\nY j,τ = y j,τ\n}τ≤t−κi\nj∈N (κi ) i\n]\n(2)\nand the MMSE estimate for each agent i is given by the expectation of x conditioned all the accessed information:\nxi,t = E [ X ∣ ∣ ∣ ∣ { Yi,τ = yi,τ }τ≤t , { Y j,τ = y j,τ }τ≤t−1\nj∈N(1)i , · · · ,\n{\nY j,τ = y j,τ\n}τ≤t−κi\nj∈N (κi ) i\n]\n. (3)\nCorrespondingly, we define the random variable X̂i,t as follows\nX̂i,t △ = E\n[\nX ∣ ∣ ∣ ∣ {\nY i,τ\n}τ≤t , · · · , {\nY j,τ\n}τ≤t−κi\nj∈N (κi ) i\n]\n.\n1For notational simplicity, we denote Ni △ = N(1)i and πi △ = π (1) i ."
    }, {
      "heading" : "2.1. ODOL Algorithm",
      "text" : "Importantly, since the state and the observation noise are independent Gaussian random parameters, we propose the ODOL algorithm constructing the distributed-MMSE estimator (3) in an iterative way. To this end, we collect all received information at time t as\nzi,t = col {\nyi,t, yi1,t−1, ..., yiπi ,t−1,\n..., y j1,t−κi , ..., y j π\n(κi) i\n,t−κi\n}\n.\nThen, the iterations of the ODOL algorithm are given by\nKi,t = Σ̂i,t−1H̄ T i\n(\nH̄iΣ̂i,t−1H̄ T i + Σ̄ni )−1 ,\nx̂i,t = ( I − Ki,tH̄i ) x̂i,t−1 + Ki,tzi,t, Σ̂i,t = ( I − Ki,tH̄i ) Σ̂i,t−1,\nwhere2 H̄i △ =\n( Pi ⊗ Iq ) H, H △ = col {H1, · · · ,Hm}, Σ̄ni △ = ( Pi ⊗ Ip ) Σn ( Pi ⊗ Ip )T , Σn △ =\ndiag { Σn1 , · · · ,Σnm } , and Pi is the corresponding permutation matrix."
    }, {
      "heading" : "3. Distributed-MMSE Estimation with Disclosure of Local Estimate",
      "text" : "Since the underlying state and the observation noises are independent Gaussian random parameters, the conditional expectation of the state given the observations, i.e., the MMSE estimate, is an affine combination of the observations. Hence, in this section we aim to exploit the affine transformation in order to reduce the disclosed amount of information so that each agent exchanges the necessary information in an efficient way. We point out that, as shown in the following example, disclosure of the local estimates is not sufficient to achieve the distributed MMSE performance in general. Consider a cycle network of 4 agents where agent-1 is directly connected with agents 2 and 3. At time t = 1, we obtain\nx̂2,1 = E [ X ∣ ∣ ∣Y\n2,1 = y2,1,Y2,0 = y2,0,Y1,0 = y1,0,Y4,0 = y4,0\n]\n,\nx̂3,1 = E [ X ∣ ∣ ∣Y\n3,1 = y3,1,Y3,0 = y3,0,Y1,0 = y1,0,Y4,0 = y4,0\n]\n.\nSince x̂2,1 and x̂3,1 are conditioned on Y4,0 = y4,0 commonly, the MMSE estimator x̂1,2 could not be obtained by just processing the current estimates, i.e.,\nx̂1,2 , E [ X ∣ ∣ ∣Y1,2 = y1,2, X̂1,1 = x̂1,1, X̂2,1 = x̂2,1,\nX̂3,1 = x̂3,1 ] .\nIn particular, for i.i.d. observations Y1, Y2 and Y3 we have\nE [ X ∣ ∣ ∣Y1,Y2,Y3 ] , E [ X ∣ ∣ ∣E [ X|Y1,Y2 ] , E [ X|Y2,Y3 ]] .\nHence, optimal processing of estimates should be elaborately considered. In the following, we analytically show that the MMSE estimate could be obtained through the disclosure of local estimates over “tree networks” but not in general.\n2If the inverse fails to exist, a pseudo inverse can replace the inverse [25]."
    }, {
      "heading" : "3.1. Tree Networks",
      "text" : "A network has a “tree structure” if its corresponding graph is a tree, i.e., connected and undirected without any cycles [26]. As an example, the conventional star or line networks have tree structures. We remark that for an arbitrary network topology we can also construct the spanning tree of the network and eliminate the cycles. In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].\nImportantly, the following theorem shows that over tree networks we can achieve the performance of the oracle algorithm through the disclosure of the local estimates only.\nTheorem 3.1: Consider the distributed estimation framework over a tree network. Then, the distributed-MMSE estimator (3) could also be obtained by\nx̂i,t = E [ X ∣ ∣ ∣ ∣ ∣ { Yi,τ = yi,τ }τ≤t , { X̂ j,τ = x̂ j,τ }τ≤t−1\nj∈Ni\n]\n, (4)\ni.e., each agent can only disclose its own local estimate to achieve the MMSE performance.\nProof: Initially, agent i has access to yi,0 only and the MMSE estimator is x̂i,0 = E[X|Yi,0 = yi,0]. At time t = 1, the MMSE estimator is given by\nx̂i,1 = E [ X ∣ ∣ ∣ {\nY i,τ = yi,τ }τ≤1 , { Y j,0 = y j,0 }\nj∈Ni\n]\n, (5)\nwhich can be written as\nx̂i,1 = E [ X ∣ ∣ ∣ {\nY i,τ = yi,τ }τ≤1 , { E[X ∣ ∣ ∣Y j,0 = y j,0] }\nj∈Ni\n]\n,\n= E [ X ∣ ∣ ∣ {\nY i,τ = yi,τ }τ≤1 , { X̂ j,0 = x̂ j,0 }\nj∈Ni\n]\n. (6)\nAs seen in Fig. 2, over a tree network, for k ∈ {1, · · · , κi} we have\nN(k)i = ⋃\nj∈Ni\n(\nN(k)i ∩ N (k−1) j\n)\n. (7)\nNote that the sets in (7) are disjoint as (\nN(k)i ∩ N (k−1) j1\n) ∩ (\nN(k)i ∩ N (k−1) j2\n)\n= ∅ (8)\nfor all j1, j2 ∈ Ni and j1 , j2. Notably, over a tree network, by (8), we can partition the collection set defined in (1) as follows\n{ y j,τ }\nj∈N(k)i =\n{ {\ny j,τ }\nj∈N(k)i ∩N (k−1) j1\n,\n· · · , { y j,τ }\nj∈N(k)i ∩N (k−1) jπi\n}\n. (9)\nLet the set of all accessed information by the ith agent at time t = 2 be\nZi,2 △ =\n{{\nyi,τ }τ≤2 , { y j,τ }τ≤1 j∈Ni , { yk,0 } k∈N(2)i\n}\n.\nWe also define\nẐ j,i,1 △ =\n{ =y j,1 ︷ ︸︸ ︷ {\nyk,1 }\nk∈N(1)i ∩N (0) j\n, { yk,0 }\nk∈N(2)i ∩N (1) j\n}\nsuch that by (9) we have Zi,2 = { yi,2, Ẑ j1,i,1, · · · , Ẑ jπi ,i,1, Zi,1 }\n(10)\nand\nẐ j,i,1 = Z j,1 \\ {\n=Z j,0 ︷︸︸︷\ny j,0 , yi,0 } . (11)\nHence, by (10) and (11), we obtain\nx̂i,2 = E [ X ∣ ∣ ∣ { Yi,τ = yi,τ }τ≤2 , { Z j,0 = Z j,0 }\nj∈Ni ,\n{\nZ j,1 \\ {\nZ j,0,Yi,0\n} = Z j,1 \\ { Z j,0, yi,0 } }\nj∈Ni\n]\n. (12)\nAfter some algebra, (12) could also be obtained by\nx̂i,2 = E [ X ∣ ∣ ∣ ∣ {\nY i,τ = yi,τ }τ≤2 , { E [ X ∣ ∣ ∣Z j,τ = Z j,τ ]}τ≤1\nj∈Ni\n]\n,\n= E [ X ∣ ∣ ∣ {\nY i,τ = yi,τ }τ≤2 , { X̂ j,τ = x̂ j,τ }τ≤1\nj∈Ni\n]\n. (13)\nAt time t = 3, (10) yields\nẐ j,i,2 = Z j,2 \\ { Z j,1 ∪\n=Ẑi, j,1 ︷ ︸︸ ︷ {\nZi,1 \\ { Zi,0 ∪ Z j,0 } } } . (14)\nCorrespondingly, (14) leads to\nẐ j,i,t = Z j,t \\ { Z j,t−1 ∪ Ẑi, j,t−1 }\nimplying that Ẑ j,i,t is constructible from Zi,τ and Z j,τ for τ ≤ t. Hence, for t > 0 the MMSE estimator over tree networks is given by (4) and the proof is concluded.\nRemark 3.1: When the expectation of the state is conditioned on the infinite number of observations over even a constructed spanning tree, only a finite number of the observations is devoid compared to the case over a fully connected network. Hence, even if we construct the spanning tree of that network, we would still achieve the distributed MMSE performance over a fully connected (or centralized) network asymptotically. As an illustrative example, in Fig. 8, we observe that the MMSE performance over the fully connected, star and line networks are asymptotically the same. Similarly, in [32, 33, 34], the authors show that the performance of the diffusion based algorithms could approach the performance of a fully connected network under certain regularity conditions.\nIn the sequel, we propose the optimal and efficient distributed online learning (OEDOL) algorithm that achieves the distributed-MMSE performance over tree networks iteratively."
    }, {
      "heading" : "3.2. OEDOL Algorithm",
      "text" : "We remark that the MMSE estimate x̂i,t linearly depends on previous estimate x̂i,t−1. In order to extract the new information, we need to eliminate the previously received information at each instant on the neighboring agents. This brings in additional computational complexity. On the contrary, agents can just disclose the new information, i.e., ẑi,t, after eliminating all the previously exchanged information. Since we are conditioning on the linear combinations of the conditioned variables without effecting their spanned space, i.e., ẑi,t is computable from x̂i,τ for τ ≤ t and vice versa, we can still achieve the MMSE performance by reduced computational load, yet.\nAt time t, agent i observes yi,t and receives z̄i,t = col { ẑ j1,t−1, · · · , ẑ jπi ,t−1 }\n. Here, we determine the content of the received information to extract the innovation within them and utilize this innovation in the update of the local estimate. In particular, the OEDOL algorithm is given by\nx̂i,t = Ai,tx̂i,t−1 + Bi,tyi,t + Ci,twi,t−1, (15)\nΣ̂i,t = Ai,tΣ̂i,t−1, (16)\nwhere wi,t−1 is the innovation term extracted from the received information. Ai,t, Bi,t, and Ci,t are the corresponding weighting matrices defined as\n[ Bi,t Ci,t ] = Σ̂i,t−1H̃ T i,t−1×\n(\nH̃i,t−1Σ̂i,t−1H̃ T i,t−1 + G̃i,t−1\n)−1 , (17)\nAi,t = I − Bi,tHi − Ci,tH̄i,t−1, (18)\nwhere H̃i,t = col{Hi, H̄i,t}, G̃i,t = diag{Σni , Ḡi,t} and Ḡi,t = diag{Gi,t}. The intermediate parame-\nters H̄i,t and Gi,t evolve according to\nH̄i,t =\n  B j1,tH j1 + C j1,tH̄ j1,t−1 ...\nB jπi ,tH jπi + C jπi ,tH̄ jπi ,t−1\n  −\n  C j1,tH̄ (i) j1,t−1 ...\nC jπi ,tH̄ (i) jπi ,t−1\n  , (19)\nGi,t =\n  B j1,tΣn j1 B T j1,t + C j1,tḠ j1,t−1C T j1,t ...\nB jπi ,tΣn jπi B T jπi ,t + C jπi ,tḠ jπi ,t−1C T jπi ,t\n  −\n  C(i)j1,tG (i) j1,t−1 ( C(i)j1,t )T ...\nC(i)jπi ,t G(i)jπi ,t−1\n(\nC(i)jπi ,t\n)T\n \n(20)\nand we initialize the parameters as H̄ j,τ = 0 and G j,τ = 0 for τ < 0. The recursion of the innovation parameter wi,t is given by\nwi,t = z̄i,t − Di,tẑi,t−1 + Ti,twi,t−2, (21)\nwhere Di,t = col {\nC(i)j1,t, · · · ,C (i) jπi ,t\n}\n(22)\nand\nTi,t △ =\n  C(i)j1,tC ( j1) i,t−1 · · · 0 ... . . . ...\n0 · · · C(i)jπi ,t C ( jπi ) i,t−1\n  . (23)\nThen, the agents disclose ẑi,t = x̂i,t − Ai,tx̂i,t−1.\nThe detailed description of the algorithm is provided in Table 1.\nRemark 3.2: In (15), the combination matrices Ai,t,Bi,t and Ci,t are independent from the streaming data although they are time-varying. Hence they can be computed before-hand. In that case, as an example for p = q = 1, the computational complexity of the iterations is O ( π̄2 ) where π̄ is the average of the cardinalities of the first-order neighborhoods across the network. Otherwise, the computational complexity of the algorithm is given by O ( m(π̄)3 ) , while it is O ( m3 )\nfor the oracle algorithm. Note that over the tree we have m − 1 edges and correspondingly π̄ is expected to be small. Hence, the optimal diffusion strategy over tree networks also reduces the computational complexity in general (in addition to the substantial reduction in communication). In Table 2, we tabulate the computational complexities of the introduced algorithms.\nWhile constructing the spanning tree, we cancel certain communication links in order to avoid multi-path information propagation. However, we also observe that in a fully connected network we can achieve the performance of the ODOL algorithm, i.e., the distributed-MMSE performance, through the disclosure of local observations. In particular, since all of the agents are connected, each agent can receive the observations across the network directly. Correspondingly, in a fully connected network, we can achieve identical performance with the\nODOL algorithm only through the disclosure of the local estimates as stated in the following corollary formally.\nCorollary 3.1: Consider the distributed estimation framework over a fully connected network. Then, the distributed-MMSE estimator (3) could also be obtained by (4), where agents disclose local estimates only.\nProof: Over a fully connected network, κi = 1 and the MMSE estimator is given by\nx̂i,t = E [ X ∣ ∣ ∣ ∣ ∣ { Y i,τ = yi,τ }τ≤t , { Y j,τ = y j,τ }τ≤t−1\nj∈Ni\n]\n(24)\nand we can also obtain (24) by (6). Here, we have\nẐ j,i,t = Z j,t \\ {Zi,t \\ {yi,t}},\nwhich yields\nx̂i,t = E [ X ∣ ∣ ∣ {\nY i,τ = yi,τ }τ≤t , { Z j,τ = Z j,τ }τ≤t−1\nj∈Ni ,\n{\nZ j,t−1 \\ { Zi,t−1 \\ Yi,t−1 }\n= Z j,t−1 \\ { Zi,t−1 \\ {yi,t−1} }}\nj∈Ni\n]\n(25)\nand (25) is also given by (4) and the proof is concluded.\nWe point out that the optimal algorithms can achieve the desired performance provided that the model assumptions are satisfied. If a link or node failure occurs, the algorithms may not achieve the optimal performance. However, once we detect a link failure, we can redesign the algorithms by eliminating this link in the new reconfiguration. Hence, through such strategies, we can increase the robustness of the introduced algorithms.\nIn the following, we enhance the learning rate further for an arbitrary network topology by exploiting the structure of the fully connected sub-networks that we call as “cell”."
    }, {
      "heading" : "3.3. Tree Networks Involving Cell Structures",
      "text" : "We define a “cell structure” as a sub-network in which all agents are connected to each other. Intuitively, considering a cell structure as a “single” agent, we can involve the cell (i.e., all the agents in the cell) in the tree such that we can still achieve the distributed-MMSE performance through the disclosure of local estimates (although we may have loops in the cell). We list the features of the cell structures, e.g., seen in Fig. 3, as follows:\n• Agents out of a cell can connect to at most one of the agents within that cell.\n• A cell structure consists of at least 2 agents.\n• An agent can belong to more than one cell.\n• Two different agents cannot belong to more than one cell at the same time.\n• All of the agents belong to at least a cell in a connected network.\n• Each agent has also the knowledge of the cells of the other agents.\n• Each agent labels its cells from its own and its first order neighbor’s point of view. As an example, for the agent i, Ci,i1 denotes the cell involving both i and i1. Note that if the same cell also includes i2, Ci,i1 = Ci,i2 .\nThe following theorem shows that we can achieve the performance of the oracle algorithm over tree networks involving cell structures through the disclosure of the local estimates only.\nTheorem 3.2: Consider the distributed estimation framework over tree networks involving cell structures. Then, the distributed-MMSE estimator (3) could also be obtained by (4).\nProof: We point out that Ẑ j,i,t denotes the set of new information received by i over j at time t. Initially, we have Ẑ j,i,0 = Z j,0 = {y j,0} and the MMSE estimator is also given by (6) over this network topology. Note that the information received by j at t = 1 is given by Z j,1 = {\ny j,1, {\n=Ẑk, j,0 ︷︸︸︷\nyk,0 }\nk∈N j , Z j,0\n}\n, which yields\nZ j,i,1 = Z j,1 \\\n   Z j,0 ∪ ⋃\nk∈Ci, j\n{ yk,0 }\n   ,\n= Z j,1 \\\n   Z j,0 ∪ ⋃\nk∈Ci, j\nẐk, j,0\n  \nand Ẑ j, j,t = ∅ by definition. Due to the cell structure, we have\n⋃\nk∈C j,i\nẐk, j,0 = Ẑi, j,0 ∪ ⋃\nk∈Ci, j\\ j\nẐk,i,0,\n= Zi,0 ∪ ⋃\nk∈Ci, j\\ j\nZk,0,\nwhich leads to\nx̂i,2 = E [ X ∣ ∣ ∣ ∣ {\nY i,τ = yi,τ }τ≤2 , { Z j,0 = Z j,0 }\nj∈Ni ,\n{\nZ j,1 \\\n{\nZ j,0 ∪Zi,0 ∪ ⋃\nk∈Ci, j\\ j\nZk,0\n}}\n= Z j,1 \\ { Z j,0 ∪ Zi,0 ∪ ⋃\nk∈Ci, j\\ j\nZk,0 }}\nj∈Ni\n]\n.\nWe point out that Ci, j ⊂ Ni and we obtain (13). Correspondingly, for t > 0 we have\nẐ j,i,t = Z j,t \\\n   Z j,t−1 ∪ Ẑi, j,t−1 ∪ ⋃\nk∈Ci, j\\ j\nẐk,i,t−1\n  \nand Ẑ j,i,t is constructible by the sets Z j,τ for j ∈ Ni and τ ≤ t. Hence, for t > 0 we obtain (4) and the proof is concluded.\nNote that we can have loops within the cell structures and still achieve the optimal performance through the diffusion of the local estimates only. In addition to the enhanced estimation rate, this will also increase the robustness of the introduced strategies against the link failures. In the sequel, we provide the sub-optimal extensions of the algorithms for practical applications."
    }, {
      "heading" : "4. Sub-optimal Approaches",
      "text" : "Minimization of the cost measure (2) optimally requires relatively excessive computations. We aim to mitigate the problem sub-optimally yet in a computationally efficient approach while achieving comparable performance with the optimal case. As an example, we can approximate the cost measure (2) through time-windowing as follows\nx̃i,t = minx E [ ∥ ∥ ∥X − x ∥ ∥ ∥ 2 ∣ ∣ ∣ ∣ { Y i,τ = yi,τ }t−κ<τ≤t ,\n{\nY j,τ = y j,τ\n}t−κ<τ≤t−1\nj∈N(1)i , · · · ,\n{\nY j,τ = y j,τ\n}t−κ<τ≤t−κi\nj∈N (κi ) i\n]\n.\nIn Fig. 4, we illustrate the time-windowing approach over the aggregated observations. We define a memory element mt denoting the expectation of the zero-mean state conditioned on all observation data time-stamped at t, i.e., mt △ = E [ X ∣ ∣ ∣ {\nY i,t = yi,t\n}\ni∈N\n]\n. We have mt = Myt\nand M △ = ΣxHT\n( HΣxHT + Σn )−1 , where H △ = col{H1, · · · ,Hm}, Σn △ = diag{Σn1 , · · · ,Σnm} and\nyt △ = col{y1,t, · · · , ym,t}. Correspondingly, x(e)i,t−1 represents the extracted information from the estimate x̃i,t−1 via the memory element mt−κ. In particular, x (e) i,t−1 is defined as follows\nx(e)i,t−1 △ = E\n[\nX ∣ ∣ ∣ {\nY i,τ = yi,τ }t−κ+1<τ≤t−1 ,\n· · · , {\nY j,τ = y j,τ\n}t−κ+1<τ≤t−κi−1\nj∈N (κi ) i\n]\n.\nThis yields x(e)i,t−1 = Niy (c) i,t−1 where y (c) i,t−1 is the vector collecting all observation data\nwithin that window (See Fig. 4) and Ni △ = ΣxĤ T i\n(\nĤiΣxĤ T i + Σ̂ni )−1 . We define Ĥi △ =\ncol {\nHi,Hi,H (1) i , · · · ,Hi,H (1) i , · · · ,H (κi) i\n}\nand H( j)i △ = col\n{\nH j1 , · · · ,H j π\n( j) i\n}\n. Correspondingly, Σ̂ni △ =\ndiag {\nΣni ,Σni ,Σ (1) ni , · · · ,Σni ,Σ (1) ni , · · · ,Σ (κi) ni\n}\nand Σ( j)ni △ = diag\n{\nΣn j1 , · · · ,Σn j\nπ ( j) i\n}\n. Then, after some\nalgebra the estimate is given by x̃i,t−1 = Kimt−κ + Lix (e) i,t−1 where\nKi △ =\n( I − MHNiĤi )−1 − ( I − NiĤiMH )−1 NiĤi,\nLi △ = (I − MH)\n( I − NiĤiMH )−1 .\nHence, we obtain the sub-optimal distributed online learning (SDOL) algorithm as\nx̃i,t = Aix̃i,t−1 + Biyi,t + Ciri,t−1 − Diyt−κ, (26)\nwhere ri,t−1 is a vector consisting of currently received observation data from the neighboring agents. The combination matrices defined as\n[ Bi Ci ] = Σx,iH̄ T i ( H̄iΣx,iH̄ T i + Σ̄ni )−1 , (27)\nAi △ =\n( I − [ Bi Ci ] H̄i ) L−1i , (28)\nDi △ = AiKiM, (29)\nwhereΣxi △ = (I−NiĤi)Σx. In Table 3, we tabulate the detailed description of the SDOL algorithm. Note that SDOL algorithm is based on the aggregation of information. Correspondingly, we can apply the time-windowing approach to the disclosure of local estimates and formulate a suboptimal efficient distributed online learning algorithm (SEDOL).\nIn the following we provide several remarks about the practical implementation of the SDOL algorithm.\nRemark 4.1:\n• We point out that in the update (26) the matrices Ai, Bi, Ci, and Di are independent from the data and they are time invariant. Hence, they can be pre-computed and installed onto the agents. Then, the SDOL algorithm basically takes the linear average of the previous estimate, the current measurement and received data, and observation data at time t − κ.\n• Different from the conventional approaches, the SDOL algorithm requires to memorize previous observations. Note that if q < p/m, storing observation data individually rather than a linear combination, i.e., mt, might be more efficient in terms of memory usage.\n• The computational complexity of the SDOL algorithm is O(p2 + 2pmq) (or say O(p2) for q ≪ p/m) and the algorithm requires disclosure of a data vector with (m−1)×q dimensions.\n• Finally, the steady-state MSE of the SDOL algorithm is given by\nMSE = Tr { AiLiΣxi } .\nIn the sequel, we also provide illustrative simulations showing the enhanced tracking performance due to the proposed algorithm over several distributed network scenarios."
    }, {
      "heading" : "5. Illustrative Examples",
      "text" : "In this section, we examine the performance of the introduced strategies under different scenarios. Consider a network of m = 20 agents where each agent i observes an underlying state x ∈ Rp through yi,t = hTi x + ni,t. The unknown state x is a realization of standard-normal distribution. The observation noise is a zero-mean white Gaussian random process. For comparison, we choose hi ∈ Rp randomly from a standard-normal distribution. We define a global MSE performance measure over a network as 1/m\n∑m i=1 MSE(i), where MSE(i) represents the MSE of\nthe ith agent. We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36]. We utilize several combination rules. In the uniform combination rule [20], the combination weights given by agent i for neighbor j are chosen as\nλi, j =\n{\n1/(πi + 1) if j ∈ Ni ∪ {i} 0 otherwise\nsince Ni excludes i. Correspondingly, in the relative variance rule [37, 38], λi, j is given by\nλi, j =\n   σ−2n j ∑ k∈Ni∪{i} σ−2nk if j ∈ Ni ∪ {i}\n0 otherwise.\nIn the D-LMS and consensus algorithms, we set µ = 0.1. In the D-RLS algorithm, we use the Laplacian combination rule [39] for the incremental update and the Metropolis combination rule for the spatial update.\nIn Figs. 5 and 6, we compare the time evolution of the global MSE of all algorithms for spaceinvariant and space-variant noise statistics, respectively for p = 1. We set the standard deviations\nof the noise parameters from a folded standard normal distribution in the space-variant case. The network topology is chosen arbitrarily and we can readily construct a corresponding spanning tree of that network. As an example, we can construct the spanning tree based on the paths from the most connected agent i to the others, i.e., i = arg max j π j, and then we eliminate the multipaths [27]. The relative-variance cooperation rule that considers the noise statistics to adjust the combination weights performs better than the uniform combination rule for space-variant noise profiles across the network. In Fig. 7, we compare the performance of the proposed algorithms for relatively large state vector, i.e., p = 10, and space variant noise profiles. In Figs. 5, 6 and 7 we observe that the proposed algorithms achieve superior performance compared to the other algorithms through the optimal weight selection for the MMSE performance. Additionally, the performance of the OEDOL and ODOL algorithms are close to each other even though the OEDOL algorithm operates over the corresponding spanning tree.\nIn order to examine the influence of the network topology on the MMSE performance, we provide Fig. 8. The fully connected network (or the centralized network) achieves the best possible connection among the agents whereas the line network yields the least possible connection. Over a star network, the furthest distance between the agents is 2 and a single agent, i.e., the so-called pseudo-centralized agent, can access to the information of its neighbors directly. Notably, the star and line networks are the spanning tree of the fully connected network. We observe that the MMSE over the star network is close to the MMSE over the fully connected network. Additionally, the MMSE over even the line network converges to the MMSE over the fully connected network asymptotically.\nFor practical implementations, we propose the SDOL algorithm. In Fig. 9, we examine the influence of the time-windowing depth on the global MSE performance of the SDOL algorithm. We use the SDOL algorithms with time-windowing depths κ = 20 and κ = 50. The time evolution of the MSE performance differs from the MMSE behavior because the SDOL algorithms inherently assume that even at initial time, i.e., t = 0, agents have legitimate observations from\nt = κ to t = 0. By excluding this data in time through sliding time-window, the SDOL algorithms eventually reach the best performance at the corresponding time-windowing depths and maintain this performance afterwards.\nFinally, we evaluate the MSE performance of the SDOL algorithm for abruptly changing state. Assume that the state vector changes at time t = 400 abruptly. In particular, we have chosen a new realization from the standard normal distribution. In Fig. 10, we compare the SDOL algorithms with the D-LMS algorithm. Note that we set the step size of the D-LMS algorithm as µ = 0.025 so that at steady state (if there were no state change) the D-LMS and SDOL with\nκ = 20 algorithms achieve the same MSE performance for fair tracking performance comparison. The sliding time-window provides relative robustness against abrupt state changes due to the exclusion of the effect of the previous observations. Hence, the SDOL algorithm can also provide enhance tracking performance in the practical applications."
    }, {
      "heading" : "6. Conclusion",
      "text" : "The distributed algorithms have attracted significant attention due to their wide spread applicability to highly complex structures from biological systems to social and economical networks.\nHowever, there are still challenges for disclosure and utilization of information among agents. We introduce the ODOL algorithm achieving the MMSE performance for Gaussian state and noise statistics and the LMMSE performance for arbitrary statistical profiles. The ODOL algorithm has consensus-like iterations for streaming data over an arbitrary network topology via the aggregation of information at each agent rather than propagation of information through the disclosure of local estimates. Importantly, the disclosure of the local estimate is sufficient only over the certain network topologies, e.g., over the introduced tree network involving cell structures. Moreover, the aggregation of information based approach can require excessive communication load. Therefore, in order to reduce the communication load, we exploit the network structures. We introduce the OEDOL algorithm that achieves the MMSE performance through the disclosure of local estimate over tree networks. We also observe that the MSE performance of the ODOL and OEDOL algorithms are close to each other over even a highly connected arbitrary network while the OEDOL provides reduced communication load in general. Finally, we introduce time-windowing approach for practical applications due to the reduced complexity. The sub-optimal approaches possess consensus-like iterations with time-invariant combination weights that should be calculated only once. Additionally, the sub-optimal approaches provide enhanced tracking performance against changing state due to the sliding of time-window."
    }, {
      "heading" : "7. Acknowledgment",
      "text" : "This work is in part supported by the Outstanding Researcher Programme of Turkish Academy of Sciences and TUBITAK projects, Contract no: 112E161 and 113E517."
    } ],
    "references" : [ {
      "title" : "A",
      "author" : [ "M.K. Banavar", "J.J. Zhang", "B. Chakraborty", "H. Kwon", "Y. Li", "H. Jiang", "A. Spanias", "C. Tepedelenlioglu", "C. Chakrabarti" ],
      "venue" : "P.-Suppappola, An overview of recent advances on distributed and agile sensing algorithms and implementation, Digital Signal Processing 39 ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Diffusion strategies for adaptation and learning over networks: An examination of distributed strategies and network behavior",
      "author" : [ "A.H. Sayed", "S.-Y. Tu", "J. Chen", "X. Zhao", "Z.J. Towfic" ],
      "venue" : "IEEE Signal Processing Magazine 30 (3) ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed estimation in diffusion networks using affine least-squares combiners",
      "author" : [ "J. Fernandez-Bes", "L.A. Azpicueta-Ruiz", "J. Arenas-Garcia", "M.T.M. Silva" ],
      "venue" : "Digital Signal Processing 36 ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Decentralized activation in sensor networks-global games and adaptive filtering games",
      "author" : [ "V. Krishnamurthy" ],
      "venue" : "Digital Signal Processing 21 (5) ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Non-Bayesian social learning",
      "author" : [ "A. Jadbabaie", "P. Molavi", "A. Sandroni", "A. Tahbaz-Salehi" ],
      "venue" : "Games and Economic Behavior 76 (1) ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Dynamic games and applications",
      "author" : [ "D. Acemoglu", "A. Ozdaglar" ],
      "venue" : "Opinion dynamics and learning in social networks 1 (1) ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Social and Economic Networks",
      "author" : [ "M. Jackson" ],
      "venue" : "Princeton University Press, Princeton, N.J.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis",
      "author" : [ "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing 56 (7) ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Diffusion strategies for distributed Kalman filtering and smoothing",
      "author" : [ "F.S. Cattivelli", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Automatic Control 55 (9) ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Online learning of dynamic parameters in social networks",
      "author" : [ "S. Shahrampour", "A. Rakhlin", "A. Jadbabaie" ],
      "venue" : "in: Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed LMS for consensus-based in-network adaptive processing",
      "author" : [ "I. Schizas", "G. Mateos", "G. Giannakis" ],
      "venue" : "IEEE Transactions on Signal Processing 57 (6) ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Compressive diffusion strategies over distributed networks for reduced communication load",
      "author" : [ "M.O. Sayin", "S.S. Kozat" ],
      "venue" : "IEEE Transactions on Signal Processing 62 (20) ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Chemical Kinetics and Dynamics",
      "author" : [ "J.I. Steinfeld", "J.S. Francisco", "W.L. Hase" ],
      "venue" : "Pearson",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Single bit and reduced dimension diffusion strategies over distributed networks",
      "author" : [ "M.O. Sayin", "S.S. Kozat" ],
      "venue" : "IEEE Signal Processing Letters 20 (10) ",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed kalman filtering for sensor networks",
      "author" : [ "R. Olfati-Saber" ],
      "venue" : "in: Proceedings of 46th IEEE Conference on Decision and Control",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "SOI: Distributed Kalman filtering with low-cost communications using the sign of innovations",
      "author" : [ "A. Ribeiro", "G.B. Giannakis", "S.I. Roumeliotis" ],
      "venue" : "IEEE Transactions on Signal Processing 54 (12) ",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Distributed Kalman filtering: a bibliographic review",
      "author" : [ "M.S. Mahmoud", "H.M. Khalid" ],
      "venue" : "IET Control Theory and Applications 7 (4) ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Optimal decentralized Kalman filter and Lainiotis filter",
      "author" : [ "N. Assimakis", "M. Adam", "M. Koziri", "S. Voliotis", "K. Asimakis" ],
      "venue" : "Digital Signal Processing 23 (1) ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Agreeing to disagree",
      "author" : [ "R.J. Aumann" ],
      "venue" : "The Annals of Statistics 4 (6) ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Convergence in multiagent coordination",
      "author" : [ "V.D. Blondel", "J.M. Hendrickx", "A. Olshevsky", "J.N. Tsitsiklis" ],
      "venue" : "consensus, and flocking, in: Proceedings of 44th IEEE Conference on Decision and Control (CDC-ECC)",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Locally constructed algorithms for distributed computations in ad-hoc networks",
      "author" : [ "D.S. Scherber", "H.C. Papadopoulos" ],
      "venue" : "in: Proceedings of Information Processing Sensor Networks (IPSN)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Equation of state calculations by fast computing machines",
      "author" : [ "N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller" ],
      "venue" : "The Journal of Chemical Physics 21 (6) ",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1953
    }, {
      "title" : "Fundamentals of Adaptive Filtering",
      "author" : [ "A.H. Sayed" ],
      "venue" : "Wiley, New York",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Reaching a consensus",
      "author" : [ "M.H. DeGroot" ],
      "venue" : "Journal of the American Statistical Association 69 (345) ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Optimal Filtering",
      "author" : [ "B.D.O. Anderson", "J.B. Moore" ],
      "venue" : "Prentice-Hall, Inc., Englewood Cliffs, NJ",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Implementing Discrete Mathematics: Combinatorics and Graph Theory with Mathematica",
      "author" : [ "S. Skiena" ],
      "venue" : "Addison- Wesley, Reading, MA",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Spanning Trees and Optimization Problems (Discrete Mathematics and Its Applications)",
      "author" : [ "B.Y. Wu", "K.-M. Chao" ],
      "venue" : "CRC Press, New York",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A distributed algorithm for minimum-weight spanning trees",
      "author" : [ "R.G. Gallager", "P.A. Humblet", "P.M. Spira" ],
      "venue" : "ACM Transactions on Programming Languages and Systems 5 (1) ",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Distributed Computing: A Locality-Sensitive Approach",
      "author" : [ "D. Peleg" ],
      "venue" : "SIAM, Philadelphia, PA",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Unconditional lower bounds on the time-approximation tradeoffs for the distributed minimum spanning tree problem",
      "author" : [ "M. Elkin" ],
      "venue" : "in: Proceedings of 36th ACM Symposium on Theory of Computing (STOC)",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Distributed algorithms for constructing approximate minimum spanning trees in wireless sensor networks",
      "author" : [ "M. Khan", "G. Pandurangan", "V.S.A. Kumar" ],
      "venue" : "IEEE Transactions on Parallel and Distributed Systems 20 (1) ",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Optimal combination rules for adaptation and learning over networks",
      "author" : [ "J. Chen", "A.H. Sayed" ],
      "venue" : "in: Proceedings of 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Diffusion recursive least-squares for distributed estimation over adaptive networks",
      "author" : [ "F.S. Cattivelli", "C.G. Lopes", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing 56 (5) ",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Diffusion strategies outperform consensus strategies for distributed estimation over adaptive networks",
      "author" : [ "S.Y. Tu", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing 60 (12) ",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Diffusion LMS for distributed estimation over adaptive networks",
      "author" : [ "F.S. Cattivelli", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing 58 (3) ",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Optimal combination rules for adaptation and learning over networks",
      "author" : [ "S.-Y. Tu", "A.H. Sayed" ],
      "venue" : "in: Proceedings of 4th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Consensus problems in networks of agents with switching topology and time-delays",
      "author" : [ "R. Olfati-Saber", "R.M. Murray" ],
      "venue" : "IEEE Transactions on Automatic Control 49 (9) ",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3].",
      "startOffset" : 114,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : ", fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3].",
      "startOffset" : 114,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : ", fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3].",
      "startOffset" : 114,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 8,
      "context" : ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "In chemical kinetics, this phenomena is called as the rate determining step (RDS) such that speed of a chain of reactions can be approximately determined by the speed of the slowest reaction [13].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12].",
      "startOffset" : 157,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12].",
      "startOffset" : 157,
      "endOffset" : 168
    }, {
      "referenceID" : 11,
      "context" : "To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12].",
      "startOffset" : 157,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "In [14, 12], the authors study this problem by compressing the data before the transmission and extracting the compressed information adaptively.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "In [14, 12], the authors study this problem by compressing the data before the transmission and extracting the compressed information adaptively.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 14,
      "context" : "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "As explained in [19], once agent-1 transmits an information to agent-2, this information becomes common knowledge between the agents.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : ", the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22].",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 20,
      "context" : ", the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : ", the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : ", each agent observes diverse signal-to-noise ratios, by ignoring the variation in noise, these rules yield severe decline in the estimation performance [2].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 1,
      "context" : "In such cases the agents can even perform better without cooperation [2].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11].",
      "startOffset" : 221,
      "endOffset" : 231
    }, {
      "referenceID" : 7,
      "context" : "Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11].",
      "startOffset" : 221,
      "endOffset" : 231
    }, {
      "referenceID" : 10,
      "context" : "Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11].",
      "startOffset" : 221,
      "endOffset" : 231
    }, {
      "referenceID" : 22,
      "context" : "We assume that the variance of the noise signals are known, which can also be readily estimated from the data [23].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "Naturally, the agents can learn the true state, under certain regularity conditions [23], irrespective of cooperation among them, provided that their own measurements are not biased and sufficient to estimate the true state in time.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "However, through the cooperation of agents, we can increase the learning rate significantly [2].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "Since the communication load is crucial for the applicability of the distributed learning algorithms [12, 14], we also seek to reduce the communication load yet achieve the MMSE performance.",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Since the communication load is crucial for the applicability of the distributed learning algorithms [12, 14], we also seek to reduce the communication load yet achieve the MMSE performance.",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].",
      "startOffset" : 142,
      "endOffset" : 156
    }, {
      "referenceID" : 23,
      "context" : "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].",
      "startOffset" : 142,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].",
      "startOffset" : 142,
      "endOffset" : 156
    }, {
      "referenceID" : 7,
      "context" : "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].",
      "startOffset" : 142,
      "endOffset" : 156
    }, {
      "referenceID" : 24,
      "context" : "2If the inverse fails to exist, a pseudo inverse can replace the inverse [25].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : ", connected and undirected without any cycles [26].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 27,
      "context" : "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 28,
      "context" : "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 29,
      "context" : "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 30,
      "context" : "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 31,
      "context" : "Similarly, in [32, 33, 34], the authors show that the performance of the diffusion based algorithms could approach the performance of a fully connected network under certain regularity conditions.",
      "startOffset" : 14,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 32,
      "context" : "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 33,
      "context" : "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].",
      "startOffset" : 238,
      "endOffset" : 242
    }, {
      "referenceID" : 19,
      "context" : "In the uniform combination rule [20], the combination weights given by agent i for neighbor j are chosen as λi, j = { 1/(πi + 1) if j ∈ Ni ∪ {i} 0 otherwise since Ni excludes i.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 34,
      "context" : "Correspondingly, in the relative variance rule [37, 38], λi, j is given by",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 35,
      "context" : "Correspondingly, in the relative variance rule [37, 38], λi, j is given by",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 36,
      "context" : "In the D-RLS algorithm, we use the Laplacian combination rule [39] for the incremental update and the Metropolis combination rule for the spatial update.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 26,
      "context" : ", i = arg max j π j, and then we eliminate the multipaths [27].",
      "startOffset" : 58,
      "endOffset" : 62
    } ],
    "year" : 2017,
    "abstractText" : "We construct optimal estimation algorithms over distributed networks for state estimation in the mean-square error (MSE) sense. Here, we have a distributed collection of agents with processing and cooperation capabilities. These agents continually observe a noisy version of a desired state of the nature through a linear model and seek to learn this state by interacting with each other. Although this problem has attracted significant attention and extensively been studied in several different fields including machine learning theory to signal processing, all the well-known strategies achieve suboptimal learning performance in the MSE sense. To this end, we provide algorithms that achieve distributed minimum MSE (MMSE) performance over an arbitrary network topology based on the aggregation of information at each agent. This approach differs from the diffusion of information across network, i.e., exchange of local estimates per time instance. Importantly, we show that exchange of local estimates is sufficient only over the certain network topologies. By inspecting these network structures, we also propose strategies that achieve the distributed MMSE performance also through the diffusion of information such that we can substantially reduce the communication load while achieving the best possible MSE performance. For practical implementations we provide approaches to reduce the complexity of the algorithms through the time-windowing of the observations. Finally, in the numerical examples, we demonstrate the superior performance of the introduced algorithms in the MSE sense due to optimal estimation.",
    "creator" : "LaTeX with hyperref package"
  }
}