{
  "name" : "1702.02661.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Inductive Pairwise Ranking: Going Beyond the n log(n) Barrier",
    "authors" : [ "U.N. Niranjan", "Arun Rajkumar" ],
    "emails" : [ "un.niranjan@uci.edu", "arun.rajkumar@xerox.com" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Ranking from pairwise comparisons or preferences is an ubiquitous problem in machine learning, statistics and theoretical computer science. In the so-called non-active setting, one is given comparison results ofm pairs pre-selected from among all pairs of n items where each pair is compared at leastK times. Particularly, the learner does not get to choose which pairs are to be compared. The goal is then to estimate a suitable ordering of the items, using the observed comparison results, that conforms to the true ordering, assuming one exists, up to the desired error in a suitably defined error measure.\nIn practical ranking applications, we often have side information associated with the items that need to be ranked – such a scenario is referred to as the inductive setting. An advantage in this inductive learning setting is that, in addition to ranking a given set of items, one is also able to rank new unseen items that may introduced after parameter learning. Motivated by these factors, we wish to leverage the\n∗Part of the work done while interning at Xerox Research Centre India. Copyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\navailable side information to compute an ordering more efficiently than existing techniques. This is relevant in many practical applications; for instance, in addition to using a minimal amount of customer preference data, (a) using food characteristics like nutrition, preparation method, etc could help in finding the top-rated dishes of a restaurant, (b) using car features like engine type, body type, etc could help elicit useful trends for automotive industry.\nOur Contributions: To the best of our knowledge, our work is the first to derive a provable and efficient method for ranking in the non-active inductive setting. Our novelty and technical contributions can be summarized along the following axes:\n1. Model: We generalize existing models so that we can incorporate (a) features, and (b) feature correlations associated with the items to be ranked. We show that our model subsumes many existing and popular ranking models.\n2. Algorithm: Our algorithm uses two key subroutines namely, (a) noisy inductive matrix completion, and (b) approximate pairwise ranking algorithm (Copeland 1951).\n3. Guarantee: We derive the guarantee that our algorithm obtains, with high probability, an -accurate recovery using Ω(max(log n/ 2, d4 log3 n/ 3n2)) independent pairwise comparisons chosen uniformly at random.\n4. Experiments: We substantiate our theoretical results by demonstrating sample complexity gains on both synthetic and real-world experiments.\nWe would like to emphasize upfront that it is the sole focus of this paper to study the practically motivated regime of d n in detail. Furthermore, we note that our sample complexity results do not violate the standard Ω(n log n) lower bounds for comparison-based sorting algorithms since we develop an algorithm that effectively ranks in the feature space rather than the item space."
    }, {
      "heading" : "Related Work and Background",
      "text" : "We now give a brief overview of relevant work in ranking models followed by a brief background regarding tools from inductive matrix completion theory which will be crucial in proving our sample complexity bounds.\nar X\niv :1\n70 2.\n02 66\n1v 1\n[ cs\n.L G\n] 9\nF eb\n2 01\n7\nRanking Models : In the simplest terms, the ranking problem involves estimating the best ordering items according to some observed preferences. A early thread of ranking literature has its beginnings in economics involving choice models (Luce 1959); other related works in social choice theory include (Lu and Boutilier 2011) and (Caragiannis, Procaccia, and Shah 2013). A certain deterministic version of the ranking problem is also studied as the sorting problem which is central in theoretical computer science.\n1. Random Utility (RU) models: Starting with the seminal work of (Bradley and Terry 1952), the Bradley-TerryLuce (BTL) model has become a landmark model for ranking. In the vanilla version of this model, the probability that item i beats item j is given by Pij = wiwi+wj where w ∈ Rn+ is the parameter vector to be estimated from data; the ith entry in w denotes the score associated with item i. Thurstone (Thurstone 1927) model is also a wellknown statistical model; here, Pij = Φ(si − sj) where Φ is the standard normal Cumulative Distribution Function (CDF) and s ∈ Rn is the score vector. These classic models fall under the so-called Random Utility (RU) Models (Marschak and others 1959).\n2. Item Feature (IF) models: Extending the BTL model, statistical models that utilize side information are presented in (Cattelan 2012). Recently, (Chen and Joachims 2016) presented the blade-chest ranking model which studied the stochastic intransitive setting. Their algorithm involves regularized maximum likelihood estimation for which tight sample complexity properties are not known. Despite the above works, to the best of our knowledge, there are no known models utilizing feature information while have provable sample-efficient algorithms for estimation and ranking.\n3. Low Rank (LR) models: Recently, (Rajkumar and Agarwal 2016) – unifying classic models such as BTL and Thurstone models – defined a generic class of preference matrices which have low rank under transformations involving suitable link functions. Upon such a transformation, connections of the ranking problem to matrix completion theory become clear. Subsequently, they use wellknown matrix completion results to derive sample complexity guarantees for ranking. However, their model does not utilize side information that may be available.\nThis list is by no means exhaustive; while there exist several other ranking methods (eg, ranking-SVM(Joachims 2002)), there are no known sample complexity guarantees associated with these.\nInductive Matrix Completion: The matrix completion task (Candès and Recht 2009) is to fill-in the missing entries of a partially observed matrix, which is possible efficiently under a low-rank assumption on the underlying matrix. Oftentimes, side information may be available which further makes this task potentially easier. This is the Inductive Matrix Completion (IMC) problem which is formally defined as the optimization problem, Ẑ = arg minZ `((A >ZB)ij ,Mij) where A ∈ Rd1×n1 and B ∈\nRd2×n2 are known feature matrices, Z ∈ Rd1×d2 is a rank-r unknown latent parameter matrix, (i, j) ∈ Ξ ⊆ [n] × [n] is the support set corresponding to the (uniformly sampled) observed entries and ` is any loss function, the squared loss being the most commonly chosen one. Once the estimate Ẑ is obtained using the training set indexed by Ξ, predictions may then be performed as M̂ij = (A>ẐB)ij for any (i, j) ∈ Ξc. The known solution techniques with recovery guarantees are:\n1. Non-convex algorithm (via alternating minimization): This approach entails parameterizing Z = UV> and performing alternating projected least squares updates on U and V. The tightest known guarantee for this approach involves a sample complexity of Ω(d2r3κ2 log(d)) and a convergence rate ofO(log(1/ )) (Jain and Dhillon 2013).\n2. Convex relaxation (via trace-norm formulation): This approach entails relaxing the rank constraints to a nuclear norm penalty. Existence of a unique optimum can be shown with high probability (Xu, Jin, and Zhou 2013) and is characterized a sample complexity of Ω(dr log(d) log(n)). Despite the non-smoothness, a subgradient descent algorithm provably converges with a rate of O(1/ √ ) (Ji and Ye 2009). Noisy features are handled\nin (Chiang, Hsieh, and Dhillon 2015)."
    }, {
      "heading" : "Feature-aware Ranking",
      "text" : ""
    }, {
      "heading" : "Notation and Preliminaries",
      "text" : "General notation: Unless stated otherwise, we use lowercase letters for scalars, upper-case letters for universal constants, lower-case bold-face letters for vectors and uppercase bold-face letters for matrices; specifically, P denotes a preference matrix. For any matrix M ∈ Ra×b, let ‖M‖∞ = maxi,j |Mij |, ‖M‖∗ = ∑min{a,b} i=1 σi(M) where σi(M) are\nthe singular values of M and ‖M‖F = √∑a i=1 ∑b j=1M 2 ij . I denotes the identity matrix whose dimensions would be implied from the context; similarly, depending on the context, 0 denotes a vector or matrix of zeros of the appropriate dimension. Next, let Pmin = mini 6=j Pij and ∆ = mini 6=j |ψ(Pij)− ψ(1/2)|. Let Ξ be the support set of the observed entries of a matrix and let m = |Ξ|. Define projection of a matrix on the support set B = RΞ(A) as: Bij = Aij if (i, j) ∈ Ξ and Bij = 0 if (i, j) /∈ Ξ. Items and features: Let n be the number of items to be ranked. Let Sn denote the symmetric group on n items. Let each item have a d-dimensional feature vector associated with it, ie, fi ∈ Rd,∀i ∈ [n]; concatenating these, we obtain the feature matrix F = [f1, . . . , fn] ∈ Rd×n. Link functions: Any ψ : [0, 1] → R which is a strictly increasing bijective function is a valid link function. For example, ψ could be the logit function, which is the inverse of the sigmoid function, defined as, ψ(x) := log ( x\n1−x\n) for\nx ∈ [0, 1]; another example is the probit function defined as ψ(x) = Φ−1(x) where Φ is the standard normal CDF. When we apply the link function to a matrix, we mean that the transformation applied entry-wise.\nPreference matrices: Let Pn := {P ∈ [0, 1]n×n|Pij + Pji = 1} denote the set of all pairwise preference matrices over n items. Let the set of stochastic-transitive matrices be PSTn := {P ∈ Pn|Pij > 1/2, Pjk > 1/2 =⇒ Pik > 1/2} and the set of stochastic-intransitive matrices be PSIn := {P ∈ Pn|Pij > 1/2, Pjk > 1/2 =⇒ Pik < 1/2}. Let PRUn be the set of preference matrices associated with unary random utility models (which are described in the next section). Let PIFn := {P ∈ Pn|Pij = ψ−1(w>(fi − fj))} for some w ∈ Rd.\nLet r ≤ n. Define the set of preference matrices having rank-r under the link function ψ as Pn (ψ, r) := {P ∈ Pn| rank (ψ(P)) ≤ r}. Next, define the set of preference matrices having rank-r under the link function ψ with the associated feature matrix A ∈ Rd×n as Pn (ψ, r,A) := {P ∈ Pn (ψ, r) |ψ(P) = A>LA} where L ∈ Rd×d is an unknown rank-r latent matrix (which is a function of the parameters of the ranking model) and A = [a1, . . . ,an] ∈ Rd×n is the known feature matrix whose ith column is the feature vector corresponding to the ith item. Let κ = σmin(A)/σmax(A) be the inverse condition number of the feature matrix A. Let i P j iff Pij > 1/2. Denoting the indicator function by 1, we define the distance between a permutation σ ∈ Sn and a preference matrix P ∈ Pn as:\ndist (σ,P) := ( n 2 )−1∑ i<j 1 ((i P j) ∧ (σ(i) σ(j)))\n+ ( n 2 )−1∑ i<j 1 ((j P i) ∧ (σ(j) σ(i)))\nNote that the above distance measure essentially counts the fraction of pairs on which σ and P disagree, and can be thought of as a normalized 0− 1 loss function."
    }, {
      "heading" : "Feature Low Rank Model",
      "text" : "Random Utility (RU) models, arising in discrete choice theory, dating back to (Marschak and others 1959), characterize the probability of an item i beating item j, Pij , using a prior on the (latent) score associated with those items, wi ∈ R and wj ∈ R. The most popular pairwise ranking models including BTL and Thurstone models fit in this framework. In particular, it is well-known that if wi ∼ Gumbel(0, 1), we obtain the BTL model; for completeness, we justify it below:\nPij = Pr(wi > wj) = Pr(wi − wj > 0) ξ1 =\ne−(wi−wj)\n1 + e−(wi−wj)\nwhere ξ1 follows from the fact that the difference of two independent standard Gumbel distributed random variables follows the standard logistic distribution. Similarly, if wi ∼ N (0, 1), we obtain the Thurstone model. The underlying commonality in these models is the simple observation that the prior distribution is on the scores, which are unary terms. Notably, the recent result by (Rajkumar and Agarwal 2016) shows that under the inverse transformation of the CDF of the difference of the latent score variables, the preference probability matrix is low-rank for BTL and Thurstone models. Further, they extended this result to a broader class of\nlow-rank models in which the preference matrices are lowrank when the link function is set to be this inverse CDF.\nOne angle of motivation for this paper stems from the intuitive thought that the scores associated with an item i in RU models can be generalized to functions involving, not just unary terms but also, pairwise terms, ie, the score of item i with respect to item j is given by an energy function Eij that has a bilinear form. From this point onwards, for simplicity, we detail the generalization of the RU models encompassing the BTL model, ie, we posit that Eij has a standard Gumbel distribution and consequently, we choose the link function ψ to be the logit function. It is noteworthy that our results will hold under any link function for the corresponding prior.\nWe now propose the enery-based generative model, which we call Feature Low Rank (FLR) model, defined via the preference matrix specified as follows:\nPij = e−Eij\ne−Eij + e−Eji (1)\nHere, we define the energy function associated with the pair of items (i, j) to be of the formEij := f>i w+f > i Wfj where w ∈ Rd and W ∈ Rd×d are the unknown latent parameters (vector and matrix parameters repectively) to be estimated, and fi and fj are the known feature vectors associated with items i and j respectively. It is clear from Equation (1) that a key advantage of the proposed model is the additional ability to incorporate side information in the form of feature vectors and feature correlations in a latent space described by W. In matrix notation,\nψ(P) = (1g> + F>W>F)− (g1> + F>WF) = (ΣV>)>L(ΣV>) (2)\nwhere g := F>w is column vector in Rn, 1 ∈ Rn is the all-ones column vector, F = UΣV> is the full SVD of F (such that U ∈ Rd×d,V ∈ Rn×n are orthonormal matrices with Σ ∈ Rd×n as the d × d diagonal matrix of singular values padded with zeros) and L := U>(1w> − w1> + W> −W)U (such that Σ−1ii = σ −1 i and Σ −1 ij = 0 if i 6= j). It is now clear that the sufficient condition for P ∈ Pn(ψ, r,ΣV>) is that rank(L) ≤ r. Now, we describe the generality of the FLR model in Equation (1) by showing that it subsumes many existing models and has much more expressiveness.\nProposition 1. The LR model is a special case of the FLR model, ie, Pn(ψ, r) ⊆ Pn(ψ, r,A). Corollary 1. Let F = I and ψ be the logit link function. From Proposition 1, it is easy to see the following special cases from Equation (1).\n1. Let W = xy>. If w = 0, then P ∈ Pn (ψ, 2). If w 6= 0, then P ∈ Pn (ψ, 4). 2. If W is symmetric, then W −W> = 0 and hence P ∈ Pn (ψ, 2). 3. Let Λ be a diagonal r× r matrix; let {X,Y} ∈ Rn×r be orthonormal matrices. If W = XΛr×rY> + M where M is a symmetric matrix, then ψ(P) ∈ Pn (ψ, 2r + 2).\nProposition 2. The unary RU models are special cases of the FLR model, ie, PRUn ⊆ Pn(ψ, r,A). Corollary 2. The BTL and Thurstone models are obtained as special cases of the FLR model under the logit and the probit transormations of P respectively. This follows from Proposition 1 (or Corollary 1-part (1)) above together with Propositions 6 and 7 of (Rajkumar and Agarwal 2016).\nProposition 3. Regression-based models with item-specific features in (Cattelan 2012) are special cases of the FLR model, ie, PIFn ⊆ Pn(ψ, r,A). Corollary 3. Let d n. Then we recover the blade-chest model (Chen and Joachims 2016) as a special case of the FLR model by setting rank(W) = O(d) and w = 0. Next, when d ≥ n, it is clear from Theorem 1 of (Chen and Joachims 2016) that such preference matrices degenerate into matrices in Pn(ψ, n,A) where ψ is the logit function. Moreover, it is easy to see that the FLR model admits both stochastic-transitive and stochastic-intransitive preference matrices.\nDue to space constraints, proofs of Propositions 1, 2 and 3 are given in the appendix. To summarize, we have shown how to instantiate several previously proposed ranking models as special cases of our FLR model in Table 1."
    }, {
      "heading" : "Problem Setup and Solution Approach",
      "text" : "Once we have the generative ranking model as developed in the previous section, the objective in our learning problem is then to find the permutation of n items that minimizes the number of violations with respect to the true underlying preference matrix P, ie, to find the best ranking σ̂ in the sense that,\nσ̂ = arg min σ dist(σ,P)\nThe input is the pairwise comparison dataset S = {(i, j, ykij)} which consists of comparison results of pairs (i, j) from a survey involving K users where each user with index k assigns ykij = 1 if he prefers i to j and y k ij = 0 if he prefers j to i. Note that it is not necessary that all pairs of items be compared; our algorithm is able to handle noisy and incomplete data. Since the true preference matrix P is unknown, our algorithm instead proceeds by using the empirical preference matrix P̂ computed from the available ykij ; it is to be noted, even then, our analysis guarantees that\ndist(σ̂,P) is good as opposed to just dist(σ̂, P̂). Additionally, in our inductive setting, the feature information is encoded by fi ∈ Rd for every item i and concatenated to form the feature matrix F ∈ Rd×n."
    }, {
      "heading" : "Algorithm",
      "text" : "We present our main algorithm for inductive ranking in Algorithm 3. The input data consist of the set of pairwise comparison results S = {(i, j, {ykij})}, (i, j) ∈ Ξ ⊆ [n] × [n], k ∈ [K], ykij ∈ {0, 1} and the feature matrix F ∈ Rd×n. The algorithm assumes the link function and the rank as input parameters. The subroutines used are:\n1. Noisy matrix completion with features (Subroutine 1): Note that to solve our ranking problem and derive the associated recovery guarantee, it suffices, as we have done, to use the specified trace-norm program as a black-box method; hence, we assume that we have access to an oracle that gives us the solution to the convex program. The details of how the solution to this program may be found numerically is beyond the scope of this work – for further details regarding some possible sub-gradient algorithms, we refer the reader to (Chiang, Hsieh, and Dhillon 2015) and (Ji and Ye 2009).\n2. γ-approximate pairwise ranking procedure (Subroutine 2): Let σ̂ ∈ Sn be the output of any Pairwise Ranking (PR) procedure with respect to an underlying preference matrix P. For a constant γ > 1, σ̂ is said to be γ-approximate if dist(σ̂,P) ≤ γminσ∈Sn dist(σ,P). Any constant factor approximate ranking procedure maybe used. Specifically, we use the Copeland procedure (Copeland 1951) as a black-box method which has a 5-approximation guarantee (Coppersmith, Fleischer, and Rudra 2006). This method involves simply sorting the items according to a score which is computed for every item i as ∑n j=1 1(P̂ ij > 1/2)."
    }, {
      "heading" : "Analysis",
      "text" : "In this section, we state and prove our main result. Theorem 1 (Guaranteed rank aggregation with sub-linear sample complexity using item features). Let P ∈ (Pn(ψ, r,A)∩PSTn ) be the true underlying preference matrix according to which the pairwise comparison dataset S = {(i, j, {ykij})} is generated. Let ψ be L-Lipschitz\nSubroutine 1 IMC: Inductive Matrix Completion Input: Mij for (i, j) ∈ Ξ ⊆ [n]× [n], feature matrix F. Output: Completed matrix M.\n1: Solve the convex program:\nẐ = arg min ZL ∥∥RΞ(M− F>ZLF)∥∥2F s.t. ‖ZL‖∗ ≤ CL 2: return ψ(P̂)← F>ẐF.\nSubroutine 2 PR: Pairwise Ranking (Copeland Procedure) Input: Preference matrix M ∈ Rn×n. Output: Ranking σ̂.\n1: Threshold: ∀(i, j), M̃ij ← 1(Mij > 1/2). 2: Compute row-sum of M̃: v← M̃1. 3: return σ̂ ← Sort(v).\nin [Pmin2 , 1 − Pmin 2 ]. Let Ξ be the set of pairs of items compared such that the number of pairs compared is |Ξ| = m > 48C 2 2d 2 log(n)(1+γ)2\nκ8 2∆4 where Ξ is chosen uniformly at random from among all possible subsets of item pairs of size m. Let each pair in Ξ be compared independently K ≥ 16(1+γ)mL\n2 log(n) n2∆2 times where ∆ =\nmini 6=j |ψ(Pij)− ψ(1/2)|. Then, with probability atleast 1 − 3/n3, for any > 0, Algorithm 3 returns an estimated permutation σ̂ such that dist(σ̂,P) ≤ . Remark 1. The key take-away message in Theorem 1 is the reduction in sample complexity possible due to efficient utilization of features and feature correlations, associated with the items to be ranked, by Algorithm 3. For instance, when d = O(1), which is often the case in practice, we reduce the required total number of comparisons to be made to Ω(log(n)). Thus, we achieve a very significant gain since the total number of comparisons is poly-logarithmic as opposed to quadratic in the number of items. This is especially crucial in large-scale machine learning applications. Remark 2. Another point to be noted from Theorem 1 is that, under the uniform sampling assumption, when features associated with items are known, it is more important that we compare sufficient (precisely, Ω(log n)) number of different pairs rather than high number of comparisons per pair. Furthermore, he total number of comparisons needed in Theorem 1 is given by the product mK which is Ω(max(log n/ 2, d4 log3 n/ 3n2)). We now present the proof of Theorem 1. We shall prove the theorem under the Bernoulli sampling model (where each entry of an n × n matrix is observed independently with probability 1/n2) rather than the uniform sampling model (wherein Ξ is chosen uniformly at random from among all possible subsets of item pairs of size m); the equivalence between the two is well-known (see, for instance, Section 7.1 of (Candès et al. 2011)).\nProof. Let P̂ij be the empirical probability estimate of Pij . Note that we compute P̂ij = 1K ∑K k=1 y k ij for (i, j) ∈\nAlgorithm 3 IPR: Inductive Pairwise Ranking Input: Set of comparison results S = {(i, j, {ykij})}, fea-\nture matrix F, link function ψ, target rank r. Output: Ranking of n items, σ̂ ∈ Sn.\n1: Construct the partially observed empirical preference matrix using S as:\nP̂ij =  1 K ∑K k=1 y k ij if (i, j) ∈ Ξ 1 K ∑K k=1(1− ykij) if (j, i) ∈ Ξ\n1/2 if i = j or (i, j) /∈ Ξ\n2: Compute SVD of F = UΣV> and set A← ΣV>. 3: Use a noisy inductive matrix completion subroutine: ψ(P̂)← IMC(ψ(P̂),A). 4: Take the inverse transform of the truncated r-SVD of the completed matrix estimate: Q← ψ−1(Pr(ψ(P̂))). 5: Using a pairwise ranking subroutine: σ̂ ← PR(Q). 6: return σ̂.\nΞ from the given pairwise comparison dataset, S = {(i, j, {ykij})}. From Equation (2), ψ(P) = A>LA where A = ΣV>. Since we use the empirical estimate for Pij , we have noise due to sampling error only over Ξ, ie, ψ(P̂) = ψ(P) + N = A>LA + N where\n|Nij | = { 0 if (i, j) /∈ Ξ∣∣∣ψ( 1K ∑Kk=1 ykij)− ψ(Pij)∣∣∣ if (i, j) ∈ Ξ\nNow, we solve the trace-norm regularized convex program corresponding to the noisy inductive matrix completion problem:\n{L,N} = arg min ZN,ZL ∥∥∥RΞ(ψ(P̂)− (A>ZLA + ZN))∥∥∥2 F\n+ λL ‖ZL‖∗ + λN ‖ZN‖∗ and let ψ(P̂) = A>LA + N be the link-transformed completed (estimate) matrix where N be the estimated noise matrix. This is equivalent to solving the problem:\n{L,N} = arg min ZN,ZL ∥∥∥RΞ(ψ(P̂)− (A>ZLA + ZN))∥∥∥2 F\ns.t. ‖ZL‖∗ ≤ CL, ‖ZN‖∗ ≤ CN We set CN = 0 and CL = ∥∥∥(A>)†ψ(P̂)(A)†∥∥∥\n∗ which\nmay be upper bounded, by Lemma 3 of (Chiang, Hsieh, and Dhillon 2015) as CL ≤ dC′κ4 for a constant C\n′. We now recall Theorem 1 from (Chiang, Hsieh, and Dhillon 2015). Let δ < 1/d and A be well-conditioned, specifically, κ4 ≤ C2d for constant C2. The expected squared loss under Bernoulli sampling is bounded as, with probability at least 1− δ:∥∥∥ψ(P̂)− ψ(P̂)∥∥∥2\nF n2 ≤ C1 min\n( CN √ log(2n)\nm ,\n√ CN √ n\nm\n)\n+ C2d\nκ4\n√ log(2/δ)\nm (3)\nwhere C1 and C2 are constants. By triangle inequality,∥∥∥ψ(P̂)− ψ(P̂)∥∥∥ F = ∥∥∥ψ(P̂)− (ψ(P) + N)∥∥∥ F\n≥ ∥∥∥ψ(P̂)− ψ(P)∥∥∥\nF − ‖N‖F\nUsing CN = 0 in Equation (3), with probability at least 1− δ,\n1\nn ∥∥∥ψ(P̂)− ψ(P)∥∥∥ F ≤ ( C2d κ4 √ log(2/δ) m )1/2 + 1 n ‖N‖F\nLet K ≥ mL 2 log(n) τ2 where τ = n\n√\n1+γ ∆ 4 . Substituting the\nbounds for the N terms from Lemma 1 and using the union bound, with probability at least 1− δ − 1/n3,\n∥∥∥ψ(P̂)− ψ(P)∥∥∥ F ≤ n ( C2d κ4 √ log(2/δ) m ) 1 2 + τ\n≤ n\n( C2d\nκ4\n√ log(2/δ)\nm\n) 1 2\n+ n\n√\n1 + γ\n∆\n4\nNow, setting m > 16C 2 2d 2 log(2/δ)(1+γ)2\nκ8 2∆4 and δ = 2/n 3, we obtain, with probability 1− 3/n3, ∥∥∥ψ(P̂)− ψ(P)∥∥∥\nF ≤ n √\n1+γ ∆ 2 . Then, arguments similar to the proof of Theorem\n13 of (Rajkumar and Agarwal 2016) yield our result.\nLemma 1 (Characterization of noise due to finite-sample effects). Under the conditions of Theorem 1, let m item pairs be compared such that the number of comparisons per item pair is K ≥ mL\n2 log(n) τ2 . Then, with probability atleast\n1− 1/n3, ‖N‖F ≤ τ . Due to space limitations, the proof of Lemma 1 is given in the appendix."
    }, {
      "heading" : "Experimental Results",
      "text" : "In this section, we conduct a systematic empirical investigation of the performance of our ranking method and justify our theoretical claim in the previous section. The goal of this study is two-fold: (a) to verify the correctness of our algorithm, and (b) to show that by using features and feature correlations, our IPR algorithm has a better sample complexity thereby improving upon the LRPR algorithm that does not take into account the available side information."
    }, {
      "heading" : "Synthetic Simulations",
      "text" : "For a given set of n = 500 items, we consider three main problem parameters: (1) m – the number of item pairs compared (Figure 1), (2)K – the number of comparisons per pair (Figure 2), (3) d – the dimensionality of features (Figure 3). We study the performance of both IPR and LRPR algorithms by varying each of the problem parameters while fixing the others. We note that by making use of side information, IPR outperforms LRPR in all the cases as shown in the sample complexity plots. All the accuracy results presented are obtained by averaging over five runs.\nData generation: We consider three representative preference matrices derived from Equation (1): (a) Model-1: we set W = 0, (b) Model-2: we construct a general W ; here, we generate Wij∼U(0, 1), and (c) Model-3: we construct a low-rank W, ie, rank(W) = 2 < d; here we generate Wij∼U(0, 1) and then truncating W by setting all but its top two singular values to zero. In all the three models, we generate wi∼U(0, 1). The features are generate as Fij∼U(0, 1); to ensure that the features are wellconditioned, we perform the full SVD of feature matrix F and set all its singular values to 1.\nParameter settings: For IPR, we choose λL = 10−2 and λN = 102. Note that LRPR allows for the rank of the problem to be automatically determined. In the same spirit, though Step-5 of Algorithm 3 requires the knowledge of\nthe true rank, we choose not to perform this truncation step thereby including the error induced by the smaller singular values resulting from noise due to sampling in our distance estimate – even then, IPR outperforms LRPR."
    }, {
      "heading" : "Real-data Simulations",
      "text" : "We apply our method on two popular preference learning datasets. We briefly describe the data and the results (Figure 4) we obtain below:\n1. Sushi: This data (Kamishima and Akaho 2009) is from a survey of 5000 customers. Each customer orders 10 sushi dishes according to their preferences. The goal, then, is to estimate a global ranking of these sushi dishes using these observations from customers. Each sushi has six features such as price, taste and so on. We construct the complete preference matrix P ∈ [0, 1]10×10 using the preferences of all the customers and consider this to be ground truth preference matrix. An interesting observation was that, over five runs of the algorithms, IPR gets two out of the top four sushi dishes right most of the times namely, ‘amaebi’ and ‘ikura’; on the other hand, LRPR does not succeed in recovering these always.\n2. Car: The task in this dataset (Abbasnejad et al. 2013) is find an order of preference among ten cars. This data was collected by surveying 60 customers regarding there preferences among pairs of cars drawn from the set of ten cars. Each car has four features including engine, transmission and so on. We construct the ground truth preference matrix P ∈ [0, 1]10×10 by aggregating the pairwise preferences of all the customers. An interesting trend we found was that customers generally preferred sedans over SUVs and non-hybrid vehicles over hybrid vehicles."
    }, {
      "heading" : "Discussion and Future Directions",
      "text" : "In this paper, we have proposed and characterized the FLR model together with the guaranteed IPR algorithm that utilizes available side information of the items to be ranked to provably reduce the sample complexity for ranking from Ω(n log n) to possibly as low as Ω(log n). A future research direction is to see if mixture models for ranking such as the recently proposed topic modeling approach (Ding, Ishwar, and Saligrama 2015) could fit into our framework while admitting sample-efficient estimation algorithms."
    }, {
      "heading" : "Proof of Proposition 1",
      "text" : "Proof. We prove this by showing that every P ∈ Pn(ψ, r) is in Pn(ψ, r,A) but not the other way around. By the definition of a preference matrix corresponding to the LR model, if P ∈ Pn(ψ, r), then rank(ψ(P)) ≤ r. Similarly, for the FLR model, if P ∈ Pn(ψ, r,A), then ψ(P) = A>LA and rank(ψ(P)) ≤ r; in other words, rank(L) ≤ r. Now setting A = I, we have Pn(ψ, r) = Pn(ψ, r,A). On the other hand, if A 6= I, we have Pn(ψ, r) ( Pn(ψ, r,A)."
    }, {
      "heading" : "Proof of Proposition 2",
      "text" : "Proof. Let w be the unary score vector in RU models. The result then follows by setting the energy function of item i with respect to item j in the FLR model to be the unary score corresponding to item i in the RU model, ie, by simply setting F = I and W = 0 which leads to Eij = wi."
    }, {
      "heading" : "Proof of Proposition 3",
      "text" : "Proof. This is immediate by setting W = 0. For concreteness, we choose ψ to be the logit link function. Setting W = 0 in Equation 1, we obtain\nPij = e−w\n>fi\ne−w>fi + e−w >fj\nObserve that ψ(Pij) = w>fj − w>fi. Writing this in matrix notation, ψ(P) = 1w>F − F>w1>. Note that ψ(P) ∈ Rn×n is a rank-2 skew-symmetric matrix. Let L := (VΣ−1)>ψ(P)(VΣ−1) = U>(1w> − w1>)U. Now, note that L ∈ Rd×d is also a rank-2 skew-symmetric matrix. Thus, P ∈ Pn(logit, 2,ΣV>) since ψ(P) = (ΣV>)>L(ΣV>). In addition, note that the FLR model accounts for feature correlations when W 6= 0."
    }, {
      "heading" : "Proof of Lemma 1",
      "text" : "Proof. For any support Ξ, define the following event:\nGΞ := (∣∣∣P̂ij − Pij∣∣∣ < Pmin/2 ∀(i, j) ∈ Ξ)\nBy Hoeffding’s bound, Pr(GΞ) ≥ 1 − 12n3 whenever K ≥ 11 log(n)/P 2min. Let L be the Lipschitz constant of ψ and set K ≥ mL\n2 log(n) τ2 . Using the inequality that ‖N‖F ≤\n√ m ‖N‖∞, we have\nPr(‖N‖F ≥ τ) ≤ Pr ( ‖N‖∞ ≥\nτ√ m ) = Pr ( ∃(i, j) ∈ Ξ : ∣∣∣ψ(P̂ij)− ψ(Pij)∣∣∣ ≥ τ√ m\n) ≤\n∑ (i,j)∈Ξ Pr (∣∣∣ψ(P̂ij)− ψ(Pij)∣∣∣ ≥ τ√ m ∣∣∣GΞ)Pr(GΞ) + Pr(GcΞ)\n≤ ∑\n(i,j)∈Ξ\nPr (∣∣∣P̂ij − Pij∣∣∣ ≥ τ L √ m ∣∣∣GΞ)Pr(GΞ) + 1 2n3\n≤ ∑\n(i,j)∈Ξ\nPr (∣∣∣P̂ij − Pij∣∣∣ ≥ τ L √ m ) + 1 2n3\n≤ 1 2n3 + 1 2n3 = 1 n3"
    } ],
    "references" : [ {
      "title" : "E",
      "author" : [ "E. Abbasnejad", "S. Sanner", "Bonilla" ],
      "venue" : "V.; Poupart, P.; et al.",
      "citeRegEx" : "Abbasnejad et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "M",
      "author" : [ "R.A. Bradley", "Terry" ],
      "venue" : "E.",
      "citeRegEx" : "Bradley and Terry 1952",
      "shortCiteRegEx" : null,
      "year" : 1952
    }, {
      "title" : "and Recht",
      "author" : [ "E.J. Candès" ],
      "venue" : "B.",
      "citeRegEx" : "Candès and Recht 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "E",
      "author" : [ "Candès" ],
      "venue" : "J.; Li, X.; Ma, Y.; and Wright, J.",
      "citeRegEx" : "Candès et al. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A",
      "author" : [ "Caragiannis, I.", "Procaccia" ],
      "venue" : "D.; and Shah, N.",
      "citeRegEx" : "Caragiannis. Procaccia. and Shah 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Joachims",
      "author" : [ "S. Chen" ],
      "venue" : "T.",
      "citeRegEx" : "Chen and Joachims 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "I",
      "author" : [ "K.-Y. Chiang", "C.J. Hsieh", "Dhillon" ],
      "venue" : "S.",
      "citeRegEx" : "Chiang. Hsieh. and Dhillon 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A",
      "author" : [ "Copeland" ],
      "venue" : "H.",
      "citeRegEx" : "Copeland 1951",
      "shortCiteRegEx" : null,
      "year" : 1951
    }, {
      "title" : "Ordering by weighted number of wins gives a good ranking for weighted tournaments",
      "author" : [ "Fleischer Coppersmith", "D. Rudra 2006] Coppersmith", "L. Fleischer", "A. Rudra" ],
      "venue" : "In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm,",
      "citeRegEx" : "Coppersmith et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Coppersmith et al\\.",
      "year" : 2006
    }, {
      "title" : "A topic modeling approach to ranking",
      "author" : [ "Ishwar Ding", "W. Saligrama 2015] Ding", "P. Ishwar", "V. Saligrama" ],
      "venue" : "In AISTATS",
      "citeRegEx" : "Ding et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2015
    }, {
      "title" : "I",
      "author" : [ "P. Jain", "Dhillon" ],
      "venue" : "S.",
      "citeRegEx" : "Jain and Dhillon 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Ye",
      "author" : [ "S. Ji" ],
      "venue" : "J.",
      "citeRegEx" : "Ji and Ye 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and Akaho",
      "author" : [ "T. Kamishima" ],
      "venue" : "S.",
      "citeRegEx" : "Kamishima and Akaho 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and Boutilier",
      "author" : [ "T. Lu" ],
      "venue" : "C.",
      "citeRegEx" : "Lu and Boutilier 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "R",
      "author" : [ "Luce" ],
      "venue" : "D.",
      "citeRegEx" : "Luce 1959",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Binary choice constraints on random utility indicators",
      "author" : [ "Marschak", "J others 1959] Marschak" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Marschak and Marschak,? \\Q1959\\E",
      "shortCiteRegEx" : "Marschak and Marschak",
      "year" : 1959
    }, {
      "title" : "and Agarwal",
      "author" : [ "A. Rajkumar" ],
      "venue" : "S.",
      "citeRegEx" : "Rajkumar and Agarwal 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "L",
      "author" : [ "Thurstone" ],
      "venue" : "L.",
      "citeRegEx" : "Thurstone 1927",
      "shortCiteRegEx" : null,
      "year" : 1927
    }, {
      "title" : "Speedup matrix completion with side information: Application to multi-label learning",
      "author" : [ "Jin Xu", "M. Zhou 2013] Xu", "R. Jin", "Z.-H. Zhou" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Xu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We study the problem of ranking a set of items from nonactively chosen pairwise preferences where each item has feature information with it. We propose and characterize a very broad class of preference matrices giving rise to the Feature Low Rank (FLR) model, which subsumes several models ranging from the classic Bradley–Terry–Luce (BTL) (Bradley and Terry 1952) and Thurstone (Thurstone 1927) models to the recently proposed blade-chest (Chen and Joachims 2016) and generic low-rank preference (Rajkumar and Agarwal 2016) models. We use the technique of matrix completion in the presence of side information to develop the Inductive Pairwise Ranking (IPR) algorithm that provably learns a good ranking under the FLR model, in a sample-efficient manner. In practice, through systematic synthetic simulations, we confirm our theoretical findings regarding improvements in the sample complexity due to the use of feature information. Moreover, on popular real-world preference learning datasets, with as less as 10% sampling of the pairwise comparisons, our method recovers a good ranking.",
    "creator" : "LaTeX with hyperref package"
  }
}