{
  "name" : "1704.05204.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "HPSLPred: An Ensemble Multi-label Classifier for Human Protein Subcellular Location Prediction with Imbalanced Source",
    "authors" : [ "Shixiang Wan", "Quan Zou" ],
    "emails" : [ "shixiangwan@gmail.com,", "zouquan@nclab.net" ],
    "sections" : [ {
      "heading" : null,
      "text" : "traditional biology experiments are expensive and time-consuming, so more and more research interests tend to a series of machine learning approaches for predicting protein subcellular location. There are two main difficult problems among the existing state-of-the-art methods. First, most of the existing techniques are designed to deal with the multi-class but not the multi-label classification, which ignores the connection between the multiple labels. In reality, multiple location proteins implicate that there are vital and unique biological significances worthy of special focus, which cannot be ignored. Second, the techniques for handling imbalanced data in multi-label classification problem is significant but less. For solving the two issues, we have developed an ensemble multi-label classifier called HPSLPred which can be applied for the multi-label classification with imbalanced protein source. For the conveniences of users, a user-friendly webserver for HPSLPred was established at http://server.malab.cn/HPSLPred.\nKeywords: subcellular location, ensemble classifier, multi-label, imbalance source"
    }, {
      "heading" : "1. Introduction",
      "text" : "Identification of protein subcellular localization has a key role in genome annotation, function prediction and vaccine target identification. In briefly, there are two significant aspects. First, the rare and necessary perceptiveness or hints can be discovered from subcellular location information. Normally, the proteins have the exclusive localizations after synthesized in ribosome1. If not, the aberrant subcellular location may result in diseases2. Second, the interaction between proteins exposes the molecular function mechanism and complex physiological processes3. Recognizing the protein functions based on the context subcellular location environment is one of the most important and hottest research issues in proteomics and drug design research4.\nThe traditional biology experiments like cell culture, cell separation and protein extraction are costly and time-consuming5, so more and more research interests tend to a series of machine learning approaches for predicting protein subcellular location. Meanwhile, protein sequences indexed has an immense number. Specifically, there are 553,231 protein sequences now according to the release on 01- Dec-2016 at UniProtKB6, which is far more than 3,939 protein sequences in 1986. With the rapid increment of protein sequences, there is a strong demand to evolve more accurate and efficient computational approaches based on machine learning.\nDuring the past several decades, a series of approaches surged on subcellular localization7-14, which have indelible contributions on bioinformatics though these works have their own limitations. Simply, we divide all state-of-the-art works into two types: (1). work on improving precision based more subcellular location sites, which is from only two sites8 to five sites9, to twelve sites10,15 and so on. (2). integrate more different feature methods to make sequences express more comprehensively, such as the amino acid composition method9,16, the pseudo amino acid composition method17, the various modes18-26, and the sequential evolution information27.\nAlthough it is abundant in subcellular location methods, two main issues need to be ameliorated. First, most methods are based on multi-class prediction line, which implicates that a protein sequence will only be classified in one location site. Actually, a lots of protein sequences exist in multiple sites simultaneously, which has proved by Millar et al28. We cannot ignore the influences in separate sites, maybe they act a unique biological role in one site and worthy of particular research29,30. Second, the techniques for handling imbalanced data in multi-label classification problem is significant but less.\nImbalanced source may result in terrible recall rate for the fewer samples’ label and discrediting the classification results31. Based on machine learning technique, we propose a novel and ensemble multilabel classifier HPSLPred for human protein subcellular location prediction with imbalanced source to improve such two shortcomings. Generally, there are four parts to construct a protein subcellular location prediction model32. First, data collection and cleaning. Second, feature extraction from protein sequences. Third, blueprint a multi-label classifier for prediction. Forth, build a user-friendly webserver.\nIn this paper, we propose a novel and ensemble classifier with imbalanced source for human protein subcellular location prediction. The second section of this paper depicts the procedure of constructing original subcellular location source. In this section, after data processes, data cleaning and redundancy eliminating, we acquired 9,895 human protein sequences with the lowest similarity as the test dataset. Next, we introduce the algorithms of protein sequences feature extraction, the method about how to handle the imbalanced source, performance metrics and the novel approach of building multi-label ensemble classifier called HPSLPred, which is based on the compound features and optimal dimension searching. The third section shows the detailed results of HPSLPred compared with 188-dimensional classical features and PseAAC features. After comparing with state-of-the-art methods, our method achieves a higher accuracy, a lower running time and more user-friendly web server. The last section describes the future developmental trend on protein subcellular localization research."
    }, {
      "heading" : "2. Material and Methods",
      "text" : ""
    }, {
      "heading" : "2.1 Data construction with human protein",
      "text" : "A series of databases for protein subcellular location, such as UniprotKB6, LOCATE33, PSORTdb34, Arabidopsis Subcellular DB35, Yeast Subcellular DB36, Plant-PLoc37, LOCtarget38, LOC3D39, DBSubloc40, PA-GOSUB41 and so on. We collected human protein sequences through API provided from the UniprotKB database which is popular and comprehensive. After eliminating repeated samples, the number of original sequences is 11,689, including approximately 200 subcellular location. We statistic and sort each location’s number, then pick out the first ten locations as labels of protein sequences. The chosen ten locations respectively are: cytoplasm, nucleus, cell membrane, membrane,\nsecreted, cytoskeleton, cell projection, endoplasmic reticulum membrane, cell junction and mitochondrion. For more convenience expression, we name these locations respectively as: Class1, Class2, Class3, Class4, Class5, Class6, Class7, Class8, Class9 and Class10. The first fifty locations under space limitation and chosen ten locations are shown in Table 1.\nSince parts of protein subcellular locations do not belong to anyone of chosen ten locations, such sequences had been removed and 10,613 protein sequences were left. Then setting variant threshold and getting rid of redundance among sequences by CD-HIT42, we acquired 9,895 samples when the lowest threshold value is 0.7, which is our final test dataset \uD835\uDC9F. Compared with the integrated source1, the number of protein sequences in our dataset \uD835\uDC9F exceeds the sample’s number in their work. All location numbers under different threshold value are shown in Table 2.\nTable2. All location numbers under different threshold value in CD-HIT Threshold\nvalue Proteins’ number Class1 Class2 Class3 Class4 Class5 Class6 Class7 Class8 Class9 Class10\n0.9 10,324 3,523 3,687 1,755 1,740 1,151 838 544 573 467 397 0.8 10,132 3,459 3,626 1,693 1,716 1,113 822 534 564 459 395 0.7 9,858 3,374 3,520 1,611 1,677 1,090 800 521 550 443 392\nFrom Table 2, we can be aware of that there are parts of protein sequences belong to multiple labels. Suppose that the number of belonging to multiple i labels is \uD835\uDC41\uD835\uDC56 (\uD835\uDC56 = 1,2,3 … 10), we statistic \uD835\uDC41\uD835\uDC56 in Table 3.\n\uD835\uDC411 means the sample with belonging to one label, which is the single-label. \uD835\uDC41\uD835\uDC56 (2 ≤ \uD835\uDC56 ≤ 10) means the multi-label sample. The number of multi-label samples is about 30.60% of all samples, which cannot be ignored.\nIn traditional subcellular location works, many ripe methods converted a multi-label job into a multiclass job, relates a sample \uD835\uDCAE\uD835\uDC56 (\uD835\uDC56 ∈ \uD835\uDC5B, \uD835\uDC5B \uD835\uDC56\uD835\uDC60 \uD835\uDC61ℎ\uD835\uDC52 \uD835\uDC61\uD835\uDC5C\uD835\uDC61\uD835\uDC4E\uD835\uDC59 \uD835\uDC5B\uD835\uDC62\uD835\uDC5A\uD835\uDC4F\uD835\uDC52\uD835\uDC5F \uD835\uDC5C\uD835\uDC53 \uD835\uDC4E\uD835\uDC59\uD835\uDC59 \uD835\uDC60\uD835\uDC4E\uD835\uDC5A\uD835\uDC5D\uD835\uDC59\uD835\uDC52\uD835\uDC60) with a single-label \uD835\uDC3F\uD835\uDC57 (\uD835\uDC57 ∈ \uD835\uDC59, \uD835\uDC59 \uD835\uDC56\uD835\uDC60 \uD835\uDC61ℎ\uD835\uDC52 \uD835\uDC61\uD835\uDC5C\uD835\uDC61\uD835\uDC4E\uD835\uDC59 \uD835\uDC5B\uD835\uDC62\uD835\uDC5A\uD835\uDC4F\uD835\uDC52\uD835\uDC5F \uD835\uDC5C\uD835\uDC53 \uD835\uDC4E\uD835\uDC59\uD835\uDC59 \uD835\uDC59\uD835\uDC4E\uD835\uDC4F\uD835\uDC52\uD835\uDC59\uD835\uDC60). A single-label dataset \uD835\uDCAE is made up of n samples like (\uD835\uDCAE\uD835\uDC56 , \uD835\uDC3F\uD835\uDC57). It is apparent that multi-class classification will bring forth repeated sequences source when expressing different labels, which is superfluous to classifier. To solve this tissue, many approaches converted a multi-label job into more single-label jobs. All results will be converted back into multi-label statement after single-label classifications. There are a series of state-of-the-art conversion methods, such as Support Vector Machines43, Naive Bayes44 and k Nearest Neighbor45 methods. We will consider more single-label classifiers and multi-label classifiers with our dataset \uD835\uDC9F."
    }, {
      "heading" : "2.2 Handling imbalanced source",
      "text" : "Given that subcellular location problem is a multi-label problem, many methods transformed this problem into more single-label problems. There are a series of state-of-the-art methods on this issue, and all of them can be summarized as two fundamental methods46: the label combination method47,48 (CM) and the binary relevance method49-51 (BM).\nFor CM method, abstract and united (atomic) label is integrated from the original multiple labels. The label \uD835\uDC3F\uD835\uDC57(\uD835\uDC57 ∈ \uD835\uDC59) will be transformed into ℒ, which is the atomic label corresponding a diverse label subset. Advantage of this methods is that the coaction of all labels can be expressed as rich as possible, but it loses sight of individual influence which is crucial for subcellular location problem.\nFor BM method, it converses a multi-label issue into multiple one binary issues based each label. Multiple binary classifiers (\uD835\uDC361, \uD835\uDC362, … \uD835\uDC36\uD835\uDC59) will be trained respectively, each of which predict a binary value linked with each label \uD835\uDC3F\uD835\uDC57 ∈ \uD835\uDC59. Our dataset \uD835\uDC9F can be formulated as:\n\uD835\uDC9F = \uD835\uDC9F1⋃\uD835\uDC9F2⋃\uD835\uDC9F3⋃ … ⋃\uD835\uDC9F\uD835\uDC59 (\uD835\uDC59 \uD835\uDC56\uD835\uDC60 \uD835\uDC61\uD835\uDC5C\uD835\uDC61\uD835\uDC4E\uD835\uDC59 \uD835\uDC5B\uD835\uDC62\uD835\uDC5A\uD835\uDC4F\uD835\uDC52\uD835\uDC5F \uD835\uDC5C\uD835\uDC53 \uD835\uDC4E\uD835\uDC59\uD835\uDC59 \uD835\uDC59\uD835\uDC4E\uD835\uDC4F\uD835\uDC52\uD835\uDC59\uD835\uDC60) (1) Disadvantage of this methods is that BM disregards label interactions among every labels when forecasting. But for our dataset, there are no necessary to construct bridges for different subcellular locations since they are independent of each other. In addition, BM maybe generate imbalanced datasets for each label after transforming processes, which can be overcome showed in this paper but only more time complexity. Therefore, BM-based methods will be applied in our approach.\nNow, we will describe particular procedure about how to handle the imbalanced source generated by BM methods. The methods of handling imbalanced source are mainly two basic issues: (a). create a new\nclassifier algorithm or modify one or more the existing algorithms for transforming imbalanced presentation. (b). improving data preprocess such as resampling for dispelling imbalanced presentation. Modifying algorithm needs more complex work or more comprehensive source to validate its performance, while improving data preprocesses are more elastic and better in aiming at a specific problem. Besides, there are a series of ensemble approaches, such as altering ensemble classifier algorithm during some learning stages and inserting cost-sensitive learning process, applied for dealing with imbalanced datasets.\nEmpirically, some works have proved that resampling techniques are valid for equilibrating the class distribution. Moreover, these techniques are standalone with the specific classifier. Resampling techniques can be categorized into three families31: (a). Under-Sampling (US). (b). Over-Sampling (OS). (c). Hybrid-Sampling (HS). US method produces a subset from the original dataset to cut down superfluous class samples based majority class samples. OS methods produces a superset from the original dataset to increase majority class samples based superfluous class samples. Hence HS method will integrate US method and OS method and produce an eclectic dataset. Actually, the easiest preprocesses methods from all such three families are random under sampling and random over sampling. The weakness of the former is that all samples cannot be full use of classification, which will not produce an optimal dataset. Additionally, the later maybe result in overfitting because of existing sample copies according to some works52.\nFor solving the shortcoming of random over sampling, several methods were designed, such as ‘Synthetic Minority Oversampling Technique’ (SMOTE53), Borderline-SMOTE54, Adaptive Synthetic Sampling55, Safe-Level-SMOTE56 and SPIDER257 approaches. However, the same shortcoming of these methods is short of consideration about neighboring instances, which will result in the increment of overlapping between classes. Regarding random under sampling, based data cleaning line, the proposed approaches trend to remit the influence of losing useful data, such as the Wilson’s edited nearest neighbor (ENN58), the one-sided selection (OSS59), Support Vector Machines (SVM43) and so on. The main idea about these methods is that finding the superfluous class samples whose number is as same as the majority class samples and close to the decision boundary, combined balanced dataset with the majority class samples. According to figure 1, searching the balanced samples near the decision boundary based support vector machine algorithm is our approach on handling imbalanced source.\nThe majority class samples The superfluous class samples\nThe selected superfluous class samples whose number is as same as the majority class samples\nSupport Vector Machine\nDecision boundary Decision boundary\nSelected instances close to boundary\nFigure 1. Searching the balanced samples near the decision boundary based SVM algorithm\nAs a popular supervised learning algorithm, SVM was put forward by Vapnik60. SVM can generate a linear decision boundary to tell from all samples61. Additionally, the new test samples can be distinguished from classification regulation. If the test samples are not separable, the kernel function is able to map the samples to a high-order feature space until the optimal decision boundary can be ensured. There has been a mature theory and practice foundation on SVM methods, which is more understandable and convincing than loosely heuristic algorithms like some black boxes.\nIn this study, the LIBSVM package62 severs as an realization of SVM. The radial basis function\n(RBF) is taken as the kernel function, which is defined as:\n\uD835\uDCA6(\uD835\uDCCD\uD835\uDC56 , \uD835\uDCCD\uD835\uDC57) = \uD835\uDC52\uD835\uDC65\uD835\uDC5D (−\uD835\uDEFE‖\uD835\uDCCD\uD835\uDC56 − \uD835\uDCCD\uD835\uDC57‖ 2 ) (2)\nThe values of γ and regularization parameter \uD835\uDC9E in RBF function are optimized on the dataset by cross-validation. For the multi-state classification, the one-versus-one strategy is used.\nUp to now, we have particularly described about how to transform a multi-label problem with imbalanced source into multiple problems with balanced source. Suppose that a balanced source for the special label \uD835\uDCBE dealt with SVM is \uD835\uDC9F\uD835\uDCBE ℬ(\uD835\uDCBE ∈ \uD835\uDC59), which can be formulated as:\n\uD835\uDC9Fℬ = \uD835\uDC46\uD835\uDC49\uD835\uDC40(\uD835\uDC9F1)⋃\uD835\uDC46\uD835\uDC49\uD835\uDC40(\uD835\uDC9F2)⋃\uD835\uDC46\uD835\uDC49\uD835\uDC40(\uD835\uDC9F3) … ⋃\uD835\uDC46\uD835\uDC49\uD835\uDC40(\uD835\uDC9F\uD835\uDC59) (3) \uD835\uDC9F\uD835\uDCBE ℬ = \uD835\uDC46\uD835\uDC49\uD835\uDC40(\uD835\uDC9F\uD835\uDCBE) (\uD835\uDC59 \uD835\uDC56\uD835\uDC60 \uD835\uDC61\uD835\uDC5C\uD835\uDC61\uD835\uDC4E\uD835\uDC59 \uD835\uDC5B\uD835\uDC62\uD835\uDC5A\uD835\uDC4F\uD835\uDC52\uD835\uDC5F \uD835\uDC5C\uD835\uDC53 \uD835\uDC4E\uD835\uDC59\uD835\uDC59 \uD835\uDC59\uD835\uDC4E\uD835\uDC4F\uD835\uDC52\uD835\uDC59\uD835\uDC60) (4)"
    }, {
      "heading" : "2.3 Features for subcellular localization",
      "text" : "In this section, all human protein sequences in dataset \uD835\uDC9F will be extracted feature vectors based on machine learning. For giving more comprehensive consideration to protein sequences’ properties, our approach consider two state-of-the-art feature models: 188-dimension63 and Pse-in-One64,65.\n188-dimension feature model contains eight types of physical chemical properties, respectively, hydrophobicity, normalized van der Waals volume, polarity, polarizability, charge, surface tension, secondary structure, and solvent accessibility. For the components of 188-dimension, the first 20 dimensions stand for the proportions of the 20 kinds of amino acids, the rest dimensions are made up of eight kinds of more comprehensive physical chemical properties which contain twenty-one attributes. Hence, each protein sequence will bring forth 188 (20 + (21) × 8) numerical values. Pse-in-One feature model contains seven extraction methods, respectively, auto covariance (AC, 22D), cross covariance (CC, 22D) and auto-cross covariance (ACC, 22D) which based on autocorrelation technique; parallel correlation pseudo amino acid composition (PC-PseAAC, 22D), series correlation pseudo amino acid composition (SC-PseAAC, 26D), general parallel correlation pseudo amino acid composition (PCPseAAC-General, 22D) and general series correlation pseudo amino acid composition (SC-PseAACGeneral, 26D) which based on pseudo amino acid composition technique."
    }, {
      "heading" : "2.4 HPSLPred: a novel and ensemble multi-label classifier",
      "text" : "HPSLPred is an ensemble multi-label classifier with imbalanced source, and we design more comprehensive 350D feature model, automatic optimal dimension searching and ensemble classifiers for it to achieve the best performance.\nBased on the dataset transforming method introduced in data construction, the feature model of our approach integrated all seven extraction methods in Pse-in-One and 188-dimension methods, then produce 350D (188D + 5 × 22D + 2 × 26D = 350D) hybrid features model for designing HPSLPred, which expresses protein sequences’ properties sufficiently in different viewpoints. Hybrid features model can be seen in some works, and they can achieve more accurate prediction but slower.\nHowever, it is no deny that hybrid features contains some redundant expressions, so we apply an unsupervised dimension reduction method called Max-Relevance-Max-Distance(MRMD66) for trimming down dimensionality. MRMD method picks out the target samples with low redundancy and strong relevance from by the large amount of relevance (Max-Relevance) and distance (Max-Distance) rule. Pearson's correlation coefficient is employed for calculating the relevance value, and three kinds of distance functions are adopted to calculate the redundancy value. Actually, the larger relevance, the larger feature’s distance and the lower subset’s redundancy. Suppose that the labels’ number is ℓ, the optimal condition of MRMD algorithm can be formulated as:\nmax(\uD835\uDC40\uD835\uDC45\uD835\uDC56 + \uD835\uDC40\uD835\uDC37\uD835\uDC56) (\uD835\uDC56 ∈ ℓ) (5) Sometimes the key point of the specific issues is diverse on estimating the importance between\nMR and MD, hence adding weights on the original criterion is a wisely strategy. The new criterion can be formulated as:\nmax(\uD835\uDC64\uD835\uDC5F × \uD835\uDC40\uD835\uDC45\uD835\uDC56 + \uD835\uDC64\uD835\uDC51 × \uD835\uDC40\uD835\uDC37\uD835\uDC56) 6) where the variables \uD835\uDC64\uD835\uDC5F(0 < \uD835\uDC64\uD835\uDC5F ≤ 1) and \uD835\uDC64\uD835\uDC51(0 < \uD835\uDC64\uD835\uDC51 ≤ 1) are the weights of MR and MD\nrespectively.\nBased on the 350D feature model and MRMD dimension reduction algorithm, automatic optimal dimension searching67 which is integrated into HPSLPred framework. The whole automatic optimal dimension searching procedure is conceived as some multiple thread processes for the lowest time complexity. Suppose that there is a balanced dataset whose dimension is 350D, the first step, we will run MRMD algorithm to cut down dimension to 10D and compute the performance metrics at such state. Next, elevate 10D to the new dimension from the former, continue to cut down to such new dimension and compute the performance metrics again. Dimension reduction process is designed as eight separate calculating threads which calculates at the same time. In other words, this approach will calculate 80 dimensional span, which makes time complexity far lower. Finally, the first round of the whole procedure named roughly process is accomplished after achieving 340D. Meanwhile, this methods will search a space whose dimensional span is 10D, then the second round named particular process will go on in such span, which is designed as ten separate calculating threads. After the second round, our\nmethod will search the best dimension according to the optimal performance metrics. The whole procedure is called the two layer optimal dimension searching, which has proved as a not bad method.\nIn many state-of-the-art works of machine learning tools, Mulan68, WEKA69 and scikit-learn70 are excellent up to now. In this study, we reconstruct an ensemble multi-label classifier HPSLPred with the help of such three software tools. Advantage of Mulan is that it can classify multi-label dataset directly and has an intimate connection with WEKA, which is the main character we will compare with. Advantage of WEKA is that it integrates fully machine learning algorithms including about classification, cluster, and association, but that multi-label dataset cannot be handled directly is the main shortcoming. Advantage of scikit-learn is that it has a faster running-speed, more flexible interface for developing, even modify the core parts of one algorithm, while it cannot provide friendly user interface, and solution on multi-label dataset is shorted as well. However, after joining our strategies about handling multi-label imbalanced source, scikit-learn’s shortcoming has been overcome, and we named such novel approach as HPSLPred based its proud running-speed and better performance. By contrast, (a). Mulan will classify the original multi-label source, and referred classifiers respectively are BRkNN, HOMER, MLkNN, IBLR_ML and DMLkNN. (b). WEKA will classify the multiple single-label balanced dataset transformed from the original multi-label source according to the above, and referred classifiers respectively are IBk, Random Forest and J48. (c). HPSLPred will classify the dataset as same as WEKA’s dataset and add the two layer optimal dimension searching approach, and referred classifiers in scikit-learn respectively are Naive Bayes, Logistic Regression, SGD, Decision Tree, Nearest Neighbors, Extra Trees, Random Forest, LinearSVC, Bagging, AdaBoost, Gradient Boosting and LibSVM. Besides, the maximum precision of all such twelve classifiers on one label will be considered as this label’s precision. More comparison information will be showed in next section."
    }, {
      "heading" : "2.5 Performance metrics",
      "text" : "There are several performance metrics on multi-label classification problem, such as Hamming loss, Coverage, OneError, IsError, Ranking loss and Average precision (AP). The metric that the most relevant to multi-label classification system performance is AP value, which ranges 0 to 1. AP metric can be formulated as:\n\uD835\uDC34\uD835\uDC43(\uD835\uDC53) = 1\n\uD835\uDC41 ∑\n1 \uD835\uDCAE\uD835\uDC56 ∑\n|\uD835\uDC3F\uD835\uDC56|\n\uD835\uDC5D\uD835\uDC5F\uD835\uDC52\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61\uD835\uDC53(\uD835\uDC65\uD835\uDC56 , \uD835\uDF09) \uD835\uDF09∈\uD835\uDC66\uD835\uDC56\n\uD835\uDC41\n\uD835\uDC56=1\n\uD835\uDC3F = {\uD835\uDF09′|\uD835\uDC5D\uD835\uDC5F\uD835\uDC52\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61\uD835\uDC53(\uD835\uDC65\uD835\uDC56 , \uD835\uDF09 ′) ≤ \uD835\uDC5D\uD835\uDC5F\uD835\uDC52\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61\uD835\uDC53(\uD835\uDC65\uD835\uDC56 , \uD835\uDF09), \uD835\uDF09 ′ ∈ \uD835\uDC66\uD835\uDC56}\nWhere \uD835\uDC41 is all samples’ number; \uD835\uDCAE\uD835\uDC56 is the number of the samples with label \uD835\uDC66\uD835\uDC56; \uD835\uDC5D\uD835\uDC5F\uD835\uDC52\uD835\uDC51\uD835\uDC56\uD835\uDC50\uD835\uDC61\uD835\uDC53(\uD835\uDC65\uD835\uDC56 , \uD835\uDF09) is the precision on sample \uD835\uDC65\uD835\uDC56 with label \uD835\uDF09. Average precision will be applied for a mainly standard in a series of comparison experiments."
    }, {
      "heading" : "3. Results",
      "text" : "Based a series of detailed descriptions in above sections, we will validate whether our approach is better or not. First of all, with 188D feature model, our approach will test the performance of Mulan, WEKA and HPSLPred based scikit-learn. The precision of each label or each subcellular location will be displayed clearly. Similarity, we also show the performance about PseAAC-26D feature model and 350D hybrid feature model, which includes an extra presentation on the two layer optimal dimension searching. Additionally, we modestly compared our approach with some state-of-the-art methods to increase persuasion. Except such comprehensive experiments, we offer all source code in public and a user-friendly webserver to researchers."
    }, {
      "heading" : "3.1 Comparative experiments based on 188D classical features",
      "text" : "The 188D dimension feature model takes full consideration of the proportions of amino acids and eight types of physical chemical properties, which has proved as an efficient feature model. For our original 188D multi-label source \uD835\uDC9F188\uD835\uDC37, three experiments will be arranged as following: Experiment (a). Based on BRkNN, HOMER, MLkNN, IBLR_ML and DMLkNN, Mulan classifies the \uD835\uDC9F188\uD835\uDC37 directly by ten folds crossing validation. Experiment (b). As the way of we described, firstly, multi-label source should be processed into more single-label datasets through BM method, which including enough positive samples and negative samples. Then balanced dataset will be extracted by SVM decision boundary from each single-label imbalanced dataset. All balanced datasets is named \uD835\uDC9F188\uD835\uDC37 \uD835\uDC35 . Based on IBk, Random Forest and J48, WEKA classifies the \uD835\uDC9F188\uD835\uDC37 \uD835\uDC35 directly by ten folds crossing validation.\nExperiment (c). Based on Naive Bayes, Logistic Regression, SGD, Decision Tree, Nearest Neighbors, Extra Trees, Random Forest, LinearSVC, Bagging, AdaBoost, Gradient Boosting and LibSVM, HPSLPred classifies the \uD835\uDC9F188\uD835\uDC37 \uD835\uDC35 directly by ten folds crossing validation. HPSLPred selects the maximum precision of all twelve classifiers as such label’s precision through grid parameters searching, then the mean value of all label’s precision is average precision.\nAll experiments’ results are displayed in Table 4 and Figure 3. The first conclusion is that balanced datasets have a better performance than multi-label dataset. IBLR_ML achieves the highest precision 68.63% in five multi-label classifiers belonging to Mulan. For three WEKA classifiers, RandomForest achieves 68.20% precision, which is much better than others. HPSLPred makes full use of the ensemble classifiers’ advantage and reaches 72.45% precision that exceeds Mulan and WEKA apparently. Each label’s precision in HPSLPred is higher than others’ classifier, which is not an occasional result. Hence it should attribute to all various classifiers which explains implications of classification in different viewpoints. In order to validate availability based more feature model comprehensively, our experiments will go on."
    }, {
      "heading" : "3.2 Comparative experiments based on PseAAC-26D classical features",
      "text" : "In this subsection, we will show all experiments about PseAAC-26D features model. PseAAC is one of protein sequences features extraction methods based pseudo amino acid composition in Pse-in-One, which has achieved very good results in many previous works. For our original PseAAC-26D multilabel source \uD835\uDC9F26\uD835\uDC37, three experiments are as following: Experiment (d). Mulan will sort the \uD835\uDC9F26\uD835\uDC37 based a series of multi-label classifiers. Experiment (e). Suppose that \uD835\uDC9F26\uD835\uDC37\n\uD835\uDC35 is all single-label balanced datasets transformed by \uD835\uDC9F26\uD835\uDC37, WEKA will itemize the \uD835\uDC9F26\uD835\uDC37 \uD835\uDC35 based a series of single-label classifiers.\nExperiment (f). HPSLPred will classify the \uD835\uDC9F26\uD835\uDC37 \uD835\uDC35 based scikit-learn single-label classifiers. All experiments’ results are displayed in Table 5 and Figure 4. From the information given, it can be concluded that HPSLPred achieves the best AP as same as the experiments showed in last subsection. However, each classifier’s AP under PseAAC feature model cannot lump together. There are much difference between WEKA and Mulan. In the comparative experiments in this round, Mulan with multilabel source is still better than Weka with single-label balanced datasets, which fully affirms the previous excellent works. For three classifiers of WEKA, only J48 is better but not the highest one. For five classifiers of Mulan, only HOMER is better but it is still the lowest one. The main shortcoming of traditional single-label classifiers is that they cannot deal with several feature expressions with disparate dataset dynamically. But the ensemble classifier based scikit-learn grid parameters searching, HPSLPred, overcomes this weakness and achieves the highest AP 74.21%, which is about 6% higher than the second best algorithm."
    }, {
      "heading" : "3.3 Comparative experiments based on hybrid 350D features and dimensional searching",
      "text" : "According to the matter in the second section, Pse-in-One contains seven feature extraction methods, and PseAAC is one of all methods. Hybrid 350D feature model is generated from all Pse-in-One methods and 188D methods. Based such hybrid feature model, we will display a series of experiments which is similar with the above. For our original 350D multi-label source \uD835\uDC9F350\uD835\uDC37, three experiments are as following:\nExperiment (g). Mulan will classify the \uD835\uDC9F350 based on five multi-label classifiers. Experiment (h). Suppose that \uD835\uDC9F350\uD835\uDC37\n\uD835\uDC35 is all single-label balanced datasets transformed by \uD835\uDC9F350\uD835\uDC37 , WEKA will itemize the \uD835\uDC9F350\uD835\uDC37 \uD835\uDC35 based three single-label classifiers.\nExperiment (i). HPSLPred will classify the \uD835\uDC9F350\uD835\uDC37 \uD835\uDC35 based ensemble scikit-learn single-label classifiers. All experiments’ results are displayed in Table 6 and Figure 5. From the viewpoint of AP\nperformance, the results of 350D feature model improves better than 188D and PseAAC. It illustrates that the degree of coincidence on expressing features has some imparity distributions, and they has themselves’ contribution on hybrid 350D model. From the viewpoint of each class’s precision, they have some different degrees of improvement, which proves the overall improvement of AP is not an accident but an inner regularity. But it is undeniable that there must be a coincidence among the interpretation. For example, the performance matric of HPSLPred is not as high as other classifiers, so maybe some redundancy exists in it and our approach need to add the two layer optimal dimension searching method to reach the best. As described in last section, for the first round, algorithm will cut down dimension to 10D and compute the performance metrics meanwhile. Scan all dimension by stepping 10D and finish this round. For the second round, searching the each dimension in the highest span in the first round. For our experiments, this span is from 20D to 30D, the optimal AP will bring forth from there. By the way, all procedures we mentioned are integrated into the novel HPSLPred classifier. Figure 6 shows the best AP 75.89%, which is the whole highest average precision we consider about."
    }, {
      "heading" : "3.4 Comparison with state-of-the-art methods",
      "text" : "Table7. Accuracy comparison with state-of-the-art methods.\nMethod Average Precision\nHum-mPLoc 2.0 67.95%\nmGOF-Loc 65.71%\nIMMMLGP 68.76%\nXiaotong Guo’s approach 69.52% HPSLPred 75.89%\nProtein subcellular location research is always one of the hottest issues in cell biology and\nbioinformatics. Therefore, more and more excellent approaches based machine learning are merged now, such as Hum-mPLoc 2.071, mGOF-Loc72, IMMMLGP72, and Xiaotong Guo’s approach1. The first two methods are based on single-label source, while the last two methods are based on multi-label source. For Hum-mPLoc 2.0 and mGOF-Loc methods, we will transform the original multi-label source into more single-label datasets for them. For IMMMLGP and Xiaotong Guo’s approach methods, the original source is well. Surprisingly, HPSLPred still achieves the best AP 75.89%, comparatively, Xiaotong Guo’s approach is as same as Mulan directly, but its performance is proud. The classical methods such as Hum-mPLoc 2.0, mGOF-Loc and IMMMLGP have not bad performance. Additionally, it can be concluded that the ensemble multi-label classifier has a brighter development prospect than multi-class classifiers in the future."
    }, {
      "heading" : "4. Discussion",
      "text" : "In this section, a series of experiments showed above will be compared with each other, and we will analysis the merit and defect of each method conclusively. From three experiments (a), (b), and (c) based 188D feature model, we sum up three vital conclusions: (1). HPSLPred classifier integrates twelve kinds of basic classifiers and adds grid parameters searching, which makes it reach the highest AP value. (2). Mulan based multi-label classifiers is better than WEKA based classical single-label classifiers, which proves that Mulan is an excellent work. (3). each label’s precision is correspond to its AP value, which eliminate the coincidence of results. From three experiments (d), (e), and (f) based PseAAC-26D feature model, we sum up a new conclusion: there are no apparent relation between PseAAC’s results and 188’s results, so new expression exists in them. Finally, from three experiments (g), (h), and (i) based hybrid 350D feature model, we sum up a crucial conclusion: hybrid 350D feature model contains redundant expressions, which needs feature selection. After that, we acquire the best AP. The method or classifier based hybrid feature model for protein subcellular location is named HPSLPred. HPSLPred has a convincing performance, especially after a series of comparison with state-of-the-art methods."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this study, we generate hybrid 350D feature based 188D feature and all methods of Pse-in-One, transform the original multi-label imbalanced source into multiple single-label imbalanced datasets based BM method, transform single-label imbalanced datasets into single-label balanced datasets based SVM decision boundary, integrate twelve basic classifiers based scikit-learn and match optimal parameters by grid searching, search the optimal dimension by MRMD and two round dimension reduction, finally achieve the highest AP value. Moreover, we utilize multi-threaded technology for accelerating multiple processes in dimensionality reduction. This is our novel and ensemble classifier HPSLPred with imbalanced source. For the conveniences of users, a user-friendly webserver for HPSLPred was established at http://server.malab.cn/HPSLPred.\nIn future work, we want to develop a more comprehensive protein database and keep updated, providing researchers with high-quality research object. Additionally, multi-threaded technology and GPU parallel technology greatly reduces the program's time complexity. Meanwhile, our algorithm strategy should keep pace with the times, make full use of powerful hardware performance and create more efficient multi-label classifiers for improving the precision of subcellular localization problem."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The work was supported by the Natural Science Foundation of China (No. 61370010) and the\nNatural Science Foundation of Fujian Province of China (No.2014J01253)."
    } ],
    "references" : [ {
      "title" : "Protein subcellular location prediction",
      "author" : [ "Chou", "K.-C", "D.W. Elrod" ],
      "venue" : "Protein engineering",
      "citeRegEx" : "Chou et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Chou et al\\.",
      "year" : 1999
    }, {
      "title" : "An overview on predicting the subcellular location of a protein",
      "author" : [ "Feng", "Z.-P" ],
      "venue" : "In silico biology",
      "citeRegEx" : "Feng and Z..P.,? \\Q2002\\E",
      "shortCiteRegEx" : "Feng and Z..P.",
      "year" : 2002
    }, {
      "title" : "Subcellular localization of the yeast proteome",
      "author" : [ "A Kumar" ],
      "venue" : "Genes & development",
      "citeRegEx" : "Kumar,? \\Q2002\\E",
      "shortCiteRegEx" : "Kumar",
      "year" : 2002
    }, {
      "title" : "The WEKA data mining software: an update",
      "author" : [ "M Hall" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Hall,? \\Q2011\\E",
      "shortCiteRegEx" : "Hall",
      "year" : 2011
    }, {
      "title" : "A top-down approach to enhance the power of predicting human protein subcellular",
      "author" : [ "Shen", "H.-B", "Chou", "K.-C" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Protein subcellular localization prediction is an important and challenging problem. The traditional biology experiments are expensive and time-consuming, so more and more research interests tend to a series of machine learning approaches for predicting protein subcellular location. There are two main difficult problems among the existing state-of-the-art methods. First, most of the existing techniques are designed to deal with the multi-class but not the multi-label classification, which ignores the connection between the multiple labels. In reality, multiple location proteins implicate that there are vital and unique biological significances worthy of special focus, which cannot be ignored. Second, the techniques for handling imbalanced data in multi-label classification problem is significant but less. For solving the two issues, we have developed an ensemble multi-label classifier called HPSLPred which can be applied for the multi-label classification with imbalanced protein source. For the conveniences of users, a user-friendly webserver for HPSLPred was established at http://server.malab.cn/HPSLPred.",
    "creator" : "Microsoft® Word 2013"
  }
}