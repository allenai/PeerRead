{
  "name" : "1705.10112.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dynamics of core of language vocabulary",
    "authors" : [ "Valery D. Solovyev", "Vladimir V. Bochkarev", "Anna V. Shevlyakova" ],
    "emails" : [ "maki.solovyev@mail.ru" ],
    "sections" : [ {
      "heading" : null,
      "text" : "diachronic text corpora, especially Google Books Ngram. This article discusses the question of core change rate and the degree to which the core words cover the texts. Different periods of the last three centuries and six main European languages presented in Google Books Ngram are compared. The main result is high stability of core change rate, which is analogous to stability of the Swadesh list.\nKeywords: core of vocabulary, language dynamics, Google Books Ngram"
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper, we investigate the dynamics of the overall structure of the language vocabulary from a\ncognitive point of view. Traditionally, two components of the language vocabulary are distinguished: the center and periphery. The former contains highly stable words of maximum frequency (go, read, etc.) and provides stability to the language; the periphery contains the words that have become outdated or, on the contrary, have just appeared in the language, and thus, guarantees greater flexibility to it. We will present some quantitative characteristics of the dynamics of the center.\nTo do it, we should answer the following questions. How to determine the core? What is the size of\nthe core? What is the rate of change of the core? What is the overall frequency of the core words? We will refer to Google Books Ngram corpus to answer these questions (https://books.google.com/ngrams). Similar problems were considered in [1, 2]. The frequency approach is a standard approach used to study core formation. In this paper, we consider two kinds of frequency: the word occurrence frequency in the corpus and the share of books in which the word occurs. Though these approaches are rather close, yet there are some differences.\nThe first question to answer is how to determine the core. It’s impossible to define a clear boundary\nof the core. For example, the known Swadesh wordlists contain 40, 100 or 200 items. In [1] the core contains 100 words. It appears to be too limited. Let us note that Basic English contains 850 words, and the basic set of root words of Esperanto contains 900 items. The Voice of America’s Special English [3] and Wikipedia in Simple English use, correspondingly, about 1500 and 2000 words. The basic vocabularies for foreigners [4], creole [5] and pidgin languages [6] contain 1.5 to 3 thousand words.\nIn [2] the core is composed of 1000 most frequent words (the first 100 words constitute what is called\nthe head, and words 101 to 1000 form the body), and the periphery consists of the following in frequency 6000 words. In [9] the size of core vocabulary that provides a specified percentage of word usage based on the Google Books Ngram data is calculated. Thus, 2300 most frequently used English words have the total relative frequency of 75 %.\nWe carry out calculations not only for one fixed core, but for consecutive variants: for 1000, 2000,\n…, 8000 most frequent words, covering the whole range described above.\nThe following data preprocessing which allowed reducing the number of mistakes in the used data\nbase was performed in this work. Only lexical 1-grams were selected which consisted only of the corresponding alphabet letters and one apostrophe in some cases. To normalize and calculate the relative frequencies, the number of lexical 1-grams was calculated for each year (as distinct from the Google Books\ncan result in incorrect conclusions based on these data. We used the method explained in [9], i.e. if the number of word forms corresponding to some part of speech doesn`t exceed 1 % of total frequency of the given word form, such word forms were marked and not used during further analysis."
    }, {
      "heading" : "2 Rate of change of the core",
      "text" : "When considering the rate of change of the core, we calculate the share of words of the core excluded\nfrom it during a given period. Figure 1 shows the relevant data for an interval of 50 years in English language. Changes of word frequencies can be due to both language evolution and random factors. To eliminate these factors, frequencies of word usage were studied throughout rather long 50-year intervals: 1676-1725, 1726-1775, … 1976-2008. Then, the words were ranked in decreasing frequency order and the percentage of words, which dropped out from the core of the successive 50-year interval were calculated. For example, the columns of the diagram marked “1825” show the percentage of core words for the period 1776-1825 which dropped out from the core in 1826-1875.\nWe observe a rather steady rate of updating of the core in the last 300 years: an average of 13-15%\nof the words drop out of the core in 50 years. Of course, it does not mean that these words disappear from language, only their frequency decreases, and they are forced out from the core by other words. There is not enough data in Google Books Ngram for the previous period (1500–1700), and therefore they are not provided here. Curiously, the updating rates of the core decrease during the Victorian era and increase in the first half of the 20th century. Also, it should be noted that the found mean value of 13-15% almost does not depend on the core size in the range from 1 to 8 thousand.\nWhen the core is defined through the share of books, the following changes occur in its content. If\nwe select all English words that are found at least in one out of two books, we obtain a wordlist of 2302 items. We can construct, for comparison, a list with the same quantity of most frequent words. In spite of\n0.25), both lists overlap by 79%. At the same time, the differences between the lists are quite essential – there are 482 words that appear just in one list.\nWords included in list 1 seem to be, according to the intuitive perception of the language, the most\nsuitable for the core group of words. List 2 contains words that can hardly be attributed with certainty to the core vocabulary. These words correspond, first of all, to geo-graphical names and vocabulary with related meaning (for example, Africa, African, Rome, Berlin, Japan, Japanese, Spain, Spanish, India, Indians, Canada, California, Virginia, Asia), proper names/appellations (Wilson, Richard, Louis, Oxford), parts of words/letters that entered the list accidentally (ff), abbreviations (cf, vol., al, ibid.), articles and prefixes in loanwords and found foreign vocabulary (der, des, du, le, les, un, el), words belonging more to professional vocabulary than to common (carbon, oxygen, copper, equation, electron, protein), loanwords (bureau), words connected chiefly with political actions (socialist, colonial, empire, queen). However, according to the intuitive notion of language core, it is difficult to ascribe the words from the specified groups to the core, but we should not deny their importance for English-speaking society. In the culturological context, the words Oxford and queen for British people are undoubtedly important, as well as the words California, Africa and Virginia for Americans; additionally, professional words come into broad use together with the growth of public aware-ness.\nAs for the dynamics of the core (updating by 13-15% in 50 years), it practically does not change,\nregardless of these two ways of determination.\nLet us consider the structure of the core from the perspective of the parts of speech. In the latest\nversion of Google Books Ngram, English words have been marked as parts of speech with 95% accuracy [10]. In figure 2 we can see the share of each part of speech around the year 1800 and today.\nX stands for abbreviations, foreign words or words whose membership to a part of speech has not\nbeen determined. In 200 years the share of nouns and verbs has diminished. Figure 3 shows the dynamics of the parts of speech. The algorithm for marking the parts of speech works with higher accuracy in the case of modern words; this is why the share of X is the one declining most rapidly.\nAs one would expect, the parts of speech with the highest content, i.e. nouns and verbs (about 45%),\ndrop out at the highest rate, while auxiliary parts of speech, articles, conjunctions, etc. (about 15 to 20%), do it at the lowest rate.\nSimilar data are obtained for the main European languages (fig. 4) representing three different\nbranches of Indo-European languages: Slavic, Romance and German, which separated just a few thousand years ago. This is somewhat similar to Swadesh results. Russian rather stands out from the general picture. The social upheavals in the beginning of the 20th century (the socialist revolution, which led to radical economic, political, cultural changes) were reflected in the vocabulary core."
    }, {
      "heading" : "3 Degree of covering of texts by the core",
      "text" : "The important characteristic of core words is to what extent they are efficient for communication.\nFormally, this can be presented by percentage of core words in the texts, in other words by the degree to which the core words cover these texts. Let us analyze now the change of the total frequency of words of the core, that is the degree of covering of texts by these words. If one considers the core for the language state in 1800 (for a higher stability in calculations one takes the interval 1795–1805 and defines the core in\nleft window).\nFor a 1000-word core the overall frequency falls in 200 years approximately from 0.7 to 0.6.\nFrequency curves for cores of bigger sizes look similarly. This effect may be explained not only by the obsolescence of the words of the core (their removal from the core), i.e. by the up-dating of the language, but also by the extension of the vocabulary, which in general grants greater expressive opportunities to the language and, naturally, leads to the reduction of the share of old words. According to data provided in [7], the number of words in English language grew from 544,000 in 1900 up to 1,022,000 in 2000, i.e. almost twice.\nIf one considers the modern core (years 2000–2008), the dynamics of its frequency looks as follows\n(fig. 5, the right window). Here two tendencies confront. On the one hand, it is evident that two hundred\nof words of the core in general falls. And these two tendencies approximately counterbalance each other. The overall frequency of words for a core with 4 thousand words remains at the level of approximately 0.8, for a core of 1000 words it slightly falls from 0.67 to 0.65. The next graph (fig. 6) explains the essence of the processes taking place. Here we can see separately the words that are present in the core both in 1800 and in 2000, and also the words present in one of them but not in the other.\nThe overall frequency for the words remaining in the core during these two centuries decreases from\n0.7 to 0.6. The frequency of the words that drop out of the core decreases, and that of the words entering the core increases, and this augmentation is more intensive than the loss of frequency of the previous group.\nThese data must be taken into account when analyzing the frequency dynamics of different groups\nof rather-high-frequency vocabulary. The frequency dynamics of basic emotions are studied in [8]. Data for English are presented in figure 7 (taken from [8]). One can see that the overall frequency of emotive vocabulary considerably decreases from 1800 to 2000. A priori this can be explained either by a reduction of emotionality of people (or at least that of texts) during this period, or by a general reduction of the frequency of all the words of the core, which includes also the considered emotive words. Comparison of the frequencies shows that the main acting factor is the first one. The frequency of emotive vocabulary decreased approximately by 50%, while the overall frequency of the words of the whole core decreased just by 15%. Thus, the reduction of the frequency of emotive vocabulary cannot be explained only by the reduction of the frequency of the whole core."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In the article, the lexicon structure is considered from cognitive point of view distinguishing the\ncenter (core – the most frequently used lexis) and periphery. The core size is evaluated differently in different papers – from 1 to 8 thousand words. In our paper, the calculations are performed for all core sizes in this range. The core change data are presented for the first time. It turned out that the core has steadily changed during the last 300 years – approximately 15% of words is substituted every 50 years. The result is obtained for different languages (which are presented in Google Books Ngram) and is, to some extent, analogous to the results obtained by Swodesh concerning the stability of words from his list. The size of texts covered by the core words is counted (or the total frequency of core words). It was found that the core\neffect can be explained not only by core words obsolescence (removing from the core), i.e. by language updating but also by lexicon expansion which offers significant expression opportunities to a language and results in decreasing of old words percentage.\nAcknowledgements. This research was supported by the Russian Foundation for Basic Research\n(grant № 15-06-07402)."
    }, {
      "heading" : "5 References",
      "text" : "1. Perc, Matjaz: Evolution of the most common English words and phrases over the centuries. J. R. Soc.\nInterface. 9, pp. 3323–3328 (2012)\n2. Cocho, G., Flores, J., Gershenson, C., Pineda, C., Sánchez, S.: Rank Diversity of Languages: Generic\nBehavior in Computational Linguistics. PLoS ONE 10(4): e0121898. (2015). doi:10.1371/journal. pone.0121898. 3. Beare, K.: Voice of America Special English Dictionary. English as 2nd Language.\nhttp://esl.about.com/cs/reference/a/aavoa.htm.\n4. Takala, S. Estimating students’ vocabulary sizes in foreign language teaching. In: Practice and\nProblems in Language Testing, vol. 8, pp. 157–165. Afinla. https://www.jyu.fi/hum/laitokset/solki/ afinla/julkaisut/arkisto/40/takala (1985) 5. Hall, R.A.: Haitian Creole: Grammar, Texts, Vocabulary. American Folklore Society, Philadelphia\n(1953)\n6. Romaine, S.: Pidgin and Creole Languages. Longman, London (1988) 7. Michel, J.-B., Shen, Y.K., Aiden, A.P, Veres, A., Gray, M.K., et al.: Quantitative analysis of culture\nusing millions of digitized books. Science 331: 176–182. (2011)\n8. Bochkarev, V. V., Solovyev, V. D.: Quantitative analysis of trends in the use of words with negative\nand positive connotations in Russian and English languages. (in Russian) In: Proceedings of the VI International Conference on Cognitive Science. Kaliningrad State University. (2014) 9. Bochkarev, V., Solovyev, V., Wichmann, S.: Universals versus historical contingencies in lexical\nevolution. J. R. Soc. Interface. 11, 20140841 (2014)\n10. Lin, Y., Michel, J.-B., Aiden, E.L., Orwant, J., Brockman, W., Petrov, S.: Syntactic Annotations for the\nGoogle Books Ngram Corpus. In: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics Volume 2: Demo Papers (2012)."
    } ],
    "references" : [ {
      "title" : "Evolution of the most common English words and phrases over the centuries",
      "author" : [ "Perc", "Matjaz" ],
      "venue" : "J. R. Soc. Interface. 9, pp. 3323–3328",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Rank Diversity of Languages: Generic Behavior in Computational Linguistics",
      "author" : [ "G. Cocho", "J. Flores", "C. Gershenson", "C. Pineda", "S. Sánchez" ],
      "venue" : "PLoS ONE 10(4): e0121898.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Practice and Problems in Language Testing, vol",
      "author" : [ "Takala", "S. Estimating students’ vocabulary sizes in foreign language teaching. In" ],
      "venue" : "8, pp. 157–165. Afinla. https://www.jyu.fi/hum/laitokset/solki/ afinla/julkaisut/arkisto/40/takala",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Haitian Creole: Grammar, Texts, Vocabulary",
      "author" : [ "R.A. Hall" ],
      "venue" : "American Folklore Society, Philadelphia",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1953
    }, {
      "title" : "Pidgin and Creole Languages",
      "author" : [ "S. Romaine" ],
      "venue" : "Longman, London",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Quantitative analysis of culture using millions of digitized books",
      "author" : [ "Michel", "J.-B.", "Y.K. Shen", "Aiden", "A.P", "A. Veres", "Gray", "M.K" ],
      "venue" : "Science 331: 176–182.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Quantitative analysis of trends in the use of words with negative and positive connotations in Russian and English languages",
      "author" : [ "V.V. Bochkarev", "V.D. Solovyev" ],
      "venue" : "(in Russian) In: Proceedings of the VI International Conference on Cognitive Science. Kaliningrad State University.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Universals versus historical contingencies in lexical evolution",
      "author" : [ "V. Bochkarev", "V. Solovyev", "S. Wichmann" ],
      "venue" : "J. R. Soc. Interface. 11, 20140841",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Syntactic Annotations for the Google Books Ngram Corpus",
      "author" : [ "Y. Lin", "Michel", "J.-B.", "E.L. Aiden", "J. Orwant", "W. Brockman", "S. Petrov" ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics Volume 2: Demo Papers",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Similar problems were considered in [1, 2].",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "Similar problems were considered in [1, 2].",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "In [1] the core contains 100 words.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "The basic vocabularies for foreigners [4], creole [5] and pidgin languages [6] contain 1.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "The basic vocabularies for foreigners [4], creole [5] and pidgin languages [6] contain 1.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "The basic vocabularies for foreigners [4], creole [5] and pidgin languages [6] contain 1.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "In [2] the core is composed of 1000 most frequent words (the first 100 words constitute what is called the head, and words 101 to 1000 form the body), and the periphery consists of the following in frequency 6000 words.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "In [9] the size of core vocabulary that provides a specified percentage of word usage based on the Google Books Ngram data is calculated.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "We used the method explained in [9], i.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "In the latest version of Google Books Ngram, English words have been marked as parts of speech with 95% accuracy [10].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "According to data provided in [7], the number of words in English language grew from 544,000 in 1900 up to 1,022,000 in 2000, i.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "The frequency dynamics of basic emotions are studied in [8].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "Data for English are presented in figure 7 (taken from [8]).",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "Dynamics of total frequency of English emotive vocabulary [8]",
      "startOffset" : 58,
      "endOffset" : 61
    } ],
    "year" : 2017,
    "abstractText" : "Studies of the overall structure of vocabulary and its dynamics became possible due to creation of diachronic text corpora, especially Google Books Ngram. This article discusses the question of core change rate and the degree to which the core words cover the texts. Different periods of the last three centuries and six main European languages presented in Google Books Ngram are compared. The main result is high stability of core change rate, which is analogous to stability of the Swadesh list.",
    "creator" : "Microsoft® Word 2013"
  }
}