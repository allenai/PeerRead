{
  "name" : "1508.03891.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Refinement-Based Architecture for Knowledge Representation and Reasoning in Robotics",
    "authors" : [ "Mohan Sridharan", "Michael Gelfond", "Jeremy Wyatt" ],
    "emails" : [ "m.sridharan@auckland.ac.nz", "michael.gelfond@ttu.edu", "s.zhang9@csuohio.edu", "jlw@cs.bham.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Robots1 are increasingly being used to assist humans in homes, offices and other complex domains. To truly assist humans in such domains, robots need to be re-taskable and robust. We consider a robot to be re-taskable if its reasoning system enables it to achieve a wide range of goals in a wide range of environments. We consider a robot to be robust if it is able to cope with unreliable sensing, unreliable actions, changes in the environment agents, and the existence of atypical environments, by representing and reasoning with different description of knowledge and uncertainty. While there have been many attempts, satisfying these desiderata remains an open research problem.\nRobotics and artificial intelligence researchers have developed many approaches for robot reasoning, drawing on ideas from two very different classes of systems for knowledge representation and reasoning, based on logic and probability theory respectively. Systems based on logic incorporate compositionally structured commonsense knowledge about objects and relations, and support powerful generalization of reasoning to new situations. Systems based on probability reason optimally (or near optimally) about the effects of numerically quantifiable uncertainty in sensing\n1We use the terms “robot” and “agent” interchangeably in this paper.\nar X\niv :1\n50 8.\n03 89\n1v 3\n[ cs\n.R O\nand action. There have been many attempts to combine the benefits of these two classes of systems, including work on joint (i.e., logic-based and probabilistic) representations of state and action, and algorithms for planning and decisionmaking in such formalisms. These approaches provide significant expressive power, but they also impose a significant computational burden. More efficient (and often approximate) reasoning algorithms for such unified probabilisticlogical paradigms are being developed. However, practical robot systems that combine abstract task-level planning with probabilistic reasoning, link, rather than unify, their logic-based and probabilistic representations, primarily because roboticists often need to trade expressivity or correctness guarantees for computational speed. Information close to the sensorimotor level is often represented probabilistically to quantitatively model the uncertainty in sensing and actuation, with the robot’s beliefs including statements such as “the robotics book is on the shelf with probability 0.9”. At the same time, logic-based systems are used to reason with (more) abstract commonsense knowledge, which may not necessarily be natural or easy to represent probabilistically. This knowledge may include hierarchically organized information about object sorts (e.g., a cookbook is a book), and default information that holds in all but a few exceptional situations (e.g., “books are typically found in the library”). These representations are linked, in that the probabilistic reasoning system will periodically commit particular claims about the world being true, with some residual uncertainty, to the logical reasoning system, which then reasons about those claims as if they were true. There are thus languages of different expressive strengths, which are linked within an architecture.\nThe existing work in architectures for robot reasoning has some key limitations. First, many of these systems are driven by the demands of robot systems engineering, and there is little formalization of the corresponding architectures. Second, many systems employ a logical language that is indefeasible, e.g., first order predicate logic, and incorrect commitments can lead to irrecoverable failures. Our proposed architecture addresses these limitations. It represents and reasons about the world, and the robot’s knowledge of it, at two granularities. A fine-resolution description of the domain, close to the data obtained from the robot’s sensors and actuators, is reasoned about probabilistically, while a coarse-resolution description of the domain, including commonsense knowledge, is reasoned about using nonmonotonic logic. Our architecture precisely defines the coupling between the representations at the two granularities, enabling the robot to represent and efficiently reason about commonsense knowledge, what the robot does not know, and how actions change the robot’s knowledge. The interplay between the two types of knowledge is viewed as a conversation between, and the (physical and mental) actions of, a logician and a statistician. Consider, for instance, the following exchange:\nLogician: the goal is to find the robotics book. I do not know where it is, but I know that books are typically in the library and I am in the library. We should first look for the robotics book in the library.\nLogician→ Statistician: look for the robotics book in the library. You only need to reason about the robotics book and the library.\nStatistician: In my representation of the world, the library is a set of grid cells. I shall determine how to locate the book probabilistically in these cells considering the probabilities of movement failures and visual processing failures.\nStatistician: I visually searched for the robotics book in the grid cells of the library, but did not find the book. Although there is a small probability that I missed the book, I am prepared to commit that the robotics book is not in the library.\nStatistician→ Logician: here are my observations from searching the library; the robotics book is not in the library. Logician: the robotics book was not found in the library either because it was not there, or because it was moved to\nanother location. The next default location for books is the bookshelf in the lab. We should go look there next.\nand so on... where the representations used by the logician and the statistician, and the communication of information between them, is coordinated by a controller. This imaginary exchange illustrates key features of our approach:\n• Reasoning about the states of the domain, and the effects of actions, happens at different levels of granularity, e.g., the logician reasons about rooms, whereas the statistician reasons about grid cells in those rooms.\n• For any given goal, the logician computes a plan of abstract actions, and each abstract action is executed probabilistically as a sequence of concrete actions planned by the statistician.\n• The effects of the coarse-resolution (logician’s) actions are non-deterministic, but the statistician’s fine-resolution action effects, and thus the corresponding beliefs, have probabilities associated with them.\n• The coarse-resolution knowledge base (of the logician) may include knowledge of things that are irrelevant to the current goal. Probabilistic reasoning at fine resolution (by statistician) only considers things deemed relevant to the current coarse-resolution action.\n• Fine-resolution probabilistic reasoning about observations and actions updates probabilistic beliefs, and highly likely statements (e.g., probability > 0.9) are considered as being completely certain for subsequent coarseresolution reasoning (by the logician)."
    }, {
      "heading" : "1.1 Technical Contributions",
      "text" : "The design of our architecture is based on tightly-coupled transition diagrams at two levels of granularity. A coarseresolution description includes commonsense knowledge, and the fine-resolution transition diagram is defined as a refinement of the coarse-resolution transition diagram. For any given goal, non-monotonic logical reasoning with the coarse-resolution system description and the system’s recorded history, results in a sequence of abstract actions. Each such abstract action is implemented as a sequence of concrete actions by zooming to a part of the fine-resolution transition diagram relevant to this abstract action, and probabilistically modeling the non-determinism in action outcomes. The technical contributions of this architecture are summarized below.\nAction language extensions. An action language is a formalism used to model action effects, and many action languages have been developed and used in robotics, e.g., STRIPS, PDDL [19], BC [32], and ALd [17]. We extend ALd in two ways to make it more expressive. First, we allow fluents (domain properties that can change) that are nonBoolean, which allows us to compactly model a much wider range of situations. Second, we allow non-deterministic causal laws, which captures the non-deterministic effects of the robot’s actions, not only in probabilistic but also qualitative terms. This extended version of ALd is used to describe the coarse-resolution and fine-resolution transition diagrams of the proposed architecture.\nDefaults, histories and explanations. Our architecture makes three contributions related to reasoning with default knowledge and histories. First, we expand the notion of the history of a dynamic domain, which typically includes a record of actions executed and observations obtained (by the robot), to support the representation of (prioritized) default information. We can, for instance, say that a textbook is typically found in the library and, if it is not there, it is typically found in the auxiliary library. Second, we define the notion of a model of a history with defaults in the initial state, enabling the robot to reason with such defaults. Third, we limit reasoning with such expanded histories to the coarse resolution, and enable the robot to efficiently (a) use default knowledge to compute plans to achieve the desired goal; and (b) reason with history to generate explanations for unexpected observations. For instance, in the absence of knowledge about the locations of a specific object, the robot can construct a plan using the object’s default location to speed up search. Also, the robot can build a revised model of the history to explain subsequent observations that contradict expectations based on initial assumptions.\nTightly-coupled transition diagrams. The next set of contributions are related to the relationship between different models of the domain used by the robot, i.e., the tight coupling between the transition diagrams at two resolutions. First, we provide a formal definition of one transition diagram being a refinement of another, and use this definition to formalize the notion of the coarse-resolution transition diagram being refined to obtain the fine-resolution transition diagram—the fact that both transition diagrams are described in the same language facilitates their construction and this formalization. A coarse-resolution state is, for instance, magnified to provide multiple states at the fineresolution—the corresponding ability to reason about space at two different resolutions is central for scaling to larger environments. We find two resolutions to be practically sufficient for many robot tasks, and leave extensions to other resolutions as an open problem. Second, we define randomization of a fine-resolution transition diagram, replacing deterministic causal laws by non-deterministic ones. Third, we formally define and automate zooming to a part of the\nfine-resolution transition diagram relevant to a specific coarse-resolution transition, allowing the robot, while executing any given abstract action, to avoid considering parts of the fine-resolution diagram irrelevant to this action, e.g., a robot moving between two rooms only considers its location in the cells in those rooms.\nDynamic generation of probabilistic representations. The next set of innovations connect the contributions described so far to quantitative models of action and observation uncertainty. First, we use a semi-supervised algorithm, the randomized fine-resolution transition diagram, prior knowledge (if any), and experimental trials, to collect statistics and compute probabilities of fine-resolution action outcomes and observations. Second, we provide an algorithm that, for any given abstract action, uses these computed probabilities and the zoomed fine-resolution description to automatically construct the data structures for, and thus significantly limit the computational requirements of, probabilistic reasoning. Third, based on the coupling between transition diagrams at the two resolutions, the outcomes of probabilistic reasoning update the coarse-resolution history for subsequent reasoning.\nMethodology and architecture. The final set of contributions are related to the overall architecture. First, for the design of the software components of robots that are re-taskable and robust, we articulate a methodology that is rather general, provides a path for proving correctness of these components, and enables us to predict the robot’s behavior. Second, the proposed knowledge representation and reasoning architecture combines the representation and reasoning methods from action languages, declarative programming, probabilistic state estimation and probabilistic planning, to support reliable and efficient operation. The domain representation for logical reasoning is translated into a program in SPARC [2], an extension of CR-Prolog, and the representation for probabilistic reasoning is translated into a partially observable Markov decision process (POMDP) [27]. CR-Prolog [4] (and thus SPARC) incorporates consistency-restoring rules in Answer Set Prolog (ASP)—in this paper, the terms ASP, CR-Prolog and SPARC are often used interchangeably—and has a close relationship with our action language, allowing us to reason efficiently with hierarchically organized knowledge and default knowledge, and to pose state estimation, planning, and explanation generation within a single framework. Also, using an efficient approximate solver to reason with POMDPs supports a principled and quantifiable trade-off between accuracy and computational efficiency in the presence of uncertainty, and provides a near-optimal solution under certain conditions [27, 37]. Third, our architecture avoids exact, inefficient probabilistic reasoning over the entire fine-resolution representation, while still tightly coupling the reasoning at different resolutions. This intentional separation of non-monotonic logical reasoning and probabilistic reasoning is at the heart of the representational elegance, reliability and inferential efficiency provided by our architecture.\nThe proposed architecture is evaluated in simulation and on a physical robot finding and moving objects in an indoor domain. We show that the architecture enables a robot to reason with violation of defaults, noisy observations, and unreliable actions, in larger, more complex domains, e.g., with more rooms and objects, than was possible before."
    }, {
      "heading" : "1.2 Structure of the Paper",
      "text" : "The remainder of the paper is organized as follows. Section 2 introduces a domain used as an illustrative example throughout the paper, and Section 3 discusses related work in knowledge representation and reasoning for robots. Section 4 presents the methodology associated with the proposed architecture, and Section 5 introduces definitions of basic notions used to build mathematical models of the domain. Section 5.1 describes the action language used to describe the architecture’s coarse-resolution and fine-resolution transition diagrams, and Section 5.2 introduces histories with initial state defaults as an additional type of record, describes models of system histories, and reduces planning with the coarse-resolution domain representation to computing the answer set of the corresponding ASP program. Section 6 provides the logician’s domain representation base on these definitions. Next, Section 7 describes the (a) refinement of the coarse-resolution transition diagram to obtain the fine-resolution transition diagram; (b) randomization of the fine-resolution system description; (c) collection of statistics to compute the probability of action outcomes and observations; and (d) zooming to the part of the randomized system description relevant to the execution of any given abstract action. Next, Section 8 describes how a POMDP is constructed and solved to obtain a policy that implements the abstract action as a sequence of concrete actions. The overall control loop of the architecture is described in Section 9. Section 10 describes the experimental results in simulation and on a mobile robot, followed by conclusions in Section 11. In what follows, we refer to the functions and abstract actions of the coarse-resolution\ntransition diagram as being “high level”, using H as the subscript or superscript. Concrete functions and actions of the fine-resolution transition diagram are referred to as being “low level”, using L as the subscript or superscript."
    }, {
      "heading" : "2 Illustrative Example: Office Domain",
      "text" : "The following domain (with some variants) will be used as an illustrative example throughout the paper.\nExample 1. [Office Domain] Consider a robot that is assigned the goal of moving specific objects to specific places in an office domain. This domain contains:\n• The sorts: place, thing, robot, and ob ject, with ob ject and robot being subsorts of thing. Sorts textbook, printer and kitchenware, are subsorts of the sort ob ject. Sort names and constants are written in lower-case, while variable names are in uppercase.\n• Four specific places: o ff ice, main library, aux library, and kitchen. We assume that these places are accessible from each other without the need to navigate any corridors, and that doors between these places are open.\n• An instance of the sort robot, called rob1. Also, a number of instances of subsorts of the sort ob ject.\nAs an extension of this illustrative example that will be used in the experimental trials on physical robots, consider the robot shown in Figure 1(b) operating in an office building whose map is shown in Figure 1(a). Assume that the robot can (a) build and revise the domain map based on laser range finder data; (b) visually recognize objects of interest; and (c) execute actuation commands, although neither the information extracted from sensor inputs nor the action execution is completely reliable. Next, assume that the robot is in the study corner and is given the goal of fetching the robotics textbook. Since the robot knows that books are typically found in the main library, ASP-based reasoning provides a plan of abstract actions that require the robot to go to the main library, pick up the book and bring it back. For the first abstract action, i.e., for moving to the main library, the robot can focus on just the relevant part of the fine-resolution representation, e.g., the cells through which the robot must pass, but not the robotics book that is irrelevant at this stage of reasoning. It then creates and solves a POMDP for this movement sub-task, and executes a sequence of concrete movement actions until it believes that it has reached the main library with high probability. This information is used to reason at the coarse resolution, prompting the robot to execute the next abstract action to pick up the robotics book. Now, assume that the robot is unable to pick up the robotics book because it fails to find the book in the main library despite a thorough search. This observation violates what the robot expects to see based on default knowledge, but the robot explains this by understanding that the book was not in the main library to begin with, and creates a plan to go to the auxiliary library, the second most likely location for textbooks. In this case, assume that the robot finds the book and completes the task. The proposed architecture enables such robot behavior."
    }, {
      "heading" : "3 Related Work",
      "text" : "The objective of this paper is to enable robots to represent and reason with logic-based and probabilistic descriptions of domain knowledge and degrees of belief. We review some related work below.\nThere are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40]. These formulations, by themselves, are not well-suited for reasoning with commonsense knowledge, e.g., default reasoning and non-monotonic logical reasoning. In parallel, research in classical planning and logic programming has provided many algorithms for knowledge representation and reasoning, which have been used on mobile robots. These algorithms typically require a significant amount of prior knowledge of the domain and the agent’s capabilities, and the preconditions and effects of the actions. Many of these algorithms are based on first-order logic, and do not support capabilities such as nonmonotonic logical reasoning, default reasoning, and the ability to merge new, unreliable information with the current beliefs in a knowledge base. Other logic-based formalisms address some of these limitations. This includes, for instance, theories of reasoning about action and change, as well as Answer Set Prolog (ASP), a non-monotonic logic programming paradigm, which is well-suited for representing and reasoning with commonsense knowledge [8, 18]. An international research community has developed around ASP, with applications in cognitive robotics [15] and other non-robotics domains. For instance, ASP has been used for planning and diagnostics by one or more simulated robot housekeepers [14], and for representation of domain knowledge learned through natural language processing by robots interacting with humans [11]. ASP-based architectures have also been used for the control of unmanned aerial vehicles in dynamic indoor environments [6, 7]. However, ASP does not support quantitative models of uncertainty, whereas a lot of information available to robots is represented probabilistically to quantitatively model the uncertainty in sensor input processing and actuation.\nMany approaches for reasoning about actions and change in robotics and artificial intelligence (AI) are based on action languages, which are formal models of parts of natural language used for describing transition diagrams. There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29]. In robotics applications, we often need to represent and reason with recursive state constraints, non-boolean fluents and non-deterministic causal laws. We expanded ALd , which already supports recursive state constraints, to address there requirements. We also expanded the notion of histories to include initial state defaults. Action language BC also supports the desired capabilities but it allows causal laws specifying default values of fluents at arbitrary time steps, and is thus too powerful for our purposes and occasionally poses difficulties with representing all exceptions to such defaults when the domain is expanded.\nRobotics and AI researchers have designed algorithms and architectures based on the understanding that robots interacting with the environment through sensors and actuators need both logical and probabilistic reasoning capabilities. For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28]. Another example is the behavior control of a robot that included semantic maps and commonsense knowledge in a probabilistic relational representation, and then used a continual planner to switch between decision-theoretic and classical planning procedures based on degrees of belief [24]. The performance of such architectures can be sensitive to the choice of threshold for switching between the different planning procedures, and the use of first order logic in these architectures limits the expressiveness and use of commonsense knowledge. More recent work has used a three-layered organization of knowledge (instance, default and diagnostic), with knowledge at the higher level modifying that at the lower levels, and a three-layered architecture (competence layer, belief layer and deliberative layer) for distributed control of information flow, combining first-order logic and probabilistic reasoning for open world planning [23]. Declarative programming has also been combined with continuous-time planners for path planning in mobile robot teams [42]. More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].\nCombining logical and probabilistic reasoning is a fundamental problem in AI, and many principled algorithms have been developed to address this problem. For instance, a Markov logic network combines probabilistic graphical\nmodels and first order logic, assigning weights to logic formulas [39]. Bayesian Logic relaxes the unique name constraint of first-order probabilistic languages to provide a compact representation of distributions over varying sets of objects [36]. Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33]. Despite significant prior research, knowledge representation and reasoning for robots collaborating with humans continues to present many open problems. Algorithms based on first-order logic do not support non-monotonic logical reasoning, and do not provide the desired expressiveness for capabilities such as default reasoning—it is not always possible to express degrees of belief and uncertainty quantitatively, e.g., by attaching probabilities to logic statements. Other algorithms based on logic programming do not support one or more of the capabilities such as reasoning about relations as in causal Bayesian networks; incremental addition of probabilistic information; reasoning with large probabilistic components; or dynamic addition of variables to represent open worlds. Our prior work has developed architectures that support different subsets of these capabilities. For instance, we developed an architecture that coupled planning based on a hierarchy of POMDPs [47, 52] with ASPbased inference. The domain knowledge included in the ASP knowledge base of this architecture was incomplete and considered default knowledge, but did not include a model of action effects. ASP-based inference provided priors for POMDP state estimation, and observations and historical data from comparable domains were considered for reasoning about the presence of target objects in the domain [53]. Building on recent work [44, 51], this paper describes a general refinement-based architecture for knowledge representation and reasoning in robotics. The architecture enables robots to represent and reason with descriptions of incomplete domain knowledge and uncertainty at different levels of granularity, tailoring sensing and actuation to tasks to support scaling to larger, complex domains."
    }, {
      "heading" : "4 Design Methodology",
      "text" : "Our proposed architecture is based on a design methodology. A designer following this methodology will:\n1. Provide a coarse-resolution description of the robot’s domain in action language ALd together with the description of the initial state.\n2. Provide the necessary domain-specific information for, and construct and examine correctness of, the fineresolution refinement of the coarse-resolution description.\n3. Provide domain-specific information and randomize the fine-resolution description of the domain to capture the non-determinism in action execution.\n4. Run experiments and collect statistics to compute probabilities of the outcomes of actions and the reliability of observations.\n5. Provide these components, together with any desired goal, to a reasoning system that directs the robot towards achieving this goal.\nThe reasoning system implements an action loop that can be viewed as an interplay between a logician and statistician (Section 1 and Section 9). In this paper, the reasoning system uses ASP-based non-monotonic logical reasoning, POMDP-based probabilistic reasoning, models and descriptions constructed during the design phase, and records of action execution and observations obtained from the robot. The following sections describe components of the architecture, design methodology steps, and the reasoning system. We first define some basic notions, specifically action description and domain history, which are needed to build mathematical domain models."
    }, {
      "heading" : "5 Action Language and Histories",
      "text" : "This section first describes extensions to action language ALd to support non-boolean fluents and non-deterministic causal laws (Section 5.1). Next, Section 5.2 expands the notion of the history of a dynamic domain to include initial state defaults, defines models of such histories, and describes how these models can be computed. The subsequent sections describe the use of these models (of action description and history) to provide the coarse-resolution description of the domain, and to build more refined fine-resolution models of the domain.\n5.1 ALd with non-boolean fluents and non-determinism Action languages are formal models of parts of natural language used for describing transition diagrams. In this paper, we extend action language ALd [16, 17, 18] (we preserve the old name for simplicity) to allow non-boolean fluents and non-deterministic causal laws. A system description of ALd consists of a sorted signature containing a collection of basic sorts organized into an inheritance hierarchy, and three special sorts: statics, f luents and actions. Statics are domain properties whose truth values cannot be changed by actions (e.g., locations of walls and doors), fluents are properties whose values can be changed by actions (e.g., location of the robot), and actions are sets of elementary actions that can be executed in parallel. Fluents of ALd are divided into basic and defined. The defined fluents are boolean, do not obey laws of inertia, and are defined in terms of other fluents, whereas basic fluents obey laws of inertia (thus often called inertial fluents in the knowledge representation literature) and are directly changed by actions. There are two types of basic fluents. The first one is related to the physical properties of the domain—fluents of this type can be changed by actions that change the physical state. The second one, called a basic knowledge fluent is changed by knowledge producing actions that only change the agent’s knowledge about the domain. Atoms in ALd are of the form f (x̄) = y, where y and elements of x̄ are variables or properly typed object constants2—when convenient, we also write this as f (x̄,y). If f is boolean, we use the standard notation f (x̄) and ¬ f (x̄). Literals are expressions of the form f (x̄) = y and f (x̄) 6= y. For instance, in our example domain (Example 1), static next to(Pl1,Pl2) says that places Pl1 and Pl2 are next to each other, basic fluent loc(T h) = Pl says that thing T h is located in place Pl, basic fluent in hand(R,Ob) says that robot R is holding object Ob, and action move(R,Pl) moves robot R to place Pl.\nALd allows five types of statements: deterministic causal laws, non-deterministic causal laws, state constraints, definitions, and executability conditions. Deterministic causal laws are of the form:\na causes f (x̄) = y if body (1)\nwhere a is an action, f is a basic fluent, and body is a collection of literals. Statement 1 says that if a is executed in a state satisfying body, the value of f in any resulting state would be y. Non-deterministic causal laws are of the form:\na causes f (x̄) : {Y : p(Y )} if body (2)\nwhere p is a unary boolean function and p(Y ) is a boolean literal, or:\na causes f (x̄) : sort name if body (3)\nwhere Statement 2 says that if a were to be executed in a state satisfying body, f may take on any value from the set {Y : p(Y )}∩range( f ) in the resulting state. Statement 3 says that f may take any value from {sort name∩range( f )}. If the body of a causal law is empty, the if will be omitted. In the context of Example 1, the deterministic causal law:\nmove(R,Pl) causes loc(R) = Pl\nsays that a robot R moving to place Pl will end up in Pl. Examples of other forms of the causal law are provided later.\nState constraints are of the form: f (x̄) = y if body (4)\nwhere f is a basic fluent or static. The state constraint says that f (x̄) = y must be true in every state satisfying body. For instance, the constraint:\nloc(Ob) = Pl if loc(R) = Pl, in hand(R,Ob)\nguarantees that the object grasped by a robot shares the robot’s location.\nThe definition of a defined fluent f (x̄) is a collection of statements of the form:\nf (x̄) if body (5)\n2This representation of relations as functions, in the context of ASP, is based on prior work [5].\nAs in logic programming definitions, f (x̄) is true if it follows from the truth of at least one of its defining rules. Otherwise, f (x̄) is false.\nExecutability conditions are statements of the form:\nimpossible a0, . . . ,ak if body (6)\nwhich implies that in a state satisfying body, actions a0, . . .ak cannot be executed simultaneously. For instance, the following executability condition:\nimpossible move(R,Pl) if loc(R) = Pl\nimplies that a robot cannot move to a location if it is already there. A collection of statements of ALd forms a system description D . The semantics of D is given by a transition diagram τ(D) whose nodes correspond to possible states of the system. A diagram contains an arc 〈σ1,a,σ2〉 if, after the execution of action a in a state σ1, the system may move into state σ2. We define the states and transitions of τ(D) in terms of answer sets of logic programs, as described below; see [16, 18] for more details.\nRecall that an interpretation of the signature of D is an assignment of a value to each f (x̄) from the signature. An interpretation can be represented by the collection of atoms of the form f (x̄) = y, where y is the value of f (x̄). For any interpretation σ , let σnd denote the collection of all atoms of σ formed by basic fluents and statics—”nd” stands for non-defined. Also, let Πc(D), where c stands for constraints, denote the logic program defined as follows:\n1. For every state constraint (Statement 4) and definition (Statement 5), program Πc(D) contains:\nf (x̄) = y← body\n2. For every defined fluent f , Πc(D) contains the closed world assumption (CWA):\n¬ f (x̄)← body, not f (x̄)\nwhere, unlike classical negation “¬ a” that implies “a is believed to be false”, default negation “not a” only implies that “a is not believed to be true”.\nWe can now define states of τ(D).\nDefinition 1. [State of τ(D)] An interpretation σ is a state of the transition diagram τ(D) if it is the unique answer set of program Πc(D)∪σnd .\nThe uniqueness of an answer set is guaranteed for a large class of system descriptions that are well-founded. Although well-foundedness is not easy to check, the broad syntactic condition called weak-acyclicity, which is easy to check, is a sufficient condition for well-foundedness [17]. All system descriptions discussed henceforth in this paper are assumed to be well-founded.\nOur definition of transition relation of τ(D) is also based on the notion of the answer set of a logic program.\nDefinition 2. [Transition of τ(D)] To describe a transition 〈σ0,a,σ1〉, we construct a program Π(D ,σ0,a) consisting of:\n• Logic programming encoding Π(D) of system description D .\n• Initial state σ0.\n• Set of actions a.\nThe answer sets of this program determine the states the system can move into after the execution of a in σ0. The encoding Π(D) of system description D consists of the encoding of the signature of D and rules obtained from statements of D , as described below.\n• Encoding of the signature: we start with the encoding sig(D) of signature of D .\n– For each basic sort c, sig(D) contains: sort name(c). – For each subsort link 〈c1,c2〉 of the hierarchy of basic sorts, sig(D) contains: s link(c1,c2). – For each membership link 〈x,c〉 of the hierarchy, sig(D) contains: m link(x,c). – For every function symbol f : c1× . . .cn→ c, the signature sig(D) contains the domain: dom( f ,c1, . . . ,cn),\nand range: range( f ,c).\n– For every static g of D , sig(D) contains: static(g). – For every f (x̄) where f is a basic fluent, sig(D) contains: f luent(basic, f (x̄)). – For every f (x̄) where f is a defined fluent, sig(D) contains: f luent(de f ined, f (x̄)). – For every action a of D , sig(D) contains: action(a).\nWe also need axioms describing the hierarchy of basic sorts:\nsubsort(C1,C2)← s link(C1,C2) subsort(C1,C2)← s link(C1,C), subsort(C,C2) member(X ,C)← m link(X ,C)\nmember(X ,C1)← m link(X ,C0), subsort(C0,C1)\n• Encoding of statements of D: for this encoding we need two steps that stand for the beginning and the end of a transition. This is sufficient for describing a single transition; however, we later describe longer chains of events and let steps range over [0,n] for some constant n. To allow an easier generalization of the program, we encode steps by using constant n for the maximum number of steps, as follows:\nstep(0..n)\nwhere n will be assigned a specific value based on the goals under consideration, e.g., #const n= 3. We also need a relation val( f (x1, . . . ,xn),y, i), which states that the value of f (x1, . . . ,xn) at step i is y; and relation occurs(a, i), which states that compound action a occurred at step i, i.e., occurs({a0, . . . ,ak}, i) =de f {occurs(ai) : 0≤ i≤ k}. We use this notation to encode statements of D as follows:\n– For every causal law (Statements 2-3), where the range of f is {y1, . . . ,yk}, Π(D) contains a rule:\nval( f (x̄),y1, I +1) or . . .or val( f (x̄),yk, I +1)←val(body, I), occurs(a, I), I < n\nwhere val(body, I) is obtained by replacing every literal fm(x̄m) = z from body by val( fm(x̄m),z, I). To encode that due to this action, f (x̄) only takes a value that satisfies property p, Π(D) contains a constraint:\n← val( f (x̄),Y, I +1), not val(p(Y ), true, I)\nand rules:\nsatis f ied(p, I)← val(p(Y ), true, I) ¬occurs(a, I)← not satis f ied(p, I)\n– For every state constraint and definition (Statements 4, 5), Π(D) contains:\nval( f (x̄),y, I)← val(body, I)\n– Π(D) contains the CWA for defined fluents:\nval(F, f alse, I)← f luent(de f ined,F), not val(F, true, I)\n– For every executability condition (Statement 6), Π(D) contains:\n¬occurs(a0, I) or . . . or ¬occurs(ak, I)←val(body, I), I < n\n– Π(D) contains the Inertia Axiom:\nval(F,Y, I +1)← f luent(basic,F), val(F,Y, I), not ¬val(F,Y, I +1), I < n\n– Π(D) contains CWA for actions:\n¬occurs(A, I)← not occurs(A, I), I < n\n– Finally, we need the rule: ¬val(F,Y1, I)← val(F,Y2, I), Y1 6= Y2\nwhich says that a fluent can only have one value at each time step.\nThis completes the construction of encoding Π(D) of system description D . Please note that the axioms described above are shorthand for the set of ground instances obtained from them by replacing variables by (the available) ground terms from the corresponding sorts.\nTo continue with our definition of transition 〈σ0,a,σ1〉, we describe the two remaining parts of program Π(D ,σ0,a), the encoding val(σ0,0) of initial state σ0, and the encoding occurs(a,0) of action a:\nval(σ0,0) =de f {val( f (x̄),y,0) : ( f (x̄) = y) ∈ σ0} occurs(a,0) =de f {occurs(ai,0) : ai ∈ a}\nTo complete program Π(D ,σ0,a), we simply gather our description of the system’s laws, together with the description of the initial state and the actions that occur in it:\nΠ(D ,σ0,a) =de f Π(D)∪ val(σ0,0)∪occurs(a,0)\nNow we are ready to define the notion of transition of τ(D). Let a be a non-empty collection of actions, and σ0 and σ1 be states of the transition diagram τ(D) defined by a system description D . A state-action-state triple 〈σ0,a,σ1〉 is a transition of τ(D) iff Π(D ,σ0,a) has an answer set AS such that σ1 = { f (x̄) = y : val( f (x̄),y,1) ∈ AS}."
    }, {
      "heading" : "5.2 Histories with defaults",
      "text" : "A dynamic domain’s recorded history is usually a collection of records of the form obs(L, I), which says that a literal is observed at step I, e.g., obs(loc(tb1) = o ff ice,0) denotes the observation of textbook tb1 in the o ff ice—when convenient, this will also be written as obs(loc(tb1,o ff ice), true,0); and hpd(action,step), which says that a specific action happened happen at a given step, e.g., hpd(move(rob1,kitchen),1). In addition to the statements described above, we introduce an additional type of historical record:\ninitial default f (x̄) = y if body (7)\nwhere f (x̄) is a basic fluent. We illustrate the use of such initial state defaults with an example, before providing a formal description.\nExample 2. [Example of defaults] Consider the following statements about the locations of textbooks in the initial state in our illustrative example. Textbooks are typically in the main library. If a textbook is not there, it is typically in the auxiliary library. If a textbook is checked out, it can usually be found in the office. These defaults can be represented as:\ninitial default loc(X) = main library if textbook(X) % Default d1 (8)\ninitial default loc(X) = aux library if textbook(X), % Default d2 (9) loc(X) 6= main library\ninitial default loc(X) = o ff ice if textbook(X), % Default d3 (10) loc(X) 6= main library, loc(X) 6= aux library\nwhere we use the fluent {loc : thing→ place}. Intuitively, a history Ha with the above statements entails: val(loc(tb1)= main library, true,0) for textbook tb1. History Hb that adds obs(loc(tb1) 6= main library,0) as an observation to Ha renders default d1 (Statement 8) inapplicable; it entails: val(loc(tb1) = aux library, true,0) based on default d2 (Statement 9). A history Hc that adds observation: obs(loc(tb1) 6= aux library,0) to Hb should entail: val(loc(tb1) = o ff ice, true,0). Adding observation obs(loc(tb1) 6= main library,1) to Ha results in history Hd that defeats default d1 because, if this default’s conclusion is true in the initial state, it is also true at step 1 (by inertia), which contradicts our observation. Default d2 will conclude that this book is initially in the aux library; the inertia axiom will propagate this information to entail: val(loc(tb1) = aux library, true,1). Figure 2 illustrates the beliefs of a robot corresponding to these four histories. Please see example2.sp at https://github.com/mhnsrdhrn/refine-arch for an example of the complete program in SPARC.\nThe history H of the system will define collection of its models, i.e., trajectories of the system considered possible by the agent recording this history. To define such models, consider program Π(D ,H ) obtained by adding to Π(D):\n• Record of observations and actions from H .\n• Rules for every default (Statement 7):\nval( f (x̄),y,0)←val(body,0), (11) not ¬val( f (x̄),y,0)\nval( f (x̄),Y,0) +←val(body,0), % CR rule (12) range( f ,C),\nmember(Y,C),\nY 6= y is de f ined( f (x̄))←val(body,0)\nwhere the second rule is a consistency restoring (CR) rule, which states that to restore consistency of the program one may assume that the conclusion of the default is not the expected one. For more details about CR rules, please see [18].\n• A rule defining initial values of fluents through observations:\nis de f ined( f (x̄))← obs( f (x̄) = y,0) (13)\n• A rule for every basic fluent:\nval( f (x̄),y1,0) or . . . or val( f (x̄),yn,0)← not is de f ined( f (x̄)) (14)\nwhere {y1, . . . ,yn} are elements in the range of f not occurring in the head of any initial default of H . This rule states that if a fluent is not defined in the initial state by the head of an initial default, or by an observation, it must take on some possible value from its range.\n• A reality check [3]: ←val(F,Y1, I), obs(F = Y2, I), Y1 6= Y2 (15) ←val(F,Y1, I), obs(F 6= Y1, I)\nwhich states that an observation of a fluent shall return the fluent’s expected value.\n• And a rule: occurs(A, I)← hpd(A, I) (16)\nTo define a model of the history, we also need the following auxiliary definitions.\nDefinition 3. [Defined Sequence] We say that a set S of literals defines a sequence 〈σ0,a0,σ1, . . . ,an−1,σn〉 if\n• For every 0≤ i≤ n, ( f (x̄) = y) ∈ σi iff val( f (x̄),y, i)) ∈ S.\n• For every 0≤ i < n, e ∈ ai iff occurs(e, i) ∈ S.\nDefinition 4. [Compatible initial states] A state σ of τ(D) is compatible with a description I of the initial state of history H if:\n• σ satisfies all observations of I ; and\n• σ contains the closure of the union of statics of D and the set { f = y : obs( f = y,0) ∈I }∪{ f 6= y : obs( f 6= y,0) ∈I }.\nLet Ik be the description of the initial state of history Hk. States in Example 2 compatible with Ia, Ib, Ic must then contain {loc(tb1) = main library}, {loc(tb1) = aux library}, and {loc(tb1) = o ff ice} respectively. There are multiple such states, which differ by the location of robot. Since Ia = Id , they have the same compatible states.\nNext, we define models of history H , i.e., paths of the transition diagram τ(D) of D compatible with H .\nDefinition 5. [Models] A path M = 〈σ0,a0,σ1, . . . ,an−1,σn〉 of τ(D) is a model of history H of D , with description I of its initial state, if there is a collection E of obs statements such that:\n1. If obs( f = y,0) ∈ E then f 6= y is the head of one of the defaults of I . Similarly, for obs( f 6= y,0).\n2. The initial state of M is compatible with the description: IE = I ∪E.\n3. Path M satisfies all observations of fluents and action occurrences in H .\n4. There is no collection E0 of init statements which has less elements than E and satisfies the conditions above.\nWe will refer to E as an explanation of H . For example, consider the four histories described in Example 2. Models of Ha, Hb, and Hc are paths consisting of initial states compatible with Ia, Ib, and Ic—the corresponding explanations are empty. However, in the case of Hd , the situation is different—the predicted location of tb1 will be different from the observed one. The only explanation of this discrepancy is that tb1 is an exception to the first default. Adding E = {obs(loc(tb1) 6= main library,0)} to Id will resolve this problem.\nWe illustrate this definition by some more examples.\nExample 3. [Examples of Models] Consider a system description Da with basic boolean fluents f and g and a history Ha:\ninitial default ¬g if f\n{ f ,¬g}, {¬ f ,g}, and {¬ f ,¬g} are models of 〈Da,Ha〉 and σ = { f ,g} is not. The latter is not surprising since even though σ may be physically possible, the agent, relying on the default, will not consider σ to be compatible with the default since the history gives no evidence that the default should be violated.\nIf the agent were to record an observation: obs(g,0), the only states compatible with the resulting history Hb (i.e.,the models) would be { f ,g} and {¬ f ,g}. Next, we expand our system description by a basic fluent h and a state constraint:\nh if ¬g\nIn this case, to compute models of a history Hc of a system Db, where Hc consists of the default and an observation: obs(¬h,0), we need CR rules (see second rule of Statement 11). The models are { f ,¬h,g} and {¬ f ,¬h,g}.\nNext, consider a system description Dc with basic fluents f , g, and h, the initial default, action a, a causal law:\na causes h if ¬g\nand a history Hd consisting of obs( f ,0), hpd(a,0); 〈{ f ,¬g,h},a,{ f ,¬g,h}〉 and 〈{ f ,¬g,¬h},a,{ f ,¬g,h}〉 are the two models of Hd . Finally, history He obtained by adding obs(¬h,1) to Hd has a single model 〈{ f ,g,¬h},a,{ f ,g,h}〉. The new observation is an indirect exception to the initial default, which is resolved using the corresponding CR rule.\nComputing models of system histories: The definition of models of a history H of a system D (see Definition 5) suggests a simple algorithm for computing a model of H . We only need to use an (existing) answer set solver to compute an answer set AS of a program Π(D ,H ), and check if AS defines a path of τ(D). To do the latter, we need to check that for every 0≤ i≤ n, the set:\n{ f (x̄) = y : val( f (x̄),y, i) ∈ AS}\nis a state of τ(D), and that triples 〈σi,ai,σi+1〉 defined by AS are transitions of τ(D). A triple 〈σi,a,σi+1〉 is defined by AS if σi = { f (x̄) = y : val( f (x̄),y, i)∈AS}, σi+1 = { f (x̄) = y : val( f (x̄),y, i+1)∈AS}, and ai = {e : occurs(e, i)∈AS}. Although this check can be done using the definitions of state and transition, the following theorem shows that for a large class of system descriptions, this computation can (fortunately) be avoided.\nProposition 1. [Models and Answer Sets] A path M = 〈σ0,a0,σ1, . . . ,σn−1,an〉 of τ(D) is a model of history H n iff there is an answer set AS of a program Π(D ,H ) such that:\n1. A fluent literal ( f = y) ∈ σi iff val( f ,y, i) ∈ AS,\n2. A fluent literal ( f 6= y1) ∈ σi iff val( f ,y2, i) ∈ AS,\n3. An action e ∈ ai iff occurs(e, i) ∈ AS.\nThe proof of this proposition is in Appendix A. This proposition allows us to reduce the task of planning to computing answer sets of a program obtained from Π(D ,H ) by adding the definition of a goal, a constraint stating that the goal must be achieved, and a rule generating possible future actions of the robot. In other words, the same process of computing answer sets can be used for inference, planning and diagnostics."
    }, {
      "heading" : "6 Logician’s Domain Representation",
      "text" : "We are now ready for the first step of our design methodology, i.e., specify the transition diagram of the logician.\n1. Specify the transition diagram, τH , which will be used by the logician for high-level reasoning, including planning and diagnostics.\nThis step is accomplished by providing the signature and ALd axioms of system description DH defining this diagram. We will use standard techniques for representing knowledge in action languages, e.g., [18]. We illustrate this process by describing the domain representation for the office domain described in Example 1).\nExample 4. [Logician’s domain representation] The system description DH of the domain in Example 2 consists of a sorted signature (ΣH ) and axioms describing the transition diagram τH . ΣH defines the names of objects and functions available for use by the logician. As described in Example 1, the sorts are: place, thing, robot, and ob ject, with ob ject and robot being subsorts of thing. The sort ob ject has subsorts such as: textbook, printer and kitchenware. The statics include a relation next to(place, place), which describes if two places are next to each other. The signatures of the fluents of the domain are: loc : thing→ place and in hand : robot × ob ject → boolean. The value of loc(T h) = Pl if thing T h is located at place Pl. The fluent in hand(R,Ob) is true if robot R is holding object Ob. These are basic fluents subject to the laws of inertia. The domain has three actions: move(robot, place), grasp(robot,ob ject), and putdown(robot,ob ject). The domain dynamics are defined using axioms that consist of causal laws:\nmove(R,Pl) causes loc(R) = Pl (17) grasp(R,Ob) causes in hand(R,Ob) putdown(R,Ob) causes ¬in hand(R,Ob)\nstate constraints: loc(Ob) = Pl if loc(R) = Pl, in hand(R,Ob) (18) next to(P1,P2) if next to(P2,P1)\nand executability conditions:\nimpossible move(R,Pl) if loc(R) = Pl (19) impossible move(R,Pl2) if loc(R) = Pl1, ¬next to(Pl1,Pl2)\nimpossible A1, A2 if A1 6= A2 impossible grasp(R,Ob) if loc(R) = Pl1, loc(Ob) = Pl2, Pl1 6= Pl2 impossible grasp(R,Ob) if in hand(R,Ob) impossible putdown(R,Ob) if ¬in hand(R,Ob)\nThe part of ΣH described so far, the sort hierarchy and the signatures of functions, is unlikely to undergo substantial changes for any given domain. However, the last step in the constructions of ΣH is likely to undergo more frequent revisions—it populates the basic sorts of the hierarchy with specific objects; e.g robot = {rob1}, place = {r1, . . . ,rn} where rs are rooms, textbook = {tb1, . . . tbm}, kitchenware = {cup1,cup2, plate1, plate2} etc. Ground instances of the axioms are obtained by replacing variables by ground terms from the corresponding sorts.\nThe transition diagram τH described by DH is too large to depict in a picture. The top part of Figure 3(a) shows the transitions of τH corresponding to a move between two places. The only fluent shown there is the location of the robot rob1—the values of other fluents remain unchanged and are not shown here. The actions of this coarseresolution transition diagram τH of the logician, as described above, are assumed to be deterministic, and the values of its fluents are assumed to be observable. These assumptions allow the robot to do fast, tentative planning and diagnostics necessary for achieving its assigned goals.\nThe domain representation described above should ideally be tested extensively. This can be done by including various recorded histories of the domain, which may include histories with prioritized defaults (Example 2), and using the resulting programs to solve various reasoning tasks.\nThe logician’s model of the world thus consists of the system description (Example 4), initial state defaults (Example 2), and recorded history of actions and observations. The logician achieves any given goal by first translating the model (of the world) to an ASP program—see Sections 5.1, 5.2. For planning and diagnostics, this program is passed to an ASP solver—we use SPARC, which expands CR-Prolog and provides explicit constructs to specify objects, relations, and their sorts [2]. Please see example4.sp at https://github.com/mhnsrdhrn/refine-arch for the SPARC version of the complete program. The solver returns the answer set of the program. Atoms of the form:\noccurs(action,step)\nbelonging to this answer set, e.g., occurs(a1,1), . . . ,occurs(an,n), represent the shortest sequence of abstract actions, i.e., the shortest plan for achieving the logician’s goal. Prior research results in the theory of action languages and ASP ensure that the plan is provably correct [18]. In a similar manner, suitable atoms in the answer set can be used for diagnostics, e.g., to explain unexpected observations in terms of exogenous actions."
    }, {
      "heading" : "7 Refinement, Zoom and Randomization",
      "text" : "For any given goal, each abstract action in the plan created by reasoning with the coarse-resolution domain representation is implemented as a sequence of concrete actions by the statistician. To do so, the robot probabilistically reasons about the part of the fine-resolution transition diagram relevant to the abstract action to be executed. This section defines refinement, randomization, and the zoom operation, which are necessary to build the fine-resolution models for such probabilistic reasoning, along with the corresponding steps of the design methodology."
    }, {
      "heading" : "7.1 Refinement",
      "text" : "The second step of the design methodology corresponds to the construction of a fine-resolution transition diagram τL from the coarse-resolution transition diagram τH . We illustrate this step by constructing the signature of τL and the axioms of its system description DL for the office domain of Example 1. This construction is not entirely algorithmic— although the signature of τH and its axioms in Example 4 will play an important role, the construction will also depend on the result of the increase in the resolution, which is domain dependent. Note, however, that any input provided by the designer is during the initial design phase—at run-time, all steps of planning and execution are algorithmic and automated. We begin with some terminology.\nIf examining an object of ΣH at higher resolution leads to the discovery of new structure(s), the object is said to have been magnified. Newly discovered parts of the magnified objects are referred to as its refined components. We can now construct the signature of τL.\n2. Constructing the refinement τL of τH . (a) Signature ΣL of τL\nA signature ΣL is said to refine signature ΣH if:\n• For every basic sort stH of ΣH , whose elements were magnified by the increase in the resolution, ΣL contains a (a) coarse-resolution version st∗L = stH ; and (b) fine-resolution version stL = {o1, . . . ,om} consisting of the components of magnified elements of stH . We also refer to the fine-resolution version of an original sort as its fine-resolution counterpart.\nFor instance, for the sort place = {r1, . . . ,rn} in ΣH (representing rooms), we have the coarse-resolution copy:\nplace∗ = {r1, . . . ,rn}\nand its fine-resolution counterpart:\nplace = {c1, . . . ,cm}\nwhere c1, . . . ,cm are newly discovered cells in the rooms. Although the original version and its fine-resolution counterpart have the same name, it does not cause a problem because they belong to different signatures. Instead, it proves to be convenient for the construction of axioms of DL. Also, for basic sort stH of ΣH whose elements were not magnified by the increase in resolution, ΣL contains stL = stH .\n• ΣL contains static relation component(oi,o), which holds iff object oi ∈ stL is a newly discovered component of magnified object o from st∗L .\nContinuing with Example 1 and Example 4, we have:\ncomponent : place× place∗→ boolean\nwhere component(c,r) is true iff cell c is part of room r.\n• For every function symbol f : st1, . . . ,stn→ st0 of ΣH :\n– If signature of f contains magnified sorts, ΣL contains function symbol f ∗ : st ′1, . . . ,st ′ n→ st ′0, where st ′i =\nst∗i if sti is magnified and st ′ i = sti otherwise.\n– f : st1, . . . ,stn→ st0 is a function symbol of ΣL.\nIn our example, ΣL will include, for instance:\nloc∗ : thing→ place∗\nand loc : thing→ place\nAlthough the second fluent looks identical to its coarse-resolution counterpart in ΣH , the meaning of place in it is different—elements of sort place are rooms in ΣH , but are cells in ΣL. In a similar manner, ΣL will include both next to(place, place) and next to∗(place∗, place∗)—the former describes two cells that are next to each other while the latter describes two rooms that are next to each other.\n• Actions of ΣL include (a) every action in ΣH with its magnified parameters replaced by their fine-resolution counterparts; and (b) knowledge-producing action:\ntest(robot, f luent,value)\nwhich activates algorithms on the robot R to check if the value of an observable fluent F in a given state is Y . Note that this action only changes basic knowledge fluents (see below).\nIn our example, the sort action of ΣL will have (a) original actions grasp and putdown; (b) action move(robot,cell) of a robot moving to an (adjacent) cell; and (c) test action for testing the values of each observable fluent, e.g., instances of test(R, loc(T h),C) and test(R, in hand(R,O), true) for specific cells and objects.\n• ΣL includes basic knowledge fluents directly observed, indirectly observed and can be tested, and defined fluents may discover and observed. These fluents are used to describe observations of the environment and the axioms governing them—details provided below in the context of the axioms in DL.\n2. Constructing the refinement τL of τH . (b) Axioms of DL\nAxioms of the refined system description DL include:\n• All axioms of DH .\nAlthough this set of axioms is syntactically identical to the axioms of DH , their ground instantiations are different because their variables may range over different sorts. Continuing with our example of refining the description in Example 4, while variables for places were ground using names of rooms in DH , they are ground by names of cells in DL. There are also differences in the definition of statics, as discussed further below.\n• Axioms relating coarse-resolution domain properties and their fine-resolution counterparts. Assuming that the only sort of the signature of the original fluent f influenced by the increase in resolution is its range, the axiom has the form:\nf ∗(X1, . . . ,Xm) = Y if component(C1,X1), . . . , component(Cm,Xm), component(C,Y ), (20) f (C1, . . . ,Cm) =C\nwhere, for non-magnified object O, component(O,O) holds true. In our example, we have:\nloc∗(T h) = Rm if component(C,Rm), loc(T h) =C next to∗(Rm1,Rm2) if component(C1,Rm1), component(C2,Rm2), next to(C1,C2)\nIn general, we need to add component(Ci,Xi) to the body of the rule for those Xi that were affected by the increase in resolution, and change the value of the domain property appropriately in the head and the body of the rule. A key requirement is that the head of the axiom holds true in the coarse-resolution system description if and only if the body of the axiom holds in the fine-resolution (i.e., refined) system description.\n• Axioms for observing the environment.\nIn addition to actions that change domain properties, the refinement includes actions and fluents for observing the environment. As stated above, the values of fluents of DL will be determined by action test(robot, f luent,value). Even though the second parameter of this action is an observable fluent, its value may not always be testable by a sensor, e.g., we assume that a robot cannot check if an object is located in a cell unless the robot is also located in that cell. This is represented by a domain-dependent basic knowledge fluent:\ncan be tested : robot× f luent× value→ boolean\nwhose definition is supplied by the designer. In our example, the corresponding axioms may be:\ncan be tested(R, loc(T h),C) if loc(R) =C can be tested(R, in hand(R,O),V )\nNext, to model the results of sensing, we use another basic knowledge fluent:\ndirectly observed : robot× f luent× value→ outcomes\nwhere: outcomes = {true, f alse,undet}\nis a sort with three possible values true, false, and undetermined. The initial value of directly observed, for all its parameters, is undet. The direct effect of test(R,F,Y ) is then described by the following causal laws:\ntest(R,F,Y ) causes directly observed(R,F,Y ) = true if F = Y. (21) test(R,F,Y ) causes directly observed(R,F,Y ) = f alse if F 6= Y.\nand the executability condition:\nimpossible test(R,F,Y ) if ¬can be tested(R,F,Y ) (22)\nIn the context of the refinement of Example 4, if a robot rob1 located in cell c is checking for an object o, directly observed(rob1, loc(o),c) will be true iff o is in c during testing; it will be false iff o is not in c. These values will be preserved by inertia until the state is observed to have changed when the same cell is tested again. If robot rob1 has not yet tested a cell c for object o, the value of directly observed(r, loc(o),c) will remain undet.\nIn addition to directly observing the fine-resolution fluents using action test, the robot should be able to test the values of observable coarse-resolution fluents inherited from ΣH . For instance, although rob1 cannot directly observe if object o is in a room r, it can do so indirectly if o is observed in cell c of room r—o is indirectly observed to not be in r if all the cells in r have been examined without observing o. Such a relationship is assumed to hold for all magnified fluents. This assumption is axiomatized using a basic knowledge fluent:\nindirectly observed : robot× f luent∗× value∗→ outcomes\nwhich will initially be set to undet, and a defined fluent:\nmay discover : robot× f luent∗× value∗→ boolean.\nwhere may discover(R,F∗,Y ) holds if robot R may discover if the value of F∗, copy of a coarse-resolution fluent whose value is determined by the value of its components, is Y . The axioms for an indirect observation are:\nindirectly observed(R,F∗,Y ) = true if directly observed(R,F,C) = true, (23) component(C,Y )\nindirectly observed(R,F∗,Y ) = f alse if indirectly observed(R,F∗,Y ) 6= true, ¬may discover(R,F∗,Y )\nmay discover(R,F∗,Y ) if indirectly observed(R,F∗,Y ) 6= true, component(C,Y ),\ndirectly observed(R,F,C) = undet\nwhere F and F∗ are the fine-resolution counterpart and the coarse-resolution version (respectively) of a fluent in ΣH that has been magnified. In our example, the axioms for fluent loc will look as follows:\nindirectly observed(R, loc∗(O),Room) = true if directly observed(R, loc(O),C) = true, component(C,Room).\nindirectly observed(R, loc∗(O),Room) = f alse if indirectly observed(R, loc∗(O),Room) 6= true, ¬may discover(R, loc∗(O),Room).\nmay discover(R, loc∗(O),Room) if indirectly observed(R, loc∗(O),Room) 6= true, component(C,Room),\ndirectly observed(R, loc(O),C) = undet.\nFinally, a defined fluent is used to say that any fluent observed directly or indirectly has been observed:\nobserved : robot× f luent× value→ boolean\nwhere observed(R,F,Y ) is true iff the most recent observation of F returned value Y . The following axioms relate this fluent with the basic knowledge fluents defined earlier:\nobserved(R,F,Y ) = true if directly observed(R,F,Y ) = true (24) observed(R,F,Y ) = true if indirectly observed(R,F,Y ) = true\nThis completes our construction of DL. We are now ready to provide a formal definition of refinement.\nDefinition 6. [Refinement of a state] A state δ of τL is said to be a refinement of a state σ of τH if:\n• For every magnified domain property f that is from the signature of ΣH :\nf (x) = y ∈ σ iff f ∗(x) = y ∈ δ\n• For every other domain property of ΣH : f (x) = y ∈ σ iff f (x) = y ∈ δ\nDefinition 7. [Refinement of a system description] Let DL and DH be system descriptions with transition diagrams τL and τH respectively. DL is a refinement of DH if:\n1. States of τL are the refinements of states of τH .\n2. For every transition 〈σ1,aH ,σ2〉 of τH , every fluent f in a set F of observable fluents, and every refinement δ1 of σ1, there is a path P in τL from δ1 to a refinement δ2 of σ2 such that:\n(a) Every action of P is executed by the robot which executes aH . (b) Every state of P is a refinement of σ1 or σ2, i.e., no unrelated fluents are changed. (c) observed(R, f ,Y ) = true ∈ δ2 iff ( f = Y ) ∈ δ2 and observed(R, f ,Y ) = f alse ∈ δ2 iff ( f 6= Y ) ∈ δ2.\nProposition 2. [Refinement] Let DH and DL be the coarse-resolution and fine-resolution system descriptions for the office domain in Example 1. Then DL is a refinement of DH . The proof of this proposition is in Appendix B. As stated in Section 4, it is the designer’s responsibility to establish that the fine-resolution system description of any given domain is a refinement of the coarse-resolution system description."
    }, {
      "heading" : "7.2 Randomization",
      "text" : "The system description DL of transition diagram τL, obtained by refining transition diagram τH , is insufficient to implement a coarse-resolution transition T = 〈σ1,aH ,σ2〉 ∈ τH . We still need to capture the non-determinism in action execution and observations, which brings us to the third step of the design methodology (Section 4).\n3. Provide domain-specific information and randomize the fine-resolution description of the domain to capture the non-determinism in action execution.\nThis step models the non-determinism by first creating DLR, the randomized fine-resolution system description, by:\n• Replacing each action’s deterministic causal laws in DL by non-deterministic ones; and\n• Modifying the signature by declaring each affected fluent as a random fluent, i.e., define the set of values the fluent can choose from when the action is executed. A defined fluent may be introduced to describe this set of values in terms of other variables.\nFor instance, consider a robot moving to a specific cell in the o ff ice. During this move, the robot can reach the desired cell or one of the neighboring cells. The causal law for the move action in DL can therefore be (re)stated as:\nmove(R,C2) causes loc(R) = {C : range(loc(R),C)} (25)\nwhere the robot can only move to a cell that is next to its current cell location:\nimpossible move(R,C2) if loc(R) =C1, ¬next to(C1,C2) (26)\nand the relation range is a defined fluent that is given by:\nrange(loc(R),C) if loc(R) =C range(loc(R),C) if loc(R) =C1, next to(C,C1)\nwhere the cell the robot is currently in, and the cells next to this cell, are all within the range of the robot. In addition, the fluent affected by the change in this causal law is declared as a random fluent and its definition is changed to:\nloc(X) = {Y : p(Y )}\nwhere a thing’s location is one of a set of values that satisfy a given property—in the current example that considers a robot’s location, this property is range as described above. In a similar manner, the non-deterministic version of the test action used to determine the robot’s cell location in the o ff ice, is given by:\ntest(rob1, loc(rob1),ci) causes directly observed(rob1, loc(rob1),ci) = {true, f alse} if loc(rob1) = ci\nwhich indicates that the result of the test action may not always be as expected, and ci are cells in the o ff ice. Similar to refinement, it is the designer’s responsibility to provide domain-specific information needed for randomization.\nCollecting statistics: Once the fine-resolution system description has been randomized, experiments are run and statistics are collected to compute the probabilities of action outcomes and the reliability of observations. This corresponds to the fourth step of the design methodology (Section 4).\n4. Run experiments, collect statistics, and compute probabilities of action outcomes and the reliability of observations.\nSpecifically, we need to compute the:\n• Causal probabilities for the outcomes of actions; and\n• Quantitative model for observations, which provides the probability of the observations being correct.\nThis collection of statistics is typically a one-time process performed in an initial training phase. Also, the statistics are computed separately for each basic fluent in DLR. To collect the statistics, we consider one non-deterministic causal law in DLR at a time. We sample some ground instances of this causal law, e.g., corresponding to different atoms in the causal law. The robot then executes the action corresponding to this sampled instance multiple times, and collects statistics (i.e., counts) of the number of times each possible outcome (i.e., value) is obtained. The robot also collects information about the amount of time taken to execute each action.\nAs an example, consider a ground instance of the non-deterministic causal law for move, considering specific locations of a robot in a specific room:\nmove(rob1,c2) causes loc(R) = {c1,c2,c3}\nwhere rob1 in cell c1 can end up in one of three possible cells when it tries to move to c2. In ten attempts to move to c2, assume that rob1 remains in c1 in one trial, reaches c2 in eight trials, and reaches c3 in one trial. The maximum likelihood estimates of the probabilities of these outcomes are then 0.1, 0.8 and 0.1 respectively—the probability of rob1 moving to other cells is zero. Similar statistics are collected for other ground instances of this causal law, and averaged to compute the statistics for the fluent loc for rob1. The same approach is used to collect statistics for other causal laws and fluents, including those related to knowledge actions and basic knowledge fluents. For instance, assume that the collected statistics indicate that testing for the presence of a textbook in a cell requires twice as much computational time (and thus effort) as testing for the presence of a printer. This information, and the relative accuracy of recognizing textbook and printers, will be used to determine the relative value of executing the corresponding test actions (see Section 8.2).\nThere are some important caveats related to the collection of statistics. First, the collection of statistics depends on the availability of relevant ground truth information, e.g., the actual location of rob1 after executing move(rob1,c2)— this information is provided by an external high-fidelity sensor during the training phase, or by a human. Second, although we do not do so in our experiments, it is possible to use heuristic functions to model the computational effort, and to update the statistics incrementally over time—if any heuristic functions are used, the designer has to make them available to automate subsequent steps of our control loop. Third, considering all ground instances of one causal law at a time can require a lot of training in complex domains, but this is often unnecessary. For instance, it is often the case that the statistics of moving from a cell to one of its neighbors is the same for cells in a room and any given robot. In a similar manner, if the robot and an object are (are not) in the same cell, the probability of the robot observing (not observing) the object is often the same for any cell. The designer thus only considers representative samples of the distinct cases to collect statistics, e.g., statistics corresponding to moving between cells will be collected in two different rooms only if these statistics are expected to be different because the rooms have different floor coverings."
    }, {
      "heading" : "7.3 Zoom",
      "text" : "Reasoning probabilistically about the entire randomized fine-resolution system description can become computationally intractable. For any given transition T = 〈σ1,aH ,σ2〉 ∈ τH , this intractability could be offset by limiting fineresolution probabilistic reasoning to the part of transition diagram τLR whose states are the refinements of σ1 and σ2. For instance, for the state transition corresponding to a robot moving from the o ff ice to the kitchen in Example 4, i.e.,\naH = move(rob1,kitchen), we could only consider states of τLR in which the robot’s location is a cell in the o ff ice or the kitchen. However, these states would still contain fluents and actions not relevant to the execution of aH , e.g., locations of domain objects, and the grasp action. What we need is a fine-resolution transition diagram τLR(T ) whose states contain no information unrelated to the execution of aH , while its actions are limited to those which may be useful for such an execution. In the case of aH = move(rob1,kitchen), for instance, states of τLR(T ) should not contain any information about domain objects. In the proposed architecture, the controller constructs such a zoomed fineresolution system description DLR(T ) in two steps. First, a new action description is constructed by focusing on the transition T , creating a system description DH(T ) that consists of ground instances of DH built from object constants of ΣH relevant to T . In the second step, the refinement of DH(T ) is extracted from DLR to obtain DLR(T ). We first consider the requirements of the zoom operation.\nDefinition 8. [Requirements of zoom] The following are the requirements the zoom operation should satisfy:\n1. Every path in the zoomed transition diagram should correspond to a path in the transition diagram before zooming. In other words, for every path Pz of τLR(T ) between states δ z1 ⊆ δ1 and δ z 2 ⊆ δ2, where δ1 and δ2 are\nrefinements of σ1 and σ2 respectively, there is a path P between states δ1 and δ2 in τLR.\n2. Every path in the transition diagram before zooming should correspond to a path in the zoomed transition diagram. In other words, for every path P of τLR, formed by actions of τLR(T ), between states δ1 and δ2 that are refinements of σ1 and σ2 respectively, there is a path Pz of τLR(T ) between states δ z1 ⊆ δ1 and δ z 2 ⊆ δ2.\n3. Paths in τLR(T ) should be of sufficiently high probability for the probabilistic solver to find them.\nTo construct such a zoomed system description DLR(T ) defining transition diagram τLR(T ), we begin by defining relObConH(T ), the collection of object constants of signature ΣH of DH relevant to transition T .\nDefinition 9. [Constants relevant to a transition] For any given (ground) transition T = 〈σ1,aH ,σ2〉 of τH , by relObConH(T ) we denote the minimal set of object constants of signature ΣH of DH closed under the following rules:\n1. Object constants from aH are in relObConH(T );\n2. If f (x1, . . . ,xn) = y belongs to σ1 or σ2, but not both, then x1, . . . ,xn,y are in relObConH(T );\n3. If the body B of an executability condition of aH contains an occurrence of a term f (x1, . . . ,xn) and f (x1, . . . ,xn)= y ∈ σ1 then x1, . . . ,xn,y are in relObConH(T ).\nConstants from relObConH(T ) are said to be relevant to T . In the context of Example 4, consider transition T = 〈σ1,grasp(rob1,cup1),σ2〉 such that loc(rob1) = kitchen and loc(cup1) = kitchen are in σ1. Then, relObConH(T ) consists of rob1 of sort robot and cup1 of sort kitchenware (based on the first rule above), and kitchen of sort place (based on the third rule above and fourth axiom in Statement 19 in Example 4). For more details, see Example 6.\nNow we are ready for the first step of the construction of DLR(T ). Object constants of the signature ΣH(T ) of the new system description DH(T ) are those of relObConH(T ). Basic sorts of ΣH(T ) are non-empty intersections of basic sorts of ΣH with relObConH(T ). The domain properties and actions of ΣH(T ) are those of ΣH restricted to the basic sorts of ΣH(T ), and the axioms of DH(T ) are restrictions of axioms of DH to ΣH(T ). It is easy to show that the system descriptions DH and DH(T ) satisfy the following requirement—for any transition T = 〈σ1,aH ,σ2〉 of transition diagram τH corresponding to system description DH , there exists a transition 〈σ1(T ),aH ,σ2(T )〉 in transition diagram τH(T ) corresponding to system description DH(T ), where σ1(T ) and σ2(T ) are obtained by restricting σ1 and σ2 (respectively) to the signature ΣH(T ).\nIn the second step, the zoomed system description DLR(T ) is constructed by refining the system description DH(T ). Unlike the description of refinement in Section 7.1, which requires the designer to supply domain-specific information, we do not need any additional input from the designer for refining DH(T ) and can automate the entire zoom operation. We now provide a formal definition of the zoomed system description.\nDefinition 10. [Zoomed system description] For a coarse-resolution transition T , DLR(T ) with signature ΣLR(T ) is said to be the zoomed fine-resolution system description if:\n1. Basic sorts of ΣLR(T ) are those of DLR that are refined counterparts of the basic sorts of DH(T ).\n2. Functions of ΣLR(T ) are those of DLR restricted to the basic sorts of ΣLR(T ).\n3. Actions of ΣLR(T ) are those of DLR restricted to the basic sorts of ΣLR(T ).\n4. Axioms of DLR(T ) are those of DLR restricted to the signature ΣLR(T ).\nConsider T = 〈σ1,move(rob1,kitchen),σ2〉 such that loc(rob1) = o ff ice ∈ σ1. The basic sorts of ΣLR(T ) include robotzL = {rob1}, place ∗z L = {o ff ice,kitchen} and place z L = {ci : ci ∈ kitchen∪ o ff ice}. The functions of ΣLR(T ) include loc∗(rob1) taking values from place∗zL , loc(rob1) taking values from place z L, range(loc(rob1), place z L), statics next to∗(place∗zL , place ∗z L ) and next to(place z L, place z L), properly restricted functions related to testing the values of fluent terms etc. The actions include move(rob1,ci) and test(rob1, loc(rob1),ci), where ci are individual elements of placezL. Finally, restricting the axioms of DLR to the signature ΣLR(T ) removes causal laws for grasp and put down, and the first state constraint corresponding to Statement 18 in DLR. Furthermore, in the causal law and executability condition for move, we only consider cells in the kitchen or the o ff ice.\nBased on Definition 7 of refinement and Proposition 2, it is easy to show that the system descriptions DH(T ) and DLR(T ) satisfy the following requirement—for any transition 〈σ1(T ),aH ,σ2(T )〉 in transition diagram τH(T ) corresponding to system description DH(T ), where σ1(T ) and σ2(T ) are obtained by restricting states σ1 and σ2 (respectively) of DH to signature ΣH(T ), there exists a path in τLR(T ) between every refinement δ z1 of σ1(T ) and a refinement δ z2 of σ2(T ). We now provide two examples of constructing the zoomed system description.\nExample 5. [First example of zoom] As an illustrative example, consider the transition T = 〈σ1,move(rob1,kitchen),σ2〉 such that loc(rob1) = o ff ice∈ σ1. In addition to the description in Example 4, assume that the domain includes (a) boolean fluent broken(robot); and (b) fluent color(robot) taking a value from a set of colors—there is also an executability condition:\nimpossible move(Rb,Pl) if broken(Rb)\nIntuitively, color(Rb) and broken(Rb), where Rb 6= rob1, are not relevant to aH , but broken(rob1) is relevant. Specifically, based on Definition 9, relObConH(T ) consists of rob1 of sort robot, and {kitchen,o ff ice} of sort place—basic sorts of ΣH(T ) are intersections of these sorts with those of ΣH . The domain properties and actions of signature ΣH(T ) are restricted to these basic sorts, and axioms of DH(T ) are those of DH restricted to ΣH(T ), e.g., they only include suitably ground instances of the first axiom in Statement 17, the second axiom in Statement 18, and the first three axioms in Statement 19.\nNow, the signature ΣLR(T ) of the zoomed system description DLR(T ) has the following:\n• Basic sorts robotzL = {rob1}, place ∗z L = {o ff ice,kitchen} and place z L = {ci : ci ∈ kitchen∪o ff ice}.\n• Functions that include (a) fluents loc(robotzL) and loc∗(robot z L) that take values from place z L and place ∗z L respec-\ntively, and range(loc(robotzL), place z L); (b) static relations next to ∗(place∗zL , place ∗z L ) and next to(place z L, place z L); (c) broken(robotzL); (d) knowledge fluents, e.g., directly observed(robot z L, loc(robot z L), place z L), etc.\n• Actions that include (a) move(robotzL, place z L); and (b) test(robot z L, loc(robot z L), place z L).\nThe axioms of DLR(T ) are those of DLR restricted to ΣLR(T ), e.g., they include:\nmove(rob1,c j) causes loc(rob1) = {C : range(loc(rob1),C)} test(rob1,loc(rob1),c j) causes directly observed(rob1, loc(rob1),c j) = {true, f alse} if loc(rob1) = c j\nimpossible move(rob1,c j) if loc(rob1) = ci, ¬next to(c j,ci) impossible move(rob1,c j) if broken(rob1)\nwhere range(loc(rob1),C) may hold for C ∈ {ci,c j,ck}, which are within the range of the robot’s current location (ci), and are elements of placezL. Assuming the robot is not broken, each state of τLR(T ) thus includes an atom of the form loc(rob1) = ci, where ci is a cell in the kitchen or the o ff ice, ¬broken(rob1), direct observations of this atom, e.g., directly observed(rob1, loc(rob1),ci) = true, and statics such as next to(ci,c j) etc. Specific actions include move(rob1,ci) and test(rob1, loc(rob1),ci).\nAs an extension to this example, if rob1 is holding textbook tb1 before executing aH = move(rob1,kitchen), i.e., in hand(rob1, tb1) ∈ σ1, then ΣH(T ) also includes tb1 of sort textbook, and ΣLR(T ) includes ob jectzL = {tb1}. The functions of DLR(T ) include basic fluent in hand(robotzL,ob ject z L) and the corresponding knowledge fluents, and the actions and axioms are suitably restricted.\nExample 6. [Second example of zoom] Consider the transition T = 〈σ1,grasp(rob1,cup1),σ2〉 such that loc(rob1) = kitchen is in σ1. Note that this example does not have the additional fluents (e.g., broken) considered in Example 5. Based on Definition 9, relObConH(T ) consists of rob1 of sort robot and cup1 of sort kitchenware, and kitchen of sort place—signature ΣH(T ) and system description DH(T ) are constructed in a manner similar to that described in Example 5.\nNow, the signature ΣLR(T ) of the zoomed system description DLR(T ) has the following: • Basic sorts robotzL = {rob1}, place ∗z L = {kitchen}, place z L = {ci : ci ∈ kitchen}, and ob ject z L = {cup1}.\n• Functions that include (a) basic non-knowledge fluents loc(robotzL) and loc(ob ject z L) that take values from\nplacezL, and loc ∗(robotzL) and loc ∗(ob jectzL) that take values from place ∗z L ; (b) fluent range(loc(robot z L), place z L); (c) static relations next to∗(place∗zL , place ∗z L ) and next to(place z L, place z L); (d) knowledge fluents restricted to the basic sorts and fluents, etc.\n• Actions such as (a) move(robotzL, place z L); (b) grasp(robot z L,ob ject z L); (c) putdown(robot z L,ob ject z L); (d) knowledge-\nproducing actions test(robotzL, loc(robot z L), place z L) and test(robot z L, loc(ob ject z L), place z L), etc.\nThe axioms of DLR(T ) are those of DLR restricted to the signature ΣLR(T ). These axioms include:\nmove(rob1,c j) causes loc(rob1) = {C : range(loc(rob1),C)} grasp(rob1,cup1) causes in hand(rob1,cup1) = {true, f alse}\ntest(rob1,loc(rob1),c j) causes directly observed(rob1, loc(rob1),c j) = {true, f alse} if loc(rob1) = c j test(rob1,loc(cup1),c j) causes directly observed(rob1, loc(cup1),c j) = {true, f alse} if loc(cup1) = c j\nimpossible move(rob1,c j) if loc(rob1) = ci, ¬next to(c j,ci) impossible grasp(rob1,cup1) if loc(rob1) = ci, loc(cup1) = c j, ci 6= c j\nwhere range(loc(rob1),C) may hold for C ∈ {ci,c j,ck}, which are within the range of the robot’s current location (ci), and are elements of placezL. The states of τLR(T ) thus include atoms of the form loc(rob1) = ci and loc(cup1) = c j, where ci and c j are values in placezL, in hand(rob1,cup1), observations, e.g., directly observed(rob1, loc(rob1),ci) = true, statics such as next to(ci,c j), etc. Specific actions include move(rob1,ci), grasp(rob1,cup1), putdown(rob1,cup1), test(rob1, loc(rob1),ci) and test(rob1, loc(cup1),ci).\nIn Examples 5 and 6, the statistics collected earlier (Section 7.2) can be used to assign probabilities to the outcomes of actions, e.g., if action move(rob1,c1) is executed, the probabilities of the outcomes may be:\nP(loc(rob1) = c1) = 0.85\nP(loc(rob1) =Cl | range(loc(rob1),Cl),Cl 6= c1) = 0.15 |Cl| ; where Cl = {Cl : range(loc(rob1),Cl),Cl 6= c1}\nSimilarly, if the robot has to search for a textbook cup1 once it reaches the kitchen, and if a test action is executed to determine the location of a textbook cup1 in cell ci in the kitchen, the probabilities of the outcomes may be:\nP ( directly observed(rob1, loc(cup1,ci) = true ∣∣∣ loc(cup1) = ci)= 0.9\nP ( directly observed(rob1, loc(cup1),ci) = f alse ∣∣∣ loc(cup1) = ci)= 0.1\nGiven DLR(T ) and the probabilistic information, the robot now has to execute a sequence of concrete actions that implement the desired transition T = 〈σ1,aH ,σ2〉. For instance, a robot searching for cup1 in the kitchen can check cells in the kitchen for cup1 until either the cell location of cup1 is determined with high probability (e.g., ≥ 0.9), or all cells are examined without locating cup1. In the former case, the probabilistic belief can be elevated to a fully certain statement, and the robot reasons about the action outcome and observations to infer that cup1 is in the kitchen, whereas the robot infers that cup1 is not in the kitchen in the latter case. Such a probabilistic implementation of an abstract action as a sequence of concrete actions is accomplished by constructing and solving a POMDP, and repeatedly invoking the corresponding policy to choose actions until termination, as described below."
    }, {
      "heading" : "8 POMDP Construction and Probabilistic Execution",
      "text" : "In this section, we describe the construction of a POMDP P(T ) as a representation of the zoomed system description DLR(T ) and the learned probabilities of action outcomes (Section 7.2), and the use of P(T ) for the fine-resolution implementation of transition T = 〈σ1,aH ,σ2〉 of τH . First, Section 8.1 summarizes the use of a POMDP to compute a policy for selecting one or more concrete actions that implement any given abstract action aH . Section 8.2 then describes the steps of the POMDP construction in more detail."
    }, {
      "heading" : "8.1 POMDP overview",
      "text" : "A POMDP is described by a tuple 〈AP,SP,bP0 ,ZP,T P,OP,RP〉 for specific goal state(s). This formulation of a POMDP builds on the standard formulation [27], and the tuple’s elements are:\n• AP: set of concrete actions available to the robot.\n• SP: set of p-states to be considered for probabilistic implementation of aH . A p-state is a projection of states of DLR(T ) on the set of atoms of the form f (t) = y, where f (t) is a basic non-knowledge fine-resolution fluent term, or a special p-state called the terminal p-state. We use the term “p-state” to differentiate between the states represented by the POMDP and the definition of state we use in this paper.\n• bP0 : initial belief state, where a belief state is a probability distribution over SP.\n• ZP: set of observations. An observation is a projection of states of DLR(T ) on the set of atoms of basic knowledge fluent terms corresponding to the robot’s observation of the value of a fine-resolution fluent term, e.g., directly observed(robot, f (t),y) = outcome, where y is a possible outcome of the fluent term f (t). For simplicity, we use the observation none to replace all instances that have undet as the outcome.\n• T P : SP×AP×SP→ [0,1], the transition function, which defines the probability of transitioning to each p-state when particular actions are executed in particular p-states.\n• OP : SP×AP×ZP → [0,1], the observation function, which defines the probability of each observation in ZP when particular actions are executed in particular p-states.\n• RP : SP×AP×SP→ℜ, the reward specification, which encodes the relative immediate reward of taking specific actions in specific p-states.\nThe p-states are considered to be partially observable because they cannot be observed with complete certainty, and the POMDP reasons with probability distributions over the p-states, called belief states. Note that this formulation is only based on a system description and does not include any history of observations and actions—in a standard POMDP formulation, the current p-state is assumed to be the result of all information obtained in previous time steps, i.e., the p-state is assumed to implicitly include the history of observations and actions.\nThe use of a POMDP has two phases (1) policy computation; and (2) policy execution. The first phase computes policy πP : BP → AP that maps belief states to actions, using an algorithm that maximizes the utility (i.e., expected cumulative discounted reward) over a planning horizon—we use a point-based approximate solver [37]. In the second\nphase, the computed policy is used to repeatedly choose an action in the current belief state, updating the belief state after executing the action and receiving an observation. Belief revision is based on Bayesian updates:\nbPt+1(s P t+1) ∝ O(s P t+1,a P t+1,o P t+1)∑\nsPt\nT (sPt ,a P t+1,s P t+1) ·bPt (sPt ) (27)\nwhere bPt+1 is the belief state at time t + 1. The belief update continues until policy execution is terminated. In our case, policy execution terminates when doing so has a higher (expected) utility than continuing to execute the policy. This happens when either the belief in a specific p-state is very high (e.g., ≥ 0.8), or none of the p-states have a high probability associated with them after invoking the policy several times—the latter case is interpreted as the failure to execute the coarse-resolution action under consideration. We will use “POMDP-1” to refer to the process of constructing a POMDP, computing the policy, and using this policy to implement the desired abstract action."
    }, {
      "heading" : "8.2 POMDP construction",
      "text" : "Next, we describe the construction of POMDP P(T ) for the fine-resolution probabilistic implementation of transition T = 〈σ1,aH ,σ2〉 ∈ τH , using DLR(T ) and the statistics collected in the training phase (Section 7.2). We illustrate these steps using examples based on the domain described in Example 1, including the example described in Appendix C.\nActions: the set AP of actions of P(T ) consists of concrete actions from the signature of DLR(T ) and new terminal actions that terminate policy execution. We use a single terminal action—if AP is to include domain-specific terminal actions, it is the designer’s responsibility to specify them. For the discussion below, it will be useful to partition AP into three subsets (1) AP1 , actions that cause a change in the p-states; (2) A P 2 , knowledge-producing actions for testing the values of fluents; and (3) AP3 , terminal actions that terminate policy execution. The example in Appendix C includes (a) actions from AP1 that move the robot to specific cells, e.g., move-0 and move-1 cause robot to move to cell 0 and 1 respectively, and the grasp action; (b) test actions from AP2 to check if the robot or target object (textbook) are in specific cells; and (c) action finish from AP3 that terminates policy execution. P-states, initial belief state and observations: the following steps are used to construct SP, ZP and bP0 .\n1. Construct ASP program Πc(DLR(T ))∪ δ nd . Here, Πc(DLR(T )) is constructed as described in Definition 1 (Section 5.1), and δ nd is a collection of (a) atoms formed by statics; and (b) disjunctions of atoms formed by basic fluent terms. Each disjunction is of the form { f (t) = y1 ∨ . . . ∨ f (t) = yn}, where {y1, . . . ,yn} are possible values of basic fluent term f (t). Observe that AS is an answer set of Πc(DLR(T ))∪ δ nd iff it is an answer set of Πc(DLR(T ))∪g for some g ∈G, where G is the collection of sets of literals obtained by assigning unique values to each basic fluent term f (t) in δ nd . This statement follows from the definition of answer set and the splitting set theorem.\n2. Compute answer set(s) of ASP program Πc(DLR(T ))∪δ nd . Based on the observation in Step-1 above, and the well-foundedness of DLR(T ), it is easy to show that each answer set is unique and is a state of DLR(T ).\n3. From each answer set, extract atoms of the form f (t) = y, where f (t) is a basic non-knowledge fine-resolution fluent term, to obtain an element of SP. Basic fluent terms corresponding to a coarse-resolution domain property, e.g., room location of the robot, are not represented probabilistically and thus not included in SP. We refer to such a projection of a state δ of DLR(T ) as the p-state defined by δ . Also include in SP an “absorbing” terminal p-state absb that is reached when a terminal action from AP3 is executed.\n4. From each answer set, extract atoms formed by basic knowledge fluent terms corresponding to the robot sensing a fine-resolution fluent term’s value, to obtain elements of ZP, e.g., directly observed(robot, f (t),y) = outcome. We refer to such a projection of a state δ of DLR(T ) as an observation defined by δ . As described earlier, for simplicity, observation none replaces all instances in ZP that have undet as the outcome.\n5. If all p-states are equally likely, the initial belief state bP0 is a uniform distribution. If some other distribution is to be used as the initial belief state, this information has to be provided by the designer. For instance, the designer may distribute (1− ε), where ε is a small number such as 0.1, over p-states known to be more likely a priori, and distribute ε over the remaining p-states.\nIn the example in Appendix C, abstract action grasp(rob1, tb1) has to be executed in the o ff ice. To do so, the robot has to move and find tb1 in the o ff ice. Example 6 shows the corresponding DLR(T ), and δ nd includes (a) atoms formed by statics, e.g., next to(c1,c2) where c1 and c2 are neighboring cells in the o ff ice; and (b) disjunctions such as {loc(rob1) = c1 ∨ . . . ∨ loc(rob1) = cn} and {loc(tb1) = c1 ∨ . . . ∨ loc(tb1) = cn}, where {c1, . . . ,cn} ∈ o ff ice. In Step 3, p-states such as {loc(rob1) = c1, loc(tb1) = c1, ¬in hand(rob1,c1)} are extracted from the answer sets. In Step 4, observations such as directly observed(rob1, loc(rob1),c1) = true and directly observed(rob1, loc(tb1),c1) = f alse are extracted from the answer sets. Finally, the initial belief state bP0 is set as a uniform distribution (Step 5).\nTransition function and observation function: a transition between p-states of P(T ) is defined as 〈si,a,s j〉 ∈ T P iff there is an action a ∈ AP1 and a transition 〈δx,a,δy〉 of DLR(T ) such that si and s j are p-states defined by δx and δy respectively. The probability of 〈si,a,s j〉 ∈ T P equals the probability of 〈δx,a,δy〉. In a similar manner, 〈si,a,z j〉 ∈OP iff there is an action a ∈ AP2 and a transition 〈δx,a,δy〉 of DLR(T ) such that si and z j are a p-state and an observation defined by δx and δy respectively. The probability of 〈si,a,z j〉 ∈ OP equals the probability of 〈δx,a,δy〉.\nWe construct T P and OP from DLR(T ) and the statistics collected in the initial training phase (as described in Section 7.2). First, we augment DLR(T ) with causal laws for proper termination:\nf inish causes absb\nimpossible AP if absb\nNext, we note that actions in AP1 cause p-state transitions but provide no observations, while actions in A P 2 do not cause p-state changes but provide observations, and terminal actions in AP3 cause transition to the absorbing state and provide no observations. To use state of the art POMDP solvers, we need to represent T P and OP as a collection of tables, one for each action. More precisely, T Pa [si,s j] = p iff 〈si,a,s j〉 ∈ T P and its probability is p. In a similar manner, OPa [si,z j] = p iff 〈si,a,z j〉 ∈ OP and its probability is p. Algorithm 1 describes the construction of T P and OP.\nSome specific steps of Algorithm 1 are elaborated below.\n• After initialization, Lines 3–12 of Algorithm 1 handle special cases. For instance, any terminal action will cause a transition to the terminal p-state and provide no observations (Lines 4-5).\n• An ASP program of the form Π(DLR(T ),si,Dis j(A)) (Lines 12, 15) is defined as Π(DLR(T ))∪ val(si,0)∪ Dis j(A). Here, Dis j(A) is a disjunction of the form {occurs(a1,0)∨ . . .∨occurs(an,0)}, where {a1, . . . ,an} ∈A. Lines 14-16 construct and compute answer sets of such a program to identify all possible p-state transitions as a result of actions in AP1 —Lines 17-19 construct and compute answer set of such a program to identify possible observations as a result of actions in AP2 .\n• Line 16 extracts a statement of the form occurs(ak ∈ AP1 ,0), and p-state s j ∈ SP, from each answer set AS, to obtain p-state transition 〈si,ak,s j〉. As stated earlier, a p-state is extracted from an answer set by extracting atoms formed by basic non-knowledge fluent terms.\n• Line 19 extracts a statement of the form occurs(a j ∈ AP2 ,0), and observation z j ∈ ZP, from each answer set AS, to obtain triple 〈si,ak,z j〉. As described earlier, an observation is extracted from an answer set by extracting atoms formed by basic knowledge fluent terms.\n• Use probabilities obtained experimentally (Section 7.2) to set probabilities of p-state transitions (Line 16) and observations (Line 19).\nIn the example in Appendix C, a robot in the o ff ice has to pick up a textbook tb1 believed to be in the o ff ice. This example assumes that a move action from one cell to a neighboring cell succeeds with probability 0.95—with probability 0.05 the robot remains in its current cell. It is also assumed that with probability 0.95 the robot observes (does not observe) the textbook when it exists (does not exist) in the cell the robot is currently in. The corresponding T P and OP, constructed for this example, are shown in Appendix C.\nThe correctness of the approach used to extract p-state transitions and observations, in Lines 16, 19 of Algorithm 1, is based on the following propositions.\nAlgorithm 1: Constructing POMDP transition function T P and observation function OP\nInput: SP, AP, ZP, DLR(T ); transition probabilities for actions ∈ AP1 ; observation probabilities for actions ∈ AP2 . Output: POMDP transition function T P and observation function OP.\n1 Initialize T P as |SP|× |SP| identity matrix for each action. 2 Initialize OP as |SP|× |ZP| matrix of zeros for each action. /* Handle special cases */ 3 for each a j ∈ AP3 do 4 T Pa j(∗,absb) = 1 5 OPa j(∗,none) = 1 6 end 7 for each action a j ∈ AP1 do 8 OPa j(∗,none) = 1 9 end\n10 for each a j ∈ AP do 11 OPa j(absb,none) = 1 12 end\n/* Handle normal transitions */\n13 for each p-state si ∈ SP do /* Construct and set probabilities of p-state transitions */ 14 Construct ASP program Π(DLR(T ),si,Dis j(AP1 )). 15 Compute answer sets AS of ASP program. 16 From each AS ∈ AS, extract p-state transition 〈si,ak,s j〉, and set the probability of T Pak [si,s j].\n/* Construct and set probabilities of observations */\n17 Construct ASP program Π(DLR(T ),si,Dis j(AP2 )). 18 Compute answer sets AS of ASP program. 19 From each AS ∈ AS, extract triple 〈si,ak,z j〉, and set value of OPak [si,z j]. 20 end 21 return T P and OP.\nProposition 3. [Extracting p-state transitions from answer sets]\n• If 〈si,a,s j〉 ∈ T P then there is an answer set AS of program Π(DLR(T ),si,Dis j(AP1 )) such that s j = { f (x̄) = y : f (x̄) = y ∈ AS and is basic}.\n• For every answer set AS of program Π(DLR(T ),si,Dis j(AP1 )) and s j = { f (x̄) = y : f (x̄) = y ∈ AS and is basic}, 〈si,a,s j〉 ∈ T P.\nProposition 4. [Extracting observations from answer sets]\n• If 〈si,a,z j〉 ∈ OP then there is an answer set AS of program Π(DLR(T ),si,Dis j(AP2 )) such that z j = { f (x̄) = y : f (x̄) = y ∈ AS and is basic}.\n• For every answer set AS of program Π(DLR(T ),si,Dis j(AP1 )) and z j = { f (x̄) = y : f (x̄) = y ∈ AS and is basic}, 〈si,a,z j〉 ∈ OP.\nIt is possible to show that these propositions are true based on the definition of an answer set, the definition of the zoomed system description DLR(T ), and the definition of the POMDP components.\nReward specification: the reward function RP assigns a real-valued reward to each p-state transition, as described in Algorithm 2. Specifically, for any state transition with a non-zero probability in T P:\nAlgorithm 2: Construction of POMDP reward function RP\nInput: SP, AP, and T P; statistics regarding accuracy and time taken to execute non-terminal actions. Output: Reward function RP.\n/* Consider each possible p-state transition */\n1 for each (s,a,s′) ∈ SP×AP×SP with T P(s,a,s′) 6= 0 do /* Consider terminal actions first */ 2 if a ∈ AP3 then 3 if s′ is a goal p-state then 4 RP(s,a,s′) = large positive value. 5 else 6 RP(s,a,s′) = large negative value. 7 end\n/* Rewards are costs for non-terminal actions */\n8 else 9 Set RP(s,a,s′) based on relative computational effort and accuracy.\n10 end 11 end 12 return RP\n1. If it involves a terminal action from AP3 , the reward is a large positive (negative) value if this action is chosen after (before) achieving the goal p-state.\n2. If it involves non-terminal actions, reward is a real-valued cost (i.e., negative reward) of action execution.\nHere, any p-state s ∈ SP defined by state δ of DLR(T ) that is a refinement of σ2 in transition T = 〈σ1,aH ,σ2〉 is a goal p-state. In Appendix C, we assign large positive reward (100) for executing f inish when textbook tb1 is in the robot’s grasp, and large negative reward (−100) for terminating before tb1 has been grasped. We assign a fixed cost (−1) for all other (i.e., non-terminal) actions. However, as stated earlier, this cost can be a heuristic function of both relative computational effort and accuracy, using domain expertise and statistics collected experimentally. For instance, statistics may indicate that a knowledge-producing action that determines an object’s color takes twice as much time as the action that determines the object’s shape, which can be used to set RP(∗,shape,∗) = −1 and RP(∗,color,∗) =−2. The reward function, in turn, influences the (a) rate of convergence during policy computation; and (b) accuracy of results during policy execution. Appendix C describes the reward function for a specific example.\nComputational efficiency: Solving POMDPs can be computationally expensive, even with state of the art approximate solvers. For specific tasks such as path planning, it may also be possible to use specific heuristic or probabilistic algorithms that are more computationally efficient than a POMDP. However, POMDPs provide a (a) principled and quantifiable trade-off between accuracy and computational efficiency in the presence of uncertainty in both sensing and actuation; and (b) near-optimal solution if the POMDP’s components are modeled correctly. With a POMDP, the computational efficiency can be improved further by “factoring” the p-state estimation problem into sub-problems that model actions and observations influencing one fluent independent of those influencing other fluents. Such a factoring of the problem is not always possible, e.g., when a robot is holding a textbook in hand, the robot’s location and the textbook’s location are not independent. Instead, in our architecture, we preserve such constraints while still constructing a POMDP for the relevant part of the domain to significantly reduce the computational complexity of solving the POMDP. Furthermore, many of the POMDPs required for a given domain can be precomputed, solved and reused. For instance, if the robot has constructed a POMDP for the task of locating a textbook in a room, the POMDP for locating a different book (or even a different object) in the same room may only differ in the values of some transition probabilities, observation probabilities, and rewards. Note that this similarity between tasks may not hold in non-stationary domains, in which the elements of the POMDP tuple (e.g., set of p-states), and the collected statistics (e.g., transition probabilities), may need to be revised over time.\nComputational error: Although the outcomes of POMDP policy execution are non-deterministic, following an optimal policy produced by an exact POMDP solver is most likely (among all such possible policies) to take the robot to a goal p-state if the following conditions hold:\n• The coarse-resolution transition diagram τH of the domain has been constructed correctly;\n• The statistics collected in the initial training phase (Section 7.2) correctly model the domain dynamics; and\n• The reward function is constructed to suitably reward desired behavior.\nThis statement is based on existing literature [27, 35, 43]. We use an approximate POMDP solver for computational efficiency, and an exact belief update (Equation 27), which provides a bound on the regret (i.e., loss in value) achieved by following the computed policy in comparison with the optimal policy [37]. We can thus only claim that the outcomes of executing of our policy are approximately correct with high probability. We can also provide a bound on the margin of error [37]. For instance, if the probability associated with a statement in the fine-resolution representation is p, the margin of error in a commitment made to the history H (in the coarse-resolution representation) based on this statement is (1− p). If a set of statements with probabilities pi are used to arrive at a conclusion that is committed to H , (1−∏i pi) is the corresponding error."
    }, {
      "heading" : "9 Reasoning System and Control Loop of Architecture",
      "text" : "Next, we give a more detailed description of the reasoning system and control loop of our architecture for building intelligent robots. For this description, we (once again) view a robot as consisting of a logician and a statistician, who communicate through a controller, as described in Section 1 and shown in Figure 4. For any given goal, the logician takes as input the system description DH that corresponds to a coarse-resolution transition diagram τH , recorded history H with initial state defaults (see Example 2), and the current coarse-resolution state σ1 (potentially inferred from observations). If recent recorded observations differ from the logician’s predictions, the discrepancies are diagnosed and a plan comprising one or more abstract actions is computed to achieve the goal. Planning and diagnostics are reduced to computing answer sets of the CR-Prolog program Π(DH ,H ). For a given goal, the controller uses the\ntransition T corresponding to the next abstract action aH in the computed plan to zoom to DLR(T ), the part of the randomized fine-resolution system description DLR that is relevant to the T . A POMDP is then constructed from DLR(T ) and the learned probabilities, and solved to obtain a policy. The POMDP and the policy are communicated to the statistician who invokes the policy repeatedly to implement the abstract action aH as a sequence of (more) concrete actions. When the POMDP policy is terminated, the corresponding observations are sent to the controller. The controller performs inference in DLR(T ), recording the corresponding coarse-resolution action outcomes and observations in the coarse-resolution history H , which is used by the logician for subsequent reasoning.\nAlgorithm 3: Control loop Input: coarse-resolution system description DH and history H ; randomized fine-resolution system description\nDLR; coarse-resolution description of the goal; coarse-resolution initial state σ1. Output: robot is in a state satisfying the goal; reports failure if this is impossible.\n1 while goal is not achieved do 2 Logician uses DH and H to find a possible plan, aH1 , . . . ,a H n to achieve the goal. 3 if no plan exists then 4 return failure 5 end 6 i := 1 7 continue1 := true 8 while continue1 do 9 Check pre-requisites of aHi .\n10 if pre-requisites not satisfied then 11 continue1 := false 12 else 13 Controller zooms to DLR(T ), the part of DLR relevant to transition T = 〈σ1,aHi ,σ2〉 and constructs a POMDP. 14 Controller solves POMDP to compute a policy to implement aHi . 15 continue2 := true 16 while continue2 do 17 Statistician invokes policy to select and execute an action, obtain observation, and update belief state. 18 if terminal action executed then 19 Statistician communicates observations to the controller. 20 continue2 = false 21 i := i+1 22 continue1 := (i < n+1) 23 end 24 Controller performs fine-resolution inference, recording action outcomes and observations in H . 25 σ1 = current coarse-resolution state. 26 end 27 end 28 end\nAlgorithm 3 describes the overall control loop for achieving the assigned goal. Correctness of this algorithm (with a certain margin of error) is ensured by:\n1. Applying the planning and diagnostics algorithm discussed in Section 5.2 for planning with τH and H ;\n2. Using the formal definitions of refinement and zoom described in Section 7; and\n3. Using a POMDP to probabilistically plan an action sequence and executing it for each aH of the logician’s plan, as discussed in Section 8.\nThe probabilistic planning is also supported by probabilistic state estimation algorithms that process inputs from sensors and actuators. For instance, the robot builds a map of the domain and estimates its position in the map using a Particle Filter algorithm for Simultaneous Localization and Mapping (SLAM) [49]. This algorithm represents the true underlying probability distribution over the possible states using samples drawn from a proposal distribution. Samples more likely to represent the true state, determined based on the degree of match between the expected and actual sensor observations of domain landmarks, are assigned higher (relative) weights and re-sampled to incrementally converge to the true distribution. Implementations of the particle filtering algorithm are used widely in the robotics literature to track multiple hypotheses of system state. A similar algorithm is used to estimate the pose of the robot’s arm. On the physical robot, other algorithms used to process specific sensor inputs. For instance, we use existing implementations of algorithms to process camera images, which are the primary source of information to identify specific domain objects. The robot also uses an existing implementation of a SLAM algorithm to build a domain map and localize itself in the map. These algorithms are summarized in Section 10, when we discuss experiments on physical robots."
    }, {
      "heading" : "10 Experimental Setup and Results",
      "text" : "This section describes the experimental setup and results of evaluating the architecture’s capabilities."
    }, {
      "heading" : "10.1 Experimental setup",
      "text" : "The proposed architecture was evaluated in simulation and on a physical robot. As stated in Section 8, statistics of action execution, e.g., observed outcomes of all actions and computation time for knowledge producing actions, are collected in an initial training phase. These statistics are used by the controller to compute the relative utility of different actions, and the probabilities of obtaining different action outcomes and observations. The simulator uses these statistics to simulate the robot’s movement and perception. In addition, the simulator represents objects using probabilistic functions of features extracted from images, with the corresponding models being acquired in an initial training phase—see [52] for more details about such models.\nIn each experimental trial, the robot’s goal was to find and move specific objects to specific places—the robot’s location, the target object, and locations of domain objects were chosen randomly. An action sequence extracted from an answer set of the ASP program provides a plan comprising abstract actions, each of which is executed probabilistically. Our proposed architecture, henceforth referred to as “PA”, was compared with: (1) POMDP-1; and (2) POMDP-2, which revises POMDP-1 by assigning specific probability values to default statements to bias the initial belief. The performance measures were: (a) success, the fraction (or %) of trials in which the robot achieved the assigned goals; (b) planning time, the time taken to compute a plan to achieve the assigned goal; and (c) the average number of actions that were executed to achieve the desired goal. We evaluate the following three key hypotheses:\nH1 PA simplifies design in comparison with architectures based on purely probabilistic reasoning and increases confidence in the correctness of the robot’s behavior;\nH2 PA achieves the assigned goals more reliably and efficiently than POMDP-1; and\nH3 Our representation for defaults improves reliability and efficiency in comparison with not using defaults or assigning specific probability values to defaults.\nWe examine the first hypothesis qualitatively in the context of some execution traces grounded in the illustrative domain described in Example 1 (Section 10.2). We then discuss the quantitative results corresponding to the experimental evaluation of the other two hypotheses in simulation and on physical robots (Section 10.3)."
    }, {
      "heading" : "10.2 Execution traces",
      "text" : "The following (example) execution traces illustrate some of the key capabilities of the proposed architecture.\nExecution Example 1. [Planning with default knowledge] Consider the scenario in which a robot is assisting with a meeting in the o ff ice, i.e., loc(rob1,o ff ice), and is assigned a goal state that contains:\nloc(cup1,o ff ice)\nwhere the robot’s goal is to move coffee cup cup1 to the o ff ice.\n• The plan of abstract actions, as created by the logician, is:\nmove(rob1,kitchen)\ngrasp(rob1,cup1)\nmove(rob1,o ff ice)\nputdown(rob1,cup1)\nNote that this plan uses initial state default knowledge that kitchenware are usually found in the kitchen. Each abstract action in this plan is executed by computing and executing a sequence of concrete actions.\n• To implement move(rob1,kitchen), the controller constructs DLR(T ) by zooming to the part of DLR relevant to this action. For instance, only cells in the kitchen and the o ff ice are possible locations of rob1, and move is the only action that can change the physical state, in the fine-resolution representation.\n• DLR(T ) is used to construct and solve a POMDP to obtain an action selection policy, which is provided to the statistician. The statistician repeatedly invokes this policy to select actions (until a terminal action is selected) that are executed by the robot. In the context of Figure 3(b), assume that the robot moved from cell c1 ∈ o ff ice to c5 ∈ kitchen (through cell c2 ∈ o ff ice) with high probability.\n• The direct observation from the POMDP, directly observed(rob1, loc(rob1),c5) = true, is used by the controller for inference in DLR(T ) and DL, e.g., to produce observed(rob1, loc(rob1),kitchen). The controller adds this information to the coarse-resolution history H of the logician, e.g., obs(loc(rob1) = kitchen,1). Since the first abstract action has had the expected outcome, the logician sends the next abstract action in the plan, grasp(rob1,cup1) to the controller for implementation.\n• A similar sequence of steps is performed for each abstract action in the plan, e.g., to grasp cup1, the robot locates the coffee cup in the kitchen and then picks it up. Subsequent actions cause rob1 to move cup1 to the o ff ice, and put cup1 down to achieve the assigned goal.\nExecution Example 2. [Planning with unexpected failure] Consider the scenario in which a robot in the o ff ice is assigned the goal of fetching textbook tb1, i.e., the initial state includes loc(rob1,o ff ice), and the goal state includes:\nloc(tb1,o ff ice)\nThe coarse-resolution system description DH and history H , along with the goal, are passed on to the logician.\n• The plan of abstract actions, as created by the logician, is:\nmove(rob1,main library)\ngrasp(rob1, tb1)\nmove(rob1,o ff ice)\nputdown(rob1, tb1)\nThis plan uses the initial state default knowledge that textbooks are typically in the main library (Statement 8). Each abstract action in this plan is executed by computing and executing a sequence of concrete actions.\n• Assume that loc(rob1,main library), i.e., that the robot is in the main library after successfully executing the first abstract action. To execute the grasp(rob1, tb1) action, the controller constructs DLR(T ) by zooming to the part of DLR relevant to this action. For instance, only cells in the main library are possible locations of rob1 and tb1 in the fine-resolution representation.\n• DLR(T ) is used to construct and solve a POMDP to obtain a action selection policy, which is provided to the statistician. The statistician repeatedly invokes this policy to select actions (until a terminal action is selected) that are executed by the robot. In the context of Figure 3(b), if r2 is the main library, the robot may move to and search for tb1 in each cell in r2, starting from its current location.\n• The robot unfortunately does not find tb1 in any of the cells in the main library in the second step. These observations from the POMDP, i.e., directly observed(rob1, loc(tb1),ci) = f alse for each ci ∈ main library, are used by the controller for inference in DLR(T ) and DL, e.g., to produce observed(rob1, loc(tb1),main library)= f alse. The controller adds this information to the coarse-resolution history H of the logician, e.g., obs(loc(tb1) 6= main library,2).\n• The inconsistency caused by the observation is resolved by the logician using a CR rule, and the new plan is created based on the second initial state default that a textbook not in the main library is typically in the aux library (Statement 9):\nmove(rob1,aux library)\ngrasp(rob1, tb1)\nmove(rob1,o ff ice)\nputdown(rob1, tb1)\n• This time, the robot is able to successfully execute each abstract action in the plan, i.e., it is able to move to the aux library, find tb1 and grasp it, move back to the o ff ice, and put tb1 down to achieve the assigned goal.\nBoth these examples illustrate key advantages provided by the formal definitions, e.g., of the different system descriptions and the tight coupling between the system descriptions, which are part of the proposed architecture:\n1. Once the designer has provided the domain-specific information, e.g., for refinement or computing probabilities of action outcomes, no further input is necessary to automate planning, diagnostics, and execution for any given goal.\n2. Attention is automatically directed to only the relevant part of the available knowledge at the appropriate resolution. For instance, reasoning by the logician (statistician) is restricted to a coarse-resolution (fine-resolution) system description. It is thus easier to understand, and to identify and fix errors in, the observed behavior, in comparison with architectures that consider all the available knowledge or only support probabilistic reasoning [53].\n3. There is smooth transfer of control and relevant knowledge between the components of the architecture, and confidence in the correctness of the robot’s behavior. Also, the proposed methodology supports the use of this architecture on different robots in different domains, e.g., Section 10.3 describes the result of using this architecture on mobile robots in two different indoor domains.\nNext, we describe the experimental evaluation of the hypotheses H2 and H3 in simulation and on a mobile robot."
    }, {
      "heading" : "10.3 Experimental results",
      "text" : "To evaluate hypothesis H2, we first compared PA with POMDP-1 in a set of trials in which the robot’s initial position is known but the position of the object to be moved is unknown. The solver used in POMDP-1 was evaluated with different fixed amounts of time for computing action policies. Figure 5 summarizes the results; each point is the average of 1000 trials, and we set (for ease of interpretation) each room to have four cells. The brown-colored plots\nin Figure 5 represent the ability to successfully achieve the assigned goal (y-axis on the left), as a function of the number of cells in the domain. The blue-colored plots show the number of actions executed before termination. For the plots corresponding to POMDP-1, the number of actions the robot is allowed to execute before it has to terminate is set to 50. We note that PA significantly improves the robot’s ability to achieve the assigned goal in comparison with POMDP-1. As the number of cells (i.e., size of the domain) increases, it becomes computationally difficult to generate good policies with POMDP-1. The robot needs a greater number of actions to achieve the goal and there is a loss in accuracy if the limit on the number of actions the robot can execute before termination is reduced. While using POMDP-1, any incorrect observations (e.g., false positive sightings of objects) significantly impacts the ability to successfully complete the trials. PA, on the other hand, focuses the robot’s attention on relevant regions of the domain (e.g., specific rooms and cells), and it is thus able to recover from errors and operate efficiently.\nNext, we evaluated the time taken by PA to generate a plan as the size of the domain increases. We characterize domain size based on the number of rooms and the number of objects in the domain. We conducted three sets of experiments in which the robot reasons with: (1) all available knowledge of domain objects and rooms; (2) only knowledge relevant to the assigned goal—e.g., if the robot knows an object’s default location, it need not reason about other objects and rooms in the domain to locate this object; and (3) relevant knowledge and knowledge of an additional\n20% of randomly selected domain objects and rooms. Figures 6(a)-6(c) summarize these results. We observe that using just the knowledge relevant to the goal to be accomplished significantly reduces the planning time. PA supports the identification of such knowledge based on the refinement and zooming operations described in Section 7. As a result, robots equipped with PA will be able to generate appropriate plans for domains with a large number of rooms and objects. Furthermore, if we only use a probabilistic approach (POMDP-1), it soon becomes computationally intractable to generate a plan for domains with many objects and rooms. These results are not shown in Figure 6, but they are documented in prior papers evaluating just the probabilistic component of the proposed architecture [47, 52].\nTo evaluate hypothesis H3, i.e., to evaluate our representation and use of default knowledge, we first conducted trials in which PA was compared with PA∗, a version that does not include any default knowledge, e.g., when the robot is asked to fetch a textbook, there is no prior knowledge regarding the location of textbooks, and the robot explores the closest location first. Figure 7 summarizes the average number of actions executed per trial as a function of the number of rooms in the domain—each sample point in this figure is the average of 10000 trials. The goal in each trial is (as before) to move a specific object to a specific place. We observe that our (proposed) representation and use of default knowledge significantly reduces the number of actions (and thus time) required to achieve the assigned goal.\nNext PA was compared with POMDP-2, a version of POMDP-1 that assigns specific probability values to default knowledge (e.g., “textbooks are in the library with probability 0.9”) and suitably revises the initial belief state. The goal (once again) was to find and move objects to specific locations, and we measured the ability to successfully achieve the assigned goal and the number of actions executed before termination. Figures 8-9 summarize the corresponding results under two extreme cases representing a perfect match (mismatch) between the default locations and ground truth locations of objects. In Figure 8, the ground truth locations of target objects (unknown to the robot) match the default locations of the objects, i.e., there are no exceptions to the default statements. We observe that as the probability assigned to the default statement increases, the number of actions executed by the robot decreases and\nthe fraction of trials completed successfully increases. However, for larger values along the x-axis, the difference in the robot’s performance for two different values of the probability (assigned to defaults) is not that significant. In Figure 8, the ground truth locations of the target objects never match the default locations of the objects, i.e., unknown to the robot, all trials correspond to exceptions to the default knowledge. In this case, the robot executes many more actions before termination and succeeds in a smaller fraction of trials as the probability value assigned to default statements increases. We also repeated these experimental trials after varying the extent to which the ground truth locations of objects matched their default locations. We noticed that when the probability assigned to default statements accurately reflects the ground truth, the number of trials in which the robot successfully achieves the goal increases and approaches the performance obtained with PA. However, recall that computing the probabilities of default statements accurately takes a lot of time and effort. Also, these probabilities may change over time and the robot’s ability to achieve the assigned goals may be sensitive to these changes, making it difficult to predict the robot’s behavior with confidence. In addition, it is all the more challenging to accurately represent and efficiently use probabilistic information about prioritized defaults (e.g., Example 2). In general, we observed that the effect of assigning a probability value to defaults is arbitrary depending on factors such as (a) the numerical value chosen; and (b) the degree of match between ground truth and the default information. For instance, if a large probability is assigned to the default knowledge that books are typically in the library, but the book the robot has to move is an exception to the default (e.g., a cookbook), it takes significantly longer for POMDP-2 to revise (and recover from) the initial belief. PA, on the other hand, supports elegant representation of, and reasoning with, defaults and exceptions to these defaults.\nRobot Experiments: In addition to the trials in simulated domains, we implemented and evaluated PA with POMDP1 on physical robots using the Robot Operating System (ROS). We conducted experimental trials with two robot platforms (see Figure 1) in variants of the domain described in Example 1. Visual object recognition is based on learned object models that consist of appearance-based and contextual visual cues [34]. Since, in each trial, the robot’s initial location and the target object(s) are chosen randomly, it is difficult to compute a meaningful estimate of variance, and statistical significance is established through paired trials. In each paired trial, for each approach being compared (e.g., PA or POMDP-1), the target object(s), the robot’s initial location, and the location of domain objects are the same, and the robot has the same initial domain knowledge.\nFirst, we conducted 50 trials on two floors of our Computer Science department building. This domain includes places in addition to those included in our illustrative example, e.g., Figure 1(a) shows a subset of the domain map of the third floor of the building, and Figure 1(b) shows the Peoplebot wheeled robot platform used in these trials. The robot is equipped with a stereo camera, laser range finder, microphone, speaker, and a laptop running Ubuntu Linux that performs all the processing. The domain maps are learned and revised by the robot using laser range finder data and the existing ROS implementation of a SLAM algorithm [13]. This robot has a manipulator arm that can be moved to reachable 3D locations relative to the robot. However, since robot manipulation is not a focus of this work, once the robot is next to the desired object, it extends its gripper and asks for the object to be placed in it. For experimental trials\non the third floor, we considered 15 rooms, which includes faculty offices, research labs, common areas and a corridor. To make it feasible to use POMDP-1 in such large domains, we used our prior work on a hierarchical decomposition of POMDPs for visual sensing and information processing that supports automatic belief propagation across the levels of the hierarchy and model generation in each level of the hierarchy [47, 52]. The experiments included paired trials, e.g., over 15 trials (each), POMDP-1 takes 1.64 as much time as PA (on average) to move specific objects to specific places. For these paired trials, this 39% reduction in execution time provided by PA is statistically significant: p-value = 0.0023 at the 95% significance level.\nConsider a trial in which the robot’s objective is to bring a specific textbook to the place named study corner. The robot uses default knowledge to create a plan of abstract actions that causes the robot to move to and search for the textbook in the main library. When the robot does not find this textbook in the main library after searching using a suitable POMDP policy, replanning by the logician causes the robot to investigate the aux library. The robot finds the desired textbook in the aux library and moves it to the target location. A video of such an experimental trial can be viewed online at http://youtu.be/8zL4R8te6wg\nTo explore the applicability of PA in different domains, we also conducted 40 experimental trials using the Turtlebot wheeled robot platform in Figure 1(c) in a variant of the illustrative domain in Example 1. This domain had three rooms in the Electrical Engineering department building arranged to mimic a robot operating as a robot butler, with additional objects (e.g., tables, chairs, food items etc). The robot was equipped with a Kinect (RGB-D) sensor, a laser range finder, and a laptop running Ubuntu Linux that performs all the processing. As before, the robot used the ROS implementation of a SLAM algorithm, and a hierarchical decomposition of POMDPs for POMDP-1. This robot did not have a manipulator arm—once it reached a location next to the location of the desired object, it asks for the object to be placed on it. The experiments included paired trials, e.g., in 15 paired trials, POMDP-1 takes 2.3 as much time as PA (on average) to move specific objects to specific places—this reduction in execution time by PA is statistically significant at the 95% significance level.\nConsider a trial in which the robot’s goal was to fetch a bag of crisps for a human. The robot uses default knowledge about the location of the bag of crisps, to create a plan of abstract actions that causes the robot to first move to the kitchen and search for the bag of crisps. The robot finds the bag of crisps, asks for the bag to be placed on it (since it has no manipulator), and moves back to table1 in lab1 (the location of the human who wanted the crisps) only to be told that it has brought a bag of chocolates instead. The robot diagnoses the cause for this error (human gave it the incorrect bag in the kitchen), goes back and fetches the correct bag (of crisps) this time. A video of this trial can be viewed online at https://vimeo.com/136990534"
    }, {
      "heading" : "11 Conclusions",
      "text" : "This paper described a knowledge representation and reasoning architecture that combines the complementary strengths of declarative programming and probabilistic graphical models. The architecture is based on tightly-coupled transition diagrams that represent domain knowledge, and the robot’s abilities and goals, at two levels of granularity. The architecture makes several key contributions.\n• Action language ALd is extended to support non-Boolean fluents and non-deterministic causal laws, and is used to describe the coarse-resolution and fine-resolution transition diagrams.\n• The notion of history of a dynamic domain is extended to include default knowledge in the initial state, and a model of this history is defined. These definitions are used to define a notion of explanation of unexpected observations, and to provide an algorithm for coarse-resolution planning and diagnostics. The algorithm is based on the translation of a history into a program of CR-Prolog and computed answer sets of this program. The desired plan and, if necessary, explanations are extracted from this answer set.\n• A formal definition is provided of one transition diagram being a refinement of another transition diagram, and the fine-resolution diagram is defined as a refinement of the coarse-resolution transition diagram of the domain.\n• The randomization of the fine-resolution transition diagram is defined, and an algorithm is provided for experimental collection of statistics. These statistics are used to compute the probabilities of action outcomes and observations at the fine-resolution.\n• A formal definition is provided for zooming to a part of the randomized fine-resolution diagram relevant to the execution of any given coarse-resolution (abstract) action. This definition is used to automate the zoom operation during execution of the coarse-resolution plan.\n• We also provide an algorithm that uses the computed probabilities and the zoomed part of the fine-resolution transition diagram, to automatically construct data structures appropriate for the probabilistic implementation of any given abstract action. The outcomes of probabilistic reasoning update the coarse-resolution history for subsequent reasoning.\n• Finally, and possibly one of the major contributions, is that we articulate a general methodology for the design of software components of robots that are re-taskable and robust. It simplifies the use of this architecture in other domains, provides a path to predict the robot’s behavior, and thus increases confidence in the correctness of the robot’s behavior.\nIn this paper, the domain representation for non-monotonic logical reasoning at coarse-resolution is translated to a CRProlog program, while the representation for probabilistic reasoning is translated to a POMDP. These choices allow us to reason reliably and efficiently with hierarchically organized knowledge, and to provide a single framework for inference, planning and explanation generation, and for a quantifiable trade off between accuracy and computational efficiency in the presence of probabilistic models of uncertainty in sensing and actuation. Experimental results in simulation and on physical robots indicate that the architecture supports reasoning at the sensorimotor level and the cognitive level with violation of defaults, noisy observations and unreliable actions, and has the potential to scale well to complex domains.\nThe proposed architecture open up many directions for further research, some of which relax the constraints imposed in the design of our current architecture. First, we will further explore the tight coupling between the transition diagrams, and between logical and probabilistic reasoning, in dynamic domains. We have, for instance, explored different resolutions for reasoning probabilistically [12], and investigated the inference, planning and diagnostics capabilities of architectures that reason at different resolutions [53]. However, we have so far not explored non-stationary domains, a limiting constraint that we seek to relax in future work. Second, our architecture has so far focused on a single robot, although we have instantiated the architecture in different domains. Another direction of further research is to extend the architecture to enable collaboration between a team of robots working towards a shared goal. It is theoretically possible to extend our architecture to work on multiple robots, but it will open up challenging questions and choices regarding communication (between robots) and propagation of beliefs of a robot and its teammates. Third, the proposed architecture has focused on representation and reasoning with incomplete knowledge, but a robot collaborating with humans in a dynamic domain also needs to be able to learn from its experiences. Preliminary work in this direction, e.g., based on combining relational reinforcement learning with declarative programming, has provided some promising results [46, 45], and we seek to further explore this direction of work in the future. The long-term objective is to better understand the coupling between non-monotonic logical reasoning and probabilistic reasoning, and to use this understanding to develop architectures that enable robots to assist humans in complex domains."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by the U.S. Office of Naval Research Science of Autonomy Award N00014-13-1-0766 (Mohan Sridharan, Shiqi Zhang), the Asian Office of Aerospace Research and Development award FA2386-16-1-4071 (Mohan Sridharan), and the EC-funded Strands project FP7-IST-600623 (Jeremy Wyatt). Opinions and conclusions in this paper are those of the authors."
    }, {
      "heading" : "A Proof of Proposition 1",
      "text" : "In this section, we prove Proposition 1, which states that:\nA path M = 〈σ0,a0,σ1, . . . ,σn−1,an〉 of τ(D) is a model of history H n iff there is an answer set AS of a program Π(D ,H ) such that:\n1. A fluent literal ( f = y) ∈ σi iff val( f ,y, i) ∈ AS,\n2. A fluent literal ( f 6= y1) ∈ σi iff val( f ,y2, i) ∈ AS,\n3. An action e ∈ ai iff occurs(e, i) ∈ AS.\nFor our proof, we will need the following Lemma:\nLemma 1. Let σ0 be a state of the transition diagram of system description D , 〈a0, . . . ,an−1〉 be a sequence of actions from the D’s signature, and\nP =de f Π(D)∪ val(σ0,0)∪{occurs(ai, i) : 0≤ i < n}\nIf D is well-founded then every answer set of P defines a path of τ(D).\nProof of the proposition3: First, we use the Splitting Set Theorem for CR-Prolog [4] to simplify Π(D ,H ) by eliminating from it occurrences of atoms formed by relations obs and hpd. Let us denote the set of such atoms by U . To apply the theorem it is sufficient to notice that no cr-rule of the program contains atoms from U and that rules whose heads are in U have empty bodies. Hence, U is a splitting set of Π(D ,H ). By Splitting Set Theorem, set AS is an answer set of Π(D ,H ) iff AS = Obs∪AS′ where Obs is the collection of atoms formed by obs and hpd from history H , and AS′ is an answer set of the program P1 obtained from Π(D ,H ) by:\n• Replacing every ground instance: val( f (x̄),y,0)← obs( f (x̄) = y,0), such that obs( f (x̄) = y,0) ∈ Obs, of the rule in Statement 13, by: val( f (x̄),y,0) and removing all the remaining ground instances of this rule.\n• Replacing every ground instance:\n←val( f ,y1, i), obs( f = y2, i),\ny1 6= y2.\nof the rule in Statement 15 such that obs( f = y2, i) ∈ Obs and y1 6= y2 by: ← val( f ,y1, i) and removing all the remaining ground instances of this rule.\n• Replacing every ground instance: occurs(a, i)← hpd(a, i), such that hpd(a, i)∈Obs, of the rule in Statement 16, by: occurs(a, i) and removing all the remaining ground instances of this rule.\nSet AS defines a path M of τ(D) iff AS′ defines a path M of τ(D). Our goal now is to find a state σ0 of τ(D) such that AS′ is an answer set of a program:\nP =de f Π(D)∪ val(σ0,0)∪{occurs(ai, i) : hpd(ai, i) ∈H and 0≤ i < n}.\nOnce this is established, the conclusion of the proposition will follow immediately from Lemma 1.\nLet σ0 = { f (x̄) = y : val( f (x̄),y,0) ∈ AS′}, and consider a series of transformations of Π(D ,H ) such that AS′ is an answer set of each program in this series and that the last program of the series is P .\n3This proof uses the observations that are listed later in this section. They are easy to prove, and either belong to the folklore of the field or appear as lemmas in early ASP papers.\nStep 1: By the definition of answer set of CR-Prolog, AS′ is an answer set of an ASP program P2 obtained from P1 by replacing its CR-rules with a (possibly empty) collection of their ASP counterparts, i.e., rules of the form:\nval( f (x̄),yk,0)←val(body,0), (28) range( f ,c),\nmember(yk,c),\nyk 6= y.\nStep 2: From Observation 1, we have that AS′ is an answer set of P2 iff AS′ is an answer set of program P3 obtained from P2 by removing rules of the form← val( f ,y1, i).\nStep 3: From Observation 2 and definition of σ0, we have that AS′ is an answer set of P3 iff AS′ is an answer set of program P4 = P3∪ val(σ0,0).\nStep 4: The fact that AS′ satisfies rules of P4 and the definition of σ0 imply that every ground instance of rules in Statements 11 and 28, either has a literal in the body which does not belong to AS′ or a literal in the head which is an atomic statement in P4. Hence, by Observations 3 and 4, AS′ is an answer set of a program P5 obtained from P4 by removing these instances. This completes our transformation.\nNote that program P5 is of the form:\nΠ(D)∪ val(σ0,0)∪{occurs(ai, i) : 0≤ i < n}.\nTo show that P5 is equal to program P from Lemma 1, it remains to be shown that σ0 is a state of τ(D), i.e., that σ0 is an interpretation of the signature of D that is a unique answer set of program Πc(D)∪σnd0 . To prove that σ0 is an interpretation we need to show that for every f (x̄) there is y such that f (x̄) = y ∈ σ0. Consider three cases:\n1. There is y such that f (x̄) = y is the head of a default. In this case, there is a rule r of the form (in Statement 11) in P2 obtained from this default. There are again two possibilities:\n• The body of r is satisfied by AS′. In this case val( f (x̄),y,0) ∈ AS′ and hence f (x̄) = y ∈ σ0 • There is some literal in the body of r which is not satisfied by AS′. In this case, Statement 14 guarantees\nthat f (x̄) = yi for some yi is in AS′ and, hence, in σ0.\n2. f (x̄) is a basic fluent and there is no atom formed by f (x̄) which belong to the head of some default (Statement 7). In this case, the existence of yi such that val( f (x̄),yi,0) ∈ AS′, and hence in σ0, is guaranteed by Statement 14.\n3. f (x̄) is a defined fluent. In this case, its value is assigned by one of the existing rules (e.g., the CWA).\nTo show that σ0 is the unique answer set of R1 = Πc(D)∪σnd0 we first observe that σ0 is an answer set of R1 iff it is an answer set of the encoding R2 of R1 which consists of ground instances of rules corresponding to state constraints, definitions and CWA, and facts of the form val( f (x̄),y,0) where f (x̄) is a basic fluent and f (x̄) = y ∈ σ0. To finish proving that σ0 is a state, we need to show that val(σ0,0) is an answer set of R2. To do that, it is sufficient to notice that val(σ0,0) is a splitting set of program P5 and hence, By Splitting Set Theorem, AS′ = ASb ∪ASt where ASb is the answer set of R2 and ASt does not contain atoms with a time step 0. Clearly, ASb = σ0. Finally, we need to recall that D is well-founded and, hence, by the definition of well-foundedness, R2 cannot have multiple answer sets.\nObservation 1: If Π is an ASP program and C is a collection of rules with the empty heads then AS′ is an answer set of Π∪C iff AS′ is an answer set of Π which satisfies rules of C.\nObservation 2: If AS′ is an answer set of an ASP program Π and S0 ⊆ AS′, AS′ is an answer set of the program obtained from Π by adding to it the encoding of literals from S0.\nObservation 3: If AS′ is an answer set of an ASP program Π then AS′ is an answer set of a program obtained from Π by removing rules whose bodies are not satisfied by AS′.\nObservation 4: If AS′ is an answer set of an ASP program Π then AS′ is an answer set of a program obtained from Π by removing rules whose heads have occurrences of atoms which are facts of Π.\nTo formulate the next observation we need some notation and terminology. If α is a mapping from atoms to atoms then for any rule r by α(r) we mean the rule obtained from r by applying α to all occurrences of atoms in r. Similarly for a collection of literals.\nASP programs P1 and P2 are called isomorphic if there is a one to one correspondence α between literals of P1 and literals of P2 such that a rule r ∈ P1 iff the rule α(r) ∈ P2 (i.e. r ∈ P1 implies that α(r) ∈ P2 and r ∈ P2 implies that α−1(r) ∈ P1).\nObservation 5: If P1 and P2 are isomorphic with an isomorphism α , then AS′ is an answer set of P1 iff α(AS′) is an answer set of P2.\nProof of Lemma 1 (sketch): Let D , σ0 and P be as in Lemma 1. To simplify the argument we assume that P contain no statics which can be eliminated using the splitting set theorem without any effect on statements formed by relations val and occurs.\nBy Pn we denote the grounding of Π(D) with time steps ranging from 0 to n combined with the set val(σ0,0)∪ {occurs(ai, i) : 1≤ i < n}. To prove the Lemma, it is sufficient to show that every answer set AS′ of Pn defines a path of τ(D). This proof is by induction on n.\n1. The base case (n= 0) is immediate since val(σ0,0) is a subset of an answer set AS′ of P0 and hence the path 〈σ0〉 is defined by AS′.\n2. Assume that the lemma holds for and show that it holds for Pn.\nThe program Pn can be represented as: Pn =P0∪R0. Let us denote the set of literals occurring in the heads of rules from P0 by U . It can be checked that U is a splitting set of Pn and therefore, by Splitting Set Theorem, every answer set AS′ of Pn can be represented as AS′ = AS0 ∪AS1 where AS0 is an answer set of P0 and AS1 is an answer set of partial evaluation of R0 with respect to U and AS0.\nLet us first show that AS0 defines a single transition path 〈σ0,a0,σ1〉 of τ(D). The definition of transition and the fact that σ0 is a state of τ(D) implies that the only thing which needs proving is that σ1 = { f (x̄) = y : val( f (x̄),y,1) ∈ S0} is a state. The proof consists of the following two steps:\n1. Show that σ1 is an interpretation, i.e. for every f (x̄) there is y such that f (x̄) = y ∈ σ1.\n(a) P0 contains a rule:\nval( f (x̄),y1,1) or . . .or val( f (x̄),yk,1)←val(body,0), (29) occurs(a,0).\nor a rule:\nval( f (x̄),y,1)← val(body,1) (30)\nwhose body is satisfied by AS0. To satisfy such rules AS0 must contain val( f (x̄),y,1) for some value y.\n(b) If case (a) above does not hold then consider two cases: f is basic and f is defined. To deal with the first case note that, since σ0 is a state, there is some y such that f (x̄) = y∈ σ0. Then, by Inertia Axiom f (x̄) will have the same value at step one, i.e. val( f (x̄),y,1) is in AS0. If f is defined then val( f (x̄), f alse,1) is in AS0 due to CWA for defined fluents. In both cases, f (x̄) = y ∈ σ1.\n2. Uniqueness of an answer set of Πc(D)∪σnd1 follows from the well-foundedness of D . Thus, σ1 is a state and 〈σ0,a0,σ1〉 is a transition defined by AS0.\nTo complete the induction, we notice that, by Splitting Set Theorem, AS′ is an answer set of Pn iff it is an answer set of AS0 ∪R0. The latter program can be represented as: (AS0 \\ val(σ1,1))∪Q, where: Q = val(σ1,1)∪R. Note that Q is isomorphic to Pn−1, where the isomorphism, α , is obtained by simply replacing a time-step i in each atom of R by i− 1. By inductive hypothesis and Observation 5 (above), we have that an answer set AS1 of Q defines a path 〈σ1,a1, . . . ,σn〉 in τ(D). Since by Splitting Set Theorem AS′ = AS0 ∪AS1, AS′ defines a path 〈σ0,a0,σ1,a1, . . . ,σn〉. This completes the proof of the proposition."
    }, {
      "heading" : "B Proof of Proposition 2",
      "text" : "In this section, we examine Proposition 2, which states that:\nLet DH and DL be the coarse-resolution and fine-resolution system descriptions for the office domain in Example 1. Then DL is a refinement of DH .\nTo establish this proposition, we need to establish that the relationship between DH (Example 4) and DL (Section 7.1) satisfies the conditions of Definition 7, as stated below.\nSystem description DL is a refinement of system description DH if:\n1. States of τL are the refinements of states of τH .\n2. For every transition 〈σ1,aH ,σ2〉 of τH , every fluent f in a set F of observable fluents, and every refinement δ1 of σ1, there is a path P in τL from δ1 to a refinement δ2 of σ2 such that:\n(a) Every action of P is executed by the robot which executes aH .\n(b) Every state of P is a refinement of σ1 or σ2, i.e., no unrelated fluents are changed. (c) observed(R, f ,Y ) = true ∈ δ2 iff ( f = Y ) ∈ δ2 and observed(R, f ,Y ) = f alse ∈ δ2 iff ( f 6= Y ) ∈ δ2.\nThe first condition in Definition 7 is that states of τL be refinements of states of τH , as specified by Definition 6. To establish this condition, let δ be a state of τL, i.e., a unique answer set of a program ΠL with statements such as:\ncomponent(c1,o f f ice) ... next to(c1,c2), next to(c2,c5) ... loc(rob1) = c1, loc(cup1) = c6\nwhich include atoms for (a) cells that are components of rooms; (b) adjacent cells that are accessible from each other; and (c) initial locations of the robot and a coffee cup. Program ΠL has axioms for loc such as:\nloc(cup1) =C if loc(rob1) =C, in hand(rob1,cup1) loc∗(cup1) = Rm if loc(cup1) =C, component(C,Rm) loc∗(rob1) = Rm if loc(rob1) =C, component(C,Rm)\ndefined (here) in the context of textbook cup1, and axioms for static relation next to such as:\nnext to(C2,C1) if next to(C1,C2) next to∗(Rm1,Rm2) if next to(C1,C2), component(C1,Rm1), component(C2,Rm2), Rm1 6= Rm2\nThe program ΠL also includes axioms related to the basic knowledge fluents corresponding to observations such as can be tested(rob1, loc(cup1),C), and the CWA for defined knowledge fluents such as may discover(rob1, loc∗(cup1),Rm).\nPlease see refined.sp at https://github.com/mhnsrdhrn/refine-arch for an example of the complete program (in SPARC), with additional axioms for planning.\nWe need to show that there is a state σ ∈ τH such that δ is a refinement of σ . We will do so by construction. Let σ be the union of two sets of atoms:\nσ = σ∗m ∪ σnm (31) σ∗m = { f = y : f is a magnified coarse-resolution domain property, ( f ∗ = y) ∈ δ} σnm = { f = y : f is a non-magnified coarse-resolution domain property, ( f = y) ∈ δ}\nwhere σ∗m is a collection of atoms of domain properties that are magnified during refinement, and σnm is a collection of atoms of domain properties that are not magnified during refinement. Continuing with our example in the context of Figure 3, σ∗m includes {loc∗(rob1) = r1, loc∗(cup1) = r2, next to∗(r1,r2)}, where r1 = o f f ice and r2 = kitchen, and σnm includes ¬in hand(rob1,cup1). Based on Definition 6, δ is a refinement of σ . We now need to establish that σ is a state, i.e., an answer set of program ΠH that consists of all the facts in σ , and the axioms of DH , the coarse-resolution system description. To do so, we need to establish that the facts in σ satisfy the axioms in ΠH . This holds true by construction of σ and the system description DL, thus establishing the first condition of Definition 7.\nNext, we establish the second condition in Definition 7. We do so by construction, and by considering two representative transitions—other transitions can be addressed in a similar manner. In our discussion below, descriptions of states omit negative literals for simplicity. We also often omit atoms formed of statics, fluents unchanged by the transition, knowledge fluents, and fluents whose values are unknown. Furthermore, unless otherwise stated, the set F of observable fluents includes all fluents.\nFirst, consider the transition corresponding to action aH = move(rob1,kitchen) that moves the robot from the o ff ice to the kitchen in Figure 3. Considering just the fluent that changes due to this transition, {loc(rob1) = o f f ice} ∈ σ1 and {loc(rob1) = kitchen} ∈ σ2. Without loss of generality, consider a refinement δ1 of σ1 that has atoms such as:\nloc∗(rob1) = r1 loc(rob1) = c2 directly observed(rob1, loc(rob1),c2)\ndirectly observed(rob1, loc(rob1),c5) = undet\nwhich implies that the robot is in cell c2 in room r1, which is the o ff ice, and the value of directly observed is undetermined for all its parameters other than the current location of the robot. Our description of delta1 omits atoms such as next to(c1,c2), next to(c5,c6), and atoms corresponding to some other fluents. Now, we construct a path P1 from τL that corresponds to two state transitions. The first transition corresponds to executing action move(rob1,c5), changing the state from δ1 to δ1,a that has atoms such as:\nδ1,a = {loc(rob1) = c5, loc∗(rob1) = r2}\nand corresponds to the robot’s movement from cell c2 in room r1 to neighboring cell c5 in room r2. The second transition of the constructed path P1 corresponds to action test(rob1, loc(rob1),c5), which causes a transition to state:\nδ2 ={loc(rob1) = c5, loc∗(rob1) = r2, directly observed(rob1, loc(rob1),c5), observed(rob1, loc(rob1),c5), observed(rob1, loc∗(rob1),r2}\nwhere the newly added atoms correspond to observing the robot’s location. We observe that δ2 is a state of τL and a refinement of state σ2 of τH (see Definition 6). To establish the second condition of Definition 7, we need to show that the chosen coarse-resolution transition, constructed path P1 from τL, set F of observable fluents, and refinement δ1 of σ1, satisfy requirements (a)-(c). To establish requirement (a), notice that every action of path P1 is executed by a robot that executes the coarse-resolution action aH . To establish requirement (b), notice that state δ1,a is a refinement of σ2—we already know that δ1 and δ2 are refinements of σ1 and σ2 respectively—i.e., every state of P1 is a refinement\nof σ1 or σ2 and unrelated fluents remain unchanged. Finally, to establish requirement (c), notice that Statements 21, 23 and 24 (in the construction of DL) ensure that for each f ∈ F , observed(rob1, f ,Y ) = true ∈ δ2 iff ( f = Y ) ∈ δ2 and observed(rob1, f ,Y ) = f alse ∈ δ2 iff ( f 6= Y ) ∈ δ2.\nAs a second representative example of a transition, consider the robot, which is in the kitchen, executing aH = grasp(rob1,cup1) to pick up the coffee cup cup1 that is known to be in the kitchen. Considering just the fluents that change as a result of this transition, the state σ3 ∈ τH has {loc(rob1) = kitchen, loc(cup1) = kitchen}. Now, consider a refinement δ3 of σ3 that has the atoms such as:\nloc∗(rob1) = r2, loc(rob1) = c5 loc∗(cup1) = r2, loc(cup1) = c6 directly observed(rob1, loc(rob1),c5)\ndirectly observed(rob1, loc(rob1),c6) = undet\nwhich implies that the robot is in cell c5 in room r2, coffee cup cup1 is in cell c6 in r2, and the value of directly observed is undetermined for all its parameters other than the current locations of the robot and the coffee cup. Now, consider path P2 from τL that corresponds to four state transitions. The first transition involves executing action move(rob1,c6), which changes the state from δ3 to δ3,a that has atoms such as:\nδ1,a ={loc(rob1) = c6, loc∗(rob1) = r2, loc(cup1) = c6, loc∗(cup1) = r2}\nand corresponds to the robot moving from cell c5 to cell c6 in room r2. The second transition corresponds to executing action test(rob1, loc(rob1),c6), which changes the state to delta3,b that has atoms such as:\nδ3,b ={loc(rob1) = c6, loc∗(rob1) = r2, loc(cup1) = c6, loc∗(cup1) = r2, directly observed(rob1, loc(rob1),c6), observed(rob1, loc(rob1),c6), observed(rob1, loc∗(rob1),r2}\nwhere we notice that the newly added atoms correspond to observing the robot’s location. The third transition along path P2 corresponds to action grasp(rob1,cup1), which changes the state to δ3,c that has atoms such as:\nδ3,c ={loc(rob1) = c6, loc∗(rob1) = r2, loc(cup1) = c6, loc∗(cup1) = r2, in hand(rob1,cup1), directly observed(rob1, loc(rob1),c6), observed(rob1, loc(rob1),c6), observed(rob1, loc∗(rob1),r2}\nwhere we notice that cup1 is now in the robot’s grasp. Finally, action test(rob1, in hand(rob1,cup1), true) causes a transition to state δ4 that has atoms such as:\nδ4 ={loc(rob1) = c6, loc∗(rob1) = r2, loc(cup1) = c6, loc∗(cup1) = r2, in hand(rob1,cup1), directly observed(rob1, loc(rob1),c6), observed(rob1, loc(rob1),c6), observed(rob1, loc∗(rob1),r2,\ndirectly observed(rob1, in hand(rob1,cup1), true), observed(rob1, in hand(rob1,cup1), true)}\nWe observe that δ4 is a state of τL and a refinement of state σ4 of τH . To establish the second condition of Definition 7, we need to show that the chosen coarse-resolution transition, constructed path P2 from τL, set F of observable fluents, and refinement δ3 of σ3, satisfy requirements (a)-(c). To establish requirement (a), notice that every action of P2 has to be executed to implement aH . Next, to establish requirement (b), we notice that δ3,a and δ3,b are refinements of σ3, while δ3,c and δ4are refinements of σ4, i..e, every state of P2 is a refinement of σ3 or σ4 and unrelated fluents remain unchanged. Finally, to establish requirement (c), notice that Statements 21, 23 and 24 (in the construction of DL) ensure that for each f ∈ F , observed(rob1, f ,Y ) = true ∈ δ2 iff ( f = Y ) ∈ δ2 and observed(rob1, f ,Y ) = f alse ∈ δ2 iff ( f 6= Y ) ∈ δ2.\nThe steps described above can be repeated for other coarse-resolution transitions. We have thus shown that DL is indeed a refinement of DH . These steps can also be verified by solving refined.sp after suitably revising the initial state and goal state."
    }, {
      "heading" : "C POMDP Construction Example",
      "text" : "In this section, we provide an illustrative example of constructing a POMDP for a specific abstract action that needs to be implemented as a sequence of concrete actions whose effects are modeled probabilistically.\nExample 7. [Example of POMDP construction] Consider abstract action aH = grasp(rob1, tb1), with the robot and textbook in the o ff ice, in the context of Example 4. The corresponding zoomed system description DLR(T ) is in Example 6. For ease of explanation, assume the following transition probabilities, observation probabilities, and rewards—these values would typically be computed by the robot in the initial training phase (Section 7.2):\n• Any move from a cell to a neighboring cell succeeds with probability 0.85. Since there are only two cells in this room, the robot remains in the same cell if move does not succeed.\n• The grasp action succeeds with probability 0.95; otherwise it fails.\n• If the thing being searched for in a cell exists in the cell, 0.95 is the probability of successfully finding it.\n• All non-terminal actions have unit cost. A correct answer receives a large positive reward (100), whereas an incorrect answer receives a large negative reward (−100).\nThe elements of the corresponding POMDP are described (below) in the format of the approximate POMDP solver used in our experiments [37]. As described in Section 8.2, please note that:\n• Executing a terminal action causes a transition to a terminal state.\n• Actions that change the p-state do not provide any observations.\n• Knowledge-producing actions do not change the p-state.\ndiscount: 0.99\nvalues: reward\n% States, actions and observations as enumerated lists\nstates: robot-0-object-0-inhand robot-1-object-1-inhand robot-0-object-0-not-inhand\nrobot-0-object-1-not-inhand robot-1-object-0-not-inhand\nrobot-1-object-1-not-inhand absb\nactions: move-0 move-1 grasp test-robot-0 test-robot-1 test-object-0 test-object-1\ntest-inhand finish\nobservations: robot-found robot-not-found object-found object-not-found\ninhand not-inhand none\n% Transition function format.\n% T : action : S x S’ -> [0, 1]\n% Probability of transition from first element of S to that of S’ is\n% in the top left corner of each matrix"
    }, {
      "heading" : "T: move-0",
      "text" : "1 0 0 0 0 0 0\n0.85 0.15 0 0 0 0 0\n0 0 1 0 0 0 0\n0 0 0 1 0 0 0\n0 0 0.85 0 0.15 0 0\n0 0 0 0.85 0 0.15 0\n0 0 0 0 0 0 1"
    }, {
      "heading" : "T: move-1",
      "text" : "0.15 0.85 0 0 0 0 0\n0 1 0 0 0 0 0\n0 0 0.15 0 0.85 0 0\n0 0 0 0.15 0 0.85 0\n0 0 0 0 1 0 0\n0 0 0 0 0 1 0\n0 0 0 0 0 0 1"
    }, {
      "heading" : "T: grasp",
      "text" : "1 0 0 0 0 0 0\n0 1 0 0 0 0 0\n0.95 0 0.05 0 0 0 0\n0 0 0 1 0 0 0\n0 0 0 0 1 0 0\n0 0.95 0 0 0 0.05 0\n0 0 0 0 0 0 1"
    }, {
      "heading" : "T: test-robot-0 identity",
      "text" : ""
    }, {
      "heading" : "T: test-robot-1 identity",
      "text" : ""
    }, {
      "heading" : "T: test-object-0 identity",
      "text" : ""
    }, {
      "heading" : "T: test-object-1 identity",
      "text" : ""
    }, {
      "heading" : "T: test-inhand identity",
      "text" : ""
    }, {
      "heading" : "T: finish uniform",
      "text" : "% Observation function format(s)\n% O : action : s_i : z_i -> [0, 1] (or)\n% : S x Z -> [0, 1]\n% In each matrix, first row provides probability of each possible\n% observation in the first p-state in S"
    }, {
      "heading" : "O: move-0 : * : none 1",
      "text" : ""
    }, {
      "heading" : "O: move-1 : * : none 1",
      "text" : "O: grasp : * : none 1"
    }, {
      "heading" : "O: test-robot-0",
      "text" : "0.95 0.05 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0 0 0 0 0 0 1"
    }, {
      "heading" : "O: test-robot-1",
      "text" : "0.05 0.95 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0.05 0.95 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0.95 0.05 0 0 0 0 0\n0 0 0 0 0 0 1"
    }, {
      "heading" : "O: test-object-0",
      "text" : "0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0 0 0 0 1"
    }, {
      "heading" : "O: test-object-1",
      "text" : "0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0.05 0.95 0 0 0\n0 0 0.95 0.05 0 0 0\n0 0 0 0 0 0 1"
    }, {
      "heading" : "O: test-inhand",
      "text" : "0 0 0 0 0.95 0.05 0\n0 0 0 0 0.95 0.05 0\n0 0 0 0 0.05 0.95 0\n0 0 0 0 0.05 0.95 0\n0 0 0 0 0.05 0.95 0\n0 0 0 0 0.05 0.95 0\n0 0 0 0 0 0 1"
    }, {
      "heading" : "O: finish : * : none 1",
      "text" : "% Reward function format\n% R : action : s_i : s_i’ : real value\nR: finish : robot-0-object-0-inhand : * : -100"
    }, {
      "heading" : "R: finish : robot-1-object-1-inhand : * : 100",
      "text" : ""
    }, {
      "heading" : "R: finish : robot-0-object-0-not-inhand : * : -100",
      "text" : ""
    }, {
      "heading" : "R: finish : robot-0-object-1-not-inhand : * : -100",
      "text" : ""
    }, {
      "heading" : "R: finish : robot-1-object-0-not-inhand : * : -100",
      "text" : ""
    }, {
      "heading" : "R: finish : robot-1-object-1-not-inhand : * : -100",
      "text" : ""
    }, {
      "heading" : "R: move-0 : * : * : -1",
      "text" : ""
    }, {
      "heading" : "R: move-1 : * : * : -1",
      "text" : ""
    }, {
      "heading" : "R: grasp : * : * : -1",
      "text" : ""
    }, {
      "heading" : "R: test-robot-0 : * : * : -1",
      "text" : ""
    }, {
      "heading" : "R: test-robot-1 : * : * : -1",
      "text" : ""
    }, {
      "heading" : "R: test-object-0: * : * : -1",
      "text" : ""
    }, {
      "heading" : "R: test-object-1: * : * : -1",
      "text" : "R: test-inhand : * : * : -1"
    } ],
    "references" : [ {
      "title" : "Integrated Perception and Planning in the Continuous Space: A POMDP Approach",
      "author" : [ "Haoyu Bai", "David Hsu", "Wee Sun Lee" ],
      "venue" : "International Journal of Robotics Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Towards Answer Set Programming with Sorts",
      "author" : [ "Evgenii Balai", "Michael Gelfond", "Yuanlin Zhang" ],
      "venue" : "In International Conference on Logic Programming and Nonmonotonic Reasoning, Corunna,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Diagnostic Reasoning with A-Prolog",
      "author" : [ "Marcello Balduccini", "Michael Gelfond" ],
      "venue" : "Theory and Practice of Logic Programming,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Logic Programs with Consistency-Restoring Rules",
      "author" : [ "Marcello Balduccini", "Michael Gelfond" ],
      "venue" : "In AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Language ASPf with Arithmetic Expressions and Consistency-Restoring Rules",
      "author" : [ "Marcello Balduccini", "Michael Gelfond" ],
      "venue" : "In Workshop on Answer Set Programming and Other Computing Paradigms (ASPOCP) at ICLP,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "An ASP-Based Architecture for Autonomous UAVs in Dynamic Environments: Progress Report",
      "author" : [ "Marcello Balduccini", "William C. Regli", "Duc N. Nguyen" ],
      "venue" : "In International Workshop on Non-Monotonic Reasoning (NMR), Vienna,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Towards an ASP-Based Architecture for Autonomous UAVs in Dynamic Environments (Extended Abstract)",
      "author" : [ "Marcello Balduccini", "William C. Regli", "Duc N. Nguyen" ],
      "venue" : "In International Conference on Logic Programming",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Knowledge Representation, Reasoning and Declarative Problem Solving",
      "author" : [ "Chitta Baral" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Probabilistic Reasoning with Answer Sets",
      "author" : [ "Chitta Baral", "Michael Gelfond", "Nelson Rushton" ],
      "venue" : "Theory and Practice of Logic Programming,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Continual Planning and Acting in Dynamic Multiagent Environments",
      "author" : [ "Michael Brenner", "Bernhard Nebel" ],
      "venue" : "Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Toward Open Knowledge Enabling for Human-Robot Interaction",
      "author" : [ "Xiaoping Chen", "Jiongkun Xie", "Jianmin Ji", "Zhiqiang Sui" ],
      "venue" : "Human-Robot Interaction,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "What Happened and Why? A Mixed Architecture for Planning and Explanation Generation in Robotics",
      "author" : [ "Zenon Colaco", "Mohan Sridharan" ],
      "venue" : "In Australasian Conference on Robotics and Automation (ACRA),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "A Solution to the Simultaneous Localization and Map Building (SLAM) Problem",
      "author" : [ "G. Dissanayake", "P. Newman", "S. Clark" ],
      "venue" : "IEEE Transactions on Robotics and Automation,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2001
    }, {
      "title" : "Answer Set Programming for Collaborative Housekeeping Robotics: Representation, Reasoning, and Execution",
      "author" : [ "Esra Erdem", "Erdi Aker", "Volkan Patoglu" ],
      "venue" : "Intelligent Service Robotics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Applications of Action Languages to Cognitive Robotics",
      "author" : [ "Esra Erdem", "Volkan Patoglu" ],
      "venue" : "In Correct Reasoning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Yet Another Modular Action Language",
      "author" : [ "Michael Gelfond", "Daniela Inclezan" ],
      "venue" : "In International Workshop on Software Engineering for Answer Set Programming,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Some Properties of System Descriptions of ALd",
      "author" : [ "Michael Gelfond", "Daniela Inclezan" ],
      "venue" : "Journal of Applied Non-Classical Logics, Special Issue on Equilibrium Logic and Answer Set Programming,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Knowledge Representation, Reasoning and the Design of Intelligent Agents",
      "author" : [ "Michael Gelfond", "Yulia Kahl" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Automated Planning: Theory and Practice",
      "author" : [ "Malik Ghallab", "Dana Nau", "Paolo Traverso" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "A Switching Planner for Combined Task and Observation Planning",
      "author" : [ "Moritz Göbelbecker", "Charles Gretton", "Richard Dearden" ],
      "venue" : "In National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Model Checking with Probabilistic Tabled Logic Programming",
      "author" : [ "Andrey Gorlin", "C.R. Ramakrishnan", "Scott A. Smolka" ],
      "venue" : "Theory and Practice of Logic Programming,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Reasoning about Uncertainty",
      "author" : [ "Joseph Halpern" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2003
    }, {
      "title" : "Robot Task Planning and Explanation in Open and Uncertain Worlds",
      "author" : [ "Marc Hanheide", "Moritz Gobelbecker", "Graham Horn", "Andrzej Pronobis", "Kristoffer Sjoo", "Patric Jensfelt", "Charles Gretton", "Richard Dearden", "Miroslav Janicek", "Hendrik Zender", "Geert-Jan Kruijff", "Nick Hawes", "Jeremy Wyatt" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Exploiting Probabilistic Knowledge under Uncertain Sensing for Efficient Robot Behaviour",
      "author" : [ "Marc Hanheide", "Charles Gretton", "Richard Dearden", "Nick Hawes", "Jeremy Wyatt", "Andrzej Pronobis", "Alper Aydemir", "Moritz Gobelbecker", "Hendrik Zender" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Automated Handwashing Assistance for Persons with Dementia using Video and a Partially Observable Markov Decision Process",
      "author" : [ "Jesse Hoey", "Pascal Poupart", "Axel Bertoldi", "Tammy Craig", "Craig Boutilier", "Alex Mihailidis" ],
      "venue" : "Computer Vision and Image Understanding,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Integrated Common Sense Learning and Planning in POMDPs",
      "author" : [ "Brendan Juba" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Planning and Acting in Partially Observable Stochastic Domains",
      "author" : [ "Leslie Kaelbling", "Michael Littman", "Anthony Cassandra" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1998
    }, {
      "title" : "Integrated Task and Motion Planning in Belief Space",
      "author" : [ "Leslie Kaelbling", "Tomas Lozano-Perez" ],
      "venue" : "International Journal of Robotics Research,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Planning in Action Language BC while Learning Action Costs for Mobile Robots",
      "author" : [ "Piyush Khandelwal", "Fangkai Yang", "Matteo Leonetti", "Vladimir Lifschitz", "Peter Stone" ],
      "venue" : "In International Conference on Automated Planning and Scheduling (ICAPS),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "Extending the Soar Cognitive Architecture",
      "author" : [ "John E. Laird" ],
      "venue" : "In International Conference on Artificial General Intelligence, Memphis,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "An Unified Cognitive Architecture for Physical Agents",
      "author" : [ "Patrick Langley", "Dongkyu Choi" ],
      "venue" : "In The Twenty-first National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2006
    }, {
      "title" : "Action Language BC: Preliminary Report",
      "author" : [ "Joohyun Lee", "Vladimir Lifschitz", "Fangkai Yang" ],
      "venue" : "In International Joint Conference on Artificial Intelligence (IJCAI), Beijing,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "A Probabilistic Extension of the Stable Model Semantics",
      "author" : [ "Joohyung Lee", "Yi Wang" ],
      "venue" : "In AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Move and the Robot will Learn: Vision-based Autonomous Learning of Object Models",
      "author" : [ "Xiang Li", "Mohan Sridharan" ],
      "venue" : "In International Conference on Advanced Robotics,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2013
    }, {
      "title" : "Algorithms for Sequential Decision Making",
      "author" : [ "Michael Littman" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1996
    }, {
      "title" : "BLOG: Probabilistic Models with Unknown Objects. In Statistical Relational Learning",
      "author" : [ "Brian Milch", "Bhaskara Marthi", "Stuart Russell", "David Sontag", "Daniel L. Ong", "Andrey Kolobov" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2006
    }, {
      "title" : "Planning under Uncertainty for Robotic Tasks with Mixed Observability",
      "author" : [ "Sylvie C. Ong", "Shao Wei Png", "David Hsu", "Wee Sun Lee" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2010
    }, {
      "title" : "Abducing through Negation as Failure: Stable Models within the Independent Choice Logic",
      "author" : [ "David Poole" ],
      "venue" : "Journal of Logic Programming,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2000
    }, {
      "title" : "Learning Accuracy and Availability of Humans who Help Mobile Robots",
      "author" : [ "Stephanie Rosenthal", "Manuela Veloso", "Anind Dey" ],
      "venue" : "In National Conference on Artificial Intelligence, San Francisco,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2011
    }, {
      "title" : "Symbolic Dynamic Programming for First-order POMDPs",
      "author" : [ "Scott Sanner", "Kristian Kersting" ],
      "venue" : "In National Conference on Artificial Intelligence (AAAI), July",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2010
    }, {
      "title" : "Cognitive Factories with Multiple Teams of Heterogeneous Robots: Hybrid Reasoning for Optimal Feasible Global Plans",
      "author" : [ "Zeynep G. Saribatur", "Esra Erdem", "Volkan Patoglu" ],
      "venue" : "In International Conference on Intelligent Robots and Systems (IROS), Chicago,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2014
    }, {
      "title" : "The Optimal Control of Partially Observable Markov Processes",
      "author" : [ "Edward J. Sondik" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1971
    }, {
      "title" : "Using Knowledge Representation and Reasoning Tools in the Design of Robots. In IJCAI Workshop on Knowledge-based Techniques for Problem Solving and Reasoning (KnowProS)",
      "author" : [ "Mohan Sridharan", "Michael Gelfond" ],
      "venue" : null,
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2016
    }, {
      "title" : "Should I do that? Using Relational Reinforcement Learning and Declarative Programming to Discover Domain Axioms",
      "author" : [ "Mohan Sridharan", "Ben Meadows" ],
      "venue" : "In International Conference on Developmental Learning and Epigenetic Robotics (ICDL-EpiRob),",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2016
    }, {
      "title" : "What can I not do? Towards An Architecture for Reasoning about and Learning Affordances",
      "author" : [ "Mohan Sridharan", "Ben Meadows" ],
      "venue" : "In International Conference on Automated Planning and Scheduling (ICAPS), Pittsburgh,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2017
    }, {
      "title" : "Planning to See: A Hierarchical Aprroach to Planning Visual Actions on a Robot using POMDPs",
      "author" : [ "Mohan Sridharan", "Jeremy Wyatt", "Richard Dearden" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2010
    }, {
      "title" : "Planning for Human- Robot Teaming in Open Worlds",
      "author" : [ "Kartik Talamadupula", "J. Benton", "Subbarao Kambhampati", "Paul Schermerhorn", "Matthias Scheutz" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2010
    }, {
      "title" : "Probabilistic Robotics",
      "author" : [ "Sebastian Thrun", "Wolfram Burgard", "Dieter Fox" ],
      "venue" : null,
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2005
    }, {
      "title" : "Dynamically Constructed (PO)MDPs for Adaptive Robot Planning",
      "author" : [ "Shiqi Zhang", "Piyush Khandelwal", "Peter Stone" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI), San Francisco,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2017
    }, {
      "title" : "Towards An Architecture for Knowledge Representation and Reasoning in Robotics",
      "author" : [ "Shiqi Zhang", "Mohan Sridharan", "Michael Gelfond", "Jeremy Wyatt" ],
      "venue" : "In International Conference on Social Robotics (ICSR), Sydney, Australia,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2014
    }, {
      "title" : "Active Visual Planning for Mobile Robot Teams using Hierarchical POMDPs",
      "author" : [ "Shiqi Zhang", "Mohan Sridharan", "Christian Washington" ],
      "venue" : "IEEE Transactions on Robotics,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2013
    }, {
      "title" : "Mixed Logical Inference and Probabilistic Planning for Robots in Unreliable Worlds",
      "author" : [ "Shiqi Zhang", "Mohan Sridharan", "Jeremy Wyatt" ],
      "venue" : "IEEE Transactions on Robotics,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2015
    }, {
      "title" : "CORPP: Commonsense Reasoning and Probabilistic Planning, as Applied to Dialog with a Mobile Robot",
      "author" : [ "Shiqi Zhang", "Peter Stone" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : ", STRIPS, PDDL [19], BC [32], and ALd [17].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 31,
      "context" : ", STRIPS, PDDL [19], BC [32], and ALd [17].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : ", STRIPS, PDDL [19], BC [32], and ALd [17].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "The domain representation for logical reasoning is translated into a program in SPARC [2], an extension of CR-Prolog, and the representation for probabilistic reasoning is translated into a partially observable Markov decision process (POMDP) [27].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 26,
      "context" : "The domain representation for logical reasoning is translated into a program in SPARC [2], an extension of CR-Prolog, and the representation for probabilistic reasoning is translated into a partially observable Markov decision process (POMDP) [27].",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 3,
      "context" : "CR-Prolog [4] (and thus SPARC) incorporates consistency-restoring rules in Answer Set Prolog (ASP)—in this paper, the terms ASP, CR-Prolog and SPARC are often used interchangeably—and has a close relationship with our action language, allowing us to reason efficiently with hierarchically organized knowledge and default knowledge, and to pose state estimation, planning, and explanation generation within a single framework.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 26,
      "context" : "Also, using an efficient approximate solver to reason with POMDPs supports a principled and quantifiable trade-off between accuracy and computational efficiency in the presence of uncertainty, and provides a near-optimal solution under certain conditions [27, 37].",
      "startOffset" : 255,
      "endOffset" : 263
    }, {
      "referenceID" : 36,
      "context" : "Also, using an efficient approximate solver to reason with POMDPs supports a principled and quantifiable trade-off between accuracy and computational efficiency in the presence of uncertainty, and provides a near-optimal solution under certain conditions [27, 37].",
      "startOffset" : 255,
      "endOffset" : 263
    }, {
      "referenceID" : 0,
      "context" : "There are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40].",
      "startOffset" : 182,
      "endOffset" : 197
    }, {
      "referenceID" : 19,
      "context" : "There are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40].",
      "startOffset" : 182,
      "endOffset" : 197
    }, {
      "referenceID" : 24,
      "context" : "There are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40].",
      "startOffset" : 182,
      "endOffset" : 197
    }, {
      "referenceID" : 38,
      "context" : "There are many recent examples of researchers using probabilistic graphical models such as POMDPs to formulate tasks such as planning, sensing, navigation, and interaction on robots [1, 20, 25, 40].",
      "startOffset" : 182,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "This includes, for instance, theories of reasoning about action and change, as well as Answer Set Prolog (ASP), a non-monotonic logic programming paradigm, which is well-suited for representing and reasoning with commonsense knowledge [8, 18].",
      "startOffset" : 235,
      "endOffset" : 242
    }, {
      "referenceID" : 17,
      "context" : "This includes, for instance, theories of reasoning about action and change, as well as Answer Set Prolog (ASP), a non-monotonic logic programming paradigm, which is well-suited for representing and reasoning with commonsense knowledge [8, 18].",
      "startOffset" : 235,
      "endOffset" : 242
    }, {
      "referenceID" : 14,
      "context" : "An international research community has developed around ASP, with applications in cognitive robotics [15] and other non-robotics domains.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "For instance, ASP has been used for planning and diagnostics by one or more simulated robot housekeepers [14], and for representation of domain knowledge learned through natural language processing by robots interacting with humans [11].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "For instance, ASP has been used for planning and diagnostics by one or more simulated robot housekeepers [14], and for representation of domain knowledge learned through natural language processing by robots interacting with humans [11].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 5,
      "context" : "ASP-based architectures have also been used for the control of unmanned aerial vehicles in dynamic indoor environments [6, 7].",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 6,
      "context" : "ASP-based architectures have also been used for the control of unmanned aerial vehicles in dynamic indoor environments [6, 7].",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 31,
      "context" : "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : "There are many different action languages such as STRIPS, PDDL [19], BC [32], and ALd [17], which have been used for different applications [10, 29].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 29,
      "context" : "For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28].",
      "startOffset" : 188,
      "endOffset" : 200
    }, {
      "referenceID" : 30,
      "context" : "For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28].",
      "startOffset" : 188,
      "endOffset" : 200
    }, {
      "referenceID" : 46,
      "context" : "For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28].",
      "startOffset" : 188,
      "endOffset" : 200
    }, {
      "referenceID" : 27,
      "context" : "For instance, architectures have been developed to support hierarchical representation of knowledge and axioms in first-order logic, and probabilistic processing of perceptual information [30, 31, 48], while deterministic and probabilistic algorithms have been combined for task and motion planning on robots [28].",
      "startOffset" : 309,
      "endOffset" : 313
    }, {
      "referenceID" : 23,
      "context" : "Another example is the behavior control of a robot that included semantic maps and commonsense knowledge in a probabilistic relational representation, and then used a continual planner to switch between decision-theoretic and classical planning procedures based on degrees of belief [24].",
      "startOffset" : 283,
      "endOffset" : 287
    }, {
      "referenceID" : 22,
      "context" : "More recent work has used a three-layered organization of knowledge (instance, default and diagnostic), with knowledge at the higher level modifying that at the lower levels, and a three-layered architecture (competence layer, belief layer and deliberative layer) for distributed control of information flow, combining first-order logic and probabilistic reasoning for open world planning [23].",
      "startOffset" : 389,
      "endOffset" : 393
    }, {
      "referenceID" : 40,
      "context" : "Declarative programming has also been combined with continuous-time planners for path planning in mobile robot teams [42].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 52,
      "context" : "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 48,
      "context" : "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 28,
      "context" : "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].",
      "startOffset" : 320,
      "endOffset" : 324
    }, {
      "referenceID" : 43,
      "context" : "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].",
      "startOffset" : 452,
      "endOffset" : 456
    }, {
      "referenceID" : 44,
      "context" : "More recent work has combined a probabilistic extension of ASP with POMDPs for commonsense inference and probabilistic planning in human-robot dialog [54], used a probabilistic extension of ASP to determine some model parameters of POMDPs [50], used ASP-based architecture to support learning of action costs on a robot [29], and combined logic programming and relational reinforcement learning to interactively and cumulatively discover domain axioms [45] and affordances [46].",
      "startOffset" : 473,
      "endOffset" : 477
    }, {
      "referenceID" : 35,
      "context" : "Bayesian Logic relaxes the unique name constraint of first-order probabilistic languages to provide a compact representation of distributions over varying sets of objects [36].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 37,
      "context" : "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 21,
      "context" : "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 39,
      "context" : "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].",
      "startOffset" : 252,
      "endOffset" : 259
    }, {
      "referenceID" : 32,
      "context" : "Other examples include independent choice logic [38], PRISM [21], probabilistic first-order logic [22], first-order relational POMDPs [26, 41], and Plog that assigns probabilities to different possible worlds represented as answer sets of ASP programs [9, 33].",
      "startOffset" : 252,
      "endOffset" : 259
    }, {
      "referenceID" : 45,
      "context" : "For instance, we developed an architecture that coupled planning based on a hierarchy of POMDPs [47, 52] with ASPbased inference.",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 50,
      "context" : "For instance, we developed an architecture that coupled planning based on a hierarchy of POMDPs [47, 52] with ASPbased inference.",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 51,
      "context" : "ASP-based inference provided priors for POMDP state estimation, and observations and historical data from comparable domains were considered for reasoning about the presence of target objects in the domain [53].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 42,
      "context" : "Building on recent work [44, 51], this paper describes a general refinement-based architecture for knowledge representation and reasoning in robotics.",
      "startOffset" : 24,
      "endOffset" : 32
    }, {
      "referenceID" : 49,
      "context" : "Building on recent work [44, 51], this paper describes a general refinement-based architecture for knowledge representation and reasoning in robotics.",
      "startOffset" : 24,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "In this paper, we extend action language ALd [16, 17, 18] (we preserve the old name for simplicity) to allow non-boolean fluents and non-deterministic causal laws.",
      "startOffset" : 45,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "In this paper, we extend action language ALd [16, 17, 18] (we preserve the old name for simplicity) to allow non-boolean fluents and non-deterministic causal laws.",
      "startOffset" : 45,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "In this paper, we extend action language ALd [16, 17, 18] (we preserve the old name for simplicity) to allow non-boolean fluents and non-deterministic causal laws.",
      "startOffset" : 45,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "2This representation of relations as functions, in the context of ASP, is based on prior work [5].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "We define the states and transitions of τ(D) in terms of answer sets of logic programs, as described below; see [16, 18] for more details.",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "We define the states and transitions of τ(D) in terms of answer sets of logic programs, as described below; see [16, 18] for more details.",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "Although well-foundedness is not easy to check, the broad syntactic condition called weak-acyclicity, which is easy to check, is a sufficient condition for well-foundedness [17].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "For more details about CR rules, please see [18].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "• A reality check [3]: ←val(F,Y1, I), obs(F = Y2, I), Y1 6= Y2 (15) ←val(F,Y1, I), obs(F 6= Y1, I)",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 17,
      "context" : ", [18].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "For planning and diagnostics, this program is passed to an ASP solver—we use SPARC, which expands CR-Prolog and provides explicit constructs to specify objects, relations, and their sorts [2].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 17,
      "context" : "Prior research results in the theory of action languages and ASP ensure that the plan is provably correct [18].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 26,
      "context" : "This formulation of a POMDP builds on the standard formulation [27], and the tuple’s elements are: • AP: set of concrete actions available to the robot.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "• T P : SP×AP×SP→ [0,1], the transition function, which defines the probability of transitioning to each p-state when particular actions are executed in particular p-states.",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "• OP : SP×AP×ZP → [0,1], the observation function, which defines the probability of each observation in ZP when particular actions are executed in particular p-states.",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 36,
      "context" : ", expected cumulative discounted reward) over a planning horizon—we use a point-based approximate solver [37].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 26,
      "context" : "This statement is based on existing literature [27, 35, 43].",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 34,
      "context" : "This statement is based on existing literature [27, 35, 43].",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 41,
      "context" : "This statement is based on existing literature [27, 35, 43].",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 36,
      "context" : ", loss in value) achieved by following the computed policy in comparison with the optimal policy [37].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 36,
      "context" : "We can also provide a bound on the margin of error [37].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 47,
      "context" : "For instance, the robot builds a map of the domain and estimates its position in the map using a Particle Filter algorithm for Simultaneous Localization and Mapping (SLAM) [49].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 50,
      "context" : "In addition, the simulator represents objects using probabilistic functions of features extracted from images, with the corresponding models being acquired in an initial training phase—see [52] for more details about such models.",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 51,
      "context" : "It is thus easier to understand, and to identify and fix errors in, the observed behavior, in comparison with architectures that consider all the available knowledge or only support probabilistic reasoning [53].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 45,
      "context" : "These results are not shown in Figure 6, but they are documented in prior papers evaluating just the probabilistic component of the proposed architecture [47, 52].",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 50,
      "context" : "These results are not shown in Figure 6, but they are documented in prior papers evaluating just the probabilistic component of the proposed architecture [47, 52].",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 33,
      "context" : "Visual object recognition is based on learned object models that consist of appearance-based and contextual visual cues [34].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "The domain maps are learned and revised by the robot using laser range finder data and the existing ROS implementation of a SLAM algorithm [13].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 45,
      "context" : "To make it feasible to use POMDP-1 in such large domains, we used our prior work on a hierarchical decomposition of POMDPs for visual sensing and information processing that supports automatic belief propagation across the levels of the hierarchy and model generation in each level of the hierarchy [47, 52].",
      "startOffset" : 299,
      "endOffset" : 307
    }, {
      "referenceID" : 50,
      "context" : "To make it feasible to use POMDP-1 in such large domains, we used our prior work on a hierarchical decomposition of POMDPs for visual sensing and information processing that supports automatic belief propagation across the levels of the hierarchy and model generation in each level of the hierarchy [47, 52].",
      "startOffset" : 299,
      "endOffset" : 307
    }, {
      "referenceID" : 11,
      "context" : "We have, for instance, explored different resolutions for reasoning probabilistically [12], and investigated the inference, planning and diagnostics capabilities of architectures that reason at different resolutions [53].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 51,
      "context" : "We have, for instance, explored different resolutions for reasoning probabilistically [12], and investigated the inference, planning and diagnostics capabilities of architectures that reason at different resolutions [53].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 44,
      "context" : ", based on combining relational reinforcement learning with declarative programming, has provided some promising results [46, 45], and we seek to further explore this direction of work in the future.",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 43,
      "context" : ", based on combining relational reinforcement learning with declarative programming, has provided some promising results [46, 45], and we seek to further explore this direction of work in the future.",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Proof of the proposition3: First, we use the Splitting Set Theorem for CR-Prolog [4] to simplify Π(D ,H ) by eliminating from it occurrences of atoms formed by relations obs and hpd.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 36,
      "context" : "The elements of the corresponding POMDP are described (below) in the format of the approximate POMDP solver used in our experiments [37].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "% T : action : S x S’ -> [0, 1] % Probability of transition from first element of S to that of S’ is % in the top left corner of each matrix T: move-0 1 0 0 0 0 0 0 0.",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "% Observation function format(s) % O : action : s_i : z_i -> [0, 1] (or) % : S x Z -> [0, 1] % In each matrix, first row provides probability of each possible % observation in the first p-state in S O: move-0 : * : none 1",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "% Observation function format(s) % O : action : s_i : z_i -> [0, 1] (or) % : S x Z -> [0, 1] % In each matrix, first row provides probability of each possible % observation in the first p-state in S O: move-0 : * : none 1",
      "startOffset" : 86,
      "endOffset" : 92
    } ],
    "year" : 2017,
    "abstractText" : "This paper describes an architecture that combines the complementary strengths of probabilistic graphical models and declarative programming to enable robots to represent and reason with logic-based and probabilistic descriptions of uncertainty and domain knowledge. An action language is extended to support non-boolean fluents and nondeterministic causal laws. This action language is used to describe tightly-coupled transition diagrams at two levels of granularity, refining a coarse-resolution transition diagram of the domain to obtain a fine-resolution transition diagram. The coarse-resolution system description, and a history that includes (prioritized) defaults, are translated into an Answer Set Prolog (ASP) program. For any given goal, inference in the ASP program provides a plan of abstract actions. To implement each such abstract action probabilistically, the part of the fine-resolution transition diagram relevant to this action is identified, and a probabilistic representation of the uncertainty in sensing and actuation is included and used to construct a partially observable Markov decision process (POMDP). The policy obtained by solving the POMDP is invoked repeatedly to implement the abstract action as a sequence of concrete actions, with the corresponding observations being recorded in the coarse-resolution history and used for subsequent reasoning. The architecture is evaluated in simulation and on a mobile robot moving objects in an indoor domain, to show that it supports reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains.",
    "creator" : "LaTeX with hyperref package"
  }
}