{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2015", "title": "Neural CRF Parsing", "abstract": "This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches. Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network. Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case. Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts). Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). In combination with sparse features, our system achieves 91.1 F1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.", "histories": [["v1", "Mon, 13 Jul 2015 22:23:51 GMT  (539kb,D)", "http://arxiv.org/abs/1507.03641v1", "Accepted for publication at ACL 2015"]], "COMMENTS": "Accepted for publication at ACL 2015", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["greg durrett", "dan klein"], "accepted": true, "id": "1507.03641"}
