{
  "name" : "1508.01633.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Distributed Asynchronous SGD with Variance Reduction",
    "authors" : [ "Ruiliang Zhang", "Shuai Zheng", "James T. Kwok" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 8.\n01 63\n3v 1\n[ cs\n.L G\n] 7\nA ug\n2 01\n5 1\nIndex Terms—SGD, Variance reduction, SVRG, Dsitributed Optimization\nI. INTRODUCTION\nIn recent years, there have been a rapid growth of data. A popular approach to address this challenge is by using stochastic gradient descent (SGD) and its variants [1]–[3]. However, it is often difficult to store and process a big data set on one single machine. Thus, there is now growing interest in distributed machine learning algorithms. The data set is partitioned into subsets, assigned to multiple machines, and the optimization problem solved in a distributed manner. In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6]. In this paper, we will focus on the latter, which is more scalable. Usually, one of these machines is a server while the rest are workers. The workers store the data subsets, perform local computations and send their updates to the server, which then aggregates the local information and performs the actual update on the model parameter. Note that workers only need to communicate with the server but not among them. Such a distributed computing model has been commonly used in many recent large-scale machine learning implementations [2], [3], [5], [6].\nOften, machines in these systems have to run synchronously [5], [7]. In each iteration, information from all workers need to be aggregated before the server can update. This can be expensive due to communication overhead and random network delay. It also suffers from the straggler problem [8], in which the system can move forward only at the pace of the slowest worker.\nTo alleviate these problems, asynchronicity is introduced [2], [6], [9]–[11]. The server is allowed to use only staled\nR. Zhang, S. Zheng and J. T. Kwok are with the Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong.\n(delayed) information from the workers, and thus only needs to wait for a much smaller number of workers in each iteration. Promising theoretical/empirical results have been reported. One prominent example of asynchronous SGD is the downpour SGD [2]. Each worker independently reads the parameter from the server, computes the local gradient, and sends it back to the server. The server then immediately updates the parameter using the worker’s gradient information. Using an adaptive learning rate [12], downpour SGD achieves state-of-the-art performance.\nHowever, in order for these algorithms to converge, the learning rate has to decrease not only with the number of iterations (as in standard single-machine SGD [1]), but also with the maximum delay τ (i.e., the duration between the time the gradient is computed by the worker and it is used by the server) among workers. Unlike [10], note that downpour SGD does not impose constraints on τ , and no convergence guarantee is provided.\nIn practice, a decreasing learning rate leads to slower convergence [1], [13]. Recently, Feyzmahdavian et al [14] proposed the delayed proximal gradient method in which the delayed gradient is used to update an analogously delayed model parameter (but not its current one). It is shown that even with a constant learning rate, the algorithm converges linearly to within ǫ of the optimal solution. However, to achieve a small ǫ, the learning rate needs to be small, which again means slow convergence.\nRecently, there has been the flourish development of variance reduction techniques for SGD [13], [15]–[18]. The idea is to use past gradients to progressively reduce the stochastic gradient’s variance, so that a constant learning rate can again be used. However, these algorithms are designed for single machines and its properties in a distributed asynchronous environment remain unknown.\nMotivated by these, we propose in this paper a distributed asynchronous SGD-based algorithm with variance reduction. By incorporating the stochastic variance reduction gradient, a constant learning rate can be used. The algorithm is easy to implement and highly scalable. Moreover, it can be shown to converge linearly to the optimal solution. To the best of our knowledge, this is the first such result for distributed asynchronous SGD-based algorithms with constant learning rate. A prototype is implemented on the Google Cloud Computing Platform. Experiments on several big data sets from the Pascal Large Scale Learning Challenge demonstrate that it outperforms the state-of-the-art.\n2"
    }, {
      "heading" : "II. BACKGROUND",
      "text" : "In this paper, we consider the following optimization problem\nmin w\nF (w) ≡ 1\nN\nN ∑\ni=1\nfi(w). (1)\nIn many machine learning applications, w ∈ Rd is the model parameter to be learned, N is the number of training samples, and each fi : Rd → R is a (possibly regularized) loss associated with sample i. The following assumptions are commonly made.\nAssumption 2.1: Each fi is Li-smooth, i.e., fi(x) ≤ fi(y)+ 〈∇fi(y), x− y〉+ Li 2 ‖x− y‖2 ∀x, y.\nAssumption 2.2: F is µ-strongly convex, i.e., F (x) ≥ F (y) + 〈F (y), x− y〉+ µ\n2 ‖x− y‖2 ∀x, y."
    }, {
      "heading" : "A. Delayed Proximal Gradient (DPG) [14]",
      "text" : "At iteration t, a worker uses a copy of w, which has been delayed by τt iterations (denoted wt−τt ), to compute the stochastic gradient gt−τt = ∇fi(wt−τt). This delayed gradient is used to update the correspondingly delayed parameter copy wt−τt to ŵt−τt = wt−τt − ηgt−τt , where η is a constant learning rate. This ŵt−τt is then sent to the server, which obtains the new iterate wt+1 as a convex combination of the current wt and ŵt−τt :\nwt+1 = (1− θ)wt + θŵt−τt , (2)\nwhere θ ∈ (0, 1]. It can be shown that the {wt} sequence converges linearly to the optimal solution w∗ within a tolerance of ǫ, i.e., E [F (wt)− F (w∗)] ≤ ρt(F (w0)− F (w∗)) + ǫ, for some ρ < 1 and ǫ > 0. The tolerance ǫ can be reduced by reducing η, though at the expense of increasing ρ and thus slowing down convergence."
    }, {
      "heading" : "B. Stochastic Variance Reduced Gradient [13]",
      "text" : "The SGD, though simple and scalable, has a slower convergence rate than batch gradient descent [19]. As noted in [13], the underlying reason is that the stepsize of SGD has to be decreasing so as to control the gradient’s variance. Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].\nIn this paper, we focus on one of these techniques, namely the stochastic variance reduction gradient (SVRG) [13] (Algorithm 1). It is advantageous is that no extra space is needed for the intermediate gradients or dual variables. The algorithm proceeds in stages. At the beginning of each stage, the gradient ∇F (w̃) = 1\nN\n∑N\ni=1 ∇fi(w̃) is computed on the whole data set using a past parameter estimate w̃ (which is updated across stages). For each subsequent iteration t in this stage, the approximate gradient ∇̂f(wt) = ∇fi(wt) − ∇fi(w̃) + ∇F (w̃) is used, where i is a sample randomly selected from {1, 2, . . . , N}. Even with a constant learning rate η, it can be shown that the (expected) variance of ∇̂f(wt) goes to zero progressively, and the algorithm achieves linear convergence.\nAlgorithm 1 Stochastic variance reduced gradient (SVRG) [13].\n1: Initialize w̃0; 2: for s = 1, 2, ... do 3: w̃ = w̃s−1; 4: ∇F (w̃) = 1\nN ∑N i=1 ∇fi(w̃);\n5: w0 = w̃; 6: for t = 1, 2, . . . ,m do 7: randomly pick i ∈ {1, . . . , N}; 8: wt+1 = wt − η∇̂f(wt); 9: end for\n10: set w̃s = wt for randomly chosen t ∈ {0, . . . ,m− 1}; 11: end for"
    }, {
      "heading" : "III. PROPOSED ALGORITHM",
      "text" : "Both DPG and SVRG allow the use of a constant, and thus, larger learning rate than typically used by SGD. However, while DPG is a distributed algorithm, it only converges to a neighborhood of the optimal solution. Better approximation quality comes at the expense of slower convergence. On the other hand, SVRG can converge to the optimal solution, but is only designed for use on single machines. Its use and convergence properties in the distributed asynchronous learning setting remain unexplored. In this section, we propose a hybrid of the two that combines the advantages of both. Similar to DPG and SVRG, it also uses a constant learning rate, but with a guaranteed linear convergence rate to the optimal solution in a distributed learning setting."
    }, {
      "heading" : "A. Update using Delayed Gradients",
      "text" : "Recall that the server update is based on delayed gradients in distributed asynchronous learning. Here, we replace the update rule of SVRG (line 8 in algorithm 1) by\nwt+1 = wt − (η∇̂f(wt−τt) + θ(wt − wt−τt)). (3)\nObviously, when there is no delay (i.e., τt = 0), (3) reduces to standard (serial) SVRG. The change to wt in (3) has two components. The first component η∇̂f(wt−τt) is a variancereduced gradient as in SVRG. However, it is noisier as it is evaluated from the delayed parameter wt−τt . The second component θ(wt − wt−τt) then acts like a momentum term. With θ ∈ (0, 1), update (3) can also be rewritten as the following convex combination\nwt+1 = (1− θ)wt + θ(wt−τt − β∇̂f(wt−τt)),\nwhere β = η/θ. This is similar to the update in (2), but with the gradient gt−τt replaced by its variance-reduced version ∇̂f(wt−τt). Equation (3) can also be written as\nwt+1 = (1−θ)wt+θwt−τt−η(∇fi(w t−τt)−∇fi(w̃))−η∇F (w̃), (4) which will be useful in Section III-B2. Moreover, as in other SGD-based algorithms, we will use mini-batches to reduce the stochastic gradient’s variance [20] and communication cost [2], [3]. ∇̂f(wt−τt) in (3) is then replaced by 1 Bt ∑ i∈Bt (∇fi(w t−τt)−∇fi(w̃)) + ∇F (w̃), where Bt is a mini-batch of size B in the tth iteration.\n3"
    }, {
      "heading" : "B. Distributed Implementation",
      "text" : "We have a scheduler, a server and P workers. The server keeps a clock (denoted by an integer value t), the most updated copy of parameter w, a past parameter estimate w̃ and the corresponding full gradient ∇F (w̃) evaluated on the whole training set D (of N samples). We divide D into P disjoint subsets D1,D2, . . . ,DP , where Dp is owned by worker p. The number of samples in Dp is denoted np. Each worker p also keeps a local copy w̃p of w̃.\nIn the following, a task refers to an event timestamped by the scheduler. It can be issued by the scheduler or a worker, and received by either the server or workers. Each worker can only process one task at a time. There are two types of tasks, update task and evaluation task, and will be discussed in more detail in the sequel. A worker may also pull the parameter from the server by sending a request, which carries the type and timestamp of the task being run by the worker.\n1) Scheduler: The scheduler (Algorithm 2) runs in stages. In each stage, it first issues m update tasks to the workers, where m is usually a multiple of ⌈N/B⌉ as in SVRG [13]. After spawning enough tasks, the server measures the progress by issuing an evaluation task to the server and all workers. As will be seen in Section III-B3, the server ensures that evaluation is carried out only after all the update tasks for the current stage have finished. If the progress meets the stopping condition, the scheduler informs the server and all workers to stop (by issuing a STOP command); otherwise, it moves to the next stage and sends more update tasks.\nAlgorithm 2 Scheduler.\n1: for s = 1, . . . , S do 2: for k = 1, . . . ,m do 3: pick worker p with probability np\nN ;\n4: issue an update task to the worker with timestamp t = (s− 1)m+ k; 5: end for 6: issue an evaluation task (with timestamp t = sm + 1) to workers and server; 7: wait and collect progress information from workers; 8: if progress satisfies some stopping condition then 9: issue a STOP command to the workers and server;\n10: end if 11: end for\n2) Worker: At stage s, when worker p receives an update task with timestamp t, it sends a parameter pull request to the server. This request will not be responded by the server until it finishes all tasks with timestamps before t − τ . The parameter τ determines the maximum duration between the time the gradient is computed by a worker and till it is used by the server. A larger τ allows more asynchronicity, but also adds noise to the gradient and thus may slow down convergence.\nLet ŵp,t be the parameter value pulled. Worker p selects a mini-batch Bt ⊂ Dp (of size B) randomly from its local data set, and then computes\nw̄p,t = θŵp,t − η\nB\n∑\ni∈Bt\n(∇fi(ŵp,t)−∇fi(w̃p)) . (5)\nOn comparison with (4), it can be seen that w̄p,t corresponds to the term θwt−τt − η(∇fi(wt−τt) −∇fi(w̃)). This w̄p,t is then pushed to the server by issuing an update task.\nWhen a worker receives an evaluation task, it again sends a parameter pull request to the server. As will be seen in Section III-B3, the pulled ŵp,t will always be the latest w kept by the server in the current stage. Hence, the ŵp,t’s pulled by all workers are indeed the same. Worker p then updates w̃p as w̃p = ŵp,t, and computes and pushes the corresponding gradient ∇Fp(w̃p) = 1np ∑ i∈Dp ∇fi(w̃p) to the server. To inform the scheduler of its progress, worker p also computes its contribution to the optimization objective ∑\ni∈Dp fi(w̃p))\nand pushes it to the scheduler. The whole worker procedure is shown in Algorithm 3.\nAlgorithm 3 Worker p receiving an update/evaluation task t at stage s.\n1: sends a parameter pull request to the server; 2: waits for response from server; 3: if task t is an update task then 4: pick a mini-batch subset Bt randomly from local data set; 5: compute w̄p,t using (5), and push to server as an update task; 6: else 7: set w̃p = ŵp,t; 8: evaluate ∇Fp(w̃p) on the local subset, and push it to the server as an update task; 9: computes and pushes the local objective value to the\nscheduler; 10: end if\n3) Server: There are two threads running on the server. One is a daemon thread that responds to parameter pull requests from workers; and the other is a computing thread for handling update tasks from workers and evaluation tasks from the scheduler.\nWhen the daemon thread receives a parameter pull request, it reads the type and timestamp t it contains. If the request is from a worker running an update task, it checks whether all update tasks before t − τ have finished. If not, the request is pushed to a buffer; otherwise, it pushes its w value to the requesting worker. Thus, this controls the allowed asynchronicity. On the other hand, if the request is from a worker executing an evaluation task, the daemon thread does not push w to the workers until all update tasks before t have finished. This ensures that the w pulled by the worker is the most upto-date for the current stage. Procedure for the daemon thread is shown in Algorithm 4.\nWhen the computing thread receives an update task (with timestamp t) from worker p, the w̄p,t contained inside is read. Recall that w̄p,t corresponds to the term θwt−τt − η(∇fi(w\nt−τt)−∇fi(w̃)) in (4). The server thus finishes the remaining update as\nw ← (1− θ)w + w̄p,t − η∇F (w̃), (6)\nand marks this task as finished. During the update, the computing thread locks w so that the daemon thread cannot access\n4 Algorithm 4 Daemon thread of the server.\n1: repeat 2: if pull request buffer is not empty then 3: for each request with timestamp t in the buffer do 4: set τ̂ = 0; 5: if request is triggered by an update task then 6: set τ̂ = τ ; 7: end if 8: if all update tasks before t− τ̂ have finished then 9: push w to the requesting worker;\n10: remove request from buffer; 11: end if 12: end for 13: else 14: sleep for a while; 15: end if 16: until STOP command is received;\nuntil the update is finished. When the server receives an evaluation task, it synchronizes all workers and aggregates their local gradients as ∇F (w̃) = ∑P\np=1 qp∇Fp(w̃p), where qp = np N\n, as all w̃p’s are the same and equal to w̃ (Section III-B2). The complete procedure for the computing thread is shown in Algorithm 5.\nAlgorithm 5 Computing thread of the server.\n1: repeat 2: wait for tasks; 3: if an update task received then 4: update w using (6), and mark this task as finished; 5: else 6: waits for all update tasks to finish; 7: collects local full gradients from workers and update ∇F (w̃); 8: end if 9: until STOP command is received."
    }, {
      "heading" : "C. Convergence Analysis",
      "text" : "For simplicity of analysis, we do not consider the use of mini-batch. The following Theorem shows linear convergence of the proposed algorithm. Note that in contrast to the delayed proximal gradient method (Section II-A), here we have convergence to the optimal solution w∗ of (1), not just to within a tolerance of ǫ. To the best of our knowledge, this is the first such result for distributed asynchronous SGD-based algorithms with constant learning rate.\nTheorem 3.1: Let L = max{Li}Ni=1, and\nγ =\n(\n1− 2η(µ− ηL2\nθ )\n) m\n1+τ + ηL2\nθµ− ηL2 .\nWith η ∈ (0, µθ 2L2 ) and m sufficiently large such that γ < 1.1 The sequence {w̃S} output from the algorithm converges to\n1As L > µ, it is easy to see that both 1− 2η(µ− ηL 2\nθ ) and ηL\n2\nθµ−ηL2 are\nless than 1, and thus γ < 1 can be guaranteed.\nw∗ linearly:\nE[F (w̃S)− F (w∗)] ≤ γS [F (w̃0)− F (w∗)].\nWhen τ < P , the server can serve at most τ workers. Thus, for maximum parallelism, τ should increase with P . However, γ also increases with τ , and a larger m and/or S is needed to achieve the same solution quality. This compromise will be empirically studied in Section IV-C."
    }, {
      "heading" : "IV. EXPERIMENTS",
      "text" : "In this section, we consider the ℓ2-regularized logistic regression problem:\nmin w\n1\nN\nN ∑\ni=1\nlog(1 + exp(−yix T i w)) +\nλ 2 ‖w‖2,\nwhere {(xi, yi)}Ni=1 are the training samples, and λ is a regularization parameter (fixed to 0.01). Experiments are performed on the Epsilon, OCR and DNA data sets (Table I) from the Pascal Large Scale Learning Challenge2. The mini-batch size B is set to 128 for Epsilon, 500 for OCR and 50,000 for DNA.\nWe perform experiments using the Google Cloud Computing Platform3. Each computing node is a google cloud n1-highmem-2 instance with dual-core and 13GB memory. Each worker/scheduler/server is an instance. The system is implemented in C++, with the ZeroMQ4 package for communication."
    }, {
      "heading" : "A. Comparison with Other Distributed Asynchronous SGDbased Algorithms",
      "text" : "In this experiment, 16 workers are used, and we also set τ = 16. The following asynchronous SGD-based algorithms are compared: (i) downpour SGD [2] (denoted “downpour”) with the adaptive learning rate in Adagrad [12]; (ii) a variant of Downpour SGD (denoted “downpour-τ”), in which the staled stochastic gradients have a maximum delay of τ . As shown in [10], [11], this leads to better performance in practice; (iii) delayed proximal gradient [14]; (iv) the proposed algorithm, which will be called “distributed variance-reduced stochastic gradient decent” (distr-VRSGD). The number of stages S is set to 50, and the number of iterations m in each stage is ⌈N/B⌉. For fair comparison, the other algorithms are run for mS iterations. We also include the (synchronous) SVRG, by setting τ = 0. Parameter tuning for all algorithms is based on a validation set.\nFigures 1(a)-1(c) show convergence of the objective w.r.t. number of stages. As can be seen, SVRG is the fastest as its\n2http://argescale.ml.tu-berlin.de/ 3http://cloud.google.com 4http://zeromq.org\ngradients are not delayed and thus have the highest quality. However, in terms of wall clock time (Figures 1(d)-1(f)), SVRG is the slowest because of its synchronized nature and thus high communication cost. Among the distributed asyn-\n6 chronous algorithms, the proposed algorithm has the fastest convergence w.r.t. both the number of iterations and running time for all three data sets. Figure 2 shows a breakdown of the total time into computation time and network communication time. As can be seen, the proposed algorithm reduces the communication time the most, as it requires the fewest number of iterations."
    }, {
      "heading" : "B. Different Number of Workers",
      "text" : "In this experiment, we vary the number of workers from 1 to 16. With fewer workers, each worker needs to store and process a larger data subset. As each computing node in our configuration has 13GB memory, we only perform experiments on the smallest Epsilon data set.\nFigure 3(a) shows that both computation time and communication time decrease with the number of workers. Note that the most expensive step in the algorithm is on gradient evaluations.5 In each stage, there are m iterations, each of which involves a mini-batch of size B. Hence, this requires a total of O(B) gradient evaluations. At the end of each stage, an additional O(N) gradient evaluations are required for performance evaluation. Hence, each worker spends O((mB + N)/P ) time on computation. As for the communication time, having more workers implies that more parameter pull requests, tasks and data can be sent simultaneously between the server and workers. Hence, communication time also decreases. As a result, the speedup with the number of workers is close to linear.\nHowever, recall that at the end of each stage, the server needs to aggregate local gradients ∇Fp(w̃p)’s from all workers. This requires a synchronous barrier across all workers. With more and more workers, the server may have to wait for a larger number of workers (Figure 3(c)). This increases communication time, and slows down speedup."
    }, {
      "heading" : "C. Effect of τ",
      "text" : "In this experiment, τ is varied from 0 to 32. We fix m, and run the algorithm for a sufficient number of S stages until a target objective value is attained. Timing results on the three data sets are shown in Figure 4. As τ increases, more asynchronicity is allowed and there is a significant reduction in communication time. On the other hand, recall that in the convergence analysis, the convergence factor in each stage is given by γ in Theorem 3.1. Increasing τ means more stages are needed, and this leads to higher computation and communication time. Hence, when τ becomes very large, the computation and communication time increase again. Moreover, as can be seen, the choice of τ = P , which has been used in Sections IV-A and IV-B, is a good compromise."
    } ],
    "references" : [ {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "Proceedings of the International Conference on Computational Statistics, 2010, pp. 177–186. 5 The scheduler and server operations are simple and their computational loads are negligible.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le" ],
      "venue" : "Advances in Neural Information Processing Systems, 2012, pp. 1223–1231.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Distributed asynchronous online learning for natural language processing",
      "author" : [ "K. Gimpel", "D. Das", "N.A. Smith" ],
      "venue" : "Proceedings of the Fourteenth Conference on Computational Natural Language Learning, 2010, pp. 213–222.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "F. Niu", "B. Recht", "C. Ré", "S. Wright" ],
      "venue" : "Advances in Neural Information Processing Systems 24, 2011.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Communication-efficient distributed optimization using an approximate Newton-type method",
      "author" : [ "O. Shamir", "N. Srebro", "T. Zhang" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning, 2014, pp. 1000–1008.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Asynchronous distributed ADMM for consensus optimization",
      "author" : [ "R. Zhang", "J. Kwok" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning, 2014, pp. 1701–1709.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1–122, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Loose synchronization for large-scale networked systems",
      "author" : [ "J. Albrecht", "C. Tuttle", "A. Snoeren", "A. Vahdat" ],
      "venue" : "Proceedings of the USENIX Annual Technical Conference, 2006, pp. 301–314.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "A. Agarwal", "J. Duchi" ],
      "venue" : "Advances in Neural Information Processing Systems 24, 2011.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "More effective distributed ML via a stale synchronous parallel parameter server",
      "author" : [ "Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J. Kim", "P. Gibbons", "G. Gibson", "G. Ganger", "E. Xing" ],
      "venue" : "Advances in Neural Information Processing Systems 26, 2013, pp. 1223–1231.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Communication efficient distributed machine learning with the parameter server",
      "author" : [ "M. Li", "D.G. Andersen", "A.J. Smola", "K. Yu" ],
      "venue" : "Advances in Neural Information Processing Systems, 2014, pp. 19–27.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research, vol. 12, pp. 2121–2159, 2011.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 315–323.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A delayed proximal gradient method with linear convergence rate",
      "author" : [ "H.R. Feyzmahdavian", "A. Aytekin", "M. Johansson" ],
      "venue" : "Proceedings of the International Workshop on Machine Learning for Signal Processing, 2014, pp. 1–6.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "A. Defazio", "F. Bach", "S. Lacoste-Julien" ],
      "venue" : "Advances in Neural Information Processing Systems, 2014, pp. 1646–1654.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for finite training sets",
      "author" : [ "N.L. Roux", "M. Schmidt", "F.R. Bach" ],
      "venue" : "Advances in Neural Information Processing Systems, 2012, pp. 2663– 2671.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research, vol. 14, no. 1, pp. 567–599, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "L. Xiao", "T. Zhang" ],
      "venue" : "SIAM Journal on Optimization, vol. 24, no. 4, pp. 2057–2075, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Optimization with first-order surrogate functions",
      "author" : [ "J. Mairal" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning, 2013.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sample size selection in optimization methods for machine learning",
      "author" : [ "R.H. Byrd", "G.M. Chin", "J. Nocedal", "Y. Wu" ],
      "venue" : "Mathematical programming, vol. 134, no. 1, pp. 127–155, 2012.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A popular approach to address this challenge is by using stochastic gradient descent (SGD) and its variants [1]–[3].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "A popular approach to address this challenge is by using stochastic gradient descent (SGD) and its variants [1]–[3].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "In general, the distributed architecture may have shared memory [4] or distributed memory [2], [3], [5], [6].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "Such a distributed computing model has been commonly used in many recent large-scale machine learning implementations [2], [3], [5], [6].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "Such a distributed computing model has been commonly used in many recent large-scale machine learning implementations [2], [3], [5], [6].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "Such a distributed computing model has been commonly used in many recent large-scale machine learning implementations [2], [3], [5], [6].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "Such a distributed computing model has been commonly used in many recent large-scale machine learning implementations [2], [3], [5], [6].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "Often, machines in these systems have to run synchronously [5], [7].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "Often, machines in these systems have to run synchronously [5], [7].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "It also suffers from the straggler problem [8], in which the system can move forward only at the pace of the slowest worker.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "To alleviate these problems, asynchronicity is introduced [2], [6], [9]–[11].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "To alleviate these problems, asynchronicity is introduced [2], [6], [9]–[11].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "To alleviate these problems, asynchronicity is introduced [2], [6], [9]–[11].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 10,
      "context" : "To alleviate these problems, asynchronicity is introduced [2], [6], [9]–[11].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "One prominent example of asynchronous SGD is the downpour SGD [2].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "Using an adaptive learning rate [12], downpour SGD achieves state-of-the-art performance.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "However, in order for these algorithms to converge, the learning rate has to decrease not only with the number of iterations (as in standard single-machine SGD [1]), but also with the maximum delay τ (i.",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 9,
      "context" : "Unlike [10], note that downpour SGD does not impose constraints on τ , and no convergence guarantee is provided.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "In practice, a decreasing learning rate leads to slower convergence [1], [13].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "In practice, a decreasing learning rate leads to slower convergence [1], [13].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "Recently, Feyzmahdavian et al [14] proposed the delayed proximal gradient method in which the delayed gradient is used to update an analogously delayed model parameter (but not its current one).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "Recently, there has been the flourish development of variance reduction techniques for SGD [13], [15]–[18].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Recently, there has been the flourish development of variance reduction techniques for SGD [13], [15]–[18].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "Recently, there has been the flourish development of variance reduction techniques for SGD [13], [15]–[18].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "Delayed Proximal Gradient (DPG) [14]",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "Stochastic Variance Reduced Gradient [13]",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 18,
      "context" : "The SGD, though simple and scalable, has a slower convergence rate than batch gradient descent [19].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "As noted in [13], the underlying reason is that the stepsize of SGD has to be decreasing so as to control the gradient’s variance.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 12,
      "context" : "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 14,
      "context" : "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 15,
      "context" : "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 17,
      "context" : "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 18,
      "context" : "Recently, by observing that the training set is always finite in practice, a number of techniques have been developed that reduce this variance and thus allows the use of a constant stepsize [13], [15], [16], [18], [19].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 12,
      "context" : "In this paper, we focus on one of these techniques, namely the stochastic variance reduction gradient (SVRG) [13] (Algorithm 1).",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "Algorithm 1 Stochastic variance reduced gradient (SVRG) [13].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 19,
      "context" : "Moreover, as in other SGD-based algorithms, we will use mini-batches to reduce the stochastic gradient’s variance [20] and communication cost [2], [3].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "Moreover, as in other SGD-based algorithms, we will use mini-batches to reduce the stochastic gradient’s variance [20] and communication cost [2], [3].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "Moreover, as in other SGD-based algorithms, we will use mini-batches to reduce the stochastic gradient’s variance [20] and communication cost [2], [3].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 12,
      "context" : "In each stage, it first issues m update tasks to the workers, where m is usually a multiple of ⌈N/B⌉ as in SVRG [13].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "The following asynchronous SGD-based algorithms are compared: (i) downpour SGD [2] (denoted “downpour”) with the adaptive learning rate in Adagrad [12]; (ii) a variant of Downpour SGD (denoted “downpour-τ”), in which the staled stochastic gradients have a maximum delay of τ .",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "The following asynchronous SGD-based algorithms are compared: (i) downpour SGD [2] (denoted “downpour”) with the adaptive learning rate in Adagrad [12]; (ii) a variant of Downpour SGD (denoted “downpour-τ”), in which the staled stochastic gradients have a maximum delay of τ .",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "As shown in [10], [11], this leads to better performance in practice; (iii) delayed proximal gradient [14]; (iv) the proposed algorithm, which will be called “distributed variance-reduced stochastic gradient decent” (distr-VRSGD).",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "As shown in [10], [11], this leads to better performance in practice; (iii) delayed proximal gradient [14]; (iv) the proposed algorithm, which will be called “distributed variance-reduced stochastic gradient decent” (distr-VRSGD).",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "As shown in [10], [11], this leads to better performance in practice; (iii) delayed proximal gradient [14]; (iv) the proposed algorithm, which will be called “distributed variance-reduced stochastic gradient decent” (distr-VRSGD).",
      "startOffset" : 102,
      "endOffset" : 106
    } ],
    "year" : 2015,
    "abstractText" : "With the recent proliferation of large-scale learning problems, there have been a lot of interest on distributed machine learning algorithms, particularly those that are based on stochastic gradient descent (SGD) and its variants. However, existing algorithms either suffer from slow convergence due to the inherent variance of stochastic gradients, or have a fast linear convergence rate but at the expense of poorer solution quality. In this paper, we combine their merits together by proposing a distributed asynchronous SGD-based algorithm with variance reduction. A constant learning rate can be used, and it is also guaranteed to converge linearly to the optimal solution. Experiments on the Google Cloud Computing Platform demonstrate that the proposed algorithm outperforms state-of-the-art distributed asynchronous algorithms in terms of both wall clock time and solution quality.",
    "creator" : "LaTeX with hyperref package"
  }
}