{
  "name" : "1312.6802.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Suffix Stripping Problem as an Optimization Problem",
    "authors" : [ "B. P. Pande", "Pawan Tamta", "H.S. Dhami" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1\nStemming or suffix stripping, an important part of the modern Information Retrieval systems, is to find the root word (stem) out of a given cluster of words. Existing algorithms targeting this problem have been developed in a haphazard manner. In this work, we model this problem as an optimization problem. An Integer Program is being developed to overcome the shortcomings of the existing approaches. The sample results of the proposed method are also being compared with an established technique in the field for English language. An AMPL code for the same IP has also been given.\nKeywords: Affix removal, Stemming, Information Retrieval (IR), Conflation,\nand Integer Program (IP)."
    }, {
      "heading" : "1. Introduction",
      "text" : "Suffix stripping is an important tool in the toolbox of Information Retrieval (IR) systems. In simple words, the problem can be expressed as: Given a word say TALKLESS, we have to remove word endings to get the stem word, TALK. This stem must be the root of all the inflected or variant forms of TALK such as TALKS, TALKING and TALKED. In this example TALK is the stem as it is the root of above inflected forms. Stemming or conflation is the process of finding the invariant string of a word that represents the root of the word. This is an active tool in IR environment to conflate inflected words that may be related to same meaning. Stemming can enhance the retrieval effectiveness, research has shown that it enhances the recall [11]. Stemming algorithms have been studied in computer science since the 1960s. The first published stemmer was coined by Julie Beth Lovins in 1968 [13]. Stemmers are very common elements in query systems, such as Information Retrieval systems, Web search engines and so on. Google search adopted word stemming in 2003. Examples of products using stemming algorithms are search engines such as Lycos and Google, and also thesauruses and other products using Natural Language\nProcessing for the purpose of IR. Stemming is used to determine domain vocabularies in domain analysis [7]. Many commercial companies have been using stemming since the 1980s and have produced lexical and algorithmic stemmers in many languages. The Snowball stemmers [20] have also been compared with such commercial lexical stemmers with varying results [22].\nThere is a rich literature of conflation algorithms. Automatic implementation of stemming algorithm may either depends on a pure linguistic approach, where prior morphological knowledge of a particular language is needed, or employs statistical techniques based on some statistical principles that may infer word formation rules for a targeted language. The linguistic techniques generally apply a set of transformation rules and endeavor to cut off known affixes. It is evident that these rules have to be carried out by the linguistic experts in a particular language which incurs, obviously, a manual labor. Some highlighted language dependent conflation algorithms are Lovin’s algorithm [13], Dawson’s algorithm [5], Porter algorithm [19], Paise/Husk algorithm [17], Krovetz algorithm [12], Harmans’s ‘S’ stemmer [10] etc. Some recent trends can also be seen towards rule based stemming [1], [6], [18], [21].\nAnother limitation of linguistic techniques is their inability to work in a multilingual environment. Statistical techniques endeavor to cope up with this and allow dealing with new languages in the system even if little or no linguistic knowledge is available thereby ensuring minimum effort. This can be a crucial advantage especially for IR purpose where search is triggered for documents written in different languages. Successor variety word segmentation by Hafer and Weiss [9], Corpus based stemming by Jinxi Xu and W. Bruce [23], N-gram stemming by James Mayfield and Paul McNamee [15], hidden markov model (HMM) based automatic stemmer by Massimo Melucci and Nicola Orio [16], YASS stemmer by Prasenjit Majumder, et al [14], Context sensitive stemming by Peng Funchun et al. [8] are some of the important literature articles under statistical domain of stemming.\nThe principal urge is the development of the algorithms to tighten the gap between two categories. In this paper, we represent the suffix stripping problem as an optimization problem to fulfill the gap between two categories. We decided to target the multilingual environment. We address the problem as an optimization problem of maximization and frame an Integer Programming solution for it.\nIn section 2, basic definitions are being given. Section 3 covers the method with the development of the IP for suffix stripping. In Section 4, application of our technique is being shown over a set of word clusters. Section 5 highlights the conclusions and future work. In appendices, A.1 gives a comparison of outputs of our technique and Porter’s Snowball stems [20] and in appendix A.2, an AMPL code for the proposed IP is given."
    }, {
      "heading" : "2. Preliminaries and Definitions",
      "text" : "The process of conflation doesn’t guarantee an output stem to be correct from linguistic point of view. Let us hypothesize a variable N-gram model where next character or symbol of a word is predicted using preceding symbols (2 ≤ N ≤ word length). We are interested in forming bins of variable lengths where each bin contains symbol sequences scanned most recent. More particularly, we denote a given word W of length N as\nW=w1w2w3…wi …wN\nWhere wi is any character, i.e. wi A, set of alphabets. We define the frequency f(w1w2…wi) as number of times a character sequence w1w2…wi appears in a training corpus. We are interested in relative frequencies i.e. how many times a particular symbol wj follows the string w1w2…wi. These can be exploited as a probability estimate, maximum likelihood estimate (MLE). Mathematically, we define it as:\n(2.1)\nWe’ve observed the following thing- a word in an inflectional language is composed of an essential stem and two arbitrary components: prefix and suffix. By arbitrary we mean that any random word may have both, at least one or no inflectional component. So, given a word, if we move ahead starting from the first character, we are supposed to cross an arbitrary sequence of prefix, followed by stem, followed by suffix. Thus, we can have three states, prefix, stem and suffix and there is/are transition(s) from one state to another in a word formation process. This assumption gives the idea of two arbitrary split points in a given word: one between prefix and stem, the other between stem and suffix. We calculate the MLE probability estimate for each next symbol in an input word, starting from symbol at 2 nd position. We infer if there is any relation between this estimate and partial word length (N-gram). From the test data, we observe that there is continuous increase in the value of probability estimate unless the split point occurs."
    }, {
      "heading" : "3. Formulation of the Algorithm",
      "text" : "We emphasize to mention two objectives which an efficient algorithm must fulfill. First, the output obtained must be the root of all inflected words. Second, the algorithm must be language independent. A meaningful stem can be obtained by taking sequential probabilities into account. Before modeling the Integer Program for the problem, we discuss the following observations from the sample data:\n1. In most of the cases, the highest probability is not unique across N-grams\nof a word.\n2. Generally, the probabilities increase with the increase in the word length,\na sudden decrease is subject to the transition from one state to another.\n3. A continuous increase in the probabilities identifies the whole word as a\nstem.\n4. Probabilities equal to 1 at last three or more positions may be associated\nwith a common suffix, like ing.\n5. Constant probabilities at the end for more than three positions may also\nindicate whole word as stem.\n6. A continuous increasing sequence of probabilities at the end is associated\nwith a consolidated ending, which can be removed to get a stem.\nA general insight says to take an N-gram of a word as the stem which has the highest MLE score. But, as mentioned above, it is not unique. Moreover, sample results have shown that this method has yielded only 23 acceptable stems from a sample of 100 words. We therefore hypothesize how to incorporate all such observations in a single algorithm? While incorporating all these observations, we encounter randomness as the biggest problem. By considering all observations and objectives, we propose an Integer Program for suffix stripping problem. The strength of the algorithm lies in its capability to address all observations and to fulfill the objectives simultaneously. Moreover, since based on a mere probabilistic estimate, our Integer Programming technique is applicable to any language for which N-gram corpus frequencies are available.\n3.1 Development of the Integer Program and Algorithm\nFor a given word W of length n, let Ce (e=0, 1…n-1) be the MLE score of an Ngram, where C1 corresponds to 2-gram, C2 corresponds to 3-gram and so on. We assign a special MLE value of 0 to the first character of the input word, i.e. to the 1-gram, which means C0=0. Let γe be the binary variable associated with Ce, e=0, 1 … (n-1).\nWe define a variable γN (where N=n-1) corresponding to the whole word as:\n(3.1.1)\nThe integer program is given by\n(3.1.2)\nSubject to constraints\n(3.1.3)\n(3.1.4)\nWhere in equation (3.1.2) is considered as a constant. Constraint (3.1.3) has been set to incorporate all six points observed above. It allows only those two consecutive characters as the part of a stem for which we have an increasing sequence of probabilities. The objective function considers the sequence of maximum length. By this idea, as a solution to the integer program, we get the sequence of γ variables having value 1. The γ variables which are not a part of the solution are set to value 0. We get the sequence of zero and non zero variables by this Integer Program. We pick that sequence which is larger (either of 0s or 1s) and the partial word string (N-gram) corresponding to the last entry of this sequence is taken as the stem. In case of ties, we prefer to take the latter sequence, i.e. longer word string or N-gram."
    }, {
      "heading" : "4. Experimental Results",
      "text" : "To test our approach empirically, we first need sequential frequencies of Ngrams from a rich corpus. For English language, we relied on COCA [4]. This corpus has two benefits; first it has a rich collection of English words and second, it’s easy to calculate the N-gram frequencies using wild card [*]. Let’s comprehend the working of our approach with the manifestation of the proposed IP. We consider the English word Parsons as a testing candidate. The sequential frequencies from COCA and probabilities are being tabled as under:\nTable: 4.1: COCA frequencies for the word Parsons\nWord N-Gram Frequency (f) PMLE (Ce)\nParsons\nP 1863235 0 Pa 536621 .288 Par 250520 .466 Pars 2284 .009 Parso 606 .265 Parson 606 1 Parsons 542 .894\nFrom equation 3.1.2, this can be formulated as:\nMax Z = .288γ1 + .466 γ2 + .009 γ3 + .265 γ4 + γ5 + .894×0\nSubject to the constraints\n.466 γ2 - .288 γ1 ≥ 0 .009 γ3 - .466 γ2 ≥ 0 .265 γ4 - .009 γ3 ≥ 0\nγ5 - .265 γ4 ≥ 0\nAs we can see, C6< C5 here, so by equation (3.1.1) we have taken γ6=0. On solving this IP, we get the values of the variables as: γ1= γ2=0 and γ3 =γ4 =γ5=1. Sequence of non-zero variables i.e. 1s is longer than that of 0s, therefore the last variable is identified as γ5. The stem is the word sequence (N-gram) associated with γ5, which is Parson.\nLet’s apply the IP technique over some other language, consider the word Dificilmente (Portuguese). The sequential frequencies, taken from the corpus Corpus Do Português [3], are given as under:\nTable: 4.2: Corpus Do Português frequencies for the word Dificilmente\nWord N-Gram Frequency (f) PMLE (Ce)\nDificilmente\nD 737348 0 Di 62719 .085 Dif 5714 .091 Difi 1639 .286 Dific 1639 1 Difici 190 .115 Dificil 190 1\nDificilm 178 .936 Dificilme 178 1 Dificilmen 178 1 Dificilment 178 1 Dificilmente 178 1\nHere γ11=1 and on solving the IP, we get γ1=γ2=γ3=γ4=γ5=γ6=0 and γ7=γ8=γ9=γ10=1. Again, sequence of 0s is longer than that of 1s, the last variable is identifies as γ6 and the N-gram corresponding to γ6 is taken as stem viz. Dificil.\nUnderneath we are giving a table exhibiting the stems resulted with our IP based conflation technique. Five different clusters are being taken into consideration. First five clusters are of English words, and the sixth is of Spanish words. For latter set of words, the sequential frequencies are taken from the corpus Corpus Del Español [2].\nTable: 4.3: Clusters and their IP stems\nCluster Word Stem Cluster Word Stem\n1\nCreate Creat\n4\nChange Change\nCreates Creat Changes Change Created Creat Changed Change Creating Creat Changing Chang Creative Creat Changeable Change\n2\nInclude Includ\n5\nComplete Complete\nIncludes Includ Completes Complete Including Includ Completed Complete Included Includ Completing Complet\n3\nAnnounce Announc\n6\nTrabajan Trabaj\nAnnounces Announc Trabajar Trabaj Announced Announc Trabajado Trabaj Announcing Announc Trabajador Trabaj"
    }, {
      "heading" : "5. Conclusions and Future work",
      "text" : "The efficiency of current work is being concluded on the basis of the output obtained for 100 randomly chosen words, and comparing them with outputs of Porter’s Snowball stemmer [20] (Appendix A.1). For English language, Porter’s stemmer is being used very widely and has become the de facto standard algorithm. The survey has been categorized in three sections, first section contains the results identical to that of Porter (58), second section illustrates the\nmorphologically better results (18) and in third section, we put the results close/inferior to that of Porter (24).\nWe endeavored to develop a probabilistic stemmer which should be language independent and came out with one which is dependent on mere character frequencies rather than morphological knowledge. This equips our technique to survive in a multilingual environment where need for specific morphological knowledge has been lessened. The notion of sequential probability estimate has been coined and for the first time in literature, the conflation problem is redefined as an optimization problem for which an Integer Program based solution is devised by us.\nThis work can have a two folded future prospects, one is the measurement of the hardness of the problem. A linear programming relaxation and development of some inequalities may result in some polynomial bounds. Second the development of new facet thereby defining inequalities that may enhance its efficiency and may put it more close to the morphological stem."
    }, {
      "heading" : "6. References",
      "text" : "1. Araujo Lourdes, Zaragoza Hugo, Pérez-Agüera Jose R., Pérez-Iglesias\nJoaquín: Structure of morphologically expanded queries: A genetic algorithm approach, Data & Knowledge Engineering, Volume 69, Issue 3, 279-289 (2010).\n2. Corpus Del Español. Available at < http://www.corpusdelespanol.org/>,\nvisited 21 December 2013.\n3. Corpus Do Português. Available at\n<http://www.corpusdoportugues.org/>, visited 21 December 2013.\n4. Corpus of Contemporary American English (COCA). Available at\n<http://corpus.byu.edu/coca/>, visited 21 December 2013.\n5. Dawson John: Suffix removal and word conflation. ALLC Bulletin,\nVolume 2, No. 3, 33-46 (1974).\n6. English Joshua S.: English Stemming Algorithm, Pragmatic Solutions,\nInc., 1-3 (2005).\n7. Frakes, W. et al.: DARE: Domain Analysis and Reuse Environment,\nAnnals of Software Engineering (5). 125-141 (1998).\n8. Funchun Peng, Nawaaz Ahmed, Xin Li and Yumao Lu.: Context\nsensitive stemming for web search. Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, 639-646 (2007).\n9. Hafer M. and S. Weiss: Word Segmentation by Letter Successor\nVarieties, Information Storage and Retrieval, 10, 371-85 (1974).\n10. Harman Donna: How effective is suffixing? Journal of the American\nSociety for Information Science, 42, 7-15 (1991).\n11. Kraaij Wessel and Pohlmann Renee: Viewing stemming as recall\nenhancement. Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, 40-48 (1996).\n12. Krovetz Robert: Viewing morphology as an inference process.\nProceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, 191-202 (1993).\n13. Lovins, J. B.: Development of a stemming algorithm. Mechanical\nTranslation and Computational Linguistics, 11, 22-31 (1968).\n14. Majumder Prasenjit, et al.: YASS: Yet another suffix stripper. ACM\nTransactions on Information Systems. 25(4), Article No. 18 (2007).\n15. Mayfield James and McNamee Paul: Single N-gram stemming.\nProceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, 415-416 (2003).\n16. Melucci Massimo and Orio Nicola: Design, Implementation, and\nEvaluation of a Methodology for Automatic Stemmer Generation. Journal of the American Society for Information Science and Technology, 58(5), 673–686 (2007).\n17. Paice Chris D.: Another stemmer. ACM SIGIR Forum, 24(3). 56-61\n(1990).\n18. Pande B. P. and Dhami H. S.: Application of Natural Language\nProcessing Tools in Stemming. International Journal of Computer Applications, 27(6):14-19 (2011).\n19. Porter M. F.: An algorithm for suffix stripping. Program 14, 130-137\n(1980).\n20. Porter, M. F.: Snowball: A language for stemming algorithms (2001).\nAvailable at <http://snowball.tartarus.org/>, visited 22 December 2013.\n21. Tamah Eiman , Shammari-Al.: Towards an error free stemming, IADIS\nEuropean Conference Data Mining, 160-163 (2008).\n22. Tomlinson Stephen: Lexical and Algorithmic Stemming Compared for 9\nEuropean Languages with Hummingbird SearchServer TM at CLEF 2003. CLEF 2003: 286-300 (2003).\n23. Xu Jinxi and Croft Bruce W.: Corpus-based stemming using co-\noccurrence of word variants, ACM Transactions on Information Systems. Volume 16 (1)1, 61-81 (1998).\nAppendices\nA.1: Outputs of IP stemmer and Porter stemmer (Snowball) over 100 randomly chosen English words:\n1: Outputs which are identical to Porter’s S. No. Word IP stem Porter (Snowball) stem\n1 Parsons Parson Parson 2 Dilution Dilut Dilut 3 Agreement Agreement Agreement 4 Passion Passion Passion 5 Cutter Cutter Cutter 6 Museums Museum Museum 7 Fleet Fleet Fleet 8 Haze Haze Haze 9 Manace Menac Menac\n10 Training Train Train 11 Ordaining Ordain Ordain 12 Abject Abject Abject 13 Overseas Oversea Oversea 14 Predicted Predict Predict 15 Lyric Lyric Lyric 16 Admonishing Admonish Admonish 17 Quiet Quiet Quiet 18 Unloaded Unload Unload 19 Alto Alto Alto 20 Casual Casual Casual 21 Admiring Admir Admir 22 Believing Believ Believ 23 Borrowed Borrow Borrow 24 Borrowing Borrow Borrow 25 Consulted Consult Consult 26 Consulting Consult Consult 27 Deceived Deceiv Deceiv 28 Deceiving Deceiv Deceiv 29 Employed Employ Employ 30 Employing Employ Employ 31 Explained Explain Explain 32 Explaining Explain Explain 33 Finished Finish Finish 34 Finishing Finish Finish 35 Gathered Gather Gather 36 Gathering Gather Gather\n37 Improved Improv Improv 38 Improving Improv Improv 39 Laughed Laugh Laugh 40 Laughing Laugh Laugh 41 Listened Listen Listen 42 Listening Listen Listen 43 Plucked Pluck Pluck 44 Plucking Pluck Pluck 45 Preached Preach Preach 46 Preaching Preach Preach 47 Enormously Enorm Enorm 48 Monthly Month Month 49 Solemnly Solemn Solemn 50 Frightfully Fright Fright 51 Swiftly Swift Swift 52 Blissfully Bliss Bliss 53 Viciously Vicious Vicious 54 Hopelessly Hopeless Hopeless 55 Briskly Brisk Brisk 56 Anxiously Anxious Anxious 57 Inwardly Inward Inward 58 Miserable Miser Miser\n2: Outputs which are morphologically better than Porter’s S. No. Word IP stem Porter (Snowball) stem\n1 Vacancy Vacancy Vacanc 2 Ninety Nine Nineti 3 Bloodier Blood Bloodier 4 Despotic Despotic Despot 5 Eleventh Eleven Eleventh 6 Macabre Macabre Macabr 7 Quizzical Quizzical Quizzic 8 Undeveloped Undeveloped Undevelop 9 Carrying Carry Carri\n10 Forbidden Forbid Forbidden 11 Abnormally Abnormal Abnorm 12 Diligently Diligent Dilig 13 Jubilantly Jubilant Jubil 14 Thankfully Thankful Thank 15 Reluctantly Reluctant Reluct 16 Wonderfully Wonderful Wonder 17 Delightfully Delightful Delight 18 Obnoxiously Obnoxious Obnoxi\n3: Outputs which are close/ inferior than Porter’s\nS. No. Word IP stem Porter (Snowball) stem\n1 Refill Refi Refill 2 Stallion Stall Stallion 3 Braid Bra Braid 4 Midwife Midwife Midwif 5 Prophet Prop Prophet 6 Laceration Lacerat Lacer 7 Librarian Librar Librarian 8 Mainstream Mainstre Mainstream 9 Substitute Substitute Substitut\n10 Freehand Freeh Freehand 11 Recognized Recogni Recogn 12 Devastating Devastat Devast 13 Abused Abuse Abus 14 Abusing Abus/Abusing Abus 15 Admired Admi Admir 16 Believed Belie Believ 17 Carried Carrie Carri 18 Decorated Decorat Decor 19 Decorating Decorati Decor 20 Forbidding Forbidd Forbid 21 Mended Men/Mende Mend 22 Mending Mending Mend 23 Nipped Nipp Nip 24 Nipping Nipping Nip\nA.2: AMPL program for the proposed IP\n#----------Integer Program for Suffix stripping-----#----------------------Model------------------------------param n; param N=n-1;\nset I={1..N}; var Gama{I} binary; param C{I}; param const;\nmaximize z: sum {e in 1..N-1}\nC[e]*Gama[e]+const;\nsubject to\ndifference{x in 1..N-2}:\nC[x+1]*Gama[x+1]-C[x]*Gama[x]>=0;\n#------------Data: For the word PARSONS---------------\ndata; param n:=7; param C:= 1 .288 2 .466 3 .009 4 .265 5 1 6 .894 ; var Gama[6] 0; param const:=0;\noption solver cplex; solve ;\ndisplay z, Gama;"
    } ],
    "references" : [ {
      "title" : "Structure of morphologically expanded queries: A genetic algorithm approach, Data & Knowledge Engineering, Volume",
      "author" : [ "Araujo Lourdes", "Zaragoza Hugo", "Pérez-Agüera Jose R", "Pérez-Iglesias Joaquín" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Suffix removal and word conflation",
      "author" : [ "Dawson John" ],
      "venue" : "ALLC Bulletin, Volume 2, No. 3, 33-46",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "English Stemming Algorithm",
      "author" : [ "S. English Joshua" ],
      "venue" : "Pragmatic Solutions, Inc.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "DARE: Domain Analysis and Reuse Environment, Annals of Software Engineering (5)",
      "author" : [ "W Frakes" ],
      "venue" : "125-141",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Context sensitive stemming for web search",
      "author" : [ "Funchun Peng", "Nawaaz Ahmed", "Xin Li", "Yumao Lu." ],
      "venue" : "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, 639-646",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Word Segmentation by Letter Successor Varieties",
      "author" : [ "Hafer M", "S. Weiss" ],
      "venue" : "Information Storage and Retrieval,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1974
    }, {
      "title" : "How effective is suffixing",
      "author" : [ "Harman Donna" ],
      "venue" : "Journal of the American Society for Information Science,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1991
    }, {
      "title" : "Viewing stemming as recall enhancement",
      "author" : [ "Kraaij Wessel", "Pohlmann Renee" ],
      "venue" : "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, 40-48",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Viewing morphology as an inference process",
      "author" : [ "Krovetz Robert" ],
      "venue" : "Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, 191-202",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Development of a stemming algorithm",
      "author" : [ "J.B. Lovins" ],
      "venue" : "Mechanical Translation and Computational Linguistics, 11, 22-31",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "YASS: Yet another suffix stripper",
      "author" : [ "Majumder Prasenjit" ],
      "venue" : "ACM Transactions on Information Systems. 25(4), Article No. 18",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Single N-gram stemming",
      "author" : [ "Mayfield James", "McNamee Paul" ],
      "venue" : "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval, 415-416",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Design, Implementation, and Evaluation of a Methodology for Automatic Stemmer Generation",
      "author" : [ "Melucci Massimo", "Orio Nicola" ],
      "venue" : "Journal of the American Society for Information Science and Technology, 58(5), 673–686",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Another stemmer",
      "author" : [ "D. Paice Chris" ],
      "venue" : "ACM SIGIR Forum, 24(3). 56-61",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Application of Natural Language Processing Tools in Stemming",
      "author" : [ "P. Pande B.", "S. Dhami H." ],
      "venue" : "International Journal of Computer Applications, 27(6):14-19",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An algorithm for suffix stripping",
      "author" : [ "F. Porter M." ],
      "venue" : "Program 14, 130-137",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Snowball: A language for stemming algorithms",
      "author" : [ "M.F. Porter" ],
      "venue" : "Available at <http://snowball.tartarus.org/>,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    }, {
      "title" : "Towards an error free stemming",
      "author" : [ "Tamah Eiman", "Shammari-Al" ],
      "venue" : "IADIS European Conference Data Mining,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Lexical and Algorithmic Stemming Compared for 9 European Languages with Hummingbird SearchServer  TM at CLEF 2003",
      "author" : [ "Tomlinson Stephen" ],
      "venue" : "CLEF 2003: 286-300",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Corpus-based stemming using cooccurrence of word variants, ACM Transactions on Information Systems",
      "author" : [ "Xu Jinxi", "Croft Bruce W." ],
      "venue" : "Volume 16 (1)1, 61-81",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Stemming can enhance the retrieval effectiveness, research has shown that it enhances the recall [11].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "The first published stemmer was coined by Julie Beth Lovins in 1968 [13].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Stemming is used to determine domain vocabularies in domain analysis [7].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : "The Snowball stemmers [20] have also been compared with such commercial lexical stemmers with varying results [22].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "The Snowball stemmers [20] have also been compared with such commercial lexical stemmers with varying results [22].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "Some highlighted language dependent conflation algorithms are Lovin’s algorithm [13], Dawson’s algorithm [5], Porter algorithm [19], Paise/Husk algorithm [17], Krovetz algorithm [12], Harmans’s ‘S’ stemmer [10] etc.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "Some highlighted language dependent conflation algorithms are Lovin’s algorithm [13], Dawson’s algorithm [5], Porter algorithm [19], Paise/Husk algorithm [17], Krovetz algorithm [12], Harmans’s ‘S’ stemmer [10] etc.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "Some highlighted language dependent conflation algorithms are Lovin’s algorithm [13], Dawson’s algorithm [5], Porter algorithm [19], Paise/Husk algorithm [17], Krovetz algorithm [12], Harmans’s ‘S’ stemmer [10] etc.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "Some highlighted language dependent conflation algorithms are Lovin’s algorithm [13], Dawson’s algorithm [5], Porter algorithm [19], Paise/Husk algorithm [17], Krovetz algorithm [12], Harmans’s ‘S’ stemmer [10] etc.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : "Some highlighted language dependent conflation algorithms are Lovin’s algorithm [13], Dawson’s algorithm [5], Porter algorithm [19], Paise/Husk algorithm [17], Krovetz algorithm [12], Harmans’s ‘S’ stemmer [10] etc.",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : "Some highlighted language dependent conflation algorithms are Lovin’s algorithm [13], Dawson’s algorithm [5], Porter algorithm [19], Paise/Husk algorithm [17], Krovetz algorithm [12], Harmans’s ‘S’ stemmer [10] etc.",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 0,
      "context" : "Some recent trends can also be seen towards rule based stemming [1], [6], [18], [21].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "Some recent trends can also be seen towards rule based stemming [1], [6], [18], [21].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "Some recent trends can also be seen towards rule based stemming [1], [6], [18], [21].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "Some recent trends can also be seen towards rule based stemming [1], [6], [18], [21].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "Successor variety word segmentation by Hafer and Weiss [9], Corpus based stemming by Jinxi Xu and W.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 19,
      "context" : "Bruce [23], N-gram stemming by James Mayfield and Paul McNamee [15], hidden markov model (HMM) based automatic stemmer by Massimo Melucci and Nicola Orio [16], YASS stemmer by Prasenjit Majumder, et al [14], Context sensitive stemming by Peng Funchun et al.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 11,
      "context" : "Bruce [23], N-gram stemming by James Mayfield and Paul McNamee [15], hidden markov model (HMM) based automatic stemmer by Massimo Melucci and Nicola Orio [16], YASS stemmer by Prasenjit Majumder, et al [14], Context sensitive stemming by Peng Funchun et al.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Bruce [23], N-gram stemming by James Mayfield and Paul McNamee [15], hidden markov model (HMM) based automatic stemmer by Massimo Melucci and Nicola Orio [16], YASS stemmer by Prasenjit Majumder, et al [14], Context sensitive stemming by Peng Funchun et al.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 10,
      "context" : "Bruce [23], N-gram stemming by James Mayfield and Paul McNamee [15], hidden markov model (HMM) based automatic stemmer by Massimo Melucci and Nicola Orio [16], YASS stemmer by Prasenjit Majumder, et al [14], Context sensitive stemming by Peng Funchun et al.",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "[8] are some of the important literature articles under statistical domain of stemming.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "1 gives a comparison of outputs of our technique and Porter’s Snowball stems [20] and in appendix A.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "The efficiency of current work is being concluded on the basis of the output obtained for 100 randomly chosen words, and comparing them with outputs of Porter’s Snowball stemmer [20] (Appendix A.",
      "startOffset" : 178,
      "endOffset" : 182
    } ],
    "year" : 2013,
    "abstractText" : "Stemming or suffix stripping, an important part of the modern Information Retrieval systems, is to find the root word (stem) out of a given cluster of words. Existing algorithms targeting this problem have been developed in a haphazard manner. In this work, we model this problem as an optimization problem. An Integer Program is being developed to overcome the shortcomings of the existing approaches. The sample results of the proposed method are also being compared with an established technique in the field for English language. An AMPL code for the same IP has also been given.",
    "creator" : "Microsoft® Office Word 2007"
  }
}