{
  "name" : "1605.07869.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Variational Neural Machine Translation",
    "authors" : [ "Biao Zhang", "Deyi Xiong", "Jinsong Su" ],
    "emails" : [ "zb@stu.xmu.edu.cn,", "jssu@xmu.edu.cn", "dyxiong@suda.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (NMT) is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than\nphrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).\nMost NMT models take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into a distributed representation, and a neural decoder generates the corresponding target sentence y according to the distributed representation1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this encoder-decoder framework. They are encoded into hidden states of the encoder and the decoder.\nUnlike the vanilla encoder-decoder framework, we model underlying semantics of bilingual sentence pairs explicitly. We assume that there exists a continuous latent variable z from this underlying semantic space. And this variable together with x guide the translation process, i.e. p(y|z,x). With this assumption, the original conditional probability evolves into the following formulation:\np(y|x) = ∑ z p(y, z|x) = ∑ z p(y|z,x)p(z)\n(1) Although this latent variable enables us to explicitly model underlying semantics of translation pairs, the incorporation of it into the above probability model has two challenges: 1) the poste-\n1In this paper, we use bold symbols to denote variables, and plain symbols to denote their values. Without specific statement, all these variables are multivariate.\nar X\niv :1\n60 5.\n07 86\n9v 1\n[ cs\n.C L\n] 2\n5 M\nay 2\n01 6\namssymb amsmath\nrior inference in this model is intractable; 2) largescale training, which lays the ground for the datadriven NMT, is accordingly problematic.\nIn order to address these issues, we propose a variational encoder-decoder model to neural machine translation (VNMT), motivated by the recent success of variational neural models (Rezende et al., 2014; Kingma and Welling, 2014). Figure 1 illustrates the graphic representation of VNMT. Since the source and target part of a sentence pair should share the same semantics, we can induce the underlying semantics of the sentence pair from either the source or target sentence. This further allows us to approximate the intractable posterior with a deep neural network only from the source side (see the dashed arrows in Figure 1). With respect to efficient learning, we apply a reparameterization technique (Rezende et al., 2014; Kingma and Welling, 2014) on the variational lower bound. This enables us to use standard stochastic gradient optimization for the proposed model training. Specifically, there are three essential components in VNMT (detailed architecture is illustrated in Figure 2):\n• A variational neural encoder transforms source sentence into a distributed representation, which is the same as the encoder of NMT (Bahdanau et al., 2014) (see section 3.1). • A variational neural approximator infers the\nrepresentation of z according to the learned source representation (i.e. q(z|x)), where the reparameterization technique is employed (see section 3.2). • And a variational neural decoder integrates\nthe latent representation of z to guide the generation of target sentence (i.e. p(y|z,x)) (see section 3.3).\nAugmented with the posterior approximation and reparameterization, our VNMT can be trained end-to-end. This makes our model not only efficient in translation, but also simple in implementation. To train our model, we employ the conventional maximum likelihood estimation. Experiments on NIST Chinese-English translation tasks show that VNMT achieves significant improvements over a state-of-the-art SMT and NMT system."
    }, {
      "heading" : "2 Background: Variational Autoencoder",
      "text" : "In this section, we briefly reviews the variational autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014), one of the most classical variational neural models. Yet another reason to particularly introduce the variational autoencoder in this paper is to ensure the integrity of the proposed variational NMT as it also belongs to the family of encoder-decoders.\nGiven an observed variable x, VAE introduces a continuous latent variable z, and assumes that x is generated from z, i.e.,\npθ(x, z) = pθ(x|z)pθ(z) (2)\nwhere θ denotes the parameter of the model, pθ(z) is the prior, and pθ(x|z) is the conditional distribution that models the generation procedure. Typically, pθ(z) is treated as a simple Gaussian distribution, and deep non-linear neural networks are used to perform the generation, i.e., pθ(x|z).\nSimilar to our model, the integration of z in Eq. (2) raises challenges for posterior inference as well as large-scale learning. To tackle these problems, VAE adopts two techniques: neural approximation and reparameterization.\nNeural Approximation employs deep neural networks to approximate the posterior inference model qφ(z|x), where φ is the variational parameter. For the posterior approximation, VAE equips qφ(z|x) with a diagonal Gaussian N (µ, diag(σ2)), and parameterize its mean µ and variance σ2 with deep neural networks respectively.\nReparameterization reparameterizes z as a function of µ and σ, rather than using the standard sampling method. In practice, VAE leverages the “location-scale” property of Gaussian distribution, and uses the following reparameterization:\nz̃ = µ+ σ (3)\nwhere is a standard Gaussian variable that plays a role of introducing noises, and denotes an element-wise product.\nWith these two techniques, VAE bridges the gap between the generative model pθ(x|z) and the posterior inference model qφ(z|x), and operates as an end-to-end neural network. This facilitates its optimization since we can apply the standard backpropagation to the following variational lower bound to compute its gradient.\nLVAE(θ, φ;x) =− KL(qφ(z|x)||pθ(z)) +Eqφ(z|x)[log pθ(x|z)] ≤ log pθ(x) (4)\nKL(Q||P ) is the Kullback-Leibler divergence between two distributionsQ and P . Intuitively, VAE can be considered as a specific regularized version of the standard autoencoder. Because of the variations in Eq. (3), VAE learns the representation of the latent variable not as single points, but as soft ellipsoidal regions in the latent space, forcing the representation to fill the space rather than memorizing the training data as isolated representations. Therefore, the latent variable z is able to capture the variations in the observed variable x.\nWe follow the spirit of VAE to introduce a latent variable z into the translation model p(y|x). We will give the detailed description in the next section."
    }, {
      "heading" : "3 Variational Neural Machine Translation",
      "text" : "Different from previous work, we introduce a latent variable z to model the underlying semantic space. Formally, given the definition in Eq. (1) and Eq. (4), the variational lower bound of VNMT\ncan be formulated as follows:\nLVNMT(θ, φ;x,y) = −KL(qφ(z|x)||pθ(z)) +Eqφ(z|x)[log pθ(y|z,x)] (5)\nwhere qφ(z|x) is our posterior approximator, and pθ(y|z,x) is the decoder with the guidance from z. Based on this formulation, VNMT can be decomposed into three components, each of which is modeled by a neural network: a variational neural approximator that models qφ(z|x) (see part (b) in Figure 2), a variational neural decoder that models pθ(y|z,x) (see part (c) in Figure 2), and a variational neural encoder that provides the basic representation of a source sentence for the above two modules (see part (a) in Figure 2). Following the flow illustrated in Figure 2, we describe part (a), (b) and (c) successively.\nNotice that we approximate the posterior to be conditioned on x alone, rather than y or (x,y). This is sound and reasonable because bilingual sentences are semantic equivalent, which means that either y or x is capable of inferring the underlying semantics of sentence pairs, i.e., the representation of latent variable z."
    }, {
      "heading" : "3.1 Variational Neural Encoder",
      "text" : "As shown in Figure 2 (a), the variational neural encoder aims at encoding an input source sentence (x1, x2, . . . , xTf ) into a continuous vector. In this paper, we adopt the encoder architecture proposed by Bahdanau et al. (2014), which is a bidirectional RNN that consists of a forward RNN and backward RNN. The forward RNN reads source sentence from left to right while the backward RNN in the opposite direction (see the parallel arrows in\nFigure 2 (a)):\n−→ h i = RNN( −→ h i−1, Exi) (6) ←− h i = RNN( ←− h i+1, Exi) (7)\nwhere Exi ∈ Rdw is the dw-dimensional embedding for source word xi, and −→ h i, ←− h i ∈ Rdf are df -dimensional hidden states generated in two directions. Following Bahdanau et al. (2014), we employ the Gated Recurrent Unit (GRU) as our RNN unit due to its capacity in capturing longdistance dependencies.\nWe further concatenate each pair of hidden states at each time step to build a set of annotation vectors (h1, h2, . . . , hTf ), where\nhTi = [−→ h Ti ; ←− h Ti ] In this way, each annotation vector hi ∈ R2df encodes information about the i-th word with respect to all the other surrounding words in the source sentence. Therefore, these annotation vectors are desirable for the following modeling."
    }, {
      "heading" : "3.2 Variational Neural Approximator",
      "text" : "As the posterior inference model p(z|x) is intractable in most cases, we adopt an approximation method to simplify the posterior inference. Conventional models usually employ the meanfield approaches. However, a major limitation of this approach is its inability to capture the true posterior of z due to its oversimplification. Following the spirit of VAE, we use neural networks for better approximation in this paper.\nSimilar to previous work (Kingma and Welling, 2014; Rezende et al., 2014), we let qφ(z|x) be a multivariate Gaussian distribution with a diagonal covariance structure:\nqφ(z|x) = N (z;µ, σ2I) (8)\nwhere the mean µ and s.d. σ of the approximate posterior are the outputs of the neural network as shown in Figure 2 (b). The reason of choosing Gaussian distribution is twofold: 1) it is a natural choice for describing continuous variables; 2) it belongs to the family of “location-scale” distributions, which is required for the following reparameterization.\nWe first synthesize the source-side information via a mean-pooling operation over the annotation\nvectors:\nhf = 1\nTf Tf∑ i hi (9)\nWith this source representation, we perform a nonlinear transformation that projects it onto our concerned latent semantic space:\nh′z = g(W (1) z hf + b (1) z ) (10)\nwhere W (1)z ∈ Rdz×2df , b(1)z ∈ Rdz is the parameter matrix and bias term respectively, dz is the dimensionality of the latent space, and g(·) is an element-wise activation function, which we set to be tanh(·) throughout our experiments.\nIn this latent space, we further obtain the abovementioned Gaussian parameters µ and log σ2 through linear regression:\nµ =Wµh ′ z + bµ (11)\nlog σ2 =Wσh ′ z + bσ (12)\nwhere Wµ,Wσ ∈ Rdz×dz , bµ, bσ ∈ Rdz are the parameters, and µ, log σ2 are both dz-dimension vectors.\nSimilar to the Eq. (3), the final representation for latent variable z can be reparameterized as hz = µ + σ , ∼ N (0, I). During decoding, we set hz to be the mean of p(z|x), i.e., µ. Intuitively, the reparameterization bridges the gap between the model pθ(y|z,x) and the inference model qφ(z|x). In other words, it connects these two neural networks. This is important since it enables the stochastic gradient optimization via standard backpropagation.\nTo perform translation in the target language, we further project the representation of latent variable hz onto the target space:\nhe = g(W (2) z hz + b (2) z ) (13)\nwhere W (2)z ∈ Rd′e×dz , b(2)z ∈ Rd′e are parameters, and d′e is the dimensionality of the target space. The transformed he is then integrated into our decoder. Notice that because of the noise from , the representation he is not fixed for the same source sentence and model parameters. This is crucial for VNMT to learn to be insensitive to small noises."
    }, {
      "heading" : "3.3 Variational Neural Decoder",
      "text" : "Given the source sentence x and the latent variable z, our decoder defines the probability over transla-\ntion y as a joint probability of ordered conditionals:\np(y|z,x) = Te∏ j=1 p(yj |y<j , z,x) (14)\nwhere,\np(yj |y<j , z,x) = g′(yj−1, sj , cj)\nThe feed forward model g′(·) (see the yellow arrows in Figure 2) and context vector cj =∑\ni αjihi (see the “⊕” in Figure 2) are the same as (Bahdanau et al., 2014). The difference between our decoder and Bahdanau et al.’s decoder (2014) lies in that in addition to the context vector, our decoder integrates the representation of the latent variable, i.e. he, into the computation of sj , which is denoted by the bold red dashed arrow in Figure 2 (c).\nFormally, the hidden state sj in our decoder is calculated by\nsj = (1− zj) sj−1 + zj s̃j ,\nwhere2,\ns̃j = tanh(WEyj + U [rj sj−1] + Ccj + V he) zj = σ(WzEyj + Uzsj−1 + Czcj + Vzhe)\nrj = σ(WrEyj + Ursj−1 + Crcj + Vrhe)\nHere, rj , zj , s̃j denotes the reset gate, update gate and candidate activation in GRU respectively, and Eyj ∈ Rdw is the dw-dimensional word embedding for target word. W, Wz, Wr ∈ Rde×dw , U, Uz, Ur ∈ Rde×de , C, Cz, Cr ∈ Rde×2df , and V, Vz, Vr ∈ Rde×d′e are parameter weights. The initial hidden state s0 is initialized in the same way by Bahdanau et al. (2014) (see the arrow to s0 in Figure 2).3\nIn our model, the latent variable can affect the representation of hidden state sj through the gate between rj and zj . This allows our model to access the semantic information of z indirectly since the prediction of yj+1 depends on sj ."
    }, {
      "heading" : "3.4 Model Training",
      "text" : "As shown in Eq. (5), to optimize our model, we need to compute an expectation over the approximate posterior, that is, Eqφ(z|x)[·]. This expectation, again, is intractable. Following VAE, we\n2We omit the bias term for clarity. 3Notice that we do not incorporate the latent representation into the calculation of initial hidden state because we find that the model could suffer from the noise from he in our preliminary experiments.\nAlgorithm 1 Training Algorithm of VNMT. Inputs: A, the maximum number of iterations;\nM , the number of instances in one batch; L, the number of samples;\nθ, φ← Initialize parameters repeat D ← getRandomMiniBatch(M) ← getRandomStandardGaussianNoise() δ←∇θ,φL(θ, φ;D, ) θ, φ← parameterUpdater(θ, φ; δ) until convergence of parameters (θ, φ) or the maximum number of iterations A is reached\nemploy the Monte Carlo method to estimate this expectation:\nEqφ(z|x)[log pθ(y|z,x)] ' 1\nL L∑ l=1 log pθ(y|x,h(l)z )\n(15) where L is the number of samples.\nAs all intractable computations are eliminated, the joint training objective for a training instance (x,y) is defined as follows:\nL(θ, φ) ' 1 2 dz∑ k=1 [1 + log(σ2k)− µ2k − σ2k]\n+ 1\nL L∑ l=1 Te∑ j=1 log pθ(yj |y<j ,x,h(l)z ) (16)\nwhere h(l)z = µ+ σ (l) and (l) ∼ N (0, I)\nThe first term is the KL divergence in Eq. (5) which can be computed and differentiated without estimation (see (Kingma and Welling, 2014) for details). And the second term is the approximate expectation, which is also differentiable. Suppose that L is 1 (which is employed in our experiments), then our second term will be degenerated to the objective of conventional NMT. Intuitively, the VNMT is a regularized version of NMT whose regularization is exactly the first term.\nSince the objective function in Eq. (16) is differentiable, we is able to optimize the model parameter θ and variational parameter φ jointly using standard gradient ascent techniques. The training procedure for VNMT is summarized in Algorithm 1."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Setup",
      "text" : "To evaluate the effectiveness of the proposed VNMT, we conducted experiments on the NIST Chinese-English translation tasks. Our training\ndata4 consists of 2.9M sentence pairs, with 80.9M Chinese words and 86.4M English words respectively. We used the NIST 2005 dataset as our development set, and the NIST 2002, 2003, 2004, 2006, 2008 datasets as our test sets. We employed the case-insensitive BLEU-4 metric (Papineni et al., 2002) to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test.\nWe compared our model against two state-ofthe-art SMT and NMT systems:\n• Moses (Koehn and Hoang, 2007): the conventional phrase-based SMT system. • GroundHog (Bahdanau et al., 2014): the at-\ntentional NMT system.\nFor Moses, we followed all the default settings except for the language model. We trained a 4- gram language model on the Xinhua section of the English Gigaword corpus (306M words) using the SRILM5 toolkit with modified Kneser-Ney smoothing.\nFor GroundHog, we set the maximum length of training sentences to be 50 words, and preserved the most frequent 30K words as the source and target vocabulary respectively. Following Bahdanau et al. (2014), we set dw = 620, df = 1000, de = 1000, and M = 80. All other settings are the same as the default configuration (for RNNSearch). During decoding, we used the beam-search algorithm, and set the beam size to 10.\nFor our VNMT, we initialized its parameters with the RNNSearch in GroundHog. The settings of our model are the same as that of GroundHog, except for some parameters specific to VNMT. Following the VAE, we set the sampling number L = 1. Additionally, we set d′e = 2df = 2000 ac-\n4This corpus is a combination of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News).\n5http://www.speech.sri.com/projects/srilm/download.html\ncording to preliminary experiments. We used the Adadelta algorithm for parameterUpdate in Algorithm 1. With regard to the dimension of latent semantic space dz , we tried several different settings: 1000, 2000, 3000, and 4000.\nWe implemented our VNMT based on GroundHog6. Both NMT systems are trained on a Telsa K40 GPU. In one hour, the GroundHog system processes about 1100 batches, while our VNMT processes 400∼750 batches when dz ranges from 4000 to 1000."
    }, {
      "heading" : "4.2 Translation Results",
      "text" : "Table 1 summarizes the BLEU scores of different systems on the Chinese-English translation tasks. No matter which dimensionality we set for dz , VNMT consistently improves translation quality in terms of BLEU on all test sets. Specifically, when dz = 4000, VNMTx obtains the best average results that gain 0.71 and 1.20 BLEU points over Moses and GroundHog respectively. This indicates that explicitly modeling underlying semantics by a latent variable is indeed beneficial for neural machine translation.\nWith respect to the dimensionality dz of the latent variable, we do not observe consistent improvements on each test set as dz increases. The reason may be that our test sets have different data\n6https://github.com/lisa-groundhog/GroundHog\ndistributions. However, the consistent improvements on average results may suggest that relatively larger value for dz is preferred.\nIn order to show the deep difference between VNMT and GroundHog, we further divide our test sets into 6 disjoint groups according to the length of source sentences. Figure 3 shows the BLEU scores of these two neural models. We find that the performance curve of our VNMT model always appears to be on top of that of GroundHog with a certain margin. Overall, these obvious improvements on all groups in terms of the length of source sentences indicate that VNMT outperforms the vanilla NMT, no matter how long source sentences are."
    }, {
      "heading" : "4.3 Translation Analysis",
      "text" : "Table 2 shows a translation example that helps understand the advantage of VNMT over NMT.7 As the source sentence in this example is long (more than 40 words), the translation generated by Moses is relatively messy and incomprehensible. In contrast, translations generated by neural models (both GroundHog and VNMT) are much more fluent and comprehensible. However, there are essential differences between GroundHog and our VNMT. Specifically, GroundHog does not translate the phrase “官员” at the beginning of the source sentence. The translation of the clause “体 现了双方可贵的和平诚意。” at the end of\n7Only one example is displayed due to the space limit.\nthe source sentence is completely lost. In contrast, our VNMT model does not miss these fragments and is able to convey the meaning of entire source sentence to the target side.\nFrom these examples, we can find that although attention networks can help NMT trace back to relevant parts of source sentences for predicting target translations, capturing the semantics of entire sentences still remains a big challenge for neural machine translation. Since NMT implicitly models variable-length source sentences with fixedsize hidden vectors, some details of source sentences (e.g., the red sequences of words in Table 2) may not be encoded in these vectors at all. VNMT seems to be able to capture these details through a latent variable that explicitly model underlying semantics of source sentences. The promising results suggest that VNMT provides a new mechanism to model and encode sentence semantics."
    }, {
      "heading" : "5 Related Work",
      "text" : "There are roughly two lines of research related to our work: neural machine translation and variational neural model. We describe them in succession."
    }, {
      "heading" : "5.1 Neural Machine Translation",
      "text" : "The idea of performing encoding and decoding from a source sentence to a target sentence originates from early neural translation models. Kalchbrenner and Blunsom (2013) adopt a con-\nvolutional neural network to encode source sentences, and then use a recurrent neural network (RNN) to generate target translations. Cho et al. (2014) further propose the Encoder-Decoder framework with two RNNs. However, the abovementioned work mainly focus on computing the score/probability of bilingual phrases, rather than entire translations.\nWith regard to NMT, Sutskever et al. (2014) employ two multilayered Long Short-Term Memory (LSTM) models that first encode a source sentence into a single vector and then decode the translation word by word until a special end token is generated. In order to deal with issues caused by encoding all source-side information into a fixed-length vector, Bahdanau et al. (2014) introduce attentionbased NMT that aims at automatically concentrating on relevant source parts for predicting target words during decoding. The incorporation of the attention mechanism allows NMT to cope better with long sentences, and makes it really comparable to or even superior to conventional SMT.\nFollowing the success of attentional NMT, a number of approaches and models have been proposed for NMT recently, which can be grouped into different categories according to their motivations: dealing with rare words or large vocabulary (Jean et al., 2015; Luong et al., 2015b; Sennrich et al., 2015b), learning better attentional structures (Luong et al., 2015a), integrating SMT techniques (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016), character-level NMT (Ling et al., 2015; Costa-Jussà and Fonollosa, 2016), the exploitation of monolingual corpora (Gulcehre et al., 2015; Sennrich et al., 2015a) and memory network (Meng et al., 2015). All these models are designed within the discriminative encoder-decoder framework, leaving the explicit exploration of underlying semantics with a variational model an open problem."
    }, {
      "heading" : "5.2 Variational Neural Model",
      "text" : "In order to perform efficient inference and learning in directed probabilistic models on large-scale dataset, Kingma and Welling (2014) as well as Rezende et al. (2014) introduce variational neural networks. Typically, these models utilize an neural inference model to approximate the intractable posterior, and optimize model parameters jointly with a reparameterized variational lower bound using the standard stochastic gradient technique.\nThis approach is of growing interest due to its success in various tasks.\nIn this respect, Kingma et al. (2014) revisit the approach to semi-supervised learning with generative models and further develop new models that allow effective generalization from a small labeled dataset to a large unlabeled dataset. Chung et al. (2015) incorporate latent variables into the hidden state of a recurrent neural network, while Gregor et al. (2015) combine a novel spatial attention mechanism that mimics the foveation of human eyes, with a sequential variational auto-encoding framework that allows the iterative construction of complex images. Very recently, Miao et al. (2015) propose a generic variational inference framework for generative and conditional models of text.\nThe most related work to ours is that of Bowman et al. (2015), where they develop a variational autoencoder for unsupervised generative language modeling. The major difference is that they focus on the monolingual language model, while we adapt this technique to bilingual translation. Although variational neural models have been widely used in NLP-related tasks, the adaptation and utilization of variational neural model to machine translation, to the best of our knowledge, has never been investigated before."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this paper, we have presented a variational model for neural machine translation that incorporates a continuous latent variable to model the underlying semantics of sentence pairs. We approximate the posterior distribution with neural networks and reparameterize the variational lower bound. This enables our model to be an end-to-end neural network that can be optimized through conventional stochastic gradient algorithms. Comparing with the conventional attention-based NMT, our model is better at translating long sentences. It also greatly benefits from a special regularization term brought with this latent variable. Experiments on Chinese-English translation tasks verified the effectiveness of our model.\nIn the future, since the latent variable in our model is at the sentence level, we want to explore more fine-grained latent variables for neural machine translation, such as the Recurrent Latent Variable Model (Chung et al., 2015). We are also interested in applying our model to other similar tasks, e.g., conversations."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Proc. of ICLR",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating Sentences from a Continuous Space",
      "author" : [ "Bowman et al.2015] S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bowman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Agreementbased Joint Training for Bidirectional Attentionbased Neural Machine Translation",
      "author" : [ "Cheng et al.2015] Y. Cheng", "S. Shen", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu" ],
      "venue" : null,
      "citeRegEx" : "Cheng et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
      "author" : [ "Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "A recurrent latent variable model for sequential data",
      "author" : [ "Chung et al.2015] Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C. Courville", "Yoshua Bengio" ],
      "venue" : "In Proc. of NIPS",
      "citeRegEx" : "Chung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2015
    }, {
      "title" : "Character-based Neural Machine Translation",
      "author" : [ "Costa-Jussà", "Fonollosa2016] M.R. Costa-Jussà", "J.A.R. Fonollosa" ],
      "venue" : null,
      "citeRegEx" : "Costa.Jussà et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Costa.Jussà et al\\.",
      "year" : 2016
    }, {
      "title" : "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder",
      "author" : [ "S. Feng", "S. Liu", "M. Li", "M. Zhou" ],
      "venue" : "NMT Model. ArXiv e-prints, January",
      "citeRegEx" : "Feng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2016
    }, {
      "title" : "DRAW: A recurrent neural network for image generation. CoRR, abs/1502.04623",
      "author" : [ "Gregor et al.2015] Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra" ],
      "venue" : null,
      "citeRegEx" : "Gregor et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gregor et al\\.",
      "year" : 2015
    }, {
      "title" : "On Using Monolingual Corpora in Neural Machine Translation",
      "author" : [ "Gulcehre et al.2015] C. Gulcehre", "O. Firat", "K. Xu", "K. Cho", "L. Barrault", "H.-C. Lin", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : null,
      "citeRegEx" : "Gulcehre et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2015
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "Jean et al.2015] Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio" ],
      "venue" : "In Proc. of ACL-IJCNLP,",
      "citeRegEx" : "Jean et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2013
    }, {
      "title" : "Auto-Encoding Variational Bayes",
      "author" : [ "Kingma", "Welling2014] Diederik P Kingma", "Max Welling" ],
      "venue" : "In Proc. of ICLR",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling" ],
      "venue" : "In Proc. of NIPS,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Factored translation models",
      "author" : [ "Koehn", "Hoang2007] Philipp Koehn", "Hieu Hoang" ],
      "venue" : "In Proc. of EMNLP-CoNLL,",
      "citeRegEx" : "Koehn et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2007
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn" ],
      "venue" : "In Proc. of EMNLP",
      "citeRegEx" : "Koehn.,? \\Q2004\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Character-based Neural Machine Translation",
      "author" : [ "Ling et al.2015] W. Ling", "I. Trancoso", "C. Dyer", "A. W Black" ],
      "venue" : null,
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Luong et al.2015a] Thang Luong", "Hieu Pham", "Christopher D. Manning" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Addressing the rare word problem in neural machine translation",
      "author" : [ "Luong et al.2015b] Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba" ],
      "venue" : "In Proc. of ACL-IJCNLP,",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "A Deep Memory-based Architecture for Sequence-to-Sequence Learning",
      "author" : [ "Meng et al.2015] F. Meng", "Z. Lu", "Z. Tu", "H. Li", "Q. Liu" ],
      "venue" : null,
      "citeRegEx" : "Meng et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural Variational Inference for Text Processing",
      "author" : [ "Miao et al.2015] Y. Miao", "L. Yu", "P. Blunsom" ],
      "venue" : null,
      "citeRegEx" : "Miao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2015
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Salim Roukos", "Todd Ward", "Wei-Jing Zhu" ],
      "venue" : "In Proc. of ACL,",
      "citeRegEx" : "Papineni et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving Neural Machine Translation Models with Monolingual Data",
      "author" : [ "B. Haddow", "A. Birch" ],
      "venue" : null,
      "citeRegEx" : "Sennrich et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural Machine Translation of Rare Words with Subword Units",
      "author" : [ "B. Haddow", "A. Birch" ],
      "venue" : null,
      "citeRegEx" : "Sennrich et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Minimum Risk Training for Neural Machine Translation",
      "author" : [ "Shen et al.2015] S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks. CoRR, abs/1409.3215",
      "author" : [ "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Coverage-based neural machine translation. CoRR, abs/1601.04811",
      "author" : [ "Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li" ],
      "venue" : null,
      "citeRegEx" : "Tu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 368
    }, {
      "referenceID" : 25,
      "context" : "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 368
    }, {
      "referenceID" : 0,
      "context" : "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 368
    }, {
      "referenceID" : 9,
      "context" : "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 368
    }, {
      "referenceID" : 24,
      "context" : "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 368
    }, {
      "referenceID" : 18,
      "context" : "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 368
    }, {
      "referenceID" : 26,
      "context" : "Due to these advantages over traditional SMT system, NMT has recently attracted growing interest from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 368
    }, {
      "referenceID" : 3,
      "context" : "Most NMT models take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into a distributed representation, and a neural decoder generates the corresponding target sentence y according to the distributed representation1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).",
      "startOffset" : 264,
      "endOffset" : 329
    }, {
      "referenceID" : 25,
      "context" : "Most NMT models take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into a distributed representation, and a neural decoder generates the corresponding target sentence y according to the distributed representation1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).",
      "startOffset" : 264,
      "endOffset" : 329
    }, {
      "referenceID" : 0,
      "context" : "Most NMT models take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into a distributed representation, and a neural decoder generates the corresponding target sentence y according to the distributed representation1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).",
      "startOffset" : 264,
      "endOffset" : 329
    }, {
      "referenceID" : 21,
      "context" : "In order to address these issues, we propose a variational encoder-decoder model to neural machine translation (VNMT), motivated by the recent success of variational neural models (Rezende et al., 2014; Kingma and Welling, 2014).",
      "startOffset" : 180,
      "endOffset" : 228
    }, {
      "referenceID" : 21,
      "context" : "With respect to efficient learning, we apply a reparameterization technique (Rezende et al., 2014; Kingma and Welling, 2014) on the variational lower bound.",
      "startOffset" : 76,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "• A variational neural encoder transforms source sentence into a distributed representation, which is the same as the encoder of NMT (Bahdanau et al., 2014) (see section 3.",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 21,
      "context" : "In this section, we briefly reviews the variational autoencoder (VAE) (Kingma and Welling, 2014; Rezende et al., 2014), one of the most classical variational neural models.",
      "startOffset" : 70,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we adopt the encoder architecture proposed by Bahdanau et al. (2014), which is a bidirectional RNN that consists of a forward RNN and backward RNN.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Following Bahdanau et al. (2014), we employ the Gated Recurrent Unit (GRU) as our RNN unit due to its capacity in capturing longdistance dependencies.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : "Similar to previous work (Kingma and Welling, 2014; Rezende et al., 2014), we let qφ(z|x) be a multivariate Gaussian distribution with a diagonal covariance structure:",
      "startOffset" : 25,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "The feed forward model g′(·) (see the yellow arrows in Figure 2) and context vector cj = ∑ i αjihi (see the “⊕” in Figure 2) are the same as (Bahdanau et al., 2014).",
      "startOffset" : 141,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "The feed forward model g′(·) (see the yellow arrows in Figure 2) and context vector cj = ∑ i αjihi (see the “⊕” in Figure 2) are the same as (Bahdanau et al., 2014). The difference between our decoder and Bahdanau et al.’s decoder (2014) lies in that in addition to the context vector, our decoder integrates the representation of the latent variable, i.",
      "startOffset" : 142,
      "endOffset" : 238
    }, {
      "referenceID" : 0,
      "context" : "The initial hidden state s0 is initialized in the same way by Bahdanau et al. (2014) (see the arrow to s0 in Figure 2).",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "We employed the case-insensitive BLEU-4 metric (Papineni et al., 2002) to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : ", 2002) to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test.",
      "startOffset" : 71,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "• GroundHog (Bahdanau et al., 2014): the attentional NMT system.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "Following Bahdanau et al. (2014), we set dw = 620, df = 1000, de = 1000, and M = 80.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "Cho et al. (2014) further propose the Encoder-Decoder framework with two RNNs.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 24,
      "context" : "With regard to NMT, Sutskever et al. (2014) employ two multilayered Long Short-Term Memory (LSTM) models that first encode a source sentence into a single vector and then decode the translation word by word until a special end token is generated.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "In order to deal with issues caused by encoding all source-side information into a fixed-length vector, Bahdanau et al. (2014) introduce attentionbased NMT that aims at automatically concentrating on relevant source parts for predicting target words during decoding.",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "Following the success of attentional NMT, a number of approaches and models have been proposed for NMT recently, which can be grouped into different categories according to their motivations: dealing with rare words or large vocabulary (Jean et al., 2015; Luong et al., 2015b; Sennrich et al., 2015b), learning better attentional structures (Luong et al.",
      "startOffset" : 236,
      "endOffset" : 300
    }, {
      "referenceID" : 2,
      "context" : ", 2015a), integrating SMT techniques (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016), character-level NMT (Ling et al.",
      "startOffset" : 37,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : ", 2015a), integrating SMT techniques (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016), character-level NMT (Ling et al.",
      "startOffset" : 37,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : ", 2015a), integrating SMT techniques (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016), character-level NMT (Ling et al.",
      "startOffset" : 37,
      "endOffset" : 112
    }, {
      "referenceID" : 26,
      "context" : ", 2015a), integrating SMT techniques (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016), character-level NMT (Ling et al.",
      "startOffset" : 37,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : ", 2016), character-level NMT (Ling et al., 2015; Costa-Jussà and Fonollosa, 2016), the exploitation of monolingual corpora (Gulcehre et al.",
      "startOffset" : 29,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : ", 2015; Costa-Jussà and Fonollosa, 2016), the exploitation of monolingual corpora (Gulcehre et al., 2015; Sennrich et al., 2015a) and memory network (Meng et al.",
      "startOffset" : 82,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : ", 2015a) and memory network (Meng et al., 2015).",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "In order to perform efficient inference and learning in directed probabilistic models on large-scale dataset, Kingma and Welling (2014) as well as Rezende et al. (2014) introduce variational neural networks.",
      "startOffset" : 147,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : "In this respect, Kingma et al. (2014) revisit the approach to semi-supervised learning with generative models and further develop new models that allow effective generalization from a small labeled dataset to a large unlabeled dataset.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Chung et al. (2015) incorporate latent variables into the hidden state of a recurrent neural network, while Gregor et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "Chung et al. (2015) incorporate latent variables into the hidden state of a recurrent neural network, while Gregor et al. (2015) combine a novel spatial attention mechanism that mimics the foveation of human eyes, with a sequential variational auto-encoding framework that allows the iterative construction of complex images.",
      "startOffset" : 0,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Chung et al. (2015) incorporate latent variables into the hidden state of a recurrent neural network, while Gregor et al. (2015) combine a novel spatial attention mechanism that mimics the foveation of human eyes, with a sequential variational auto-encoding framework that allows the iterative construction of complex images. Very recently, Miao et al. (2015) propose a generic variational inference framework for generative and conditional models of text.",
      "startOffset" : 0,
      "endOffset" : 360
    }, {
      "referenceID" : 1,
      "context" : "The most related work to ours is that of Bowman et al. (2015), where they develop a variational autoencoder for unsupervised generative language modeling.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "In the future, since the latent variable in our model is at the sentence level, we want to explore more fine-grained latent variables for neural machine translation, such as the Recurrent Latent Variable Model (Chung et al., 2015).",
      "startOffset" : 210,
      "endOffset" : 230
    } ],
    "year" : 2017,
    "abstractText" : "Models of neural machine translation are often from a discriminative family of encoder-decoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoder-decoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform an efficient posterior inference, we build a neural posterior approximator that is conditioned only on the source side. Additionally, we employ a reparameterization technique to estimate the variational lower bound so as to enable standard stochastic gradient optimization and large-scale training for the variational model. Experiments on NIST Chinese-English translation tasks show that the proposed variational neural machine translation achieves significant improvements over both stateof-the-art statistical and neural machine translation baselines.",
    "creator" : "LaTeX with hyperref package"
  }
}