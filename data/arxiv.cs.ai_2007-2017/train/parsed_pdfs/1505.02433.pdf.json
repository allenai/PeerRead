{
  "name" : "1505.02433.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probabilistic Belief Embedding for Knowledge Base Completion",
    "authors" : [ "Miao Fan", "Qiang Zhou", "Andrew Abel", "Thomas Fang Zheng", "Ralph Grishman" ],
    "emails" : [ "fanmiao.cslt.thu@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper contributes a novel embedding model which measures the probability of each candidate belief 〈h, r, t,m〉 in a large-scale knowledge repository via simultaneously learning distributed representations for entities (h and t), relations (r), and even the words in relation mentions (m). It facilitates knowledge completion by means of simple vector operations to discover new beliefs. Given an imperfect belief, we can not only infer the missing entities, predict the unknown relations, but also tell the plausibility of that belief, just by exploiting the learnt embeddings of available evidence. To demonstrate the scalability and the effectiveness of our model, we conduct experiments on several large-scale repositories which contain hundreds of thousands of beliefs from WordNet, Freebase and NELL, and compare the results of a number of tasks, entity inference, relation prediction and triplet classification, with cutting-edge approaches. Extensive experimental results show that the proposed model outperforms other state-of-the-art methods, with significant improvements identified."
    }, {
      "heading" : "1 Introduction",
      "text" : "Information extraction (Sarawagi, 2008; Grishman, 1997), the study of extracting structured beliefs from unstructured online texts to populate knowledge bases, has drawn much attention in recent years because of the explosive growth in the number of web pages. Thanks to long-term efforts of experts, crowd sourcing, and even machine learning techniques, several web-scale knowledge repositories, such as\nWordnet1, Freebase2 and NELL3, have been constructed. WordNet (Miller, 1995) and Freebase (Bollacker et al., 2007; Bollacker et al., 2008) follow the RDF format that represents each belief as a triplet, i.e. 〈head entity, relation, tail entity〉. NELL (Carlson et al., 2010a) extends each triplet with a relation mention which is a snatch of extracted free text to indicate the corresponding relation. Here we take a belief recorded in NELL as an example: 〈city : caroline, citylocatedinstate, stateorprovince : maryland, county and state of〉 , in which county and state of is the mention between the head entity city : caroline and the tail entity stateorprovince : maryland, to indicate the relation citylocatedinstate. In some cases, NELL also provides the confidence of each belief automatically learnt by machines.\nAlthough colossal quantities of beliefs have been gathered, state-of-the-art work (West et al., 2014) reports that our knowledge bases are far from complete. For instance, nearly 97% of people in the Freebase database have no records about their parents, whereas we can still find clues as to their immediate family in many cases by searching on the web and looking up their Wiki.\nTo populate the incomplete knowledge repositories, scientists either compare the performance of relation extraction between two named entities on manually annotated text datasets, such as ACE4 and MUC5, or look for effective approaches for improving the accuracy of link prediction within the knowledge graphs constructed by the repositories without using extra free texts.\nRecently, studies on text-based knowledge\n1http://wordnet.princeton.edu/ 2https://www.freebase.com/ 3http://rtw.ml.cmu.edu/rtw/ 4http://www.itl.nist.gov/iad/mig/\ntests/ace/ 5http://www.itl.nist.gov/iaui/894.02/ relatedprojects/muc/\nar X\niv :1\n50 5.\n02 43\n3v 4\n[ cs\n.A I]\n2 2\nM ay\n2 01\n5\ncompletion have benefited significantly from a paradigm known as Distantly Supervised Relation Extraction (Mintz et al., 2009) (DSRE), which bridges the gap between structured knowledge bases and unstructured free texts. It alleviates the labor of manual annotation by means of automatically aligning each triplet 〈h, r, t〉 from knowledge bases to the corresponding relation mention m in free texts. However state-of-the-art research (Fan et al., 2014a) points out that DSRE still suffers from the problem of sparse and noisy features. Although Fan et al. (2014a) fix the issue to some extent by making use of low-dimensional matrix factorization, their approach was identified as unable to handle large-scale datasets.\nFortunately, knowledge embedding techniques (Bordes et al., 2011; Bordes et al., 2014) enable us to encode the high-dimensional sparse features into low-dimensional distributed representations. A simple but effective model is TransE (Bordes et al., 2013), which trains a vector representation for each entity and relation in large-scale knowledge bases without considering any text information. Even though Weston et al. (Weston et al., 2013), Wang et al. (Wang et al., 2014a) and Fan et al. (Fan et al., 2015a) broaden this field by adding word embeddings, there is still no comprehensive and elegant model that can integrate such largescale heterogeneous resources to satisfy multiple subtasks of knowledge completion including entity inference, relation prediction and triplet classification.\nTherefore, the contribution of this paper is a proposed novel embedding model which measures the probability of each belief 〈h, r, t,m〉 in largescale repositories. It breaks through the limitation of heterogeneous data, and establishes the connection between structured knowledge graphs and unstructured free texts. The distributed representations for entities (h and t), relations (r), as well as the words in relation mentions (m) are simultaneously learnt within the uniform framework of the probabilistic belief embedding (PBE) we propose. Knowledge completion is facilitated by means of simple vector operations to discover new beliefs. Given an imperfect belief, we can not only infer the missing entities, predict the unknown relations, but tell the plausibility of the belief as well, just by means of the learnt vector representations of available data. To prove the effectiveness and the scalability of PBE, we perform extensive\nexperiments with multiple tasks, including entity inference, relation prediction and triplet classification, for knowledge completion, and evaluate both our model and other cutting-edge approaches with appropriate metrics on several large-scale datasets which contain hundreds of thousands of beliefs from WordNet, Freebase and NELL. Detailed comparison results demonstrate that the proposed model outperforms other state-of-the-art approaches with significant improvements identified."
    }, {
      "heading" : "2 Related Work",
      "text" : "Embedding-based inference models usually design various scoring functions fr(h, t) to measure the plausibility of a triplet 〈h, r, t〉. The lower the dissimilarity of the scoring function fr(h, t) is, the higher the compatibility of the triplet will be.\nUnstructured (Bordes et al., 2013) is a naive model which exploits the occurrence information of the head and the tail entities without considering the relation between them. It defines a scoring function ||h − t||, and this model obviously can not discriminate between a pair of entities involving different relations. Therefore, Unstructured is commonly regarded as the baseline approach.\nDistance Model (SE) (Bordes et al., 2011) uses a pair of matrices (Wrh,Wrt), to characterize a relation r. The dissimilarity of a triplet is calculated by ||Wrhh−Wrtt||1. As identified by Socher et al. (Socher et al., 2013), the separating matrices Wrh and Wrt weaken the capability of capturing correlations between entities and corresponding relations, despite the model taking the relations into consideration.\nSingle Layer Model, proposed by Socher et al. (Socher et al., 2013) thus aims to alleviate the shortcomings of the Distance Model by means of the non-linearity of a single layer neural network g(Wrhh + Wrtt + br), in which g = tanh. The linear output layer then gives the scoring function: uTr g(Wrhh +Wrtt + br).\nBilinear Model (Sutskever et al., 2009; Jenatton et al., 2012) is another model that tries to fix the issue of weak interaction between the head and tail entities caused by the Distance Model with a relation-specific bilinear form: fr(h, t) = hTWrt.\nNeural Tensor Network (NTN) (Socher et al., 2013) designs a general scoring function: fr(h, t) = u T r g(h\nTWrt + Wrhh + Wrtt + br), which combines the Single Layer and Bilinear\nModels. This model is more expressive as the second-order correlations are also considered in the non-linear transformation function, but the computational complexity is rather high.\nTransE (Bordes et al., 2013) is a canonical model different from all the other prior arts, which embeds relations into the same vector space as entities by regarding the relation r as a translation from h to t, i.e. h+r = t. It works well on beliefs with the ONE-TO-ONE mapping property but performs badly with multi-mapping beliefs. Given a series of facts associated with a ONE-TO-MANY relation r, e.g. 〈h, r, t1〉, 〈h, r, t2〉, ..., 〈h, r, tm〉, TransE tends to represent the embeddings of entities on the MANY-side as extremely close to each other with very little discrimination.\nTransM (Fan et al., 2014b) exploits the structure of the whole knowledge graph, and adjusts the learning rate, which is specific to each relation based on the multiple mapping property of the relation.\nTransH (Wang et al., 2014b) is, to the best knowledge of the authors, the state of the art approach. It improves TransE by modeling a relation as a hyperplane, which makes it more flexible with regard to modeling beliefs with multimapping properties.\nDue to the diverse feature spaces between unstructured texts and structured beliefs, one key challenge of connecting natural language and knowledge is to project the features into the same space and to merge them together for knowledge completion. Fan et al. (Fan et al., 2015a) have recently proposed JRME to jointly learn the embedding representations for both relations and mentions in order to predict unknown relations between entities in NELL. However, the functionality of their latest method is limited to the relation prediction task (see section 5.3), as the correlations between entities and relations are ignored. Therefore, we desire a comprehensive model that can simultaneously consider entities, relations and even the relation mentions, and can integrate the heterogeneous resources to support multiple subtasks of knowledge completion, such as entity inference, relation prediction and triplet classification."
    }, {
      "heading" : "3 Model",
      "text" : "The intuition of the subsequent theory is that: Not each belief we have learnt, i.e.\n〈head entity, relation, tail entity,mention〉 abbreviated as 〈h, r, t,m〉, is perfect and complete enough (Fan et al., 2015b). We thus explore modeling the probability of each belief, i.e. Pr(h, r, t,m). It is assumed that Pr(h, r, t,m) is collaboratively influenced by Pr(h|r, t), Pr(t|h, r) and Pr(r|h, t,m), where Pr(h|r, t) stands for the conditional probability of inferring the head entity h given the relation r and the tail entity t, Pr(t|h, r) represents the conditional probability of inferring the tail entity t given the head entity h and the relation r, and Pr(r|h, t,m) denotes the conditional probability of predicting the relation r between the head entity h and the tail entity t with the relation mention m extracted from free texts. Therefore, we define that the probability of a belief equals to the geometric mean of Pr(h|r, t)Pr(r|h, t,m)Pr(t|h, r) as shown in the subsequent equation,\nPr(h, r, t,m) = 3 √ Pr(h|r, t)Pr(r|h, t,m)Pr(t|h, r).\n(1) We assume that we have a certain repository ∆, such as WordNet, which contains thousands of beliefs validated by experts. The learning object is intuitively set to maximize Lmax, where\nLmax = ∏\n〈h,r,t,m〉∈∆\nPr(h, r, t,m). (2)\nIn most cases, we can automatically build much larger but imperfect knowledge bases as well via crowdsouring (Freebase) and machine learning techniques (NELL). However, each belief of NELL has a confidence-weighted score c to indicate its plausibility to some extent. Therefore, we propose an alternative goal which aims to minimize Lmin, in which\nLmin = ∏\n〈h,r,t,m,c〉∈∆\n1 2 [Pr(h, r, t,m)− c]2. (3)\nTo facilitate the optimization progress, we prefer using the log likelihood ofLmax andLmin, and the learning targets can be further processed as follows,\narg max h,r,t,m\nlogLmax\n= arg max h,r,t,m ∑ 〈h,r,t,m〉∈∆ logPr(h, r, t,m)\n= arg max h,r,t,m ∑ 〈h,r,t,m〉∈∆ 1 3 [logPr(h|r, t)\n+ logPr(r|h, t,m) + logPr(t|h, r)];\n(4)\narg min h,r,t,m\nlogLmin\n= arg min h,r,t,m ∑ 〈h,r,t,m,c〉∈∆ 1 2 [logPr(h, r, t,m)− log c]2\n= arg min h,r,t,m ∑ 〈h,r,t,m,c〉∈∆ 1 2 {1 3 [logPr(h|r, t)\n+ logPr(r|h, t,m) + logPr(t|h, r)]− log c}2. (5)\nThe advantage of the conversions above is that we can separate the factors out, compared with Equation (1). Therefore, the remaining challenge is to identify the approaches to use to model Pr(h|r, t), Pr(r|h, t,m) and Pr(t|h, r). Pr(r|h, t,m) leverages the evidences from two different resources to predict the relation. If the concurrence of the two entities (h and t) in knowledge bases is independent of the appearance of the relation mention m from free texts, we can factorize Pr(r|h, t,m) as shown by Equation (6):\nPr(r|h, t,m) = Pr(r|h, t)Pr(r|m). (6)\nWe then need to consider formulating Pr(h|r, t), Pr(r|h, t), Pr(t|h, r) and Pr(r|m), respectively.\nFigure 1(a) illustrates the traditional way of recording knowledge as triplets. The triplets 〈h, r, t〉 can construct a knowledge graph in which entities (h and t) are nodes and the relation (r) between them is a directed edge from the head entity (h) to the tail entity (t). This kind of symbolic representation, whilst being very efficient for storing, is not flexible enough for statistical learning\napproaches (Bordes et al., 2011). However, once each of the elements, including entities and relations in the knowledge repository, are projected into the same embedding space, we can use:\nD(h, r, t) = −||h + r− t||+ α, (7)\nwhich is a simple vector operation to measure the distance between h + r and t, where h, r and t are encoded in d dimensional vectors, and α is the bias parameter. To estimate the conditional probability of appearing t given h and r, i.e. Pr(t|h, r), however, we need to adopt the softmax function as follows,\nPr(t|h, r) = exp D(h,r,t)∑\nt′∈Et exp D(h,r,t′)\n, (8)\nwhere Et is the set of tail entities which contains all possible entities t′ appearing in the tail position. Similarly, we can regard Pr(h|r, t) and Pr(r|h, t) as\nPr(h|r, t) = exp D(h,r,t)∑\nh′∈Eh exp D(h′,r,t)\n(9)\nand\nPr(r|h, t) = exp D(h,r,t)∑\nr′∈R exp D(h,r′,t)\n, (10)\nin which Eh is the set of head entities which contains all possible entities h′ appearing in the head position, and R is the set of all candidate relations r′.\nIn addition to this, Figure 1(c) shows that free texts can provide fruitful contexts between two recognized entities, but the one-hot6 feature space\n6http://en.wikipedia.org/wiki/One-hot\nis rather high and sparse. Therefore, we can also project each words in relation ’mentions’ into the same embedding space of entities and relations. To measure the similarity between the mention m and the corresponding relation r, we adopt the inner product of their embeddings as shown by Equation (11),\nF (r,m) = WTφ(m)r + β, (11)\nwhere W is the matrix of Rnv×d containing nv vocabularies with d dimensional embeddings, φ(m) is the sparse one-hot representation of the mention indicating the absence or presence of words, r ∈ Rd is the embedding of relation r, and β is the bias parameter. Similar to Equations (8), (9) and (10), the conditional probability of predicting relation r given mention m, i.e. Pr(r|m) can be defined as,\nPr(r|m) = exp F (r,m)∑\nr′∈R exp F (r′,m)\n. (12)\nOverall, this section has shown that we can model the probability of a belief via jointly embedding the entities, relations and even the words in mentions, as demonstrated by Figure 1(b)."
    }, {
      "heading" : "4 Algorithm",
      "text" : "To search for the optimal solutions of Equation (4) and (5), we can use Stochastic Gradient Descent (SGD) to update the embeddings of entities, relations and words of mentions in iterative fashion. However, it is computationally intensive to calculate the normalization terms in Pr(h|r, t), Pr(r|h, t), Pr(t|h, r) and Pr(r|m) according to the definitions made by Equation (8), (9), (10) and (12) respectively. For instance, if we directly calculate the value of Pr(h|r, t) for just one belief, tens of thousands expD(h\n′,r,t) need to be re-valued, as there are tens of thousands of candidate entities h′ in Eh. Inspired by the work of Mikolov et al. (Mikolov et al., 2013), we have developed an efficient approach that adopts the negative sampling technique to approximate the conditional probability functions, i.e. Equations (8), (9), (10) and (12), by being transformed to binary classification problems as shown respectively by the subsequent equations,\nlogPr(h|r, t) ≈ logPr(1|h, r, t) + k∑\ni=1\nEh′iPr(h′∈Eh) logPr(0|h ′ i, r, t),\n(13)\nlogPr(t|h, r) ≈ logPr(1|h, r, t)\n+ k∑ i=1 Et′iPr(t′∈Et) logPr(0|h, r, t ′ i),\n(14)\nlogPr(r|h, t) ≈ logPr(1|h, r, t)\n+ k∑ i=1 Er′iPr(r′∈R) logPr(0|h, r ′ i, t),\n(15)\nlogPr(r|m) ≈ logPr(1|r,m)\n+ k∑ i=1 Er′iPr(r′∈R) logPr(0|r ′ i,m),\n(16)\nwhere we sample k negative beliefs and discriminate them from the positive case. For the simple binary classification problems mentioned above, we choose the logistic function with the offset shown in Equation (17) to estimate the probability that the given triplet 〈h, r, t〉 is correct:\nPr(1|h, r, t) = 1 1 + exp−D(h,r,t) + , (17)\nand with the offset η shown in Equation (18) to tell the probability of the occurrence of r and m:\nPr(1|r,m) = 1 1 + exp−F (r,m) + η. (18)"
    }, {
      "heading" : "5 Experiment",
      "text" : "Besides its access to the efficient SGD algorithm, the learnt embeddings by PBE can contribute more effectiveness on multiple subtasks of knowledge completion, such as entity inference, relation prediction, and triplet classification."
    }, {
      "heading" : "5.1 Dataset",
      "text" : "To demonstrate the wide adaptability and significant effectiveness of our approach, we prepare three datasets as shown by Table 1, i.e. NELL50K, WN-100K, FB-500K from the repositories of NELL (Carlson et al., 2010b), WordNet (Miller, 1995) and Freebase (Bollacker et al., 2007; Bollacker et al., 2008) respectively. The NELL (Mitchell et al., 2015) designed and maintained by Carnegie Mellon University is an outstanding system which runs 24 hours/day and never stops learning the beliefs on the Web. We use a relatively small dataset NELL-50K which contains about 50 thousand confidence-weighted beliefs from NELL. Each belief of NELL-50K has a relation mention m in addition to a triplet 〈h, r, t〉. WN-100K is made by experts, and owns only 11\nkinds of relations but much more entities. Therefore, it is a sparse repository in which fewer entities have connections. The third dataset (FB500K7) we adopt is released by Bordes et al. (Bordes et al., 2013). It is a large crowdsourcing dataset extracted from Freebase, in which each belief is a triplet without a confidence score.\nAs the words in relation mentions will be further concerned in the relation prediction subtask, we also show the vocabulary size of each dataset. However, WN-100K and FB-500K only contain triplets as beliefs, so their vocabulary sizes are null.\nFor the triplet classification subtask, the head or the tail entity can be randomly replaced with another one to produce a negative training example. But in order to build much tough validation and testing datasets, we constrain that the picked entity should once appear at the same position. For example, (Pablo Picaso, nationality, U.S.) is a potential negative example rather than the obvious nonsense (Pablo Picaso, nationality, Van Gogh), given a positive triplet (Pablo Picaso, nationality, Spain)."
    }, {
      "heading" : "5.2 Entity inference",
      "text" : "One of the benefits of knowledge embedding is that simple vector operations can apply to entity inference which contributes to knowledge graph completion. Given a wrecked triplet, like 〈h, r, ?〉 or 〈?, r, t〉, the subtask needs to compute the arg maxh∈Eh Pr(h|r, t), with the help of the entity and relation embeddings. In the meanwhile, arg maxt∈Et Pr(t|h, r) will help us to find the\n7We change the original name of the dataset (FB15K), so as to follow the naming conventions in our paper. Related studies on this dataset can be looked up from the website https://www.hds.utc.fr/everest/ doku.php?id=en:transe\nbest tail entity given the head entity h and the relation r."
    }, {
      "heading" : "5.2.1 Metric",
      "text" : "For each testing belief, all the other entities that appear in the training set take turns to replace the head entity. Then we get a bunch of candidate triplets. The plausibility of each candidate triplet is firstly computed by various scoring functions, such as Pr(h|r, t) in PBE, and then sorted in ascending order. Finally, we locate the ground-truth triplet and record its rank. This whole procedure runs in the same way when replacing the tail entity, so that we can gain the mean results. We use two metrics, i.e. Mean Rank and Mean Hit@10 (the proportion of ground truth triplets that rank in Top 10), to measure the performance. However, the results measured by those metrics are relatively raw, as the procedure above tends to generate false negative triplets. In other words, some of the candidate triplets rank rather higher than the ground truth triplet just because they also appear in the training set. We thus filter out those triplets to report more reasonable results."
    }, {
      "heading" : "5.2.2 Performance",
      "text" : "We compare PBE with the state-of-the-art TransH (Wang et al., 2014b), TransM (Fan et al., 2014b), TransE (Bordes et al., 2013) mentioned in Section 2 evaluated on NELL-50K, WN-100K and FB-500K datasets. We tune the parameters of each previous model based on the validation set, and select the combination of parameters which leads to the best performance. Table 2, 3 and 4 demonstrate that PBE outperforms the prior arts on almost all the metrics. Overall, it achieves significant improvements (relative increment) on all three datasets, with NELL-50K: {Mean Rank Raw: 4.9% ⇑, Hit@10 Raw: 4.2% ⇑, Mean Rank\nFilter: 3.7% ⇑, Hit@10 Filter: 8.3% ⇓}, WN100K: {Mean Rank Raw: 20.3% ⇑, Hit@10 Raw: 136.8% ⇑, Mean Rank Filter: 20.5% ⇑, Hit@10 Filter: 146.3% ⇑} and FB-500K: {Mean Rank Raw: 15.8% ⇑, Hit@10 Raw: 27.3% ⇑, Mean Rank Filter: 13.3% ⇑, Hit@10 Filter: 10.4% ⇑}."
    }, {
      "heading" : "5.3 Relation prediction",
      "text" : "The scenario of this subtask is that: given a pair of entities and the text mentions indicating the semantic relations between them, i.e. 〈h, ?, t,m〉, this subtask computes the arg maxr∈R Pr(r|h, t)Pr(r|m) to predict the best relations."
    }, {
      "heading" : "5.3.1 Metric",
      "text" : "We compare the performances between our models and other state-of-the-art approaches, with the metrics as follows,\nAverage Rank: Each candidate relation will gain a score calculated by Equation (7). We sort them in ascent order and compare with the corresponding ground-truth belief. For each belief in the testing set, we get the rank of the correct relation. The average rank is an aggregative indicator, to some extent, to judge the overall performance on relation extraction of an approach.\nHit@10: Besides the average rank, scientists from the industrials concern more about the accuracy of extraction when selecting Top10 relations. This metric shows the proportion of beliefs that we predict the correct relation ranked in Top10.\nHit@1: It is a more strict metric that can be referred by automatic system, since it demonstrates the accuracy when just picking the first predicted relation in the sorted list."
    }, {
      "heading" : "5.3.2 Performance",
      "text" : "Table 5 illustrates the results of experiments on relation prediction with all the three datasets, respectively. We find out that text mentions within the NELL-50K contribute a lot on predicting the correct relations. All of results show that PBE performs best compared with the latest approaches. The relative increments are NELL-50K: {Mean Rank: 59.7% ⇑, Hit@10: 10.0% ⇑, Hit@1: 30.0% ⇑}, WN-100K: { Mean Rank: 41.1% ⇑, Hit@10: 0.1% ⇑, Hit@1: 276.2% ⇑ } and FB-500K: { Mean Rank: 95.7% ⇑, Hit@10: 148.2% ⇑, Hit@1: 327.6% ⇑ }."
    }, {
      "heading" : "5.4 Triplet classification",
      "text" : "Triplet classification is another inference related task proposed by Socher et al. (Socher et al., 2013) which focuses on searching a relation-specific threshold σr to identify whether a triplet 〈h, r, t〉 is plausible. If the probability of a testing triplet (h, r, t) computed by Pr(h|r, t)Pr(r|h, t)Pr(t|h, r) is below the relation-specific threshold σr, it is predicted as positive, otherwise negative."
    }, {
      "heading" : "5.4.1 Metric",
      "text" : "We use classification accuracy to measure the performances among the competing methods. Specifically, we sum up the correctness of each triplet 〈h, r, t〉 via comparing the probability of the triplet and the relation-specific threshold σr, which can be searched via maximizing the classification accuracy on the validation triplets which belong to the relation r."
    }, {
      "heading" : "5.4.2 Performance",
      "text" : "Compared with several of the latest approaches, i.e. TransH (Wang et al., 2014b), TransM(Fan et al., 2014b) and TransE (Bordes et al., 2013), the proposed PBE approach still outperforms them with the improvements that NELL-50K: {Accuracy: 7.9% ⇑}, WN-100K: {Accuracy: 5.6% ⇑} and FB-500K: {Accuracy: 5.6% ⇑}, as shown in Table 6."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper proposed an elegant probabilistic model to tackle the problem of embedding beliefs which contain both structured knowledge and unstructured free texts, by firstly measuring the probability of a given belief 〈h, r, t,m〉. To efficiently learn the embeddings for each entity, relation, and word in mentions, we also adopted the negative sampling technique to transform the original model and display the algorithm based on stochastic gradient descent to search for the optimal solution. Extensive knowledge completion experiments, including entity inference, relation prediction and triplet classification, showed that our approach achieves significant improvement when tested with three large-scale repositories, compared with other state-of-the-art methods."
    }, {
      "heading" : "Acknowledgement",
      "text" : "The paper is dedicated to all the members of CSLT8 and Proteus Group 9. It was supported by National Program on Key Basic Research Project (973 Program) under Grant 2013CB329304 and National Science Foundation of China (NSFC) under Grant No. 61373075, when the first author was a joint-supervision Ph.D. candidate of Tsinghua University and New York University."
    } ],
    "references" : [ {
      "title" : "Freebase: A shared database of structured general human knowledge",
      "author" : [ "Robert Cook", "Patrick Tufts" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Bollacker et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2007
    }, {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor" ],
      "venue" : "In Proceedings of the 2008 ACM SIGMOD international",
      "citeRegEx" : "Bollacker et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning structured embeddings of knowledge bases",
      "author" : [ "Jason Weston", "Ronan Collobert", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bordes et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2011
    }, {
      "title" : "Translating embeddings for modeling multi-relational data",
      "author" : [ "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "A semantic matching energy function for learning with multirelational data",
      "author" : [ "Xavier Glorot", "Jason Weston", "Yoshua Bengio" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "Toward an architecture for never-ending language learning",
      "author" : [ "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka Jr.", "Tom M. Mitchell" ],
      "venue" : "In Proceedings of the Twenty-Fourth",
      "citeRegEx" : "Carlson et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2010
    }, {
      "title" : "Toward an architecture for never-ending language learning",
      "author" : [ "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka Jr.", "Tom M. Mitchell" ],
      "venue" : "In Proceedings of the Twenty-Fourth Conference",
      "citeRegEx" : "Carlson et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2010
    }, {
      "title" : "http://cslt.riit.tsinghua.edu.cn/ http://nlp.cs.nyu.edu/index.shtml",
      "author" : [ "Fan et al.2014a] Miao Fan", "Deli Zhao", "Qiang Zhou", "Zhiyuan Liu", "Thomas Fang Zheng", "Edward Y" ],
      "venue" : null,
      "citeRegEx" : "Fan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2014
    }, {
      "title" : "Distant supervision for relation extraction with matrix completion",
      "author" : [ "Chang." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 839–849, Baltimore, Maryland, June.",
      "citeRegEx" : "Chang.,? 2014a",
      "shortCiteRegEx" : "Chang.",
      "year" : 2014
    }, {
      "title" : "Transition-based knowledge graph embedding with relational mapping properties",
      "author" : [ "Fan et al.2014b] Miao Fan", "Qiang Zhou", "Emily Chang", "Thomas Fang Zheng" ],
      "venue" : "In Proceedings of the 28th Pacific Asia Conference on Language,",
      "citeRegEx" : "Fan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2014
    }, {
      "title" : "Jointly embedding relations and mentions for knowledge population",
      "author" : [ "Fan et al.2015a] Miao Fan", "Kai Cao", "Yifan He", "Ralph Grishman" ],
      "venue" : "arXiv preprint arXiv:1504.01683",
      "citeRegEx" : "Fan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning embedding representations for knowledge inference on imperfect and incomplete repositories. arXiv preprint arXiv:1503.08155",
      "author" : [ "Fan et al.2015b] Miao Fan", "Qiang Zhou", "Thomas Fang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Fan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2015
    }, {
      "title" : "Information extraction: Techniques and challenges. In International Summer School on Information Extraction: A Multidisciplinary Approach to an Emerging Information Technology, SCIE",
      "author" : [ "Ralph Grishman" ],
      "venue" : null,
      "citeRegEx" : "Grishman.,? \\Q1997\\E",
      "shortCiteRegEx" : "Grishman.",
      "year" : 1997
    }, {
      "title" : "A latent factor model for highly multi-relational data",
      "author" : [ "Nicolas Le Roux", "Antoine Bordes", "Guillaume Obozinski" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Jenatton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jenatton et al\\.",
      "year" : 2012
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A. Miller" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Miller.,? \\Q1995\\E",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky" ],
      "venue" : "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint",
      "citeRegEx" : "Mintz et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Danqi Chen", "Christopher D Manning", "Andrew Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Modelling relational data using bayesian clustered tensor factorization",
      "author" : [ "Ruslan Salakhutdinov", "Joshua B Tenenbaum" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2009
    }, {
      "title" : "Knowledge graph and text jointly embedding",
      "author" : [ "Wang et al.2014a] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge graph embedding by translating on hyperplanes",
      "author" : [ "Wang et al.2014b] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen" ],
      "venue" : "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge base completion via search-based question answering",
      "author" : [ "West et al.2014] Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin" ],
      "venue" : null,
      "citeRegEx" : "West et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "West et al\\.",
      "year" : 2014
    }, {
      "title" : "Connecting language and knowledge bases with embedding models for relation extraction",
      "author" : [ "Weston et al.2013] Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods",
      "citeRegEx" : "Weston et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Information extraction (Sarawagi, 2008; Grishman, 1997), the study of extracting structured beliefs from unstructured online texts to populate knowledge bases, has drawn much attention in recent years because of the explosive growth in the number of web pages.",
      "startOffset" : 23,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "WordNet (Miller, 1995) and Freebase (Bollacker et al.",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "WordNet (Miller, 1995) and Freebase (Bollacker et al., 2007; Bollacker et al., 2008) follow the RDF format that represents each belief as a triplet, i.",
      "startOffset" : 36,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "WordNet (Miller, 1995) and Freebase (Bollacker et al., 2007; Bollacker et al., 2008) follow the RDF format that represents each belief as a triplet, i.",
      "startOffset" : 36,
      "endOffset" : 84
    }, {
      "referenceID" : 21,
      "context" : "Although colossal quantities of beliefs have been gathered, state-of-the-art work (West et al., 2014) reports that our knowledge bases are far from complete.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "completion have benefited significantly from a paradigm known as Distantly Supervised Relation Extraction (Mintz et al., 2009) (DSRE), which bridges the gap between structured knowledge bases and unstructured free texts.",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "However state-of-the-art research (Fan et al., 2014a) points out that DSRE still suffers from the problem of sparse and noisy features. Although Fan et al. (2014a) fix the issue to some extent by making use of low-dimensional matrix factorization, their approach was identified as unable to handle large-scale datasets.",
      "startOffset" : 35,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : "Fortunately, knowledge embedding techniques (Bordes et al., 2011; Bordes et al., 2014) enable us to encode the high-dimensional sparse features into low-dimensional distributed representations.",
      "startOffset" : 44,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "Fortunately, knowledge embedding techniques (Bordes et al., 2011; Bordes et al., 2014) enable us to encode the high-dimensional sparse features into low-dimensional distributed representations.",
      "startOffset" : 44,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "A simple but effective model is TransE (Bordes et al., 2013), which trains a vector representation for each entity and relation in large-scale knowledge bases without considering any text information.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "(Weston et al., 2013), Wang et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "Unstructured (Bordes et al., 2013) is a naive model which exploits the occurrence information of the head and the tail entities without considering the relation between them.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "Distance Model (SE) (Bordes et al., 2011) uses a pair of matrices (Wrh,Wrt), to characterize a relation r.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "(Socher et al., 2013), the separating matrices Wrh and Wrt weaken the capability of capturing correlations between entities and corresponding relations, despite the model taking the relations into consideration.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 17,
      "context" : "(Socher et al., 2013) thus aims to alleviate the shortcomings of the Distance Model by means of the non-linearity of a single layer neural network",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : "Bilinear Model (Sutskever et al., 2009; Jenatton et al., 2012) is another model that tries to fix the issue of weak interaction between the head and tail entities caused by the Distance Model with a relation-specific bilinear form: fr(h, t) = hWrt.",
      "startOffset" : 15,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "Bilinear Model (Sutskever et al., 2009; Jenatton et al., 2012) is another model that tries to fix the issue of weak interaction between the head and tail entities caused by the Distance Model with a relation-specific bilinear form: fr(h, t) = hWrt.",
      "startOffset" : 15,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "Neural Tensor Network (NTN) (Socher et al., 2013) designs a general scoring function: fr(h, t) = u T r g(h Wrt + Wrhh + Wrtt + br), which combines the Single Layer and Bilinear",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "TransE (Bordes et al., 2013) is a canonical model different from all the other prior arts, which embeds relations into the same vector space as entities by regarding the relation r as a translation from h to t, i.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "This kind of symbolic representation, whilst being very efficient for storing, is not flexible enough for statistical learning approaches (Bordes et al., 2011).",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 14,
      "context" : "(Mikolov et al., 2013), we have developed an efficient approach that adopts the negative sampling technique to approximate the conditional probability functions, i.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : ", 2010b), WordNet (Miller, 1995) and Freebase (Bollacker et al.",
      "startOffset" : 18,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : ", 2010b), WordNet (Miller, 1995) and Freebase (Bollacker et al., 2007; Bollacker et al., 2008) respectively.",
      "startOffset" : 46,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : ", 2010b), WordNet (Miller, 1995) and Freebase (Bollacker et al., 2007; Bollacker et al., 2008) respectively.",
      "startOffset" : 46,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "(Bordes et al., 2013).",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : ", 2014b), TransE (Bordes et al., 2013) mentioned in Section 2 evaluated on NELL-50K, WN-100K and FB-500K datasets.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "(Socher et al., 2013) which focuses on searching a relation-specific threshold σr to identify whether a triplet 〈h, r, t〉 is plausible.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : ", 2014b) and TransE (Bordes et al., 2013), the proposed PBE approach still outperforms them with the improvements that NELL-50K: {Accuracy: 7.",
      "startOffset" : 20,
      "endOffset" : 41
    } ],
    "year" : 2015,
    "abstractText" : "This paper contributes a novel embedding model which measures the probability of each candidate belief 〈h, r, t,m〉 in a large-scale knowledge repository via simultaneously learning distributed representations for entities (h and t), relations (r), and even the words in relation mentions (m). It facilitates knowledge completion by means of simple vector operations to discover new beliefs. Given an imperfect belief, we can not only infer the missing entities, predict the unknown relations, but also tell the plausibility of that belief, just by exploiting the learnt embeddings of available evidence. To demonstrate the scalability and the effectiveness of our model, we conduct experiments on several large-scale repositories which contain hundreds of thousands of beliefs from WordNet, Freebase and NELL, and compare the results of a number of tasks, entity inference, relation prediction and triplet classification, with cutting-edge approaches. Extensive experimental results show that the proposed model outperforms other state-of-the-art methods, with significant improvements identified.",
    "creator" : "LaTeX with hyperref package"
  }
}