{
  "name" : "1606.01868.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unifying Count-Based Exploration and Intrinsic Motivation",
    "authors" : [ "Marc G. Bellemare", "Sriram Srinivasan", "Georg Ostrovski" ],
    "emails" : [ "bellemare@google.com", "srsrinivasan@google.com", "ostrovski@google.com", "schaul@google.com", "saxton@google.com", "munos@google.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Efficient exploration is fundamentally about managing uncertainties. In reinforcement learning, these uncertainties pertain to the unknown nature of the reward and transition functions.1 The traditional unit of frequentist certainty is undoubtedly the count: an integer describing the number of observations of a certain type. Most Bayesian exploration schemes use counts in their posterior estimates, for example when the uncertainty over transition probabilities is captured by an exponential family prior (Dearden et al., 1998; Duff, 2002; Poupart et al., 2006).\nIn simple domains, efficient exploration is theoretically well-understood, if not yet solved. There are near-optimal algorithms for multi-armed bandits (Bubeck and Cesa-Bianchi, 2012); the hardness of exploration in Markov Decision Processes (MDPs) is well-understood (Jaksch et al., 2010; Lattimore and Hutter, 2012). By stark contrast, contemporary practical successes in reinforcement learning (e.g. Mnih et al., 2015; Silver et al., 2016) still rely on simple forms of exploration, for example -greedy policies – what Thrun (1992) calls undirected exploration.\nAt the heart of the theory–practice disconnect is the problem of counting in large state spaces. To achieve efficient exploration, current algorithms use counts to build (or assume) confidence intervals around empirical estimates. In large state spaces, however, empirical counts provide little to no traction: few, if any states are visited more than once. Consequently, some generalization is required. However, save for some recent work in continuous state spaces for which a good metric is given (Pazis and Parr, 2016), there has not yet been a thoroughly convincing attempt at generalizing counts.\nIntrinsic motivation (Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013) offers a different perspective on exploration. Intrinsic motivation (IM) algorithms typically use novelty signals – sur-\n1We will not discuss partial observability in this paper.\nar X\niv :1\n60 6.\n01 86\n8v 1\n[ cs\n.A I]\n6 J\nun 2\nrogates for extrinsic rewards – to drive curiosity within an agent, influenced by classic ideas from psychology (White, 1959). To sketch out some recurring themes, these novelty signals might be prediction error (Singh et al., 2004; Stadie et al., 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015). The idea also finds roots in continual learning (Ring, 1997). In Thrun’s taxonomy, intrinsic motivation methods fall within the category of error-based exploration. However, while in toto a promising approach, intrinsic motivation has so far yielded few theoretical guarantees, save perhaps for the work of Maillard (2012) and the universally curious agent of Orseau et al. (2013).\nWe provide what we believe is the first formal evidence that intrinsic motivation and count-based exploration are but two sides of the same coin. Our main result is to derive a pseudo-count from a sequential density model over the state space. We only make the weak requirement that such a model should be learning-positive: observing x should not immediately decrease its density. In particular, counts in the usual sense correspond to the pseudo-counts implied by the data’s empirical distribution. We expose a tight relationship between the pseudo-count, a variant of Schmidhuber’s compression progress which we call prediction gain, and Bayesian information gain.\nThe pseudo-counts we introduce here are best thought of as “function approximation for exploration”. We bring them to bear on Atari 2600 games from the Arcade Learning Environment (Bellemare et al., 2013), focusing on games where myopic exploration fails. We extract our pseudo-counts from a simple sequential model and use them within a variant of model-based interval estimation with bonuses (Strehl and Littman, 2008). We apply them to both an experience replay setting and an actor-critic setting, and find improved performance in both cases. Our approach produces dramatic progress on the reputedly most difficult Atari 2600 game, MONTEZUMA’S REVENGE: within a fraction of the training time, our agent explores a significant portion of the first level and obtains significantly higher scores than previously published agents."
    }, {
      "heading" : "2 Notation",
      "text" : "We consider an alphabet X . This alphabet may be finite, e.g. the binary alphabet, all grayscale images of certain fixed dimensions, or the set of English letters; or it may be countable, e.g. the set of English sentences. We denote a sequence of length n from this alphabet by x1:n ∈ Xn, the set of finite sequences from X by X ∗, write x1:nx to mean the concatenation of x1:n and a symbol x ∈ X , and denote the empty string by .\nA sequential density model over X is a mapping from X ∗ to probability distributions over X . That is, for each x1:n ∈ Xn the model provides a probability distribution denoted\nρn(x) := ρ(x ; x1:n).\nWhen the model induces a probability distribution over sequences from X , rather than simply a mapping from sequences to distributions, we may understand ρn(x) to be the usual conditional probability of Xn+1 = x given X1 . . . Xn = x1:n. If further ρ(x1:n) > 0 for all sequences which the environment might produce, we appeal to the chain rule and write\nρ(x1:n) = n∏ t=1 ρ(xt |x1:t−1) ⇐⇒ ρ(xt |x1:t−1) = ρ(x1:t) ρ(x1:t−1) .\nWe will call such a model a universal (sequential density) model.\nOur yardstick will be the empirical distribution µn derived from the sequence x1:n. If Nn(x) := N(x, x1:n) is the number of occurrences of a symbol x in the sequence x1:n, then\nµn(x) := µ(x ; x1:n) := Nn(x)\nn .\nWe call the Nn the empirical count function, or simply empirical count. Note that µn is a sequential density model but not a universal model.\nWe say that X is a factored alphabet if it is the Cartesian product of k sub-alphabets, i.e. X := X1 × · · · × Xk. We then write the ith factor of a symbol x ∈ X as xi, and write the sequence of the ith factor across x1:n as xi1:n. While the concept of a factored alphabet easily extends to countable products of sub-alphabets (e.g. English sentences), in the interest of clarity we set aside this case for the time being."
    }, {
      "heading" : "2.1 Markov Decision Processes and models over joint alphabets.",
      "text" : "In the sequel, the alphabet X will sometimes play the role of the state space in a Markov Decision Process (X ,A, P,R, γ) with action set A, transition function P , reward function R and discount factor γ. The need will naturally arise for a sequential density model over a joint alphabet X × Y , where Y may be a) the action setA or b) the setZ of achievable finite horizon returns (undiscounted sums of rewards). When Y is small and finite and we are given a sequential density model ρ over X , we can construct a reasonable sequential density model over the joint alphabet X ×Y by means of a chain rule-like construct (Veness et al., 2015). For y ∈ Y , y1:n ∈ Yn define the index set\nσn(y) := σ(y ; y1:n) := {t : yt = y},\nand write xσn(y) := (xt : t ∈ σn(y)) for the subsequence of x1:n for which yt = y. We construct the model\nρ(x, y ; x1:n, y1:n) := ρ(x ; xσn(y))µY(y ; y1:n),\nwhere µY is the empirical distribution over Y . Thus to model pairs (x, y) we partition x1:n into disjoint subsequences according to the values taken in y1:n, and use a separate copy of the model on each subsequence.\nIf ρ is a universal model, we may find it convenient to replace the empirical distribution over Y by a universal estimator such as the Dirichlet-multinomial; the resulting joint distribution is then a universal model over X × Y ."
    }, {
      "heading" : "2.2 Count-based exploration.",
      "text" : "Count-based exploration algorithms use the state-action visit countNn(x, a) to explore according to the principle of optimism in the face of uncertainty. We now review two such algorithms to illustrate how visit counts can be used to drive exploration.\nModel-based interval estimation (MBIE; Strehl and Littman, 2008) explores by acting optimistically with respect to the agent’s empirical model of the environment. The algorithm maintains the empirical estimates P̂ and R̂ of the transition and reward functions, respectively. The optimism takes the form of an L1 confidence interval over both P̂ and R̂; this confidence interval is derived from Nn(x, a). A closely related algorithm, UCRL2, exhibits low regret in the average-reward setting (Jaksch et al., 2010).\nModel-based interval estimation with exploratory bonus (MBIE-EB) is a simplification which augments the estimated reward function at every state and solves Bellman’s equation (Bellman, 1957):\nV (x) = max a∈A\n[ R̂(x, a) + E\nx′∼P̂ (x,a) γV (x′) + β√ Nn(x, a)\n] ,\nwhere β is a theoretically-derived constant. MBIE and MBIE-EB are both PAC-MDP: they provide a high-probability guarantee that the agent will act optimally everywhere except for a polynomial number of steps.\nBayesian Exploration Bonus (BEB; Kolter and Ng, 2009) replaces the empirical estimates P̂ and R̂ with Dirichlet estimators. The Dirichlet estimator over X (with a Laplace prior) estimates the transition function as\nP̂n(x ′;x, a) :=\nNn(x, a, x ′) + 1 Nn(x, a) + |X | ,\nwhere Nn(x, a, x′) is the number of observed transitions (x, a, x′). Technical differences aside, BEB operates in a similar fashion as MBIE-EB but with a different exploration bonus:\nV (x) = max a∈A\n[ R̂(x, a) + E\nx′∼P̂ (x,a) γV (x′) +\nβ\nNn(x, a) + |X |\n] .\nAs shown by Kolter and Ng, BEB is not PAC-MDP. Rather, it offers the weaker PAC-BAMDP guarantee of acting Bayes-optimally, i.e. optimally with respect to the agent’s prior, everywhere except for a polynomial number of steps."
    }, {
      "heading" : "3 From Predictions to Counts",
      "text" : "In the introduction we argued that in many practical settings, states are rarely revisited. This precludes answering the question “how novel is this state?” with the empirical count, which is almost always zero. Nor is the problem solved by a Bayesian approach: even variable-alphabet estimators (e.g. Friedman and Singer, 1999; Hutter, 2013; Bellemare, 2015) must assign a small, diminishing probability to yet-unseen states. In large spaces, counting (in the usual sense) is irrelevant; to estimate the certainty of an agent’s knowledge, we must instead look for a quantity which generalizes across states. In this section we derive such a quantity. We call it a pseudo-count, as it extends the familiar notion from Bayesian estimation.\nAs a working example, consider the following scenario: A commuter has recently landed in a large metropolis, and each morning at the train station she observes k = 3 factors: the weather (x1 ∈ RAIN, SUN), the time of day (x2 ∈ EARLY, LATE), and the general crowdedness of the station (x3 ∈ BUSY, QUIET). We suppose our commuter has made n = 10 observations: x1 = (SUN, LATE, QUIET), and x2 . . . x10 = (RAIN, EARLY, BUSY). She would like to obtain a count for the unseen state xNOVEL = (SUN, LATE, BUSY).\nBecause estimating the joint distribution is unwieldy, she forms a sequential density model ρ over X as a product of independent factor models:\nρn(x) = k∏ i=1 µ(xi ; xi1:n)\nwhere each µ(· ; xi1:n) is the marginal empirical distribution for the ith factor. The key idea is to notice that, although Nn(xNOVEL) = 0, the independent factor model assigns a nonzero probability to this state: ρn(xNOVEL) = 0.12 × 0.9 > 0. Our aim is to convert the positive probability output by our commuter’s model into a reasonable, positive surrogate for the empirical count Nn. We begin by introducing a new quantity, the recoding probability of a symbol x:\nρ′n(x) := ρ(x ; x1:nx).\nThis is the probability assigned to x by our sequential density model after observing a new occurrence of x. The term “recoding” is inspired from the statistical compression literature, where coding costs are inversely related to probabilities (Cover and Thomas, 1991). When ρ is a universal model,\nρ′n(x) = Prρ(Xn+2 = x |X1 . . . Xn = x1:n, Xn+1 = x).\nDefinition 1. A sequential density model ρ is learning-positive if for all x1:n ∈ Xn and all x ∈ X , ρ′n(x) ≥ ρn(x).\nWe postulate two unknowns: a pseudo-count function N̂n(x), and a pseudo-count total n̂. We relate these two unknowns through two constraints:\nρn(x) = N̂n(x)\nn̂ ρ′n(x) =\nN̂n(x) + 1\nn̂+ 1 . (1)\nWe require that the sequential density model’s increase in prediction of x, after observing one instance of x itself, must correspond to a unit increase in pseudo-count. The pseudo-count itself is derived from solving the linear system (1):\nN̂n(x) = ρn(x)(1− ρ′n(x)) ρ′n(x)− ρn(x) . (2)\nIn our example, we find that the recoding probability of the novel state is ρ′n(xNOVEL) = ( 2 11 ) 2( 1011 ) ≈ 0.03. Consequently, our pseudo-count is N̂n(xNOVEL) = 0.416. As desired, this quantity is strictly positive.\nThe system (1) yields N̂n(x) = 0 (with n̂ = ∞) when ρn(x) = ρ′n(x) = 0, and is inconsistent when ρn(x) < ρ′n(x) = 1. From a practical perspective, such cases may arise from poorly behaved density models, but are easily accounted for. From here onwards we will thus assume a well-defined N̂n(x). We deduce the following:\n1. N̂n(x) ≥ 0 if and only if ρ is learning-positive;\n2. N̂n(x) = 0 if and only if ρn(x) = 0; and\n3. N̂n(x) =∞ if and only if ρn(x) = ρ′n(x).\nIf ρn is the empirical distribution then the pseudo-count recovers the empirical count: N̂n = Nn. Similarly, if ρn is a Dirichlet estimator then N̂n recovers the usual notion of pseudo-count. In Section 5 we shall see that in fact N̂n remains well-behaved for a much larger class of sequential density models.\nIn lieu of the pseudo-count N̂n we may be tempted to use the “naive” pseudo-count\nÑn(x) := nρn(x).\nBy analogy with the empirical distribution this may seem a simple and reasonable alternative. However, the analogy is flawed: there need not be a simple relationship between the density model and the sequence length n, for example when ρn(x) is the output of a neural network. By making explicit the total pseudo-count n̂, we allow the probabilities to be normalized by a quantity much larger or smaller than n. In fact, for many natural density models the naive pseudo-count assigns conservatively low counts to x, in the sense that Ñn(x) ≤ N̂n(x). We will provide a more general argument in favour of our pseudo-count in Section 5."
    }, {
      "heading" : "3.1 An approximation to Equation (2).",
      "text" : "We conclude this section with an approximation to the pseudo-count described by (2). We begin by solving (1) slightly differently:\nN̂n(x) + 1 N̂n(x) = ρ′n(x)(n̂+ 1) ρn(x)n̂\n1 + 1 N̂n(x) = ρ′n(x)(n̂+ 1) ρn(x)n̂\nN̂n(x) =\n( ρ′n(x)(n̂+ 1) ρn(x)n̂ − 1 )−1\n≈ ρn(x) ρ′n(x)− ρn(x) ,\nwhere we made the approximation n̂ ≈ n̂ + 1. Examination of (2) shows this is equivalent to supposing that ρ′n(x) ≈ 0, which is typical of large alphabets: for example, the most frequent English word, “the”, has relative frequency 4.7%; while the twentieth most frequent, “at”, has a relative frequency of barely 0.33%.2"
    }, {
      "heading" : "4 (Pseudo-)Counting Salient Events",
      "text" : "As an illustrative example, we employ our method to estimate the number of occurrences of certain salient events in Atari 2600 video games. We use the Arcade Learning Environment (Bellemare et al., 2013). We focus on two games: FREEWAY and PITFALL! (Figure 1, screenshots). We will demonstrate a number of desirable properties of our pseudo-counts:\n1. They exhibit credible magnitudes,\n2. the ordering of state frequency is respected,\n3. they grow linearly (on average) with real counts,\n4. they are roughly zero for novel events, e.g. the first occurrence of the salient event, and\n5. they are robust in the presence of nonstationary data.\n2Word rankings from Wikipedia http://bit.ly/1Nf6HHx, relative frequencies from the Google Ngram Viewer http://bit.ly/1Sg89YT.\nThese properties suggest that our pseudo-count is the correct way to generalize counting to large state space. We emphasize that we compute N̂n(x) from a pixel-level density model.\nFREEWAY. In Freeway, the agent must navigate a chicken across a busy road. We define the salient event as the moment when the agent reaches the other side. As is the case for many Atari 2600 games, this naturally salient event is associated with an increase in score, which ALE translates into a positive reward (here, +1). After crossing, the chicken is teleported back to the bottom of the screen. To highlight the robustness of our pseudo-count, we consider a policy which applies the DOWN action for 250,000 frames, then UP for 250,000 frames, then another period of DOWN and one of UP. Note that the salient event can only occur during the UP periods.\nPITFALL! In Pitfall, the agent is tasked with recovering treasures from the jungle. Echoing the pioneering work of Diuk et al. (2008) on the Atari 2600, we define the “salient event” to occur whenever the agent is located in the area rightwards from the starting screen. Due to the pecularities of the game dynamics, this event occurs in bursts and at irregular intervals, being interspersed among visits to other areas. We use a uniformly random policy. The full trial lasts one million frames.\nWhile both salient events are clearly defined, they actually occur within a number of screen configurations. In FREEWAY, the car positions vary over the course of an episode. In PITFALL!, the avatar’s exact pose and location varies and various other critters complicate the scene.\nWe use a simplified version of the CTS sequential density model for Atari 2600 frames proposed by Bellemare et al. (2014) and used for density estimation proper by Veness et al. (2015). While the CTS model is rather impoverished in comparison to state-of-the-art modelling algorithms (e.g. Oh et al., 2015), its count-based nature results in extremely fast learning, making it an appealing candidate for exploration. Further details on the model may be found in the appendix.\nFor comparison, we report the pseudo-counts for both the salient event and a reference event, averaged over time intervals of 10,000 frames. Both reference events correspond to being in a specific location: in FREEWAY, the reference location is the chicken’s start position; in PITFALL!, it is the area immediately to left of the starting screen.\nExamining the pseudo-counts depicted in Figure 1 confirms that they exhibit the desirable properties listed above. In particular:\n• In FREEWAY, the salient event count is almost zero until the first occurrence of the event.\n• This count increases slightly during the 3rd period, since the salient and reference events share some common structure.\n• Throughout, the pseudo-count for the less frequent salient event remains smaller than the reference’s.\n• In PITFALL! the salient event occurs sporadically. Despite this, its average pseudo-count grows with each visit.\nAnecdotally, our PITFALL! experiment reveals that, while the random agent easily reaches the lefthand area, it actually spends far fewer frames there than in the right-hand area (19,068 vs 74,002). While the right-hand area is hard to reach (as evidenced by the uneven pseudo-count curve), it is also hard to leave.\nWe emphasize that the pseudo-counts are a fraction of the real counts (inasmuch as we can define “real”!) in both tasks. In FREEWAY, the starting position has been visited about 140,000 times by the end of the trial, and the salient event has occurred 1285 times. Furthermore, the ratio of pseudocounts for different states need not follow the ratio of real counts, as we shall show with Theorem 1 below."
    }, {
      "heading" : "5 Properties of Pseudo-Counts",
      "text" : "In this section we outline interesting properties of the pseudo-count N̂n. We first provide a consistency result describing the limiting behaviour of the ratio N̂n/Nn. We then instantiate this result for a broad class of models, including the CTS model used in the previous section. Finally, we expose a relationship between our pseudo-count and a density-based approach to value function approximation called Compress and Control (Veness et al., 2015)."
    }, {
      "heading" : "5.1 Relation of pseudo-count to empirical count.",
      "text" : "Consider a fixed, infinite sequence x1, x2, . . . fromX . We define the limit of a sequence of functions( f(x ; x1:n) : n ∈ N ) with respect to the length n of the subsequence x1:n. We additionally assume that the empirical distribution µn converges pointwise to a distribution µ, and write µ′n(x) for the recoding probability of x under µn. We begin with two assumptions on our sequential density model.\nAssumption 1. The limits\n(a) r(x) := lim n→∞\nρn(x) µn(x) (b) ṙ(x) := lim n→∞ ρ′n(x)− ρn(x) µ′n(x)− µn(x)\nexist and ṙ(x) > 0.\nAssumption (a) states that ρ should eventually assign a probability to x which is proportional to the limiting empirical distribution µ(x). Since ρ is a probability distribution, it must be that either r(x) = 1 for all x, or r(x) < 1 for at least one state.In general we expect this quantity to be much smaller than unity: generalization requires committing probability mass to states that may never be seen, as was the case in the example of Section 3. Assumption (b), on the other hand, imposes a restriction on the relative learning rate of the two estimators. Since both r(x) and µ(x) exist, Assumption 1 also implies that ρn(x) and ρ′n(x) have a common limit.\nWe begin with a simple lemma which will prove useful throughout.\nLemma 1. The rate of change of the empirical distribution, µ′n(x)− µn(x), is such that\nn ( µ′n(x)− µn(x) ) = 1− µ′n(x).\nProof. We expand the definition of µn and µ′n:\nn ( µ′n(x)− µn(x) ) = n\n[ Nn(x) + 1\nn+ 1 − Nn(x) n ] = [ n\nn+ 1\n( Nn(x) + 1 ) −Nn(x) ] = [ 1− Nn(x) + 1\nn+ 1 ] = 1− µ′n(x).\nUsing this lemma, we derive an asymptotic relationship between Nn and N̂n.\nTheorem 1. Under Assumption 1, the limit of the ratio of pseudo-counts N̂n(x) to empirical counts Nn(x) exists for all x. This limit is\nlim n→∞\nN̂n(x) Nn(x) = r(x) ṙ(x)\n( 1− µ(x)r(x)\n1− µ(x)\n) .\nProof. We expand the definition of N̂n(x) and Nn(x):\nN̂n(x) Nn(x) = ρn(x)(1− ρ′n(x)) Nn(x)(ρ′n(x)− ρn(x))\n= ρn(x)(1− ρ′n(x))\nnµn(x)(ρ′n(x)− ρn(x))\n= ρn(x)(µ\n′ n(x)− µn(x)) µn(x)(ρ′n(x)− ρn(x)) 1− ρ′n(x) n(µ′n(x)− µn(x))\n= ρn(x)\nµn(x) µ′n(x)− µn(x) ρ′n(x)− ρn(x) 1− ρ′n(x) 1− µ′n(x) ,\nwith the last line following from Lemma 1. Under Assumption 1, all terms of the right-hand side converge as n→∞. Taking the limit on both sides,\nlim n→∞\nN̂n(x)\nNn(x)\n(a) =\nr(x) ṙ(x) lim n→∞ 1− ρ′n(x) 1− µ′n(x)\n(b) = r(x)\nṙ(x) 1− µ(x)r(x) 1− µ(x) ,\nwhere (a) is justified by the existence of the relevant limits and ṙ(x) > 0, and (b) follows from writing ρ′n(x) as µn(x)ρ ′ n(x)/µn(x), where all limits involved exist.\nThe relative rate of change, whose convergence to ṙ(x) we require, plays an essential role in the ratio of pseudo- to empirical counts. To see this, consider a sequence (xn : n ∈ N) generated i.i.d. from a distribution µ over a finite alphabet, and a sequential density model defined from a sequence of nonincreasing step-sizes (αn : n ∈ N):\nρn(x) = (1− αn)ρn−1(x) + αnI {xn = x} , with initial condition ρ0(x) = |X |−1. For αn = n−1, this sequential density model is the empirical distribution. For αn = n−2/3, we may appeal to well-known results from stochastic approximation (e.g. Bertsekas and Tsitsiklis, 1996) and find that almost surely\nlim n→∞ ρn(x) = µ(x) but lim n→∞ ρ′n(x)− ρn(x) µ′n(x)− µn(x) =∞.\nSince µ′n(x)−µn(x) = n−1(1−µ′n(x)), we may therefore think of Assumption 1b as also requiring ρ to converge at a rate of Θ(1/n) for a comparison with the empirical count Nn to be meaningful. Note, however, that a sequential density model that does not satisfy Assumption 1b may still yield useful (but incommensurable) pseudo-counts."
    }, {
      "heading" : "5.2 Directed graphical models as sequential density models.",
      "text" : "We next show that directed graphical models (Wainwright and Jordan, 2008) satisfy Assumption 1. A directed graphical model describes a probability distribution over a factored alphabet. To the ith factor xi is associated a parent set π(i) ⊆ {1, . . . , i− 1}. Let xπ(i) denote the value of the factors in the parent set. The ith factor model is ρin(x i ; xπ(i)) := ρi(xi ; x1:n, x π(i)), with the understanding that ρi is allowed to make a different prediction for each value of xπ(i). The symbol x is assigned the joint probability\nρGM(x ; x1:n) := k∏ i=1 ρin(x i ; xπ(i)).\nCommon choices for ρin include the conditional empirical distribution and the Dirichlet estimator.\nProposition 1. Suppose that each factor model ρin converges to the conditional probability distribution µ(xi |xπ(i)) and that for each xi with µ(xi |xπ(i)),\nlim n→∞\nρi(xi ; x1:nx, x π(i))− ρi(xi ; x1:n, xπ(i))\nµ(xi ; x1:nx, xπ(i))− µ(xi ; x1:n, xπ(i)) = 1.\nThen for all x with µ(x) > 0, the sequential density model ρGM satisfies Assumption 1 with\nr(x) =\n∏k i=1 µ(x\ni |xπ(i)) µ(x)\nand ṙ(x) =\n∑k i=1 ( 1− µ(xi |xπ(i)) )∏ j 6=i µ(x\nj |xπ(j)) 1− µ(x) .\nThe CTS density model we used above is in fact a particular kind of induced graphical model. The result above thus describes how the pseudo-counts computed in Section 4 are asymptotically related to the empirical counts.\nCorollary 1. Let φ(x) > 0 with ∑ x∈X φ(x) <∞ and consider the count-based estimator\nρn(x) = Nn(x) + φ(x) n+ ∑ x′∈X φ(x ′) ."
    }, {
      "heading" : "If N̂n is the pseudo-count corresponding to ρn then N̂n(x)/Nn(x)→ 1 for all x with µ(x) > 0.",
      "text" : "In the appendix we prove a slightly stronger result which allows φ(x) to vary with x1:n; the above is a special case of this result. Hence, pseudo-counts derived from atomic (i.e., single-factor) sequential density models exhibit the correct behaviour: they asymptotically match the empirical counts."
    }, {
      "heading" : "5.3 Relationship to a kind of dual value function.",
      "text" : "The Compress and Control method (Veness et al., 2015) applies sequential density models to the problem of reinforcement learning. It uses these models to learn a form of dual value function based on stationary distributions (Wang et al., 2008). In the Markov Decision Process (MDP) setting, a policy π is a mapping from states to distributions over actions. The finite horizon value function for this policy is the expected sum of rewards over a horizon H ∈ N:\nV π(x) = Eπ [ H∑ t=1 r(xt, at) |x1 = x ] .\nConsider the set of all achievable returns Z := {z ∈ R : Pr{ ∑H t=1 r(xt, at) = z} > 0}. Let us assume that Z is finite (e.g., r(x, a) takes on a finite number of values). The value function can be rewritten as\nV π(x) = ∑ z∈Z z Pr { H∑ t=1 r(xt, at) = z |x1 = x } = E [z |x1 = x] ,\nwhere the probability depends on the distribution over finite trajectories jointly induced by the policy π and the MDP’s transition function. Let us write Pr{z |x} := Pr{ ∑H t=1 r(xt, at) = z |x1 = x}\nfor conciseness, and define Pr{x | z} and Pr{z} similarly. Using Bayes’ rule, V π(x) = ∑ z∈Z z Pr{z |x}\n= ∑ z∈Z z Pr{x | z}Pr{z}∑ z′∈Z Pr{x | z′}Pr{z′} .\nThe Compress and Control method exploits the finiteness of Z to model the Pr{x | z} terms using |Z| copies of a sequential density model over X . We call the copy corresponding to z ∈ Z the zconditional model. Its prediction is ρn(· ; z), with the subscript n indicating the dependency on the sequence of state-return pairs (x, z)1:n. Since Z is finite, we predict the return z using the empirical distribution (other estimators, such as the Dirichlet, may also be employed). Define\nρn(x, z) := µn(z)ρn(x ; z) ρn(x) := ∑ z∈Z ρn(x, z).\nThe Compress and Control value function after observing (x, z)1:n is V̂n(x) := ∑ z∈Z z ρn(x, z) ρn(x) . (3)\nThe reliance of both Compress and Control and our pseudo-count method on a sequential density model suggest a connection between value function and pseudo-counts. We believe this relationship is the natural extension of the close relationship between tabular value estimation using a decaying step-size (e.g. Even-Dar and Mansour, 2001; Azar et al., 2011) and count-based exploration in the tabular setting (e.g. Brafman and Tennenholtz, 2002; Strehl and Littman, 2008; Jaksch et al., 2010; Szita and Szepesvári, 2010). Fix (x, z)1:n, let Nn(x, z) be the number of occurences of (x, z) ∈ X × Z in this sequence, and let\nµn(z |x) := { Nn(x,z) Nn(x)\nif Nn(x) > 0 0 otherwise.\nWe begin by defining the empirical value function for this fixed sequence: Vn(x) = ∑ z∈Z zµn(z |x), (4)\nwhich Veness et al. (2015) showed converges to V π(x) when (x, z)1:n is drawn from the ergodic Markov chain jointly induced by a finite-state MDP and π. Let N̂n(x, z) be the pseudo-count derived from ρn(x, z) and define the pseudo-empirical distribution µ̂n:\nN̂n(x) := ∑ z∈Z N̂n(x, z) µ̂n(z |x) :=\n{ N̂n(x,z)\nN̂n(x) if N̂n(x) > 0\n0 otherwise.\nThe following exposes the surprising relationship between the pseudo-count N̂ and the empirical Compress and Control value function V̂ πn (x). Proposition 2. Consider a sequence of state-return pairs (x, z)1:n. Let ρn(x ; z) be the sequential density model used to form the value function V̂n, with ρn(x) and ρn(x, z) defined as in the text. Let N̂n(x, z) be the pseudo-count formed from ρn(x, z). For any x\nV̂n(x) = ∑ z∈Z z N̂n(x, z) N̂zn(x) ,\nwhere\nN̂zn(x) := ∑ z̃∈Z ρn(x, z̃)(1− ρ′(x, z)) ρ′n(x, z)− ρn(x, z) ."
    }, {
      "heading" : "In particular, if N̂n(x) > 0 then",
      "text" : "V̂n(x) = ∑ z∈Z zµ̂n(z |x)\n[ N̂n(x)\nN̂zn(x)\n] .\nProof. The proof of the proposition follows from rearranging (3) and expanding the definition of N̂n(x, z) and N̂n(x).\nProposition 2 shows that the Compress and Control value function resembles (4), with an additional warping factor N̂n/N̂zn applied to the pseudo-empirical distribution. This warping factor arises from the potential discrepancy in learning speeds between the return-conditional density models, as must occur when the return distribution is not uniform. When the warping factor is asymptotically 1, we can use Theorem 1 (under the usual ergodicity conditions) to derive relative upper and lower bounds on the asymptotic approximate value function\nV̂ π(x) := lim n→∞ V̂n(x).\nProposition 2 immediately suggests a novel algorithm which directly estimates V̂n(x) from pseudocounts and obviates the warping factor. We leave the study of this algorithm as a future research direction."
    }, {
      "heading" : "6 The Connection to Intrinsic Motivation",
      "text" : "Intrinsic motivation algorithms seek to explain what drives behaviour in the absence of, or even contrary to, extrinsic reward (Barto, 2013). To mirror terminology from machine learning, we may think of intrinsically motivated behaviour as unsupervised exploration. As we now show, there is a surprisingly close connection between the novelty signals typical of intrinsic motivation algorithms and count-based exploration with pseudo-counts.\nInformation gain is a frequently appealed-to quantity in the intrinsic motivation literature (e.g. Schaul et al., 2011; Orseau et al., 2013). Here, information gain refers to the change in posterior within a mixture model ξ defined over a classM of sequential density models. A mixture model, itself a universal sequential density model, predicts according to a weighted combination of models fromM:\nξn(x) := ξ(x |x1:n) := ∫ ρ∈M wn(ρ)ρ(x |x1:n)dρ,\nwith wn(ρ) the posterior weight of ρ. This posterior is defined recursively, starting from a prior distribution w0 overM:\nwn+1(ρ) := wn(ρ, xn+1) wn(ρ, x) := wn(ρ)ρ(x |x1:n)\nξn(x) . (5)\nInformation gain is the change in posterior, in Kullback-Leibler divergence, that would result from observing x: IGn(x) := IG(x ; x1:n) := KL ( wn(·, x) ‖wn ) .\nExpected information gain is the expectation of the same quantity taken with respect to the model’s prediction ξn(x). In a Bayesian sense, expected information gain is the appropriate measure of exploration when extrinsic rewards are ignored: an agent which maximizes long-term expected information gain is “optimally curious” (Orseau et al., 2013).\nComputing the information gain of a complex density model is often undesirable, if not downright intractable. However, a quantity which we call the prediction gain (PG) provides us with a good approximation of the information gain.3 We define the prediction gain of a sequential density model ρ as the difference between the recoding log-probability and log-probability of x:\nPGn(x) := log ρ′n(x)− log ρn(x). Observe that prediction gain is nonnegative if and only if ρ is learning-positive. Its relationship to both the pseudo-count N̂n and information gain is the main result of this paper. Theorem 2. Consider a sequence x1:n ∈ Xn. Let ξ be a mixture model over M with relevant quantities IGn and PGn. Let N̂n be the pseudo-count function defined in (2) with ξ taking the role of the sequential density model. Then\nIGn(x) ≤ PGn(x) ≤ N̂n(x)−1 ∀x. 3It seems highly unlikely that the PG has not been studied before, or its relationship to the information gain\nnot made clear. At the present, however, we have yet to find it mentioned anywhere.\nThis result informs us on the nature of information-driven exploration: an exploration algorithm which maximizes the information gain within a (mixture) density model also maximizes a lower bound on the inverse of the pseudo-count. Consequently, such an algorithm must be related to Kolter and Ng’s count-based Bayes-optimal algorithm (Section 2.2). Linking the two quantities is prediction gain, related to novelty signals typically found in the intrinsic motivation literature (e.g. Schmidhuber, 2008).4 The following provides an identity connecting information gain and prediction gain.\nLemma 2. Consider a fixed x ∈ X and let w′n(x) := wn(ρ, x) be the posterior of ξ overM after observing x. Let w′′n(x) := w ′ n(ρ, x) be the same posterior after observing x a second time. Then\nPGn(x) = KL(w′n ‖wn) + KL(w′n ‖w′′n) = IGn(x) + KL(w′n ‖w′′n).\nProof (Theorem 2). The inequality IGn(x) ≤ PGn(x) follows directly from Lemma 2 and the nonnegativity of the Kullback-Leibler divergence. For the inequality PGn(x) ≤ N̂n(x)−1, we write\nN̂n(x) −1 = (1− ξ′n(x))−1 ξ′n(x)− ξn(x) ξn(x)\n= (1− ξ′n(x))−1 ( ξ′n(x) ξn(x) − 1 )\n(a) = (1− ξ′n(x))−1 ( ePGn(x) − 1 ) (b) ≥ ePGn(x) − 1 (c) ≥ PGn(x),\nwhere (a) follows by definition of prediction gain, (b) from ξ′n(x) ∈ [0, 1), and (c) from the inequality ex ≥ x+ 1.\nSince ex − 1→ x as x→ 0, we further deduce that for PGn(x) close to zero we have N̂n(x)−1 ≈ PGn(x). Hence the two quantities agree on unsurprising events. Theorem 2 informs us on the nature of prediction-based exploration methods:\nThe density model need not be a mixture. While information gain requires a mixture model, prediction gain is well-defined for any sequential density model.\nThe density model need not be universal. Computing N̂n and PGn does not require well-defined conditional probabilities.\nThe density model should be consistent with the empirical distribution over x1:n. To emulate count-based exploration, we should seek to approximate µn. In particular:\nNovelty signals derived from transition functions yield suboptimal exploration. Using ρn(x\n′ ; x, a) for exploration, rather than ρn(x, a), corresponds to using a pseudo-count proportional to the number of transitions (x, a) → x′. For stochastic environments, this ignores the L1 constraint on the unknown transition function (see e.g. Strehl and Littman, 2008) and is unnecessarily conservative.\nLearning progress is not change in probability. Previous work has used a novelty signal proportional to the distance between ρ′n and ρn (L1 distance: Oudeyer et al. 2007; L2 distance: Stadie et al. 2015). Our derivation suggests using instead the L1 distance normalized by ρn(x).\nCompression progress may be PAC-BAMDP. Since an exploration bonus proportional to Nn(x, a)\n−1 leads to PAC-BAMDP algorithms (Kolter and Ng, 2009; Araya-López et al., 2012), we hypothesize that similar bounds can be derived for our pseudo-counts. In turn, these bounds should inform the case of an exploration bonus proportional to the PG.\n4Although related to Schmidhuber’s compression progress, prediction gain is in fact a slightly different quantity."
    }, {
      "heading" : "7 Pseudo-Counts for Exploration",
      "text" : "In this section we demonstrate the use of pseudo-counts in guiding exploration. We return to the Arcade Learning Environment, but now use the same CTS sequential density model to augment the environmental reward with a count-based exploration bonus. Unless otherwise specified, all of our agents are trained on the stochastic version of the Arcade Learning Environment."
    }, {
      "heading" : "7.1 Exploration in hard games.",
      "text" : "From 60 games available through the Arcade Learning Environment, we identified those for which exploration is hard, in the sense that an -greedy policy is clearly inefficient (a rough taxonomy is given in the appendix). From these hard games, we further selected five games for which\n1. a convolutional deep network can adequately represent the value function, and\n2. our CTS density model can reasonably represent the empirical distribution over states.\nWe will pay special attention to MONTEZUMA’S REVENGE, one of the hardest Atari 2600 games available through the ALE. MONTEZUMA’S REVENGE is infamous for its hostile, unforgiving environment: the agent must navigate a maze composed of different rooms, each filled with a number of traps. The rewards are far and few in between, making it almost impossible for undirected exploration schemes to succeed. To date, the best agents require hundreds of millions of frames to attain nontrivial performance levels, and visit at best two or three rooms out of 72; most published agents (e.g. Bellemare et al., 2012; van Hasselt et al., 2016; Schaul et al., 2016; Mnih et al., 2016; Liang et al., 2016) fail to match even a human beginner’s performance.\nWe defined an exploration bonus of the form\nR+n (x, a) := β(N̂n(x) + 0.01) −1/2, (6)\nwhere β = 0.05 was selected from a short parameter sweep. The small added constant is necessary only for numerical stability and did not significantly affect performance.5 Note that R+ is a nonstationary function which depends on the agent’s history.\nWe train our agents’ Q-functions using Double DQN (van Hasselt et al., 2016) save that we mix the Double Q-Learning target ∆QDOUBLE(x, a) with the Monte Carlo return from (x, a); both target and return use the combined reward function (x, a) 7→ (R+R+n )(x, a). The new target is\n∆Q(xt, at) := (1− η)∆QDOUBLE(xt, at) + η [ ∞∑ s=0 γs ( (R+R+n )(xt+s, at+s) ) −Q(xt, at) ] .\nMixing in a 1-step target with the Monte Carlo return is best thought of as a poor man’s eligibility traces, with this particular form chosen for its computational efficiency. Since DQN updates are done via experience replay, having to wait until the Monte Carlo return is available to perform updates does not significantly slow down learning. We experimented with dynamic exploration bonuses, where R+n (x, a) is computed at replay time, as well as static bonuses where R + n (x, a) is computed when the sample is inserted into the replay memory. Although the former yields somewhat improved results, we use the latter here for computational efficiency. The parameter η = 0.1 was chosen from a coarse parameter sweep, and yields markedly improved performance over ordinary Double DQN (η = 0.0) on almost all Atari 2600 games.\nOther Double DQN parameters were kept at default values. In particular, we use the DQN decay schedule in order to produce reasonable exploration for DQN; additional experiments suggest that using exploration bonuses removes the need for such a schedule. The discount factor is 0.99.\nWe also compare using exploration bonuses to the optimistic initialization trick proposed by (Machado et al., 2014). In this formulation of optimistic initialization, the agent receives a small negative penalty c at every step, and a reward of (1− γ)−1c at termination. Here we used c = −(1− γ) to be compatible with DQN’s reward clipping.\n5Double DQN, which we use here, requires that rewards be clipped within the range [−1, 1], making the regularization much less important. In other settings this constant may play a larger role.\nFigure 2 depicts the result of our experiment averaged across 5 training seeds. DQN suffers from high inter-trial variance in FREEWAY due to the game’s reward sparsity. In this game optimistic initialization overcomes this sparsity, but in the other games it yields performance similar to DQN. By contrast, the count-based exploration bonus enables us to make quick progress on a number of games – most dramatically in MONTEZUMA’S REVENGE, which we will analyze in detail in the following section. We quickly reach a high performance level in FREEWAY (the “bump” in the curve corresponds to the performance level attained by repeatedly applying the UP action, i.e. 21 points). We see fast progress in VENTURE, a map-based game. In H.E.R.O. we find that -greedy exploration performs surprisingly well once the Monte-Carlo return is used. At the moment it seems a different approach is needed to make significant progress in PRIVATE EYE."
    }, {
      "heading" : "7.2 Exploration in MONTEZUMA’S REVENGE.",
      "text" : "MONTEZUMA’S REVENGE is divided into three levels, each composed of 24 rooms arranged in a pyramidal shape (Figure 3). As discussed above, each room poses a number of challenges: to escape the very first room, the agent must climb ladders, dodge a creature, pick up a key, then backtrack to open one of two doors. The number of rooms reached by an agent is therefore a good measure of its ability. By accessing the game RAM, we recorded the location of the agent at each step during the course of training.6 We computed the visit count to each room, averaged over epochs each lasting one million frames. From this information we constructed a map of the agent’s “known world”, that is, all rooms visited at least once.\nFigure 4 paints a clear picture: after 50 million frames, the agent using exploration bonuses has seen a total of 15 rooms, while the no-bonus agent has seen two. At that point in time, our agent achieves an average score of 2461; by 100 million frames, this figure stands at 3439, higher than anything previously reported.7 We believe the success of our method in this game is a strong indicator of the usefulness of pseudo-counts for exploration.\n6We emphasize that the game RAM is not made available to the agent, and is solely used here in our behavioural analysis.\n7A video of our agent playing Montezuma’s Revenge is available at https://youtu.be/0yI2wJ6F8r0 ."
    }, {
      "heading" : "7.3 Improving exploration for actor-critic methods.",
      "text" : "We next turn our attention to actor-critic methods, specifically the A3C (asynchronous actor-critic) algorithm of Mnih et al. (2016). One appeal of actor-critic methods is that their explicit separation of policy and Q-function parameters allows for a richer behaviour space. This very separation, however, often leads to deficient exploration: to produce any sensible results, the A3C policy parameters must be regularized with an entropy cost (Mnih et al., 2016). As we now show, our count-based exploration bonus leads to significantly improved A3C performance.\nWe first trained A3C on 60 Atari 2600 games, with and without the exploration bonus given by (6). We refer to our augmented algorithm as A3C+. From a parameter sweep over 5 training games we found the parameter β = 0.01 to work best. Summarily, we find that A3C fails to learn in 15 games, in the sense that the agent does not achieve a score 50% better than random. In comparison, there are only 10 games for which A3C+ fails to improve on the random agent; of these, 8 are games where DQN fails in the same sense. Details and full results are given in the appendix.\nTo demonstrate the benefits of augmenting A3C with our exploration bonus, we computed a baseline score (Bellemare et al., 2013) for A3C+ over time. If rg is the random score on game g, ag the performance of A3C on g after 200 million frames, and sg,t the performance of A3C+ at time t, then the corresponding baseline score at time t is\nzg,t := sg,t −min{rg, ag}\nmax{rg, ag} −min{rg, ag} .\nFigure 5 shows the median and first and third quartile of these scores across games. Considering the top quartile, we find that A3C+ reaches A3C’s final performance on at least 15 games within 100 million frames, and in fact reaches much higher performance levels by the end of training."
    }, {
      "heading" : "7.4 Comparing exploration bonuses.",
      "text" : "Next we compare the effect of using different exploration bonuses derived from our density model. We consider the following variants:\n• no exploration bonus, • N̂n(x)−1/2, as per MBIE-EB (Strehl and Littman, 2008); • N̂n(x)−1, as per BEB (Kolter and Ng, 2009); and • PGn(x), related to compression progress (Schmidhuber, 2008).\nThe exact form of these bonuses is analogous to (6). We compare these variants after 10, 50, 100, and 200 million frames of training, using the same experimental setup as in the previous section. To\ncompare scores across 60 games, we use inter-algorithm score distributions (Bellemare et al., 2013). Inter-algorithm scores are normalized so that 0 corresponds to the worst score on a game, and 1, to the best. If g ∈ {1, . . . ,m} is a game and zg,a the inter-algorithm score on g for algorithm a, then the score distribution function is\nf(x) := |{g : zg,a ≥ x}|\nm .\nThe score distribution effectively depicts a kind of cumulative distribution, with a higher overall curve implying better scores across the gamut of Atari 2600 games. A higher curve at x = 1 implies top performance on more games; a higher curve at x = 0 indicates the algorithm does not perform poorly on many games. The scale parameter β was optimized to β = 0.01 for each variant separately.\nFigure 6 shows that, while the PG initially achieves strong performance, by 50 million frames all three algorithms perform equally well. By 200 million frames, the 1/ √ N̂ exploration bonus outperforms both PG and no bonus. The PG achieves a decent, but not top-performing score on all games. We hypothesize that the poor performance of the 1/N bonus stems from too abrupt a decay from a large to small intrinsic reward, although more experiments are needed. As a whole, these results show how using PG offers an advantage over the baseline A3C algorithm, which is furthered by using our count-based exploration bonus."
    }, {
      "heading" : "8 Future Directions",
      "text" : "The last few years have seen tremendous advances in learning representations for reinforcement learning. Surprisingly, these advances have yet to carry over to the problem of exploration. In this paper, we reconciled counting, the fundamental unit of certainty, with prediction-based heuristics and intrinsic motivation. Combining our work with more ideas from deep learning and better density models seems a plausible avenue for quick progress in practical, efficient exploration. We now conclude by outlining a few research directions we believe are promising.\nInduced metric. In describing our approach we purposefully omitted the question of where the generalization comes from. It seems plausible that the choice of sequential density model induces a metric over the state space. A better understanding of this induced metric should allow us to shape the density model to accelerate exploration.\nUniversal density models. Universal density models such as Solomonoff induction (Hutter, 2005) learn the structure of the state space at a much greater rate than the empirical estimator, violating Assumption 1b. Quantifying the behaviour of a pseudo-count derived from such a model is an open problem.\nCompatible value function. There may be a mismatch in the learning rates of the sequential density model and the value function. DQN learns much more slowly than our CTS models. To obtain optimal performance, it may be of interest to design value functions compatible with density models (or vice-versa). Alternatively, we may find traction in implementing a form of forgetting into our density models.\nThe continuous case. Although we focused here on countable alphabets, we can as easily define a pseudo-count in terms of probability density functions. Under additional assumptions, the resulting N̂n(x) describes the pseudo-count associated with an infinitesimal neighbourhood of x. At present it is unclear whether this provides us with the right counting notion for continuous spaces."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Laurent Orseau, Alex Graves, Joel Veness, Charles Blundell, Shakir Mohamed, Ivo Danihelka, Ian Osband, Matt Hoffman, Greg Wayne, and Will Dabney for their excellent feedback early and late in the writing."
    }, {
      "heading" : "A Proof of Proposition 1",
      "text" : "By hypothesis, ρin → µ(xi |xπ(i)). Combining this with µn(x)→ µ(x) > 0,\nr(x) = lim n→∞\nρDGM(x ; x1:n)\nµn(x)\n= lim n→∞\n∏k i=1 ρ i n(x i ; xπ(i))\nµn(x)\n=\n∏k i=1 µ(x\ni |xπ(i)) µ(x) .\nSimilarly,\nṙ(x) = lim n→∞ ρ′DGM(x ; x1:n)− ρDGM(x ; x1:n) µ′n(x)− µn(x)\n(a) = lim\nn→∞\n( ρ′DGM(x ; x1:n)− ρDGM(x ; x1:n) ) n\n1− µ′n(x)\n= lim n→∞\n( ρ′DGM(x ; x1:n)− ρDGM(x ; x1:n) ) n\n1− µ(x) ,\nwhere in (a) we used the identity n(µ′n(x)− µn(x)) = 1− µ′n(x) derived in the proof of Theorem 1. Now\nṙ(x) = (1− µ(x))−1 lim n→∞\n( ρ′DGM(x ; x1:n)− ρDGM(x ; x1:n) ) n\n= (1− µ(x))−1 lim n→∞ ( k∏ i=1 ρi(xi ; x1:nx, x π(i))− k∏ i=1 ρi(xi ; x1:n, x π(i)) ) n.\nLet ci := ρi(xi ; x1:n, xπ(i)) and c′i := ρ i(xi ; x1:nx, x π(i)). The difference of products above is( k∏ i=1 ρi(xi ; x1:nx, x π(i))− k∏ i=1 ρi(xi ; x1:n, x π(i)) ) = ( c′1c ′ 2 . . . c ′ k − c1c2 . . . ck ) = (c′1 − c1)(c′2 . . . c′k) + c1(c′2 . . . c′k − c2 . . . ck)\n= k∑ i=1 (c′i − ci) (∏ j<i cj )(∏ j>i c′j ) ,\nand\nṙ(x) = (1− µ(x))−1 lim n→∞ k∑ i=1 n(c′i − ci) (∏ j<i cj )(∏ j>i c′j ) .\nBy the hypothesis on the rate of change of ρi and the identity n ( µ(xi ; x1:nx, x π(i))− µ(xi ; x1:n, xπ(i)) )\n= 1− µ(xi |xπ(i)), we have lim n→∞ n(c′i − ci) = 1− µ(xi |xπ(i)).\nSince the limits of c′i and ci are both µ(x i |xπ(i)), we deduce that\nṙ(x) =\n∑k i=1 ( 1− µ(xi |xπ(i) )∏ j 6=i µ(x\nj |xπj(x)) 1− µ(x) .\nNow, if µ(x) > 0 then also µ(xi ; xπ(i)) > 0 for each factor xi. Hence ṙ(x) > 0."
    }, {
      "heading" : "B Proof of Lemma 2",
      "text" : "We rewrite the posterior update rule (5) to show that for any ρ ∈M and any x ∈ X , ξn(x)\nρ(x) =\nwn(ρ)\nwn(ρ, x) .\nWrite Ew′n := Eρ∼w′n(·). Now\nPGn(x) = log ξ′n(x)\nξn(x) = Ew′n\n[ log ξ′n(x)\nξn(x) ] = Ew′n [ log w′n(ρ)\nw′′n(ρ)\nw′n(ρ)\nwn(ρ) ] = Ew′n [ log w′n(ρ)\nwn(ρ)\n] + Ew′n [ log w′n(ρ)\nw′′n(ρ) ] = IGn(x) + KL(w′n ‖w′′n)."
    }, {
      "heading" : "C Proof of Corollary 1",
      "text" : "We shall prove the following, which includes Corollary 1 as a special case. Lemma 3. Consider φ : X × X ∗ → R+. Suppose that for all (xn : n ∈ N) and every x ∈ X\n1. lim n→∞ 1 n ∑ x∈X φ(x, x1:n) = 0, and\n2. lim n→∞\n( φ(x, x1:nx)− φ(x, x1:n) ) = 0.\nLet ρn(x) be the count-based estimator\nρn(x) = Nn(x) + φ(x, x1:n) n+ ∑ x∈X φ(x, x1:n) ."
    }, {
      "heading" : "If N̂n is the pseudo-count corresponding to ρn then N̂n(x)/Nn(x)→ 1 for all x with µ(x) > 0.",
      "text" : "Condition 2 is satisfied if φn(x, x1:n) = un(x)φn with φn monotonically increasing in n (but not too quickly!) and un(x) converging to some distribution u(x) for all sequences (xn : n ∈ N). This is the case for most atomic sequential density models.\nProof. We will show that the condition on the rate of change required by Proposition 1 is satisfied under the stated conditions. Let φn(x) := φ(x, x1:n), φ′n(x) := φ(x, x1:nx), φn := ∑ x∈X φn(x)\nand φ′n := ∑ x∈X φ ′ n(x). By hypothesis,\nρn(x) = Nn(x) + φn(x)\nn+ φn ρ′n(x) =\nNn(x) + φ ′ n(x) + 1\nn+ φ′n + 1 .\nNote that we do not require φn(x) = φ′n(x). Now\nρ′n(x)− ρn(x) = n+ φn n+ φn ρ′n(x)− ρn(x)\n= n+ 1 + φ′n n+ φn ρ′n(x)− ρn(x)− (1 + (φ′n − φn))ρ′n(x) n+ φn\n= 1\nn+ φn\n[ (Nn(x) + 1 + φ ′ n(x)− (Nn(x) + φn(x))− (1 + (φ′n − φn))ρ′n(x) ] = 1\nn+ φn\n[ 1− ρ′n(x) + ( φ′n(x)− φn(x) ) − ρ′n(x) ( φ′n − φn )] .\nUsing Lemma 1 we deduce that\nρ′n(x)− ρn(x) µ′n(x)− µn(x) = n n+ φn 1− ρ′n(x) + φ′n(x)− φn(x) + ρ′n(x)(φ′n − φn) 1− µ′n(x) .\nSince φn = ∑ x φn(x) and similarly for φ ′ n, then φ ′ n(x) − φn(x) → 0 pointwise implies that φ′n − φn → 0 also. For any µ(x) > 0,\n0 ≤ lim n→∞\nφn(x)\nNn(x)\n(a) ≤ lim n→∞\n∑ x∈X φn(x)\nNn(x)\n= lim n→∞\n∑ x∈X φn(x)\nn\nn\nNn(x)\n(b) = 0,\nwhere a) follows from φn(x) ≥ 0 and b) is justified by n/Nn(x)→ µ(x)−1 > 0 and the hypothesis that ∑ x∈X φn(x)/n→ 0. Therefore ρn(x)→ µ(x). Hence\nlim n→∞ ρ′n(x)− ρn(x) µ′n(x)− µn(x) = lim n→∞ n n+ φn 1− ρ′n(x) 1− µ′n(x) = 1.\nSince ρn(x)→ µ(x), we further deduce from Theorem 1 that\nlim n→∞\nN̂n(x) Nn(x) = 1.\nThe condition µ(x) > 0, which was also needed in Proposition 1, is necessary for the ratio to converge to 1: for example, if Nn(x) grows as O(log n) but φn(x) grows as O( √ n) (with |X | finite) then N̂n(x) will grow as the larger √ n."
    }, {
      "heading" : "D Experimental Methods",
      "text" : "D.1 CTS sequential density model.\nOur alphabet X is the set of all preprocessed Atari 2600 frames. Each raw frame is composed of 210×160 7-bit NTSC pixels (Bellemare et al., 2013). We preprocess these frames by first converting them to grayscale (luminance), then downsampling to 42× 42 by averaging over pixel values. Aside from this preprocessing, our model is effectively very similar to the model used by Bellemare et al. (2014) and Veness et al. (2015). The CTS sequential density model treats x ∈ X as a factored observation, where each (i, j) pixel corresponds to a factor xi,j . The parents of this factor are its upper-left neighbours, i.e. pixels (i − 1, j), (i, j − 1), (i − 1, j − 1) and (i + 1, j − 1) (in this order). The probability of x is then the product of the probability assigned to its factors. Each factor is modelled using a location-dependent CTS model, which predicts the pixel’s colour value conditional on some, all, or possibly none, of the pixel’s parents.\nD.2 A taxonomy of exploration.\nWe provide in Table 1 a rough taxonomy of the Atari 2600 games available through the ALE in terms of the difficulty of exploration.\nWe first divided the games into two groups: those for which local exploration (e.g. -greedy) is sufficient to achieve a high scoring policy (easy), and those for which it is not (hard). For example, SPACE INVADERS versus PITFALL!. We further divided the easy group based on whether an -greedy scheme finds a score exploit, that is maximizes the score without achieving the game’s stated objective. For eaxmple, KUNG-FU MASTER versus BOXING. While this distinction is not directly used here, score exploits lead to behaviours which are optimal from an ALE perspective but uninteresting to humans. We divide the games in the hard category into dense reward games (MS. PAC-MAN) and sparse reward games (MONTEZUMA’S REVENGE).\nD.3 Exploration in MONTEZUMA’S REVENGE.\nThe agent’s current room number ranges from 0 to 23 (Figure 3) and is stored at RAM location 0x83. Figure 7 shows the set of rooms explored by our DQN agents at different points during training.\nWe remark that without mixing in the Monte-Carlo return, our bonus-based agent still explores significantly more than the no-bonus agent. However, the deep network seems unable to maintain a sufficiently good approximation to the value function, and performance quickly deteriorates. Comparable results using the A3C method provide another example of the practical importance of eligibility traces and return-based methods in reinforcement learning.\nD.4 Improving exploration for actor-critic methods.\nOur implementation of A3C was along the lines mentioned in Mnih et al. (2016) using 16 threads. Each thread, corresponding to an actor learner maintains a copy of the density model. All the threads are synchronized with one of the threads at regular intervals of 250,000 steps. We followed the same training procedure as that reported in the A3C paper with the following additional steps: We update our density model with the observations generated by following the policy. During the policy gradient step, we compute the intrinsic rewards by querying the density model and add it to the extrinsic rewards before clipping them in the range [−1, 1] as was done in the A3C paper. This resulted in minimal overhead in computation costs and the memory footprint was manageable (< 32 GB) for most of the Atari games. Our training times were almost the same as the ones reported\nin the A3C paper. We picked β = 0.01 after performing a short parameter sweep over the training games. The choice of training games is the same as mentioned in the A3C paper.\nThe games on which DQN achieves a score of 150% or less of the random score are: ASTEROIDS, DOUBLE DUNK, GRAVITAR, ICE HOCKEY, MONTEZUMA’S REVENGE, PITFALL!, SKIING, SURROUND, TENNIS, TIME PILOT.\nThe games on which A3C achieves a score of 150% or less of the random score are: BATTLE ZONE, BOWLING, ENDURO, FREEWAY, GRAVITAR, KANGAROO, PITFALL!, ROBOTANK, SKIING, SOLARIS, SURROUND, TENNIS, TIME PILOT, VENTURE.\nThe games on which A3C+ achieves a score of 150% or less of the random score are: DOUBLE DUNK, GRAVITAR, ICE HOCKEY, PITFALL!, SKIING, SOLARIS, SURROUND, TENNIS, TIME PILOT, VENTURE.\nOur experiments involved the stochastic version of the Arcade Learning Environment (ALE) without a terminal signal for life loss, which is now the default ALE setting. Briefly, the stochasticity is achieved by accepting the agent action at each frame with probability 1− p and using the agents previous action during rejection. We used the ALE’s default value of p = 0.25 as has been previously used in Bellemare et al. (2016). For comparison, Table 2 also reports the deterministic + life loss setting also used in the literature.\nAnecdotally, we found that using the life loss signal, while helpful in achieving high scores in some games, is detrimental in MONTEZUMA’S REVENGE. Recall that the life loss signal was used by Mnih et al. (2015) to treat each of the agent’ lives as a separate episode. For comparison, after 200 million frames A3C+ achieves the following average scores: 1) Stochastic + Life Loss: 142.50; 2) Deterministic + Life Loss: 273.70 3) Stochastic without Life Loss: 1127.05 4) Deterministic without Life Loss: 273.70. The maximum score achieved by 3) is 3600, in comparison to the maximum of 500 achieved by 1) and 3). This large discrepancy is not unsurprising when one considers that losing a life in MONTEZUMA’S REVENGE, and in fact in most games, is very different from restarting a new episode.\nStochastic ALE Deterministic ALE A3C A3C+ DQN A3C A3C+ DQN\nALIEN 1968.40 1848.33 1802.08 1658.25 1945.66 1418.47 AMIDAR 1065.24 964.77 781.76 1034.15 861.14 654.40\nASSAULT 2660.55 2607.28 1246.83 2881.69 2584.40 1707.87 ASTERIX 7212.45 7262.77 3256.07 9546.96 7922.70 4062.55 ASTEROIDS 2680.72 2257.92 525.09 3946.22 2406.57 735.05 ATLANTIS 1752259.74 1733528.71 77670.03 1634837.98 1801392.35 281448.80 BANK HEIST 1071.89 991.96 419.50 1301.51 1182.89 315.93 BATTLE ZONE 3142.95 7428.99 16757.88 3393.84 7969.06 17927.46 BEAM RIDER 6129.51 5992.08 4653.24 7004.58 6723.89 7949.08\nBERZERK 1203.09 1720.56 416.03 1233.47 1863.60 471.76 BOWLING 32.91 68.72 29.07 35.00 75.97 30.34\nBOXING 4.48 13.82 66.13 3.07 15.75 80.17 BREAKOUT 322.04 323.21 85.82 432.42 473.93 259.40 CENTIPEDE 4488.43 5338.24 4698.76 5184.76 5442.94 1184.46\nCHOPPER COMMAND 4377.91 5388.22 1927.50 3324.24 5088.17 1569.84 CRAZY CLIMBER 108896.28 104083.51 86126.17 111493.76 112885.03 102736.12\nDEFENDER 42147.48 36377.60 4593.79 39388.08 38976.66 6225.82 DEMON ATTACK 26803.86 19589.95 4831.12 39293.17 30930.33 6183.58 DOUBLE DUNK 0.53 -8.88 -11.57 0.19 -7.84 -13.99 ENDURO 0.00 749.11 348.30 0.00 694.83 441.24 FISHING DERBY 30.42 29.46 -27.83 32.00 31.11 -8.68\nFREEWAY 0.00 27.33 30.59 0.00 30.48 30.12 FROSTBITE 290.02 506.61 707.41 283.99 325.42 506.10\nGOPHER 5724.01 5948.40 3946.13 6872.60 6611.28 4946.39 GRAVITAR 204.65 246.02 43.04 201.29 238.68 219.39 H.E.R.O. 32612.96 15077.42 12140.76 34880.51 15210.62 11419.16 ICE HOCKEY -5.22 -7.05 -9.78 -5.13 -6.45 -10.34 JAMES BOND 424.11 1024.16 511.76 422.42 1001.19 465.76\nKANGAROO 47.19 5475.73 4170.09 46.63 4883.53 5972.64 KRULL 7263.37 7587.58 5775.23 7603.84 8605.27 6140.24\nKUNG-FU MASTER 26878.72 26593.67 15125.08 29369.90 28615.43 11187.13 MONTEZUMA’S REVENGE 0.06 142.50 0.02 0.17 273.70 0.00\nMS. PAC-MAN 2163.43 2380.58 2480.39 2327.80 2401.04 2391.89 NAME THIS GAME 6202.67 6427.51 3631.90 6087.31 7021.30 6565.41\nPHOENIX 12169.75 20300.72 3015.64 13893.06 23818.47 7835.20 PITFALL -8.83 -155.97 -84.40 -6.98 -259.09 -86.85 POOYAN 3706.93 3943.37 2817.36 4198.61 4305.57 2992.56\nPONG 18.21 17.33 15.10 20.84 20.75 19.17 PRIVATE EYE 94.87 100.00 69.53 97.36 99.32 -12.86\nQ*BERT 15007.55 15804.72 5259.18 19175.72 19257.55 7094.91 RIVER RAID 10559.82 10331.56 8934.68 11902.24 10712.54 2365.18 ROAD RUNNER 36933.62 49029.74 31613.83 41059.12 50645.74 24933.39 ROBOTANK 2.13 6.68 50.80 2.22 7.68 40.53 SEAQUEST 1680.84 2274.06 1180.70 1697.19 2015.55 3035.32\nSKIING -23669.98 -20066.65 -26402.39 -20958.97 -22177.50 -27972.63 SOLARIS 2156.96 2175.70 805.66 2102.13 2270.15 1752.72\nSPACE INVADERS 1653.59 1466.01 1428.94 1741.27 1531.64 1101.43 STAR GUNNER 55221.64 52466.84 47557.16 59218.08 55233.43 40171.44\nSURROUND -7.79 -6.99 -8.77 -7.10 -7.21 -8.19 TENNIS -12.44 -20.49 -12.98 -16.18 -23.06 -8.00\nTIME PILOT 7417.08 3816.38 2808.92 9000.91 4103.00 4067.51 TUTANKHAM 250.03 132.67 70.84 273.66 112.14 75.21 UP AND DOWN 34362.80 8705.64 4139.20 44883.40 23106.24 5208.67 VENTURE 0.00 0.00 54.86 0.00 0.00 0.00 VIDEO PINBALL 53488.73 35515.91 55326.08 68287.63 97372.80 52995.08 WIZARD OF WOR 4402.10 3657.65 1231.23 4347.76 3355.09 378.70 YAR’S REVENGE 19039.24 12317.49 14236.94 20006.02 13398.73 15042.75\nZAXXON 121.35 7956.05 2333.52 152.11 7451.25 2481.40 Times Best 26 24 8 26 25 9\nTable 2: Average score after 200 million training frames for A3C and A3C+ (with\n√ N̂n bonus),\nwith a DQN baseline for comparison."
    } ],
    "references" : [ {
      "title" : "Near-optimal BRL using optimistic local transitions",
      "author" : [ "M. Araya-López", "V. Thomas", "O. Buffet" ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning.",
      "citeRegEx" : "Araya.López et al\\.,? 2012",
      "shortCiteRegEx" : "Araya.López et al\\.",
      "year" : 2012
    }, {
      "title" : "Speedy Q-learning",
      "author" : [ "M.G. Azar", "R. Munos", "M. Gavamzadeh", "H.J. Kappen" ],
      "venue" : "Advances in Neural Information Processing Systems 24.",
      "citeRegEx" : "Azar et al\\.,? 2011",
      "shortCiteRegEx" : "Azar et al\\.",
      "year" : 2011
    }, {
      "title" : "Intrinsic motivation and reinforcement learning",
      "author" : [ "A.G. Barto" ],
      "venue" : "Intrinsically Motivated Learning in Natural and Artificial Systems, pages 17–47. Springer.",
      "citeRegEx" : "Barto,? 2013",
      "shortCiteRegEx" : "Barto",
      "year" : 2013
    }, {
      "title" : "Skip context tree switching",
      "author" : [ "M. Bellemare", "J. Veness", "E. Talvitie" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning, pages 1458–1466.",
      "citeRegEx" : "Bellemare et al\\.,? 2014",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2014
    }, {
      "title" : "Count-based frequency estimation using bounded memory",
      "author" : [ "M.G. Bellemare" ],
      "venue" : "Proceedings of the 24th International Joint Conference on Artificial Intelligence.",
      "citeRegEx" : "Bellemare,? 2015",
      "shortCiteRegEx" : "Bellemare",
      "year" : 2015
    }, {
      "title" : "The Arcade Learning Environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research, 47:253–279.",
      "citeRegEx" : "Bellemare et al\\.,? 2013",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Increasing the action gap: New operators for reinforcement learning",
      "author" : [ "M.G. Bellemare", "G. Ostrovski", "A. Guez", "P.S. Thomas", "R. Munos" ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Bellemare et al\\.,? 2016",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2016
    }, {
      "title" : "Investigating contingency awareness using Atari 2600 games",
      "author" : [ "M.G. Bellemare", "J. Veness", "M. Bowling" ],
      "venue" : "Proceedings of the 26th AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Bellemare et al\\.,? 2012",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2012
    }, {
      "title" : "Dynamic programming",
      "author" : [ "R.E. Bellman" ],
      "venue" : "Princeton University Press, Princeton, NJ.",
      "citeRegEx" : "Bellman,? 1957",
      "shortCiteRegEx" : "Bellman",
      "year" : 1957
    }, {
      "title" : "Neuro-Dynamic Programming",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific.",
      "citeRegEx" : "Bertsekas and Tsitsiklis,? 1996",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis",
      "year" : 1996
    }, {
      "title" : "R-max - a general polynomial time algorithm for near optimal reinforcement learning",
      "author" : [ "R. Brafman", "M. Tennenholtz" ],
      "venue" : "Journal of Machine Learning Research, 3:213–231.",
      "citeRegEx" : "Brafman and Tennenholtz,? 2002",
      "shortCiteRegEx" : "Brafman and Tennenholtz",
      "year" : 2002
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Machine Learning, 5(1):1–122.",
      "citeRegEx" : "Bubeck and Cesa.Bianchi,? 2012",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi",
      "year" : 2012
    }, {
      "title" : "Elements of information theory",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "Cover and Thomas,? 1991",
      "shortCiteRegEx" : "Cover and Thomas",
      "year" : 1991
    }, {
      "title" : "Bayesian Q-learning",
      "author" : [ "R. Dearden", "N. Friedman", "S. Russell" ],
      "venue" : "Proceedings of the Fifteenth National Conference on Artificial Intelligence, pages 761–768.",
      "citeRegEx" : "Dearden et al\\.,? 1998",
      "shortCiteRegEx" : "Dearden et al\\.",
      "year" : 1998
    }, {
      "title" : "An object-oriented representation for efficient reinforcement learning",
      "author" : [ "C. Diuk", "A. Cohen", "M.L. Littman" ],
      "venue" : "Proceedings of the 25th International Conference on Machine Learning, pages 240–247. ACM.",
      "citeRegEx" : "Diuk et al\\.,? 2008",
      "shortCiteRegEx" : "Diuk et al\\.",
      "year" : 2008
    }, {
      "title" : "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes",
      "author" : [ "M.O. Duff" ],
      "venue" : "PhD thesis, University of Massachusetts Amherst.",
      "citeRegEx" : "Duff,? 2002",
      "shortCiteRegEx" : "Duff",
      "year" : 2002
    }, {
      "title" : "Convergence of optimistic and incremental Q-learning",
      "author" : [ "E. Even-Dar", "Y. Mansour" ],
      "venue" : "Advances in Neural Information Proceesing Systems 14.",
      "citeRegEx" : "Even.Dar and Mansour,? 2001",
      "shortCiteRegEx" : "Even.Dar and Mansour",
      "year" : 2001
    }, {
      "title" : "Efficient bayesian parameter estimation in large discrete domains",
      "author" : [ "N. Friedman", "Y. Singer" ],
      "venue" : "Advances in Neural Information Processing Systems 11.",
      "citeRegEx" : "Friedman and Singer,? 1999",
      "shortCiteRegEx" : "Friedman and Singer",
      "year" : 1999
    }, {
      "title" : "Universal artificial intelligence: Sequential decisions based on algorithmic probability",
      "author" : [ "M. Hutter" ],
      "venue" : "Springer.",
      "citeRegEx" : "Hutter,? 2005",
      "shortCiteRegEx" : "Hutter",
      "year" : 2005
    }, {
      "title" : "Sparse adaptive dirichlet-multinomial-like processes",
      "author" : [ "M. Hutter" ],
      "venue" : "Proceedings of the Conference on Online Learning Theory.",
      "citeRegEx" : "Hutter,? 2013",
      "shortCiteRegEx" : "Hutter",
      "year" : 2013
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "T. Jaksch", "R. Ortner", "P. Auer" ],
      "venue" : "Journal of Machine Learning Research, 11:1563–1600.",
      "citeRegEx" : "Jaksch et al\\.,? 2010",
      "shortCiteRegEx" : "Jaksch et al\\.",
      "year" : 2010
    }, {
      "title" : "Near-bayesian exploration in polynomial time",
      "author" : [ "Z.J. Kolter", "A.Y. Ng" ],
      "venue" : "Proceedings of the 26th International Conference on Machine Learning.",
      "citeRegEx" : "Kolter and Ng,? 2009",
      "shortCiteRegEx" : "Kolter and Ng",
      "year" : 2009
    }, {
      "title" : "PAC bounds for discounted MDPs",
      "author" : [ "T. Lattimore", "M. Hutter" ],
      "venue" : "Proceedings of the Conference on Algorithmic Learning Theory.",
      "citeRegEx" : "Lattimore and Hutter,? 2012",
      "shortCiteRegEx" : "Lattimore and Hutter",
      "year" : 2012
    }, {
      "title" : "State of the Art Control of Atari Games Using Shallow Reinforcement Learning",
      "author" : [ "Y. Liang", "M.C. Machado", "E. Talvitie", "M.H. Bowling" ],
      "venue" : "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems.",
      "citeRegEx" : "Liang et al\\.,? 2016",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain-independent optimistic initialization for reinforcement learning",
      "author" : [ "M.C. Machado", "S. Srinivasan", "M. Bowling" ],
      "venue" : "arXiv preprint arXiv:1410.4604.",
      "citeRegEx" : "Machado et al\\.,? 2014",
      "shortCiteRegEx" : "Machado et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical optimistic region selection driven by curiosity",
      "author" : [ "Maillard", "O.-A." ],
      "venue" : "Advances in Neural Information Processing Systems 25.",
      "citeRegEx" : "Maillard and O..A.,? 2012",
      "shortCiteRegEx" : "Maillard and O..A.",
      "year" : 2012
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1602.01783.",
      "citeRegEx" : "Mnih et al\\.,? 2016",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Variational information maximisation for intrinsically motivated reinforcement learning",
      "author" : [ "S. Mohamed", "D.J. Rezende" ],
      "venue" : "Advances in Neural Information Processing Systems 28.",
      "citeRegEx" : "Mohamed and Rezende,? 2015",
      "shortCiteRegEx" : "Mohamed and Rezende",
      "year" : 2015
    }, {
      "title" : "Action-conditional video prediction using deep networks in atari games",
      "author" : [ "J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2845–2853.",
      "citeRegEx" : "Oh et al\\.,? 2015",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2015
    }, {
      "title" : "Universal knowledge-seeking agents for stochastic environments",
      "author" : [ "L. Orseau", "T. Lattimore", "M. Hutter" ],
      "venue" : "Proceedings of the Conference on Algorithmic Learning Theory.",
      "citeRegEx" : "Orseau et al\\.,? 2013",
      "shortCiteRegEx" : "Orseau et al\\.",
      "year" : 2013
    }, {
      "title" : "Intrinsic motivation systems for autonomous mental development",
      "author" : [ "P. Oudeyer", "F. Kaplan", "V. Hafner" ],
      "venue" : "IEEE Transactions on Evolutionary Computation, 11(2):265–286.",
      "citeRegEx" : "Oudeyer et al\\.,? 2007",
      "shortCiteRegEx" : "Oudeyer et al\\.",
      "year" : 2007
    }, {
      "title" : "Efficient PAC-optimal exploration in concurrent, continuous state MDPs with delayed updates",
      "author" : [ "J. Pazis", "R. Parr" ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Pazis and Parr,? 2016",
      "shortCiteRegEx" : "Pazis and Parr",
      "year" : 2016
    }, {
      "title" : "An analytic solution to discrete bayesian reinforcement learning",
      "author" : [ "P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan" ],
      "venue" : "Proceedings of the 23rd International Conference on Machine Learning.",
      "citeRegEx" : "Poupart et al\\.,? 2006",
      "shortCiteRegEx" : "Poupart et al\\.",
      "year" : 2006
    }, {
      "title" : "CHILD: A first step towards continual learning",
      "author" : [ "M.B. Ring" ],
      "venue" : "Machine Learning, 28(1):77–104.",
      "citeRegEx" : "Ring,? 1997",
      "shortCiteRegEx" : "Ring",
      "year" : 1997
    }, {
      "title" : "Prioritized experience replay",
      "author" : [ "T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver" ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Schaul et al\\.,? 2016",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2016
    }, {
      "title" : "Curiosity-driven optimization",
      "author" : [ "T. Schaul", "Y. Sun", "D. Wierstra", "F. Gomez", "J. Schmidhuber" ],
      "venue" : "IEEE Congress on Evolutionary Computation.",
      "citeRegEx" : "Schaul et al\\.,? 2011",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2011
    }, {
      "title" : "A possibility for implementing curiosity and boredom in model-building neural controllers",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "From animals to animats: proceedings of the first international conference on simulation of adaptive behavior.",
      "citeRegEx" : "Schmidhuber,? 1991",
      "shortCiteRegEx" : "Schmidhuber",
      "year" : 1991
    }, {
      "title" : "Driven by compression progress",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Knowledge-Based Intelligent Information and Engineering Systems. Springer.",
      "citeRegEx" : "Schmidhuber,? 2008",
      "shortCiteRegEx" : "Schmidhuber",
      "year" : 2008
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis" ],
      "venue" : null,
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "An intrinsic reward mechanism for efficient exploration",
      "author" : [ "O. Simsek", "A.G. Barto" ],
      "venue" : "Proceedings of the 23rd International Conference on Machine Learning.",
      "citeRegEx" : "Simsek and Barto,? 2006",
      "shortCiteRegEx" : "Simsek and Barto",
      "year" : 2006
    }, {
      "title" : "Intrinsically motivated reinforcement learning",
      "author" : [ "S. Singh", "A.G. Barto", "N. Chentanez" ],
      "venue" : "Advances in Neural Information Processing Systems 16.",
      "citeRegEx" : "Singh et al\\.,? 2004",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2004
    }, {
      "title" : "Incentivizing exploration in reinforcement learning with deep predictive models",
      "author" : [ "B.C. Stadie", "S. Levine", "P. Abbeel" ],
      "venue" : "arXiv preprint arXiv:1507.00814.",
      "citeRegEx" : "Stadie et al\\.,? 2015",
      "shortCiteRegEx" : "Stadie et al\\.",
      "year" : 2015
    }, {
      "title" : "An information-theoretic approach to curiosity-driven reinforcement learning",
      "author" : [ "S. Still", "D. Precup" ],
      "venue" : "Theory in Biosciences, 131(3):139–148.",
      "citeRegEx" : "Still and Precup,? 2012",
      "shortCiteRegEx" : "Still and Precup",
      "year" : 2012
    }, {
      "title" : "An analysis of model-based interval estimation for Markov decision processes",
      "author" : [ "A.L. Strehl", "M.L. Littman" ],
      "venue" : "Journal of Computer and System Sciences, 74(8):1309 – 1331.",
      "citeRegEx" : "Strehl and Littman,? 2008",
      "shortCiteRegEx" : "Strehl and Littman",
      "year" : 2008
    }, {
      "title" : "Model-based reinforcement learning with nearly tight exploration complexity bounds",
      "author" : [ "I. Szita", "C. Szepesvári" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning.",
      "citeRegEx" : "Szita and Szepesvári,? 2010",
      "shortCiteRegEx" : "Szita and Szepesvári",
      "year" : 2010
    }, {
      "title" : "The role of exploration in learning control",
      "author" : [ "S.B. Thrun" ],
      "venue" : "Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches, pages 1–27.",
      "citeRegEx" : "Thrun,? 1992",
      "shortCiteRegEx" : "Thrun",
      "year" : 1992
    }, {
      "title" : "Deep reinforcement learning with double Q-learning",
      "author" : [ "H. van Hasselt", "A. Guez", "D. Silver" ],
      "venue" : "In Proceedings of the 30th AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "Compress and control",
      "author" : [ "J. Veness", "M.G. Bellemare", "M. Hutter", "A. Chua", "G. Desjardins" ],
      "venue" : "Proceedings of the 29th AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Veness et al\\.,? 2015",
      "shortCiteRegEx" : "Veness et al\\.",
      "year" : 2015
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning, 1(1-2):1–305.",
      "citeRegEx" : "Wainwright and Jordan,? 2008",
      "shortCiteRegEx" : "Wainwright and Jordan",
      "year" : 2008
    }, {
      "title" : "Dual representations for dynamic programming",
      "author" : [ "T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans" ],
      "venue" : "Journal of Machine Learning Research, pages 1–29.",
      "citeRegEx" : "Wang et al\\.,? 2008",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2008
    }, {
      "title" : "Motivation reconsidered: the concept of competence",
      "author" : [ "R.W. White" ],
      "venue" : "Psychological review, 66(5):297.",
      "citeRegEx" : "White,? 1959",
      "shortCiteRegEx" : "White",
      "year" : 1959
    }, {
      "title" : "The CTS sequential density model treats x ∈ X as a factored observation, where each (i, j) pixel corresponds to a factor x . The parents of this factor are its upper-left neighbours, i.e. pixels (i − 1, j)",
      "author" : [ "Bellemare" ],
      "venue" : null,
      "citeRegEx" : "Bellemare,? \\Q2015\\E",
      "shortCiteRegEx" : "Bellemare",
      "year" : 2015
    }, {
      "title" : "tain a sufficiently good approximation to the value function, and performance quickly deteriorates. Comparable results using the A3C method provide another example of the practical importance of eligibility traces and return-based methods in reinforcement learning",
      "author" : [ "Mnih" ],
      "venue" : null,
      "citeRegEx" : "Mnih,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih",
      "year" : 2016
    }, {
      "title" : "Our experiments involved the stochastic version of the Arcade Learning Environment (ALE) without a terminal signal for life loss, which is now the default ALE setting. Briefly, the stochasticity is achieved by accepting the agent action at each frame with probability 1− p and using the agents previous action during rejection. We used the ALE’s default value of p = 0.25",
      "author" : [ "Bellemare" ],
      "venue" : null,
      "citeRegEx" : "Bellemare,? \\Q2016\\E",
      "shortCiteRegEx" : "Bellemare",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Most Bayesian exploration schemes use counts in their posterior estimates, for example when the uncertainty over transition probabilities is captured by an exponential family prior (Dearden et al., 1998; Duff, 2002; Poupart et al., 2006).",
      "startOffset" : 181,
      "endOffset" : 237
    }, {
      "referenceID" : 15,
      "context" : "Most Bayesian exploration schemes use counts in their posterior estimates, for example when the uncertainty over transition probabilities is captured by an exponential family prior (Dearden et al., 1998; Duff, 2002; Poupart et al., 2006).",
      "startOffset" : 181,
      "endOffset" : 237
    }, {
      "referenceID" : 33,
      "context" : "Most Bayesian exploration schemes use counts in their posterior estimates, for example when the uncertainty over transition probabilities is captured by an exponential family prior (Dearden et al., 1998; Duff, 2002; Poupart et al., 2006).",
      "startOffset" : 181,
      "endOffset" : 237
    }, {
      "referenceID" : 11,
      "context" : "There are near-optimal algorithms for multi-armed bandits (Bubeck and Cesa-Bianchi, 2012); the hardness of exploration in Markov Decision Processes (MDPs) is well-understood (Jaksch et al.",
      "startOffset" : 58,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : "There are near-optimal algorithms for multi-armed bandits (Bubeck and Cesa-Bianchi, 2012); the hardness of exploration in Markov Decision Processes (MDPs) is well-understood (Jaksch et al., 2010; Lattimore and Hutter, 2012).",
      "startOffset" : 174,
      "endOffset" : 223
    }, {
      "referenceID" : 22,
      "context" : "There are near-optimal algorithms for multi-armed bandits (Bubeck and Cesa-Bianchi, 2012); the hardness of exploration in Markov Decision Processes (MDPs) is well-understood (Jaksch et al., 2010; Lattimore and Hutter, 2012).",
      "startOffset" : 174,
      "endOffset" : 223
    }, {
      "referenceID" : 39,
      "context" : "By stark contrast, contemporary practical successes in reinforcement learning (e.g. Mnih et al., 2015; Silver et al., 2016) still rely on simple forms of exploration, for example -greedy policies – what Thrun (1992) calls undirected exploration.",
      "startOffset" : 78,
      "endOffset" : 123
    }, {
      "referenceID" : 32,
      "context" : "However, save for some recent work in continuous state spaces for which a good metric is given (Pazis and Parr, 2016), there has not yet been a thoroughly convincing attempt at generalizing counts.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 37,
      "context" : "Intrinsic motivation (Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013) offers a different perspective on exploration.",
      "startOffset" : 21,
      "endOffset" : 75
    }, {
      "referenceID" : 31,
      "context" : "Intrinsic motivation (Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013) offers a different perspective on exploration.",
      "startOffset" : 21,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "Intrinsic motivation (Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013) offers a different perspective on exploration.",
      "startOffset" : 21,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "There are near-optimal algorithms for multi-armed bandits (Bubeck and Cesa-Bianchi, 2012); the hardness of exploration in Markov Decision Processes (MDPs) is well-understood (Jaksch et al., 2010; Lattimore and Hutter, 2012). By stark contrast, contemporary practical successes in reinforcement learning (e.g. Mnih et al., 2015; Silver et al., 2016) still rely on simple forms of exploration, for example -greedy policies – what Thrun (1992) calls undirected exploration.",
      "startOffset" : 59,
      "endOffset" : 441
    }, {
      "referenceID" : 51,
      "context" : "rogates for extrinsic rewards – to drive curiosity within an agent, influenced by classic ideas from psychology (White, 1959).",
      "startOffset" : 112,
      "endOffset" : 125
    }, {
      "referenceID" : 41,
      "context" : "To sketch out some recurring themes, these novelty signals might be prediction error (Singh et al., 2004; Stadie et al., 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 42,
      "context" : "To sketch out some recurring themes, these novelty signals might be prediction error (Singh et al., 2004; Stadie et al., 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 40,
      "context" : ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 37,
      "context" : ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 43,
      "context" : ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).",
      "startOffset" : 108,
      "endOffset" : 159
    }, {
      "referenceID" : 28,
      "context" : ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015).",
      "startOffset" : 108,
      "endOffset" : 159
    }, {
      "referenceID" : 34,
      "context" : "The idea also finds roots in continual learning (Ring, 1997).",
      "startOffset" : 48,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "We bring them to bear on Atari 2600 games from the Arcade Learning Environment (Bellemare et al., 2013), focusing on games where myopic exploration fails.",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 44,
      "context" : "We extract our pseudo-counts from a simple sequential model and use them within a variant of model-based interval estimation with bonuses (Strehl and Littman, 2008).",
      "startOffset" : 138,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015). The idea also finds roots in continual learning (Ring, 1997). In Thrun’s taxonomy, intrinsic motivation methods fall within the category of error-based exploration. However, while in toto a promising approach, intrinsic motivation has so far yielded few theoretical guarantees, save perhaps for the work of Maillard (2012) and the universally curious agent of Orseau et al.",
      "startOffset" : 33,
      "endOffset" : 484
    }, {
      "referenceID" : 2,
      "context" : ", 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015). The idea also finds roots in continual learning (Ring, 1997). In Thrun’s taxonomy, intrinsic motivation methods fall within the category of error-based exploration. However, while in toto a promising approach, intrinsic motivation has so far yielded few theoretical guarantees, save perhaps for the work of Maillard (2012) and the universally curious agent of Orseau et al. (2013). We provide what we believe is the first formal evidence that intrinsic motivation and count-based exploration are but two sides of the same coin.",
      "startOffset" : 33,
      "endOffset" : 542
    }, {
      "referenceID" : 48,
      "context" : "When Y is small and finite and we are given a sequential density model ρ over X , we can construct a reasonable sequential density model over the joint alphabet X ×Y by means of a chain rule-like construct (Veness et al., 2015).",
      "startOffset" : 206,
      "endOffset" : 227
    }, {
      "referenceID" : 44,
      "context" : "Model-based interval estimation (MBIE; Strehl and Littman, 2008) explores by acting optimistically with respect to the agent’s empirical model of the environment.",
      "startOffset" : 32,
      "endOffset" : 64
    }, {
      "referenceID" : 20,
      "context" : "A closely related algorithm, UCRL2, exhibits low regret in the average-reward setting (Jaksch et al., 2010).",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "Model-based interval estimation with exploratory bonus (MBIE-EB) is a simplification which augments the estimated reward function at every state and solves Bellman’s equation (Bellman, 1957):",
      "startOffset" : 175,
      "endOffset" : 190
    }, {
      "referenceID" : 21,
      "context" : "Bayesian Exploration Bonus (BEB; Kolter and Ng, 2009) replaces the empirical estimates P̂ and R̂ with Dirichlet estimators.",
      "startOffset" : 27,
      "endOffset" : 53
    }, {
      "referenceID" : 19,
      "context" : "Nor is the problem solved by a Bayesian approach: even variable-alphabet estimators (e.g. Friedman and Singer, 1999; Hutter, 2013; Bellemare, 2015) must assign a small, diminishing probability to yet-unseen states.",
      "startOffset" : 84,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "Nor is the problem solved by a Bayesian approach: even variable-alphabet estimators (e.g. Friedman and Singer, 1999; Hutter, 2013; Bellemare, 2015) must assign a small, diminishing probability to yet-unseen states.",
      "startOffset" : 84,
      "endOffset" : 147
    }, {
      "referenceID" : 12,
      "context" : "The term “recoding” is inspired from the statistical compression literature, where coding costs are inversely related to probabilities (Cover and Thomas, 1991).",
      "startOffset" : 135,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "We use the Arcade Learning Environment (Bellemare et al., 2013).",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "Echoing the pioneering work of Diuk et al. (2008) on the Atari 2600, we define the “salient event” to occur whenever the agent is located in the area rightwards from the starting screen.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "We use a simplified version of the CTS sequential density model for Atari 2600 frames proposed by Bellemare et al. (2014) and used for density estimation proper by Veness et al.",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "We use a simplified version of the CTS sequential density model for Atari 2600 frames proposed by Bellemare et al. (2014) and used for density estimation proper by Veness et al. (2015). While the CTS model is rather impoverished in comparison to state-of-the-art modelling algorithms (e.",
      "startOffset" : 98,
      "endOffset" : 185
    }, {
      "referenceID" : 48,
      "context" : "Finally, we expose a relationship between our pseudo-count and a density-based approach to value function approximation called Compress and Control (Veness et al., 2015).",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 49,
      "context" : "We next show that directed graphical models (Wainwright and Jordan, 2008) satisfy Assumption 1.",
      "startOffset" : 44,
      "endOffset" : 73
    }, {
      "referenceID" : 48,
      "context" : "The Compress and Control method (Veness et al., 2015) applies sequential density models to the problem of reinforcement learning.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 50,
      "context" : "It uses these models to learn a form of dual value function based on stationary distributions (Wang et al., 2008).",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "We believe this relationship is the natural extension of the close relationship between tabular value estimation using a decaying step-size (e.g. Even-Dar and Mansour, 2001; Azar et al., 2011) and count-based exploration in the tabular setting (e.",
      "startOffset" : 140,
      "endOffset" : 192
    }, {
      "referenceID" : 44,
      "context" : ", 2011) and count-based exploration in the tabular setting (e.g. Brafman and Tennenholtz, 2002; Strehl and Littman, 2008; Jaksch et al., 2010; Szita and Szepesvári, 2010).",
      "startOffset" : 59,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : ", 2011) and count-based exploration in the tabular setting (e.g. Brafman and Tennenholtz, 2002; Strehl and Littman, 2008; Jaksch et al., 2010; Szita and Szepesvári, 2010).",
      "startOffset" : 59,
      "endOffset" : 170
    }, {
      "referenceID" : 45,
      "context" : ", 2011) and count-based exploration in the tabular setting (e.g. Brafman and Tennenholtz, 2002; Strehl and Littman, 2008; Jaksch et al., 2010; Szita and Szepesvári, 2010).",
      "startOffset" : 59,
      "endOffset" : 170
    }, {
      "referenceID" : 48,
      "context" : "which Veness et al. (2015) showed converges to V (x) when (x, z)1:n is drawn from the ergodic Markov chain jointly induced by a finite-state MDP and π.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "Intrinsic motivation algorithms seek to explain what drives behaviour in the absence of, or even contrary to, extrinsic reward (Barto, 2013).",
      "startOffset" : 127,
      "endOffset" : 140
    }, {
      "referenceID" : 30,
      "context" : "Information gain is a frequently appealed-to quantity in the intrinsic motivation literature (e.g. Schaul et al., 2011; Orseau et al., 2013).",
      "startOffset" : 93,
      "endOffset" : 140
    }, {
      "referenceID" : 30,
      "context" : "In a Bayesian sense, expected information gain is the appropriate measure of exploration when extrinsic rewards are ignored: an agent which maximizes long-term expected information gain is “optimally curious” (Orseau et al., 2013).",
      "startOffset" : 209,
      "endOffset" : 230
    }, {
      "referenceID" : 21,
      "context" : "Since an exploration bonus proportional to Nn(x, a) −1 leads to PAC-BAMDP algorithms (Kolter and Ng, 2009; Araya-López et al., 2012), we hypothesize that similar bounds can be derived for our pseudo-counts.",
      "startOffset" : 85,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "Since an exploration bonus proportional to Nn(x, a) −1 leads to PAC-BAMDP algorithms (Kolter and Ng, 2009; Araya-López et al., 2012), we hypothesize that similar bounds can be derived for our pseudo-counts.",
      "startOffset" : 85,
      "endOffset" : 132
    }, {
      "referenceID" : 35,
      "context" : "To date, the best agents require hundreds of millions of frames to attain nontrivial performance levels, and visit at best two or three rooms out of 72; most published agents (e.g. Bellemare et al., 2012; van Hasselt et al., 2016; Schaul et al., 2016; Mnih et al., 2016; Liang et al., 2016) fail to match even a human beginner’s performance.",
      "startOffset" : 175,
      "endOffset" : 290
    }, {
      "referenceID" : 26,
      "context" : "To date, the best agents require hundreds of millions of frames to attain nontrivial performance levels, and visit at best two or three rooms out of 72; most published agents (e.g. Bellemare et al., 2012; van Hasselt et al., 2016; Schaul et al., 2016; Mnih et al., 2016; Liang et al., 2016) fail to match even a human beginner’s performance.",
      "startOffset" : 175,
      "endOffset" : 290
    }, {
      "referenceID" : 23,
      "context" : "To date, the best agents require hundreds of millions of frames to attain nontrivial performance levels, and visit at best two or three rooms out of 72; most published agents (e.g. Bellemare et al., 2012; van Hasselt et al., 2016; Schaul et al., 2016; Mnih et al., 2016; Liang et al., 2016) fail to match even a human beginner’s performance.",
      "startOffset" : 175,
      "endOffset" : 290
    }, {
      "referenceID" : 24,
      "context" : "We also compare using exploration bonuses to the optimistic initialization trick proposed by (Machado et al., 2014).",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 26,
      "context" : "This very separation, however, often leads to deficient exploration: to produce any sensible results, the A3C policy parameters must be regularized with an entropy cost (Mnih et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "To demonstrate the benefits of augmenting A3C with our exploration bonus, we computed a baseline score (Bellemare et al., 2013) for A3C+ over time.",
      "startOffset" : 103,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "We next turn our attention to actor-critic methods, specifically the A3C (asynchronous actor-critic) algorithm of Mnih et al. (2016). One appeal of actor-critic methods is that their explicit separation of policy and Q-function parameters allows for a richer behaviour space.",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 44,
      "context" : "We consider the following variants: • no exploration bonus, • N̂n(x), as per MBIE-EB (Strehl and Littman, 2008); • N̂n(x), as per BEB (Kolter and Ng, 2009); and • PGn(x), related to compression progress (Schmidhuber, 2008).",
      "startOffset" : 85,
      "endOffset" : 111
    }, {
      "referenceID" : 21,
      "context" : "We consider the following variants: • no exploration bonus, • N̂n(x), as per MBIE-EB (Strehl and Littman, 2008); • N̂n(x), as per BEB (Kolter and Ng, 2009); and • PGn(x), related to compression progress (Schmidhuber, 2008).",
      "startOffset" : 134,
      "endOffset" : 155
    }, {
      "referenceID" : 38,
      "context" : "We consider the following variants: • no exploration bonus, • N̂n(x), as per MBIE-EB (Strehl and Littman, 2008); • N̂n(x), as per BEB (Kolter and Ng, 2009); and • PGn(x), related to compression progress (Schmidhuber, 2008).",
      "startOffset" : 203,
      "endOffset" : 222
    }, {
      "referenceID" : 5,
      "context" : "compare scores across 60 games, we use inter-algorithm score distributions (Bellemare et al., 2013).",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "Universal density models such as Solomonoff induction (Hutter, 2005) learn the structure of the state space at a much greater rate than the empirical estimator, violating Assumption 1b.",
      "startOffset" : 54,
      "endOffset" : 68
    } ],
    "year" : 2016,
    "abstractText" : "We consider an agent’s uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use sequential density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary sequential density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA’S REVENGE.",
    "creator" : "LaTeX with hyperref package"
  }
}