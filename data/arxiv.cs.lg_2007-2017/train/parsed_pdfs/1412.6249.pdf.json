{
  "name" : "1412.6249.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "PURINE: A BI-GRAPH BASED DEEP LEARNING FRAME- WORK",
    "authors" : [ "Min Lin", "Shuo Li", "Xuan Luo", "Shuicheng Yan" ],
    "emails" : [ "linmin@nus.edu.sg", "lishuo@nus.edu.sg", "luoxuan@nus.edu.sg", "eleyans@nus.edu.sg" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The need for training deep neural networks on large-scale datasets has motivated serveral research works that aim to accelerate the training process by parallelising the training on multiple CPUs or GPUs. There are two different ways to parallelize the training. (1) Model parallelism: the model is distributed to different computing nodes (Sutskever et al., 2014) (2) Data parallelism: different nodes train on different samples for the same model (Seide et al., 2014; Chilimbi et al., 2014). Some of the works even use a hybrid of them (Krizhevsky, 2014; Dean et al., 2012; Le, 2013). For data parallelism, there are also two schemes regarding communication between the peers. (1) the allreduce approach where all updates from the peers are aggregated at the synchronization point and the averaged update is broadcasted back to the peers (Seide et al., 2014; Krizhevsky, 2014). (2) the parameter server approach handles the reads and writes of the parameters asynchronously (Dean et al., 2012; Le, 2013; Chilimbi et al., 2014). Efficient implementations of the various parallelization schemes described by previous works are non-trivial.\nTo facilitate the implementation of various parallelization schemes, we built a bigraph-based deep learning framework called “Purine”. It is named “Purine”, which is an analog of caffeine in molecular structure, because we’ve benefited a lot from the open source Caffe framework (Jia et al., 2014) in our research and the math functions used in Purine are ported from Caffe."
    }, {
      "heading" : "2 BI-GRAPH ABSTRACTION",
      "text" : "Purine abstracts the processing procedure of deep neural networks into directed bipartite graphs (Bi-Graphs). The Bi-Graph contains two types of vertices, tensors and operators. Directed edges are only between tensors and operators and there is no interconnection within tensors or operators. Figure 1 illustrates the Bi-Graph for the convolution layer defined in Caffe.\nAll feed-forward neural nets can be represented by a directed acyclic bipartite graph, which can be solved by a universal task dispatcher. There are several works that use similar abstractions. For example, the dataflow graph in Dryad (Isard et al., 2007) and Pig Latin (Olston et al., 2008) are the same as the Bi-Graph abstraction introduced in this paper. Graphlab (Low et al., 2010) proposed\nar X\niv :1\n41 2.\n62 49\nv1 [\ncs .N\nE ]\n1 9\nD ec\n2 01\n4\na more general abstraction which is applicable to iterative algorithms. However, these systems are designed for general problems and do not support GPU."
    }, {
      "heading" : "2.1 TASK DISPATCHER",
      "text" : "Purine solves the Bi-Graph by scheduling the operators within the Bi-Graph with an event-driven task dispatcher. Execution of an operator is triggered when all the incoming tensors are ready. A tensor is ready when all its incoming operators have completed computation. The computation of the Bi-Graph starts from the sources of the graph and stops when all the sinks are reached. This process is scheduled by the task dispatcher."
    }, {
      "heading" : "2.2 ITERATIONS",
      "text" : "Although it has been argued in (Low et al., 2010) that the directed acyclic graph could not effectively express iterative algorithms as the graph structure would depend on the number of iterations. We overcome this by iteration of the graphs. Because the task dispatcher waits until all the sinks of the graph are reached, it acts as a synchronization point. Thus parallelizable operations can be put in a single graph, while sequential tasks (iterations) are implemented by iteration of graphs. A concrete example is shown in Figure 2."
    }, {
      "heading" : "3 PARALLIZATION",
      "text" : "Parallelization of the Bi-Graph on a cluster of CPUs or GPUs or mixed can be easily implemented by introducing a “location” property for the tensors and operators. The “location” property uniquely identifies the computation resource (CPUs/GPUs) on which a tensor/operator should be allocated. The “location” property comprises two fields: hostname and device id. In a multi-machine cluster, hostname identifies the machine that the vertice resides on. Device id specifies whether the tensor/operator should be allocated on CPU or GPU and the ordinal of the GPU if there are multiple GPUs installed on a single machine. Besides the “location” property, another property “thread” is assigned to operators because both CPU and GPU support multithreading. Operators with the same thread id will be queued in the same thread, while those with different ids are parallelized whenever possible. It is up to the user to decide the assignment of the graph over the computation resources."
    }, {
      "heading" : "3.1 COPY OPERATOR",
      "text" : "In the multidevice setting, data located on one device are not directly accessible by operators on another. Thus a special “Copy” operator is introduced to cross the boundary, connecting parts of the Bi-Graph on individual devices. The Copy operators, just like other operators, are scheduled by the task dispatcher. Therefore it is straightforward to overlap copy operations with other computation tasks by assigning different threads to them."
    }, {
      "heading" : "3.2 TASK DISPATCHER",
      "text" : "In the case of single machine and multiple devices, only one dispatcher process is launched. Operators are associated to their threads and scheduled by the global task dispatcher. In the case of multiple machines and multiple devices, individual dispatcher processes are launched on each of the machines. Copy operators that copy data from machine A to machine B are sinks on machine A and sources on machine B. This way, each machine only needs to schedule its own subgraph and no global scheduling mechanism or communication between dispatchers is necessary."
    }, {
      "heading" : "3.3 MODEL PARALLELISM",
      "text" : "We demonstrate how model parallelism can be implemented in Purine by taking a two-layer fully connected neural network as example. It can be extended to deeper networks easily. As is shown in Figure 3, execution of the two-layer network can be divided into three sequential steps. They are labeled as A, B, C correspondingly. To keep resources busy all the time, the network is replicated three times and executed in order."
    }, {
      "heading" : "3.4 DATA PARALLELISM",
      "text" : "Data parallelism is a simple yet straightforward way to parallelize deep networks. In data parallelism, computation peers each keep a replicate of the deep network. The communication between peers can be either synchronous or asynchronous. In the synchonous case, the gradients from peers are gathered by the parameter server. The updated parameter is calculated and copied back to all the peers.\nA hybrid of data parallelism and model parallelism has previously been proposed by Krizhevsky (2014) in which the convolution layers use data parallelism and fully connected layers use model parallelism. This is based on the observation that the number of parameters is large and thus the communication cost is big for fully connected layers. The hybrid approach greatly reduces the communication cost. A different approach to reduce communication overhead is to overlap the data transfer with computations. Double buffering is proposed by Seide et al. (2014) to break a minibatch in half and exchange the gradients of the first half while doing computaion of the second half.\nWith the scheduling of the task dispatcher in Purine, we propose a more straightforward way to hide the communication overhead. We show that data parallelism is feasible even for fully connected layers. Especially when the network is very deep. Since the fully connected layers are usually at the top of the neural networks, exchange of the parameter gradients can be overlapped with the backward computation of the convolution layers. As is shown in Figure 4, exchange of gradients in\nthe higher layer can be overlapped with the computation of lower layers. Gradient exchange of lower layers could be less of a problem because they usually have a much smaller number of parameters."
    }, {
      "heading" : "4 RESULTS",
      "text" : "We carried out experiments on the Purine framework with data parallelism on GoogLeNet (Szegedy et al., 2014). Data parallelism often results in larger batch sizes, which are unfavorable for SGD convergence demonstrated by previous studies. In this paper we ignored the possible change in convergence rate but instead studied how much more data can be processed per unit time with the parallelization.\nWe compared the number of images processed per second for GoogLeNet with different numbers of GPUs for data parallelism. The batch size we use is 128 per GPU. As is shown in Table 1, the\nspeed increases linearly with more GPUs added when the number of GPUs is within 4 (on the same machine). With 8 GPUs on two machines, the performance increases about 7.3 fold. Note that the machines are connected by gigabit ethernet and thus data on GPU need to go through CPU memory to be tranferred over the ethernet. The latency and speed are expected to improve if the hardware is upgraded to 10 gigabit ethernet or infinity band.\nRunning GoogLeNet with 4 GPUs on a single machine is profiled and shown in Figure 5. It can be seen that the memory copy of model parameters between CPUs and GPUs is fully overlapped with the computations in the backward pass. The only overhead is in the first layer of the network, which results in the gap between iterations. Since it is more favorable to have smaller batch size in SGD, we further reduced the batch size from 128 to 64 and 32. Because the communication computation ratio is not very large with the batch size of 128, it is possible to use smaller batch sizes without hampering the speed.\nTable 2 shows the processing speed with different batch sizes. The acceleration ratio is not reduced much with batch size 64 as compared to 128. We can still achieve 3.63 fold acceleration with four GPUs when the batch size is set to 32."
    } ],
    "references" : [ {
      "title" : "Project adam: Building an efficient and scalable deep learning training system",
      "author" : [ "Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman" ],
      "venue" : "In Proceedings of the 11th USENIX conference on Operating Systems Design and Implementation,",
      "citeRegEx" : "Chilimbi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chilimbi et al\\.",
      "year" : 2014
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dean et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 2012
    }, {
      "title" : "Dryad: distributed dataparallel programs from sequential building blocks",
      "author" : [ "Michael Isard", "Mihai Budiu", "Yuan Yu", "Andrew Birrell", "Dennis Fetterly" ],
      "venue" : "In ACM SIGOPS Operating Systems Review,",
      "citeRegEx" : "Isard et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Isard et al\\.",
      "year" : 2007
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell" ],
      "venue" : "In Proceedings of the ACM International Conference on Multimedia,",
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "One weird trick for parallelizing convolutional neural networks",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : "arXiv preprint arXiv:1404.5997,",
      "citeRegEx" : "Krizhevsky.,? \\Q2014\\E",
      "shortCiteRegEx" : "Krizhevsky.",
      "year" : 2014
    }, {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Quoc V Le" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Le.,? \\Q2013\\E",
      "shortCiteRegEx" : "Le.",
      "year" : 2013
    }, {
      "title" : "Graphlab: A new framework for parallel machine learning",
      "author" : [ "Yucheng Low", "Joseph Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin", "Joseph M Hellerstein" ],
      "venue" : "arXiv preprint arXiv:1006.4990,",
      "citeRegEx" : "Low et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2010
    }, {
      "title" : "Pig latin: a not-so-foreign language for data processing",
      "author" : [ "Christopher Olston", "Benjamin Reed", "Utkarsh Srivastava", "Ravi Kumar", "Andrew Tomkins" ],
      "venue" : "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,",
      "citeRegEx" : "Olston et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Olston et al\\.",
      "year" : 2008
    }, {
      "title" : "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns",
      "author" : [ "Frank Seide", "Hao Fu", "Jasha Droppo", "Gang Li", "Dong Yu" ],
      "venue" : "In Fifteenth Annual Conference of the International Speech Communication Association,",
      "citeRegEx" : "Seide et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Seide et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "arXiv preprint arXiv:1409.4842,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "(1) Model parallelism: the model is distributed to different computing nodes (Sutskever et al., 2014) (2) Data parallelism: different nodes train on different samples for the same model (Seide et al.",
      "startOffset" : 77,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : ", 2014) (2) Data parallelism: different nodes train on different samples for the same model (Seide et al., 2014; Chilimbi et al., 2014).",
      "startOffset" : 92,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : ", 2014) (2) Data parallelism: different nodes train on different samples for the same model (Seide et al., 2014; Chilimbi et al., 2014).",
      "startOffset" : 92,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "Some of the works even use a hybrid of them (Krizhevsky, 2014; Dean et al., 2012; Le, 2013).",
      "startOffset" : 44,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "Some of the works even use a hybrid of them (Krizhevsky, 2014; Dean et al., 2012; Le, 2013).",
      "startOffset" : 44,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "Some of the works even use a hybrid of them (Krizhevsky, 2014; Dean et al., 2012; Le, 2013).",
      "startOffset" : 44,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "(1) the allreduce approach where all updates from the peers are aggregated at the synchronization point and the averaged update is broadcasted back to the peers (Seide et al., 2014; Krizhevsky, 2014).",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "(1) the allreduce approach where all updates from the peers are aggregated at the synchronization point and the averaged update is broadcasted back to the peers (Seide et al., 2014; Krizhevsky, 2014).",
      "startOffset" : 161,
      "endOffset" : 199
    }, {
      "referenceID" : 1,
      "context" : "(2) the parameter server approach handles the reads and writes of the parameters asynchronously (Dean et al., 2012; Le, 2013; Chilimbi et al., 2014).",
      "startOffset" : 96,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "(2) the parameter server approach handles the reads and writes of the parameters asynchronously (Dean et al., 2012; Le, 2013; Chilimbi et al., 2014).",
      "startOffset" : 96,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "(2) the parameter server approach handles the reads and writes of the parameters asynchronously (Dean et al., 2012; Le, 2013; Chilimbi et al., 2014).",
      "startOffset" : 96,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "It is named “Purine”, which is an analog of caffeine in molecular structure, because we’ve benefited a lot from the open source Caffe framework (Jia et al., 2014) in our research and the math functions used in Purine are ported from Caffe.",
      "startOffset" : 144,
      "endOffset" : 162
    }, {
      "referenceID" : 2,
      "context" : "For example, the dataflow graph in Dryad (Isard et al., 2007) and Pig Latin (Olston et al.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : ", 2007) and Pig Latin (Olston et al., 2008) are the same as the Bi-Graph abstraction introduced in this paper.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "Graphlab (Low et al., 2010) proposed",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "2 ITERATIONS Although it has been argued in (Low et al., 2010) that the directed acyclic graph could not effectively express iterative algorithms as the graph structure would depend on the number of iterations.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "A hybrid of data parallelism and model parallelism has previously been proposed by Krizhevsky (2014) in which the convolution layers use data parallelism and fully connected layers use model parallelism.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "A hybrid of data parallelism and model parallelism has previously been proposed by Krizhevsky (2014) in which the convolution layers use data parallelism and fully connected layers use model parallelism. This is based on the observation that the number of parameters is large and thus the communication cost is big for fully connected layers. The hybrid approach greatly reduces the communication cost. A different approach to reduce communication overhead is to overlap the data transfer with computations. Double buffering is proposed by Seide et al. (2014) to break a minibatch in half and exchange the gradients of the first half while doing computaion of the second half.",
      "startOffset" : 83,
      "endOffset" : 560
    }, {
      "referenceID" : 10,
      "context" : "4 RESULTS We carried out experiments on the Purine framework with data parallelism on GoogLeNet (Szegedy et al., 2014).",
      "startOffset" : 96,
      "endOffset" : 118
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we introduce a novel deep learning framework, termed Purine. In Purine, a deep network is expressed as a bipartite graph (bi-graph), which is composed of interconnected operators and data tensors. With the bi-graph abstraction, networks are easily solvable with event-driven task dispatcher. We then demonstrate that different parallelism schemes over GPUs and/or CPUs on single or multiple PCs can be universally implemented by graph composition. Scheduled by the task dispatcher, memory transfers are fully overlapped with other computations, which greatly reduce the communication overhead and help us achieve approximate linear acceleration. This eases researchers from coding for various parallelization schemes, and the same dispatcher can be used for solving variant graphs.",
    "creator" : "LaTeX with hyperref package"
  }
}