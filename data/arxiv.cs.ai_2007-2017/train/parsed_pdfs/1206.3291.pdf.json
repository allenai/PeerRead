{
  "name" : "1206.3291.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Hierarchical POMDP Controller Optimization by Likelihood Maximization",
    "authors" : [ "Marc Toussaint", "Laurent Charlin" ],
    "emails" : [ "mtoussai@cs.tu-berlin.de", "lcharlin@cs.toronto.edu", "ppoupart@cs.uwaterloo.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Planning can often be simplified by decomposing the task into smaller tasks arranged hierarchically. Charlin et al. [4] recently showed that the hierarchy discovery problem can be framed as a non-convex optimization problem. However, the inherent computational difficulty of solving such an optimization problem makes it hard to scale to realworld problems. In another line of research, Toussaint et al. [18] developed a method to solve planning problems by maximumlikelihood estimation. In this paper, we show how the hierarchy discovery problem in partially observable domains can be tackled using a similar maximum likelihood approach. Our technique first transforms the problem into a dynamic Bayesian network through which a hierarchical structure can naturally be discovered while optimizing the policy. Experimental results demonstrate that this approach scales better than previous techniques based on non-convex optimization."
    }, {
      "heading" : "1 Introduction",
      "text" : "Planning in partially observable domains is notoriously difficult. However, many planning tasks naturally decompose into subtasks that may be arranged hierarchically. For instance, the design of a soccer playing robot is often decomposed into low-level skills such as intercepting the ball, controlling the ball, passing the ball, etc. [16]. Similarly, prompting systems that assist older adults with activities of daily living (e.g., handwashing [8]) can be naturally decomposed into subtasks for each step of an activity. When a decomposition or hierarchy is known a priori, several approaches have demonstrated that planning can be simplified and performed faster [13, 7]. However, the hierarchy is\nnot always known or easy to specify, and the optimal policy may only decompose approximately. To that effect, Charlin et al. [4] showed how a hierarchy can be discovered automatically by formulating the planning problem as a non-convex quartically constrained optimization problem with variables corresponding to the parameters of the policy, including its hierarchical structure. Unfortunately, the inherent computational difficulty of solving this optimization problem prevents the approach from scaling to real-world problems. Furthermore, it is not clear that automated hierarchy discovery simplifies planning since the space of policies remains the same.\nWe propose an alternative approach that demonstrates that hierarchy discovery (i) can be done efficiently and (ii) performs a policy search with a different bias than non-hierarchical approaches that is advantageous when there exists good hierarchical policies. The approach combines Murphy and Paskin’s [10] factored encoding of hierarchical structures (see also [17]) into a dynamic Bayesian network (DBN) with Toussaint et al.’s [18] maximum-likelihood estimation technique for policy optimization. More precisely, we encode POMDPs with hierarchical controllers into a DBN in such a way that the policy and hierarchy parameters are entries of some conditional probability tables. We also consider factored policies that are more general than hierarchical controllers. The policy and hierarchy parameters are optimized with the expectationmaximization (EM) algorithm [5]. Since each iteration of EM essentially consists of inference queries, the approach scales easily.\nSect. 2 briefly introduces partially observable Markov decision processes, controllers and policy optimization by maximum likelihood estimation. Sect. 3 reviews previous work on hierarchical modeling and how to use a dynamic Bayesian network to encode a hierarchical structure. Sect. 4 describes our proposed approach, which combines a dynamic Bayesian network encoding with maximum likelihood estimation to simultane-\nously optimize a hierarchy and the controller. Sect. 5 demonstrates the scalability of the proposed approach on benchmark problems. Finally, Sect. 6 summarizes the paper and discusses future work."
    }, {
      "heading" : "2 Background",
      "text" : "Throughout the paper we denote random variables by upper case letters (e.g., X), values of random variables by their corresponding lower case letters (e.g., x ∈ dom(X)) and sets of values by upper case letters with math calligraphy (e.g., X = {x1, x2, x3}). We now review POMDPs (Sect. 2.1), how to represent policies as finite state controllers (Sect. 2.2) and how to optimize bounded controllers (Sect. 2.3)."
    }, {
      "heading" : "2.1 POMDPs",
      "text" : "Partially observable Markov decision processes (POMDPs) provide a natural and principled framework for planning. A POMDP can be formally defined by a tuple 〈S,A,O, ps, ps′|as, po′|s′a, ras〉 where S is the set of states s, A is the set of actions a, O is the set of observations o, ps = Pr(S0 = s) is the initial state distribution (a.k.a. initial belief), ps′|as = Pr(St+1 = s′ |At = a, St = s) is the transition distribution, po′|s′a = Pr(Ot+1 = o′ |St+1 = s′, At = a) is the observation distribution and ras = R(At = a, St = s) is the reward function. Throughout the paper, it is assumed that S, A and O are finite and discrete. The goal is to select actions to maximize the rewards. At any point in time, the information available to select the next action consists of the history of past actions and observations. Hence a policy π is defined as a mapping from histories to actions. However, since histories grow with time, it is common practice to summarize histories with a fixed-length sufficient statistic such as the belief distribution bs = Pr(S= s), which corresponds to the state distribution (conditioned on the history of past actions and observations). The belief distribution b can be updated at each time step, based on the action a taken and the observation o′ made according to Bayes’ theorem: bao ′ s′ = k ∑ s bsps′|aspo′|s′a (k is a normalization constant). Policies can then be defined as mappings from beliefs to actions (e.g., π(b) = a). The value V π(b) of a policy π starting in belief b is measured by the discounted sum of expected rewards: V π(b) = ∑ t γ\ntEbt|π[rπ(bt)bt ] where rab = ∑ s bsras. An optimal policy π\n∗ is a policy with the highest value V ∗ for all beliefs: V ∗(b) ≥ V π ∀π, b. The optimal value function also satisfies Bellman’s equation: V ∗(b) = maxa rab + ∑ o′ po′|abV ∗(bao ′ )\nwhere po′|ab = ∑ ss′ bsps′|aspo′|s′a."
    }, {
      "heading" : "2.2 Finite State Controllers",
      "text" : "A convenient representation for an important class of policies consists of finite state controllers [6]. Instead of using beliefs as sufficient statistics of histories, the idea is to use a finite internal memory to retain relevant bits of information from histories. Each configuration of this memory can be thought of as a node in a finite state controller, where nodes select actions to be executed and edges indicate how to update nodes based on the observations received. A controller with a finite set N of nodes n can encode a stochastic policy π with three distributions: Pr(N0 =n) = pn (initial node distribution), Pr(At=a |Nt=n) = pa|n (action selection distribution) and Pr(Nt+1 = n′ |Nt = n,Ot+1 = o′) = pn′|no′ (successor node distribution). Such a policy can be executed by starting in a node n sampled from pn, executing an action a sampled from pa|n, receiving observation o′, transitioning to node n′ sampled from pn′|no′ and so on. The value of a controller can be computed by solving a linear system: Vns = ∑ a pa|n[ras+\nγ ∑ s′o′n′ ps′|aspo′|s′apn′|no′Vn′s′ ] ∀ns. The value at a\ngiven belief b is then V π(b) = ∑ n ∑ s bspnVns."
    }, {
      "heading" : "2.3 Policy Optimization",
      "text" : "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18]. We briefly describe the last technique since we will use it in Sect. 4.\nToussaint et al. [18] recently proposed to convert POMDPs into equivalent dynamic Bayesian networks (DBNs) by normalizing the rewards and to optimize a policy by maximizing the likelihood of the normalized rewards. Let R̃ be a binary variable corresponding to normalized rewards. The reward function ras is then replaced by a reward distribution pr̃|sat = Pr(R̃ = r̃ |At = a, St = s, T = t) that assigns probability ras/(rmax − rmin) to R̃ = 1 and 1 − ras/(rmax − rmin) to R̃ = 0 (rmin = minas ras and rmax = maxas ras). An additional time variable T is introduced to simulate the discount factor and the summation of rewards. Since a reward is normally discounted by a factor γt when earned t time steps in the future, the prior pt = Pr(T = t) is set to γt(1−γ) where the factor (1−γ) ensures that ∑∞ t=0 pt = 1. The resulting dynamic Bayesian network is illustrated in Fig. 1. It can be thought of as a mixture of finite processes of length t with a 0-1 reward R̃ earned at the end of the process. The nodes Nt encode the internal memory of the controller. Given the controller distributions pn, pa|n and pn′|no′ , it is possible to evaluate the controller\nby computing the likelihood of R̃ = 1. More precisely, V π(ps) = (Pr(R̃=1)− rmin)/[(rmax − rmin)(1− γ)].\nOptimizing the policy can be framed as maximizing the likelihood of R̃ = 1 by varying the distributions pn, pa|n and pn′|no′ encoding the policy. Toussaint et al. use the expectation-maximization (EM) algorithm. Since EM is guaranteed to increase the likelihood at each iteration, the controller’s value increases monotonically. However, EM is not guaranteed to converge to a global optimum. An important advantage of the EM algorithm is its simplicity both conceptually and computationally. In particular, the computation consists of inference queries that can be computed using a variety of exact and approximate algorithms."
    }, {
      "heading" : "3 Hierarchical Modeling",
      "text" : "While optimizing a bounded controller allows an effective search in the space of bounded policies, such an approach is clearly suboptimal since the optimal controller of many problems grows doubly exponentially with the planning horizon and may be infinite for infinite horizons. Alternatively, hierarchical representations permit the representation of structured policies with exponentially fewer parameters. Several approaches were recently explored to model and learn hierarchical structures in POMDPs. Pineau et al. [13] sped up planning by exploiting a user specified action hierarchy. Hansen et al. [7] proposed hierarchical controllers and an alternative planning technique that also exploits a user specified hierarchy. Charlin et al. [4] proposed recursive controllers (which subsume hierarchical controllers) and an approach that discovers the hierarchy while optimizing a controller. We briefly review recursive controllers in Sect. 3.1 since\nwe will empirically compare our approach to the nonconvex optimization techniques used to optimize recursive controllers. In another line of research, Murphy and Paskin [10] proposed to model hierarchical hidden Markov models (HMMs) with a dynamic Bayesian network (DBN). Theocharous et al. [17] also used DBNs to model hierarchical POMDPs. We briefly review this DBN encoding in Sect. 3.2 since we will use it in our approach to model factored controllers."
    }, {
      "heading" : "3.1 Recursive Controllers",
      "text" : "A recursive controller [4] consists of a recursive automaton with concrete nodes n and abstract nodes n̄. Abstract nodes call a subcontroller before selecting an action. A controller is said to be recursive when it can call itself, essentially encoding an infinite hierarchy. Formally, a recursive controller is parametrized by an action selection distribution for each node (e.g., pa|n and pa|n̄), a successor node distributions for each node (e.g., pn′|no′ and pn′|n̄o′) and a child node distribution for each abstract node (e.g., pn′|n̄)1. Execution of a recursive controller is performed by executing the action selected by each node visited and continuing to the successor node selected by the observation made. However, when an abstract node is visited, before executing the action selected, its subcontroller is called and started in the child node selected by the child node distribution. A subcontroller returns control to its parent node when a special end node is reached. Charlin et al. [4] show that optimizing a recursive controller with a fixed number of concrete and abstract nodes can be framed as a non-convex quartically constrained optimization problem. The hierarchical structure is discovered as the controller is optimized since the variables of the optimization problem include the child node distributions which implicitly encode the hierarchy. Three techniques based on a general non-linear solver, a mixed-integer non-linear approximation and a form of bounded hierarchical policy iteration are experimented with, but do not scale beyond toy problems. Furthermore, Charlin et al. do not demonstrate whether searching in the space of hierarchical controllers can speed up planning. Although it is clear that planning is simplified when a hierarchy is given a priori since the policy space is reduced, it is not clear that hierarchy discovery is beneficial since the policy space remains the same while the parameter space changes. In Sect. 5, we demonstrate that hierarchy discovery can be beneficial when a simple hierarchical policy of high value exists.\n1The pa|n and pn′|no′ distributions are combined in one distribution pn′a|no′ in [14]"
    }, {
      "heading" : "3.2 Hierarchical HMMs",
      "text" : "Murphy and Paskin [10] proposed to model hierarchical hidden Markov models (HMMs) as dynamic Bayesian networks (DBNs). The idea is to convert a hierarchical HMM of L levels into a dynamic Bayesian network of L state variables, where each variable encodes abstract states at the corresponding level. Here, abstract states can only call sub-HMMs at the previous level. Fig. 2 illustrates a two-level hierarchical HMMs encoded as a DBN. The state variables Slt are indexed by the time step t and the level l. The Et variables indicate when a base-level sub-HMM has ended, returning its control to the top level HMM. The toplevel abstract state transitions according to the top HMM, but only when the exit variable Et indicates that the base-level concrete state is an exit state. The base-level concrete state transitions according to the base-level HMM. When an exit state is reached, the next base-level state is determined by the next toplevel abstract state. Factored HMMs subsume hierarchical HMMs in the sense that there exists an equivalent factored HMM for every hierarchical HMM. In Sect. 4.1, we will use a similar technique to convert hierarchical controllers into factored controllers."
    }, {
      "heading" : "4 Factored Controllers",
      "text" : "We propose to combine the DBN encoding techniques of Murphy et al. [10] and Toussaint et al. [18] to convert a POMDP with a hierarchical controller into a mixture of DBNs. The hierarchy and the controller are simultaneously optimized by maximizing the reward likelihood of the DBN. We also consider factored controllers which subsume hierarchical controllers."
    }, {
      "heading" : "4.1 DBN Encoding",
      "text" : "Fig. 3a illustrates two consecutive slices of one DBN in the mixture (rewards are omitted) for a three-level hierarchical controller. Consider a POMDP defined by the tuple 〈S,A,O, ps, ps′|as, po′|s′a, ras〉 and a threelevel hierarchical (non-recursive) controller defined by\nthe tuple 〈pa|nl , pnl−1|nl , pn′l|nlo′〉 ∀l. The conditional probability distributions of the mixture of DBNs (denoted by p̂) are:\n• transition distribution: p̂s′|as = ps′|as\n• observation distribution: p̂o′|s′a = po′|s′a\n• reward distribution: p̂r̃|as = (ras − rmin)/(rmax − rmin)\n• mixture distribution: p̂t = (1− γ)γt\n• action distribution: p̂a|n0 = pa|n0\n• base level node distribution: p̂n′0|n0n′1o′e0 = { pn′0|n′1 if e0 =exit pn′0|o′n0 otherwise\n• middle level node distribution: p̂n′1|n1n′2o′e0e1\n=  pn ′1|n′2 if e1 =exit\npn′1|o′n1 if e0 =exit and e1 6=exit δn′1n1 otherwise\n• top level node distribution: p̂n′2|o′n2e1 = { pn′2|o′n2 if e1 =exit δn′2n2 otherwise\n• base-level exit distribution: p̂e0|n0 = {\n1 if n0 is an end node 0 otherwise\n• middle-level exit distribution: p̂e1|n1e0 = {\n1 if e0 =exit and n1 is an end node 0 otherwise\nWhile the Elt variables help clarify when the end of a sub-controller is reached, they are not necessary. Eliminating them yields a simpler DBN illustrated in Fig. 3b. The conditional probability distributions of the N lt variables become:\n• base level node distribution: p̂n′0|n0n′1o′ = { pn′0|n′1 if n0 is an end node pn′0|o′n0 otherwise\n• middle level node distribution: p̂n′1|n1n′2o′\n=  pn ′1|n′2 if n1 and n0 are end nodes pn′1|o′n1 if n0 is an end node, but not n1\nδn′1n1 otherwise\n• top level node distribution: p̂n′2|n2o′e1 = { pn′2|n2o′ if n1 and n0 are end nodes δn′2n2 otherwise\nNote that ignoring the above constraints in the conditional distributions yields a factored controller that is more flexible than a hierarchical controller since the conditional probability distributions of the N lt variables do not have to follow the structure imposed by a hierarchy"
    }, {
      "heading" : "4.2 Maximum Likelihood Estimation",
      "text" : "Following Toussaint et al.’s technique [18], we optimize a factored controller by maximizing the reward likelihood. Since the policy parameters are conditional probability distributions of the DBN, the EM algorithm can be used to optimize them. Computation alternates between the E and M steps below. We denote by ntop and nbase the top and base nodes in a given time slice. We also denote by φ(V ) and φ(v) the parents of V and a configuration of the parents of V .\nE-step: expected frequency of the hidden variables Entop = Pr(N top 0 =n\ntop|R̃=1) Eanbase = ∑ t Pr(At=a,N base t =n\nbase|R̃=1) En′lφ(n′l) =∑\nt Pr(N l t+1 =n ′l, φ(N lt+1)=φ(n l t+1)|R̃=1) ∀l\nM-step: relative frequency computation pntop = Entop/ ∑ ntop Entop\npa|nbase = Eanbase/ ∑ aEanbase\npn′l|φ(n′l) = En′lφ(n′l)/ ∑ n′l En′lφ(n′l) ∀l"
    }, {
      "heading" : "4.2.1 Parameter initialization",
      "text" : "W.l.o.g. we initialize the start node N top0 of the top layer to be the first node (i.e., Pr(N top0 =1) = 1). The node conditional distributions pn′l|φ(n′l) are initialized randomly as a mixture of three distributions:\npn′l|φ(n′l) ∝ c1 + c2Un′lφ(n′l) + c3δn′lnl\nThe mixture components are a uniform distribution, a random distribution Uφ(n′l) (an array of uniform random numbers in [0, 1]), and a term enforcing nl to stay unchanged. For the node distributions at the base level we choose c1 = 1, c2 = 1, c3 = 0 and for all other levels we choose c1 =1, c2 =1, c3 =10. Similarly we initialize the action probabilities as\npa|nbase ∝ c1 + c2Uanbase + c3δa(nbase%a)\nwith c1 = 1, c2 = 1, c3 = 100, where the last term enforces each node nbase= i to be associated with action a= i%a."
    }, {
      "heading" : "4.2.2 E-step",
      "text" : "To speed up the computation of the inference queries in the E-step, we compute intermediate terms using a forward-backward procedure. Let tmax be the largest value of T , then a simple scheme that answers each query separately takes O(t2max) time since there are O(tmax) queries and each query takes O(tmax) time to run over the entire network. However, since part of the computation is duplicated in several queries, it is possible to compute intermediate terms α and β in O(tmax) time from which each expectation can be computed in constant time (w.r.t. tmax). To simplify notation, N and n denote all the nodes and their joint configuration in a given time slice.\nForward term: αtns = Pr(Nt=n, St=s) α0ns = pnps αtn′s′ = ∑ n,s α t−1 ns pn′s′|ns\nBackward term: βτns = Pr(R̃= 1|Nt−τ = n, St−τ = s, T = t) β0ns = ∑ a pa|nras\nβτns = ∑ n′,s′ pn′s′|nsβ τ−1 n′s′\nTo fully take advantage of the structure of the DBN, we first marginalize the DBN w.r.t. the observations and actions to get the DBN in Fig. 3c. This 2-slice DBN corresponds to the joint transition distribution pn′s′|ns used in the above equations. Then we compile this 2-slice DBN into the junction tree (actually junction chain) given in Fig. 3d.\nLet βns = ∑ τ Pr(T = τ)β τ ns and αns = ∑ t Pr(T = t)αtns, then the last two expectations of the E-step\ncan be computed as follows:2 Eanbase ∝ ∑ s,n−{nbase} αnspa|nbase [ ras+\nγ ∑ s′,o′,n′ ps′|aspo′|s′apn′|o′nβn′s′ ] En′lφ(n′l) ∝ ∑ s,s′,a,n−φ(n′l),n′−l αnspa|nbaseps′|as\npo′|s′apn′|o′n [ ras + γβn′s′ ] ∀l"
    }, {
      "heading" : "4.2.3 M-step",
      "text" : "The standard M-step adjusts each parameter pv|φ(v) by normalizing the expectations computed in the Estep, i.e., pnewv|φ(v) ∝ Evφ(v). To speed up convergence, we instead use a variant that performs a soften greedy M-step. In the greedy M-step, each parameter pnewv|φ(v) is greedily set to 1 when v = argmaxv̄ fv̄φ(v̄) and 0 otherwise, where fvφ(v) = Evφ(v)/poldv|φ(v). The greedy M-step can be thought of as the limit of an infinite sequence of alternating partial E-step and standard M-step where the partial E-step keeps f fixed. The combination of a standard M-step with this specific partial E-step updates pv|φ(v) by a multiplicative factor proportional to fvφ(v). In the limit, the largest fvφ(v) ends up giving all the probability to the corresponding pv|φ(v). EM variants with certain types of partial E-steps ensure monotonic improvement of the likelihood when the hidden variables are independent [11]. This is not the case here, however by softening the greedy M-step we can still obtain monotonic improvement most of the time while speeding up convergence. We update pv|φ(v) as follows:\nv∗ = argmax v fvφ(v)\npnewv|φ(v) ∝ p old v|φ(v)[δvv∗ + c+ ] .\nFor c = 0 and = 0 this is the greedy M-step. We use c = 3 which softens (shortens) the step and improves convergence. Furthermore, adding small Gaussian noise ∼ N (0, 10−3) helps to escape local minima."
    }, {
      "heading" : "4.2.4 Complexity",
      "text" : "For a flat controller, the number of parameters (neglecting normalization) is |O||N |2 for pn′|o′n and |A||N | for pa|n. The complexity of the forward (backward) procedure is O(tmax(|N ||S|2 + |N |2|S|)) where the two terms correspond to the size of the two cliques for inference in the 2-slice DBN after O and A are eliminated. The complexity of computing the expectations from α and β is O(|N ||A|(|S|2 + |S||O|)+ |N |2|S||O|), which corresponds to the clique sizes of the 2-slice DBN including O and A.\nIn comparison, 2-level hierarchical and factored controllers with |N top| = |N base| = |N |0.5 nodes at each\n2The first expectation of the E-step does not need to be computed since Pr(N top0 = 1) = 1.\nlevel have fewer parameters and a smaller complexity, but also a smaller policy space due to the structure imposed by the hierarchy/factorization. While there is a tradeoff between policy space and complexity, hierarchical and factored controllers are often advantageous in practice since they can find more quickly a good hierarchical/factored policy when there exists one.\nA 2-level factored controller with |N |0.5 nodes at each level has 2|O||N |1.5 parameters for pn′top|o′nbasentop and pn′base|n′topo′nbase , and |A||N |0.5 parameters for pa|nbase . The complexity of the forward (backward) procedure is O(tmax(|N ||S|2 + |N |1.5|S|)) and the complexity of computing the expectations is O(|N ||A|(|S|2 + |S||O|) + |N |1.5|O||S| + |N |2|O|). A 2-level hierarchical controller is further restricted and therefore has fewer parameters, but the same time complexity."
    }, {
      "heading" : "5 Experiments",
      "text" : "We first compared the performance of the maximum likelihood (ML) approach to previous optimizationbased approaches from [4]. Table 2 summarizes the results for 2-layer controllers with certain combinations of |N base| and |N top|. The problems include paint, shuttle and 4x4 maze (previously used in [4]) and three additional problems: chain-of-chains (described below), hand-washing (reduced version from [8]) and cheese-taxi (variant from [12]). On the first three problems, ML reaches the same values as the previous optimization-based approaches, but with larger controllers. We attribute this to EM’s weaker ability to avoid local optima than the optimization-based approaches. However, the optimization-based approaches run out of memory on the last three problems (memory needs exceed 2 Gb of RAM), while ML scales gracefully (as analyzed in Sect. 4.2.4). ML approach demonstrates that hierarchy discovery can be tackled with tractable algorithms. We also report the values reached with a state of the art point-based value\nTable 2: V ∗ denotes optimal values (with truncated trajectories) [3] except for handwashing and cheese-taxi where we show the optimal value of the equivalent fully-observable problem. HSVI2 found a solution in less than 1s for every problem except handwashing where the algorithm was halted after 12 hours of computation. The ML approach optimizes a factored controller for 200 EM iterations with a planning horizon of tmax=100. (5,3) nodes means |N base|= 5 and |N top|= 3. For cheese-taxi, we get a maximum value of 2.25. N/A indicates that the solver did not complete successfully. All tests are done on a dual-core x64 processor @2.2GHz.\nProblem |S|, |A|, |O| V ∗ HSVI2 Best results from [4] ML approach (avg. over 10 runs) V nodes t(s) V nodes t(s) V paint 4, 4, 2 3.28 3.29±0.04 (1,3) <1 3.29 (5,3) 0.96±0.3 3.26±0.004 shuttle 8, 3, 5 32.7 32.9±0.8 (1,3) 2 31.87 (5,3) 2.81±0.2 31.6±0.5 4x4 maze 16, 4, 2 3.7 3.75±0.1 (1,2) 30 3.73 (3,3) 2.8±0.8 3.72±8e−5 chain-of-chains 10, 4, 1 157.1 157.1±0 (3,3) 10 0.0 (10,3) 6.4±0.2 151.6±2.6 handwashing 84, 7, 12 61052 N/A N/A (10,5) 655±2 984±1 cheese-taxi 33, 7, 10 65.3 2.53±0.3 N/A (10,3) 311±14 −9±11(2.25∗)\niteration method (HSVI2 [15]).\nThe next question is whether there are computational savings when automatically discovering a hierarchy. Recall that previous work has shown that policy optimization is simplified when a hierarchy is known a priori since the space of policies is restricted. The next experiment demonstrates that policy optimization while discovering a hierarchy can be done faster and/or yield higher value when there exists good hierarchical policies. Table 3 compares the performance when optimizing flat, hierarchical and factored controllers on chain-of-chains, hand-washing and cheesetaxi. Here, the factored and hierarchical controllers have two levels and correspond respectively to the DBNs in Fig. 3(a) and 3(b).3 The x-axis is the number of nodes for flat controllers and the product of the number of nodes at each level for hierarchical and factored controllers. Taking the product is justified by the fact that the equivalent flat controllers of some hierarchical/factored controllers require that many nodes. The graphs in the right column of Table 3 demonstrate that hierarchical and factored controllers can be optimized faster, confirming the analysis done in Sect. 4.2.4. There is no difference in computational complexity between the strictly hierarchical and unconstrained factored architectures. Recall however that the efficiency gains of the hierarchical and factored controllers are obtained at the cost of a restricted policy space. Nevertheless, the graphs in the left column of Table 3 suggest that hierarchical/factored controllers can still find equally good policies when there exist one. Factored controllers are generally the most robust. With a sufficient number of nodes, they find the best policies on all three problems. Note that factored and hierarchical controllers need at least a number of nodes equal to the number of actions in the base layer in order to represent a policy that uses all actions.\n3Factored controllers are hierarchical controllers where the restrictions imposed by the Et variables are removed.\nA\n0.84 0.16\nLevel 1\nLevel 0 D C B\n0 1 2 3\nFigure 4: Hierarchical controller learnt for the chainof-chains. The diamond indicates an exit node, for which p̂e0|n0 = 1.\nThis explains why hierarchical and factored controllers with less than 4 base nodes (for chain-of-chains) and 7 base nodes (for hand-washing and cheese-taxi) do poorly. The optimization of flat controllers tend to get stuck in local optima if too many nodes are used. Comparing the unconstrained factored architecture versus hierarchical, we find that the additional constraints in the hierarchical controller make the optimization problem harder although there are less parameters to optimize. As a result, EM gets stuck more often in local optima.\nWe also examine whether learnt hierarchies make intuitive sense. Good policies for the cheese-taxi and handwashing problems can often be represented hierarchically, however the hierarchical policies found didn’t match hierarchies expected by the authors. Since these are non-trivial problems for which there may be many ways to represent good policies in a hierarchical fashion that is not intuitive, we designed the chain-ofchains problem, which is much simpler to analyze. The optimal policy of this problem consists of executing n times the same chain of n actions followed by a submit action to earn the only reward. The optimal policy requires n2 + 1 nodes for flat controllers and n+ 1 nodes at each level for hierarchical controllers. For n = 3, ML found a hierarchical controller of 4 nodes at each level, illustrated in Fig. 4. The controller starts in node 0. Nodes at level 1 are abstract and descend into concrete nodes at level 0 by following the dashed\nedges. Control is returned to level 1 when an end node (denoted by a diamond) is reached. Here, the optimal policy is to do A-B-C three times followed by D. Hence a natural hierarchy would abstract A-B-C and D into separate subcontrollers. While the controller in Fig. 4 is not completely optimal (the vertical transition from abstract node 0 should have probability 1 of reaching node A), it found an equivalent, but less intuitive abstraction by having subcontrollers that do A-B-C and D-A-B-C. This suggests that for real-world problems there will be many valid abstractions that are not easily interpretable by humans and the odds that an automated procedure finds an intuitive hierarchy without any additional guidance are slim."
    }, {
      "heading" : "6 Conclusion",
      "text" : "The key advantage of maximum likelihood is that it can exploit the factored structure in a controller architecture. This facilitates hierarchy discovery when the hierarchical structure of the controller is encoded into a corresponding dynamic Bayesian network (DBN). Our complexity analysis and the empirical run time analysis confirm the favorable scaling. In particular, we solved problems like handwashing and cheese-taxi that could not be solved with the previous approaches in [4]. Compared to flat controllers, factored controllers are faster to optimize and less sensitive to local optima when they have many nodes. Our current implementation does not exploit any factored structure\nin the state, action and observation space, however we envision that a factored implementation would naturally scale to large factored POMDPs.\nFor the chain-of-chains problem, maximum likelihood finds a valid hierarchy. For other problems like handwashing, there might be many hierarchies and the one found by our algorithm is usually hard to interpret. We cannot expect our method to find a hierarchy that is human readable. Interestingly, although the strictly hierarchical architectures have less parameters to optimize, they seem to be more susceptible to local optima as compared to a factored but otherwise unconstrained controller. Future work will investigate various heuristics to escape local optima during optimization.\nIn this paper we made explicit assumptions about the structure – we prefixed the structure of the DBN to mimic a strict hierarchy or a level-wise factorization and we fixed the number of nodes in each level. However, the DBN framework allows us to build on existing methods for structure learning of graphical models. A promising extension would be to use such structure learning techniques to optimize the factored structure of the controller. Since the computational complexity for evaluating (training) a single structure is reasonable, techniques like MCMC could sample and evaluate a variety of structures. This variety might also help to circumvent local optima, which currently define the most dominant limit of our approach."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Part of this work was completed while Charlin was at\nthe University of Waterloo. Toussaint acknowledges sup-\nport by the German Research Foundation (DFG), Emmy\nNoether fellowship TO 409/1-3. Poupart and Charlin were\nsupported by grants from the Natural Sciences and Engi-\nneering Research Council of Canada, the Canada Founda-\ntion for Innovation and the Ontario Innovation Trust."
    } ],
    "references" : [ {
      "title" : "Solving POMDPs using quadratically constrained linear programs",
      "author" : [ "C. Amato", "D. Bernstein", "S. Zilberstein" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Stochastic local search for POMDP controllers",
      "author" : [ "D. Braziunas", "C. Boutilier" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "Exact and approximate algorithms for partially observable Markov decision processes",
      "author" : [ "A. Cassandra" ],
      "venue" : "PhD thesis, Brown University, Dept. of Computer Science,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "Automated hierarchy discovery for planning in par-  tially observable environments",
      "author" : [ "L. Charlin", "P. Poupart", "R. Shioda" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A. Dempster", "N. Laird", "D. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1977
    }, {
      "title" : "An improved policy iteration algorithm for partially observable MDPs",
      "author" : [ "E. Hansen" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Synthesis of hierarchical finite-state controllers for POMDPs",
      "author" : [ "E. Hansen", "R. Zhou" ],
      "venue" : "In ICAPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Assisting persons with dementia during handwashing using a partially observable Markov decision process",
      "author" : [ "J. Hoey", "A. von Bertoldi", "P. Poupart", "A. Mihailidis" ],
      "venue" : "ICVS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Learning finite-state controllers for partially observable environments",
      "author" : [ "N. Meuleau", "L. Peshkin", "K.-E. Kim", "L. Kaelbling" ],
      "venue" : "In UAI,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Linear time inference in hierarchical HMMs",
      "author" : [ "K. Murphy", "M. Paskin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2001
    }, {
      "title" : "A view of the EM algorithm that justifies incremental, sparse, and other variants",
      "author" : [ "R. Neal", "G. Hinton" ],
      "venue" : "Learning in Graphical Models. Kluwer,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Tractable Planning Under Uncertainty: Exploiting Structure",
      "author" : [ "J. Pineau" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Policycontingent abstraction for robust robot control",
      "author" : [ "J. Pineau", "G. Gordon", "S. Thrun" ],
      "venue" : "In UAI,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2003
    }, {
      "title" : "Bounded finite state controllers",
      "author" : [ "P. Poupart", "C. Boutilier" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2003
    }, {
      "title" : "Heuristic search value iteration for POMDPs",
      "author" : [ "T. Smith", "R. Simmons" ],
      "venue" : "In UAI,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "A layered approach to learning client behaviors in the RoboCup soccer server",
      "author" : [ "P. Stone", "M. Veloso" ],
      "venue" : "Applied Artificial Intelligence,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1998
    }, {
      "title" : "Representing hierarchical POMDPs as DBNs for multi-scale robot localization",
      "author" : [ "G. Theocharous", "K. Murphy", "L. Pack Kaelbling" ],
      "venue" : "In ICRA,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Probabilistic inference for solving (PO)MDPs",
      "author" : [ "M. Toussaint", "S. Harmeling", "A. Storkey" ],
      "venue" : "Technical Report EDI-INF-RR-0934, School of Informatics, University of Edinburgh,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "[4] recently showed that the hierarchy discovery problem can be framed as a non-convex optimization problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "[18] developed a method to solve planning problems by maximumlikelihood estimation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : ", handwashing [8]) can be naturally decomposed into subtasks for each step of an activity.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : "When a decomposition or hierarchy is known a priori, several approaches have demonstrated that planning can be simplified and performed faster [13, 7].",
      "startOffset" : 143,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "When a decomposition or hierarchy is known a priori, several approaches have demonstrated that planning can be simplified and performed faster [13, 7].",
      "startOffset" : 143,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "[4] showed how a hierarchy can be discovered automatically by formulating the planning problem as a non-convex quartically constrained optimization problem with variables corresponding to the parameters of the policy, including its hierarchical structure.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "The approach combines Murphy and Paskin’s [10] factored encoding of hierarchical structures (see also [17]) into a dynamic Bayesian network (DBN) with Toussaint et al.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : "The approach combines Murphy and Paskin’s [10] factored encoding of hierarchical structures (see also [17]) into a dynamic Bayesian network (DBN) with Toussaint et al.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "’s [18] maximum-likelihood estimation technique for policy optimization.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : "The policy and hierarchy parameters are optimized with the expectationmaximization (EM) algorithm [5].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "A convenient representation for an important class of policies consists of finite state controllers [6].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].",
      "startOffset" : 220,
      "endOffset" : 223
    }, {
      "referenceID" : 17,
      "context" : "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 17,
      "context" : "[18] recently proposed to convert POMDPs into equivalent dynamic Bayesian networks (DBNs) by normalizing the rewards and to optimize a policy by maximizing the likelihood of the normalized rewards.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] sped up planning by exploiting a user specified action hierarchy.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "[7] proposed hierarchical controllers and an alternative planning technique that also exploits a user specified hierarchy.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] proposed recursive controllers (which subsume hierarchical controllers) and an approach that discovers the hierarchy while optimizing a controller.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "In another line of research, Murphy and Paskin [10] proposed to model hierarchical hidden Markov models (HMMs) with a dynamic Bayesian network (DBN).",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : "[17] also used DBNs to model hierarchical POMDPs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "A recursive controller [4] consists of a recursive automaton with concrete nodes n and abstract nodes n̄.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "[4] show that optimizing a recursive controller with a fixed number of concrete and abstract nodes can be framed as a non-convex quartically constrained optimization problem.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "The pa|n and pn′|no′ distributions are combined in one distribution pn′a|no′ in [14]",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "Murphy and Paskin [10] proposed to model hierarchical hidden Markov models (HMMs) as dynamic Bayesian networks (DBNs).",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 9,
      "context" : "[10] and Toussaint et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] to convert a POMDP with a hierarchical controller into a mixture of DBNs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "’s technique [18], we optimize a factored controller by maximizing the reward likelihood.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "The mixture components are a uniform distribution, a random distribution Uφ(n′l) (an array of uniform random numbers in [0, 1]), and a term enforcing n to stay unchanged.",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 10,
      "context" : "EM variants with certain types of partial E-steps ensure monotonic improvement of the likelihood when the hidden variables are independent [11].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "We first compared the performance of the maximum likelihood (ML) approach to previous optimizationbased approaches from [4].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "The problems include paint, shuttle and 4x4 maze (previously used in [4]) and three additional problems: chain-of-chains (described below), hand-washing (reduced version from [8]) and cheese-taxi (variant from [12]).",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "The problems include paint, shuttle and 4x4 maze (previously used in [4]) and three additional problems: chain-of-chains (described below), hand-washing (reduced version from [8]) and cheese-taxi (variant from [12]).",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 11,
      "context" : "The problems include paint, shuttle and 4x4 maze (previously used in [4]) and three additional problems: chain-of-chains (described below), hand-washing (reduced version from [8]) and cheese-taxi (variant from [12]).",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 2,
      "context" : "Table 2: V ∗ denotes optimal values (with truncated trajectories) [3] except for handwashing and cheese-taxi where we show the optimal value of the equivalent fully-observable problem.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "Problem |S|, |A|, |O| V ∗ HSVI2 Best results from [4] ML approach (avg.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "iteration method (HSVI2 [15]).",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "In particular, we solved problems like handwashing and cheese-taxi that could not be solved with the previous approaches in [4].",
      "startOffset" : 124,
      "endOffset" : 127
    } ],
    "year" : 2008,
    "abstractText" : "Planning can often be simplified by decomposing the task into smaller tasks arranged hierarchically. Charlin et al. [4] recently showed that the hierarchy discovery problem can be framed as a non-convex optimization problem. However, the inherent computational difficulty of solving such an optimization problem makes it hard to scale to realworld problems. In another line of research, Toussaint et al. [18] developed a method to solve planning problems by maximumlikelihood estimation. In this paper, we show how the hierarchy discovery problem in partially observable domains can be tackled using a similar maximum likelihood approach. Our technique first transforms the problem into a dynamic Bayesian network through which a hierarchical structure can naturally be discovered while optimizing the policy. Experimental results demonstrate that this approach scales better than previous techniques based on non-convex optimization.",
    "creator" : "TeX"
  }
}