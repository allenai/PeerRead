{"conference": "ICLR 2017 conference submission", "title": "Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM", "abstract": "Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to numerical stability and quantifiability of the likelihood,  RBM is commonly used with Bernoulli units. Here, we consider an alternative member of exponential family RBM with leaky rectified linear units -- called leaky RBM.  We first study the joint and marginal distributions of leaky RBM under different leakiness, which provides us important insights by connecting the leaky RBM model and truncated Gaussian distributions.  The connection leads us to a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; -- i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations.  This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and more accurate than the commonly used annealed importance sampling (AIS).  We further demonstrate that the proposed sampling algorithm enjoys faster mixing property than contrastive divergence algorithm, which benefits the training without any additional computational cost.", "reviews": [{"is_meta_review": true, "comments": "The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.\n\n It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.\n 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.\n 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.\n 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.\n \n This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Upload revision", "OTHER_KEYS": "Chun-Liang Li", "comments": "We thank all reviewers for their careful reading of the paper and constructive feedback. We have extensively revised the paper to address these requests for clarifications and experiments. Here, we explain some of these and point to related updates in our revision. Please check the individual replies below the reviews.\n", "IS_META_REVIEW": false, "DATE": "06 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "No Title", "is_meta_review": false, "comments": "The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.\n\n It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.\n 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.\n 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.\n 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "23 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "A new model of RBM is proposed, where the conditional of the hidden is a leaky ReLU. In addition an annealed AIS sampler is also proposed to test the learned models quantifiably", "is_meta_review": false, "comments": "\nBased on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.\n\nPro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.\n\nCon: \nBecause of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.\n\nOn the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.\n\nThis paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "19 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "No Title", "is_meta_review": false, "comments": "The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.\n\nThis is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. \n\nUnfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.\n\nPROS:\nIntroduces an energy function having the leaky-relu as an activation function\nIntroduces a novel sampling procedure based on annealing the leakiness parameter\nSimilar sampling scheme shown to outperform AIS\n\nCONS:\nResults are somewhat out of date\nMissing experiments on binary datasets (more comparable to prior RBM work)\nMissing PCD baseline\nCost of projection method\n", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "18 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "(anonymous)", "TITLE": "some claims are incorrect, experiments should be improved", "is_meta_review": false, "comments": "This paper proposed a new variant of RBM, which has a nonlinearity of leaky ReLU, in contrast to the sigmoid function nonlinearity in RBM. By gradually annealing the leakiness coefficient (corresponding to from Gaussian to non-Gaussian model), the authors can sample from their model with a higher mixing rate. With the same idea annealing leakiness, they show they can estimate the partition function of the new model more accurately. \n\n\nMain comments:\n\nThe proposed model can only account for real-valued data. However, RBM is primarily used to model binary data, real-valued RBM (Gaussian-RBM) is not a well-recognized model for real-valued data. So, to demonstrate the superiority of the model, the author should also include the comparison with binary data. And it is also not enough to only compare two datasets for a newly proposed model.\n\nThe claim that the marginal distribution of visible variables is truncated Gaussian is incorrect. For a truncated normal, the values of variables are constrained to be within some region, e.g. requiring variable v from the region a1", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Clarity and better comparisons", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "04 Dec 2016", "is_meta_review": false}, {"TITLE": "Derivations and baselines", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Why not plot generated samples by the Leaky-ReLU RBM? ", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}], "SCORE": 5, "authors": "Chun-Liang Li, Siamak Ravanbakhsh, Barnabas Poczos", "KEYWORDS": "We study fundamental property of leaky RBM. We link the leaky RBM and truncated Gaussian distribution and propose a novel sampling algorithm without additional computation cost.", "accepted": false, "id": ""}
