{
  "name" : "1409.7612.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semi-supervised Classification for Natural Language Processing",
    "authors" : [ "Rushdi Shams" ],
    "emails" : [ "rshams@csd.uwo.ca." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 9.\n76 12\nv1 [\ncs .C\nL ]\n2 5\nSe p\n20 14\nSemi-supervised Classification for Natural Language Processing\nRushdi Shams Department of Computer Science, University of Western Ontario,\nLondon, ON N6A 5B7, Canada. email: rshams@csd.uwo.ca.\nAbstract—Semi-supervised classification is an interesting idea where classification models are learned from both labeled and unlabeled data. It has several advantages over supervised classification in natural language processing domain. For instance, supervised classification exploits only labeled data that are expensive, often difficult to get, inadequate in quantity, and require human experts for annotation. On the other hand, unlabeled data are inexpensive and abundant. Despite the fact that many factors limit the wide-spread use of semi-supervised classification, it has become popular since its level of performance is empirically as good as supervised classification. This study explores the possibilities and achievements as well as complexity and limitations of semi-supervised classification for several natural langue processing tasks like parsing, biomedical information processing, text classification, and summarization.\nKeywords: Semi-supervised learning, classification, natural language processing, data mining.\nI. INTRODUCTION\nClassical supervised methods use labeled data to train their classifier models. These methods are widespread and used in many different domains including natural language processing. The key material used in different natural language processing tasks is text. The number of text, however, is increasing everyday due to the pervasive use of computing. There are more unlabeled than labeled text since data labeling is expensive due to engagement of human for data annotation. The process also consumes time. These difficulties have serious effects on supervised learning since a good fit of a classifier model requires as much labeled data as possible for its training [1].\nSemi-supervised learning can be a good means to overcome the aforementioned problems. The basic principle of semisupervised learning is simple: use both unlabeled and labeled data to generate classifier models. This makes semi-supervised learning substantially useful for a domain like natural language processing because of a good note, unlabeled text is inexpensive, abundant, and more available than labeled text. In addition, empirically, semi-supervised models have good performance records. On many tasks, they are as good as supervised models and in most cases, they are better than cluster-based, unsupervised models [2]. However, many of these results are not conclusive and proper care therefore should be taken due to some serious concerns related to semisupervised learning.\nThis study explores the use of semi-supervised learning for natural language processing. Interestingly, like traditional\nmachine learning methods, semi-supervised learning can be used to solve classification, regression, and clustering problems. The particular focus of this study, however, is on semisupervised classification. In this study, popular research papers and classic books are explored to outline the possibilities and achievements of semi-supervised classification for natural language processing. As well, thorough investigations are carried out, both from theoretical and empirical data, to explain the complexity and limitations of this method. Natural language processing is one of the largest research areas of artificial intelligence. The scope of this study is, however, limited to most popular tasks such as parsing, biomedical information processing, text classification, and summarization.\nThe organization of the paper is as follows. Section II presents an overview of semi-supervised learning that includes learning problems, and different types of semi-supervised algorithms and learning techniques. Following that Section III outlines the use of semi-supervised classification for different natural language processing tasks. In Section IV, several considerations and conclusions are drawn."
    }, {
      "heading" : "II. OVERVIEW OF SEMI-SUPERVISED LEARNING",
      "text" : "Unlike supervised and unsupervised learning, semisupervised learning exploits both labeled and unlabeled data. To start with, semi-supervised methods train models with a very little labeled data. Surprisingly, test results show that marginal labeled data are sufficient to train models with good fit for semi-supervised learning [3]. The generated models are then applied on unlabeled data in an attempt to label them. The confidence of the models in labeling them is measured against a confidence threshold set a priori by users. Note that\nlearning algorithms often have their own confidence measures that generally depend on their working principles. For instance, class probability values for each data instance are considered as confidence measures for Naı̈ve Bayes models [4]. For an unlabeled data, if the models reach the pre-set confidence threshold, then the newly labeled data are added to the pool of originally labeled data. This process continues unless (i) the models’ confidences for the labels stop reaching the threshold, or (ii) the models confidently label all the unlabeled data and there are no unlabeled data remaining in the dataset. The interesting cycle of labeling and re-labeling of semi-supervised learning is illustrated in Figure 1."
    }, {
      "heading" : "A. Learning Problems",
      "text" : "Semi-supervised learning problems can be broadly categorized into two groups: (i) transductive learning and (ii) inductive learning. Transductive learning is like a take-home exam. This group of semi-supervised learning tries to evaluate the goodness of a model assumption on unlabeled data after training a classifier with the available labeled data. Inductive learning, on the contrary, is often seen as an in-class exam— it evaluates the goodness of a model assumption on unseen, unlabeled test data after training a classifier with both labeled and unlabeled data. Figure 1 shows the boundaries between these two types of semi-supervised learning. While the entire cycle in the figure illustrates inductive learning, steps 1—3 describe transductive learning."
    }, {
      "heading" : "B. Working Principle",
      "text" : "Figure 2 can be referred to understand how semi-supervised learning works with a very few labeled but abundant unlabeled data. Figure 2a shows that based on the position of a positive (x = 1) and a negative (x = −1) labeled data, a supervised decision boundary is drawn right at x = 0 based on the average of the data points. However, given only these two labeled data and 100 unlabeled data (represented by green dots in Figure 2b), this supervised decision boundary still remains at x = 0. In contrast, had a semi-supervised classifier been used, the boundary would have shifted more to the right (say some point at x = 0.4) than the supervised decision boundary. This shift is due to the distribution of unlabeled data points considering the position of the positive and negative examples. In this particular case, the semi-supervised classifier assumes that the green dots near to the red cross point form one kind of data\ndistribution while the green dots near to the blue circle form a different one. Interestingly, semi-supervised learning fails in many intriguing cases, where the distributions of labeled and unlabeled data are not as distinguishable as seen in Figure 2."
    }, {
      "heading" : "C. Types of Algorithms",
      "text" : "There are several semi-supervised algorithms and most of them can be categorized into two groups based on their properties: (i) generative algorithms and (ii) discriminative algorithms. The models generated by these two types of algorithms are therefore called generative and discriminative models, respectively. The following can explain the key difference between the two types of models. Say, we are given a set of speeches given by human presenters. As well, a set of languages are provided. The task is to simply classify every speech into one of the languages. This learning problem can be solved in either of the following two ways: first, the learner learns each language and then attempts to classify the speeches according to its learning. Second, the learner learns the differences among the speeches according to various attributes or features present in them and then attempts to classify the speeches according to its learning. Note that for the second case, the learner does not need to learn all the languages. The prior is called a generative learner and the latter is known as a discriminative learner. Let us take a look into these two types of algorithms mathematically. Say, we are given a set of instances x and their classes y in the form of (x, y): (1, 0), (1, 0), (2, 0), (2, 1). Generative algorithms attempt to find out the joint probability, p(x, y)\nfrom these data (see Figure 3a) while discriminative algorithms calculate their conditional probability, p(x|y) (Figure 3b). Now, for supervised algorithms a discriminative model predicts the label y from training example x as follows:\nf(x) = argmax y p(y|x). (1)\nHowever, from the Baye’s theorem, we know that\np(y|x) = p(x|y)p(y)\np(x) . (2)\nHowever, for Equation 1, p(x) can be ignored since it finds the function, f(x) for the maximum value of y. Therefore, ignoring p(x) in Equation 2 gives us\nf(x) = argmax y p(x|y)p(y). (3)\nInterestingly, Equation 3 is what supervised, generative algorithms use to induce their models. In other words, for supervised algorithms, Equation 1 is used to find out class boundaries based on the given training instances x and Equation 3 is used to generate x for any given y. The latter, however, is not found as easily for semi-supervised algorithms as for supervised algorithms. The first and foremost reason for this is that in semi-supervised problems, the algorithms cannot completely ignore p(x) because most of what it has are the distributions of training examples (i.e., p(x)). Moreover, for semi-supervised algorithms, a very few class labels are provided (for training examples) and therefore from the few given y’s, the conditional probabilities, p(x|y) are difficult to generate. This is a key difference between supervised and semi-supervised algorithms. An example is provided to understand the difference better. For semi-supervised algorithms, Equation 1 can be substituted by\np(y|x) = p(x|y)p(y)\n∑ y′ p(x|y′)p(y′) , (4)\nwhere y′ denotes the classes of the few given training examples x. Equation 4 has a probability density function p(x|y) in its numerator. If the distribution of x comes from a Gaussian and it is a function of mean vector and covariance matrix of the Gaussian, then using a Maximum Likelihood Estimate, the mean vector and covariance matrix can be tuned to maximize the density function. Thereafter, this tuning can be optimized using an Expectation-Maximization (EM) algorithm. Note that according to the distribution of x, different algorithms use different techniques for tuning and optimizing the density function p(x|y) in Equation 4. Among the semi-supervised algorithms, Transductive Support Vector Machine (TSVM) and graph-based methods are generative algorithms while EM and self-learning are discriminative algorithms."
    }, {
      "heading" : "D. Types of Learning",
      "text" : "The semi-supervised learning can be broadly categorized into three: (i) self-training, (ii) co-training, and (iii) active learning.\n1) Self-training: In self-training, from a set of initially labeled data L, a classifier, C1 is generated. This classifier is then applied on a set of initially unlabeled data U . According to a pre-set confidence threshold, the classifications of unlabeled data are observed. If the classifier’s confidence reaches the threshold, the newly classified instances are concatenated with L to produce a set Lnew and removed from U to produce Unew. A second classifier, C2 is generated from Lnew, and thereafter applied on Unew. This cycle continues until the classifier converges—which means that either (a) all the unlabeled data are confidently labeled by the classifier or (b) the classifier’s confidence stops reaching the threshold for several cycles. Self-training is very simple and particularly useful if the supervised algorithm is difficult to modify. Nonetheless, selftraining performs poorly for datasets that contain large number of outliers.\n2) Co-training: In contrast to self-training, for co-training, two partitions L1 and L2 are created from the initially labeled data L. The partitions are based on two different sets of attributes or features V1 and V2 (in semi-supervised literature, they are often referred to as views). Then, two classifiers independently generates respective models F1 and F2 from L1 and L2 using V1 and V2. Following that, from the unlabeled data pool U , k most confident predictions of F1 are added to L2 and k most confident predictions of F2 are added to L1. These added examples are removed from U . F1 is re-trained with L1 and F2 is re-trained with L2. This cycle continues until the classifiers converge. Finally, using a voting or averaging method, test data are classified. Note that co-training can be seen as self-training with two or more classifiers. Co-training is very useful if the attributes or features naturally split into two distinguishable sets. However, there are two important conditions that should be met for co-training to work. Given enough labeled data,\n1) each view alone should be sufficient to make good classifications and 2) the co-trained algorithms should individually perform good.\n3) Active Learning: Finally, for active learning a model is generated from labeled data and attempts to classify unlabeled instances. The classification it makes is then provided to a human expert called the oracle for her judgment. The correctly labeled instances according to the oracle are then added to the pool of labeled data while the instances with incorrect labels remain in the unlabeled data pool. This process continues until the unlabeled data pool becomes empty. Active learning is very useful for limited available data (both labeled and unlabeled). Because of the presence of an oracle, this semi-supervised learning is slow and almost always expensive."
    }, {
      "heading" : "III. SEMI-SUPERVISED CLASSIFICATION FOR NATURAL LANGUAGE PROCESSING",
      "text" : "In this section, different natural language processing applications of semi-supervised classification are discussed. The discussion is mainly based on the findings from several classic and state-of-the-art literature from the domain of parsing, text classification, text summarization, and biomedical information mining."
    }, {
      "heading" : "A. Parsing",
      "text" : "Steedman et al. [5] found that self-training has very small effects on parser improvements. Similar results are reported by Clark et al. [6] who applied self-training to part-of-speech (POS) tagging. The only works that reported successful execution of self-training to improve parsers are very few [7] [8]. This paper concentrates on the work of McClosky et al. because they do not adapt the parser in use that because adaptation has some drastic effects on self-training. Rather than using an adaptive parser, the Charniak parser used in their research utilized both labeled and unlabeled data that come from the same source domain. Using of a re-ranker besides the parser is also what makes their work different than many contemporary work. The parser uses third order Markov grammar and five probability distributions that are conditioned with more than five linguistic attributes. Firstly, the parser produces 50-best parses for the sentences in the datasets. Secondly, a maximum entropy re-ranker with over a million attributes re-ranks these parses. The experiment is extensive: datasets used in this experiment are Penn treebank section 2− 21 for training (approximately 40, 000 wall street journal articles), section 23 for testing, and section 24 for heldout data. 24 million LA Times articles were used as unlabelled data collected from the North American News Text Corpus (NANC). The authors experiment with and without the reranker as they added unlabelled sentences to their labeled data pool. They found that the parser performs better with the reranker system. The improvement reported is about 1.1% Fscore—among which the self-trained parser contributes 0.8% and the re-ranker contributes 0.3%). The authors also did some experiments with sentences in section 1, 22, and 24 to see how the self-trained parser performs at sentence level. Each sentence in these sections was labelled as better, no change or worse compared to the baseline F-score for the sentences. Interestingly, the outcomes showed that the parser had improvement neither for unknown words nor for prepositional phrases. However, there was an explicit improvement for intermediate-length sentences but no improvement for the extremes (Goldilocks effect). The parser performs poorly for conjunctions.\nZhu [9], however, asserted that in semi-supervised classification, unlabeled sentences for which the parser accuracy is unusually better than normal should be restricted to be included in the pool of labeled data. McClosky et al. [7], however, stated that they did not followed this approach particularly. The speed of the semi-supervised Charniak parser is similar to its supervised version but it requires more memory to execute the cycles involved in self-training. Also, the labeled and unlabeled data were collected from two different datasets (although they are both newspaper sources) that usually limits the success of self-training. Nevertheless, the experiment is a success and this question is unanswered in their paper."
    }, {
      "heading" : "B. Text Classification",
      "text" : "Semi-supervised classification has been used widely in natural language processing tasks such as spam classification, which is a form of text classification. The results in 2006 ECML/PKDD spam discovery challenge [10] indicated that spam filters based on semi-supervised classification outperformed supervised filters. Extensive experiments showed that\nsemi-supervised filters work better when source of available labeled examples differs from those to be classified. Interestingly, Mojdeh and Cormack [11] found completely different results when they re-designed the challenge with different collections of email datasets.\nThe 2006 ECML/PKDD discovery challenge had two interesting tasks. The first task is called the Delayed Feedback where the filters are trained with emails T1 and then they classify some test emails t1. In their second cycle of training, they are trained with T1 and t1 combined and the training continues for the entire dataset for the challenge. The best (1−AUC) reported in the challenge is a remarkable 0.01%. The second task for the challenge is called the Cross-user Train where the classifiers are trained on one particular set of emails and then tested on a completely different set of emails. The best (1−AUC) reported for this task is greater than the first task: 0.1%. The best performing filters in the challenge were all semi-supervised filters and based on support vector machines SVM and TSVM [12], Dynamic Markov Compression (DMC) [13], and Logistic regression with self-training (LR) [14]. On the other hand, in 2007 TREC Spam Track Challenge [15], the participating spam filters were trained with publicly available emails and their model accuracy was tested on emails collected from user inboxes (i.e., personalized emails). In an attempt to see whether semi-supervised filters perform as good as it was reported in [10], Mojdeh and Cormack [11] reproduced the work by replacing the datasets of ECML/PKDD challenge with TREC challenge datasets. The delayed feedback task was reproduced as follows: first, 10, 000 messages were used for training and the next 60, 000 messages were divided into six batches each containing 10, 000 messages. Second, the remaining 5, 000 messages were kept for testing the models. On the other hand, to reproduce the Cross-user Train task, 30, 338 messages from particular user inboxes were used for training while 45, 081 messages from other users were used for model evaluation.\nThe experimental outcomes showed that for both the tasks, the semi-supervised versions of DMC, LR, and TSVM underperformed for LREC dataset. Their respective 1−AUC scores for the delayed feedback task were 0.090, 0.046, and 0.230. On the other hand, the 1−AUC of their supervised versions were 0.016, 0.049, and 0.030 for the task. For the cross-user task, the 1−AUC of the semi-supervised DMC, LR, and TSVM filters were 9.97, 10.72, and 24.3, respectively. For the same task, their supervised versions performed way better. The authors also reported a cross-corpus experiment to reproduce the results of ECML/PKDD Challenge. Here, the first 10, 000 messages from the TREC 2005 dataset were considered. Besides, the TREC 2007 dataset was split into 10, 000 message segments. The outcomes again showed that self-training is harmful for the filters. Except the TSVM filter, the rest of the two semi-supervised filters failed to perform as good as their supervised versions.\nKeeping the aforementioned results in mind, we can say that semi-supervised classification is applicable to text classification but the performance depends on the labeled and unlabeled training data, and the source from which the data are derived."
    }, {
      "heading" : "C. Extractive Text Summarization",
      "text" : "Wong et al. [16] have conducted a comparative study where they produced extractive summaries by using both supervised and semi-supervised classifiers. The authors used four traditional groups of attributes to train their classifiers: (1) surface (2) relevance (3) event, and (4) content attributes. They tried different combinations of the attributes and found that the classifiers had produced better summaries when the surface, relevance, and content attributes were combined. The novelty of their work is that they used supervised SVM as well as its semi-supervised version called probabilistic SVM or PSVM to generate classifiers and compared their performances. As performance measure they considered ROUGE scores and found that the ROUGE-I score of their SVM classifier is 0.396 while the human ROUGE-I was 0.42 when compared to the gold standard summaries. On the other hand, the co-training with the PSVM and Naı̈ve Bayes classifiers produced summaries that have ROUGE-I of 0.366. Although this performance is not better than what they found with the supervised SVM or human summaries, it was better than supervised PSVM and Naı̈ve Bayes classifiers. Note that as their datasets, the authors used the DUC 2001 dataset1. The dataset contains 30 clusters of relevant documents. Each cluster comes with model summaries created by the dataset annotators. 50, 100, 200, and 400-word summaries are provided for each cluster. Among the clusters, 25 are used as training data while the remaining 5 clusters are used for testing. The authors also concluded that the ROUGE-I scores of their classifier are better if they produce 400-word summaries for the test clusters.\nNevertheless, the reported methodology of the paper has some serious drawbacks. Many of the methods used in this research are not in line with what had been found by classic empirical studies. For instance, the co-training is done on the same attribute space that violates the primary hypothesis of cotraining: two classifiers used in co-training should use separate views (see Section II-D2). Secondly, the authors selected the set of attributes (surface, relevance, and content attributes) by only considering the performance of PSVM with them and ignoring the performance of the supervised Naı̈ve Bayes with them."
    }, {
      "heading" : "D. Biomedical Information Mining",
      "text" : "Now-a-days, there is much impetus for information mining from biomedical research papers. Researchers put significant effort to mine secondary information such as protein interaction relations from biomedical research papers to help identify primary information like DNA replication, genotypephenotype relations, and signaling pathways. The first and foremost task for protein interaction relation miners is to classify sentences in research papers that describe one or more protein interactions. These sentences are called the candidate sentences. A number of supervised tools are developed to classify candidate sentences from biomedical articles (see for example [17], [18], and [19]). However, the first semisupervised approach for the task was reported by Erkan et al. [20]. Their approach identified candidate sentences using similarities of the paths present between two protein names found from the dependency parses of the sentences. What follows are\n1Download at: http://duc.nist.gov\nthe brief descriptions of their method. The authors produced dependency trees for each sentence from two given datasets. The paths between two protein names in the parse trees were then analyzed. According to these paths, the sentences were labeled and treated as the gold standard for the tool’s evaluation. Given the paths, two distance-based measures, cosine similarity and edit distance, were used by their tool to find out interactions between the proteins. These measures were provided to both supervised and semi-supervised algorithms to generate models to classify the sentences in the datasets. The labels predicted by the supervised and semi-supervised classifiers were then evaluated against the gold standard. According to the outcomes, the semi-supervised classifiers performed better than their supervised versions by a wide margin. Four algorithms were used to generate the classifiers among which two are supervised (SVM and K-Nearest Neighbor (KNN)) and the rest were their respective semi-supervised versions (TSVM and Harmonic Functions). The distance-based measures were used to generate attributes for the classifiers and were extracted from two datasets named AIMED and ChristineBrun (CB). The AIMED dataset contains 4, 026 sentences of which 951 describe protein interactions while the CB dataset is composed of 4, 056 sentences of which 2, 202 describe protein interactions. Each of the four algorithms then generated a classifier from the two sets of attributes found from the two distance measures. Experimental outcomes show that for the AIMED dataset, TSVM with edit distance attributes performed the best with 59.96% F-score. This F-score was significantly better than the F-scores found using the supervised classifiers. Comparisons showed that the F-score with TSVM was significantly better than those reported by two contemporary work [18] [21]. On the other hand, the tool performed even better on the CB dataset where its TSVM classifier with edit distance based attributes produced an F-score of 85.20%. Similar to the result found with the AIMED dataset, the performances of the supervised classifiers were not satisfactory. The authors also examined the effect of the size of the labeled training data for the classifiers. In the case of AIMED, the authors found that with small labeled training data, semi-supervised algorithms were better. In addition, SVM performed poorly with less training data but as more data became available for its training, it started to perform well. On the other hand, for the CB dataset, KNN performed poorly with much labeled data. Interestingly, SVM performed competitively with the semisupervised classifiers with more labeled data.\nNote that TSVM is susceptible to the distribution of the labeled data. However, the work did not report any test on the data distribution. The AIMED dataset, in addition, has class imbalance problem that seriously affects the performance of TSVM classifiers. This can be seen as the limitation of the work since it did not explain why in their case the TSVM classifier performed better than the rest."
    }, {
      "heading" : "IV. CONCLUSIONS",
      "text" : "The findings of empirical research on parsing, text classification, text summarization and biomedical information mining are investigated in this study. According to most of them, semi-supervised classification has substantial advantages over supervised classification when labeled data are difficult to manage and unlabeled data are abundant. This paper also outlines the theories behind the success of semi-supervised\nclassification. According to the theories, there is no free lunch for semi-supervised classification rather its success depends on underlying data distribution, data complexity, model assumption, choice of proper algorithm, problem in hand, and most of all—experience. Surprisingly, the investigation has found that the classic studies often do not consider the do’s and don’ts suggested by the theories. Despite the success reported in the empirical studies, it is therefore inconclusive whether semisupervised classification can be really as useful as supervised classification.\nThe complexity associated with semi-supervised classification limits its use. This can be seen from the illustration in Figure 4. It shows the use of labeled and unlabeled data in semi-supervised classification. Each dot in the illustration represents a paper that uses semi-supervised classification. While the light gray dots represent older papers, the dark gray dots represent recent papers. We can come to two conclusions from this data:\n1) there are not much reported work that implement semi-supervised classification and a bulk of the reported work are old and 2) although the main purpose of using semi-supervised classification is the abundance of unlabeled data, the amount of unlabeled data used in research are at most 106 so far—in layman’s term, which is just above the number of people in a stadium.\nNevertheless, semi-supervised classification is the only option until now to deal the natural language processing problems where there are more unlabeled than labeled data. This study, however, points out the following suggestions for dealing with semi-supervised classification more effectively:\n1) The model assumption for semi-supervised algo-\nrithms must match the problem in hand. For instance, if the classes produce well-clustered data then expectation-maximization is a good algorithm to choose; if the attribute space can be naturally split into two sets then co-training is preferred; if two points with similar attribute values tend to be in the same class then graph-based method (not discussed in this paper) can be a reasonable choice; if SVM performs well on labeled data then TSVM is a natural extension; and given the supervised algorithm is complicated and difficult to modify, self-training is useful.\n2) The distributions of both labeled and unlabeled data need to be investigated. TSVM, for instance, performs poorly with unlabeled data that have highly overlapped positive and negative distribution since it assumes that its decision boundary would go right through the densest region. Therefore, in this case a TSVM classifier usually produces a lot of false positives and false negatives. 3) The proportion of labeled and unlabeled data is important to notice before choosing an algorithm. However, there is not conclusive remark on how the proportion affects the overall classification performance. 4) It has been found empirically that there is an effect of dependency among attributes on semi-supervised classification. To be more specific, with fewer labeled examples, the number of dependent attributes should be kept as low as possible. 5) Data noises should be investigated as they have effect on classification performance. It is easier to detect noise in the labeled data than in unlabeled data. Note that data noise has less effect on semi-supervised classification than supervised classification. 6) The labeled and unlabeled data, usually, are collected from different sources and this can affect the classification performance. If the labeled and unlabeled data are collected from completely different sources and their properties differ, then rather than using semisupervised classification, transfer learning and selftaught classification are encouraged to use [22]."
    } ],
    "references" : [ {
      "title" : "Text bundling: Statistics based data-reduction.",
      "author" : [ "L. Shih", "J.D. Rennie", "Y.-H. Chang", "D.R. Karger" ],
      "venue" : "AAAI Press,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Introduction to Semi-Supervised Learning",
      "author" : [ "X. Zhu", "A.B. Goldberg", "R. Brachman", "T. Dietterich" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Semi-Supervised Learning, 1st ed",
      "author" : [ "O. Chapelle", "B. Schlkopf", "A. Zien" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Estimating continuous distributions in bayesian classifiers",
      "author" : [ "G.H. John", "P. Langley" ],
      "venue" : "Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, ser. UAI’95. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1995, pp. 338–345.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Bootstrapping statistical parsers from small datasets",
      "author" : [ "M. Steedman", "M. Osborne", "A. Sarkar", "S. Clark", "R. Hwa", "J. Hockenmaier", "P. Ruhlen", "S. Baker", "J. Crim" ],
      "venue" : "Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - Volume 1, ser. EACL ’03. Stroudsburg, PA, USA: Association for Computational Linguistics, 2003, pp. 331–338. [Online]. Available: http://dx.doi.org/10.3115/1067807.1067851",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Bootstrapping pos taggers using unlabelled data",
      "author" : [ "S. Clark", "J.R. Curran", "M. Osborne" ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4, ser. CONLL ’03. Stroudsburg, PA, USA: Association for Computational Linguistics, 2003, pp. 49–55. [Online]. Available: http://dx.doi.org/10.3115/1119176.1119183",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Effective self-training for parsing",
      "author" : [ "D. Mcclosky", "E. Charniak", "M. Johnson" ],
      "venue" : "In Proc. N. American ACL (NAACL, 2006, pp. 152–159.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Map adaptation of stochastic grammars",
      "author" : [ "M. Bacchiani", "M. Riley", "B. Roark", "R. Sproat" ],
      "venue" : "Comput. Speech Lang., vol. 20, no. 1, pp. 41–68, Jan. 2006. [Online]. Available: http://dx.doi.org/10.1016/j.csl.2004.12.001",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Semi-supervised learning literature survey",
      "author" : [ "X. Zhu" ],
      "venue" : "Computer Sciences, University of Wisconsin-Madison, Tech. Rep. 1530, 2005. [Online]. Available: http://pages.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Ecml-pkdd discovery challenge 2006 overview",
      "author" : [ "S. Bickel" ],
      "venue" : "Proceedings of the ECML-PKDD Discovery Challenge Workshop, 2006.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Semi-supervised spam filtering: does it work?",
      "author" : [ "M. Mojdeh", "G.V. Cormack" ],
      "venue" : "Eds. ACM,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Advances in kernel methods",
      "author" : [ "T. Joachims" ],
      "venue" : "B. Schölkopf, C. J. C. Burges, and A. J. Smola, Eds. Cambridge, MA, USA: MIT Press, 1999, ch. Making Large-scale Support Vector Machine Learning Practical, pp. 169–184. [Online]. Available: http://dl.acm.org/citation.cfm?id=299094.299104",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Spam filtering using statistical data compression models",
      "author" : [ "A. Bratko", "G.V. Cormack", "D. R", "B. Filipic", "P. Chan", "T.R. Lynam", "T.R. Lynam" ],
      "venue" : "Journal of Machine Learning Research, vol. 7, pp. 2673–2698, 2006.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Harnessing unlabeled examples through iterative application of dynamic markov modeling",
      "author" : [ "G.V. Cormack" ],
      "venue" : "In Proceedings of the ECML-PKDD Discovery Challenge Workshop, 2006.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Trec 2006 spam track overview",
      "author" : [ "G. Cormack" ],
      "venue" : "Proceedings of TREC 2006, 2006.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Extractive summarization using  supervised and semi-supervised learning",
      "author" : [ "K.-F. Wong", "M. Wu", "W. Li" ],
      "venue" : "Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, ser. COLING ’08. Stroudsburg, PA, USA: Association for Computational Linguistics, 2008, pp. 985–992. [Online]. Available: http://dl.acm.org/citation.cfm?id=1599081.1599205",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Extracting protein-protein interaction information from biomedical text with svm.",
      "author" : [ "T. Mitsumori", "M. Murata", "Y. Fukuda", "K. Doi", "H. Doi" ],
      "venue" : "IEICE Transactions, vol. 89-D,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Extracting information on protein-protein interactions from biological literature based on machine learning approaches",
      "author" : [ "K. Sugiyama", "K. Hatano", "M. Yoshikawa", "S. Uemura" ],
      "venue" : "Genome Informatics Series, pp. 699–700, 2003.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Semi-supervised classification for extracting protein interaction sentences using dependency parsing",
      "author" : [ "G. Erkan", "A. Özgür", "D. Radev" ],
      "venue" : "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 2007, pp. 228–237.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Biomedical information extraction with predicate-argument structure patterns",
      "author" : [ "A. Yakushiji", "Y. Miyao", "Y. Tateisi", "J. Tsujii" ],
      "venue" : "Proceedings of the 11th Annual Meeting of the Association for Natural Language Processing, 2005, pp. 60–69.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Self-taught learning: Transfer learning from unlabeled data",
      "author" : [ "R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng" ],
      "venue" : "Proceedings of the 24th International Conference on Machine Learning, ser. ICML ’07. New York, NY, USA: ACM, 2007, pp. 759–766. [Online]. Available: http://doi.acm.org/10.1145/1273496.1273592",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "These difficulties have serious effects on supervised learning since a good fit of a classifier model requires as much labeled data as possible for its training [1].",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "On many tasks, they are as good as supervised models and in most cases, they are better than cluster-based, unsupervised models [2].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "Surprisingly, test results show that marginal labeled data are sufficient to train models with good fit for semi-supervised learning [3].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "2: Supervised and semi-supervised decision boundaries drawn by a random classifier for two labeled and 100 unlabeled data [2].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "For instance, class probability values for each data instance are considered as confidence measures for Naı̈ve Bayes models [4].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "[5] found that self-training has very small effects on parser improvements.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] who applied self-training to part-of-speech (POS) tagging.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "The only works that reported successful execution of self-training to improve parsers are very few [7] [8].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "The only works that reported successful execution of self-training to improve parsers are very few [7] [8].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "Zhu [9], however, asserted that in semi-supervised classification, unlabeled sentences for which the parser accuracy is unusually better than normal should be restricted to be included in the pool of labeled data.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "[7], however, stated that they did not followed this approach particularly.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "The results in 2006 ECML/PKDD spam discovery challenge [10] indicated that spam filters based on semi-supervised classification outperformed supervised filters.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "Interestingly, Mojdeh and Cormack [11] found completely different results when they re-designed the challenge with different collections of email datasets.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "The best performing filters in the challenge were all semi-supervised filters and based on support vector machines SVM and TSVM [12], Dynamic Markov Compression (DMC) [13], and Logistic regression with self-training (LR) [14].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "The best performing filters in the challenge were all semi-supervised filters and based on support vector machines SVM and TSVM [12], Dynamic Markov Compression (DMC) [13], and Logistic regression with self-training (LR) [14].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 13,
      "context" : "The best performing filters in the challenge were all semi-supervised filters and based on support vector machines SVM and TSVM [12], Dynamic Markov Compression (DMC) [13], and Logistic regression with self-training (LR) [14].",
      "startOffset" : 221,
      "endOffset" : 225
    }, {
      "referenceID" : 14,
      "context" : "On the other hand, in 2007 TREC Spam Track Challenge [15], the participating spam filters were trained with publicly available emails and their model accuracy was tested on emails collected from user inboxes (i.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "In an attempt to see whether semi-supervised filters perform as good as it was reported in [10], Mojdeh and Cormack [11] reproduced the work by replacing the datasets of ECML/PKDD challenge with TREC challenge datasets.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "In an attempt to see whether semi-supervised filters perform as good as it was reported in [10], Mojdeh and Cormack [11] reproduced the work by replacing the datasets of ECML/PKDD challenge with TREC challenge datasets.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "[16] have conducted a comparative study where they produced extractive summaries by using both supervised and semi-supervised classifiers.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "A number of supervised tools are developed to classify candidate sentences from biomedical articles (see for example [17], [18], and [19]).",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "A number of supervised tools are developed to classify candidate sentences from biomedical articles (see for example [17], [18], and [19]).",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 18,
      "context" : "[20].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Comparisons showed that the F-score with TSVM was significantly better than those reported by two contemporary work [18] [21].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "Comparisons showed that the F-score with TSVM was significantly better than those reported by two contemporary work [18] [21].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "Light gray dots mean older papers while dark gray dots mean newer papers [9].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 20,
      "context" : "If the labeled and unlabeled data are collected from completely different sources and their properties differ, then rather than using semisupervised classification, transfer learning and selftaught classification are encouraged to use [22].",
      "startOffset" : 235,
      "endOffset" : 239
    } ],
    "year" : 2014,
    "abstractText" : "Semi-supervised classification is an interesting idea where classification models are learned from both labeled and unlabeled data. It has several advantages over supervised classification in natural language processing domain. For instance, supervised classification exploits only labeled data that are expensive, often difficult to get, inadequate in quantity, and require human experts for annotation. On the other hand, unlabeled data are inexpensive and abundant. Despite the fact that many factors limit the wide-spread use of semi-supervised classification, it has become popular since its level of performance is empirically as good as supervised classification. This study explores the possibilities and achievements as well as complexity and limitations of semi-supervised classification for several natural langue processing tasks like parsing, biomedical information processing, text classification, and summarization.",
    "creator" : "LaTeX with hyperref package"
  }
}