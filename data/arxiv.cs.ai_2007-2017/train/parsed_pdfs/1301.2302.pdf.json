{
  "name" : "1301.2302.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Toward General Analysis of Recursive Probability Models",
    "authors" : [ "George Luger" ],
    "emails" : [ "@cs." ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 INTRODUCTION\nThe limitations of flat Bayesian Networks (BNs) using simple random variables have been widely noted by researchers [Xiang et al., 1993; Laskey and Mahoney, I 997]. These limitations have motivated a variety of recent research projects in hierarchical and composable Bayesian models [Koller and Pfeffer, 1997; Koller and Pfeffer, 1998; Laskey and Mahoney, 1997; Pfeffer et al., 1999; Xiang et al., 2000]. Most of these new Bayesian modeling formalisms support model decomposition, often based on an object-oriented approach. Although these approaches provide more expressive and/or succinct representational frameworks, few of these change the class of models that can be represented.\nRecent research has addressed this issue. One example is the functional stochastic modeling language proposed by Koller et al. [ 1997]. Their language is Turing complete, allowing the representation of a much broader class of models. Pless et al. [2000] extends and refines this proposed framework to one which is more object-oriented and which allows hierarchical encapsulation of models. Both languages provide the ability to use functions to represent general stochastic relationships. They both also use lazy evaluation to allow computation over potentially infinite distributions. Pfeffer [2000] and Pfeffer and Koller [2000] have also proposed a Turing complete framework based on approximate inference.\nAnother approach to the representation problem for stochastic inference is the extension of the usual propositional nodes for Bayesian inference to the more general language of first-order logic. Kersting and De Raedt [2000] associated first-order rules with uncertainty parameters as the basis for creating Bayesian networks as well as more complex models. Poole [1993] gives an earlier approach which develops an approximate algorithm for another Turing complete probabilistic logic language.\nThese approaches all have in common the development of recursive models that bring together inference in Bayesian Networks with more complex models such as stochastic context free grammars. The result aims at allowing the construction and inference in novel Bayesian models. All of these methods depend on caching of partial results for efficiency purposes, just as efficient inference in Bayesian Networks requires the storage of intermediate values.\nIn this paper we offer an extension to the traditional A. calculus that provides a foundation for building Turing complete stochastic modeling languages. We have also developed a class of exact stochastic inference algorithms based on the traditional reductions in the A.-calculus. We further propose the use of deBruijn [ 1972] notation to support effective caching mechanisms for efficient computation. As noted above, caching offers an important technique for support of efficient inference in stochastic networks.\n430 PLESS & LUGER UAJ 2001\nThe problem with using the A.-calculus directly is that it is quite natural to develop two or more expressions that are equivalent, only differing in the choice of bound variable names. Furthermore, variable substitution is complicated by the requirement of variable binding and capture, which often makes substitutions quite expensive.\ndeBruijn notation addresses both of these issues by replacing arbitrary variable names with explicitly specified positive integers, which addresses the naming problem. It simultaneously renders the variable capture problem easy, by eliminating the arbitrary variable names that can be accidentally bound, and thus makes the substitution problem less computationally expensive.\nAs a final note, other recent research has viewed stochastic modeling in terms of stochastic functions [Pearl, 2000; Koller, and Pfeffer, 1997]. For example, Pearl's [2000] recent book constructs a formalism for \"causality\" in terms of stochastic functions. We have expanded these ideas based on an extension of the A. calculus, in which the stochastic functions themselves become first class objects, to offer a formal structure for such modeling.\n2 THE EXTENDED A.-CALCULUS FORMALISM\nWe now present a formal grammar reflecting our extension of the A.-calculus to describe stochastic distributions. The goal of this effort is to propose an extended form that also supports an inference algorithm as a set of standard transformations and reductions of A. calculus forms. Thus, inference in our modeling language is equivalent to finding normal forms in the A.-calculus. We also modify our language through the use of deBruijn notation [deBruijn, 1972]. This notation replaces arbitrarily chosen variable names with uniquely determined positive integers. As a result all expressions that are a.-equivalent in standard notation are identical under deBruijn notation. This is very useful in both constructing distributions as weii as in caching partial results.\n2.1 Syntax\nWe next present a pseudo-BNF grammar to describe our stochastic extension of the traditional A.-calculus:\n<expr> ::= <var> I <'A> I <application> I <distribution> <var> ::= <integer> <A.> ::= (A. <expr>) <application> :: = (<expr>1 <expr>2) <distribution>:: = L; <expr>;: <p>i p E (0, 1]\nThus, our stochastic A.-calculus contains the same elements as standard A.-calculus: variables, A.-abstractions, and function applications. In addition, in our stochastic A.-\ncalculus, it is legal to have an expression which is itself a distribution of expressions.\nOne difficulty mentioned earlier with standard A.-calculus is that there is an unfortunate representational indeterminacy in the arbitrary choice of bound variable names. Thus two completely equivalent (under a.-rule) expressions can have different forms. This presents a problem as we need to be able to combine probabilities of the same expression occurring within a distribution. Therefore we use deBruijn notation to give each expression a canonical form. This canonical form also makes 0(1) caching of the evaluation of expressions possible.\nAnother advantage of deBruijn notation is that it simplifies substitution (as discussed in section 3) and allows for the reuse of entire sub-expressions, which allows faster substitution when performing A. reductions.\nIt should be noted that deBruijn [ 1972] proposed the notation as an improvement for machine manipulation of expressions and not for human use. Our purpose in developing the stochastic A.-calculus is to provide the expressiveness of a higher order representation and an effective framework for inference. We are not aiming to develop a user-friendly language in this paper. In actual model development, one might use a high level language similar to the other languages discussed in the introduction, and then compile that language to our stochastic A.-calculus.\nWhen using deBruijn notation, we denote a variable by a positive integer. This number indicates how many As one must go out to find the one \"A. to which that variable is bound. We denote a \"A.-abstraction with the form Q>. e) where e is some legal expression. For example (A. 1) represents the identity function. In A.-calculus, boolean values are often represented by functions that take two arguments. true returns the first one, and false the second. In this notation true becomes (A. (\"A. 2)) and false is (\"A. (\"A. 1)), or in an abbreviated form (A.A. 2) and (U 1) respectively.\nFor a further example we use deBruijn notation to describe the S operator from combinatory logic. The S operator may be described by the rule Sxyz := (xz)(yz) which is equivalent to the standard I.. term (hlyA.z.(xz)(yz)). In deBruijn notation, this becomes (A.t...A. (3 1)(2 1)).\nFunction application is as one might expect: We have (er e2), where e1 is an expression whose value will be applied as a function call on C2, where C2 must also be a valid expression. We describe distributions as a set of expressions annotated with probabilities. An example would be a distribution that is 60% true and 40% false. Using the representation for boolean values given above, the resulting expression would be: {(A.A. 2): 0.6, (A.A. 1): 0.4}. Note that we use a summation notation in our BNF specification. The set notation is convenient for denoting a particular distribution, while the summation notation is better for expressing general rules and algorithms.\nUAI2001 PLESS & LUGER 431\n2.2 SEMANTICS\nWe next develop a specification for the semantics of our language. For expressions that do not contain distributions, the semantics (like the syntax) of the language is the same as that of the normal A-calculus. We have extended this semantics to handle distributions.\nA distribution may be thought of as a variable whose value will be determined randomly. It can take on the value of any element of its set with a probability given by the annotation for that element. For example, if T denotes true as represented above, and F represents false, the distribution {T: 0.6, F: 0.4} represents the distribution over true and false with probability 0.6 and 0.4 respectively.\nA distribution applied to an expression is viewed as equivalent to the distribution of each element of the distribution applied to the expression, weighted by the annotated probability. An expression applied to a distribution is likewise the distribution of the expression applied to each element of the distribution annotated by the corresponding probability. Note that in both these situations, when such a distribution is formed it may be necessary to combine syntactically identical terms by adding the annotated probabilities.\nUnder our grammar it is possible to construct distributions that contain other distributions directly within them. For example { T: 0.5, { T: 0.6, F: 0.4}: 0.5} is the same as { T: 0.8, F: 0.2}. One could explicitly define a reduction (see below) to handle this case, but in this paper we assume that the construction of distributions automatically involves a similar flattening just as it involves the combination of probabilities of syntactically identical terms.\nIn other situations an application of a function to an expression follows the standard substitution rules for the A-calculus with one exception: The substitution cannot be applied to a general expression unless it is known that the expression is not reducible to a distribution with more than one term. For example, an expression of the form ((A. e1) (A. e2)) can always be reduced to an equivalent expression by substituting e2 into e1 because (A. ez) is not reducible. We describe this situation formally with our presentation of the reductions in the next section on stochastic inference.\nThere is an important implication of the abo ve semantics. Every application of a function whose body includes a distribution causes an independent sampling of that distribution. There is no correlation between these samples. On the other hand, a function applied to a distribution induces a complete correlation between instances of the bound variables in the body of the function.\nFor example, using the symbols T and F as described earlier, we produce two similar expressions. The first version, Q.. 1 F l) {T: 0.6, F: 0.4}, demonstrates the induced correlations. This expression is equivalent to F (false). This expression is always false because the two\n1 's in the expression are completely correlated (see the discussion of the inference reductions below for a more formal demonstration). Now to construct the second version, let G = (A. {T: 0.6, F: 0.4} ). Thus G applied to anything produces the distribution { T: 0.6, F: 0.4}. So the second version ((G T) F (G T)) looks similar to the first one in that they both look equivalent to ( {T: 0.6, F: 0.4} F {T: 0.6, F: 0.4} ). The second version is equivalent because separate calls to the same f unction produce independent distributions. The first is not equivalent because of the induced correlation.\nFinally, it should be noted that we can express Bayesian Networks and many other more complex stochastic models, including Hidden Markov Models, with our language. Using the Y operator of combinatory logic [Hindley and Seldin, 1989], any recursive function can be represented in standard as well as stochastic A.-calculus. This operator has the property that Yf = f (Yf) in the standard A.-calculus. In our extended formalism, the Y operator is no longer a fixed-point operator for all expressions, but can be used to construct recursive functions. Thus the language is Turing complete, and can represent everything that other Turing complete languages can. For illustration, we next show how to represent the traditional Bayesian Network in our stochastic A.-calculus.\n2.3 AN EXAMPLE: REPRESENTING BAYESIAN NETWORKS (BNs)\nTo express a BN, we first construct a basic expression for each variable in the network. These expressions must then be combined to form an expression for a query. At first we just show the process for a query with no evidence. The technique for adding evidence will be shown later. A basic expression for a variable is simply a stochastic function of its parents.\nTo form an expression for the query, one must form each variable in tum by passing in the distribution for its parents as arguments. When a variable has more than one child, an abstraction must be formed to bind its value to be passed to each child separately.\nOur example BN has three Boolean variables: A, B, and C. Assume A is true with probability of 0.5. If A is true, then B is always true, otherwise B is true with probability of0.2. Finally, C is true when either A or B is true. Any conditional probability table can be expressed in this way, but the structured ones given in this example yield more terse expressions. The basic expressions (represented with both standard and deBruijn notation) are shown below:\n432 PLESS & LUGER UA12001\nA = {T: 0.5, F: 0.5} 8 = (/.A.(A T {T: 0.2, F: 0.8}))\n= (A. 1 T {T: 0.2, F: 0.8}) C = (/.AAB.(A T B)) = (A.A. 2 T 1)\nThe complete expression for the probability distribution for C is then ((A C 1 (B 1)) A). One can use this to express the conditional probability distribution that A is true given that Cis true: ((A. (C 1 (B 1)) 1 N) A) where N is an arbitrary tenn (not equivalent to T or F) that denotes the case that is conditioned away. To infer this probability distribution, one can use the reductions (defined below) to get to a normal fonn. This will be a distribution over T, F, and N, with the subsequent marginalizing away ofN.\nIn general, to express evidence, one can create a new node in the BN with three states. One state is that the evidence is false, the second is the evidence and the variable of interest are true, and the third represents the evidence is true and the variable of interest is false. One can then get the distribution for the variable of interest by marginalizing away the state representing the evidence being false. The extension to non boolean variables of interest is straightforward.\nOf course, a language with functions as first class objects can express more than Bayesian Networks. It is capable of expressing the same set of stochastic models as the earlier Turing complete modeling languages as proposed [Koller et al., 1997; Pless et al., 2000; Pfeffer, 2000; Pfeffer and Koller, 2000]. Any of those languages could be implemented as a layer on top of our stochastic A. calculus. In Pless et al. [2000] the modeling language is presented in tenns of an outer language for the user which is then transformed into an inner language appropriate for inference. Our stochastic A.-calculus could also be used as a compiled fonn of a more user friendly outer language.\nIn summary, we have created a Turing complete specification for representing stochastic reasoning. We have proposed an extension to the standard A.-calculus under deBruijn notation to give an effective form for inference (discussed in the next section}. Our specification can be used to de-couple the design of high level stochastic languages from the development of efficient inference schemes.\n3 STOCHASTIC INFERENCE THROUGH/.. REDUCTIONS\nWe next describe exact stochastic inference through the traditional methodology of the A.-calculus, a set of A. reductions. In addition to the �and 11 reductions, we also define a new form: y reductions.\n13: ((A. e1 ) �) 7 substitute( e1, e2) Yt.: ((Li fi: Pi) e) 7 Li (fi e): Pi )'R: (f L; e;: p;) 7 L; (f e;): p, rr: (A.(e 1)) 7 e\nWe have defined 13 reductions in a fashion similar to standard A.-calculus. Since we are using deBruijn notation, a. transfonnations become unnecessary (as there are no arbitrary dummy variable names). 13 and 11 reductions are similar to their conventional counterparts (see deBruijn [1972] for restrictions on when they may be applied). In our case the difference is that 13 reductions are more restricted in that expressions that are reducible to distributions cannot be substituted. Similarly rr reductions are restricted to those expressions (A. ( e 1)) where e cannot be reduced to a distribution. In addition to those two standard reductions we define two additional reductions that we term Yr. and \"fR. The y reductions are based on the fact that function application and distributions distribute.\nOne important advantage of using deBruijn notation is the ability to reuse expressions when perfonning substitutions. We next present a simple algorithm for substitutions when e2 is a closed expression:\nlevel(expr) = case ex.pr var 7 expr (A.e) 7 max(level(e) - 1, 0) (e1 e2) � max(level(e1 ), level(e2)) Li ei: Pi 7 maxi( level( ei))\nsubstitute((/... e1 ) , 9:2) = substitute(eh e2. 1)\nsubstitute(expr, a, L) = if level(expr) < L then expr else case expr\nvar7 a (A.e) 7 (A5ubstitute(e, a, L+1)) (e1 e2) 7 (substitute(e1 . a, L) substitute(e2, a, L)) Li ei: Pi 7 Li substitute(ej, a, L): �\nThis algorithm is designed to maximize the reuse of sub expressions. When a new expression is built, a non negative integer value, called the level, is associated with it. This value is the maximum number of 'As that have to surround the expression for it to be closed. The level is defined recursively, and is calculated directly from the sub-expressions from which the newly created expression is derived.\nFor a variable, its level is the number denoting the variable. For a A.-abstraction, the level is derived from the level of the expression in the body of the abstraction, but reduced by one (to a minimum of zero) due to the A. For forms that combine expressions (applications and distributions) the level is the maximum level of the sub expressions being combined. The level function reflects this recursive construction.\nThe level value (whose construction doesn't increase the asymptotic time for building expressions) is valuable for avoiding unnecessary substitutions. The substitute function defines how to substitute an expression ez into a A.-abstraction 0- e!). This results in a call to the three parameter recursive version of substitute. The first\nUA12001 PLESS & LUGER 433\nparameter is the expression being substituted into, the second is the expression being substituted, and the last is the variable value that has to be replaced by the second argument.\nIf the variable to be substituted is greater than the level of the expression, then there cannot be any substitutions needed as the variable number is greater than the number of A.s required to close the expression. In this case the expression can be directly returned (and reused). Otherwise, if the expression to be substituted is a variable, under the assumption that the original expression was closed, it must be the variable to be replaced. Thus, the substituting expression (second argument) can be returned (and reused). For A-abstractions, the substitution is performed on the body of the abstraction, but substituting for a variable one larger in value. The result is then placed back into a A.-abstraction. Finally, for the combining forms, the substitution is performed on all of the sub expressions and then recombined.\nAs noted earlier, we have defined two additional reductions that we call ')'L and 'YR· The 'YR reduction is essential for reducing applications where the � reduction cannot be applied. Continuing the example introduced earlier in the paper:\nYR (A. 1 F 1 ){T: 0.6, F: 0.4} �\n{((A. 1 F 1) T): 0.6, ((A. 1 F 1) F): 0.4)}\nNow since both T and F do not contain distributions, � reductions can be applied:\n� {((A. 1 F 1) T): 0.6, ((A 1 F 1) F): 0.4)} �\n{(T F T): 0.6, ((A 1 F 1) F): 0.4}\n� {(T F T): 0.6, ((A 1 F 1) F): 0.4} �\n{(T F T): 0.6, (F F F): 0.4}\nAnd now, using the definitions of T and F it is easy to see that (T F T) and (F F F) both are reducible to F.\n4 INFERENCE\nThe task of inference in our stochastic A.-calculus is the same as the problem of finding a normal form for an expression. In standard A.-calculus, a normal form is a term to which no � reduction can be applied. In the stochastic version, this must be modified to be any term to which no � or y reduction can be applied. It is a relatively simple task to extend the Church-Rosser theorem [Hindley and Seldin, 1986; deBruijn, 1972] to show that this normal form, when it exists for a given expression, is unique. Thus one can construct inference algorithms to operate in a manner similar to doing evaluation in a A. calculus system. Just as it is possible to produce complete function evaluation algorithms in standard A.-calculus, the stochastic A.-calculus admits complete inference schemes.\n4.1 A SIMPLE INFERENCE ALGORITHM\nWe next show a simple algorithm for doing such evaluation. This algorithm doesn't reduce to a normal form, rather to the equivalent of a weak head normal form [Reade, 1989].\npeval( expr) = case expr (Ae) � expr (e1 e2) � papply(peval(e1). 8-<!) Li ei: Pi 7 I. peval(ei): Pi\npapply(f, a) = case f Li �: Pi� L. papply(t a)::Pi (A fe) 7 case a\n(A. e) 7 peval(substitute(f, a)) (e1 e2) � papply(f, peval(a)) Li ei: Pi� Li papply(f, ei): Pi\npeval and papply are the extended version of eval and apply from languages such as LISP. peval implements left outermost first evaluation for function applications ((e1 e2)). For A-abstractions, (A e), no further evaluation is needed (it would be if one wanted a true normal form). For distributions, it evaluates each term in the set and then performs a weighted sum.\npapply uses a ')'L reduction when a distribution is being applied to some operand. When a A-abstraction is being applied, its behavior depends on the operand. When the operand is an abstraction, it applies a � reduction. If the operand is an application, it uses eager evaluation (evaluating the operand). When the operand is a distribution, it applies a 'YR reduction.\n4.2 EFFICIENCY ISSUES\nWe have presented this simple, but not optimal, algorithm for purposes of clarity. One key problem is that it uses lazy evaluation only when the operand is a A-abstraction. One would like to use lazy evaluation as much as possible. An obvious improvement would be to check to see if the bound variable in an operator is used at least one time. If it is not used then it doesn't matter whether the expression evaluates to a distribution or not, lazy evaluation can be applied.\nAnother potential improvement is to expand the set of cases in which it is determined that the operand cannot be reduced to a distribution. To make this determination in all cases is as hard as evaluating the operand, which is exactly what one tries to avoid through lazy evaluation. However, some cases may be easy to detect. For example, an expression that doesn't contain any distributions in its parse tree clearly will not evaluate to a distribution. One approach might be to use a typed A.-calculus to identify whether or not an expression could be reduced to a distribution.\nFinally, we may tailor the algorithm using the reductions in different orders for particular application domains. The\n434 PLESS & LUGER UAI2001\nalgorithm we presented doesn't utilize the l) reduction, which may help in some cases. Also identifying more cases when � reductions can be applied may allow for more efficient algorithms in specific applications.\nWe propose that employing the simple algorithm with the suggested improvements (both shown above) will essentially replicate the variable elimination algorithm for inference on BNs. The order for variable elimination is implicitly defined by the way that the BN is translated into a A.-expression. The use of A.-expressions to form conditional probability tables also allows the algorithm to exploit context specific independence [Boutilier et al, 1996}.\n4.3 CACHING\nEfficient computational inference in probabilistic systems generally involves the saving and reuse of partial and intermediate results [Koller et a!., 1997]. Algorithms for BBNs as well as for HMMs and other stochastic problems are often based on some form of dynamic programming [Dechter, 1996, Koller et a!., 1997]. Using deBruijn notation makes caching expressions easy. Without the ambiguity that arises from the arbitrary choice of variable names (:£-equivalence), one needs only to find exact matches for expressions.\nBecause it is possible in A.-calculus to use the Y (fixed point) operator from combinatory logic to represent recursion, there are no circular structures that need to be cached. Thus, only trees need be represented for caching of purely deterministic expressions. To cache distributions (which is necessary for non-deterministic caching) one needs to be able to cache and retrieve sets.\nOne way to accomplish this is to use the hashing method of Wegman and Carter [1981]. They propose a probabilistic method for producing hash values (fingerprints) for sets of integers. Their method is to associate each integer appearing in some set with a random bit string of fixed length. The bit strings for a particular set of integers are combined using the exclusive-or operation. Two sets are considered to be the same if the fingerprints for the two are the same. There is a probability that a false match can be found this way, which can be made arbitrarily low by increasing the string length.\nThis method can be used for caching the sets of weighted expressions (distributions) by associating a random bit string with each probability-expression pair that exists in some distribution. One can use a similar assignment of random strings to the integers representing variables. Also such a string can be assigned to fingerprint the A. in A. abstractions. One can assign such fingerprints to unique pairs that occur in building up expressions. In this way one can form a hash function which (given the values of the sub-expression from which the expression is formed) can be computed without increasing the asymptotic time for expression construction.\n5. APPROXIMATION\nOne of the strengths of viewing stochastic inference in terms of the A.-calculus with reductions is that it allows analysis of different parts of the expression to be handled differently. One way that can occur is to use different reduction orders on different parts of the expression. A powerful approach is to use approximations on different parts of the expression.\nOne may choose at some point in the evaluation to replace an expression with a reasonable distribution over the possible values the expression could potentially evaluate. Doing this at a fixed recursion level in the algorithm suggested above essentially gives the approximation suggested by Pfeffer and Koller [2000]. Other approximations include removing low probability elements from a distribution prior to performing a y reduction. This is analogous to the approximation for Bayesian Networks proposed by Jensen and Anderson [1990]. Furthermore, a portion of the expression may be sampled with a Monte-Carlo algorithm.\nFinally one can perform an improper � reduction when it is not allowed under stochastic A.-calculus: namely an application where the argument is reducible to a distribution and the function uses the argument more than once in its body. This last approximation corresponds to making an independence assumption that isn't directly implied by the form of the expression. That is, it assumes that the different instances of the argument in the body of the function are independent. The stochastic A.-calculus provides a framework for mixing and combining all of these different forms of approximation.\n6. CONCLUSIONS AND FUTURE WORK\nWe have presented a formal framework for recursive modeling languages. It is important to maintain the distinction between our modeling language and a traditional programming language. Our language is designed to construct and satisfy queries on stochastic models, not to build programs. Some of our design decisions follow from this fact. We have made function application on a distribution result in essentially sampling from that distribution. If one wants to pass a distribution to a function as an abstract object, rather than sample from it, one must wrap the distribution in a fv..abstraction. We believe that the use of distributions without sampling will be rare unless the distribution is parameterized, in which case a function is needed anyway.\nThe result of the above decision is that the concept of abstract equality of expressions in our formalism is not the same as in standard fv..calculus. In standard A.-calculus, equality between expressions can be defined in terms of equality of behavior when the expressions are applied to arguments. In our A.-calculus, it is possible. for two expressions to behave the same way when applied to any argument, but to differ in behavior when used as an argument to some other expression.\nUAI2001 PLESS & LUGER 435\nThere are a number of paths that would be interesting to follow. It would be useful to analyze the efficiency of various algorithms on standard problems, such as polytrees [Pearl, 1988], where the efficiency of the optimal algorithm is known. This may point to optimal reduction orderings and other improvements to inference. We are also looking at constructing formal models of the semantics of the language. Finally, we are considering the implications of moving from the pure A.-calculus presented here to an applicative A.-calculus. The results of that representational change, along with type inference mechanisms, may be important for further development in the theory of recursive stochastic modeling languages.\nAcknowledgements\nThis work was supported by NSF Grant 115-9800929. We would also like to thank Carl Stern and Barak Pearlmutter for many important discussions on our approach.\nReferences\n[Boutilier et al, 1996] C. Boutilier, N. Friedman, M. Goldszmidt, D. Koller. Context-Specific Independence in Bayesian Networks. In Proceedings of the Twelfth Annual Conference on Uncertainty in Artificial Intelligence (UAI96). 1996.\n[deBruijn, 1972] N.G. deBruijn. Lambda Calculus Notation with Nameless Dummies, A Tool for Automatic Formula Manipulation, with Application to the Church Rosser Theorem. In lndagationes mathematicae. 34:381- 392 1972.\n[Dechter, 1996] R. Dechter. Bucket elimination: A unifying framework for probabilistic inference. In Proceedings of the Twelfth Annual Conference on Uncertainty in Artificial Intelligence (UAI-96). 1996.\n[Hindley and Seldin, 1989] J.R. Hindley and J.P. Seldin. Introduction to Combinators and A.-Calculus. Cambridge, UK: Cambridge University Press. 1989.\n[Jensen and Andersen, 1990] F. V. Jensen and S. K. Anderson. Approximations in Bayesian Belief Universes for Knowledge-Based Systems. In Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence (UAI-90), Cambridge, MIT Press. 1990.\n[Kersting and De Raedt, 2000] Kristian Kersting and Luc De Raedt. In J. Cussens and A. Frisch, editors, Proceedings of the Work-in-Progress Track at the lOth International Conference on Inductive Logic Programming, pages 138- 155, 2000.\n[Koller et a!., 1997] D. Koller, D. McAllester, and A. Pfeffer. Effective Bayesian Inference for Stochastic Programs. In Proceedings of American Association of Artificial Intelligence Conference, Cambridge: MIT Press. 1997.\n[Koller and Pfeffer, 1997] D. Koller and A. Pfeffer. Object-oriented Bayesian Networks. In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97), San Francisco: Morgan Kaufmann. 1997.\n[Koller and Pfeffer, 1998] D. Koller and A. Pfeffer. Probabilistic Frame-Based Systems. In Proceedings of American Association of Artificial Intelligence Conference, Cambridge: MIT Press. 1998.\n[Laskey and Mahoney, 1997] K. Laskey and S. Mahoney. Network Fragments: Representing Knowledge for Constructing Probabilistic Models. In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97), San Francisco: Morgan Kaufmann. 1997.\n[Pearl, 1988] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, San Mateo CA: Morgan Kaufmann. 1988.\n[Pearl, 2000] J. Pearl. Causality. Cambridge, UK: Cambridge University Press. 2000.\n[Pfeffer, 2000] A. Pfeffer. Probabilistic Reasoning for Complex systems. Ph.D. Dissertation, Stanford University. 2000.\n[Pfeffer and Koller, 2000] A. Pfeffer and D. Koller. Semantics and Inference for Recursive Probability Models. In Proceedings of the Seventeenth National Conference on Artificial Intelligence. 538-544 Cambridge: MIT Press. 2000.\n[Pfeffer et al., 1999] A. Pfeffer, D. Koller, B. Milch, and K. Takusagawa. SPOOK: A System for Probabilistic Object-Oriented Knowledge Representation. In Proceedings of the 15th Annual Conference on Uncertainty in AI (UAI), San Francisco: Morgan Kaufmann. 1999.\n[Pless et a!., 2000] D. Pless, G. Luger, and C. Stem. A New Object-Oriented Stochastic Modeling Language. In Proceedings of the lASTED International Conference, Zurich: lASTED/ACTA Press. 2000.\n[Poole, 1993] D. Poole. Logic Programming, Abduction and Probability: a top-down anytime algorithm for estimating prior and posterior probabilities, New Generation Computing. 11(3-4), 377-400, 1993.\n(Reade, 1989] C. Reade. Elements of Functional Programming. New York: Addison-Wesley. 1989.\n[Wegman and Carter, 1981] M.N. Wegman and J. L Carter. New has functions and their use in authentication and set equality. Journal of Computer and System Sciences. 22(3): 265-279, 1981\n436 PLESS & LUGER UAI2001\n[Xiang et al., 2000] Y. Xiang, K.G. Olesen and F.V. Jensen. Practical Issues in Modeling Large Diagnostic Systems with Multiply Sectioned Bayesian Networks, International Journal of Pattern Recognition and\nArtificial Intelligence. 2000.\n[Xiang et al., 1993] Y. Xiang, D. Poole, and M. Beddoes. Multiply Sectioned Bayesian Networks and Junction Forests for Large Knowledge-Based Systems. Computationallntelligence, 9(2): 171-220. 1993."
    } ],
    "references" : [ {
      "title" : "Object-oriented Bayesian Networks",
      "author" : [ "Koller", "Pfeffer", "1997] D. Koller", "A. Pfeffer" ],
      "venue" : null,
      "citeRegEx" : "Koller et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Koller et al\\.",
      "year" : 1997
    }, {
      "title" : "Probabilistic Frame-Based Systems",
      "author" : [ "Koller", "Pfeffer", "1998] D. Koller", "A. Pfeffer" ],
      "venue" : null,
      "citeRegEx" : "Koller et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Koller et al\\.",
      "year" : 1998
    }, {
      "title" : "Logic Programming, Abduction and Probability: a top-down anytime algorithm for estimating prior and posterior",
      "author" : [ "D. Poole" ],
      "venue" : "[Poole,",
      "citeRegEx" : "Poole.,? \\Q1993\\E",
      "shortCiteRegEx" : "Poole.",
      "year" : 1993
    }, {
      "title" : "Practical Issues in Modeling Large Diagnostic Systems with Multiply Sectioned",
      "author" : [ "Xiang et al", "2000] Y. Xiang", "K.G. Olesen", "F.V. Jensen" ],
      "venue" : "Bayesian Networks,",
      "citeRegEx" : "al. et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2000
    }, {
      "title" : "Multiply Sectioned Bayesian Networks and Junction Forests for Large Knowledge-Based Systems. Computationallntelligence",
      "author" : [ "Xiang et al", "1993] Y. Xiang", "D. Poole", "M. Beddoes" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "One example is the functional stochastic modeling language proposed by Koller et al. [ 1997]. Their language is Turing complete, allowing the representation of a much broader class of models. Pless et al. [2000] extends and refines this proposed framework to one which is more object-oriented and which allows hierarchical encapsulation of models.",
      "startOffset" : 71,
      "endOffset" : 212
    }, {
      "referenceID" : 0,
      "context" : "One example is the functional stochastic modeling language proposed by Koller et al. [ 1997]. Their language is Turing complete, allowing the representation of a much broader class of models. Pless et al. [2000] extends and refines this proposed framework to one which is more object-oriented and which allows hierarchical encapsulation of models. Both languages provide the ability to use functions to represent general stochastic relationships. They both also use lazy evaluation to allow computation over potentially infinite distributions. Pfeffer [2000] and Pfeffer and Koller [2000] have also proposed a Turing complete framework based on approximate inference.",
      "startOffset" : 71,
      "endOffset" : 559
    }, {
      "referenceID" : 0,
      "context" : "One example is the functional stochastic modeling language proposed by Koller et al. [ 1997]. Their language is Turing complete, allowing the representation of a much broader class of models. Pless et al. [2000] extends and refines this proposed framework to one which is more object-oriented and which allows hierarchical encapsulation of models. Both languages provide the ability to use functions to represent general stochastic relationships. They both also use lazy evaluation to allow computation over potentially infinite distributions. Pfeffer [2000] and Pfeffer and Koller [2000] have also proposed a Turing complete framework based on approximate inference.",
      "startOffset" : 71,
      "endOffset" : 589
    }, {
      "referenceID" : 2,
      "context" : "Poole [1993] gives an earlier approach which develops an approximate algorithm for another Turing complete probabilistic logic language.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "It is capable of expressing the same set of stochastic models as the earlier Turing complete modeling languages as proposed [Koller et al., 1997; Pless et al., 2000; Pfeffer, 2000; Pfeffer and Koller, 2000].",
      "startOffset" : 124,
      "endOffset" : 206
    }, {
      "referenceID" : 0,
      "context" : "It is capable of expressing the same set of stochastic models as the earlier Turing complete modeling languages as proposed [Koller et al., 1997; Pless et al., 2000; Pfeffer, 2000; Pfeffer and Koller, 2000]. Any of those languages could be implemented as a layer on top of our stochastic A.­ calculus. In Pless et al. [2000] the modeling language is presented in tenns of an outer language for the user which is then transformed into an inner language appropriate for inference.",
      "startOffset" : 125,
      "endOffset" : 325
    } ],
    "year" : 2011,
    "abstractText" : "There is increasing interest within the research community in the design and use of recursive probability models. There remains concern about computational complexity costs and the fact that computing exact solutions can be intractable for many nonrecursive models. Although inference is undecidable in the general case for recursive problems, several research groups are actively developing computational techniques for recursive stochastic languages. We have developed an extension to the traditional A.­ calculus as a framework for families of Turing complete stochastic languages. We have also developed a class of exact inference algorithms based on the traditional reductions of the A.­ calculus. We further propose that using the deBruijn notation (a A.-calculus notation with nameless dummies) supports effective caching in such systems, as the reuse of partial solutions is an essential component of efficient computation. Finally, our extension to the A.-calculus offers a foundation and general theory for the construction of recursive stochastic modeling languages as well as promise for effective caching and efficient approximation algorithms for inference.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}