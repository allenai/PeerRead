{
  "name" : "1606.03976.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bounding and Minimizing Counterfactual Error",
    "authors" : [ "Uri Shalit", "Fredrik D. Johansson" ],
    "emails" : [ "shalit@cs.nyu.edu", "frejohk@chalmers.se", "dsontag@cs.nyu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Causal inference questions are central to policy makers and scientists across many fields. Examples abound: in healthcare one is often interested in the relative efficacy of different medications; in economic policy, policy makers debate the effect of job training; in marketing, companies are interested in the causal effect of an online ad on a customer’s buying habits. However, traditional causal inference methods are sometimes ill-suited to take advantage of the vast increase in the size and scope of data available across these fields. Here, we build on earlier work by [20], and show how modern machine learning techniques of representation learning [4] along with using integral probability metrics [30] can be fruitfully applied to problems of causal inference.\nOur work relies on the framework of counterfactual inference as a means towards causal inference, associated with the Rubin-Neyman potential outcomes framework [27]. It assumes that for a context x ∈ X , and a treatment or intervention t ∈ {0, 1}, there are two potential outcomes: Y0(x) for t = 0, and Y1(x) for t = 1. For example, x can denote the set of lab tests and demographic factors of a diabetic patient, t = 0 denote the standard medication for controlling blood sugar, t = 1 denotes a\n∗Equal contribution\nar X\niv :1\n60 6.\n03 97\n6v 1\n[ st\nat .M\nL ]\nnew medication, and Y0(x) and Y1(x) indicate the patient’s blood sugar level after treatments t = 0 and t = 1, respectively. We are usually interested in knowing τ(x) = Y1(x)− Y0(x), i.e. what is the treatment effect of t = 1. The fundamental problem of causal inference is, that, for a given context x, we only ever observe Y1(x) or Y0(x), but never both of them. Moreover, in many cases of interest, the distribution of the treatment assignment t is dependent on the context x, as is often the case in observational studies [26]. For example, medications might be prescribed based on specific coverage policies, and job training might only be given to those motivated enough to seek it.\nWe call the joint distribution over contexts and treatment the factual distribution, pF (x, t). The counterfactual distribution is pCF (x, t) = pF (x, 1 − t). The factual (observed) outcomes for unit xi ∈ X with treatment ti ∈ {0, 1} are: yF (xi) = tiY1(xi) + (1 − ti)Y0(xi). Similarly, the counterfactual outcomes are yF (xi) = (1− ti)Y1(xi) + tiY0(xi). Although we never have access to yCF (xi), we do have access to yF (xi). Therefore, one approach would to this problem would be to estimate Y0 and Y1 directly from the factual sample, using ordinary machine learning techniques. However, when generalizing to the counterfactual sample we have another source of variance beyond those considered in the classic supervised learning formulation. This is the variance incurred by learning from one distribution and performing inference on another [2, 23]. This problem is strongly related to the problem of covariate shift [20], and indeed our bound and algorithm could be easily adapted for use in covariate shift problems, in line with recently proposed methods [13].\nRecently, [20] have argued that the covariate shift variance can be controlled by learning a so-called “balanced representation”: a representation of the data that makes the factual and counterfactual distributions more similar. In their work, [20] study the performance of learning a balanced representation and then fitting a linear ridge-regression model on top of it. They bound the relative error of fitting a ridge-regression using the counterfactual distribution versus fitting a ridge-regression using the factual distribution. Unfortunately, the relative error term they bound is not informative regarding the absolute quality of the representation, neither on the factual nor on the counterfactual distribution.\nIn this paper we prove what we consider a much more informative bound. We show that under mild assumptions about the invertability of the learned representation, the expected counterfactual loss is upper-bounded by a sum of two terms: the expected factual loss, and a constant C times the distance between the distributions of the treated (t = 1) and control (t = 0) populations in the representation space. The intuition behind this bound is clear: generalizing to the counterfactual distribution is no harder than generalizing to the factual distribution from which we can sample, plus a term measuring the difference between the two distributions. Here, we use a general family of distance functions between distributions called Intergral Probability Metrics (IPM) [24, 30]. In particular we concentrate on two IPMs: The Wasserstein metric [32, 8]and the Maximum Mean Discrepancy metric (MMD, [15]). Both have been successfully applied in recent years to a wide variety of applications [16, 25, 11, 9, 12]. For the Wasserstein metric, we give an explicit term upper-bounding the constant C. Our bound is not limited to linear hypotheses, and holds for a wide class of loss functions, including the squared loss over a compact domain, the absolute loss, and the logistic loss, making our approach applicable to both classification and regression tasks.\nThe bound we derive points the way to family of algorithms: jointly learn a hypothesis and a representation which minimize a weighted sum of the factual loss (the standard supervised machine learning objective), and the distance between the control and treated distributions induced by the representation. In the Experiments section 4 we apply algorithms based on multi-layer neural nets as representations and hypotheses, along with MMD or Wasserstein distributional distance metrics; see Figure 1 for the basic architercture. These algorithms are conceptually simpler than the ones proposed by [20], since they do not require a two-stage fitting procedure, and avoid the need to compute nearest neighbors. We show that our methods achieve better performance on the synthetic tasks presented by [20]. We also show that our methods can achieve competitive results on a real world causal inference benchmark: the widely used National Supposed Work survey [22, 10, 29].\nOur contributions in this paper are as follows: (1) We prove a novel, simple and meaningful upper bound on the counterfactual loss. (2) We show the relation between minimizing counterfactual loss and minimizing the error in estimating treatment effect, which is often the quantity we are truly interested in. (3) Our bound and algorithm are applicable to a very wide family of loss functions and models for classification and regression, compared with the previous paper’s restriction to linear\nmodels and absolute loss. (4) Our experiments show the benefit of our method, and we show results on a real-world causal inference benchmark, the National Work Survey dataset [22, 29]."
    }, {
      "heading" : "2 Counterfactual learning bound",
      "text" : "In this section we will prove several bounds relating the learning of representations, factual and counterfactual loss, and error in estimating the treatment effect for unit x. The most important quantity we bound is `CF : the expected loss of a model, with expectation taken over the counterfactual distribution, defined below. The bounds are expressed in terms of `F : the expected loss of the same model over the factual distribution, along with a distance measure between the distribution of treated and control units. The term `F is the classic machine learning generalization error, and in turn can be upper bounded using the empirical error over the observations and model complexity terms, applying well-known machine learning theory [28]. All proofs are in the supplement."
    }, {
      "heading" : "2.1 Problem setup",
      "text" : "We will employ the following assumptions and notations. The most important notations are in the Notation box in the supplement. The space of covariates is a bounded subset X ⊂ Rd. The outcome space is Y ⊂ R. Treatment is a binary variable. There exists a factual distribution pF (x, t) defined jointly over the covariate and treatment space X ×{0, 1}. The treated and control distributions are the factual distribution conditioned on treatment: pt=1(x) := pF (x|t = 1), and pt=0(x) := pF (x|t = 0), respectively. The counterfactual distribution is the factual distribution with the treatment assignment flipped: pCF (x, t) := pF (x, 1 − t) = pF (1 − t|x)pF (x). Note that if treatment assignment is independent of the context x, and pF (t|x) = 12 , then the counterfactual and factual distributions are identical; this is exactly the case of a randomized-control trial, where treatment is assigned randomly with equal probability to units.\nIn this paper we employ the Rubin-Neyman potential outcomes framework [27]: We assume there exist two deterministic functions which are the true, unknown labeling functions Yt : X → Y , for the two treatment assignments t = 0, 1. For a labeled sample (x1, t1, yF1 ), . . . (xn, tn, y F n ) ⊂ X × {0, 1} × Y from the factual distribution, we have that the factual labels are yFi = tiY1(xi) + (1− ti)Y0(xi). Throughout this paper we will discuss representation functions of the form Φ : X → R, whereR is the representation space. We make the following assumption about Φ:\nAssumption 1. The representation Φ is a twice-differentiable, one-to-one function. Without loss of generality we will assume thatR is the image of X under Φ.\nDefinition 1. Define Ψ : R → X to be the inverse of Φ, such that Ψ(Φ(x)) = x for all x ∈ X .\nThe representation Φ pushes forward the treated and control distributions into the new spaceR: we denote the induced distribution by pFΦ , defined overR× {0, 1}. Define pCFΦ analogously. We also define pt=1Φ (r) := p F Φ(r|t = 1), pt=0Φ (r) := pFΦ(r|t = 0), to be the treated and control distributions induced overR. For a one-to-one Φ, the distributions pFΦ and pCFΦ can be obtained by the standard change of variables formula, using the determinant of the Jacobian of Ψ(r).\nLet L : Y × Y → R+ be a loss function.\nDefinition 2. Let h : R × {0, 1} → R be an hypothesis defined over the representation space R. The expected factual loss and counterfactual losses of h and Φ are, respectively:\n`F (h,Φ) = ∫ X×{0,1} L (Yt(x), h(Φ(x), t)) p F (x, t) dxdt = ∫ R×{0,1} !L (Yt(Ψ(r)), h(r, t)) p F Φ(r, t) drdt,\n`CF (h,Φ) = ∫ X×{0,1} !L (Yt(x), h(Φ(x), t)) p CF (x, t) dxdt = ∫ R×{0,1} L (Yt(Ψ(r)), h(r, t)) p CF Φ (r, t) drdt,\nwhere the equalities follow from change of variables in integration and the definition of pFΦ(r, t), pCFΦ (r, t). As noted above, the term `F is simply the generalization error for the hypothesis h(Φ, t) over the factual distribution. The term `CF is the generalization of the same hypothesis over the counterfactual distribution.\nOur proof relies on the notion of an Integral Probability Metric, which is a family of metrics between probability distributions [30, 24]. For two probability density functions p, q defined over S ⊆ Rd, and for a function family F of functions g : S → R, we have that\nIPMF(p, q) := sup g∈F ∣∣∣∣∫ S g(s)(p(s)− q(s)) ds ∣∣∣∣ . Integral probability metrics are always symmetric and obey the triangle inequality, and trivially satisfy IPMF(p, p) = 0. For rich enough function families F, we also have that IPMF(p, q) = 0 =⇒ p = q, and then IPMF is a true metric. Examples of function families F for which IPMF is a true metric are the family of all bounded countinuous functions, the family of all 1-Lipschitz functions [30], and the unit-ball of functions in a universal reprodcuing Hilbert kernel space [15]. Here, we will employ an extended notion of IPM, where the probabilities are not necessarily normalized. We call this Unnormalized Integral Probability Metric. For two probability distribution functions as above, and positive scalars up, uq , we have:\nUIPMF(upp, uqq) := sup g∈F ∣∣∣∣∫ S g(s) (upp(s)− uqq(s)) ds ∣∣∣∣ ."
    }, {
      "heading" : "2.2 Bounds",
      "text" : "Assumption 2. Let u = pF (t = 1) be the marginal probability of treatment, and assume 0 < u < 1. Theorem 1 (General bound). Let F be a family of functions f : R → R, and UIPMF(·, ·) the unnormalized integral probability metric induced by F. Let Y0, Y1 be the true labeling functions. Let h : R× {0, 1} → Y be an hypothesis. Let Φ : X → R be a one-to-one representation function, with inverse Ψ. Assume that for t = 0, 1, we have that fΦ,h(r) := L(Yt(Ψ(r)), h(r, t)) ∈ F. Then:\n`CF (h,Φ) ≤ `F (h,Φ) + 2UIPM ( u · pt=1Φ , (1− u) · pt=0Φ ) . (1)\nThe upper bound in Eq (1) is itself a sum of two expectation terms. For an empirical sample from pF (x, t), and a family of representations and hypotheses, we can further upper bound `F by the empirical loss and a model complexity term using standard arguments [28]. The UIPM term can be consistently estimated from finite samples for the function families we use below [30].\nChoosing a small function family F will make the bound tighter.However, choosing too small a family could result in an imcomputable bound. For example, for F = {fΦ,h} we will have that IPMF = `CF (Φ, h)− `F (Φ, h), which we cannot compute since we do not know the counterfactual outcomes. In addition, for some function families there is no known way to efficiently compute the IPM distance or to take its gradients. In this paper we employ two function families for which there are available optimization tools. The first is the family of 1-Lipschitz functions, which leads to IPM being the Wasserstein distance [32, 30], denoted Wass(p, q). The second is the family of norm-1 reproducing kernel Hilbert space (RKHS) functions, leading to the MMD metric [15, 30], denoted MMD(p, q). Both the Wasserstein and MMD metrics have efficient algorithms which are consistent and can be applied in the finite sample case [30], and have been used for various machine learning tasks in recent years [16, 15, 8, 9]. In supplement C we show how to reduce the calculation of unnormalized Wasserstein and MMD to calculating standard Wasserstein and MMD distances.\nThe Wasserstein metric Assumption 3. For the loss L, for all y ∈ Y , both L(·, y) and L(y, ·) are Lipschitz functions with Lipschitz constant upper bounded by KL.\nExamples of loss function which follow Assumption 3 are the absolute loss |y1− y2|, the logistic loss for binary labels log(1 + e−y1y2), and the squared loss (y1 − y2)2 if Y is bounded. We now employ a measure bounding the degree to which Φ is information-preserving. For this we use the reciprocal condition number of the Jacobian matrix of Φ.\nDefinition 3. Let ∂Φ(x)∂x be the Jacobian matrix of Φ at point x, i.e. the matrix of the partial derivatives of Φ. Let σmax(A) and σmin(A) denote respectively the largest and smallest singular\nvalues of the matrix A. Define ρ(Φ) = supx∈X σmax ( ∂Φ(x) ∂x ) /σmin ( ∂Φ(x) ∂x ) .\nIt is an immediate result that ρ(Φ) ≥ 1. We will call a representation function Φ : X → R Jacobiannormalized if supx∈X σmax ( ∂Φ(x) ∂x ) = 1. Note that any non-constant representation function Φ can\nbe Jacobian-normalized by scaling it with 1/ supx∈X σmax ( ∂Φ(x) ∂x ) .\nTheorem 2 (Wasserstein bound). Let Φ be a one-to-one, Jacobian-normalized representation function. Let K be the Lipschitz constant of the functions Y0, Y1 on X . Let KL be the Lipschitz constant of the loss function L. Let h : R× {0, 1} → R be an hypothesis with Lipschitz constant bK. Then:\n`CF (h,Φ) ≤ `F (h,Φ) + 2 (ρ(Φ) + b) ·K ·KL ·Wass(u · pt=1Φ , (1− u) · pt=0Φ ). (2)\nWe examine the constant (ρ(Φ) + b) ·K in Theorem 2. K, the Lipschitz constant of Y0 and Y1, is not under our control and measures an aspect of the complexity of the true underlying functions we wish to approximate. The term KL depends on our choice of loss function. The term b comes from our assumption that the hypothesis h has norm bK. Note that smaller b, while reducing the bound, might force the factual loss term `F (h,Φ) to be larger since a small b implies a less flexible h. Finally, consider the term ρ(Φ). For specific families of representation Φ, ρ(Φ) can be upper bounded. For example, if Φ is a single-layer neural net with weight matrix W and tanh non-linearity, then one can show that if the norm and the inverse condition number of W are bounded, then so is ρ(Φ).\nThe MMD metric We prove a Theorem similar to 2, for the case of Reproducing Kernel Hilbert Space function spaces, with the corresponding Maximum Mean Discrepancy metric. For lack of space we state the results in the supplement. Unlike in Theorem 2, the MMD case has a constant which is harder to explicitly bound; we defer a deeper look into this issue to future work.\nThe above Theorems hold for any given h and Φ obeying the theorems’ conditions. Therefore, we can attempt to minimize the upper bounds in Eqs. (1) and (2) with respect to Φ and h, as we suggest in Algorithm 1, in order to minimize the counterfactual loss.\nProof idea The full proof of the Theorems and Lemmas above is given in the supplement. The main idea is bounding the difference `CF − `F in terms of an IPM between the counterfactual and factual distributions. Once we have that, we have that\n`CF = `F + (`CF − `F ) ≤ `F + IPMF(PCF , pF ).\nWe then show that we can in fact calculate IPMF(PCF , pF ) in terms of an IPM between the treatment and control distributions IPMF(pt=1, pt=0).\nCounterfactual loss and treatment effect estimation In section B of the supplement, we show how the counterfactual loss relates to the error in estimating the true treatment effect Y1(x)− Y0(x). We show that in the transductive setting where one of the potential outcomes is known, minimizing counterfactual loss is equivalent to minimizing the error in treatment effect estimation. For the inductive setting we show that the error can be bounded by the sum of the factual and counterfactual errors. Putting this together with our bounds above, we see that the error in estimating treatment effect is also upper bounded by a weighted sum of the factual error and an IPM distance term, justifying our theoretical and empirical approach.\nAlgorithm 1 Counterfactual balanced regression with integral probability metrics\n1: Input: Factual sample (x1, t1, yF1 ), . . . , (xn, tn, yFn ), scaling parameter α > 0, loss function L (·, ·), representation network ΦW with initial weights by W, outcome network hV with intial weights V, function family F for IPM loss 2: while not converged do 3: Sample m control {(xij , 0, yFij )} m j=1 and m ′ treated units {(xik , 1, yFik)} m+m′ k=m 4: Calculate the gradient of the imbalance penalty: g1 = ∇W IPMF ( {ΦW(xij )}mj=1, {ΦW(xik)}m ′ k=m+1\n) 5: Calculate the gradients of the empirical loss:\ng2 = ∇V ∑ j L ( hV(ΦW(xij ),tij ),y F ij ) m+m′ , g3 = ∇W 1 m+m′ ∑ j L ( hV(ΦW(xij ),tij ),y F ij ) m+m′\n6: Obtain step size scalar or matrix η with standard neural net methods e.g. RMSProp 7: update W←W − η(αg1 + g3) and V← V − η(g2) 8: check convergence criterion 9: end while"
    }, {
      "heading" : "3 Our approach",
      "text" : "We propose a general framework for counterfactual estimation, based on the theoretical results of Section 2. Our algorithm is a single regularized minimization procedure which simultaneously fits both a balanced representation of the data, and a hypothesis for the outcome. This is in contrast to [20] who proposed a two-step procedure corresponding to their theoretical results based on the discrepancy distance [7]. We note that our framework is also more flexible in practice, as our theory supports multiple measures of balance that can can be minimized efficiently; this is only rarely true for variants of the discrepancy distance We minimize the following objective.\nmin Φ,h\n1\nn n∑ i=1 L (h(Φ(xi), ti) , yi) + α · IPMF ({Φ(xi)}i:ti=0, {Φ(xi)}i:ti=1) (3)\nHere, IPMF(·, ·) is the (empirical) integral probability metric defined by the function family F. In Section 2, we show that under certain conditions when IPMF is the maximum mean discrepancy or the Wasserstein distance, (3) is an upper bound on the counterfactual error.\nIn this work, we let Φ(x) and h(Φ, t) be parameterized by a single neural network. This means that we can learn rich, non-linear representations and hypotheses with large flexibility. In [20], the authors considered using a linear variable selection model, and while our model allows for a sparse diagonal first layer which generalizes their framework, in practice, we have observed no gain from the variable selection. Our approach, in the neural network parameterization, is visualized in Figure 1. We train our models by minimizing (3) using stochastic gradient descent, as described in Algorithm 1. The details of how to obtain the gradient g1 with respect to the IPM can be found in the supplement."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate our framework in counterfactual regression and classification tasks, using different functions to measure imbalance, including the Wasserstein distance and the MMD, and compare to established methods. We report the absolute error or bias in estimating the average and individual treatment effects, ATE and ITE , as well as the Precision in Estimation of Heterogeneous Effect\n(PEHE)[19], PEHE = √\n1 n ∑n i=1[(Y1(xi)− Y0(xi))− (g(xi, 1)− g(xi, 0))]2, where g(x, t) is the\npredicted outcome for individual x under treatment t. We also report the RMSE of the predicted factual outcome (RMSEfact) or the binary classification error (Errfact). Standard methods for hyperparameter selection such as cross-validation are not applicable, as there are no samples of the counterfactual outcome. In simulated experiments, counterfactuals are available and we follow [20] by fitting hyperparameters on held-out set of repeated experiments. In experiments with real outcome, we use a surrogate for the counterfactual outcome to estimate the error, namely yFj(i) – the factual outcome of the nearest neighbor j(i) to i, that is in the opposite treatment group.\nOur approach is implemented as a feed-forward neural network with fully-connected ReLU layers, trained using RMSProp, with a small l2 weight decay, λ = 10−3. Two architectures are evaluated. CFR-4-0 consists of 4 ReLU layers used for representation, and a single linear output layer. Following the notation of Fiugre 1, dr = 4, do = 0. CFR-2-2 consists of 2 ReLU representation layers, 2 ReLU layers after the treatment has been added, and a single linear output layer, dr = 2, do = 2, see Figure 1. For the IHDP data we use layers of 25 hidden units each. For Jobs, layers have 50 units. These architectures were selected based on factual validation error alone, by splitting the data.\nIn regression tasks, we compare our method to Ordinary Least Squares (OLS), Doubly Robust Regression (DR) [1], Bayesian Additive Regression Trees2 (BART) [5], Causal Forests3 (C.Forests) [33] as well as the Balancing Linear Regression (BLR) and Balancing Neural Network (BNN) methods proposed by [20]. For DR, we estimate propensity scores using logistic regression. We also compare to a variable selection procedure dubbed LASSO + Ridge (L+R) in which a ridge regression model is fit to the variables selected by LASSO. In classification tasks we compare to Logistic Regression (LR), `1-regularized Logistic Regression (`1-LR) instead of OLS and L+R."
    }, {
      "heading" : "4.1 Simulated outcome - IHDP",
      "text" : "Hill [19] compiled a semi-simulated dataset for counterfactual inference based on the Infant Health and Development Program (IHDP), in which the outcome is simulated. The covariates stem from a randomized experiment studying the effects of child care and home visits on (future) cognitive test scores. Imbalance in the covariates has been artificially introduced by removing a biased subset of the treatment population. The dataset comprises 747 observations (139 treated, 608 control) and 25 covariates measuring aspects of children and their mothers, see Hill (2011) [19]. We use the log-linear outcome model implemented as setting “B” in the NPCI package4. We also evaluate our method on another semi-synthetic dataset called News [20], the results on which are in the supplement.\nThe results of the experiments on IHDP are presented in Table 1. We see that in general, non-linear estimators such as BART, CFR, BNN and Causal Forests fair the best on both datasets. On the whole, using neural networks, even without balancing, give very good results. For a comparison of different distance measures, the MMD and its square (MMD2) with linear and Gaussian-RBF kernels, the Wasserstein distance (Wass), and the linear discrepancy (LinDisc), see Figure 2. We see that MMD2 (lin) performs the best, and the Wasserstein and MMD (rbf) the worst, although this might improve by chosing the variance parameter σ.\nWe investigate the effects of varying imbalance by constructing subsampled variants of the IHDP dataset. First, we fit a propensity score model, to form estimates p̂(t) of the treatment probability p(t). Then, repeatedly with probability q we remove the remaining control observation that has propensity score p̂(t) closest to 1 and with probability 1 − q, we remove a control observation uniformly at random. For high values of q, this gives a more imbalanced dataset, and for small q a more balanced. In our experiments, we consider q = 0, 0.5, 1.0. In total, we remove m = 347 observations from\n2https://cran.r-project.org/web/packages/BayesTree 3https://github.com/susanathey/causalTree/tree/forestCode 4https://github.com/vdorie/npci\neach set, leaving 400. In Figure 2, we see that as imbalance (q) increases, the relative gain from using our method, and the penalty needed, increase as well. This is expected as for data with no overlap at all, the best you can do is to disregard the covariates completely, corresponding to α =∞."
    }, {
      "heading" : "4.2 Real-world outcome - Jobs",
      "text" : "LaLonde [22] conducted a well-known observational study based on the National Supported Work (NSW) program. It combines a randomized study based on the NSW with observational data to form a larger, observational dataset [29]. We refer to this dataset as Jobs. The original outcome to predict is the 1978 earnings and the 8 original covariates include age, education, ethnicity, as well as earnings in 1974 and 1975. To evaluate our framework for classification, we construct an alternative binary task of predicting unemployment in the Jobs study, i.e. the event that y = 0. We use the augmented feature set of Dehejia & Wahba [10]. In the notation of Smith et al. [29], we use the LaLonde experimental sample (297 treated, 425 control) and the PSID comparison group (2490 control). We can compute the average treatment effect on the treated (ATT = $886), as all treated were part of the original randomized experiment. In total, there were 482 (15%) people unemployed by the end of the study. We select parameters based on the PEHEnn criteria defined above.\nThe results of the binary Jobs experiments are presented in Table 1. The results for the original, continuous task is in the supplement. On the whole, our proposed methods CFR-2-2 and CFR-4-0 perform well. The results only show error in the average effect, something that even linear methods can estimate well. However, since we achieve comparable average effect error, as well as better individual factual error, we expect that the counterfactual error is lower for our method."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper we give a meaningful and intuitive bound for the problem of learning representations for counterfactual inference. Our bound relates counterfactual inference to the classic machine learning problem of learning from finite samples, along with methods for measuring distributional distances from finite samples. The bound lends itself naturally to the creation of learning algorithms; we focus on using neural nets as representations and hypotheses. We apply our theory-guided approach leads to both synthetic and real-world tasks, showing that in every case our method matches or outperforms the most recent methods proposed for these tasks. Important open questions are theoretical considerations in choosing the imbalance weight α, and how to best derive confidence intervals for our model’s predictions, as well as exploring in more depths the connection to domain adaptation problems."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We wish to thank Esteban Tabak and Marco Cuturi for fruitful conversations. We also wish to thank Stefan Wager for his help with the code for Causal Forests. DS and US were supported by NSF CAREER award #1350965."
    }, {
      "heading" : "A Proofs",
      "text" : "A.1 Definitions, assumptions, and auxiliary lemmas\nNotation: pF (x, t), pCF (x, t): factual and counterfactual distributions on X × {0, 1} u = pF (t = 1): the marginal probability of treatment. pt=1(x) = pF (x|t = 1): treated distribution. pt=0(x) = pF (x|t = 0): control distribution. Φ: representation function mapping from X toR. Ψ: the inverse function of Φ, mapping fromR to X . pFΦ(r, t), p CF Φ (r, t): factual and counterfactual distributions induced by Φ onR× {0, 1}. pt=1Φ (r), p t=0 Φ (r): treated and control distributions induced by Φ onR. Y0(x), Y1(x): true labeling functions for t = 0, t = 1. L(·, ·): loss function, from Y × Y to R+. `F (h,Φ), `CF (h,Φ): expected loss of h(Φ(x), t) with respect to the factual, counterfactual distributions. IPMF(p, q): the integral probability metric distance induced by function family F between distributions p and q. UIPMF(up · p, uq · q): the unnormalized integral probability metric distance induced by function family F between distributions p and q scaled respectively by up, uq ∈ R+\nWe first define the necessary distributions and prove some simple results about them. Definition 4. Let the factual and counterfactual distributions pF (x, t) and pCF (x, t) be distributions over X × {0, 1} such that pF (t|x) = pCF (1− t|x), where X ⊂ Rd. Assumption 4. The marginal distribution 0 < pF (t = 1) < 1. Assumption 5. The marginal distribution of the covariates x is equal for the factual and counterfactual distributions: pF (x) = pCF (x).\nDefinition 5. Let pt=1(x) ≡ pF (x|t = 1), and pt=0(x) ≡ pF (x|t = 0) denote respectively the treatment and control distributions.\nLet Φ : X → R be a representation function. We will assume that Φ is differentiable.\nAssumption 6. The representation function Φ is one-to-one. Without loss of generality we will assume thatR is the image of X under Φ, and define Ψ : R → X to be the inverse of Φ, such that Ψ(Φ(x)) = x for all x ∈ X . Definition 6. For a representation function Φ : X → R, and for a distribution pF defined over X , let pFΦ be the distribution induced by Φ over R. Define pCFΦ analogously. Also define pt=1Φ (r) ≡ pFΦ(r|t = 1), pt=0Φ (r) ≡ pFΦ(r|t = 0), to be the treatment and control distributions induced overR.\nFor a one-to-one Φ, the distributions pFΦ and p CF Φ can be obtained by the standard change of variables formula, using the determinant of the Jacobian of Ψ(r). See [3] for the case of a mapping Φ between spaces of different dimensions. Lemma 1. pFΦ(r) = pCFΦ (r)\nProof. Let JΨ(r) be the absolute of the determinant of the Jacobian of Ψ(r). Then by the change of variable formula we have:\npFΦ(r) = JΨ(r) · pF (Ψ(r)) = JΨ(r) · pCF (Ψ(r)) = pCFΦ (r), since by Assumption 5 pF (Ψ(r)) = pCF (Ψ(r)).\nLemma 2. For all r ∈ R, t ∈ {0, 1}:\npFΦ(t|r) = pF (t|Ψ(r)), pCFΦ (t|r) = pCF (t|Ψ(r)\nProof. Let JΨ(r) be the absolute of the determinant of the Jacobian of Ψ(r).\npFΦ(t|r) = pFΦ(t, r)\npFΦ(r)\n(a) =\npF (t,Ψ(r))JΨ(r) pF (Ψ(r))JΨ(r) = pF (t,Ψ(r)) pF (Ψ(r)) = pF (t|Ψ(r)),\nwhere equality (a) is by the change of variable formula. The proof is identical for pCFΦ .\nLemma 3. For all r ∈ R: pCFΦ (r, t) = p F Φ(r|1− t)pFΦ(1− t)\nProof. pCFΦ (r, t) = p CF Φ (r)p CF Φ (t|r) = pFΦ(r)pFΦ(1− t|r), since by Lemma 1 we have pCFΦ (r) = p F Φ(r), and by Definition 4 and Lemma 2\nLet L : Y × YR+ be a loss function, e.g. the logistic loss or squared loss. Definition 7. Let Φ : X → R be a representation function. Let h : R×{0, 1} → Y be an hypothesis defined over the representation spaceR. Let Yt : X → R be the true labeling functions, for t = 0, 1. The expected factual loss and counterfactual losses of h and Φ are, respectively:\n`F (h,Φ) = ∫ R×{0,1} L (Yt(Ψ(r)), h(r, t) ) p F Φ(r, t) drdt\n`CF (h,Φ) = ∫ R×{0,1} L (Yt(Ψ(r)), h(r, t) ) p CF Φ (r, t) drdt,\nwhere Ψ is the inverse function of Φ. Definition 8. Let F be a function family consisting of functions g : S → R. For a pair of distributions p1, p2 over S, define the Integral Probability Metric:\nIPMF(p1, p2) = sup g∈F ∣∣∣∣∫ S g(s) (p1(s)− p2(s)) ds ∣∣∣∣ IPMF(·, ·) defines a pseudo-metric on the space of probability functions over S , and for sufficiently large function families, IPMF(·, ·) is a proper metric [24]. Examples of sufficiently large functions families includes the set of bounded continuous functions, the set of 1-Lipschitz functions, and the set of unit norm functions in a universal Reproducing Norm Hilbert Space. The latter two give rise to the Wasserstein and Maximum Mean Discrepancy metrics, respectively [15, 30]. We note that for function families F such as the three mentioned above, for which g ∈ F =⇒ −g ∈ F, the absolute value can be omitted from definition 8.\nDefinition 9. Let F be a function family consisting of functions g : S → R. For a pair of distributions p1, p2 over S, and a pair of positive scalar u1, u2, define the Unnormalized Integral Probability Metric:\nUIPMF(u1p1, u2p2) = sup g∈F ∣∣∣∣∫ S g(s) (u1p1(s)− u2p2(s)) ds ∣∣∣∣ UIPMF can be considered as a distance function between positive measures on S , see also [18, 14, 6].\nLemma 4. For constant v ≥ 0, we have that UIPMF(v · u1p1, v · u2p2) = v · UIPMF(u1p1, u2p2). In particular, UIPMF(vp1, vp2) = v · IPMF(p1, p2).\nThe proof is immediate from Definitions 8 and 9.\nA.2 General IPM bound\nLemma 5. Let u = pF (t = 1). For a function g : R× {0, 1} → R and the distributions pCFΦ , pFΦ:∫ R×{0,1} g(r, t) ( pCFΦ (r, t)− pFΦ(r, t) ) drdt =∫\nR g(r, 0)\n( u · pt=1Φ (r)− (1− u) · pt=0Φ (r) ) dr + ∫ R g(r, 1) ( (1− u) · pt=0Φ (r)− u · pt=1Φ (r) ) dr\nProof.∫ R×{0,1} g(r, t) ( pCFΦ (r, t)− pFΦ(r, t) ) drdt =∫\nR g(r, 0)\n( pCFΦ (r, 0)− pFΦ(r, 0) ) dr + ∫ R g(r, 1) ( pCFΦ (r, 1)− pFΦ(r, 1) ) dr (a) = (4)∫\nR g(r, 0)\n( u · pFΦ(r|t = 1)− (1− u) · pFΦ(r|t = 0) ) dr+∫\nR g(r, 1)\n( (1− u) · pFΦ(r|t = 0)− u · pFΦ(r|t = 1) ) dr =∫\nR g(r, 0)\n( u · pt=1Φ (r)− (1− u) · pt=0Φ (r) ) dr + ∫ R g(r, 1) ( (1− u) · pt=0Φ (r)− u · pt=1Φ (r) ) dr,\nwhere equality (a) is due to Lemma 3 and since pFΦ(t) = p F (t).\nWe prove a very slightly more general version of Theorem 1 from the main paper. The only difference being that we allow a scaled version of L(Yt(Ψ(r)), h(r, t)) to be in the function family F. This modification simplifies the following proofs.\nTheorem 3. Let pt=1Φ , pt=0Φ be defined as in Definition 6. Let u = pF (t = 1). Let F be a family of functions f : R → R, and denote by UIPMF(·, ·) the unnormalized integral probability metric induced by F. Let Y0, Y1 be the true labeling functions. Let h : R× {0, 1} → Y be an hypothesis. Let Φ : X → R be a one-to-one representation function, with inverse Ψ. Assume there exists a constant B > 0, such that for t = 0, 1, we have that fΦ,h(r) := 1B · L(Yt(Ψ(r)), h(r, t)) ∈ F. Then we have:\n`CF (h,Φ) ≤ `F (h,Φ) + 2B · UIPM ( u · pt=1Φ , (1− u) · pt=0Φ ) . (5)\nProof.\n`CF (h,Φ) =\n`F (h,Φ) + (`CF (h,Φ)− `F (h,Φ)) =\n`F (h,Φ) + ∫ R×{0,1} L (Yt(Ψ(r)), h(r, t) ) ( pCFΦ (r, t)− pFΦ(r, t) ) drdt = (6)\n`F (h,Φ) + ∫ R L (Y0(Ψ(r)), h(r, 0) ) ( u · pt=1Φ (r)− (1− u) · pt=0Φ (r) ) dr\n+ ∫ R L (Y1(Ψ(r)), h(r, 1) ) ( (1− u) · pt=0Φ (r)− u · pt=1Φ (r) ) dr =\n`F (h,Φ) +B · ∫ R 1 B L (Y0(Ψ(r)), h(r, 0) ) ( u · pt=1Φ (r)− (1− u) · pt=0Φ (r) ) dr\n+B · ∫ R 1 B L (Y1(Ψ(r)), h(r, 1) ) ( (1− u) · pt=0Φ (r)− u · pt=1Φ (r) ) dr ≤ (7)\n`F (h,Φ) +B · sup g∈F ∣∣∣∣∫ R g(r) ( u · pt=1Φ (r)− (1− u) · pt=0Φ (r) ) dr ∣∣∣∣ +B · sup\ng∈F ∣∣∣∣∫ R g(r) ( (1− u) · pt=0Φ (r)− u · pt=1Φ (r) ) dr ∣∣∣∣ = `F (h,Φ) + 2B · UIPM ( u · pt=1Φ , (1− u) · pt=0Φ ) ,\nwhere equality (6) is by Lemma 5, and inequality (7) is by the assumption that L(Yt(Ψ(r)), h(r, t)) ∈ F for t = 0, 1.\nWe also have an immediate corollary: Corollary 1. Under the conditions of Theorem 3 above, if pF (t = 1) = 12 , then:\n`CF (h,Φ) ≤ `F (h,Φ) + IPM ( pt=1Φ , p t=0 Φ ) . (8)\nProof. The result follows using Lemma 4.\nThe essential point in the proof of Theorem 3 is inequality 7. Note that on the l.h.s. of the inequality, we need to evaluate the expectations of L (Y0(Ψ(r)), h(r, 0) ) over pt=1 and L (Y1(Ψ(r)), h(r, 1) ) over pt=0. Both of these expectations are in general unavailable, since they require us to evaluate treatment outcomes on the control, and control outcomes on the treated. We therefore upper bound these unknowable quantities by taking a supremum over a function family which includes L (Y0(Ψ(r)), h(r, 0) ) and L (Y1(Ψ(r)), h(r, 1) ). The upper bound ignores the outcome information, and amounts to measuring a distance between two distributions we have samples from: the control and treated distribution. Note that for a randomized trial with p(t = 1) = 12 , we have that IPM(pt=1Φ , p t=0 Φ ) = 0, and indeed in that case `CF (h,Φ) = `F (h,Φ).\nThe crucial condition in Theorem 3 is that the function fΦ,h(r) := L (Yt(Ψ(r)), h(r, t) ) is in F. We now look into two specific function families F, and evaluate what does this inclusion condition entail.\nA.3 The family of 1-Lipschitz functions\nFor S ⊂ Rd, a function f : S → R has Lipschitz constant K if for all x, y ∈ S, |f(x) − f(y)| ≤ K‖x− y‖. If f is differentiable, then a sufficient condition for K-Lipschitz constant is if ‖∂f∂s ‖ ≤ K for all s ∈ S. For simplicity’s sake we assume throughout this subsection that the true labeling functions Y0, Y1 and the loss L are differentiable. However, this assumption could be relaxed to a mere Lipschitzness assumption.\nAssumption 7. There exists a constant K > 0 such that for all x ∈ X , t ∈ {0, 1}, ‖∂Yt(x)∂x ‖ ≤ K. Assumption 8. The loss function L is differentiable and there exists a constant KL > 0 such that∣∣∣dL(y1,y2)dyi ∣∣∣ ≤ KL for i = 1, 2.\nLoss functions which obey Assumption 8 include the log-loss, hinge-loss, absolute loss, and for compact Y also the squared loss. When we let F in Definition 8 be the family of 1-Lipschitz functions, we obtain the so-called 1-Wasserstein distance between distributions, which we denote Wass(·, ·). It is well known that Wass(·, ·) is indeed a metric between distributions [32]. The Wasserstein distance has been extended to unnormalized distributions (i.e. positive measures) by several authors. We use the formulation of [18], who has shown how to reduce the problem of Wasserstein distance between unnormalized distributions into the ordinary Wasserstein distance, by adding a point “at infinity”. This has been followed through by [14], and we use the latter’s algorithmic framework in the experimental section of our paper. For a much deeper mathematical treatment of the subject, we refer the reader to [6].\nDefinition 10. Let ∂Φ(x)∂x be the Jacobian matrix of Φ at point x, i.e. the matrix of the partial derivatives of Φ. Let σmax(A) and σmin(A) denote respectively the largest and smallest singular\nvalues of a matrix A. Define ρ(Φ) = supx∈X σmax ( ∂Φ(x) ∂x ) /σmin ( ∂Φ(x) ∂x ) .\nIt is an immediate result that ρ(Φ) ≥ 1. Definition 11. We will call a representation function Φ : X → R Jacobian-normalized if supx∈X σmax ( ∂Φ(x) ∂x ) = 1.\nNote that any non-constant representation function Φ can be Jacobian-normalized by a simple scalar multiplication.\nLemma 6. Assume that Φ is a Jacobian-normalized representation, and let Ψ be its inverse. Define Ỹt : R → R by Ỹt(r) ≡ Yt(Ψ(r)). The Lipschitz constant of Ỹt is bounded by ρ(Φ)K, where K is from Assumption 7, and ρ(Φ) as in Definition 10.\nProof. Let Ψ : R → X be the inverse of Φ, which exists by the assumption that Φ is one-to-one. Let ∂Φ(x) ∂x be the Jacobian matrix of Φ evaluated at x, and similarly let ∂Ψ(r) ∂r be the Jacobian matrix of Ψ evaluated at r. Note that ∂Ψ(r)∂r · ∂Φ(x) ∂x = I for r = Φ(x), since Ψ ◦ Φ is the identity function on X . Therefore for any r ∈ R and x = Ψ(r):\nσmax\n( ∂Ψ(r)\n∂r\n) =\n1\nσmin ( ∂Φ(x) ∂x ) , (9) where σmax(A) and σmin(A) are respectively the largest and smallest singular values of the matrix A, i.e. σmax(A) is the spectral norm of A.\nFor x = Ψ(r) and t ∈ {0, 1}, we have by the chain rule:\n‖∂Ỹt(r) ∂r ‖ = ‖∂Yt(Ψ(r)) ∂r ‖ = ‖∂Yt(Ψ(r)) ∂Ψ(r) ∂Ψ(r) ∂r ‖ ≤ (10)\n‖∂Ψ(r) ∂r ‖‖∂Yt(Ψ(r)) ∂Ψ(r) ‖ = (11)\n1\nσmin ( ∂Φ(x) ∂x )‖∂Yt(x) ∂x ‖ ≤ (12)\nK\nσmin ( ∂Φ(x) ∂x ) ≤ ρ(Φ)K, (13) where inequality (10) is by the matrix norm inequality, equality (11) is by (9), inequality (12) is by assumption 7 on the norms of the gradient of Yt(x) w.r.t x , and inequality (13) is by Definition 10 of ρ(Φ), the assumption that Φ is Jacobian-normalized, and noting that singular values are necessarily non-negative.\nLemma 7. Under the conditions of Theorem 3, further assume that Y0, Y1 have gradients bounded by K as in 7, that h has bounded gradient norm bK, that the loss L has bounded gradient norm KL, and that Φ is Jacobian-normalized. Then the Lipschitz constant of L (Yt(Ψ(r)), h(r, t) ) is upper bounded by KL ·K (ρ(Φ) + b) for t = 0, 1.\nProof. Using the chain rule, we have that:\n∂L ∂r =\n∂L\n∂Yt(Ψ(r))\n∂Yt(Ψ(r))\n∂r +\n∂L\n∂h(r, t)\n∂h(r, t)\n∂r ≤\nKlKρ(Φ) +Kl · bK = KL ·K (ρ(Φ) + b) ,\nwhere the inequality is due to Lemma 6.\nTheorem 4. Let u = pF (t = 1) be the marginal probability of treatment, and assume 0 < u < 1. Let Φ : X → R be a one-to-one, Jacobian-normalized representation function. Let K be the Lipschitz constant of the functions Y0, Y1 on X . Let KL be the Lipschitz constant of the loss function L. Let h : R× {0, 1} → R be an hypothesis with Lipschitz constant bK. Then:\n`CF (h,Φ) ≤ `F (h,Φ) + 2 (ρ(Φ) + b) ·K ·KL ·Wass(u · pt=1Φ , (1− u) · pt=0Φ ). (14)\nProof. We will apply Theorem 3 with F = {f : R → R s.t. f is 1-Lipschitz}. By Lemma 7, we have that for B = (ρ(Φ) + b) ·K ·KL, the function 1BL (Yt(Ψ(r)), h(r, t) ) ∈ F. Inequality (14) then holds as a special case of Theorem 3.\nThe assumption that Φ is normalized is rather natural, as we do not expect a certain scale from a representation. Furthermore, below we show that in fact the Wasserstein distance is positively homogeneous with respect to the representation Φ. Therefore, in Theorem 4, we can indeed assume that Φ is normalized. The specific choice of Jacobian-normalized scaling yields what is in our opinion a more interpretable result in terms of the inverse condition number ρ(Φ).\nLemma 8. The Wasserstein distance is positive homogeneous for scalar transformations of the underlying space. Let p, q be probability density functions defined over X . For alpha > 0 and the mapping Φ(x) = αx, let pα and qα be the distributions on αX induced by Φ. Then:\nWass (pα, qα) = αWass (p, q) .\nProof. Following [32, 21], we use another characterization of the Wasserstein distance. LetMp,q be the set of mass preserving maps from X to itself which map the distribution p to the distribution q. That is,Mp,q = {M : X → X s.t. q(M(S)) = p(S) for all measurable bounded S ⊂ X}. We then have that:\nWass(p, q) = inf M∈Mp,q ∫ X ‖M(x)− x‖p(x) dx. (15)\nIt is known that the infimum in (15) is actually achievable [32, Theorem 5.2]. Denote byM∗ : X → X the map achieving the infimum for Wass(p, q) . Define M∗α : αX → αX , by M∗α(x′) = αM∗(x ′\nα ), where x′ = αx. M∗α maps pα to qα, and we have that ‖M∗α(x′)− x′‖ = α‖M∗(x)− x‖. Therefore M∗α achieves the infimum for the pair (pα, qα), and we have that Wass (pα, qα) = αWass (p, q).\nA.4 Functions in the unit ball of a RKHS\nLetHx,Hr be a reproducing kernel Hilbert space, with corresponding kernels kx(·, ·), kr(·, ·). We have for all x ∈ X that kx(·, x) is its Hilbert space mapping, and similarly kr(·, r) for all r ∈ R. Recall that the major condition in Theorem 3 is that L (Yt(Ψ(r)), h(r, t) ) ∈ F. The function space F we use here is F = {g ∈ Hr s.t. ‖g‖Hr ≤ 1}. We will focus on the case where L is the squared loss, and we will make the following two assumptions:\nAssumption 9. There exist fY1 , fY2 ∈ Hx such that Yt(x) = 〈 fYt , kx(x, ·) 〉 Hx\n, i.e. the true labeling functions Y0, Y1 are inHx. Further assume that ‖fYt ‖Hx ≤ K.\nAssumption 10. Let Φ : X → Y be an invertible representation function, and let Ψ be its inverse. We assume there exists a bounded linear operator ΓΦ : Hr → Hx such that 〈 fYt , kx(Ψ(r), ·) 〉 Hx\n=〈 fYt ,ΓΦkr(r, ·) 〉 Hx\n. We further assume that the Hilbert-Schmidt norm (operator norm) ‖ΓΦ‖HS of ΓΦ is bounded by KΦ.\nThe two assumptions above amount to assuming that Φ can be represented as one-to-one linear map between the two Hilbert spacesHx andHr. Under Assumptions 9 and 10 about Y0, Y1, and Φ, we have that Yt(Ψ(r)) = 〈 Γ∗Φf Y t , kr(r, ·) 〉 Hr\n, where Γ∗Φ is the adjoint operator of ΓΦ [17].\nLemma 9. Let h : R×{0, 1} → R be an hypothesis, and assume that there exist fht ∈ Hr such that h(r, t) = 〈 fht , kr(r, ·) 〉 Hr\n, such that ‖fht ‖Hr ≤ b. Under Assumption 9 about Y0, Y1, we have that L (Yt(Ψ(r)), h(r, t) ) = (Yt(Ψ(r))− h(r, t))2 is in the tensor Hilbert space Hr ⊗Hr. Moreover, the norm of (Yt(Ψ(r))− h(r, t))2 inHr ⊗Hr is upper bounded by 4 ( K2ΦK 2 + b2 ) . Proof. By linearity of the Hilbert space, we have that Yt(Ψ(r))− h(r, t) = 〈 Γ∗Φf Y t , kr(r, ·) 〉 Hr −〈\nfht , kr(r, ·) 〉 Hr = 〈 Γ∗Φf Y t − fht , kr(r, ·) 〉 Hr\n. By a well known result [31, Theorem 7.25], the product (Yt(Ψ(r))− h(r, t)) · (Yt(Ψ(r))− h(r, t)) lies in the tensor product space Hr ⊗Hr, and is equal to 〈 (Γ∗Φf Y t − fht )⊗ (Γ∗ΦfYt − fht ), kr(r, ·)⊗ kr(r, ·) 〉 Hr⊗Hr\n. The norm of this function in Hr ⊗Hr is ‖Γ∗ΦfYt − fht ‖2Hr . This is the general Hilbert space version of the fact that for a vector w ∈ Rd one has that ‖ww>‖F = ‖w‖22, where ‖ · ‖F is the matrix Frobenius norm, and ‖ · ‖22 is the square of the standard Euclidean norm. Now we have that:\n‖Γ∗ΦfYt − fht ‖2Hr ≤ (16) 2‖Γ∗ΦfYt ‖2Hr + 2‖f h t ‖2Hr ≤ (17) 2‖Γ∗Φ‖2HS‖fYt ‖2Hx + 2‖f h t ‖2Hr = (18) 2‖ΓΦ‖2HS‖fYt ‖2Hx + 2‖f h t ‖2Hr leq (19) 2K2ΦK 2 + 2b2. (20)\nInequality (16) is because for any Hilbert spaceH, ‖a− b‖2H ≤ 2‖a‖2H + 2‖b‖2H. Inequality (17) is by the definition of the operator norm. Equality (18) is because the norm of the adjoint operator is equal to the norm of the original operator, where we abused the notation ‖ · ‖HS to mean both the norm of operators fromHx toHr and vice-versa. Finally, inequality (19) is by Assumptions 9 and 10, and by the premise on the norm of fht .\nTheorem 5. Let u = pF (t = 1) be the marginal probability of treatment, and assume 0 < u < 1. Let Φ : X → R be a one-to-one representation function which obeys Assumption 10 with corresponding operator ΓΦ with operator norm KΦ. Let the functions Y0, Y1 obey Assumption 9, with bounded Hilbert space norm K . Let h : R × {0, 1} → R be an hypothesis, and assume that there exist fht ∈ Hr such that h(r, t) = 〈 fht , kr(r, ·) 〉 Hr\n, such that ‖fht ‖Hr ≤ b. Assume that `F and `CF are defined with respect to L being the squared loss. Then:\n`CF (h,Φ) ≤ `F (h,Φ) + 4 ( K2ΦK 2 + b2 ) ·MMD(u · pt=1Φ , (1− u) · pt=0Φ ), (21)\nwhere `CF and `F use the squared loss.\nProof. We will apply Theorem 3 with F = f ∈ Hr ⊗Hr s.t. ‖f‖Hr⊗Hr ≤ 1. By Lemma 9, we have that for B = 2 ( K2ΦK 2 + b2 )\nand L being the squared loss, 1BL (Yt(Ψ(r)), h(r, t) ) ∈ F. Inequality (21) then holds as a special case of Theorem 3."
    }, {
      "heading" : "B Treatment effect estimation loss",
      "text" : "Definition 12. The treatment effect for unit x is:\nτ(x) = Y1(x)− Y0(x).\nWe now show that minimizing the counterfactual loss `CF is closely tied to minimizing the error in estimating the individualized treatment effect τ(x) = Y1(x)− Y0(x). We note that the statements in this subsection do not specifically rely on the representation learning aspect of our work, and could also be phrased without using the representation function Φ.\nLet g : X × {0, 1} → Y by an hypothesis. For example, we could have that g(x, t) = h(Φ(x), t) for a representation Φ and hypothesis h defined over the output of Φ. Definition 13. The inductive treatment effect estimate for unit x is:\nτ̂g(x) = g(x, 1)− g(x, 0). Definition 14. The transductive treatment effect estimate for unit xi with treatment assignment ti is:\nτ ′g(xi, ti) = { Y1(xi)− g(x1, 0) if ti = 1 g(xi, 1)− Y0(xi) if ti = 0\nDefinition 15. Let Yt : X → R be the true labeling functions for t = 0, 1. The expected Precision in Estimation of Heterogeneous Effect (PEHE) loss of g is:\n`PEHE(g) = ∫ X |τ̂g(x)− τ(x)|a p(x) dx\nThe expected Individualized Treatment Effect (ITE) loss of g is:\n`ITE(g) = ∫ X×{0,1} ∣∣τ ′g(x, t)− τ(x)∣∣a pF (x, t) dxdt where a = 1 corresponds to the absolute loss, and a = 2 corresponds to the squared loss.\nNote that the transductive treatment effect and the corresponding `ITE loss are defined with respect to the treatment assignment, whereas the inductive treatment effect and the corresponding `PEHE loss are defined irrespective of the treatment assignment.\nWe show that for the transductive case where we assume that for a unit xi either Y1(xi) or Y0(xi) is known, minimizing `CF with a squared loss is equivalent to minimizing Ex∼p̂(x) [ (τ ′(x)− τ(x))2 ] .\nFor the inductive case, we show that Ex∼p(x) [ (τ̂(x)− τ(x))2 ] is upper bounded by 2`F + 2`CF\nwhere `F and `CF are w.r.t. to the squared loss, while Ex∼p(x) [|τ̂(x)− τ(x)|] is upper bounded by `F + `CF where `F and `CF are w.r.t. to the absolute loss.\nThe intuition for both results is simple: for the transductive case, the only unknown quantity are the counterfactual outcomes, which we seek to know. For the inductive case, we incur an additional error on top of the transductive case. This error comes from inferring the unknown factual outcomes for new samples, which is the classic machine learning inductive learning scenario. Lemma 10. For an hypothesis g : X × {0, 1} → Y such that g(x, t) = h(Φ(x), t), and using the squared loss or absolute loss, we have:\n`ITE(g) = `CF (h,Φ).\nProof. `ITE(g) = (22)∫ X |(Y1(x)− g(x, 0))− (Y1(x)− Y0(x))|a pF (x, t = 1) dx\n+ ∫ X |(g(x, 1)− Y0(x))− (Y1(x)− Y0(x))|a pF (x, t = 0) dx =∫\nX |Y0(x)− g(x, 0)|a pF (x, t = 1)dx+ ∫ X |g(x, 1)− (Y1(x)|a pF (x, t = 0) dx = (23)∫\nX |Y0(Ψ(r))− h(r, 0)|a pFΦ(r, t = 1) dr + ∫ R |h(r, 1)− (Y1(Ψ(r))|a pFΦ(r, t = 0) dx =∫\nX |Y0(Ψ(r))− h(r, 0)|a pCFΦ (r, t = 0) dr + ∫ R |h(r, 1)− (Y1(Ψ(r))|a pFΦ(r, t = 0) dx,=\n`CF (h,Φ)\nAlgorithm 2 Counterfactual balanced regression with integral probability metrics\n1: Input: Factual sample (x1, t1, yF1 ), . . . , (xn, tn, yFn ), scaling parameter α > 0, loss function L (·, ·), representation network ΦW with initial weights by W, outcome network hV with intial weights V, function family F for IPM loss 2: while not converged do 3: Sample a mini-batch with m′ control and m treated units\n(xi1 , 0, y F i1 ), . . . , (xim , 0, y F im ), (xim+1 , 1, y F im+1 ), . . . , (xim+m′ , 1, y F im+m′ )\n4: Calculate the gradient of the imbalance penalty: g1 = ∇W IPMF ( {ΦW(xij )}mj=1, {ΦW(xik)}m ′ k=m+1 ) 5: Calculate the gradients of the empirical loss:\ng2 = ∇V 1m+m′ ∑m+m′ j=1 L ( hV(ΦW(xij ), tij ), y F ij ) g3 = ∇W 1m+m′ ∑m+m′ j=1 L ( hV(ΦW(xij ), tij ), y F ij\n) 6: Obtain step size scalar or matrix η with standard neural net methods e.g. RMSProp 7: update W←W − η(αg1 + g3) and V← V − η(g2) 8: check convergence criterion 9: end while\nwhere (22) is by decomposing τ ′g over t = 0, 1, (23) is by the change of variable formula and definition of pFΦ , and Ψ is the inverse function of Φ.\nLemma 11. For an hypothesis g : X × {0, 1} → Y such that g(x, t) = h(Φ(x), t), and using the loss L(y1, y2) = |y1 − y2|a for a = 1, 2, we have:\n`PEHE(g) ≤ a · (`CF (h,Φ) + `F (h,Φ)) .\nProof. For a = 1, 2:\n`PEHE(g) =∫ X |(g(x, 1)− g(x, 0))− (Y1(x)− Y0(x))|a p(x) dx =∫\nX |(g(x, 1)− Y1(x)) + (Y0(x)− g(x, 0))|a p(x) dx ≤ (24) a · ∫ X |(g(x, 1)− Y1(x))|a + |(Y0(x)− g(x, 0))|a p(x) dx = (25)\na ∫ X |(g(x, 1)− Y1(x))|a + |(Y0(x)− g(x, 0))|a pF (x, t = 0) dx\n+ a ∫ X |(g(x, 1)− Y1(x))|a + |(Y0(x)− g(x, 0))|a pF (x, t = 1) dx =\na ∫ X |g(x, 1)− Y1(x)|a + |(Y0(x)− g(x, 0))|a pCF (x, t = 1) dx∫\nX |g(x, 1)− Y1(x)|a + |(Y0(x)− g(x, 0))|a pF (x, t = 1) dx = (26)\na (`CF (h,Φ) + `F (h,Φ)) ,\nwhere (24) is by the inequality |x + y|a ≤ a(|x|a + |y|a) for a = 1, 2, (25) is because p(x) = p(x, t = 0) + p(x, t = 1), and (26) follows from the same change of variable substitution made in (23) in Lemma 10 above."
    }, {
      "heading" : "C Algorithmic details",
      "text" : "We give details about the algorithms used in our framework. First, we restate Algorithm 1.\nAlgorithm 3 Computing the stochastic gradient of the Wasserstein distance\n1: Input: Factual (x1, t1, yF1 ), . . . , (xn, tn, yFn ), representation network ΦW with current weights by W 2: Randomly sample a mini-batch with m treated and m′ control units (xi1 , 0, y F i1 ), . . . , (xim , 0, y F im ), (xim+1 , 1, y F im+1 ), . . . , (xi2m , 1, y F i2m ) 3: Calculate the m×m pairwise distance matrix between all treatment and control pairs M(ΦW): Mkl(Φ) = ‖ΦW(xik)− ΦW(xim+l)‖ 4: Calculate the approximate optimal transport matrix T ∗ using Algorithm 3 of [9], with input M(ΦW) 5: Calculate the gradient: g1 = ∇W 〈T ∗,M(ΦW)〉\nC.1 Minimizing the Wasserstein distance\nIn general, computing (and minimizing) the Wasserstein distance involves solving a linear program, which may be prohibitively expensive for many practical applications. Cuturi citecuturi2013sinkhorn showed that an approximation based on entropic regularization can be obtained through the SinkhornKnopp matrix scaling algorithm, at orders of magnitude faster speed. Dubbed Sinkhorn distances [8], the approximation is computed using a fixed-point iteration involving repeated multiplication with a kernel matrix K. We can use the algorithm of [8] in our framework. See Algorithm 3 for an overview of how to compute the gradient g1 in Algorithm 2. When computing g1, disregarding the gradient ∇WT ∗ amounts to minimizing an upper bound on the Sinkhorn transport. For non-uniform populations p(t) 6= 1/2, our method can still be applied using the unnormalized version of the Wasserstein distance, using ideas presented by [18, 14], and explored much more deeply by [6]. The Sinkhorn transport can still be used to approximate the Wasserstein distance, but with small modifications. Let λ and δ be parameters and M̃ the matrix\nM̃ = [ M δ δ 0 ] .\nThen, define an nt + 1-dimensional vector a\na = [p(t), ..., p(t), 1− p(t)]>\nand an nc + 1-dimensional vector b\nb = [1− p(t), ..., 1− p(t), p(t)]>\nwhere nt and nc are the number of treated and controls. Then, to get T ∗, apply Algorithm 3 of [9] on M̃, a and b.\nWhile our framework is agnostic to the parameterization of Φ, our experiments focus on the case where Φ is a neural network. For convenience of implementation, we may represent the fixedpoint iterations of the Sinkhorn algorithm as a recurrent neural network, where the states ut evolve according to\nut+1 = nt./(ncK(1./(u > t K) >)) .\nHere, K is a kernel matrix corresponding to a metric such as the euclidean distance, Kij = e−λ‖Φ(xi)−Φ(xj)‖2 , and nc, nt are the sizes of the control and treatment groups. In this way, we can minimize our entire objective with most of the frameworks commonly used for training neural networks, out of the box.\nC.2 Minimizing the maximum mean discrepancy\nThe MMD of treatment populations in the representation Φ, for a kernel k(·, ·) can be written as,\nMMDk({ΦW(xij )}mj=1, {ΦW(xik)}m ′ k=m+1) = (27)\n1\nm(m− 1) m∑ j=1 m∑ k=1,k 6=j k(ΦW(xij ),ΦW(xik)) (28)\n+ 2\nmm′ m∑ j=1 m+m′∑ k=m k(ΦW(xij ),ΦW(xik)) (29)\n+ 1\nm′(1−m′) m∑ j=1 m′∑ k=m,k 6=j k(ΦW(xij ),ΦW(xik)) (30)\nThe unnormalized version, where the marginal probability of treatment u 6= 1/2, is\nMMDk({ΦW(xij )}mj=1, {ΦW(xik)}m ′ k=m+1) = (31)\n2p(t)2\nm(m− 1) m∑ j=1 m∑ k=1,k 6=j k(ΦW(xij ),ΦW(xik)) (32)\n+ 4p(t)(1− p(t))\nmm′\nm∑ j=1 m+m′∑ k=m k(ΦW(xij ),ΦW(xik)) (33)\n+ 2(1− p(t))2\nm′(1−m′) m∑ j=1 m′∑ k=m,k 6=j k(ΦW(xij ),ΦW(xik)) (34)\nThe (unnormalized) linear maximum-mean discrepancy can be written as a distance between means. In the notation of Algorithm 2,\nMMD = 2 ∥∥∥∥∥∥ 1m m∑ j=1 p(t)ΦW(xij )− (1− p(t)) 1 m′ m′∑ k=m+1 ΦW(xik) ∥∥∥∥∥∥ 2\nLet\nf(W) = p(t) 1\nm m∑ j=1 ΦW(xij )− (1− p(t)) 1 m′ m′∑ k=m+1 ΦW(xik)\nThen the gradient of the MMD with respect to W is,\ng1 = 2 df(W)\ndW\nf(W)\n‖f(W)‖2 ."
    }, {
      "heading" : "D Empirical results",
      "text" : "D.1 General observation\nWhile not visible in the tables due to averaging, for some of the 1000 realizations of the IHDP dataset the imbalance penalty does not have an effect (neither positive nor negative). We believe this is due to these sets already being fairly balanced. We also note that in some cases, methods with linear hypothesis in the treatment, perform the best by removing any influence of the covariates on the model. This is indeed what happens for L+R and BLR on IHDP.\nD.2 IHDP and News\nJohansson et al. [20] introduced another semi-simulated dataset dubbed News, based on topicmodeling of a collection of news articles. Covariates represent counts of words in a pre-specified\nvocabulary representing words that are significant to at least one of the topics. The outcome and treatment models are based on the topic-distribution of documents. The set comprises 5000 observations (articles) with 3477 covariates (words). The results of both the IHDP and News experiments are presented in Table 2. For the News data, we train representation layers with 400 units and output layers with 200 units. For DR on News, we perform the logistic regression on the first 100 principal components of the data.\nOn News, we don’t see a significant gain from using the proposed imbalance penalty (compare e.g. MMD and α = 0), but neither does the penalty hurt the performance. A possible explanation is that the dataset is not very imbalanced to begin with.\nD.3 IHDP - Increasing imbalance\nT-SNE visualizations of the subsampled IHDP for q = 0 and q = 1 respectively can be seen in Figure 3.\nThe results of both the continuous and binary task are presented in Table 3.\nFigure 4 shows the bias in the estimated ATE for our CFR-4-0 method, OLS, and a naive estimator. We can see that when we don’t impose balance penalties, our neural network method is slightly better than OLS, in terms of estimating the ATE. This is expected as we are free to use non-linear interactions between the covariates, but still restricted to hypotheses that are linear functions of the treatment variable. As we impose balance penalties, increasing α, the performance improves until it dominates the behavior and the method reduces to the naive estimator. This is also expected as the model disregards the covariates completely when α→∞."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "There is intense interest in applying machine learning methods to problems of<lb>causal inference which arise in applications such as healthcare, economic policy,<lb>and education. In this paper we use the counterfactual inference approach to causal<lb>inference, and propose new theoretical results and new algorithms for performing<lb>counterfactual inference. Building on an idea recently proposed by Johansson et al.<lb>[20], our results and methods rely on learning so-called “balanced” representations:<lb>representations that are similar between the factual and counterfactual distributions.<lb>We give a novel, simple and intuitive bound, showing that the expected counter-<lb>factual error of a representation is bounded by a sum of the factual error of that<lb>representation and the distance between the factual and counterfactual distributions<lb>induced by the representation. We use Integral Probability Metrics to measure<lb>distances between distributions, and focus on two special cases: the Wasserstein<lb>distance and the Maximum Mean Discrepancy (MMD) distance. Our bound leads<lb>directly to new algorithms, which are simpler and easier to employ compared to<lb>those suggested in [20]. Experiments on real and simulated data show the new<lb>algorithms match or outperform state-of-the-art methods.",
    "creator" : "LaTeX with hyperref package"
  }
}