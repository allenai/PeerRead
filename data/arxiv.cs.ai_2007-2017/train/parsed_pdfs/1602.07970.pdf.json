{
  "name" : "1602.07970.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Causal Discovery from Subsampled Time Series Data by Constraint Optimization",
    "authors" : [ "Antti Hyttinen", "Sergey Plis", "Matti Järvisalo", "Frederick Eberhardt", "David Danks" ],
    "emails" : [ "ANTTI.HYTTINEN@HELSINKI.FI", "S.M.PLIS@GMAIL.COM", "MATTI.JARVISALO@CS.HELSINKI.FI", "FDE@CALTECH.EDU", "DDANKS@CMU.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper focuses on causal structure estimation from time series data in which measurements are obtained at a coarser timescale than the causal timescale of the underlying system. Previous work has shown that such subsampling can lead to significant errors about the system’s causal structure if not properly taken into account. In this paper, we first consider the search for the system timescale causal structures that correspond to a given measurement timescale structure. We provide a constraint satisfaction procedure whose computational performance is several orders of magnitude better than previous approaches. We then consider finite-sample data as input, and propose the first constraint optimization approach for recovering the system timescale causal structure. This algorithm optimally recovers from possible conflicts due to statistical errors. More generally, these advances allow for a robust and non-parametric estimation of system timescale causal structures from subsampled time series data.\nKeywords: causality; causal discovery; graphical models; time series; constraint satisfaction; constraint optimization."
    }, {
      "heading" : "1. Introduction",
      "text" : "Time-series data has long constituted the basis for causal modeling in many fields of science (Granger, 1969; Hamilton, 1994; Lütkepohl, 2005). Despite the often very precise measurements at regular time points, the underlying causal interactions that give rise to the measurements often occur at a much faster timescale than the measurement frequency. While information about time order is generally seen as simplifying causal analysis, time series data that undersamples the generating process can be misleading about the true causal connections (Dash and Druzdzel, 2001; Iwasaki and Simon, 1994). For example, Figure 1a shows the causal structure of a process unrolled over discrete time steps, and Figure 1c shows the corresponding structure of the same process, obtained by marginalizing every second time step. If the subsampling rate is not taken into account, we might conclude that optimal control of V2 requires interventions on both V1 and V3, when the influence of V3 on V2 is, in fact, completely mediated by V1 (and so intervening only on V1 suffices).\nar X\niv :1\n60 2.\n07 97\n0v 2\n[ cs\n.A I]\n1 3\nStandard methods for estimating causal structure from time series either focus exclusively on estimating a transition model at the measurement timescale (e.g., Granger causality (Granger, 1969, 1980)) or combine a model of measurement timescale transitions with so-called “instantaneous” or “contemporaneous” causal relations that (are supposed to) capture any interactions that are faster than the measurement process (e.g., SVAR) (Lütkepohl, 2005; Hamilton, 1994; Hyvärinen et al., 2010). In contrast, we follow Plis et al. (2015a,b) and Gong et al. (2015), and explore the possibility of identifying (features of) the causal process at the true timescale from data that subsample this process.\nIn this paper, we provide an exact inference algorithm based on using a general-purpose Boolean constraint solver (Biere et al., 2009; Gebser et al., 2011), and demonstrate that it is orders of magnitudes faster than the current state-of-the-art method by Plis et al. (2015b). At the same time, our approach is much simpler and allows inference in more general settings. We then show how the approach naturally integrates possibly conflicting results obtained from the data. Moreover, unlike the approach by Gong et al. (2015), our method does not depend on a particular parameterization of the underlying model and scales to a more reasonable number of variables."
    }, {
      "heading" : "2. Representation",
      "text" : "We assume that the system of interest relates a set of variables Vt = {V t1 , . . . , V tn} defined at discrete time points t ∈ Z with continuous (∈ Rn) or discrete (∈ Zn) values (Entner and Hoyer, 2010). We distinguish the representation of the true causal process at the system timescale from the time series data that are obtained at the measurement timescale. Following Plis et al. (2015b), we assume that the true between-variable causal interactions at the system timescale constitute a firstorder Markov process; that is, that the independence Vt ⊥ Vt−k|Vt−1 holds for all k > 1. The parametric models for these causal structures are structural vector autoregressive (SVAR) processes or dynamic (discrete/continuous variable) Bayes nets. Since the system timescale can be arbitrarily fast (and causal influences take time), we assume that there is no “contemporaneous” causation of the form V ti → V tj (Granger, 1988). We also assume that Vt−1 contains all common causes of variables in Vt. These assumptions jointly express the widely used causal sufficiency assumption (see Spirtes et al. (1993)) in the time series setting.\nThe system timescale causal structure can thus be represented by a causal graph G1 consisting (as in a dynamic Bayes net) only of arrows of the form V t−1i → V tj , where i = j is permitted (see Figure 1a for an example). Since the causal process is time invariant, the edges repeat through t. In accordance with Plis et al. (2015b), for any G1 we use a simpler, rolled graph representation, denoted by G1, where Vi → Vj ∈ G1 iff V t−1i → V tj ∈ G1. Figure 1b shows the rolled graph representation G1 of G1 in Figure 1a.\nTime series data are obtained from the above process at the measurement timescale, given by some (possibly unknown) integral sampling rate u. The measured time series sample Vt is at times t, t − u, t − 2u, . . .; we are interested in the case of u > 1, i.e., the case of subsampled data. A different route to subsampling would use continuous-time models as the underlying system timescale structure. However, some series (e.g., transactions such as salary payments) are inherently discrete time processes (Gong et al., 2015), and many continuous-time systems can be approximated arbitrarily closely as discrete-time processes. Thus, we focus here on discrete-time causal structures as a justifiable, and yet simple, basis for our non-parametric inference procedure.\nThe structure of this subsampled time series can be obtained from G1 by marginalizing the intermediate time steps. Figure 1c shows the measurement timescale structure G2 corresponding to subsampling rate u = 2 for the system timescale causal structure in Figure 1a. Each directed edge in G2 corresponds to a directed path of length 2 in G1. For arbitrary u, the formal relationship between Gu and G1 edges is\nV t−ui → V tj ∈ Gu ⇔ V t−u i V t j ∈ G1, where denotes a directed path.1\nSubsampling a time series additionally induces “direct” dependencies between variables in the same time step (Wei, 1994). The bi-directed arrow V t1 ↔ V t2 in Figure 1c is an example: V t−1 1 is an unobserved (in the data) common cause of V t1 and V t 2 in G\n1 (see Figure 1a). Formally, the system timescale structure G1 induces bi-directed edges in the measurement timescale Gu for i 6= j as follows:\nV ti ↔ V tj ∈ Gu ⇔ ∃(V ti V t−kc V tj ) ∈ G1, k < u.\nJust as G1 represents the rolled version of G1, Gu represents the rolled version of Gu: Vi → Vj ∈ Gu iff V t−ui → V tj ∈ Gu and Vi ↔ Vj ∈ Gu iff V ti ↔ V tj ∈ Gu.\nThe relationship between G1 and Gu—that is, the impact of subsampling—can be concisely represented using only the rolled graphs:\nVi → Vj ∈ Gu ⇔ Vi u Vj ∈ G1 (1) Vi ↔ Vj ∈ Gu ⇔ ∃(Vi <u Vc <u Vj) ∈ G1, i 6= j (2)\nwhere u denotes a path of length u and <u denotes a path shorter than u (of the same length on each arm of a common cause). Using the rolled graph notation, the logical encodings in Section 3 are considerably simpler.\n1. We assume a type of faithfulness assumption (see Spirtes et al. (1993)), such that influences along (multiple) paths between nodes do not exactly cancel in Gu.\nDanks and Plis (2013) demonstrated that, in the infinite sample limit, the causal structure G1 at the system timescale is in general underdetermined, even when the subsampling rate u is known and small. Consequently, even when ignoring estimation errors, the most we can learn is an equivalence class of causal structures at the system timescale. We define H to be the estimated version of Gu, a graph over V obtained or estimated at the measurement timescale (with possibly unknown u). Multiple G1 can have the same structure as H for distinct u, which poses a particular challenge when u is unknown. If H is estimated from data, it is possible, due to statistical errors, that no Gu has the same structure as H. With these observations, we are ready to define the computational problems focused on in this work.\nTask 1 Given a measurement timescale structure H (with possibly unknown u), infer the (equivalence class of) causal structures G1 consistent withH (i.e. Gu = H by Eqs. 1 and 2).\nWe also consider the corresponding problem when the subsampled time series is directly provided as input, rather than Gu.\nTask 2 Given a dataset of measurements of V obtained at the measurement timescale (with possibly unknown u), infer the (equivalence class of) causal structures G1 (at the system timescale) that are (optimally) consistent with the data.\nSection 3 provides a solution to Task 1, and Section 4 provides a solution to Task 2.\n3. Finding Consistent G1s\nWe first focus on Task 1. We discuss the computational complexity of the underlying decision problem, and present a practical Boolean constraint satisfaction approach that empirically scales up to significantly larger graphs than previous state-of-the-art algorithms."
    }, {
      "heading" : "3.1 On Computational Complexity",
      "text" : "Considering the task of finding a single G1 consistent with a given H, a variant of the associated decision problem is related to the NP-complete problem of finding a matrix root.\nTheorem 1 Deciding whether there is a G1 that is consistent with the directed edges of a given H is NP-complete for any fixed u ≥ 2.\nProof Membership in NP follows from a guess and check: guess a candidate G1, and deterministically check whether the length-u paths of G1 correspond to the edges of H (Plis et al., 2015b). For NP-hardness, for any fixed u ≥ 2, there is a straightforward reduction from the NP-complete problem of determining whether a Boolean B matrix has a uth root (Kutz, 2004)2 for a given n× n Boolean matrix B, interpret B as the directed edge relation of H, i.e., H has the edge (i, j) iff Au(i, j) = 1. It is then easy to see that there is a G1 that is consistent with the obtained H iff B = Au for some binary matrix A (i.e., a uth root of B).\nIf u is unknown, then membership in NP can be established in the same way by guessing both a candidate G1 and a value for u. Theorem 1 ignores the possible bi-directed edges in H (whose presence/absence is also harder to determine reliably from practical sample sizes; see Section 4.3).\n2. Multiplication of two values in {0, 1} is defined as the logical-or, or equivalently, the maximum operator.\nKnowledge of the presences and absences of such edges in H can restrict the set of candidate G1s. For example, in the special case whereH is known to not contain any bi-directed edges, the possible G1s have a fairly simple structure: in any G1 that is consistent with H, every node has at most one successor.3 Whether this knowledge can be used to prove a more fine-grained complexity result for special cases is an open question."
    }, {
      "heading" : "3.2 A SAT-Based Approach",
      "text" : "Recently, the first exact search algorithm for finding the G1s that are consistent with a given H for a known u was presented by Plis et al. (2015b); it represents the current state-of-the-art. Their approach implements a specialized depth-first search procedure for the problem, with domain-specific polynomial time search-space pruning techniques. As an alternative, we present here a Boolean satisfiability based approach. First, we represent the problem exactly using a rule-based constraint satisfaction formalism. Then, for a given input H, we employ an off-the-shelf Boolean constraint satisfaction solver for finding a G1 that is guaranteed to be consistent with H (if such G1 exists). Our approach is not only simpler than the approach of Plis et al. (2015b), but as we will show, it also significantly improves the current state-of-the-art in runtime efficiency and scalability.\nWe use here answer set programming (ASP) as the constraint satisfaction formalism (Niemelä, 1999; Simons et al., 2002; Gebser et al., 2011). It offers an expressive declarative modelling language, in terms of first-order logical rules, for various types of NP-hard search and optimization problems. To solve a problem via ASP, one first needs to develop an ASP program (in terms of ASP rules/constraints) that models the problem at hand; that is, the declarative rules implicitly represent the set of solutions to the problem in a precise fashion. Then one or multiple (optimal, in case of optimization problems) solutions to the original problem can be obtained by invoking an off-theshelf ASP solver, such as the state-of-the-art Clingo system (Gebser et al., 2011) used in this work. The search algorithms implemented in the Clingo system are extensions of state-of-theart Boolean satisfiability and optimization techniques which can today outperform even specialized domain-specific algorithms, as we show here.\nWe proceed by describing a simple ASP encoding of the problem of finding a G1 that is consistent with a givenH. The input—the measurement timescale structureH—is represented as follows. The input predicate node/1 represents the nodes of H (and all graphs), indexed by 1 . . . n. The presence of a directed edge X → Y between nodes X and Y is represented using the predicate edgeh/2 as edgeh(X,Y). Similarly, the fact that an edge X → Y is not present is represented using the predicate no edgeh/2 as no edgeh(X,Y). The presence of a bidirected edge X ↔ Y between nodes X and Y is represented using the predicate confh/2 as confh(X,Y) (X < Y ), and the fact that an edge X ↔ Y is not present is represented using the predicate no confh/2 as no confh(X,Y).\nIf u is known, then it can be passed as input using u(U); alternatively, it can be defined as a single value in a given range (here set to 1, . . . , 5 as an example):\nurange(1..5). % Define a range of u:s\n1 { u(U): urange(U) } 1. % u(U) is true for only one U in the range\n3. To see this, assume X has two successors, Y and Z, s.t. Y 6= Z in G1. Then Gu will contain a bi-directed edge Y ↔ Z for all u ≥ 2, which contradicts the assumption thatH has no bi-directed edges.\nSolution G1s are represented via the predicate edge1/2, where edge1(X,Y) is true iff G1 contains the edge X → Y . In ASP, the set of candidate solutions (i.e., the set of all directed graphs over n nodes) over which the search for solutions is performed, is declared via the so-called choice construct within the following rule, stating that candidate solutions may contain directed edges between any pair of nodes.\n{ edge1(X,Y) } :- node(X), node(Y).\nThe measurement timescale structure Gu corresponding to the candidate solution G1 is represented using the predicates edgeu(X,Y) and confu(X,Y), which are derived in the following way. First, we declare the mapping from a given G1 to the corresponding Gu by declaring the exact length-L paths in a non-deterministically chosen candidate solution G1. For this, we declare rules that compute the length-L paths inductively for all L ≤ U , using the predicate path(X,Y,L) to represent that there is a length-L path from X to Y .\n% Derive all directed paths up to length U path(X,Y,1) :- edge1(X,Y). path(X,Y,L) :- path(X,Z,L-1), edge1(Z,Y), L <= U, u(U).\nSecond, to obtain Gu, we encode Equations 1 and 2 with the following rules that form predicates edgeu/2 and confu/2 describing the edges G1 induces on the measurement timescale structure.\n% Paths of length U, correspond to measurement timescale edges edgeu(X,Y) :- path(X,Y,L), u(L).\n% Paths of equal length (<U) from a single node result in bi-directed edges confu(X,Y) :- path(Z,X,L), path(Z,Y,L), node(X;Y;Z), X < Y, L < U, u(U).\nFinally, we declare constraints that require that the Gu represented by the edgeu/2 and confu/2 predicates is consistent with the input H. This is achieved with the following rules, which enforce that the edge relations of Gu andH are exactly the same for any solution G1.\n:- edgeh(X,Y), not edgeu(X,Y). :- no_edgeh(X,Y), edgeu(X,Y). :- confh(X,Y), not confu(X,Y). :- no_confh(X,Y), confu(X,Y).\nOur ASP encoding of Task 1 consists of the rules just described. The set of solutions of the encoding correspond exactly to the G1s consistent with the inputH."
    }, {
      "heading" : "3.3 Runtime Comparison",
      "text" : "Both our proposed SAT-based approach and the recent specialized search algorithm MSL (Plis et al., 2015b) are correct and complete, so we focus on differences in efficiency, using the implementation of MSL by the original authors. Our approach allows for searching simultaneously over a range of values of u, but Plis et al. (2015b) focused on the case u = 2; hence, we restrict the comparison to u = 2.\nWe simulated system timescale graphs with varying density and number of nodes (see Section 4.3 for exact details), and then generated the measurement timescale structures for subsampling rate u = 2. This structure was given as input to the inference procedures. Note that the input consisted here of graphs for which there always is a G1, so all instances were satisfiable. The task of the algorithms was to output up to 1000 (system timescale) graphs in the equivalence class. The ASP encoding was solved by Clingo using the flag -n 1000 for the solver to enumerate 1000 solution graphs (or all, in cases where there were less than 1000 solutions).\nThe running times of the MSL algorithm and our approach (SAT) on 10-node input graphs with different edge densities are shown in Figure 2. Figure 2 (right) shows the scalability of the two approaches in terms of increasing number of nodes in the input graphs and fixed 10% edge density. Our declarative approach clearly outperforms MSL. 10-node input graphs, regardless of edge density, are essentially trivial for our approach, while the performance of MSL deteriorates noticeably as the density increases. For varying numbers of nodes in 10% density input graphs, our approach scales up to 65 nodes with a one hour time limit; even for 70 nodes, 25 graphs finished in one hour. In contrast, MSL reaches only 35 nodes; our approach uses only a few seconds for those graphs. The scalability of our algorithm allows for investigating the influence of edge density\nfor larger graphs. Figure 3 (left) plots the running times of our approach (when enumerating all solutions) for u = 2 on 20-node input graphs of varying densities. Finally, Figure 3 (right) shows the scalability of our approach in the more challenging task of enumerating all solutions over the range u = 1, . . . , 5 simultaneously. This also demonstrates the generality of our approach: it is not restricted to solving for individual values of u separately."
    }, {
      "heading" : "4. Learning from Undersampled Data",
      "text" : "Due to statistical errors in estimating H and the sparse distribution of Gu in “graph space”, there will often be no G1s that are consistent withH. Given such anH, neither the MSL algorithm nor our approach in the previous section can output a solution, and they simply conclude that no solution G1 exists for the input H. In terms of our constraint declarations, this is witnessed by conflicts among the constraints for any possible solution candidate. Given the inevitability of statistical errors, we should not simply conclude that no consistent G1 exists for such an H. Rather, we should aim to learn G1s that, in light of the underlying conflicts, are “optimally close” (in some well-defined sense of “optimality”) to being consistent with H. We now turn to this more general problem setting, and propose what (to the best of our knowledge) is the first approach to learning, by employing constraint optimization, from undersampled data under conflicts. In fact, we can use the ASP formulation already discussed—with minor modifications—to address this problem.\nIn this more general setting, the input consists of both the estimated graph H, and also (i) weights w(e ∈ H) indicating the reliability of edges present in H; and (ii) weights w(e 6∈ H) indicating the reliability of edges absent inH. Since Gu is G1 subsampled by u, the task is to find a G1 that minimizes the objective function:\nf(G1, u) = ∑ e∈H I[e 6∈ Gu] · w(e ∈ H) + ∑ e6∈H I[e ∈ Gu] · w(e 6∈ H),\nwhere the indicator function I(c) = 1 if the condition c holds, and I(c) = 0 otherwise. Thus, edges that differ between the estimated input H and the Gu corresponding to the solution G1 are penalized by the weights representing the reliability of the measurement timescale estimates. In the following, we first outline how the ASP encoding for the search problem without optimization is easily generalized to enable finding optimal G1 with respect to this objective function. We then describe alternatives for determining the weights w, and present simulation results on the relative performance of the different weighting schemes."
    }, {
      "heading" : "4.1 Learning by Constraint Optimization",
      "text" : "To model the objective function for handling conflicts, only simple modifications are needed to our ASP encoding: instead of declaring hard constraints that require that the paths induced by G1 exactly correspond to the edges in H, we soften these constraints by declaring that the violation of each individual constraint incurs the associated weight as penalty. In the ASP language, this can be expressed by augmenting the input predicates edgeh(X,Y) with weights: edgeh(X,Y,W) (and similarly for no edgeh, confh and no confh). Here the additional argument W represents the weight w((x → y) ∈ H) given as input. The following expresses that each conflicting presence of an edge inH and Gu is penalized with the associated weight W .\n:˜ edgeh(X,Y,W), not edgeu(X,Y). [W,X,Y,1] :˜ no_edgeh(X,Y,W), edgeu(X,Y). [W,X,Y,1] :˜ confh(X,Y,W), not confu(X,Y). [W,X,Y,2] :˜ no_confh(X,Y,W), confu(X,Y). [W,X,Y,2]\nThis modification provides an ASP encoding for Task 2; that is, the optimal solutions to this ASP encoding correspond exactly to the G1s that minimize the objective function f(G1, u) for any u and inputH with weighted edges."
    }, {
      "heading" : "4.2 Weighting Schemes",
      "text" : "We use two different schemes for weighting the presences and absences of edges in H according to their reliability. To determine the presence/absence of an edge X → Y in H we simply test the corresponding independence Xt−1 ⊥ Y t | Vt−1 \\Xt−1. To determine the presence/absence of an edge X ↔ Y inH, we run the independence test: Xt ⊥ Y t | Vt−1.\nThe simplest approach is to use uniform weights on the estimation result ofH:\nw(e ∈ H) = 1 ∀e ∈ H, w(e 6∈ H) = 1 ∀e 6∈ H.\nUniform edge weights resemble the search on the Hamming cube ofH that Plis et al. (2015b) used to address the problem of finding G1s whenH did not correspond to any Gu.\nA more intricate approach is to use pseudo-Boolean weights following Hyttinen et al. (2014); Sonntag et al. (2015); Margaritis and Bromberg (2009). They used Bayesian model selection to obtain reliability weights for independence tests. Instead of a p-value and a binary decision, these types of tests give a measurement of reliability for an independence/dependence statement as a Bayesian probability. We can directly use their approach of attaching log-probabilities as the reliability weights for the edges. For details, see Section 4.3 of Hyttinen et al. (2014). Again, we only compute weights for the independence tests mentioned above in the estimation ofH."
    }, {
      "heading" : "4.3 Simulations",
      "text" : "We use simulations to explore the impact of the choice of weighting schemes on the accuracy and runtime efficiency of our approach. For the simulations, system timescale structures G1 and the associated data generating models were constructed in the following way. To guarantee connectedness of the graphs, we first formed a cycle of all nodes in a random order (following Plis et al. (2015b)). We then randomly sampled additional directed edges until the required density was obtained. Recall that there are no bidirected edges in G1. We used Equations 1 and 2 to generate the measurement timescale structure Gu for a given u. When sample data were required, we used linear Gaussian structural autoregressive processes (order 1) with structure G1 to generate data at the system timescale, where coefficients were sampled from the two intervals ±[0.2, 0.8]. We then discarded intermediate samples to get the particular subsampling rate.4\nFigure 4 shows the accuracy of the different methods in one setting: subsampling rate u = 2, network size n = 6, average degree 3, sample size N = 200, and 100 data sets in total. The positive predictions correspond to presences of edges; when the method returned several members in the equivalence class, we used mean solution accuracy to measure the output accuracy. The\n4. Clingo only accepts integer weights; we multiplied weights by 1000 and rounded to the nearest integer.\nx-axis numbers correspond to the adjustment parameters for the statistical independence tests (pvalue threshold for uniform weights, prior probability of independence for all others). The two left columns (black and red) show the true positive rate and false positive rate of H estimation (compared to the true G2), for the different types of edges, using different statistical tests. For estimation from 200 samples, we see that the structure of G2 can be estimated with good tradeoff of TPR and FPR with the middle parameter values, but not perfectly. The presence of directed edges can be estimated more accurately. More importantly, the two rightmost columns in Figure 4 (green and blue) show the accuracy of G1 estimation. Both weighting schemes produce good accuracy for the middle parameter values, although there are some outliers. The pseudo-Boolean weighting scheme still outperforms the uniform weighting scheme, as it produces high TPR with low FPR for a range of threshold parameter values (especially for 0.4).\nFinally, the running times of our approach are shown in Figure 5 with different weighting schemes, network sizes (n), and sample sizes (N ). The subsampling rate was again fixed to u = 2, and average node degree was 3. The independence test threshold used here corresponds to the accuracy-optimal parameters in Figure 4. The pseudo-Boolean weighting scheme allows for much faster solving: for n = 7, it finishes all runs in a few seconds (black line), while the uniform weighting scheme (red line) takes tens of minutes. Thus, the pseudo-Boolean weighting scheme provides the best performance in terms of both computational efficiency and accuracy. Second, the sample size has a significant effect on the running times: larger sample sizes take less time. For n = 9 runs, N = 200 samples (blue line) take longer than N = 500 (cyan line). Intuitively, statistical tests should be more accurate with larger sample sizes, resulting in fewer conflicting constraints. For N = 1000, the global optimum is found here for up to 12-node graphs, though in a considerable amount of time."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we introduced a constraint optimization based solution for the problem of learning causal timescale structures from subsampled measurement timescale graphs and data. Our approach considerably improves the state-of-art; in the simplest case (subsampling rate u = 2), we extended the scalability by several orders of magnitude. Moreover, our method generalizes to handle different or unknown subsampling rates in a computationally efficient manner. Unlike previous methods, our method can operate directly on finite sample input, and we presented approaches that recover, in an optimal way, from conflicts arising from statistical errors. We expect that this considerably simpler approach will allow for the relaxation of additional model space assumptions in the future. In particular, we plan to use this framework to learn the system timescale causal structure from subsampled data when latent time series confound our observations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "AH was supported by Academy of Finland Centre of Excellence in Computational Inference Research COIN (grant 251170). SP was supported by NSF IIS-1318759 & NIH R01EB005846. MJ was supported by Academy of Finland Centre of Excellence in Computational Inference Research COIN (grant 251170) and grants 276412, 284591; and Research Funds of the University of Helsinki. FE was supported by NSF 1564330. DD was supported by NSF IIS-1318815 & NIH U54HG008540 (from the National Human Genome Research Institute through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health."
    } ],
    "references" : [ {
      "title" : "Learning causal structure from undersampled time series",
      "author" : [ "D. Danks", "S. Plis" ],
      "venue" : "In NIPS 2013 Workshop on Causality,",
      "citeRegEx" : "Danks and Plis.,? \\Q2013\\E",
      "shortCiteRegEx" : "Danks and Plis.",
      "year" : 2013
    }, {
      "title" : "Caveats for causal reasoning with equilibrium models",
      "author" : [ "D. Dash", "M. Druzdzel" ],
      "venue" : "In Proc. ECSQARU,",
      "citeRegEx" : "Dash and Druzdzel.,? \\Q2001\\E",
      "shortCiteRegEx" : "Dash and Druzdzel.",
      "year" : 2001
    }, {
      "title" : "On causal discovery from time series data using FCI",
      "author" : [ "D. Entner", "P. Hoyer" ],
      "venue" : "Proc. PGM,",
      "citeRegEx" : "Entner and Hoyer.,? \\Q2010\\E",
      "shortCiteRegEx" : "Entner and Hoyer.",
      "year" : 2010
    }, {
      "title" : "Potassco: The Potsdam answer set solving collection",
      "author" : [ "M. Gebser", "B. Kaufmann", "R. Kaminski", "M. Ostrowski", "T. Schaub", "M. Schneider" ],
      "venue" : "AI Communications,",
      "citeRegEx" : "Gebser et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gebser et al\\.",
      "year" : 2011
    }, {
      "title" : "Discovering temporal causal relations from subsampled data",
      "author" : [ "M. Gong", "K. Zhang", "B. Schoelkopf", "D. Tao", "P. Geiger" ],
      "venue" : "In Proc. ICML,",
      "citeRegEx" : "Gong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2015
    }, {
      "title" : "Investigating causal relations by econometric models and cross-spectral methods",
      "author" : [ "C. Granger" ],
      "venue" : null,
      "citeRegEx" : "Granger.,? \\Q1969\\E",
      "shortCiteRegEx" : "Granger.",
      "year" : 1969
    }, {
      "title" : "Testing for causality: a personal viewpoint",
      "author" : [ "C. Granger" ],
      "venue" : "Journal of Economic Dynamics and Control,",
      "citeRegEx" : "Granger.,? \\Q1980\\E",
      "shortCiteRegEx" : "Granger.",
      "year" : 1980
    }, {
      "title" : "Some recent development in a concept of causality",
      "author" : [ "C. Granger" ],
      "venue" : "Journal of Econometrics,",
      "citeRegEx" : "Granger.,? \\Q1988\\E",
      "shortCiteRegEx" : "Granger.",
      "year" : 1988
    }, {
      "title" : "Time series analysis, volume 2",
      "author" : [ "J. Hamilton" ],
      "venue" : null,
      "citeRegEx" : "Hamilton.,? \\Q1994\\E",
      "shortCiteRegEx" : "Hamilton.",
      "year" : 1994
    }, {
      "title" : "Constraint-based causal discovery: Conflict resolution with answer set programming",
      "author" : [ "A. Hyttinen", "F. Eberhardt", "M. Järvisalo" ],
      "venue" : "In Proc. UAI,",
      "citeRegEx" : "Hyttinen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hyttinen et al\\.",
      "year" : 2014
    }, {
      "title" : "Estimation of a structural vector autoregression model using non-gaussianity",
      "author" : [ "A. Hyvärinen", "K. Zhang", "S. Shimizu", "P. Hoyer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hyvärinen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hyvärinen et al\\.",
      "year" : 2010
    }, {
      "title" : "Causality and model abstraction",
      "author" : [ "Y. Iwasaki", "H. Simon" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Iwasaki and Simon.,? \\Q1994\\E",
      "shortCiteRegEx" : "Iwasaki and Simon.",
      "year" : 1994
    }, {
      "title" : "The complexity of Boolean matrix root computation",
      "author" : [ "M. Kutz" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Kutz.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kutz.",
      "year" : 2004
    }, {
      "title" : "New introduction to multiple time series analysis",
      "author" : [ "H. Lütkepohl" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Lütkepohl.,? \\Q2005\\E",
      "shortCiteRegEx" : "Lütkepohl.",
      "year" : 2005
    }, {
      "title" : "Efficient Markov network discovery using particle filters",
      "author" : [ "D. Margaritis", "F. Bromberg" ],
      "venue" : "Computational Intelligence,",
      "citeRegEx" : "Margaritis and Bromberg.,? \\Q2009\\E",
      "shortCiteRegEx" : "Margaritis and Bromberg.",
      "year" : 2009
    }, {
      "title" : "Logic programs with stable model semantics as a constraint programming paradigm",
      "author" : [ "I. Niemelä" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence,",
      "citeRegEx" : "Niemelä.,? \\Q1999\\E",
      "shortCiteRegEx" : "Niemelä.",
      "year" : 1999
    }, {
      "title" : "Rate-agnostic (causal) structure learning",
      "author" : [ "S. Plis", "D. Danks", "C. Freeman", "V. Calhoun" ],
      "venue" : "In Proc. NIPS,",
      "citeRegEx" : "Plis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Plis et al\\.",
      "year" : 2015
    }, {
      "title" : "Mesochronal structure learning",
      "author" : [ "S. Plis", "D. Danks", "J. Yang" ],
      "venue" : "In Proc. UAI,",
      "citeRegEx" : "Plis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Plis et al\\.",
      "year" : 2015
    }, {
      "title" : "Extending and implementing the stable model semantics",
      "author" : [ "P. Simons", "I. Niemelä", "T. Soininen" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Simons et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Simons et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning optimal chain graphs with answer set programming",
      "author" : [ "D. Sonntag", "M. Järvisalo", "J. Peña", "A. Hyttinen" ],
      "venue" : "In Proc. UAI,",
      "citeRegEx" : "Sonntag et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sonntag et al\\.",
      "year" : 2015
    }, {
      "title" : "Time series analysis",
      "author" : [ "W. Wei" ],
      "venue" : null,
      "citeRegEx" : "Wei.,? \\Q1994\\E",
      "shortCiteRegEx" : "Wei.",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Introduction Time-series data has long constituted the basis for causal modeling in many fields of science (Granger, 1969; Hamilton, 1994; Lütkepohl, 2005).",
      "startOffset" : 107,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "Introduction Time-series data has long constituted the basis for causal modeling in many fields of science (Granger, 1969; Hamilton, 1994; Lütkepohl, 2005).",
      "startOffset" : 107,
      "endOffset" : 155
    }, {
      "referenceID" : 13,
      "context" : "Introduction Time-series data has long constituted the basis for causal modeling in many fields of science (Granger, 1969; Hamilton, 1994; Lütkepohl, 2005).",
      "startOffset" : 107,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "While information about time order is generally seen as simplifying causal analysis, time series data that undersamples the generating process can be misleading about the true causal connections (Dash and Druzdzel, 2001; Iwasaki and Simon, 1994).",
      "startOffset" : 195,
      "endOffset" : 245
    }, {
      "referenceID" : 11,
      "context" : "While information about time order is generally seen as simplifying causal analysis, time series data that undersamples the generating process can be misleading about the true causal connections (Dash and Druzdzel, 2001; Iwasaki and Simon, 1994).",
      "startOffset" : 195,
      "endOffset" : 245
    }, {
      "referenceID" : 13,
      "context" : ", SVAR) (Lütkepohl, 2005; Hamilton, 1994; Hyvärinen et al., 2010).",
      "startOffset" : 8,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : ", SVAR) (Lütkepohl, 2005; Hamilton, 1994; Hyvärinen et al., 2010).",
      "startOffset" : 8,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : ", SVAR) (Lütkepohl, 2005; Hamilton, 1994; Hyvärinen et al., 2010).",
      "startOffset" : 8,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "In this paper, we provide an exact inference algorithm based on using a general-purpose Boolean constraint solver (Biere et al., 2009; Gebser et al., 2011), and demonstrate that it is orders of magnitudes faster than the current state-of-the-art method by Plis et al.",
      "startOffset" : 114,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : ", V t n} defined at discrete time points t ∈ Z with continuous (∈ Rn) or discrete (∈ Zn) values (Entner and Hoyer, 2010).",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "Since the system timescale can be arbitrarily fast (and causal influences take time), we assume that there is no “contemporaneous” causation of the form V t i → V t j (Granger, 1988).",
      "startOffset" : 167,
      "endOffset" : 182
    }, {
      "referenceID" : 4,
      "context" : ", transactions such as salary payments) are inherently discrete time processes (Gong et al., 2015), and many continuous-time systems can be approximated arbitrarily closely as discrete-time processes.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "(2015a,b) and Gong et al. (2015), and explore the possibility of identifying (features of) the causal process at the true timescale from data that subsample this process.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : ", 2009; Gebser et al., 2011), and demonstrate that it is orders of magnitudes faster than the current state-of-the-art method by Plis et al. (2015b). At the same time, our approach is much simpler and allows inference in more general settings.",
      "startOffset" : 8,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : ", 2009; Gebser et al., 2011), and demonstrate that it is orders of magnitudes faster than the current state-of-the-art method by Plis et al. (2015b). At the same time, our approach is much simpler and allows inference in more general settings. We then show how the approach naturally integrates possibly conflicting results obtained from the data. Moreover, unlike the approach by Gong et al. (2015), our method does not depend on a particular parameterization of the underlying model and scales to a more reasonable number of variables.",
      "startOffset" : 8,
      "endOffset" : 400
    }, {
      "referenceID" : 2,
      "context" : ", V t n} defined at discrete time points t ∈ Z with continuous (∈ Rn) or discrete (∈ Zn) values (Entner and Hoyer, 2010). We distinguish the representation of the true causal process at the system timescale from the time series data that are obtained at the measurement timescale. Following Plis et al. (2015b), we assume that the true between-variable causal interactions at the system timescale constitute a firstorder Markov process; that is, that the independence Vt ⊥ Vt−k|Vt−1 holds for all k > 1.",
      "startOffset" : 97,
      "endOffset" : 311
    }, {
      "referenceID" : 2,
      "context" : ", V t n} defined at discrete time points t ∈ Z with continuous (∈ Rn) or discrete (∈ Zn) values (Entner and Hoyer, 2010). We distinguish the representation of the true causal process at the system timescale from the time series data that are obtained at the measurement timescale. Following Plis et al. (2015b), we assume that the true between-variable causal interactions at the system timescale constitute a firstorder Markov process; that is, that the independence Vt ⊥ Vt−k|Vt−1 holds for all k > 1. The parametric models for these causal structures are structural vector autoregressive (SVAR) processes or dynamic (discrete/continuous variable) Bayes nets. Since the system timescale can be arbitrarily fast (and causal influences take time), we assume that there is no “contemporaneous” causation of the form V t i → V t j (Granger, 1988). We also assume that Vt−1 contains all common causes of variables in Vt. These assumptions jointly express the widely used causal sufficiency assumption (see Spirtes et al. (1993)) in the time series setting.",
      "startOffset" : 97,
      "endOffset" : 1025
    }, {
      "referenceID" : 2,
      "context" : ", V t n} defined at discrete time points t ∈ Z with continuous (∈ Rn) or discrete (∈ Zn) values (Entner and Hoyer, 2010). We distinguish the representation of the true causal process at the system timescale from the time series data that are obtained at the measurement timescale. Following Plis et al. (2015b), we assume that the true between-variable causal interactions at the system timescale constitute a firstorder Markov process; that is, that the independence Vt ⊥ Vt−k|Vt−1 holds for all k > 1. The parametric models for these causal structures are structural vector autoregressive (SVAR) processes or dynamic (discrete/continuous variable) Bayes nets. Since the system timescale can be arbitrarily fast (and causal influences take time), we assume that there is no “contemporaneous” causation of the form V t i → V t j (Granger, 1988). We also assume that Vt−1 contains all common causes of variables in Vt. These assumptions jointly express the widely used causal sufficiency assumption (see Spirtes et al. (1993)) in the time series setting. The system timescale causal structure can thus be represented by a causal graph G1 consisting (as in a dynamic Bayes net) only of arrows of the form V t−1 i → V t j , where i = j is permitted (see Figure 1a for an example). Since the causal process is time invariant, the edges repeat through t. In accordance with Plis et al. (2015b), for any G1 we use a simpler, rolled graph representation, denoted by G1, where Vi → Vj ∈ G1 iff V t−1 i → V t j ∈ G1.",
      "startOffset" : 97,
      "endOffset" : 1389
    }, {
      "referenceID" : 20,
      "context" : "1 Subsampling a time series additionally induces “direct” dependencies between variables in the same time step (Wei, 1994).",
      "startOffset" : 111,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : "1 Subsampling a time series additionally induces “direct” dependencies between variables in the same time step (Wei, 1994). The bi-directed arrow V t 1 ↔ V t 2 in Figure 1c is an example: V t−1 1 is an unobserved (in the data) common cause of V t 1 and V t 2 in G 1 (see Figure 1a). Formally, the system timescale structure G1 induces bi-directed edges in the measurement timescale Gu for i 6= j as follows: V t i ↔ V t j ∈ G ⇔ ∃(V t i V t−k c V t j ) ∈ G, k < u. Just as G1 represents the rolled version of G1, Gu represents the rolled version of Gu: Vi → Vj ∈ Gu iff V t−u i → V t j ∈ Gu and Vi ↔ Vj ∈ Gu iff V t i ↔ V t j ∈ Gu. The relationship between G1 and Gu—that is, the impact of subsampling—can be concisely represented using only the rolled graphs: Vi → Vj ∈ G ⇔ Vi u Vj ∈ G (1) Vi ↔ Vj ∈ G ⇔ ∃(Vi <u Vc <u Vj) ∈ G, i 6= j (2) where u denotes a path of length u and <u denotes a path shorter than u (of the same length on each arm of a common cause). Using the rolled graph notation, the logical encodings in Section 3 are considerably simpler. 1. We assume a type of faithfulness assumption (see Spirtes et al. (1993)), such that influences along (multiple) paths between nodes do not exactly cancel in G.",
      "startOffset" : 112,
      "endOffset" : 1130
    }, {
      "referenceID" : 12,
      "context" : "For NP-hardness, for any fixed u ≥ 2, there is a straightforward reduction from the NP-complete problem of determining whether a Boolean B matrix has a uth root (Kutz, 2004)2 for a given n× n Boolean matrix B, interpret B as the directed edge relation of H, i.",
      "startOffset" : 161,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "We use here answer set programming (ASP) as the constraint satisfaction formalism (Niemelä, 1999; Simons et al., 2002; Gebser et al., 2011).",
      "startOffset" : 82,
      "endOffset" : 139
    }, {
      "referenceID" : 18,
      "context" : "We use here answer set programming (ASP) as the constraint satisfaction formalism (Niemelä, 1999; Simons et al., 2002; Gebser et al., 2011).",
      "startOffset" : 82,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "We use here answer set programming (ASP) as the constraint satisfaction formalism (Niemelä, 1999; Simons et al., 2002; Gebser et al., 2011).",
      "startOffset" : 82,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "Then one or multiple (optimal, in case of optimization problems) solutions to the original problem can be obtained by invoking an off-theshelf ASP solver, such as the state-of-the-art Clingo system (Gebser et al., 2011) used in this work.",
      "startOffset" : 198,
      "endOffset" : 219
    }, {
      "referenceID" : 14,
      "context" : "2 A SAT-Based Approach Recently, the first exact search algorithm for finding the G1s that are consistent with a given H for a known u was presented by Plis et al. (2015b); it represents the current state-of-the-art.",
      "startOffset" : 152,
      "endOffset" : 172
    }, {
      "referenceID" : 14,
      "context" : "2 A SAT-Based Approach Recently, the first exact search algorithm for finding the G1s that are consistent with a given H for a known u was presented by Plis et al. (2015b); it represents the current state-of-the-art. Their approach implements a specialized depth-first search procedure for the problem, with domain-specific polynomial time search-space pruning techniques. As an alternative, we present here a Boolean satisfiability based approach. First, we represent the problem exactly using a rule-based constraint satisfaction formalism. Then, for a given input H, we employ an off-the-shelf Boolean constraint satisfaction solver for finding a G1 that is guaranteed to be consistent with H (if such G1 exists). Our approach is not only simpler than the approach of Plis et al. (2015b), but as we will show, it also significantly improves the current state-of-the-art in runtime efficiency and scalability.",
      "startOffset" : 152,
      "endOffset" : 791
    }, {
      "referenceID" : 16,
      "context" : "3 Runtime Comparison Both our proposed SAT-based approach and the recent specialized search algorithm MSL (Plis et al., 2015b) are correct and complete, so we focus on differences in efficiency, using the implementation of MSL by the original authors. Our approach allows for searching simultaneously over a range of values of u, but Plis et al. (2015b) focused on the case u = 2; hence, we restrict the comparison to u = 2.",
      "startOffset" : 107,
      "endOffset" : 354
    }, {
      "referenceID" : 14,
      "context" : "Uniform edge weights resemble the search on the Hamming cube ofH that Plis et al. (2015b) used to address the problem of finding G1s whenH did not correspond to any Gu.",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "A more intricate approach is to use pseudo-Boolean weights following Hyttinen et al. (2014); Sonntag et al.",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "A more intricate approach is to use pseudo-Boolean weights following Hyttinen et al. (2014); Sonntag et al. (2015); Margaritis and Bromberg (2009).",
      "startOffset" : 69,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "A more intricate approach is to use pseudo-Boolean weights following Hyttinen et al. (2014); Sonntag et al. (2015); Margaritis and Bromberg (2009). They used Bayesian model selection to obtain reliability weights for independence tests.",
      "startOffset" : 69,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "A more intricate approach is to use pseudo-Boolean weights following Hyttinen et al. (2014); Sonntag et al. (2015); Margaritis and Bromberg (2009). They used Bayesian model selection to obtain reliability weights for independence tests. Instead of a p-value and a binary decision, these types of tests give a measurement of reliability for an independence/dependence statement as a Bayesian probability. We can directly use their approach of attaching log-probabilities as the reliability weights for the edges. For details, see Section 4.3 of Hyttinen et al. (2014). Again, we only compute weights for the independence tests mentioned above in the estimation ofH.",
      "startOffset" : 69,
      "endOffset" : 567
    }, {
      "referenceID" : 9,
      "context" : "A more intricate approach is to use pseudo-Boolean weights following Hyttinen et al. (2014); Sonntag et al. (2015); Margaritis and Bromberg (2009). They used Bayesian model selection to obtain reliability weights for independence tests. Instead of a p-value and a binary decision, these types of tests give a measurement of reliability for an independence/dependence statement as a Bayesian probability. We can directly use their approach of attaching log-probabilities as the reliability weights for the edges. For details, see Section 4.3 of Hyttinen et al. (2014). Again, we only compute weights for the independence tests mentioned above in the estimation ofH. 4.3 Simulations We use simulations to explore the impact of the choice of weighting schemes on the accuracy and runtime efficiency of our approach. For the simulations, system timescale structures G1 and the associated data generating models were constructed in the following way. To guarantee connectedness of the graphs, we first formed a cycle of all nodes in a random order (following Plis et al. (2015b)).",
      "startOffset" : 69,
      "endOffset" : 1074
    } ],
    "year" : 2016,
    "abstractText" : "This paper focuses on causal structure estimation from time series data in which measurements are obtained at a coarser timescale than the causal timescale of the underlying system. Previous work has shown that such subsampling can lead to significant errors about the system’s causal structure if not properly taken into account. In this paper, we first consider the search for the system timescale causal structures that correspond to a given measurement timescale structure. We provide a constraint satisfaction procedure whose computational performance is several orders of magnitude better than previous approaches. We then consider finite-sample data as input, and propose the first constraint optimization approach for recovering the system timescale causal structure. This algorithm optimally recovers from possible conflicts due to statistical errors. More generally, these advances allow for a robust and non-parametric estimation of system timescale causal structures from subsampled time series data.",
    "creator" : "LaTeX with hyperref package"
  }
}