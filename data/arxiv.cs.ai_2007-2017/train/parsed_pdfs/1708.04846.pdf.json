{
  "name" : "1708.04846.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Maximum A Posteriori Inference in Sum-Product Networks",
    "authors" : [ "Jun Mei", "Yong Jiang", "Kewei Tu" ],
    "emails" : [ "meijun@shanghaitech.edu.cn", "jiangyong@shanghaitech.edu.cn", "tukw@shanghaitech.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "for fixed 0 ≤ < 1, where n is the input size. For the algorithmic part, we first present an exact MAP solver that runs reasonably fast and could handle SPNs with up to 1k variables and 150k arcs in our experiments. We then present a new approximate MAP solver with a good balance between speed and accuracy, and our comprehensive experiments on real-world datasets show that it has better overall performance than existing approximate solvers."
    }, {
      "heading" : "1 Introduction",
      "text" : "SPNs are a class of probabilistic graphical models known for its tractable marginal inference (Poon and Domingos, 2011). In the previous work, SPNs were mainly employed to do marginal inference. On the other hand, although MAP inference is widely used in many applications in natural language processing, computer vision, speech recognition, etc., MAP inference in SPNs has not been widely studied.\nSome previous work on MAP inference focuses on selective SPNs (Peharz et al., 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017). Huang et al. (2006) presented an exact solver for MAP based on deterministic arithmetic circuits. Peharz et al. (2016) showed that most probable explanation (MPE), a special case of MAP without hidden variables, is tractable on selective SPNs.\nSelectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b). For SPNs without the selectivity assumption, Peharz (2015) showed that MPE in SPNs is NP-hard by reducing SAT to MPE. Peharz et al. (2016) showed a different proof based on the NP-hardness results from Bayesian networks. Conaty et al. (2017) discussed approximation complexity of MAP in SPNs and gave several useful theoretical results.\nIn this paper, we investigate MAP inference in SPNs from both theoretical and algorithmic perspectives. For the theoretical part, we make the following two contributions. First, we define a special MAP inference problem called MAX that has no evidence and hidden variables, and we show that MAP can be reduced to MAX in linear time. This implies that to study MAP we can instead focus on MAX, which has a much simpler form. Second, we show that it is NP-hard to approximate the MAP problem to 2n\nfox fixed 0 ≤ < 1, where n is the input size. This result is similar to a theorem proved by Conaty et al. (2017), but we use a proof strategy that is arguably simpler than theirs. For the algorithmic part, we present an exact MAP solver and an approximate MAP solver. Our comprehensive experiments on real-world datasets show that our exact solver runs reasonably fast\nar X\niv :1\n70 8.\n04 84\n6v 1\n[ cs\n.A I]\n1 6\nA ug\n2 01\nand could handle SPNs with up to 1k variables and 150k arcs within ten minutes; our approximate solver provides a good trade-off between speed and accuracy and has better overall performance than previous approximate methods."
    }, {
      "heading" : "2 Background",
      "text" : "We adapt the notations from Peharz et al. (2015). A random variable is denoted as an upper-case letter, e.g. X, Y . The corresponding lower-case letter x denotes a value X can assume. The set of all the values X can assume is denoted as val(X). Thus x ∈ val(X).\nA set of variables is denoted as a boldface upper-case letter, e.g. X = {X1, X2, . . . , XN}. The corresponding boldface lower-case letter x denotes a compound value X can assume. The set of all the compound values X can assume is denoted as val(X), i.e. val(X) = ×Nn=1val(Xn). Thus x ∈ val(X). For X ∈ X, x[X] denotes the projection of x onto X. Thus x[X] ∈ val(X). For Y ⊆ X, x[Y] denotes the projection of x onto Y. Thus x[Y] ∈ val(Y).\nA compound value x is also a complete evidence, assigning each variable in X a value. Partial evidence about X is defined as X ⊆ val(X). Partial evidence about X is defined as X := ×Nn=1Xn. Thus X ⊆ val(X). For X ∈ X, we define X [X] := {x[X] | x ∈ X}. Thus X [X] ⊆ val(X). For Y ⊆ X, we define X [Y] := {x[Y] | x ∈ X}. Thus X [Y] ⊆ val(Y)."
    }, {
      "heading" : "2.1 Network polynomials",
      "text" : "Darwiche (2003) introduced network polynomials. λX=x ∈ R denotes the so-called indicator for X and x. λ denotes a vector collecting all the indicators of X.\nDefinition 1 (Network Polynomial). Let Φ be an unnormalized distribution over X with finitely many values. The network polynomial fΦ is defined as\nfΦ(λ) := ∑\nx∈val(X) Φ(x) ∏ X∈X λX=x[X]. (1)\nWe define λX=x(x) as a function of x ∈ val(X) and λ(x) denotes the corresponding vector-valued function, collecting all λX=x(x):\nλX=x(x) = { 1 if x = x[X] 0 otherwise.\n(2)\nIt can be easily verified that fΦ(λ(x)) = Φ(x) since when we input λ(x) to fΦ, all but one of the terms in the summation evaluate to 0. We extend Eq. 2 to a function of partial evidence X :\nλX=x(X ) = {\n1 if x ∈ X [X] 0 otherwise.\n(3)\nLet λ(X ) be the corresponding vector-valued function. It can also be shown that fΦ(λ(X )) =∑ x∈X Φ(x), i.e. the network polynomial returns the unnormalized probability measure for partial evidence X . In particular, fΦ(val(X)) returns the normalization constant of Φ. We should note that, although the indicators are restricted to {0, 1} by Eq. 2 and Eq. 3, they are actually real-valued variables. Therefore, taking the first derivative with respect to some λX=x yields\n∂fΦ ∂λX=x (λ(X )) = Φ(x,X [X \\ {X}]). (4)\nThis means the derivative on the left hand side in Eq. 4 actually evaluates Φ for modified evidence {x} ×X [X \\ {X}]. This technique is used in our exact MAP solver to be introduced in Section 4."
    }, {
      "heading" : "2.2 Sum-product networks",
      "text" : "SPNs over variables with finitely many values are defined as follows:\nDefinition 2 (Sum-Product Networks). Let X be variables with finitely many values and λ their indicators. A sum-product network S = (G,w) over X is a rooted directed acyclic graph G = (V,A) with nonnegative parameters w. All leaves of G are indicators and all internal nodes are either sums or products. Denote the set of children of node N as ch(N). A sum node S computes a weighted sum S(λ) = ∑ C∈ch(S) wS,CC(λ), where the weight wS,C ∈ w is associated with the arc (S,C) ∈ A. A\nproduct node computes P(λ) = ∏\nC∈ch(S) C(λ). The output of S is the function R(λ) computed by the root R and denoted as S(λ).\nThe scope of node N, denoted as sc(N), is defined as\nsc(N) = { {X} if N is some indicator λX=x ∪C∈ch(N)sc(C) otherwise.\n(5)\nWe say an SPN is complete if for each sum S, we have sc(C) = sc(C′),∀C,C′ ∈ ch(S). We say an SPN is decomposable if for each product P, we have sc(C) ∩ sc(C′) = ∅,∀C,C′ ∈ ch(P),C 6= C′. The output function of a complete and decomposable SPN is actually a network polynomial. While there exist SPNs that are not decomposable, in this paper we follow the majority of the previous work and focus on complete and decomposable SPNs.\nNow we define MAP inference formally. Using Eq. 2 and Eq. 3, we define S(x) := S(λ(x)) and S(X ) := S(λ(X )). For variables X, we use Q,E,H to denote query, evidence and hidden variables, where Q ∪E ∪H = X, Q 6= ∅ and Q,E,H are disjoint. Given Q,E,H and an evidence e ∈ val(E), the MAP inference in the SPN S over variables X is defined as\nMAPS(Q, e,H) := arg max q∈val(Q)\nS({q} × {e} × val(H)). (6)\nNote that MAP inference is typically defined using conditional probabilities, but it is easy to show that our definition is equivalent to the classical definition."
    }, {
      "heading" : "3 Theoretical results",
      "text" : ""
    }, {
      "heading" : "3.1 MAX inference",
      "text" : "MAP inference splits X into three parts: query, evidence and hidden variables. We define a special case of MAP inference without evidence and hidden variables, which we call MAX inference:\nMAXS := arg max x∈val(X)\nS(x) (7)\nWe can reduce every MAP problem to a MAX problem in linear time. Without loss of generality, we assume the root of an SPN is a sum (otherwise we can always add a new sum root node linking to the old root with weight 1). Given an SPN S and a MAP problem with Q, e,H, Algorithm 1 modifies S and returns a new SPN denoted as S ′ such that ∀q ∈ val(Q),S ′(q) = S({q}×{e}×val(H)), which implies MAXS′ = MAPS(Q, e,H). The algorithm runs as follows. We first calculate the value wN for each node N, which is later multiplied into the arc weights of certain ancestor sum nodes of N. Intuitively, we do bottom-up precomputing of the node values and store the precomputed values in the weights. After that, we remove every node N and its arcs if sc(N) ⊆ E ∪H and output the resulting SPN. Using the terminology of knowledge compilation (Darwiche and Marquis, 2002) and negation normal forms (Darwiche, 2001), the algorithm performs conditioning on the evidence variables, projects the SPN onto the query variables, and then makes simplifications to the SPN structure.\nThis reduction implies that any efficient algorithm for solving MAX can also be used to efficiently solve MAP; furthermore, any approximation algorithm for MAX can be used to approximate MAP to the same factor. Therefore, in the next two sections we will focus on algorithms that solve MAX.\nAlgorithm 1 Calculate MAP2MAXS(Q, e,H) 1: for all N ∈ V in reverse topological order do 2: wN ← 1 3: if N is a leaf λX=x s.t. X ∈ E and e[X] 6= x then 4: wλX=x ← 0 5: if N is a sum S then 6: for all C ∈ ch(S) do 7: wS,C ← wS,CwC . multiply wC into wS,C 8: if sc(S) ⊆ E ∪H then 9: wS ← ∑ C∈ch(S) wS,C . otherwise, wS = 1\n10: if N is a product P then 11: wP ← ∏ C∈P wC 12: for all N ∈ V do 13: if sc(N) ⊆ E ∪H then 14: remove N and the arcs/weights associated with N"
    }, {
      "heading" : "3.2 Approximation complexity",
      "text" : "It has been shown in the literature that MAP inference in Bayesian networks (BNs) is hard. Denote the size of an SPN S and a BN B as |S| and |B| respectively. Theorem 6 in (De Campos, 2011) indicates that for any fixed 0 ≤ < 1 it is NP-hard to approximate MAP in tree-structured BNs to 2|B| . We can transfer this result to SPNs.\nLemma 1. Given a tree-structured BN B, we can construct an SPN S representing the same distribution with size |S| ∈ O(|B|) in linear time.\nSee the proof of Lemma 1 in the Appendix A.\nTheorem 1. For any fixed 0 ≤ δ < 1, it is NP-hard to approximate MAP in SPNs to 2|S|δ .\nProof. Suppose there exists fixed 0 ≤ δ < 1 s.t. it is not NP-hard to approximate MAP in SPNs to 2|S| δ\n. Given a tree-structured BN B, we can construct an SPN S in linear time that represents the same distribution as B. Since |S| ∈ O(|B|), there exist constants b, c > 0 s.t. |S| ≤ c|B| if |B| ≥ b. Define two constants τ = (1− δ)/2 and b′ = max{b, cδ/τ}. Given a MAP problem in B, we can solve it exactly in constant time if |B| < b′. In the following we consider the case of |B| ≥ b′. According to our assumption, we can approximate MAP in the constructed SPN S to 2|S|δ in polynomial time. Since S and B represent the same distribution, we have also approximated MAP in B to 2|S|δ . Since |B| ≥ b′, we have 2|S|\nδ ≤ 2(c|B|)δ ≤ 2|B|τ |B|δ = 2|B| where = δ + τ < 1. Therefore, there exists a constant s.t. it takes polynomial time to approximate MAP in B to 2|B| , contradicting De Campos’s theorem.\nThm. 1 suggests that it is almost impossible to find a practical and useful approximation bound for MAP inference in SPNs. Note that in parallel to our work, Conaty et al. (2017) gave a similar result to Thm. 1. In comparison with their work, we employ a different proof strategy which is arguably simpler than theirs."
    }, {
      "heading" : "4 Exact solver",
      "text" : "Since MAP inference is NP-hard, no efficient exact solver exists. However, with a combination of pruning, heuristic, and optimization techniques, we can make exact inference reasonably fast in practice. In this section, we introduce two pruning techniques, a heuristic, and an optimization technique in\nAlgorithm 2 Calculate x = MAXS 1: x← a initial sample . using any initialization method, for example, random initialization 2: x← Search(val(X),x)\n3: function Search(X ,x) 4: X ← a variable with |X [X]| > 1 . |X [X]| = 1 means the value of X is determined 5: if no such X exists then . all variables are determined 6: return x′ where x′ is the only element in X . because now |X | = 1 is guaranteed 7: for all x ∈ X [X] do . consider all possible values of variable X 8: X ′ ← {x} ×X [X \\ {X}] . new smaller space 9: X ′ ←MarginalChecking(X ′,x) or ForwardChecking(X ′,x)\n10: if X ′ 6= ∅ then 11: x← Search(X ′,x) 12: return x\n13: function MarginalChecking(X ,x) 14: if S(X ) > S(x) then . check in linear time if there can be better samples than x in space X 15: return X 16: return ∅\n17: function ForwardChecking(X ,x) 18: repeat 19: calculate Dx ← ∂S∂x (X ) for every x simultaneously . can be done in linear time 20: for all X ∈ X do 21: for all x ∈ X [X] do 22: if S(x) ≥ Dx then . S(x) can be cached 23: X ← (X [X] \\ {x})×X [X \\ {X}] . remove x from X 24: until X is no longer changed 25: return X . X is now shrunk and may become ∅\norder to build a practical exact solver. We will focus on solving MAX since any MAP problem can be efficiently converted to a MAX problem. Algorithm 2 shows our algorithm framework. The function Search has two augments: X is the remaining space to be explored and x is the current best sample.\nWe first introduce a pruning technique called Marginal Checking (MC). MC computes S(X ) which is the summation of scores of all the samples in X . If it is less than or equal to the score of the current best sample x, then there cannot be any sample in X with a higher score than x and therefore we can safely prune space X .\nWe can go one step further and check and prune the subspaces of X . This leads to a new pruning technique which we call Forward Checking (FC). For each X ∈ X and x ∈ X [X], we consider the subspace {x} ×X [X \\ {X}]. If the subspace does not have a higher score than x, then we prune the subspace by removing value x from X (Line 23). The scores of all the subspaces can be computed simultaneously in linear time by taking partial derivatives (Eq. 4). Note that once we prune a subspace by removing a value from X , other subspaces are shrunk and their scores have to be rechecked. For example, the subspace {x1, x2} × {y} is shrunk to {x2} × {y} if we remove x1. Therefore, we repeat Line 19-23 until X is no longer changed.\nNow we introduce a heuristic called Ordering, which is inspired by similar techniques for solving constraint satisfaction problems. At Line 4, we need to choose an undetermined variable X. Instead of choosing randomly, we choose the variable with the fewest remaining values, i.e., arg minX∈X |X [X]|, which would then lead to fewer search branches. At Line 7, we need to try every value x ∈ X [X]. We order these values by their corresponding space scores S({x} × X [X \\ {X}]), because we expect a higher score implies that the subspace is more likely to contain a better sample and finding a better sample earlier leads to more effective pruning.\nFinally, we introduce an optimization technique called Stage. Once the value of a variable X is determined, it is never changed in the corresponding sub-search-tree. We can treat such determined variables as evidence in MAP inference and reduce the size of the SPN by running Algorithm 1. By doing this, we reduce the amount of computation in the sub-search-tree. Note that, however, the procedure of creating a smaller SPN incurs some overhead. To prevent the overhead from overtaking the benefit, we only do this once every few levels in the search tree.\nSince FC is more advanced than MC with similar time complexity, our final exact solver is built by combining FC, Ordering and Stage. Note that our exact solver is actually an anytime algorithm that can terminate at any time and return the current best sample. Thus, our exact solver can also be used as an approximate solver when there is a time budget."
    }, {
      "heading" : "5 Approximate solvers",
      "text" : "Thm. 1 states that approximating MAP inference in SPNs is very hard. However, in practice it is possible to design approximate solvers with good performance on most data. In this section, we briefly introduce existing approximate methods and then present a new method. Again, when describing the algorithms, we assume the MAP problem has been converted to a MAX problem."
    }, {
      "heading" : "5.1 Existing methods",
      "text" : "Best Tree (BT) BT, first used by Poon and Domingos (2011), runs in three steps: first, it changes all the sum nodes in the SPN to max nodes; second, it calculates the values of all the nodes from bottom up; third, in a recursive top-down manner starting from the root node, it selects the child of each max node with the largest value. The selected leaf nodes in the third step represent the approximate MAP solution of BT. We name this method Best Tree because we can show that it actually finds the parse tree of the SPN with the largest value. Tu (2016) showed that any decomposable SPN can be seen as a stochastic context-free And-Or grammar, and following their work we can define a parse tree of an SPN as follows.\nDefinition 3 (Parse Tree). Given an SPN S = (G,w), a parse tree T = (G′,w′) is an SPN where G′ = (V ′, A′) is a subgraph of G and w′ is the subset of w containing weights of the arcs in A′. G′ is recursively constructed as follows: 1) we add the root R of G into V ′; 2) when a sum S is added into V ′, add exactly one of its children C into V ′ and the corresponding arc (S,C) into A′; 3) when a product P is added into V ′, add all its children to V ′ and all the corresponding arcs to A′. The value of the parse tree is the product of the weights in w′.\nThe notion of parse trees has been used before in the SPN and arithmetic circuit literature under different terms, e.g., induced trees in (Zhao et al., 2016). We use the term “parse trees” because our approximate solver is inspired by the formal grammar literature.\nNormalized Greedy Selection (NG) NG was also used first by Poon and Domingos (2011). It is very similar to BT except that in the first step, NG does not change sum nodes to max nodes. We name this method Normalized Greedy Selection because it can be seen as greedily constructing a parse tree in a recursive top-down manner by selecting for each sum node the child with the largest weight in the locally normalized SPN (Peharz et al., 2015).\nArgmax-Product (AMAP) AMAP was proposed by Conaty et al. (2017). It does |ch(S)| times bottom-up evaluation on every sum S in the SPN, so it has quadratic time complexity, while BT and NG both have linear time complexity.\nAlgorithm 3 Calculate x̂ = KBT (S) 1: for all N ∈ V in reverse topological order do 2: if N is a leaf λ then 3: Mλ ← bestK({1}) . bestK(M) returns a multiset with at most K best elements in M 4: if N is a sum S then 5: MS ← bestK(]C∈ch(S){wS,C ×m | m ∈MC}) . in time O(|ch(S)|+K log |ch(S)|) 6: if N is a product P then 7: MP ← bestK({ ∏ m∈M′ m |M\n′ ∈ ×C∈ch(P)MC}) . in time O(K|ch(P)| logK) 8: S← {x corresponding to m | m ∈MR} . R is the root; top-down backtracking in time O(K|V |) 9: x̂← argmaxx∈S S(x) . in time O(K|S|)\nBeam Search (BS) Hill climbing has been used in MAP inference of arithmetic circuits (Park, 2002; Darwiche, 2003), a type of models closely related to SPNs. BS is an extension of hill climbing with K samples. In each round, it evaluates all the samples that result from changing the value of one variable in the existing samples, and then it keeps the top K samples. The evaluation of all such samples in each round can be done in linear time using Eq. 4. The number K is called the beam size."
    }, {
      "heading" : "5.2 K-Best Tree method",
      "text" : "It can be shown that the set of leaves of a parse tree T (Def. 3) corresponds to a single sample x. We denote this relation as T ∼ x. On the other hand, a sample may correspond to more than one parse tree. Formally, ∀T ∼ x, we have T (x) = T (val(X)) and T (x) ≤ S(x). Furthermore, S(x) = ∑ T ∼x T (x) (Zhao et al., 2016). We say a sample is ambiguous with respect to an SPN if it corresponds to more than one parse tree of the SPN. We say an SPN is ambiguous if there exist some ambiguous samples with respect to the SPN. Non-ambiguity is also known as selectivity in (Peharz et al., 2014a). Recall that BT finds the sample with the best parse tree of the SPN. It is easy to show that BT finds the exact solution to the MAX problem if the SPN is unambiguous (Peharz et al., 2016). However, BT cannot find good solutions if the input SPN is very ambiguous, as will be shown in our experiments.\nHere we propose an extension of BT called K-Best Tree (KBT) that finds the top K parse trees with the largest values (Algorithm 3). KBT is motivated by our empirical finding that even for ambiguous SPNs, in many cases the exact MAX solution corresponds to at least one parse tree with a large (although not necessarily the largest) value. If at least one parse tree of the exact solution is among the top K parse trees, KBT will be able to find the exact solution. Note that the K best trees that KBT finds may not correspond to K unique samples, since there may exist different parse trees corresponding to the same sample.\nSimilar to BT, KBT runs in two steps. In the bottom-up step, we calculate K best subtrees rooted at each node. In the top-down step, we backtrack to find the K samples corresponding to the K best trees. After that, we evaluate the K samples on the SPN and return the best one. When we set K = 1, KBT reduces to BT. Notice that the set notation in Algorithm 3 denotes multisets.\nThe time complexity stated at Line 5 and Line 7 can be achieved using a priority queue, as explained in Appendix B. Overall, the time complexity of KBT is O(|S|K logK). When K is a constant, the time complexity is linear in the SPN size. There is a trade-off between the running time and accuracy of the algorithm. A large K would likely improve the quality of the result but would lead to more running time."
    }, {
      "heading" : "6 Experiments",
      "text" : "We evaluated the MAP solvers on twenty widely-used real-world datasets (collected from applications and data sources such as click-through logs, plant habitats, collaborative filtering, etc.) from Gens and Domingos (2013), with variable numbers ranging from 16 to 1556. We used the LearnSPN method\n(Gens and Domingos, 2013) to obtain an SPN for each dataset. The numbers of arcs of the learned SPNs range from 6471 to 2,598,116. The detailed statistics of the learned SPNs are shown in Appendix C. We generated MAP problems with different proportions of query (Q), evidence (E) and hidden (H) variables. For each dataset and each proportion, we generated 1000 different MAP problems by randomly dividing the variables into Q/E/H variables. When running the solvers, we bounded the running time for one MAP problem by 10 minutes. We ran our experiments on Intel(R) Xeon(R) CPU E5-2697 v4 @ 2.30GHz."
    }, {
      "heading" : "6.1 Exact solver",
      "text" : "We evaluated four combinations of the techniques introduced in Section 4: Marginal Checking (MC), Forward Checking (FC), FC with Ordering (FC+O), and FC with both Ordering and Stage (FC+O+S).\nFigure 2 shows, for each dataset and with the Q/E/H proportion being 3/3/4, the number of times each method finished running within 10 minutes on the 1000 problems. Results for additional Q/E/H proportions can be found in Appendix D. It can be seen that for the datasets with the smallest variable numbers and SPN sizes, all four methods finished running within ten minutes. On the other datasets, FC clearly beats MC and adding Ordering and Staging brings further improvement. Our best method, FC+O+S, can be seen to handle SPNs with up to 1556 variables (“Ad”) and 147,599 arcs (“Accidents”).\nThe last four columns of Figure 4 show the average running time of the four methods (with a 10-minute time limit for each problem). On the first three datasets, which have very small variable numbers and SPN sizes, MC is actually faster than the other three methods. This is most likely because on these datasets the overhead of FC and the two additional techniques dominates the running time. On the other datasets, the benefit of FC and the two techniques can be clearly observed."
    }, {
      "heading" : "6.2 Approximate solver",
      "text" : "We evaluated all the approximate solvers described in Section 5, as well as the approximate versions of our exact solvers. For BS, we tested beam sizes of 1, 10 and 100. For KBT, we tested K = 10 and 100. We measure the performance of a solver on each dataset with each Q/E/H proportion by its running time and winning count. The winning count is defined as the number of problems on which the solver outputs a solution with the highest score among all the solvers. Since our exact solvers are anytime algorithms, we also evaluated them as approximate solvers with a 10-minute time budget.\nFigure 3 shows, for each method and each Q/E/H proportion, the average running time vs. the winning counts averaged over all the datasets. We can see from the figure that the best-tree based methods, BT (=KBT1), KBT10 and KBT100, dominate the other methods with less running time and higher winning counts. Increasing K with KBT improves winning counts but slows down the solver, as one would expect. In terms of running time, BT and NG are much faster than the other methods, while (FC+O+S), the approximate version of the exact solver, is by far the slowest. KBT100 clearly has the highest winning counts, followed by (FC+O+S), KBT10 and AMAP. Furthermore, we see that with the proportion of hidden variables increasing, the winning counts of most methods (except AMAP, KBT100 and KBT10) fall significantly. We believe this is because with more hidden variables, the MAP problem becomes more difficult, as reflected by the fact that the reduced SPN from Algorithm 1 becomes exponentially more ambiguous.\nFigure 4 and 5 show the running time and winning counts of all the methods on each dataset under the Q/E/H proportion of 3/3/4. The figures for additional Q/E/H proportions can be found in the supplementary material. We can see that AMAP failed to produce any result within ten minutes on the “20 Newsgroup” dataset, and on the other 19 datasets it actually has higher winning counts but significantly longer running time than KBT100. For the approximate versions of the exact solvers, we can see that even when they were terminated before they could finish, (FC+O) and (FC+O+S) still achieve very competitive winning counts, which is in sharp contrast to (FC). This suggests that the Ordering heuristic is very effective in guiding the search towards good solutions earlier."
    }, {
      "heading" : "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)",
      "text" : ""
    }, {
      "heading" : "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)",
      "text" : ""
    }, {
      "heading" : "Dataset MC FC FC+O FC+O+S",
      "text" : ""
    }, {
      "heading" : "7 Conclusion",
      "text" : "Theoretically, we defined a new inference problem called MAX and presented linear-time reduction from MAP to MAX. This suggests that we can focus on the much simpler MAX problem when studying MAP inference. We also showed that it is almost impossible to find a practical bound for approximate MAP solvers.\nAlgorithmically, we presented an exact solver based on exhaustive search with pruning, heuristic, and optimization techniques, and an approximate solver based on finding the top K parse trees of the input SPN. Our comprehensive experiments show that the exact solver is reasonably fast and the approximate solver has better overall performance than existing methods."
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "Lemma 1. Given a tree-structured BN B, we can construct an SPN S representing the same distribution with size |S| ∈ O(|B|) in linear time.\nProof. As mentioned in (Zhao et al., 2015), given a BN, there exists an algorithm1 that builds an SPN representing the same distribution. We adapt their algorithm for tree-structured BNs (Algorithm 4: BN2SPN). Given a tree-structured BN B, we would like to show that |S| ∈ O(|B|) where S = BN2SPN(B). We denote the numbers of weights, sum nodes, sum arcs, product arcs in S as |w|, |VS|, |AS|, |AP|, and denote the number of parameters and the number of all the values of all the variables in B as #P , #x. Since we use a map caching result of BuildSPN(N′, x′), we go through Line 7-14 at most #x times. At Line 3 and 12, whenever we add a sum arc, we use a different parameter in B, thus |AS| ∈ O(#P ) and |w| ∈ O(#P ). According to Line 8 and 10, whenever a product arc is added, it links to an indicator or a sum node, thus |AP| ∈ O(#x+ |VS|) ⊆ O(#x+ #P ) = O(#P ). Therefore, |S| ∈ O(|w|+ |AS|+ |AP|) ⊆ O(#P ) ⊆ O(|B|), i.e. |S| ∈ O(|B|). Furthermore, the algorithm runs in linear time.\nAlgorithm 4 Calculate S = BN2SPN(B) 1: create sum node R as the root 2: for all x ∈ XR′ do . XR′ corresponding to the root R′ of B 3: add BuildSPN(R′, x) as child of R with weight P (x)\n4: function BuildSPN(N′, x′) . node N′ in B; value x′ corresponding to N′ 5: if key (N′, x′) exists in M then . M is a global map caching results 6: return M(N′, x′) 7: create a product node P 8: add λXN′=x′ as child of P . XN′ corresponding to N ′\n9: for all C′ ∈ ch(N′) do 10: create a sum node S as child of P 11: for all x ∈ val(XC′) do . XC′ corresponding to C′ 12: add BuildSPN(C′, x) as child of S with weight P (x | x′) 13: store (N′, e)→ P in map M 14: return P"
    }, {
      "heading" : "B Time complexity of KBT",
      "text" : "We analyze the time complexity of KBT (Algorithm 3). To execute Line 5, we first push the best value in the multiset of each child into the priority queue and then pop K times. Whenever we pop a value m, we push into the queue the next best value (if one exists) in the multiset of the child that m belongs to. The size of the queue is |ch(S)|. The number of pushing is |ch(S)|+K and the number of popping is K. The time complexity is therefore O(|ch(S)|+K log |ch(S)|) if we use Fibonacci heap as the priority queue.\nTo execute Line 7, we keep performing pairwise merging of the multisets of the children until we get a single multiset left. When merging two multisets, we first push into a priority queue the product of the best values from the two multisets and then pop K times. Whenever we pop a product m1×m2, we push into the queue two new products m′1 ×m2 and m1 ×m′2 if we have not pushed them, where m′1 and m′2 are the next best values after m1 and m2 in the two multisets respectively. Thus when merging two multisets, we pop for at most K times and push for 2K + 1 times. We merge |ch(P)| − 1 times. Therefore, the time complexity is O(K|ch(P)| logK) if using Fibonacci heap.\n1See also http://spn.cs.washington.edu/faq.shtml\nOverall, the time complexity of KBT is O(|S|K logK)."
    }, {
      "heading" : "C Dataset statistics",
      "text" : "The following table shows the statistics of the SPNs learned from the 20 datasets. #Vars denotes the number of variables and #Arcs denotes the number of arcs in the learned SPN."
    }, {
      "heading" : "D Experimental results with additional Q/E/H proportions",
      "text" : ""
    }, {
      "heading" : "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)",
      "text" : ""
    }, {
      "heading" : "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)",
      "text" : ""
    }, {
      "heading" : "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)",
      "text" : ""
    }, {
      "heading" : "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)",
      "text" : ""
    }, {
      "heading" : "Dataset MC FC FC+O FC+O+S",
      "text" : ""
    }, {
      "heading" : "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)",
      "text" : ""
    }, {
      "heading" : "Dataset BT NG BS1 BS10 BS100 KBT10 KBT100 AMAP (MC) (FC) (FC+O) (FC+O+S)",
      "text" : ""
    }, {
      "heading" : "Dataset MC FC FC+O FC+O+S",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Language modeling with sum-product networks",
      "author" : [ "W.C. Cheng", "S. Kok", "H.V. Pham", "H.L. Chieu", "K.M.A. Chai" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Cheng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2014
    }, {
      "title" : "On relaxing determinism in arithmetic circuits",
      "author" : [ "A. Choi", "A. Darwiche" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Choi and Darwiche.,? \\Q2017\\E",
      "shortCiteRegEx" : "Choi and Darwiche.",
      "year" : 2017
    }, {
      "title" : "Approximation complexity of maximum a posteriori inference in sum-product networks",
      "author" : [ "D. Conaty", "D.D. Mauá", "C.P. de Campos" ],
      "venue" : null,
      "citeRegEx" : "Conaty et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conaty et al\\.",
      "year" : 2017
    }, {
      "title" : "Decomposable negation normal form",
      "author" : [ "A. Darwiche" ],
      "venue" : "JACM,",
      "citeRegEx" : "Darwiche.,? \\Q2001\\E",
      "shortCiteRegEx" : "Darwiche.",
      "year" : 2001
    }, {
      "title" : "A differential approach to inference in Bayesian networks",
      "author" : [ "A. Darwiche" ],
      "venue" : "JACM,",
      "citeRegEx" : "Darwiche.,? \\Q2003\\E",
      "shortCiteRegEx" : "Darwiche.",
      "year" : 2003
    }, {
      "title" : "A knowledge compilation",
      "author" : [ "A. Darwiche", "P. Marquis" ],
      "venue" : "map. JAIR,",
      "citeRegEx" : "Darwiche and Marquis.,? \\Q2002\\E",
      "shortCiteRegEx" : "Darwiche and Marquis.",
      "year" : 2002
    }, {
      "title" : "New complexity results for MAP in Bayesian networks",
      "author" : [ "C.P. De Campos" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Campos.,? \\Q2011\\E",
      "shortCiteRegEx" : "Campos.",
      "year" : 2011
    }, {
      "title" : "Discriminative learning of sum-product networks",
      "author" : [ "R. Gens", "P. Domingos" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Gens and Domingos.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gens and Domingos.",
      "year" : 2012
    }, {
      "title" : "Learning the structure of sum-product networks",
      "author" : [ "R. Gens", "P. Domingos" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Gens and Domingos.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gens and Domingos.",
      "year" : 2013
    }, {
      "title" : "Solving MAP exactly by searching on compiled arithmetic circuits",
      "author" : [ "J. Huang", "M. Chavira", "A. Darwiche" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Huang et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning arithmetic circuits",
      "author" : [ "D. Lowd", "P. Domingos" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Lowd and Domingos.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lowd and Domingos.",
      "year" : 2008
    }, {
      "title" : "MAP complexity results and approximation methods",
      "author" : [ "J.D. Park" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Park.,? \\Q2002\\E",
      "shortCiteRegEx" : "Park.",
      "year" : 2002
    }, {
      "title" : "Foundations of sum-product networks for probabilistic modeling",
      "author" : [ "D.-I.R. Peharz" ],
      "venue" : "PhD thesis, Aalborg University,",
      "citeRegEx" : "Peharz.,? \\Q2015\\E",
      "shortCiteRegEx" : "Peharz.",
      "year" : 2015
    }, {
      "title" : "Learning selective sum-product networks",
      "author" : [ "R. Peharz", "R. Gens", "P. Domingos" ],
      "venue" : "In ICML Workshop on Learning Tractable Probabilistic Models,",
      "citeRegEx" : "Peharz et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Peharz et al\\.",
      "year" : 2014
    }, {
      "title" : "Modeling speech with sum-product networks: Application to bandwidth extension",
      "author" : [ "R. Peharz", "G. Kapeller", "P. Mowlaee", "F. Pernkopf" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "Peharz et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Peharz et al\\.",
      "year" : 2014
    }, {
      "title" : "On theoretical properties of sum-product networks",
      "author" : [ "R. Peharz", "S. Tschiatschek", "F. Pernkopf", "P.M. Domingos", "B. BioTechMed-Graz" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Peharz et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Peharz et al\\.",
      "year" : 2015
    }, {
      "title" : "On the latent variable interpretation in sum-product networks",
      "author" : [ "R. Peharz", "R. Gens", "F. Pernkopf", "P. Domingos" ],
      "venue" : null,
      "citeRegEx" : "Peharz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Peharz et al\\.",
      "year" : 2016
    }, {
      "title" : "Sum-product networks: A new deep architecture",
      "author" : [ "H. Poon", "P. Domingos" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Poon and Domingos.,? \\Q2011\\E",
      "shortCiteRegEx" : "Poon and Domingos.",
      "year" : 2011
    }, {
      "title" : "Learning sum-product networks with direct and indirect variable interactions",
      "author" : [ "A. Rooshenas", "D. Lowd" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Rooshenas and Lowd.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rooshenas and Lowd.",
      "year" : 2014
    }, {
      "title" : "Stochastic And-Or grammars: A unified framework and logic perspective",
      "author" : [ "K. Tu" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Tu.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tu.",
      "year" : 2016
    }, {
      "title" : "On the relationship between sum-product networks and Bayesian networks",
      "author" : [ "H. Zhao", "M. Melibari", "P. Poupart" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    }, {
      "title" : "A unified approach for learning the parameters of sum-product networks",
      "author" : [ "H. Zhao", "P. Poupart", "G. Gordon" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "SPNs are a class of probabilistic graphical models known for its tractable marginal inference (Poon and Domingos, 2011).",
      "startOffset" : 94,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017).",
      "startOffset" : 85,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017).",
      "startOffset" : 138,
      "endOffset" : 204
    }, {
      "referenceID" : 10,
      "context" : ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017).",
      "startOffset" : 138,
      "endOffset" : 204
    }, {
      "referenceID" : 1,
      "context" : ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017).",
      "startOffset" : 138,
      "endOffset" : 204
    }, {
      "referenceID" : 18,
      "context" : "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al.",
      "startOffset" : 79,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b).",
      "startOffset" : 154,
      "endOffset" : 221
    }, {
      "referenceID" : 0,
      "context" : "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b).",
      "startOffset" : 154,
      "endOffset" : 221
    }, {
      "referenceID" : 0,
      "context" : ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017). Huang et al. (2006) presented an exact solver for MAP based on deterministic arithmetic circuits.",
      "startOffset" : 180,
      "endOffset" : 226
    }, {
      "referenceID" : 0,
      "context" : ", 2014a), which is also known as determinism in the context of knowledge compilation (Darwiche and Marquis, 2002) and arithmetic circuits (Darwiche, 2003; Lowd and Domingos, 2008; Choi and Darwiche, 2017). Huang et al. (2006) presented an exact solver for MAP based on deterministic arithmetic circuits. Peharz et al. (2016) showed that most probable explanation (MPE), a special case of MAP without hidden variables, is tractable on selective SPNs.",
      "startOffset" : 180,
      "endOffset" : 325
    }, {
      "referenceID" : 0,
      "context" : "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b). For SPNs without the selectivity assumption, Peharz (2015) showed that MPE in SPNs is NP-hard by reducing SAT to MPE.",
      "startOffset" : 180,
      "endOffset" : 282
    }, {
      "referenceID" : 0,
      "context" : "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b). For SPNs without the selectivity assumption, Peharz (2015) showed that MPE in SPNs is NP-hard by reducing SAT to MPE. Peharz et al. (2016) showed a different proof based on the NP-hardness results from Bayesian networks.",
      "startOffset" : 180,
      "endOffset" : 362
    }, {
      "referenceID" : 0,
      "context" : "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b). For SPNs without the selectivity assumption, Peharz (2015) showed that MPE in SPNs is NP-hard by reducing SAT to MPE. Peharz et al. (2016) showed a different proof based on the NP-hardness results from Bayesian networks. Conaty et al. (2017) discussed approximation complexity of MAP in SPNs and gave several useful theoretical results.",
      "startOffset" : 180,
      "endOffset" : 465
    }, {
      "referenceID" : 0,
      "context" : "Selectivity, however, is not guaranteed in most of the SPN learning algorithms (Gens and Domingos, 2012, 2013; Rooshenas and Lowd, 2014) and applications (Poon and Domingos, 2011; Cheng et al., 2014; Peharz et al., 2014b). For SPNs without the selectivity assumption, Peharz (2015) showed that MPE in SPNs is NP-hard by reducing SAT to MPE. Peharz et al. (2016) showed a different proof based on the NP-hardness results from Bayesian networks. Conaty et al. (2017) discussed approximation complexity of MAP in SPNs and gave several useful theoretical results. In this paper, we investigate MAP inference in SPNs from both theoretical and algorithmic perspectives. For the theoretical part, we make the following two contributions. First, we define a special MAP inference problem called MAX that has no evidence and hidden variables, and we show that MAP can be reduced to MAX in linear time. This implies that to study MAP we can instead focus on MAX, which has a much simpler form. Second, we show that it is NP-hard to approximate the MAP problem to 2 fox fixed 0 ≤ < 1, where n is the input size. This result is similar to a theorem proved by Conaty et al. (2017), but we use a proof strategy that is arguably simpler than theirs.",
      "startOffset" : 180,
      "endOffset" : 1168
    }, {
      "referenceID" : 12,
      "context" : "We adapt the notations from Peharz et al. (2015). A random variable is denoted as an upper-case letter, e.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "1 Network polynomials Darwiche (2003) introduced network polynomials.",
      "startOffset" : 22,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "Using the terminology of knowledge compilation (Darwiche and Marquis, 2002) and negation normal forms (Darwiche, 2001), the algorithm performs conditioning on the evidence variables, projects the SPN onto the query variables, and then makes simplifications to the SPN structure.",
      "startOffset" : 47,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Using the terminology of knowledge compilation (Darwiche and Marquis, 2002) and negation normal forms (Darwiche, 2001), the algorithm performs conditioning on the evidence variables, projects the SPN onto the query variables, and then makes simplifications to the SPN structure.",
      "startOffset" : 102,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "Note that in parallel to our work, Conaty et al. (2017) gave a similar result to Thm.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "Best Tree (BT) BT, first used by Poon and Domingos (2011), runs in three steps: first, it changes all the sum nodes in the SPN to max nodes; second, it calculates the values of all the nodes from bottom up; third, in a recursive top-down manner starting from the root node, it selects the child of each max node with the largest value.",
      "startOffset" : 33,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "Best Tree (BT) BT, first used by Poon and Domingos (2011), runs in three steps: first, it changes all the sum nodes in the SPN to max nodes; second, it calculates the values of all the nodes from bottom up; third, in a recursive top-down manner starting from the root node, it selects the child of each max node with the largest value. The selected leaf nodes in the third step represent the approximate MAP solution of BT. We name this method Best Tree because we can show that it actually finds the parse tree of the SPN with the largest value. Tu (2016) showed that any decomposable SPN can be seen as a stochastic context-free And-Or grammar, and following their work we can define a parse tree of an SPN as follows.",
      "startOffset" : 33,
      "endOffset" : 557
    }, {
      "referenceID" : 21,
      "context" : ", induced trees in (Zhao et al., 2016).",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "We name this method Normalized Greedy Selection because it can be seen as greedily constructing a parse tree in a recursive top-down manner by selecting for each sum node the child with the largest weight in the locally normalized SPN (Peharz et al., 2015).",
      "startOffset" : 235,
      "endOffset" : 256
    }, {
      "referenceID" : 12,
      "context" : "Normalized Greedy Selection (NG) NG was also used first by Poon and Domingos (2011). It is very similar to BT except that in the first step, NG does not change sum nodes to max nodes.",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "Argmax-Product (AMAP) AMAP was proposed by Conaty et al. (2017). It does |ch(S)| times bottom-up evaluation on every sum S in the SPN, so it has quadratic time complexity, while BT and NG both have linear time complexity.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "Beam Search (BS) Hill climbing has been used in MAP inference of arithmetic circuits (Park, 2002; Darwiche, 2003), a type of models closely related to SPNs.",
      "startOffset" : 85,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "Beam Search (BS) Hill climbing has been used in MAP inference of arithmetic circuits (Park, 2002; Darwiche, 2003), a type of models closely related to SPNs.",
      "startOffset" : 85,
      "endOffset" : 113
    }, {
      "referenceID" : 21,
      "context" : "Furthermore, S(x) = ∑ T ∼x T (x) (Zhao et al., 2016).",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "It is easy to show that BT finds the exact solution to the MAX problem if the SPN is unambiguous (Peharz et al., 2016).",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : ") from Gens and Domingos (2013), with variable numbers ranging from 16 to 1556.",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "(Gens and Domingos, 2013) to obtain an SPN for each dataset.",
      "startOffset" : 0,
      "endOffset" : 25
    } ],
    "year" : 2017,
    "abstractText" : "Sum-product networks (SPNs) are a class of probabilistic graphical models that allow tractable marginal inference. However, the maximum a posteriori (MAP) inference in SPNs is NP-hard. We investigate MAP inference in SPNs from both theoretical and algorithmic perspectives. For the theoretical part, we reduce general MAP inference to its special case without evidence and hidden variables; we also show that it is NP-hard to approximate the MAP problem to 2 for fixed 0 ≤ < 1, where n is the input size. For the algorithmic part, we first present an exact MAP solver that runs reasonably fast and could handle SPNs with up to 1k variables and 150k arcs in our experiments. We then present a new approximate MAP solver with a good balance between speed and accuracy, and our comprehensive experiments on real-world datasets show that it has better overall performance than existing approximate solvers.",
    "creator" : "LaTeX with hyperref package"
  }
}