{
  "name" : "1603.03627.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning from Imbalanced Multiclass Sequential Data Streams Using Dynamically Weighted Conditional Random Fields",
    "authors" : [ "Roberto L. Shinmoto Torres", "Damith C. Ranasinghe", "Qinfeng Shi", "Anton van den Hengel" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Developments in emerging wireless sensor technologies is enabling a multitude of applications, particularly in healthcare practice for applications such as location\n∗Corresponding author\nar X\niv :1\n60 3.\n03 62\n7v 1\n[ cs\ntracking, medical monitoring of patients and recognition of performed activities. The recognition of activities in older people is of particular interest as a means of preventing injuries from events such as falls by providing an early intervention, or identification of function decline, as in those with Alzheimer’s or Parkinson’s disease, to enact preventive interventions.\nOne of the main challenges in human activity recognition is the fact that sensor data is usually imbalanced as data from all possible activities are not necessarily equally distributed. This is because people naturally perform some activities that are of longer duration than others. For instance, in the context of patient monitoring in hospitals or nursing homes, resting activities such as lying on bed (i.e. sleeping) or sitting on a chair or on the bed are of longer duration than ambulating activities, where destinations, for example a rest room, are very close. Furthermore, data is more easily collected for certain activities than others; for instance, data from activities performed closer to the sensing infrastructure (e.g. motion sensors on ceilings) can be more easily collected than those activities performed farther from the sensors.\nThis paper presents a novel method for learning from imbalanced data using conditional random fields. We propose a class-wise cost parameter based classifier, that is able to consider the dependencies between activities. The classifier considers the influence of individual classes for learning from sequential imbalanced multiclass datasets. The cost parameters (weights) are not fixed as they are dynamically adjusted during the training process; while the classifier seeks to optimize the model’s expected overall F -score to minimize both false positives (false alarms) and false negatives (missed classifications). The performance of our approach is evaluated in three case studies in the context of recognizing activities of older people instrumented with a single body worn batteryless sensor. The results are validated with an external dataset where levels of imbalance were incremented."
    }, {
      "heading" : "1.1 Scope and Background",
      "text" : "Imbalanced data can negatively affect the training of machine learning algorithms as the classifier can be biased to prefer the majority class [1, 2]. This can be a serious problem as minority classes are of great importance in applications such as human activity monitoring. For example, older people previously assessed as being at risk of falling performing a short duration ambulation—as opposed to large amounts of time spent in resting postures such as lying in bed—are potentially at a risk of falling and injury [3]. Hence it is important to increase the overall classification performance and, in particular, the classifier performance in identifying minority classes such as ambulation in our example.\nAnother challenge arising from the nature of human activity recognition problems is the difficulty of collecting and labelling large datasets from activities of daily living (ADL). This is indeed the case for applications such as monitoring patients in acute hospitals where collecting data to learn activities of hospitalized older people is very difficult due to physical limitations resulting from their older age and associated ailments[4, 5]. Therefore, a classifier which is highly accurate at predicting all activity classes in datasets where availability of training data is scarce is highly desirable.\nGiven that human activities are sequential and a person can perform the same activity in different manners; we base our classifier in conditional random\nfields (CRFs) [6], a graphical model for structured classification that captures dependency relationships between performed activities as described by sensor observations. We evaluate our proposed classifier with three case studies from healthy and hospitalized older people using a battery-less body worn sensor where the data streams from the wearable sensors are irregular and sparse, noisy and class imbalanced. We confirm our findings using a publicly available dataset for human activity recognition using battery powered body worn sensors; in this scenario we modified the imbalance to levels similar or higher than those of our case studies.\nThe study presented in this article is part of our ongoing research aimed at recognizing activities by older people in hospital and nursing home settings for falls prevention, as described in [7, 8, 9]. This paper presents a method for learning from imbalanced sensor data streams, with limited training data, using conditional random fields."
    }, {
      "heading" : "1.2 Related works",
      "text" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22]. The approach presented in this article is based on the latter.\nThe main issue with re-sampling techniques [10, 11, 12] is that the removal or introduction of data can modify the sequence structure and its meaning. This is an issue in some real world applications that require maintaining the original data structure. For example, modifying parts of a sentence for text classification can effectively change the meaning of the message. Similarly, in human activity recognition the time sequence is important to determine the flow of movement or activities and introducing data can change the sequence of activities and the way it is analyzed, e.g. affecting transition probabilities in Markov chains.\nDecision threshold methods such as that of [14] achieved similar results to re-sampling techniques and used receiver operating characteristic (ROC) curves to decide which decision threshold produces the best performance. However, ROC curves depend on measuring specificity which does not reflect the errors in imbalanced data; this is due to specificity of the minority class being conditioned to its true negative measurement which includes the true positives of the majority class and thus leading to over optimistic results.\nIn the case of cost parameter methods, these require the inclusion of fixed class-wise costs into the objective function of the classifier during training to reinforce the learning of the under-represented classes. Generally, cost sensitive learning approaches have been reported to perform better than re-sampling techniques in some applications [1]. Some cost parameters have the form of a cost matrix that weighs each possible misclassification case, giving higher costs to misclassifications of a minority class observation in comparison to majority classes [15, 16, 17]. Costs have also been used to rescale the data. This is done by re-weighting or re-sampling the training samples; or moving decision thresholds according to their costs. These costs are usually user provided, e.g. from a cost matrix; these methods are reported to work well in binary data and only in some multiclass cases [17]. Moreover, these costs are fixed during training, but in real world applications —as is our case— costs can change for\nvarious reasons[17]. The method of Huang et al. [21] introduced a fixed set of weights for each class for a binary SVM algorithm. The binary weights ratio was inversely proportional to their respective class population ratio in the training data. This achieved a marginal improvement for the minority class accuracy at the cost of possible overall accuracy reduction. In Jiang et al. [20], weights calculated from the misclassification cost of each class were introduced into Bayesian network classifiers.\nThe study of Gimpel et al. [23], focused on improving the classification performance by modifying costs according to specific performance tasks (task-wise) such as improving recall, precision or both (as in F -score) [23] as opposed to classification error minimization as in previous studies. The method itself did not consider class specific parameters and parameter calculations required an extensive validation process as parameters were not learned in training. The introduction of weights in CRF (WCRF) is not new; however, previous approaches only considered using a fixed set of weights during training for optimization [22]; however, finding an optimal set of weights [23, 20] require an extensive validation process.\nThese previously mentioned methods [20, 22, 23] require empirical calculation of parameters. This process can be cumbersome and computationally expensive. For example, in an extensive grid search for suitable weights, the number of validation operations is of the form VM, where V is the cardinality of the parameters’ value range and M the number of parameters. In addition, objective function optimization based on classification error (1-accuracy) minimization is not suitable for imbalanced data. This is because the resulting measure, accuracy, is largely favoured by the dominant class and does not provide performance information regarding the predicted minority class [24].\nOther studies, such as that of Gama et al. [19] dynamically changed the cost of the naive Bayes classifier by verifying and introducing classification errors on each iteration; this method optimized the squared loss function as opposed to the 0–1 loss function. However, this study was only compared to the performance of a naive Bayes approach. In Pang et al. [18], dynamically learned parameters were introduced into a binary linear proximal SVM (LPSVM) objective function where weights were proportional to the ratio of the other class population. The study of Pletscher et al. [25] presented a method for generalizing structured classifiers such as CRF and Structured SVM (SSVM) and introduced a cost parameter given by the Hamming distance between predicted output and ground truth which was dynamically calculated during training. Although the methods in [18, 25] presented dynamically calculated cost parameters, the model optimization was based on classification error minimization. Other studies offered alternative methods [26, 27]. The method of Soda [26] decided between an unbalanced or a balanced classifier for every observation and measured its performance based on accuracy; while Beyan et al. [27] proposed a hierarchical method based on clustering and outlier detection that, in general, was not significantly better than other methods. Moreover, both studies did not consider multi-class problems.\nThe study of Dimitroff et al. [28] considered a learning algorithm for binary classification using a maximum likelihood model (weighted maximum entropy) to optimize the expected F -score during training where weights were calculated autonomously. Our study extends weighted maximum entropy [28], originally\nproposed for binary classification problems, to structured prediction using CRF. We use CRF in order to model the dependency between consecutive activities described by sensor observations in the training sequences and applied in previous research [9, 29].\nIn summary, most real-life data is imbalanced and methods that modify the data structure can also change the meaning of the events being studied, specially in the case of sequential data. Similarly, the use of optimization metrics such as accuracy are affected by the majority class performance. In the case of cost sensitive learning methods, most studies used a fixed set of weights where obtaining an optimal set requires a large amount of validation processes. In other dynamically calculated weights methods, the models were optimized based on accuracy. Our proposed method is an alternative that considers dynamically calculated weights that optimize the overall F-score to reduce false positives and false negatives for multi-class structured prediction using CRF."
    }, {
      "heading" : "1.3 Paper Contributions",
      "text" : "Our approach extends existing knowledge by solving current limitations: degradation of sequential data by re-sampling; learning models controlled by measures (accuracy and specificity) biased towards the dominant class; and inclusion of class specific parameters that are fixed or require extensive validation or optimization. We make the following contributions:\n1. We present a novel cost sensitive learning method for imbalanced multiclass data classification in real time, based on weighted conditional random fields (WCRF) where the optimization process is based on maximizing the expected overall F -score. In this method, the cost parameters are dynamically computed during training. To our knowledge, this is the first attempt for multiclass classifier optimization based on F -score metric to learn from imbalanced data where the cost parameters are also learned, in particular, for graphical models such as CRF.\n2. We apply our method to two scenarios; the first considers three case studies of sequential data streams from healthy and hospitalized older people using a batteryless body worn sensor over their clothing in clinical and hospital settings; and we generalize our results with a second scenario that considers a battery powered body worn sensor dataset with increased imbalance.\n3. We achieve better overall performance in comparison to other CRF based classifiers and similar (p-value > 0.44) or higher performance than other support vector machine (SVM) based methods in the context of scarce training data, which is often the case with practical applications using supervised methods to obtain highly accurate predictive models."
    }, {
      "heading" : "2 Proposed Dynamically Weighted Learning",
      "text" : "Method"
    }, {
      "heading" : "2.1 Background",
      "text" : "In this section we briefly revisit CRFs, a probabilistic graphical model for structured classification [6, 30], as background to defining our method in Section 2.2.\nLet us assume a sequence of input observations {xt}Tt=1 and their corresponding labels {yt}Tt=1, where yt ∈ {1 · · ·K} and K is the number of classes to infer. The advantage of CRF is that it constructs pairwise relationships between adjacent hidden variables and their corresponding observations, a property from the first order Markov assumption. The probability distribution in CRF is given by the conditional probability defined as\np(y|x, λ) = 1 Z exp\n( T∑ φt (yt−1, yt, x;λ) ) , (1)\nZ(x) = ∑ y1···T exp\n( T∑ φ(yt−1, yt, x;λ) ) . (2)\nHere the potential function exp(φ(yt−1, yt, x;λ)) follows the logistic model function\nφ(yt−1, yt, x;λ) = λ 1f(yt−1, yt) + λ 2f(yt, x) (3) where λ = ( λ1, λ2 ) are the model parameters to be estimated during training and f(.) are transition and emission feature functions that produce boolean values. The term Z(x) is the partition function and normalizes the conditional probability.\nDuring model training, we seek to maximize the conditional log likelihood L, defined as:\nL(λ) = log p(y|x), (4) L(λ) = T∑( λ1f(yt−1, yt) + λ 2f(yt, x) ) − log(Z(x)). (5)\nSince L is a convex function we apply a quasi-Newton method for estimation of model parameters λ such as the L-BFGS optimization algorithm. The partition function considers a summation over all possible values of x and y. We calculate the value of Z(x) using the belief propagation (sum–product) algorithm which recursively calculates the passing of messages over all elements in the tree. In the case of linear chain graphical models, belief propagation provides an exact solution for the calculation of Z(x), given by Z(x) = ∑ yT αT , where αt are the messages propagating forward in the algorithm."
    }, {
      "heading" : "2.2 Dynamically Weighted Conditional Random Fields (dWCRF)",
      "text" : "In this section we detail our dynamically weighted CRF (dWCRF) approach for structured predictions. The main motivation for the implementation of a weighted approach is to address the negative effects of imbalanced data on\nlearning. In addition, classification performance measurements such as accuracy (1-error) is not a suitable metric as results are biased towards the majority class. Hence, we require the minimization of false positives (FP ) or false alarms; and false negatives (FN) or missed classifications. Intuitively, this means increasing both true positives (TP ) and true negatives (TN) of all participating classes. We use the expected F -score (or F-measure) [28], as an optimization metric for our model as it considers FN and FP in its definition.\nIn this context, we introduce a cost term in our classifier objective function to give a higher cost to errors in the minority classes to reduce the effects of imbalance on the classifier. We consider the weighted log-likelihood function\nL(λ,w) = T∑ t=1 wt log p(yt|xt, λ) (6)\nwhere wt is a scalar weight for each element of the training sequence of length T (weight vector represented by [wt] T t=1). In [28], Dimitroff et al. demonstrated using the Pareto optimality concept that there exists a set of weights wFβ for which λF , the parameter that optimizes the expected Fβ-score, coincides with weighted maximum likelihood optimization parameter λwML. More information on Pareto efficiency can be found in [28, 31]. However, the approach followed in [28] was for a binary Maximum Entropy classifier. In this article, we extend this previous work to multiclass classification using dWCRF; we consider the expected overall Fβ-score (F̄ ), given by the mean expected Fβ-score over all participating classes as\nF̄ = 1\nK K∑ k=1 ( (1 + β2)Precisionk.Recallk β2 · Precisionk + Recallk ) (7)\nwhere β ∈ R is non-negative and balances the contributions from precision and recall. Henceforth, for simplicity, we consider β = 1, where precision and recall have the same influence; i.e. the harmonic mean of both precision and recall. Let us assume λ̂ to be the maximizer of F̄ , and considering that TPk = ∑ i:yi=k p(yi = k|xi, λ) and FPk = ∑ i:yi 6=k p(yi = k|xi, λ); expanding and operating in (7), can be rewritten as\nF̄ (λ) = 2\nK\n[ TP1\nTP1 + T1 + FP1 + · · ·+ TPK TPK + TK + FPK\n] (8)\nwhere Tk are the number of elements of class k in the training sequence, i.e.∑ k(Tk) = T . From (8), we want to show that λ̂ is an element of the Pareto optimal set of the multicriteria optimization problem (MOP)\nmax λ {TP1, TP2, · · · , TPK}. (9)\nWe do not consider the FP term from (8) as we are interested in maximizing TP s and reducing FP s; moreover, increasing TP{1···K}\\u (set of all TPs except\nthat for class u) will reduce FPk=u. If we consider that λ̂ is not Pareto efficient in the MOP in (9), then there is a λ0 such that (TP1(λ0), · · · , TPK(λ0)) dominates (TP1(λ̂), · · · , TPK(λ̂)); i.e. at least one of the objectives is improved by λ0 compared to that of λ̂. Since the expression in (8) increases as TPk increases,\nimplying that F̄ (λ0) > F̄ (λ̂); this contradicts the initial assumption that λ̂ maximizes F̄ .\nWe can also observe that the Pareto optimal set of (9) is contained in that of the MOP\nmax λ {p(y1|x1, λ), p(y2|x2, λ) · · · , p(yT |xT , λ)} (10)\nthis is because if we assume a λ that is Pareto optimal for (9) but not for (10), then we have a λ0 that improves at least one of the objectives in (10) without decreasing the others. This means the K -tuple (TP1(λ0), · · · , TPK(λ0)) dominates (TP1(λ), · · · , TPK(λ)), which contradicts the assumption that λ is Pareto optimal for (9). Hence the F̄ optimizer λ̂ is also Pareto optimal for (10)\nand therefore λ̂ is Pareto optimal for the MOP\nmax λ {log p(y1|x1, λ), · · · , log p(yT |xT , λ)} (11)\ngiven that log(.) is a strictly increasing function. The Pareto optimal set of (11) can be obtained by maximizing non-negative linear combinations of its objectives [31]. This means there is a set of weights (w1, · · · , wT ) such that\nλ̂F̄ = arg max λ ( T∑ t=1 wt log p(yt|xt, λ) )\n= arg max λ\n(L(λ,w)) (12)\nwhere the rightmost expression corresponds to the weighted log-likelihood as expressed in (6). Our work above expands the proof in [28] for binary classification to multiclass classification."
    }, {
      "heading" : "2.3 Weights Estimation",
      "text" : "Now we are interested in computing the set of weights w in dWCRF that maximizes the function F̄ . We use the previous result in (12), which indicates that the objective functions F̄ and weighted log-likelihood have gradients equal to zero at the optimal λ̂. We have the gradient of the function F̄\n∇λ{F̄ (λ̂)} = ∑ t:yt=1 ∂TP1 F̄ (λ̂)∇λp(yt|xt, λ̂) + · · ·+ ∑ t:yt=K ∂TPK F̄ (λ̂)∇λp(yt|xt, λ̂)\n(13)\nand the gradient of the log-likelihood function:\n∇λL(λ̂) = T∑ t=1\nwt\np(yt|xt, λ̂) ∇λp(yt|xt, λ̂) (14)\nwhere ∇λ{F̄ (λ̂)} = ∇λL(λ̂) = 0 at the optimal parameter λ̂. Considering the expression in (13) we can obtain the partial derivative:\n∂F (λ) ∂TPk = q\n[ d\ndTPk\n( TP1\nTP1 +N1 + FP1\n) + · · ·+ d\ndTPk\n( TPK\nTPK +NK + FPK )] (15)\nwhere q = 2/K and given that previously we have considered FPk to be a function of all TP s other than k, (15) can be expressed as:\n(16) ∂F (λ)\n∂TPk = q Nk + FPk (TPk +Nk + FPk)2\n+ q ∑\nj:1···K\\k\n−TPj dFPjdTPk (TPj +Nj + FPj)2\nwe consider that the derivative term in (16) are close to zero at the optimal (λ→ λ̂) and that the derivative is much smaller than its quadratic denominator term; hence we eliminate the summation term from (16), resulting in\n∂F (λ̂) ∂TPk ' q Nk + FPk (TPk +Nk + FPk)2 . (17)\nThe resulting weights for optimizing the weighted maximum likelihood and the corresponding expected overall F -score can now be defined as:\nwi =  p(yi = 1|xi, λ̂)∂TP1 F̄ (λ̂) if yi = 1\n· · · p(yi = K|xi, λ̂)∂TPK F̄ (λ̂) if yi = K.\n(18)\nWe also consider a parameter τ > 0 that corresponds to the number of times L is computed during the optimization process. We use this parameter to apply λ considerations for (17), by executing first homogeneous weights as in linear chain CRF. Hence the resulting weights have the form:\nwi,τ =\n{ q if the number interations < τ\nwi, as in (18) if the number interations ≥ τ. (19)"
    }, {
      "heading" : "2.4 Real Time Inference",
      "text" : "Usually, class inference process for CRF is performed for complete sequences of data where methods as the forward-backwards or Viterbi algorithms are applied to the test segment and complete sequence of labels is returned [32]. In a previous study, we have underscored the importance of real time prediction of activities [29] where we applied the belief propagation method to obtain the marginal probabilities of the last received observation; we use the current sensor observation and the information from the last inference made on the previous observation. This is given by the form:\n(20)m(yt) = 1\nZt exp (φ (yt, x))∑ yt−1 (exp (φ (yt−1, yt))m (yt−1))  where m(yt) is the marginal probability corresponding to the t\nth observation xt and Zt corresponds to the normalizing term so the marginals at a given time t sum to unity and prevents the occurrence of floating point underflow. The assigned label corresponds to the activity that has the highest marginal probability."
    }, {
      "heading" : "3 Experimental Studies",
      "text" : "This section presents the experimental framework and corresponding results. We evaluate our dWCRF method using datasets from two different human activity recognition approaches: i) using battery powered body worn sensors (BPBW); and ii) batteryless body worn sensors (BLBW)."
    }, {
      "heading" : "3.1 Problem Description",
      "text" : "Both scenarios, BPBW and BLBW, consider sequential data from the sensors; these are time series X = {xt}Tt=1, where xt ∈ Rd, and associated with a sequence of activity labels Y = {yt}Tt=1, where yt ∈ Y = {1 · · ·K}, and K is the number of activity labels to predict. In sequential data problems, we assume that sequences are i.i.d. from each other; that is, given the set of M training training labeled sequences D = {(Xm, Ym)}Mm=1, variables in (Xi, Yi) are independent of those in (Xj , Yj) for i 6= j. However, dependency relationships between variables in a sequence cannot be assumed. Hence, given a testing sequence T = {(X,Y )}, we are interested in predicting individual class labels ŷt for every individual observation xt using our trained dWCRF model."
    }, {
      "heading" : "3.2 Statistical Analysis",
      "text" : "In this study, we determine class specific performance measurements: true positives (TP ) are the correctly predicted activity labels. False positives (FP ) are those predicted labels that are misclassified, thus do not match with the ground truth. False negatives (FN) correspond to those ground truth classes that were missed. True negatives (TN) are those non-target (not intended) classes that were correctly identified by the system.\nIn addition, we evaluate the performance of each class k using the harmonic mean of Precision (Pr) and Recall (Re):\nPrecisionk(Pr) = TPk/(TPk + FPk) (21)\nRecallk(Re) = TPk/(TPk + FNk) (22)\nF-scorek = 2×Prk×Rek Prk +Rek =\n( 1\n2×Prk +\n1\n2×Rek\n)−1 . (23)\nIn the case of overall performance, we consider the average of the class-specific performance metrics i.e. F-scoreOverall = ∑ k F-scorek/|k|.\nNote we do not evaluate metrics depending on true negatives (TN) such as specificity [33, 34, 9] as specificity does not appropriately reflect the performance of the minority class. This is because TN considers all activities other than the target activity. For example TN of a minority class includes the TP of the majority class, producing high specificity values, giving an over optimistic measurement of performance.\nWe are interested in comparing the F -score results of our classifier with other classifiers. Therefore, we compare the significance between two classification results using a two-tailed independent t-test. A p-value (p) <0.05 is considered statistically significant.\nEvaluation of these metrics in the case of the BLBW datasets was performed using a 10-fold cross validation procedure, where each fold considered complete\nsequences of activities (a trial) of different people. We considered 6 folds for training 2 folds each for testing and validation. In the case of the BPBW datasets, these were evaluated using a 4-fold cross validation for each dataset, due to the reduced number of trials per dataset; we use two folds for training and one for testing and validation respectively."
    }, {
      "heading" : "3.3 Batteryless Body Worn Sensor Datasets (BLBW)",
      "text" : "These datasets were obtained in the context of a larger project by our research group directed at the ambulatory monitoring of hospitalized older patients to prevent falls [7].\nWe evaluate three case studies based on motion information from trialled healthy and hospitalized participants using the battery-less body worn sensor [35], shown in Figure 2(a). Trial participants were requested to perform a series of broadly scripted ADLs which included: i) Sitting on bed; ii) Sitting on chair; iii) Lying on the bed; and iv) Walking to the bed, chair or door. These represent the most likely activities performed in a hospital environment by older patients. A researcher, present during the trials, annotated the labels directly into the middleware for reference as ground truth.\nWe consider that posture transitions such as sit to stand and stand to sit are integrated into the ambulation or sitting movements as data collected during posture transitions is scarce as the movements are of short duration [9]. For example, we consider a participant starts ambulating as soon as the body is not in contact with the bed or chair. Ambulation, in this case, also includes standing and any movement the participant performs while walking around the room.\nTherefore, given the limitations imposed by the physical space and movement of our target demographics, the BLBW datasets consider four classes (K = 4) to distinguish whether a person is in or has exited a resting posture. These classes are: i) Sit-on-bed; ii) Sit-on-chair; iii) Lying; and iv) Ambulating; where class Ambulating includes all other movements associated with sitting on chair, bed or lying. Details regarding the sensor platform and the case studies are explained below."
    }, {
      "heading" : "3.3.1 Sensor Platform",
      "text" : "The participants wore a flexible Wearable Wireless Identification and Sensing Platform (W2ISP) device, developed by our team [35], over a garment on top of the sternum. The W2ISP, see Figure 2(a), and based on [36], encases a triaxial accelerometer (ADXL330) and a 16 bit microcontroller (MSP430F2132). The W2ISP is part of an emerging class of batteryless sensors. In particular, the W2ISP harvests its energy using the electromagnetic field illuminating the tag from RFID antennas, which also collect the W2ISP sensor data. The main motivation for using this passive (batteryless) device compared to using other battery powered sensors are twofold: i) the device requires no maintenance as it is battery free, lightweight, inexpensive and easy to replace; and ii) frail older people, especially those with conditions such as delirium or dementia, require easy-to-use equipment [37], and our proposed sensors’ development objective is to be inconspicuous to the user i.e. concealed in the clothes.\nWe collect tri-axial acceleration signals and the received signal strength indicator (RSSI) from the sensor signals. RSSI is used as a measure of relative distance to the antenna receiving a sensor observation, specially over a short distance as in our case studies. Moreover, because the device is passive, sensor observations are not regularly collected in time, and thus increasing the complexity of the problem (see Figure 3)."
    }, {
      "heading" : "3.3.2 Case Studies",
      "text" : "Case Study 1 Fourteen healthy older volunteers, with average age of 74.6 ± 4.9 years old, completed around five trials each, based on their ability and level of fatigue. Participants were allocated into two different room configurations: Room1 and Room2, each constituting a dataset with 4 and 3 antennas deployments, shown in Figure 2(b) and (c), respectively.\nCase Study 2 Twenty five hospitalized patients, with average age of 84.4 ± 5.3 years old, performed a short sequence of ADLs due to the frailty of the participants. The patients were trialled in their respective rooms, constituting dataset Room3. In this hospital room configuration, see Figure 2(d), the displayed measurements are approximate due to differences between rooms (single or double bed), and the bed and the chair were always next to each other.\nCase Study 3 This case study investigates the performance of our method under the conditions of reduced training data; we use one dataset, Room1 from Case Study 1, where data sequences are extracted to simulate datasets of increasingly reduced number of sensor observations."
    }, {
      "heading" : "3.3.3 Class Imbalance in Datasets",
      "text" : "Two main sources of imbalance affect our datasets. The first is the duration of different activities. This is expected as, for example, lying on bed is of longer duration than ambulating. The second source of imbalance is due to the passive nature of the W2ISP sensor; this affects the powering of the device and the regularity of sensor readings. Sensor positioning and proximity to RFID antennas, posture of the participant (causing occlusion) can also affect the powering of the sensor; moreover, these conditions can change from person to person and room to room.\nFrom the room settings, we can see that Room1 intends to collect sensor observations from the complete room, whereas Room2 and Room3 are focused on obtaining data from specific areas around the bed and chair while saving on hardware infrastructure. In Room3, the small dimensions of the path between bed and chair cause ambulation time to be minimal.\nAn illustration of sensor observations and the resulting data imbalance is shown in Figure 3. Data acquired from Room1, see Figure 3(a), indicates that sensor inter-reading times when the participant is sitting on bed range from 0.2 s to 1.3 s, from 0.5 s to 4.2 s when the person is ambulating and 0.5 s to 6 s when sitting on a chair. In addition, the first observation corresponding to Ambulating and Sit-on-chair are received after 0.7 s and 4.5 s respectively.\nIn the case of Room2 and Room3, shown in Figure 3(b) and (c), sensor observations from a person lying in bed (transparent-orange background) are more\nfrequently collected due to the location of the antennas when compared to a person sitting on bed (transparent-blue background) or ambulating (transparentred background).\nClass imbalance of the datasets are shown in Figure 4. Datasets Room1 and Room3 show similar imbalance where there is a dominant class (Lying), a second dominant class (Sit-on-bed with 28.9% and Sit-on-chair with 21.3% respectively) and the minority class Ambulating has the lowest proportion in both datasets. However, Room1, has more than double the number of sensor observation of Room3 (see Figure 4); and the minority class (Ambulating) for both datasets has the same number of observations. Dataset Room2 is almost dominated by one class (Lying) and the rest of the classes have decreasing values of participation with Ambulating the minority class (1.5%). This dataset has almost the same amount of sensor readings as Room3; albeit Room3 having collected data from more participants than the other datasets."
    }, {
      "heading" : "3.3.4 Feature Extraction",
      "text" : "From the three datasets we extract features representative of activities. We use a fixed time sliding window of duration of 4 s (referred henceforth as segment)\nfrom which we extract instantaneous sensorial data corresponding to the last received observation and contextual information associated with observations in the segment. Moreover, we also use inter-segment information to further capture trend changes of the sensor signals. We selected this window method and length as it is simple to deploy and performs as well as more complex windowing methods for feature extraction [29]. Hence, the feature vector is composed of three different types of features:\nInstantaneous Features These features are strictly obtained from the last received observation corresponding to the current performed activity, shown in Table 1, and the gender of the participant which is known.\nWe consider the body tilting angle α on the midsagittal plane towards the front or back of the participant from the vertical reference [33]. The angle α is approximated from current acceleration values α ≈ arctan ( af av ) ; however, we prefer sin(α) as it is proportional to α and range limited to [-1,1]. The value of RSSI is of interest as a reference of relative proximity to a surrounding antenna. We also consider the time difference (∆t) between sensor observations as in previous research [9]. Also included are the body rotation angles approximated from current acceleration readings: yaw = arctan( alaf ) and roll = arctan( al av ), as they carry information pertinent to body rotation movements.\nContextual Information Features These features, shown in Table 2, are extracted from each segment and provide general information of what is occurring during the segment duration, i.e. complementary information to the instantaneous features in the temporal vicinity of the last received sensor observation [38].\nWe include the basic contextual information of the number of events per antenna in the segment; we also include the identification of the antenna that registered the highest and lowest RSSI in the segment, which serve as a location marker as a participant is more likely to occupy an area near the antenna reporting higher RSSI during the segment duration. Other feature is the vertical displacement measured from acceleration readings in the vertical axis (av) in\nTable 2: Contextual information features.\nFeature Description\naSUM1..M number of events per antenna amaxRSSI antenna collecting maximum received power aminRSSI antenna collecting minimum received power V disp vertical displacement MIbed−chair mutual information of bed and chair areas r[fv,fl,vl] Pearson correlation coefficient for acceleration axes\nTable 3: Inter-segment features.\nFeature Description\n∆Max[af , av, al] Difference of acceleration maxima per axis ∆Min[af , av, al] Difference of acceleration minima per axis ∆Med[af , av, al] Difference of acceleration median per axis ∆MaxRSSI1..M Difference of power maxima per antenna ∆MinRSSI1..M Difference of power minima per antenna ∆MedRSSI1..M Difference of power median per antenna\nthe segment. The mutual Information between bed and chair areas considers the occurrences of consecutive observations from two antennas focused towards the chair and bed occurring in either directions as used in [29]. We also consider the Pearson correlation coefficient of all combinations of the three acceleration components of all observations in the segment.\nInter-Segment Features These features, shown in Table 3, aim to capture information trend variations from consecutive segments and are useful as these variations are insensitive to noise in unfiltered raw sensor data.\nWe include the difference of the maxima, minima and median of the segments’ acceleration readings in the three axes with respect to the participant: vertical, frontal and lateral. In addition, we are interested in the changes of RSSI, as an indicator of position shifting, given by the difference of the segments maxima, minima and median of the RSSI readings per antenna."
    }, {
      "heading" : "3.3.5 Parameter Selection",
      "text" : "We evaluate the performance of our dWCRF model together with linear chain CRF [6]; a weighted CRF with fixed weight values (fWCRF), given by the inverse of the class distribution [22]; and a cost parameter based CRF such as the softmax-margin model (C-CRF) [23] on all three datasets, we use the L2 regularized model for each classifier of the form θ‖λ‖2. Regularization parameter θ was evaluated in the range [10−4, 10−1]. Parameter τ was chosen from the range limited by the lowest number of iterations for linear chain CRF.\nIn addition, we also compare with multiclass SVM for linear and radial basis function (RBF) kernels (L-SVM and R-SVM respectively) and the weighted SVM for both classifiers (L-WSVM and R-WSVM) [39, 40, 41]. All classifiers were trained using the extracted features described in Section 3.3.4. Selection of hyperparameters for the SVM classifiers’ regularization, C, and RBF kernel, γ, were evaluated using a grid search in the range [2−5, 25] for both parameters. SVM algorithms were evaluated using libSVM [42] toolbox in Matlab, which performs a one-vs-one approach for multiclass classification. In the case of dWCRF, CRF, fWCRF, L-SVM and R-SVM, these are the only validated parameters; in all cases, the best set of parameters that produced the highest F -score was chosen.\nThe weight parameters for C-CRF, L-WSVM and R-WSVM, are found through cross-validation, evaluating the validation set. We note that for fWCRF, the weights are fixed and determined solely on the class distribution; and are not sought by any optimization method. In the case of C-CRF, the parameters are selected to optimize F -score as in [43]; we applied an extensive grid search to obtain the optimal parameters in the value range [0, 20]. In the cases of weighted SVM, the algorithms require a weight per class, K = 4 in our case, requiring a larger grid-search evaluation of the order of N4 processes, where N is the number of elements in the range to evaluate. Instead we use the covariance matrix adaptation evolution strategy (CMA-ES) [44], a widely used evolutionary optimization algorithm, to find the optimal set of per-class parameters for these classifiers. Given the stochastic nature of the initial parameter selection for the iterative CMA-ES process, we require evaluating multiple starting values; in our case, we evaluated 350 random initial points uniformly distributed in the range [0, 20] for each classifier.\nHyperparameter validation for the tested methods was performed on a cluster of Intel 8 core E5 series Xeon microprocessors due to the large number of processes to be performed."
    }, {
      "heading" : "3.3.6 Results",
      "text" : "First, we demonstrate the overall results corresponding to the datasets from Case Study 1 (Room1 and Room2) and Case Study 2 (Room3). These were obtained by averaging all participating classes’ individual F -score are shown in Figure 5. Maximum performance variations between methods’ average F -score is about 7%. For Room1, variation of our dWCRF with the best performing classifier, R-WSVM, is 4.6% (85.4% and 90% respectively). For Room2, the maximum F -score variation is ≈ 8% where the difference between our dWCRF (81.3%) and best performing R-WSVM (83.8%) is 2.5%. In Room3, the maximum F -score variation is ≈ 10%, where dWCRF is the best performing classifier (59.0%). In addition, overall dWCRF results are not statistically significantly different to those of all other algorithms; p-values for Room1 are p > 0.72, for Room2 are p > 0.67 and for Room3 are p > 0.53. Therefore, dWCRF performs as well as other classifiers; particularly in Room3 where dWCRF mean F -score was higher than other classifiers.\nRoom1 Room2 Room3\nSit-on-bed\n(a) (b) (c)\nSit-on-chair\n(d) (e) (f)\nLying-on-bed\n(g) (h) (i)\nAmbulation\n(j) (k) (l)\n1\nFigure 6: F-score performance of all classes and datasets. Results are shown as boxplots (averages shown as diamonds). Classifiers tested are: 1: dWCRF, 2: CRF, 3: fWCRF, 4: C-CRF, 5: L-SVM, 6: R-SVM, 7: L-WSVM, 8: R-WSVM.\nIn terms of individual classes from the datasets corresponding to Case Study 1 and Case Study 2, we observe differing behaviours. For Room1, see Figure 6(left column), SVM classifiers achieve statistically significantly better results than those of dWCRF (p ≤ 0.015). In particular, for the minority class (Ambulating), the difference in performance is ≈ 10% in comparison to R-WSVM (see Figure 6(j)). For the rest of activities the differences are < 5%. In comparison to other CRF based algorithms, dWCRF has, in general, higher results in terms of mean F -score with exception of class Sitting-on-chair, with difference of 2% (p > 0.1).\nThe results for Room2, shown in Figure 6 (middle column), demonstrate that all results are very close and no classifier is statistically significantly different from dWCRF (p ≥ 0.08) with exception of Ambulation with fWCRF where dWCRF is better with statistical significance (p = 0.003). For the minority class Ambulation, dWCRF mean performance is higher by 3% compared to both L-SVM classifiers and 0.5% compared to R-SVM classifiers. However, for the Sit-on-bed class, the R-WSVM result is higher by 7%. Similarly to Room1, dWCRF has, in general, higher results than the other CRF based classifiers (p ≥ 0.20).\nThe results for individual classes in Room3, shown in Figure 6 (right column), indicates that, for the minority class Sit-on-bed, dWCRF almost doubles the performance of R-WSVM with statistical significance (p = 0.018); and is higher than the other classifiers (p ≥ 0.08). For majority class Lying-on-bed dWCRF is statistically significantly better than fWCRF (p = 0.009). For the minority class Ambulation, dWCRF is lower than R-SVM (4% difference), but is not statistically significantly different with the other classifiers (p ≥ 0.19). Compared with the other CRF based classifiers dWCRF obtains better results for all classes.\nIn the case of Case Study 3, we consider the performance of the system when learning with limited data. For simplicity of dWCRF calculations, we consider parameter τ = 1. The results from Room1 in Figure 6 (left column) indicate that SVM based algorithms perform statistically significantly better than those of dWCRF (p ≤ 0.017). A probable reason is that Room1 contains more than double the information than the other datasets (Section 3.3.3), providing more than enough support vectors to perform reliable classification. We confirm our proposition by experimenting with Room1 dataset by repeatedly reducing one sequence of activities (or trial) from each fold in order to affect each fold evenly. This process also does not affect the class distribution of the remaining population as illustrated in Figure 7(a). Each reduced dataset is tested with the classifiers dWCRF, L-SVM, L-WSVM, R-SVM and R-WSVM. The overall performance, shown in Figure 7(b) where the x -axis displays the approximate number of sensor readings in thousands, indicates that the difference between classifiers reduces as the dataset reduces.\nThe results in Figure 7(b) indicate that the performance of SVM based classifiers do not vary between each other; moreover, the performance declines after the 38k population marker. In contrast, dWCRF performance remains almost unchanged for all population markers. The performance difference between SVM classifiers and dWCRF also reduces, we have that between 27k and 18k population markers, this difference is minimal, ≈ 4% and 2%, respectively. More importantly, the differences between classes are no longer statistically significant after the first reduction (46k) with p-values of p ≥ 0.51 for 46k, p ≥ 0.56\nfor 38k, p ≥ 0.41 for 27k and p ≥ 0.44 for 18k. Notice that sensor observation populations, for both Room2 and Room3, are within the population segment between 27k and 18k; and in these datasets, dWCRF results are shown to be similar or superior than all other classifiers. This confirms our dWCRF classifier performance being not significantly different than SVM based methods after reducing Room1 population levels to those of Room2 and Room3. We also note that Room3 dataset corresponds to a real world scenario with hospitalized patients in a hospital environment. Under these real world conditions, we gathered much fewer observations for Room3 than Room1 dataset, although having recruited 25 patients (more than the other two datasets) over a period of more than six months. In fact, under most practical situations, collecting and labelling data is difficult and cumbersome. Therefore the ability of our dWCRF classifier to learn from imbalanced data when available training data is scarce is a significant result.\nFurthermore, dWCRF has an added advantage over other weighted methods in that dWCRF does not require to search for optimal weights via validation. This makes it a faster approach than using a grid search for optimizing the weights or using techniques such as CMA-ES, which requires multiple training initializations while providing no guarantee to achieve optimal results; and where the complexity of optimization increases with the number of classes."
    }, {
      "heading" : "3.4 Battery Powered Body Worn Sensor Dataset (BPBW)",
      "text" : "To further validate the generalizability of our approach, we consider other datasets that showcase sequential information; specifically human activity recognition using wearable sensors. Unfortunately, few public datasets on human activity show data imbalance and have enough data samples to modify its levels of imbalance. We chose to use the Opportunity activity recognition dataset [45], available in the UCI repository. In this dataset, four participants with multiple sensors attached to their body and the environment perform multiple daily living activities. From the multiple sensor data and features generated (243), we only select those that are related to the trunk of the participant, i.e. sensors located on the hip and the back of the participant. This gives us only 19 sensor related features plus a time related feature.\nThese sensors are considered as these are the closest to our real scenario (hospital). Moreover, the dataset has modes of locomotion labels: Stand, Walk, Sit, Lie; similarly to the BLBW datasets, the selected torso-based features are used to predict these classes. There is also a null class which we consider as an additional class where we assume the participant is doing something other than the four basic locomotion activity labels. We also considered observations where readings from both sensors were present, otherwise the observation was discarded. Finally, we subdivided the data to 5 datasets, for two main reasons: i) Each dataset contains only one trial from each participant, i.e. we train and test the system using data from different participants; ii) The original set is large (≈ 610 000 observations), which is not realistic or convenient since our goal is to evaluate performance with cases of reduced and imbalanced data. Furthermore, the main objective of this test is not to compare to the established benchmarks, but to compare different methods in situations of high data imbalance not present in the original data.\nThe original low levels of imbalance are modified for all classes except the original majority class (Stand), in order to create imbalance levels similar and greater than to those of our study cases. In this case, we remove readings from the rest of classes on all sequences, i.e. data for training and testing have been similarly reduced. Three levels of data removal are used as shown in Figure 8(a), Op1: remove up to 9 of 10 consecutive sensor readings for each activity; Op2: remove up to 11 of 12 consecutive sensor readings for each activity; and Op3: remove up to 14 of 15 consecutive sensor readings for each activity."
    }, {
      "heading" : "3.4.1 Results",
      "text" : "We test the datasets with the same methods of Case Study 3 in Section 3.3.6. the results are shown in Figure 8(b)–(d). In general, no method is statistically significantly better than the rest with p ≥ 0.174 for Op1, p ≥ 0.137 for Op2 and p ≥ 0.251 for Op3. Nonetheless, the average overall F -score for our dWCRF is in general higher than all other methods, with the exception of LWSVM, which has higher overall F -score in 5 (DS3 and DS4 in Op1, DS3 in Op2, and DS3 and DS5 in Op3) of all imbalanced datasets cases. Moreover, for dWCRF the lowest class performance in all datasets was for the class Null; whereas for L-WSVM lowest class performance for most datasets was for class Lie, which is the minority class. These results suggest that despite high overall results, dWCRF struggles with identifying undefined classes as in these datasets.\nWeights optimization for WSVM methods using CMA-ES required 200 initialization processes each dataset (5 datasets) and imbalance case (3 cases); i.e. 3000 CMA-ES processes for each L-WSVM and R-WSVM methods. These results validate our method for sequential data with high imbalance and limited training data in terms of performance."
    }, {
      "heading" : "3.5 Empirical Comparison of Parameter Validation",
      "text" : "We use the BPBW datasets to illustrate the different training times (including parameter selection) of our proposed dWCRF and other methods. From the datasets used in the previous section, we randomly select one dataset from each imbalance case: DS5 from Op1, DS3 from Op2 and DS3 from OP3.\nThe resulting times are shown in Figure 9, where the time axis is in logarithmic scale, where time represents the total time taken to for evaluating the range of validation parameters. In this case, we evaluate dWCRF with one parameter to validate (regularization parameter θ in dWCRF1), and two parameters (θ\nand τ in dWCRF2) with cardinalities of 10 and 121 respectively. LSVM had one parameter to validate with a cardinality of 11. RSVM had two parameters to validate with a total combination cardinality of 121. L-WSVM and R-WSVM added also the times of 200 CMA-ES processes with random starting points, for each method, to validate the values of 5 weight parameters. It is clear that, increasing or decreasing the number of CMA-ES processes has an impact on the total time; however, a larger number of processes are necessary to obtain a closer to optimal result as a single CMA-ES process does not guarantee an optimal solution. We note in Figure 9 the considerable difference between methods and number of parameters; however, the time to validate parameters in dWCRF is lower than that of SVM methods. These results confirm the advantages of our method delivered in the model selection phase of the training process."
    }, {
      "heading" : "4 Conclusion",
      "text" : "The present study has established that dWCRF, using a class-wise cost parameter in the objective function, improves the overall F -score of our batteryless body worn sensor datasets when compared to other CRF based classifiers and performs similar or better than other SVM based classifiers in conditions where training data availability is reduced or collecting large datasets is difficult. Moreover, the developed approach also improves F -score performance metrics on minority classes. Our method was validated using a set of battery powered body worn sensor human activity recognition datasets in conditions of high imbalance and limited training data.\nThis study also presents a method to obtain a class-wise weighted classifier for optimizing the expected overall F -score from imbalanced multiclass data. In contrast to previous approaches our method dynamically calculates class-wise cost parameters, i.e. requires no previous knowledge of the data to assign cost parameter weights and does not need to be evaluated in the validation stage. In addition, our method obtained performance results comparable to other cost functions that optimize F -score such as Softmax-Margin [23] and other SVM based classifiers, but with an extensive reduction in learning time. This is be-\ncause Softmax-Margin function and the weighted SVM methods have more than two parameters to optimize; thus require an exponential amount of validation iterations in order to compare and obtain the best set of parameters. The BLBW datasets from old people provide heterogeneous data containing variations in age, health status, physical infrastructure and settings, which were compared using the same features; these datasets are scarce as most real-life application data. In contrast, most laboratory datasets are well controlled and show little data imbalance which is not representative of real-life conditions. In terms of future work, we are interested in testing our approach with other structured classification applications where available data is scarce and imbalanced; as well as investigating the effects of our approach into models of higher order."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This study was supported by the Australian Research Council (DP130104614). This study has approval by the Human Research Ethics Committee of the Queen Elizabeth Hospital (protocol number 2011129). We also want to thank Prof. Renuka Visvanathan, Mr. Stephen Hoskins and Dr. Shailaja Nair for their support in the selection and supervision of participants for the trials."
    } ],
    "references" : [ {
      "title" : "Learning from imbalanced data",
      "author" : [ "H. He", "E.A. Garcia" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, vol. 21, pp. 1263–1284, Sept 2009.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The class imbalance problem: A systematic study",
      "author" : [ "N. Japkowicz", "S. Stephen" ],
      "venue" : "Intelligent Data Analysis, vol. 6, no. 5, p. 429, 2002.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Epidemiology of falls in residential aged care: analysis of more than 70,000 falls from residents of bavarian nursing homes",
      "author" : [ "K. Rapp", "C. Becker", "I.D. Cameron", "H.H. König", "G. Büchele" ],
      "venue" : "Journal of the American Medical Directors Association, vol. 13, no. 2, pp. 187.e1–187.e6, 2012.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Elderly activities recognition and classification for applications in assisted living",
      "author" : [ "S. Chernbumroong", "S. Cang", "A. Atkins", "H. Yu" ],
      "venue" : "Expert Systems with Applications, vol. 40, no. 5, pp. 1662–1674, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Automated activity recognition and monitoring of elderly using wireless sensors: Research challenges",
      "author" : [ "D.C. Ranasinghe", "R.L. Shinmoto Torres", "A. Wickramasinghe" ],
      "venue" : "Fifth IEEE International Workshop on Advances in Sensors and Interfaces, pp. 224–227, 2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J.D. Lafferty", "A. McCallum", "F.C.N. Pereira" ],
      "venue" : "Proceedings of the 18th International Conference on Machine Learning, (USA), pp. 282–289, Morgan Kaufmann Publishers Inc., 2001.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Framework for preventing falls in acute hospitals using passive sensor enabled radio frequency identification technology",
      "author" : [ "R. Visvanathan", "D.C. Ranasinghe", "R.L. Shinmoto Torres", "K. Hill" ],
      "venue" : "2012 Annual Interna- 25  tional Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 5858–5862, 2012.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Low cost and batteryless sensor-enabled radio frequency identification tag based approaches to identify patient bed entry and exit posture transitions",
      "author" : [ "D.C. Ranasinghe", "R.L. Shinmoto Torres", "K. Hill", "R. Visvanathan" ],
      "venue" : "Gait & Posture, vol. 39, no. 1, pp. 118 – 123, 2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sensor enabled wearable rfid technology for mitigating the risk of falls near beds",
      "author" : [ "R.L. Shinmoto Torres", "D.C. Ranasinghe", "Q. Shi", "A.P. Sample" ],
      "venue" : "Seventh IEEE International Conference on RFID, pp. 191–198, 2013.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A study of the behavior of several methods for balancing machine learning training data",
      "author" : [ "G.E.A.P.A. Batista", "R.C. Prati", "M.C. Monard" ],
      "venue" : "SIGKDD Explorations Newsletter, vol. 6, no. 1, pp. 20–29, 2004.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Inverse random under sampling for class imbalance problem and its application to multi-label classification",
      "author" : [ "M.A. Tahir", "J. Kittler", "F. Yan" ],
      "venue" : "Pattern Recognition, vol. 45, no. 10, pp. 3738–3750, 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Handling imbalanced and overlapping classes in smart environments prompting dataset",
      "author" : [ "B. Das", "N.C. Krishnan", "D.J. Cook" ],
      "venue" : "Data Mining for Service (K. Yada, ed.), vol. 3 of Studies in Big Data, pp. 199–219, Springer, 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Why does rebalancing class-unbalanced data improve auc for linear discriminant analysis",
      "author" : [ "J.-H. Xue", "P. Hall" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, pp. 1109–1112, May 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning when data sets are imbalanced and when costs are unequal and unknown",
      "author" : [ "M.A. Maloof" ],
      "venue" : "ICML-2003 Workshop on Learning from Imbalanced Data Sets II, vol. 2, pp. 2–1, 2003.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Metacost: a general method for making classifiers costsensitive",
      "author" : [ "P. Domingos" ],
      "venue" : "Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, (USA), pp. 155–164, ACM, 1999.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "The influence of class imbalance on costsensitive learning: an empirical study",
      "author" : [ "X.-Y. Liu", "Z.-H. Zhou" ],
      "venue" : "Sixth International Conference on Data Mining., pp. 970–974, Dec 2006.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "On multi-class cost-sensitive learning",
      "author" : [ "Z.-H. Zhou", "X.-Y. Liu" ],
      "venue" : "Computational Intelligence, vol. 26, no. 3, pp. 232–257, 2010.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Dynamic class imbalance learning for incremental LPSVM",
      "author" : [ "S. Pang", "L. Zhu", "G. Chen", "A. Sarrafzadeh", "T. Ban", "D. Inoue" ],
      "venue" : "Neural Networks, vol. 44, no. 0, pp. 87 – 100, 2013.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Iterative bayes",
      "author" : [ "J. Gama" ],
      "venue" : "Theoretical Computer Science, vol. 292, no. 2, pp. 417 – 430, 2003. Theoretical Aspects of Discovery Science.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Cost-sensitive bayesian network classifiers",
      "author" : [ "L. Jiang", "C. Li", "S. Wang" ],
      "venue" : "Pattern Recognition Letters, vol. 45, no. 0, pp. 211–216, 2014. 26",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Weighted support vector machine for classification with uneven training class sizes",
      "author" : [ "Y.-M. Huang", "S. xin Du" ],
      "venue" : "Proceedings of 2005 International Conference on Machine Learning and Cybernetics., vol. 7, pp. 4365– 4369, Aug 2005.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Weighted conditional random fields for supervised interpatient heartbeat classification",
      "author" : [ "G. de Lannoy", "D. Francois", "J. Delbeke", "M. Verleysen" ],
      "venue" : "IEEE Transactions on Biomedical Engineering, vol. 59, no. 1, pp. 241–247, 2012.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Softmax-margin crfs: Training log-linear models with cost functions",
      "author" : [ "K. Gimpel", "N.A. Smith" ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, (USA), pp. 733–736, Association for Computational Linguistics, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Data mining for imbalanced datasets: An overview",
      "author" : [ "N.V. Chawla" ],
      "venue" : "Data Mining and Knowledge Discovery Handbook (O. Maimon and L. Rokach, eds.), pp. 853–867, Springer, 2005.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Entropy and margin maximization for structured output learning",
      "author" : [ "P. Pletscher", "C.S. Ong", "J.M. Buhmann" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases (J. L. Balczar, F. Bonchi, A. Gionis, and M. Sebag, eds.), vol. 6323 of Lecture Notes in Computer Science, pp. 83– 98, Springer, 2010.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A multi-objective optimisation approach for class imbalance learning",
      "author" : [ "P. Soda" ],
      "venue" : "Pattern Recognition, vol. 44, no. 8, pp. 1801 – 1810, 2011.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1801
    }, {
      "title" : "Classifying imbalanced data sets using similarity based hierarchical decomposition",
      "author" : [ "C. Beyan", "R. Fisher" ],
      "venue" : "Pattern Recognition, vol. 48, no. 5, pp. 1653 – 1672, 2015.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient f measure maximization via weighted maximum likelihood",
      "author" : [ "G. Dimitroff", "G. Georgiev", "L. Toloşi", "B. Popov" ],
      "venue" : "Machine Learning, vol. 98, no. 3, pp. 435–454, 2015.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Evaluation of wearable sensor tag data segmentation approaches for real time activity classification in elderly",
      "author" : [ "R.L. Shinmoto Torres", "D.C. Ranasinghe", "Q. Shi" ],
      "venue" : "Mobile and Ubiquitous Systems: Computing, Networking, and Services (I. Stojmenovic, Z. Cheng, and S. Guo, eds.), vol. 131 of Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering, pp. 384–395, Springer, 2014.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "An introduction to conditional random fields for relational learning",
      "author" : [ "C. Sutton", "A. McCallum" ],
      "venue" : "Introduction to Statistical Relational Learning (L. Getoor and B. Taskar, eds.), pp. 93–127, The MIT Press, 2006.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "An introduction to conditional random fields",
      "author" : [ "C. Sutton", "A. McCallum" ],
      "venue" : "Foundations and Trends in Machine Learning, vol. 4, no. 4, pp. 267– 373, 2012. 27",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Ambulatory system for human motion analysis using a kinematic sensor: monitoring of daily physical activity in the elderly",
      "author" : [ "B. Najafi", "K. Aminian", "A. Paraschiv-Ionescu", "F. Loew", "C. Bula", "P. Robert" ],
      "venue" : "IEEE Transactions on Biomedical Engineering, vol. 50, no. 6, pp. 711–723, 2003.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Activity classification using a single chest mounted tri-axial accelerometer",
      "author" : [ "A. Godfrey", "A.K. Bourke", "G.M. Ólaighin", "P. van de Ven", "J. Nelson" ],
      "venue" : "Medical Engineering & Physics, vol. 33, no. 9, pp. 1127–1135, 2011.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Wearable quarter-wave microstrip antenna for passive UHF RFID applications",
      "author" : [ "T. Kaufmann", "D.C. Ranasinghe", "M. Zhou", "C. Fumeaux" ],
      "venue" : "International Journal of Antennas and Propagation, vol. 2013, 2013.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Design of an rfid-based battery-free programmable sensing platform",
      "author" : [ "A.P. Sample", "D.J. Yeager", "P.S. Powledge", "A.V. Mamishev", "J.R. Smith" ],
      "venue" : "IEEE Transactions on Instrumentation and Measurement, vol. 57, no. 11, pp. 2608–2615, 2008.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Technology studies to meet the needs of people with dementia and their caregivers: A literature review",
      "author" : [ "P. Topo" ],
      "venue" : "Journal of Applied Gerontology, vol. 28, no. 1, pp. 5–37, 2009.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Activity recognition on streaming sensor data",
      "author" : [ "N.C. Krishnan", "D.J. Cook" ],
      "venue" : "Pervasive and Mobile Computing, vol. 10, pp. 138–154, 2014.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Machine Learning, vol. 20, no. 3, pp. 273–297, 1995.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A training algorithm for optimal margin classifiers",
      "author" : [ "B.E. Boser", "I.M. Guyon", "V.N. Vapnik" ],
      "venue" : "Proceedings of the Fifth Annual Workshop on Computational Learning Theory, (USA), pp. 144–152, ACM, 1992.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Another approach to polychotomous classification",
      "author" : [ "J.H. Friedman" ],
      "venue" : "tech. rep., Department of Statistics, Stanford University, 1996.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1–27:27, 2011. Software available at http://www.csie.ntu.edu. tw/~cjlin/libsvm.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Softmax-margin training for structured loglinear models",
      "author" : [ "K. Gimpel", "N.A. Smith" ],
      "venue" : "Tech. Rep. CMU-LTI-10-008, Carnegie Mellon University, 2010.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The CMA evolution strategy: A comparing review",
      "author" : [ "N. Hansen" ],
      "venue" : "Towards a New Evolutionary Computation (J. A. Lozano, P. Larrañaga, I. Inza, and E. Bengoetxea, eds.), vol. 192 of Studies in Fuzziness and Soft Computing, pp. 75–102, Springer, 2006.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Collecting complex activity data sets in highly rich networked sensor environments",
      "author" : [ "D. Roggen", "A. Calatroni", "M. Rossi", "T. Holleczek", "K. Förster", "G. Tröster", "P. Lukowicz", "D. Bannach", "G. Pirkl", "A. Ferscha", "J. Doppler", "C. Holzmann", "M. Kurz", "G. Holl", "R. Chavarriaga", "H. Sagha", "H. Bayati", "M. Creatura", "J. d. R. Millán" ],
      "venue" : "Seventh International Conference on Networked Sensing Systems, pp. 233–240, 2010. 28",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Imbalanced data can negatively affect the training of machine learning algorithms as the classifier can be biased to prefer the majority class [1, 2].",
      "startOffset" : 143,
      "endOffset" : 149
    }, {
      "referenceID" : 1,
      "context" : "Imbalanced data can negatively affect the training of machine learning algorithms as the classifier can be biased to prefer the majority class [1, 2].",
      "startOffset" : 143,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "For example, older people previously assessed as being at risk of falling performing a short duration ambulation—as opposed to large amounts of time spent in resting postures such as lying in bed—are potentially at a risk of falling and injury [3].",
      "startOffset" : 244,
      "endOffset" : 247
    }, {
      "referenceID" : 3,
      "context" : "This is indeed the case for applications such as monitoring patients in acute hospitals where collecting data to learn activities of hospitalized older people is very difficult due to physical limitations resulting from their older age and associated ailments[4, 5].",
      "startOffset" : 259,
      "endOffset" : 265
    }, {
      "referenceID" : 4,
      "context" : "This is indeed the case for applications such as monitoring patients in acute hospitals where collecting data to learn activities of hospitalized older people is very difficult due to physical limitations resulting from their older age and associated ailments[4, 5].",
      "startOffset" : 259,
      "endOffset" : 265
    }, {
      "referenceID" : 5,
      "context" : "fields (CRFs) [6], a graphical model for structured classification that captures dependency relationships between performed activities as described by sensor observations.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 6,
      "context" : "The study presented in this article is part of our ongoing research aimed at recognizing activities by older people in hospital and nursing home settings for falls prevention, as described in [7, 8, 9].",
      "startOffset" : 192,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : "The study presented in this article is part of our ongoing research aimed at recognizing activities by older people in hospital and nursing home settings for falls prevention, as described in [7, 8, 9].",
      "startOffset" : 192,
      "endOffset" : 201
    }, {
      "referenceID" : 8,
      "context" : "The study presented in this article is part of our ongoing research aimed at recognizing activities by older people in hospital and nursing home settings for falls prevention, as described in [7, 8, 9].",
      "startOffset" : 192,
      "endOffset" : 201
    }, {
      "referenceID" : 9,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 292
    }, {
      "referenceID" : 15,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 292
    }, {
      "referenceID" : 16,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 292
    }, {
      "referenceID" : 17,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 292
    }, {
      "referenceID" : 18,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 292
    }, {
      "referenceID" : 19,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 292
    }, {
      "referenceID" : 20,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 292
    }, {
      "referenceID" : 21,
      "context" : "This section reviews previous methods developed for improving the classification of imbalanced data, such as data re-sampling [10, 11, 12, 13], adjusting decision thresholds [14] or the inclusion of cost parameters or weights into the classification algorithm [15, 16, 17, 18, 19, 20, 21, 22].",
      "startOffset" : 260,
      "endOffset" : 292
    }, {
      "referenceID" : 9,
      "context" : "The main issue with re-sampling techniques [10, 11, 12] is that the removal or introduction of data can modify the sequence structure and its meaning.",
      "startOffset" : 43,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "The main issue with re-sampling techniques [10, 11, 12] is that the removal or introduction of data can modify the sequence structure and its meaning.",
      "startOffset" : 43,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "The main issue with re-sampling techniques [10, 11, 12] is that the removal or introduction of data can modify the sequence structure and its meaning.",
      "startOffset" : 43,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "Decision threshold methods such as that of [14] achieved similar results to re-sampling techniques and used receiver operating characteristic (ROC) curves to decide which decision threshold produces the best performance.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Generally, cost sensitive learning approaches have been reported to perform better than re-sampling techniques in some applications [1].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "Some cost parameters have the form of a cost matrix that weighs each possible misclassification case, giving higher costs to misclassifications of a minority class observation in comparison to majority classes [15, 16, 17].",
      "startOffset" : 210,
      "endOffset" : 222
    }, {
      "referenceID" : 15,
      "context" : "Some cost parameters have the form of a cost matrix that weighs each possible misclassification case, giving higher costs to misclassifications of a minority class observation in comparison to majority classes [15, 16, 17].",
      "startOffset" : 210,
      "endOffset" : 222
    }, {
      "referenceID" : 16,
      "context" : "Some cost parameters have the form of a cost matrix that weighs each possible misclassification case, giving higher costs to misclassifications of a minority class observation in comparison to majority classes [15, 16, 17].",
      "startOffset" : 210,
      "endOffset" : 222
    }, {
      "referenceID" : 16,
      "context" : "from a cost matrix; these methods are reported to work well in binary data and only in some multiclass cases [17].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "various reasons[17].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 20,
      "context" : "[21] introduced a fixed set of weights for each class for a binary SVM algorithm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20], weights calculated from the misclassification cost of each class were introduced into Bayesian network classifiers.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23], focused on improving the classification performance by modifying costs according to specific performance tasks (task-wise) such as improving recall, precision or both (as in F -score) [23] as opposed to classification error minimization as in previous studies.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23], focused on improving the classification performance by modifying costs according to specific performance tasks (task-wise) such as improving recall, precision or both (as in F -score) [23] as opposed to classification error minimization as in previous studies.",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 21,
      "context" : "The introduction of weights in CRF (WCRF) is not new; however, previous approaches only considered using a fixed set of weights during training for optimization [22]; however, finding an optimal set of weights [23, 20] require an extensive validation process.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : "The introduction of weights in CRF (WCRF) is not new; however, previous approaches only considered using a fixed set of weights during training for optimization [22]; however, finding an optimal set of weights [23, 20] require an extensive validation process.",
      "startOffset" : 210,
      "endOffset" : 218
    }, {
      "referenceID" : 19,
      "context" : "The introduction of weights in CRF (WCRF) is not new; however, previous approaches only considered using a fixed set of weights during training for optimization [22]; however, finding an optimal set of weights [23, 20] require an extensive validation process.",
      "startOffset" : 210,
      "endOffset" : 218
    }, {
      "referenceID" : 19,
      "context" : "These previously mentioned methods [20, 22, 23] require empirical calculation of parameters.",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 21,
      "context" : "These previously mentioned methods [20, 22, 23] require empirical calculation of parameters.",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : "These previously mentioned methods [20, 22, 23] require empirical calculation of parameters.",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "This is because the resulting measure, accuracy, is largely favoured by the dominant class and does not provide performance information regarding the predicted minority class [24].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : "[19] dynamically changed the cost of the naive Bayes classifier by verifying and introducing classification errors on each iteration; this method optimized the squared loss function as opposed to the 0–1 loss function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18], dynamically learned parameters were introduced into a binary linear proximal SVM (LPSVM) objective function where weights were proportional to the ratio of the other class population.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] presented a method for generalizing structured classifiers such as CRF and Structured SVM (SSVM) and introduced a cost parameter given by the Hamming distance between predicted output and ground truth which was dynamically calculated during training.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Although the methods in [18, 25] presented dynamically calculated cost parameters, the model optimization was based on classification error minimization.",
      "startOffset" : 24,
      "endOffset" : 32
    }, {
      "referenceID" : 24,
      "context" : "Although the methods in [18, 25] presented dynamically calculated cost parameters, the model optimization was based on classification error minimization.",
      "startOffset" : 24,
      "endOffset" : 32
    }, {
      "referenceID" : 25,
      "context" : "Other studies offered alternative methods [26, 27].",
      "startOffset" : 42,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : "Other studies offered alternative methods [26, 27].",
      "startOffset" : 42,
      "endOffset" : 50
    }, {
      "referenceID" : 25,
      "context" : "The method of Soda [26] decided between an unbalanced or a balanced classifier for every observation and measured its performance based on accuracy; while Beyan et al.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 26,
      "context" : "[27] proposed a hierarchical method based on clustering and outlier detection that, in general, was not significantly better than other methods.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[28] considered a learning algorithm for binary classification using a maximum likelihood model (weighted maximum entropy) to optimize the expected F -score during training where weights were calculated autonomously.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "Our study extends weighted maximum entropy [28], originally",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "We use CRF in order to model the dependency between consecutive activities described by sensor observations in the training sequences and applied in previous research [9, 29].",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 28,
      "context" : "We use CRF in order to model the dependency between consecutive activities described by sensor observations in the training sequences and applied in previous research [9, 29].",
      "startOffset" : 167,
      "endOffset" : 174
    }, {
      "referenceID" : 5,
      "context" : "In this section we briefly revisit CRFs, a probabilistic graphical model for structured classification [6, 30], as background to defining our method in Section 2.",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 29,
      "context" : "In this section we briefly revisit CRFs, a probabilistic graphical model for structured classification [6, 30], as background to defining our method in Section 2.",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 27,
      "context" : "We use the expected F -score (or F-measure) [28], as an optimization metric for our model as it considers FN and FP in its definition.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 27,
      "context" : "In [28], Dimitroff et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 27,
      "context" : "More information on Pareto efficiency can be found in [28, 31].",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "However, the approach followed in [28] was for a binary Maximum Entropy classifier.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : "Our work above expands the proof in [28] for binary classification to multiclass classification.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 30,
      "context" : "Usually, class inference process for CRF is performed for complete sequences of data where methods as the forward-backwards or Viterbi algorithms are applied to the test segment and complete sequence of labels is returned [32].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 28,
      "context" : "In a previous study, we have underscored the importance of real time prediction of activities [29] where we applied the belief propagation method to obtain the marginal probabilities of the last received observation; we use the current sensor observation and the information from the last inference made on the previous observation.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 31,
      "context" : "Note we do not evaluate metrics depending on true negatives (TN) such as specificity [33, 34, 9] as specificity does not appropriately reflect the performance of the minority class.",
      "startOffset" : 85,
      "endOffset" : 96
    }, {
      "referenceID" : 32,
      "context" : "Note we do not evaluate metrics depending on true negatives (TN) such as specificity [33, 34, 9] as specificity does not appropriately reflect the performance of the minority class.",
      "startOffset" : 85,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "Note we do not evaluate metrics depending on true negatives (TN) such as specificity [33, 34, 9] as specificity does not appropriately reflect the performance of the minority class.",
      "startOffset" : 85,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "These datasets were obtained in the context of a larger project by our research group directed at the ambulatory monitoring of hospitalized older patients to prevent falls [7].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 33,
      "context" : "We evaluate three case studies based on motion information from trialled healthy and hospitalized participants using the battery-less body worn sensor [35], shown in Figure 2(a).",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "We consider that posture transitions such as sit to stand and stand to sit are integrated into the ambulation or sitting movements as data collected during posture transitions is scarce as the movements are of short duration [9].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 33,
      "context" : "The participants wore a flexible Wearable Wireless Identification and Sensing Platform (WISP) device, developed by our team [35], over a garment on top of the sternum.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 34,
      "context" : "The WISP, see Figure 2(a), and based on [36], encases a triaxial accelerometer (ADXL330) and a 16 bit microcontroller (MSP430F2132).",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 35,
      "context" : "The main motivation for using this passive (batteryless) device compared to using other battery powered sensors are twofold: i) the device requires no maintenance as it is battery free, lightweight, inexpensive and easy to replace; and ii) frail older people, especially those with conditions such as delirium or dementia, require easy-to-use equipment [37], and our proposed sensors’ development objective is to be inconspicuous to the user i.",
      "startOffset" : 353,
      "endOffset" : 357
    }, {
      "referenceID" : 28,
      "context" : "We selected this window method and length as it is simple to deploy and performs as well as more complex windowing methods for feature extraction [29].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 31,
      "context" : "We consider the body tilting angle α on the midsagittal plane towards the front or back of the participant from the vertical reference [33].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "prefer sin(α) as it is proportional to α and range limited to [-1,1].",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "We also consider the time difference (∆t) between sensor observations as in previous research [9].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 36,
      "context" : "complementary information to the instantaneous features in the temporal vicinity of the last received sensor observation [38].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 28,
      "context" : "The mutual Information between bed and chair areas considers the occurrences of consecutive observations from two antennas focused towards the chair and bed occurring in either directions as used in [29].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 5,
      "context" : "We evaluate the performance of our dWCRF model together with linear chain CRF [6]; a weighted CRF with fixed weight values (fWCRF), given by the inverse of the class distribution [22]; and a cost parameter based CRF such as the softmax-margin model (C-CRF) [23] on all three datasets, we use the L2 regularized model for each classifier of the form θ‖λ‖2.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "We evaluate the performance of our dWCRF model together with linear chain CRF [6]; a weighted CRF with fixed weight values (fWCRF), given by the inverse of the class distribution [22]; and a cost parameter based CRF such as the softmax-margin model (C-CRF) [23] on all three datasets, we use the L2 regularized model for each classifier of the form θ‖λ‖2.",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 22,
      "context" : "We evaluate the performance of our dWCRF model together with linear chain CRF [6]; a weighted CRF with fixed weight values (fWCRF), given by the inverse of the class distribution [22]; and a cost parameter based CRF such as the softmax-margin model (C-CRF) [23] on all three datasets, we use the L2 regularized model for each classifier of the form θ‖λ‖2.",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 37,
      "context" : "In addition, we also compare with multiclass SVM for linear and radial basis function (RBF) kernels (L-SVM and R-SVM respectively) and the weighted SVM for both classifiers (L-WSVM and R-WSVM) [39, 40, 41].",
      "startOffset" : 193,
      "endOffset" : 205
    }, {
      "referenceID" : 38,
      "context" : "In addition, we also compare with multiclass SVM for linear and radial basis function (RBF) kernels (L-SVM and R-SVM respectively) and the weighted SVM for both classifiers (L-WSVM and R-WSVM) [39, 40, 41].",
      "startOffset" : 193,
      "endOffset" : 205
    }, {
      "referenceID" : 39,
      "context" : "In addition, we also compare with multiclass SVM for linear and radial basis function (RBF) kernels (L-SVM and R-SVM respectively) and the weighted SVM for both classifiers (L-WSVM and R-WSVM) [39, 40, 41].",
      "startOffset" : 193,
      "endOffset" : 205
    }, {
      "referenceID" : 40,
      "context" : "SVM algorithms were evaluated using libSVM [42] toolbox in Matlab, which performs a one-vs-one approach for multiclass classification.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 41,
      "context" : "In the case of C-CRF, the parameters are selected to optimize F -score as in [43]; we applied an extensive grid search to obtain the optimal parameters in the value range [0, 20].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : "In the case of C-CRF, the parameters are selected to optimize F -score as in [43]; we applied an extensive grid search to obtain the optimal parameters in the value range [0, 20].",
      "startOffset" : 171,
      "endOffset" : 178
    }, {
      "referenceID" : 42,
      "context" : "Instead we use the covariance matrix adaptation evolution strategy (CMA-ES) [44], a widely used evolutionary optimization algorithm, to find the optimal set of per-class parameters for these classifiers.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 19,
      "context" : "Given the stochastic nature of the initial parameter selection for the iterative CMA-ES process, we require evaluating multiple starting values; in our case, we evaluated 350 random initial points uniformly distributed in the range [0, 20] for each classifier.",
      "startOffset" : 232,
      "endOffset" : 239
    }, {
      "referenceID" : 43,
      "context" : "We chose to use the Opportunity activity recognition dataset [45], available in the UCI repository.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "In addition, our method obtained performance results comparable to other cost functions that optimize F -score such as Softmax-Margin [23] and other SVM based classifiers, but with an extensive reduction in learning time.",
      "startOffset" : 134,
      "endOffset" : 138
    } ],
    "year" : 2016,
    "abstractText" : "The present study introduces a method for improving the classification performance of imbalanced multiclass data streams from wireless body worn sensors. Data imbalance is an inherent problem in activity recognition caused by the irregular time distribution of activities, which are sequential and dependent on previous movements. We use conditional random fields (CRF), a graphical model for structured classification, to take advantage of dependencies between activities in a sequence. However, CRFs do not consider the negative effects of class imbalance during training. We propose a class-wise dynamically weighted CRF (dWCRF) where weights are automatically determined during training by maximizing the expected overall F-score. Our results based on three case studies from a healthcare application using a batteryless body worn sensor, demonstrate that our method, in general, improves overall and minority class F-score when compared to other CRF based classifiers and achieves similar or better overall and class-wise performance when compared to SVM based classifiers under conditions of limited training data. We also confirm the performance of our approach using an additional battery powered body worn sensor dataset, achieving similar results in cases of high class imbalance.",
    "creator" : "LaTeX with hyperref package"
  }
}