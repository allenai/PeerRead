{
  "name" : "1610.04783.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Similarity Learning for Time Series Classification",
    "authors" : [ "Maria-Irina Nicolae", "Éric Gaussier", "Amaury Habrard", "Marc Sebban" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The presence of time series in numerous fields of application makes them the object of considerable research effort for their classification or prediction. Classification for time series represents a challenging problem, with multiple applications in fields like speech recognition, energy consumption, object identification, bioinformatics, patient care, etc. To solve such tasks, one is inherently brought to compare time series by pairs, in order to determine their closeness or common patterns. However, time series coming from real applications are most of the time not directly comparable, because of the differences in length, phase or sampling frequency. An important subsequent task for solving the previous problems becomes finding the right alignment between time moments.\nDynamic time warping [13] is the most well-known algorithm for measuring the similarity between two time series by finding the best alignment between them. Its popularity is due to its capacity to work with series of varying lengths and phases, and its performance, usually much better than that of the Euclidean distance. The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21]. Most of these approaches are designed for univariate time series [12, 18], which record the value of only one feature per time moment. When dealing with multivariate time series, one way of using these methods is to weigh features equally, but that does not take into account the semantics of the features, nor the possible difference in scale. Metric learning [4, 23, 6, 24] can address exactly this problem as it allows one to learn the weights of features and the correlations between them from the available training data. This field is well developed for feature vectors, but the results concerning time series are scarce, mostly because of the complexity of the data. Moreover, metric learning methods based for time series do not come with any theoretical insurance of improving classification results once the learned metric is plugged into a machine learning algorithm.\nIn this paper, we address this double limitation by learning similarities for time series with generalization guarantees. Our method is based on the ( , γ, τ)-good similarities learning framework [2].The learned similarity function is used to induce a linear separator with good classification guarantees in the feature space. We prove that our method has uniform stability, which allows us to derive a generalization bound for both the learned metric and the classifier. To our knowledge,\nar X\niv :1\n61 0.\n04 78\n3v 1\n[ cs\n.L G\n] 1\n5 O\nct 2\n01 6\nthis is the first approach to provide theoretical guarantees for time series classification. We prove the efficiency of our method through an experimental study on UCI datasets [16].\nThe rest of the paper is organized as follows. Section 2 presents a brief overview of state of the art in metric learning and time series. Section 3 introduces the proposed similarity learning approach, while Section 4 is dedicated to theoretical results. In Section 5, we present an experimental study comparing our method to the state of the art."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we give an overview of some background knowledge on DTW, time series classification, metric learning and the ( , γ, τ)-good framework. For the rest of this paper, we shall refer to scalars in regular font (γ), vectors in bold lower case (x) and matrices in bold upper case (M).\nDynamic time warping [13] computes the optimal alignment between two time series under a metric by finding the pairs of time indices to align. This method was designed to solve the problem of comparing time series of different lengths and phases. The warping path found by DTW is computed w.r.t. a cost matrix (typically constructed with the Euclidean distance) in quadratic time through dynamic programming. Aligning two time series means finding all the matching time moments between them (Figure 1). The alignment is well constructed if all indices in both time series are used and the warping path is continuous and monotonically increasing. This implies that the first and last points are respectively aligned. To respect monotonicity, for each step of the alignment (i, j) there are only three subsequent moves possible: (i+ 1, j), (i, j + 1) or (i+ 1, j + 1).\nIn order to overcome the computational complexity of DTW, faster alternatives were introduced, like FastDTW [21] and SparseDTW [1]. Many variants were considered to constrain the global warping path, in order to speed up the algorithm and avoid pathological warping (e.g. aligning the beginning of a series with the end of another), of which we mention the Sakoe-Chiba band [20] and the Itakura parallelogram [8].\nMetric learning [14, 4] focuses on learning the parameters of a distance or similarity function from data. The learned metric is then used to solve the task at hand in the same way as with standard metrics. The most well-known parameterized distance used in metric learning is the Mahalanobis distance, defined for a pair of vectors x and y as dM(x,y) = √ (x− y)TM(x− y). M is the square positive semi-definite (PSD) matrix parameterizing the distance, whose entries we wish to learn from data. Notice that when M is the identity matrix, the metric becomes the standard Euclidean distance. The characteristics of the data are usually modeled as constraints from side information when learning the metric. More exactly, the two main approaches are pair-based constraints (two points are similar or dissimilar) and triplet-based constraints (a given point is more similar to one point than to the other). For (semi-)supervised tasks, it is straightforward to generate the constraints from class information.\nMetric learning for feature vectors has received important attention over the past years. Most of the methods are designed for nearest neighbor classification. Large Margin Metric Learning (LMNN) [23] and Information-Theoretic Metric Learning (ITML) [6] are probably the most well-\nknown methods for feature vectors. They learn a Mahalanobis distance from triplet, respectively pair constraints by enforcing an intuitive geometric criterion: bringing the points from the same class together, while pushing those from other classes away. ITML introduces for the first time LogDet divergence regularization, used later in several other distance learning methods.\nFor time series, the notion of learning a metric has mostly been used in the sense of learning the right alignment for univariate time series [7]. To our knowledge, learning a metric as a transformation in the features of time series has only been explored by few methods. In [15], the authors propose to learn a Mahalanobis metric for multivariate time series alignment of audio data. One significant limitation of their approach is that they consider the true alignment a priori known for their audio problem, information that is not available in most of the cases. Recently, LDMLT [17] was designed to learn a Mahalanobis distance for multivariate time series from triplet constraints. The method does so with an iterative approach that minimizes the loss of the triplets under LogDet regularization, to ensure the metric stays PSD. Experiments are performed for nearest neighbor and SVM classification. However, the loss function they use for the metric learning step is not related to the losses of the classifiers using it afterward. Moreover, neither the method from [15] nor LDMLT come with guarantees that learning the metric improves performance for the given task.\n\"Good\" similarity functions The ( , γ, τ)-good framework is one of the first to relate the characteristics of a similarity function based on non necessarily PSD matrices to its performance in classification. For this, they define the notion of \"goodness\" for a similarity function. Consider a binary classification setting over labeled examples (x, l) coming from a distribution P over X × {+1,−1}. The hinge loss is defined as [1− c]+ = max(0, 1− c).\nDefinition 1. [2] K : X × X → [−1, 1] is a ( , γ, τ)-good similarity function in hinge loss for a learning problem P if there exists a random indicator function R(x) defining a probabilistic set of \"reasonable points\" such that the following conditions hold: 1. We have E(x,l)∼P [ [1− lg(x)/γ]+ ] ≤ , where g(x) = E(x′,l′),R(x′) [l′K(x,x′)|R(x′)]. 2. Prx′(R(x′)) ≥ τ .\nThis definition is based on a set of reasonable points, that are used to create the feature space. In practice, these points are obtained by drawing from P an (unlabeled) sample L = {x′1,x′2, . . . ,x′du} of du random \"landmarks\". The first condition of the definition imposes that an (1− ) proportion of examples x should be on average 2γ more similar to reasonable examples x′ of their own label than to random reasonable examples of the other label. The margin violations are averaged over all reasonable points, which is easier to satisfy than pair- or triplet-based constraints, as required by LMNN or ITML. The second condition sets the minimum number of reasonable points to a proportion of τ . In this definition, nothing is said about the form of the similarity function, so it is generic. Definition 1 can be used to learn a linear separator from an ( , γ, τ)-good similarity:\nTheorem 2. [2] Let K be an ( , γ, τ)-good similarity function in hinge loss for a learning problem P. For any 1 > 0 and 0 < δ < γ 1/4 let L be a sample of du = 2τ ( log(2/δ) + 16 log(2/δ)( 1γ)2 ) (unlabeled) landmarks drawn from P. Consider the mapping φL : X → Rdu , φLi (x) = K(x,x′i), i ∈ {1, . . . , du}. With probability 1 − δ over the random sample L, the induced distribution φL(P) in Rdu , has a separator achieving hinge loss at most + 1 at margin γ.\nIn other words, if K is ( , γ, τ)-good according to Definition 1 and enough data is available, there exists a linear separator α with error arbitrarily close to in the space φL. Given a labeled learning sample of size dl, the separator is found by solving the following linear program:\nmin α { dl∑ i=1 [ 1− du∑ j=1 αj liK(xi,xj) ] + : du∑ j=1 |αj | ≤ 1/γ } . (1)\nAs the problem is L1-constrained, tuning the value of γ may produce a sparse solution. This formulation is equivalent to a relaxed L1-norm SVM [25]. Lastly, the associated classifier takes the following form:\ny = sgn du∑ j=1 αjK(x,xj). (2)\nThe main limitation of this approach is however that the similarity function K is supposed known, and they do not provide a way to design such similarities. This issue has been addressed in [3] only for feature vectors. The objective of this paper is to provide a solution in a more complex setting of time series."
    }, {
      "heading" : "3 Similarity Learning for Time Series Classification",
      "text" : "This section presents the proposed method for learning temporal similarity functions. We start by defining the similarity to be used with time series, then present the method for learning it. Let A ∈ RtA×d be a multivariate time series of length tA and dimension d. We denote by X the space of all time series of finite length. Now consider the following binary classification problem: we are given labeled multivariate time series (A, l) drawn from a distribution P over X × {+1,−1}, possibly of different lengths, but of same dimension d."
    }, {
      "heading" : "3.1 Bilinear Similarity for Time Series",
      "text" : "For a pair of time series A and B, let CM(A,B) ∈ RtA×tB be a pairwise matrix of the cost of aligning a time moment in A to one in B under the metric parameterized by the matrix M. As we use a similarity function, CM(A,B) represents the affinity scores that we want to maximize instead of the cost to be minimized. We refer to the rows of A as a1, . . . ,atA and those of B as b1, . . . ,btB . Without loss of generality, the data is normalized as ||ai||2 = 1, i ∈ {1 . . . tA},∀A ∈ X . We will focus on an affinity matrix of form:\nCM(A,B)i,j = a T i ·M · bj ,\nwhere M is the matrix parameterizing the metric. For the pair of indices i and j, the affinity is equivalent to computing the generalized cosine similarity [19], as ai and bj are already normalized. The same operation can be written using only matrices:\nCM(A,B) = A ·M ·BT .\nCM can be used to compute the alignment between two time series with DTW. Given this affinity matrix, let Y ∈ {0, 1}tA×tB be a binary matrix encoding an alignment between A and B: YijA,B = 1 if the time moment i from A is aligned with the moment j from B and zero otherwise. The length of the alignment is noted tAB. Computing the score of aligning A and B from the affinity matrix and the alignment can be written as the following similarity function:\nKM(A,B) = Tr(CM(A,B) T ·YAB)/tAB\n= Tr(B ·MT ·AT ·YAB)/tAB.\nWhen computing the product between the affinity matrix and the alignment, the scores of the pairs of points that are aligned end up on the main diagonal of the resulting matrix. Applying the trace operator sums only these diagonal values, while discarding the others. As the value of the similarity is cumulative, we normalize it w.r.t. the length of the alignment in order to remove the bias created by very long alignments. Using KM as similarity function to compare multivariate time series allows us to take advantage of the ideal alignment, while considering an advantageous weighting of the features and cross-features for each time moment. An important property is that the metric matrix M does not have to be PSD. We shall now discuss a method for learning M from data."
    }, {
      "heading" : "3.2 Learning Good Similarities",
      "text" : "Our objective is to learn the matrix M that parameterizes the similarity function KM for usage in classification. For this, we dispose of a training set S of m time series {(Ai, li)}mi=1 drawn accordingly to P and a set L of n landmarks {(Bj , l′j)}nj=1 from the same distribution. We want to optimize the ( , γ, τ)-goodness of the proposed similarity function as presented in Definition 1:\nE(A,l) [[ 1− E(B,l′),R(B) [ll′KM(A,B))|R(B)] /γ ] + ] ≤ .\nAs this criterion is defined over true expected values, we shall improve its empirical version instead. When optimizing the goodness criterion, we do so w.r.t. the set of landmarks L. We assume for now that they are fixed. Notice that two heuristics for choosing them from data are discussed in the supplementary material. Learning the similarity w.r.t. Definition 1 is equivalent to learning the entries of the matrix M that parameterizes it and is done by solving the following optimization problem over M:\nmin M\n1\nm ∑ (A,l)∈S 1− 1 nγ n∑ j=1 ll′j KM(A,Bj)  + + λ||M||2F . (3)\nNotice that the similarity function KM is linear in M. Problem (3) is thus convex and can easily be solved. In order to avoid overfitting, the objective function is regularized with the squared Frobenius norm of the matrix M. Using this regularizer will allow us to provide theoretical guarantees for the proposed approach through uniform stability. Tuning the regularization parameter λ controls the tradeoff between fitting the data and limiting the complexity of the hypothesis. We call the proposed method Similarity Learning for Time Series (SLTS). After solving Problem (3), KM is plugged in Equation (1) in order to learn the linear separator α. Having a formulation based on landmarks implies that the value of the similarity function (and indirectly of the alignment using DTW) only needs to be computed for the data points w.r.t. the set of landmarks. As computing KM is expensive, the lower the number of landmarks, the faster the computation."
    }, {
      "heading" : "4 Theoretical Guarantees",
      "text" : "Learning the metric by solving Problem (3) places our approach in the ( , γ, τ) framework, which enforces the theoretical guarantees from Theorem 2 for the learned classifier. In this section, we derive a generalization bound for SLTS using the notion of uniform stability [5]. This bound provides a link between the empirical loss we are minimizing under regularization in Equation (3) and the value we want to minimize, the true loss. We start by making the following notations. Let the empirical loss function for an example (A, l) ∼ P be\n`(M, (A, l)) = 1− 1 n n∑ j=1 ll′j KM(A,Bj)/γ  + .\nAccording to Problem (3), SLTS minimizes the empirical risk of the learned matrix M over the whole training set S:\nÊS(M) = 1\nm ∑ (A,l)∈S `(M, (A, l)).\nAccording to Definition 1, the error that the algorithm should minimize is the true expectation:\nEP(M) = E(A,l)∼P [`(M, (A, l))] .\nWe will denote by Si the training set obtained from S by replacing the ith example with a new one coming from the same distribution. We now define the uniform stability of an algorithm.\nDefinition 3 (Uniform stability [5]). A learning algorithm has a uniform stability in κm , with κ ≥ 0 constant, if ∀i,\nsup (A,l)∼P |`(M, (A, l))− `(Mi, (A, l))| ≤ κ m ,\nwhere M is the metric learned on the training set S, and Mi is the metric learned on Si.\nUniform stability ensures a certain robustness of the learned metric w.r.t. small variations in the training set. This property enables us to derive a generalization bound on the true error of an algorithm.To prove the stability of SLTS, we first need to show that the considered loss function is bounded and k-lipschitz: the smaller k, the more stable the algorithm. We do so in Lemmas 4 and 5. All the proofs of the Lemmas in this section are presented in the supplementary material.\nLemma 4 (Bound on the loss function). Let (A, l) be an example and M the minimizer of Problem (3). Then\n`(M, (A, l)) ≤ √ 2d\nγ √ λ .\nLemma 5 (k-lipschitz continuity). Let M and M′ be two matrices and (A, l) an example. The loss function ` is k-lipschitz with k = √ 2d γ such that:\n|`(M, (A, l))− `(M′, (A, l))| ≤ k||M−M′||F .\nThe property of k-lipschitzness implies that the loss variation is proportional to the difference between M and M′. We can now prove that our approach has uniform stability.\nLemma 6. Given a training sample S of m examples drawn i.i.d. from P, our algorithm SLTS has uniform stability in κ/m with κ = 4dγ2λ .\nHaving now shown the uniform stability of SLTS, we are ready to derive the generalization bound. For this, Lemmas 7 and 8 are necessary, providing bounds on quantities that intervene in the proof of the bound. Let RS = EP(M)− ÊS(M). We need to bound the quantities ES [RS ] and |RS −RSi |.\nLemma 7. For a learning method of estimation error RS and satisfying a uniform stability of κ/m, we have:\nES [RS ] ≤ κ\nm .\nLemma 8. For any metric M learned by solving Problem (3) on a training set S of m samples, and a loss function ` bounded according to Lemma 4, we have:\n|RS −RSi | ≤ 2κ\nm +\n√ 2d\nmγ √ λ .\nTheorem 9 (Generalization bound). With probability 1− δ, for any matrix M learned by solving Problem (3), we have:\nEP(M) ≤ ES(M) + 4d\nγ2λm +\n( 4d\nγ2λ +\n1\nγ\n√ 2d\nλ )√ 2 log 2δ m .\nProof. Using McDiarmid’s inequality and Lemma 8, we can write:\nPr[RS − E[RS ] ≥ ] ≤ 2 exp ( − 2 2\nm ( 2κ+p m\n)2 ) . (4)\nBy setting δ = 2 exp ( − 2 2\nm( 2κ+pm ) 2\n) in Inequality (4), we obtain:\n= √√√√ 2 m ( 4d γ2λ + 1 γ √ 2d λ )2 log 2 δ .\nThen, with probability 1− δ,RS = E(M)− ÊS(M) < E[RS ] + ⇐⇒ EP(M) < ÊS(M) + κm + . Replacing the values of κ and in the previous inequality yields the bound.\nThe result from Theorem 9 shows the consistency of the proposed similarity learning approach. The bound converges with a standard rate of 1/ √ m in the number of samples. According to [22], the presence of the number of features d in the numerator of the bound is to be expected and shows that the approach may suffer from the curse of dimensionality. High values of d can be compensated by increasing either the size of S, or the value of the regularization parameter λ, present in the denominator. SLTS minimizes the empirical error of the ( , γ, τ) framework, thus reducing the error rate . By plugging the metric learned by SLTS into the framework, we obtain a guarantee on the performance of the associated classifier."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we present the results of the experiments conducted to evaluate the performance of the proposed method. In the first experiment, we show that learning the matrix M brings additional information for linear classification. We also analyze the influence of the number of landmarks on SLTS. The second study provides a comparison of SLTS to the state-of-the-art algorithms while the third part illustrates the capacity of SLTS to learn a discriminant metric in the feature space created by the landmarks. Finally, we provide a discussion over the choice of landmarks, followed by an additional experiment meant to compare a few of heuristics for landmarks selection. We conduct the experimental study on multivariate time series datasets coming from UCI Machine Learning Repository [16], containing between 47-8800 instances. We start by giving the description of the datasets used for the experiments in Table 1. In the case of Auslan, we only use the 25 first classes instead of the total of 95, as done in precedent studies [17]. The dataset Robot execution failure contains five subtasks (LP 1-5), that are treated separately.\nWe compare our method against the following classic algorithms:\n• Standard nearest neighbor classifier (1NN);\n• Linear SVM under L2 regularization;\n• Linear classifier from [2], presented in Equation (2) (called BBS from now on);\n• LDMLT [17] with a nearest neighbor classifier;\n• SLTS, the similarity learning method proposed in this chapter, which is then used to learn a global linear classifier using the formulation in [2].\nTo propose a fair comparative study, all the methods that do not learn a metric use the proposed bilinear form as similarity function (with M set to the identity matrix) computed with the DTW alignment on the scalar product. As confirmed by the experiment presented in the supplementary material, landmarks are randomly chosen for BBS and SLTS. We use all the classifiers in their binary version, in a one-vs-rest setting. We recall here that each time moment is normalized to ensure the L2 norm equals 1. For this experimental study, we have access to a standard training/test partitioning for Japanese vowels and Arabic digits datasets, while Robot execution failure (LP1-LP5) and Auslan are randomly split to 70% training/30% test data. For all datasets, we retain 30% of the training set for hyperparameter tuning. We perform experiments on 10 different splits and present the average result with a 95% confidence interval. Cross-validation is performed to tune the following parameters: C ∈ {2−6, . . . , 29} for SVM, γ ∈ {10−4, . . . , 101} for BBS, both when used separately or joint to SLTS, and λ ∈ {0.1, 1, 10} for SLTS.\nBehavior of SLTS and impact of the number of landmarks We (i) show that SLTS improves linear classification compared to BBS and (ii) analyze the influence of the quantity of landmarks on the accuracy obtained for BBS and SLTS. We consider the range of up to 50% of the size of the training set as landmarks for small datasets, or up to 100 landmarks for the others. The\nresults of this study are presented in Figure 2. The accuracy of SLTS is almost always higher than that of BBS, showing the improvement that can be obtained through similarity learning. When a reasonable quantity of data is available (Figures 2(a)-2(c)), SLTS achieves a performance close to its best value even with a few landmarks, thus performing well even with a low quantity of data. Overall, BBS has difficulties providing a good classifier based on a small number of landmarks, but the results of the method significantly improve with more landmarks. We explain the high variability of the results of BBS and SLTS on LP1-LP5 by the small sizes of the tasks.\nClassification performance comparison The results of the comparison of SLTS and BBS with other methods are displayed in Table 2 (both SLTS and BBS are based on the maximum number of landmarks from the previous experiment). No confidence interval in the table values means that the train/test split of the data is already provided, and the output of the method is deterministic. As one can note, among global methods relying on a linear classification (i.e., SLTS, BBS, and SVM), both SLTS and BBS perform better than SVM (they are on a par on Japanese vowels, slightly below on Auslan, and above on Arabic digits and Robot exec. failure). Using a Student t-test for paired samples on the average reveals that SLTS is significantly better than BBS and SVM. This shows the usefulness of the ( , γ, τ)-good framework as well as the importance of metric learning in this framework. The comparison of SLTS with local methods (as 1NN and LDMLT) yields more contrasted results. On all datasets except Robot exec. failure, 1NN is significantly below SLTS according to a Student t-test. However, compared to LDMLT, SLTS is on a par on Japanese vowels, below on Auslan and Robot exec. failure, and above on Arabic digits (a Student t-test on the average does not reveal any significant difference between the two methods). LDMLT relies on both a local method and a metric learned, which suggests again that learning a metric is beneficial on these datasets. This said, LDMLT learns a distance, whereas all the other methods rely on a similarity. The comparison between the two should thus be taken with caution as distances and similarities can yield very different results [19].\nVisualization of the similarity space To illustrate the transformation induced in the feature space by learning the metric, we propose a visualization experiment on the Japanese vowels dataset using 10 landmarks chosen randomly. We compute the value of the similarity function KM for all the data w.r.t. the landmarks, first without metric learning (M = I), then with the metric learned for each of the 9 classes. In all the cases, we apply PCA to the values of the similarity function and plot the first two components. We thus obtain a 2D representation of the feature space, of which we present in Figure 3 the case of the initial feature space and that of the metric learned for the first three classes. In similarity space with no metric learning (Figure 3(a)), all the data points are mixed, independently of their label. In Figures 3(b)-3(d), each metric linearly separates the class it has learned to discriminate from the others. For the learned similarities, the first two components of PCA explain around 98% of the variance, while with no similarity learning this value is around 86%. This study proves that learning an ( , γ, τ)-good similarity function changes the representation space towards better class discrimination, making it suitable for learning a large margin linear separator.\nHeuristics for choosing the landmarks We have previously assumed we have access to a set of landmarks for the construction of the feature space. We will now discuss two heuristics for choosing the most representative points in the training set as landmarks, before presenting\nexperimental results concerning the performance of each of these methods. K-Medoids [11] is a classical clustering technique. The resulting medoids representing the clusters are points of the initial dataset, that will be subsequently used as landmarks. Dselect [10] was proposed as a landmarks selection algorithm that optimizes a criterion of diversity. Starting with a randomly chosen landmark, at each iteration the algorithm greedily adds to the set of landmarks the training point that is least similar to the ones already selected. Note that for both selection heuristics the number of landmarks needs to be set in advance. Also, none of these methods exploits the information from the labels of the time series. In the case where no prior information is available for the classification task, the set of landmarks can also be selected randomly from the training set, with the risk of relying upon non informative landmarks.\nWe now present in Tables 3 and 4 the classification results after learning the similarity with SLTS on landmarks selected using the presented heuristics. DSelect and KMedoids are compared against landmarks selected randomly as baseline, in order to determine if they are indeed informative. We perform these experiments on two small datasets, Japanese vowels and LP1. The mass of chosen landmarks is selected as a percentage of the total size of the training set and goes up to 50%. For Japanese vowels (Table 3), all three methods perform almost the same for all amounts of landmarks. DSelect reaches its best performance when the selected points represent 10-20% of the training set, while KMedoids works best around 30% mass of landmarks. Overall, DSelect and Random heuristics yield better performance than KMedoids. The results using Random show that SLTS can learn well ever when no computational effort is put into choosing the landmarks. In the case of LP1 (Table 4), and in contrast to the Japanese vowels dataset, the performance of all the heuristics improves when increasing the number of landmarks. The best results are obtained for 40% mass of landmarks in the case of DSelect and Random, and 50% for KMedoids. For this dataset, the results are less stable, inducing larger confidence intervals. For this reason, even though the best accuracy is attained by DSelect, its improvement over Random is not necessarily significant. KMedoids is this time also the least performant heuristic.\nKMedoids and DSelect have by themselves a computational complexity that is not to be ignored when working on large datasets. Even so, their main disadvantage for time series is not the algorithmic complexity in itself, but the necessary precomputations. One needs to compute the value of the similarity function for all pairs of time series, including the alignment, in order to be able to apply these heuristics. This limitation goes directly against the main advantage of working with methods based on landmarks, like SLTS. In view of this aspect and the previous experimental results, we have only considered the Random heuristic when comparing SLTS against state of the art algorithms on bigger datasets."
    }, {
      "heading" : "6 Conclusion and Perspectives",
      "text" : "In this paper, we address the problem of learning a global linear classifier for multivariate time series through similarity learning. We propose a bilinear similarity function that takes into account the optimal alignment. Our method comes with a generalization bound on the error of the metric and of the classifier and is the first to provide classification performance guarantees for the learned\nsimilarity in the case of time series. The experimental study proves the usefulness of the ( , γ, τ)good framework as well as the importance of metric learning in this setting. Future work should include learning Mahalanobis metrics, as suggested by the results of LDMLT. We also plan on trying to capture local temporal information by learning multiple metrics, as well as on studying the impact of different regularizers on the matrix M."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Funding for this project was provided by a grant from Région Rhône-Alpes."
    }, {
      "heading" : "A Proofs of Lemmas",
      "text" : "This section contains the proofs of Lemmas 4 to 8, as well as defining some additional lemmas necessary for these proofs.\nTo prove Lemma 4 from the paper, we need two additional lemmas. Lemma A bounds the Frobenius norm of the learned matrix M, while Lemma B puts a bound on the Frobenius norm of a subpart of the similarity function. For the sake of clarity, lemmas introduced in this supplementary material are numbered with capital letters.\nLemma A. If M is the optimal solution of Problem (3), we have:\n||M||F ≤ 1√ λ .\nProof. Since M is the optimal solution of Problem (3), we have:\nRS(M) ≤ RS(0) 1\nm ∑ (A,l)∈S `(M, (A, l)) + λ||M||2F\n≤ 1 m ∑ (A,l)∈S `(0, (A, l)) + λ||0||2F\nλ||M||2F ≤ 1\nm ∑ (A,l)∈S `(0, (A, l)) (5)\nλ||M||2F ≤ 1 (6)\n||M||F ≤ 1√ λ\nInequality (5) is a result of the fact that the hinge loss is always positive, while Inequality (6) comes from noting that the loss is bounded by 1/m when the metric is set to zero.\nLemma B (Technical lemma). Let A ∈ RtA×d and B ∈ RtB×d be two examples, and YAB ∈ {0, 1}tA×tB of length tAB. Then\n||AT ·YAB ·B||F ≤ tAB √ 2d.\nProof.\n||AT ·YAB ·B||F\n= √√√√√ d∑ i=1 d∑ t=1  tA∑ j=1 tB∑ k=1 aijyjkbkt 2\n= √√√√√ d∑ i=1 d∑ t=1 2 tA∑ j=1 tB∑ k=1 tA∑ j′=1 tB∑ k′=1 (aijyjkbkt)(aij′yj′k′bk′t)− ∑ jk (aijyjkbkt)2 \n≤ √∑\ni,t 2 ∑ j,k ∑ j′,k′ |(aijyjkbkt)(aij′yj′k′bk′t)|\n= √ 2 ∑ j,k yjk ∑ j′,k′ yj′k′ + ∑ i,t aijaij′bktbk′t\n≤ √ 2t2AB| ∑ i aijaij′ ∑ t bktbk′t|\n= √ 2t2AB maxi aij ∑ i |aij |max t bkt ∑ t |bkt|\n= √\n2t2AB maxi aij ||ai||1 max t bkt||bk||1 ≤ √ 2t2AB √ d||ai||2 √ d||bk||2\n≤ √\n2t2AB · d\n= tAB √ 2d.\nWe are now able to present the proof of Lemma 4:\nProof of Lemma 4.\n`(M, (A, l)) = 1− 1 n ∑ (B,l′)∈L ll′KM(A,B)/γ  +\n≤ ∣∣∣∣∣∣l 1n ∑\n(B,l′)∈L\nl′KM(A,B)/γ ∣∣∣∣∣∣ (7) ≤ 1 n ∑ (B,l′)∈L |l′KM(A,B)/γ| (8)\n≤ 1 nγ ∑ (B,l′)∈L ∣∣Tr(MTATYABB)/tAB∣∣ ≤ 1 nγ ∑ (B,l′)∈L 1 tAB ||M||F ||ATYABB||F\n≤ 1 nγ ∑ (B,l′)∈L 1 tAB 1√ λ tAB √ 2d (9) ≤ √ 2d\nγ √ λ .\nEquation (7) comes from the 1-lipschitzness of the hinge loss. Inequality (8) is obtained by applying triangle inequality. We obtain line (9) by applying Lemmas A and B.\nProof of Lemma 5.\n|`(M, (A, l))− `(M′, (A, l))|\n= ∣∣∣∣∣∣ 1− 1 n ∑ (B,l′)∈L ll′KM(A,B)/γ  + − 1− 1 n ∑ (B,l′)∈L ll′KM′(A,B)/γ  + ∣∣∣∣∣∣ ≤ ∣∣∣∣∣∣ 1n ∑\n(B,l′)∈L\nll′KM(A,B)/γ − 1\nn ∑ (B,l′)∈L ll′KM′(A,B)/γ ∣∣∣∣∣∣ (10)\n= 1\nnγ ∣∣∣∣∣∣ ∑\n(B,l′)∈L\nl′ (KM(A,B)−KM′(A,B)) ∣∣∣∣∣∣ ≤ 1 nγ ∑ (B,l′)∈L |KM(A,B)−KM′(A,B)| (11)\n= 1\nnγ ∑ (B,l′)∈L ∣∣Tr((M−M′)T ·AT ·YAB ·B)/tAB∣∣ ≤ 1 nγ ∑ (B,l′)∈L 1 tAB ||(M−M′)T ·AT ·YAB ·B)||1 (12)\n≤ 1 nγ ||M−M′||F ∑ (B,l′)∈L 1 tAB ||AT ·YAB ·B||F (13) ≤ √ 2d\nγ ||M−M′||F . (14)\nInequality (10) comes from the 1-lipschitzness of the hinge loss. Inequality (11) is obtained by applying triangle inequality. By using Lemma B on line (13), we obtain the lemma.\nRecall the following notation for the objective function from Equation (3): RS(M) := ÊS(M) + λ||M||2F . The following lemma is used for the proof of the uniform stability of an algorithm.\nLemma C. Let RS(·) and RSi(·) be the functions to optimize, M and Mi their corresponding minimizers, and λ the regularization parameter used. Let ∆M = M −Mi. Then we have, for t ∈ [0, 1]:\n||M||2F − ||M− t∆M||2F + ||Mi||2F − ||Mi + t∆M||2F ≤ 2kt\nλm ||∆M||F .\nThe proof is similar to the one of Lemma 20 in [5], thus we shall omit it. We use the previous lemma to prove the uniform stability of our approach.\nProof of Lemma 6. By setting t = 12 in Lemma C, we obtain after some computations:\n1 2 ||∆M||2F ≤ k λm ||∆M||F ,\nwhich implies:\n||∆M||F ≤ 2k\nλm .\nSince our loss is k-lipschitz, we have:\n|`(M, (A, l))− `(Mi, (A, l))| ≤ k||∆M||F = 2k2\nλm\nFor this loss function, k = √ 2d γ , and setting κ = 4d γ2λ proves the lemma.\nProof of Lemma 7.\nES [RS ] ≤ ES [E(A,l)[`(M, (A, l))]− ÊS(M)]\n≤ ES,(A,l) ∣∣∣∣∣∣`(M, (A, l))− 1m ∑\n(Ai,li)∈S\n`(M, (Ai, li)) ∣∣∣∣∣∣ \n≤ ES,(A,l) ∣∣∣∣∣∣ 1m ∑\n(Ai,li)\n(`(M, (A, l))− `(M, (Ai, li))) ∣∣∣∣∣∣ \n≤ ES,(A,l) ∣∣∣∣∣∣ 1m ∑\n(Ai,li)\n( `(Mi, (Ai, li))− `(M, (Ai, li)) )∣∣∣∣∣∣  (15)\n≤ κ m . (16)\nInequality (15) comes from the fact that changing one point with another from the same distribution does not affect the expected value, while Inequality (16) results from applying triangle inequality and uniform stability (Lemma 6).\nProof of Lemma 8.\n|RS −RSi | =|EP(M)− ÊS(M)− EP(Mi) + ÊSi(Mi)| =|EP(M)− ÊS(M)− EP(Mi) + ÊSi(Mi)− ÊS(Mi) + ÊS(Mi)| ≤|EP(M)− EP(Mi)|+ |ÊS(Mi)− ÊS(M)|+ |ÊSi(Mi)− ÊS(Mi)| (17) ≤E(A,l)[|`(M, (A, l))− `(Mi, (A, l))|] + |ÊS(Mi)− ÊS(M)|+ |ÊSi(Mi)− ÊS(Mi)| (18) ≤ κ m + |ÊS(Mi)− ÊS(M)|+ |ÊSi(Mi)− ÊS(Mi)| (19) ≤ κ m + 1 m ∑ (A,l)∈S\n∣∣`(Mi, (A, l))− `(M, (A, l))∣∣ +|ÊSi(Mi)− ÊS(Mi)| ≤ κ m + κ m + |ÊSi(Mi)− ÊS(Mi)| (20) = 2κ\nm +\n1 m |`(Mi, (Ai, li))− `(Mi, (A, l))| (21)\n≤2κ m + 1 m |`(Mi, (Ai, li))| (22)\n≤2κ m +\n√ 2d\nmγ √ λ\n(23)\nInequalities (17) and (18) come from triangle inequality. Inequalities (19) and (20) come from the uniform stability of our algorithm (Lemma 6). Line (21) comes from the fact that S and Si differ only by example i. We can write Inequality (22) because the loss is always positive, and we get line (23) by bounding the value of the loss function (Lemma 4)."
    } ],
    "references" : [ {
      "title" : "Sparsedtw: A novel approach to speed up dynamic time warping",
      "author" : [ "G. Al-Naymat", "S. Chawla", "J. Taheri" ],
      "venue" : "CoRR, abs/1201.2969,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Improved guarantees for learning via similarity functions",
      "author" : [ "M.-F. Balcan", "A. Blum", "N. Srebro" ],
      "venue" : "COLT, pages 287–298. Omnipress,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Similarity learning for provably accurate sparse linear classification",
      "author" : [ "A. Bellet", "A. Habrard", "M. Sebban" ],
      "venue" : "ICML, pages 1871–1878,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Metric Learning",
      "author" : [ "A. Bellet", "A. Habrard", "M. Sebban" ],
      "venue" : "Morgan & Claypool Publishers,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Stability and generalization",
      "author" : [ "O. Bousquet", "A. Elisseeff" ],
      "venue" : "JMLR, 2:499–526,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Information-theoretic metric learning",
      "author" : [ "J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon" ],
      "venue" : "ICML, pages 209–216. ACM,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning multiple temporal matching for time series classification",
      "author" : [ "C. Frambourg", "A.D. Chouakria", "É. Gaussier" ],
      "venue" : "IDA, volume 8207 of LNCS, pages 198–209. Springer,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Minimum prediction residual principle applied to speech recognition",
      "author" : [ "F. Itakura" ],
      "venue" : "IEEE Trans. on ASSP, 23(1):67–72,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Weighted dynamic time warping for time series classification",
      "author" : [ "Y.-S. Jeong", "M.K. Jeong", "O.A. Omitaomu" ],
      "venue" : "Pattern Recogn., 44(9):2231–2240, Sept.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Similarity-based learning via data driven embeddings",
      "author" : [ "P. Kar", "P. Jain" ],
      "venue" : "NIPS, pages 1998–2006. Curran Associates, Inc.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Clustering by means of medoids",
      "author" : [ "L. Kaufman", "P. Rousseeuw" ],
      "venue" : "Statistical Data Analysis Based on the L1-Norm and Related Methods, pages 405–416. North-Holland,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "On the need for time series data mining benchmarks: A survey and empirical demonstration",
      "author" : [ "E. Keogh", "S. Kasetty" ],
      "venue" : "Data Mining and Knowledge Discovery, 7(4):349–371,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The symmetric time-warping problem: From continuous to discrete",
      "author" : [ "J.B. Kruskall", "M. Liberman" ],
      "venue" : "Time Warps, String Edits and Macromolecules: The Theory and Practice of String Comparison. Addison-Wesley,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Metric learning: A survey",
      "author" : [ "B. Kulis" ],
      "venue" : "Foundations and Trends in Machine Learning, 5(4):287–364,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Metric learning for temporal sequence alignment",
      "author" : [ "R. Lajugie", "D. Garreau", "F. Bach", "S. Arlot" ],
      "venue" : "NIPS, pages 1817–1825. Curran Associates, Inc.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning a mahalanobis distance-based dynamic time warping measure for multivariate time series classification",
      "author" : [ "J. Mei", "M. Liu", "Y.F. Wang", "H. Gao" ],
      "venue" : "IEEE Trans. on Cybernetics, 46(6):1363–1374,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Time series classification by class-specific Mahalanobis distance measures",
      "author" : [ "Z. Prekopcsák", "D. Lemire" ],
      "venue" : "Adv. Data Analysis and Classification, 6(3):185–200,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Online and batch learning of generalized cosine similarities",
      "author" : [ "A.M. Qamar", "É. Gaussier" ],
      "venue" : "ICDM, pages 926–931,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Dynamic-programming algorithm optimization for spoken word recognition",
      "author" : [ "H. Sakoe", "S. Chiba" ],
      "venue" : "Trans. on ASSP, 26(1):43–49,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "FastDTW: toward accurate dynamic time warping in linear time and space",
      "author" : [ "S. Salvador", "P. Chan" ],
      "venue" : "KDD workshop on mining temporal and sequential data,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Sample complexity of learning mahalanobis distance metrics",
      "author" : [ "N. Verma", "K. Branson" ],
      "venue" : "NIPS, pages 2584–2592,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K. Weinberger", "L. Saul" ],
      "venue" : "JMLR, 10:207–244,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distance metric learning, with application to clustering with side-information",
      "author" : [ "E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell" ],
      "venue" : "NIPS, volume 15, pages 505–512,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "1-norm support vector machines",
      "author" : [ "J. Zhu", "S. Rosset", "T. Hastie", "R. Tibshirani" ],
      "venue" : "NIPS, 16(1):49–56,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Dynamic time warping [13] is the most well-known algorithm for measuring the similarity between two time series by finding the best alignment between them.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].",
      "startOffset" : 178,
      "endOffset" : 195
    }, {
      "referenceID" : 18,
      "context" : "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].",
      "startOffset" : 178,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].",
      "startOffset" : 178,
      "endOffset" : 195
    }, {
      "referenceID" : 0,
      "context" : "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].",
      "startOffset" : 178,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].",
      "startOffset" : 178,
      "endOffset" : 195
    }, {
      "referenceID" : 11,
      "context" : "Most of these approaches are designed for univariate time series [12, 18], which record the value of only one feature per time moment.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "Most of these approaches are designed for univariate time series [12, 18], which record the value of only one feature per time moment.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "Metric learning [4, 23, 6, 24] can address exactly this problem as it allows one to learn the weights of features and the correlations between them from the available training data.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : "Metric learning [4, 23, 6, 24] can address exactly this problem as it allows one to learn the weights of features and the correlations between them from the available training data.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "Metric learning [4, 23, 6, 24] can address exactly this problem as it allows one to learn the weights of features and the correlations between them from the available training data.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 22,
      "context" : "Metric learning [4, 23, 6, 24] can address exactly this problem as it allows one to learn the weights of features and the correlations between them from the available training data.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Our method is based on the ( , γ, τ)-good similarities learning framework [2].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "Dynamic time warping [13] computes the optimal alignment between two time series under a metric by finding the pairs of time indices to align.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 19,
      "context" : "In order to overcome the computational complexity of DTW, faster alternatives were introduced, like FastDTW [21] and SparseDTW [1].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "In order to overcome the computational complexity of DTW, faster alternatives were introduced, like FastDTW [21] and SparseDTW [1].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 18,
      "context" : "aligning the beginning of a series with the end of another), of which we mention the Sakoe-Chiba band [20] and the Itakura parallelogram [8].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 7,
      "context" : "aligning the beginning of a series with the end of another), of which we mention the Sakoe-Chiba band [20] and the Itakura parallelogram [8].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 13,
      "context" : "Metric learning [14, 4] focuses on learning the parameters of a distance or similarity function from data.",
      "startOffset" : 16,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "Metric learning [14, 4] focuses on learning the parameters of a distance or similarity function from data.",
      "startOffset" : 16,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "Large Margin Metric Learning (LMNN) [23] and Information-Theoretic Metric Learning (ITML) [6] are probably the most well-",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "Large Margin Metric Learning (LMNN) [23] and Information-Theoretic Metric Learning (ITML) [6] are probably the most well-",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "For time series, the notion of learning a metric has mostly been used in the sense of learning the right alignment for univariate time series [7].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 14,
      "context" : "In [15], the authors propose to learn a Mahalanobis metric for multivariate time series alignment of audio data.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "Recently, LDMLT [17] was designed to learn a Mahalanobis distance for multivariate time series from triplet constraints.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "Moreover, neither the method from [15] nor LDMLT come with guarantees that learning the metric improves performance for the given task.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "[2] K : X × X → [−1, 1] is a ( , γ, τ)-good similarity function in hinge loss for a learning problem P if there exists a random indicator function R(x) defining a probabilistic set of \"reasonable points\" such that the following conditions hold: 1.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Let K be an ( , γ, τ)-good similarity function in hinge loss for a learning problem P.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 23,
      "context" : "This formulation is equivalent to a relaxed L1-norm SVM [25].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "This issue has been addressed in [3] only for feature vectors.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 17,
      "context" : "For the pair of indices i and j, the affinity is equivalent to computing the generalized cosine similarity [19], as ai and bj are already normalized.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "In this section, we derive a generalization bound for SLTS using the notion of uniform stability [5].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "Definition 3 (Uniform stability [5]).",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 20,
      "context" : "According to [22], the presence of the number of features d in the numerator of the bound is to be expected and shows that the approach may suffer from the curse of dimensionality.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 15,
      "context" : "In the case of Auslan, we only use the 25 first classes instead of the total of 95, as done in precedent studies [17].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "We compare our method against the following classic algorithms: • Standard nearest neighbor classifier (1NN); • Linear SVM under L2 regularization; • Linear classifier from [2], presented in Equation (2) (called BBS from now on); • LDMLT [17] with a nearest neighbor classifier; • SLTS, the similarity learning method proposed in this chapter, which is then used to learn a global linear classifier using the formulation in [2].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "We compare our method against the following classic algorithms: • Standard nearest neighbor classifier (1NN); • Linear SVM under L2 regularization; • Linear classifier from [2], presented in Equation (2) (called BBS from now on); • LDMLT [17] with a nearest neighbor classifier; • SLTS, the similarity learning method proposed in this chapter, which is then used to learn a global linear classifier using the formulation in [2].",
      "startOffset" : 238,
      "endOffset" : 242
    }, {
      "referenceID" : 1,
      "context" : "We compare our method against the following classic algorithms: • Standard nearest neighbor classifier (1NN); • Linear SVM under L2 regularization; • Linear classifier from [2], presented in Equation (2) (called BBS from now on); • LDMLT [17] with a nearest neighbor classifier; • SLTS, the similarity learning method proposed in this chapter, which is then used to learn a global linear classifier using the formulation in [2].",
      "startOffset" : 424,
      "endOffset" : 427
    }, {
      "referenceID" : 17,
      "context" : "The comparison between the two should thus be taken with caution as distances and similarities can yield very different results [19].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 10,
      "context" : "K-Medoids [11] is a classical clustering technique.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 9,
      "context" : "Dselect [10] was proposed as a landmarks selection algorithm that optimizes a criterion of diversity.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "Then we have, for t ∈ [0, 1]: ||M||F − ||M− t∆M||F + ||M||F − ||M + t∆M||F ≤ 2kt λm ||∆M||F .",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "The proof is similar to the one of Lemma 20 in [5], thus we shall omit it.",
      "startOffset" : 47,
      "endOffset" : 50
    } ],
    "year" : 2016,
    "abstractText" : "Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance. Most of these applications need to be able to compare these structured data. In this context, dynamic time warping (DTW) is probably the most common comparison measure. However, not much research effort has been put into improving it by learning. In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification. Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification. The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers.",
    "creator" : "LaTeX with hyperref package"
  }
}