{
  "name" : "1602.02950.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Spoofing detection under noisy conditions: a preliminary investigation and an initial database",
    "authors" : [ "Xiaohai Tian", "Zhizheng Wu", "Xiong Xiao", "Eng Siong Chng", "Haizhou Li" ],
    "emails" : [ "aseschng}@ntu.edu.sg,", "zhizheng.wu@ed.ac.uk,", "hli@i2r.a-star.edu.sg" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Recently, automatic speaker verification (ASV) has been significantly advanced to the point of mass-market adoption [1–4]. However, most of current ASV systems assume human voices, and there are concerns that whether the systems can still achieve robust performance in the face of diverse spoofing attacks. A spoofing attack is that an attacker attempts to manipulate an ASV result for a target genuine speaker to obtain access permission. A significant amount of evidences have confirmed the vulnerability of current state-of-the-art ASV systems under spoofing attacks as reviewed in [5]. This has led to the active development of spoofing countermeasures, also called spoofing detection, that is to discriminate human and spoofed speech.\nIn the past several years, spoofing detection for speaker recognition has been studied on a variety of diverse datasets. In [6, 7], the Wall Street Journal (WSJ) corpus was used to assess countermeasures for speech synthesis attacks. In [8], the publicly available RSR2015 corpus was used to evaluate spoofing detection for replay attacks. In [9, 10], synthetic speech from the Blizzard challenge [11] was used for speech synthesis spoofing detection. In [12], a recently released spoofing and anti-spoofing (SAS) corpus as a standard spoofing database was used to assess speech synthesis and voice conversion spoofing countermeasures. We note that WSJ, SAS and Blizzard challenge databases were recorded by high-quality microphones in sound-proofing environment, while the RSR2015 corpus was\nrecorded by multiple mobile devices in a quiet office room. All these databases do not have any significant channel and/or additive noise. These databases allow us to focus on spoofing effects but do not simulate practical scenarios of ASV applications.\nThere are also some studies that use data with channel noise. The National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) 2006 database which has significant telephone channel noise was used to assess voice conversion spoofing countermeasures in [13–17]. In [18], a so-called AVspoof database includes replay, speech synthesis and voice conversion spoofing attacks to simulate realistic scenarios, which re-recorded synthetic or voiceconverted speech using multiple mobile devices.\nIn general, all the databases used in the past spoofing detection studies do not consider additive noise, even the recent standard spoofing detection databases: SAS1 and ASVspoof 2015 challenge2 databases. However, in practical scenarios, it is hard to avoid additive and/or channel noise. Hence, another concern for ASV deployment arises that whether currently developed spoofing detection algorithms/systems are still effective under noisy conditions.\nIn this work, we focus on spoofing detection under additive noisy conditions. We perform a preliminary investigation of spoofing detection under noisy conditions using current stateof-the-art countermeasure techniques; and also briefly introduce an initial database we built for the task3. In general, we aim to answer the following questions:\n• Do current state-of-the-art spoofing detection algorithms work well under additive noisy conditions?\n• How additive noises affect the spoofing detection performance?\n• What kind of noise is more serious to degrade the performance of spoofing detection algorithms?\nWe believe better understanding of above questions and the noisy database will drive the development of generalised and noise robust spoofing detection algorithms.\n1SAS corpus is available at: http://dx.doi.org/10.7488/ ds/252\n2ASVspoof 2015 corpus is available at: http://dx.doi.org/ 10.7488/ds/298\n3The noisy database will be publicly-available for free under a CCBY license.\nar X\niv :1\n60 2.\n02 95\n0v 1\n[ cs\n.L G\n] 9\nF eb"
    }, {
      "heading" : "2. Noisy Database",
      "text" : "In order to represent the practical application scenarios for spoofing detection, we attempt to design a database in additive noisy environments based on the ASVspoof 2015 challenge database [19].\nASVspoof database is a spoofing and anti-spoofing database, consisting of both genuine (human speech) and ten types of spoofed speech (named as S1-S10 in ASVspoof 2015 challenge) implemented by three speech synthesis and seven voice conversion spoofing algorithms. The ASVspoof database contains three subsets, including training, development and evaluation sets. The training and development sets only contain known attacks (S1-S5); while the evaluation set consists of both known and unknown (S6-S10) attacks. More details and protocols about the ASVspoof database can be found in [19].\nThis noisy version aims to quantify the effects of current spoofing detection algorithms in noise conditions and to facilitate future assessment work in this task. In this section, we will briefly introduce the types of noise to be added, and the procedure of adding noise."
    }, {
      "heading" : "2.1. Noise signals",
      "text" : "Five types of noise signals, representing the probable application scenarios, are used for the construction of the noisy ASVspoof database. A subset of three types of noise, white noise, speech babble and vehicle interior noise, are selected from NOISEX-92 database [20]. Another two types mixed noise, street noise and cafe noise, are selected from QUTNOISE database [21]. These are standard types of additive noise used for speech recognition [21–23], speaker verification [24, 25] and speech enhancement [26]. We briefly describe these noises as follows:\n• White Noise: The random signal with a constant power spectral density.\n• Babble Noise: Speech babble and the recording is made in a canteen with 100 people speaking.\n• Volvo Noise: Vehicle interior noise and the recording is made in Volvo 340 on an asphalt road, rainy conditions.\n• Street Noise: Mixed noise, which is made at the roadside near inner-city, mainly consisting of road traffic noise, pedestrian traffic noise and bird noise.\n• Cafe Noise: Mixed noise, which is made in outdoor cafe environment, mainly consisting of speech babble and kitchen noise from the cafe environment.\nFigure 1 shows the spectrogram of all the five noises. We can classify the noises into stationary noise and non-stationary noise. White noise and Volvo noise are stationary noise. While babble noise, street noise and cafe noise are recorded in nonstationary noise environment, whose the magnitude and phase spectrogram are varied over time."
    }, {
      "heading" : "2.2. Adding noise",
      "text" : "The data from ASVspoof database are considered as clean data. Noise is artificially added to the clean data. The Filtering and Noise Adding Tool (FaNT)4 is used for the adding noise process. The noisy signals are generated by adding the clean speech and noise files together at various SNRs. As the silence periods appear in many speech files of ASVspoof database, it\n4http://dnt.kr.hs-niederrhein.de/\nis important to calculate the SNR only based on the sections of speech signal. The bandpass filter or frequency weighting are often used for SNR calculation, to ensure the SNR are appropriate and comparable. In this work, to add the noise based on the human hearing perception, we define the SNR as the ratio of signal to noise energy after filtering both signals with the Aweighting filter. This filter emphasizes the frequencies around 3 kHz to 6 kHz where the human ear is most sensitive and give a lower response for the very high and very low frequencies to which the ear is less sensitive [27]. Hence, noises such as Volvo noise, whose energy distribution concentrates more on very low frequency (below 1 kHz), tend to have higher energy at the same SNR level.\nFor each clean signal in the development and evaluation sets of ASVspoof database, fifteen noisy versions of the signal are generated consisting of five types of noise in three SNR levels. Given a clean signal, for each noise type, we take a segment of the noise signal with equal length as the clean signal but random starting point from the whole noise file. Then the noise segment is scaled and added to the clean signal in 20 dB, 10 dB and 0 dB SNR levels.\nAfter the adding noise process, the clipping may occur, especially at low SNR levels due to high noise energy. In order to maintain a stable spectrogram representation of the signal, the signal is scaled to avoid the clipping.\nAll the five types of noise are added to the ASVspoof database at the SNRs of 20 dB, 10dB, and 0 dB. Hence, the ASVspoof noisy database is fifteen times of the clean database."
    }, {
      "heading" : "3. Benchmarking system",
      "text" : "In order to demonstrate the utility of the ASVspoof noisy database, we conduct a series of experiments to examine the performance of spoofing speech detection system on a range of SNRs in all five noise scenarios.\nThe detection system, as shown in Figure 2, consists of (a) the feature extraction module to extract six types of feature used\nfor classification; (b) the classification module to calculate the score for each feature; (c) the score fusion module to fused the scores obtained from six classifiers. The details of these three modules are introduced as follows."
    }, {
      "heading" : "3.1. Feature extraction",
      "text" : "Similar to our previous system described in [28, 29], six types of feature are extracted. As shown in Figure 2 (a), given a noisy waveform, the Hamming window and direct current (DC) offset removal are applied on each analysis frame. Then short-time Fourier transform (STFT) is applied on the speech signal using analysis window of 25ms with 15ms overlap. The FFT length is chosen to be 512 and the dimension of all the original features are 256. For the n-th frame, the magnitude and phase spectrum, |X(n, ω)| and θ(n, ω), are obtained by\nX(n, ω) = |X(n, ω)|ejθ(n,ω), (1)\nAfter that, two magnitude-based features, namely log magnitude spectrum (LMS) and residual log magnitude spectrum (RLMS) are derived from magnitude spectrum. Four phasebased features, namely instantaneous frequency derivative (IF), baseband phase difference (BPD), group delay (GD) and modified group delay (MGD), are derived from phase spectrum.\nThe features are summarized as follows:\n• LMS: The log magnitude spectrum feature can be expressed as LMS(n, ω) = log(|X(n, ω)|). The LMS contains the formant information, harmonic structure and all the spectral detail of speech signal.\n• RLMS: The residual log magnitude spectrum feature is the LMS extracted from the linear predictive coding (LPC) residual signal. As the formant information is removed, this feature can better analyses the harmonic structure and spectral details.\n• IF: Instantaneous frequency [30] is the derivative of the phase along time axis and captures the temporal information of phase. It is defined as:\nIF(n, ω) = princ(θ(n, ω)− θ(n− 1, ω)). (2)\nwhere princ(·) represents the principal value operator, mapping the input onto [−π;π] interval by adding integer numbers of 2π.\n• BPD: Baseband phase difference [31] is another phase feature derived from IF and baseband STFT. For the n-th frame, the BPD is calculated as:\nBPD(n, ω) = princ(IF(n, ω)− Ωkl), (3)\nwhere Ωk = 2πk/L is the normalized angular frequencies.\n• GD: Group delay [32] is a representation of filter phase response, which is defined as the negative derivative of the Fourier transform phase. It is a frame-based feature, used to capture the phase distortion along frequency axis.\nGD(n, ω) = princ{θ(n, ω)− θ(n, ω − 1)}, (4)\n• MGD: A variation of GD, which can obtain a more clear phase pattern than GD. The MGD [32] feature of frame n is calculated as:\nτ(n, ω) = XR(n, ω)YR(n, ω) +XI(n, ω)YI(n, ω)\n|S(n, ω)|2γ ,\n(5)\nMGD(n, ω) = τ(n, ω)\n|τ(n, ω)| |τ(n, ω)| α, (6)\nwhere XR(n, ω) and XI(n, ω) denote the real and image part of STFT for x(l); while YR(n, ω) and YI(n, ω) denote the real and image part of STFT for lx(l), respectively. S(n, ω) is the smoothed spectrum of |X(n, ω)|. Based on experimental results, the values of γ and α are set to 0.7 and 0.2 respectively.\nIn Fig 3 and Fig 4, we present the LMS and IF features extracted from clean and the noisy signals. The noisy signals include the white and street noise scenarios at the SNR of 20 dB, 10 dB and 0 dB. We observe that both LMS and IF feature are distorted by additive noise significantly. The patterns become more blurred for lower SNRs."
    }, {
      "heading" : "3.2. Classifier",
      "text" : "Figure 2 (b) shows the classification part of the detection system. Our previous multilayer perceptron (MLP) based spoofing speech detection system [29] is used in this work. Each of the features mentioned above with its delta and acceleration coefficients is used as the input vector to train its own classifier. The MLP, which contains one hidden layer with 2,048 sigmoid nodes, is used to predict the posterior probability of the input vector being synthetic speech. The score is calculated by averaging the posterior probabilities of all the frames over the utterance. Noted that, in this work, all the MLP classifier are trained from clean data."
    }, {
      "heading" : "3.3. Evaluation metrics and fusion",
      "text" : "The equal error rate (EER), where false acceptance rate and miss rejection rate become equal, is used to evaluate the system performance.\nAs described in Section 3.1, different features are designed to detect different types of artifacts. In order to benefit the advantages of each feature and improve the system stability, a score level fusion is applied. Figure 2 (c) shows score fusion of the detection system.\nThe fusion is applied on the feature-based results in different noise scenarios. Based on our preliminary experiments, the\nIF: Clean condition\nFrame number 0 100 200\nF re\nqu en\ncy (\nkH z)\n8\n6\n4\n2\n0\nIF: white-SNR-20 dB\nFrame number 0 100 200\nF re\nqu en\ncy (\nkH z)\n8\n6\n4\n2\n0\nIF: white-SNR-10 dB\nFrame number 0 100 200\nF re\nqu en\ncy (\nkH z)\n8\n6\n4\n2\n0\nIF: white-SNR-0 dB\nFrame number 0 100 200\nF re\nqu en\ncy (\nkH z)\n8\n6\n4\n2\n0\nIF: street-SNR-20 dB\nFrame number 0 100 200\nF re\nqu en\ncy (\nkH z)\n8\n6\n4\n2\n0\nIF: street-SNR-10 dB\nFrame number 0 100 200\nF re\nqu en\ncy (\nkH z)\n8\n6\n4\n2\n0\nIF: street-SNR-0 dB\nFrame number 0 100 200\nF re\nqu en\ncy (\nkH z)\n8\n6\n4\n2\n0\nFigure 4: Demonstration of the IF features for utterance D14 1000302, in both clean and noise scenarios.\nweighted summation fusion, tuned on the development data set, exhibits the over-fitting effects at street and cafe noise scenarios. To avoid this problem, the scores of all systems are simply averaged to produce the final score.\nIn this work, the Bosaris toolkit5 is used to compute the EERs of each feature and the fused system."
    }, {
      "heading" : "4. Experiments",
      "text" : ""
    }, {
      "heading" : "4.1. Experimental setups",
      "text" : "The database used in the experiments consist of three subsets, including training set, development set and evaluation set. The training set is clean speech data taken from ASVspoof database. As the training set consists of clean data only, it models the speech without noise distortion and represents all the speech information. The best performance of the clean classifier is obtained in the case of testing on clean data, which can be found in our previous work [29].\nThe development and evaluation sets are chosen from the\n5https://sites.google.com/site/bosaristoolkit/\nnoisy ASVspoof database, including five noise scenarios at three different SNRs as described in Section 2. Because the classifier used in these experiments is the same as that of our previous work [29], these results are comparable with the results in clean condition."
    }, {
      "heading" : "4.2. Evaluation results",
      "text" : "As the results on the development set are similar to that of the S1 to S5 on the evaluation set, only the feature-based results of the evaluation set are reported. The results are shown independently as the known attacks (S1-S5), the unknown attacks (S6-S9) and the unknown attacks generated by waveform concatenation (S10). The EERs are listed in Table 1 for both noisy dataset and clean dataset using the clean classifier.\nWe first analyse the effect of noisy data for the detection system using different features. In general, across all the five noise scenarios, the systems perform worse than that of clean condition. As expected, in most spoof attacks, the detection performance deteriorates as SNR decreases. We notice that, in most noisy scenarios, the magnitude-based features, LMS\nand RLMS, perform worse than the phase-based features, IF, BPD, GD and MGD. In particular, in all the spoofing attacks, the RLMS obtains much higher EERs than other features. This may be due to that the LPC filter is not robust in noisy environments [33], which affects the quality of RLMS. Among the phase-based features, IF and BPD outperform other features in terms of the average EERs over all the noise scenarios. We also found that, some features are effective for particular noise scenario. For example, in the babble noise scenario, the MGD is capable to obtain low error rates. While in white, street, and cafe noises, IF and BPD perform much better than other features.\nNext, we compare the performance across different types of attack in noise conditions. Because S1-S5 attacks are available for training, even in noisy condition, the lower error rates are obtained in these attacks than the rest types of attack. Although the error rates of S6-S9 is higher than that of S1-S5, the results still comparable. This is consistent with the results in clean condition [29]. For S10, even apply the score fusion, the error rates of all the features are significantly higher than that of S1S9. Hence, we conclude that in both clean and noisy conditions, the detection of S10 is still the most challenge task among the spoofing attacks."
    }, {
      "heading" : "4.3. Fusion results",
      "text" : "Table 2 presents the results of fused systems on both development and evaluation sets. We first examine the results at different SNRs. In all the noise scenarios at the SNR of 20 dB, the system performance degradation significant differ for different noise scenarios. On the development set, the EERs of different fuse systems are between 2.52% (cafe noise) to 8.32% (Volvo noise); while on the evaluation set, the EERs are varies between 5.25% (white noise) to 14.31% (Volvo noise). However, at the SNR of 0 dB, all the systems performance degrade significantly on both development and evaluation set. Fig 3 and Fig 4 show that most of the feature patterns are lost in such low SNR.\nThen, we analyse the fused results in different noise scenarios. Among all the noise scenarios, the system under white noise scenario performs best, which constantly achieves lowest error rates. Especially, at low SNRs, 10 dB and 0 dB, the system under white noise outperform that of other noise scenarios significantly. As discussed in Section 2.2, compare to other types\nof noise, the Volvo noise tends to give higher energy at the same SNR level, resulting in more distortions in the noisy signals and higher EERs.\nFor the non-stationary noisy conditions, the features distorted by such noise are time-varying. Consequently, in these noise conditions, the system performance degrades more than white noise scenarios. This provides more challenge to the system to detect the spoofed attacks in such noisy conditions."
    }, {
      "heading" : "5. Conclusions",
      "text" : "In this paper, we constructed a noisy database for spoofing and anti-spoofing research. This database is generated from the ASVspoof database, adding five types of additive noise at three SNR levels. To provide the benchmark results, we use the state-of-the-art spoofing detection system to detect the spoofing attacks in noisy conditions. The preliminary results using the classifier trained from clean data shown that,\n• the performance of detection systems degrade in all the noise scenarios. The system performance deteriorates as SNR decreases;\n• in noisy environments, even the best EER is about 4% lower (white noise at 20 dB) than that of clean condition;\n• in general, the non-stationary noises affect the detection system more seriously;\n• the system performance varies significantly under different noise scenarios and the phase-based features are noise robust than magnitude-based features.\nIn this paper, we only presented benchmark results to demonstrate the vulnerability of current spoofing detection systems under additive noise conditions. In future work, we will use convolutional noise, such as channel noise and reverberate noise, to simulate the spoofing attacks in more complex noise scenarios. Moreover, the classifier presented in this work was trained from the clean data. There are also plans to exam the effectiveness of multi-condition training for spoofing detection under noisy conditions. Finally, a proper score fusion method will be investigated to both avoid the over-fitting effects and improve the system performance."
    }, {
      "heading" : "6. Acknowledgment",
      "text" : "This research is supported by the National Research Foundation Singapore under its Interactive Digital Media (IDM) Strategic Research Programme. This work is also supported by the DSO funded project MAISON DSOCL14045, Singapore."
    }, {
      "heading" : "7. References",
      "text" : "[1] Kong Aik Lee, Bin Ma, and Haizhou Li, “Speaker veri-\nfication makes its debut in smartphone,” in IEEE Signal Processing Society Speech and language Technical Committee Newsletter, February 2013.\n[2] Mikhail Khitrov, “Talking passwords: voice biometrics for data access and security,” Biometric Technology Today, vol. 2013, no. 2, pp. 9–11, 2013.\n[3] Brett Beranek, “Voice biometrics: success stories, success factors and what’s next,” Biometric Technology Today, vol. 2013, no. 7, pp. 9–11, 2013.\n[4] Wenchao Meng, Duncan Wong, Steven Furnell, and Jianying Zhou, “Surveying the development of biometric user authentication on mobile phones,” IEEE Communications Surveys and Tutorials, 2015.\n[5] Zhizheng Wu, Nicholas Evans, Tomi Kinnunen, Junichi Yamagishi, Federico Alegre, and Haizhou Li, “Spoofing and countermeasures for speaker verification: a survey,” Speech Communication, vol. 66, pp. 130–153, 2015.\n[6] P. L. De Leon, M. Pucher, J. Yamagishi, I. Hernaez, and I. Saratxaga, “Evaluation of speaker verification security and detection of HMM-based synthetic speech,” IEEE Trans. Audio, Speech and Language Processing, vol. 20, no. 8, pp. 2280–2290, 2012.\n[7] Zhizheng Wu, Xiong Xiao, Eng Siong Chng, and Haizhou Li, “Synthetic speech detection using temporal modulation feature,” in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013.\n[8] Zhizheng Wu, Sheng Gao, Eng Siong Chng, and Haizhou Li, “A study on replay attack and anti-spoofing for textdependent speaker verification,” in Proc. Asia-Pacific Signal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2014.\n[9] Phillip L De Leon, Bryan Stewart, and Junichi Yamagishi, “Synthetic speech discrimination using pitch pat-\nSPEECH, 2012.\n[10] Jon Sanchez, Ibon Saratxaga, Inma Hernaez, Eva Navas, Daniel Erro, and Tuomo Raitio, “Toward a universal synthetic speech spoofing detection using phase information,” IEEE Trans. on Information Forensics and Security, vol. 10, no. 4, pp. 810–820, 2015.\n[11] Simon King, “Measuring a decade of progress in text-tospeech,” Loquens, vol. 1, no. 1, pp. e006, 2014.\n[12] Zhizheng Wu, Phillip L. De Leon, Cenk Demiroglu, Ali Khodabakhsh, Simon King, Zhen-Hua Ling, Daisuke Saito, Bryan Stewart, Tomoki Toda, Mirjam Wester, and Junichi Yamagishi, “Anti-spoofing for text-independent speaker verification: An initial database, comparison of countermeasures, and human performance,” IEEE/ACM Transactions on Audio, Speech and Language Processing, 2016.\n[13] Zhizheng Wu, Eng Siong Chng, and Haizhou Li, “Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition,” in Proc. Interspeech, 2012.\n[14] Zhizheng Wu, Tomi Kinnunen, Eng Siong Chng, Haizhou Li, and Eliathamby Ambikairajah, “A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case,” in Proc. Asia-Pacific Signal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012.\n[15] Federico Alegre, Asmaa Amehraye, and Nicholas Evans, “Spoofing countermeasures to protect automatic speaker verification from voice conversion,” in Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013.\n[16] Elie Khoury, Tomi Kinnunen, Aleksandr Sizov, Zhizheng Wu, and Sébastien Marcel, “Introducing i-vectors for joint anti-spoofing and speaker verification,” in Proc. Interspeech, 2014.\n[17] Aleksandr Sizov, Elie Khoury, Tomi Kinnunen, Zhizheng Wu, and Sebastien Marcel, “Joint speaker verification and antispoofing in the i-vector space,” IEEE Trans. on Information Forensics and Security, vol. 10, no. 4, pp. 821– 832, 2015.\n[18] Serife Kucur Ergunay, Elie Khoury, Alexandros Lazaridis, and Sebastien Marcel, “On the vulnerability of speaker\nverification to realistic voice spoofing,” in IEEE International Conference on Biometrics Theory, Applications and Systems (BTAS), 2015.\n[19] Zhizheng Wu, Tomi Kinnunen, Nicholas Evans, Junichi Yamagishi, Cemal Hanilçi, Md Sahidullah, and Aleksandr Sizov, “ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge,” in Proc. Interspeech, 2015.\n[20] A. P. Varga, H. J. M. Steeneken, M. Tomlinson, and D. Jones, “The NOISEX-92 study on the effect of additive noise on automatic speech recognition,” in Tech. Rep., DRA Speech Research Unit, 1992.\n[21] David Dean, Ahilan Kanagasundaram, Houman Ghaemmaghami, Md Hafizur Rahman, and Sridha Sridharan, “The QUT-NOISE-SRE protocol for the evaluation of noisy speaker recognition,” in Proc. Interspeech, 2015.\n[22] Andrew Varga and Herman JM Steeneken, “Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems,” Speech communication, vol. 12, no. 3, pp. 247–251, 1993.\n[23] Hans-Günter Hirsch and David Pearce, “The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions,” in ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW), 2000.\n[24] Andrzej Drygajlo and Mounir El-Maliki, “Speaker verification in noisy environments with combined spectral subtraction and missing feature theory,” in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 1998.\n[25] Rahim Saeidi, Jouni Pohjalainen, Tomi Kinnunen, and Paavo Alku, “Temporally weighted linear prediction features for tackling additive noise in speaker verification,” IEEE Signal Processing Letters, vol. 17, no. 6, pp. 599– 602, 2010.\n[26] Nathalie Virag, “Single channel speech enhancement based on masking properties of the human auditory system,” IEEE Transactions on Speech and Audio Processing, vol. 7, no. 2, pp. 126–137, 1999.\n[27] Harvey Fletcher and Wilden A Munson, “Loudness, its definition, measurement and calculation*,” Bell System Technical Journal, vol. 12, no. 4, pp. 377–430, 1933.\n[28] Xiong Xiao, Xiaohai Tian, Steven Du, Haihua Xu, Eng Siong Chng, and Haizhou Li, “Spoofing speech detection using high dimensional magnitude and phase features: The ntu approach for ASVspoof 2015 challenge,” in Proc. Interspeech, 2015.\n[29] Xiaohai Tian, Zhizheng Wu, Xiong Xiao, Eng Siong Chng, and Haizhou Li, “Spoofing detection from a feature representation perspective,” in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.\n[30] Leigh D Alsteris and Kuldip K Paliwal, “Short-time phase spectrum in speech processing: A review and some experimental results,” Digital Signal Processing, vol. 17, no. 3, pp. 578–616, 2007.\n[31] Michal Krawczyk and Timo Gerkmann, “STFT phase reconstruction in voiced speech for an improved singlechannel speech enhancement,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 22, no. 12, pp. 1931–1940, 2014.\n[32] Bayya Yegnanarayana and Hema A Murthy, “Significance of group delay functions in spectrum estimation,” IEEE Transactions on Signal Processing, vol. 40, no. 9, pp. 2281–2289, 1992.\n[33] Steven M Kay, “The effects of noise on the autoregressive spectral estimator,” IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 27, no. 5, pp. 478–485, 1979."
    } ],
    "references" : [ {
      "title" : "Speaker verification makes its debut in smartphone",
      "author" : [ "Kong Aik Lee", "Bin Ma", "Haizhou Li" ],
      "venue" : "IEEE Signal Processing Society Speech and language Technical Committee Newsletter, February 2013.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Talking passwords: voice biometrics for data access and security",
      "author" : [ "Mikhail Khitrov" ],
      "venue" : "Biometric Technology Today, vol. 2013, no. 2, pp. 9–11, 2013.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Voice biometrics: success stories, success factors and what’s next",
      "author" : [ "Brett Beranek" ],
      "venue" : "Biometric Technology Today, vol. 2013, no. 7, pp. 9–11, 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Surveying the development of biometric user authentication on mobile phones",
      "author" : [ "Wenchao Meng", "Duncan Wong", "Steven Furnell", "Jianying Zhou" ],
      "venue" : "IEEE Communications Surveys and Tutorials, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Spoofing and countermeasures for speaker verification: a survey",
      "author" : [ "Zhizheng Wu", "Nicholas Evans", "Tomi Kinnunen", "Junichi Yamagishi", "Federico Alegre", "Haizhou Li" ],
      "venue" : "Speech Communication, vol. 66, pp. 130–153, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Evaluation of speaker verification security and detection of HMM-based synthetic speech",
      "author" : [ "P.L. De Leon", "M. Pucher", "J. Yamagishi", "I. Hernaez", "I. Saratxaga" ],
      "venue" : "IEEE Trans. Audio, Speech and Language Processing, vol. 20, no. 8, pp. 2280–2290, 2012.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Synthetic speech detection using temporal modulation feature",
      "author" : [ "Zhizheng Wu", "Xiong Xiao", "Eng Siong Chng", "Haizhou Li" ],
      "venue" : "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A study on replay attack and anti-spoofing for textdependent speaker verification",
      "author" : [ "Zhizheng Wu", "Sheng Gao", "Eng Siong Chng", "Haizhou Li" ],
      "venue" : "Proc. Asia-Pacific Signal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Synthetic speech discrimination using pitch pat-  tern statistics derived from image analysis",
      "author" : [ "Phillip L De Leon", "Bryan Stewart", "Junichi Yamagishi" ],
      "venue" : "INTER- SPEECH, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Toward a universal synthetic speech spoofing detection using phase information",
      "author" : [ "Jon Sanchez", "Ibon Saratxaga", "Inma Hernaez", "Eva Navas", "Daniel Erro", "Tuomo Raitio" ],
      "venue" : "IEEE Trans. on Information Forensics and Security, vol. 10, no. 4, pp. 810–820, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Measuring a decade of progress in text-tospeech",
      "author" : [ "Simon King" ],
      "venue" : "Loquens, vol. 1, no. 1, pp. e006, 2014.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Anti-spoofing for text-independent speaker verification: An initial database, comparison of countermeasures, and human performance",
      "author" : [ "Zhizheng Wu", "Phillip L. De Leon", "Cenk Demiroglu", "Ali Khodabakhsh", "Simon King", "Zhen-Hua Ling", "Daisuke Saito", "Bryan Stewart", "Tomoki Toda", "Mirjam Wester", "Junichi Yamagishi" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech and Language Processing, 2016.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition",
      "author" : [ "Zhizheng Wu", "Eng Siong Chng", "Haizhou Li" ],
      "venue" : "Proc. Interspeech, 2012.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case",
      "author" : [ "Zhizheng Wu", "Tomi Kinnunen", "Eng Siong Chng", "Haizhou Li", "Eliathamby Ambikairajah" ],
      "venue" : "Proc. Asia-Pacific Signal Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Spoofing countermeasures to protect automatic speaker verification from voice conversion",
      "author" : [ "Federico Alegre", "Asmaa Amehraye", "Nicholas Evans" ],
      "venue" : "Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Introducing i-vectors for joint anti-spoofing and speaker verification",
      "author" : [ "Elie Khoury", "Tomi Kinnunen", "Aleksandr Sizov", "Zhizheng Wu", "Sébastien Marcel" ],
      "venue" : "Proc. Interspeech, 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Joint speaker verification and antispoofing in the i-vector space",
      "author" : [ "Aleksandr Sizov", "Elie Khoury", "Tomi Kinnunen", "Zhizheng Wu", "Sebastien Marcel" ],
      "venue" : "IEEE Trans. on Information Forensics and Security, vol. 10, no. 4, pp. 821– 832, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the vulnerability of speaker  verification to realistic voice spoofing",
      "author" : [ "Serife Kucur Ergunay", "Elie Khoury", "Alexandros Lazaridis", "Sebastien Marcel" ],
      "venue" : "IEEE International Conference on Biometrics Theory, Applications and Systems (BTAS), 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge",
      "author" : [ "Zhizheng Wu", "Tomi Kinnunen", "Nicholas Evans", "Junichi Yamagishi", "Cemal Hanilçi", "Md Sahidullah", "Aleksandr Sizov" ],
      "venue" : "Proc. Interspeech, 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The NOISEX-92 study on the effect of additive noise on automatic speech recognition",
      "author" : [ "A.P. Varga", "H.J.M. Steeneken", "M. Tomlinson", "D. Jones" ],
      "venue" : "Tech. Rep., DRA Speech Research Unit, 1992.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "The QUT-NOISE-SRE protocol for the evaluation of noisy speaker recognition",
      "author" : [ "David Dean", "Ahilan Kanagasundaram", "Houman Ghaemmaghami", "Md Hafizur Rahman", "Sridha Sridharan" ],
      "venue" : "Proc. Interspeech, 2015.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems",
      "author" : [ "Andrew Varga", "Herman JM Steeneken" ],
      "venue" : "Speech communication, vol. 12, no. 3, pp. 247–251, 1993.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions",
      "author" : [ "Hans-Günter Hirsch", "David Pearce" ],
      "venue" : "ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW), 2000.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Speaker verification in noisy environments with combined spectral subtraction and missing feature theory",
      "author" : [ "Andrzej Drygajlo", "Mounir El-Maliki" ],
      "venue" : "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 1998.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Temporally weighted linear prediction features for tackling additive noise in speaker verification",
      "author" : [ "Rahim Saeidi", "Jouni Pohjalainen", "Tomi Kinnunen", "Paavo Alku" ],
      "venue" : "IEEE Signal Processing Letters, vol. 17, no. 6, pp. 599– 602, 2010.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Single channel speech enhancement based on masking properties of the human auditory system",
      "author" : [ "Nathalie Virag" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing, vol. 7, no. 2, pp. 126–137, 1999.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Loudness, its definition, measurement and calculation",
      "author" : [ "Harvey Fletcher", "Wilden A Munson" ],
      "venue" : "Bell System Technical Journal, vol. 12, no. 4, pp. 377–430, 1933.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1933
    }, {
      "title" : "Spoofing speech detection using high dimensional magnitude and phase features: The ntu approach for ASVspoof 2015 challenge",
      "author" : [ "Xiong Xiao", "Xiaohai Tian", "Steven Du", "Haihua Xu", "Eng Siong Chng", "Haizhou Li" ],
      "venue" : "Proc. Interspeech, 2015.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Spoofing detection from a feature representation perspective",
      "author" : [ "Xiaohai Tian", "Zhizheng Wu", "Xiong Xiao", "Eng Siong Chng", "Haizhou Li" ],
      "venue" : "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Short-time phase spectrum in speech processing: A review and some experimental results",
      "author" : [ "Leigh D Alsteris", "Kuldip K Paliwal" ],
      "venue" : "Digital Signal Processing, vol. 17, no. 3, pp. 578–616, 2007.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "STFT phase reconstruction in voiced speech for an improved singlechannel speech enhancement",
      "author" : [ "Michal Krawczyk", "Timo Gerkmann" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing, vol. 22, no. 12, pp. 1931–1940, 2014.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1931
    }, {
      "title" : "Significance of group delay functions in spectrum estimation",
      "author" : [ "Bayya Yegnanarayana", "Hema A Murthy" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 40, no. 9, pp. 2281–2289, 1992.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "The effects of noise on the autoregressive spectral estimator",
      "author" : [ "Steven M Kay" ],
      "venue" : "IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 27, no. 5, pp. 478–485, 1979.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1979
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Introduction Recently, automatic speaker verification (ASV) has been significantly advanced to the point of mass-market adoption [1–4].",
      "startOffset" : 129,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "Introduction Recently, automatic speaker verification (ASV) has been significantly advanced to the point of mass-market adoption [1–4].",
      "startOffset" : 129,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "Introduction Recently, automatic speaker verification (ASV) has been significantly advanced to the point of mass-market adoption [1–4].",
      "startOffset" : 129,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "Introduction Recently, automatic speaker verification (ASV) has been significantly advanced to the point of mass-market adoption [1–4].",
      "startOffset" : 129,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "A significant amount of evidences have confirmed the vulnerability of current state-of-the-art ASV systems under spoofing attacks as reviewed in [5].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "In [6, 7], the Wall Street Journal (WSJ) corpus was used to assess countermeasures for speech synthesis attacks.",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "In [6, 7], the Wall Street Journal (WSJ) corpus was used to assess countermeasures for speech synthesis attacks.",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "In [8], the publicly available RSR2015 corpus was used to evaluate spoofing detection for replay attacks.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "In [9, 10], synthetic speech from the Blizzard challenge [11] was used for speech synthesis spoofing detection.",
      "startOffset" : 3,
      "endOffset" : 10
    }, {
      "referenceID" : 9,
      "context" : "In [9, 10], synthetic speech from the Blizzard challenge [11] was used for speech synthesis spoofing detection.",
      "startOffset" : 3,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "In [9, 10], synthetic speech from the Blizzard challenge [11] was used for speech synthesis spoofing detection.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : "In [12], a recently released spoofing and anti-spoofing (SAS) corpus as a standard spoofing database was used to assess speech synthesis and voice conversion spoofing countermeasures.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 12,
      "context" : "The National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) 2006 database which has significant telephone channel noise was used to assess voice conversion spoofing countermeasures in [13–17].",
      "startOffset" : 219,
      "endOffset" : 226
    }, {
      "referenceID" : 13,
      "context" : "The National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) 2006 database which has significant telephone channel noise was used to assess voice conversion spoofing countermeasures in [13–17].",
      "startOffset" : 219,
      "endOffset" : 226
    }, {
      "referenceID" : 14,
      "context" : "The National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) 2006 database which has significant telephone channel noise was used to assess voice conversion spoofing countermeasures in [13–17].",
      "startOffset" : 219,
      "endOffset" : 226
    }, {
      "referenceID" : 15,
      "context" : "The National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) 2006 database which has significant telephone channel noise was used to assess voice conversion spoofing countermeasures in [13–17].",
      "startOffset" : 219,
      "endOffset" : 226
    }, {
      "referenceID" : 16,
      "context" : "The National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE) 2006 database which has significant telephone channel noise was used to assess voice conversion spoofing countermeasures in [13–17].",
      "startOffset" : 219,
      "endOffset" : 226
    }, {
      "referenceID" : 17,
      "context" : "In [18], a so-called AVspoof database includes replay, speech synthesis and voice conversion spoofing attacks to simulate realistic scenarios, which re-recorded synthetic or voiceconverted speech using multiple mobile devices.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "Noisy Database In order to represent the practical application scenarios for spoofing detection, we attempt to design a database in additive noisy environments based on the ASVspoof 2015 challenge database [19].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 18,
      "context" : "More details and protocols about the ASVspoof database can be found in [19].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "A subset of three types of noise, white noise, speech babble and vehicle interior noise, are selected from NOISEX-92 database [20].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : "Another two types mixed noise, street noise and cafe noise, are selected from QUTNOISE database [21].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "These are standard types of additive noise used for speech recognition [21–23], speaker verification [24, 25] and speech enhancement [26].",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "These are standard types of additive noise used for speech recognition [21–23], speaker verification [24, 25] and speech enhancement [26].",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 22,
      "context" : "These are standard types of additive noise used for speech recognition [21–23], speaker verification [24, 25] and speech enhancement [26].",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : "These are standard types of additive noise used for speech recognition [21–23], speaker verification [24, 25] and speech enhancement [26].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 24,
      "context" : "These are standard types of additive noise used for speech recognition [21–23], speaker verification [24, 25] and speech enhancement [26].",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 25,
      "context" : "These are standard types of additive noise used for speech recognition [21–23], speaker verification [24, 25] and speech enhancement [26].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 26,
      "context" : "This filter emphasizes the frequencies around 3 kHz to 6 kHz where the human ear is most sensitive and give a lower response for the very high and very low frequencies to which the ear is less sensitive [27].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 27,
      "context" : "Feature extraction Similar to our previous system described in [28, 29], six types of feature are extracted.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 28,
      "context" : "Feature extraction Similar to our previous system described in [28, 29], six types of feature are extracted.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : "• IF: Instantaneous frequency [30] is the derivative of the phase along time axis and captures the temporal information of phase.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 30,
      "context" : "• BPD: Baseband phase difference [31] is another phase feature derived from IF and baseband STFT.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 31,
      "context" : "• GD: Group delay [32] is a representation of filter phase response, which is defined as the negative derivative of the Fourier transform phase.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 31,
      "context" : "The MGD [32] feature of frame n is calculated as:",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 28,
      "context" : "Our previous multilayer perceptron (MLP) based spoofing speech detection system [29] is used in this work.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 28,
      "context" : "The best performance of the clean classifier is obtained in the case of testing on clean data, which can be found in our previous work [29].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 28,
      "context" : "Because the classifier used in these experiments is the same as that of our previous work [29], these results are comparable with the results in clean condition.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : "Clean indicates the results of our previous work [29].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 32,
      "context" : "This may be due to that the LPC filter is not robust in noisy environments [33], which affects the quality of RLMS.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "This is consistent with the results in clean condition [29].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 28,
      "context" : "Clean indicates the results of our previous work [29].",
      "startOffset" : 49,
      "endOffset" : 53
    } ],
    "year" : 2016,
    "abstractText" : "Spoofing detection for automatic speaker verification (ASV), which is to discriminate between live speech and attacks, has received increasing attentions recently. However, all the previous studies have been done on the clean data without significant additive noise. To simulate the real-life scenarios, we perform a preliminary investigation of spoofing detection under additive noisy conditions, and also describe an initial database for this task. The noisy database is based on the ASVspoof challenge 2015 database and generated by artificially adding background noises at different signal-to-noise ratios (SNRs). Five different additive noises are included. Our preliminary results show that using the model trained from clean data, the system performance degrades significantly in noisy conditions. Phasebased feature is more noise robust than magnitude-based features. And the systems perform significantly differ under different noise scenarios.",
    "creator" : "LaTeX with hyperref package"
  }
}