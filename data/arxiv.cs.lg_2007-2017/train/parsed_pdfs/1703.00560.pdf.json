{
  "name" : "1703.00560.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis",
    "authors" : [ "Yuandong Tian" ],
    "emails" : [ "<yuandong@fb.com>." ],
    "sections" : [ {
      "heading" : null,
      "text" : "∑K j=1 σ(w ᵀ j x) with cen-\ntered d-dimensional spherical Gaussian input x (σ=ReLU). We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and fixed parameters w∗. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters (“out-of-plane“) are not isolated and form manifolds, and characterize inplane critical-point-free regions for two ReLU case. On the other hand, convergence to w∗ for one ReLU node is guaranteed with at least (1 − )/2 probability, if weights are initialized randomly with standard deviation upper-bounded by O( / √ d), consistent with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w∗ (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings."
    }, {
      "heading" : "1. Introduction",
      "text" : "Despite empirical success of deep learning (e.g., Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al., 2014) and Speech Recognition (Hinton et al., 2012)), it remains elusive how and why simple methods like gradient descent can solve the complicated non-convex optimization in its training proce-\n1Facebook AI Research. Correspondence to: Yuandong Tian <yuandong@fb.com>.\ndure. In this paper, we focus on the following two-layered ReLU network:\ng(x;w) = K∑ j=1 σ(wᵀj x), (1)\nHere σ(x) = max(x, 0) is the ReLU nonlinearity. We consider the setting that a student network is optimized to minimize the l2 distance between its prediction and the supervision provided by a teacher network of the same architecture with fixed parameters w∗. Note that Eqn. 1 is highly non-convex and has exponential number of critical points.\nTo analyze Eqn. 1, we introduce a simple analytic formula for population gradient, when inputs x are sampled from zero-mean spherical Gaussian. Using this formula, critical point and convergence analysis follow.\nFor critical points, we show that critical points outside the principal hyperplane (the subspace spanned by w∗) form manifolds. We also characterize the region in the principal hyperplane that has no critical points, in two ReLU case.\nWe also analyze the convergence behavior under the population gradient. Using Lyapunov method (LaSalle & Lefschetz, 1961), for single ReLU case we prove that gradient descent converges to w∗ with at least (1 − )/2 probability, if initialized randomly with standard deviation upper-bounded by O( / √ d), verifying common initialization techniques (Bottou, 1988; Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012),. For multiple ReLU case, when the teacher parameters {wj}Kj=1 form a set of orthonormal basis, we prove that (1) a symmetric weight initialization gets stuck at a saddle point and (2) a particular infinitesimal perturbation of (1) leads to convergence towards w∗ or its permutation. This behavior is known as spontaneous symmetry breaking in physics, in which the population gradient field enjoys invariance under a certain kind of symmetry, but the solution breaks it. Although such behaviors have been known empirically, to our knowledge, this paper first formally characterizes them in 2-layered ReLU network. ar X iv :1\n70 3.\n00 56\n0v 1\n[ cs\n.L G\n] 2\nM ar\n2 01\n7\nStudent Network\n(a)\nw⇤w\nTeacher Network\n(b) 1 1 1\nX\ng\nj wj\nFigure 1. (a) We consider the student and teacher network as nonlinear neural networks with ReLU nonlinearity. The student network updates its weight w from the output of the teacher with fixed weights w∗. (b) The 2-layered ReLU network structure (Eqn. 1) discussed in this paper. The first layer contains fixed weights of value 1, while the second layers has K ReLU nodes. Each node j has a d-dimensional weight wj to be optimized. Teacher network has the same architecture as the student."
    }, {
      "heading" : "2. Related Works",
      "text" : "For multilayer linear network, many works analyze its critical points and convergence behaviors. (Saxe et al., 2013) analyzes its dynamics of gradient descent and (Kawaguchi, 2016) shows every local minimum is global. On the other hand, very few theoretical works have been done for nonlinear networks. (Mei et al., 2016) shows the global convergence for a single nonlinear node whose derivatives of activation σ′, σ′′, σ′′′ are bounded and σ′ > 0. Similar to our approach, (Saad & Solla, 1996) also uses the student-teacher setting and analyzes the student dynamics when the teacher’s parameters w∗ are orthonormal. However, their activation is Gaussian error function erf(x), and only the local behaviors of the two critical points (the initial saddle point near the origin and w∗) are analyzed. Recent paper (Zhang et al., 2017) analyzes a similar teacher-student setting on 2-layered network when the involved function is harmonic, but it is unclear how the conclusion is generalized to ReLU case. To our knowledge, our close-form formula for 2-layered ReLU network is novel, as well as the critical point and convergence analysis.\nMany previous works analyze nonlinear network based on the assumption of independent activations: the activations of ReLU (or other nonlinear) nodes are independent of the input and/or mutually independent. For example, (Choromanska et al., 2015a;b) relates the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u). (Kawaguchi, 2016) proves that every local minimum in nonlinear network is global based on similar assumptions. (Soudry & Carmon, 2016) shows the global optimality of the local minimum in a two-layered ReLU network, when independent multiplicative Bernoulli noise is applied to the activations. In practice, activations that share the input are highly dependent. Ignoring such depen-\ndency misses important behaviors, and may lead to misleading conclusions. In this paper, no assumption of independent activations is made. Instead, we assume input to follow spherical Gaussian distribution, which gives more realistic and interdependent activations during training.\nFor sigmoid activation, (Fukumizu & Amari, 2000) gives complicated conditions for a local minimum to be global when adding a new node to a 2-layered network. (Janzamin et al., 2015) gives guarantees for parameter recovery of a 2-layered network learnt with tensor decomposition. In comparison, we analyze ReLU networks trained with gradient descent, which is more popular in practice."
    }, {
      "heading" : "3. Problem Definition",
      "text" : "Denote N as the number of samples and d as the input dimension. The N -by-d matrix X is the input data and w∗ is the fixed parameter of the teacher network. Given the current estimation w, we have the following l2 loss:\nJ(w) = 1\n2 ‖g(X;w∗)− g(X;w)‖2, (2)\nHere we focus on population loss EX [J ], where the input X is assumed to follow spherical Gaussian distribution N (0, I). Its gradient is the population gradient EX [∇Jw(w)] (abbrev. E [∇J ]). In this paper, we study critical points E [∇J ] = 0 and vanilla gradient dynamics wt+1 = wt − ηE [∇J(wt)], where η is the learning rate."
    }, {
      "heading" : "4. The Analytical Formula",
      "text" : "Properties of ReLU. ReLU nonlinearity has useful properties. We define the gating functionD(w) ≡ diag(Xw > 0) as an N -by-N binary diagonal matrix. Its l-th diagonal element is a binary variable showing whether the neuron is activated for sample l. Using this notation, σ(Xw) = D(w)Xw which means D(w) selects the output of a linear neuron, based on their activations. Note thatD(w) only depends on the direction of w but not its magnitude.\nD(w) is also “transparent” with respect to derivatives. For example, at differentiable regions, Jacobianw[σ(Xw)] = σ′(Xw)X = D(w)X . This gives a very concise rule for gradient descent update in ReLU networks.\nOne ReLU node. Given the properties of ReLU, the population gradient E [∇J ] can be written as:\nE [∇J ] = EX [XᵀD(w) (D(w)Xw −D(w∗)Xw∗)] (3) Intuitively, this term vanishes when w → w∗, and should be around N2 (w − w\n∗) if the data are evenly distributed, since roughly half of the samples are blocked. However, such an estimation fails to capture the nonlinear behavior.\nIf we define Population Gating (PG) function F (e,w) ≡\nXᵀD(e)D(w)Xw, then E [∇J ] can be written as:\nE [∇J ] = E [F (w/‖w‖,w)]− E [F (w/‖w‖,w∗)] . (4)\nInterestingly, F (e,w) has an analytic formula if the data X follow spherical Gaussian distribution:\nTheorem 1 Denote F (e,w) = XᵀD(e)D(w)Xw where e is a unit vector, X = [x1,x2, · · · ,xN ]ᵀ is the N -by-d data matrix and D(w) = diag(Xw > 0) is a binary diagonal matrix. If xi ∼ N (0, I) (and thus bias-free), then:\nE [F (e,w)] = N\n2π [(π − θ)w + ‖w‖ sin θe] (5)\nwhere θ = ∠(e,w) ∈ [0, π] is the angle between e and w.\nSee the link1 for the proof of all theorems. Note that we do not require X to be independent between samples. Intuitively, the first mass term N2π (π−θ)w aligns with w and is proportional to the amount of activated data whose ReLU are on. When θ = 0, the gating function is fully on and half of the data contribute to the term; when θ = π, the gating function is completely switched off. The gate is controlled by the angle between w and the control signal e. The second asymmetric term is aligned with e, and is proportional to the asymmetry of the activated data samples with respect to e (Fig. 2).\nNote that the expectation analysis smooths out ReLU and leaves only one singularity at the origin, where E [∇J ] is not continuous. That is, if approaching from different directions towards w = 0, E [∇J ] is different.\nWith the close form of F , E [∇J ] also has a close form:\nE [∇J ] = N 2 (w−w∗)+N 2π\n( θw∗ − ‖w\n∗‖ ‖w‖ sin θw\n) (6)\nwhere θ = ∠(w,w∗) ∈ [0, π]. The first term is from linear approximation, while the second term shows the nonlinear behavior.\nFor linear case, D ≡ I (no gating) and thus ∇J ∝ XᵀX(w − w∗). For spherical Gaussian input X , EX [XᵀX] = I and E [∇J ] ∝ w−w∗. Therefore, the dynamics has only one critical point and global convergence follows, which is consistent with its convex nature.\nExtension to other distributions. From its definition, E [F (e,w)] = E [XᵀD(e)D(w)Xw] is linear to ‖w‖, regardless of the distribution of X . On the other hand, isotropy in spherical Gaussian distribution leads to the fact that E [F (e,w)] only depends on angles between vectors. For other isotropic distributions, we could similarly derive:\nE [F (e,w)] = A(θ)w + ‖w‖B(θ)e (7) 1http://yuandong-tian.com/ssb-supp.pdf\nwhere A(0) = N/2 (gating fully on), A(π) = 0 (gating fully off), and B(0) = B(π) = 0 (no asymmetry when w and e are aligned). Although we focus on spherical Gaussian case, many following analysis, in particular critical point analysis, can also be applied to Eqn. 7.\nMultiple ReLU node. For Eqn. 1 that contains K ReLU node, we could similarly write down the population gradient with respect to wj (note that ej = wj/‖wj‖):\nE [ ∇wjJ ] = K∑ j′=1 E [F (ej ,wj′)]− K∑ j′=1 E [ F (ej ,w ∗ j′) ] (8)"
    }, {
      "heading" : "5. Critical Point Analysis",
      "text" : "By solving Eqn. 8 (the normal equation, E [ ∇wjJ ] = 0), we could identify all critical points of g(x). However, it is highly nonlinear and cannot be solved easily. In this paper, we provide conditions for critical points using the structure of Eqn. 8. Following the analysis, the case study forK = 2 gives examples for saddle points and regions without critical points.\nFor convenience, we define Π∗ as the Principal Hyperplane spanned by K ground truth weight vectors. Note that Π∗ is at most K dimensional. {wj}Kj=1 is said to be in-plane, if all wj ∈ Π∗. Otherwise it is out-of-plane."
    }, {
      "heading" : "5.1. Normal Equation",
      "text" : "The normal equation {E [ ∇wjJ ] = 0}Kj=1 contain Kd scalar equations and can be written as the following:\nY Eᵀ = B∗W ∗ᵀ (9)\nwhere Y = diag(sin Θᵀw̄ − sin Θ∗ᵀw̄∗) + (π11ᵀ − Θᵀ)diagw̄ and B∗ = π11ᵀ − (Θ∗)ᵀ. Here θ∗j ′\nj ≡ ∠(wj ,w∗j′), θ j′\nj ≡ ∠(wj ,wj′), Θ = [θij ] (i-th row, j-th column of Θ is θij) and Θ ∗ = [θ∗ij ].\nNote that Y and B∗ are both K-by-K matrices that only\ndepend on angles and magnitudes, and hence rotational invariant. This leads to the following theorem characterizing the structure of out-of-plane critical points:\nTheorem 2 If d ≥ K+ 2, then out-of-plane critical points (solutions of Eqn. 9) are non-isolated and lie in a manifold.\nThe intuition is to construct a rotational matrix that is not identity matrix but keeps Π∗ invariant. Such matrices form a Lie group L that transforms critical points to critical points. Then for any out-of-plane critical point, there is one matrix in L that changes at least one of its weights, yielding a non-isolated different critical point.\nNote that Thm. 2 also works for any general isotropic distribution, in which E [F (e,w)] has the form of Eqn. 7. This is due to the symmetry of the input X , which in turn affects the geometry of critical points. The theorem also explains why we have flat minima (Hochreiter et al., 1995; Dauphin et al., 2014) often occuring in practice."
    }, {
      "heading" : "5.2. In-Plane Normal Equation",
      "text" : "To analyze in-plane critical points, it suffices to study gradient projections on Π∗. When {wj} is full-rank, the projections could be achieved by right-multiplying both sides by {ej′}, which gives K2 equations:\nM(Θ)w̄ = M∗(Θ,Θ∗)w̄∗ (10)\nThis again shows decomposition of angles and magnitudes, and linearity with respect to the norms of weight vectors. Here w̄ = [‖w1‖, ‖w2‖, . . . , ‖wK‖]ᵀ and similarly for w̄∗. M and M∗ are K2-by-K matrices that only depend on angles. Entries of M and M∗ are:\nmjj′,k = (π − θkj ) cos θkj′ + sin θkj cos θ j j′ (11) m∗jj′,k = (π − θ∗kj ) cos θ∗kj′ + sin θ∗kj cos θ j j′ (12)\nHere index j is the j-th column of Eqn. 9, j′ is from projection vector ej′ and k is the k-th weight magnitude.\nDiagnoal constraints. For “diagonal” constraints (j, j) of Eqn. 10, we have cos θjj = 1 and mjj,k = h(θ k j ), m ∗ jj,k = h(θ∗kj ), where h(θ) = (π− θ) cos θ+ sin θ. Therefore, we arrive at the following subset of the constraints:\nMrw̄ = M ∗ r w̄ ∗ (13)\nwhere Mr = h(Θᵀ) and M∗r = h(Θ ∗ᵀ) are both K-byK matrices. Note that if Mr is full-rank, then we could solve w̄ from Eqn. 13 and plug it back in Eqn. 10 to check whether it is indeed a critical point. This gives necessary conditions for critical points that only depend on angles.\nSeparable Property. Interestingly, the plugging back operation leads to conditions that are separable with respect to ground truth weight (Fig. 3). To see this, we first define the following quantity Ljj′ which is a function between a single (rather than K) ground truth unit weight vector e∗ and all current unit weights {el}Kl=1:\nLjj′({θ∗l },Θ) = m∗jj′ − vᵀM−1r mjj′ (14)\nwhere θ∗l = ∠(e ∗, el) is the angle between e∗ and el, v = v({θ∗l }) = [h(θ∗1), . . . , h(θ∗K)]ᵀ, and m∗jj′ = (π − θ∗j ) cos θ∗j′ + sin θ∗j cos θ j j′ (like Eqn. 12). Note that v({θ∗jl }) is the j-th column of M∗r . Fig. 3 illustrates the case when K = 2. Ljj′ has the following properties:\nProposition 1 Ljj′({θ∗l },Θ) = 0 when there exists l so that e∗ = el. In addition, Ljj({θ∗l },Θ) = 0 always.\nIntuitively, Ljj′ characterizes the relative geometric relationship among e∗ and {el}. It is like determinant of a matrix whose columns are {el} and e∗. With Ljj′ , we have the following necessary conditions for critical points:\nTheorem 3 If w̄∗ 6= 0, and for a given parameter w, Ljj′({θ∗kl },Θ) > 0 (or < 0) for all 1 ≤ k ≤ K, then w cannot be a critical point."
    }, {
      "heading" : "5.3. Case study: K = 2 network",
      "text" : "In this case, Mr and M∗r are 2-by-2 matrices. Here we discuss the case that both w1 and w2 are in Π∗.\nSaddle points. When θ12 = 0 (w1 and w2 are collinear), Mr = π11\nᵀ is singular since e1 and e2 are identical. From Eqn. 9, if θ∗11 = θ ∗2 1 , i.e., they are both aligned with the bisector angle of w∗1 and w ∗ 2 , and πw̄\nᵀ1 = h ( θ∗1∗2/2 ) (w̄∗)ᵀ1, then the current solution is a saddle point. Note that this gives one constraint for two weight magnitudes, and thus there exist infinite solutions.\nRegion without critical points. We rely on the following conjecture that is verified empirically in an exhaustive manner (Sec. 7.2). It characterizes zero-crossings of a 2D function on a closed region [0, 2π]× [0, π]. In comparison, in-plane 2 ReLU network has 6 parameters and is more difficult to handle: 8 for w1, w2, w∗1 and w ∗ 2 , minus the rotational and scaling symmetries.\nw1\nw1\nw⇤ w1\nw⇤1 w1\nw⇤1(a) (b)\nConjecture 1 If e∗ is in the interior of Cone(e1, e2), then L12(θ ∗ 1 , θ ∗ 2 , θ 1 2) > 0. If e ∗ is in the exterior, then L12 < 0.\nThis is also empirically true for L21. Combined with Thm. 3, we know that (Fig. 4):\nTheorem 4 If Conjecture 1 is correct, then for 2 ReLU network, (w1,w2) (w1 6= w2) is not a critical point, if they both are in Cone(w∗1,w ∗ 2), or both out of it.\nOn the other hand, when exact one ground truth weight is inside Cone(w1,w2), it is not sure whether (w1,w2) is a critical point."
    }, {
      "heading" : "6. Convergence Analysis",
      "text" : "Application of Eqn. 5 also yields interesting convergence analysis. We focus on infinitesimal analysis, i.e., when learning rate η → 0 and the gradient update becomes a first-order differential equation:\ndw/dt = −EX [∇wJ(w)] (15)\nThen the populated objective EX [J ] does not increase:\ndE [J ] /dt = −E [∇J ]ᵀ dw/dt = −E [∇J ]ᵀ E [∇J ] ≤ 0 (16)\nThe goal of convergence analysis is to determine specific weight initializations w0 that leads to convergence to w∗ following the gradient descent dynamics (Eqn. 15)."
    }, {
      "heading" : "6.1. Single ReLU case",
      "text" : "Using Lyapunov method (LaSalle & Lefschetz, 1961), we show that the gradient dynamics (Eqn. 15) converges to w∗ when w0 ∈ Ω = {w : ‖w −w∗‖ < ‖w∗‖}:\nTheorem 5 When w0 ∈ Ω = {w : ‖w −w∗‖ < ‖w∗‖}, following the dynamics of Eqn. 15, the Lyapunov function V (w) = 12‖w − w\n∗‖2 has dV/dt < 0 and the system is asymptotically stable and thus wt → w∗ when t→ +∞.\nThe intuition is to represent dV/dt as a 2-by-2 bilinear form of vector [‖w‖, ‖w∗‖], and the bilinear coefficient matrix, as a function of angles, is negative definite (except for w = w∗). Note that similar approaches do not apply to\nregions including the origin because at the origin, the population gradient is discontinuous. Ω does not include the origin and for any initialization w0 ∈ Ω, we could always find a slightly smaller subset Ω′δ = {w : ‖w − w∗‖ ≤ ‖w∗‖−δ} with δ > 0 that covers w0, and apply Lyapunov method within. Note that the global convergence claim in (Mei et al., 2016) for l2 loss does not apply to ReLU, since it requires σ′(x) > 0.\nRandom Initialization. How to sample w0 ∈ Ω without knowing w∗? Uniform sampling around origin with radius r ≥ γ‖w∗‖ for any γ > 1 results in exponentially small success rate (r/‖w∗‖)d ≤ γ−d in high-dimensional space. A better idea is to sample around the origin with very small radius (but not at w = 0), so that Ω looks like a hyperplane near the origin, and thus almost half samples are useful (Fig. 5(a)), as shown in the following theorem:\nTheorem 6 The dynamics in Eqn. 6 converges to w∗ with probability at least (1 − )/2, if the initial value w0 is sampled uniformly from Br = {w : ‖w‖ ≤ r} with r ≤ √ 2π d+1‖w ∗‖.\nThe idea is to lower-bound the probability of the shaded area (Fig. 5(b)). Thm. 6 gives an explanation for common initialization techniques (Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012; Bottou, 1988) that uses random variables with O(1/ √ d) standard deviation."
    }, {
      "heading" : "6.2. Multiple ReLU case",
      "text" : "For multiple ReLUs, Lyapunov method on Eqn. 8 yields no decisive conclusion. Here we focus on the symmetric property of Eqn. 8 and discuss a special case, that the teacher parameters {w∗j}Kj=1 and the initial weights {w0j}Kj=1 respect the following symmetry: wj = Pjw and w∗j = Pjw\n∗, where Pj is an orthogonal matrix whose collection P ≡ {Pj}Kj=1 forms a group. Without loss of generality, we set P1 as the identity. Then from Eqn. 8 the population gradient becomes:\nE [ ∇wjJ ] = PjE [∇w1J ] (17)\nThis means that if all wj and w∗j are symmetric under group actions, so does their population gradients. Therefore, the trajectory {wt} also respects the symmetry (i.e., Pjw t 1 = w t j) and we only need to solve one equation for E [∇wJ ] instead of K (here e = w/‖w‖):\nE [∇wJ ] = K∑ j′=1 E [F (e, Pj′w)]− K∑ j′=1 E [F (e, Pj′w∗)]\n(18)\nEqn. 18 has interesting properties, known as Spontaneous Symmetric-Breaking (SSB) in physics (Brading & Castellani, 2003), in which the equations of motion respect a certain symmetry but its solution breaks it (Fig. 6). In our language, despite that the population gradient field E [∇wJ ] and the objective E [J ] are invariant to the group transformation P , i.e., for w∗ → Pjw∗, E [J ] and E [∇wJ ] remain the same, its solution is not (Pjw 6= w). Furthermore, since P is finite, as we will see, the final solution converges to different permutations of w∗ due to infinitesimal perturbations of initialization.\nTo illustrate such behaviors, consider the following example in which {w∗j}Kj=1 forms an orthonormal basis and under this basis, P is a cyclic group in which Pj circularly shifts dimension by j − 1 (e.g., P2[1, 2, 3]ᵀ = [3, 1, 2]ᵀ). In this case, if we start with w0 = x0w∗ + ∑ j 6=1 Pjw ∗ j = [x0, y0, . . . , y0] under the basis of w∗, then Eqn. 18 is further reduced to a convergent 2D nonlinear dynamics and Thm. 7 holds (Please check Supplementary Materials for the associated close-form of the 2D dynamics):\nTheorem 7 For a bias-free two-layered ReLU network g(x;w) = ∑ j σ(w ᵀ j x) that takes spherical Gaussian inputs, if the teacher’s parameters {w∗j} form orthnomal bases, then (1) when the student parameters is initialized to be [x0, y0, . . . , y0] under the basis of w∗, where (x0, y0) ∈ Ω = {x ∈ (0, 1], y ∈ [0, 1], x > y}, then Eqn. 8 converges to teacher’s parameters {w∗j} (or (x, y) = (1, 0)); (2) when x0 = y0 ∈ (0, 1], then it converges to a saddle point x = y = 1πK ( √ K − 1− arccos(1/ √ K) + π).\nThm. 7 suggests that when w0 = [y0, x0, . . . , y0], the system converges to P2w∗, etc. Since |x0 − y0| can be arbitrarily small, a slightest perturbation around x0 = y0 leads to a different fixed point Pjw∗ for some j. Unlike single ReLU case, the initialization in Thm. 7 is w∗-dependent, and serves as an example for the branching behavior.\nThm. 7 also suggests that for convergence, x0 and y0 can be arbitrarily small, regardless of the magnitude of w∗, showing a global convergence behavior. In comparison, (Saad & Solla, 1996) uses Gaussian error function (σ = erf) as the activation, and only analyzes local behaviors near the two fixed points (origin and w∗).\nIn practice, even with noisy initialization, Eqn. 18 and the original dynamics (Eqn. 8) still converge to w∗ (and its transformations). We leave it as a conjecture, whose proof may lead to an initialization technique for 2-layered ReLU that is w∗-independent. Conjecture 2 If the initialization w0 = x0w∗ + y0 ∑ j 6=1 Pjw\n∗ + , where is noise and (x0, y0) ∈ Ω, then Eqn. 8 also converges to w∗ with high probability."
    }, {
      "heading" : "7. Simulations",
      "text" : "7.1. The analytical solution to F (e,w)\nWe verify E [F (e,w)] = E [XᵀD(e)D(w)Xw] (Eqn. 5) with simulation. We randomly pick e and w so that their angle ∠(e,w) is uniformly distributed in [0, π]. The analytical formula E [F (e,w)] is compared with F (e,w), which is computed via sampling on the input X that follows spherical Gaussian distribution. We use relative RMS error: err = ‖E [F (e,w)] − F (e,w)‖/‖F (e,w)‖. Fig. 7(a) shows the error distribution with respect to angles. For small θ, the gating function D(w) and D(e) mostly overlap and give a reliable estimation. When θ → π,D(w) andD(e)overlap less and the variance grows. Note that our convergence analysis operate on θ ∈ [0, π/2] and is not affected. In the following, we sample angles from [0, π/2].\nFig. 7(a) shows that the accuracy of the formula increases with more samples. We also examine other zero-mean distributions of X , e.g., uniform distribution in [−1/2, 1/2]. Fig. 7(d) shows that the formula still works for large d. Note that the error is computed up to a global scale, due to different normalization constants in probability distributions. To prove the usability of Eqn. 5 for more general distributions remains open."
    }, {
      "heading" : "7.2. Empirical Results in critical point analysis K = 2",
      "text" : "Conjecture 1 can be reduced to enumerate a complicated but 2D function via exhaustive sampling. In comparison, a full optimization of 2-ReLU network constrained on principal hyperplane Π∗ involves 6 parameters (8 parameters\nminus 2 degrees of symmetry) and is more difficult to handle. Fig. 10 shows that empirically L12 has no extra zerocrossing other than e∗ = e1 or e2. As shown in Fig. 10(c), we have densely enumerated θ12 ∈ [0, π] and e∗ on a 104 × 104 grid without finding any counterexamples."
    }, {
      "heading" : "7.3. Convergence analysis for multiple ReLU nodes",
      "text" : "Fig. 8(a) and (b) shows the 2D vector field in Thm 7. Fig. 8(c) shows the 2D trajectory towards convergence to the teacher’s parameters w∗. Interestingly, even when we initialize the weights as [10−3, 0]ᵀ, whose direction is aligned with w∗ at [1, 0]ᵀ, the gradient descent still takes detours to reach the destination. This is because at the beginning of optimization, all ReLU nodes explain the training error in the same way (both x and y increases); when the “obvious” component is explained, the error pushes some nodes to explain other components. Hence, specialization follows (x increases but y decreases).\nFig. 9 shows empirical convergence for K ≥ 2, when the initialization deviates from initialization [x, y, . . . , y]\nin Thm. 7. Unless the deviation is large, w converges to w∗. For more general network g2(x) = ∑K j=1 ajσ(w ᵀ j x), when aj > 0 convergence follows. When some aj is negative, the network fails to converge to w∗, even when the student is initialized with the true values {a∗j}Kj=1."
    }, {
      "heading" : "8. Extension to multilayer ReLU network",
      "text" : "A natural question is whether the proposed method can be extended to multilayer ReLU network. In this case, there is similar subtraction structure for gradient as Eqn. 3:\nProposition 2 Denote [c] as all nodes in layer c. Denote u∗j and uj as the output of node j at layer c of the teacher and student network, then the gradient of the parameters wj immediate under node j ∈ [c] is:\n∇wjJ = XᵀcDjQj ∑ j′∈[c] (Qj′uj′ −Q∗j′u∗j′) (19)\nwhere Xc is the data fed into node j, Qj and Q∗j are N - by-N diagonal matrices. For any node k ∈ [c + 1], Qk =∑ j∈[c] wjkDjQj and similarly for Q ∗ k.\nThe 2-layered network in this paper is a special case with Qj = Q ∗ j = I . Despite the difficulty that Qj is now depends on the weights of upper layers, and the input Xc is not necessarily Gaussian distributed, Proposition 2 gives a mathematical framework to explore the structure of gradient. For example, a similar definition of Population Gradient function is possible."
    }, {
      "heading" : "9. Conclusion and Future Work",
      "text" : "In this paper, we study the gradient descent dynamics of a 2-layered bias-free ReLU network. The network is trained using gradient descent to reproduce the output of a teacher network with fixed parameters w∗ in the sense of l2 norm. We propose a novel analytic formula for population gradient when the input follows zero-mean spherical Gaussian distribution. This formula leads to interesting critical point and convergence analysis. Specifically, we show that critical points out of the hyperplane spanned by w∗ are not isolated and form manifolds. For two ReLU case, we characterize regions that contain no critical points. For convergence analysis, we show guaranteed convergence for a single ReLU case with random initialization whose standard deviation is on the order of O(1/ √ d). For multiple ReLU case, we show that an infinitesimal change of weight initialization leads to convergence to different optima.\nOur work opens many future directions. First, Thm. 2 characterizes the non-isolating nature of critical points in the case of isotropic input distribution, which explains why often practical solutions of NN are degenerated. What if the input distribution has different symmetries? Will such symmetries determine the geometry of critical points? Second, empirically we see convergence cases that are not covered by the theorems, suggesting the conditions imposed by the theorems can be weaker. Finally, how to apply similar anal-\nysis to broader distributions and how to generalize the analysis to multiple layers are also open problems.\nAcknowledgement We thank Léon Bottou, Ruoyu Sun, Jason Lee, Yann Dauphin and Nicolas Usunier for discussions and insightful suggestions."
    } ],
    "references" : [ {
      "title" : "Reconnaissance de la parole par reseaux connexionnistes",
      "author" : [ "Bottou", "Léon" ],
      "venue" : "In Proceedings of Neuro Nimes",
      "citeRegEx" : "Bottou and Léon.,? \\Q1988\\E",
      "shortCiteRegEx" : "Bottou and Léon.",
      "year" : 1988
    }, {
      "title" : "Symmetries in physics: philosophical reflections",
      "author" : [ "Brading", "Katherine", "Castellani", "Elena" ],
      "venue" : null,
      "citeRegEx" : "Brading et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Brading et al\\.",
      "year" : 2003
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "Gérard Ben", "LeCun", "Yann" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2015
    }, {
      "title" : "Open problem: The landscape of the loss surfaces of multilayer networks",
      "author" : [ "Choromanska", "Anna", "LeCun", "Yann", "Arous", "Gérard Ben" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory, COLT",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2015
    }, {
      "title" : "Local minima and plateaus in hierarchical structures of multilayer perceptrons",
      "author" : [ "Fukumizu", "Kenji", "Amari", "Shun-ichi" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Fukumizu et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Fukumizu et al\\.",
      "year" : 2000
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Xavier", "Bengio", "Yoshua" ],
      "venue" : "In Aistats,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "Computer Vision anad Pattern Recognition",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Simplifying neural nets by discovering flat minima",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1995
    }, {
      "title" : "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
      "author" : [ "Janzamin", "Majid", "Sedghi", "Hanie", "Anandkumar", "Anima" ],
      "venue" : "CoRR abs/1506.08473,",
      "citeRegEx" : "Janzamin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Janzamin et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning without poor local minima",
      "author" : [ "Kawaguchi", "Kenji" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kawaguchi and Kenji.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kawaguchi and Kenji.",
      "year" : 2016
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Stability by lyapunov’s second method with applications",
      "author" : [ "J.P. LaSalle", "S. Lefschetz" ],
      "venue" : "New York: Academic Press.,",
      "citeRegEx" : "LaSalle and Lefschetz,? \\Q1961\\E",
      "shortCiteRegEx" : "LaSalle and Lefschetz",
      "year" : 1961
    }, {
      "title" : "Efficient backprop",
      "author" : [ "LeCun", "Yann A", "Bottou", "Léon", "Orr", "Genevieve B", "Müller", "Klaus-Robert" ],
      "venue" : "In Neural networks: Tricks of the trade,",
      "citeRegEx" : "LeCun et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2012
    }, {
      "title" : "The landscape of empirical risk for non-convex losses",
      "author" : [ "Mei", "Song", "Bai", "Yu", "Montanari", "Andrea" ],
      "venue" : "arXiv preprint arXiv:1607.06534,",
      "citeRegEx" : "Mei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynamics of on-line gradient descent learning for multilayer neural networks",
      "author" : [ "Saad", "David", "Solla", "Sara A" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Saad et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Saad et al\\.",
      "year" : 1996
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya" ],
      "venue" : "arXiv preprint arXiv:1312.6120,",
      "citeRegEx" : "Saxe et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Simonyan", "Karen", "Zisserman", "Andrew" ],
      "venue" : "International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2015
    }, {
      "title" : "No bad local minima: Data independent training error guarantees for multilayer neural networks",
      "author" : [ "Soudry", "Daniel", "Carmon", "Yair" ],
      "venue" : "arXiv preprint arXiv:1605.08361,",
      "citeRegEx" : "Soudry et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Soudry et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks. In Advances in neural information processing",
      "author" : [ "Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Electron-proton dynamics in deep learning",
      "author" : [ "Zhang", "Qiuyi", "Panigrahy", "Rina", "Sachdeva", "Sushant" ],
      "venue" : "arXiv preprint arXiv:1702.00458,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : ", Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al.",
      "startOffset" : 18,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : ", Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al.",
      "startOffset" : 18,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : ", Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al.",
      "startOffset" : 18,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : ", 2012), Natural Language Processing (Sutskever et al., 2014) and Speech Recognition (Hinton et al.",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "Using Lyapunov method (LaSalle & Lefschetz, 1961), for single ReLU case we prove that gradient descent converges to w∗ with at least (1 − )/2 probability, if initialized randomly with standard deviation upper-bounded by O( / √ d), verifying common initialization techniques (Bottou, 1988; Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012),.",
      "startOffset" : 274,
      "endOffset" : 348
    }, {
      "referenceID" : 13,
      "context" : "Using Lyapunov method (LaSalle & Lefschetz, 1961), for single ReLU case we prove that gradient descent converges to w∗ with at least (1 − )/2 probability, if initialized randomly with standard deviation upper-bounded by O( / √ d), verifying common initialization techniques (Bottou, 1988; Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012),.",
      "startOffset" : 274,
      "endOffset" : 348
    }, {
      "referenceID" : 16,
      "context" : "(Saxe et al., 2013) analyzes its dynamics of gradient descent and (Kawaguchi, 2016) shows every local minimum is global.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "(Mei et al., 2016) shows the global convergence for a single nonlinear node whose derivatives of activation σ′, σ′′, σ′′′ are bounded and σ′ > 0.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 21,
      "context" : "Recent paper (Zhang et al., 2017) analyzes a similar teacher-student setting on 2-layered network when the involved function is harmonic, but it is unclear how the conclusion is generalized to ReLU case.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "(Janzamin et al., 2015) gives guarantees for parameter recovery of a 2-layered network learnt with tensor decomposition.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 8,
      "context" : "The theorem also explains why we have flat minima (Hochreiter et al., 1995; Dauphin et al., 2014) often occuring in practice.",
      "startOffset" : 50,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "Note that the global convergence claim in (Mei et al., 2016) for l2 loss does not apply to ReLU, since it requires σ′(x) > 0.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "6 gives an explanation for common initialization techniques (Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012; Bottou, 1988) that uses random variables with O(1/ √ d) standard deviation.",
      "startOffset" : 60,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "6 gives an explanation for common initialization techniques (Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012; Bottou, 1988) that uses random variables with O(1/ √ d) standard deviation.",
      "startOffset" : 60,
      "endOffset" : 134
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we explore theoretical properties of training a two-layered ReLU network g(x;w) = ∑K j=1 σ(w T j x) with centered d-dimensional spherical Gaussian input x (σ=ReLU). We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and fixed parameters w∗. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters (“out-of-plane“) are not isolated and form manifolds, and characterize inplane critical-point-free regions for two ReLU case. On the other hand, convergence to w∗ for one ReLU node is guaranteed with at least (1 − )/2 probability, if weights are initialized randomly with standard deviation upper-bounded by O( / √ d), consistent with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w∗ (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.",
    "creator" : "LaTeX with hyperref package"
  }
}