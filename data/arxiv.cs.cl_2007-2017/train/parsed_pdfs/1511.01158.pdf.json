{
  "name" : "1511.01158.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distributed Deep Learning for Answer Selection",
    "authors" : [ "Minwei Feng" ],
    "emails" : [ "<mfeng@us.ibm.com", "bingxia@us.ibm.com", "zhou>@us.ibm.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n01 15\n8v 1\n[ cs\n.L G\n] 3\nN ov\n2 01"
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep Learning technology [6] has been widely adopted in various AI tasks and has achieved the state-of-the-art performance. One practical challenge of Deep Learning is the highly time consuming training procedure. It is not unusual to see the reported training time in the magnitude of days or even weeks in research papers. However, this is rarely acceptable for practical commercial usage (e.g. training as a service on the cloud) where quick response time is expected by customers. Even for research environment the long time computation could stop scientists from running as many experiments as needed and slow down the R&D cycle. Hence the distributed deep learning training has become a crucial research direction along with the advancement of deep learning itself on the algorithm side.\nVarious infrastructures and experimental results have been published recently and many of those systems are from leading IT companies. Most of those papers report results on computer vision benchmark tasks like CIFAR10 or ImageNet. In this paper, we focus on the answer selection benchmark task from question answering (QA). It is common to observe the epoch speed (training data processing speed) increased when more computing resources are adopted. However, this does not necessarily guarantee that the convergence speed is also improved. The ultimate goal is the convergence speedup as the customers will expect models with the same accuracy available within much shorter time when the cost is increased for more computing resources. The goal of this paper is to demonstrate the sublinear scalability of the distributed training on convergence speed. We have compared the latest technologies, including SGD [1] , MSGD [9] , DOWNPOUR [3] and EASGD/EAMSGD [11].\nThe rest of the paper is organized as follows: section 2 is the summary of related work; section 3 will describe the answer selection benchmark task; we demonstrate the MPI-based infrastructure in section 4 and the review of the distributed training algorithms is given in section 5 . Experimental results are reported in section 6 and finally conclusions are drawn in section 7 ."
    }, {
      "heading" : "2 Related Works",
      "text" : "Various systems have been proposed for distributed deep learning. One of the pioneering work is Google’s Distbelief system [3] in which DOWNPOUR has been proposed. The system has multiple parameter servers and clients. Most of other work follow the same spirit of DOWNPOUR. The system Adam [2] is another similar framework which has many engineering features like reduced memory copies and mitigating the impact of slow machines. IBM’s Rudra system [5] is a\nA Q HLQA CNNQA\nP + R\nP + R Similarity\nFigure 1: Architecture. HL is the hidden layer with Tanh activation function. CNN is the convolutional neural networks. P stands for maxpooling and R stands for the ReLU activation function. QA means the weights of corresponding layer are shared by Q and A.\nmaster-client based distributed framework where the servers are organized as a tree structure to save communication overhead. A parameter server framework is proposed in [7] that supports flexible consistency models, elastic scalability and continuous fault tolerance. [7] provides the APIs so that other framework like MXNet1 can utilize it. The platform Petuum [10] supports a synchronization model with bounded staleness. Compared to the previous work, the main contribution of this paper is that we study a different task answer selection and state-of-the-art algorithms have been carefully compared."
    }, {
      "heading" : "3 Answer Selection Task",
      "text" : "This section describes the benchmark test used in this paper. Different from many previous work, we work on a QA task: answer selection. The paper [4] created an open task (including the released corpus) which serves as a benchmark for comparison purpose. For the detailed description of the data and task please refer to [4]. A summary is given here to make the paper self-contained.\nAnswer selection is an important subtask of QA. Given a question q and an answer candidate pool {a1, a2, ..., as} for that question (s is the pool size), the goal is to find the best answer candidate ak, 1 ≤ k ≤ s . If the selected answer ak is inside the ground truth set (one question could have more than one correct answer) of question q , the question q is considered to be answered correctly, otherwise it is answered incorrectly. Figure 1 depicts a deep learning approach for this task. The idea is to learn a distributed vector representation of a given question and its answer candidates and then use a similarity metric to measure the matching degree. The similarity metric is Geometric mean of Euclidean and Sigmoid Dot product (GESD) k(x, y) = 11+‖x−y‖ · 1 1+exp(−(x⊺y+1)) . x and y are the vector representations of Q and A. The answer candidate which gives the highest similarity value will be selected as the answer output. The training of this approach is computational expensive due to the usage of the hinge loss. During training, for each question Q there is a positive answer A+(the ground truth). A training instance is then constructed by pairing this A+ with a negative answer A−(a wrong answer) sampled from the whole answer space. The deep learning framework generates vector representations for the question and the two candidates: VQ , VA+ and VA− . The GESD similarities GESD(VQ, VA+) and GESD(VQ, VA−) are calculated and their difference is compared to a margin m: GESD(VQ, VA+) − GESD(VQ, VA−) < m . If this condition is not satisfied, there is no update to the model and a new negative example is sampled until the margin is less than m ( this repetitive negative sampling procedure is time-consuming and to reduce running time we set maximum 100 times in this paper)."
    }, {
      "heading" : "4 MPI-based Framework",
      "text" : "We have chosen Message Passing Interface (MPI) technology to make the master and client communicate with each other. Message passing is designed for application like deep learning where frequent communications need to be conducted and has been reported to be more efficient than other higher level toolkit like Spark [8] . Figure 2 demonstrates our MPI framework. There are two types of process: client and server which are allocated across the high performance computing clusters. The clients conduct the forward pass/backward pass of the neural network and send the update messages to servers. The servers hold a central model. They receive the messages from clients and update the central model and send the latest model back to the clients. In this paper we always\n1https://github.com/dmlc/mxnet\nuse the non-blocking communication (MPI_ISend/MPI_IRecv) to increase the overall speed. To reduce the communication overhead, we split the model into partitions and set up multiple servers. Each server is responsible for the storage and update of one model partition. In this paper, we take a symmetric structure: the amount of client and server is same. For the method comparison purpose, the accuracy of test corpus should be calculated during the training procedure. Therefore we add a tester which will get the latest model from the servers and run testing over the test corpus periodically. We use the popular toolkit MPICH 2 in this work."
    }, {
      "heading" : "5 Distributed Training Algorithms",
      "text" : "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] . The DOWNPOUR and EASGD algorithms have been given in Table 1 and Table 2 . In both tables the loop is endless because the purpose of this work is to compare the convergence speed. In practice Until forever should be Until stop criteria satisfied. x̃ represents the central model maintained by the servers. xi is the model hold by the client i. gi ti (xi) is the local gradient calculation by client i at time step ti. η represents the learning rate. Notice there is a τ in both algorithms. It is a crucial hyper parameter which controls the communication frequency. For DOWNPOUR τ controls the gradient exchange frequency and for EASGD τ controls the elastic difference α(xi − x̃) exchange frequency.\nAlgorithm 1 DOWNPOUR: on client i and the server\nInput: learning rate η, communication period τ ∈ N Initialize: randomly initialized x̃, xi = x̃, vi = 0, ti = 0\nRepeat if (τ divides ti) then x̃← x̃+ vi\nxi ← x̃\nvi ← 0 end xi ← xi − ηgi\nti (xi)\nvi ← vi − ηgi ti (xi)\nti ← ti + 1 Until forever\nTable 1: DOWNPOUR\nAlgorithm 2 EASGD: on client i and the server\nInput: learning rate η, moving rate α, communication period τ ∈ N Initialize: randomly initialized x̃, xi = x̃, ti = 0\nRepeat if (τ divides ti) then xi ← xi − α(xi − x̃) x̃← x̃+ α(xi − x̃)\nend xi ← xi − ηgi\nti (xi)\nti ← ti + 1 Until forever\nTable 2: EASGD"
    }, {
      "heading" : "6 Experimental Results",
      "text" : "Table 3 demonstrates the comparison results of convergence speedup. For the method of SGD and MSGD, we let the training keeps running for 5 days. For other methods which use multiple servers and clients, the running time limit is then set to be 12 hours. This is to save the computing resources so that more experiments can be scheduled. Also in practice it is much less meaningful if the running time is still prohibitive when large amount of computing resources are used. As shown in [4], the accuracy on the test1 corpus is around 63%. Hence for the comparison purpose, we define the convergence speed as the time needed for the accuracy score of test1 corpus to climb up to 63%. SGD is set as the baseline.\nTable 3a shows the performance of different methods under various learning rates and it is clear that the learning rate makes a crucial impact to the convergence speed. In Figure 3 and Figure 4, selected runs in Table 3a have been plotted. We then fix the learning rate and tune the communication period τ in Table 3b. Notice in 3a, DOWNPOUR with lr = 0.05 achieves 57x speed up. However, in Figure 4 it (yellow solid square) shows that DOWNPOUR with lr = 0.05 will diverge in the end. So for the study in Table 3b, the learning rate is set to be 0.01 for DOWNPOUR. From the results in Table 3b we find both EASGD/EAMSGD will suffer from the increased communication period while DOWNPOUR is immune to the performance loss with increased communication period. This is an important property, especially when the client quantity is large which could cause heavy load\n2http://www.mpich.org/\nto the network. It means for DOWNPOUR we can achieve faster convergence with more clients and at the same time by increasing the communication period the network load is controlled without performance loss.\nThe best speedup of various methods are as follows: (1) DOWNPOUR 39 times/48 workers, 47 times/60 workers and 83 times/120 workers; (2) EASGD 14 times/48 workers; (3) EAMSGD 31 times/48 workers."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have conducted an empirical study for the answer selection task which is a crucial component of QA. We build the framework with MPI. The state-of-the-art algorithms have been compared, including SGD/MSGD, DOWNPOUR and EASGD/EAMSGD. To our best knowledge, it is the first time that the experimental results for distributed deep learning have been reported on a QA task. This work proves the significance of the distributed training. A 83x speedup is achievable with the deployment of 120 workers, which is a huge gain for practical productivity. We realize that due to the lack of a solid mathematical foundation, the distributed deep learning is still a trial-anderror procedure. Our experiences show that the hyper parameter tuning can play a crucial role for the convergence speed. On the other hand, the task itself could change the performance. For example, in [11] the EASGD/EAMSGD work better than DOWNPOUR for the image classification tasks while for this answer selection task DOWNPOUR performs better. From Figure 4, we find DOWNPOUR converges faster than EAMSGD while EAMSGD can keep higher accuracy value during the latter half of the 12 hours. For future work we plan to combine EAMSGD and DOWNPOUR so that both methods could benefit from each other and further improvement could be achieved."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors thank Sixin Zhang for implementation guidance, helpful discussion and valuable feedback."
    } ],
    "references" : [ {
      "title" : "Online learning in neural networks. chapter Online Learning and Stochastic Approximations",
      "author" : [ "Léon Bottou" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1998
    }, {
      "title" : "Project adam: Building an efficient and scalable deep learning training system",
      "author" : [ "Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman" ],
      "venue" : "In Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc’Aurelio Ranzato", "Andrew W. Senior", "Paul A. Tucker", "Ke Yang", "Andrew Y. Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Applying deep learning to answer selection: A study and an open task",
      "author" : [ "Minwei Feng", "Bing Xiang", "Michael R. Glass", "Lidan Wang", "Bowen Zhou" ],
      "venue" : "In Proceedings of the 2015 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Model Accuracy and Runtime Tradeoff in Distributed Deep Learning",
      "author" : [ "S. Gupta", "W. Zhang", "J. Milthorpe" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Scaling distributed machine learning with the parameter server",
      "author" : [ "Mu Li", "David G. Andersen", "Jun Woo Park", "Alexander J. Smola", "Amr Ahmed", "Vanja Josifovski", "James Long", "Eugene J. Shekita", "Bor-Yiing Su" ],
      "venue" : "In 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Large-scale logistic regression and linear support vector machines using spark",
      "author" : [ "Chieh-Yen Lin", "Cheng-Hao Tsai", "Ching-Pei Lee", "Chih-Jen Lin" ],
      "venue" : "IEEE International Conference on Big Data,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML-13),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Petuum: A new platform for distributed machine learning on big data",
      "author" : [ "Eric P. Xing", "Qirong Ho", "Wei Dai", "Jin-Kyu Kim", "Jinliang Wei", "Seunghak Lee", "Xun Zheng", "Pengtao Xie", "Abhimanu Kumar", "Yaoliang Yu" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Deep learning with elastic averaging SGD",
      "author" : [ "Sixin Zhang", "Anna Choromanska", "Yann LeCun" ],
      "venue" : "In Proceedings of the 2015 Conference on Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We have compared the latest technologies, including SGD [1] , MSGD [9] , DOWNPOUR [3] and EASGD/EAMSGD [11].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "We have compared the latest technologies, including SGD [1] , MSGD [9] , DOWNPOUR [3] and EASGD/EAMSGD [11].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "We have compared the latest technologies, including SGD [1] , MSGD [9] , DOWNPOUR [3] and EASGD/EAMSGD [11].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "We have compared the latest technologies, including SGD [1] , MSGD [9] , DOWNPOUR [3] and EASGD/EAMSGD [11].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "One of the pioneering work is Google’s Distbelief system [3] in which DOWNPOUR has been proposed.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "The system Adam [2] is another similar framework which has many engineering features like reduced memory copies and mitigating the impact of slow machines.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "IBM’s Rudra system [5] is a",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "A parameter server framework is proposed in [7] that supports flexible consistency models, elastic scalability and continuous fault tolerance.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "[7] provides the APIs so that other framework like MXNet1 can utilize it.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "The platform Petuum [10] supports a synchronization model with bounded staleness.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "The paper [4] created an open task (including the released corpus) which serves as a benchmark for comparison purpose.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 3,
      "context" : "For the detailed description of the data and task please refer to [4].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "Message passing is designed for application like deep learning where frequent communications need to be conducted and has been reported to be more efficient than other higher level toolkit like Spark [8] .",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 0,
      "context" : "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "We have compared state-of-the-art algorithms: stochastic gradient descent (SGD) [1] , momentum stochastic gradient descent (MSGD) [9], DOWNPOUR [3] , elastic averaging stochastic gradient descent (EASGD) [11] and its variation momentum EASGD (EAMSGD) [11] .",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 3,
      "context" : "As shown in [4], the accuracy on the test1 corpus is around 63%.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "For example, in [11] the EASGD/EAMSGD work better than DOWNPOUR for the image classification tasks while for this answer selection task DOWNPOUR performs better.",
      "startOffset" : 16,
      "endOffset" : 20
    } ],
    "year" : 2015,
    "abstractText" : "This paper is an empirical study of the distributed deep learning for a question answering subtask: answer selection. Comparison studies of SGD, MSGD, DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results show that the message passing interface based distributed framework can accelerate the convergence speed at a sublinear scale. This paper demonstrates the importance of distributed training: with 120 workers, an 83x speedup is achievable and running time is decreased from 107.9 hours to 1.3 hours, which will benefit the productivity significantly.",
    "creator" : "gnuplot 5.0 patchlevel 0"
  }
}