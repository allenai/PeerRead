{
  "name" : "1702.08533.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Competing Bandits: Learning under Competition",
    "authors" : [ "Yishay Mansour", "Aleksandrs Slivkins", "Zhiwei Steven Wu" ],
    "emails" : [ "mansour@tau.ac.il", "slivkins@microsoft.com", "wuzhiwei@cis.upenn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n08 53\n3v 1\n[ cs\n.G T\n] 2\n7 Fe\nb 20\nAs a model, we consider competition between two multi-armed bandit algorithms faced with the same bandit instance. Users arrive one by one and choose among the two algorithms, so that each algorithm makes progress if and only if it is chosen. We ask whether and to which extent competition incentivizes innovation: adoption of better algorithms. We investigate this issue for several models of user response, as we vary the degree of rationality and competitiveness in the model. Effectively, we map out the “competition vs. innovation” relationship, a well-studied theme in economics."
    }, {
      "heading" : "1 Introduction",
      "text" : "Learning from interactions with users is ubiquitous in modern customer-facing systems, from product recommendations to web search to spam detection to content selection to fine-tuning the interface. Many systems purposefully implement exploration: making potentially suboptimal choices for the sake of acquiring new information. Randomized controlled trials, a.k.a. A/B testing, are an industry standard, with a number of companies such as Optimizely offering tools and platforms to facilitate them. Many companies use more sophisticated exploration methodologies based on multi-armed bandits, a well-known theoretical framework for exploration and making decisions under uncertainty.\nSystem that engages in exploration typically need to compete against one another; most importantly, they compete for users. This creates an interesting tension between exploration and competition. In a nutshell, while exploring may be essential for improving the service tomorrow, it may degrade quality andmake users leave today, in which case there will be no users to learn from! Thus, users play three distinct roles: they are customers that generate revenue, they are sources of data for learning, and they are self-interested agents which choose among the competing systems.\n*Tel Aviv University. Email: mansour@tau.ac.il †Microsoft Research-New York City. Email: slivkins@microsoft.com ‡University of Pennsylvania. Email: wuzhiwei@cis.upenn.edu\nWe initiate a study of the interplay between exploration and competition. The main high-level question is: whether and to which extent competition incentivizes adoption of better exploration algorithms. This translates into a number of more concrete questions. While it is commonly assumed that better learning technology always helps, is this so for our setting? In other words, would a better learning algorithm result in higher utility for a principal? Would it be used in an equilibrium of the “competition game”? Also, does competition lead to better social welfare compared to a monopoly? We investigate these questions for several models, as we vary the capacity of users to make rational decisions and the severity of competition between the learning systems. (In our models, the two are coupled as they are controlled by the same “knob”.)\nThe relationship between the severity of competition among firms and the quality of technology adopted as a result of this competition is a familiar theme in economics literature, known as “competition vs. innovation”. We frame our contributions in terms of the “inverted-U relationship”, a conventional wisdom regarding “competition vs. innovation” (see Figure 1)."
    }, {
      "heading" : "1.1 Our model",
      "text" : "We define a game in which two firms (principals) simultaneously engage in exploration and compete for users (agents). These two process are interlinked, as exploration decisions are experienced by users and informed by their feedback. We need to specify several conceptual pieces: how the principals and agents interact, what is the machine learning problem faced by each principal, and details of the game between the principals and the agents. Each piece can get extremely complicated in isolation, let alone jointly, so we strive for simplicity. Thus, the game is as follows:\n• A new agent arrives in each round, and chooses among the two principals. The principal chooses an action (e.g., a list of web search results to show to the agent), the user experiences this action, and reports a reward.\n• Each principal faces a very basic and well-studied version of the multi-armed bandit problem: for each arriving agent, it chooses from a fixed set of actions (a.k.a. arms) and receives a reward drawn independently from a fixed distribution specific to this action.\n• What happens with a given agent is only observed by this agent and the principal chosen by this agent. Principals simultaneously announce their learning algorithms before the agents\nstart arriving, and cannot change them afterwards. All agents share the same Bayesian prior on the rewards and the same “decision rule” for choosing among the principals.\nOur model side-steps many potential complexities, including, resp.: (i) agents who arrivemultiple times and may potentially learn over time and/or manipulate the principals’ learning algorithms, (ii) numerous well-motivated generalizations ofmulti-armed bandits studied inmachine learning, particularly ones that concern rewards that change over time. (iii) agents and principals secondguessing and gaming one another as the game progresses. In particular, each agent has welldefined beliefs about the agents that came before, and therefore is capable of making a decision, and each principal’s “strategy” boils down to a multi-armed bandit algorithm (which is oblivious to the game-theoretic aspects of the model)."
    }, {
      "heading" : "1.2 Our results",
      "text" : "Our results depend crucially on agents’ “decision rule” for choosing among the principals. The simplest and perhaps the most obvious rule is to select the principal which maximizes their expected utility; we refer to it as HardMax. We find that HardMax is not conducive to innovation. In fact, each principal’s dominating strategy is to do no purposeful exploration whatsoever, and instead always choose an action that maximizes expected reward given the current information; we call this algorithm DynamicGreedy. While this algorithm may potentially try out different actions over time and acquire useful information, it is known to be dramatically bad in many important cases of multi-armed bandits — precisely because it does not explore on purpose, and may therefore fail to discover best/better actions. Further, we show that HardMax is very sensitive to tie-breaking when both principals have exactly the same expected utility according to agents’ beliefs. If tie-breaking is probabilistically biased — say, principal 1 is always chosen with probability strictly larger than 12 — then this principal has a simple “winning strategy” no matter what the other principal does.\nWe relax HardMax to allow each principal to be chosen with some fixed baseline probability. One intuitive interpretation is that there are “random agents” who choose a principal uniformly at random, and each arriving agent is either HardMax or “random” with some fixed probability. We call this model HardMax&Random. We find that innovation helps in a big way: a sufficiently better algorithm is guaranteed to win all agents after an initial learning phase. While the precise notion of “sufficiently better algorithm” is rather subtle, we note that commonly known “smart” bandit algorithms typically defeat the commonly known “naive” ones, and the latter typically defeat DynamicGreedy. However, there is a substantial caveat: one can defeat any algorithm by interleaving it with DynamicGreedy (see Section 5 for details). This has two undesirable corollaries: a better algorithm may sometimes lose, and pure Nash equilibrium typically does not exist.\nWe further relax the decision rule so that the probability of choosing a given principal varies smoothly as a function of the difference between principals’ expected rewards; we call it SoftMax. For this model, the “better algorithmwins” result holds under much weaker assumptions on what constitutes a better algorithm. This is the most technical result of the paper. The competition in this setting is necessarily much more relaxed: typically, both principals attract approximately half of the agents as time goes by (but a better algorithm may attract slightly more).\nEconomic implications. Our models differ in terms of rationality in agents’ decision-making: from fully rational decisions with HardMax to relaxed rationality with HardMax&Random to an even more relaxed rationality with SoftMax. The decision rule also controls the severity of competition\nbetween the principals: from cut-throat competition with HardMax to a more relaxed competition with HardMax&Random to an even more relaxed competition with SoftMax. Further, uniform choice among principals corresponds to no rationality and no competition.\nThe results discussed above imply an inverted-U relationship between rationality/competition and innovation, in the spirit of Figure 1, where innovation refers to the quality of multi-armed bandit algorithms selected in an equilibrium. Further, we find another, technically different inverted-U relationship, wherewe vary rationality/competition inside the HardMax&Randommodel, and we measure innovation via the marginal utility of switching to a better algorithm.\nRemark. Much of the challenge in this paper, both conceptual and technical, was in setting up the theorems rather than proving them. Apart from making the modeling choices described in Section 1.1, it was crucial to interpret the results and intuitions from the literature onmulti-armed bandits so as to formulate meaningful assumptions which are productive in our setting."
    }, {
      "heading" : "1.3 Map of the paper.",
      "text" : "We survey related work (Section 2), lay out the model and preliminaries (Section 3), and proceed to analyze the three main models, HardMax, HardMax&Random and SoftMax (in Sections 4, 5, 6, resp.). We discuss economic implications in Section 7. Appendix A provides some pertinent background on multi-armed bandits."
    }, {
      "heading" : "2 Related work",
      "text" : "Exploration. Multi-armed bandits (MAB) is a particularly elegant and tractable abstraction for tradeoff between exploration and exploitation: essentially, between acquisition and usage of information. MAB problems have been studied in Economics, Operations Research and Computer Science for many decades, see (Bubeck and Cesa-Bianchi, 2012; Gittins et al., 2011) for background on regret-minimizing and Bayesian formulations, respectively. A discussion of industrial applications of MAB can be found in Agarwal et al. (2016).\nThe literature on MAB is vast and multi-threaded. The most related thread concerns regretminimizingMAB formulations with IID rewards (Lai and Robbins, 1985; Auer et al., 2002a). This thread includes “smart” MAB algorithms that combine exploration and exploitation, such as UCB1 (Auer et al., 2002a) and Successive Elimination (Even-Dar et al., 2006). Specific algorithms, and ‘naive” MAB algorithms that separate exploration and exploitation, such as Explore-thenExploit and ǫ-Greedy.\nThe three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.g., Athey and Segal, 2013; Bergemann and Välimäki, 2010; Kakade et al., 2013), pay-per-click ad auctions with unknown click probabilities (e.g., Babaioff et al., 2014; Devanur and Kakade, 2009; Babaioff et al., 2015), coordinating search andmatching by selfinterested agents (Kleinberg et al., 2016), as well as human computation (e.g., Ho et al., 2014; Ghosh and Hummel, 2013; Singla and Krause, 2013).\nBolton and Harris (1999); Keller et al. (2005); Gummadi et al. (2012) studiedmodels with selfinterested agents jointly performing exploration, with no principal to coordinate them.\nThere is a superficial similarity— in name only — between this paper and the line of work on “dueling bandits” (e.g., Yue et al., 2012; Yue and Joachims, 2009). The latter is not about competing bandit algorithms, but rather about scenarios where in each round two arms are chosen to be presented to a user, and the algorithm only observes which arm has “won the duel”.\nOur setting is closely related to the “dueling algorithms” framework (Immorlica et al., 2011) which studies competition between two principals, each running an algorithm for the same problem. However, this work considers algorithms for offline / full input scenarios, whereas we focus on online machine learning and the explore-exploit-incentives tradeoff therein. Also, this work specifically assumes binary payoffs (i.e., win or lose) for the principals.\nOther related work in economics. The competition vs. innovation relationship and the invertedU shape thereof have been introduced (among many other ideas) in a classic book (Schumpeter, 1942), and remained an important theme in the literature ever since (e.g., Aghion et al., 2005; Vives, 2008). Production costs aside, this literature treats innovation as a priori beneficial for the firm. Our setting is very different, as innovation in exploration algorithms may potentially hurt the firm.\nA line of work on platform competition, starting with Rysman (2009), concerns competition between firms (platforms) that improve as they attract more users (network effect); seeWeyl and White (2014) for a recent survey. This literature is not concerned with innovation, and typically models network effects exogenously, whereas in our model network effects are endogenous (they are created by MAB algorithms, an essential part of the model).\nRelaxed versions of rationality similar to ours are found in several notable lines of work. For example, “randomagents” (a.k.a. noise traders) can side-step the “no-trade theorem” (Milgrom and Stokey, 1982), a famous impossibility result in financial economics. SoftMax model is closely related to the literature on product differentiation, starting from Hotelling (1929), see Perloff and Salop (1985) for a notable later paper.\nThere is a large literature on non-existence of equilibria due to small deviations (which is related to the corresponding result for HardMax&Random), starting with Rothschild and Stiglitz (1976) in the context of health insurance markets. Notable recent papers (Veiga and Weyl, 2016; Azevedo and Gottlieb, 2017) emphasize the distinction between HardMax and versions of SoftMax.\nWhile agents’ rationality and severity of competition are usually modeled separately in the literature, it is not unusual to have themmodeled with the same “knob” (e.g., Gabaix et al., 2016)."
    }, {
      "heading" : "3 Basic model and preliminaries",
      "text" : "Principals and agents. There are two principals and T agents. The game proceeds in rounds (we will sometimes refer to them as global rounds). In each round t ∈ [T ], the following interaction takes place. A new agent arrives and chooses one of the two principals. The principal chooses a recommendation: an action at ∈ A, where A is a fixed set of actions (same for both principals and all rounds). The agent follows this recommendation, receives a reward rt ∈ [0,1], and reports it back to the principal.\nThe rewards are i.i.d. with a common prior. More formally, for each action a ∈ A there is a parametric family ψa(·) of reward distributions, parameterized by the mean reward µa. (The paradigmatic case is 0-1 rewards with a given expectation.) The mean reward vector µ = (µa : a ∈ A) is drawn from prior distribution Pmean before round 1. Whenever a given action a ∈ A is chosen,\nthe reward is drawn independently from distributionψa(µa). The prior Pmean and the distributions (ψa(·) : a ∈ A) constitute the (full) Bayesian prior on rewards, denoted P .\nEach principal commits to a learning algorithm for making recommendations. This algorithm follows a protocol of multi-armed bandits (MAB). Namely, the algorithm proceeds in time-steps:1 each time it is called, it outputs a chosen action a ∈ A and then inputs the reward for this action. The algorithm is called only in global rounds when the corresponding principal is chosen.\nThe information structure is as follows. The prior P is known to everyone. The mean rewards µa are not revealed to anybody. Each agent knows both principals’ algorithms, and the global round when (s)he arrives. Each principal is completely unaware of the rounds when the other is chosen.\nSome terminology. The two principals are called “principal 1” and “principal 2”. The algorithm of principal i ∈ {1,2} is called “algorithm i” and denoted algi . The agent in global round t is called “agent t”; the chosen principal is denoted it .\nThroughout, E[·] denotes expectation over all applicable randomness. Bayesian-expected rewards. Consider the performance of a given algorithm algi , i ∈ {1,2}, when it is run in isolation (i.e., without competition, just as a bandit algorithm). Let rewi(n) denote its Bayesian-expected reward for the n-th step.\nNow, going back to our game, fix global round t and let ni(t) denote the number of global rounds before t in which this principal is chosen. Then:\nE[rt | principal i is chosen in round t and ni(t) = n] = rewi(n+1) (∀n ∈ N).\nAgents’ response. Each agent t chooses principal it as as follows: it chooses a distribution over the principals, and then draws independently from this distribution. Let pt be the probability of choosing principal 1 according to this distribution. Below we specify pt ; we need to be careful so as to avoid a circular definition.\nLet It be the information available to agent t before the round. Assume It suffices to form posteriors for quantities ni(t), i ∈ {1,2}, denote them byNi,t . Then for each principal i,\nPMRi(t) := E[rt | It and it = i] = E[rewi(ni(t) + 1) | It] = E n∼Ni,t [rewi(n+1)].\nThis quantity represents the posterior mean reward for principal i at round t, according to information It ; hence the notation PMR. In general, probability pt is defined by the posterior mean rewards PMRi (t) for both principals. We assume a somewhat more specific shape:\npt = fresp ( PMR1(t)− PMR2(t) ) . (1)\nHere fresp : [−1,1] → [0,1] is the response function, which is the same for all agents. We assume that the response function is known to all agents.\nTo make the model well-defined, it remains to argue that information It is indeed sufficient to form posteriors on n1(t) and n2(t). This can be easily seen using induction on t.\nSince all agents arrive with identical information (other than knowing which global round they arrive in), it follows that all agents have identical posteriors for ni,t (for a given principal i and a given global round t). This posterior is denoted Ni,t.\n1These time-steps will sometimes be referred to as local steps/rounds, so as to distinguish them from “global rounds” defined before. We will omit the local vs. local distinction when clear from the context.\nResponse functions. We use the response function fresp to characterize the amount of rationality and competitiveness in our model. We assume that fresp is monotonically non-decreasing, is larger than 12 on the interval (0,1], and smaller than 1 2 on the interval [−1,0). Beyond that, we consider three specific models, listed in the order of decreasing rationality and competitiveness (see Figure 2):\n• HardMax: fresp equals 0 on the interval [−1,0) and 1 on the interval (0,1]. In words, agents choose the better principal with probability 1. • HardMax&Random: fresp equals ǫ on the interval [−1,0) and 1−ǫ′ on the interval (0,1], where ǫ,ǫ′ ∈ (0, 12 ) are some positive constants. In words, each agent is a HardMax agent with probability 1− ǫ − ǫ′, and with the remaining probability she makes a random choice. • SoftMax: fresp(·) lies in the interval [ǫ,1− ǫ], ǫ > 0, and is “smooth” around 0 (in the sense defined precisely in Section 6).\nUnless specified otherwise, fresp is symmetric, in the sense that fresp(−x) + fresp(x) = 1 for any x ∈ [0,1]. This implies fair tie-breaking: fresp(0) = 12 , and ǫ = ǫ′ in the definitions above. MAB algorithms. We characterize the inherent quality of an MAB algorithm in terms of its Bayesian Instantaneous Regret (henceforth, BIR), a standard notion from machine learning:\nBIR(n) := E µ∼Pmean\n[\nmax a∈A µa\n]\n− rew(n), (2)\nwhere rew(n) is the Bayesian-expected reward of the algorithm for the n-th step, when the algorithm is run in isolation. We are primarily interested in how BIR scales with n; we treat K , the number of arms, as a constant unless specified otherwise.\nWe will emphasize several specific algorithms or classes thereof:\n• “smart”MAB algorithms that combine exploration and exploitation, such as UCB1 Auer et al. (2002a) and Successive Elimination Even-Dar et al. (2006). These algorithms achieve BIR(n) ≤ Õ(n−1/2) for all priors and all (or all but a very few) steps n. This bound is known to be tight for any fixed n. 2\n• “naive” MAB algorithms that separate exploration and exploitation, such as Explore-thenExploit and ǫ-Greedy. These algorithms have dedicated rounds in which they explore by\n2This follows from the lower-bound analysis in Auer et al. (2002b).\nchoosing an action uniformly at random. When these rounds are known in advance, the algorithm suffers constant BIR in such rounds. When the “exploration rounds” are instead randomly chosen by the algorithm, one can usually guarantee an inverse-polynomial upper bound BIR, but not as good as the one above: namely, BIR(n) ≤ Õ(n−1/3). This is the best possible upper bound on BIR for the two algorithms mentioned above.\n• DynamicGreedy: at each step, recommends the best action according to the current posterior: an action a with the highest posterior expected reward E[µa | I ], where I is the information available to the algorithm so far. DynamicGreedy has (at least) a constant BIR for some reasonable priors, i.e., BIR(n) >Ω(1).\n• StaticGreedy: always recommends the prior best action,i.e., an action a with the highest prior mean reward Eµ∼Pmean [µa]. This algorithm typically has constant BIR.\nWe focus onMAB algorithms such that BIR(n) is non-increasing; we call such algorithmsmonotone. While some reasonable MAB algorithms may occasionally violate monotonicity, they can usually be easily modified so that monotonicity violations either vanish altogether, or only occur at very specific rounds (so that agents are extremely unlikely to exploit them in practice).\nMore background and examples can be found in Appendix A. In particular, we prove that DynamicGreedy is monotone.\nCompetition game between principals. Some of our results explicitly study the game between the two principals. We model it as a simultaneous-move game: before the first agent arrives, each principal commits to an MAB algorithm. Thus, choosing a pure strategy in this game corresponds to choosing an MAB algorithm (and, implicitly, announcing this algorithm to the agents).\nPrincipal’s utility is primarily defined as the market share, i.e., the number of agents that chose this principal. Principals are risk-neutral, in the sense that they optimize their expected utility.\nAssumptions on the prior. We make some technical assumptions for the sake of simplicity. First, each action a has a positive probability of being the best action according to the prior:\n∀a ∈ A : Pr µ∼Pmean [µa > µa′ ∀a′ ∈ A] > 0. (3)\nSecond, posterior mean rewards of actions are pairwise distinct. That is, for any step and any feasible history h of an MAB algorithm at that step,3 it holds that\nE[µa | h] , E[µa′ | h] ∀a,a′ ∈ A. (4)\nIn particular, prior mean rewards of actions are pairwise distinct: E[µa] , E[µ ′ a] for any a,a ′ ∈ A. This property is generic, e.g., it can be easily ensured by a small random perturbation of the prior.\nSomemore notation. Without loss of generality, we label actions as A = [K] and sort them according to their prior mean rewards, so that E[µ1] > E[µ2] > . . . > E[µK ].\nFix principal i ∈ {1,2} and (local) step n. The arm chosen by algorithm algi at this step is denoted ai,n, and the corresponding BIR is denoted BIRi(n). History of algi up to this step is denoted Hi,n.\nWrite PMR(a | E) = E[µa | E] for posterior mean reward of action a given event E. 3The history of an MAB algorithm at a given step comprises the chosen actions and the observed rewards in all\nprevious steps in the execution of this algorithm."
    }, {
      "heading" : "3.1 Generalizations",
      "text" : "Our results can be extended compared to the basic model described above. First, unless specified otherwise, our results allow a more general notion of principal’s utility that can depend on both the market share and agents’ rewards. Namely, principal i collects Ui(rt) units of utility in each global round t when she is chosen (and 0 otherwise), where Ui(·) is some fixed non-decreasing function with Ui(0) > 0. In a formula,\nUi := ∑T t=1 1{it=i} ·Ui(rr ). (5)\nSecond, our results carry over, with little or no modification of the proofs, to much more general versions of MAB, as long as it satisfies the i.i.d. property. In each round, an algorithm can see a context before choosing an action (as in contextual bandits) and/or additional feedback other than the reward after the reward is chosen (as in, e.g., semi-bandits), as long as the contexts are drawn from a fixed distribution, and the (reward, feedback) pair is drawn from a fixed distribution that depends only on the context and the chosen action. The Bayesian prior P needs to be a more complicated object, to make sure that PMR and BIR are well-defined. Mean rewardsmay also have a known structure, such as Lipschitzness, convexity, or linearity; such structure can be incorporated via P . All these extensions have been studied extensively in the literature onMAB, and account for a substantial segment thereof; see Bubeck and Cesa-Bianchi (2012) for background and details."
    }, {
      "heading" : "3.2 Chernoff Bounds",
      "text" : "Weuse an elementary concentration inequality known asChernoff Bounds, in a formulation fromMitzenmacher and (2005).\nTheorem 3.1 (Chernoff Bounds). Consider n i.i.d. random variables X1 . . .Xn with values in [0,1]. Let X = 1n ∑n i=1Xi be their average, and let ν = E[X]. Then:\nmin( Pr[X − ν > δν], Pr[ν −X > δν] ) < e−νnδ2/3 for any δ ∈ (0,1)."
    }, {
      "heading" : "4 Full rationality (HardMax)",
      "text" : "In this section, we will consider the version in which the agents are fully rational, in the sense that their response function is HardMax. We show that principals are not incentivized to explore— i.e., to deviate from DynamicGreedy. The core technical result is that if one principal adopts DynamicGreedy, then the other principal loses all agents as soon as he deviates.\nTo make this more precise, let us say that two MAB algorithms deviate at (local) step n if there is an action a ∈ A and a realization h of step-n history such that h is feasible for both algorithms, and under this history the two algorithms choose action a with different probability.\nTheorem4.1. Assume HardMax response function with fair tie-breaking. Assume that alg1 is DynamicGreedy, and alg2 deviates from DynamicGreedy starting from some (local) step n0 < T . Then all agents in global rounds t ≥ n0 select principal 1.\nCorollary 4.2. The competition game between principals has a unique Nash equilibirium: both principals choose DynamicGreedy.\nRemark 4.3. This corollary holds under a more general model which allows time-discounting: namely, the utility of each principal i in each global round t is Ui,t(rt) if this principal is chosen, and 0 otherwise, where Ui,t(·) is an arbitrary non-decreasing function with Ui,t(0) > 0."
    }, {
      "heading" : "4.1 Proof of Theorem 4.1",
      "text" : "The proof starts with two auxiliary lemmas: that deviating from DynamicGreedy implies a strictly smaller Bayesian-expected reward, and that HardMax implies a “sudden-death” property: if one agent chooses principal 1 with certainty, then so do all subsequent agents do. We re-use these lemmas in Section 4.2.\nLemma 4.4. With algorithms as in Theorem 4.1 we have rew1(n0) > rew2(n0). Proof. Since the two algorithms coincide on the first n0 − 1 steps, it follows by symmetry that histories H1,n0 and H2,n0 have the same distribution. We use a coupling argument: w.l.o.g., we assume the two histories coincide, H1,n0 =H2,n0 =H.\nAt local step n0, DynamicGreedy chooses an action a1,n0 which maximizes the posterior mean reward given history H: for any realization h ∈ support(H) and any action a ∈ A\nPMR(a1,n0 |H = h) ≥ PMR(a |H = h). (6) Since the two algorithms deviate at step n0, there is a realization h ∈ support(H) and an action a ∈ A such that Pr[a = a2,n0 , a1,n0 | H = h] > 0. Inequality (6) is strict for this (h,a) pair by assumption (4). Integrating (6) over a ∼ (a2,n0 | H = h) and h ∼ H, we obtain rew1(n0) > rew2(n0). Here (a2,n0 |H = h) denotes the conditional distribution of a2,n0 given H = h.\nLemma 4.5. Suppose alg1 is monotone, and PMR1(t0) > PMR2(t0) for some global round t0. Then PMR1(t) > PMR2(t) for all subsequent rounds t. Proof. Formally, let’s use induction on t, with the base case t = t0. LetN =N1,t0 be the agents’ posterior distribution for n1,t0, #global rounds before t0 in which principal 1 is chosen. By induction, all agents from t0 to t − 1 chose principal 1. Therefore,\nPMR1(t) = E n∼N [rew1(n+1+ t − t0)] ≥ E n∼N [rew1(n+1)] = PMR1(t0) > PMR2(t0) = PMR2(t),\nwhere the first inequality holds because alg1 is monotone, and the second is the base case.\nProof of Theorem 4.1. Since the two algorithms coincide on the first n0−1 steps, it follows by symmetry that rew1(n) = rew2(n) for any n < n0. By Lemma 4.4, rew1(n0) > rew2(n0).\nRecall that ni(t) is the number of global rounds s < t in which principal i is chosen, and Ni,t is the agents’ posterior distribution for this quantity. By symmetry, each agent t < n0 chooses a principal uniformly at random. It follows that N1,n0 = N2,n0 (denote both distributions by N for brevity), and N (n0 − 1) > 0. Therefore:\nPMR1(n0) = E n∼N [rew1(n+1)] =\nn0−1 ∑\nn=0\nN (n) · rew1(n+1)\n>N (n0 − 1) · rew2(n0) + n0−2 ∑\nn=0\nN (n) · rew2(n+1)\n= E n∼N [rew2(n+1)] = PMR2(n0) (7)\nSo, agent n0 chooses principal 1. By Lemma 4.5, all subsequent agents choose principal 1, too.\n4.2 HardMaxwith biased tie-breaking\nThe HardMax model is very sensitive to the tie-breaking rule. For starters, if ties are broken deterministically in favor of principal 1, then principal 1 can get all agents no matter what the other principal does, simply by using StaticGreedy.\nTheorem 4.6. Assume HardMax response function with fresp(0) = 1 (ties are always broken in favor of principal 1). If alg1 is StaticGreedy, then all agents choose principal 1.\nProof. Agent 1 chooses principal 1 because of the tie-breaking rule, and the subsequent agents choose principal 1 by an induction argument similar to the one in the proof of Lemma 4.5.\nA more challenging scenario is when the tie-breaking is biased in favor of principal 1, but not deterministically so: fresp(0) > 1 2 . Then this principal also has a “winning strategy” no matter what the other principal does. Specifically, principal 1 can get all but the first few agents, under a mild technical assumption that DynamicGreedy deviates from StaticGreedy. Principal 1 can use DynamicGreedy, or any other monotoneMAB algorithm that coincides with DynamicGreedy in the first few steps.\nTheorem 4.7. Assume HardMax response function with fresp(0) > 1 2 (i.e., tie-breaking is biased in favor of principal 1). Assume the prior P is such that DynamicGreedy deviates from StaticGreedy starting from some step n0. Suppose that principal 1 runs a monotone MAB algorithm that coincides with DynamicGreedy in the first n0 steps. Then all agents t ≥ n0 choose principal 1.\nProof. The proof re-uses Lemmas 4.4 and 4.5, which do not rely on fair tie-breaking. Because of the biased tie-breaking, for each global round t we have\nPMR1(t) ≥ PMR2(t)⇒ Pr[it = 1] > 12 . (8)\nRecall that it is the principal chosen in global round t. Letm0 be the first round when alg2 deviates from DynamicGreedy, or DynamicGreedy deviates from StaticGreedy, whichever comes sooner. Note that rew1(n) = rew2(n) for each step n < m0, by definition of m0, and rew1(n) ≥ rew2(n) by Lemma 4.4. To summarize:\nrew1(n) ≥ rew2(n) for all steps n ≤m0. (9)\nWe claim that Pr[it = 1] > 1 2 for all global rounds t ≤ m0. We prove this claim using induction on t. The base case t = 1 holds by (8) and the fact that in step 1, DynamicGreedy chooses the arm with the highest prior mean reward. For the induction step, we assume that Pr[it = 1] > 1 2 for all global rounds t < t0, for some t0 ≤ m0. It follows that distribution N1,t0 stochastically dominates distributionN2,t0.4 Observe that\nPMR1(t0) = E n∼N1,t0 [rew1(n+1)] ≥ E n∼N2,t0 [rew2(n+1)] = PMR2(t0). (10)\nSo the induction step follows by (8). Claim proved.\n4For random variables X,Y on R, we say that X stochastically dominates Y if Pr[X ≥ x] ≥ Pr[Y ≥ x] for any x ∈ R.\nNow let us focus on global round m0, and denote Ni =Ni,m0 . By the above claim,\nN1 stochastically dominatesN2, and moreoverNi(m0 − 1) >Ni(m0 − 1). (11)\nBy definition of m0, either (i) alg2 deviates from DynamicGreedy starting from local step m0, which implies rew1(m0) > rew2(m0) by Lemma 4.4, or (ii) DynamicGreedy deviates from StaticGreedy starting from local step m0, which implies rew1(m0) > rew1(m0 − 1) by Lemma A.4. In both cases, using (9) and (11), it follows that the inequality in (10) is strict for t0 =m0.\nTherefore, agent m0 chooses principal 1, and by Lemma 4.5 so do all subsequent agents."
    }, {
      "heading" : "5 Relaxed rationality: HardMax & Random",
      "text" : "This section is dedicated to the HardMax&Random response model, where each principal is always chosen with some positive baseline probability. The main technical result for this model states that a principal with asymptotically better BIR wins by a large margin: after a “learning phase” of constant duration, all agents choose this principal with maximal possible probability fresp(1). For example, a principal with BIR(n) ≤ Õ(n−1/2) wins over a principal with BIR(n) ≥ Ω(n−1/3). However, this positive result comes with a significant caveat detailed in Section 5.1.\nWe formulate and prove a cleaner version of the result, followed by amore general formulation developed in a subsequent Remark 5.2. We need to express a property that alg1 eventually catches up and surpasses alg2, even if initially it receives only a fraction of traffic. For the cleaner version, we assume that both algorithms arewell-defined for an infinite time horizon, so that their BIR does not depend on the time horizon T of the game. Then this property can be formalized as:\n(∀ǫ > 0) BIR1(ǫn)/BIR2(n)→ 0. (12)\nIn fact, a weaker version of (12) suffices: denoting ǫ0 = 1 2fresp(−1), for some constant n0 we have\n(∀n ≥ n0) BIR1(ǫ0n)/BIR2(n) < 12 . (13)\nWe also need a very mild technical assumption on the “bad” algorithm:\n(∀n ≥ n0) BIR2(n) > 2e−ǫ0n/6. (14)\nTheorem 5.1. Assume HardMax&Random response function. Suppose both algorithms are well-defined for an infinite time horizon, and satisfy (13) and (14). Then each agent t ≥ n0 chooses principal 1 with maximal possible probability fresp(1).\nProof. Consider global round t ≥ n0. Recall that each agent chooses principal 1 with probability at least fresp(−1) > 0, and denote ǫ0 = fresp(−1)/2. Then E[n1(t + 1)] ≥ 2ǫ0 t. By Chernoff Bounds (Theorem 3.1), we have that n1(t + 1) ≥ ǫ0t holds with probability at least 1 − q, where q = exp(−ǫ0t/6).\nWe need to prove that PMR1(t)− PMR2(t) > 0. For any m1 and m2, consider the quantity\n∆(m1,m2) := BIR2(m2 +1)− BIR1(m1 +1).\nWhenever m1 ≥ ǫ0t − 1 and m2 < t, it holds that\n∆(m1,m2) ≥ ∆(ǫ0t, t) ≥ BIR2(t)/2.\nThe above inequalities follow, resp., from algorithms’ monotonicity and (13). Now,\nPMR1(t)− PMR2(t) = E m1∼N1,t , m2∼N2,t [∆(m1,m2)]\n≥ −q + E m1∼N1,t , m2∼N2,t [∆(m1,m2) |m1 ≥ ǫ0t − 1] ≥ BIR2(t)/2− q > 0 (by (14)).\nRemark 5.2. Many standard MAB algorithms in the literature are parameterized by the time horizon T . Regret bounds for such algorithms usually include a polylogarithmic dependence on T . In particular, a typical upper bound for BIR has the following form:\nBIR(n | T ) ≤ polylog(T ) · n−γ for some γ ∈ (0, 12 ]. (15)\nHere we write BIR(n | T ) to emphasize the dependence on T . We generalize (13) to handle the dependence on T : for some n0 = n0(T ) ∈ polylog(T ),\n(∀n ≥ n0(T )) BIR1(ǫ0n | T ) BIR2(n | T ) < 1 2 . (16)\nIn this holds, we say that alg1 BIR-dominates alg2. We prove a version of Theorem 5.1 in which algorithms are parameterized with time horizon T and condition (13) is replaced with (16); its proof is very similar and is omitted.\nTo state a game-theoretic corollary of Theorem 5.1, we consider a version of the competition game between the two principals in which they can only choose from a finite set A of monotone MAB algorithms. One of these algorithms is “better” than all others; we call it the special algorithm. Unless specified otherwise, it BIR-dominates all other allowed algorithms. The other algorithms satisfy (14). We call this game the restricted competition game.\nCorollary 5.3. Assume HardMax&Random response function. Consider the restricted competition game with special algorithm alg. Then, for any sufficiently large time horizon T , this game has a unique Nash equilibrium: both principals choose alg."
    }, {
      "heading" : "5.1 A little greedy goes a long way",
      "text" : "Given any monotone MAB algorithm other than DynamicGreedy, we design a modified algorithm which learns at a slower rate, yet “wins the game” in the sense of Theorem 5.1. As a corollary, the competition game with unrestricted choice of algorithms typically does not have a Nash equilibrium.\nGiven an algorithm alg1 that deviates from DynamicGreedy starting from step n0 and a “mixing” parameter p, we will construct a modified algorithm as follows.\n1. The modified algorithm coincides with alg1 (and DynamicGreedy) for the first n0 − 1 steps;\n2. In each step n ≥ n0, alg1 is invokedwith probability 1−p, andwith the remaining probability p one does the “greedy choice”: chooses an action with the largest posterior mean reward given the current information collected by alg1.\nFor a cleaner comparison between the two algorithms, the modified algorithm does not record rewards received in steps with the “greedy choice”. Parameter p > 0 is the same for all steps.\nTheorem 5.4. Assume symmetric HardMax&Random response function. Let ǫ0 = fresp(±1) be the baseline probability. Suppose alg1 deviates from DynamicGreedy starting from some step n0. Let alg2 be the modified algorithm, as described above, with mixing parameter p such that (1−ǫ0)(1−p) > ǫ0. Then each agent t ≥ n0 chooses principal 2 with maximal possible probability 1− ǫ0.\nCorollary 5.5. Suppose that both principals can choose any monotone MAB algorithm, and assume the symmetric HardMax&Random response function. Then for any time horizon T , the only possible pure Nash equilibrium is one when both principals choose DynamicGreedy. Moreover, no pure Nash equilibrium exists when some algorithm “dominates” DynamicGreedy in the sense of (16) and the time horizon T is sufficiently large.\nRemark 5.6. The modified algorithm performs exploration at a slower rate. Let us argue how this may translate into a larger BIR compared to the original algorithm. Let BIR′1(n) be the BIR of the “greedy choice” after after n− 1 steps of alg1. Then\nBIR2(n) = E m∼Binomial(n,1−p)\n[ (1− p) · BIR1(m) + p · BIR′1(m) ] . (17)\nIn particular, suppose BIR1(n) ∼ n−γ and BIR′1(n) ≥ c BIR1(n), for some constants γ ∈ (0,1) and c > 1−γ . Then using Jensen’s inequality, for all n ≥ n0 and small enough p > 0 it holds that\nBIR2(n) ≥ (1− p + pc) · BIR1((1− p)n) ≥ α BIR1(n), for some constant α > 1.\n(The last inequality follows by plugging in BIR1(n) ∼ n−γ and using the fact that (1− p)γ < 1− pγ .)\nProof of Theorem 5.4. Let rew′1(n) denote the Bayesian-expected reward of the “greedy choice” after after n− 1 steps of alg1. Note that rew1(·) and rew′1(·) are non-decreasing: the former because alg1 is monotone and the latter because the “greedy choice” is optimized given an increasing set of observations. Therefore, the modified algorithm alg2 is monotone by (17).\nBy definition of the ”greedy choice”, rew1(n) ≥ rew′1(n) for all steps n, . Moreover, by Lemma 4.4, alg1 has a strictly larger rew(n0) compared to DynamicGreedy; so, rew1(n0) > rew2(n0).\nLet alg denote a copy of alg1 that is running “inside” the modified algorithm alg2. Let m2(t) be the number of global rounds before t in which the agent chooses principal 2 and alg is invoked; in other words, it is the number of agents seen by alg before global round t. LetM2,t be the agents’ posterior distribution for m2(t).\nWe claim that in each global round t ≥ n0, distribution M2,t stochastically dominates distribution N1,t, and PMR1(t) < PMR2(t). We use induction on t. The base case t = n0 holds because M2,t =N1,t (because the two algorithms coincide on the first n0−1 steps), and PMR1(n0) < PMR2(n0) is proved as in (7), using the fact that rew1(n0) < rew2(n0).\nThe induction step is proved as follows. The induction hypothesis for global round t−1 implies that agent t − 1 is seen by alg with probability (1− ǫ0)(1− p), which is strictly larger than ǫ0, the\nprobability with which this agent is seen by alg2. Therefore,M2,t stochastically dominatesN1,t.\nPMR1(t) = E n∼N1,t [rew1(n+1)]\n≤ E m∼M2,t [rew1(m+1)] (18)\n< E m∼M2,t\n[ (1− p) · rew1(m+1) + p · rew′1(m+1) ]\n(19)\n= PMR2(t).\nHere inequality (18) holds because rew1(·) is monotone and M2,t stochastically dominates N1,t, and inequality (19) holds because rew1(n0) < rew2(n0) andM2,t(n0) > 0.5"
    }, {
      "heading" : "6 SoftMax response function",
      "text" : "This section is devoted to the SoftMaxmodel. We recover a positive result under the assumptions fromTheorem 5.1 (albeit with a weaker conclusion), and then proceed to amuchmore challenging result under weaker assumptions. We start with a formal definition:\nDefinition 6.1. A response function fresp is SoftMax if the following conditions hold:\n• fresp(·) is bounded away from 0 and 1: fresp(·) ∈ [ǫ,1− ǫ] for some ǫ ∈ (0, 12 ), • the response function fresp(·) is “smooth” around 0:\n∃constants δ0, c0, c′0 > 0 ∀x ∈ [−δ0,δ0] c0 ≤ f ′resp(x) ≤ c′0. (20)\n• fair tie-breaking: fresp(0) = 12 . Our first result is a version of Theorem 5.1, with the same assumptions about the algorithms and essentially the same proof. The conclusion is much weaker: we can only guarantee that each agent t ≥ n0 chooses principal 1 with probability slightly larger than 12 . This is essentially unavoidable in a typical case when both algorithms satisfy BIR(n)→ 0, by Definition 6.1. Theorem 6.2. Assume SoftMax response function. Suppose alg1 has better BIR in the sense of (16), and alg2 satisfies technical condition (14). Then each agent t ≥ n0 chooses principal 1 with probability\nPr[it = 1] ≥ 12 + c0 4 BIR2(t). (21)\nProof Sketch. We follow the steps in the proof of Theorem 5.1 to derive\nPMR1(t)− PMR2(t) ≥ BIR2(t)/2− q, where q = exp(−ǫ0t/6).\nThis is at least BIR2(t)/4 by (14). Then (21) follows by the smoothness condition (20).\nWe recover a version of Corollary 5.3, if principal’s utility is the number of users (rather than the more general model in (5)). We also need a mild technical assumption that cumulative Bayesian regret (BReg) tends to infinity. BReg is a standard notion from the literature (along with BIR):\nBReg(n) := n · E µ∼Pmean\n[\nmax a∈A µa\n] − n ∑\nn=1\nrew(n′) = n ∑\nn′=1\nBIR(n′). (22)\n5If rew1(·) is strictly increasing, then inequality (18) is strict, too; this is becauseM2,t (t − 1) >N1,t (t − 1).\nCorollary 6.3. Assume that response function is SoftMax, and principal’s utility is the number of users. Consider the restricted competition game with special algorithm alg, and assume that all other allowed algorithms satisfy BReg(n)→∞. Then, for any sufficiently large time horizon T , this game has a unique Nash equilibrium: both principals choose alg.\nFurther, we prove a much more challenging result in which the “BIR-dominance” (16) is replaced with a much weaker condition: for some n0(T ) ∈ polylog(T ) and constants β0,α0 ∈ (0,1/2),\n(∀n ≥ n0(T )) BIR1((1− β0)n | T )\nBIR2(n | T ) < 1−α0. (23)\nIn this holds, we say that alg1 weakly BIR-dominates alg2. Note that while the BIR-dominance condition (16) involves sufficiently small multiplicative factors (resp., ǫ0 and 1 2 ), the new condition replaces them with factors that can be arbitrarily close to 1. We need a slightly stronger version of the technical assumption (14): for any ǫ > 0, there exists n(ǫ) such that for\n(∀n ≥ n(ǫ)) BIR2(n) > e−ǫn. (24)\nTheorem 6.4. Assume SoftMax response function. Suppose alg1 weakly-BIR-dominates alg2, and the latter satisfies (24). Then there exists some T such that each agent t ≥ T ′ chooses principal 1 with probability\nPr[it = 1] ≥ 12 + c0α0 4 BIR2(t). (25)\nThe main idea behind our proof is that even though alg1 may have a slower rate of learning in the beginning, it will gradually catch up and surpass alg2. We will describe this process in two phases. In the first phase, alg1 receives a random agent with probability at least fresp(−1) > 0 in each round. Although this is may be a slow rate, the difference in BIR between the two algorithms is gradually diminishing. After a sufficiently long time, alg1 attracts each agent with probability at least 1/2 −O(β0). Then the game enters the second phase: both algorithms receive agents at a rate close to 12 , and the fractions of agents received by both algorithms — n1(t)/t and n2(t)/t — also converge to 12 . In the end of the second phase, and in each global round afterwards, the agent counts n1(t) and n2(t) fit into the weak-BIR-dominance condition, in the sense that both are larger than n0(T ), and n1(t) ≥ (1 − β0) n2(t). So now alg1 actually provides better rewards, which gets reflected in the PMR’s eventually. Accordingly, from then on alg1 attracts agents at the rate slightly larger than 12 . We prove that the “bump” over the 1 2 is at least on the order of BIR2(t).\nProof of Theorem 6.4. Let ǫ0 = fresp(−1)\n2 , and so each agent chooses alg1 with probability at least 2ǫ0. Let β1 = min{c′0δ0,β0/20} with δ0 defined in (20). First, we will show that for any β1 ∈ (0,1), there exists some sufficiently large T1 such that BIR1(ǫ0T1) ≤ β1/c′0. For any t ≥ T1, we know E[n1(t + 1)] ≥ 2ǫ0 t, and by Chernoff Bounds (Theorem 3.1), we have n1(t + 1) ≥ ǫ0t holds with probability at least 1− q1(t) with q1(t) = exp(ǫ0t/6). It follows that for any t ≥ T1,\nPMR2(t)− PMR1(t) = E m1∼N1,t , m2∼N2,t [BIR1(m1 +1)− BIR2(m2 +1)]\n≤ q1(t) + E m1∼N1,t [BIR1(m1 +1) |m1 ≥ ǫ0t − 1]− BIR2(t) ≤ BIR1(ǫ0T1) ≤ β1/c′0\nwhere the second inequality follows from (14). Since the response function fresp is c ′ 0-Lipschitz in the neighborhood of [−δ0,δ0], each agent after round T1 will choose alg1 with probability at least\npt ≥ 1 2 − c′0 (PMR2(t)− PMR1(t)) ≥ 1 2 − β1.\nNext, we will show that there exists a sufficiently large T2 such that for any t ≥ T1 +T2, we can guarantee that with high probability, n1(t) >max{n0, (1−β0)n2(t)}, where n0 is defined in (23). Let us first lower bound the number agents received by alg1 after some number of rounds t = T1 +T ′ for any T ′ ≥ T1. Since each agent chooses alg1 with probability at least 1/2 − β1, by Chernoff Bounds (Theorem 3.1) we have with probability at least 1 − q2(t) that the number of agents that choose alg1 is at least (1/2−β1)T ′−A, whereA = β1T ′/8 and function q2(x) = e−c x for some constant c. Note that the number of agents received by alg2 is at most T1 + (1+ β1)T\n′/2+A. Then as long as we have T2 ≥max{ 3T1(1−β0) ,8n0}, we can guarantee that for any t ≥ T1+T2, n1(t) >\nn2(t)(1− β0) and n1(t) > n0 with probability at least 1− q2(t). Finally, we will argue that in each round t ≥ T1 +T2, we can guarantee that\nPr[it = 1] ≥ 1 2 + c0α0BIR2(t) 4\nNote that the weak BIR-dominance condition in (23) implies that for any t ≥ T1 + T2 with probability at least 1− q2(t), BIR1(n1(t)) < (1−α0)BIR2(n2(t)). It follows that for any t ≥ T1 +T2,\nPMR1(t)− PMR2(t) = E m1∼N1,t , m2∼N2,t [BIR2(m2 +1)− BIR1(m1 +1)]\n≥ (1− q2)α0BIR2(t)− q2 ≥ α0BIR2(t)/4\nwhere the last inequality holds as long as q2 ≤ α0BIR2(t)/4, and is implied by the condition in (24) as long as T2 is sufficiently large. Hence, by the definition of our SoftMax response function and assumption in (20), we have\nPr[it = 1] ≥ 1 2 + c0α0BIR2(t) 4 .\nCorollary 6.5. Assume that response function is SoftMax, and principal’s utility is the number of users. Consider the restricted competition game in which the special algorithm alg weakly-BIR-dominates the other allowed algorithms, and the latter satisfy BReg(n) → ∞. Then, for any sufficiently large time horizon T , there is a unique Nash equilibrium: both principals choose alg."
    }, {
      "heading" : "7 Economic implications",
      "text" : "We frame our contributions in terms of the relationship between competition and innovation, i.e., between the extent to which the game between the two principals is competitive, and the degree of innovation that these models incentivize. Competition is controlled via the response function fresp, and innovation refers to the quality of the technology (MAB algorithms) adopted\nby the principals. The competition vs. innovation relationship is well-studied in the economics literature, and is commonly known to often follow an inverted-U shape, as in Figure 1 (see Section 2 for citations). Competition in our models is closely correlated with rationality: the extent to which agents make rational decisions, and indeed rationality is what fresp controls directly.\nMain story. Our main story concerns the restricted competition game between the two principals where one allowed algorithm alg is “better” than the others. We measure innovation in terms of whether and when alg is chosen in an equilibrium. We vary competition/rationality by changing the response function from HardMax (full rationality, very competitive environment) to HardMax&Random to SoftMax (less rationality and competition). We find a competition/rationality vs. innovation relationship which goes as follows:\nHardMax: no innovation: DynamicGreedy is chosen over alg.\nHardMax&Random: some innovation: alg is chosen as long as it BIR-dominates.\nSoftMax: more innovation: alg is chosen as long as it weakly-BIR-dominates.6\nThis follows, resp., from Corollaries 4.2, 5.3 and 6.3. We can complete these three bullets to an inverted-U relationship if we include the uniform choice between the principals, which corresponds to the least amount of rationality. When principals’ utility is the number of agents, uniform choice provides no incentives to innovate.7 See Figure 3 for a stylized depiction of the inverted-U relationship.\nSecondary story. Let us zoom in on the symmetric HardMax&Randommodel. Competition/rationality within this model is controlled by the baseline probability ǫ0 = fresp(±1), which goes smoothly between the two extremes of HardMax and the uniform choice (resp., ǫ0 = 0 and ǫ0 = 1 2 ). For clarity, we assume that principal’s utility is the number of agents.\n6This is a weaker condition, so the innovation (switching to a better algorithm alg) happens in a broader range of scenarios.\n7On the other hand, if principals’ utility is somewhat aligned with agents’ welfare, as in (5), then a monopolist principal is incentivized to choose the best possible MAB algorithm (namely, to minimize cumulative Bayesian regret BReg(T )). Accordingly, monopoly would result in better social welfare than competition, as the latter is likely to split the market and cause each principal to learn more slowly. This is a very generic and well-known effect regarding economies of scale.\nWe consider the marginal utility of switching to a better algorithm. Suppose initially both principals use some algorithm alg, and principal 1 ponders switching to another algorithm alg’ which BIR-dominates alg. We are interested in the corresponding increase in utility; we refer to this increase as incentive-to-innovate (i2i), and we use it to quantify innovation.\nWe find the following competition/rationality vs. innovation relationship:\n• ǫ0 = 0 (HardMax): i2i can be negative if alg is DynamicGreedy.\n• ǫ0 near 0: only a small i2i can be guaranteed, as it may take a long time for alg ′ to “catch\nup” with alg, and hence less time to reap the benefits.\n• “medium-range” ǫ0: large i2i, as alg ′ learns fast and gets most agents.\n• ǫ0 near 1 2 : small i2i, as principal 1 gets most agents for free no matter what.\nThe familiar inverted-U shape is depicted in Figure 4.\nAcknowledgment The authors would like to thank GlenWeyl for discussions of related work in economics."
    }, {
      "heading" : "A Background on multi-armed bandits",
      "text" : "This appendix provides some pertinent background on multi-armed bandits (MAB). We discuss BIR andmonotonicity of severalMAB algorithms, touching upon: DynamicGreedy and StaticGreedy (Section A.1), “naive” MAB algorithms that separate exploration and exploitation (Section A.2), and “smart” MAB algorithms that combine exploration and exploitation (Section A.3).\nAs we do throughout the paper, we focus on MAB with i.i.d. rewards and a Bayesian prior; we call it Bayesian MAB for brevity.\nA.1 DynamicGreedy and StaticGreedy\nWe provide an example when DynamicGreedy and StaticGreedy have constant BIR, and prove monotonicity of DynamicGreedy. For the example, it suffices to consider deterministic rewards (for each action a, the realized reward is always equal to themean µa) and independent priors (according to the prior Pmean, random variables µ1 , . . . ,µK are mutually independent) each of full support.\nThe following claim is immediate from the definition of the CDF function\nClaim A.1. Assume independent priors. Let Fi be the CDF of the mean reward µi of action ai ∈ A. Then, for any numbers z2 > z1 > E[µ2] we have Pr[µ1 ≤ z1 and µ2 ≥ z2] = F1(z1)(1− F2(z2)).\nWe can now draw an immediate corollary of the above claim\nCorollary A.2. Consider any problem instance of Bayesian MAB with two actions and independent priors which are full support. Then:\n(a) With constant probability, StaticGreedy has a constant BIR for all steps. (b) Assuming deterministic rewards, with constant probability DynamicGreedy has a constant BIR for all steps.\nRemark A.3. A similar result holds for rewards which are distributed as Bernoulli random variables. In this case we consider accumulative reward of an action as a random walk, and use a high probability variation of the law of iterated logarithms. (Details omitted.)\nNext, we show that DynamicGreedy is monotone.\nLemma A.4. DynamicGreedy is monotone, in the sense that rew(n) is non-decreasing. Further, rew(n) is strictly increasing for every time step n with Pr[an , an+1] > 0.\nProof. We prove by induction on n that rew(n) ≤ rew(n + 1) for DynamicGreedy. Let an be the random variable recommended at time t, then E[µan |In] = rew(n). We can rewrite this as:\nrew(n) = E In [E rn [µan |rn,In]] = EIn+1 [µan |In+1]\nsince In+1 = (In, rn). At time n+1 DynamicGreedy will select an action an+1 such that:\nrew(n+1) = E[µan+1 |In+1] ≥ E[µan |In] = rew(n)\nwhich proves the monotonicity. In cases that Pr[an , an+1] > 0] we have a strict inequality, since with some probability we select a better action then the realization of an.\nA.2 “Naive” MAB algorithms that separate exploration and exploitation\nMAB algorithm ExplorExploit (m) initially explores each action with m agents and for the remaining T − |A|m agents recommends the action with the highest observed average. In the explore phase it assigns a random permutation of the mK recommendations.\nLemma A.5. The ExplorExploit (T 2/3 log |A|/δ) algorithm has, with probability 1 − δ, for any n ≥ |A|T 2/3 we have BIR (n) =O(T −1/3). In addition, ExplorExploit (m) is monotone.\nProof. In the explore phase we we approximate for each action a ∈ A, the value of µa by µ̂a. Using the standard Chernoff bounds we have that with probability 1− δ, for every action a ∈ A we have |µa − µ̂a| ≤ T −1/3.\nLet a∗ = argmaxaµa and aee the action that ExplorExploit selects in the explore phase after the first |A|T 2/3 agents. Since µ̂a∗ ≤ µ̂aee , this implies that µa∗ −µaee =O(T −1/3).\nTo show that ExplorExploit (m) is monotone, we need to show only that rew(mK) ≤ rew(mK+ 1). This follows since for any t < mK we have rew(t) = rew(t +1), since the recommended action is uniformly distributed for each time t. Also, for any t ≥ mK + 1 we have rew(t) = rew(t + 1) since we are recommending the same exploration action. The proof that rew(mK) ≤ rew(mK + 1) is the same as for DynamicGreedy in Lemma A.4.\nWe can also have a a phased version which we call PhasedExplorExploit (mt), where time is partition in to phases. In phase t we have mt agents and a random subset of K explore the actions (each action explored by a single agent) and the other agents exploit. (This implies that we need that mt ≥ K for all t. We also assume that mt is monotone in t.)\nLemma A.6. Consider the case that K = 2 and the rewards of the actions are Bernoulli r.v. with parameter µi and ∆ = µ1 − µ2. Algorithm PhasedExplorExploit (mt) is monotone and for mt = √ t it has BIR(n) =O(n−1/3 + e−O(∆ 2n2/3))). we show that the algorithm .\nProof. We first show that it is monotone. Recall that µ1 > µ2. Let Si = ∑t\nj=1 ri,j be the sum of the rewards of action i up to phase t. We need to show that Pr[S1 > S2] + (1/2)Pr[S1 = S2] is monotonically increasing in t. Consider the random variableZ = S1−S2. At each phase it increases by +1 with probability µ1(1− µ2), decreases by −1 with probability (1− µ1)µ2 and otherwise does not change.\nConsider the values of Z up to phase t. We really care only about the probability that is shifted from positive to negative and vice versa.\nFirst, consider the probability that Z = 0. We can partition it to S1 = S2 = r events, and let p(r, r) be the probability of this event. For each such event, we have p(r, r)µ1 moved to Z = +1 and p(r, r)µ2 moved to Z = −1. Since µ1 > µ2 we have that p(r, r)µ1 ≥ p(r, r)µ2 (note that p(r, r) might be zero, so we do not have a strict inequality).\nSecond, consider the probability that Z = +1 or Z = −1. We can partition it to S1 = r +1;S2 = r and S1 = r;S2 = r + 1 events, and let p(r + 1, r) and p(r, r + 1) be the probabilities of those events. It is not hard to see that p(r + 1, r)µ2 = p(r, r + 1)µ1. This implies that the probability mass moved from Z = +1 to Z = 0 is identical to that moved from Z = −1 to Z = 0.\nWe have showed that Pr[S1 > S2] + (1/2)Pr[S1 = S2] and therefore the expected valued of the exploit action is non-decreasing. Since we have that the size of the phases are increasing, the BIR is strictly increasing between phases and identical within each phase.\nWe now analyze the BIR regret. Note that agent n is in phase O(n2/3) and the length of his phase is O(n1/3). The BIR has two parts. The first is due to the exploration, which is at most O(n−1/3). The second is due to the probability that we exploit the wrong action. This happens with probability Pr[S1 < S2] + (1/2)Pr[S1 = S2] which we can bound using a Chernoff bound by e−O(∆ 2n2/3), since we explored each action O(n2/3) times.\nRemark A.7. Actually we have a tradeoff depending on the parameter mt between the regret due to exploration and exploitation. (Note that the monotonicity is always guarantee assuming mt is monotone.) If we can set that mt = 2 t then at time n we have 2/n probability of an exploit action. For the explore action we are in phase logn so the probability of a sub-optimal explore action is n−O(∆ −2). This should give us BIR(n) =O(n−O(∆ −2)).\nA.3 “Smart” MAB algorithms that combine exploration and exploitation\nMAB algorithm SuccesiveEliminationReset works as follows. It keeps a set of surviving actions As ⊆ A, where initially As = A. The agents are partition into phases, where each phase is a random permutation of the non-eliminated actions. Let µ̂i,t be the average of the rewards of action i up to phase t and µ̂∗ = maxi µ̂i,t . We eliminate action i at the end of phase t, i.e., delete it from As, if µ̂∗t − µ̂i,t > log(T /δ)/ √ t. In SuccesiveEliminationReset we simply reset the algorithm with A = As −Ae,t , where Ae,t is the set of eliminated actions after phase t. Namely, we restart µ̂i,t and ignore the old rewards before the elimination.\nLemma A.8. The algorithm SuccesiveEliminationReset, has, with probability 1 − δ, BIR (n) = O(log(T /δ)/ √ n/K).\nProof. Let the best action be a∗ = argmaxaµa. With probability 1 − δ at any time n we have that for any action i ∈ As that |µ̂i − µi | ≤ log(T /δ)/ √ n/K , and a∗ ∈ As. This implies that any action a\nsuch µa∗ − µa > 3log(T /δ)/ √ n/K is eliminated. Therefore, any action in As has BIR (n) of at most\n6log(T /δ)/ √ n/K .\nLemma A.9. Assume that if µi ≥ µj then the rewards ri stochastically dominates the rewards rj . Then, SuccesiveEliminationReset is monotone\nProof. Consider the first time T an action is eliminated, and let T = τ be a realized value of T . Then, clearly for n < τ we have rew(n) = rew(1) .\nConsider two actions a1,a2 ∈ A, such that µa1 ≥ µa2 . At time T = τ, the probability that a1 is eliminated is smaller than the probability that a2 is eliminated. This follows since µ̂a1 stochastically dominates µ̂a2 , which implies that for any threshold θ we have Pr[µ̂a1 ≥ θ] ≥ Pr[µ̂a2 ≥ θ].\nAfter the elimination we consider the expected reward of the eliminated action ∑\ni∈Aµiqi , where qi is the probability that action i was eliminated in time T = τ. We have that qi ≤ qi+1, from the probabilities of elimination.\nThe sum ∑ i∈Aµiqi with qi ≤ qi+1 and ∑ i qi = 1 is maximized by setting qi = 1/ |A|. (We can see that if there are qi , 1/ |A|, then there are two qi < qi+1, and one can see that setting both to (qi + qi+1)/2 increases the value.) Therefore we have that the rew(τ) ≥ rew(τ − 1).\nNow we can continue by induction. For the induction, we can show the property for any remaining set of at most k−1 actions. Themain issue is that SuccesiveEliminationReset restarts from scratch, so we can use induction."
    } ],
    "references" : [ {
      "title" : "Multiworld testing: A system for experimentation, learning, and decision-making, 2016. A white paper, available at https://github.com/Microsoft/mwt-ds/raw/master/images/MWT-WhitePaper.pdf",
      "author" : [ "Alekh Agarwal", "Sarah Bird", "Markus Cozowicz", "Miro Dudik", "John Langford", "Lihong Li", "Luong Hoang", "Dan Melamed", "Siddhartha Sen", "Robert Schapire", "Alex Slivkins" ],
      "venue" : null,
      "citeRegEx" : "Agarwal et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2016
    }, {
      "title" : "Competition and innovation: An inverted u relationship",
      "author" : [ "Philippe Aghion", "Nicholas Bloom", "Richard Blundell", "Rachel Griffith", "Peter Howitt" ],
      "venue" : "Quaterly J. of Economics,",
      "citeRegEx" : "Aghion et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Aghion et al\\.",
      "year" : 2005
    }, {
      "title" : "An efficient dynamic mechanism",
      "author" : [ "Susan Athey", "Ilya Segal" ],
      "venue" : null,
      "citeRegEx" : "Athey and Segal.,? \\Q2013\\E",
      "shortCiteRegEx" : "Athey and Segal.",
      "year" : 2013
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Perfect competition in markets with adverse",
      "author" : [ "Eduardo Azevedo", "Daniel Gottlieb" ],
      "venue" : "selection. Econometrica,",
      "citeRegEx" : "Azevedo and Gottlieb.,? \\Q2017\\E",
      "shortCiteRegEx" : "Azevedo and Gottlieb.",
      "year" : 2017
    }, {
      "title" : "Characterizing truthful multiarmed bandit mechanisms",
      "author" : [ "Moshe Babaioff", "Yogeshwer Sharma", "Aleksandrs Slivkins" ],
      "venue" : "SIAM J. on Computing,",
      "citeRegEx" : "Babaioff et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Babaioff et al\\.",
      "year" : 2014
    }, {
      "title" : "Truthful mechanisms with implicit payment computation",
      "author" : [ "Moshe Babaioff", "Robert Kleinberg", "Aleksandrs Slivkins" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Babaioff et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Babaioff et al\\.",
      "year" : 2015
    }, {
      "title" : "Economic recommendation systems",
      "author" : [ "Gal Bahar", "Rann Smorodinsky", "Moshe Tennenholtz" ],
      "venue" : "In 16th ACM EC,",
      "citeRegEx" : "Bahar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahar et al\\.",
      "year" : 2016
    }, {
      "title" : "The dynamic pivot mechanism",
      "author" : [ "Dirk Bergemann", "Juuso Välimäki" ],
      "venue" : null,
      "citeRegEx" : "Bergemann and Välimäki.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bergemann and Välimäki.",
      "year" : 2010
    }, {
      "title" : "Crowdsourcing exploration. Management Science, 2017",
      "author" : [ "Kostas Bimpikis", "Yiangos Papanastasiou", "Nicos Savva" ],
      "venue" : null,
      "citeRegEx" : "Bimpikis et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Bimpikis et al\\.",
      "year" : 2017
    }, {
      "title" : "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems",
      "author" : [ "Sébastien Bubeck", "Nicolo Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Optimal design for social learning",
      "author" : [ "Yeon-Koo Che", "Johannes Hörner" ],
      "venue" : "First draft:",
      "citeRegEx" : "Che and Hörner.,? \\Q2015\\E",
      "shortCiteRegEx" : "Che and Hörner.",
      "year" : 2015
    }, {
      "title" : "The price of truthfulness for pay-per-click auctions",
      "author" : [ "Nikhil Devanur", "Sham M. Kakade" ],
      "venue" : "In 10th ACM EC,",
      "citeRegEx" : "Devanur and Kakade.,? \\Q2009\\E",
      "shortCiteRegEx" : "Devanur and Kakade.",
      "year" : 2009
    }, {
      "title" : "Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems",
      "author" : [ "Eyal Even-Dar", "ShieMannor", "YishayMansour" ],
      "venue" : "J. of Machine Learning Research (JMLR),",
      "citeRegEx" : "Even.Dar et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2006
    }, {
      "title" : "Incentivizing exploration",
      "author" : [ "Peter Frazier", "David Kempe", "Jon M. Kleinberg", "Robert Kleinberg" ],
      "venue" : "In ACM EC,",
      "citeRegEx" : "Frazier et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Frazier et al\\.",
      "year" : 2014
    }, {
      "title" : "The impact of competition on prices with numerous firms",
      "author" : [ "Xavier Gabaix", "David Laibson", "Deyuan Li", "Hongyi Li", "Sidney Resnick", "Casper G. de Vries" ],
      "venue" : "J. of Economic Theory,",
      "citeRegEx" : "Gabaix et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gabaix et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning and incentives in user-generated content: multiarmed bandits with endogenous arms",
      "author" : [ "Arpita Ghosh", "Patrick Hummel" ],
      "venue" : "In ITCS,",
      "citeRegEx" : "Ghosh and Hummel.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ghosh and Hummel.",
      "year" : 2013
    }, {
      "title" : "Multi-Armed Bandit Allocation Indices",
      "author" : [ "John Gittins", "Kevin Glazebrook", "Richard Weber" ],
      "venue" : null,
      "citeRegEx" : "Gittins et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gittins et al\\.",
      "year" : 2011
    }, {
      "title" : "Mean field equilibria of multiarmed bandit games",
      "author" : [ "Ramakrishna Gummadi", "Ramesh Johari", "Jia Yuan Yu" ],
      "venue" : "In 13th ACM EC,",
      "citeRegEx" : "Gummadi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gummadi et al\\.",
      "year" : 2012
    }, {
      "title" : "Adaptive contract design for crowdsourcingmarkets: Bandit algorithms for repeated principal-agent problems",
      "author" : [ "Chien-Ju Ho", "Aleksandrs Slivkins", "Jennifer Wortman Vaughan" ],
      "venue" : "In 15th ACM EC,",
      "citeRegEx" : "Ho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 2014
    }, {
      "title" : "Stability in competition",
      "author" : [ "Harold Hotelling" ],
      "venue" : "The Economic Journal,",
      "citeRegEx" : "Hotelling.,? \\Q1929\\E",
      "shortCiteRegEx" : "Hotelling.",
      "year" : 1929
    }, {
      "title" : "Optimal dynamic mechanism design and the virtual-pivot mechanism",
      "author" : [ "Sham M. Kakade", "Ilan Lobel", "Hamid Nazerzadeh" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Kakade et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2013
    }, {
      "title" : "Strategic Experimentation with Exponential Bandits",
      "author" : [ "Godfrey Keller", "Sven Rady", "Martin Cripps" ],
      "venue" : null,
      "citeRegEx" : "Keller et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Keller et al\\.",
      "year" : 2005
    }, {
      "title" : "Descending price optimally coordinates search",
      "author" : [ "Robert D. Kleinberg", "Bo Waggoner", "E. Glen Weyl" ],
      "venue" : "Working paper,",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2016
    }, {
      "title" : "Implementing the wisdom of the crowd",
      "author" : [ "Ilan Kremer", "Yishay Mansour", "Motty Perry" ],
      "venue" : "J. of Political Economy,",
      "citeRegEx" : "Kremer et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kremer et al\\.",
      "year" : 2014
    }, {
      "title" : "Asymptotically efficient Adaptive Allocation Rules",
      "author" : [ "Tze Leung Lai andHerbert Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Robbins.",
      "year" : 1985
    }, {
      "title" : "Bayesian incentive-compatible bandit exploration",
      "author" : [ "Yishay Mansour", "Aleksandrs Slivkins", "Vasilis Syrgkanis" ],
      "venue" : "In 15th ACM EC,",
      "citeRegEx" : "Mansour et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mansour et al\\.",
      "year" : 2015
    }, {
      "title" : "Bayesian exploration: Incentivizing exploration in bayesian games",
      "author" : [ "Yishay Mansour", "Aleksandrs Slivkins", "Vasilis Syrgkanis", "Steven Wu" ],
      "venue" : "Working paper,",
      "citeRegEx" : "Mansour et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mansour et al\\.",
      "year" : 2016
    }, {
      "title" : "Information, trade and common knowledge",
      "author" : [ "Paul Milgrom", "Nancy Stokey" ],
      "venue" : "J. of Economic Theory,",
      "citeRegEx" : "Milgrom and Stokey.,? \\Q1982\\E",
      "shortCiteRegEx" : "Milgrom and Stokey.",
      "year" : 1982
    }, {
      "title" : "Probability and Computing: Randomized Algorithms and Probabilistic Analysis",
      "author" : [ "Michael Mitzenmacher", "Eli Upfal" ],
      "venue" : null,
      "citeRegEx" : "Mitzenmacher and Upfal.,? \\Q2005\\E",
      "shortCiteRegEx" : "Mitzenmacher and Upfal.",
      "year" : 2005
    }, {
      "title" : "Equilibrium with product differentiation",
      "author" : [ "Jeffrey M. Perloff", "Steven C. Salop" ],
      "venue" : "Review of Economic Studies,",
      "citeRegEx" : "Perloff and Salop.,? \\Q1985\\E",
      "shortCiteRegEx" : "Perloff and Salop.",
      "year" : 1985
    }, {
      "title" : "Equilibrium in competitive insurance markets: An essay on the economics of imperfect information",
      "author" : [ "Michael Rothschild", "Joseph Stiglitz" ],
      "venue" : "Quaterly J. of Economics,",
      "citeRegEx" : "Rothschild and Stiglitz.,? \\Q1976\\E",
      "shortCiteRegEx" : "Rothschild and Stiglitz.",
      "year" : 1976
    }, {
      "title" : "Truthful incentives in crowdsourcing tasks using regret mini",
      "author" : [ "Joseph Schumpeter. Capitalism", "Socialism", "Democracy. Harper", "Brothers", "1942. Adish Singla", "Andreas Krause" ],
      "venue" : null,
      "citeRegEx" : "2009",
      "shortCiteRegEx" : "2009",
      "year" : 1942
    }, {
      "title" : "Innovation and competitive pressure",
      "author" : [ "2016. Xavier Vives" ],
      "venue" : "J. of Industrial Economics,",
      "citeRegEx" : "Vives.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vives.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "MAB problems have been studied in Economics, Operations Research and Computer Science for many decades, see (Bubeck and Cesa-Bianchi, 2012; Gittins et al., 2011) for background on regret-minimizing and Bayesian formulations, respectively.",
      "startOffset" : 108,
      "endOffset" : 161
    }, {
      "referenceID" : 18,
      "context" : "MAB problems have been studied in Economics, Operations Research and Computer Science for many decades, see (Bubeck and Cesa-Bianchi, 2012; Gittins et al., 2011) for background on regret-minimizing and Bayesian formulations, respectively.",
      "startOffset" : 108,
      "endOffset" : 161
    }, {
      "referenceID" : 14,
      "context" : ", 2002a) and Successive Elimination (Even-Dar et al., 2006).",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.",
      "startOffset" : 169,
      "endOffset" : 319
    }, {
      "referenceID" : 15,
      "context" : "The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.",
      "startOffset" : 169,
      "endOffset" : 319
    }, {
      "referenceID" : 25,
      "context" : "The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.",
      "startOffset" : 169,
      "endOffset" : 319
    }, {
      "referenceID" : 27,
      "context" : "The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.",
      "startOffset" : 169,
      "endOffset" : 319
    }, {
      "referenceID" : 10,
      "context" : "The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.",
      "startOffset" : 169,
      "endOffset" : 319
    }, {
      "referenceID" : 8,
      "context" : "The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.",
      "startOffset" : 169,
      "endOffset" : 319
    }, {
      "referenceID" : 28,
      "context" : "The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.",
      "startOffset" : 169,
      "endOffset" : 319
    }, {
      "referenceID" : 9,
      "context" : ", 2016), dynamic auctions (e.g., Athey and Segal, 2013; Bergemann and Välimäki, 2010; Kakade et al., 2013), pay-per-click ad auctions with unknown click probabilities (e.",
      "startOffset" : 26,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : ", 2016), dynamic auctions (e.g., Athey and Segal, 2013; Bergemann and Välimäki, 2010; Kakade et al., 2013), pay-per-click ad auctions with unknown click probabilities (e.",
      "startOffset" : 26,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : ", 2013), pay-per-click ad auctions with unknown click probabilities (e.g., Babaioff et al., 2014; Devanur and Kakade, 2009; Babaioff et al., 2015), coordinating search andmatching by selfinterested agents (Kleinberg et al.",
      "startOffset" : 68,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : ", 2013), pay-per-click ad auctions with unknown click probabilities (e.g., Babaioff et al., 2014; Devanur and Kakade, 2009; Babaioff et al., 2015), coordinating search andmatching by selfinterested agents (Kleinberg et al.",
      "startOffset" : 68,
      "endOffset" : 146
    }, {
      "referenceID" : 24,
      "context" : ", 2015), coordinating search andmatching by selfinterested agents (Kleinberg et al., 2016), as well as human computation (e.",
      "startOffset" : 66,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : ", 2016), as well as human computation (e.g., Ho et al., 2014; Ghosh and Hummel, 2013; Singla and Krause, 2013).",
      "startOffset" : 38,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "A discussion of industrial applications of MAB can be found in Agarwal et al. (2016). The literature on MAB is vast and multi-threaded.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "A discussion of industrial applications of MAB can be found in Agarwal et al. (2016). The literature on MAB is vast and multi-threaded. The most related thread concerns regretminimizingMAB formulations with IID rewards (Lai and Robbins, 1985; Auer et al., 2002a). This thread includes “smart” MAB algorithms that combine exploration and exploitation, such as UCB1 (Auer et al., 2002a) and Successive Elimination (Even-Dar et al., 2006). Specific algorithms, and ‘naive” MAB algorithms that separate exploration and exploitation, such as Explore-thenExploit and ǫ-Greedy. The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.g., Athey and Segal, 2013; Bergemann and Välimäki, 2010; Kakade et al., 2013), pay-per-click ad auctions with unknown click probabilities (e.g., Babaioff et al., 2014; Devanur and Kakade, 2009; Babaioff et al., 2015), coordinating search andmatching by selfinterested agents (Kleinberg et al., 2016), as well as human computation (e.g., Ho et al., 2014; Ghosh and Hummel, 2013; Singla and Krause, 2013). Bolton and Harris (1999); Keller et al.",
      "startOffset" : 63,
      "endOffset" : 1341
    }, {
      "referenceID" : 0,
      "context" : "A discussion of industrial applications of MAB can be found in Agarwal et al. (2016). The literature on MAB is vast and multi-threaded. The most related thread concerns regretminimizingMAB formulations with IID rewards (Lai and Robbins, 1985; Auer et al., 2002a). This thread includes “smart” MAB algorithms that combine exploration and exploitation, such as UCB1 (Auer et al., 2002a) and Successive Elimination (Even-Dar et al., 2006). Specific algorithms, and ‘naive” MAB algorithms that separate exploration and exploitation, such as Explore-thenExploit and ǫ-Greedy. The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.g., Athey and Segal, 2013; Bergemann and Välimäki, 2010; Kakade et al., 2013), pay-per-click ad auctions with unknown click probabilities (e.g., Babaioff et al., 2014; Devanur and Kakade, 2009; Babaioff et al., 2015), coordinating search andmatching by selfinterested agents (Kleinberg et al., 2016), as well as human computation (e.g., Ho et al., 2014; Ghosh and Hummel, 2013; Singla and Krause, 2013). Bolton and Harris (1999); Keller et al. (2005); Gummadi et al.",
      "startOffset" : 63,
      "endOffset" : 1363
    }, {
      "referenceID" : 0,
      "context" : "A discussion of industrial applications of MAB can be found in Agarwal et al. (2016). The literature on MAB is vast and multi-threaded. The most related thread concerns regretminimizingMAB formulations with IID rewards (Lai and Robbins, 1985; Auer et al., 2002a). This thread includes “smart” MAB algorithms that combine exploration and exploitation, such as UCB1 (Auer et al., 2002a) and Successive Elimination (Even-Dar et al., 2006). Specific algorithms, and ‘naive” MAB algorithms that separate exploration and exploitation, such as Explore-thenExploit and ǫ-Greedy. The three-way tradeoff between exploration, exploitation and incentives has been studied in several other settings: incentivizing exploration in a recommendation system (Che and Hörner, 2015; Frazier et al., 2014; Kremer et al., 2014;Mansour et al., 2015; Bimpikis et al., 2017; Bahar et al., 2016;Mansour et al., 2016), dynamic auctions (e.g., Athey and Segal, 2013; Bergemann and Välimäki, 2010; Kakade et al., 2013), pay-per-click ad auctions with unknown click probabilities (e.g., Babaioff et al., 2014; Devanur and Kakade, 2009; Babaioff et al., 2015), coordinating search andmatching by selfinterested agents (Kleinberg et al., 2016), as well as human computation (e.g., Ho et al., 2014; Ghosh and Hummel, 2013; Singla and Krause, 2013). Bolton and Harris (1999); Keller et al. (2005); Gummadi et al. (2012) studiedmodels with selfinterested agents jointly performing exploration, with no principal to coordinate them.",
      "startOffset" : 63,
      "endOffset" : 1386
    }, {
      "referenceID" : 34,
      "context" : "innovation relationship and the invertedU shape thereof have been introduced (among many other ideas) in a classic book (Schumpeter, 1942), and remained an important theme in the literature ever since (e.g., Aghion et al., 2005; Vives, 2008).",
      "startOffset" : 201,
      "endOffset" : 241
    }, {
      "referenceID" : 29,
      "context" : "noise traders) can side-step the “no-trade theorem” (Milgrom and Stokey, 1982), a famous impossibility result in financial economics.",
      "startOffset" : 52,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "Notable recent papers (Veiga and Weyl, 2016; Azevedo and Gottlieb, 2017) emphasize the distinction between HardMax and versions of SoftMax.",
      "startOffset" : 22,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : ", Aghion et al., 2005; Vives, 2008). Production costs aside, this literature treats innovation as a priori beneficial for the firm. Our setting is very different, as innovation in exploration algorithms may potentially hurt the firm. A line of work on platform competition, starting with Rysman (2009), concerns competition between firms (platforms) that improve as they attract more users (network effect); seeWeyl and White (2014) for a recent survey.",
      "startOffset" : 2,
      "endOffset" : 302
    }, {
      "referenceID" : 1,
      "context" : ", Aghion et al., 2005; Vives, 2008). Production costs aside, this literature treats innovation as a priori beneficial for the firm. Our setting is very different, as innovation in exploration algorithms may potentially hurt the firm. A line of work on platform competition, starting with Rysman (2009), concerns competition between firms (platforms) that improve as they attract more users (network effect); seeWeyl and White (2014) for a recent survey.",
      "startOffset" : 2,
      "endOffset" : 433
    }, {
      "referenceID" : 1,
      "context" : ", Aghion et al., 2005; Vives, 2008). Production costs aside, this literature treats innovation as a priori beneficial for the firm. Our setting is very different, as innovation in exploration algorithms may potentially hurt the firm. A line of work on platform competition, starting with Rysman (2009), concerns competition between firms (platforms) that improve as they attract more users (network effect); seeWeyl and White (2014) for a recent survey. This literature is not concerned with innovation, and typically models network effects exogenously, whereas in our model network effects are endogenous (they are created by MAB algorithms, an essential part of the model). Relaxed versions of rationality similar to ours are found in several notable lines of work. For example, “randomagents” (a.k.a. noise traders) can side-step the “no-trade theorem” (Milgrom and Stokey, 1982), a famous impossibility result in financial economics. SoftMax model is closely related to the literature on product differentiation, starting from Hotelling (1929), see Perloff and Salop (1985) for a notable later paper.",
      "startOffset" : 2,
      "endOffset" : 1048
    }, {
      "referenceID" : 1,
      "context" : ", Aghion et al., 2005; Vives, 2008). Production costs aside, this literature treats innovation as a priori beneficial for the firm. Our setting is very different, as innovation in exploration algorithms may potentially hurt the firm. A line of work on platform competition, starting with Rysman (2009), concerns competition between firms (platforms) that improve as they attract more users (network effect); seeWeyl and White (2014) for a recent survey. This literature is not concerned with innovation, and typically models network effects exogenously, whereas in our model network effects are endogenous (they are created by MAB algorithms, an essential part of the model). Relaxed versions of rationality similar to ours are found in several notable lines of work. For example, “randomagents” (a.k.a. noise traders) can side-step the “no-trade theorem” (Milgrom and Stokey, 1982), a famous impossibility result in financial economics. SoftMax model is closely related to the literature on product differentiation, starting from Hotelling (1929), see Perloff and Salop (1985) for a notable later paper.",
      "startOffset" : 2,
      "endOffset" : 1078
    }, {
      "referenceID" : 1,
      "context" : ", Aghion et al., 2005; Vives, 2008). Production costs aside, this literature treats innovation as a priori beneficial for the firm. Our setting is very different, as innovation in exploration algorithms may potentially hurt the firm. A line of work on platform competition, starting with Rysman (2009), concerns competition between firms (platforms) that improve as they attract more users (network effect); seeWeyl and White (2014) for a recent survey. This literature is not concerned with innovation, and typically models network effects exogenously, whereas in our model network effects are endogenous (they are created by MAB algorithms, an essential part of the model). Relaxed versions of rationality similar to ours are found in several notable lines of work. For example, “randomagents” (a.k.a. noise traders) can side-step the “no-trade theorem” (Milgrom and Stokey, 1982), a famous impossibility result in financial economics. SoftMax model is closely related to the literature on product differentiation, starting from Hotelling (1929), see Perloff and Salop (1985) for a notable later paper. There is a large literature on non-existence of equilibria due to small deviations (which is related to the corresponding result for HardMax&Random), starting with Rothschild and Stiglitz (1976) in the context of health insurance markets.",
      "startOffset" : 2,
      "endOffset" : 1300
    }, {
      "referenceID" : 3,
      "context" : "• “smart”MAB algorithms that combine exploration and exploitation, such as UCB1 Auer et al. (2002a) and Successive Elimination Even-Dar et al.",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "• “smart”MAB algorithms that combine exploration and exploitation, such as UCB1 Auer et al. (2002a) and Successive Elimination Even-Dar et al. (2006). These algorithms achieve BIR(n) ≤ Õ(n−1/2) for all priors and all (or all but a very few) steps n.",
      "startOffset" : 80,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "These algorithms have dedicated rounds in which they explore by 2This follows from the lower-bound analysis in Auer et al. (2002b).",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "All these extensions have been studied extensively in the literature onMAB, and account for a substantial segment thereof; see Bubeck and Cesa-Bianchi (2012) for background and details.",
      "startOffset" : 127,
      "endOffset" : 158
    } ],
    "year" : 2017,
    "abstractText" : "Most modern systems strive to learn from interactions with users, and many engage in exploration: making potentially suboptimal choices for the sake of acquiring new information. We initiate a study of the interplay between exploration and competition—how such systems balance the exploration for learning and the competition for users. Here the users play three distinct roles: they are customers that generate revenue, they are sources of data for learning, and they are self-interested agents which choose among the competing systems. As a model, we consider competition between two multi-armed bandit algorithms faced with the same bandit instance. Users arrive one by one and choose among the two algorithms, so that each algorithm makes progress if and only if it is chosen. We ask whether and to which extent competition incentivizes innovation: adoption of better algorithms. We investigate this issue for several models of user response, as we vary the degree of rationality and competitiveness in the model. Effectively, we map out the “competition vs. innovation” relationship, a well-studied theme in economics.",
    "creator" : "LaTeX with hyperref package"
  }
}