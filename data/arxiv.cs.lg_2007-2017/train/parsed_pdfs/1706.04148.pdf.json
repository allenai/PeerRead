{
  "name" : "1706.04148.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks",
    "authors" : [ "Massimo Quadrana", "Alexandros Karatzoglou", "Balázs Hidasi", "Paolo Cremonesi" ],
    "emails" : [ "massimo.quadrana@polimi.it", "alexk@tid.es", "balazs.hidasi@gravityrd.com", "paolo.cremonesi@polimi.it" ],
    "sections" : [ {
      "heading" : null,
      "text" : "CCS CONCEPTS • Information systems → Data mining; Recommender systems;\nKEYWORDS deep learning, recurrent neural networks, gated recurrent units, recommender systems, session-based recommendation ACM Reference format: Massimo Quadrana, Alexandros Karatzoglou, Balázs Hidasi, and Paolo Cremonesi. 2017. Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks. In Proceedings of ACM Recsys conference, Como, Italy, August 26–29, 2017 (RecSys’17), 9 pages. DOI: 10.1145/nnnnnnn.nnnnnnn"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "In many online systems where recommendations are applied, interactions between a user and the system are organized into sessions. A session is a group of interactions that take place within a given time frame. Sessions from a user can occur on the same day, or over several days, weeks, or months. A\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). RecSys’17, Como, Italy © 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxxx/YY/MM. . . $15.00 DOI: 10.1145/nnnnnnn.nnnnnnn\nsession usually has a goal, such as finding a good restaurant in a city, or listening to music of a certain style or mood. Providing recommendations in these domains poses unique challenges that until recently have been mainly tackled by applying conventional recommender algorithms [10] on either the last interaction or the last session (session-based recommenders). Recurrent Neural Networks (RNN’s) have been recently used for the purpose of session-based recommendations [7] outperforming item-based methods by 15% to 30% in terms of ranking metrics. In session-based recommenders, recommendations are provided based solely on the interactions in the current user session, as user are assumed to be anonymous. But in many of these systems there are cases where a user might be logged-in (e.g. music streaming services) or some form of user identifier might be present (cookie or other identifier). In these cases it is reasonable to assume that the user behavior in past sessions might provide valuable information for providing recommendations in the next session. A simple way of incorporating past user session information in session-based algorithm would be to simply concatenate past and current user sessions. While this seems like a reasonable approach we will see in the experimental section that this does not yield the best results. In this work we describe a novel algorithm based on RNN’s that can deal with both cases: (i) session-aware recommenders, when user identifiers are present and propagate information from the previous user session to the next, thus improving the recommendation accuracy, and (ii) sessionbased recommenders, when there are no past sessions (i.e., no user identifiers). The algorithm is based on a Hierarchical RNN where the hidden state of a lower-level RNN at the end of one user session is passed as an input to a higher-level RNN which aims at predicting a good initialization (i.e., a good context vector) for the hidden state of the lower RNN for the next session of the user. We evaluate the Hierarchical RNN’s on two datasets from industry comparing them to the plain session-based RNN and to item-based collaborative filtering. Hierarchical RNN’s outperform both alternatives by a healthy margin."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Session-based recommendations. Classical CF methods (e.g. matrix factorization) break down in the session-based setting\nar X\niv :1\n70 6.\n04 14\n8v 1\n[ cs\n.L G\n] 1\n3 Ju\nn 20\n17\nwhen no user profile can be constructed from past user behavior. A natural solution to this problem is the item-to-item recommendation approach [11, 16]. In this setting an itemto-item similarity matrix is precomputed from the available session data, items that are often clicked together in sessions are deemed to be similar. These similarities are then used to create recommendations. While simple, this method has been proven to be effective and is widely employed. Though, these methods only take into account the last click of the user, in effect ignoring the information of the previous clicks. Recurrent Neural Models. RNNs are the deep models of choice when dealing with sequential data [12]. RNNs have been used in image and video captioning, time series prediction, natural language processing and much more. Long Short-Term Memory (LSTM) [9] networks are a type of RNNs that have been shown to work particularly well, it includes additional gates that regulate when and how much to take the input into account and when to reset the hidden state. This helps with the vanishing gradient problem that often plagues the standard RNN models. Slightly simplified version of LSTM – that still maintains all their properties – are Gated Recurrent Units (GRUs) [3] which we use in this work. RNNs were first used to model session data in [7]. The recurrent neural network is trained with a ranking loss on a one-hot representation of the session (clicked) item-IDs. The RNN is then used to provide recommendations after each click for new sessions. This work only focused on the clicked item-IDs in the current session while here we aim at modeling the user behavior across sessions as well. RNNs were also used to jointly model the content or features of items together with click-sequence interactions [8]. By including item features extracted for example, from the thumbnail image of videos or the textual description of a product, the so-called parallelRNN model provided superior recommendation quality wrt. ‘plain’ RNNs. In [19] proposed data augmentation techniques to improve the performance of the RNN for session-based recommendations, these techniques have though the side effect of increasing training times as a single session is split into several sub-sessions for training. RNN’s have also been used in more standard user-item collaborative filtering settings where the aim is to model the evolution of the user and items factors [20] [4] where the results are though less impressive, with the proposed methods barely outperforming standard matrix factorization methods. Finally a sequence to sequence model with a version of Hierarchical Recurrent Neural Networks was used for generative context-aware query suggestion in [17]."
    }, {
      "heading" : "3 MODEL",
      "text" : "In this section we describe the proposed Hierarchical RNN (HRNN henceforth) model for personalized session-based recommendation."
    }, {
      "heading" : "3.1 Session-based Recurrent Neural Network",
      "text" : "Our model is based on the session-based Recurrent Neural Network (RNN henceforth) model presented in [7]. RNN is\nbased on a single Gated Recurrent Unit (GRU) layer that models the interactions of the user within a session. The RNN takes as input the current item ID in the session and outputs a score for each item representing the likelihood of being the next item in the session. Formally, for each session \uD835\uDC46\uD835\uDC5A = {\uD835\uDC56\uD835\uDC5A,1, \uD835\uDC56\uD835\uDC5A,2, ..., \uD835\uDC56\uD835\uDC5A,\uD835\uDC41\uD835\uDC5A}, RNN computes the following session-level representation \uD835\uDC60\uD835\uDC5A,\uD835\uDC5B = \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60 (︀ \uD835\uDC56\uD835\uDC5A,\uD835\uDC5B, \uD835\uDC60\uD835\uDC5A,\uD835\uDC5B−1 )︀ , \uD835\uDC5B = 1, ..., \uD835\uDC41\uD835\uDC5A − 1 (1)\nwhere \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60 is the session-level GRU and \uD835\uDC60\uD835\uDC5A,\uD835\uDC5B its hidden state at step \uD835\uDC5B, being \uD835\uDC60\uD835\uDC5A,0 = 0 (the null vector), and \uD835\uDC56\uD835\uDC5A,\uD835\uDC5B is the one-hot vector of the current item ID 1. The output of the RNN is a score \uD835\uDC5F\uD835\uDC5A,\uD835\uDC5B for every item in the catalog indicating the likelihood of being the next item in the session (or, equivalently, its relevance for the next step in the session)\n\uD835\uDC5F\uD835\uDC5A,\uD835\uDC5B = \uD835\uDC54 (︀ \uD835\uDC60\uD835\uDC5A,\uD835\uDC5B )︀ , \uD835\uDC5B = 1, ..., \uD835\uDC41\uD835\uDC5A − 1 (2)\nwhere \uD835\uDC54 (︀ · )︀ is a non-linear function like softmax or tanh depending on the loss function. During training, scores are compared to a one-hot vector of the next item ID in the session to compute the loss. The network can be trained with several ranking loss functions such as cross-entropy, BPR [14] and TOP1 [7]. In this work, the TOP1 loss always outperformed other ranking losses, so we consider only it in the rest of the paper. The TOP1 loss is the regularized approximation of the relative rank of the relevant item. The relative rank of the relevant item is given by 1\uD835\uDC41\uD835\uDC46 · \uD835\uDC41\uD835\uDC46 \uD835\uDC57=1 \uD835\uDC3C{\uD835\uDC5F\uD835\uDC60,\uD835\uDC57 > \uD835\uDC5F\uD835\uDC60,\uD835\uDC56} where \uD835\uDC5F\uD835\uDC60,\uD835\uDC57 is the sore of a sampled ’irrelevant item’. \uD835\uDC3C{·} is approximated with a sigmoid. To force the scores of negative examples (’irrelevant items’) towards zero a regularization term is added to the loss. The final loss function is as follows: \uD835\uDC3F\uD835\uDC60 =\n1 \uD835\uDC41\uD835\uDC46 · \uD835\uDC41\uD835\uDC46\uD835\uDC57=1 \uD835\uDF0E (︀ \uD835\uDC5F\uD835\uDC60,\uD835\uDC57 − \uD835\uDC5F\uD835\uDC60,\uD835\uDC56 )︀ + \uD835\uDF0E (︀ \uD835\uDC5F2\uD835\uDC60,\uD835\uDC57 )︀ RNN is trained efficiently with session-parallel mini-batches. At each training step, the input to \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60 is the stacked one-hot representation of the current item ID of a batch of sessions. The session-parallel mechanism keeps the pointers to the current item of every session in the mini-batch and resets the hidden state of the RNN when sessions end. To further reduce the computational complexity, the loss is computed over the current item IDs and a sample of ‘negative’ items. Specifically, the current item ID of each session is used as positive item and the IDs of the remaining sessions in the mini-batch as ‘negative’ items when computing the loss. This makes explicit negative item sampling unnecessary and enables popularity-based sampling. However, since useridentifiers are unknown in pure session-based scenarios, there are good chances that negative samples will be ‘contaminated’ by positive items the user interacts with in other sessions."
    }, {
      "heading" : "3.2 Personalized Session-based Hierarchical Recurrent Neural Network",
      "text" : "Our HRNN model builds on top of RNN by: (i) adding an additional GRU layer to model information across user sessions and to track the evolution of the user interests over 1To simplify the explanation we use a notation similar to [17].\ntime; (ii) using a powerful user-parallel mini-batch mechanism for efficient training.\n3.2.1 Architecture. Beside the session-level GRU, our HRNN model adds one user-level GRU (\uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC62\uD835\uDC60\uD835\uDC5F) to model the user activity across-sessions. Figure 1 shows a graphical representation of HRNN . At each time step, recommendations are generated by \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60, as in RNN. However, when a session ends, the user representation is updated. When a new session starts, the hidden state of \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC62\uD835\uDC60\uD835\uDC5F is used to initialize \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60 and, optionally, propagated in input to \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60. Formally, for each user \uD835\uDC62 with sessions \uD835\uDC36\uD835\uDC62 = {\uD835\uDC46\uD835\uDC62,1, \uD835\uDC46\uD835\uDC62,2, ..., \uD835\uDC46\uD835\uDC62,\uD835\uDC40\uD835\uDC62}, the user-level GRU takes as input the session-level representations \uD835\uDC60\uD835\uDC62,1, \uD835\uDC60\uD835\uDC62,2, ..., \uD835\uDC60\uD835\uDC62,\uD835\uDC40\uD835\uDC62 , being \uD835\uDC60\uD835\uDC62,\uD835\uDC5A = \uD835\uDC60\uD835\uDC62,\uD835\uDC5A,\uD835\uDC41\uD835\uDC5A the last hidden state of \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60 of each user session \uD835\uDC46\uD835\uDC62,\uD835\uDC5A. The user-level representation \uD835\uDC50\uD835\uDC62,\uD835\uDC5A is then computed as\n\uD835\uDC50\uD835\uDC62,\uD835\uDC5A = \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC62\uD835\uDC60\uD835\uDC5F (︀ \uD835\uDC60\uD835\uDC62,\uD835\uDC5A, \uD835\uDC50\uD835\uDC62,\uD835\uDC5A−1 )︀ , \uD835\uDC5A = 1, ..., \uD835\uDC40\uD835\uDC62 (3)\nwhere \uD835\uDC50\uD835\uDC5A,0 = 0 (the null vector). The input to the user-level GRU is connected to the last hidden state of the sessionlevel GRU. In this way, the user-level GRU can track the evolution of the user across-sessions and, in turn, model the dynamics user interests seamlessly. Notice that the user-level representation is kept fixed throughout the session and it is updated only when the session ends. The user-level representation is then used to initialize the hidden state of the session-level GRU. Given \uD835\uDC50\uD835\uDC62,\uD835\uDC5A, the initial hidden state \uD835\uDC60\uD835\uDC5A+1,0 of the session-level GRU for the following session is set to\n\uD835\uDC60\uD835\uDC5A+1,0 = tanh (︀ \uD835\uDC4A\uD835\uDC56\uD835\uDC5B\uD835\uDC56\uD835\uDC61\uD835\uDC50\uD835\uDC62,\uD835\uDC5A + \uD835\uDC4F\uD835\uDC56\uD835\uDC5B\uD835\uDC56\uD835\uDC61 )︀ (4)\nwhere \uD835\uDC4A\uD835\uDC56\uD835\uDC5B\uD835\uDC56\uD835\uDC61 and \uD835\uDC4F\uD835\uDC56\uD835\uDC5B\uD835\uDC56\uD835\uDC61 are the initialization weights and biases respectively. In this way, the contextual information relative to previous user sessions is transferred at session-level. Beside initialization, the \uD835\uDC50\uD835\uDC62,\uD835\uDC5A can be optionally propagated in input to the session-level GRU. Session-level representations are now computed as follows\n\uD835\uDC60\uD835\uDC5A,\uD835\uDC61 = \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60 (︀ \uD835\uDC56\uD835\uDC5A,\uD835\uDC5B, \uD835\uDC60\uD835\uDC5A,\uD835\uDC5B−1, \uD835\uDC50\uD835\uDC62,\uD835\uDC5A−1 )︀ , \uD835\uDC5B = 1, ..., \uD835\uDC41\uD835\uDC5A − 1\n(5) The model is trained end-to-end using back-propagation [15]. The weights of \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC62\uD835\uDC60\uD835\uDC5F are updated only between sessions, i.e. when a session ends and when the forthcoming session starts. However, when the user representation is propagated in input to \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60, the weights of \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC62\uD835\uDC60\uD835\uDC5F are updated also within sessions even if \uD835\uDC50\uD835\uDC62,\uD835\uDC5A is kept fixed. We also tried with propagating the user-level representation to the final prediction layer (i.e., by adding term \uD835\uDC50\uD835\uDC62,\uD835\uDC5A in Equation 2) but we always incurred into severe degradation of the performances, even wrt. simple session-based RNN. We therefore discarded this setting from this discussion. Note here that the \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC62\uD835\uDC60\uD835\uDC5F does not simple pass on the hidden state of the previous user session to the next but also learns (during training) how user sessions evolve during time. We will see in the experimental section that this is crucial in achieving increased performance. In effect \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC62\uD835\uDC60\uD835\uDC5F computes and evolves a user profile that is based on the previous\nuser sessions, thus in effect personalizing the \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60. In the original RNN users who had clicked/interacted with the same sequence of items in a session would get the same recommendations, in HRNN this is not anymore the case, recommendations will be influenced by the the users past sessions as well. In summary, we considered two different HRNN settings:\n∙ HRNN Init, in which \uD835\uDC50\uD835\uDC62,\uD835\uDC5A is used only to initialize the representation of the next session. ∙ HRNN All, in which \uD835\uDC50\uD835\uDC62,\uD835\uDC5A is used for initialization and propagated in input at each step of the next session\nIn HRNN Init, the session-level GRU can exploit the contextual information along with the session-level dynamics of the user interest. HRNN All instead enforces the usage of the user representation at session-level at the expense of a greater complexity of the model. As we will see, this can lead to substantially different results depending on the recommendation scenario.\n3.2.2 Learning. For the sake of efficiency in training, we have edited the session-parallel mini-batch mechanism described in [8] to account for user identifiers during training (see Figure 2). We first group sessions by user and then sort session events within each group by time-stamp. We then order users at random. At the first iteration, the first item of the first session of the first \uD835\uDC35 users form the input to the HRNN ; the second item in each session form its output. The output is then used as input for the next iteration, and so on. When a session in the mini-batch ends, Equation 3 is used to update the hidden state of \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC62\uD835\uDC60\uD835\uDC5F and Equation 4 to initialize the hidden state of \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60 for the forthcoming session, if any. When a user has been processed completely, the hidden states of both \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC62\uD835\uDC60\uD835\uDC5F and \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60 are reset and the next user is put in its place in the mini-batch. With user-parallel mini-batches we can train HRNN s efficiently over users having different number of sessions and sessions of different length. Moreover, this mechanism allows to sample negative items in a user-independent fashion, hence reducing the chances of contamination of the negative samples with actual positive items. The sampling procedure is still popularity-based, since the likelihood for an item to appear in the mini-batch is proportional to its popularity. Both properties are known to be beneficial for pairwise learning with implicit user feedback [13]."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In this section we describe our experimental setup and provide an in-depth discussion of the obtained results."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We used two datasets for our experiments. The first is the XING 2 Recsys Challenge 2016 dataset [1] that contains interactions on job postings for 770k users over a 80-days\n2https://www.xing.com/en\nperiod. User interactions come with time-stamps and interaction type (click, bookmark, reply and delete). We named this dataset XING. The second dataset is a proprietary dataset from a Youtube-like video-on-demand web site. The dataset tracks the videos watched by 13k users over a 2-months period. Viewing events lasting less than a fixed threshold are not tracked. We named this dataset VIDEO. We manually partitioned the interaction data into sessions by using a 30-minute idle threshold. For the XING dataset we discarded interactions having type ‘delete’. We also discarded repeated interactions of the same type within sessions to reduce noise (e.g. repeated clicks on the same job ad within a session). We then preprocessed both datasets as follows. We removed items with support less than 20 for XING and 10\nfor VIDEO since items with low support are not optimal for modeling. We removed sessions with < 3 interactions to filter too-short and poorly informative sessions, and kept users with ≥ 5 sessions to have sufficient across-session information for proper modeling of returning users. The test set is build with the last session of each user. The remaining sessions form the training set. We also filtered items in the test set that do not belong to the training set. This partitioning allows to run the evaluation over users having different amounts of historical sessions in their profiles, hence to measure the recommendation quality over users having different degrees of activity with the system (see Section 4.3.1 for an in-depth analysis). We further partitioned the training set with the same procedure to tune the hyper-parameters of the algorithms. The characteristics of the datasets are summarized in Table 1."
    }, {
      "heading" : "4.2 Baselines and parameter tuning",
      "text" : "We compared our HRNN model against several baseline approaches: Personal Pop (PPOP), Item-KNN, RNN and RNN Concat:\n∙ Personal Pop (PPOP) recommends the item with the largest number of interactions by the user. ∙ Item-KNN computes an item-to-item cosine similarity based on the co-occurrence of items within sessions. ∙ RNN adopts the same model described in [7]. This model uses the basic GRU with a TOP1 loss function and session-parallel minibatching: sessions from the\nsame user are fed to the RNN independently from each other. ∙ RNN Concat is the same as RNN, but sessions from the same user are concatenated into a single session.\nWe optimize the neural models for TOP1 loss using AdaGrad [5] with momentum for 10 epochs3. We used dropout regularization [18] on the hidden states of RNN and HRNN . We applied dropout also to \uD835\uDC3A\uD835\uDC45\uD835\uDC48\uD835\uDC60\uD835\uDC52\uD835\uDC60 initialization for HRNN (Equation 4). In order to assess how the capacity of the network impacts the recommendation quality, in our experiments we considered both small networks with 100 hidden units per GRU layer and large networks with 500 hidden units per GRU layer. We tuned the hyper-parameters of each model (baselines included) on the validation set using random search [2]. To help the reproducibility of our experiments, we report the hyper-parameters used in our experiments on the XING dataset in Table 2 4. Dropout probabilities for HRNN s are relative to the user-level GRU, session-level GRU and initialization in this order. The optimal neighborhood size for Item-KNN is 300 for both datasets. Neural models were trained on Nvidia K80 GPUs equipped with 12GB of GPU memory. Training times vary from ∼5 minutes for the small RNN model on XING to ∼30 minutes for the large HRNN All on VIDEO. We want to highlight that training times do not significantly differ between RNN and HRNN s, with HRNN All being the most computationally expensive model due to the higher complexity of its architecture (see Figure 1)."
    }, {
      "heading" : "4.3 Results",
      "text" : "We evaluate wrt. the sequential next-item prediction task, i.e. given an event of the user session, we evaluate how well the algorithm predicts the following event. All RNN-based models are fed with events in the session one after the other, and we check the rank of the item selected by the user in the next 3Increasing the number of epochs did not significantly improve the loss in all models. 4Source code available at https://github.com/mquad/hgru4rec\nevent. In addition, HRNN models and RNN Concat are ‘bootstrapped’ with all the the sessions in the user history prior to the test one. This step slows down the evaluation but it is necessary to properly set the internal representations (e.g., the user-level representation for HRNN s) before evaluation starts. Notice that the evaluation metrics are still computed only over events in the test set, so evaluation remains fair5. As recommender systems can suggest only few items at once, the relevant item should be amongst the first few items in the recommendation list. We therefore evaluate the recommendation quality in terms of Recall@5, Precision@5 and Mean Reciprocal Rank (MRR@5). In sequential next-item prediction, Recall@5 is equivalent to the hit-rate metric, and measures the proportion of cases out of all test cases in which the relevant item is amongst the top-5 items. This is an accurate model for certain practical scenarios where no recommendation is highlighted and their absolute order does not matter, and strongly correlates with important KPIs such as CTR [6]. Precision@5 measures the fraction of correct recommendations in the top-5 positions of each recommendation list. MRR@5 is the reciprocal rank of the relevant item, where the reciprocal rank is manually set to zero if the rank is greater than 5. MRR takes the rank of the items into account, which is important in cases where the order of recommendations matters. Table 3 summarizes the results for both XING and VIDEO datasets. We trained each neural model for 10 times with different random seeds 6 and report the average results. We used Wilcoxon signed-rank test to assess significance of the difference between the proposed HRNN models and the stateof-the-art session-based RNN and the naïve personalization strategy used by RNN Concat.\nResults on XING. On this dataset, the simple personalized popularity baseline is a very competitive method, capable of outperforming the more sophisticated Item-KNN baseline by large margins. As prior studies on the dataset have already\n5Since RNN Concat is the only method capable of recommending the first event in the user sessions, we discarded the first prediction of each test session obtained with this method for a fair comparison. 6The random seed controls the initialization of the parameters of the network, which in turn can lead substantially different results in absolute terms. Even though, we have not observed substantial differences in the relative performances of neural models when using different random seeds.\nshown, users’ activity within and across sessions has a high degree of repetitiveness. This makes the generation of ‘nontrivial’ personalized recommendations in this scenario very challenging [1]. This is further highlighted by the poor performance of session-based RNN that is always significantly worse than PPOP independently of the capacity of the network. Nevertheless, personalized session-based recommendation can overcome its limitations and achieve superior performance in terms of Recall and Precision with both small and large networks. HRNN s significantly outperform RNN Concat in terms of Recall and Precision (up to +3%/+1% with small/large networks), and provide significantly better MRR with large networks (up to +5.4% with HRNN All). Moreover, HRNN s significantly outperform the strong PPOP baseline of ∼+11% in Recall and Precision, while obtaining comparable MRR. This is a significant result in a domain where more trivial personalization strategies are so effective. The comparison between the two HRNN variants does not highlight significant differences, apart from a small (∼2%) advantage of HRNN All over HRNN Init in MRR. The differences in terms of Recall and Precision are not statistically significant. Having established the superiority of HRNN s over session-based recommendation and trivial concatenation, we resort to the VIDEO dataset to shed further light on the differences between the proposed personalized session-based recommendation solutions.\nResults on VIDEO. The experiments on this dataset exhibit drastically different results from the XING dataset. Item-KNN baseline significantly outperforms PPOP, and session-based RNN can outperform both baselines by large margins. This is in line with past results over similar datasets [7, 8]. RNN Concat has comparable Recall and Precision with respect to session-based RNNs and interestingly significantly better MRR. This suggest that straight concatenation does\nnot enhance the retrieval capabilities of the RNN recommender but strengthens is ability in ranking items correctly. However, HRNN Init has significantly better performance than all baselines. It significantly outperforms all baselines and RNNs (up to 6.5% better Recall and 2.3% MRR wrt. RNN Concat). In other words, also in this scenario the more complex cross-session dynamics modeled by HRNN provide significant advantages in the overall recommendation quality. We will investigate on the possible reasons for these results in the following sections. It is worth noting that HRNN All performs poorly in this scenario. We impute the contextenforcing policy used in this setting for the severe degradation of the recommendation quality. One possible explanation could be that the consumption of multimedia content (videos in our case) is a strongly session-based scenario, much stronger than in the job search scenario represented in XING. Users may follow general community trends and have longterm interests. However, their activity within a session can be totally disconnected from her more recent sessions and even from her general interests (for example, users having a strong general interest over extreme-sport videos may occasionally watch cartoon movie trailers). HRNN Init models the user taste dynamics and lets the session-level GRU free to exploit them according to the actual evolution of the user interests within session. Its greater flexibility leads to superior recommendation quality.\n4.3.1 Analysis on the user history length. We investigate deeper the behavior of Hierarchical RNN models. Since we expect the length of the user history to have an impact on the recommendation quality, we breakdown the evaluation by the number of sessions in the history of the user. This serves as a proxy for the ‘freshness’ of the user within the system, and allows to evaluate recommenders under different amounts of historical information about the user modeled by the user-level GRU before a session begins. To this purpose, we partitioned user histories into two groups: ’Short’ user\nhistories having ≤ 6 sessions and ’Long’ user histories having 6 or more. The statistics on the fraction of sessions belonging to each group for both datasets are reported in Table 4. Since our goal is to measure the impact of the complex acrosssession dynamics used in HRNN wrt. traditional RNN, we restrict these analysis to RNN-based recommenders only. For each algorithm, we compute the average Recall@5 and MRR@5 per test session grouped by history length 7. To enhance the robustness of the experimental results, we run the evaluation 10 times with different random seeds and report the median value per algorithm. Figure 3 shows the results on XING. As the length of the user history grows, we can notice that Recall slightly increases and MRR slightly decreases in MRR for all methods, session-based RNN included. The relative performance between methods does not changes significantly between short and long user histories, with HRNN All being the best performing model with 13% better Recall@5 and 14-16% better MRR@5 wrt. session-based RNN. HRNN Init has\n7The analysis on Precision@5 returns similar results to Recall@5, so we omit it here also for space reasons.\nperformance comparable to RNN Concat and HRNN All in accordance to our previous findings. Figure 4 shows the results on VIDEO. Recall of all methods – session-based RNN included – improves with the length of the user history. MRR instead improves with history length only for RNN Concat and HRNN Init. This highlights the need for effective personalization strategies to obtain superior recommendation quality at session-level for users that heavily utilize the system. Moreover, the performance gain of HRNN Init wrt. session-based RNN grows from 5%/12% (short) to 7%/19% (long) in Recall@5/MRR@5, further highlighting the quality of our personalization strategy. Coherently with our previous findings, HRNN All does not perform well in this scenario, and its performances are steady (or even decrease) between the two groups. In summary, the length of the user history has a significant impact on the recommendation quality as expected. In loosely session-bounded domains like XING, in which the user activity is highly repetitive and less diverse across sessions, enforcing the user representation in input at session-level provides slightly better performance over the simpler initializationonly approach. However, in the more strongly session-based scenario, in which the user activity across-sessions has a higher degree of variability and may significantly diverge from the user historical interest and tastes, the simpler and more efficient HRNN Init variant has significantly better recommendation quality.\n4.3.2 Analysis within sessions. Here we breakdown by number of events within the session in order to measure the impact of personalization within the user session. We limit the analysis to sessions having length ≥ 5 (6,736 sessions for XING and 8,254 for VIDEO). We compute the average values of each metric groped by position within the session (Beginning, Middle and End). Beginning refers to the first 2 events of the session, Middle to the 3rd and 4th, and End to any event after the 4th. As in the previous analysis, we focus on RNN-based models and report the median of the averages values of each metric computed over 10 runs with different random seeds. Results for Recall@5 and MRR@5 are shown in Figure 5 and Figure 6 for XING and VIDEO respectively. On XING, the performance of all methods increses with the number of previous items in the session, suggesting that the user context at session-level is properly leveraged by all RNN-based models. However, there is a wide margin between personalized and ‘pure’ session-based models. All HRNN s have similar Recall@5 and are comparable to RNN Concat. Interestingly, the gain in MRR@5 of HRNN All wrt. to both RNN and RNN Concat grows with the number of items processed, meaning that in this scenario contextual information becomes more useful as the user session continues. HRNN Init has constantly better MRR that RNN Concat, with wider margins at the beginning and at the and of the session. On VIDEO, the behavior within session is different. We can notice that both Recall and MRR increase between the\nbeginning and end of a session as expected. HRNN Init exhibits a large improvement over RNN and RNN Concat at the beginning of the session (up to 10% better Recall and 20-25% better MRR respectively). This conforms with the intuition that past user activity can be effectively used to predict the first actions of the user in the forthcoming session with greater accuracy. After the first few events the gain in Recall of personalized over pure session-based models reduces, while the gain in MRR stays stable. In other words, after a few events, the session-level dynamics start to prevail over longer-term user interest dynamics, making personalization strategies less effective. However, personalization still provides superior ranking quality all over the session, as testified by the higher MRR of both HRNN Init and RNN Concat over RNN. Moreover, a better recommendation quality at beginning of the session has higher chances to increase the chances of retention of the user than later on in the session. Finally, HRNN All is always the worse method, further underpinning the superiority of the HRNN Init variant.\n4.3.3 Experiments on a large-scale dataset. We validated HRNN s over a larger version of the VIDEO dataset used in the previous experiments. This dataset is composed by the interactions of 810k users on 380k videos over the same 2 months periods, for a total of 33M events and 8.5M sessions 8. We named this dataset VIDEOXXL. Due to our limited computational resources, we could only test small networks (100 hidden units for RNN and for 100+100 HRNN s) on this 8We applied the same preprocessing steps used for VIDEO.\nlarge-scale dataset. We run all RNNs and HRNN s once using the same hyper-parameters learned on the small VIDEO dataset. Although not optimal, this approach provides a first approximation on the applicability of our solution under a more general setting. For the same reason, we do not provide an exhaustive analysis on the experimental results as done for the smaller datasets. To speed up evaluation, we computed the rank of the relevant item compared to the 50,000 most supported items, as done in [8]. Results are summarized in Table 5 and confirm our previous findings on the small VIDEO dataset. RNN Concat is not an effective and constantly underperforms session-based RNN. The same happens for HRNN All. Nevertheless, HRNN Init outperforms session-based RNN by large margins (up to ∼10% better Recall@5 and MRR@5). These results further confirm the superiority of HRNN Init for personalized session-based recommendation."
    }, {
      "heading" : "5 CONCLUSIONS AND FUTURE WORK",
      "text" : "In this paper we addressed the problem of personalizing session-based recommendation by proposing a model based Hierarchical RNN, that extends previous RNN-based session modeling with one additional GRU level that models the user activity across sessions and the evolution of her interests over time. HRNN s provide a seamless way of transferring the knowledge acquired on the long-term dynamics of the user interest to session-level, and hence to provide personalized session-based recommendations to returning users. The proposed HRNN s model significantly outperform both state-of-the-art session-based RNNs and the other basic personalization strategies for session-based recommendation on two real-world datasets having different nature. In particular, we noticed that the simpler approach that only initializes the session-level representation with the evolving representation of the user (HRNN Init) gives the best results. We delved into the dynamics of session-based RNN models within and across-sessions, providing extensive evidences of the superiority of the proposed HRNN Init approach and setting new state-of-the-art performances for session-based recommendation. As future works, we plan to investigate over other domains like personalized music recommendation and advertisement, and on how item and user features can be effectively added to the model to refine the user representation over time and improve session-based recommendation even further."
    } ],
    "references" : [ {
      "title" : "Random search for hyperparameter optimization",
      "author" : [ "James Bergstra", "Yoshua Bengio" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "On the properties of neural machine translation: Encoder–decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio" ],
      "venue" : "In SSST-8: 8th Workshop on Syntax, Semantics and Structure in Statistical Translation,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Collaborative filtering with recurrent neural networks",
      "author" : [ "Robin Devooght", "Hugues Bersini" ],
      "venue" : "arXiv preprint arXiv:1608.07400,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback. In ECML- PKDD’12",
      "author" : [ "B. Hidasi", "D. Tikk" ],
      "venue" : "Part II,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Session-based recommendations with recurrent neural networks",
      "author" : [ "Balázs Hidasi", "Alexandros Karatzoglou", "Linas Baltrunas", "Domonkos Tikk" ],
      "venue" : "CoRR, abs/1511.06939,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Parallel recurrent neural network architectures for feature-rich session-based recommendations",
      "author" : [ "Balázs Hidasi", "Massimo Quadrana", "Alexandros Karatzoglou", "Domonkos Tikk" ],
      "venue" : "In Proceedings of the 10th ACM Conference on Recommender Systems, RecSys",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "Towards scalable and accurate item-oriented recommendations",
      "author" : [ "Noam Koenigstein", "Yehuda Koren" ],
      "venue" : "In Proceedings of the 7th ACM Conference on Recommender Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Amazon.com recommendations: Item-to-item collaborative filtering",
      "author" : [ "G. Linden", "B. Smith", "J. York" ],
      "venue" : "Internet Computing,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "A critical review of recurrent neural networks for sequence learning",
      "author" : [ "Zachary C Lipton", "John Berkowitz", "Charles Elkan" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Improving pairwise learning for item recommendation from implicit feedback",
      "author" : [ "Steffen Rendle", "Christoph Freudenthaler" ],
      "venue" : "In Proceedings of the 7th ACM International Conference on Web Search and Data Mining,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Bpr: Bayesian personalized ranking from implicit feedback",
      "author" : [ "Steffen Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Neurocomputing: Foundations of research. chapter Learning Internal Representations by Error Propagation, pages 673–695",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1988
    }, {
      "title" : "Item-based collaborative filtering recommendation algorithms",
      "author" : [ "Badrul Sarwar", "George Karypis", "Joseph Konstan", "John Riedl" ],
      "venue" : "Int. Conf. on World Wide Web,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion",
      "author" : [ "Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "Jian-Yun Nie" ],
      "venue" : "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Improved recurrent neural networks for session-based recommendations",
      "author" : [ "Yong Kiam Tan", "Xinxing Xu", "Yong Liu" ],
      "venue" : "In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Recurrent recommender networks",
      "author" : [ "Chao-Yuan Wu", "Amr Ahmed", "Alex Beutel", "Alexander J. Smola", "How Jing" ],
      "venue" : "In Proceedings of the Tenth ACM International Conference on Web Search and  Data Mining,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Providing recommendations in these domains poses unique challenges that until recently have been mainly tackled by applying conventional recommender algorithms [10] on either the last interaction or the last session (session-based recommenders).",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "Recurrent Neural Networks (RNN’s) have been recently used for the purpose of session-based recommendations [7] outperforming item-based methods by 15% to 30% in terms of ranking metrics.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "recommendation approach [11, 16].",
      "startOffset" : 24,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "recommendation approach [11, 16].",
      "startOffset" : 24,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "RNNs are the deep models of choice when dealing with sequential data [12].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Long Short-Term Memory (LSTM) [9] networks are a type of RNNs that have been shown to work particularly well, it includes additional gates that regulate when and how much to take the input into account and when to reset the hidden state.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "Slightly simplified version of LSTM – that still maintains all their properties – are Gated Recurrent Units (GRUs) [3] which we use in this work.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 5,
      "context" : "RNNs were first used to model session data in [7].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "RNNs were also used to jointly model the content or features of items together with click-sequence interactions [8].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "In [19] proposed data augmentation techniques to improve the performance of the RNN for session-based recommendations, these techniques have though the side effect of increasing training times as a single session is split into several sub-sessions for training.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "RNN’s have also been used in more standard user-item collaborative filtering settings where the aim is to model the evolution of the user and items factors [20] [4] where the results are though less impressive, with the proposed methods barely outperforming standard matrix factorization methods.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 2,
      "context" : "RNN’s have also been used in more standard user-item collaborative filtering settings where the aim is to model the evolution of the user and items factors [20] [4] where the results are though less impressive, with the proposed methods barely outperforming standard matrix factorization methods.",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 15,
      "context" : "Finally a sequence to sequence model with a version of Hierarchical Recurrent Neural Networks was used for generative context-aware query suggestion in [17].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "Our model is based on the session-based Recurrent Neural Network (RNN henceforth) model presented in [7].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "The network can be trained with several ranking loss functions such as cross-entropy, BPR [14] and TOP1 [7].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "The network can be trained with several ranking loss functions such as cross-entropy, BPR [14] and TOP1 [7].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "1To simplify the explanation we use a notation similar to [17].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : ", Nm − 1 (5) The model is trained end-to-end using back-propagation [15].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "For the sake of efficiency in training, we have edited the session-parallel mini-batch mechanism described in [8] to account for user identifiers during training (see Figure 2).",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "Both properties are known to be beneficial for pairwise learning with implicit user feedback [13].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "∙ RNN adopts the same model described in [7].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "We optimize the neural models for TOP1 loss using AdaGrad [5] with momentum for 10 epochs3.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "We used dropout regularization [18] on the hidden states of RNN and HRNN .",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "We tuned the hyper-parameters of each model (baselines included) on the validation set using random search [2].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "This is an accurate model for certain practical scenarios where no recommendation is highlighted and their absolute order does not matter, and strongly correlates with important KPIs such as CTR [6].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : "This is in line with past results over similar datasets [7, 8].",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "This is in line with past results over similar datasets [7, 8].",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "To speed up evaluation, we computed the rank of the relevant item compared to the 50,000 most supported items, as done in [8].",
      "startOffset" : 122,
      "endOffset" : 125
    } ],
    "year" : 2017,
    "abstractText" : "Session-based recommendations are highly relevant in many modern on-line services (e.g. e-commerce, video streaming) and recommendation settings. Recurrent Neural Networks have recently been shown to perform very well in sessionbased settings. While in many session-based recommendation domains user identifiers are hard to come by, there are also domains in which user profiles are readily available. We propose a seamless way to personalize RNN models with cross-session information transfer and devise a hierarchical RNN model that relies end evolves latent hidden states of the RNN’s across user sessions. Results on two industry datasets show large improvements over the session-only RNN’s.",
    "creator" : "LaTeX with hyperref package"
  }
}