{
  "name" : "1609.06616.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text∗",
    "authors" : [ "John J. Nay" ],
    "emails" : [ "john.j.nay@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Methods have been developed to efficiently obtain representations of words in Rd that capture subtle semantics across the dimensions of the vectors (Collobert and Weston, 2008). For instance, after suf-\n∗ Forthcoming paper in 2016 Proceedings of Empirical Methods in Natural Language Processing Workshop on NLP and Computational Social Science.\nficient training, relationships encoded in difference vectors can be uncovered with vector arithmetic: vec(“king”) - vec(“man”) + vec(“woman”) returns a vector close to vec(“queen”) (Mikolov et al. 2013a).\nApplying this powerful notion of distributed continuous vector space representations of words, we embed institutions and the words from their law and policy documents into shared semantic space. We can then combine positively and negatively weighted word and government vectors into the same query, enabling complex, targeted and subtle similarity computations. For instance, which government branch is more characterized by “validity and truth”?\nWe apply this method, Gov2Vec, to a unique corpus of Supreme Court opinions, Presidential actions, and official summaries of Congressional bills. The model discerns meaningful differences between House, Senate, President and Court vectors. We also learn more fine-grained institutional representations: individual Presidents and Congresses (2-year terms). The method implicitly learns important latent relationships between these government actors that was not provided during training. For instance, their temporal ordering was learned from only their text. The resulting vectors are used to explore differences between actors with respect to policy topics."
    }, {
      "heading" : "2 Methods",
      "text" : "A common method for learning vector representations of words is to use a neural network to predict a target word with the mean of its context words’ vectors, obtain the gradient with back-propagation of the prediction errors, and update vectors in the\nar X\niv :1\n60 9.\n06 61\n6v 1\n[ cs\n.C L\n] 2\n1 Se\np 20\ndirection of higher probability of observing the correct target word (Bengio et al. 2003; Mikolov et al. 2013b). After iterating over many word contexts, words with similar meaning are embedded in similar locations in vector space as a by-product of the prediction task (Mikolov et al. 2013b). Le and Mikolov (2014) extend this word2vec method to learn representations of documents. For predictions of target words, a vector unique to the document is concatenated with context word vectors and subsequently updated. Similarly, we embed institutions and their words into a shared vector space by averaging a vector unique to an institution with context word vectors when predicting that institution’s words and, with back-propagation and stochastic gradient descent, update representations for institutions and the words (which are shared across all institutions).1\nThere are two hyper-parameters for the algorithm that can strongly affect results, but suitable values are unknown. We use a tree of Parzen estimators search algorithm (Bergstra et al. 2013) to sample from parameter space2 and save all models estimated. Subsequent analyses are conducted across all models, propagating our uncertainty in hyperparameters. Due to stochasticity in training and the\n1We use a binary Huffman tree (Mikolov et al. 2013b) for efficient hierarchical softmax prediction of words, and conduct 25 epochs while linearly decreasing the learning rate from 0.025 to 0.001.\n2vector dimensionality, uniform(100, 200), and maximum distance between the context and target words, uniform(10, 25)\nuncertainty in the hyper-parameter values, patterns robust across the ensemble are more likely to reflect useful regularities than individual models.\nGov2Vec can be applied to more fine-grained categories than entire government branches. In this context, there are often relationships between word sources, e.g. Obama after Bush, that we can incorporate into the learning process. During training, we alternate between updating GovVecs based on their use in the prediction of words in their policy corpus and their use in the prediction of other word sources located nearby in time. We model temporal institutional relationships, but any known relationships between entities, e.g. ranking Congresses by number of Republicans, could also be incorporated into the Structured Gov2Vec training process (Fig. 1).\nAfter training, we extract (M + S) × dj × J parameters, where M is the number of unique words, S is the number of word sources, and dj the vector dimensionality, which varies across the J models (we set J = 20). We then investigate the most cosine similar words to particular vector combinations, argmaxv∗∈V1:N cos(v∗, 1W ∑W i=1wi × si), where cos(a, b) = ~a·~b ‖~a‖‖~b‖\n, wi is one ofW WordVecs or GovVecs of interest, V1:N are theN most frequent words in the vocabulary ofM words (N < M to exclude rare words during analysis) excluding the W query words, si is 1 or -1 for whether we’re positively or negatively weighting wi. We repeat similarity queries over all J models, retain words with > C cosine similarity, and rank word results based on their frequency and mean cosine similarity across the ensemble."
    }, {
      "heading" : "3 Data",
      "text" : "We created a unique corpus of 59 years of all Supreme Court opinions (1937-1975, 1991-2010), 227 years of all Presidential Memorandum, Determinations, and Proclamations, and Executive Orders (1789-2015), and 42 years of official summaries of all bills introduced in Congress (1973- 2014). We used official summaries rather than full bill text because full texts are only available from 1993 and summaries are available from 1973. We scraped all Presidential Memorandum (1,465), Determinations (801), Executive Orders (5,634), and Proclamations (7,544) from the American Presi-\ndency Project website. The Sunlight Foundation downloaded official bill summaries from the GPO, which we downloaded. We downloaded Supreme Court Decisions issued 1937–1975 (Vol. 300-422) from the GPO, and the PDFs of Decisions issued 1991–2010 (Vol. 502-561) from the Court. We removed HTML artifacts, whitespace, stop words, words occurring only once, numbers, and punctuation, and converted to lower-case."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 WordVec-GovVec Similarities",
      "text" : "We tested whether our learned vectors captured meaningful differences between branches. Fig. 2 displays similarities between these queries and the branches, which reflect a priori known differences.\nGov2Vec has unique capabilities that summary statistics, e.g. word frequency, lack: it can compute similarities between any source and word as long as the word occurs at least in one source, whereas word counting cannot provide meaningful similarities when a word never occurs in a source’s corpus. Most importantly, Gov2Vec can combine complex combinations of positively and negatively weighted vectors in a similarity query."
    }, {
      "heading" : "4.2 GovVec-GovVec Similarities",
      "text" : "We learned representations for individual Presidents and Congresses by using vectors for these higher resolution word sources in the word prediction task. To investigate if the representations capture important latent relationships between institutions, we compared the cosine similarities between the Congresses over time (93rd–113th) and the corresponding sitting Presidents (Nixon–Obama) to the bill veto rate. We expected that a lower veto rate would be reflected in more similar vectors, and, indeed, the Congress-President similarity and veto rate are negatively correlated (Spearman’s ρ computed on raw veto rates and similarities: -0.74; see also Fig. 3).3\nAs a third validation, we learn vectors from only text and project them into two dimensions with principal components analysis. From Fig. 4 it’s evident that temporal and institutional relationships were implicitly learned.4 One dimension almost perfectly rank orders Presidents and Congresses by time and another separates the President from Congress."
    }, {
      "heading" : "4.3 GovVec-WordVec Policy Queries",
      "text" : "Fig. 5 (top) asks: how does Obama and the 113th House differ in addressing climate change and how does this vary across environmental and economic contexts? The most frequent word across the en-\n3Leveraging temporal relationships in the learning process, Structured Gov2Vec, and just using the text, yield very similar (impressive) results on this task. Figs. 3 and 4 and the correlation reported are derived from the text-only Gov2Vec results.\n4These are the only results reported in this paper from a single model within the ensemble. We conducted PCA on other models in the ensemble and the same relationships hold.\nsemble (out of words with > 0.35 similarity to the query) for the Obama-economic quadrant is “unprecedented.” “Greenhouse” and “ghg” are more frequent across models and have a higher mean similarity for Obama-Environmental than 113th HouseEnvironmental.\nFig. 5 (bottom) asks: how does the House address war from “oil” and “terror” perspectives and how does this change after the 2001 terrorist attack.5 Compared to the 106th, both the oil and terrorism panels in the 107th (when 9-11 occurred) have words more cosine similar to the query (further to the right) suggesting that the 107th House was closer to the topic of war, and the content changes to primarily strong verbs such as instructs, directs, requires, urges, and empowers."
    }, {
      "heading" : "5 Additional Related Work",
      "text" : "Political scientists model text to understand political processes (Grimmer 2010; Roberts et al. 2014); however, most of this work focuses on variants of topic models. Djuric et al. (2015) apply a learning procedure similar to Structured Gov2Vec to streaming documents to learn representations of documents that are similar to those nearby in time. Structured Gov2Vec applies this joint hierarchical learning process (using entities to predict words and other entities) to non-textual entities. Kim et al. (2014) and Kulkarni et al. (2015) train neural language models for each year of a time ordered corpora to detect changes in words. Instead of learning models for distinct times, we learn a global model with embeddings for time-dependent entities that can be included in queries to analyze change. Kiros et al. (2014) learn embeddings for text attributes by treating them as gating units to a word embedding tensor. Their process is more computationally intensive than ours."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "We learned vector representations of text meta-data on a novel data set of legal texts that includes case,\n5For comparisons across branches, e.g. 113th House and Obama, Structured Gov2Vec learned qualitatively more useful representations so we plot that here. For comparisons within Branch, e.g. 106th and 107th House, to maximize uniqueness of the word sources to obtain more discriminating words, we use text-only Gov2Vec.\nstatutory, and administrative law. The representations effectively encoded important relationships between institutional actors that were not explicitly provided during training. Finally, we demonstrated fine-grained investigations of policy differences between actors based on vector arithmetic. More generally, the method can be applied to measuring similarity between any entities producing text, and used for recommendations, e.g. what’s the closest thinktank vector to the non-profit vector representation of the Sierra Club?\nMethodologically, our next goal is to explore where training on non-textual relations, i.e. Structural Gov2Vec, is beneficial. It seems to help stabilize representations when exploiting temporal relations, but political relations may prove to be even more useful. Substantively, our goal is to learn a large collection of vectors for government actors at different resolutions and within different contexts6 to address a range of targeted policy queries."
    } ],
    "references" : [ {
      "title" : "A Neural Probabilistic Language Model",
      "author" : [ "Bengio", "Yoshua", "Rjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "J. Mach. Learn. Res. 3 (March): 1137–55.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures",
      "author" : [ "Bergstra", "James S.", "Daniel Yamins", "David Cox." ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learn-",
      "citeRegEx" : "Bergstra et al\\.,? 2013",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2013
    }, {
      "title" : "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning",
      "author" : [ "Collobert", "Ronan", "Jason Weston." ],
      "venue" : "Proceedings of the 25th International Conference on Machine Learning. 160–167. ACM.",
      "citeRegEx" : "Collobert et al\\.,? 2008",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2008
    }, {
      "title" : "Hierarchical Neural Language Models for For instance, learn a vector for the 111th House using its text and temporal relationships to other Houses, learn a vec",
      "author" : [ "Djuric", "Nemanja", "Hao Wu", "Vladan Radosavljevic", "Mihajlo Grbovic", "Narayan Bhamidipati" ],
      "venue" : null,
      "citeRegEx" : "Djuric et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Djuric et al\\.",
      "year" : 2015
    }, {
      "title" : "A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases.",
      "author" : [ "Grimmer", "Justin" ],
      "venue" : "Political Analysis",
      "citeRegEx" : "Grimmer and Justin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Grimmer and Justin.",
      "year" : 2010
    }, {
      "title" : "Temporal Analysis of Language Through Neural Language Models",
      "author" : [ "Kim", "Yoon", "Yi-I. Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov." ],
      "venue" : "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Sci-",
      "citeRegEx" : "Kim et al\\.,? 2014",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2014
    }, {
      "title" : "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations",
      "author" : [ "Kiros", "Ryan", "Richard Zemel", "Ruslan R Salakhutdinov." ],
      "venue" : "Advances in Neural Information Processing Systems 27, edited by Z. Ghahramani, M.",
      "citeRegEx" : "Kiros et al\\.,? 2014",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistically Significant Detection of Linguistic Change",
      "author" : [ "Kulkarni", "Vivek", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena." ],
      "venue" : "Proceedings of the 24th International Conference on World Wide Web, 625–35. WWW ’15. New York, NY, USA:",
      "citeRegEx" : "Kulkarni et al\\.,? 2015",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed Representations of Sentences and Documents",
      "author" : [ "Le", "Quoc", "Tomas Mikolov." ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning, 1188–96.",
      "citeRegEx" : "Le et al\\.,? 2014",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2014
    }, {
      "title" : "Linguistic Regularities in Continuous Space Word Representations",
      "author" : [ "T. Mikolov", "W.T. Yih", "G. Zweig." ],
      "venue" : "HLT-NAACL, 746–51.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed Representations of Words and Phrases and Their Compositionality",
      "author" : [ "Mikolov", "Tomas", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26, edited by C. J. C. Burges,",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Structural Topic Models for Open-Ended Survey Responses.",
      "author" : [ "Roberts", "Margaret E", "Brandon M. Stewart", "Dustin Tingley", "Christopher Lucas", "Jetson LederLuis", "Shana Kushner Gadarian", "Bethany Albertson", "David G. Rand" ],
      "venue" : null,
      "citeRegEx" : "Roberts et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "ficient training, relationships encoded in difference vectors can be uncovered with vector arithmetic: vec(“king”) - vec(“man”) + vec(“woman”) returns a vector close to vec(“queen”) (Mikolov et al. 2013a).",
      "startOffset" : 182,
      "endOffset" : 204
    }, {
      "referenceID" : 0,
      "context" : "direction of higher probability of observing the correct target word (Bengio et al. 2003; Mikolov et al. 2013b).",
      "startOffset" : 69,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "direction of higher probability of observing the correct target word (Bengio et al. 2003; Mikolov et al. 2013b).",
      "startOffset" : 69,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "After iterating over many word contexts, words with similar meaning are embedded in similar locations in vector space as a by-product of the prediction task (Mikolov et al. 2013b).",
      "startOffset" : 157,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "We use a tree of Parzen estimators search algorithm (Bergstra et al. 2013) to sample from parameter space2 and save all models estimated.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "Due to stochasticity in training and the We use a binary Huffman tree (Mikolov et al. 2013b) for efficient hierarchical softmax prediction of words, and conduct 25 epochs while linearly decreasing the learning rate from 0.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "direction of higher probability of observing the correct target word (Bengio et al. 2003; Mikolov et al. 2013b). After iterating over many word contexts, words with similar meaning are embedded in similar locations in vector space as a by-product of the prediction task (Mikolov et al. 2013b). Le and Mikolov (2014) extend this word2vec method to learn representations of documents.",
      "startOffset" : 70,
      "endOffset" : 316
    } ],
    "year" : 2016,
    "abstractText" : "We compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space. We apply our method, Gov2Vec, to Supreme Court opinions, Presidential actions, and official summaries of Congressional bills. The model discerns meaningful differences between government branches. We also learn representations for more finegrained word sources: individual Presidents and (2-year) Congresses. The similarities between learned representations of Congresses over time and sitting Presidents are negatively correlated with the bill veto rate, and the temporal ordering of Presidents and Congresses was implicitly learned from only text. With the resulting vectors we answer questions such as: how does Obama and the 113th House differ in addressing climate change and how does this vary from environmental or economic perspectives? Our work illustrates vectorarithmetic-based investigations of complex relationships between word sources. We are extending this to create a comprehensive legal semantic map.",
    "creator" : "LaTeX with hyperref package"
  }
}