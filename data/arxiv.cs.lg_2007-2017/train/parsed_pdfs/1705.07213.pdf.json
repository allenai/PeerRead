{
  "name" : "1705.07213.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Securing Deep Neural Nets against Adversarial Attacks with Moving Target Defense",
    "authors" : [ "Sailik Sengupta", "Tathagata Chakraborti", "Subbarao Kambhampati" ],
    "emails" : [ "sailiks@asu.edu", "tchakra2@asu.edu", "rao@asu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep Neural Networks are presently state-of-the-art systems in image classification and are used in many important tasks such as recognizing handwritten digits on cheques [1], object classification for automated surveillance [2] and autonomous vehicles [3]. If an adversary could intentionally make these classification systems misclassify inputs, there can be serious consequences. For example, begin able to misclassify the digit ‘1’ as ‘9’ might help an adversary withdraw more money than the amount handwritten on a bank cheque. In fact, in [4] and [5] authors show how models for\nar X\niv :1\n70 5.\n07 21\n3v 1\n[ cs\n.L G\n] 1\n9 M\nhandwritten digit recognition built using the MNIST dataset can be easily affected. In [4], road signs saying ‘stop’ is misclassified, which can make an autonomous vehicle behave dangerously. Such attack mechanisms also exist for state-of-the-art vision systems that recognize faces, which may be used for authentication, target identification etc. as shown in [6]. Furthermore, the manipulated image generated by an adversary is, in almost all cases, indistinguishable from the original image when viewed by a human observer. Some of these attacks are illustrated in Figure 1.\nA lot of these attacks exploit the fact that Deep Neural Networks (DNNs) projects data into higher dimensional spaces which are scarcely populated, and decision boundaries often have high (uninformed) biases towards regions where training data is sparse. This helps adversaries in finding perturbations to modify a legitimate testing data towards a decision boundary so that it is misclassified. Formally, if D̂(x) denotes the class of an image x output by a Deep Neural Network D̂, a perturbation p when added to the image x tries to ensure that D̂(x) 6= D̂(x+ p). Minimum perturbations, in addition, try to minimize the value ||p||2, which ensures that the changed image x+ p is indistinguishable form the original image x to humans. The methods mentioned earlier try to find efficient ways by which these minimum perturbations can be generated. The defenses against such attacks, at a high level, try to generate such adversarial samples by themselves and use them as a part of the training data (with the correct labels) so that the classifier now has points in empty regions of the higher dimensional space. Unfortunately, these are attack-specific and do not always work.\nRecent work on generating universal perturbation [8] is one case where all existing defense mechanisms fall short. In this attack, a single perturbation image is generated for all test samples of a specific classifier. This generated perturbation, when added to the original test images, can adversely bring down the classification accuracy of even the state-of-the-art classifiers (ResNet-152 [9]) from 95.5% to even as low as 14.6% (see Figure 1).\nIn this paper, we propose Moving Target Defense (MTD) as a general technique for defending Neural Networks against such attacks. In section 2, we introduce the reader to the state-of-the-art attacks on Deep Neural Networks for image classification, followed by recent literature in the fields of cybersecurity and multi-agent systems that form the backbone of our approach. Section 3 describes in detail how we adopt the MTD framework for defense of DNNs, formalizing the notion of differential immunity in such systems. We then motivate and formulate this as a Repeated Bayesian Game such that the objective of obtaining high accuracy on valid test images while being able to reduce the misclassification error on images generated by an adversary simply boils down to finding the Stackelberg Equilibrium of the game. In section 4, we show experimental results on six popular neural networks that have excelled in classifying ILSCRV’s ImageNet in recent years. We specifically show that we are able to reduce the misclassification damage from 93.7% (for VGG-F) to 58.12% in the case of an adversary while still maintain an accuracy of 91.72% for non-perturbed test images without any change whatsoever to the underlying neural networks themselves! We also show\nempirically how the networks provide differential immunity as a system as more and more networks are added to the ensemble, thus providing a pathway to secure neural nets against adversarial attacks even as they are highly vulnerable individually. Lastly, in section 5, we conclude the paper and highlight some promising future directions."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Attacks on Deep Neural Networks and Existing Defense Strategies",
      "text" : "In this section we will do a brief review of existing work on adversarial attacks on deep neural networks and efforts being made to combat them.\n• Gradient-based perturbations: Recent literature [10, 4, 5] has shown multiple ways of generating adversarial samples for a test image input to a DNN. In these works, either (i) the input features whose partial derivatives on the DNN’s Loss Functions are high are modified by a small amount to make the DNN misclassify them, or (ii) the geometric space around a point is examined to find the closest class-separation boundary. Apart from considering that the test image which is to be modified is available, which is similar to a chosen ciphertext attack, these attacks often make further assumptions pertaining to knowledge about the network being attacked.\n• Black-box attacks: Latest work on black-box attacks against DNNs use distillation techniques and assume the presence of an oracle that provides test labels for a list of images the adversary provides [7], similar to chosen plaintext attacks. Besides these assumptions, computing a perturbation for a single image is time consuming, making them less practical. The general defense against these types of attacks involves first generating such adversarial images for the training data. The generated images along with the expected labels are then used to fine tune the parameters of the neural network in the training phase. This helps the DNN to reduce its bias in the unexplored high dimensional space, reducing the effectiveness of the perturbations.\n• Universal perturbations: This is capable of creating a single image for the set of all test samples and is able to make a DNN misclassify a vast majority of test samples [8]. This DNN-specific single perturbation is also shown to effectively misclassify any input image when added as noise to it. Thus, although it might take time to generate this single perturbation, only one “universal\" perturbation image per network needs be computed. Importantly, none of the defense mechanisms for image-specific attacks work well for these universal perturbation attacks.\nThere has been some effort in trying to protect machine learning systems attacks like the ones above by using randomization techniques such as in [11]. Unfortunately, it does not define a formal framework which can be used for DNNs. Further, while such works try to prevent misclassification under attack, they land up reducing the classification accuracy significantly. On the contrary, work on using ensemble models for DNNs simply try to increase classification accuracy for legitimate users but have no protection against adversarial modifications of test images [12]. There has also been work on using ensemble models to detect adversarial samples for the MINST dataset [13]. This requires the distribution of how a sample from a specific class is likely to be misclassified into another class, which is either unavailable or inefficient to obtain in most cases. Furthermore, the idea of using a classifier (or learning a controller) to classify an input image as legitimate or adversarial is highly insecure since that classifier (or controller) itself can be adversarially attacked. Thus, existing works for securing DNNs against adversarially modified samples are mostly attack/dataset specific or fail to reason about attacker strategies."
    }, {
      "heading" : "2.2 Moving Target Defense (MTD) and its Adoption for Security of DNNs",
      "text" : "Moving Target Defense (MTD) is a paradigm used in software security that tries to prevent an attacker for executing an attack by constantly switching between multiple software systems [14]. Practical use of MTDs in Web Application Systems have been shown to enhance system security [15]. To our knowledge, in this paper we design the first general purpose security framework for Neural Networks using MTD (MTD-NN).\nDevising strategies for MTD systems have been shown to be a difficult problem. [16] shows that in order to provide guarantees on the security of such systems, it is necessary to reason about these\nattacks in a multi-agent game theoretic fashion, which leads to defense strategies that outperform randomized strategies previously regarded as the best policies to build robust systems with.\nIn this paper, we thus compile the user interaction in a image classification application driven by a set of DNNs into a Repeated Bayesian Game, similar to [16], providing provable guarantees on the expected performance and security of the DNNs. We empirically show that MTD-NN provides security against Universal Perturbations, against which effective mechanisms do not exist at present.\nFurther, note that our framework, although motivated to provide security for DNNs, can also be used for securing any Machine Learning (ML) model against any form of attacks, since it the strategies are independent of the attack type as well as dataset or algorithm used internally. Notice also that the game theoretic reasoning ensures that the security mechanism cannot be strategically manipulated by the attacker. Lastly, MTD-NN also reasons about trying to balance between providing security while affecting the accuracy of the system only by a small amount, a consideration often absent in present works on design of security mechanisms for DNNs."
    }, {
      "heading" : "3 Moving Target Defense for Deep Neural Networks (MTD-NN)",
      "text" : "As we explained before, in a Moving Target Defense system, the defender has multiple system configurations. The attacker has a set of attacks that it can do, which affects some of the configurations in the defender’s system. Given an input to the system, the defender selects one of the configurations to run the input and returns the output generated by that system. Since the attacker does not know which system was specifically selected, it is no longer as effective as before (Figure 3). Thus, although randomization in selecting a configuration for classification is needed, it has to be done in a non-trivial fashion for ensuring maximum security. A potential downside of such a framework is that it might\nland up reducing the accuracy of the overall system in classifying normal images. Thus, we want to retain good classification performance while guaranteeing high security.\nIn this section, we first describe the agents in our framework and actions they can execute, which include describing the defender and her DNN configurations for the MTD framework and the user, her types (adversarial and legitimate) and actions of these types. Lastly, we show that randomized switching over the set of defender’s configurations needs to reason about the MTD-NN system in a game theoretic fashion, helping us to obtain a selection strategy that maximizes classification accuracy and security for the defender’s system."
    }, {
      "heading" : "3.1 Defender Configurations",
      "text" : "The configuration space for the defender in the MTD framework for DNNs are a set of different DNNs that are trained on the same task but not-affected by the same attack. For classifying images, Convolutional Neural Networks (CNNs) are known to produce the best results. Thus, although the different DNN configurations used by the defender might be different in the number of layers, parameters, hyperparameters or activation functions, they will still have to use CNN units to produce comparable results. Formally, let N denote the set of defender configurations. In our case, N is a set of six neural networks we will use for our MTD-NN system (see Table 1)."
    }, {
      "heading" : "3.2 User types and Universal Perturbation",
      "text" : "Our second player, namely the users, are of two types–Legitimate User (L) and the Adversary (A). L tries to use the MTD-NN system for classifying images for a specific task without any adversarial intent. These are the target users of all the present DNN designers, who just try to improve accuracy. The second type, i.e. the adversary A, is essentially trying to manipulate input images in a way so as\nto make the DNNs misclassify the label for these inputs. L has a single action that gives input for classification, where as the attacker uses multiple attack actions, which we now define.\nUniversal Perturbation are single images generated by an adversary, specific to a DNN, that when XOR-ed with the test images so as to generate modified test images, result in the system misclassifying actual labels of the test image [8]. In essence, once generated for a certain DNN, they are effective for modifying any test image to be input to the network. Thus, these perturbation are ‘universal’ in terms of being affective against all test images for a specific DNN.\nLet U the set of universal perturbation (or attacks) the attacker generates for our MTD-NN system. A strong adversary who might know the different architectures we use, will generate a universal perturbation for each of the networks in our system (as shown in Table 2). Although she can use it to generate malicious inputs for the other configurations as well, we will see that they are not as effective as using them on the intended network due to the concept of differential immunity."
    }, {
      "heading" : "3.3 Differential Immunity",
      "text" : "Ideally, given an attack (image), we would like it to be effective for a certain configuration and ineffective for all the others. Whenever this property holds for all attacks against an MTD system, we can be rest assured that there is atleast something to be gained by switching between multiple configurations. We refer to this as differential immunity.\nTo define differential immunity δ formally, let us consider a function f : N × U → R+0 that given a certain configuration and an attack, returns a non-negative impact value. This impact value is proportional to the harm caused to the defender or the reward for an adversary. For example, given a neural network n(∈ N) used for classification of a modified image using the attack u(∈ U), f(n, u) can be the fooling rate. The fooling rate refers to the percentage of test samples that are misclassifed by the network n when the universal perturbation u is used to modify the test images. We now define differential immunity as follows.\nδ = min u maxn f(n, u)−minn f(n, u) + 1 maxn f(n, u) + 1 = min u\n( 1− minn f(n, u)\nmaxn f(n, u) + 1 ) where minn f(n, u) and maxn f(n, u) denote the minimum and maximum impact that an attacker can cause provided it uses the attack u. Notice that if the maximum and minimum impact differ by a wide margin, then the differential immunity of the MTD system should be higher. This is represented in the numerator. The denominator ensures that an attack which has high impact reduces the differential immunity of a system compared to a low impact attack even when their maximum and minimum values differ by the same margin. The +1 factors in both the numerator and denominator for the function has two objectives, of which one is to avoid division by zero. For the other, note that when the minn f(n, u) and maxn f(n, u) have the same value, if the impact is higher, we will have less δ and vice-versa. Lastly, notice that 0 ≤ δ ≤ 1 since the function f maps to non-positive values. For neural networks, generating differentially immune configurations is not a trivial problem. As shown by existing work [5], simple ideas like partitioning the training data and training multiple (C)NNs on the disjoint sets of this data does not make the networks differentially immune. In our experiments, we show that changing the architecture between the various CNNs that have been popular in the last few years (due to accuracy boost on the ImageNet competitions) does provide a weak notion of differential immunity, i.e., an attack although applicable to many of the defender configurations, is not as effective for all configurations."
    }, {
      "heading" : "3.4 Moving Target Defense Architecture as a Repeated Bayesian Game",
      "text" : "Although we might have designed MTD-NN, it will only be secure when it uses a random mixed strategy. For deterministic (or pure) strategies, an attacker knows which configuration the system is using to classify a test image. Thus, it can pick the most effective attack, which is the universal perturbation relevant to the DNN configuration that the defender will use for classification. This defeats the purpose of an MTD system. Selecting a uniform random strategy (URS), i.e. picking up any DNN in the MTD-NN system with equal probability (in an unbiased manner) in often not be the best approach in multi-agent systems [16]. Hence, for the design of an effective strategy, we formulate our system as a Repeated Bayesian Game with two players–the defender and the user.\nThe use of our MTD-NN framework for DNNs used in safety critical systems should also ensure that the accuracy of classification for legitimate users is not affected. Hence, one might want to associate relative importance to each of the user types (L and A). There is no point in using a DNN configuration that has low accuracy (picked with equal probability by URS) in the fear that the input image might be adversarial. In essence, one might want the neural network to be effective for the legitimate users and alongside increase the accuracy of classification for the adversary generated images, making this a multi-objective optimization. Fortunately, this can easily be done by using the probability of player types in our game theoretic framework. Thus, the two types of the second player, i.e. the users (L and A), have a probability associated with them, making this a Bayesian Game. Notice that the adversary can observe the strategy of the defender for a reasonable amount of time and reason about their attacks when attacking the system. The defender has to account for this when making his strategy, making this a Repeated Game.\nThe formulated game is a non-zero sum game. The rewards in the normal form game matrix for the defender and the player types are defined as follows:\n• For the Legitimate User (Table 1, the L and the defender get a reward value that represents the accuracy of the DNN system. Thus, for a DNN architecture with accuracy (say) 93%, we have the reward entry (93, 93) in the normal form game corresponding to the defender’s action of using this DNN for classification and the L’s action.\n• For the Adversary (Table 2), the reward values is given by f(n, u), which corresponds to the fooling rate. The defender’s reward in this case is the accuracy of the DNN when classifying modified input images, which is fooling rate subtracted from 100. As an example, if the fooling rate of the adversary is (say) 93% when using u against a DNN n selected by the MTD-NN strategy, then we have the reward entry (7, 93)."
    }, {
      "heading" : "3.5 Defender’s Strategy for Switching",
      "text" : "A defender has to launch a system up front for the game to even begin. This imparts a leader-follower paradigm to the Repeated Bayesian Game we defined above. One can notice that satisfying the multi-objective criterion for our system is essentially solving for the Stackelberg Equilibrium, which gives the optimal switching policy for the defender. Ideally, for each i(∈ N), there is an attack j(∈ U) which is the universal perturbation generated for the DNN i. Let us denote the strategy vector\nfor the defender as ~x and his rewards as RDi,j when the defender choses to use the i-th network and user selects the j-th action. Also, let the strategy vectors for the users be ~qA and ~qL, and the rewards be RAi,j and R L i,j for the adversary and the legitimate user.\nWe will now use the DOBSS solver [21] to optimize the defender’s reward give the attacker chooses to maximize her reward.\nmax x,q ∑ i∈N (α · ∑ j∈U RDi,j xiq A j + (1− α) ·RDi,j xiqLj ) (1)\ns.t. ∑ i∈N xi = 1\n∑ j∈U qDj = 1 ∀ D ∈ {A,L}\n0 ≤ vD − ∑ i∈N RDi,jxi ≤ (1− nDa )M ∀ D ∈ {A,L}\n0 ≤ xi ≤ 1 ∀ i ∈ N qj ∈ {0, 1} ∀ j ∈ U\nwhere α, the probability of the adversary A attacking an application, is assigned a value closer to zero, when the defender wants to switch less and M is a large positive number. Presently, DNN architectures that try to maximize classification accuracy are special cases for our objective function where α = 0. In this case, the defender just uses the best configuration (ResNet-152 in Table 1) that maximizes accuracy since he does not feel that his system will be attacked."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "For our experiments, we use the six different architectures that have excelled in ILSVRC 2012 [22] validation set (50, 000 images). These architectures (shown in Table 1 and 2) are now the defender’s configurations amongst which she must switch to confuse the attacker who is trying to make the system misclassify a certain test image. We use the Universal Perturbations (UP) provided in [8] as attack actions of the adversary A. These UPs are generated by making sure that the l −∞ norm of the perturbations are less than a bound ξ = 10. The rewards for this game is shown in Table 2 for A and Table 1 for L. Using the formula for differential immunity, we find that the differential immunity of our system is 0.34, which although poor, is to our knowledge the highest generated so-far.\nIn Figure 3, we see that the objective function value (given by Equation 1) for the MTD-NN framework contrasted with the objective values of each of the comprising network when the probability of an adversary type α varies. When, α = 0, using the most accurate network will maximize the objective. Hence, the plots for ResNet-152 and MTD-NN start at the same place. The MTD-NN in this case uses a pure strategy of using ResNet-152 and not switching at all. As adversarial test samples become more ubiquitous, the accuracy of the networks drops. Thus, to stay protected, the system starts to switch in between the networks. The accuracy of classification for non-manipulated images (queried by L) is 91.73%. When the system receives only adversarial samples, i.e. α = 1, the accuracy of MTD-NN is > 42% where as it reaches < 20% for most of the single DNN architectures. The strategy in this case is ~x = (0, 0.171, 0.241, 0, 0.401, 0.187). Although at first glance, this might not seem to be a great accuracy by itself, we note that this is for the purely adversarial case where there are no existing defense mechanisms against universal perturbations. Our approach was able to increase the accuracy of DNNs by a margin of 20% for attack images alone, while still maintaining an accuracy of 91.73% for legitimate users. We note that this robustness is achieved without considering the inner workings of the individual DNNs (which are, of course, themselves expected to become less vulnerable with further research, thus driving up the security of the entire system in the worst case).\nOur framework has an added benefit of discarding the networks that will not be helpful in increasing security due to the lack of differential immunity. In our constructed MTD-NN, the zero values in ~x denote that the DNNs VGG-F and VGG-16 should not be used. Thus, we see that only four networks contributed to the optimal defense strategy. We further explore the participation of individual\nnetworks in the equilibria in Figure 4.The results in the figure clearly show that while it is useful to have multiple networks providing differential immunity (as testified by the improvement of accuracy in adversarial conditions), the leveling-off of the objective values with more DNNs in the mix does underline that there is much room for research in actively developing DNNs with different datasets, architectures, activation functions, hyperparameters, etc. that can provide greater differential immunity. An ensemble of such networks equipped with MTD can provide significant gains in both security and accuracy."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we formulated a Moving Target Defense system for securing Deep Neural Networks MTD-NN by exploiting the differential immunity of the individual networks. We show that by casting the user interaction as a Repeated Bayesian Game, we are able to find effective strategies that minimize the fooling rate for test images modified by an adversary as well as maximize the accuracy of classification for legitimate test images. Using this approach, we were able to show an increase of > 20% classification accuracy under full attack in experiments conducted on six popular DNNs.\nThe method provided is general and can be used for any machine learning model and dataset, and provides a degree of protection against any attack. As future work, we plan to cast other machine learning models in this framework and use existing attacks to test the resilience of our system. The\nnon-trivial concepts of designing systems that are differentially immune becomes an important aspect for these systems to perform in a secure manner, as motivated by the final part of our experiments. Note that a very simple approach of adopting a MTD framework for DNNs at the architecture level of a single network would be the use of randomized dropouts when classifying test samples where each input sample uses a different set of neural nodes and pathways. Unfortunately, this does not work well in practice since most of the award winning architectures we used in our MTD-NN framework (which are in fact different in depth, weights, hyperparameters, structure, etc.) only provide a weak form of differential immunity (δ = 0.34). The question of what changes to a DNN will guarantee higher differential immunity needs further investigation.\nAcknowledgments. This research is supported in part by ONR grants N00014161-2892, N00014-131-0176, N00014-13-1-0519, N00014-15-1-2027, & the NASA grant NNX17AD06G."
    } ],
    "references" : [ {
      "title" : "Automatic processing of handwritten bank cheque images: a survey",
      "author" : [ "R Jayadevan", "Satish R Kolhe", "Pradeep M Patil", "Umapada Pal" ],
      "venue" : "International Journal on Document Analysis and Recognition (IJDAR),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Tracking and object classification for automated surveillance",
      "author" : [ "Omar Javed", "Mubarak Shah" ],
      "venue" : "Vision—ECCV",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Road traffic sign detection and classification",
      "author" : [ "Arturo De La Escalera", "Luis E Moreno", "Miguel Angel Salichs", "José María Armingol" ],
      "venue" : "IEEE transactions on industrial electronics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1997
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "In Security and Privacy (EuroS&P),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition",
      "author" : [ "Mahmood Sharif", "Sruti Bhagavatula", "Lujo Bauer", "Michael K Reiter" ],
      "venue" : "In Proceedings of the SIGSAC Conference on Computer and Communications Security,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Practical black-box attacks against machine learning",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "In Proceedings of the Conference on Computer and Communications Security,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2017
    }, {
      "title" : "Universal adversarial perturbations",
      "author" : [ "Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Deepfool: a simple and accurate method to fool deep neural networks",
      "author" : [ "Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Adversarial pattern classification using multiple classifiers and randomisation",
      "author" : [ "Battista Biggio", "Giorgio Fumera", "Fabio Roli" ],
      "venue" : "Structural, Syntactic, and Statistical Pattern Recognition,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Robustness to adversarial examples through an ensemble of specialists",
      "author" : [ "Mahdieh Abbasi", "Christian Gagné" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "Towards a theory of moving target defense",
      "author" : [ "Rui Zhuang", "Scott A DeLoach", "Xinming Ou" ],
      "venue" : "In Proceedings of the First ACM Workshop on Moving Target Defense,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Toward a Moving Target Defense for Web Applications",
      "author" : [ "Marthony Taguinod", "Adam Doupé", "Ziming Zhao", "Gail-Joon Ahn" ],
      "venue" : "In Proceedings of the IEEE International Conference on Information Reuse and Integration (IRI),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "A game theoretic approach to strategy generation for moving target defense in web applications",
      "author" : [ "Sailik Sengupta", "Satya Gautam Vadlamudi", "Subbarao Kambhampati", "Adam Doupé", "Ziming Zhao", "Marthony Taguinod", "Gail-Joon Ahn" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2017
    }, {
      "title" : "Return of the devil in the details: Delving deep into convolutional nets",
      "author" : [ "Ken Chatfield", "Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell" ],
      "venue" : "In Proceedings of the 22nd ACM international conference on Multimedia,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Playing games for security: An efficient exact algorithm for solving bayesian stackelberg games",
      "author" : [ "Praveen Paruchuri", "Jonathan P Pearce", "Janusz Marecki", "Milind Tambe", "Fernando Ordonez", "Sarit Kraus" ],
      "venue" : "In Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep Neural Networks are presently state-of-the-art systems in image classification and are used in many important tasks such as recognizing handwritten digits on cheques [1], object classification for automated surveillance [2] and autonomous vehicles [3].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "Deep Neural Networks are presently state-of-the-art systems in image classification and are used in many important tasks such as recognizing handwritten digits on cheques [1], object classification for automated surveillance [2] and autonomous vehicles [3].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 2,
      "context" : "Deep Neural Networks are presently state-of-the-art systems in image classification and are used in many important tasks such as recognizing handwritten digits on cheques [1], object classification for automated surveillance [2] and autonomous vehicles [3].",
      "startOffset" : 253,
      "endOffset" : 256
    }, {
      "referenceID" : 3,
      "context" : "In fact, in [4] and [5] authors show how models for ar X iv :1 70 5.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "In fact, in [4] and [5] authors show how models for ar X iv :1 70 5.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "Figure 1: Adversial attacks on DDNs - Gradient based perturbations (left) on handwritten digit recognition [4], black box attack (middle) [7] on stop signs using images which are practically indistinguishable, and universal perturbations (right) [8] when added to the original images on the left is able to make the DNN misclassify it on the right.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "Figure 1: Adversial attacks on DDNs - Gradient based perturbations (left) on handwritten digit recognition [4], black box attack (middle) [7] on stop signs using images which are practically indistinguishable, and universal perturbations (right) [8] when added to the original images on the left is able to make the DNN misclassify it on the right.",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "Figure 1: Adversial attacks on DDNs - Gradient based perturbations (left) on handwritten digit recognition [4], black box attack (middle) [7] on stop signs using images which are practically indistinguishable, and universal perturbations (right) [8] when added to the original images on the left is able to make the DNN misclassify it on the right.",
      "startOffset" : 246,
      "endOffset" : 249
    }, {
      "referenceID" : 3,
      "context" : "In [4], road signs saying ‘stop’ is misclassified, which can make an autonomous vehicle behave dangerously.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "as shown in [6].",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "Recent work on generating universal perturbation [8] is one case where all existing defense mechanisms fall short.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "This generated perturbation, when added to the original test images, can adversely bring down the classification accuracy of even the state-of-the-art classifiers (ResNet-152 [9]) from 95.",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "• Gradient-based perturbations: Recent literature [10, 4, 5] has shown multiple ways of generating adversarial samples for a test image input to a DNN.",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "• Gradient-based perturbations: Recent literature [10, 4, 5] has shown multiple ways of generating adversarial samples for a test image input to a DNN.",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "• Gradient-based perturbations: Recent literature [10, 4, 5] has shown multiple ways of generating adversarial samples for a test image input to a DNN.",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "• Black-box attacks: Latest work on black-box attacks against DNNs use distillation techniques and assume the presence of an oracle that provides test labels for a list of images the adversary provides [7], similar to chosen plaintext attacks.",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 7,
      "context" : "• Universal perturbations: This is capable of creating a single image for the set of all test samples and is able to make a DNN misclassify a vast majority of test samples [8].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 10,
      "context" : "There has been some effort in trying to protect machine learning systems attacks like the ones above by using randomization techniques such as in [11].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : "On the contrary, work on using ensemble models for DNNs simply try to increase classification accuracy for legitimate users but have no protection against adversarial modifications of test images [12].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : "There has also been work on using ensemble models to detect adversarial samples for the MINST dataset [13].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "Moving Target Defense (MTD) is a paradigm used in software security that tries to prevent an attacker for executing an attack by constantly switching between multiple software systems [14].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 14,
      "context" : "Practical use of MTDs in Web Application Systems have been shown to enhance system security [15].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "[16] shows that in order to provide guarantees on the security of such systems, it is necessary to reason about these",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "In this paper, we thus compile the user interaction in a image classification application driven by a set of DNNs into a Repeated Bayesian Game, similar to [16], providing provable guarantees on the expected performance and security of the DNNs.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : "Universal Perturbation are single images generated by an adversary, specific to a DNN, that when XOR-ed with the test images so as to generate modified test images, result in the system misclassifying actual labels of the test image [8].",
      "startOffset" : 233,
      "endOffset" : 236
    }, {
      "referenceID" : 4,
      "context" : "As shown by existing work [5], simple ideas like partitioning the training data and training multiple (C)NNs on the disjoint sets of this data does not make the networks differentially immune.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "picking up any DNN in the MTD-NN system with equal probability (in an unbiased manner) in often not be the best approach in multi-agent systems [16].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 16,
      "context" : "MTD-NN System VGG-F [17] (92.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "9) CaffeNet [18] (83.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "6) GoogLeNet [19] (93.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 19,
      "context" : "3) VGG-16 [20] (92.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "5) VGG-19 [20] (92.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 8,
      "context" : "5) ResNet-152 [9] (95.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 20,
      "context" : "We will now use the DOBSS solver [21] to optimize the defender’s reward give the attacker chooses to maximize her reward.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "For our experiments, we use the six different architectures that have excelled in ILSVRC 2012 [22] validation set (50, 000 images).",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "We use the Universal Perturbations (UP) provided in [8] as attack actions of the adversary A.",
      "startOffset" : 52,
      "endOffset" : 55
    } ],
    "year" : 2017,
    "abstractText" : "Deep Neural Networks (DNNs) are presently the state-of-the-art for image classification tasks. However, recent works have shown that these systems can be easily fooled to misidentify images by modifying the image in particular ways, often rendering them practically useless. Moreover, defense mechanisms proposed in the literature so far are mostly attack-specific and prove to be ineffective against new attacks. Indeed, recent work on universal perturbations can generate a single modification for all test images that is able to make existing networks misclassify 90% of the time. Presently, to our knowledge, no defense mechanisms are effective in preventing this. As such, the design of a general defense strategy against a wide range of attacks for Neural Networks becomes a challenging problem. In this paper, we derive inspiration from recent advances in the field of cybersecurity and multi-agent systems, and propose to use the concept of Moving Target Defense (MTD) for increasing the robustness of well-known deep networks trained on the ImageNet dataset towards such adversarial attacks. In using this technique, we formalize and exploit the notion of differential immunity of different networks to specific attacks. To classify a single test image, we pick one of the trained networks each time and then use its classification output. To ensure maximum robustness, we generate an effective strategy by formulating this interaction as a Repeated Bayesian Stackelberg Game (BSG) with a Defender (who hosts the classification networks) and Users (both Legitimate users and Attackers). As a network switching strategy, we compute a Strong Stackelberg Equilibrium that optimizes the accuracy of prediction while at the same time reduces the misclassification rate on adversarial modification of test images. We show that while our approach produces an accuracy of 92.79% for the legitimate users, attackers can only misclassify images 58% (instead of 93.7%) of the time even when they select the best attack available to them. This is at least twice as good, to sometimes even an order of magnitude better, compared the accuracy rates of the worst affected networks.",
    "creator" : "LaTeX with hyperref package"
  }
}