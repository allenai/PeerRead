{
  "name" : "1611.05010.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm",
    "authors" : [ "Kejun Huang", "Xiao Fu", "Nicholas D. Sidiropoulos" ],
    "emails" : [ "huang663@umn.edu", "xfu@umn.edu", "nikos@ece.umn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 1.\n05 01\n0v 1\n[ st\nat .M\nL ]\n1 5"
    }, {
      "heading" : "1 Introduction",
      "text" : "Given a large collection of text data, e.g., documents, tweets, or Facebook posts, a natural question is what are the prominent topics in these data. Mining topics from a text corpus is motivated by a number of applications, from commercial design, news recommendation, document classification, content summarization, and information retrieval, to national security. Topic mining, or topic modeling, has attracted significant attention in the broader machine learning and data mining community [1].\nIn 2003, Blei et al. proposed a Latent Dirichlet Allocation (LDA) model for topic mining [2], where the topics are modeled as probability mass functions (PMFs) over a vocabulary and each document is a mixture of the PMFs. Therefore, a word-document text data corpus can be viewed as a matrix factorization model. Under this model, posterior inference-based methods and approximations were proposed [2, 3], but identifiability issues – i.e., whether the matrix factors are unique – were not considered. Identifiability, however, is essential for topic modeling since it prevents the mixing of topics that confounds interpretation.\nIn recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11]. Essentially, these algorithms are based on the so-called separable nonnegative matrix factorization (NMF) model [12]. The key assumption is that every topic has an ‘anchor word’ that only appears\n∗These authors contributed equally.\nin that particular topic. Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10]. The former class has a serious complexity issue, as it lifts the number of variables to the square of the size of vocabulary (or documents); the latter, although computationally very efficient, usually suffers from error propagation, if at some point one anchor word is incorrectly identified. Furthermore, since all the anchor word-based approaches essentially convert topic identification to the problem of seeking the vertices of a simplex, most of the above algorithms require normalizing each data column (or row) by its ℓ1 norm. However, normalization at the factorization stage is usually not desired, since it may destroy the good conditioning of the data matrix brought by pre-processing and amplify noise [8].\nUnlike many NMF-based methods that work directly with the word-document data, the approach proposed by Arora et al. [9, 10] works with the pairwise word-word correlation matrix, which has the advantage of suppressing sampling noise and also features better scalability. However, [9, 10] did not relax the anchor-word assumption or the need for normalization, and did not explore the symmetric structure of the co-occurrence matrix – i.e., the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].\nThe anchor-word assumption is reasonable in some cases, but using models without it is more appealing in more critical scenarios, e.g., when some topics are closely related and many key words overlap. Identifiable models without anchor words have been considered in the literature; e.g., [13, 14, 15] make use of third or higher-order statistics of the data corpus to formulate the topic modeling problem as a tensor factorization problem. There are two major drawbacks with this approach: i) third- or higher-order statistics require a lot more samples for reliable estimation relative to their lower-order counterparts (e.g., second-order word correlation statistics); and ii) identifiability is guaranteed only when the topics are uncorrelated – where a super-symmetric parallel factor analysis (PARAFAC) model can be obtained [13, 14]. Uncorrelatedness is a restrictive assumption [10]. When the topics are correlated, the model becomes a Tucker model which is not identifiable in general; identifiability needs more assumptions, e.g., sparsity of topic PMFs [15].\nContributions. In this work, our interest lies in topic mining using word-word correlation matrices like in [9, 10], because of its potential scalability and noise robustness. We propose an anchorfree identifiable model and a practically implementable companion algorithm. Our contributions are two-fold: First, we propose an anchor-free topic identification criterion. The criterion aims at factoring the word-word correlation matrix using a word-topic PMF matrix and a topic-topic correlation matrix via minimizing the determinant of the topic-topic correlation matrix. We show that under a so-called sufficiently scattered condition, which is much milder than the anchor-word assumption, the two matrices can be uniquely identified by the proposed criterion. We emphasize that the proposed approach does not need to resort to higher-order statistics tensors to ensure topic identifiability, and it can naturally deal with correlated topics, unlike what was previously available in topic modeling, to the best of our knowledge. Second, we propose a simple procedure for handling the proposed criterion that only involves eigen-decomposition of a large but sparse matrix, plus a few small linear programs – therefore highly scalable and well-suited for topic mining. Unlike greedy pursuit-based algorithms, the proposed algorithm does not involve deflation and is thus free from error propagation; it also does not require normalization of the data columns / rows. Carefully designed experiments using the TDT2 and Reuters text corpora showcase the effectiveness of the proposed approach."
    }, {
      "heading" : "2 Background",
      "text" : "Consider a document corpus D ∈ RV ×D, where each column of D corresponds to a document and D(v, d) denotes a certain measurement of word v in document d, e.g., the word-frequency of term v in document d or the term frequency–inverse document frequency (tf-idf) measurement that is often used in topic mining. A commonly used model is\nD ≈ CW , (1) where C ∈ RV×F is the word-topic matrix, whose f -th column C(:, f) represents the probability mass function (PMF) of topic f over a vocabulary of words, and W (f, d) denotes the weight of topic f in document d [2, 13, 10]. Since matrix C and W are both nonnegative, (1) becomes\na nonnegative matrix factorization (NMF) model – and many early works tried to use NMF and variants to deal with this problem [16]. However, NMF does not admit a unique solution in general, unless both C and W satisfy some sparsity-related conditions [17]. In recent years, much effort has been put in devising polynomial time solvable algorithms for NMF models that admit unique factorization. Such models and algorithms usually rely on an assumption called “separability” in the NMF literature [12]:\nAssumption 1 (Separability / Anchor-Word Assumption) There exists a set of indices Λ = {v1, . . . , vF } such that C(Λ, :) = Diag(c), where c ∈ RF .\nIn topic modeling, it turns out that the separability condition has a nice physical interpretation, i.e., every topic f for f = 1, . . . , F has a ‘special’ word that has nonzero probability of appearing in topic f and zero probability of appearing in other topics. These words are called ‘anchor words’ in the topic modeling literature. Under Assumption 1, the task of matrix factorization boils down to finding these anchor words v1, . . . , vF since D(Λ, :) = Diag(c)W — which is already a scaled version of W — and then C can be estimated via (constrained) least squares.\nAlgorithm 1: Successive Projection Algorithm [6]\ninput : D; F . Σ = 1TDT X = DTΣ−1 (normalization); Λ = ∅; for f = 1, . . . , F do\nv̂f ← argmaxv∈{1,...,V } ‖X(:, v)‖2; Λ ← [Λ, v̂f ]; Θ ← argminΘ ‖X −X(:,Λ)Θ‖2F ; X ← X −X(:,Λ)Θ;\nend output :Λ\nMany algorithms have been proposed to tackle this index-picking problem in the context of separable NMF, hyperspectral unmixing, and text mining. The arguably simplest algorithm is the so-called successive projection algorithm (SPA) [6] that is presented in Algorithm 1. SPA-like algorithms first define a normalized matrix X = DTΣ−1 where Σ = Diag(1TDT ) [11]. Note that X = GS where G(:, f) = W T(f,:)/‖W (f,:)‖1 and S(f, v) = C(v,f)‖W (f,:)‖1 ‖C(v,:)‖1‖D(v,:)‖1 . Consequently, we have 1TS = 1 T if W ≥ 0, meaning the columns of X all lie on the simplex spanned by the columns of G, and the vertices of the simplex correspond to the anchor words. Also, the columns of S all live in the unit simplex. After normalization, SPA sequentially identifies the vertices of the data simplex, in conjunction with a deflation procedure. The algorithms in\n[8, 10, 11] can also be considered variants of SPA, with different deflation procedures and pre-/postprocessing. In particular, the algorithm in [8] avoids normalization — for real-word data, normalization at the factorization stage may amplify noise and damage the good conditioning of the data matrix brought by pre-processing, e.g., the tf-idf procedure [8]. To pick out vertices, there are also algorithms using linear programming and sparse optimization [7, 5], but these have serious scalability issues and thus are less appealing.\nIn practice D may contain considerable noise, and this has been noted in the literature. In [9, 10, 14, 15], the authors proposed to use second and higher-order statistics for topic mining. Particularly, Arora et al. [9, 10] proposed to work with the following matrix:\nP = E{DDT} = CECT , (2) where E = E{WW T } can be interpreted as a topic-topic correlation matrix. The matrix P is by definition a word-word correlation matrix, but also has a nice interpretation: if D(v, d) denotes the frequency of word v occurring in document d, P (i, j) is the likelihood that term i and j co-occur in a document [9, 10]. There are two advantages in using P : i) if there is zero-mean white noise, it will be significantly suppressed through the averaging process; and ii) the size of P does not grow with the size of the data if the vocabulary is fixed. The latter is a desired property when the number of documents is very large, and we pick a (possibly limited but) manageable vocabulary to work with. Problems with similar structure to that of P also arise in the context of graph models, where communities and correlations appear as the underlying factors. The algorithm proposed in [10] also makes use of Assumption 1 and is conceptually close to Algorithm 1. The work in [13, 14, 15] relaxed the anchor-word assumption. The methods there make use of three or higher-order statistics, e.g., P ∈ RV ×V×V whose (i, j, k)th entry represents the co-occurrence of three terms. The work in [13, 14] showed that P is a tensor satisfying the parallel factor analysis (PARAFAC) model and thus C is uniquely identifiable, if the topics are uncorrelated, which is a restrictive assumption\n(a counter example would be politics and economy). When the topics are correlated, additional assumptions like sparsity are needed to restore identifiability [15]. Another important concern is that reliable estimates of higher-order statistics require much larger data sizes, and tensor decomposition is computationally cumbersome as well.\nRemark 1 Among all the aforementioned methods, the deflation-based methods are seemingly more efficient. However, if the deflation procedure in Algorithm 1 (the update of Θ) has constraints like in [8, 11], there is a serious complexity issue: solving a constrained least squares problem with FV variables is not an easy task. Data sparsity is destroyed after the first deflation step, and thus even first-order methods or coordinate descent as in [8, 11] do not really help. This point will be exemplified in our experiments."
    }, {
      "heading" : "3 Anchor-Free Identifiable Topic Mining",
      "text" : "In this work, we are primarily interested in mining topics from the matrix P because of its noise robustness and scalability. We will formulate topic modeling as an optimization problem, and show that the word-topic matrix C can be identified under a much more relaxed condition, which includes the relatively strict anchor-word assumption as a special case."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "Let us begin with the modelP = CECT , subject to the constraint that each column ofC represents the PMF of words appearing in a specific topic, such that CT1 = 1, C ≥ 0. Such a symmetric matrix decomposition is in general not identifiable, as we can always pick a non-singular matrix A ∈ RF×F such that AT1 = 1, A ≥ 0, and define C̃ = CA, Ẽ = A−1CA−1, and then P = C̃ẼC̃ T with C̃ T 1 = 1, C̃ ≥ 0. We wish to find an identification criterion such that under some mild conditions the corresponding solution can only be the ground-truth E and C up to some trivial ambiguities such as a common column permutation. To this end, we propose the following criterion:\nminimize E∈RF×F ,C∈RV ×F\n| detE|, subject to P = CECT ,CT1 = 1,C ≥ 0. (3)\nThe first observation is that if the anchor-word assumption is satisfied, the optimal solutions of the above identification criterion are the ground-truth C and E and their column-permuted versions. Formally, we show that:\nProposition 1 Let (C⋆,E⋆) be an optimal solution of (3). If the separability / anchor-word assumption (cf. Assumption 1) is satisfied and rank(P ) = F , then C⋆ = CΠ and E⋆ = ΠTEΠ , where Π is a permutation matrix.\nThe proof of Proposition 1 can be found in the supplementary material. Proposition 1 is merely a ‘sanity check’ of the identification criterion in (3): It shows that the criterion is at least a sound one under the anchor-word assumption. Note that, when the anchor-word assumption is satisfied, SPAtype algorithms are in fact preferable over the identification criterion in (3), due to their simplicity. The point of the non-convex formulation in (3) is that it can guarantee identifiability of C and E even when the anchor-word assumption is grossly violated. To explain, we will need the following.\nAssumption 2 (sufficiently scattered) Let cone(CT )∗ denote the polyhedral cone {x : Cx ≥ 0}, and K denote the second-order cone {x : ‖x‖2 ≤ 1Tx}. Matrix C is called sufficiently scattered if it satisfies that: (i) cone(CT )∗ ⊆ K, and (ii) cone(CT )∗ ∩ bdK = {λef : λ ≥ 0, f = 1, . . . , F}, where bdK denotes the boundary of K, i.e., bdK = {x : ‖x‖2 = 1Tx}.\nOur main result is based on this assumption, whose first consequence is as follows:\nLemma 1 If C ∈ RV×F is sufficiently scattered, then rank(C) = F . In addition, given rank(P ) = F , any feasible solution Ẽ ∈ RF×F of Problem (3) has full rank and thus | det Ẽ| > 0.\nLemma 1 ensures that any feasible solution pair (C̃, Ẽ) of Problem (3) has full rank F when the ground-truth C is sufficiently scattered, which is important from the optimization perspective –\notherwise | det Ẽ| can always be zero which is a trivial optimal solution of (3). Based on Lemma 1, we further show that:\nTheorem 1 Let (C⋆,E⋆) be an optimal solution of (3). If the ground truth C is sufficiently scattered (cf. Assumption 2) and rank(P ) = F , then C⋆ = CΠ and E⋆ = ΠTEΠ , where Π is a permutation matrix.\nThe proof of Theorem 1 is relegated to the supplementary material. In words, for a sufficiently scattered C and an arbitrary square matrix E, given P = CECT , C and E can be identified up to permutation via solving (3). To understand the sufficiently scattered condition and Theorem 2, it is better to look at the dual cones. The notation cone(CT )∗ = {x : Cx ≥ 0} comes from the fact that it is the dual cone of the conic hull of the row vectors of C, i.e., cone(CT ) = {CTθ : θ ≥ 0}. A useful property of dual cone is that for two convex cones, if K1 ⊆ K2, then K∗2 ⊆ K∗1 , which means the first requirement of Assumption 2 is equivalent to\nK∗ ⊆ cone(CT ). (4) Note that the dual cone of K is another second-order cone [12], i.e., K∗ = {x|xT1 ≥√ F − 1‖x‖2}, which is tangent to and contained in the nonnegative orthant. Eq. (4) and the definition of K∗ in fact give a straightforward comparison between the proposed sufficiently scattered condition and the existing anchor-word assumption. An illustration of Assumptions 1 and 2 is shown in Fig. 1 (a)-(b) using an F = 3 case, where one can see that sufficiently scattered is much more relaxed compared to the anchor-word assumption: if the rows of the word-topic matrixC are geometrically scattered enough so that cone(CT ) contains the inner circle (i.e., the second-order cone K∗), then the identifiability of the criterion in (3) is guaranteed. However, the anchor-word assumption requires that cone(CT ) fulfills the entire triangle, i.e., the nonnegative orthant, which is far more restrictive. Fig. 1(c) shows a case where rows of C are not “well scattered” in the non-negative orthant, and indeed such a matrix C cannot be identified via solving (3).\nRemark 2 A salient feature of the criterion in (3) is that it does not need to normalize the data columns to a simplex — all the arguments in Theorem 1 are cone-based. The upshot is clear: there is no risk of amplifying noise or changing the conditioning of P at the factorization stage. Furthermore, matrix E can be any symmetric matrix; it can contain negative values, which may cover more applications beyond topic modeling where E is always nonnegative and positive semidefinite. This shows the surprising effectiveness of the sufficiently scattered condition.\nThe sufficiently scattered assumption appeared in identifiability proofs of several matrix factorization models [17, 18, 19] with different identification criteria. Huang et al. [17] used this condition to show the identifiability of plain NMF, while Fu et al. [19] related the sufficiently scattered condition to the so-called volume-minimization criterion for blind source separation. Note that volume\nminimization also minimizes a determinant-related cost function. Like the SPA-type algorithms, volume minimization works with data that live in a simplex, therefore applying it still requires data normalization, which is not desired in practice. Theorem 1 can be considered as a more natural application of the sufficiently scattered condition to co-occurrence/correlation based topic modeling, which explores the symmetry of the model and avoids normalization."
    }, {
      "heading" : "3.2 AnchorFree: A Simple and Scalable Algorithm",
      "text" : "The identification criterion in (3) imposes an interesting yet challenging optimization problem. One way to tackle it is to consider the following approximation:\nminimize E,C\n∥ ∥P −CECT ∥ ∥\n2 F + µ| detE|, subject to C ≥ 0, CT1 = 1, (5)\nwhere µ ≥ 0 balances the data fidelity and the minimal determinant criterion. The difficulty is that the term CECT makes the problem tri-linear and not easily decoupled. Plus, tuning a good µ may also be difficult. In this work, we propose an easier procedure of handling the determinantminimization problem in (3), which is summarized in Algorithm 2, and referred to as AnchorFree. To explain the procedure, first notice that P is symmetric and positive semidefinite. Therefore, one can apply square root decomposition to P = BBT , where B ∈ RV×F . We can take advantage of well-established tools for eigen-decomposition of sparse matrices, and there is widely available software that can compute this very efficiently. Now, we have B = CE1/2Q,QTQ = QQT = I, and E = E1/2E1/2; i.e., the representing coefficients of CE1/2 in the range space of B must be orthonormal because of the symmetry of P . We also notice that\nminimize E,C,Q\n| detE1/2Q|, subject to B = CE1/2Q, CT1 = 1, C ≥ 0, QTQ = I, (6)\nhas the same optimal solutions as (3). Since Q is unitary, it does not affect the determinant, so we further let M = QTE−1/2 and obtain the following optimization problem\nmaximize M\n| detM |, subject to MTBT1 = 1,BM ≥ 0. (7)\nBy our reformulation, C has been marginalized and we have only F 2 variables left, which is significantly smaller compared to the variable size of the original problem V F + F 2, where V is the vocabulary size. Problem (7) is still non-convex, but can be handled very efficiently. Here, we propose to employ the solver proposed in [18], where the same subproblem (7) was used to solve a dynamical system identification problem. The idea is to apply the co-factor expansion to deal with the determinant objective function, first proposed in the context of non-negative blind source separation [20]: if we fix all the columns of M except the f th one, detM becomes a linear function with respect to M(:, f), i.e., detM =\n∑F k=1(−1)f+kM(k, f) detM̄k,f = aTM(:, f), where\na = [a1, . . . , aF ] T , ak = (−1)f+k detM̄k,f , ∀ k = 1, ..., F , and M̄k,f is a matrix obtained by removing the kth row and f th column of M . Maximizing |aTx| subject to linear constraints is still a non-convex problem, but we can solve it via maximizing both aTx and −aTx, followed by picking the solution that gives larger absolute objective. Then, cyclically updating the columns of M results in an alternating optimization (AO) algorithm. The algorithm is computationally lightweight: each linear program only involves F variables, leading to a worst-case complexity of O(F 3.5) flops even when the interior-point method is employed, and empirically it takes 5 or less AO iterations to converge. In the supplementary material, simulations on synthetic data are given, showing that Algorithm 2 can indeed recover the ground truth matrix C and E even when matrix C grossly violates the separability / anchor-word assumption."
    }, {
      "heading" : "4 Experiments",
      "text" : "Data In this section, we apply the proposed algorithm and the baselines to two popular text mining datasets, namely, the NIST Topic Detection and Tracking (TDT2) and the Reuters-21578 corpora. We use a subset of the TDT2 corpus consisting of 9,394 documents which are single-category articles belonging to the largest 30 categories. The Reuters-21578 corpus is the ModApte version where 8,293 single-category documents are kept. The original vocabulary sizes of the TDT2 and the Reuters dataset are 36, 771 and 18, 933, respectively, and stop words are removed for each trial\nAlgorithm 2: AnchorFree input : D, F . P ← Co-Occurrence(D); P = BBT , M ← I; repeat\nfor f = 1, . . . , F do ak = (−1)f+k detM̄k,f , ∀ k = 1, ..., F ; // remove k-th row and f-th column of M to obtain M̄k,f mmax = argmaxx a\nTx s.t. Bx ≥ 0, 1TBx = 1; mmin = argminx a\nTx s.t. Bx ≥ 0, 1TBx = 1; M(:, f) = argmaxmmax,mmin(|aTmmax|, |aTmmin|);\nend until convergence; C⋆ = BM ; E⋆ = (C T ⋆ C⋆) −1CT⋆ PC⋆(C T ⋆ C⋆)\n−1; output :C⋆, E⋆\nof the experiments. We use the standard tf-idf data as the D matrix, and estimate the correlation matrix using the biased estimator suggested in [9]. A standard pre-processing technique, namely, normalized-cut weighted (NCW) [21], is applied to D; NCW is a well-known trick for handling the unbalanced-cluster-size problem. For each trial of our experiment, we randomly draw F categories of documents, form the P matrix, and apply the proposed algorithm and the baselines.\nBaselines We employ several popular anchor word-based algorithms as baselines. Specifically, the successive projection algorithm (SPA) [6], the successive nonnegative projection algorithm (SNPA) [11], the XRAY algorithm [8], and the fast anchor words (FastAnchor) [10] algorithm. Since we are interested in word-word correlation/co-occurrence based mining, all the algorithms are combined with the framework provided in [10] and the efficient RecoverL2 process is employed for estimating the topics after the anchors are identified.\nEvaluation To evaluate the results, we employ several metrics. First, coherence (Coh) is used to measure the single-topic quality. For a set of words V , the coherence is defined as Coh = ∑\nv1,v2∈V log (freq(v1,v2)+ǫ/freq(v2)) , where v1 and v2 denote the indices of two words in the vocabulary, freq(v2) and freq(v1, v2) denote the numbers of documents in which v1 appears and v1 and v2 co-occur, respectively, and ǫ = 0.01 is used to prevent taking log of zero. Coherence is considered well-aligned to human judgment when evaluating a single topic — a higher coherence score means better quality of a mined topic. However, coherence does not evaluate the relationship between different mined topics; e.g., if the mined F topics are identical, the coherence score can still be high but meaningless. To alleviate this, we also use the similarity count (SimCount) that was adopted in [10] — for each topic, the similarity count is obtained simply by adding up the overlapped words of the topics within the leading N words, and a smaller SimCount means the mined topics are more distinguishable. When the topics are very correlated (but different), the leading words of the topics may overlap with each other, and thus using SimCount might still not be enough to evaluate the results. We also include clustering accuracy (ClustAcc), obtained by using the mined C⋆ matrix to estimate the weights W of the documents, and applying k-means to W . Since the ground-truth labels of TDT2 and Reuters are known, clustering accuracy can be calculated, and it serves as a good indicator of topic mining results.\nTable 1 shows the experiment results on the TDT2 corpus. From F = 3 to 25, the proposed algorithm (AnchorFree) gives very promising results: for the three considered metrics, AnchorFree consistently gives better results compared to the baselines. Particularly, the ClustAcc’s obtained by AnchorFree are at least 30% higher compared to the baselines for all cases. In addition, the single-topic quality of the topics mined by AnchorFree is the highest in terms of coherence scores; the overlaps between topics are the smallest except for F = 20 and 25.\nTable 2 shows the results on the Reuters-21578 corpus. In this experiment, we can see that XRAY is best in terms of single-topic quality, while AnchorFree is second best when F > 6. For SimCount,\nAnchorFree gives the lowest values when F > 6. In terms of clustering accuracy, the topics obtained by AnchorFree again lead to much higher clustering accuracies in all cases.\nIn terms of the runtime performance, one can see from Fig. 2(a) that FastAnchor, SNPA, XRAY and AnchorFree perform similarly on the TDT2 dataset. SPA is the fastest algorithm since it has a recursive update [6]. The SNPA and XRAY both perform nonnegative least squares-based deflation, which is computationally heavy when the vocabulary size is large, as mentioned in Remark 1. AnchorFree uses AO and small-scale linear programming, which is conceptually more difficult compared to SNPA and XRAY. However, since the linear programs involved only have F variables and the number of AO iterations is usually small (smaller than 5 in practice), the runtime performance is quite satisfactory and is close to those of SNPA and XRAY which are greedy algorithms. The runtime performance on the Reuters dataset is shown in Fig. 2(b), where one can see that the deflation-based methods are faster. The reason is that the vocabulary size of the Reuters corpus is much smaller compared to that of the TDT2 corpus (18,933 v.s. 36,771).\nTable 3 shows the leading words of the mined topics by FastAnchor and AnchorFree from an F = 5 case using the TDT2 corpus. We only present the result of FastAnchor since it gives qualitatively the best benchmark – the complete result given by all baselines can be found in the supplementary material. We see that the topics given by AnchorFree show clear diversity: Lewinsky scandal, General Motors strike, Space Shuttle Columbia, 1997 NBA finals, and a school shooting in Jonesboro, Arkansas. FastAnchor, on the other hand, exhibit great overlap on the first and the second mined topics. Lewinsky also shows up in the fifth topic mined by FastAnchor, which is mainly about the 1997 NBA finals. This showcases the clear advantage of our proposed criterion in terms of giving more meaningful and interpretable results, compared to the anchor-word based approaches."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we considered identifiable anchor-free correlated topic modeling. A topic estimation criterion based on the word-word co-occurrence/correlation matrix was proposed and its identifiability conditions were proven. The proposed approach features topic identifiability guarantee under much milder conditions compared to the anchor-word assumption, and thus exhibits better robustness to model mismatch. A simple procedure that only involves one eigen-decomposition and a few small linear programs was proposed to deal with the formulated criterion. Experiments on real text corpus data showcased the effectiveness of the proposed approach."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work is supported in part by the National Science Foundation (NSF) under the project numbers NSF-ECCS 1608961 and NSF IIS-1247632 and in part by the Digital Technology Initiative (DTI) Seed Grant, University of Minnesota."
    }, {
      "heading" : "A Proof of Proposition 1",
      "text" : "Let us denote a feasible solution of Problem (3) in the manuscript as (C̃, Ẽ), and let C♮ andE♮ stand for the ground-truth word-topic PMF matrix and the topic correlation matrix, respectively. Note that we can represent any feasible solution as C̃ = C♮A, Ẽ = A −1C♮A −1 where A ∈ RF×F is an invertible matrix. Given rank(P ) = F and that Assumption 1 holds, we must have\nrank(C̃) = rank(Ẽ) = F,\nfor any solution pair (C̃, Ẽ). In fact, if the anchor-word assumption holds, then there is a nonsingular diagonal submatrix in C♮, so rank(C♮) = F , and the same holds for C̃ = C♮A since A is invertible. By the assumption rank(P ) = F and the equality P = C♮E♮CT♮ = C̃ẼC̃ T , one can see that all the factors must have full column rank. Therefore, | det Ẽ| > 0 for any feasible Ẽ – a trivial solution cannot arise under the model considered.\nFurthermore, C̃ satisfies C̃ T 1 = 1 and C̃ ≥ 0 since C̃ is a solution to Problem (3). Because the\nrows of Diag(c) all appear in the rows of C under Assumption 1, a matrix A satisfies C̃(Λ, :) = C(Λ, :)A ≥ 0 if and only if A ≥ 0. Also note that ATCT1 = 1 ⇒ AT1 = 1. Then, we have that\n| detA| ≤ F ∏\nf=1\n‖A(:, f)‖2 ≤ F ∏\nf=1\n‖A(:, f)‖1 = F ∏\nf=1\nA(:, f)T1 = 1, (8)\nwhere the first bounding step is the Hadamard inequality, the second comes from elementary properties of vector norms, and for non-negative vectors the ℓ1 norm is simply the sum of all elements. The first inequality becomes equality if and only if A is a column-orthogonal matrix, and the second holds with equality if and only if A(:, f) for f = 1, . . . , F are unit vectors. Therefore, for nonnegative matrices the equalities in (8) hold if and only if A is a permutation matrix. As a result, any alternative solution Ẽ has the form that Ẽ = A−1E♮A −1, and we simply have that\n| det Ẽ| = | detA−1 detE♮ detA−1| = | detE|| detA|−2 ≥ | detE♮|, where equality holds if and only if A is a permutation matrix. This means that for optimal solutions that satisfy P = C⋆E⋆C T ⋆ , we have C⋆ = C♮Π and E⋆ = Π\nTE♮Π , and achieve minimal value | detE⋆|, where Π is a permutation matrix. Q.E.D."
    }, {
      "heading" : "B Proof of Lemma 1",
      "text" : "If C is sufficiently scattered, it satisfies\ncone(CT )∗ ⊆ K. (9) Suppose that C is rank-deficient. Then, all the vectors that lie in the null space of C satisfy Cx = 0, which implies that for x ∈ N (C) we have\nCx ≥ 0. (10) Eq. (10) and Eq. (9) together imply that\nN (C) ⊆ K. However, a null space cannot be contained in a second-order cone, which is a contradiction.\nWe now show that any feasible solution pair (Ẽ, C̃) has full rank. Denote the ground-truth wordtopic PMF matrix as C♮, and the correlation matrix between topics as E♮. Under Assumption 2, the ground-truth C♮ has full column rank, and thus E♮ ∈ RF×F has full rank when rank(P ) = F . Now, since any other feasible solution can be written as C = C♮A, E = A−1E♮A−1, where A is invertible, we have that any feasible solution pair (Ẽ, C̃) has full rank and det Ẽ is bounded away from zero. Q.E.D."
    }, {
      "heading" : "C Proof of Theorem 1",
      "text" : "Denote the ground truth word-topic PMF matrix as C♮, and the correlation matrix between topics as E♮. What we observe is their product\nP = C♮E♮C T ♮ ,\nand we want to infer, from the observation P , what the matrices C♮ and E♮ are. The method proposed in this paper is via solving (3), repeated here\nminimize E,C\n| detE|\nsubject to P = CECT\nCT1 = 1,C ≥ 0.\nNow, denote one optimal solution of the above as C⋆ and E⋆, and Theorem 1 claims that if C♮ is sufficiently scattered (cf. Assumption 2), then there exists a permutation matrix Π such that\nC⋆ = C♮Π , E⋆ = Π TE♮Π .\nBecause rank(P ) = F , and both C♮ and C⋆ have F columns, this means C♮ and C⋆ span the same column space, therefore there exists a non-singular matrix A such that\nC⋆ = C♮A, E⋆ = A −1E♮A −T .\nIn terms of problem (3), C♮ and E♮ are clearly feasible, which yields an objective value detE♮. Since we assume (C⋆,E⋆) is an optimal solution of (3), we have that\n| detE⋆| = | detA−1 detE♮ detA−T | ≤ | detE♮|,\nimplying | detA| ≥ 1. (11)\nOn the other hand, since C⋆ is feasible for (3), we also have that\nC♮A ≥ 0,ATCT♮ 1 = AT1 = 1. Geometrically, the inequality constraint C♮A ≥ 0 means that columns of A are contained in cone(CT♮ ) ∗. We assume C♮ is sufficiently scattered, therefore\nA(:, f) ∈ cone(CT♮ )∗ ⊆ K, or equivalently ‖A(:, f)‖2 ≤ 1TA(:, f). Then for matrix A, we have that\n| detA| ≤ F ∏\nf=1\n‖A(:, f)‖2 ≤ F ∏\nf=1\n1 TA(:, f) = 1. (12)\nCombining (11) and (12), we conclude that\n| detA| = 1.\nFurthermore, if (12) holds as an equality, we must have\n‖A(:, f)‖2 = 1TA(:, f), ∀ f = 1, ..., F, which, geometrically, means that the columns of A all lie on the boundary of K. However, since C♮ is sufficiently scattered,\ncone(CT♮ ) ∗ ∩ bdK = {λef : λ ≥ 0, f = 1, ..., F},\nso A(:, f) being contained in cone(CT♮ ) ∗ then implies that columns of A can only be selected from the columns of the identity matrix I. Together with the fact that A should be non-singular, we have that A can only be a permutation matrix. Q.E.D."
    }, {
      "heading" : "D Synthetic Experiments",
      "text" : "In this section we give simulation results showing the word-topic PMF matrix C and topic correlation matrix E can indeed be exactly recovered even in the absence of anchor words, using synthetic data. For a given vocabulary size V = 1000 and number of topics F increasing from 5 to 30, ground truth matrices C♮ and E♮ are synthetically generated: the entries of C♮ are first drawn from an i.i.d. exponential distribution, and then approximately 50% of the entries are randomly set to zero, according to an i.i.d. Bernoulli distribution; matrix E♮ is obtained from RTR/F , where entries of the F × F matrix R are drawn from an i.i.d. Gaussian distribution. In this way, C♮ is sufficiently scattered with very high probability, but is very unlikely to satisfy the separability / anchor-word assumption.\nUsing the synthetically generatedC♮ and E♮, we set the word co-occurrence matrix P = C♮E♮C T ♮ , and apply various topic modeling algorithms on P to try to recover C♮ and E♮, including our proposed AnchorFree described in Algorithm 2. Denoting the output of any algorithm as C⋆ and E⋆, before we compare them with the ground truth C♮ and E♮, we need to fix the permutation ambiguity. This task can be formulated as the following optimization problem\nminimize Π ‖C⋆ −C♮Π‖2F subject to Π is a permutation matrix\nwhich is equivalent to the linear assignment problem, and can be solved efficiently via the Hungarian algorithm. After optimally matching the columns of C⋆ and C♮, the estimation errors ‖C⋆ − C♮‖2F and ‖E⋆ − E♮‖2F given by different methods are shown in Table 4 and 5, where each estimation error is averaged over 10 Monte-Carlo trials. Based on the results shown in Table 4 and 5, several comments are in order:\n1. The anchor-word-based algorithms are not able to recover the ground-truth C♮ and E♮, since the separability / anchor-word assumption is grossly violated;\n2. AnchorFree, on the other hand, recovers C♮ and E♮ almost perfectly in all the cases under test, which supports our claim in Theorem 1;\n3. Even though the identification criterion (3) is a non-convex optimization problem, the proposed procedure empirically always works, which is obviously encouraging and deserves future study."
    }, {
      "heading" : "E Complete Results of the Illustrative Example",
      "text" : "The complete results of the illustrative example in the manuscript are presented in Tables 6-10. One observation is that FastAnchor, SPA and SNPA give the same anchor words and topics with different orders. XRAY gives different anchor words and topics, but the topics mined are qualitatively worse compared to those of FastAnchor, SPA and SNPA. The proposed AnchorFree algorithm yields five clean topics."
    } ],
    "references" : [ {
      "title" : "Probabilistic topic models",
      "author" : [ "D.M. Blei" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "T.L. Griffiths", "M. Steyvers" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Computing a nonnegative matrix factorization– provably",
      "author" : [ "S. Arora", "R. Ge", "R. Kannan", "A. Moitra" ],
      "venue" : "In ACM symposium on Theory of Computing,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Factoring nonnegative matrices with linear programs",
      "author" : [ "B. Recht", "C. Re", "J. Tropp", "V. Bittorf" ],
      "venue" : "In Proc. NIPS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Fast and robust recursive algorithms for separable nonnegative matrix factorization",
      "author" : [ "N. Gillis", "S.A. Vavasis" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Robustness analysis of hottopixx, a linear programming model for factoring nonnegative matrices",
      "author" : [ "N. Gillis" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Fast conical hull algorithms for near-separable non-negative matrix factorization",
      "author" : [ "A. Kumar", "V. Sindhwani", "P. Kambadur" ],
      "venue" : "In Proc. ICML-12,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Learning topic models–going beyond SVD",
      "author" : [ "S. Arora", "R. Ge", "A. Moitra" ],
      "venue" : "In Proc. FOCS",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "A practical algorithm for topic modeling with provable guarantees",
      "author" : [ "S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu" ],
      "venue" : "In Proc. ICML-13,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Successive nonnegative projection algorithm for robust nonnegative blind source separation",
      "author" : [ "N. Gillis" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "When does non-negative matrix factorization give a correct decomposition into parts",
      "author" : [ "D. Donoho", "V. Stodden" ],
      "venue" : "In Proc. NIPS 2013,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "A spectral algorithm for latent Dirichlet allocation",
      "author" : [ "A. Anandkumar", "Y.-K. Liu", "D.J. Hsu", "D.P. Foster", "S.M. Kakade" ],
      "venue" : "In Proc. NIPS",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Two SVDs suffice: Spectral decompositions for probabilistic topic modeling and latent Dirichlet allocation",
      "author" : [ "A. Anandkumar", "S.M. Kakade", "D.P. Foster", "Y.-K. Liu", "D. Hsu" ],
      "venue" : "Technical report,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "When are overcomplete topic models identifiable? uniqueness of tensor Tucker decompositions with structured sparsity",
      "author" : [ "A. Anandkumar", "D.J. Hsu", "M. Janzamin", "S.M. Kakade" ],
      "venue" : "In Proc. NIPS",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Locally consistent concept factorization for document clustering",
      "author" : [ "D. Cai", "X. He", "J. Han" ],
      "venue" : "IEEE Trans. Knowl. Data Eng.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Non-negative matrix factorization revisited: Uniqueness and algorithm for symmetric decomposition",
      "author" : [ "K. Huang", "N. Sidiropoulos", "A. Swami" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Principled neuro-functional connectivity discovery",
      "author" : [ "K. Huang", "N.D. Sidiropoulos", "E.E. Papalexakis", "C. Faloutsos", "P.P. Talukdar", "T.M. Mitchell" ],
      "venue" : "In Proc. SIAM Conference on Data Mining (SDM),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Blind separation of quasi-stationary sources: Exploiting convex geometry in covariance domain",
      "author" : [ "X. Fu", "W.-K. Ma", "K. Huang", "N.D. Sidiropoulos" ],
      "venue" : "IEEE Trans. Signal Process.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Convex analysis for non-negative blind source separation with application in imaging",
      "author" : [ "W.-K. Ma", "T.-H. Chan", "C.-Y. Chi", "Y. Wang" ],
      "venue" : "Convex Optimization in Signal Processing and Communications,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Topic mining, or topic modeling, has attracted significant attention in the broader machine learning and data mining community [1].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : "proposed a Latent Dirichlet Allocation (LDA) model for topic mining [2], where the topics are modeled as probability mass functions (PMFs) over a vocabulary and each document is a mixture of the PMFs.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "Under this model, posterior inference-based methods and approximations were proposed [2, 3], but identifiability issues – i.",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Under this model, posterior inference-based methods and approximations were proposed [2, 3], but identifiability issues – i.",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 7,
      "context" : "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 8,
      "context" : "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 9,
      "context" : "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 10,
      "context" : "In recent years, considerable effort has been invested in designing identifiable models and estimation criteria as well as polynomial time solvable algorithms for topic modeling [4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 11,
      "context" : "Essentially, these algorithms are based on the so-called separable nonnegative matrix factorization (NMF) model [12].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].",
      "startOffset" : 114,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].",
      "startOffset" : 114,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].",
      "startOffset" : 151,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].",
      "startOffset" : 151,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].",
      "startOffset" : 151,
      "endOffset" : 165
    }, {
      "referenceID" : 9,
      "context" : "Based on this assumption, two classes of algorithms are usually employed, namely linear programming based methods [5, 7] and greedy pursuit approaches [11, 6, 8, 10].",
      "startOffset" : 151,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "However, normalization at the factorization stage is usually not desired, since it may destroy the good conditioning of the data matrix brought by pre-processing and amplify noise [8].",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "[9, 10] works with the pairwise word-word correlation matrix, which has the advantage of suppressing sampling noise and also features better scalability.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "[9, 10] works with the pairwise word-word correlation matrix, which has the advantage of suppressing sampling noise and also features better scalability.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "However, [9, 10] did not relax the anchor-word assumption or the need for normalization, and did not explore the symmetric structure of the co-occurrence matrix – i.",
      "startOffset" : 9,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : "However, [9, 10] did not relax the anchor-word assumption or the need for normalization, and did not explore the symmetric structure of the co-occurrence matrix – i.",
      "startOffset" : 9,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].",
      "startOffset" : 95,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].",
      "startOffset" : 95,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : ", the algorithms in [9, 10] are essentially the same asymmetric separable NMF algorithms as in [4, 6, 8].",
      "startOffset" : 95,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : ", [13, 14, 15] make use of third or higher-order statistics of the data corpus to formulate the topic modeling problem as a tensor factorization problem.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 13,
      "context" : ", [13, 14, 15] make use of third or higher-order statistics of the data corpus to formulate the topic modeling problem as a tensor factorization problem.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 14,
      "context" : ", [13, 14, 15] make use of third or higher-order statistics of the data corpus to formulate the topic modeling problem as a tensor factorization problem.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 12,
      "context" : ", second-order word correlation statistics); and ii) identifiability is guaranteed only when the topics are uncorrelated – where a super-symmetric parallel factor analysis (PARAFAC) model can be obtained [13, 14].",
      "startOffset" : 204,
      "endOffset" : 212
    }, {
      "referenceID" : 13,
      "context" : ", second-order word correlation statistics); and ii) identifiability is guaranteed only when the topics are uncorrelated – where a super-symmetric parallel factor analysis (PARAFAC) model can be obtained [13, 14].",
      "startOffset" : 204,
      "endOffset" : 212
    }, {
      "referenceID" : 9,
      "context" : "Uncorrelatedness is a restrictive assumption [10].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : ", sparsity of topic PMFs [15].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "In this work, our interest lies in topic mining using word-word correlation matrices like in [9, 10], because of its potential scalability and noise robustness.",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "In this work, our interest lies in topic mining using word-word correlation matrices like in [9, 10], because of its potential scalability and noise robustness.",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "where C ∈ R is the word-topic matrix, whose f -th column C(:, f) represents the probability mass function (PMF) of topic f over a vocabulary of words, and W (f, d) denotes the weight of topic f in document d [2, 13, 10].",
      "startOffset" : 208,
      "endOffset" : 219
    }, {
      "referenceID" : 12,
      "context" : "where C ∈ R is the word-topic matrix, whose f -th column C(:, f) represents the probability mass function (PMF) of topic f over a vocabulary of words, and W (f, d) denotes the weight of topic f in document d [2, 13, 10].",
      "startOffset" : 208,
      "endOffset" : 219
    }, {
      "referenceID" : 9,
      "context" : "where C ∈ R is the word-topic matrix, whose f -th column C(:, f) represents the probability mass function (PMF) of topic f over a vocabulary of words, and W (f, d) denotes the weight of topic f in document d [2, 13, 10].",
      "startOffset" : 208,
      "endOffset" : 219
    }, {
      "referenceID" : 15,
      "context" : "a nonnegative matrix factorization (NMF) model – and many early works tried to use NMF and variants to deal with this problem [16].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : "However, NMF does not admit a unique solution in general, unless both C and W satisfy some sparsity-related conditions [17].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 11,
      "context" : "Such models and algorithms usually rely on an assumption called “separability” in the NMF literature [12]:",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "Algorithm 1: Successive Projection Algorithm [6] input : D; F .",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "The arguably simplest algorithm is the so-called successive projection algorithm (SPA) [6] that is presented in Algorithm 1.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "SPA-like algorithms first define a normalized matrix X = DΣ where Σ = Diag(1D ) [11].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "The algorithms in [8, 10, 11] can also be considered variants of SPA, with different deflation procedures and pre-/postprocessing.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "The algorithms in [8, 10, 11] can also be considered variants of SPA, with different deflation procedures and pre-/postprocessing.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "The algorithms in [8, 10, 11] can also be considered variants of SPA, with different deflation procedures and pre-/postprocessing.",
      "startOffset" : 18,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "In particular, the algorithm in [8] avoids normalization — for real-word data, normalization at the factorization stage may amplify noise and damage the good conditioning of the data matrix brought by pre-processing, e.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : ", the tf-idf procedure [8].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "To pick out vertices, there are also algorithms using linear programming and sparse optimization [7, 5], but these have serious scalability issues and thus are less appealing.",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "To pick out vertices, there are also algorithms using linear programming and sparse optimization [7, 5], but these have serious scalability issues and thus are less appealing.",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "In [9, 10, 14, 15], the authors proposed to use second and higher-order statistics for topic mining.",
      "startOffset" : 3,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "In [9, 10, 14, 15], the authors proposed to use second and higher-order statistics for topic mining.",
      "startOffset" : 3,
      "endOffset" : 18
    }, {
      "referenceID" : 13,
      "context" : "In [9, 10, 14, 15], the authors proposed to use second and higher-order statistics for topic mining.",
      "startOffset" : 3,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "In [9, 10, 14, 15], the authors proposed to use second and higher-order statistics for topic mining.",
      "startOffset" : 3,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "[9, 10] proposed to work with the following matrix:",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "[9, 10] proposed to work with the following matrix:",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "The matrix P is by definition a word-word correlation matrix, but also has a nice interpretation: if D(v, d) denotes the frequency of word v occurring in document d, P (i, j) is the likelihood that term i and j co-occur in a document [9, 10].",
      "startOffset" : 234,
      "endOffset" : 241
    }, {
      "referenceID" : 9,
      "context" : "The matrix P is by definition a word-word correlation matrix, but also has a nice interpretation: if D(v, d) denotes the frequency of word v occurring in document d, P (i, j) is the likelihood that term i and j co-occur in a document [9, 10].",
      "startOffset" : 234,
      "endOffset" : 241
    }, {
      "referenceID" : 9,
      "context" : "The algorithm proposed in [10] also makes use of Assumption 1 and is conceptually close to Algorithm 1.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "The work in [13, 14, 15] relaxed the anchor-word assumption.",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "The work in [13, 14, 15] relaxed the anchor-word assumption.",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "The work in [13, 14, 15] relaxed the anchor-word assumption.",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "The work in [13, 14] showed that P is a tensor satisfying the parallel factor analysis (PARAFAC) model and thus C is uniquely identifiable, if the topics are uncorrelated, which is a restrictive assumption",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "The work in [13, 14] showed that P is a tensor satisfying the parallel factor analysis (PARAFAC) model and thus C is uniquely identifiable, if the topics are uncorrelated, which is a restrictive assumption",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "When the topics are correlated, additional assumptions like sparsity are needed to restore identifiability [15].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "However, if the deflation procedure in Algorithm 1 (the update of Θ) has constraints like in [8, 11], there is a serious complexity issue: solving a constrained least squares problem with FV variables is not an easy task.",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "However, if the deflation procedure in Algorithm 1 (the update of Θ) has constraints like in [8, 11], there is a serious complexity issue: solving a constrained least squares problem with FV variables is not an easy task.",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "Data sparsity is destroyed after the first deflation step, and thus even first-order methods or coordinate descent as in [8, 11] do not really help.",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Data sparsity is destroyed after the first deflation step, and thus even first-order methods or coordinate descent as in [8, 11] do not really help.",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "Note that the dual cone of K is another second-order cone [12], i.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "The sufficiently scattered assumption appeared in identifiability proofs of several matrix factorization models [17, 18, 19] with different identification criteria.",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "The sufficiently scattered assumption appeared in identifiability proofs of several matrix factorization models [17, 18, 19] with different identification criteria.",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : "The sufficiently scattered assumption appeared in identifiability proofs of several matrix factorization models [17, 18, 19] with different identification criteria.",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "[17] used this condition to show the identifiability of plain NMF, while Fu et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] related the sufficiently scattered condition to the so-called volume-minimization criterion for blind source separation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Here, we propose to employ the solver proposed in [18], where the same subproblem (7) was used to solve a dynamical system identification problem.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : "The idea is to apply the co-factor expansion to deal with the determinant objective function, first proposed in the context of non-negative blind source separation [20]: if we fix all the columns of M except the f th one, detM becomes a linear function with respect to M(:, f), i.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 8,
      "context" : "We use the standard tf-idf data as the D matrix, and estimate the correlation matrix using the biased estimator suggested in [9].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "Specifically, the successive projection algorithm (SPA) [6], the successive nonnegative projection algorithm (SNPA) [11], the XRAY algorithm [8], and the fast anchor words (FastAnchor) [10] algorithm.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "Specifically, the successive projection algorithm (SPA) [6], the successive nonnegative projection algorithm (SNPA) [11], the XRAY algorithm [8], and the fast anchor words (FastAnchor) [10] algorithm.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "Specifically, the successive projection algorithm (SPA) [6], the successive nonnegative projection algorithm (SNPA) [11], the XRAY algorithm [8], and the fast anchor words (FastAnchor) [10] algorithm.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "Specifically, the successive projection algorithm (SPA) [6], the successive nonnegative projection algorithm (SNPA) [11], the XRAY algorithm [8], and the fast anchor words (FastAnchor) [10] algorithm.",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "Since we are interested in word-word correlation/co-occurrence based mining, all the algorithms are combined with the framework provided in [10] and the efficient RecoverL2 process is employed for estimating the topics after the anchors are identified.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "To alleviate this, we also use the similarity count (SimCount) that was adopted in [10] — for each topic, the similarity count is obtained simply by adding up the overlapped words of the topics within the leading N words, and a smaller SimCount means the mined topics are more distinguishable.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "SPA is the fastest algorithm since it has a recursive update [6].",
      "startOffset" : 61,
      "endOffset" : 64
    } ],
    "year" : 2016,
    "abstractText" : "In topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words – i.e., words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic. This paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art.",
    "creator" : "LaTeX with hyperref package"
  }
}