{
  "name" : "1701.02810.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "OpenNMT: Open-Source Toolkit for Neural Machine Translation",
    "authors" : [ "Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016). Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization.\nAs NMT approaches are standardized, it becomes more important for the machine translation and NLP community to develop open implementations for researchers to benchmark against, learn from, and extend upon. Just as the SMT community benefited greatly from toolkits like Moses (Koehn et al., 2007) for phrase-based MT and the CDec toolkit (Dyer et al., 2010) for syntax-based MT, NMT toolkits can provide a foundation for the research community. A toolkit should aim to\nprovide a shared framework for developing and comparing open-source systems, while at the same time being efficient and accurate enough to be used in production contexts.\nCurrently there are several existing NMT implementations. Many systems such as those developed in industry by Google (Wu et al., 2016), Microsoft, and Baidu, are closed source, and are unlikely to be released with unrestricted licenses. Many other systems such as GroundHog, Blocks, tensorflow-seq2seq, lamtram, and our own seq2seq-attn, exist mostly as research code. These libraries provide important functionality but minimal support to production users. Perhaps most promising is the University of Edinburgh’s Nematus system originally based on NYU’s NMT system. Nematus provides high-accuracy translation, many options, clear documentation, and has been used in several successful research projects. In the development of this project, we aimed to build upon the strengths of this system, while providing additional documentation and functionality to provide a useful open-source NMT framework for the ar X iv :1 70 1.\n02 81\n0v 1\n[ cs\n.C L\n] 1\n0 Ja\nn 20\n17\nNLP community in academia and industry. With this goal in mind, we introduce OpenNMT (http://opennmt.net), an open-source framework for neural machine translation. OpenNMT is a complete NMT implementation. In addition to providing code for the core translation tasks, OpenNMT was designed with three aims: (a) prioritize first training and test efficiency, (b) maintain model modularity and readability, (c) support significant research extensibility.\nThis report describes how the first-release of the system targets these criteria. We begin by briefly surveying the background for NMT, describing the high-level implementation details, and then describing specific case studies for the three criteria. We end by showing preliminary benchmarks of the system in terms of accuracy, speed, and memory usage for several translation and translation-like tasks."
    }, {
      "heading" : "2 Background",
      "text" : "NMT has now been extensively described in many excellent tutorials (see for instance https://sites.google.com/site/ acl16nmt/home). We give a condensed overview.\nNMT takes a conditional language modeling view of translation by modeling the probability of a target sentence w1:T given a source sentence x1:S as p(w1:T |x) = ∏T 1 p(wt|w1:t−1, x; θ). This distribution is estimated using an attentionbased encoder-decoder architecture (Bahdanau et al., 2014). A source encoder recurrent neural network (RNN) maps each source word to a word vector, and processes these to a sequence of hidden vectors h1, . . . ,hS . The target decoder combines an RNN hidden representation of previously generated words (w1, ...wt−1) with source hidden vectors to predict scores for each possible next word. A softmax layer is then used to produce a next-word distribution p(wt|w1:t−1, x; θ). The source hidden vectors influence the distribution through an attention pooling layer that weights each source word relative to its expected contribution to the target prediction. The complete model is trained end-to-end to minimize the negative loglikelihood of the training corpus. An unfolded network diagram is shown in Figure 1.\nIn practice, there are also several other important aspects that contribute to model effectiveness: (a) It is important to use a gated RNN such as\nan LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Chung et al., 2014) which help the model learn long-term features. (b) Translation requires relatively large, stacked RNNs, which consist of several layers (2-16) of RNN at each time step (Sutskever et al., 2014). (c) Input feeding, where the previous attention vector is fed back into the input as well as the predicted word, has been shown to be quite helpful for machine translation (Luong et al., 2015). (d) Test-time decoding is done through beam search where multiple hypothesis target predictions are considered at each time step. Implementing these correctly can be difficult, which motivates their inclusion in an NMT framework."
    }, {
      "heading" : "3 Implementation",
      "text" : "OpenNMT is a complete library for learning, training, and deploying neural machine translation models. The system is successor to seq2seq-attn developed at Harvard, and has been completely rewritten for ease of efficiency, readability, and generalizability. It includes vanilla NMT models along with support for attention, gating, stacking, input feeding, regularization, beam search and all other options necessary for state-of-the-art performance. The system is implemented using the Torch mathematical framework and neural network library, and can be easily be extended using Torch’s internal standard neural network components.\nThe system has been developed completely in the open on GitHub at (http://github.com/ opennmt/opennmt) and is MIT licensed. The first version has primarily (intercontinental) contributions from SYSTRAN Paris and the Harvard NLP group. Since official beta release, the project has been starred by over 500 users, and there have been active development by those outside of these two organizations. The project has an active forum for community feedback.\nOne nice aspect of NMT as a model is its relative compactness. The complete OpenNMT system including preprocessing is roughly 4K lines of code. For comparison the Moses SMT framework including language modeling is over 100K lines. This makes the system easy to completely understand for newcomers. The project is fully self-contained depending on minimal number of external Lua libraries and including also a simple language independent reversible tokenization and\ndetokenization tools."
    }, {
      "heading" : "4 Design Goals",
      "text" : "As the low-level details of NMT have been covered in previous works, we focus this report on the design goals of OpenNMT. We focus particularly on three ordered criteria: system efficiency, code modularity, and model extensibility."
    }, {
      "heading" : "4.1 System Efficiency",
      "text" : "As NMT systems can take from days to weeks to train, training efficiency is a paramount concern. Slightly faster training can make be the difference between plausible and impossible experiments.\nOptimization: Memory Sharing When training GPU-based NMT models, memory size restrictions are the most common limiter of batch size, and thus directly impact training time. Neural network toolkits, such as Torch, are often designed to trade-off extra memory allocations for speed and declarative simplicity. For OpenNMT, we wanted to have it both ways, and so we implemented an external memory sharing system that exploits the known time-series control flow of NMT systems and aggressively shares the internal buffers between clones. The potential shared buffers are dynamically calculated by exploration of the network graph before starting training. This makes the system slightly less flexible than toolkits such as Element-RNN (Léonard et al., 2015), but provides a saving of 70% of GPU memory on the default model.\nOptimization: Multi-GPU OpenNMT additionally supports multi-GPU training using data parallelism. Each GPU has a replica of the master parameters and process independent batches during training phase. Two modes are available: synchronous and asynchronous training. In synchronous training, batches on parallel GPU are run simultaneously and gradients aggregated to update master parameters before resynchronization on each GPU for the following batch. In asynchronous training, batches are run independent on each GPU, and independent gradients accumulated to the master copy of the parameters. Asynchronous SGD is known to provide faster convergence (Dean et al., 2012).\nCase Study: C/Mobile/GPU Translation Training NMT systems requires significant code\ncomplexity to facilitate fast back-propagationthrough-time. At test time, the system is much less complex, and only requires (i) forwarding values through the network and (ii) running a beam search that is much simplified compared to SMT. To exploit this asymmetry, OpenNMT includes several different decoders specialized for different run-time environments: a batched CPU/GPU decoder for very quickly translating a large set of sentences, a simple single-instance decoder for use on mobile devices, and a specialized C decoder. The last decoder is particularly suited for industrial use as it can run on CPU in standard production environments. The decoder reads the structure of the network from Lua and then uses the Eigen package to implement the basic linear algebra necessary for decoding."
    }, {
      "heading" : "4.2 Modularity for Research",
      "text" : "A secondary goal was a desire for code readability for non-experts. We targeted this goal by explicitly separating out many optimizations from the core model, and by including tutorial documentation within the code. To test whether this approach would allow novel feature development we experimented with two case studies.\nCase Study: Factored Neural Translation In feature-based factored neural translation (Sennrich and Haddow, 2016), instead of generating a word at each time step, the model generates both word and associated features. For instance, the system might include words and separate case features. This extension requires modifying both the inputs and the output of the decoder to generate multiple symbols. In OpenNMT both of these aspects are abstracted from the core translation code, and therefore factored translation simply modifies the input network to instead process the featurebased representation, and the output generator network to instead produce multiple conditionally independent predictions.\nCase Study: Attention Networks The use of attention over the encoder at each step of translation is crucial for the model to perform well. The default method is to utilize the global attention mechanism. However there are many other types of attention that have recently proposed including local attention (Luong et al., 2015), sparse-max attention (Martins and Astudillo, 2016), hierarchical attention (Yang et al., 2016) among others. As this is simply a module in OpenNMT it can easily\nbe substituted. Recently the Harvard group developed a structured attention approach, that utilizes graphical model inference to compute this attention. The method is quite computationally complex; however as it is modularized by the Torch interface, it can be used in OpenNMT to substitute for standard attention."
    }, {
      "heading" : "4.3 Extensibility",
      "text" : "Deep learning is a quickly evolving field. Recently work such as variational seq2seq auto-encoders (Bowman et al., 2016) or memory networks (Weston et al., 2014), propose interesting extensions to basic seq2seq models. We next discuss a case study to demonstrate that OpenNMT is extensible to future variants.\nCase Study: Image-to-Text Recent work has shown that NMT-like systems are effective for image-to-text generation tasks. (Xu et al., 2015). This task is quite different from standard machine translation as the source sentence is now an image(!) However, the future of translation may require this style of (multi-)modal inputs (e.g. http://www.statmt.org/wmt16/ multimodal-task.html). As a case study, we adapted the im2markup system (Deng et al., 2016) to use OpenNMT. This model replaces the source RNN with a deep convolution over the source input. Excepting preprocessing, the entire adaptation requires less than 500 lines of additional code and is also open-sourced as github. com/opennmt/im2text."
    }, {
      "heading" : "5 Benchmarks",
      "text" : "We now document some preliminary runs of the model. We expect performance and memory usage to improve with further development. Public benchmarks are available at opennmt.net/ Models/, as are additional experiments such as for other non-MT tasks like summarization.\nThe main benchmarks are run using an Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz, 256GB Mem, trained on 1 GPU GeForce GTX 1080 (Pascal) with CUDA v. 8.0 (driver 375.20) and cuDNN (v. 5005). The comparison, shown in Table 1, is on English-to-German (EN→DE) using the WMT 20151 dataset. Here we compare, BLEU score, as well as training and test speed to the publicly available Nematus system. 2\nWe additionally trained a multilingual translation model following Johnson (2016). The model translates from and to French, Spanish, Portuguese, Italian, and Romanian. Training data is 4M sentences and was selected from the open parallel corpus3, specifically from Europarl, GlobalVoices and Ted. Corpus was selected to be multisource, multi-target: each sentence has its translation in the 4 other languages. Corpus was tokenized using shared Byte Pair Encoding of 32k. Comparative results between multi-way translation and each of the 20 independent training are presented in Table 2. The systematically large improvement shows that language pair benefits from training jointly with the other language pairs."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduce OpenNMT, a research toolkit for NMT that prioritizes efficiency and modularity. We hope to further develop OpenNMT to maintain strong MT results at the research frontier, providing a stable and extensible framework for production use."
    } ],
    "references" : [ {
      "title" : "Neural Machine http://statmt.org/wmt15 https://github.com/rsennrich/nematus",
      "author" : [ "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Luke Vilnis", "Oriol Vinyals", "Andrew M. Dai", "Rafal Józefowicz", "Samy Bengio" ],
      "venue" : "In Proceedings of the 20th SIGNLL Conference on Computational Natural",
      "citeRegEx" : "Bowman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Transla",
      "author" : [ "Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.3555",
      "citeRegEx" : "Chung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Systran’s pure neural machine translation system",
      "author" : [ "Crego et al.2016] Josep Crego", "Jungi Kim", "Jean Senellart" ],
      "venue" : "arXiv preprint arXiv:1602.06023",
      "citeRegEx" : "Crego et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Crego et al\\.",
      "year" : 2016
    }, {
      "title" : "What you get is what you see: A visual markup decompiler. CoRR, abs/1609.04938",
      "author" : [ "Deng et al.2016] Yuntian Deng", "Anssi Kanervisto", "Alexander M. Rush" ],
      "venue" : null,
      "citeRegEx" : "Deng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2016
    }, {
      "title" : "cdec: A decoder, alignment, and learning framework for finite-state and context-free",
      "author" : [ "Dyer et al.2010] Chris Dyer", "Jonathan Weese", "Hendra Setiawan", "Adam Lopez", "Ferhan Ture", "Vladimir Eidelman", "Juri Ganitkevitch", "Phil Blunsom", "Philip Resnik" ],
      "venue" : null,
      "citeRegEx" : "Dyer et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Effective Approaches to Attention-based Neural Machine Translation",
      "author" : [ "Hieu Pham", "Christopher D. Manning" ],
      "venue" : "In Proceedings of EMNLP",
      "citeRegEx" : "Luong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "From softmax to sparsemax: A sparse model of attention and multi-label classification",
      "author" : [ "Martins", "Astudillo2016] André FT Martins", "Ramón Fernandez Astudillo" ],
      "venue" : "arXiv preprint arXiv:1602.02068",
      "citeRegEx" : "Martins et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2016
    }, {
      "title" : "Linguistic input features improve neural machine translation",
      "author" : [ "Sennrich", "Haddow2016] Rico Sennrich", "Barry Haddow" ],
      "venue" : "arXiv preprint arXiv:1606.02892",
      "citeRegEx" : "Sennrich et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to Sequence Learning with Neural Networks",
      "author" : [ "Oriol Vinyals", "Quoc V. Le" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Yang et al.2016] Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy" ],
      "venue" : "In Proceedings of the 2016 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Yang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Neural machine translation (NMT) is a new methodology for machine translation that has led to remarkable improvements, particularly in terms of human evaluation, compared to rule-based and statistical machine translation (SMT) systems (Wu et al., 2016; Crego et al., 2016).",
      "startOffset" : 235,
      "endOffset" : 272
    }, {
      "referenceID" : 11,
      "context" : "Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al.",
      "startOffset" : 60,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "Originally developed using pure sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) and improved upon using attention-based variants (Bahdanau et al.",
      "startOffset" : 60,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization.",
      "startOffset" : 57,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : ", 2014) and improved upon using attention-based variants (Bahdanau et al., 2014; Luong et al., 2015), NMT has now become a widely-applied technique for machine translation, as well as an effective approach for other related NLP tasks such as dialogue, parsing, and summarization.",
      "startOffset" : 57,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : ", 2007) for phrase-based MT and the CDec toolkit (Dyer et al., 2010) for syntax-based MT, NMT toolkits can provide a foundation for the research community.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "This distribution is estimated using an attentionbased encoder-decoder architecture (Bahdanau et al., 2014).",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "In practice, there are also several other important aspects that contribute to model effectiveness: (a) It is important to use a gated RNN such as an LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Chung et al., 2014) which help the model",
      "startOffset" : 197,
      "endOffset" : 217
    }, {
      "referenceID" : 11,
      "context" : "(b) Translation requires relatively large, stacked RNNs, which consist of several layers (2-16) of RNN at each time step (Sutskever et al., 2014).",
      "startOffset" : 121,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "(c) Input feeding, where the previous attention vector is fed back into the input as well as the predicted word, has been shown to be quite helpful for machine translation (Luong et al., 2015).",
      "startOffset" : 172,
      "endOffset" : 192
    }, {
      "referenceID" : 8,
      "context" : "However there are many other types of attention that have recently proposed including local attention (Luong et al., 2015), sparse-max attention (Martins and Astudillo, 2016), hierarchical attention (Yang et al.",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : ", 2015), sparse-max attention (Martins and Astudillo, 2016), hierarchical attention (Yang et al., 2016) among others.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "Recently work such as variational seq2seq auto-encoders (Bowman et al., 2016) or memory networks (We-",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "(Xu et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "we adapted the im2markup system (Deng et al., 2016) to use OpenNMT.",
      "startOffset" : 32,
      "endOffset" : 51
    } ],
    "year" : 2017,
    "abstractText" : "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.",
    "creator" : "LaTeX with hyperref package"
  }
}