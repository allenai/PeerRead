{
  "name" : "1311.7251.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Spatially-Adaptive Reconstruction in Computed Tomography using Neural Networks",
    "authors" : [ "Joseph Shtok", "Michael Zibulevsky" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Computed Tomography, Low-Dose Reconstruction, Neural Networks, Supervised Learning, Filtered-BackProjection (FBP).\nI. INTRODUCTION\nComputed tomography (CT) imaging produces an attenuation map of the scanned object, by sequentially irradiating it with X-rays from several directions. The integral attenuation of the X-rays, measured by comparing the radiation intensity entering and leaving the body, forms the raw data for the CT imaging. In practice, these photon count measurements are degraded by stochastic noise, typically modeled as instances of Poisson random variables. There are also other degradation effects due to a number of physical phenomena – see e.g. [1] for a detailed account.\nGiven the projection data, known as the sinogram, a reconstruction process can be performed in order to recover the attenuation map. Various such algorithms exist, ranging from the simple and still very popular Filtered-Back-Projection (FBP) [2], and all the way to the more advanced Bayesianinspired iterative algorithms (see e.g., [3], [4]) that take the statistical nature of the measurements and the unknown image into account. Since CT relies on X-ray, which is an ionizing radiation known to be dangerous to living tissues, there is a dire and constant need to improve the reconstruction algorithms in an attempt to enable reduction of radiation dose.\nIn this work we are concerned with the question of image post-processing, following the CT reconstruction, for the purpose of getting better quality CT image, thereby permitting an eventual radiation-dose reduction. The proposed method does not focus on a specific CT reconstruction algorithm, nor the properties of the images it produces. Instead, we take a\nAll authors are with the Computer Science Department, Technion - Israel Institute of Technology, Israel.\nThe research leading to these results has received funding from the European Research Council under European Union’s Seventh Framework Program, ERC Grant agreement no. 320649. This research was supported by Gurwin Family Fund.\ngeneric approach which adapts, in an off-line learning process, to any such given algorithm. The only requirement is the access to design parameters of the reconstruction procedure which influence the nature of the output image, such as the resolution-variance trade-off.\nWe aim to exploit the fact that any reconstruction algorithm can provide more image information if instead of one fixed value of a parameter (or a vector of them) controlling the reconstruction, few different values are used (leading to different versions of the image). In order to extract this information from a collection of image versions, we use an Artificial Neural Network (ANN) [5]. The proposed method can also use other techniques for computing a non-linear multivariate regression function.\nNeural networks have been used extensively in medical imaging, particularly for the purpose of CT reconstruction (see Section III for an overview). Here we propose a new constellation, which consists in a local fusion of the different image versions, aimed at an improved reconstruction quality. We use a set of intensity values from a neighborhood of a pixel q, taken from the different versions, as inputs to the network, and train it to compute a (smaller) neighborhood of q which values are as close as possible (in Mean-SquaredError or other sense) to those found in the reference image. As we show in this paper, the proposed approach enables an improvement of the variance-resolution trade-off of a given reconstruction algorithm, i.e. producing images with a reduced amount of noise without compromising the spatial resolution and without introducing artifacts.\nThis paper is organized as follows: Sections II and III are devoted to a brief and general discussion on CT scan/reconstruction and artificial neural networks. Readers familiar with these topics may skip and start reading at Section IV, where the core concept of this work is detailed. This section also contains an illustration on one-dimensional piecewise constant signals, where it is easy to appreciate the action of the proposed algorithm and the effect of local fusion performed by a neural network. In the sequel, the proposed method is implemented on two tomographic reconstruction methods: boosting the Filtered Back-Projection (FBP) is presented in Section V and the same for Penalized Weighted Least-Squares (PWLS) method is described in Section VII. We conclude this work by discussing the computational complexity of the proposed algorithms in Section VIII, and a summary of this work and its potential implications in Section IX.\nar X\niv :1\n31 1.\n72 51\nv1 [\ncs .C\nV ]\n2 8\nN ov\n2 01\n3\n2"
    }, {
      "heading" : "II. BACKGROUND ON COMPUTED TOMOGRAPHY",
      "text" : ""
    }, {
      "heading" : "A. Mathematical Model of CT Scan",
      "text" : "In the process of a CT scan, the object is radiated with X-rays. In this work we consider a reconstruction in a plane from rays incident only to this plane (the two-dimensional tomography). From the mathematical point of view, the considered object is a function f(x) in the plane, which values are the attenuation coefficients of composing materials (i.e., tissues). When the measured photon counts are perfect, the measurements are directly related to the X-ray transform of the function f(x) as a collection of all the straight lines passing through the object, and the value associated with each such line is the integral of f(x) along it. In two dimensions, and under the assumption of a full rotated and parallel beam scan, this coincides with the Radon transform Rf .\nLet ` be a straight line from an X-ray source to a detector. The ideal photon count λ`, measured by the detector is related to Rf via the function\nλ` = λ0e −[Rf ]` , (II.1)\nwhere λ0 is the blank scan count. The scanned data is stored in a matrix which columns correspond to the sampled angle θ; each such column is referred to as a ”view” or a ”projection”, and is acquired, schematically, by a parallel array of Xrays passing through the object at the corresponding angle. The rows of the matrix, corresponding to the sampled values of the distance s, are called the ”bins” of each projection. According to the Equation (II.1), for reconstruction purposes the measurements data undergoes the log transform\ng` = −log( λ` λ0 ). (II.2)\nWe refer to g as the sinogram. The name indicates that every point in the image space traces a sine curve in this domain. Since the sinogram matrix is the (sampled) Radon transform of the original image f(x), a discrete version of the image can be reconstructed by applying the inverse Radon transform (see Section II-B).\nEach measured photon count y` is typically interpreted as an instance of the random variable Y` following a Poisson distribution Y` ∼ Poisson(λ`) [1], [6], [7]. This reflects the photon count statistics at the detectors [8]. For a random variable X ∼ Poisson(λ), the standard deviation σX satisfies σX = √ E(X), and therefore the signal-to-noise ratio of X ,\nSNR(X) = E(X)/σX = √ E(X) monotonously increases with its expected value. In the sinogram domain, the standard deviation of the error between the ideal sinogram and the one computed from the measurements, ĝ − ˆ̄g, is λ−1/2 [9], and this is well approximated by ŷ−1/2 [10]. In Figure 1 we display a sinogram matrix and the corresponding Poisson noise image. Below, one can observe the resulting reconstruction artifacts produced by the standard FBP algorithm (see next sub-section). The sinogram error image has a high-energy regions where the sinogram values are relatively high; this corresponds to the predicted behavior of the noise variance. The reconstruction from the noisy sinogram is contaminated with anisotropic noise, mainly\nin the form of streaks. Their appearance is related to large errors in sinogram values."
    }, {
      "heading" : "B. Reconstruction Algorithms for Computed Tomography",
      "text" : "There are various reconstruction algorithms that aim at computing the attenuation map of the scanned object from its projections. In this paper we shall refer and work with two such algorithms: (i) the Filtered Back-Projection (FBP) ( [2], which is a direct Radon inversion approach. This is a popular technique despite its known flaws; and (ii) an iterative reconstruction algorithm that takes the statistical nature of the unknown and the noise into account (e.g. [3]). Bayesian methods achieve better image quality than the direct Radon inversion, at the expense of longer processing time. We now describe these two methods is somewhat more details. Filtered-Back-Projection Method: Mathematically, FBP is the linear operator of the form\nTFBP = R ∗FlowFRL. (II.3)\nHere R∗ is the adjoint of the Radon transform, known in the literature as ”back-projection”. FRL is a 1-D convolution filter, applied to each individual projection (column in the sinogram matrix). It uses the Ram-Lak kernel k [11], defined in the Fourier domain by k̂(ω) = |ω|, and Flow is a low-pass filter which prevents the noise amplification at high frequencies,\n3 typical for the Ram-Lak action. In clinical CT scanners, the parameters of Flow are tuned for specific needs of the radiologist: different preset values are chosen to view bones, soft tissues, high contrast/smooth images, specific anatomical regions, etc.\nWithout the low-pass filter, the FBP is an exact inverse of the Radon transform in the continuous domain [12] for the noiseless case. Moving from theory to practice, the FBP algorithm does not perform very well. The low-pass 1-D convolution filter in the sinogram domain is not an effective remedy for the projections noise. The problem of photon starvation manifests through outlier values in the sinogram, which propagate to the output image in the form of streak artifacts. They corrupt the image contents and jeopardize its diagnostic value. Those artifacts can be explained as follows: each measured line integral is effectively smeared back over that line through the image by the back-projection; an incorrect measurement results in a (partial) line of wrong intensity in the image. Typically, the streaks radiate from bone regions or metal implants. Statistically-Based Method: The relation between f , the sought CT image, and the vector of measured counts y can be described as\nlog(y) = Af + e, (II.4)\nwhere A approximates the Radon transform and models the scan process in reality. The additive error e (which also depends of f ) stems from the statistical noise. In the Bayesian framework, the reconstruction is performed by computing the Maximum a-Posteriory (MAP) estimate of the image\nf̃ = arg max f P(f |y) = arg max f P(y|f)P(f) P(y) . (II.5)\nFor CT, an accurate statistical model for the data is quite complicated and is often replaced by a Gaussian approximation with a suitable diagonal weighting term whose components are inversely proportional to the measurement variances. This leads to a penalized weighted least-squares (PWLS) formulation, see e.g. [13]\nf̃ = arg min f ‖ log(y)−Af‖D + βR(f), (II.6)\nwhere ‖u‖D = uTDu, D is a diagonal matrix of weights, which in simplistic model are proportional to photon counts y; The penalty term R(f) also referred to as the prior, expresses assumptions on the behavior of the clean CT image. In [14] this expression is chosen as\nR(f) = ∑ q ∑ k∈N (q) ψδ(fq − fk), (II.7)\nwhere for each image location q, a scalar function ψδ(x) is the convex edge-preserving Huber penalty\nψδ(x) =  x2 2 , |x| < δ\nδ|x| − δ 2\n2 , |x| ≥ δ  , In order to minimize (II.6), we have used the L-BFGS optimization method [15]. The Matlab/C implementation of the algorithm is the courtesy of Mark Schmidt."
    }, {
      "heading" : "III. ARTIFICIAL NEURAL NETWORKS (ANN)",
      "text" : "For completeness of this paper, we provide here a brief background on ANN, and in particular their role in CT and medical imaging. ANN, mimicking after the biological networks of neurons which comprise the nervous system, are intensively used in many domains of Computer Science. In this work we focus on the multi-layer feed-forward ANN with no cycles. This is best represented by a directed, weighted graph which has an array of input nodes (data inputs), inner nodes (neurons) implementing specific (linear or non-linear) scalar functions, and another array of output nodes. The input argument of each neuron is the weighted sum of all its inputs, where the weights are associated with the edges. Those weights are learned during the network training and, effectively, define the regression function produced by the ANN.\nMore specifically, the first layer consists of m inputs, coming from the outside world; then Nl neurons are situated in the l-th layer (l > 1), and the last one contains n output nodes. Each input xi is connected to each neuron j in the second (hidden) layer by a weighted edge with weight w1i,j . The output of each neuron is connected to the input of every k-th neuron in the second layer by the weight w2j,k, and so on. Finally, each neuron of the last layer is connected to the output ys with a weight vs,j . We denote by σ the function implemented in each neuron. There is a number of popular choices for this function, for instance σ(x) = tanh(x).\nFor example, here is the explicit definition of a network with one hidden layer:\ny(x;w, v, b) = ∑ j vjσ (∑ i wi,jxj + bj ) . (III.1)\nThe weights {w, v, b} define the multi-variable regression function y = y(x) which approximates any continuous function implied by the set of training examples1. A training set for the network comprises of a collection of examples (Xk, Y k), where Xk is the vector of inputs and Y k is the true output related to this vector. Training the network consists of optimizing the weights {w, v, b} for a minimal error,\n(w, v, b) = arg min w,v,b ∑ k=1...K E ( y(Xk;w, v, b), Y k ) , (III.2)\nwhere the sum is over the training set, and E(a,b) is an error measure of some sort (e.g. E(a, b) = (a − b)2). The popular method for solution of this problem is the iterative backpropagation method [17]. A scheme of such network is depicted in Figure 2.\nSince the development of the back-propagation algorithm for ANN in mid-eighties, the image processing community (among others) has attained a powerful tool to attack virtually any regression or discrimination task. Among the wealth of applications neural networks found in this area (see [18] for\n1The Universal Approximation Theorem states that a network with just one hidden layer, where each neuron is realized as a monotonically-increasing continuous function, can uniformly approximate any given multivariate continuous function up to an arbitrary small error bound [16]. In practice, adding hidden layers shows an improvement in the ANN performance.\n4\na broad and comprehensive overview), some were designed for medical imaging. As such, Hopfield ANN were used for computer-aided screening for cervical cancer [19], breast tumors [20] and segmentation [21]. ANN are also used for compression and classification in cardiac studies [22] and ECG beat recognition [23]. Tasks of filtering, segmentation and edge detection in medical images are addressed with cellular ANN in [24]. Our group has used neural networks for optimal photon detection in scintillation crystals in PET [25].\nAs for reconstruction problems, a series of works has appeared in which the ANN replaces the overall reconstruction chain by learning the net contribution of all detector readings to each pixel in the image. For Electron Magnetic Resonance (EMR), such an algorithm is proposed in [26]. Floyd et. al. have used this approach for SPECT reconstruction [27] with feed-forward networks and also for lesion detection in this modality [28]. We remark that such naive application of the ANN for reconstruction is limited to low-resolution n×n images, since the network must have O(n2) inputs and outputs. For instance, in [26], a 64×64 image is reconstructed. Application of ANN for SPECT reconstruction was also studied by J. P. Kerr and E. B. Bartlett [29], [30].\nImaging modalities like PET and SPECT, where lowresolution images are produced, are a natural domain for ANN application. However, some works tackle also the problem of CT reconstruction where the image size is larger. Ref. [31] proposes using a neural network structure with training based on a minimization of a maximum entropy energy function. Reconstruction in Electrical Impedance Tomography was treated with ANN in [32]. Another variety, an Electrical Capacitance Tomography and an ANN-based reconstruction method for it, are described in [33].\nDespite the abundance of applications, there is still place for innovation in the domain of ANN application for medical imaging. First, the CT reconstruction problem is rarely attacked with this tool due to the high dimensions of raw data and the resulting images, which render the naive application of ANN as the black box converting measurements to image unfeasible. Indeed, in our work we do not propose such a\nscheme per se – rather, our ANN is employed to perform a locally-adaptive fusion of a number of image versions, produced by a given reconstruction algorithm upon using different configurations. This brings us naturally to the next section where we describe our algorithm."
    }, {
      "heading" : "IV. THE PROPOSED SCHEME",
      "text" : ""
    }, {
      "heading" : "A. Local Fusion with a Regression Function",
      "text" : "We consider the general setup of the non-linear inverse problem. Assume we are given the measurements vector y of the form\ny = Hx+ ξ, (IV.1)\nwhere H is some transformation, ξ represents the noise, and x is the signal to be recovered. Assume further that Tp is some restoration algorithm designed to recover x from this type of measurements, i.e.,\nx̄p = Tp(y) (IV.2)\nThe scalar parameter p controls the behavior of T and therefore influences certain characteristics of the estimate x̄. For example, when p is responsible for variance-resolution tradeoff of the algorithm, the estimate x̄p may be obtained with different noise levels and corresponding spatial resolution characteristics.\nThe described situation is common in many signal/image processing scenarios. As a basic example, we consider a simple image denoising algorithm, which recovers the signal x from noisy measurements y = x + ξ by a shift-invariant low-pass filter, realized as a 2-D convolution with prescribed kernel. For some fixed shape of this kernel (say, a simple boxcar function or a 2-D Gaussian rotation-invariant kernel), its width (spread) can be parameterized by a scalar variable p. A wider such kernel will perform a more aggressive noise reduction, by averaging the noisy signal over a larger area, at the cost of reducing the spatial resolution.\nA second, and more relevant example to this work, is from the domain of CT reconstruction. Recovery of the attenuation map is classically performed by the Filtered Back-Projection algorithm. The latter involves a 1-D low-pass filter, applied to the individual projections. As in the above example, the cut-off frequency of this filter controls the variance-resolution properties of the reconstructed image. In these examples, and also in a general such situation, no single value for the parameter p makes the best of the processing algorithm. For different signals, different values may be optimal in the sense of MSE or other quality measure. Indeed, in the same image, computed with two different values of p, different regions will get the best treatment by different values of p. For each specific case, ad-hoc considerations for tuning this scalar parameter are applied.\nIn the domain of non-parametric statistics, there is a noise reduction algorithm with proven near-optimality that devises a switch rule for selecting at each location of the signal an appropriate local filter [34]. In effect, the signal is processed by a low-pass filter adaptive to the local signal smoothness. In the context of our discussion, one can say that this algorithm performs a fusion of a number of filtered versions of a signal\n5 with varying filter parameter. The switch rule, developed for this adaptive signal smoothing, is based on the balance of the stochastic and structural noise components and model assumptions, and as such, it is very difficult to devise. Moreover, better output may be obtained if we allow to use some combination of the given image versions in each pixel, rather than selecting one of them alone. To our knowledge, no mathematical theory offering a descriptive rule for such local fusion is available for signal estimators, used for denoising or CT reconstruction.\nBorrowing from the above switch-rule idea between filters, the solution we propose for the problem described above is a local fusion of a sequence of estimates x̄p1 , ..., x̄pN with a specific regression function, learned on a training dataset consisting of similar cases. Among known regression methods, we choose to work with ANN, due to their strong adaptivity and generalization properties [5]. The supervised learning is done with a training set of examples: For each location in the processed signals, the features (input vector) are sample values extracted from the corresponding location in the sequence of reconstructed versions for this signal. The output is a small region of sample in the desired destination signal. Contemporary training algorithms employ error back-propagation to optimize the objective function, measuring the discrepancy between the correct output values and those predicted by the ANN [17]. In our work we employ the Matlab Neural Network toolbox; the training was performed with the LevenbergMarquardt algorithm [35], [36]. Our networks consist of two hidden layers. We use the function σ(x) = x/(1 + |x|), which has similar properties to the classical sigmoid and is computationally cheaper and is more robust to saturation caused by large arguments.\nIn this work, the outlined general concept is specialized to reconstruction algorithms for CT. Specifically, we consider representatives of the two types of those algorithms: the direct FBP and the iterative PWLS (Section II-B) methods. For FBP, we propose making a sweep over the cut-off frequency of its low-pass filter in the sinogram domain. This parameter controls the noise-resolution tradeoff and has a major influence on the visual impression of the resulting images. For the iterative PWLS algorithm, a sequence of images is extracted along its execution by saving a version of the CT result every few iterations. In following sections we illustrate this approach on a simple 1-D denoising problem and work out a number of applications for CT reconstruction algorithms, as detailed above. Along the way, we discuss the choice of training set and design of features extracted for the ANN."
    }, {
      "heading" : "B. An Example: ANN Fusion for 1-D Signal Denoising",
      "text" : "To illustrate the proposed concept, we begin with the simple signal denoising algorithm as mentioned above. We assume that the original signal is 1-D piece-wise constant (PWC). This choice is beneficial for the test we are about to present, since random PWC signals can easily be generated for training/testing purposes, and the effect of low-pass filter denoising is easily observed. We generate such a signal x of length n by choosing n/30 step locations uniformly in random, and choosing the intensity value for each step uniformly at random as well, in the [0,1] segment.\nAssume that such a signal x has been created and is contaminated with i.i.d. Gaussian noise ξ ∼ N (0, σn) with σn = 0.06. For the noise reduction, we perform a convolution of y = x + ξ with a Gaussian kernel G(p) = N (0, p). For some chosen values of the standard deviation p = p1, ..., p8 we obtain the sequence of estimates\nx̂i = y ∗G(pi), i = 1, ..., 8. (IV.3)\nIn Figure 3 we display an instance of such a signal, the corresponding noisy version, and a number of signal estimates obtained with convolution filters of different widths.\nIn this setup we train the ANN for a better signal restoration. For each location q in y, we extract a set of small neighborhoods of the same location q from each of the signals x̂1, ..., x̂K . Those are concatenated into one vector which serves as the ANN input. Specifically, we take a 11-samples window from each processed signal in the sequence of K = 8 signals. Thus, overall the feature vector for each location is of size 8 · 11 = 88 samples. In the training stage, every such vector is matched with a label – the correct value x(q), which is provided to the ANN as the desired output.\nFor the training procedure we generate a signal x of size n = 2 · 104 (=number of training samples) and extract the training data as described above. The obtained ANN is tested on another signal of length 300, randomly generated with the same engine. In Figure 4 such test results are presented. The neural network has improved the SNR of the best linear estimate from 19.85dB to 26.18dB, and this difference is observed in the fact that the ANN estimate fits the original signal much closer. The SNR values are calculated over an interval of 200 samples in the center of the test signal, so as to avoid boundary problems.\nThe presented algorithm has various design variables: the number, shape and width of the applied filters, the size and structure of the neural network, the structure of a input vector for each example (set of features). The questions of algorithm\n6 125 130 135 140 145 150 155 160 165 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Original Noisy Best Linear ANN Fusion\nFigure 4: Test results: a zoom-in on a portion of the clean signal x, the noisy one y (17.67dB), the SNR-best linearly restored signal x̂i (19.85 dB), and the restoration by the ANN (26.18dB).\ndesign will be pursued in the following sections, where CT reconstruction algorithms, relevant to our study, are invoked in the similar setup of performance boosting by local ANN fusion."
    }, {
      "heading" : "C. Error Measures",
      "text" : "Just before we conclude this section and move to present the specific details of boosting CT reconstruction algorithms, we should discuss the choice of the error function to use in the learning process, and the error measure to use when evaluating the quality of the reconstruction.\nC.2 Training Risk In the supervised learning procedure, we design the ANN weights so as to minimize the regression error between the ANN output and the desired labels (training output data). In many cases, the natural choice for this function would be the Mean-Squared-Error (MSE). However, in CT, we should contemplate whether MSE is the proper choice to use. Consider a homogeneous region in a CT image (corresponding to some tissue) with a small detail of a different yet similar intensity (a cavity or a lesion). The MSE penalty paid by an over-smoothing reconstruction filter that blurs this lesion is small, and therefore such faint details may be lost while leading to better MSE. The remedy for this problem could be to penalize not only for the difference in intensity values between the reference image f0 and the reconstruction f̃ , but also for the difference in the derivatives of these two images. Alternatively, We can weight the training examples so as to boost the importance of such faint edge regions, at the expense of more pronounced parts of the image, where the edges are sufficiently strong. In this spirit, building on the general error term written in Equation (III.2), we propose to use\nθ∗ = arg min θ ∑ k=1...K E ( y(Xk, θ), Y k ) (IV.4)\n= arg min θ ∑ k=1...K ρk · ( y(Xk, θ)− Y k )2 .\nIn this expression (Xk, Y k)k is the training data consisting of pairs of feature (input) vectors and their desired label (output), and the function y(Xk, θ) is the output of the ANN, governed by its control parameters θ. This is a simple weighted MSE, and the idea mentioned above is encompassed in the choice of ρk, the scalar weights assigned to the training examples. In our work we have chosen ρk to be zero for examples having a very low variance in the input image, which correspond to air regions. Specifically, the threshold is set to 10−6 times the maximal variance of Xk. A zero weight is also assigned to all the examples where the accumulated gradient over the input patch (in the idea image) is above 2% of its maximal value. The later pruning is introduced in order to avoid the bias of the very strong bone-flesh, flesh-air edges in the training process. As for the remaining examples, we assign their weight to be proportional to the accumulated gradient of the patch (again, in the ideal image). This way, the remaining informative edges get a more pronounced effect in the learning procedure.\nC.2 Quality Assessment The quality measures of CT images used in this study, are the following: • Signal-to-Noise Ratio (SNR), defined for the ideal sig-\nnal f and a deteriorated version f̂ by SNR(f, f̂) = −20log10(‖f − f̂‖2/‖f‖2). In practice, we consider the signal f̂ up to a multiplicative constant and compute\nSNR(f, f̂) = max α −20log10(‖f−αf̂‖2/‖f‖2). (IV.5)\nTo make the error measurement more meaningful, the SNR is only computed in the image region where the screened object resides, ignoring the background area. We have used an active contour technique to find the object region in the image; specifically we have used the Chan-Vese method [37]. • Windowed Signal-to Noise Ratio. The dynamic range of the HU values in a CT image is very large, from −1000 for air to 1500 − 2000 for bones. Often, the diagnostic interest lies in the soft tissues, the HU values of which are near zero (HU of water). For axial sections of thighs, we chose (by a criterion of best visibility ) the window of [b1 = −220, b2 = 350] HU; our algorithms are tuned for best reconstruction in this HU range. Therefore, an appropriate SNR measurement considers only the regions in the image that fall in this range. Technically, the reference image f and the noisy image f̂ are pre-processed before the standard SNR is computed by projecting values lower or higher than b1 and b2 respectively to these values. • Structured Similarity (SSIM) measure [38]. This measure of similarity between two images comes to replace the standard Mean Squared Error (the expression ‖f−f̂‖2 appearing in the SNR formula), which is known for its problamatic correlation with the human visual perception system (see [38] and the references 1-9 therein). SSIM compares small corresponding patches in the two images, after a normalization of the intensity and contrast. The explicit formula involves first and second moments of the local image statistics and the correlation between the two compared images. In our numerical experiments, we\n7 use the Matlab code provided by the authors of [38], which is available at https://ece.uwaterloo.ca/∼z70wang /research/ssim.\n• Spatial resolution measure: the spatial resolution properties of a non-shift-invariant reconstruction method should be characterized using a local impulse response (LIR) function, which replaces the standard point-spread function [39]. We compute the LIRs by placing sharp impulses (single pixel) in many random locations in the reference image and by taking the difference between the reconstructed images, scanned with or without the spikes. For each LIR, the Full-Width Half-Maximum (FWHM) value is computed as follows: first, the 2-D image matrix of the response function is resized into an image larger by ×16 in each axis, in order to reduce the discretization effect. Then, the number of pixels with intensity higher than half-maximum is counted and divided by the refinement factor of 256. This is the FWHM resolution measure at the specific location."
    }, {
      "heading" : "V. FBP BOOST – ALGORITHM DESIGN",
      "text" : ""
    }, {
      "heading" : "A. The Low-Pass FBP Filter Parameters",
      "text" : "The method of local fusion, advocated in the previous section, is now applied to the standard Filtered Back-Projection (FBP) algorithm for CT reconstruction. The fusion is performed over the parameters of the low-pass sinogram filter, applied before the Back-Projection. This one-dimensional lowpass filter is realized as a multiplication with the Butterworth window H in the Fourier domain, defined by\n|Ĥ(ω)| = ( 1 + ( ω\nφ0\n)2p)−1/2 . (V.1)\nWe sweep through the range of the parameter φ0 (expressing the cut-off frequency of the filter), thus changing the resolution-variance tradeoff of the FBP. We also change the parameter p, which controls the steepness of the window rolloff. While φ0 controls the amount of blur introduced during the reconstruction, the parameter p influences the texture of reconstructed image.\nIn Figure 5 we show the reconstruction for a fixed value of p = 3 and an increasing cut-off frequency φ0. Visually, the strong low-pass filter produces a cleaner image (which also have a higher SNR), but looses in the spatial resolution. The displayed sequence corresponds to values φ0 = [0.4, 0.8, 1.15, 2.0, 120,∞] (the last corresponds to no filter).\nAfter testing various combinations, we chose to use only three FBP images with cut-off frequencies φ0 = [0.4, 1.15,∞] and p = 3. Those were selected from eight images – three with the frequencies φ0 = [0.4, 0.8, 1.15] and p = 1, another three images with the same frequencies and p = 3, and the last two are obtained with φ0 = [2.0, 120] and p = 3. The reason for the restriction to three images is the smaller ANN required."
    }, {
      "heading" : "B. Design of The ANN Fusion and Training Setup",
      "text" : "Let f̃1, ..., f̃K be a given set of versions of a CT image, reconstructed by FBP with different low-pass filters in the\nFigure 5: FBP reconstruction with different cut-off frequency value. Upper to lower, Left to right: φ0 = [0.4, 0.8, 1.15, 2.0, 120,∞] (the last image is compute without the low-pass filter).\nsinogram domain.2 We describe the fusion procedure used to compute the output image f̂ of the algorithm: • For each location q in the image matrix, extract its disk-\nshaped neighborhood from each of the K images f̃i, i = 1, ...,K. The radius of the disk is set to 3 pixels (containing 29 pixels). • Compose a set of inputs for the ANN by stacking the pixel intensities from the K neighborhoods into one vector. Normalize this vector in the training stage (discussed below). • Apply the ANN to produce a set of output values, which are the intensity values in the disk-shaped neighborhood of q in the image f̂ . This disk has the same radius of 3 pixels. • By this design, each pixel in the output image is covered by 29 disk-shaped patches; its final value is computed by averaging all those contributions.\nWe detail now on the several of the steps in the list above. In the training stage, the neural network is tuned to minimize the discrepancy between true values in each output vector and those produced by the network from the set of noisy inputs. A vector of inputs is built, as described above, for a location q in a reference image f from a training set, using data from noisy reconstructions. The corresponding vector of outputs is the disk-shaped neighborhood of q in the reference image. Thus, for each image f we produce the set f̃1, ..., f̃K using pre-defined FBP filters and sample them to build the training dataset. The image is sampled on a cartesian grid, choosing every third pixel q both in horizontal and vertical directions. The pair of input and output vectors for the neural networks is an example used in the training process. Examples from all the training images f are put in one pool. A portion of this pool, having a very low variance in the inputs vector, is discarded (specifically, the threshold is set to 10−6 times the maximal variance). Those examples correspond to regions of air, since no constant patch in any kind of tissue can be observed in the noisy FBP images. This step leads to an empirical improvement in the performance of the ANN.\n2Note that all these images are produced from the very same raw sinogram, which means that the patient is exposed to radiation only once.\n8 It is generally acknowledged, that data normalization improves performance of neural networks [40]. Our data matrix A, which columns are the individual example vectors, is normalized by\nA⇐ A−min i (A(i)) and then A⇐ A/max i (A(i)). (V.2)\nThe two constants α1 = mini(A(i)) (the minimum value of the matrix A) and α2 = 1/maxi(A(i)) are stored along with the weights of the neural network, and the new data matrix in the test stage is transformed with those pre-computed constants.\nGiven intensity values in the neighborhood of a pixel q in several noisy images, the network should predict a single value in this pixel for the fusion image. However, as a step of regularization, we design the ANN to produce a vector output which is interpreted as a small neighborhood of q. the fusion image is then built from such disk-shaped overlapping patches, which are averaged to produce the final result. This is done to avoid possible artifacts, which can be produced by the network: in the training stage, if the ANN produces a single outlier intensity value, its penalty will be smaller than of a vector of such incorrect intensities. Such regularization reduces the performance the ANN can achieve on the training set, since more equations are imposed, but its performance on test images is expected to be more stable."
    }, {
      "heading" : "VI. FBP BOOST – EMPIRICAL STUDY",
      "text" : ""
    }, {
      "heading" : "A. Evaluating the Algorithm Performance",
      "text" : "In the experiments we have used sets of clinical CT images, axial body slices extracted from a 3D CT scan of a male head, abdomen and thighs. The images are courtesy of Visible Human Project. The intensity levels of those grayscale images correspond to Hounsfield Units. The training set comprises of 461 × 461 male thighs sections. The image set for ANN training consists of 12 images, from which 30, 000 examples are extracted. This number, in our experience, suffices to avoid an over-fitting for the chosen size of neural network (40 neurons in the hidden layer, 90 network inputs, overall 3720 weights). The vector of features for each example is built from the pixel neighborhoods of radius 3 pixels, coming from the three corresponding FBP reconstructions. These images are a subset of the 8 FBP reconstruction images mentioned before, seeking (manually) for the subgroup that would perform the best. The size of the input vector is 3× 29 = 87 entries.\nIn Figure 6 we present a reconstruction of a test image. This test image is taken 10cm away from the region where the training data was taken from. The middle upper image is the result of a fusion of the number of FBP versions, performed with the trained ANN. By the visual impression, the noiseresolution balance in the fused image f̂ is better than in any of the FBP versions forming it. The texture of tissues is closer to the original (observed in the reference image, upper left). The level of streaks and general noise are lower than in the central and right FBP images, and the image sharpness is higher than in the left and the central images. Thus, the fusion image enjoys the good properties offered by each of the FBP versions and is superior than any of them.\nFigure 6: Upper left: reference image. Upper middle: the ANN fusion result. Other: FBP images participating in the fusion, produced with different low-pass filters.\nRecall that the training was done with a set of weights, corresponding to our penalty component from Equation IV.4. The quantitative error measures we compute for this comparison include plain SNR, SNR weighted by those weights, the training risk and the SSIM measures. These values are given in Table I. As observed from the table, the weighted SNR of the fusion image is 1.8dB higher than the highest attainable value in FBP images. For plain SNR this increment is 1.5dB. Values of the training risk measure behave expectedly: the weights of ANN training were designed to implicitly reduce this measure for the fusion image. Indeed, it is by 20% lower than that of the optimal FBP image. Finally, the SSIM measure supports the claim the fusion image has the best visual appearance, since it admits the larger value for this measure."
    }, {
      "heading" : "B. Size of Local Neighborhood",
      "text" : "We study the algorithm performance with different amounts of local data provided for the ANN fusion. A sequence of test image reconstructions is produced, where the radius r of the pixel neighborhood, extracted for the fusion, is increased from r = 0 (single pixel) to r = 4 (49 pixels). The input vector for the ANN is built from three such neighborhoods, coming from FBP reconstructions corresponding to cut-off frequencies φ0 = [0.4, 1.15,∞] of the low-pass filter. We remark that in the special case of r = 0, the regression function learned by the network incorporates only the relations between the pixel values in the different image versions, while with larger neighborhood sizes there is also a possibility to perform some local filtering in each image.\n9 0 0.5 1 1.5 2 2.5 3 3.5 4 19 20 21 22 23 24 25 26\nNeighborhood radius r\nFusion result FBP, phi0=infty FBP, phi0=0.4 FBP, phi0=1.15\nFigure 7: Graphs of the SNR values corresponding to reconstructions with ANN fusion using input pixel neighborhoods of radius r = 0, 1, 2, 3, 4 (x-axis).\nIn Figure 7 we display graphs of SNR values3 computed for the test image. Observably, the quality increment with the neighborhood radius is exhausted around r = 4. Our choice is to use r = 3, which requires a smaller number of variables (comparing to r = 4) without almost no loss in quality. We also notice in these graphs that the fusion using only the central pixel p has a performance very close to that of the best FBP version (but slightly higher, which testifies to the necessity to provide a larger neighborhood of each pixel for a successful fusion. We should note that large neighborhood allows the network to perform a kind of directionally anisotropic filtering matched to the direction of edges.\nWe also compare two cases of output vectors produced by the ANN. In the lower row of Figure 8, the image on the right is produced by the fusion process where a single pixel is recovered by the ANN for each input vector. The image on the left is produced by computing 5-pixel neighborhoods of each pixel and averaging the overlapping regions. The visual difference between the image is negligible, and the difference in SNR is 0.2 dB in favor of the averaging approach. Judging from this (and other similar) tests, we conclude that forcing the neural networks to evaluate a number of pixels in the neighborhood of the one being recovered does not reduce its performance. We don’t have an empirical evidence that such a step is truly necessary, since no artifacts in single-pixelestimation case were observed in this test."
    }, {
      "heading" : "C. Single-Image “Fusion”",
      "text" : "A special case of the proposed method is to perform local processing with the ANN using only one FBP image. This, in fact, is a post-processing algorithm based on a regression function, which implements some non-linear local filter. In the following experiment we compare the performance of two fusion methods, one using three FBP images (sharp, normal and blurred) and another using only one FBP image produced with no low-pass filter. The results are displayed in Figure 9. Visually, in the single-image fusion case some artificial streaks\n3Very similar effect was observed with SSIM.\nFigure 8: Upper left: reference image. Upper right: best-SNR FBP reconstruction. Lower left: fusion result where ANN output size is 5 pixel. Lower right: a fusion result where the ANN produces a single pixel value.\nFigure 9: Two test images (corresponding to the upper and lower rows) of thighs sections. Left column: reference images. Middle column: ANN fusion of a single FBP image with no sinogram filter (φ0 =∞). Quality of the upper middle image: SNR = 26.13 dB; lower middle: SNR = 27.02 dB. Right column: ANN fusion of three FBP versions, corresponding to filter cut-off frequency of φ0 = [0.4, 1.15,∞]. Quality of the upper right image: SNR = 27.53 dB; lower right: SNR = 27.51 dB.\nare observed, which do not appear in the multi-image fusion (where also a lower MSE is achieved). On the other hand, the single-image fusion produces sharper images."
    }, {
      "heading" : "VII. PWLS BOOST - ALGORITHM DESIGN AND EMPIRICAL STUDY",
      "text" : ""
    }, {
      "heading" : "A. Algorithm Description",
      "text" : "The iterative PWLS algorithm (see Section II-B) can be boosted by gathering intermediate versions of the image at\n10\ndifferent numbers of iterations. The idea is to capture the gradual transformation of the image from the initial to the final state. If the initial image is a blurred one, it gradually changes along the iterations towards a sharper version; the intermediate stages contain important information that can contribute to further improve the algorithm output.\nThe method is very similar to the one proposed in the previous section. At the training stage, a CT reconstruction is performed with a high-quality reference at hand. The examples for ANN training are produced in the following manner: the vector of inputs, corresponding to a location q in the image, is assembled using neighborhoods of q in the different versions of the image, gathered along the PWLS iterations. Specifically, we take a small neighborhood of pixels from each image in this sequence (see details below). The “correct answer”, corresponding to this vector of ANN inputs, is the value of the pixel q in the reference image. As was done previously, the objective function for ANN training is augmented with weights which determine the importance of the individual examples."
    }, {
      "heading" : "B. PWLS Boost - Empirical Study",
      "text" : "We conducted numerical experiments to demonstrate the proposed method using the same setup as in the FBP experiment. Training data for the ANN was obtained using a dataset of 12 axial male thighs section images. For each, an initial image f̃ is computed with the FBP algorithm using a sinogram filter with cut-off frequency value of 2.0 (see Figure 5). The PWLS algorithm is implemented as described in Section II-B, with parameters δ = 0.02, λ = 8·10−5. We have performed 90 iterations, saving an image version every 10 iterations - overall we have a sequence of 10 images. In practice, we use three images out of this sequence, namely those from iterations 20, 60 and 80. From the first and the third images, neighborhoods of radius 4 (49 pixels) were taken for the estimation of the pixel value, and the second image contributed a neighborhood of radius 1 (5 pixels). Overall, the ANN has 2 ∗ 49 + 5 = 103 inputs. It is set to be a network with 30 neurons in the (single) hidden layer. It has a single output, set to produce only the central pixel of the provided neighborhood. These specific settings were obtained with a manual tuning of the design parameters.\nIn Figure 10 we display the fusion result along with individual PWLS reconstructions, used in the fusion process. The lower part of the figure contains the absolute-valued error images. The fusion result has a higher visual quality than any of the three underlying images. Comparing to those images, the noise level in the fusion image is the lowest, and the tissue texture is closer to the original. The sharpness is the same as in the lower middle PWLS image. The SNR values (stated in the Figure) also point to the improvement in quality. The SSIM of the fusion image is 0.95, while the sequence of PWLS results have the SSIM values of 0.93, 0.92, 0.86 (corresponding to the lower row of Figure 10, left to right). A reconstruction of an additional test image is displayed in Figure 11. The effect of the fusion observed here is similar to the one in the previous reconstruction. We conclude that the ANN-based fusion can contribute also to the iterative reconstruction, without requiring\nany additional iterations; the computational cost of the fusion, exercised after the reconstruction, is lower by an order of magnitude than that of the iterative process.\nTo summarize the fusion effect on the outcome of standard reconstruction algorithms, we display in Figure 12 images produced by both FBP and PWLS methods, before and after\n11\napplying the proposed method of the ANN-based fusion; these images were previously given in Figures 6,10.\nIn order to test the robustness of the training results, we apply the ANN trained with the thigh sections, for a reconstruction of images of other body parts – sections of the head and the abdomen. Reconstruction results are presented in Figure 13 in the same order as in the previous comparison: middle image in the upper row is the result of fusion, which components are presented in the lower row. The head reconstruction is improved substantially by the fusion process, as visual observation shows. However, the SNR values (given in Table II) point to the favor of the PWLS image corresponding to 60 iterations (lower middle image). The highest SSIM value does belong to the fusion result, though. In the case of the abdomen section, the fusion image is similar to the 40-iterations version but contains less noise; its quantitative measures are somewhat better than those of the individual PWLS images.\nAs a last experiment, we consider the special case where the ANN only performs a local filtering of the single version of the image, without a reference to the other versions. A neighborhood of radius r = 4 (49 pixels) was extracted for each location in the PWLS image, corresponding to iteration number 60. The fusion result is visually compared in Figure 14 versus the image produced from 10 PWLS versions, as before. It can be observed that the processing by ANN reduces the\nnoise appearing in the PWLS image, but it is slightly inferior to the fusion image produced from several PWLS versions."
    }, {
      "heading" : "VIII. COMPUTATIONAL COMPLEXITY OF THE METHOD",
      "text" : "We analyze the number of computations required for the proposed method in the cases of FBP and PWLS reconstruction. First, we consider the complexity of applying the ANN to perform the pixel-wise fusion from a number of image versions produced at the reconstruction stage. For an n × n image, n2 activations of the ANN is required. Typically, the dimension of the input vector of the ANN is of the order of 100 samples, the output dimension has up to 5 elements, and a single hidden layer of up to 40 neurons is used. Thus, the\n12\nnetwork contains 40·100+5·40 = 4200 weights4. Each neuron also implements a sigmoid function thus requiring 40 sigmoid calculations to produce the ANN output values. Therefore, the cost of performing a local fusion by the ANN is 4200 · n3 operations.\nWhen the method is used for the FBP reconstruction, a number of FBP versions must be produced; in our experiments, three reconstructions suffice. Each FBP reconstruction is of computational complexity of O(n3). Therefore, if no hardware changes in an existing scanner are made, producing the fusion image will require roughly four times the extent of a single reconstruction (three FBP processes and the fusion step). Of course, the regular FBP image will be available for the radiologist after the usual time of a single FBP reconstruction.\nAs for the iterative PWLS algorithm, no changes in the reconstruction process are needed, since we only sample images along the standard iterations. We do not have an accurate estimate for the time complexity of the PWLS, since it depends on the optimization method and its efficient implementation. However, the iterative process necessarily involves an application of the system matrix (O(n3) operations) in each iteration, and therefore it is by order of magnitude slower than the FBP. Adding the fusion step in the end of this process will only marginally increase the total reconstruction time."
    }, {
      "heading" : "IX. SUMMARY",
      "text" : "We have introduced a method for quality improvement for a general parametric signal estimator. The concept is to use a regression function for a local fusion of a number of estimator’s outputs, corresponding to different parameter settings. The regression proposed is realized with feed-forward artificial neural networks. The fusion process consists of two components: first, the behavior of the signal in its different versions is gathered; second, the ANN performs its own nonlinear filtering of the signal versions in small neighborhoods of the estimated pixel.\nThe proposed method is very general and CT reconstruction is only one possible application for it. The local fusion can be used to solve any linear on non-linear inverse problem where an algorithm, producing a solution estimate, exists. The proposed method will enable to incorporate the algorithm outputs, produced with different values of a core parameter, to a single improved result, thus removing the need for tuning this parameter.\nIn this work this concept was illustrated for the case of CT reconstruction, done with two basic algorithms – the FBP and the PWLS. Empirical results suggest that the local fusion can improve on the resolution variance trade-off of the given reconstruction algorithm, thus adding to the visual quality of the CT images. The post-processing method is not very timeconsuming, and the cost of the local fusion can be well below the extent of one FBP reconstruction.\n4This number can be reduced if a parallel implementation of the ANN is available, since each neuron output can be calculated separately"
    } ],
    "references" : [ {
      "title" : "Penalized-likelihood sinogram restoration for computed tomography",
      "author" : [ "J. Bian P.J. La Riviere", "P.A. Vargas" ],
      "venue" : "IEEE Trans. Med. Imag., vol. 25, no. 8, pp. 1022–1036, Aug. 2006.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Computerized tomography - the new medical x-ray technology",
      "author" : [ "L.A. Shepp", "J.B. Kruskal" ],
      "venue" : "The American Mathematical Monthly, vol. 85, no. 6, pp. 420–439, 1978.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Statistical image reconstruction for polyenergetic x-ray computed tomography",
      "author" : [ "I.A. Elbakri", "J.A. Fessler" ],
      "venue" : "IEEE Trans. Med. Imag., vol. 21, no. 2, pp. 89–99, Feb. 2002.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Penalized weighted least-squares approach to sinogram noise reduction and image reconstruction for lowdose x-ray computed tomography",
      "author" : [ "H. Lu Z. Liang J. Wang", "T. Li" ],
      "venue" : "IEEE Trans. Med. Imaging, vol. 25, no. 10, pp. 1272–1283, 2006.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Neural Networks: A Comprehensive Foundation, Macmillan",
      "author" : [ "Simon Haykin" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1994
    }, {
      "title" : "A recursive filter for noise reduction in statistical iterative tomographic imaging",
      "author" : [ "K.D. Sauer J. Hsieh J.-B. Thibault", "C.A. Bouman" ],
      "venue" : "SPIE/IS&T Conference on Computational Imaging IV. 2006, vol. 6065, pp. 60650X– 60650X–10, SPIE.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Adaptive filtering for noise reduction in x-ray computed tomography",
      "author" : [ "Anja Borsdorf" ],
      "venue" : "Ph.D. Thesis, 2009.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Noise and contrast discrimination in ct",
      "author" : [ "K.M. Hanson" ],
      "venue" : "Radiology of the Skull and Brain, Vol. V: Technical Aspects of Computed Tomography, T. H. Newton and D. G. Potts, eds., pp. 3941–3955, 1981.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Nonlinear sinogram smoothing for low-dose x-ray ct",
      "author" : [ "J.Wang J.Wen H. Lu J. Hsieh T. Li", "X. Li", "Z. Liang" ],
      "venue" : "IEEE Trans. Nucl. Sci.,, vol. 51, no. 5, pp. 25052513, 2004.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Three-dimensional Reconstruction from Radiographs and Electron Micrographs: Application of Convolutions instead of Fourier Transforms",
      "author" : [ "G.N. Ramachandran", "A.V. Lakshminarayanan" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America, vol. 68, no. 9, pp. 2236–2240, 1971.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "A splitting-based iterative algorithm for accelerated statistical x-ray ct reconstruction",
      "author" : [ "Sathish Ramani", "Jeffrey A Fessler" ],
      "venue" : "Medical Imaging, IEEE Transactions on, vol. 31, no. 3, pp. 677–688, 2012.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Iterative methods for image reconstruction",
      "author" : [ "J. Fessler" ],
      "venue" : "ISBI Tutorial. 2006, http://www.eecs.umich.edu/ fessler/papers/files/talk/08/isbinotes.pdf.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Approximation by superpositions of a sigmoidal function",
      "author" : [ "G. Cybenko" ],
      "venue" : "Mathematics Of Control, Signals, And Systems, vol. 2, no. 4, pp. 303– 314, 1989.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Learning representations by back-propagating errors",
      "author" : [ "G.E. Hinton D.E. Rumelhart", "R.J. Williams" ],
      "venue" : "Nature, vol. 323, pp. 533 – 536, 1986.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Image processing with neural networksa review",
      "author" : [ "M. Egmont-Petersen", "D. de Ridder", "H. Handels" ],
      "venue" : "Pattern Recognition, vol. 35, no. 10, pp. 2279 – 2301, 2002.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Papnet-assisted primary screening of conventional cervical smears",
      "author" : [ "C. Nagar M. Cenci", "A. Vecchione" ],
      "venue" : "Anticancer Res, vol. 20, no. 5C, pp. 3887–3889, 2000.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "An automatic method to discriminate malignant masses from normal tissue in digital mammograms",
      "author" : [ "J.H.C.L. Hendriks G.M.T. Brake", "N. Karssemeijer" ],
      "venue" : "Phys. Med. Biol., vol. 45, no. 10, pp. 2843–2857, 2000.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "The application of competitive hopfield neural network to medical image segmentation",
      "author" : [ "J.S. Lin K.S. Cheng", "C.W. Mao" ],
      "venue" : "IEEE Trans. Med. Imag., vol. 15, no. 4, pp. 560–567, 1996.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Neural networks for ecg compression and classification",
      "author" : [ "V.J. MartInez J.R. Hilera", "M. Mazo" ],
      "venue" : "Proceedings of the 3rd International Conference on Fuzzy Logic, Neural Nets and Soft Computing. 1994, pp. 121–124, Iizuka, Japan.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Ecg beat recognition using fuzzy hybrid neural network",
      "author" : [ "S. Osowski", "T.H. Linh" ],
      "venue" : "IEEE Trans. On Biomedical Engineering, vol. 48, no. 11, pp. 1265 – 1271, 2001.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Cellular neural networks and computational intelligence in medical image processing",
      "author" : [ "I. Aizenberg", "N. Aizenberg", "J. Hiltner", "C. Moraga", "E. Meyer zu Bexten" ],
      "venue" : "Image and Vision Computing, vol. 19, no. 4, pp. 177 – 183, 2001.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Optimal nonlinear line-of-flight estimation  13 in positron emission tomography",
      "author" : [ "Alexander M Bronstein", "Michael M Bronstein", "Michael Zibulevsky", "Yehoshua Y Zeevi" ],
      "venue" : "Nuclear Science, IEEE Transactions on, vol. 50, no. 3, pp. 421–426, 2003.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A neural network approach for image reconstruction in electron magnetic resonance tomography",
      "author" : [ "R. Murugesan DC Durairaj", "MC Krishna" ],
      "venue" : "Comput. Biol. Med., vol. 37, no. 10, pp. 1492–501, 2007.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An artificial neural network for spect image reconstruction",
      "author" : [ "Jr. C.E. Floyd" ],
      "venue" : "IEEE Trans. Med. Imag., vol. 10, no. 3, pp. 485–487, 1991.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "An artificial neural network for lesion detection on single-photon emission computed tomographic images",
      "author" : [ "C.E. Floyd Jr.", "G.D. Tourassi" ],
      "venue" : "Investigative radiology, vol. 27, no. 9, pp. 667–672, 1992.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Medical image processing utilizing neural networks trained on a massively parallel computer",
      "author" : [ "J.P. Kerr", "E.B. Bartlett" ],
      "venue" : "Computers in Biology and Medicine, vol. 25, no. 4, pp. 393–403, Jul 1995.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Neural network reconstruction of spect images",
      "author" : [ "E.B. Bartlett J.P. Kerr" ],
      "venue" : "J. Digital Imaging, vol. 8, no. 3, pp. 116–126, 1995.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A 2d approach to tomographic image reconstruction using a hopfield-type neural network",
      "author" : [ "R. Cierniak" ],
      "venue" : "Artificial Intelligence in Medicine, vol. 43, pp. 113–125, 2008.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A neural network image reconstruction technique for electrical impedance tomography",
      "author" : [ "A. Adler", "R. Guardo" ],
      "venue" : "IEEE Trans. Med. Imag., vol. 13, no. 4, pp. 594–600, 1994.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Three-component tomographic flow imaging using artificial neural network reconstruction",
      "author" : [ "B.S. Hoyle A.Y. Nooralahiyan" ],
      "venue" : "Chemical Engineering Science, vol. 52, no. 13, pp. 2139–2148, 1997.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "On spatial adaptive estimation of nonparametric regression",
      "author" : [ "A. Goldenshluger", "A. Nemirovsky" ],
      "venue" : "Math. Meth. Statist., vol. 6, no. 2, pp. 135–170, 1997.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "An algorithm for least-squares estimation of nonlinear parameters",
      "author" : [ "D.W. Marquardt" ],
      "venue" : "Journal of the Society for Industrial and Applied Mathematics, vol. 11, no. 2, pp. 431–441, 1963.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "The levenberg-marquardt method for nonlinear least squares curve-fitting problems",
      "author" : [ "H. Gavin" ],
      "venue" : "Department of Civil and Environmental Engineering, Duke University, 2011.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Active contours without edges",
      "author" : [ "L.Vese T.Chan" ],
      "venue" : "IEEE Trans. Im. Proc., vol. 10, no. 2, pp. 266–277, 2001.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Image quality assessment: From error visibility to structural similarity",
      "author" : [ "Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli" ],
      "venue" : "IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, Apr 2004.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Spatial resolution properties of penalized-likelihood image reconstruction: space-invariant tomographs",
      "author" : [ "J.A. Fessler", "W.L. Rogers" ],
      "venue" : "Image Processing, IEEE Transactions on, vol. 5, no. 9, pp. 1346 –1358, 1996.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Yann LeCun", "Léon Bottou", "Genevieve B Orr", "Klaus-Robert Müller" ],
      "venue" : "Neural networks: Tricks of the trade, pp. 9–50. Springer, 1998.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1] for a detailed account.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "from the simple and still very popular Filtered-Back-Projection (FBP) [2], and all the way to the more advanced Bayesianinspired iterative algorithms (see e.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : ", [3], [4]) that take the statistical nature of the measurements and the unknown image into account.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 3,
      "context" : ", [3], [4]) that take the statistical nature of the measurements and the unknown image into account.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 4,
      "context" : "In order to extract this information from a collection of image versions, we use an Artificial Neural Network (ANN) [5].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Each measured photon count y` is typically interpreted as an instance of the random variable Y` following a Poisson distribution Y` ∼ Poisson(λ`) [1], [6], [7].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "Each measured photon count y` is typically interpreted as an instance of the random variable Y` following a Poisson distribution Y` ∼ Poisson(λ`) [1], [6], [7].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "Each measured photon count y` is typically interpreted as an instance of the random variable Y` following a Poisson distribution Y` ∼ Poisson(λ`) [1], [6], [7].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "This reflects the photon count statistics at the detectors [8].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "In the sinogram domain, the standard deviation of the error between the ideal sinogram and the one computed from the measurements, ĝ − ˆ̄ g, is λ−1/2 [9], and this is well approximated by ŷ−1/2 [10].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 1,
      "context" : "In this paper we shall refer and work with two such algorithms: (i) the Filtered Back-Projection (FBP) ( [2], which is a direct Radon inversion approach.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "[3]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "It uses the Ram-Lak kernel k [11], defined in the Fourier domain by k̂(ω) = |ω|, and Flow is a low-pass filter which prevents the noise amplification at high frequencies,",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "[13]",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "In [14]",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "The popular method for solution of this problem is the iterative backpropagation method [17].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "Among the wealth of applications neural networks found in this area (see [18] for",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "1The Universal Approximation Theorem states that a network with just one hidden layer, where each neuron is realized as a monotonically-increasing continuous function, can uniformly approximate any given multivariate continuous function up to an arbitrary small error bound [16].",
      "startOffset" : 274,
      "endOffset" : 278
    }, {
      "referenceID" : 15,
      "context" : "As such, Hopfield ANN were used for computer-aided screening for cervical cancer [19], breast tumors [20] and segmentation [21].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "As such, Hopfield ANN were used for computer-aided screening for cervical cancer [19], breast tumors [20] and segmentation [21].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "As such, Hopfield ANN were used for computer-aided screening for cervical cancer [19], breast tumors [20] and segmentation [21].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 18,
      "context" : "ANN are also used for compression and classification in cardiac studies [22] and ECG beat recognition [23].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 19,
      "context" : "ANN are also used for compression and classification in cardiac studies [22] and ECG beat recognition [23].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "Tasks of filtering, segmentation and edge detection in medical images are addressed with cellular ANN in [24].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 21,
      "context" : "Our group has used neural networks for optimal photon detection in scintillation crystals in PET [25].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 22,
      "context" : "For Electron Magnetic Resonance (EMR), such an algorithm is proposed in [26].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 23,
      "context" : "have used this approach for SPECT reconstruction [27] with feed-forward networks and also for lesion detection in this modality [28].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "have used this approach for SPECT reconstruction [27] with feed-forward networks and also for lesion detection in this modality [28].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "For instance, in [26], a 64×64 image is reconstructed.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 25,
      "context" : "Bartlett [29], [30].",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 26,
      "context" : "Bartlett [29], [30].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 27,
      "context" : "[31] proposes using a neural network structure with training based on a minimization of a maximum entropy energy function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "with ANN in [32].",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 29,
      "context" : "Another variety, an Electrical Capacitance Tomography and an ANN-based reconstruction method for it, are described in [33].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 30,
      "context" : "reduction algorithm with proven near-optimality that devises a switch rule for selecting at each location of the signal an appropriate local filter [34].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "Among known regression methods, we choose to work with ANN, due to their strong adaptivity and generalization properties [5].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "Contemporary training algorithms employ error back-propagation to optimize the objective function, measuring the discrepancy between the correct output values and those predicted by the ANN [17].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 31,
      "context" : "In our work we employ the Matlab Neural Network toolbox; the training was performed with the LevenbergMarquardt algorithm [35], [36].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 32,
      "context" : "In our work we employ the Matlab Neural Network toolbox; the training was performed with the LevenbergMarquardt algorithm [35], [36].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "We generate such a signal x of length n by choosing n/30 step locations uniformly in random, and choosing the intensity value for each step uniformly at random as well, in the [0,1] segment.",
      "startOffset" : 176,
      "endOffset" : 181
    }, {
      "referenceID" : 33,
      "context" : "We have used an active contour technique to find the object region in the image; specifically we have used the Chan-Vese method [37].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 34,
      "context" : "• Structured Similarity (SSIM) measure [38].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 34,
      "context" : "system (see [38] and the references 1-9 therein).",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 34,
      "context" : "use the Matlab code provided by the authors of [38], which is available at https://ece.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 35,
      "context" : "• Spatial resolution measure: the spatial resolution properties of a non-shift-invariant reconstruction method should be characterized using a local impulse response (LIR) function, which replaces the standard point-spread function [39].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 36,
      "context" : "proves performance of neural networks [40].",
      "startOffset" : 38,
      "endOffset" : 42
    } ],
    "year" : 2013,
    "abstractText" : "We propose a supervised machine learning approach for boosting existing signal and image recovery methods and demonstrate its efficacy on example of image reconstruction in computed tomography. Our technique is based on a local nonlinear fusion of several image estimates, all obtained by applying a chosen reconstruction algorithm with different values of its control parameters. Usually such output images have different bias/variance trade-off. The fusion of the images is performed by feed-forward neural network trained on a set of known examples. Numerical experiments show an improvement in reconstruction quality relatively to existing direct and iterative reconstruction methods.",
    "creator" : "LaTeX with hyperref package"
  }
}