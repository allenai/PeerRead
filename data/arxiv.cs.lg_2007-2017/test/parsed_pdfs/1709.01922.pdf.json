{
  "name" : "1709.01922.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A COMPARISON ON AUDIO SIGNAL PREPROCESSING METHODS FOR DEEP NEURAL NETWORKS ON MUSIC TAGGING",
    "authors" : [ "Keunwoo Choi", "György Fazekas", "Mark Sandler", "Kyunghyun Cho" ],
    "emails" : [ "keunwoo.choi@qmul.ac.uk", "kyunghyun.cho@nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1. INTRODUCTION\nDeep neural networks (DNNs) have been actively used in music information retrieval (MIR) research, achieving good performances in many problems including music emotion recognition [1], singing voice detection [2], and music tagging [3]\nThe MIR research using deep learning usually focuses on optimising the hyperparameters which specify the network structure. The audio preprocessing stage is often heuristically decided and not subject to optimisation.\nAlthough deep neural networks are known to be universal function approximators [4], training efficiency and performance may vary significantly with different training methods as well as generic techniques including preprocessing the input data [5]. In other words, a deep neural network can represent any function but it does not mean it can learn any function. Therefore, both empirical decisions and domain knowledge are crucial since choosing between various preprocessing methods can be seen as a non-differentiable choice function, therefore it cannot be optimised using gradient-based learning methods.\nOne of the most common preprocessing approaches is to apply logarithmic mapping on 2-dimensional time-frequency ∗FAST IMPACt EPSRC Grant EP/L019981/1 and the European Commission H2020 research and innovation grant AudioCommons (688382). Mark Sandler acknowledges the support of the Royal Society as a recipient of a Wolfson Research Merit Award. †Kyunghyun Cho thanks the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016)\nrepresentations, i.e., log(X + α) where α can be arbitrary constants such as or 1. For time-frequency representation, mel-spectrograms have been preferred over short-time Fourier transform in many tasks [] because it was considered to have enough information despite of its smaller size, i.e., efficient yet effective. However, their performances are usually not strictly compared.\nIn this paper, we focus on audio preprocessing strategies for deep convolutional neural networks for music tagging. By providing empirical results with various preprocessing strategies, we aim to demystify the effect of audio preprocessing methods on the performances. This will help researchers design deep learning systems for music research.\n2. EXPERIMENTS AND DISCUSSIONS\nA representative network structure needs to be defined to compare the effects of audio preprocessing. A ConvNet with 2D kernels and 2D convolution axes was chosen. This showed a good performance with efficient training in a prior benchmark [6], where the model we selected was denoted k2c2, indicating 2D kernels and convolution axes. As illustrated in Figure 1, homogeneous 2D (3×3) convolutional kernels are used in every convolutional layer. The input has a single channel, 96 mel bins, and 1,360 temporal frames (1, 96, 1360). The figures in Table 1 denote the number of channels (N), kernel height and kernel width for convolutional layers and subsampling height, subsampling width for max-pooling layers. Here, the height and width corresponds to the frequency- and time-axes respectively. Exponential linear unit (ELU) is used as an activation function in all convolutional layers [7].\nFor the training of music tagger, we used the MSD with preview audio clips. The training data are 30-60s stereo mp3 files with a sampling rate of 22,050Hz and 64kbps constant bit-rate encoding. For efficient training in our experiments, we downmix and downsample the signals to 12kHz after decoding and trim the audio duration to 29s to ensure equalsized input signals. The short-time Fourier transform and\nar X\niv :1\n70 9.\n01 92\n2v 1\n[ cs\n.S D\n] 6\nS ep\n2 01\nmelspectrogram are computed using a hop size of 256 samples (16ms) with a 512 point DFT aggregated to yield 96 mel bins per frame. The preprocessing is performed using Librosa [8] and Kapre [9]. Total 224,242 tracks are used and split into train/validation/test sets, 201,672/12,633/28,537 tracks respectively.1\nDuring training, the binary cross-entropy function is used as a loss function. For the acceleration of stochastic gradient descent, we use adaptive optimisation based on ADAM [10]. The experiment is implemented in Python with Keras [11] and Theano [12] as deep learning frameworks."
    }, {
      "heading" : "2.1. Variance by different initialisation",
      "text" : "In deep learning, using K-fold cross-validation is not a standard practice for two reasons. First, with large enough data and a good split of train, validation and test sets, the model can be trained with small variance. Second, the cost of hyperparameter search is very high and it makes repeating experiments too expensive in practice. For these reasons, we do not cross-validate the ConvNet in this study. Instead, we present the results of repeated experiments with fixed network and\n1The network implementation and split setting are online: https://github.com/keunwoochoi/transfer_learning_ music and https://github.com/keunwoochoi/MSD_split_for_ tagging\ntraining hyperparameters, such as training example sequences and batch size. This experiment therefore measures the variance of the model introduced by different weight initialisations of the convolutional layers. For this, a normal distribution is used following He et al. [13], which has been shown to yield a stable training procedure.\nThe results are summarised in Figure 2. This shows the AUC scores of 15 repeated experiments on the left, as well as error-bars of their 95% confident interval (CI, 0.00069), mean average error (MAE, 0.00103) and standard deviation (Std, 0.00136) on the right. Small MAE and standard deviation indicate that we can obtain a reliable, precise score by repeating the same experiments for a sufficient number of times. The two largest differences observed between the average AUC score and that of experiment 4 and 8 (AUC differences of 0.0028 and 0.0026 respectively) indicate that we may obtain up to∼ 0.005 AUC difference among experiment instances. Based on this, we can assume that an AUC difference of < 0.005 is non-significant in this paper."
    }, {
      "heading" : "2.2. Time-frequency representations",
      "text" : "STFT and melspectrogram have been the most popular input representations for deep learning for music. Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15–20]. However, an STFT is closer to the original signal and neural networks may be able to learn a representation that is more optimal to the task. This requires large amounts of training data however, as reported in [20] where using melspectrograms outperformed STFT with a smaller dataset.\nFigure 3 shows the AUC scores obtained using melspectrogram vs. STFT while varying the size of the utilised training data with logarithmic scaling. Although there are small differences on AUC scores up to 0.007, neither of them outperforms the other, especially when enough data is provided. This rebuts a previous result in [20] because melspectrograms did not have a clear advantage here even with a small training data size. This may be due to the difference in frequency resolution of the representations used and summarised as follows.\n• STFT in [20]: 6000/129=46.5 Hz (256-point FFT with 12 kHz sampling rate)\n• STFT in our work: 6000/257=23.3 Hz (512-point FFT with 12 kHz sampling rate)\n• Melspectrogram in [20] and our work: 35.9 Hz for frequency < 1 kHz (96 mel-bins and by [21] and [8])\nIn [20], the frequency resolution of the STFT was lower than that of the melspectrogram to enable comparing them with similar number of frequency bins. On the contrary, STFT of higher frequency resolution is used in our experiment and it is found to be as good as melspectrogram in terms of performance. The model using STFT does not take advantage of finer input however. This means overall that melspectrogram may be preferred since its smaller size leads to reduced computation in training and prediction. The figure also illustrates how much the data size affects the performance. Exponentially increasing data size merely results in a linear AUC improvement. AUC starts to converge at 64% and 100%, after which a network with bigger capacity can still make an improvement."
    }, {
      "heading" : "2.3. Log-scaling of magnitudes",
      "text" : "In this section, we discuss how logarithmic scaling of magnitudes, i.e. decibel scaling, affects performance. This is considered standard preprocessing in music information retrieval. The procedure is motivated by the human perception of loudness [14] which has logarithmic relationship with the\nphysical energy of sound. Although learning a logarithmic function is a trivial task for neural networks, it can be difficult to implicitly learn an optimal nonlinear mapping when it is embedded in a complicated task. A nonlinear mapping was also shown to affect the performance in visual image recognition using neural networks [22]. Figure 4 compares the histograms of the magnitudes of time-frequency bins after zeromean unit-variance standardisation. On the left, a logarithmically compressed melspectrogram shows an approximately Gaussian distribution without any extreme values. The bins of linear melspectrogram on the right however is extremely condensed in a very small range while they range in wider region (see the small zoomed-out histogram in Figure 4 where y-axis is exponentially spaced to illustrate the long-tail distribution). This means the network should be trained with higher numerical precision to use STFT, hence more vulnerable to noise.\nAs a result, decibel-scaled melspectrograms always outperform the linear versions as shown in Fig 5, where the same-coloured bars should be compared across within {1 vs. 2} and {1s vs. 2s}. Colours indicate normalization schemes while {1 vs. 1s} and {2 vs. 2s} compare scaling effect, both of which are explained in Section 2.42. Compared to the performance differences while controlling the training set size (the pink bar charts on the right of Figure 5) the additional work introduced by not using decibel scaling can be roughly estimated by comparing these scores to those networks when the training data size is limited. While this also depends on other configurations of the task, seemingly twice the data is required to compensate for the disadvantage of not using a decibel scaled representation."
    }, {
      "heading" : "2.4. Analysis of scaling effects and frequency-axis weights",
      "text" : "Lastly, we discuss the effects of magnitude manipulation. Preliminary experiments suggested that there might be two independent aspects to investigate; i) frequency-axis weights and ii) magnitude scaling of each item in the training set. Our experiment is designed to isolate these two effects. We tested two input representations log-melspectrogram vs. melspectrogram, with three frequency weighting schemes perfrequency, A-weighting and bypass, as well as two scaling methods ×10 (on) and ×1 (off), yielding 2×3×2=12 configurations in total. We summarise the mechanism of each block as follows.\n• per-frequency stdd: Compute means and standard deviations across time, i.e., per-frequency, and standardise each frequency band using these values. The average frequency response becomes flat (equalised). This method has been used in the literature for tagging [20], singing voice detection [2] and transcription [23].\n• A-weighted: Apply the international standard IEC 2Decibel-scaled STFT also outperformed linear STFT in our unreported experiments.\n61672:2003 A-weighting curve, which approximates human perception of loudness as a function of frequency.\n• Bypass: Do not apply any processing, i.e., f : X→ X\n• per-sample stdd: Excerpt-wise normalisation with its overall mean and standard deviation, i.e., using statistics across time and frequency of each spectrogram.\n• ×10 scaler: Multiply the input spectrogram by 10, i.e., f : X→ 10X."
    }, {
      "heading" : "2.4.1. Frequency weighting",
      "text" : "This process is related to the loudness, i.e., human perception of sound energy [14], which is a function of frequency. The sensitivity of the human auditory system drops substantially below a few hundred Hz3, hence music signals typically exhibit higher energy in the lower range to compensate for the attenuation. This is illustrated in Figure 6, where uncompensated average energy measurements corresponding to the Bypass curve (shown in green) yield a peak at low frequencies. This imbalance affects neural network activations in the first layer which may influence performance. To assess this effect, we tested three frequency weighting approaches. Their typical profiles are shown in Figure 6. In all three strategies, excerpt-wise standardisation is used to alleviate scaling effects (see Section 2.4.2).\n3See equal-loudness contours e.g. in ISO 226:2003.\nOur test results show that networks using the three strategies all achieve similar AUC scores. The performance differences within four groups, {1, 1s, 2, 2s} in Figure 5 are small and none of them are governing the others. The curves in Figure 6 show the average input magnitudes over frequency. These offsets change along frequency, but the change does not seem large enough to corrupt the local patterns due to the locality of ConvNets, and therefore the network is learning useful representations without significant performance differences within each group."
    }, {
      "heading" : "2.4.2. Analysis of scaling effects",
      "text" : "We may assume a performance increase if we scale the overall magnitudes for a number of reasons. During training using gradient descent, the gradient of error with respect to weights ∂E ∂W is proportional to ∂ ∂W f(W\n>X) where f is the activation function. This means that the learning rate of a layer is proportional to the magnitude of input X . In particular, the first layer usually receives the weakest error backpropagation, hence scaling of the input may affect the overall performance.\nWe tested the effect of this with the results shown in Fig. 5. To this end, consider comparing the same-coloured bars of {1 vs. 1s} and {2 vs. 2s}. Here, the scaling factor is set to 10 for illustration, however many possible values <100 were tested and showed similar results. In summary, this hypothesis is rebutted – scaling did not affect the ma performance. The analysis of trained weights revealed that different magnitudes of the input only affects the bias of the first convolutional layer. Training with scaling set to ×10 results in 3.4 times larger mean absolute value of the biases in the first layer. This is due to batch normalization [24] which compensates for the different magnitudes by normalizing the activations of convolutional layers.\n3. CONCLUSION\nIn this paper, we have shown that input preprocessing can affect the performance. We quantify this in terms of the size of the training data required to achieve similar performances. Among several preprocessing techniques tested in this study, only logarithmic scaling of the magnitude resulted in significant improvement. In other words, the network was resilent to most modifications of the input data except logarithmic compression of magnitudes in various time-frequency representations. Althpugh we focused on the music tagging task, our results provide general knowledge applicable in many similar machine-listening problems, e.g., music genre classification or the prediction of environmental sound descriptors.\n4. REFERENCES\n[1] M. Malik, S. Adavanne, K. Drossos, T. Virtanen, D. Ticha, and R. Jarina, “Stacked convolutional and re-\ncurrent neural networks for music emotion recognition,” arXiv preprint arXiv:1706.02292, 2017.\n[2] J. Schlüter, “Learning to pinpoint singing voice from weakly labeled examples,” in Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2016, pp. 44–50.\n[3] J. Lee, J. Park, K. L. Kim, and J. Nam, “Samplelevel deep convolutional neural networks for music auto-tagging using raw waveforms,” arXiv preprint arXiv:1703.01789, 2017.\n[4] K. Hornik, “Approximation capabilities of multilayer feedforward networks,” Neural networks, vol. 4, no. 2, pp. 251–257, 1991.\n[5] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller, “Efficient backprop,” in Neural networks: Tricks of the trade. Springer, 2012, pp. 9–48.\n[6] K. Choi, G. Fazekas, M. Sandler, and K. Cho, “Convolutional recurrent neural networks for music classification,” in 2017 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2016.\n[7] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, “Fast and accurate deep network learning by exponential linear units (elus),” arXiv preprint arXiv:1511.07289, 2015.\n[8] B. McFee, M. McVicar, C. Raffel, D. Liang, O. Nieto, E. Battenberg, J. Moore, D. Ellis, R. YAMAMOTO, R. Bittner, D. Repetto, P. Viktorin, J. F. Santos, and A. Holovaty, “librosa: 0.4.1,” Oct. 2015. [Online]. Available: https://doi.org/10.5281/zenodo.32193\n[9] K. Choi, D. Joo, and J. Kim, “Kapre: On-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras,” in Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning. ICML, 2017.\n[10] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” CoRR, vol. abs/1412.6980, 2014. [Online]. Available: http://arxiv.org/abs/1412. 6980\n[11] F. Chollet, “Keras: Deep learning library for theano and tensorflow,” https://github.com/fchollet/keras, 2015.\n[12] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. Warde-Farley, and Y. Bengio, “Theano: new features and speed improvements,” arXiv preprint arXiv:1211.5590, 2012.\n[13] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on\nimagenet classification,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026–1034.\n[14] B. C. Moore, An introduction to the psychology of hearing. Brill, 2012.\n[15] A. Van den Oord, S. Dieleman, and B. Schrauwen, “Deep content-based music recommendation,” in Advances in Neural Information Processing Systems, 2013, pp. 2643–2651.\n[16] S. Dieleman and B. Schrauwen, “End-to-end learning for music audio,” in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964–6968.\n[17] J. Nam, J. Herrera, and K. Lee, “A deep bag-offeatures model for music auto-tagging,” arXiv preprint arXiv:1508.04999, 2015.\n[18] J. Schluter and S. Bock, “Improved musical onset detection with convolutional neural networks,” in Acoustics, Speech and Signal Processing, IEEE International Conference on. IEEE, 2014.\n[19] K. Ullrich, J. Schlüter, and T. Grill, “Boundary detection in music structure analysis using convolutional neural networks,” in Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014), Taipei, Taiwan, 2014.\n[20] K. Choi, G. Fazekas, and M. Sandler, “Automatic tagging using deep convolutional neural networks,” in The 17th International Society of Music Information Retrieval Conference, New York, USA. International Society of Music Information Retrieval, 2016.\n[21] M. Slaney, “Auditory toolbox,” Interval Research Corporation, Tech. Rep, vol. 10, p. 1998, 1998.\n[22] S. F. Dodge and L. J. Karam, “Understanding how image quality affects deep neural networks,” CoRR, vol. abs/1604.04004, 2016. [Online]. Available: http://arxiv.org/abs/1604.04004\n[23] S. Sigtia, E. Benetos, and S. Dixon, “An end-to-end neural network for polyphonic music transcription,” arXiv preprint arXiv:1508.01774, 2015.\n[24] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” arXiv preprint arXiv:1502.03167, 2015."
    } ],
    "references" : [ {
      "title" : "Stacked convolutional and re-  current neural networks for music emotion recognition",
      "author" : [ "M. Malik", "S. Adavanne", "K. Drossos", "T. Virtanen", "D. Ticha", "R. Jarina" ],
      "venue" : "arXiv preprint arXiv:1706.02292, 2017.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Learning to pinpoint singing voice from weakly labeled examples",
      "author" : [ "J. Schlüter" ],
      "venue" : "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2016, pp. 44–50.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Samplelevel deep convolutional neural networks for music auto-tagging using raw waveforms",
      "author" : [ "J. Lee", "J. Park", "K.L. Kim", "J. Nam" ],
      "venue" : "arXiv preprint arXiv:1703.01789, 2017.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Approximation capabilities of multilayer feedforward networks",
      "author" : [ "K. Hornik" ],
      "venue" : "Neural networks, vol. 4, no. 2, pp. 251–257, 1991.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. Müller" ],
      "venue" : "Neural networks: Tricks of the trade. Springer, 2012, pp. 9–48.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Convolutional recurrent neural networks for music classification",
      "author" : [ "K. Choi", "G. Fazekas", "M. Sandler", "K. Cho" ],
      "venue" : "2017 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2016.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Fast and accurate deep network learning by exponential linear units (elus)",
      "author" : [ "D.-A. Clevert", "T. Unterthiner", "S. Hochreiter" ],
      "venue" : "arXiv preprint arXiv:1511.07289, 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "librosa: 0.4.1",
      "author" : [ "B. McFee", "M. McVicar", "C. Raffel", "D. Liang", "O. Nieto", "E. Battenberg", "J. Moore", "D. Ellis", "R. YAMAMOTO", "R. Bittner", "D. Repetto", "P. Viktorin", "J.F. Santos", "A. Holovaty" ],
      "venue" : "Oct. 2015. [Online]. Available: https://doi.org/10.5281/zenodo.32193",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Kapre: On-gpu audio preprocessing layers for a quick implementation of deep neural network models with keras",
      "author" : [ "K. Choi", "D. Joo", "J. Kim" ],
      "venue" : "Machine Learning for Music Discovery Workshop at 34th International Conference on Machine Learning. ICML, 2017.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "CoRR, vol. abs/1412.6980, 2014. [Online]. Available: http://arxiv.org/abs/1412. 6980",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Keras: Deep learning library for theano and tensorflow",
      "author" : [ "F. Chollet" ],
      "venue" : "https://github.com/fchollet/keras, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1211.5590, 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on  imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026–1034.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An introduction to the psychology of hearing",
      "author" : [ "B.C. Moore" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Deep content-based music recommendation",
      "author" : [ "A. Van den Oord", "S. Dieleman", "B. Schrauwen" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 2643–2651.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "End-to-end learning for music audio",
      "author" : [ "S. Dieleman", "B. Schrauwen" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964–6968.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A deep bag-offeatures model for music auto-tagging",
      "author" : [ "J. Nam", "J. Herrera", "K. Lee" ],
      "venue" : "arXiv preprint arXiv:1508.04999, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Improved musical onset detection with convolutional neural networks",
      "author" : [ "J. Schluter", "S. Bock" ],
      "venue" : "Acoustics, Speech and Signal Processing, IEEE International Conference on. IEEE, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Boundary detection in music structure analysis using convolutional neural networks",
      "author" : [ "K. Ullrich", "J. Schlüter", "T. Grill" ],
      "venue" : "Proceedings of the 15th International Society for Music Information Retrieval Conference (IS- MIR 2014), Taipei, Taiwan, 2014.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automatic tagging using deep convolutional neural networks",
      "author" : [ "K. Choi", "G. Fazekas", "M. Sandler" ],
      "venue" : "The 17th International Society of Music Information Retrieval Conference, New York, USA. International Society of Music Information Retrieval, 2016.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Auditory toolbox",
      "author" : [ "M. Slaney" ],
      "venue" : "Interval Research Corporation, Tech. Rep, vol. 10, p. 1998, 1998.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Understanding how image quality affects deep neural networks",
      "author" : [ "S.F. Dodge", "L.J. Karam" ],
      "venue" : "CoRR, vol. abs/1604.04004, 2016. [Online]. Available: http://arxiv.org/abs/1604.04004",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "An end-to-end neural network for polyphonic music transcription",
      "author" : [ "S. Sigtia", "E. Benetos", "S. Dixon" ],
      "venue" : "arXiv preprint arXiv:1508.01774, 2015.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167, 2015.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep neural networks (DNNs) have been actively used in music information retrieval (MIR) research, achieving good performances in many problems including music emotion recognition [1], singing voice detection [2], and music tagging [3] The MIR research using deep learning usually focuses on optimising the hyperparameters which specify the network structure.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : "Deep neural networks (DNNs) have been actively used in music information retrieval (MIR) research, achieving good performances in many problems including music emotion recognition [1], singing voice detection [2], and music tagging [3] The MIR research using deep learning usually focuses on optimising the hyperparameters which specify the network structure.",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 2,
      "context" : "Deep neural networks (DNNs) have been actively used in music information retrieval (MIR) research, achieving good performances in many problems including music emotion recognition [1], singing voice detection [2], and music tagging [3] The MIR research using deep learning usually focuses on optimising the hyperparameters which specify the network structure.",
      "startOffset" : 232,
      "endOffset" : 235
    }, {
      "referenceID" : 3,
      "context" : "Although deep neural networks are known to be universal function approximators [4], training efficiency and performance may vary significantly with different training methods as well as generic techniques including preprocessing the input data [5].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "Although deep neural networks are known to be universal function approximators [4], training efficiency and performance may vary significantly with different training methods as well as generic techniques including preprocessing the input data [5].",
      "startOffset" : 244,
      "endOffset" : 247
    }, {
      "referenceID" : 5,
      "context" : "This showed a good performance with efficient training in a prior benchmark [6], where the model we selected was denoted k2c2, indicating 2D kernels and convolution axes.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Exponential linear unit (ELU) is used as an activation function in all convolutional layers [7].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "The preprocessing is performed using Librosa [8] and Kapre [9].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "The preprocessing is performed using Librosa [8] and Kapre [9].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "For the acceleration of stochastic gradient descent, we use adaptive optimisation based on ADAM [10].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "The experiment is implemented in Python with Keras [11] and Theano [12] as deep learning frameworks.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "The experiment is implemented in Python with Keras [11] and Theano [12] as deep learning frameworks.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "[13], which has been shown to yield a stable training procedure.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15–20].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15–20].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15–20].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15–20].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15–20].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 18,
      "context" : "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15–20].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 19,
      "context" : "Melspectrograms provide an efficient and perceptually relevant representation compared to STFT [14] and have been shown to perform well in various tasks [15–20].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 19,
      "context" : "This requires large amounts of training data however, as reported in [20] where using melspectrograms outperformed STFT with a smaller dataset.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "This rebuts a previous result in [20] because melspectrograms did not have a clear advantage here even with a small training data size.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "• STFT in [20]: 6000/129=46.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "• Melspectrogram in [20] and our work: 35.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 20,
      "context" : "9 Hz for frequency < 1 kHz (96 mel-bins and by [21] and [8])",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "9 Hz for frequency < 1 kHz (96 mel-bins and by [21] and [8])",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "In [20], the frequency resolution of the STFT was lower than that of the melspectrogram to enable comparing them with similar number of frequency bins.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "The procedure is motivated by the human perception of loudness [14] which has logarithmic relationship with the physical energy of sound.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "A nonlinear mapping was also shown to affect the performance in visual image recognition using neural networks [22].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "This method has been used in the literature for tagging [20], singing voice detection [2] and transcription [23].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "This method has been used in the literature for tagging [20], singing voice detection [2] and transcription [23].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 22,
      "context" : "This method has been used in the literature for tagging [20], singing voice detection [2] and transcription [23].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : ", human perception of sound energy [14], which is a function of frequency.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 23,
      "context" : "This is due to batch normalization [24] which compensates for the different magnitudes by normalizing the activations of convolutional layers.",
      "startOffset" : 35,
      "endOffset" : 39
    } ],
    "year" : 2017,
    "abstractText" : "Deep neural networks (DNN) have been successfully applied for music classification tasks including music tagging. In this paper, we investigate the effect of audio preprocessing on music tagging with neural networks. We perform comprehensive experiments involving audio preprocessing using different time-frequency representations, logarithmic magnitude compression, frequency weighting and scaling. We show that many commonly used input preprocessing techniques are redundant except magnitude compression.",
    "creator" : "LaTeX with hyperref package"
  }
}