{
  "name" : "1704.04347.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exploiting Cross-Sentence Context for Neural Machine Translation",
    "authors" : [ "Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu" ],
    "emails" : [ "qun.liu}@adaptcentre.ie", "tuzhaopeng@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016). The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation. Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015).\nThe continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of\n∗Corresponding Author: Zhaopeng Tu\na word. Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016). Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012). Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence. These problems are difficult to alleviate using only limited intra-sentence context.\nThe cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016). The leverage of global context for NMT, however, has received relatively little attention from the research community.1 In this paper, we propose a cross-sentence context-aware NMT model, which considers the influence of previous source sentences in the same document.2\nSpecifically, we employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences, which deploys an additional documentlevel RNN on top of the sentence-level RNN encoder (Sordoni et al., 2015). After obtaining the global context, we design several strategies to integrate it into NMT to translate the current sentence:\n• Initialization, that uses the history represen1To the best of our knowledge, our work and Jean et al. (2017) are two independently early attempts to model crosssentence context for NMT.\n2In our preliminary experiments, considering target-side history inversely harms translation performance, since it suffers from serious error propagation problems.\nar X\niv :1\n70 4.\n04 34\n7v 3\n[ cs\n.C L\n] 2\n3 Ju\nl 2 01\n7\ntation as the initial state of the encoder, decoder, or both;\n• Auxiliary Context, that uses the history representation as static cross-sentence context, which works together with the dynamic intrasentence context produced by an attention model, to good effect.\n• Gating Auxiliary Context, that adds a gate to Auxiliary Context, which decides the amount of global context used in generating the next target word at each step of decoding.\nExperimental results show that the proposed initialization and auxiliary context (w/ or w/o gating) mechanisms significantly improve translation performance individually, and combining them achieves further improvement."
    }, {
      "heading" : "2 Approach",
      "text" : "Given a source sentence xm to be translated, we consider its K previous sentences in the same document as cross-sentence context C = {xm−K , ...,xm−1}. In this section, we first model C, which is then integrated into NMT."
    }, {
      "heading" : "2.1 Summarizing Global Context",
      "text" : "As shown in Figure 1, we summarize the representation of C in a hierarchical way:\nSentence RNN For a sentence xk in C, the sentence RNN reads the corresponding words {x1,k, ..., xn,k, . . . , xN,k} sequentially and updates its hidden state:\nhn,k = f(hn−1,k, xn,k) (1)\nwhere f(·) is an activation function, and hn,k is the hidden state at time n. The last state hN,k stores order-sensitive information about all the words in xk, which is used to represent the summary of the whole sentence, i.e. Sk ≡ hN,k. After processing\neach sentence in C, we can obtain all sentencelevel representations, which will be fed into document RNN.\nDocument RNN It takes as input the sequence of the above sentence-level representations {S1, ..., Sk, ..., SK} and computes the hidden state as:\nhk = f(hk−1, Sk) (2)\nwhere hk is the recurrent state at time k, which summarizes the previous sentences that have been processed to the position k. Similarly, we use the last hidden state to represent the summary of the global context, i.e. D ≡ hK ."
    }, {
      "heading" : "2.2 Integrating Global Context into NMT",
      "text" : "We propose three strategies to integrate the history representation D into NMT:\nInitialization We useD to initialize either NMT encoder, NMT decoder or both. For encoder, we useD as the initialization state rather than all-zero states as in the standard NMT (Bahdanau et al., 2015). For decoder, we rewrite the calculation of the initial hidden state s0 = tanh(WshN ) as s0 = tanh(WshN +WDD) where hN is the last hidden state in encoder and {Ws,WD} are the corresponding weight metrices.\nAuxiliary Context In standard NMT, as shown in Figure 2 (a), the decoder hidden state for time i is computed by\nsi = f(si−1, yi−1, ci) (3)\nwhere yi−1 is the most recently generated target word, and ci is the intra-sentence context summarized by NMT encoder for time i. As shown in Figure 2 (b), Auxiliary Context method adds the representation of cross-sentence context D to jointly update the decoding state si:\nsi = f(si−1, yi−1, ci, D) (4)\nIn this strategy, D serves as an auxiliary information source to better capture the meaning of the source sentence. Now the gated NMT decoder has four inputs rather than the original three ones. The concatenation [ci, D], which embeds both intraand cross-sentence contexts, can be fed to the decoder as a single representation. We only need to modify the size of the corresponding parameter matrix for least modification effort.\nGating Auxiliary Context The starting point for this strategy is an observation: the need for information from the global context differs from step to step during generation of the target words. For example, global context is more in demand when generating target words for ambiguous source words, while less by others. To this end, we extend auxiliary context strategy by introducing a context gate (Tu et al., 2017a) to dynamically control the amount of information flowing from the auxiliary global context at each decoding step, as shown in Figure 2 (c).\nIntuitively, at each decoding step i, the context gate looks at decoding environment (i.e., si, yi−1, and ci), and outputs a number between 0 and 1 for each element in D, where 1 denotes “completely transferring this” while 0 denotes “completely ignoring this”. The global context vector D is then processed with an element-wise multiplication before being fed to the decoder activation layer.\nFormally, the context gate consists of a sigmoid neural network layer and an element-wise multiplication operation. It assigns an element-wise weight to D, computed by\nzi = σ(Uzsi−1 +Wzyi−1 + Czci) (5)\nHere σ(·) is a logistic sigmoid function, and {Wz, Uz, Cz} are the weight matrices, which are trained to learn when to exploit global context to maximize the overall translation performance. Note that zi has the same dimensionality asD, and thus each element in the global context vector has its own weight. Accordingly, the decoder hidden state is updated by\nsi = f(si−1, yi−1, ci, zi ⊗D) (6)"
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Setup",
      "text" : "We carried out experiments on Chinese–English translation task. As the document information is necessary when selecting the previous sentences, we collect all LDC corpora that contain document boundary. The training corpus consists of 1M sentence pairs extracted from LDC corpora3 with 25.4M Chinese words and 32.4M English words. We chose the NIST05 (MT05) as our development set, and NIST06 (MT06) and NIST08 (MT08) as test sets. We used case-insensitive BLEU score (Papineni et al., 2002) as our evaluation metric, and sign-test (Collins et al., 2005) for calculating statistical significance.\nWe implemented our approach on top of an open source attention-based NMT model, Nematus4 (Sennrich and Haddow, 2016; Sennrich et al., 2017). We limited the source and target vocabularies to the most frequent 35K words in Chinese and English, covering approximately 97.1% and 99.4% of the data in the two languages respectively. We trained each model on sentences of length up to 80 words in the training data with early stopping. The word embedding dimension was 600, the hidden layer size was 1000, and the batch size was 80. All our models considered the previous three sentences (i.e., K = 3) as crosssentence context.\n3The LDC corpora indexes are: 2003E07, 2003E14, 2004T07, 2005E83, 2005T06, 2006E24, 2006E34, 2006E85, 2006E92, 2007E87, 2007E101, 2007T09, 2008E40, 2008E56, 2009E16, 2009E95.\n4Available at https://github.com/EdinburghNLP/nematus."
    }, {
      "heading" : "3.2 Results",
      "text" : "Table 1 shows the translation performance in terms of BLEU score. Clearly, the proposed approaches significantly outperforms baseline in all cases.\nBaseline (Rows 1-2) NEMATUS significantly outperforms Moses – a commonly used phrasebased SMT system (Koehn et al., 2007), by 2.3 BLEU points on average, indicating that it is a strong NMT baseline system. It is consistent with the results in (Tu et al., 2017b) (i.e., 26.93 vs. 29.41) on training corpora of similar scale.\nInitialization Strategy (Rows 3-5) Initenc and Initdec improve translation performance by around +1.0 and +1.3 BLEU points individually, proving the effectiveness of warm-start with crosssentence context. Combining them achieves a further improvement.\nAuxiliary Context Strategies (Rows 6-7) The gating auxiliary context strategy achieves a significant improvement of around +1.0 BLEU point over its non-gating counterpart. This shows that, by acting as a critic, the introduced context gate learns to distinguish the different needs of the global context for generating target words.\nCombining (Row 8) Finally, we combine the best variants from the initialization and auxiliary context strategies, and achieve the best performance, improving upon NEMATUS by +2.1 BLEU points. This indicates the two types of strategies are complementary to each other."
    }, {
      "heading" : "3.3 Analysis",
      "text" : "We first investigate to what extent the mistranslated errors are fixed by the proposed system.\nWe randomly select 15 documents (about 60 sentences) from the test sets. As shown in Table 2, we count how many related errors: i) are made by NMT (Total), and ii) fixed by our method (Fixed); as well as iii) newly generated (New). About Ambiguity, while we found that 38 words/phrases were translated into incorrect equivalents, 76% of them are corrected by our model. Similarly, we solved 75% of the Inconsistency errors including lexical, tense and definiteness (definite or indefinite articles) cases. However, we also observe that our system brings relative 21% new errors.\nCase Study Table 3 shows an example. The word “腐官” (corrupt officials) is mis-translated as “enemy” by the baseline system. With the help\nof the similar word “贪官” in the previous sentence, our approach successfully correct this mistake. This demonstrates that cross-sentence context indeed helps resolve certain ambiguities."
    }, {
      "heading" : "4 Related Work",
      "text" : "While our approach is built on top of hierarchical recurrent encoder-decoder (HRED) (Sordoni et al., 2015), there are several key differences which reflect how we have generalized from the original model. Sordoni et al. (2015) use HRED to summarize a single representation from both the current and previous sentences, which limits itself to (1) it is only applicable to encoder-decoder framework without attention model, (2) the representation can only be used to initialize decoder. In contrast, we use HRED to summarize the previous sentences alone, which provides additional cross-sentence context for NMT. Our approach is more flexible at (1) it is applicable to any encoderdecoder frameworks (e.g., with attention), (2) the cross-sentence context can be used to initialize either encoder, decoder or both.\nWhile both our approach and Serban et al. (2016) use Auxiliary Context mechanism for incorporating cross-sentence context, there are two main differences: 1) we have separate parameters to better control the effects of the cross- and intrasentence contexts, while they only have one parameter matrix to manage the single representation that encodes both contexts; 2) based on the intuition that not every target word generation requires equivalent cross-sentence context, we introduce a context gate (Tu et al., 2017a) to control the amount of information from it, while they don’t.\nAt the same time, some researchers propose to use an additional set of an encoder and attention to model more information. For example, Jean et al. (2017) use it to encode and select part of the previous source sentence for generating each target word. Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments.\nOur work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong\net al., 2015), which incorporate additional source or target languages. They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduced context gate plays an important role. We quantitatively and qualitatively demonstrated that the presented model significantly outperforms a strong attention-based NMT baseline system. We release the code for these experiments at https:// www.github.com/tuzhaopeng/LC-NMT.\nOur models benefit from larger contexts, and would be possibly further enhanced by other document level information, such as discourse relations. We propose to study such models for full length documents with more linguistic features in future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the Science Foundation of Ireland (SFI) ADAPT project (Grant No.:13/RC/2106). The authors also wish to thank the anonymous reviewers for many helpful comments with special thanks to Henry Elder for his generous help on proofreading of this manuscript."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations. San Diego, USA, pages 1–15.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Incorporating global visual features into attentionbased neural machine translation",
      "author" : [ "Iacer Calixto", "Qun Liu", "Nick Campbell." ],
      "venue" : "arXiv preprint arXiv:1701.06521 .",
      "citeRegEx" : "Calixto et al\\.,? 2017",
      "shortCiteRegEx" : "Calixto et al\\.",
      "year" : 2017
    }, {
      "title" : "The trouble with smt consistency",
      "author" : [ "Marine Carpuat", "Michel Simard." ],
      "venue" : "Proceedings of the 7th Workshop on Statistical Machine Translation. Montreal, Quebec, Canada, pages 442–449.",
      "citeRegEx" : "Carpuat and Simard.,? 2012",
      "shortCiteRegEx" : "Carpuat and Simard.",
      "year" : 2012
    }, {
      "title" : "Clause restructuring for statistical machine translation",
      "author" : [ "Michael Collins", "Philipp Koehn", "Ivona Kucerova." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics. Ann Arbor, Michigan, pages 531–540.",
      "citeRegEx" : "Collins et al\\.,? 2005",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2005
    }, {
      "title" : "Multi-task learning for multiple language translation",
      "author" : [ "Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Assocaition for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "Does neural machine translation benefit from larger context",
      "author" : [ "Sebastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho" ],
      "venue" : null,
      "citeRegEx" : "Jean et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2017
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, pages",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "D. Christopher Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Bleu: A method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Philadelphia, Pennsylvania,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Nematus: a toolkit for neural machine",
      "author" : [ "Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel Läubli", "Antonio Valerio Miceli Barone", "Jozef Mokry" ],
      "venue" : null,
      "citeRegEx" : "Sennrich et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2017
    }, {
      "title" : "Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers, Berlin, Germany, chapter Linguistic Input Features",
      "author" : [ "Rico Sennrich", "Barry Haddow" ],
      "venue" : "Improve Neural Machine Translation,",
      "citeRegEx" : "Sennrich and Haddow.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sennrich and Haddow.",
      "year" : 2016
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelli-",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical recurrent encoderdecoder for generative context-aware query suggestion",
      "author" : [ "Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob Grue Simonsen", "JianYun Nie." ],
      "venue" : "Proceedings of the 24th ACM International",
      "citeRegEx" : "Sordoni et al\\.,? 2015",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Proceedings of the 2014 Neural Information Processing Systems. Montreal, Canada, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Context gates for neural machine translation",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li." ],
      "venue" : "Transactions of the Association for Computational Linguistics .",
      "citeRegEx" : "Tu et al\\.,? 2017a",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation with reconstruction",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li." ],
      "venue" : "Proceedings of the 31th AAAI Conference on Artificial Intelligence (AAAI17). San Francisco, California, USA, pages 3097–",
      "citeRegEx" : "Tu et al\\.,? 2017b",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling coverage for neural machine translation",
      "author" : [ "Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li." ],
      "venue" : "Proceedings of the 54th annual meeting of the Association for Computational Linguistics. Berlin, Germany, pages 76–85.",
      "citeRegEx" : "Tu et al\\.,? 2016",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le." ],
      "venue" : "Proceedings of the International Conference on Machine Learning, Deep Learning Workshop. pages 1–8.",
      "citeRegEx" : "Vinyals and Le.,? 2015",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Document-level consistency verification in machine translation",
      "author" : [ "Tong Xiao", "Jingbo Zhu", "Shujie Yao", "Hao Zhang." ],
      "venue" : "Machine Translation Summit. Xiamen, China, volume 13, pages 131–138.",
      "citeRegEx" : "Xiao et al\\.,? 2011",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-source neural translation",
      "author" : [ "Barret Zoph", "Kevin Knight." ],
      "venue" : "arXiv preprint arXiv:1601.00710 .",
      "citeRegEx" : "Zoph and Knight.,? 2016",
      "shortCiteRegEx" : "Zoph and Knight.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 172
    }, {
      "referenceID" : 16,
      "context" : "Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015).",
      "startOffset" : 235,
      "endOffset" : 255
    }, {
      "referenceID" : 18,
      "context" : "Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012).",
      "startOffset" : 153,
      "endOffset" : 198
    }, {
      "referenceID" : 2,
      "context" : "Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012).",
      "startOffset" : 153,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al.",
      "startOffset" : 154,
      "endOffset" : 176
    }, {
      "referenceID" : 17,
      "context" : ", 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016).",
      "startOffset" : 30,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : ", 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016).",
      "startOffset" : 30,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "Specifically, we employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences, which deploys an additional documentlevel RNN on top of the sentence-level RNN encoder (Sordoni et al., 2015).",
      "startOffset" : 236,
      "endOffset" : 258
    }, {
      "referenceID" : 5,
      "context" : "To the best of our knowledge, our work and Jean et al. (2017) are two independently early attempts to model crosssentence context for NMT.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "For encoder, we useD as the initialization state rather than all-zero states as in the standard NMT (Bahdanau et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "To this end, we extend auxiliary context strategy by introducing a context gate (Tu et al., 2017a) to dynamically control the amount of information flowing from the auxiliary global context at each decoding step, as shown in Figure 2 (c).",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "We used case-insensitive BLEU score (Papineni et al., 2002) as our evaluation metric, and sign-test (Collins et al.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : ", 2002) as our evaluation metric, and sign-test (Collins et al., 2005) for calculating statistical significance.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "We implemented our approach on top of an open source attention-based NMT model, Nematus4 (Sennrich and Haddow, 2016; Sennrich et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 139
    }, {
      "referenceID" : 9,
      "context" : "We implemented our approach on top of an open source attention-based NMT model, Nematus4 (Sennrich and Haddow, 2016; Sennrich et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "It is consistent with the results in (Tu et al., 2017b) (i.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "While our approach is built on top of hierarchical recurrent encoder-decoder (HRED) (Sordoni et al., 2015), there are several key differences which reflect how we have generalized from the original model.",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "While our approach is built on top of hierarchical recurrent encoder-decoder (HRED) (Sordoni et al., 2015), there are several key differences which reflect how we have generalized from the original model. Sordoni et al. (2015) use HRED to summarize a single representation from both the current and previous sentences, which limits itself to (1) it is only applicable to encoder-decoder framework without attention model, (2) the representation can only be used to initialize decoder.",
      "startOffset" : 85,
      "endOffset" : 227
    }, {
      "referenceID" : 14,
      "context" : "(2016) use Auxiliary Context mechanism for incorporating cross-sentence context, there are two main differences: 1) we have separate parameters to better control the effects of the cross- and intrasentence contexts, while they only have one parameter matrix to manage the single representation that encodes both contexts; 2) based on the intuition that not every target word generation requires equivalent cross-sentence context, we introduce a context gate (Tu et al., 2017a) to control the amount of information from it, while they don’t.",
      "startOffset" : 458,
      "endOffset" : 476
    }, {
      "referenceID" : 11,
      "context" : "While both our approach and Serban et al. (2016) use Auxiliary Context mechanism for incorporating cross-sentence context, there are two main differences: 1) we have separate parameters to better control the effects of the cross- and intrasentence contexts, while they only have one parameter matrix to manage the single representation that encodes both contexts; 2) based on the intuition that not every target word generation requires equivalent cross-sentence context, we introduce a context gate (Tu et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "For example, Jean et al. (2017) use it to encode and select part of the previous source sentence for generating each target word.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT. As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017). However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.",
      "startOffset" : 0,
      "endOffset" : 300
    }, {
      "referenceID" : 19,
      "context" : "Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong et al., 2015), which incorporate additional source or target languages.",
      "startOffset" : 86,
      "endOffset" : 105
    } ],
    "year" : 2017,
    "abstractText" : "In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT). First, this history is summarized in a hierarchical way. We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states. Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.",
    "creator" : "LaTeX with hyperref package"
  }
}