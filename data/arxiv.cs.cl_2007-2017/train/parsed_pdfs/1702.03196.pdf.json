{
  "name" : "1702.03196.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Universal Semantic Parsing",
    "authors" : [ "Siva Reddy", "Oscar Täckström", "Slav Petrov", "Mark Steedman", "Mirella Lapata" ],
    "emails" : [ "sivar@stanford.edu,", "slav}@google.com,", "mlap}@inf.ed.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The Universal Dependencies (UD) initiative seeks to develop cross-linguistically consistent annotation guidelines as well as a large number of uniformly annotated treebanks for many languages (Nivre et al., 2016). Such resources could advance multilingual applications of parsing, improve comparability of evaluation results, enable cross-lingual learning, and more generally support natural language understanding.\n∗Work done at the University of Edinburgh\nSeeking to exploit the benefits of UD for natural language understanding, we introduce UDEPLAMBDA, a semantic interface for UD that maps natural language to logical forms, representing underlying predicate-argument structures, in an almost language-independent manner. Our framework is based on DEPLAMBDA (Reddy et al., 2016) a recently developed method that converts English Stanford Dependencies (SD) to logical forms. The conversion process is illustrated in Figure 1 and discussed in more detail in Section 2. Whereas DEPLAMBDA works only for English, UDEPLAMBDA applies to any language for which UD annotations are available.1 Moreover, DEPLAMBDA can only process tree-structured inputs whereas UDEPLAMBDA can also process dependency graphs, which allow to handle complex constructions such as control. The different treatments of various linguistic constructions in UD compared to SD also require different handling in UDEPLAMBDA (Section 3.3).\nOur experiments focus on Freebase semantic parsing as a testbed for evaluating the framework’s multilingual appeal. We convert natural language to logical forms which in turn are converted to machine interpretable formal meaning representations for retrieving answers to questions from Freebase. To facilitate multilingual evaluation, we provide translations of the English WebQuestions (Berant et al., 2013) and GraphQuestions (Su et al., 2016) datasets to German and Spanish. We demonstrate that UDEPLAMBDA can be used to derive logical forms for these languages using a minimal amount of language-specific knowledge. Aside from developing the first multilingual semantic parsing tool for Freebase, we also experimentally show that UDEPLAMBDA outperforms strong baselines across\n1As of v1.3, UD annotations are available for 47 languages at http://universaldependencies.org.\nar X\niv :1\n70 2.\n03 19\n6v 4\n[ cs\n.C L\n] 2\n8 A\nug 2\n01 7\nlanguages and datasets. For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions. Our implementation and translated datasets are publicly available at https://github.com/sivareddyg/udeplambda."
    }, {
      "heading" : "2 DEPLAMBDA",
      "text" : "Before describing UDEPLAMBDA, we provide an overview of DEPLAMBDA (Reddy et al., 2016) on which our approach is based. DEPLAMBDA converts a dependency tree to its logical form in three steps: binarization, substitution, and composition, each of which is briefly outlined below. Algorithm 1 describes the steps of DEPLAMBDA in lines 4-6, whereas lines 2 and 3 are specific to UDEPLAMBDA.\nBinarization A dependency tree is first mapped to a Lisp-style s-expression indicating the order of semantic composition. Figure 1(b) shows the s-expression for the sentence Disney won an Oscar for the movie Frozen, derived from the dependency tree in Figure 1(a). Here, the sub-expression (dobj won (det Oscar an)) indicates that the logical form of the phrase won an Oscar is derived by composing the logical form of the label dobj with the logical form of the word won and the logical form of the phrase an Oscar, derived analogously. The s-expression can also be interpreted as a binarized tree with the dependency label as the root node, and the left and right expressions as subtrees.\nA composition hierarchy is employed to impose a strict traversal ordering on the modifiers to each head in the dependency tree. As an example, won has three modifiers in Figure 1(a), which according to the composition hierarchy are composed in the order dobj> nmod> nsubj. In constructions like coordination, this ordering is crucial to arrive at the correct semantics. Lines 7-17 in Algorithm 1 describe the binarization step.\nSubstitution Each symbol in the s-expressions is substituted for a lambda expression encoding its semantics. Words and dependency labels are assigned different types of expressions. In general, words have expressions of the following kind: ENTITY ⇒ λx.word(xa); e.g. Oscar⇒ λx.Oscar(xa) EVENT ⇒ λx.word(xe); e.g. won⇒ λx.won(xe) FUNCTIONAL⇒ λx.TRUE; e.g. an⇒ λx.TRUE\nHere, the subscripts ·a and ·e denote the types of individuals (Ind) and events (Event), respectively, whereas x denotes a paired variable (xa,xe)\nof type Ind×Event. Roughly speaking, proper nouns and adjectives invoke ENTITY expressions, verbs and adverbs invoke EVENT expressions, and common nouns invoke both ENTITY and EVENT expressions (see Section 3.3), while remaining words invoke FUNCTIONAL expressions. DEPLAMBDA enforces the constraint that every s-expression is of the type η = Ind×Event→ Bool, which simplifies the type system considerably.\nExpressions for dependency labels glue the semantics of heads and modifiers to articulate predicate-argument structure. These expressions in general take one of the following forms:\nCOPY ⇒ λ f gx.∃y. f (x)∧g(y)∧ rel(x,y) e.g. nsubj, dobj, nmod, advmod INVERT ⇒ λ f gx.∃y. f (x)∧g(y)∧ reli(y,x) e.g. amod, acl MERGE ⇒ λ f gx. f (x)∧g(x) e.g. compound, appos, amod, acl HEAD ⇒ λ f gx. f (x) e.g. case, punct, aux, mark .\nAs an example of COPY, consider the lambda expression for dobj in (dobj won (det Oscar an)): λ f gx.∃y. f (x)∧ g(y)∧ arg2(xe,ya). This expression takes two functions f and g as input, where f represents the logical form of won and g represents the logical form of an Oscar. The predicateargument structure arg2(xe,ya) indicates that the arg2 of the event xe, i.e. won, is the individual ya, i.e. the entity Oscar. Since arg2(xe,ya) mimics the dependency structure dobj(won, Oscar), we refer to the expression kind evoked by dobj as COPY.\nExpressions that invert the dependency direction are referred to as INVERT (e.g. amod in running horse); expressions that merge two subexpressions without introducing any relation predicates are referred to as MERGE (e.g. compound in movie Frozen); and expressions that simply return the parent expression semantics are referred to as HEAD (e.g. case in for Frozen). While this generalization applies to most dependency labels, several labels take a different logical form not listed here, some of which are discussed in Section 3.3. Sometimes the mapping of dependency label to lambda expression may depend on surrounding part-of-speech tags or dependency labels. For example, amod acts as INVERT when the modifier is a verb (e.g. in running horse), and as MERGE when the modifier is an adjective (e.g. in beautiful horse).2 Lines 26-32 in Algorithm 1 describe the substitution procedure.\nComposition The final logical form is computed by beta-reduction, treating expressions of the form (f x y) as the function f applied to the arguments x and y. For example, (dobj won (det Oscar an)) results in λx.∃z.won(xe)∧Oscar(za)∧ arg2(xe,za) when the expression for dobj is applied to those for won and (det Oscar an). Figure 1(c) shows the logical form for the s-expression in Figure 1(b). The binarized s-expression is recursively converted to a logical form as described in lines 18-25 in Algorithm 1."
    }, {
      "heading" : "3 UDEPLAMBDA",
      "text" : "We now introduce UDEPLAMBDA, a semantic interface for Universal Dependencies.3 Whereas DEPLAMBDA only applies to English Stanford Dependencies, UDEPLAMBDA takes advantage of the cross-lingual nature of UD to facilitate an (almost) language independent semantic interface. This is accomplished by restricting the binarization, substitution, and composition steps described above to rely solely on information encoded in the UD representation. As shown in Algorithm 1, lines 4-6 are common to both DEPLAMBDA and UDEPLAMBDA, whereas lines 2 and 3 applies only to UDEPLAMBDA. Importantly, UDEPLAMBDA is designed to not rely on lexical forms in a language\n2We use Tregex (Levy and Andrew, 2006) for substitution mappings and Cornell SPF (Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx.∃y. f (x)∧g(y)∧ amodi(ye,xa).\n3In what follows, all references to UD are to UD v1.3.\nAlgorithm 1: UDEPLAMBDA Steps 1 Function UDepLambda(depTree): 2 depGraph = Enhancement (depTree) #See Figure 2(a) for a depGraph. 3 bindedTree = SplitLongDistance (depGraph) #See Figure 2(b) for a bindedTree. 4 binarizedTree = Binarization (bindedTree) #See Figure 1(b) for a binarizedTree. 5 logicalForm = Composition (binarizedTree) 6 return logicalForm\n7 Function Binarization (tree): 8 parent = GetRootNode (tree); 9 {(label1,child1),(label2,child2) . . .}\n= GetChildNodes (parent) 10 sortedChildren = SortUsingLabelHierarchy ({(label1,child1),(label2,child2) . . .}) 11 binarziedTree.root = parent 12 for label, child ∈ sortedChildren: 13 temp.root = label 14 temp.le f t = binarziedTree 15 temp.right = Binarization(child) 16 binarziedTree = temp 17 return binarizedTree\n18 Function Composition (binarizedTree): 19 mainLF = Substitution (binarizedTree.root) 20 if binarziedTree has left and right children: 21 le f tLF = Composition (binarziedTree.le f t) 22 rightLF = Composition(binarziedTree.right) 23 mainLF = BetaReduce (mainLF, le f tLF) 24 mainLF = BetaReduce (mainLF,rightLF) 25 return mainLF\n26 Function Substitution (node): 27 logicalForms = [ ] 28 for tregexRule, template ∈ substitutionRules: 29 if tregexRule.match(node): 30 l f = GenLambdaExp (node, template) 31 logicalForms.add(l f ) 32 return logicalForms\nto assign lambda expressions, but only on information contained in dependency labels and postags.\nHowever, some linguistic phenomena are language specific (e.g. pronoun-dropping) or lexicalized (e.g. every and the in English have different semantics, despite being both determiners) and are not encoded in the UD schema. Furthermore, some cross-linguistic phenomena, such as long-distance dependencies, are not part of the core UD representation. To circumvent this limitation, a simple enhancement step enriches the original UD representation before binarization takes place (Section 3.1). This step adds to the dependency tree missing syntactic information and long-distance dependencies, thereby creating a graph. Whereas DEPLAMBDA is not able to handle graph-structured input, UDEP-\nLAMBDA is designed to work with dependency graphs as well (Section 3.2). Finally, several constructions differ in structure between UD and SD, which requires different handling in the semantic interface (Section 3.3)."
    }, {
      "heading" : "3.1 Enhancement",
      "text" : "Both Schuster and Manning (2016) and Nivre et al. (2016) note the necessity of an enhanced UD representation to enable semantic applications. However, such enhancements are currently only available for a subset of languages in UD. Instead, we rely on a small number of enhancements for our main application—semantic parsing for questionanswering—with the hope that this step can be replaced by an enhanced UD representation in the future. Specifically, we define three kinds of enhancements: (1) long-distance dependencies; (2) types of coordination; and (3) refined question word tags. These correspond to line 2 in Algorithm 1.\nFirst, we identify long-distance dependencies in relative clauses and control constructions. We follow Schuster and Manning (2016) and find these using the labels acl (relative) and xcomp (control). Figure 2(a) shows the long-distance dependency in the sentence Anna wants to marry Kristoff. Here, marry is provided with its missing nsubj (dashed arc). Second, UD conflates all coordinating constructions to a single dependency label, conj. To obtain the correct coordination scope, we refine conj to conj:verb, conj:vp, conj:sentence, conj:np, and conj:adj, similar to Reddy et al. (2016). Finally, unlike the PTB tags (Marcus et al., 1993) used by SD, the UD part-of-speech tags do not distinguish question words. Since these are crucial to question-answering, we use a small lexicon to refine the tags for determiners (DET), adverbs (ADV) and pronouns (PRON) to DET:WH, ADV:WH and PRON:WH, respectively. Specifically, we use a list of 12 (English), 14 (Spanish) and 35 (German) words, respectively. This is the only part of UDEPLAMBDA that relies on language-specific information. We hope that, as the coverage of morphological features in UD improves, this refinement can be replaced by relying on morphological features, such as the interrogative feature (INT)."
    }, {
      "heading" : "3.2 Graph Structures and BIND",
      "text" : "To handle graph structures that may result from the enhancement step, such as those in Figure 2(a), we propose a variable-binding mechanism that differs\nfrom that of DEPLAMBDA. This is indicated in line 3 of Algorithm 1. First, each long-distance dependency is split into independent arcs as shown in Figure 2(b). Here, Ω is a placeholder for the subject of marry, which in turn corresponds to Anna as indicated by the binding of Ω via the pseudo-label BIND. We treat BIND like an ordinary dependency label with semantics MERGE and process the resulting tree as usual, via the s-expression:\n(nsubj (xcomp wants (nsubj (mark (dobj marry Kristoff) to) Ω) (BIND Anna Ω)) ,\nwith the lambda-expression substitutions:\nwants, marry ∈ EVENT; to ∈ FUNCTIONAL; Anna, Kristoff ∈ ENTITY; mark ∈ HEAD; BIND ∈ MERGE; xcomp = λ f gx.∃y. f (x)∧g(y)∧xcomp(xe,ye) .\nThese substitutions are based solely on unlexicalized context. For example, the part-of-speech tag PROPN of Anna invokes an ENTITY expression.\nThe placeholder Ω has semantics λx.EQ(x,ω), where EQ(u,ω) is true iff u and ω are equal (have the same denotation), which unifies the subject variable of wants with the subject variable of marry.\nAfter substitution and composition, we get:\nλz.∃xywv.wants(ze)∧Anna(xa)∧ arg1(ze,xa)∧ EQ(x,ω) ∧marry(ye)∧xcomp(ze,ye)∧ arg1(ye,va)∧ EQ(v,ω) ∧ Kristoff(wa)∧ arg2(ye,wa) ,\nThis expression may be simplified further by replacing all occurrences of v with x and removing the unification predicates EQ, which results in:\nλz.∃xyw.wants(ze)∧Anna(xa)∧ arg1(ze,xa) ∧marry(ye)∧xcomp(ze,ye)∧ arg1(ye,xa) ∧ Kristoff(wa)∧ arg2(ye,wa) .\nThis expression encodes the fact that Anna is the arg1 of the marry event, as desired. DEPLAMBDA, in contrast, cannot handle graph-structured input, since it lacks a principled way of generating sexpressions from graphs. Even given the above s-expression, BIND in DEPLAMBDA is defined in a way such that the composition fails to unify v and x, which is crucial for the correct semantics. Moreover, the definition of BIND in DEPLAMBDA does not have a formal interpretation within the lambda calculus, unlike ours."
    }, {
      "heading" : "3.3 Linguistic Constructions",
      "text" : "Below, we highlight the most pertinent differences between UDEPLAMBDA and DEPLAMBDA, stemming from the different treatment of various linguistic constructions in UD versus SD.\nPrepositional Phrases UD uses a content-head analysis, in contrast to SD, which treats function words as heads of prepositional phrases, Accordingly, the s-expression for the phrase president in 2009 is (nmod president (case 2009 in)) in UDEPLAMBDA and (prep president (pobj in 2009)) in DEPLAMBDA. To achieve the desired semantics,\nλx.∃y.president(xa)∧president event(xe)∧ arg1(xe,xa)∧2009(ya)∧prep.in(xe,ya) ,\nDEPLAMBDA relies on an intermediate logical form that requires some post-processing, whereas UDEPLAMBDA obtains the desired logical form directly through the following entries:\nin ∈ FUNCTIONAL; 2009 ∈ ENTITY; case ∈ HEAD; president = λx.president(xa)∧president event(xe) ∧arg1(xe,xa) ; nmod = λ f gx.∃y. f (x)∧g(y)∧nmod.in(xe,ya) .\nOther nmod constructions, such as possessives (nmod:poss), temporal modifiers (nmod:tmod) and adverbial modifiers (nmod:npmod), are handled similarly. Note how the common noun president, evokes both entity and event predicates above.\nPassives DEPLAMBDA gives special treatment to passive verbs, identified by the fine-grained partof-speech tags in the PTB tag together with dependency context. For example, An Oscar was won is analyzed as λx.won.pass(xe)∧Oscar(ya)∧ arg1(xe,ya), where won.pass represents a passive event. However, UD does not distinguish between active and passive forms.4 While the labels\n4UD encodes voice as a morphological feature, but most syntactic analyzers do not produce this information yet.\nnsubjpass or auxpass indicate passive constructions, such clues are sometimes missing, such as in reduced relatives. We therefore opt to not have separate entries for passives, but aim to produce identical logical forms for active and passive forms when possible (for example, by treating nsubjpass as direct object). With the following entries, won ∈ EVENT; an, was ∈ FUNCTIONAL; auxpass ∈ HEAD; nsubjpass = λ f gx.∃y. f (x)∧g(y)∧ arg2(xe,ya) ,\nthe lambda expression for An Oscar was won becomes λx.won(xe)∧Oscar(ya)∧arg2(xe,ya), identical to that of its active form. However, not having a special entry for passive verbs may have undesirable side-effects. For example, in the reducedrelative construction Pixar claimed the Oscar won for Frozen, the phrase the Oscar won ... will receive the semantics λx.Oscar(ya)∧won(xe)∧ arg1(xe,ya), which differs from that of an Oscar was won. We leave it to the target application to disambiguate the interpretation in such cases.\nLong-Distance Dependencies As discussed in Section 3.2, we handle long-distance dependencies evoked by clausal modifiers (acl) and control verbs (xcomp) with the BIND mechanism, whereas DEPLAMBDA cannot handle control constructions. For xcomp, as seen earlier, we use the mapping λ f gx.∃y. f (x)∧g(y)∧xcomp(xe,ye). For acl we use λ f gx.∃y. f (x)∧ g(y), to conjoin the main clause and the modifier clause. However, not all acl clauses evoke long-distance dependencies, e.g. in the news that Disney won an Oscar, the clause that Disney won an Oscar is a subordinating conjunction of news. In such cases, we instead assign acl the INVERT semantics.\nQuestions Question words are marked with the enhanced part-of-speech tags DET:WH, ADV:WH and PRON:WH, which are all assigned the semantics λx.${word}(xa)∧ TARGET(xa). The predicate TARGET indicates that xa represents the variable of interest, that is the answer to the question."
    }, {
      "heading" : "3.4 Limitations",
      "text" : "In order to achieve language independence, UDEPLAMBDA has to sacrifice semantic specificity, since in many cases the semantics is carried by lexical information. Consider the sentences John broke the window and The window broke. Although it is the window that broke in both cases, our inferred logical forms do not canonicalize the relation between broke and window. To achieve this, we\nwould have to make the substitution of nsubj depend on lexical context, such that when window occurs as nsubj with broke, the predicate arg2 is invoked rather than arg1. UDEPLAMBDA does not address this problem, and leave it to the target application to infer context-sensitive semantics of arg1 and arg2. To measure the impact of this limitation, we present UDEPLAMBDASRL in Section 4.4 which addresses this problem by relying on semantic roles from semantic role labeling (Palmer et al., 2010).\nOther constructions that require lexical information are quantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material.\nAlthough in the current setup UDEPLAMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UDEPLAMBDA as a first step towards learning rules for converting UD to richer semantic representations such as PropBank, AMR, or the Parallel Meaning Bank (Palmer et al., 2005; Banarescu et al., 2013; Abzianidze et al., 2017).."
    }, {
      "heading" : "4 Cross-lingual Semantic Parsing",
      "text" : "To study the multilingual nature of UDEPLAMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed.\n5UD v1.3 has 40 dependency labels, and the number of substitution rules in UDEPLAMBDA are 61, with some labels having multiple rules, and some representing lexical semantics."
    }, {
      "heading" : "4.1 Semantic Parsing as Graph Matching",
      "text" : "UDEPLAMBDA generates ungrounded logical forms that are independent of any knowledge base, such as Freebase. We use GRAPHPARSER (Reddy et al., 2016) to map these logical forms to their grounded Freebase graphs, via corresponding ungrounded graphs. Figures 3(a) to 3(c) show the ungrounded graphs corresponding to logical forms from UDEPLAMBDA, each grounded to the same Freebase graph in Figure 3(d). Here, rectangles denote entities, circles denote events, rounded rectangles denote entity types, and edges between events and entities denote predicates or Freebase relations. Finally, the TARGET node represents the set of values of x that are consistent with the Freebase graph, that is the answer to the question.\nGRAPHPARSER treats semantic parsing as a graph-matching problem with the goal of finding the Freebase graphs that are structurally isomorphic to an ungrounded graph and rank them according to a model. To account for structural mismatches, GRAPHPARSER uses two graph transformations: CONTRACT and EXPAND. In Figure 3(a) there are two edges between x and Ghana. CONTRACT collapses one of these edges to create a graph isomorphic to Freebase. EXPAND, in contrast, adds edges to connect the graph in the case of disconnected components. The search space is explored by beam search and model parameters are estimated with the averaged structured perceptron (Collins, 2002) from training data consisting of question-answer pairs, using answer F1-score as the objective."
    }, {
      "heading" : "4.2 Datasets",
      "text" : "We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English questions and their answers, and GraphQuestions (Su et al., 2016), a recently released dataset of English questions with both their answers and grounded logical forms.\nWhile WebQuestions is dominated by simple entityattribute questions, GraphQuestions contains a large number of compositional questions involving aggregation (e.g. How many children of Eddard Stark were born in Winterfell? ) and comparison (e.g. In which month does the average rainfall of New York City exceed 86 mm? ). The number of training, development and test questions is 2644, 1134, and 2032, respectively, for WebQuestions and 1794, 764, and 2608 for GraphQuestions.\nTo support multilingual evaluation, we created translations of WebQuestions and GraphQuestions to German and Spanish. For WebQuestions two professional annotators were hired per language, while for GraphQuestions we used a trusted pool of 20 annotators per language (with a single annotator per question). Examples of the original questions and their translations are provided in Table 1."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model.\nDependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2.\nEntity Resolution We follow Reddy et al. (2016) and resolve entities in three steps: (1) potential entity spans are identified using seven handcrafted part-of-speech patterns; (2) each span is associated with potential Freebase entities according to the Freebase/KG API; and (3) the 10-best entity linking lattices, scored by a structured perceptron, are\n6https://sites.google.com/site/rmyeid/projects/polyglot.\nWebQuestions\nen What language do the people in Ghana speak? de Welche Sprache wird in Ghana gesprochen? es ¿Cuál es la lengua de Ghana?\nen Who was Vincent van Gogh inspired by? de Von wem wurde Vincent van Gogh inspiriert? es ¿Qué inspiró a Van Gogh?\nGraphQuestions\nen NASA has how many launch sites? de Wie viele Abschussbasen besitzt NASA? es ¿Cuántos sitios de despegue tiene NASA?\nen Which loudspeakers are heavier than 82.0 kg? de Welche Lautsprecher sind schwerer als 82.0 kg? es ¿Qué altavoces pesan más de 82.0 kg?\ninput to GRAPHPARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1-scores for each language and dataset.\nFeatures We use features similar to Reddy et al. (2016): basic features of words and Freebase relations, and graph features crossing ungrounded events with grounded relations, ungrounded types with grounded relations, and ungrounded answer type crossed with a binary feature indicating if the answer is a number. In addition, we add features encoding the semantic similarity of ungrounded events and Freebase relations. Specifically, we used the cosine similarity of the translation-invariant embeddings of Huang et al. (2015).7"
    }, {
      "heading" : "4.4 Comparison Systems",
      "text" : "We compared UDEPLAMBDA to four versions of GRAPHPARSER that operate on different representations, in addition to prior work.\nSINGLEEVENT This model resembles the learning-to-rank model of Bast and Haussmann (2015). An ungrounded graph is generated by connecting all entities in the question with the TARGET node, representing a single event. Note that this\n7http://128.2.220.95/multilingual/data/.\nbaseline cannot handle compositional questions, or those with aggregation or comparison.\nDEPTREE An ungrounded graph is obtained directly from the original dependency tree. An event is created for each parent and its dependents in the tree. Each dependent is linked to this event with an edge labeled with its dependency relation, while the parent is linked to the event with an edge labeled arg0. If a word is a question word, an additional TARGET predicate is attached to its entity node.\nCCGGRAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English.\nUDEPLAMBDASRL This is similar to UDEPLAMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1, arg2 and arg2, we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8"
    }, {
      "heading" : "4.5 Results",
      "text" : "Table 3 shows the performance of GRAPHPARSER with these different representations. Here and in what follows, we use average F1-score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UDEPLAMBDA consistently outperforms the SINGLEEVENT and DEPTREE representations in all languages.\nFor English, performance is on par with CCGGRAPH, which suggests that UDEPLAMBDA does not sacrifice too much specificity for universality. With both datasets, results are lower for German compared to Spanish. This agrees with the lower performance of the syntactic parser on the German portion of the UD treebank. While UDEPLAMBDASRL performs better than UDEP-\n8The parser accuracies (%) are 87.33, 81.38 and 79.91for English, German and Spanish respectively.\nLAMBDA on WebQuestions for English, we do not see large performance gaps in other settings, suggesting that GRAPHPARSER is either able to learn context-sensitive semantics of ungrounded predicates or that the datasets do not contain ambiguous nsubj, dobj and nsubjpass mappings. Finally, while these results confirm that GraphQuestions is much harder compared to WebQuestions, we note that both datasets predominantly contain single-hop questions, as indicated by the competitive performance of SINGLEEVENT on both datasets.\nTable 4 compares UDEPLAMBDA with previously published models which exist only for English and have been mainly evaluated on WebQuestions. These are either symbolic like ours (first block) or employ neural networks (second block). Results for models using additional task-specific training resources, such as ClueWeb09, Wikipedia, or SimpleQuestions (Bordes et al., 2015) are shown in parentheses. On GraphQuestions, we achieve a new state-of-the-art result with a gain of 4.8 F1points over the previously reported best result. On WebQuestions we are 2.1 points below the best model using comparable resources, and 3.8 points below the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1score to their use of the more fine-grained PTB tag set and Stanford Dependencies."
    }, {
      "heading" : "5 Related Work",
      "text" : "Our work continues the long tradition of building logical forms from syntactic representations initiated by Montague (1973). The literature is rife with\nattempts to develop semantic interfaces for HPSG (Copestake et al., 2005), LFG (Kaplan and Bresnan, 1982; Dalrymple et al., 1995; Crouch and King, 2006), TAG (Kallmeyer and Joshi, 2003; Gardent and Kallmeyer, 2003; Nesson and Shieber, 2006), and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Artzi et al., 2015). Unlike existing semantic interfaces, UDEPLAMBDA uses dependency syntax, a widely available syntactic resource.\nA common trend in previous work on semantic interfaces is the reliance on rich typed feature structures or semantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Instead, UDEPLAMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; Bédaride and Gardent, 2011). In contrast, UDEPLAMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016).\nWe evaluate UDEPLAMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability,\nscalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014)."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We introduced UDEPLAMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UDEPLAMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit applications such as summarization (Liu et al., 2015) and machine reading (Sachan and Xing, 2016)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work greatly benefited from discussions with Michael Collins, Dipanjan Das, Federico Fancellu, Julia Hockenmaier, Tom Kwiatkowski, Adam Lopez, Valeria de Paiva, Martha Palmer, Fernando Pereira, Emily Pitler, Vijay Saraswat, Nathan Schneider, Bonnie Webber, Luke Zettlemoyer, and the members of ILCC Edinburgh University, the Microsoft Research Redmond NLP group, the Stanford NLP group, and the UW NLP and Linguistics group. We thank Reviewer 2 for useful feedback. The authors would also like to thank the Universal Dependencies community for the treebanks and documentation. This research is supported by a Google PhD Fellowship to the first author. We acknowledge the financial support of the European Research Council (Lapata; award number 681760)."
    }, {
      "heading" : "1 Universal Quantification",
      "text" : "Consider the sentence Everybody wants to buy a house,1 whose dependency tree in the Universal Dependencies (UD) formalism is shown in Figure 1(a). This sentence has two possible readings: either (1) every person wants to buy a different house; or (2) every person wants to buy the same house. The two interpretations correspond to the following logical forms: (1) ∀x.person(xa)→\n[∃zyw.wants(ze)∧ arg1(ze,xa)∧buy(ye)∧xcomp(ze,ye)∧ house(wa)∧ arg1(ze,xa)∧ arg2(ze,wa)] ;\n(2) ∃w.house(wa)∧ (∀x.person(xa)→ [∃zy.wants(ze)∧ arg1(ze,xa)∧buy(ye)∧xcomp(ze,ye)∧\narg1(ze,xa)∧ arg2(ze,wa)]) ."
    }, {
      "heading" : "In (1), the existential variable w is in the scope of",
      "text" : "the universal variable x (i.e. the house is dependent on the person). This reading is commonly referred to as the surface reading. Conversely, in (2) the universal variable x is in the scope of the existential variable w (i.e. the house is independent of the person). This reading is also called inverse reading. Our goal is to obtain the surface reading logical form in (1) with UDEPLAMBDA. We do not aim to obtain the inverse reading, although this is possible with the use of Skolemization (Steedman, 2012).\nIn UDEPLAMBDA, lambda expressions for words, phrases and sentences are all of the form λx. . . .. But from (1), it is clear that we need to express variables bound by quantifiers, e.g. ∀x, while still providing access to x for composition. This demands a change in the type system since the\n1Example borrowed from Schuster and Manning (2016).\nsame variable cannot be lambda bound and quantifier bound—that is we cannot have formulas of the form λx . . .∀x . . .. In this material, we first derive the logical form for the example sentence using the type system from our main paper (Section 1.1) and show that it fails to handle universal quantification. We then modify the type system slightly to allow derivation of the desired surface reading logical form (Section 1.2). This modified type system is a strict generalization of the original type system.2 Fancellu et al. (2017) present an elaborate discussion on the modified type system, and how it can handle negation scope and its interaction with universal quantifiers.\n2Note that this treatment has yet to be added to our implementation, which can be found at https://github.com/ sivareddyg/udeplambda."
    }, {
      "heading" : "1.1 With Original Type System",
      "text" : "We will first attempt to derive the logical form in (1) using the default type system of UDEPLAMBDA. Figure 1(b) shows the enhanced dependency tree for the sentence, where BIND has been introduced to connect the implied nsubj of buy (BIND is explained in the main paper in Section 3.2). The s-expression corresponding to the enhanced tree is: (nsubj (xcomp wants (mark\n(nsubj (dobj buy (det house a)) Ω) to)) (BIND everybody Ω)) .\nWith the following substitution entries, wants, buy ∈ EVENT; everybody, house ∈ ENTITY; a, to ∈ FUNCTIONAL; Ω = λx.EQ(x,ω); nsubj= λ f gx.∃y. f (x)∧g(y)∧ arg1(xe,ya); dobj= λ f gx.∃y. f (x)∧g(y)∧ arg2(xe,ya); xcomp= λ f gx.∃y. f (x)∧g(y)∧xcomp(xe,ya); mark ∈ HEAD; BIND ∈ MERGE,\nthe lambda expression after composition becomes: λz. ∃xywv.wants(ze)∧ everybody(xa)∧ arg1(ze,xa) ∧ EQ(x,ω)∧buy(ye)∧xcomp(ze,ye)∧ arg1(ye,va) ∧ EQ(v,ω)∧ arg1(xe,ya)∧house(wa)∧ arg2(ye,wa) .\nThis expression encodes the fact that x and v are in unification, and can thus be further simplified to: (3) λz.∃xyw.wants(ze)∧ everybody(xa)∧ arg1(ze,xa)\n∧ buy(ye)∧xcomp(ze,ye)∧ arg1(ye,xa) ∧ arg1(xe,ya)∧house(wa)∧ arg2(ye,wa) .\nHowever, the logical form (3) differs from the desired form (1). As noted above, UDEPLAMBDA with its default type, where each s-expression must have the type η = Ind×Event→ Bool, cannot handle quantifier scoping."
    }, {
      "heading" : "1.2 With Higher-order Type System",
      "text" : "Following Champollion (2010), we make a slight modification to the type system. Instead of using expressions of the form λx. . . . for words, we use either λ f .∃x. . . . or λ f .∀x. . . ., where f has type η. As argued by Champollion, this higher-order form makes quantification and negation handling sound and simpler in Neo-Davidsonian event semantics. Following this change, we assign the following lambda expressions to the words in our example sentence: everybody = λ f .∀x.person(x)→ f (x) ; wants = λ f .∃x.wants(xe)∧ f (x) ; to = λ f .TRUE ; buy = λ f .∃x.buy(xe)∧ f (x) ; a = λ f .TRUE ; house = λ f .∃x.house(xa)∧ f (x) ; Ω = λ f . f (ω) .\nHere everybody is assigned universal quantifier semantics. Since the UD representation does not distinguish quantifiers, we need to rely on a small (language-specific) lexicon to identify these. To encode quantification scope, we enhance the label nsubj to nsubj:univ, which indicates that the subject argument of wants contains a universal quantifier, as shown in Figure 1(c).\nThis change of semantic type for words and sexpressions forces us to also modify the semantic type of dependency labels, in order to obey the single-type constraint of DEPLAMBDA (Reddy et al., 2016). Thus, dependency labels will now take the form λPQ f . . . ., where P is the parent expression, Q is the child expression, and the return expression is of the form λ f . . . .. Following this change, we assign the following lambda expressions to dependency labels: nsubj:univ= λPQ f .Q(λy.P(λx. f (x)∧ arg1(xe,ya))) ; nsubj= λPQ f .P(λx. f (x)∧Q(λy.arg1(xe,ya))) ; dobj= λPQ f .P(λx. f (x)∧Q(λy.arg2(xe,ya))) ; xcomp= λPQ f .P(λx. f (x)∧Q(λy.xcomp(xe,ya))) ; det, mark= λPQ f .P( f ) ; BIND = λPQ f .P(λx. f (x)∧Q(λy.EQ(y,x))) .\nNotice that the lambda expression of nsubj:univ differs from nsubj. In the former, the lambda variables inside Q have wider scope over the variables in P (i.e. the universal quantifier variable of everybody has scope over the event variable of wants) contrary to the latter.\nThe new s-expression for Figure 1(c) is (nsubj:univ (xcomp wants (mark\n(nsubj (dobj buy (det house a)) Ω) to)) (BIND everybody Ω)) .\nSubstituting with the modified expressions, and performing composition and simplification leads to the expression: (6) λ f .∀x .person(xa)→\n[∃zyw. f (z)∧wants(ze)∧ arg1(ze,xa)∧buy(ye) ∧ xcomp(ze,ye)∧ house(wa) ∧ arg1(ze,xa)∧ arg2(ze,wa)] .\nThis expression is identical to (1) except for the outermost term λ f . By applying (6) to λx.TRUE, we obtain (1), which completes the treatment of universal quantification in UDEPLAMBDA."
    } ],
    "references" : [ {
      "title" : "Quantification and negation in event semantics",
      "author" : [ "Lucas Champollion." ],
      "venue" : "Baltic International Yearbook of Cognition, Logic and Communication 6(1):3.",
      "citeRegEx" : "Champollion.,? 2010",
      "shortCiteRegEx" : "Champollion.",
      "year" : 2010
    }, {
      "title" : "Universal Dependencies to Logical Forms with Negation Scope",
      "author" : [ "Federico Fancellu", "Siva Reddy", "Adam Lopez", "Bonnie Webber." ],
      "venue" : "arXiv Preprint .",
      "citeRegEx" : "Fancellu et al\\.,? 2017",
      "shortCiteRegEx" : "Fancellu et al\\.",
      "year" : 2017
    }, {
      "title" : "Transforming Dependency Structures to Logical Forms for Semantic Parsing",
      "author" : [ "Siva Reddy", "Oscar Täckström", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata." ],
      "venue" : "Transactions of the Association for Computational",
      "citeRegEx" : "Reddy et al\\.,? 2016",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2016
    }, {
      "title" : "Enhanced English Universal Dependencies",
      "author" : [ "Sebastian Schuster", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Schuster and Manning.,? \\Q2016\\E",
      "shortCiteRegEx" : "Schuster and Manning.",
      "year" : 2016
    }, {
      "title" : "Taking Scope - The Natural Semantics of Quantifiers",
      "author" : [ "Mark Steedman." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Steedman.,? 2012",
      "shortCiteRegEx" : "Steedman.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Our framework is based on DEPLAMBDA (Reddy et al., 2016) a recently developed method that converts English Stanford Dependencies (SD) to logical forms.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "Before describing UDEPLAMBDA, we provide an overview of DEPLAMBDA (Reddy et al., 2016) on which our approach is based.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "Both Schuster and Manning (2016) and Nivre et al.",
      "startOffset" : 5,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "Both Schuster and Manning (2016) and Nivre et al. (2016) note the necessity of an enhanced UD representation to enable semantic applications.",
      "startOffset" : 5,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "We follow Schuster and Manning (2016) and find these using the labels acl (relative) and xcomp (control).",
      "startOffset" : 10,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "To obtain the correct coordination scope, we refine conj to conj:verb, conj:vp, conj:sentence, conj:np, and conj:adj, similar to Reddy et al. (2016). Finally, unlike the PTB tags (Marcus et al.",
      "startOffset" : 129,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "We use GRAPHPARSER (Reddy et al., 2016) to map these logical forms to their grounded Freebase graphs, via corresponding ungrounded graphs.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "Entity Resolution We follow Reddy et al. (2016) and resolve entities in three steps: (1) potential entity spans are identified using seven handcrafted part-of-speech patterns; (2) each span is associated with potential Freebase entities according to the Freebase/KG API; and (3) the 10-best entity linking lattices, scored by a structured perceptron, are",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "Features We use features similar to Reddy et al. (2016): basic features of words and Freebase relations, and graph features crossing ungrounded events with grounded relations, ungrounded types with grounded relations, and ungrounded answer type crossed with a binary feature indicating if the answer is a number.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "Features We use features similar to Reddy et al. (2016): basic features of words and Freebase relations, and graph features crossing ungrounded events with grounded relations, ungrounded types with grounded relations, and ungrounded answer type crossed with a binary feature indicating if the answer is a number. In addition, we add features encoding the semantic similarity of ungrounded events and Freebase relations. Specifically, we used the cosine similarity of the translation-invariant embeddings of Huang et al. (2015).7",
      "startOffset" : 36,
      "endOffset" : 527
    }, {
      "referenceID" : 2,
      "context" : "CCGGRAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "7 DEPLAMBDA (Reddy et al., 2016) – 50.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al.",
      "startOffset" : 141,
      "endOffset" : 270
    } ],
    "year" : 2017,
    "abstractText" : "Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control. In this work, we introduce UDEPLAMBDA, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs. We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDEPLAMBDA outperforms strong baselines across languages and datasets. For English, it achieves a 4.9 F1 point improvement over the state-of-the-art on GraphQuestions.",
    "creator" : "LaTeX with hyperref package"
  }
}