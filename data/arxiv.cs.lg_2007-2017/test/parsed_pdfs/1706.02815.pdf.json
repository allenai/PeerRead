{
  "name" : "1706.02815.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "From Bayesian Sparsity to Gated Recurrent Nets",
    "authors" : [ "Hao He", "Bo Xin" ],
    "emails" : [ "hehaodele@pku.edu.cn", "jimxinbo@gmail.com", "davidwipf@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many practical iterative algorithms for minimizing an energy function Ly(x), parameterized by some vector y, adopt the updating prescription\nx(t+1) = f(Ax(t) +By), (1)\nwhere t is the iteration count, A and B are fixed matrices/filters, and f is a point-wise nonlinear operator. When we treat By as a bias or exogenous input, then the progression of these iterations through time resembles activations passing through the layers (indexed by t) of a deep neural network (DNN) [23, 35, 39, 2]. It then naturally begs the question: If we have access to an ensemble of pairs {y,x∗}, where x∗ = arg minx Ly(x), can we train an appropriately structured DNN to produce a minimum of Ly(x) when presented with an arbitrary new y as input? IfA andB are fixed for all t, this process can be interpreted as training a recurrent neural network (RNN), while if they vary, a deep feedforward network with independent weights on each layer is a more apt description.\nAlthough many of our conclusions may ultimately have broader implications, in this work we focus on minimizing the ubiquitous sparse estimation problem\nLy(x) = ‖y −Φx‖22 + λ‖x‖0, (2) where Φ ∈ Rn×m is an overcomplete matrix of feature vectors, ‖ · ‖0 is the `0 norm equal to a count of the nonzero elements in a vector, and λ > 0 is a trade-off parameter. Although crucial to many\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 6.\n02 81\n5v 1\n[ cs\n.L G\n] 9\nJ un\n2 01\napplications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought. Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].\nIn most cases, these approximate algorithms can be implemented via (1), where A and B are functions of Φ, and the nonlinearity f is, for example, a hard-thresholding operator for IHT or soft-thresholding for convex relaxations. However, the Achilles’ heel of all these approaches is that they will generally not converge to good approximate minimizers of (2) if Φ has columns with a high degree of correlation [6, 11], which is unfortunately often the case in practice [40].\nTo mitigate the effects of such correlations, we could leverage the aforementioned correspondence with common DNN structures to learn something like a correlation-invariant algorithm or update rules [2], although in this scenario our starting point would be an algorithmic format with known deficiencies. But if our ultimate goal is to learn a new sparse estimation algorithm that efficiently compensates for structure in Φ, then it seems reasonable to invoke iterative algorithms known a priori to handle such correlations directly as our template for learned network layers. One important example is sparse Bayesian learning (SBL) [38], which has been shown to solve (2) using a principled, multiloop majorization-minimization approach [25] even in cases where Φ displays strong correlations [40].1 Herein we demonstrate that, when judiciously unfolded, SBL iterations can be formed into variants of long short-term memory (LSTM) cells, one of the more popular recurrent deep neural network architectures [24], or gated extensions thereof [15]. The resulting network dramatically outperforms existing methods in solving (2) with a minimal computational budget. Our high-level contributions can be summarized as follows:\n• Quite surprisingly, we demonstrate that the SBL objective, which explicitly compensates for correlated dictionaries, can be optimized using iteration structures that map directly to popular LSTM cells despite its radically different origin. This association significantly broadens recent work associating elementary, one-step iterative sparsity algorithms like (1) with simple recurrent or feedforward deep network architectures [23, 35, 39, 2].\n• At its core, any SBL algorithm requires coordinating inner- and outer-loop computations that produce expensive latent posterior variances (or related, derived quantities) and optimized coefficient estimates respectively. Although this process can in principle be accommodated via canonical LSTM cells, such an implementation will enforce that computation of latent variables rigidly map to predefined subnetworks corresponding with various gating structures, ultimately administering a fixed schedule of switching between loops. To provide greater flexibility in coordinating inner- and outer-loops, we propose a richer gated-feedback LSTM structure for sparse estimation.\n• We achieve state-of-the-art performance on several empirical tasks, including direction-ofarrival (DOA) estimation [32] and 3D geometry recovery via photometric stereo [43]. In these and other cases, our approach produces higher accuracy estimates at a fraction of the computational budget. These results are facilitated by a novel online data generation process.\n• Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25]. We envision that such a strategy can have wide-ranging implications beyond the sparse estimation problems explored herein given that it is often not obvious how to optimally tune loop execution to balance both complexity and estimation accuracy in practice."
    }, {
      "heading" : "2 Connecting SBL and LSTM Networks",
      "text" : "This section first reviews the basic SBL model, followed an algorithmic characterization of how correlation structure can be handled during sparse estimation. Later we derive specialized SBL update rules that reveal a close association with LSTM cells.\n1Note also that a recent interesting modification of approximate message passing [34] (or its unfolded, trainable deep analog that converges to the same solution [9]), can handle certain specialized forms of dictionary correlation; however, the approach does not work with the types of strong arbitrary/unconstrained correlation we consider in this work."
    }, {
      "heading" : "2.1 Original SBL Model",
      "text" : "Given an observed vector y ∈ Rn and feature dictionary Φ ∈ Rn×m, SBL assumes the Gaussian likelihood model and a parameterized zero-mean Gaussian prior for the unknown coefficients x ∈ Rm given by\np(y|x) ∝ exp [ − 12λ ‖y −Φx‖ 2 2 ] and p(x;γ) ∝ exp [ − 12x>Γ −1x ] , Γ , diag[γ], (3)\nwhere λ > 0 is a fixed variance factor and γ denotes a vector of unknown hyperparamters [38]. Because both likelihood and prior are Gaussian, the posterior p(x|y;γ) is also Gaussian, with mean x̂ satisfying x̂ = ΓΦ>Σ−1y y, with Σy , ΦΓΦ> + λI. (4) Given the lefthand-side multiplication by Γ in (4), x̂ will have a matching sparsity profile or support pattern as γ, meaning that the locations of zero-valued elements will align or supp[x̂] = supp[γ]. Ultimately then, the SBL strategy shifts from directly searching for some optimally sparse x̂, to an optimally sparse γ. For this purpose we marginalize over x and then maximize the resulting type-II likelihood function with respect to γ [30]. Conveniently, the resulting convolution-of-Gaussians integral is available in closed-form [38] such that we can equivalently minimize the negative loglikelihood\nL(γ) = − log ∫ p(y|x)p(x;γ)dx ≡ y>Σ−1y y + log |Σy|. (5)\nGiven an optimal γ so obtained, we can compute the posterior mean estimator x̂ via (4). Equivalently, this same posterior mean estimator can be obtained by an iterative reweighted `1 process described next that exposes subtle yet potent sparsity-promotion mechanisms."
    }, {
      "heading" : "2.2 Iterative Reweighted `1 Implementation",
      "text" : "Although not originally derived this way, SBL can be implemented using a modified form of iterative reweighted `1-norm optimization that exposes its agency for producing sparse estimates. In general, if we replace the `0 norm from (2) with any smooth approximation g(|x|), where g is a concave, non-decreasing function and | · | applies elementwise, then cost function descent2 can be guaranteed using iterations of the form [41]\nx(t+1) ← arg minx ‖y −Φx‖22 + λ ∑ i w (t) i |xi|, w (t+1) i ← ∂g(u)/∂ui|ui= ∣∣∣x(t+1)i ∣∣∣, ∀i.\n(6) This process can be viewed as a multi-loop, majorization-minimization algorithm [25] (a generalization of the EM algorithm [18]), whereby the inner-loop involves computing x(t+1) by minimizing a first-order, upper-bounding approximation ‖y−Φx‖22 + λ ∑ i w (t) i |xi|, while the outer-loop updates the bound/majorizer itself as parameterized by the weights w(t+1). Obviously, if g(u) = u, then w(t) = 1 for all t, and (6) reduces to the Lasso objective for `1 norm regularized sparse regression [37], and only a single iteration is required. However, one popular non-trivial instantiation of this approach assumes g(u) = ∑ i log (ui + ) with > 0 a user-defined parameter [13]. The corre-\nsponding weights then become w(t+1)i = (∣∣∣x(t+1)i ∣∣∣+ )−1 , and we observe that once any particular\nx (t+1) i becomes large, the corresponding weight becomes small and at the next iteration a weaker penalty will be applied. This prevents the overshrinkage of large coefficients, a well-known criticism of `1 norm penalties [19].\nIn the context of SBL, there is no closed-form w(t+1)i update except in special cases. However, if we allow for additional latent structure, which we later show is akin to the memory unit of LSTM cells, a viable recurrency emerges for computing these weights and elucidating their effectiveness in dealing with correlated dictionaries. In particular we have:\nProposition 1. If weights w(t+1) satisfy ( w\n(t+1) i )2 = minz:supp[z]⊆supp[γ(t)] 1 λ‖φi −Φz‖22 + ∑ j∈supp[γ(t)] z2j\nγ (t+1) j\n(7)\n2Or global convergence to some stationary point with mild additional assumptions [36].\nfor all i, then the iterations (6), with γ(t+1)j = [ w (t) j ]−1/2 ∣∣∣x(t+1)j ∣∣∣, are guaranteed to reduce or\nleave unchanged the SBL objective (5). Also, at each iteration, γ(t+1) and x(t+1) will satisfy (4).\nUnlike the traditional sparsity penalty mentioned above, with SBL we see that the i-th weight w\n(t+1) i is not dependent solely on the value of the i-th coefficient x (t+1) i , but rather on all the latent hyperparameters γ(t+1) and therefore ultimately prior-iteration weights w(t) as well. Moreover, because the fate of each sparse coefficient is linked together, correlation structure can be properly accounted for in a progressive fashion.\nMore concretely, from (7) it is immediately apparent that if φi ≈ φi′ for some indeces i and i′ (meaning a large degree of correlation), then it is highly likely that w(t+1)i ≈ w (t+1) i′ . This is simply because the regularized residual error that emerges from solving (7) will tend to be quite similar when φi ≈ φi′ . In this situation, a suboptimal solution will not be prematurely enforced by weights with large, spurious variance across a correlated group of basis vectors. Instead, weights will differ substantially only when the corresponding columns have meaningful differences relative to the dictionary as a whole, in which case such differences can help to avoid overshrinkage as before.\nA crucial exception to this perspective occurs when γ(t+1) is highly sparse, or nearly so, in which case there are limited degrees of freedom with which to model even small differences in each φi. However, such cases can generally only occur when we are in the neighborhood of ideal, maximally sparse solutions by definition [40], when different weights are actually desirable even among correlated columns for resolving the final sparse estimates."
    }, {
      "heading" : "2.3 Revised SBL Iterations",
      "text" : "The iterative reweighted `1 formulation of SBL, as elucidated as we have done through (7), can no longer be implemented via the simple recurrent structures used in the past, e.g., like (1) for learning optimal IHT or other sparsity promoting iterations. Instead, the additional latent dependencies that arise through w, x, and γ, which allow SBL to incrementally store or update learned representations (akin to incrementally accruing an optimal sparsity profile), require a more sophisticated network architecture to actualize.\nAlthough presumably there are multiple ways such an architecture could be developed, in this section we derive specialized SBL iterations that will directly map to one of the most common RNN structures, namely LSTM networks. With this in mind, the notation we adopt has been intentionally chosen to facilitate later association with LSTM cells. We first define\nw(t) , diag [ Φ> ( λI + ΦΓ(t)Φ> )−1 Φ ] 1 2 and ν(t) , u(t) + µΦ> ( y −Φu(t) ) , (8)\nwhere Γ(t) , diag [ γ(t) ] , u(t) , Γ(t)Φ> ( λI + ΦΓ(t)Φ> )−1 y, and µ > 0 is a constant. As\nwill be discussed further below,w(t) serves the exact same role as the weights from (7), hence the identical notation. We then partition our revised SBL iterations as so-called gate updates\nσ (t+1) in ←\n[ α ( γ(t) ) (∣∣∣ν(t) ∣∣∣− 2λw(t) )]\n+ , σ\n(t+1) f ← β\n( γ(t) ) , σ\n(t+1) out ←\n( w(t) )−1 ,\n(9) cell updates\nx̄(t+1) ← sign [ ν(t) ] , x(t+1) ← σ(t)f x(t) + σ (t) in x̄(t+1), (10)\nand output update γ(t+1) ← σ(t)out ∣∣∣x(t+1) ∣∣∣ , (11)\nwhere the inverse and absolute-value operators are applied element-wise when a vector is the argument, and at least for now, α and β define arbitrary functions. Moreover, denotes the Hadamard product and [·]+ sets negative values to zero and leaves positive quantities unchanged, also in an element-wise fashion, i.e., it acts just like a rectilinear (ReLU) unit [33]. Note also that the gate and cell updates in isolation can be viewed as computing a first-order, partial solution to the inner-loop weighted `1 optimization problem from (6).\nStarting from some initial γ(0) and x(0), we will demonstrate in the next section that these computations closely mirror a canonical LSTM network unfolded in time with y acting as a constant input applied at each step. Before doing so however, we must first demonstrate that (8)−(11) indeed serve to reduce the SBL objective. For this purpose we require the following definition: Definition 2. We say that the iterations (9)−(11) satisfy the monotone cell update property if\n‖y −Φu(t)‖22 + 2λ ∑ i w (t) i |u (t) i | ≥ ‖y −Φx(t+1)‖22 + 2λ ∑ i w (t) i |x (t+1) i |, ∀t. (12)\nNote that for rather inconsequential technical reasons this definition involves u(t), which can be viewed as a proxy for x(t). We then have the following: Proposition 3. The iterations (8)−(11) will reduce or leave unchanged (5) for all t provided that µ ∈ ( 0, λ/ ∥∥∥Φ>Φ ∥∥∥ ] and α and β are chosen such that the monotone cell update property holds.\nIn practical terms, the simple selections α(γ) = 1 and β(γ) = 0 will provably satisfy the monotone cell update property (see proof details in Appendix E). However, for additional flexibility, α and β could be selected to implement various forms of momentum, ultimately leading to cell updates akin to the popular FISTA [5] or monotonic FISTA [4] algorithms. In both cases, old values x(t) are precisely mixed with new factors σ(t+1)in x̄(t+1) to speed convergence. Of course the whole point of casting the SBL iterations as an RNN structure to begin with is so that we may ultimately learn these types of functions, without the need for hand-crafting suboptimal iterations up front."
    }, {
      "heading" : "2.4 Correspondences with LSTM Components",
      "text" : "This section will flesh out how the SBL iterations presented in Section 2.3 display the same structure as a canonical LSTM cell, the only differences being the shape of the nonlinearities, and the exact details of the gate subnetworks. To facilitate this objective, Figure 1 contains a canonical LSTM network structure annotated with SBL-derived quantities. We now walk through these correspondences.\nFirst, the exogenous input to the network is the observation vector y, which does not change from time-step to time-step. This is much like the strategy used by feedback networks for obtaining incrementally refined representations [46]. The output at time-step t is γ(t), which serves as the current estimate of the SBL hyperparameters. In contrast, we treat x(t) as the internal LSTM memory cell, or the latent cell state.3 This deference to γ(t) mirrors the emphasis SBL places on learning variances while treating x as hidden data, and in some sense flips the coefficient-centric script used in producing (6).\n3If we allow for peephole connections [21], it is possible to reverse these roles; however, for simplicity and the most direct mapping to LSTM cells we do not pursue this alternative here.\nProceeding further, γ(t) is fed to four separate layers/subnetworks (represented by yellow boxes in Figure 1): (i) the forget gate σf , (ii) the input gate σin, (iii) the output gate σout, and (iv) the candidate input update x̄. The forget gate computes scaling factors for each element of x(t), with small values of the gate output suggesting that we ‘forget’ the corresponding old cell state elements. Similarly the input gate determines how large we rescale signals from the candidate input update. These two re-weighted quantities are then mixed together to form the new cell state x(t+1). Finally, the output gate modulates how new γ(t+1) are created as scaled versions of the updated cell state.\nRegarding details of these four subnetworks, based on the update templates from (9) and (10), we immediately observe that the required quantities depend directly on (8). Fortunately, both ν(t) and w(t) can be naturally computed using simple feedforward subnetwork structures.4 These values can either be computed in full (ideal case), or partially to reduce the computational burden. In any event, once obtained, the respective gates and candidate cell input updates can be computed by applying final non-linearities. Note that α and β are treated as arbitrary subnetwork structures at this point that can be learned.\nA few cosmetic differences remain between this SBL implementation and a canonical LSTM network. First, the final non-linearity for LSTM gating subnetworks is often a sigmoidal activation, whereas SBL is flexible with the forget gate (via β), while effectively using a ReLU unit for the input gate and an inverse function for the output gate. Moreover, for the candidate cell update subnetwork, SBL replaces the typical tanh nonlinearity with a quantized version, the sign function, and likewise, for the output nonlinearity an absolute value operator (abs) is used. Finally, in terms of internal subnetwork structure, there is some parameter sharing given that σin, σout, and x̄ are connected via ν and w.\nOf course in all cases we need not necessarily share parameters nor abide by these exact structures. In fact there is nothing inherently optimal about the particular choices used by SBL; rather it is merely that these structures happen to reproduce the successful, yet hand-crafted SBL iterations. But certainly there is potential in replacing such iterations with learned LSTM-like surrogates, at least when provided with access to sufficient training data as in prior attempts to learn sparse estimation algorithms [23, 39, 2]."
    }, {
      "heading" : "3 The Dynamics of SBL Iterations",
      "text" : "Although SBL iterations can be molded into an LSTM structure as we have shown, there remain hints that the full potential of this association may be presently undercooked. Here we empirically examine the trajectories of SBL iterations produced via the rules derived in Section 2.3. This process will unmask certain characteristic dynamics suggestive of a richer class of recurrent network structures, inspired by sequence prediction tasks [15], to be analyzed later in Section 4 and empirically tested in Section 5."
    }, {
      "heading" : "3.1 Large Timescale Differences",
      "text" : "We first present a synthetic experiment that highlights the different time scales upon which SBL latent variables may fluctuate over the course of a typical optimization trajectory. The experimental design is as follows: First we generate a dictionary Φ via\nΦ = Φ̃BD, (13)\nwhere Φ̃ ∈ R50×100 has iid elements drawn fromN (0, 1);B ∈ R100×100 is a block-diagonal matrix with 20, 5× 5 blocks, each with unit diagonals and off-diagonals set to 0.9; and D ∈ R100×100 is a fully diagonal matrix that re-scales each column of the final Φ to have unit `2 norm, and finally multiplies by a random sign pattern. This process ensures that Φ will encompass 20 clusters of 5 adjacent columns each, with strong correlations introduced viaB. We then generate a sparse random vector x∗ ∈ R100 such that ‖x∗‖0 = 10, where the nonzero positions are randomly aligned with 10 different clusters, and the nonzero values have unit magnitude. We next compute y = Φx∗ and apply the revised SBL iterations from Section 2.3 with λ = 0.01 (or a rather arbitrary small value), α(γ) = 1, and β(γ) = 0.\n4Forw(t) the result of Proposition 1 suggests that these weights can be computed as the solution of a simple regularized regression problem, which can easily be replaced with a small network analogous to that used in [21]; similarly for ν.\nFigure 2 displays the trajectories of bothw(t) (left subplot) and x(t) (right subplot) for t = 1, . . . , 100 during execution of (8)−(11). As mentioned previously, the weights w(t) serve to incrementally focus a sequence of `1 minimization problems towards likely nonzero elements of x∗ via the process defined by (7). Unlike other existing iterative reweighted `1 approaches, with SBL these weights quickly (within just a few iterations) partition into two groups, one with smaller values near 1.0, the other with larger values in the 8-10 range (see left subplot).\nMoreover, upon closer examination we found that the index i of all weights w(t)i with a value near 1.0 correspond with dictionary columns φi in a cluster where some x ∗ j 6= 0. In contrast, all weights with a large value are associated with dictionary columns in clusters where all x∗j = 0. Consequently, these weights reflect in some sense the correct support at the cluster level, and introduce a more severe penalty to coefficients associated with what should ideally be inactive clusters. This then allows subsequent `1 iterations to more narrowly learn the correct support pattern within these favored clusters, providing empirical support to the arguments made in Section 2 for the efficacy of SBL in dealing with correlated dictionary structure.\nHowever, the secondary learning of final coefficient values within the correct clusters occurs at a radically different time scale as shown in the right subplot of Figure 2. Here we observe that even after 50 iterations it is still not clear to which final value the coefficient magnitudes x(t)i will converge too, for example, 0.0 or 1.0. Therefore we may conclude that, although the weights w(t) may rather quickly proceed to values that reflect the correlation structure of the dictionary, the final coefficient estimates take much longer to resolve. Moreover, during this time, to be effective the iterations must ‘remember’ the correct value of w(t), even if continued updates are not necessary after rapid initial convergence."
    }, {
      "heading" : "3.2 The Potential Value of an Adaptive Updating Schedule",
      "text" : "Although the previous experiment served to expose the differing scales of subsets of latent variables, it did not provide any indication of how these different scales may actually impact final estimation accuracy. For example, suppose we were able to speed up the convergence of x for any fixed value of w, would this improve the overall performance? The present simulation directly addresses this issue.\nWe begin with a similar experimental design as used in Section 3.1, although we reduce the dimensions for visualization purposes. In brief, we choose Φ ∈ R10×20, formed from 10 clusters of size 2 each, and ‖x∗‖0 = 3. Figure 3(a) displays the optimal support pattern, whereby a ‘1’ indicates the location\nof a true nonzero, and a zero otherwise. Without loss of generality, we have also reordered the dictionary columns such that the first three columns serve as nonzero locations.\nWe display recovery results using a modified version of the SBL implementation from Section 2, whereby the gate and cell update steps, which are associated with the weighted `1 norm problem from (6), are applied in varying number K of inner-loop iterations. More specifically, with α(γ) = 1, and β(γ) = 0 andw(t) fixed, these additional, reduced inner-loop iterations consist of simply computing\nν(t,k) ← x(t+1,k) + µΦ> ( y −Φx(t+1,k) )\nσ (t,k) in ← [∣∣∣ν(t,k) ∣∣∣− 2λw(t) ] +\nx̄(t+1,k) ← sign [ ν(t,k) ] (14)\nx(t+1,k+1) ← σ(t,k)in x̄(t+1,k),\nfrom k = 1, . . . ,K, where x(t+1,1) , u(t). For any fixed w(t), these iterations are guaranteed to converge to a minimum of (6), and in light of the experiment from Section 3.1, can be viewed as a direct way of constricting or shrinking the x-axis of Figure 2(right). Indeed, for sufficiently large K, oncew has converged, x will immediately follow. But is this necessarily a desirable course of action?\nSubplots (b)−(e) of Figure 3 show the support patterns of the x̂ estimate obtained via this procedure using K ∈ {1, 10, 100, 1000}. Only the K = 100 case produces a perfect recovery with matching support. It therefore follows that inner-loop iterations, when interpreted as a tunable sequence, have the potential to improve performance. Of course in advance we have no way of knowing what the best K might be. But at least we do know that the K = 1 case which emerges from the original LSTM template need not be optimal."
    }, {
      "heading" : "4 Extension to Gated Feedback Networks",
      "text" : "The discrepancy in convergence rates described in Section 3.1 occurs in part because the gate and cell updates do not fully solve the inner-loop weighted `1 optimization needed to compute a globally optimal x(t+1) givenw(t) (see Section 2.3). Varying the number of inner-loop iterations, meaning additional executions of (8)−(11) withw(t) fixed, is one heuristic for normalizing across different trajectory frequencies, but this requires additional computational overhead, and prior knowledge is needed to micro-manage iteration counts for either efficiency or final estimation quality, the latter sensitivity being exposed in Section 3.2. For example, navigating around suboptimal local minima could require adaptively adjusting the number inner-loop iterations in subtle, non-obvious ways, with no discernible rule of thumb for enhancing solution quality. We therefore arrive at an unresolved state of affairs:\n1. The latent variables which define SBL iterations can potentially follow optimization trajectories with radically different time scales, or both long- and short-term dependencies.\n2. But there is no intrinsic mechanism within the SBL framework itself for automatically calibrating the differing time scales for optimal performance.5\nThese same issues are likely to arise in other non-convex multi-loop optimization algorithms as well. It therefore behooves us to consider a broader family of model structures that can adapt these scales in a data-dependent fashion.\nIn addressing this fundamental problem, we make the following key observation: If the trajectories of various latent variables can be interpreted as activations passing through an RNN with both long- and short-term dependencies, then in developing a pipeline for optimizing such trajectories it makes sense to consider learning deep architectures explicitly designed to adaptively model such characteristic sequences. Interestingly, in the context of sequence prediction, the clockwork RNN (CW-RNN) has been proposed to cope with temporal dependencies engaged across multiple scales [29]. As shown next however, the CW-RNN enforces dynamics synced to pre-determined clock rates exactly analogous to the fixed, manual schedule for terminating inner-loops (i.e., choosing K) in existing multi-loop iterative algorithms such as SBL. So we are back at our starting point. However, later in Section 4.2 we consider an alternative architecture to automate this process."
    }, {
      "heading" : "4.1 Clockwork Networks and Fixed Inner-Loop Iterations",
      "text" : "In the context of sequence prediction, the clockwork recurrent neural network (CW-RNN) has been proposed to cope with temporal dependencies engaged across multiple scales [29]. In its most basic form, the CW-RNN begins with input, hidden, and output layers which, just like a regular RNN, are defined by\nh(t+1) = fH ( WH · h(t) +W I · z(t) ) (15)\nv(t+1) = fO ( WO · h(t+1) ) , (16)\nwhere z(t) is an input vector at time t, h(t) represents hidden layer activations, v(t+1) the output, and {W I ,WH ,WO} are input, hidden, and output weight matrices respectively. Likewise, fH and fO are the corresponding nonlinear activation functions. What differentiates the CW-RNN from this vanilla structure, is thatW I andWH are each partitioned into g different temporarlly-varying block-rows6 as\nW I =   W (t) I1\n... W\n(t) Ig\n  , WH =   W (t) H1 ...\nW (t) Hg\n  , (17)\nwhich naturally defines a corresponding segmentation of the hidden variables as\nh(t) =   h\n(t) 1 ... h(t)g\n  (18)\nsuch that, assuming separable nonlinearities,\nh (t+1) i = fH\n( W\n(t) Hi · h(t) +W (t)Ii · z\n(t) ) , ∀i = 1, . . . , g. (19)\nAdditionally, each block is assigned and ‘update period’ Ti that governs the structure across each time step t via\nW (t) Ii =\n{ W̃ Ii for (t mod Ti) = 0\n[01, . . . ,0g] otherwise (20)\nand\nW (t) Hi =\n{ W̃Hi for (t mod Ti) = 0\n[01, . . . ,0i−1, I,0i+1, . . . ,0g] otherwise. (21)\n5Note that neither extreme, i.e., a single inner-loop iteration as with K = 1, or K sufficiently large for full inner-loop convergence, need necessarily be optimal per the results from 3.2.\n6A column-wise block structure may also be assumed if desired; however, this is not required for what follows herein.\nIn brief, these weight expressions ensure that for all i we have\nh (t+1) i =\n{ fH ( W̃Hi · h(t) + W̃ Ii · z(t) ) for (t mod Ti) = 0\nh (t) i otherwise.\n(22)\nThis formulation allows the CW-RNN to handle different temporal features by assigned different Ti to different blocks. For example, a block designed to model high-frequency dynamics may assume Ti = 1, while slowly-varying components can be captured using Ti 1. The latter implies that for most iterations, the block hidden state h(t)i is not updated allowing for hard-coded long-term memory of such low-frequency dynamics.\nThis prescription exactly reflects the basic anatomy of an algorithm with g nested loops, each loop being characterized by its own set of latent variables h(t)i . As a simple example, consider the SBL updates equipped with an inner-loop as in Section 3.2. If we define h(t)1 = x (t), h(t)2 = w (t), z(t) = y, adopt T1 = 1 and T2 = K, and relabel the iteration numbers via a single consistent index (i.e., we collapse k and t into a single index), thenw(t) will only be updated once every T2 time-steps, while x(t) will be updated at all t, and the basic scheduling is identical. The only difference is that the layer-wise filters and nonlinearities are somewhat more specialized for the SBL context."
    }, {
      "heading" : "4.2 Automatically Scheduling Inner-Loops via Gated Feedback",
      "text" : "The gated feedback RNN (GF-RNN) [15] was recently developed to update the CW-RNN with an additional set of gated connections that, in effect, allow the network to learn its own clock rates. In brief, the GF-RNN involves stacked LSTM layers (or somewhat simpler gated recurrent unit (GRU) layers [14]), that are permitted to communicate bilaterally via additional, data-dependent gates that can open and close on different time-scales. In the context of SBL, this means that we no longer need strain a specialized LSTM structure with the burden of coordinating trajectory dynamics. Instead, we can stack layers that are, at least from a conceptual standpoint, designed to reflect the different dynamics of disparate variable sets such as w(t) or x(t).\nIn doing so, we are then positioned to learn new SBL update rules from training pairs {y,x∗} as described previously. At the very least, this structure should include SBL-like iterations within its capacity, but of course it is also free to explore something even better. We describe our detailed implementation next."
    }, {
      "heading" : "4.3 Network Design and Training Protocol",
      "text" : "We stack two gated recurrent layers loosely designed to mimic the relatively fast SBL adaptation to basic correlation structure, and slower resolution of final support patterns and coefficient estimates. These layers are formed from a LSTM base architecture (or sometimes a GRU for comparison purposes). For the final output layer we adopt a multi-label classification loss for predicting supp[x∗], which is the well-known ‘NP-hard’ part of sparse estimation (determining final coefficient amplitudes just requires least squares). Full network details are deferred to Appendix A, including special modifications to handle complex data as required by DOA applications.\nFor a given dictionary Φ a separate network must be trained via SGD, to which we add a unique extra dimension of randomness via an online stochastic data-generation strategy. In particular, to create samples in each mini-batch, we first generate a vector x∗ with random support pattern and nonzero amplitudes. We then compute y = Φx∗ + , where is a small Gaussian noise component. This y forms a training input sample, while supp[x∗] represents the corresponding labels. For all mini-batches, novel samples are drawn, which we have found boosts performance considerably over the fixed training sets used by current DNN approaches to sparse estimation (see Appendix A)."
    }, {
      "heading" : "5 Experiments",
      "text" : "This section presents experiments involving synthetic data and two applications."
    }, {
      "heading" : "5.1 Evaluations via Synthetic Correlated Dictionaries",
      "text" : "To reproduce experiments from [2], we generate correlated synthetic features via Φ = ∑n i=1 1 i2uiv > i , where ui ∈ Rn and vi ∈ Rm are drawn iid from a unit Gaussian distribution, and each column of Φ is subsequently rescaled to unit `2 norm. Ground truth samples x∗ have d nonzero elements drawn randomly from U [−0.5, 0.5] excluding the interval [−0.1, 0.1]. We use n=20, m=100, and vary d, with larger values producing a much harder combinatorial estimation problem (exhaustive search is not feasible here). All algorithms are presented with y and attempt to estimate supp[x∗]. We evaluate using strict accuracy, meaning percentage of trials with exact support recovery, and loose accuracy, which quantifies the percentage of true positives among the top n ‘guesses’ (i.e., largest predicted outputs).\nFigures 4(a) and 4(b) evaluate our model, averaged across 105 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A). With regard to strict accuracy, only SBL is somewhat competitive with our approach and other learning-based models are much worse; however, using loose accuracy our method is far superior than all others. Note that this is the first approach we are aware of in the literature that can convincingly outperform SBL recovering sparse solutions when a heavily correlated dictionary is present, and we hypothesize that this is largely possible because our design principles were directly inspired by SBL itself.\nTo isolate architectural factors affecting performance we conducted ablation studies: (i) with or without gated feedback, (iii) LSTM or GRU cells, and (iii) small or large (4×) model size; for each model type, the small and respectively large versions have roughly the same number of parameters. Appendix D also contains a much broader set of self-comparison studies. Figure 4(c), which shows strict accuracy results with d/n = 0.4, indicates the importance of gated feedback and to a lesser degree network size, while LSTM and GRU cells perform similarly as expected."
    }, {
      "heading" : "5.2 Practical Application I: Direction-of-Arrival (DOA) Estimation",
      "text" : "DOA estimation is a fundamental problem in sonar/radar processing [32]. Given an array of n omnidirectional sensors with d signal waves impinging upon them, the objective is to estimate the angular direction of the wave sources with respect to the sensors. For certain array geometries and known propagation mediums, estimation of these angles can be mapped directly to solving (2) in the complex domain. In this scenario, each column of Φ represents the sensor array output (a point in Cn) from a hypothetical source with unit strength at angular location θi, and can be computed using wave progagation formula [32]. The entire dictionary can be constructed by concatenating columns associated with angles forming some spacing of interest, e.g., every 1◦ across a half circle, and will be highly correlated. Given measurements y ∈ Cn, we can solve (2), with λ reflecting the noise level. The indexes of nonzero elements of x∗ will then reveal the angular locations/directions of putative sources.\nRecently SBL-based algorithms have produced state-of-the-art results solving the DOA problem [17, 22, 45], and we compare our approach against SBL here. We apply a typical experimental design from the literature involving a uniform linear array with n = 10 sensors; see Appendix\nB for background and details on how to compute Φ and specifics on how to adapt and train our GFLSTM using complex data. Four sources are then placed in random angular locations, with nonzero coefficients at {±1± i}, and we compute measurements y = Φx∗ + , with chosen from a complex Gaussian distribution to produce different SNR. Because the nonzero positions in x∗ now have physical meaning, we apply the Chamfer distance [8] as the error metric, which quantifies how close we are to true source locations (lower is better). Figure 4(d) displays the results, where our learned network outperforms SBL across a range of SNR values."
    }, {
      "heading" : "5.3 Practical Application II: 3D Geometry Recovery via Photometric Stereo",
      "text" : "Photometric stereo represents another application domain whereby approximately solving (2) using SBL has recently produced state-of-the-art results [27]. The objective here is to recover the 3D surface normals of a given scene using r images taken from a single camera using different lighting conditions. Under the assumption that these images can be approximately decomposed into a diffuse Lambertian component and sparse corruptions such as shadows and specular highlights, then surface normals at each pixel can be recovered using (2) to isolate these sparse factors followed by a final least squares post-processing step [27]. In this context, Φ is constructed using the known camera and lighting geometry, and y represents intensity measurements for a given pixel across images projected onto the nullspace of a special transposed lighting matrix (see Appendix C for more details and our full experimental design). However, because a sparse regression problem must be computed for every pixel to recovery the full scene geometry, a fast, efficient solver is paramount.\nWe compare our GFLSTM model against both SBL and the MaxSparseNet [2] (both of which outperform other existing methods). Tests are performed using the 32-bit HDR gray-scale images of objects ‘Bunny’ (256× 256) and ‘Caesar’ (300× 400) as in [27]. For (very) weakly-supervised training data, we apply the same approach as before, only we use nonzero magnitudes drawn from a Gaussian, with mean and variance loosely tuned to the photometric stereo data, consistent with [2]. Results are shown in Table 1, where we observe in all cases the DNN models are faster by a wide margin, and in the hard cases cases (smaller r) our approach produces the lowest angular error. The only exception is with r = 40; however, this is a quite easy scenario with so many images such that SBL can readily find a near optimal solution, albeit at a high computational cost. See Appendix C for error surface visualizations."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have demonstrated that gated recurrent nets carefully patterned to reflect the multi-scale optimization trajectories of multi-loop SBL iterations can lead to a considerable boost in both accuracy and efficiency. Note that simpler first-order, gradient descent-style algorithms can be ineffective when applied to sparsity-promoting energy functions with a combinatorial number of bad local optima and highly concave or non-differentiable surfaces in the neighborhood of minima. Moreover, implementing smoother approximations such as SBL with gradient descent is impractical since each gradient calculation would be prohibitively expensive. Therefore, recent learning-to-learn approaches such as [1] that rely on gradient descent are difficult to apply in the present setting."
    }, {
      "heading" : "7 Appendix A: Modeling and Training Details",
      "text" : "We first describe the basic gated feedback RNN structure, following by our particular model architecture including extensions to handle complex data. We conclude with training details and experimental settings."
    }, {
      "heading" : "7.1 Gated Feedback RNN Structure",
      "text" : "The gated feedback RNN cell [15] is a key component of our model. Detailed computing flows for a gated feedback LSTM cell (GFLSTM), which represents one particular specialization that is used in all our experiments, follow as\nc (t) j = f (t) j c (t−1) j + i (t) j c̃ (t) j\nh (t) j = o (t) j Tanh(c (t) j )\ni (t) j = σ(W ija (t) j +U ijh (t−1) j )\nf (t) j = σ(W f ja (t) j +Uf jh (t−1) j )\no (t) j = σ(W oja (t) j +Uojh (t−1) j )\ng (t) i→j = σ(W gja (t) j +Ugi→jH (t−1))\nc̃ (t) j = Tanh(W cj−1→jh (t) j−1 +\nr∑\ni=1\ng (t) i→j U ci→jh (t−1) i ),\n(23)\nwhere r is the number of stacked LSTM cells, subscript j is the index for LSTM cell in the stack, while superscript (t) indicates the time point. Therefore h(t)j and c (t) j denote the hidden state and memory cell of the j-th LSTM unit in the stack at time t. And we denote a(t)j as the input of the j-th LSTM cell, such that a(t)j = h (t) j−1(∀j > 1) and a (t) 1 = y. Besides conventional designs like an input gate i(t)j , forget gate f (t) j , and output gate, o (t) j , the stack of GFLSTM cells also includes an extra global gate computed from input a(t)j andH (t−1) = [h(t−1)1 , ..,h (t−1) r ], the concatenation of all the hidden states from the previous time step t − 1. Each g(t)i→j controls the flow from h (t−1) i to h (t) j , that is, the layer cross feedback. To make it concise, we can denote the whole computing flow of these r LSTM cells using the function fGFLSTM as\nfGFLSTM (H (t−1),y;θGFLSTM ) = [q (t),H(t)]\nθGFLSTM = [W ij ,W f j ,W oj ,U ij ,Uf j ,Uoj ,W gj ,Ugi→j ,W cj−1→j ,U ci→j ]\nq(t) = h(t)r .\n(24)"
    }, {
      "heading" : "7.2 Proposed Model Architecture and Extensions",
      "text" : "Basic Model: Although our model consists of RNN cells, once we fix the number of unfolding steps, it essentially becomes a feed-forward network. As shown in Figure 5, during the forward stage, the input is broadcasted to the lowest RNN cell at each unrolled step. After the model generates its outputs at each unrolled step, all of these outputs will be concatenated and fed into a fully connected layer to produce the final prediction. Since we opt to predict the supp[x∗] = {i : x∗i 6= 0}, we view the problem as an multi-label classification task and append a softmax layer on top of the fully connected layer. We formalize this process as\n[q(t),H(t)] = frnn(H (t−1),y;θrnn)\np = fpred([q (1), q(2), .., q(T )];θpred)\nfpred(q (all),θpred) = softmax(W predq (all) + bpred)\nq(all) = [q(1), q(2), .., q(T )],\n(25)\nwhere θrnn,θpred = [W pred, bpred] are the parameters of the RNN units and the fully connected layer respectively. q(t), h(t) denote the output of the RNN units and hidden state at each time step t. In practice, we simply take the RNN’s top layer hidden states as its output q(t). frnn represents the forward process of the RNN, which is defined by the exact structure of the RNN-cell.\nComplex Value Extension: In many real applications of sparse recovery, the format of the inputs may vary. For example, the inputs to the DOA problem of interest are complex numbers. We propose\nto deal with complex-value inputs by what we call model complexification. Specifically, RNN units consist of matrix multiplication and non-linear activations, both of which have their complex value counterparts. Thus we propose to use complex-value operations in the RNN units before finally concatenating the real and imaginary part of the outputs as the feature for the final prediction. This method is inspired by SBL, which handles real and complex value inputs with the same operator. We argue that this method is better than simply concatenating the real and imaginary parts of the input and using a regular real-valued RNN (of double the size) for prediction, since in this way the links between real and imaginary parts of a complex number are broken and therefore the RNN may potentially have to learn these links by itself, which leads to unnecessary learning difficulties."
    }, {
      "heading" : "7.3 Training Details",
      "text" : "We apply a unified training framework for all different approaches. In our experiments, models are implemented using Torch7 and experiments are run on a single NVIDIA Tesla K40M GPU card.\nTraining Hyperparameters: To provide consistency with the concept of epoch from [2], our models are trained by 600000/250 = 2400 batches with batch size equal to 250. Typically, with 400 epochs (or 800 epochs in some extreme cases) of RMSprop optimization, we converge to a satisfactory performance level, with a default initial learning rate of 0.002, factored by 0.25 every 50 epochs after the first 250 epochs of training.\nModel Hyperparameters: As for model architecture, there exists the following hyperparameters: number of RNN hidden units h, number of stacked RNN layers r and number of RNN-cell unfold steps T . In almost all of our experiments, we control model capacity mainly by the size of hidden states with fixed layer number r = 2 and unfold steps T = 11. In section 10.2, we provide more detailed ablation studies on how the number of RNN layers, unroll steps and hidden units affects the performance.\nA Useful Training Heuristic: When training with a fixed-sized dataset, as existing learning approaches to sparse estimation do [23, 39, 2], there is always the risk of overfitting. The gap between the error on training and validation sets with a fixed dataset is shown via the blue curves in Figure 6(a)) on a representative learning problem. However, since we are free to generate as much training data as we want in the sparse estimation context, at every epoch we can always use a new, unseen batch. This simple strategy completely closes the gap (the red curves) with negligible computational overhead. Figure 6(b) displays the resulting improvement on performance, as measured by the percentage of trials whereby the entire support pattern is correctly estimated (i.e. strict accuracy)."
    }, {
      "heading" : "8 Appendix B: Experimental Details for Direction-of-Arrival (DOA) Estimation",
      "text" : "This appendix contains background information, followed by experimental and training details."
    }, {
      "heading" : "8.1 Background",
      "text" : "Direction-of-arrival (DOA) estimation for sonar and radar application can be formulated by the observation model\ny(t) = d∑\nk=1\nsk(t)f(θ ∗ k) + (t), (26)\nwhere y(t) ∈ Cn is the measured sonor/radar signal at time t, f : R → Cn, and d is number of source waveforms whose magnitudes are s(t) = [s1(t), ..., sd(t)]> ∈ Cd and angular locations are θ∗ = [θ∗1 , ..θ ∗ d] > [32]. Although the location space Θ might be continuous, we may approximate it by a fixed sampling grid θ = [θ1, .., θm]. Then the problem can be rewritten as the alternative observation model\ny(t) =\nm∑\ni=1\nxi(t)φi + (t) = Φx(t) + (t), (27)\nwhere Φ = [φ1, ..,φm], φi , f(θi), and x(t) = [x1(t), ..., xm(t)]>. With sufficient resolution provided, we assume every θ∗k is contained in θ such that s(t) becomes a collection of d non-zero entries inx(t). Finally, the DOA estimation problem boils down to solving minx ‖y−Φx‖22+λ‖x‖0, whereby nonzero elements in x∗ will (approximately) correspond with locations i whereby θi ≈ f(θ∗k) for some k."
    }, {
      "heading" : "8.2 Experimental Design",
      "text" : "We make some natural assumptions in our experiment. First we consider the narrowband, far-field case which implies that incoming waves are approximately planar and each source emanates from a single point. Furthermore, we assume our sensors are arranged having a linear, uniformly spaced array geometry, i.e., uniform linear array(ULA), and a known propagation medium. Then the measurement vector y(t) obtained by sensors at time t is given by (26), where the non-linear function f is\nf(θ) = [ eiω0∆1(θ), .., eiω0∆n(θ) ]> with\nω0∆j(θ) = 2π(j − 1) Dcos(θ)\nλ0 ,∀j = 1, .., n,\n(28)\nand ω0, λ0 are the central temporal frequency and the wavelength of signals respectively. Also, ∆j(θ) is the array-geometry-dependent time delay between the first sensor and the j-th sensor for a given angle θ ∈ [0, π], while D is the distance between two nearby sensors in the ULA. Settings: In our experiments, we set m = 180 allowing an angular resolution of 1◦ over the half circle, and n = 10 sensors with D = 0.5λ0. The dictionary Φ is constructed via (26) and (28) such that the i-th column represents the sensor array output from a hypothetical source of unit strength at angular location θi. The number of different sources d is set to 4, which is represents a quite challenging problem with only 10 sensors; most sparse estimation algorithms will fail in this regime. Then we randomly pick four different source directions with magnitudes {±1± i}. Finally, a measurement vector y = Φx+ is calculated with complex Gaussian noise added to maintain a given signal noise ratio (SNR).\nMetric: We apply the symmetric Chamfer distance[8] to evaluate the estimation quality with respect to the ground truth source directions. This distance is given by θ∗ = {θ∗1 , ..θ∗d} and the prediction θ̂ = {θ̂1, ..θ̂d}\ndist(θ∗, θ̂) = ∑\nθ1∈θ∗ min θ2∈ˆθ\n|θ1 − θ2|+ ∑\nθ2∈ˆθ\nmin θ1∈θ∗\n|θ1 − θ2|. (29)\nTraining Details: For DOA experiments, our model has LSTM cells with 200 hidden units and is trained 400 epoches following our default settings. For training data generation, we tried using noise levels in the intervals [15dB, 30dB], [20dB, 40dB], [30dB, 60dB], and [60dB, 80dB], and then chose the best results."
    }, {
      "heading" : "9 Appendix C: Experimental Details for 3D Geometry Recovery via Photometric Stereo",
      "text" : "This appendix describes our photometric stereo experiments in more detail, including error surface visualizations."
    }, {
      "heading" : "9.1 Background",
      "text" : "Photometric stereo represents a useful method for recovering high-resolution surface normals from a 3D scene using 2D images taken under r different lighting conditions. One proposed model for the observation process at a single pixel is\no = ρLn+ e, (30)\nwhere the r measurments are denoted o ∈ Rr, n ∈ R3 denotes the true 3D surface normal, rows of L ∈ Rr×3 define lighting directions, ρ is the diffuse albedo, acting here as a scalar multiplier, and e represents an aggregations of shadows, specular highlights, or other corrupting influences [28, 44]. If e were not present, then the surface normals can be uniquely determined using a simple least-squares fit. However, a more robust alternative involves solving\nmin ñ,e ‖e‖0 s.t. o = Lñ+ e, (31)\nwhere ñ is the surface normal rescaled by ρ, which is equivalent to computing [28]\nmin e ‖e‖0 s.t. Projnull[L>](o) = Projnull[L>](e). (32)\nIt can be shown that this formulation has the exact same structure as (2) in the limit λ → 0, if we assume that y , Projnull[L>](o) and Φ is defined such that Φe = Projnull[L>](e)."
    }, {
      "heading" : "9.2 Experimental Design",
      "text" : "We test algorithms separately on two objects, ‘Bunny’ and ‘Caesar’ from [27]. First lighting conditions are generated whose directions are randomly selected from a hemisphere with the object placed at the center. Then 32-bit HDR gray-scale images of the object are rendered with foreground masks and a randomly chosen ρ, 0.64 for Bunny and 0.8 for Caesar. The resulting image resolution for Bunny is\nTable 2: Attributes of our models used in producing Figure 4(c) results.\nmodel Hidden Unit Size #Parameters Training Time(sec./epoch) S-Acc GRU-small 320 1296740 98.314 0.1588\nLSTM-small 272 1213220 119.605 0.3017 GFGRU-small 220 1285340 170.282 0.4651\nGFLSTM-small 200 1209300 172.013 0.4691 GRU-big 680 4958660 234.024 0.3069\nLSTM-big 600 5037700 312.642 0.2637 GFGRU-big 455 4903635 318.690 0.6028\nGFLSTM-big 425 4864650 310.447 0.6087\n(256× 256) while for Caesar it is (300× 400). Given L, we apply singular value decomposition to get Φ = Projnull[L>] and the ground truth error vector e\n∗ = o− ρLn. For training, we have to synthesize candidate sparse errors e since there is no photometric stereo database for this purpose. We adopt the basic pipeline from [2] to accomplish this. First we draw a support pattern for e uniformly at random with cardinality d sampled from the range [d1, d2]. Nonzero values of e are assigned iid random values from N (µe, σe). Finally, we can naturally compute observations y = Φe which serve as network inputs. Although d1,d1,µe, and σe are all tunable, beyond this, no attempt is made to match the true outlier distributions encountered in applications of photometric stereo. After training on synthetic data, we directly apply the resulting model to the gray-scale images without any additional application-specific tuning. During the testing stage, for each surface point, we use our model to approximately solve (32). Since the network outputs a probability map for the outlier support set, we choose k indices with the least probability as inliers and use them to compute n via least squares.\nHyperparameters: We conduct experiments under three situations using r = 10, 20, 40 images corresponding to r different lighting conditions. As for model capacity, we set the size of hidden states of LSTM cells equal to 2r. Other training settings remain default as in Section 7.3.\nVisual Results: See Figure 7."
    }, {
      "heading" : "10 Appendix D: Additional Experiments and Self-Comparisons",
      "text" : "We provide more evaluation details for generic sparse recovery problems, followed by a number of ablation studies."
    }, {
      "heading" : "10.1 Additional Details for Sparse Vector Recovery Evaluation",
      "text" : "Table 2 lists all the important attributes of our self-comparison models from Figure 4(c) in the main paper. In terms of evaluation on generic problems, we define strict accuracy(s-acc) and loose accuracy(l-acc) via\nSgt = {j : x∗j 6= 0}, Spred(d) = {j : pj is one of the d largest outputs} (33) s-acc = 1N ∑N i=1 I [ Sigt = Sipred(d) ] , l-acc = 1N ∑N i=1 |Sigt∩Sipred(n)| d , (34)\nwhere N is the number of samples."
    }, {
      "heading" : "10.2 Ablation Study for Generic Sparse Estimation Problem",
      "text" : "In Table 3, we list an ablation results of GFLSTM models with different hyperparameters for the d n = 0.4 case. Enlarging capacity generally benefits the performance especially when the capacity is relatively small. However, the effectiveness and efficiency of changing hidden size, LSTM layers, or number of unrolling steps varies. Stacking too many LSTM layers is the least efficient way the enlarge the model capacity considering the trade-off between training time and performance improvement. As for unrolling steps, insufficient steps (say under 10) by no means impairs the models ability while excessive unrolling is a waste of computation. And hidden size is a quite effective way to control the model capacity."
    }, {
      "heading" : "11 Appendix E: Technical Proofs",
      "text" : "Here we present proofs of our technical propositions.\nProof of Proposition 1\nIt has been demonstrated in [41] that using\nw (t+1) i = [ φ>i ( λI + ΦΓ(t)Φ> )−1 φi ] 1 2\n(35)\nwill satisfy the stated conditions of Proposition 1. Now assume that Γ(t) is full rank or invertible, i.e., γ\n(t) j > 0 for all j. Using the matrix inversion lemma, we have\nφ>i ( λI + ΦΓ(t)Φ> )−1 φi = 1 λφ > i ( I − 1λΦ [( Γ(t) )−1 + 1λΦ >Φ ]−1 Φ> ) φi. (36)\nGiven that the matrix inverse is a convex function, and that additive translations preserve convexity, it\nfollows that 1λ2φ > i Φ\n[( Γ(t) )−1 + 1λΦ >Φ ]−1 Φ>φi is a convex function of ( Γ(t) )−1 . Therefore\nthe negation of this term is concave, and so overall (36) is a concave function of ( Γ(t) )−1 . This then\nimplies that we can express (36) as a minimization of upper-bounding hyperplanes via\nφ>i ( λI + ΦΓ(t)Φ> )−1 φi = minz g(z) + m∑\nj=1\nf(zj)\nγi (37)\nfor some functions f and g and variational parameters z = [z1, . . . , zm]>. Such a decomposition is not unique; however, using linear algebraic manipulations, it can be easily verified that\nφ>i ( λI + ΦΓ(t)Φ> )−1 φi = minz 1 λ ‖φi −Φz‖22 + m∑\nj=1\nz2j\nγ (t) j\n(38)\nis one such viable representation.\nTo handle the more general case where some γ(t)j = 0, we use Φ̄ to denote the columns φj such that j ∈ supp[γ], and likewise Γ̄(t) and z̄ the corresponding submatrix of Γ(t) and elements of z respectively. It then naturally follows that\nφ>i ( λI + ΦΓ(t)Φ> )−1 φi = φ > i ( λI + Φ̄Γ̄ (t) Φ̄ >)−1 φi\n= min z̄\n1 λ ‖φi − Φ̄z̄‖22 +\n‖γ(t)‖ 0∑\nj=1\nz̄2j\nγ̄ (t) j\n(39)\n= min z:supp[z]⊆supp[γ(t)]\n1 λ ‖φi −Φz‖22 +\n∑\nj∈supp[γ(t)]\nz2j\nγ (t) j\n.\nProof of Proposition 3\nThe original SBL objective is given by\nL(γ) = y> ( ΦΓΦ> + λI )−1 y + log ∣∣∣ΦΓΦ> + λI ∣∣∣ , (40)\nwhere the first term is convex in γ while the second is concave, ultimately resulting in a non-convex function. For optimization purposes, it is convenient to decouple elements of γ via a series of upper bounds, the iterative minimization of which leads to LSTM-like updates given judicious choices for these bounds.\nTo begin, we have the linear upper bound\nh(γ) , log ∣∣∣ΦΓΦ> + λI ∣∣∣ ≤ h(γ̃) + (γ − γ̃)>∇h(γ̃), (41)\nwhich is always realizable for any γ̃ ∈ Rm+ given the concavity of h(γ) [10]. This bound decouples individual elements of γ into a linear summation that facilitates convenient, separable optimization. Analogously, for the data-dependent term we have\ny> ( ΦΓΦ> + λI )−1 y ≤ 1λ‖y −Φu‖22 + u>Γ −1u. (42)\nThis bound holds for all u ∈ Rm, with equality when u = ΓΦ> ( λI + ΦΓΦ> )−1 y [42].7\nAlthough the r.h.s. of (42) has effectively decoupled γ (given that Γ is diagonal, u>Γ−1u is\n7Additionally, if some γj = 0 while uj 6= 0, we simply define this bound to be infinity. All subsequent update rules are well-defined regardless.\nseparable), it has introduced new auxiliary variables u which are inter-mixed via a Φ-dependent norm. However, we can further bound this term using\nf(u) , 1λ‖y −Φu‖22 ≤ f(ũ) + (u− ũ) >∇f (ũ) + 12µ ‖u− ũ‖ 2 2 , (43)\nfor any ũ ∈ Rm provided that µ ∈ ( 0, λ/ ∥∥∥Φ>Φ ∥∥∥ ] . This occurs because ∇f(u) is Lipshitz continuous with Lipschitz constant 1λ ∥∥∥Φ>Φ\n∥∥∥, in which case a quadratic upper bound can always be constructed as in (43).\nCombining terms, we arrive at the auxiliary objective function\nL(γ, γ̃,u, ũ) , h(γ̃)+(γ − γ̃)>∇h(γ̃)+u>Γ−1u+f(ũ)+(u− ũ)>∇f (ũ)+ 12µ ‖u− ũ‖ 2 2 , (44) where γ̃, u, and ũ can be viewed in this context as additional latent variables, sometimes referred to as variational paramters. And by design, for any γ we have that\nL(γ) = min γ̃,u,ũ L(γ, γ̃,u, ũ) ≤ L(γ, γ̃,u, ũ). (45)\nAdditionally, given that this minimization can be accomplished exactly using the stated updates from Section 2. The details are as follows.\nAssume that we would like to reduce L(γ) starting from some arbitrary point γ(t). If we choose\nγ̃(t) = γ(t), u(t) = Γ(t)Φ> ( λI + ΦΓ(t)Φ> )−1 y, ũ(t) = u(t), (46)\nthen L ( γ(t) ) = L ( γ(t), γ̃(t),u(t), ũ(t) ) by construction, i.e., these values simultaneously optimize\nL(γ, γ̃,u, ũ) ≤ L(γ, γ̃,u, ũ) per our structuring of the respective bounds. Our strategy will now be to solve\nmin γ,u L ( γ, γ̃(t),u, ũ(t) ) (47)\nin closed form in order to obtain a new γ(t+1) that reduces the original objective function L(γ). For this purpose we define w(t) such that\n( w(t) )2 = ∇h ( γ̃(t) ) = diag [ Φ> ( λI + ΦΓ(t)Φ> )−1 Φ ] , (48)\nwhere the squaring operator is applied element-wise and the gradient is calculated using standard formulae. Note that this representation is always possible given that ∇h(γ̃) must always have non-negative elements since h is a non-decreasing, concave function.\nBy excluding irrelevant terms, taking derivatives, and equating to zero, it follows that ( w(t) )−1 |u| = arg min\nγ L ( γ, γ̃(t),u, ũ(t) ) ≡ arg min γ ∑\ni\n[( w\n(t) i\n)2 γ\n(t) i + u2i γ (t) i\n] . (49)\nPlugging this value into the γ-dependent terms from L ( γ, γ̃(t),u, ũ(t) ) , we find that\nγ>∇h(γ̃) + u>Γ−1u ≡ 2w(t) |u|. (50) Therefore, a conditionally optimal version of u can be achieved by solving\n(u∗)(t) , arg min u L ([ w(t) ]−1 |u|, γ̃(t),u, ũ(t) )\n≡ arg min u\n2w(t) |u|+ u>∇f ( ũ(t) ) + 12µ ∥∥∥u− ũ(t) ∥∥∥ 2\n2\n≡ arg min u 2w(t) |u|+ 12µ ∥∥∥u− [ ũ(t) − µ∇f ( ũ(t) )]∥∥∥ 2 2 . (51)\nThis expression can be optimized independently across each ui, leading to\n(u∗i ) (t) = S 2λw\n(t) i\n( ũ\n(t) i − µ\n[ ∇f ( ũ(t) )] i )\n= S 2λw\n(t) i\n( ũ\n(t) i + µ\n[ Φ> ( y −Φũ(t) )] i ) (52)\nwhere Sω is a soft threshold operator. Moreover, based on (49), it follows that\nγ(t+1) = ( w(t) )−1 ∣∣∣(u∗)(t) ∣∣∣ (53)\nwill be such that\nL ( γ(t+1) ) ≤ L ( γ(t+1), γ̃(t), (u∗)(t) , ũ(t) ) ≤ L ( γ(t), γ̃(t),u(t), ũ(t) ) = L ( γ(t) ) . (54)\nTherefore, by following the above process, L(γ) will be reduced (or left unchanged). One attractive feature of this formulation is that γ can be optimized jointly with at least one set of variational parameters (in this case u), as opposed to most majorization-minimization strategies [25] that fix the upper bound before minimizing the original variables (in this case γ).\nIf we choose α(γ) = 1 and β(γ) = 0, then these steps exactly mirror the revised SBL iterations from Section 2 once we define x(t+1) , (u∗)(t) and note that σ(t+1)in x̄(t+1) is tantamount to soft-thresholding. The more general case follows with a few additional manipulations.\nFollowing the updates described above, we have\nL ( γ(t) ) = L ( γ(t), γ̃(t),u(t), ũ(t) )\n= h ( γ̃(t) ) + ( u(t) )> ( Γ(t) )−1 u(t) + 1λ ∥∥∥y −Φu(t) ∥∥∥ 2\n2\n≥ h ( γ̃(t) ) − ( γ̃(t) )> ∇h ( γ̃(t) ) + 2w(t) ∣∣∣u(t) ∣∣∣+ 1λ ∥∥∥y −Φu(t) ∥∥∥ 2\n2\n= h ( γ̃(t) ) − ( γ̃(t) )> ∇h ( γ̃(t) ) + 2w(t) ∣∣∣u(t) ∣∣∣+ f ( ũ(t) )\n+ ( u(t) − ũ(t) )> ∇f ( ũ(t) ) + 12µ ∥∥∥u(t) − ũ(t) ∥∥∥ 2\n2 (55)\ngiven that presently u(t) = ũ(t). Previously we optimized this expression with respect to u and obtained the soft-threshold estimator (u∗)(t). However, suppose we instead evaluate at an alternative point (u′)(t) defined recursively such that\n(u′) (t) ≡ x(t+1) = β ( γ(t) ) x(t) +α ( γ(t) ) (u∗)(t) . (56)\nThen finally we have\nL ( γ(t) ) ≥ h ( γ̃(t) ) − ( γ̃(t) )> ∇h ( γ̃(t) ) + 2w(t) ∣∣∣u(t) ∣∣∣+ 1λ ∥∥∥y −Φu(t) ∥∥∥ 2\n2\n≥ h ( γ̃(t) ) − ( γ̃(t) )> ∇h ( γ̃(t) ) + 2w(t) ∣∣∣(u′)(t) ∣∣∣+ 1λ ∥∥∥y −Φ (u′)(t) ∥∥∥ 2\n2\n= h ( γ̃(t) ) + ( γ(t+1) − γ̃(t) )> ∇h ( γ̃(t) ) + ( x(t+1) )> ( Γ(t+1) )−1 x(t+1)\n+ 1λ\n∥∥∥y −Φx(t+1) ∥∥∥ 2\n2 (57)\n≥ L ( γ(t+1) ) ,\nwhere now γ(t+1) = ( w(t) )−1 ∣∣∣(u′)(t) ∣∣∣. The first inequality follows from (55), the second from the monotone cell update property, and the third via the original construction of the majorizationminimization algorithm. This process then exactly mirrors the iterations from Section 2, with guaranteed cost function descent."
    } ],
    "references" : [ {
      "title" : "Learning to learn by gradient descent by gradient descent",
      "author" : [ "M. Andrychowicz", "M. Denil", "S. Gomez", "M.W. Hoffman", "D. Pfau", "T. Schaul", "B. Shillingford", "N. de Freitas" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Maximal sparsity with deep networks",
      "author" : [ "W. Gao B. Xin", "Y. Wang", "D. Wipf" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Electromagnetic brain mapping",
      "author" : [ "S. Baillet", "J.C. Mosher", "R.M. Leahy" ],
      "venue" : "IEEE Signal Processing Magazine, pages 14–30, Nov.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "IEEE Trans. Image Processing, 18(11),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM J. Imaging Sciences, 2(1),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Iterative hard thresholding for compressed sensing",
      "author" : [ "T. Blumensath", "M.E. Davies" ],
      "venue" : "Applied and Computational Harmonic Analysis, 27(3),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Normalized iterative hard thresholding: Guaranteed stability and performance",
      "author" : [ "T. Blumensath", "M.E. Davies" ],
      "venue" : "IEEE J. Selected Topics Signal Processing, 4(2),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Distance transformations in arbitrary dimensions",
      "author" : [ "G. Borgefors" ],
      "venue" : "Computer Vision, Graphics, and Image Processing, 27(3):321–345,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "AMP-inspired deep networks for sparse linear inverse problems",
      "author" : [ "M. Borgerding", "P. Schniter", "S. Rangan" ],
      "venue" : "arXiv preprint arXiv:1612.01183,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
      "author" : [ "E. Candès", "J. Romberg", "T. Tao" ],
      "venue" : "IEEE Trans. Information Theory, 52(2):489– 509, Feb.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Decoding by linear programming",
      "author" : [ "E. Candès", "T. Tao" ],
      "venue" : "IEEE Trans. Information Theory, 51(12),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Enhancing sparsity by reweighted `1 minimization",
      "author" : [ "E. Candès", "M. Wakin", "S. Boyd" ],
      "venue" : "J. Fourier Anal. Appl., 14(5):877–905,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Gated feedback recurrent neural networks",
      "author" : [ "J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sparse channel estimation via matching pursuit with application to equalization",
      "author" : [ "S.F. Cotter", "B.D. Rao" ],
      "venue" : "IEEE Trans. on Communications, 50(3),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Root sparse Bayesian learning for off-grid DOA estimation",
      "author" : [ "J. Dai", "X. Bao", "W. Xu", "C. Chang" ],
      "venue" : "IEEE Signal Processing Letters, 24(1),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "J. Royal Statistical Society, Series B (Methodological), 39(1):1–38,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Variable selection via nonconcave penalized likelihood and its oracle properties",
      "author" : [ "J. Fan", "R. Li" ],
      "venue" : "J. American Statistical Assoc., 96,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Adaptive sparseness using Jeffreys prior",
      "author" : [ "M.A.T. Figueiredo" ],
      "venue" : "NIPS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Recurrent nets that time and count",
      "author" : [ "F.A. Gers", "J. Schmidhuber" ],
      "venue" : "International Joint Conference on Neural Networks,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Multi snapshot sparse Bayesian learning for DOA",
      "author" : [ "P. Gerstoft", "C.F. Mecklenbrauker", "A. Xenaki", "S. Nannuru" ],
      "venue" : "IEEE Signal Processing Letters, 23(20),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning fast approximations of sparse coding",
      "author" : [ "K. Gregor", "Y. LeCun" ],
      "venue" : "ICML,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, 9(8),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A tutorial on MM algorithms",
      "author" : [ "D.R. Hunter", "K. Lange" ],
      "venue" : "American Statistician, 58(1),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Robust photometric stereo using sparse regression",
      "author" : [ "S. Ikehata", "D.P. Wipf", "Y. Matsushita", "K. Aizawa" ],
      "venue" : "Computer Vision and Pattern Recognition,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Photometric stereo using sparse Bayesian regression for general diffuse surfaces",
      "author" : [ "S. Ikehata", "D.P. Wipf", "Y. Matsushita", "K. Aizawa" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, 36(9):1816–1831,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Photometric stereo using sparse Bayesian regression for general diffuse surfaces",
      "author" : [ "S. Ikehata", "D.P. Wipf", "Y. Matsushita", "K. Aizawa" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, 36(9):1816–1831,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A clockwork RNN",
      "author" : [ "J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Bayesian interpolation",
      "author" : [ "D.J.C. MacKay" ],
      "venue" : "Neural Computation, 4(3):415–447,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Sparse signal reconstruction perspective for source localization with sensor arrays",
      "author" : [ "D.M. Malioutov", "M. Çetin", "A.S. Willsky" ],
      "venue" : "IEEE Trans. Signal Processing, 53(8),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Statistical and Adaptive Signal Processing",
      "author" : [ "D.G. Manolakis", "V.K. Ingle", "S.M. Kogon" ],
      "venue" : "McGrall-Hill, Boston,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Rectified linear units improve restricted Boltzmann machines",
      "author" : [ "V. Nair", "G. Hinton" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Vector approximate message passing",
      "author" : [ "S. Rangan", "P. Schniter", "A.K. Fletcher" ],
      "venue" : "arXiv preprint arXiv:1610.03082,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning efficient sparse and low rank models",
      "author" : [ "P. Sprechmann", "A.M. Bronstein", "G. Sapiro" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, 37(9),",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A proof of convergence of the concave-convex procedure using Zangwill’s theory",
      "author" : [ "B.K. Sriperumbudu", "G.R.G. Lanckriet" ],
      "venue" : "Neural computation, 24,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "J. of the Royal Statistical Society,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Sparse Bayesian learning and the relevance vector machine",
      "author" : [ "M.E. Tipping" ],
      "venue" : "Journal of Machine Learning Research, 1,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning deep `0 encoders",
      "author" : [ "Z. Wang", "Q. Ling", "T. Huang" ],
      "venue" : "arXiv preprint arXiv:1509.00153v2,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sparse estimation with structured dictionaries",
      "author" : [ "D.P. Wipf" ],
      "venue" : "Advances in Nerual Information Processing 24,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Iterative reweighted `1 and `2 methods for finding sparse solutions",
      "author" : [ "D.P. Wipf", "S. Nagarajan" ],
      "venue" : "Journal of Selected Topics in Signal Processing (Special Issue on Compressive Sensing), 4(2), April",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Latent variable Bayesian models for promoting sparsity",
      "author" : [ "D.P. Wipf", "B.D. Rao", "S. Nagarajan" ],
      "venue" : "IEEE Trans. Information Theory, 57(9), Sept.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Photometric method for determining surface orientation from multiple images",
      "author" : [ "R.J. Woodham" ],
      "venue" : "Optical Engineering, 19(1),",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Robust photometric stereo via low-rank matrix completion and recovery",
      "author" : [ "L. Wu", "A. Ganesh", "B. Shi", "Y. Matsushita", "Y. Wang", "Y. Ma" ],
      "venue" : "Asian Conference on Computer Vision,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Off-grid direction of arrival estimation using sparse Bayesian inference",
      "author" : [ "Z. Yang", "L. Xie", "C. Zhang" ],
      "venue" : "IEEE Trans. Signal Processing, 61(1):38–43,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Feedback networks",
      "author" : [ "A.R. Zamir", "T.L. Wu", "L. Sun", "W. Shen", "J. Malik", "S. Savarese" ],
      "venue" : "arXiv preprint arXiv:1612.09508,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "When we treat By as a bias or exogenous input, then the progression of these iterations through time resembles activations passing through the layers (indexed by t) of a deep neural network (DNN) [23, 35, 39, 2].",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 34,
      "context" : "When we treat By as a bias or exogenous input, then the progression of these iterations through time resembles activations passing through the layers (indexed by t) of a deep neural network (DNN) [23, 35, 39, 2].",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 38,
      "context" : "When we treat By as a bias or exogenous input, then the progression of these iterations through time resembles activations passing through the layers (indexed by t) of a deep neural network (DNN) [23, 35, 39, 2].",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 1,
      "context" : "When we treat By as a bias or exogenous input, then the progression of these iterations through time resembles activations passing through the layers (indexed by t) of a deep neural network (DNN) [23, 35, 39, 2].",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 2,
      "context" : "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 30,
      "context" : "applications [3, 12, 16, 20, 26, 31], solving (2) is NP-hard, and therefore efficient approximations are sought.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].",
      "startOffset" : 122,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].",
      "startOffset" : 122,
      "endOffset" : 133
    }, {
      "referenceID" : 36,
      "context" : "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].",
      "startOffset" : 122,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].",
      "startOffset" : 188,
      "endOffset" : 194
    }, {
      "referenceID" : 6,
      "context" : "Popular examples with varying degrees of computational overhead include convex relaxations such as `1-norm regularization [5, 11, 37] and many flavors of iterative hard-thresholding (IHT) [6, 7].",
      "startOffset" : 188,
      "endOffset" : 194
    }, {
      "referenceID" : 5,
      "context" : "However, the Achilles’ heel of all these approaches is that they will generally not converge to good approximate minimizers of (2) if Φ has columns with a high degree of correlation [6, 11], which is unfortunately often the case in practice [40].",
      "startOffset" : 182,
      "endOffset" : 189
    }, {
      "referenceID" : 10,
      "context" : "However, the Achilles’ heel of all these approaches is that they will generally not converge to good approximate minimizers of (2) if Φ has columns with a high degree of correlation [6, 11], which is unfortunately often the case in practice [40].",
      "startOffset" : 182,
      "endOffset" : 189
    }, {
      "referenceID" : 39,
      "context" : "However, the Achilles’ heel of all these approaches is that they will generally not converge to good approximate minimizers of (2) if Φ has columns with a high degree of correlation [6, 11], which is unfortunately often the case in practice [40].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 1,
      "context" : "To mitigate the effects of such correlations, we could leverage the aforementioned correspondence with common DNN structures to learn something like a correlation-invariant algorithm or update rules [2], although in this scenario our starting point would be an algorithmic format with known deficiencies.",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 37,
      "context" : "One important example is sparse Bayesian learning (SBL) [38], which has been shown to solve (2) using a principled, multiloop majorization-minimization approach [25] even in cases where Φ displays strong correlations [40].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 24,
      "context" : "One important example is sparse Bayesian learning (SBL) [38], which has been shown to solve (2) using a principled, multiloop majorization-minimization approach [25] even in cases where Φ displays strong correlations [40].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 39,
      "context" : "One important example is sparse Bayesian learning (SBL) [38], which has been shown to solve (2) using a principled, multiloop majorization-minimization approach [25] even in cases where Φ displays strong correlations [40].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 23,
      "context" : "1 Herein we demonstrate that, when judiciously unfolded, SBL iterations can be formed into variants of long short-term memory (LSTM) cells, one of the more popular recurrent deep neural network architectures [24], or gated extensions thereof [15].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 14,
      "context" : "1 Herein we demonstrate that, when judiciously unfolded, SBL iterations can be formed into variants of long short-term memory (LSTM) cells, one of the more popular recurrent deep neural network architectures [24], or gated extensions thereof [15].",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 22,
      "context" : "This association significantly broadens recent work associating elementary, one-step iterative sparsity algorithms like (1) with simple recurrent or feedforward deep network architectures [23, 35, 39, 2].",
      "startOffset" : 188,
      "endOffset" : 203
    }, {
      "referenceID" : 34,
      "context" : "This association significantly broadens recent work associating elementary, one-step iterative sparsity algorithms like (1) with simple recurrent or feedforward deep network architectures [23, 35, 39, 2].",
      "startOffset" : 188,
      "endOffset" : 203
    }, {
      "referenceID" : 38,
      "context" : "This association significantly broadens recent work associating elementary, one-step iterative sparsity algorithms like (1) with simple recurrent or feedforward deep network architectures [23, 35, 39, 2].",
      "startOffset" : 188,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "This association significantly broadens recent work associating elementary, one-step iterative sparsity algorithms like (1) with simple recurrent or feedforward deep network architectures [23, 35, 39, 2].",
      "startOffset" : 188,
      "endOffset" : 203
    }, {
      "referenceID" : 31,
      "context" : "• We achieve state-of-the-art performance on several empirical tasks, including direction-ofarrival (DOA) estimation [32] and 3D geometry recovery via photometric stereo [43].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 42,
      "context" : "• We achieve state-of-the-art performance on several empirical tasks, including direction-ofarrival (DOA) estimation [32] and 3D geometry recovery via photometric stereo [43].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "• Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 22,
      "context" : "• Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 34,
      "context" : "• Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 38,
      "context" : "• Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 24,
      "context" : "• Although learning-to-learn style approaches [1, 23, 35, 39] have been commonly applied to relatively simple gradient descent optimization templates, this is the first successful attempt we are aware of to learn a complex, multi-loop, majorization-minimization algorithm [25].",
      "startOffset" : 272,
      "endOffset" : 276
    }, {
      "referenceID" : 33,
      "context" : "Note also that a recent interesting modification of approximate message passing [34] (or its unfolded, trainable deep analog that converges to the same solution [9]), can handle certain specialized forms of dictionary correlation; however, the approach does not work with the types of strong arbitrary/unconstrained correlation we consider in this work.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "Note also that a recent interesting modification of approximate message passing [34] (or its unfolded, trainable deep analog that converges to the same solution [9]), can handle certain specialized forms of dictionary correlation; however, the approach does not work with the types of strong arbitrary/unconstrained correlation we consider in this work.",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 37,
      "context" : "where λ > 0 is a fixed variance factor and γ denotes a vector of unknown hyperparamters [38].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 29,
      "context" : "For this purpose we marginalize over x and then maximize the resulting type-II likelihood function with respect to γ [30].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 37,
      "context" : "Conveniently, the resulting convolution-of-Gaussians integral is available in closed-form [38] such that we can equivalently minimize the negative loglikelihood L(γ) = − log ∫ p(y|x)p(x;γ)dx ≡ y>Σ−1 y y + log |Σy|.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 40,
      "context" : "In general, if we replace the `0 norm from (2) with any smooth approximation g(|x|), where g is a concave, non-decreasing function and | · | applies elementwise, then cost function descent2 can be guaranteed using iterations of the form [41]",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 24,
      "context" : "(6) This process can be viewed as a multi-loop, majorization-minimization algorithm [25] (a generalization of the EM algorithm [18]), whereby the inner-loop involves computing x by minimizing a first-order, upper-bounding approximation ‖y−Φx‖2 + λ ∑ i w (t) i |xi|, while the outer-loop updates the bound/majorizer itself as parameterized by the weights w.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "(6) This process can be viewed as a multi-loop, majorization-minimization algorithm [25] (a generalization of the EM algorithm [18]), whereby the inner-loop involves computing x by minimizing a first-order, upper-bounding approximation ‖y−Φx‖2 + λ ∑ i w (t) i |xi|, while the outer-loop updates the bound/majorizer itself as parameterized by the weights w.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 36,
      "context" : "Obviously, if g(u) = u, then w = 1 for all t, and (6) reduces to the Lasso objective for `1 norm regularized sparse regression [37], and only a single iteration is required.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "However, one popular non-trivial instantiation of this approach assumes g(u) = ∑ i log (ui + ) with > 0 a user-defined parameter [13].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "This prevents the overshrinkage of large coefficients, a well-known criticism of `1 norm penalties [19].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 35,
      "context" : "Or global convergence to some stationary point with mild additional assumptions [36].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 39,
      "context" : "However, such cases can generally only occur when we are in the neighborhood of ideal, maximally sparse solutions by definition [40], when different weights are actually desirable even among correlated columns for resolving the final sparse estimates.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 32,
      "context" : ", it acts just like a rectilinear (ReLU) unit [33].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "However, for additional flexibility, α and β could be selected to implement various forms of momentum, ultimately leading to cell updates akin to the popular FISTA [5] or monotonic FISTA [4] algorithms.",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "However, for additional flexibility, α and β could be selected to implement various forms of momentum, ultimately leading to cell updates akin to the popular FISTA [5] or monotonic FISTA [4] algorithms.",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 45,
      "context" : "This is much like the strategy used by feedback networks for obtaining incrementally refined representations [46].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 20,
      "context" : "If we allow for peephole connections [21], it is possible to reverse these roles; however, for simplicity and the most direct mapping to LSTM cells we do not pursue this alternative here.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "But certainly there is potential in replacing such iterations with learned LSTM-like surrogates, at least when provided with access to sufficient training data as in prior attempts to learn sparse estimation algorithms [23, 39, 2].",
      "startOffset" : 219,
      "endOffset" : 230
    }, {
      "referenceID" : 38,
      "context" : "But certainly there is potential in replacing such iterations with learned LSTM-like surrogates, at least when provided with access to sufficient training data as in prior attempts to learn sparse estimation algorithms [23, 39, 2].",
      "startOffset" : 219,
      "endOffset" : 230
    }, {
      "referenceID" : 1,
      "context" : "But certainly there is potential in replacing such iterations with learned LSTM-like surrogates, at least when provided with access to sufficient training data as in prior attempts to learn sparse estimation algorithms [23, 39, 2].",
      "startOffset" : 219,
      "endOffset" : 230
    }, {
      "referenceID" : 14,
      "context" : "This process will unmask certain characteristic dynamics suggestive of a richer class of recurrent network structures, inspired by sequence prediction tasks [15], to be analyzed later in Section 4 and empirically tested in Section 5.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 20,
      "context" : "Forw the result of Proposition 1 suggests that these weights can be computed as the solution of a simple regularized regression problem, which can easily be replaced with a small network analogous to that used in [21]; similarly for ν.",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 28,
      "context" : "Interestingly, in the context of sequence prediction, the clockwork RNN (CW-RNN) has been proposed to cope with temporal dependencies engaged across multiple scales [29].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 28,
      "context" : "In the context of sequence prediction, the clockwork recurrent neural network (CW-RNN) has been proposed to cope with temporal dependencies engaged across multiple scales [29].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 14,
      "context" : "The gated feedback RNN (GF-RNN) [15] was recently developed to update the CW-RNN with an additional set of gated connections that, in effect, allow the network to learn its own clock rates.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "In brief, the GF-RNN involves stacked LSTM layers (or somewhat simpler gated recurrent unit (GRU) layers [14]), that are permitted to communicate bilaterally via additional, data-dependent gates that can open and close on different time-scales.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "Plot (d) shows Chamfer distance-based errors [8] from the direction-of-arrival (DOA) experiment.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "To reproduce experiments from [2], we generate correlated synthetic features via Φ = ∑n i=1 1 i2uiv > i , where ui ∈ R and vi ∈ R are drawn iid from a unit Gaussian distribution, and each column of Φ is subsequently rescaled to unit `2 norm.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 37,
      "context" : "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 5,
      "context" : "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 38,
      "context" : "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).",
      "startOffset" : 263,
      "endOffset" : 267
    }, {
      "referenceID" : 1,
      "context" : "Figures 4(a) and 4(b) evaluate our model, averaged across 10 trials, against an array of optimizationbased approaches: SBL [38], `1 norm minimization [5], and IHT [6]; and existing learning-based DNN models: an ISTA-inspired network [23], an IHT-inspired network [39], and the best maximal sparsity net (MaxSparseNet) from [2] (detailed settings in Appendix A).",
      "startOffset" : 323,
      "endOffset" : 326
    }, {
      "referenceID" : 31,
      "context" : "DOA estimation is a fundamental problem in sonar/radar processing [32].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 31,
      "context" : "In this scenario, each column of Φ represents the sensor array output (a point in C) from a hypothetical source with unit strength at angular location θi, and can be computed using wave progagation formula [32].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 16,
      "context" : "Recently SBL-based algorithms have produced state-of-the-art results solving the DOA problem [17, 22, 45], and we compare our approach against SBL here.",
      "startOffset" : 93,
      "endOffset" : 105
    }, {
      "referenceID" : 21,
      "context" : "Recently SBL-based algorithms have produced state-of-the-art results solving the DOA problem [17, 22, 45], and we compare our approach against SBL here.",
      "startOffset" : 93,
      "endOffset" : 105
    }, {
      "referenceID" : 44,
      "context" : "Recently SBL-based algorithms have produced state-of-the-art results solving the DOA problem [17, 22, 45], and we compare our approach against SBL here.",
      "startOffset" : 93,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "Because the nonzero positions in x∗ now have physical meaning, we apply the Chamfer distance [8] as the error metric, which quantifies how close we are to true source locations (lower is better).",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : "Photometric stereo represents another application domain whereby approximately solving (2) using SBL has recently produced state-of-the-art results [27].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 26,
      "context" : "Under the assumption that these images can be approximately decomposed into a diffuse Lambertian component and sparse corruptions such as shadows and specular highlights, then surface normals at each pixel can be recovered using (2) to isolate these sparse factors followed by a final least squares post-processing step [27].",
      "startOffset" : 320,
      "endOffset" : 324
    }, {
      "referenceID" : 1,
      "context" : "We compare our GFLSTM model against both SBL and the MaxSparseNet [2] (both of which outperform other existing methods).",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "Tests are performed using the 32-bit HDR gray-scale images of objects ‘Bunny’ (256× 256) and ‘Caesar’ (300× 400) as in [27].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "For (very) weakly-supervised training data, we apply the same approach as before, only we use nonzero magnitudes drawn from a Gaussian, with mean and variance loosely tuned to the photometric stereo data, consistent with [2].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 0,
      "context" : "Therefore, recent learning-to-learn approaches such as [1] that rely on gradient descent are difficult to apply in the present setting.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "The gated feedback RNN cell [15] is a key component of our model.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "Training Hyperparameters: To provide consistency with the concept of epoch from [2], our models are trained by 600000/250 = 2400 batches with batch size equal to 250.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 22,
      "context" : "A Useful Training Heuristic: When training with a fixed-sized dataset, as existing learning approaches to sparse estimation do [23, 39, 2], there is always the risk of overfitting.",
      "startOffset" : 127,
      "endOffset" : 138
    }, {
      "referenceID" : 38,
      "context" : "A Useful Training Heuristic: When training with a fixed-sized dataset, as existing learning approaches to sparse estimation do [23, 39, 2], there is always the risk of overfitting.",
      "startOffset" : 127,
      "endOffset" : 138
    }, {
      "referenceID" : 1,
      "context" : "A Useful Training Heuristic: When training with a fixed-sized dataset, as existing learning approaches to sparse estimation do [23, 39, 2], there is always the risk of overfitting.",
      "startOffset" : 127,
      "endOffset" : 138
    }, {
      "referenceID" : 31,
      "context" : "θ ∗ d] > [32].",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 7,
      "context" : "Metric: We apply the symmetric Chamfer distance[8] to evaluate the estimation quality with respect to the ground truth source directions.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : "o = ρLn+ e, (30) where the r measurments are denoted o ∈ R, n ∈ R denotes the true 3D surface normal, rows of L ∈ Rr×3 define lighting directions, ρ is the diffuse albedo, acting here as a scalar multiplier, and e represents an aggregations of shadows, specular highlights, or other corrupting influences [28, 44].",
      "startOffset" : 305,
      "endOffset" : 313
    }, {
      "referenceID" : 43,
      "context" : "o = ρLn+ e, (30) where the r measurments are denoted o ∈ R, n ∈ R denotes the true 3D surface normal, rows of L ∈ Rr×3 define lighting directions, ρ is the diffuse albedo, acting here as a scalar multiplier, and e represents an aggregations of shadows, specular highlights, or other corrupting influences [28, 44].",
      "startOffset" : 305,
      "endOffset" : 313
    }, {
      "referenceID" : 27,
      "context" : "where ñ is the surface normal rescaled by ρ, which is equivalent to computing [28]",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : "We test algorithms separately on two objects, ‘Bunny’ and ‘Caesar’ from [27].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "We adopt the basic pipeline from [2] to accomplish this.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 40,
      "context" : "It has been demonstrated in [41] that using",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "which is always realizable for any γ̃ ∈ R+ given the concavity of h(γ) [10].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 41,
      "context" : "This bound holds for all u ∈ R, with equality when u = ΓΦ> ( λI + ΦΓΦ> )−1 y [42].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 24,
      "context" : "One attractive feature of this formulation is that γ can be optimized jointly with at least one set of variational parameters (in this case u), as opposed to most majorization-minimization strategies [25] that fix the upper bound before minimizing the original variables (in this case γ).",
      "startOffset" : 200,
      "endOffset" : 204
    } ],
    "year" : 2017,
    "abstractText" : "The iterations of many first-order algorithms, when applied to minimizing common regularized regression functions, often resemble neural network layers with prespecified weights. This observation has prompted the development of learningbased approaches that purport to replace these iterations with enhanced surrogates forged as DNN models from available training data. For example, important NPhard sparse estimation problems have recently benefitted from this genre of upgrade, with simple feedforward or recurrent networks ousting proximal gradient-based iterations. Analogously, this paper demonstrates that more powerful Bayesian algorithms for promoting sparsity, which rely on complex multi-loop majorizationminimization techniques, mirror the structure of more sophisticated long short-term memory (LSTM) networks, or alternative gated feedback networks previously designed for sequence prediction. As part of this development, we examine the parallels between latent variable trajectories operating across multiple time-scales during optimization, and the activations within deep network structures designed to adaptively model such characteristic sequences. The resulting insights lead to a novel sparse estimation system that, when granted training data, can estimate optimal solutions efficiently in regimes where other algorithms fail, including practical direction-of-arrival (DOA) and 3D geometry recovery problems. The underlying principles we expose are also suggestive of a learning process for a richer class of multi-loop algorithms in other domains.",
    "creator" : "LaTeX with hyperref package"
  }
}