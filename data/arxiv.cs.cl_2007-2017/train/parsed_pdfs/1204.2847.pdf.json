{
  "name" : "1204.2847.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Segmentation Similarity and Agreement",
    "authors" : [ "Chris Fournier", "Diana Inkpen" ],
    "emails" : [ "cfour037@eecs.uottawa.ca", "diana@eecs.uottawa.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Segmentation is the task of splitting up an item, such as a document, into a sequence of segments by placing boundaries within. The purpose of segmenting can vary greatly, but one common objective is to denote shifts in the topic of a text, where multiple boundary types can also be present (e.g., major versus minor topic shifts). Human-competitive automatic segmentation methods can help a wide range of computational linguistic tasks which depend upon the identification of segment boundaries in text.\nTo evaluate automatic segmentation methods, a method of comparing an automatic segmenter’s performance against the segmentations produced by human judges (coders) is required. Current methods of performing this comparison designate only one coder’s segmentation as a reference to compare against. A single “true” reference segmentation from a coder should not be trusted, given that interannotator agreement is often reported to be rather\npoor (Hearst, 1997, p. 54). Additionally, to ensure that an automatic segmenter does not over-fit to the preference and bias of one particular coder, an automatic segmenter should be compared directly against multiple coders.\nThe state of the art segmentation evaluation metrics (Pk and WindowDiff) slide a window across a designated reference and hypothesis segmentation, and count the number of windows where the number of boundaries differ. Window-based methods suffer from a variety of problems, including: i) unequal penalization of error types; ii) an arbitrarily defined window size parameter (whose choice greatly affects outcomes); iii) lack of clear intuition; iv) inapplicability to multiply-coded corpora; and v) reliance upon a “true” reference segmentation.\nIn this paper, we propose a new method of comparing two segmentations, called segmentation similarity 1 (S), that: i) equally penalizes all error types (unless explicitly configured otherwise); ii) appropriately responds to scenarios tested; iii) defines no arbitrary parameters; iv) is intuitive; and v) is adapted for use in a variety of popular interannotator agreement coefficients to handle multiplycoded corpora; and vi) does not rely upon a “true” reference segmentation (it is symmetric). Capitalizing on the adapted inter-annotator agreement coefficients, the relative difficulty that human segmenters have with various segmentation tasks can now be quantified. We also propose that these coefficients can be used to evaluate and compare automatic segmentation methods in terms of human agreement.\nThis paper is organized as follows. In Section 2, we review segmentation evaluation and interannotator agreement. In Section 3, we present S and\n1A software implementation of segmentation similarity (S) is available at http://nlp.chrisfournier.ca/\nar X\niv :1\n20 4.\n28 47\nv2 [\ncs .C\nL ]\n1 9\nM ay\n2 01\n2\ninter-annotator agreement coefficient adaptations. In Section 4, we evaluate S and WindowDiff in various scenarios and simulations, and upon a multiplycoded corpus."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Segmentation Evaluation",
      "text" : "Precision, recall, and their mean (Fβ-measure) have been previously applied to segmentation evaluation. Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). For segmentation, these metrics are unsuitable because they penalize near-misses of boundaries as full-misses, causing them to drastically overestimate the error. Near-misses are prevalent in segmentation and can account for a large proportion of the errors produced by a coder, and as inter-annotator agreement often shows, they do not reflect coder error, but the difficulty of the task.\nPk (Beeferman and Berger, 1999, pp. 198–200)2 is a window-based metric which attempts to solve the harsh near-miss penalization of precision, recall, and Fβ-measure. In Pk, a window of size k, where k is defined as half of the mean reference segment size, is slid across the text to compute penalties. A penalty of 1 is assigned for each window whose boundaries are detected to be in different segments of the reference and hypothesis segmentations, and this count is normalized by the number of windows.\nPevzner and Hearst (2002, pp. 5–10) highlighted a number of issues with Pk, specifically that: i) False negatives (FNs) are penalized more than false positives (FPs); ii) It does not penalize FPs that fall within k units of a reference boundary; iii) Its sensitivity to variations in segment size can cause it to linearly decrease the penalty for FPs if the size of any segments fall below k; and iv) Near-miss errors are too harshly penalized.\nTo attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were\n2Pk is a modification of Pµ (Beeferman et al., 1997, p. 43). Other modifications such as TDT Cseg (Doddington, 1998, pp. 5–6) have been proposed, but Pk has seen greater usage.\ncounted, named WindowDiff (WD). A window of size k is still slid across the text, but now penalties are attributed to windows where the number of boundaries in each segmentation differs (see Equation 1, where b(Rij) and b(Hij) represents the number of boundaries within the segments in a window of size k from position i to j, and N the number of sentences plus one), with the same normalization.\nWD(R,H) = 1\nN − k N−k∑ i=1,j=i+k (|b(Rij)−b(Hij)| > 0) (1)\nWindowDiff is able to reduce, but not eliminate, sensitivity to segment size, gives more equal weights to both FPs and FNs (FNs are, in effect, penalized less3), and is able to catch mistakes in both small and large segments. It is not without issues though; Lamprier et al. (2007) demonstrated that WindowDiff penalizes errors less at the beginning and end of a segmentation (this is corrected by padding the segmentation at each end by size k). Additionally, variations in the window size k lead to difficulties in interpreting and comparing WindowDiff’s values, and the intuition of the method remains vague.\nFranz et al. (2007) proposed measuring performance in terms of the number of words that are FNs and FPs, normalized by the number of word positions present (see Equation 2).\nRFN = 1\nN ∑ w FN(w), RFP = 1 N ∑ w FP (w) (2)\nRFN and RFP have the advantage that they take into account the severity of an error in terms of segment size, allowing them to reflect the effects of erroneously missing, or added, words in a segment better than window based metrics. Unfortunately, RFN and RFP suffer from the same flaw as precision, recall, and Fβ-measure in that they do not account for near misses."
    }, {
      "heading" : "2.2 Inter-Annotator Agreement",
      "text" : "The need to ascertain the agreement and reliability between coders for segmentation was recognized\n3Georgescul et al. (2006, p. 48) note that both FPs and FNs are weighted by 1/N−k, and although there are “equiprobable possibilities to have a [FP] in an interval of k units”, “the total number of equiprobable possibilities to have a [FN] in an interval of k units is smaller than (N−k)”, making the interpretation of a full miss as a FN less probable than as a FP.\nby Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses.\nHearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean κ (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean κ scores were calculated by comparing a coder’s segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53–54). Although mean κ scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan’s (1988) κ has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555–556).\nArtstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of α (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp. 580–582)."
    }, {
      "heading" : "3 Segmentation Similarity",
      "text" : "For discussing segmentation, a segment’s size (or mass) is measured in units, the error is quantified in potential boundaries (PBs), and we have adopted a modified form of the notation used by Artstein and Poesio (2008), where the set of:\n• Items is {i|i ∈ I} with cardinality i; • Categories is {k|k ∈ K} with cardinality k; • Coders is {c|c ∈ C} with cardinality c; • Segmentations of an item i by a coder c is {s|s ∈ S}, where when sic is specified with only one subscript, it denotes sc, for all relevant items (i); and • Types of segmentation boundaries is {t|t ∈ T} with cardinality t."
    }, {
      "heading" : "3.1 Sources of Dissimilarity",
      "text" : "Linear segmentation has three main types of errors:\n1. s1 contains a boundary that is off by n PBs in s2; 2. s1 contains a boundary that s2 does not; or 3. s2 contains a boundary that s1 does not.\nThese types of errors can be seen in Figure 1, and are conceptualized as a pairwise transposition of a boundary for error 1, and the insertion or deletion (depending upon your perspective) of a boundary for errors 2 and 3. Since we do not designate either segmentation as a reference or hypothesis, we refer to insertions and deletions both as substitutions.\nIt is important to not penalize near misses as full misses in many segmentation tasks because coders often agree upon the existence of a boundary, but disagree upon its exact location. In the previous scenario, assigning a full miss would mean that even a boundary loosely agreed-upon, as in Figure 1, error 1, would be regarded as completely disagreed-upon."
    }, {
      "heading" : "3.2 Edit Distance",
      "text" : "In S, concepts from Damereau-Levenshtein edit distance (Damereau, 1964; Levenshtein, 1966) are applied to model segmentation edit distance as two operations: substitutions and transpositions.4 These two operations represent full misses and near misses, respectively. Using these two operations, a new globally-optimal minimum edit distance is applied to a pair of sequences of sets of boundaries to model the sources of dissimilarity identified earlier.5\nNear misses that are remedied by transposition are penalized as b PBs of error (where b is the number of boundaries transposed), as opposed to the 2b PBs of errors by which they would be penalized if they were considered to be two separate substitution operations. Transpositions can also be considered over n > 2 PBs (n-wise transpositions). This is useful if, for a specific task, near misses of up to n PBs are not to be penalized as full misses (default n = 2).\nThe error represented by the two operations can also be scaled (i.e., weighted) from 1 PB each to a\n4Beeferman et al. (1997, p. 42) briefly mention using an edit distance without transpositions, but discard it in favour of Pµ.\n5For multiple boundaries, an add/del operation is added, and transpositions are considered only within boundary types.\nfraction. The distance over which an n-wise transposition occurred can also be used in conjunction with the scalar operation weighting so that a transposition is weighted using the function in Equation 3.\nte(n, b) = b− (1/b)n−2 where n ≥ 2 and b > 0 (3)\nThis transposition error function was chosen so that, in an n-wise transposition where n = 2 PBs and the number of boundaries transposed b = 2, the penalty would be 1 PB, and the maximum penalty as limn→∞ te(n) would be b PBs, or in this case 2 PBs (demonstrated later in Figure 5b)."
    }, {
      "heading" : "3.3 Method",
      "text" : "In S, we conceptualize the entire segmentation, and individual segments, as having mass (i.e., unit magnitude/length), and quantify similarity between two segmentations as the proportion of boundaries that are not transformed when comparing segmentations using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. S is a symmetric function that quantifies the similarity between two segmentations as a percentage, and applies to any granularity or segmentation unit (e.g., paragraphs, sentences, clauses, etc.).\nConsider a somewhat contrived example containing–for simplicity and brevity–only one boundary type (t = 1). First, a segmentation must be converted into a sequence of segment mass values (see Figure 2).\nThen, a pair of segmentations are converted into parallel sequences of boundary sets, where each set contains the types of boundaries present at that potential boundary location (if there is no boundary present, then the set is empty), as in Figure 3.\nThe edit distance is calculated by first identifying all potential substitution operations that could occur (in this case 5). A search for all potential nwise transpositions that can be made over n adjacent sets between the sequences is then performed, searching from the beginning of the sequence to the end, keeping only those transpositions which do not overlap and which result in transposing the most boundaries between the sequences (to minimize the edit distance). In this case, we have only one nonoverlapping 2-wise transposition. We then subtract the number of boundaries involved in transpositions between the sequences (2 boundaries) from the number of substitutions, giving us an edit distance of 4 PBs: 1 transposition PB and 3 substitution PBs.\nEdit distance, and especially the number of operations of each type performed, is useful in identifying the number of full and near misses that have occurred–which indicates whether one’s choice of transposition window size n is either too generous or too harsh. Edit distance as a penalty does not incorporate information on the severity of an error with respect to the size of a segment, and is not an easily comparable value without some form of normalization. To account for these issues, we define S so that boundary edit distance is used to subtract penalties for each edit operation that occurs, from the number of potential boundaries in a segmentation, normalizing this value by the total number of potential boundaries in a segmentation.\nS(si1, si2) = t ·mass(i)− t− d(si1, si2, T )\nt ·mass(i)− t (4)\nS, as shown in Equation 4, scales the mass of the item by the cardinality of the set of boundary types (t) because the edit distance function d(si1, si1, T ) will return a value of [0, t · mass(i)] PBs, where t ∈ Z+–while subtracting the edit distance and t.6\n6The number of potential boundaries in a segmentation si\nThe numerator is normalized by the total number of potential boundaries per boundary type. This results in a function with a range of [0, 1]. It returns 0 when one segmentation contains no boundaries, and the other contains the maximum number of possible boundaries. It returns 1 when both segmentations are identical.\nUsing the default configuration of this equation, S = 9/13 = 0.6923, a very low similarity, which WindowDiff also agrees upon (1−WD = 0.6154). The edit-distance function d(si1, si1, T ) can also be assigned values of the range [0, 1] as scalar weights (wsub, wtrp) to reduce the penalty attributed to particular edit operations, and configured to use a transposition error function (Equation 3, used by default)."
    }, {
      "heading" : "3.4 Evaluating Automatic Segmenters",
      "text" : "Coders often disagree in segmentation tasks (Hearst, 1997, p. 56), making it improbable that a single, correct, reference segmentation could be identified from human codings. This improbability is the result of individual coders adopting slightly different segmentation strategies (i.e., different granularity). In light of this, we propose that the best available evaluation strategy for automatic segmentation methods is to compare performance against multiple coders directly, so that performance can be quantified relative to human reliability and agreement.\nTo evaluate whether an automatic segmenter performs on par with human performance, interannotator agreement can be calculated with and without the inclusion of an automatic segmenter, where an observed drop in the coefficients would signify that the automatic segmenter does not perform as reliably as the group of human coders.7 This can be performed independently for multiple automatic segmenters to compare them to each other– assuming that the coefficients model chance agreement appropriately–because agreement is calculated (and quantifies reliability) over all segmentations."
    }, {
      "heading" : "3.5 Inter-Annotator Agreement",
      "text" : "Similarity alone is not a sufficiently insightful measure of reliability, or agreement, between coders.\nwith t boundary types is t ·mass(i)− t. 7Similar to how human competitiveness is ascertained by Medelyan et al. (2009, pp. 1324–1325) and Medelyan (2009, pp. 143–145) by comparing drops in inter-indexer consistency.\nChance agreement occurs in segmentation when coders operating at slightly different granularities agree due to their codings, and not their own innate segmentation heuristics. Inter-annotator agreement coefficients have been developed that assume a variety of prior distributions to characterize chance agreement, and to attempt to offer a way to identify whether agreement is primarily due to chance, or not, and to quantify reliability.\nArtstein and Poesio (2008) note that most of a coder’s judgements are non-boundaries. The class imbalance caused by segmentations often containing few boundaries, paired with no handling of near misses, causes most inter-annotator agreement coefficients to drastically underestimate agreement on segmentations. To allow for agreement coefficients to account for near misses, we have adapted S for use with Cohen’s κ, Scott’s π, Fleiss’s multi-π (π∗), and Fleiss’s multi-κ (κ∗), which are all coefficients that range from [Ae/1−Ae , 1], where 0 indicates chance agreement, and 1 perfect agreement. All four coefficients have the general form:\nκ, π, κ∗, and π∗ = Aa − Ae 1− Ae\n(5)\nFor each agreement coefficient, the set of categories is defined as solely the presence of a boundary (K = {segt|t ∈ T}), per boundary type (t). This category choice is similar to those chosen by Hearst (1997, p. 53), who computed chance agreement in terms of the probability that coders would say that a segment boundary exists (segt), and the probability that they would not (unsegt). We have chosen to model chance agreement only in terms of the presence of a boundary, and not the absence, because coders have only two choices when segmenting: to place a boundary, or not. Coders do not place non-boundaries. If they do not make a choice, then the default choice is used: no boundary. This default option makes it impossible to determine whether a segmenter is making a choice by not placing a boundary, or whether they are not sure whether a boundary is to be placed.8 For this reason, we only characterize chance agreement between coders in terms of one boundary presence category per type.\n8This could be modelled as another boundary type, which would be modelled in S by the set of boundary types T ."
    }, {
      "heading" : "3.5.1 Scott’s π",
      "text" : "Proposed by Scott (1955), π assumes that chance agreement between coders can be characterized as the proportion of items that have been assigned to category k by both coders (Equation 7). We calculate agreement (Aπa ) as pairwise mean S (scaled by each item’s size) to enable agreement to quantify near misses leniently, and chance agreement (Aπe ) can be calculated as in Artstein and Poesio (2008).\nAπa =\n∑ i∈I mass(i) · S(si1, si2)∑\ni∈I mass(i) (6)\nAπe = ∑ k∈K ( Pπe (k) )2 (7) We calculate chance agreement per category as the proportion of boundaries (segt) assigned by all coders over the total number of potential boundaries for segmentations, as shown in Equation 8.\nPπe (segt) =\n∑ c∈C ∑ i∈I |boundaries(t, sic)|\nc · ∑ i∈I ( mass(i)− 1 ) (8) This adapted coefficient appropriately estimates chance agreement in situations where there no individual coder bias."
    }, {
      "heading" : "3.5.2 Cohen’s κ",
      "text" : "Proposed by Cohen (1960), κ characterizes chance agreement as individual distributions per coder, calculated as shown in Equations 9-10 using our definition of agreement (Aπa ) as shown earlier.\nAκa = A π a (9) Aκe = ∑ k∈K Pκe (k|c1) · Pκe (k|c2) (10)\nWe calculate category probabilities as in Scott’s π, but per coder, as shown in Equation 11.\nPκe (segt|c) = ∑ i∈I |boundaries(t, sic)|∑ i∈I ( mass(i)− 1\n) (11) This adapted coefficient appropriately estimates chance agreement for segmentation evaluations where coder bias is present."
    }, {
      "heading" : "3.5.3 Fleiss’s Multi-π",
      "text" : "Proposed by Fleiss (1971), multi-π (π∗) adapts Scott’s π for multiple annotators. We use Artstein and Poesio’s (2008, p. 564) proposal for calculating actual and expected agreement, and because all\ncoders rate all items, we express agreement as pairwise mean S between all coders as shown in Equations 12-13, adapting only Equation 12.\nAπ ∗ a = 1(c 2 ) c−1∑ m=1 c∑ n=m+1 ∑ i∈I mass(i) · S(sim, sin)∑ i∈I ( mass(i)− 1\n) (12) Aπ ∗ e =\n∑ k∈K ( Pπe (k) )2 (13)"
    }, {
      "heading" : "3.5.4 Fleiss’s Multi-κ",
      "text" : "Proposed by Davies and Fleiss (1982), multi-κ (κ∗) adapts Cohen’s κ for multiple annotators. We use Artstein and Poesio’s (2008, extended version) proposal for calculating agreement just as in π∗, but with separate distributions per coder as shown in Equations 14-15.\nAκ ∗ a = A π∗ a (14)\nAκ ∗ e = ∑ k∈K ( 1(c 2 ) c−1∑ m=1 c∑ n=m+1 Pκe (k|cm) · Pκe (k|cn) ) (15)"
    }, {
      "heading" : "3.6 Annotator Bias",
      "text" : "To identify the degree of bias in a group of coders’ segmentations, we can use a measure of variance proposed by Artstein and Poesio (2008, p. 572) that is quantified in terms of the difference between expected agreement when chance is assumed to vary between coders, and when it is assumed to not.\nB = Aπ ∗ e −Aκ ∗ e (16)"
    }, {
      "heading" : "4 Experiments",
      "text" : "To demonstrate the advantages of using S, as opposed to WindowDiff (WD), we compare both metrics using a variety of contrived scenarios, and then compare our adapted agreement coefficients against pairwise meanWD9 for the segmentations collected by Kazantseva and Szpakowicz (2012).\nIn this section, because WD is a penalty-based metric, it is reported as 1−WD so that it is easier to compare against S values. When reported in this way, 1−WD and S both range from [0, 1], where 1 represents no errors and 0 represents maximal error.\n9Permuted, and with window size recalculated for each pair."
    }, {
      "heading" : "4.1 Segmentation Cases",
      "text" : "Maximal versus minimal segmentation When proposing a new metric, its reactions to extrema must be illustrated, for example when a maximal segmentation is compared to a minimal segmentation, as shown in Figure 6. In this scenario, both 1−WD and S appropriately identify that this case represents maximal error, or 0. Though not shown here, both metrics also report a similarity of 1.0 when identical segmentations are compared.\nFull misses For the most serious source of error, full misses (i.e., FPs and FNs), both metrics appropriately report a reduction in similarity for cases such as Figure 7 that is very similar (1−WD = 0.8462, S= 0.8461). Where the two metrics differ is when this type of error is increased.\nS reacts to increasing full misses linearly, whereas WindowDiff can prematurely report a maximal number of errors. Figure 5a demonstrates this effect, where for each iteration we have taken segmentations of 100 units of mass with one matching boundary at the first hypothesis boundary position,\nand uniformly increased the number of internal hypothesis segments, giving us 1 matching boundary, and [0, 98] FPs. This premature report of maximal error (at 7 FP) by WD is caused by the window size (k = 25) being greater than all of the internal hypothesis segment sizes, making all windows penalized for containing errors.\nNear misses When dealing with near misses, the values of both metrics drop (1−WD = 0.8182, S = 0.9231), but to greatly varying degrees. In comparison to full misses, WindowDiff penalizes a near miss, like that in Figure 8, far more than S. This difference is due to the distance between the two boundaries involved in a near miss; S shows, in this case, 1 PB of error until it is outside of the n-wise transposition window (where n = 2 PBs), at which point it is considered an error of not one transposition, but two substitutions (2 PBs).\nIf we wanted to completely forgive near misses up to n PBs, we could set the weighting of transpositions in S to wtrp = 0. This is useful if a specific segmentation task accepts that near misses are very probable, and that there is little cost associated with a near miss in a window of n PBs. We can also set n to a high number, i.e., 5 PBs, and use the scaled transposition error (te) function (Equation 3) to slowly increase the error from b = 1 PB to b = 2 PBs, as shown in Figure 5b, which shows how both\nmetrics react to increases in the distance between a near miss in a segment of 25 units. These configurations are all preferable to the drop of 1−WD."
    }, {
      "heading" : "4.2 Segmentation Mass Scale Effects",
      "text" : "It is important for a segmentation evaluation metric to take into account the severity of an error in terms of segment size. An error in a 100 unit segment should be considered less severe than an error in a 2 unit segment, because an extra boundary placed within a 100 unit segment (e.g., Figure 9 with m = 100) could probably indicate a weak boundary, whereas in a 4 unit segment the probability that an extra boundary exists right next to two agreed-upon boundaries should be small for most tasks, meaning that it is probable that the extra boundary is an error, and not a weak boundary.\nTo demonstrate that S is sensitive to segment size, Figure 5c shows how S and 1−WD respond when comparing segmentations configured as shown in Figure 10 (containing one match and one full miss) with linearly increasing mass (4 ≤ m ≤ 100). 1−WD will eventually indicate 0.68, whereas S appropriately discounts the error as mass is increased, approaching 1 as limm→∞. 1−WD behaves in this way because of how it calculates its window size parameter, k, which is plotted as k/m to show how its value influences 1−WD."
    }, {
      "heading" : "4.3 Variation in Segment Sizes",
      "text" : "When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11–12). 10 trials were performed for each segment size range and error probability, with 100 hypotheses generated per trial. Recreating this simulation, we compare the stability of S in comparison to WD, as shown in Table 1. We can see that WD values show substantial within-scenario variation for each segment size range, and larger standard deviations, than S."
    }, {
      "heading" : "4.4 Inter-Annotator Agreement Coefficients",
      "text" : "Here, we demonstrate the adapted inter-annotator agreement coefficients upon topical paragraph-level segmentations produced by 27 coders of 20 chapters from the novel The Moonstone by Wilkie Collins collected by Kazantseva and Szpakowicz (2012). Figure 11 shows a heat map of each chapter where the percentage of coders who agreed upon each potential boundary is represented. Comparing this heat map to the inter-annotator agreement coefficients in Table 2 allows us to better understand why certain chapters have lower reliability.\nChapter 1 has the lowest π∗S score in the table, and also the highest bias (BS). One of the reasons for this low reliability can be attributed to the chapter’s small mass (m) and few coders (|c|), which makes it more sensitive to chance agreement. Visually, the\npredominance of grey indicates that, although there are probably two boundaries, their exact location is not very well agreed upon. In this case, 1−WD incorrectly indicates the opposite, that this chapter may have relatively moderate reliability, because it is not corrected for chance agreement. 1−WD indicates that the lowest reliability is found in Chapter 19. π∗S indicates that this is one of the higher agreement chapters, and looking at the heat map, we can see that it does not contain any strongly agreed upon boundaries. In this chapter, there is little opportunity to agree by chance due to the low number of boundaries (|b|) placed, and because the judgements are tightly clustered in a fair amount of mass, the S component of π∗S appropriately takes into account the near misses observed and gives it a high reliability score.\nChapter 17 received the highest π∗S in the table, which is another example of how tight clustering of boundary choices in a large mass leads π∗S to appropriately indicate high reliability despite that there are not as many individual highly-agreed-upon boundaries, whereas 1−WD indicates that there is low reliability. 1−WD and π∗S both agree, however, that chapter 16 has high reliability.\nDespite WindowDiff’s sensitivity to near misses, it is evident that its pairwise mean cannot be used to consistently judge inter-annotator agreement, or reliability. S demonstrates better versatility when accounting for near misses, and when used as part of inter-annotator agreement coefficients, it properly takes into account chance agreement. Following Artstein and Poesio’s (2008, pp. 590–591) rec-\nommendation, and given the low bias (mean coder groupBS = 0.0061±0.0035), we propose reporting reliability using π∗ for this corpus, where the mean coder group π∗S for the corpus is 0.8904 ± 0.0392 (counting 1039 full and 212 near misses)."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "We have proposed a segmentation evaluation metric which solves the key problems facing segmentation analysis today, including an inability to: appropriately quantify near misses when evaluating automatic segmenters and human performance; penalize errors equally (or, with configuration, in a manner that suits a specific segmentation task); compare an automatic segmenter directly against human performance; require a “true” reference; and handle multiple boundary types. Using S, task-specific evaluation of automatic and human segmenters can be performed using multiple human judgements unhindered by the quirks of window-based metrics.\nIn current and future work, we will show how S can be used to analyze hierarchical segmentations, and illustrate how to apply S to linear segmentations containing multiple boundary types."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Anna Kazantseva for her invaluable feedback and corpora, and Stan Szpakowicz, Martin Scaiano, and James Cracknell for their feedback."
    } ],
    "references" : [ {
      "title" : "Inter-coder agreement for computational linguistics",
      "author" : [ "Ron Artstein", "Massimo Poesio." ],
      "venue" : "Computational Linguistics, 34(4):555–596. MIT Press, Cambridge, MA, USA.",
      "citeRegEx" : "Artstein and Poesio.,? 2008",
      "shortCiteRegEx" : "Artstein and Poesio.",
      "year" : 2008
    }, {
      "title" : "Text Segmentation Using Exponential Models",
      "author" : [ "Doug Beeferman", "Adam Berger", "John Lafferty." ],
      "venue" : "Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, 2:35–46. Association for Computational Linguistics, Strouds-",
      "citeRegEx" : "Beeferman et al\\.,? 1997",
      "shortCiteRegEx" : "Beeferman et al\\.",
      "year" : 1997
    }, {
      "title" : "Statistical models for text segmentation",
      "author" : [ "Doug Beeferman", "Adam Berger." ],
      "venue" : "Machine learning, 34(1–3):177–210. Springer Netherlands, NL.",
      "citeRegEx" : "Beeferman and Berger.,? 1999",
      "shortCiteRegEx" : "Beeferman and Berger.",
      "year" : 1999
    }, {
      "title" : "A Coefficient of Agreement for Nominal Scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and Psychological Measurement, 20(1):37–46. Sage, Beverly Hills, CA, USA.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "A technique for computer detection and correction of spelling errors",
      "author" : [ "Frederick J. Damerau." ],
      "venue" : "Communications of the ACM, 7(3):171–176. Association for Computing Machinery, Stroudsburg, PA, USA.",
      "citeRegEx" : "Damerau.,? 1964",
      "shortCiteRegEx" : "Damerau.",
      "year" : 1964
    }, {
      "title" : "Measuring agreement for multinomial data",
      "author" : [ "Mark Davies", "Joseph L. Fleiss." ],
      "venue" : "Biometrics, 38(4):1047–1051. Blackwell Publishing Inc, Oxford, UK.",
      "citeRegEx" : "Davies and Fleiss.,? 1982",
      "shortCiteRegEx" : "Davies and Fleiss.",
      "year" : 1982
    }, {
      "title" : "The topic detection and tracking phase 2 (TDT2) evaluation plan",
      "author" : [ "George R. Doddington." ],
      "venue" : "DARPA Broadcast News Transcription and Understanding Workshop, pp. 223–229. Morgan Kaufmann, Waltham, MA, USA.",
      "citeRegEx" : "Doddington.,? 1998",
      "shortCiteRegEx" : "Doddington.",
      "year" : 1998
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L. Fleiss." ],
      "venue" : "Psychological Bulletin, 76(5):378–382. American Psychological Association, Washington, DC, USA.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "User-oriented text segmentation evaluation measure",
      "author" : [ "Martin", "Franz", "J. Scott McCarley", "Jian-Ming Xu." ],
      "venue" : "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 701–702. Association for",
      "citeRegEx" : "Martin et al\\.,? 2007",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2007
    }, {
      "title" : "Estimating upper and lower bounds on the performance of word-sense disambiguation programs",
      "author" : [ "William Gale", "Kenneth Ward Church", "David Yarowsky." ],
      "venue" : "Proceedings of the 30th annual meeting of the Association for Computational Linguistics, pp.",
      "citeRegEx" : "Gale et al\\.,? 1992",
      "shortCiteRegEx" : "Gale et al\\.",
      "year" : 1992
    }, {
      "title" : "An analysis of quantitative aspects in the evaluation of thematic segmentation algorithms",
      "author" : [ "Maria Georgescul", "Alexander Clark", "Susan Armstrong." ],
      "venue" : "Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pp. 144–151. Association for",
      "citeRegEx" : "Georgescul et al\\.,? 2006",
      "shortCiteRegEx" : "Georgescul et al\\.",
      "year" : 2006
    }, {
      "title" : "TextTiling: Segmenting Text into Multi-paragraph Subtopic Passages",
      "author" : [ "Marti A. Hearst." ],
      "venue" : "Computational Linguistics, 23(1):33–64. MIT Press, Cambridge, MA, USA.",
      "citeRegEx" : "Hearst.,? 1997",
      "shortCiteRegEx" : "Hearst.",
      "year" : 1997
    }, {
      "title" : "Topical Segmentation: a Study of Human Performance",
      "author" : [ "Anna Kazantseva", "Stan Szpakowicz." ],
      "venue" : "Proceedings of Human Language Technologies: The 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics. As-",
      "citeRegEx" : "Kazantseva and Szpakowicz.,? 2012",
      "shortCiteRegEx" : "Kazantseva and Szpakowicz.",
      "year" : 2012
    }, {
      "title" : "Content Analysis: An Introduction to Its Methodology, Chapter 12",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Sage, Beverly Hills, CA, USA.",
      "citeRegEx" : "Krippendorff.,? 1980",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 1980
    }, {
      "title" : "Content Analysis: An Introduction to Its Methodology, Chapter 11",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Sage, Beverly Hills, CA, USA.",
      "citeRegEx" : "Krippendorff.,? 2004",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2004
    }, {
      "title" : "On evaluation methodologies for text segmentation algorithms",
      "author" : [ "Sylvain Lamprier", "Tassadit Amghar", "Bernard Levrat", "Frederic Saubion" ],
      "venue" : "Proceedings of the 19th IEEE International Conference on Tools with Artificial Intelligence, 2:19–26. IEEE Computer Society,",
      "citeRegEx" : "Lamprier et al\\.,? 2007",
      "shortCiteRegEx" : "Lamprier et al\\.",
      "year" : 2007
    }, {
      "title" : "Binary codes capable of correcting deletions, insertions, and reversals",
      "author" : [ "Vladimir I. Levenshtein." ],
      "venue" : "Soviet Physics Doklady, 10(8):707–710. American Institute of Physics, College Park, MD, USA.",
      "citeRegEx" : "Levenshtein.,? 1966",
      "shortCiteRegEx" : "Levenshtein.",
      "year" : 1966
    }, {
      "title" : "Human-competitive automatic topic indexing",
      "author" : [ "Olena Medelyan." ],
      "venue" : "PhD Thesis. University of Waikato, Waikato, NZ.",
      "citeRegEx" : "Medelyan.,? 2009",
      "shortCiteRegEx" : "Medelyan.",
      "year" : 2009
    }, {
      "title" : "Human-competitive tagging using automatic keyphrase extraction",
      "author" : [ "Olena Medelyan", "Eibe Frank", "Ian H. Witten." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 1318–1327. Association for Compu-",
      "citeRegEx" : "Medelyan et al\\.,? 2009",
      "shortCiteRegEx" : "Medelyan et al\\.",
      "year" : 2009
    }, {
      "title" : "Intention-based segmentation: human reliability and correlation with linguistic cues",
      "author" : [ "Rebecca J. Passonneau", "Diane J. Litman." ],
      "venue" : "Proceedings of the 31st annual meeting of the Association for Computational Linguistics, pp. 148–155). Association for Com-",
      "citeRegEx" : "Passonneau and Litman.,? 1993",
      "shortCiteRegEx" : "Passonneau and Litman.",
      "year" : 1993
    }, {
      "title" : "A critique and improvement of an evaluation metric for text segmentation",
      "author" : [ "Lev Pevzner", "Marti A. Hearst." ],
      "venue" : "Computational Linguistics, 28(1):19–36. MIT Press, Cambridge, MA, USA.",
      "citeRegEx" : "Pevzner and Hearst.,? 2002",
      "shortCiteRegEx" : "Pevzner and Hearst.",
      "year" : 2002
    }, {
      "title" : "Reliability of content analysis: The case of nominal scale coding",
      "author" : [ "William A. Scott." ],
      "venue" : "Public Opinion Quarterly, 19(3):321–325. American Association for Public Opinion Research, Deerfield, IL, USA.",
      "citeRegEx" : "Scott.,? 1955",
      "shortCiteRegEx" : "Scott.",
      "year" : 1955
    }, {
      "title" : "Nonparametric Statistics for the Behavioral Sciences",
      "author" : [ "Sidney Siegel", "N. John Castellan", "Jr." ],
      "venue" : "2nd Edition, Chapter 9.8. McGraw-Hill, New York, USA.",
      "citeRegEx" : "Siegel et al\\.,? 1988",
      "shortCiteRegEx" : "Siegel et al\\.",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "It is not without issues though; Lamprier et al. (2007) demonstrated that WindowDiff penalizes errors less at the beginning and end of a segmentation (this is corrected by padding the segmentation at each end by size k).",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "It is not without issues though; Lamprier et al. (2007) demonstrated that WindowDiff penalizes errors less at the beginning and end of a segmentation (this is corrected by padding the segmentation at each end by size k). Additionally, variations in the window size k lead to difficulties in interpreting and comparing WindowDiff’s values, and the intuition of the method remains vague. Franz et al. (2007) proposed measuring performance in terms of the number of words that are FNs and FPs, normalized by the number of word positions present (see Equation 2).",
      "startOffset" : 33,
      "endOffset" : 406
    }, {
      "referenceID" : 13,
      "context" : "Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of α (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp.",
      "startOffset" : 213,
      "endOffset" : 253
    }, {
      "referenceID" : 14,
      "context" : "Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of α (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp.",
      "startOffset" : 213,
      "endOffset" : 253
    }, {
      "referenceID" : 14,
      "context" : "by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al.",
      "startOffset" : 3,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean κ (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p.",
      "startOffset" : 80,
      "endOffset" : 455
    }, {
      "referenceID" : 8,
      "context" : "by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean κ (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean κ scores were calculated by comparing a coder’s segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53–54). Although mean κ scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan’s (1988) κ has declined in favour of other coefficients (Artstein and Poesio, 2008, pp.",
      "startOffset" : 80,
      "endOffset" : 1035
    }, {
      "referenceID" : 0,
      "context" : "Although mean κ scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan’s (1988) κ has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555–556). Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of α (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp.",
      "startOffset" : 198,
      "endOffset" : 266
    }, {
      "referenceID" : 0,
      "context" : "For discussing segmentation, a segment’s size (or mass) is measured in units, the error is quantified in potential boundaries (PBs), and we have adopted a modified form of the notation used by Artstein and Poesio (2008), where the set of:",
      "startOffset" : 193,
      "endOffset" : 220
    }, {
      "referenceID" : 16,
      "context" : "In S, concepts from Damereau-Levenshtein edit distance (Damereau, 1964; Levenshtein, 1966) are applied to model segmentation edit distance as two operations: substitutions and transpositions.",
      "startOffset" : 55,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "Proposed by Scott (1955), π assumes that chance agreement between coders can be characterized as the proportion of items that have been assigned to category k by both coders (Equation 7).",
      "startOffset" : 12,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "We calculate agreement (Aa ) as pairwise mean S (scaled by each item’s size) to enable agreement to quantify near misses leniently, and chance agreement (Ae ) can be calculated as in Artstein and Poesio (2008).",
      "startOffset" : 183,
      "endOffset" : 210
    }, {
      "referenceID" : 3,
      "context" : "Proposed by Cohen (1960), κ characterizes chance agreement as individual distributions per coder, calculated as shown in Equations 9-10 using our definition of agreement (Aa ) as shown earlier.",
      "startOffset" : 12,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "3 Fleiss’s Multi-π Proposed by Fleiss (1971), multi-π (π∗) adapts Scott’s π for multiple annotators.",
      "startOffset" : 2,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Proposed by Davies and Fleiss (1982), multi-κ (κ∗) adapts Cohen’s κ for multiple annotators.",
      "startOffset" : 12,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "To demonstrate the advantages of using S, as opposed to WindowDiff (WD), we compare both metrics using a variety of contrived scenarios, and then compare our adapted agreement coefficients against pairwise meanWD9 for the segmentations collected by Kazantseva and Szpakowicz (2012).",
      "startOffset" : 249,
      "endOffset" : 282
    }, {
      "referenceID" : 11,
      "context" : "When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation.",
      "startOffset" : 17,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "Here, we demonstrate the adapted inter-annotator agreement coefficients upon topical paragraph-level segmentations produced by 27 coders of 20 chapters from the novel The Moonstone by Wilkie Collins collected by Kazantseva and Szpakowicz (2012). Figure 11 shows a heat map of each chapter where the percentage of coders who agreed upon each potential boundary is represented.",
      "startOffset" : 212,
      "endOffset" : 245
    } ],
    "year" : 2012,
    "abstractText" : "We propose a new segmentation evaluation metric, called segmentation similarity (S), that quantifies the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. We propose several adapted inter-annotator agreement coefficients which use S that are suitable for segmentation. We show that S is configurable enough to suit a wide variety of segmentation evaluations, and is an improvement upon the state of the art. We also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance.",
    "creator" : "LaTeX with hyperref package"
  }
}