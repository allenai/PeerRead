{
  "name" : "1605.03009.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "This is a nontechnical phenomenological proof that pattern-recognition and consciousness\nare the same activity, with some speculation about the importance of this. Since Husserl, many philosophers have accepted that consciousness consists of a stream of logical connections between an ego and external objects. These connections are called \"intentions.\" Pattern recognition systems are achievable technical artifacts. The proof links a respected philosophical theory of consciousness with technical art, and may therefore enable a theoretically-grounded form of artificial intelligence called a \"synthetic intentionality,\" a being able to synthesize, generalize, select and repeat a stream of intentions. If the pattern recognition is reflexive, able to operate on the stream of intentions, and flexible, able to find new types of connections, perhaps by evolutionary programming, an SI may be a particularly strong form of AI. The article then addresses some conventional problems: Searles' Chinese room, and how an SI could \"understand\" \"meanings\" and “be creative.”\nIt seems to me that the central dispute concerning artificial intelligence is whether automata\ncan be conscious. I'd like to describe the proof that persuaded me. I haven’t seen it anywhere else, so as far as I know, it is original.\nBriefly, a philosophically respectable position is that consciousness is always\nconsciousness… Of. Some. Thing. There is a substantial body of philosophy which studies the connection between a perceiver, and the object, i.e. the meaning of that critical little word \"of.\" This body of philosophy is called phenomenology. Phenomenology is often defined as the study of experience.\nAn indication that phenomenology may be relevant to AI is that by 1930, phenomenologists\nhad uncovered the complexity of natural human intelligence. They recoiled in horror at the \"vast field of toilsome discoveries\" of which logic, mathematics and epistemology were small parts.1 This is clearly parallel with more-modern experiences in practical AI.\nEdmund Husserl, who cast phenomenology in its modern terms, describes a consciousness\nas a stream of experience of the logical connections (or “intentions”) between an \"ego\" and other things.2 His proof and evidence is widely respected by philosophers, and is beyond the scope of this paper.\nThe intentions (connections between things and an ego) include things like perception,\nbelief, observation, desire, communication and will. Note that all of these are described as \"of,\" \"with,\" \"about\" or \"to.\" In people intentions seem to occur about 10 times per second. Husserl claims that consciousness consists of sequential memories of intentions.\nIf Husserl's proofs are right, the practice of strong AI should be phenomenological\nengineering: The design of consciousness is the design of intentions between a self and objects, recorded sequentially in a memory.\nIf the connection between ego and object is an “intention,” then a mechanism that\nsynthesizes and combines intentions would be a “synthetic intentionality,” or “SI.”\nSIs.\nSo, how can this be connected? Any proof connecting phenomenology to AI would help. To make a useful proof, some phenomenology is needed. A basic research tool of\nphenomenology is an introspective mental operation called \"bracketing.\" The name comes from the idea of putting some part of your experience into “brackets,” and mentally pretending that it and its logical consequences don't exist. Bracketing is essential to the proof that follows.\nI believe that bracketing is also essential to the process of maintaining software. Basically,\none must mentally isolate parts of the program, and ignore their logical consequences. So only the new name and introspective application should be unfamiliar to many readers.3\nSo, in modern technical terms, we are talking about something very like “debugging” a\nmental experience by isolating logical slices of that experience. However, if we talk in these terms, we run the risk of assuming what we’re trying to prove. So, let’s fall back to the term “bracketing.”\nIn phenomenal experiments, one brackets some part of one's experience, and then observes\nhow one's experience would be different. The really unique thing about phenomenal experiments is that they require no equipment and little preparation. So, they're actually better than logic for making a proof. With logic, one has to start from agreed premises. Phenomenal truths are objective because they're so easy for individuals to reproduce.\nThe utility of bracketing is that one can examine the conceptual structure of one's\nexperiences in greater detail. For example, one can bracket the color or smell of an apple, and it still can be an apple. One cannot bracket \"edibility\" in an apple, and retain \"appleness.\" It doesn't take many experiments to realize that all of one's experiences are very strongly affected by the meanings that are part of one's experiences. \"Edibility\" is such a meaning.\nI feel instructed by this example because edibility is obviously very important to a biological\nbeing. Machine consciousness needn't have similar structures of meaning in order to qualify as consciousness. Of more importance to a conscious automaton might be meanings like \"compilable,\" and \"all control paths tested,\" with emotional responses similar to \"edible\" and \"nutritious.\"\nThe final philosophical lead-in is a nasty phenomenological problem called (by my teacher,\nat least) the \"object-ground\" problem. Briefly, this problem asks: \"What's an ‘object’?\" and: \"How, and why do people separate objects from backgrounds?\"\nThe lead in? Try bracketing all objects. What’s left is ground. However, there are some very interesting side-effects. When I do this, I have to remove all\nthought from consideration, because thinking is precisely \"about\" \"things.\" In order to think, or apparently to do anything conscious, people have to make connections between things, that is, \"objects,\" and themselves.\nIt occurred to me that the process of synthesizing intentions, i.e. separating “objects” from\nthe “ground” was precisely the problem that AI researchers call \"pattern recognition.\"\nThat is, pattern recognition is consciousness. But, maybe we can be more thorough, just to be sure?"
    }, {
      "heading" : "Here's the proof:",
      "text" : "For an example to generalize from, let's imagine a very simple pattern recognition program.\nLet's say that it finds square-shaped patches of zeros in a square array of numbers. I am sure that"
    }, {
      "heading" : "Lemma A1:",
      "text" : "If one brackets the program’s concept of “objects” a square can’t exist, because it’s an\nobject. The bracketed program cannot perform its function.\nLemma A2: If one brackets the program's connection between the concept of square, and a position in\nthe array, the program no longer performs its function. It can’t associate any square with any part of the array.\nLemma A3: If one brackets the program’s concept of an “observer,” or “sensor” the program no longer\nperforms its function, because it doesn’t sense anything. Therefore it certainly can’t sense a square.\nLemma A4: If one brackets the program's memory of such a connection the program fails. Knowledge is\nconventionally defined as a justified true belief, and eliminating the memory eliminates the justification for any belief by any being, that some sensory experience is a square.\nLemma A, Evidence: I think it’s clear that almost all pattern recognition programs would have similar issues, if\nbracketed in similar very general ways. There might be pathological cases that don’t reduce, but they will be remarkably interesting in their own right for their very peculiar properties. These might be ways to produce exceptionally stable synthetic intentions.\nFor the general case, removing any part of “consciousness” from a pattern recognition\nprogram causes the pattern recognition program to fail. By \"failure\" I mean that the program does not recognize a pattern from its sense-data. It is no longer a \"pattern recognition\" program.\nLemma A, Conclusion: Thus, by eliminating the concepts which are essential to consciousness: Any of: objects, the\nconnection, the former of the connection or the memory of the connection, one eliminates the concept of pattern recognition.\nLemma B, Evidence: Now imagine, a real live human being, someone who is indisputably conscious, such as\nyourself. Bracket your pattern recognition. That is, pretend to yourself that \"everything which was logically dependent on pattern recognition\" ceased to exist.\nLemma B1: One will discover that one cannot recognize any objects in such a state; Consciousness is\nremoved from consideration because it forms intentions with (logical connections to) objects.\nLemma B2: Further, that one has removed from consideration any mental connections to anything. There\nis no “of” in the consciousness of things.\nTellingly, even Husserl’s “Transcendentally pure consciousness” (when one’s consciousness\nis conscious only of itself) is removed from consideration, because one must recognize one’s own consciousness as being different from the other items of one’s mental landscape.\nLemma B3: There may be sensory experience, so-called “realia,” but there is no narrative, even as a\nsequence of connected mental pictures. In a real sense, formation of a report or observation is"
    }, {
      "heading" : "Lemma B4:",
      "text" : "Memory becomes impossible, because recognition experiences objects in time and space.\nWhen recognition is removed from consideration, space, sequence and time are also removed from consideration. Memories depend on these, and are also removed from consideration.\nWhy do I consider time and space essential items for memory? Well, the simplest example\nis food. If you remember food, the memory is utterly useless unless you can use the memory to get the food. Arguably, such a mental phenomenon is so useless that it’s not a memory.\nThis is rather a weak spot. Often people substitute a discovery procedure for a reliable\nmemory. We look up restaurants on the net, or use a cook-book or phone-book.\nHowever, I would argue then that what we are responding to is not a memory, but a hope,\nand we’re trying to convert the hope into a plan. This may eventually turn into a memory, but it simply isn’t one yet.\nLemma B Conclusion: All the items of consciousness are removed from consideration when pattern recognition is\nbracketed.\nMain lemma, combining A & B: It's just simple logic to say that if not A implies not B and not B implies not A, then the\nitems A and B are the same item in different terms.\nBy the same logic, pattern recognition is consciousness. Bracket any part of either from the\nother, and the other will cease to exist.\nSo, let’s speculate about what this means. With only simple feedback, perhaps induced by some homeostatic mechanism, an SI might\nbe motivated to search for and repeat pleasant experiences. The most fruitful application of an automaton's pattern recognition might be to its own memories and internal operations. In this way it could even recognize and improve its general schemes for solving problems. If the search methods can be automatically programmed, perhaps falling back to evolutionary programming when conscious searches fail, or even fall back to random generation of intentions. The result could be a very strong form of AI.\nNote that intentions can by passive or active, because the nature of the intention can vary.\nSo the nature of intentions might be profitably limited by the hardware that the SI is supposed to operate.\nNote that only one data structure is needed, a database of “intentions.” That is, continuing\neffort by a programmer is not clearly needed for an SI to learn by experience.\nA stream of intentions might be selectively replayed. In fact, an SI might be able to compile\na stream of intentions into optimized low-level machine code.\nThe main practical problem with this is to define \"pattern recognition\" well enough to\nimplement it. Such a definition would describe how to recognize any process of pattern recognition. Thus it would have to define all possible categories that could be recognized.\nI don't think that a usefully reductive definition is possible, but luckily, we have examples of\npattern recognition to point at instead. From these, phenomenological engineers can actually do pattern recognition, building SIs without a general definition.\nThe story here would be something like, “Smarter SIs will recognize more items and types\nof items, and therefore smarter SIs will have expanded forms of consciousness.” This puts SI\nA useful theory might come later. This has some problems. Philosophers are still arguing about whether one can recognize a\nconcept. But remember that consciousness and pattern recognition are the same. Personally, I think that the existence of the philosophers' argument itself indicates that people can be conscious of a concept, and therefore can recognize it. Normal English usage indicates that people do this all the time. Even little kids (sometimes) want to be good, and this is consciousness of a concept (goodness) if anything is.\nBut, can an SI possibly be conscious? Many philosophers argue that computers are\n\"syntactic,\" that is, they perform only symbol manipulation. Then these philosophers go on to prove that consciousness is nonsymbolic, and therefore \"can't be performed by computers.\" This is Searles’ classic “Chinese room.”\nI believe that by its very nature, pattern recognition, therefore consciousness, is an analog\nprocess. That is, its inputs are smoothly varying quantities from sensors that interact with some nonsymbolic \"real world.\"\nHowever, I also don't see any profound problem in turning those quantities into streams of\nnumbers and processing the numbers. Electrical engineers frequently use \"digital signal processing\" in place of analog circuitry. It works well. Almost all digitally-recorded music “sounds like music.” Even when it goes wrong, it sounds like “badly recorded” music. There are technical deficiencies in digitization, but they are well-understood sources of error, characterized mathematically using the “Z transform” to manage “sampling error,” “quantization error” and “frequency response.”\nThe \"non-syntactic\" nature of consciousness thus seems amenable to normal engineering\ntradeoffs between the costs and convenience of digital and analog designs. Just design the desired pattern recognition algorithm. Then use the cheaper of analog or digital implementation.\nAnother problem is that it’s common to believe that only a conscious being can perceive\nmeaning. So, recognizing concepts is at the core of consciousness.\nThere's real dispute among philosophers about what \"meaning\" is. Let’s use Wittgenstein's\nassertion that meaning is what an interpreter does when it receives symbols. This explains why meaning requires exactly a set of symbols and an interpreter. Still, Wittgenstein leaves open what the interpreter is doing, and what symbols are.\nSo, in this view, an SI could consciously understand meaning by synthesizing a new stream\nof intentions from a stream of symbols.\nAs programmers might note, Wittgenstein's definition is shockingly close, perhaps even\npracticably close, to such computer-science subjects as Turing-equivalent instruction sets and syntax-directed compilation. But, Wittgenstein's definition is too vague to be inherently mechanistic. Note that the interpreter could even be a human soul increasing in spiritual beauty because of the nourishment of God's word. Compiling to intentions can solve these issues.\nWittgenstein's \"symbols\" could be absolutely anything that is not already in the interpreter,\nincluding the experience of eating an apple. The memory of such an experience could be in the interpreter, but the experience cannot be, because it occurs only when the apple goes into the interpreter's mouth. (Note that experience requires particular equipment: e.g. a “mouth.” This is evidence that the physical structure of the interpreter is crucial to symbolic interpretations of experience.) Compiling to intentions can solve these issues, too.\nrolling weighted mental dice when making contingent decisions.\nAn SI can handle such decisions by using what amounts to an infinitely-divisible roulette\nwheel, in which different contingent choices of intentional streams are assigned to each slice of the wheel. And, there will always be some probability of a totally random choice.\nWhen the SI starts, totally random choices might be a large part of each intention. But, if a result is sufficiently unpleasant, that choice can be avoided in the future by\nreducing that choice’s slice.\nThe contingent choices without unpleasant consequences will remain contingent, making the\nSI creative throughout its behavior.\nAn interesting sidelight is that pattern recognition also appears to be the foundation of all\nsymbol manipulation. The way that people perform symbolic manipulation is that they recognize patterns of symbols, and transform them into other patterns.\nEven digital computers use analog pattern recognition, because every digital switch has to\nrecognize whether its control is on or off. If this is right, then computers are already conscious, in trivial ways. The same sense of consciousness over time is in every thermostat: Too hot or too cold, with some memory of other states, and something to do about them.\nSo, synthetic intention is not only possible, it already exists. Doing it more powerfully, and\nlinking it to languages, space, time and other realia (“sense data”) seems perfectly possible.\nBibliography\n1. Edmund Husserl, \"Ideen\", 1913. pub. as \"Ideas\", English paperback, 1962, Collier Press, section 87. 2. Ibid, Section 84, 1st paragraph. In reading this section, note that the philosophical term for the connection between an ego and object is \"intentionality.\" About the\nedition: to my knowledge, this is the only inexpensive edition of the only English translation that was corrected by the author before his death. Some other English translations appear to be mistranslated to grind the axes of the translators. A unique aid is the \"analytic index\" in the back, originated by Husserl, and corrected. The book is tough to read in English, and harder in German, but very good.\n3. Ibid. section 31. Husserl calls bracketing \"phenomenological reduction,\" or often: \"reduction.\" \"Bracketing\" is the common usage in American philosophy."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "This is a nontechnical phenomenological proof that pattern-recognition and consciousness are the same activity, with some speculation about the importance of this. Since Husserl, many philosophers have accepted that consciousness consists of a stream of logical connections between an ego and external objects. These connections are called \"intentions.\" Pattern recognition systems are achievable technical artifacts. The proof links a respected philosophical theory of consciousness with technical art, and may therefore enable a theoretically-grounded form of artificial intelligence called a \"synthetic intentionality,\" a being able to synthesize, generalize, select and repeat a stream of intentions. If the pattern recognition is reflexive, able to operate on the stream of intentions, and flexible, able to find new types of connections, perhaps by evolutionary programming, an SI may be a particularly strong form of AI. The article then addresses some conventional problems: Searles' Chinese room, and how an SI could \"understand\" \"meanings\" and “be creative.” It seems to me that the central dispute concerning artificial intelligence is whether automata can be conscious. I'd like to describe the proof that persuaded me. I haven’t seen it anywhere else, so as far as I know, it is original. Briefly, a philosophically respectable position is that consciousness is always consciousness... Of. Some. Thing. There is a substantial body of philosophy which studies the connection between a perceiver, and the object, i.e. the meaning of that critical little word \"of.\" This body of philosophy is called phenomenology. Phenomenology is often defined as the study of experience. An indication that phenomenology may be relevant to AI is that by 1930, phenomenologists had uncovered the complexity of natural human intelligence. They recoiled in horror at the \"vast field of toilsome discoveries\" of which logic, mathematics and epistemology were small parts. This is clearly parallel with more-modern experiences in practical AI. Edmund Husserl, who cast phenomenology in its modern terms, describes a consciousness as a stream of experience of the logical connections (or “intentions”) between an \"ego\" and other things. His proof and evidence is widely respected by philosophers, and is beyond the scope of this paper. The intentions (connections between things and an ego) include things like perception, belief, observation, desire, communication and will. Note that all of these are described as \"of,\" \"with,\" \"about\" or \"to.\" In people intentions seem to occur about 10 times per second. Husserl claims that consciousness consists of sequential memories of intentions. If Husserl's proofs are right, the practice of strong AI should be phenomenological engineering: The design of consciousness is the design of intentions between a self and objects, recorded sequentially in a memory. If the connection between ego and object is an “intention,” then a mechanism that synthesizes and combines intentions would be a “synthetic intentionality,” or “SI.” Of course, if Husserl is right, many modern approaches to AI are bunk, because SI is needed to produce a conscious being. Also, we’d expect successes in systems that started to resemble SIs. So, how can this be connected? Any proof connecting phenomenology to AI would help. To make a useful proof, some phenomenology is needed. A basic research tool of phenomenology is an introspective mental operation called \"bracketing.\" The name comes from the idea of putting some part of your experience into “brackets,” and mentally pretending that it and its logical consequences don't exist. Bracketing is essential to the proof that follows. I believe that bracketing is also essential to the process of maintaining software. Basically, one must mentally isolate parts of the program, and ignore their logical consequences. So only the new name and introspective application should be unfamiliar to many readers. So, in modern technical terms, we are talking about something very like “debugging” a mental experience by isolating logical slices of that experience. However, if we talk in these terms, we run the risk of assuming what we’re trying to prove. So, let’s fall back to the term “bracketing.” In phenomenal experiments, one brackets some part of one's experience, and then observes how one's experience would be different. The really unique thing about phenomenal experiments is that they require no equipment and little preparation. So, they're actually better than logic for making a proof. With logic, one has to start from agreed premises. Phenomenal truths are objective because they're so easy for individuals to reproduce. The utility of bracketing is that one can examine the conceptual structure of one's experiences in greater detail. For example, one can bracket the color or smell of an apple, and it still can be an apple. One cannot bracket \"edibility\" in an apple, and retain \"appleness.\" It doesn't take many experiments to realize that all of one's experiences are very strongly affected by the meanings that are part of one's experiences. \"Edibility\" is such a meaning. I feel instructed by this example because edibility is obviously very important to a biological being. Machine consciousness needn't have similar structures of meaning in order to qualify as consciousness. Of more importance to a conscious automaton might be meanings like \"compilable,\" and \"all control paths tested,\" with emotional responses similar to \"edible\" and \"nutritious.\" The final philosophical lead-in is a nasty phenomenological problem called (by my teacher, at least) the \"object-ground\" problem. Briefly, this problem asks: \"What's an ‘object’?\" and: \"How, and why do people separate objects from backgrounds?\" The lead in? Try bracketing all objects. What’s left is ground. However, there are some very interesting side-effects. When I do this, I have to remove all thought from consideration, because thinking is precisely \"about\" \"things.\" In order to think, or apparently to do anything conscious, people have to make connections between things, that is, \"objects,\" and themselves. It occurred to me that the process of synthesizing intentions, i.e. separating “objects” from the “ground” was precisely the problem that AI researchers call \"pattern recognition.\" That is, pattern recognition is consciousness. But, maybe we can be more thorough, just to be sure? Here's the proof: For an example to generalize from, let's imagine a very simple pattern recognition program. Let's say that it finds square-shaped patches of zeros in a square array of numbers. I am sure that this is within the state of the art, because I could program it myself. This might even be useful, if the array of numbers was from a video camera, or had mathematical interest. Lemma A1: If one brackets the program’s concept of “objects” a square can’t exist, because it’s an object. The bracketed program cannot perform its function. Lemma A2: If one brackets the program's connection between the concept of square, and a position in the array, the program no longer performs its function. It can’t associate any square with any part of the array. Lemma A3: If one brackets the program’s concept of an “observer,” or “sensor” the program no longer performs its function, because it doesn’t sense anything. Therefore it certainly can’t sense a square. Lemma A4: If one brackets the program's memory of such a connection the program fails. Knowledge is conventionally defined as a justified true belief, and eliminating the memory eliminates the justification for any belief by any being, that some sensory experience is a square. Lemma A, Evidence: I think it’s clear that almost all pattern recognition programs would have similar issues, if bracketed in similar very general ways. There might be pathological cases that don’t reduce, but they will be remarkably interesting in their own right for their very peculiar properties. These might be ways to produce exceptionally stable synthetic intentions. For the general case, removing any part of “consciousness” from a pattern recognition program causes the pattern recognition program to fail. By \"failure\" I mean that the program does not recognize a pattern from its sense-data. It is no longer a \"pattern recognition\" program. Lemma A, Conclusion: Thus, by eliminating the concepts which are essential to consciousness: Any of: objects, the connection, the former of the connection or the memory of the connection, one eliminates the concept of pattern recognition. Lemma B, Evidence: Now imagine, a real live human being, someone who is indisputably conscious, such as yourself. Bracket your pattern recognition. That is, pretend to yourself that \"everything which was logically dependent on pattern recognition\" ceased to exist. Lemma B1: One will discover that one cannot recognize any objects in such a state; Consciousness is removed from consideration because it forms intentions with (logical connections to) objects. Lemma B2: Further, that one has removed from consideration any mental connections to anything. There is no “of” in the consciousness of things. Tellingly, even Husserl’s “Transcendentally pure consciousness” (when one’s consciousness is conscious only of itself) is removed from consideration, because one must recognize one’s own consciousness as being different from the other items of one’s mental landscape. Lemma B3: There may be sensory experience, so-called “realia,” but there is no narrative, even as a sequence of connected mental pictures. In a real sense, formation of a report or observation is impossible, and therefore there is no observer. Arguably, consciousness itself does not exist in this state. That is, there is no consciousness, in a different way. Lemma B4: Memory becomes impossible, because recognition experiences objects in time and space. When recognition is removed from consideration, space, sequence and time are also removed from consideration. Memories depend on these, and are also removed from consideration. Why do I consider time and space essential items for memory? Well, the simplest example is food. If you remember food, the memory is utterly useless unless you can use the memory to get the food. Arguably, such a mental phenomenon is so useless that it’s not a memory. This is rather a weak spot. Often people substitute a discovery procedure for a reliable memory. We look up restaurants on the net, or use a cook-book or phone-book. However, I would argue then that what we are responding to is not a memory, but a hope, and we’re trying to convert the hope into a plan. This may eventually turn into a memory, but it simply isn’t one yet. Lemma B Conclusion: All the items of consciousness are removed from consideration when pattern recognition is bracketed. Main lemma, combining A & B: It's just simple logic to say that if not A implies not B and not B implies not A, then the items A and B are the same item in different terms. By the same logic, pattern recognition is consciousness. Bracket any part of either from the other, and the other will cease to exist. So, let’s speculate about what this means. With only simple feedback, perhaps induced by some homeostatic mechanism, an SI might be motivated to search for and repeat pleasant experiences. The most fruitful application of an automaton's pattern recognition might be to its own memories and internal operations. In this way it could even recognize and improve its general schemes for solving problems. If the search methods can be automatically programmed, perhaps falling back to evolutionary programming when conscious searches fail, or even fall back to random generation of intentions. The result could be a very strong form of AI. Note that intentions can by passive or active, because the nature of the intention can vary. So the nature of intentions might be profitably limited by the hardware that the SI is supposed to operate. Note that only one data structure is needed, a database of “intentions.” That is, continuing effort by a programmer is not clearly needed for an SI to learn by experience. A stream of intentions might be selectively replayed. In fact, an SI might be able to compile a stream of intentions into optimized low-level machine code. The main practical problem with this is to define \"pattern recognition\" well enough to implement it. Such a definition would describe how to recognize any process of pattern recognition. Thus it would have to define all possible categories that could be recognized. I don't think that a usefully reductive definition is possible, but luckily, we have examples of pattern recognition to point at instead. From these, phenomenological engineers can actually do pattern recognition, building SIs without a general definition. The story here would be something like, “Smarter SIs will recognize more items and types of items, and therefore smarter SIs will have expanded forms of consciousness.” This puts SI design into a continuum of technique by which SIs can be improved like any other technical artifact. A useful theory might come later. This has some problems. Philosophers are still arguing about whether one can recognize a concept. But remember that consciousness and pattern recognition are the same. Personally, I think that the existence of the philosophers' argument itself indicates that people can be conscious of a concept, and therefore can recognize it. Normal English usage indicates that people do this all the time. Even little kids (sometimes) want to be good, and this is consciousness of a concept (goodness) if anything is. But, can an SI possibly be conscious? Many philosophers argue that computers are \"syntactic,\" that is, they perform only symbol manipulation. Then these philosophers go on to prove that consciousness is nonsymbolic, and therefore \"can't be performed by computers.\" This is Searles’ classic “Chinese room.” I believe that by its very nature, pattern recognition, therefore consciousness, is an analog process. That is, its inputs are smoothly varying quantities from sensors that interact with some nonsymbolic \"real world.\" However, I also don't see any profound problem in turning those quantities into streams of numbers and processing the numbers. Electrical engineers frequently use \"digital signal processing\" in place of analog circuitry. It works well. Almost all digitally-recorded music “sounds like music.” Even when it goes wrong, it sounds like “badly recorded” music. There are technical deficiencies in digitization, but they are well-understood sources of error, characterized mathematically using the “Z transform” to manage “sampling error,” “quantization error” and “frequency response.” The \"non-syntactic\" nature of consciousness thus seems amenable to normal engineering tradeoffs between the costs and convenience of digital and analog designs. Just design the desired pattern recognition algorithm. Then use the cheaper of analog or digital implementation. Another problem is that it’s common to believe that only a conscious being can perceive meaning. So, recognizing concepts is at the core of consciousness. There's real dispute among philosophers about what \"meaning\" is. Let’s use Wittgenstein's assertion that meaning is what an interpreter does when it receives symbols. This explains why meaning requires exactly a set of symbols and an interpreter. Still, Wittgenstein leaves open what the interpreter is doing, and what symbols are. So, in this view, an SI could consciously understand meaning by synthesizing a new stream of intentions from a stream of symbols. As programmers might note, Wittgenstein's definition is shockingly close, perhaps even practicably close, to such computer-science subjects as Turing-equivalent instruction sets and syntax-directed compilation. But, Wittgenstein's definition is too vague to be inherently mechanistic. Note that the interpreter could even be a human soul increasing in spiritual beauty because of the nourishment of God's word. Compiling to intentions can solve these issues. Wittgenstein's \"symbols\" could be absolutely anything that is not already in the interpreter, including the experience of eating an apple. The memory of such an experience could be in the interpreter, but the experience cannot be, because it occurs only when the apple goes into the interpreter's mouth. (Note that experience requires particular equipment: e.g. a “mouth.” This is evidence that the physical structure of the interpreter is crucial to symbolic interpretations of experience.) Compiling to intentions can solve these issues, too. So, it’s common to believe that creativity is an essential element of consciousness. How might an SI be creative? From my personal observations, creative people do something very like rolling weighted mental dice when making contingent decisions. An SI can handle such decisions by using what amounts to an infinitely-divisible roulette wheel, in which different contingent choices of intentional streams are assigned to each slice of the wheel. And, there will always be some probability of a totally random choice. When the SI starts, totally random choices might be a large part of each intention. But, if a result is sufficiently unpleasant, that choice can be avoided in the future by reducing that choice’s slice. The contingent choices without unpleasant consequences will remain contingent, making the SI creative throughout its behavior. An interesting sidelight is that pattern recognition also appears to be the foundation of all symbol manipulation. The way that people perform symbolic manipulation is that they recognize patterns of symbols, and transform them into other patterns. Even digital computers use analog pattern recognition, because every digital switch has to recognize whether its control is on or off. If this is right, then computers are already conscious, in trivial ways. The same sense of consciousness over time is in every thermostat: Too hot or too cold, with some memory of other states, and something to do about them. So, synthetic intention is not only possible, it already exists. Doing it more powerfully, and linking it to languages, space, time and other realia (“sense data”) seems perfectly possible.",
    "creator" : "Microsoft® Word 2013"
  }
}