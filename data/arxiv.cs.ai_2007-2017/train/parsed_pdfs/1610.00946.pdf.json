{
  "name" : "1610.00946.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ERCIM NEWS 107 October 201618"
    }, {
      "heading" : "Special Theme: Machine Learning",
      "text" : "Watching a child learn reveals how well humans can learn: a child may only need a few examples of a concept to “learn it”. By contrast, the impressive results achieved with modern machine learning (in particular, by deep learning) are made possible largely by the use of huge datasets. For instance, the ImageNet database used in image recognition contains about 1.2 million labelled examples; DeepMinds's AlphaGo used more than 38 million positions to train their algorithm to play Go; and the same company used more than 38 days of play to train a neural network to play Atari 2600 games, such as Space Invaders or Breakout.\nLike children, robots have to face the real world, in which trying something might take seconds, hours, or days. And seeing the consequence of this trial might take much more. When robots share our world, they are expected to learn like humans or animals, that is, in far fewer than a million trials. Robots are not alone to be cursed by the price of data: Any learning process that involves physical tests or precise simulations (e.g., computational fluid dynamics) comes up against the same issue. In short, while data might be abundant in the virtual world, it is often a scarce resource in the physical world. I refer to this challenge as “micro-data” learning (see Figure 1).\nThe first precept of micro-data learning is to choose as wisely as possible what to test next (active learning). Since computation tends to become cheaper every year, it is often effective to trade data resources for computational resources, that is, to employ computationally intensive algorithms to select the next data point to acquire. Bayesian optimisation [1] is such a data-efficient algorithm that has recently attracted a lot of interest in the machine learning community. Using the data acquired so far, this algorithm creates a probabilistic model of the function that needs to be optimised (e.g., the\nwalking speed of a robot or the lift generated by an airfoil); it then exploits this model to identify the most promising points of the search space. It can, for example, find good values for the gait of a quadruped robot (Sony Aibo / 15 parameters to learn) in just two hours of learning.\nThe second precept of micro-data learning is to exploit every bit of information from each test. For instance, when a robotic arm tries to reach a point\nin space, the learning algorithm can perform the movement, then, at the end of the trial, measure the distance to the target. In this case, each test corresponds to a single data point. However, the algorithm can also record the position of the \"hand\" every 10ms, thus getting thousands of data points from a single test. This is a very effective\napproach for learning control strategies in robotics; for example, the Pilco algorithm can learn to balance a non-actuated pole on an actuated moving cart in 15-20 seconds (about 3-5 trials) [2].\nThe third precept of micro-data learning is to use the \"right\" prior knowledge. Most problems are indeed simply too hard to be learned from scratch in a few trials, even with the best algorithms: The quick learning ability of humans and animals is due largely to their prior\nknowledge about what could and could not work. When using priors, it is critical to make them as explicit as possible, and to make sure that the learning algorithm can question or even ignore them. In academic examples, it can also be challenging to distinguish between prior knowledge that is useful and prior knowledge that actually gives the solu-"
    }, {
      "heading" : "Micro-Data Learning:",
      "text" : ""
    }, {
      "heading" : "The Other End of the Spectrum",
      "text" : "by Jean-Baptiste Mouret (Inria)\nMany fields are now snowed under with an avalanche of data, which raises considerable\nchallenges for computer scientists. Meanwhile, robotics (among other fields) can often\nonly use a few dozen data points because acquiring them involves a process that is\nexpensive or time-consuming. How can an algorithm learn with only a few data points?\namount of data. For example, the Go player AlphaGo by DeepMind used a dataset of 38 million\npositions, and the deep reinforcement learning experiments from the same team used the\nequivalent of 38 days to learn to play Atari 2600 video games. Robotics is at the opposite end of\nthe spectrum: most of the time, it is difficult to perform more than a few dozen trials. Learning\nwith such a small amount of data is what we term \"Micro-data learning\".\nERCIM NEWS 107 October 2016 19\ntion to the algorithm, which leaves nothing to learn.\nWe focused on prior knowledge in our recent article about damage recovery in robotics [3, L1]. In this scenario, a sixlegged walking robot needs to discover a new way to walk by trial-and-error because it is damaged. Before the mission, a novel algorithm explores a large search space with a simulation of the intact robot to identify the most promising solution of each \"family\". Metaphorically, this algorithm takes the needles out of a haystack to make a stack of needles. If the robot is damaged, the learning algorithm, which is a derivative of Bayesian optimisation [1], exploits this prior knowledge to choose the best trials. In our experiments, the\nrobot discovers compensatory gaits in less than two minutes and a dozen trials, for the five damage conditions that we tested [3].\nIn this learning approach, a data-efficient learning algorithm that works with the physical, damaged robot is guided by prior knowledge based on a simulation of the intact robot. This micro-data learning algorithm makes it possible to learn a complex task in only a few trials. The subsequent challenge is to exploit more knowledge from the trials [2] and select the next trials while taking the context into account (e.g., potential obstacles).\nLink: [L1] http://www.resibots.eu"
    }, {
      "heading" : "Please contact:",
      "text" : "Jean-Baptiste Mouret Inria, France jean-baptiste.mouret@inria.fr\nAfter a history spanning over five decades, artificial general intelligence still remains out of reach. Machine learning has common roots with AI research, but focuses on more attainable goals and has achieved tremendous success in many application fields. Similarly, a universal quantum computer is still far ahead in the distant future: the criterion for this machine is to be able to simulate an arbitrary closed quantum system. Nevertheless, uses of quantum information processing are proliferating: two notable examples are quantum key distribution systems and quantum random number generators. Recently, there has been a surge of interest in the intersection of machine learning and quantum information processing. Combining ideas from these two fields leads to tremendous benefits for both. We are collaborating on several subjects in this domain between ICFO-The Institute of Photonic Sciences, the Autonomous University of Barcelona, the University of the\nBasque Country, all in Spain, as well as the University of Calgary, Canada.\nAt the highest level, abstracting of the actual algorithms and focusing on the foundations of statistical learning theory, we can ask what it means to learn with quantum data and channels, what induction and transduction mean in this setting, how we can define figures of merit to quantify performance, and eventually establish bounds on generalisation performance using sample and model complexity. We studied supervised learning, and proved that in the asymptotic limit and under an assumption of exchangeability, quantum entanglement does not break our traditional notion of induction [L1]. This is an important stepping stone towards understanding generalisation properties of quantum learning protocols.\nThe next natural question to ask is that given a universal quantum computer, what kind of protocols can we use for"
    }, {
      "heading" : "Making Learning Physical: Machine Intelligence",
      "text" : "and Quantum Resources\nby Peter Wittek (ICFO-The Institute of Photonic Sciences and University of Borås)\nIt is not only machine learning that is advancing rapidly: quantum information processing has\nwitnessed several breakthroughs in recent years. In theory, quantum protocols can offer an\nexponential speedup for certain learning algorithms, but even contemporary implementations show\nremarkable results – this new field is called quantum machine learning. The benefits work both ways:\nclassical machine learning finds more and more applicability in problems in quantum computing.\nlearning."
    } ],
    "references" : [ {
      "title" : "Taking the human out of the loop: A review of bayesian optimization",
      "author" : [ "B. Shahriari" ],
      "venue" : "Proc. of the IEEE,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Gaussian processes for data-efficient learning in robotics and control",
      "author" : [ "M.P. Deisenroth", "D. Fox", "C.E. Rasmussen" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Bayesian optimisation [1] is such a data-efficient algorithm that has recently attracted a lot of interest in the machine learning community.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "This is a very effective approach for learning control strategies in robotics; for example, the Pilco algorithm can learn to balance a non-actuated pole on an actuated moving cart in 15-20 seconds (about 3-5 trials) [2].",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 0,
      "context" : "If the robot is damaged, the learning algorithm, which is a derivative of Bayesian optimisation [1], exploits this prior knowledge to choose the best trials.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : "The subsequent challenge is to exploit more knowledge from the trials [2] and select the next trials while taking the context into account (e.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "eu References: [1] B.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "[2] M.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2016,
    "abstractText" : "Watching a child learn reveals how well humans can learn: a child may only need a few examples of a concept to “learn it”. By contrast, the impressive results achieved with modern machine learning (in particular, by deep learning) are made possible largely by the use of huge datasets. For instance, the ImageNet database used in image recognition contains about 1.2 million labelled examples; DeepMinds's AlphaGo used more than 38 million positions to train their algorithm to play Go; and the same company used more than 38 days of play to train a neural network to play Atari 2600 games, such as Space Invaders or Breakout.",
    "creator" : "Preview"
  }
}