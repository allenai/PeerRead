{
  "name" : "1609.06791.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Twitter-Network Topic Model: A Full Bayesian Treatment for Social Network and Text Modeling",
    "authors" : [ "Kar Wai Lim", "Changyou Chen" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Emergence of web services such as blog, microblog and social networking websites allows people to contribute information publicly. This user-generated information is generally more personal, informal and often contains personal opinions. In aggregate, it can be useful for reputation analysis of entities and products, natural disasters detection, obtaining first-hand news, or even demographic analysis. Twitter, an easily accessible source of information, allows users to voice their opinions and thoughts in short text known as tweets.\nLatent Dirichlet allocation (LDA) (Blei et al., 2003) is a popular form of topic model. Unfortunately, a direct application of LDA on tweets yields poor result as tweets are short and often noisy (Zhao et al., 2011), i.e. tweets are unstructured and often contain grammatical and spelling errors, as well as informal words such as user-defined abbreviations due to the 140 characters limit. LDA fails on short tweets since it is heavily dependent on word co-occurrence. Also notable is that text in tweets may contain special tokens known as hashtags; they are used as keywords and allow users to link their tweets with other tweets tagged with the same hashtag. Nevertheless, hashtags are informal since they have no standards. Hashtags can be used as both inline words or categorical labels. Hence instead of being hard labels, hashtags are best treated as special words which can be the themes of the tweets. Tweets are thus challenging for topic models, and ad hoc alternatives are used instead. In other text analysis applications, tweets are often ‘cleansed’ by NLP methods such as lexical normalization (Baldwin et al., 2013). However, the use of normalization is also criticized (Eisenstein, 2013).\nIn this paper, we propose a novel method for short text modeling by leveraging the auxiliary information that accompanies tweets. This information, complementing word co-occurrence, allows us to model the tweets better, as well as opening the door to more applications, such as user recommen-\nar X\niv :1\n60 9.\n06 79\n1v 1\n[ cs\n.C L\n] 2\n2 Se\np 20\ndation and hashtag suggestion. Our main contributions include: 1) a fully Bayesian nonparametric model called Twitter-Network (TN) topic model that models tweets very well; and 2) a combination of both the hierarchical Poisson Dirichlet process (HPDP) and the Gaussian process (GP) to jointly model text, hashtags, authors and the followers network. We also develop a flexible framework for arbitrary PDP networks, which allows quick deployment (including inference) of new variants of HPDP topic models. Despite the complexity of the TN topic model, its implementation is made relatively straightforward with the use of the framework."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : "LDA is often extended for different types of data, some notable examples that use auxiliary information are the author-topic model (Rosen-Zvi et al., 2004), the tag-topic model (Tsai, 2011), and Topic-Link LDA (Liu et al., 2009). However, these models only deal with just one kind of additional information and do not work well with tweets since they are designed for other types of text data. Note that the tag-topic model treats tags as hard labels and uses them to group text documents, which is not appropriate for tweets due to the noisy nature of hashtags. Twitter-LDA (Zhao et al., 2011) and the behavior-topic model (Qiu et al., 2013) were designed to explicitly model tweets. Both models are not admixture models since they limit one topic per document. The behavior-topic model analyzes tweets’ “posting behavior” of each topic for user recommendation. On the other hand, the biterm topic model (Yan et al., 2013) uses only the biterm co-occurrence to model tweets, discarding document level information. Both biterm topic model and Twitter-LDA do not incorporate any auxiliary information. All the above topic models also have a limitation in that the number of topics need to be chosen in advance, which is difficult since this number is not known.\nTo sidestep the need of choosing the number of topics, (Teh and Jordan, 2010) proposed Hierarchical Dirichlet process (HDP) LDA, which utilizes the Dirichlet process (DP) as nonparametric prior. Furthermore, one can replace the DP with the Poisson-Dirichlet process (PDP, also known as the Pitman-Yor process), which models the power-law of word frequencies distributions in natural languages. In natural languages, the distribution of word frequencies exhibits a power-law (Goldwater et al., 2006). For topic models, replacing the Dirichlet distribution with the PDP can yield great improvement (Sato and Nakagawa, 2010).\nSome recent work models text data with network information ((Liu et al., 2009; Chang and Blei, 2010; Nallapati et al., 2008)), however, these models are parametric in nature and can be restrictive. On the contrary, Miller et al. (Miller et al., 2009) and Lloyd et al. (Lloyd et al., 2012) model network data directly with nonparametric priors, i.e. with the Indian Buffet process and the Gaussian process respectively, but do not model text."
    }, {
      "heading" : "3 Model Summary",
      "text" : "The TN topic model makes use of the accompanying hashtags, authors, and followers network to model tweets better. The TN topic model is composed of two main components: a HPDP topic\n0 500 1000 1500 2000- 24\n00 00\n0 -2\n20 00\n00 -2\n00 00\n00\niteration\nlo g-\nlik el\nih oo\nd\nTN ATM\nHDP-LDA\nFigure 2: Log-likelihood vs. iterations\nmodel for the text and hashtags, and a GP based random function model for the followers network. The authorship information serves to connect the two together.\nWe design our HPDP topic model for text as follows. First, generate the global topic distribution µ0 that serves as a prior. Then generate the respective authors’ topic distributions ν for each author, and a miscellaneous topic distribution µ1 to capture topics that deviate from the authors’ usual topics. Given ν and µ1, we generate the topic distributions for the documents, and words (η, θ′, θ). We also explicitly model the influence of hashtags to words. Hashtag and word generation follows standard LDA and is not discussed here. Note that the tokens of hashtags are shared with the words, i.e. the hashtag #happy share the same token as the word happy. Also note that all distributions on probability vectors are modeled by the PDP, making the model a network of PDP nodes.\nThe network modeling is connected to the HPDP topic model via the author topic distributions ν, where we treat ν as inputs to the GP in the network model. The GP, denoted as F , determines the links between the authors (x). Figure 1 displays the graphical model of TN, where region a© and b© shows the network model and topic model respectively. See supplementary material1 for a detailed description. We emphasize that our treatment of the network model is different to that of (Lloyd et al., 2012). We define a new kernel function based on the cosine similarity in our network model, which provides significant improvement over the original kernel function. Also, we derive a new sampling procedure for inference due to the additive coupling of topic distributions and network connections."
    }, {
      "heading" : "4 Posterior Inference",
      "text" : "We alternatively perform Markov chain Monte Carlo (MCMC) sampling on the topic model and the network model, conditioned on each other. We derive a collapsed Gibbs sampler for the topic model, and a Metropolis-Hastings (MH) algorithm for the network model. We develop a framework to perform collapse Gibbs sampling generally on any Bayesian network of PDPs, built upon the work of (Buntine et al., 2010; Chen et al., 2011), which allows quick prototyping and development of new variants of topic model. We refer the readers to the supplementary materials for the technical details."
    }, {
      "heading" : "5 Experiments and Applications",
      "text" : "We evaluate the TN topic model quantitatively with standard topic model measures such as test-set perplexity, likelihood convergence and clustering measures. Qualitatively, we evaluate the model by visualizing the topic summaries, authors’ topic distributions and by performing an automatic labeling task. We compare our model with HDP-LDA, a nonparametric variant of the author-topic model (ATM), and the original random function network model. We also perform ablation studies to show the importance of each component in the model. The results of the comparison and ablation studies are shown in Table 1. We use two tweets corpus for experiments, first is a subset of Twitter7 dataset2 (Yang and Leskovec, 2011), obtained by querying with certain keywords (e.g. finance, sports, politics). we remove tweets that are not English with langid.py (Lui and Baldwin, 2012) and filter authors who do not have network information and who authored less than 100 tweets. The corpus consists of 60370 tweets by 94 authors. We then randomly select 90% of the dataset as training documents and use the rest for testing. Second tweets corpus is obtained from (Mehrotra et al., 2013), which contains a total of 781186 tweets. We note that we perform no word normalization to prevent any loss of meaning of the noisy text.\nExperiment Settings In all cases, we vary α from 0.3 to 0.7 on topic nodes (µ0, µ1, νi, ηm, θ′m, θm) and set α = 0.7 on vocabulary nodes (ψ, γ) to induce power-law. We initialize β to 0.5, and set its hyperprior to Gamma(0.1, 0.1). We fix the hyperparameters λ’s, s, l and σ to 1 since their values have no significant impact on model performance. In the following evaluations, we run the sampling algorithms for 2000 iterations for the training likelihood to converge. We repeat each experiment five times to reduce the estimation error of the evaluation measures. In the experiments for the TN topic model, we achieve a better computational efficiency by first running the collapsed Gibbs\n1Supplementary material is available online at the authors’ websites. 2http://snap.stanford.edu/data/twitter7.html\nTable 1: Perplexity & network log-likelihood\nPerplexity Network HDP-LDA 358.1±6.7 N/A ATM 302.9±8.1 N/A Random Function N/A −294.6±5.9 No Author 243.8±3.4 N/A No Hashtag 307.5±8.3 −269.2±9.5 No µ1 node 221.3±3.9 −271.2±5.2 No Word-tag link 217.6±6.3 −275.0±10.1 No Power-law 222.5±3.1 −280.8±15.4 No Network 218.4±4.0 N/A Full TN 208.4±3.2 −266.0±6.9\nTable 2: Labeling topics with hashtags\nTop hashtags/words\nT0 #finance #money #economy\nfinance money bank marketwatch stocks china group\nT1 #politics #iranelection #tcot politics iran iranelection tcot\ntlot topprog obama\nT2 #music #folk #pop\nmusic folk monster head pop free indie album gratuit\nTable 3: Topics by authors\nTwitter ID Top topics represented by hashtags finance yard #finance #money #realestate ultimate music #music #ultimatemusiclist #mp3 seriouslytech #technology #web #tech seriouspolitics #politics #postrank #news pr science #science #news #postrank\nTable 4: Cosine similarity\nRecommended 1st 2nd 3rd Original 0.00 0.05 0.06\nTN 0.78 0.57 0.55 Not-recommended 1st 2nd 3rd\nOriginal 0.36 0.33 0.14 TN 0.17 0.09 0.10\nsampling for 1000 iterations before the full inference procedure. In Figure 2, we can see that the TN topic model converges quickly compared to the HDP-LDA and the nonparametric ATM. Also, the training likelihood of the TN topic model becomes better sampling for the network information after 1000 iterations.\nAutomatic Topic Labeling There have been recent attempts to label topics automatically in topic modeling. Here, we show that using hashtag information allows us to get good labels for topics. Table 2 shows topics labeled by the TN topic model. More detailed topic summaries are shown in the supplementary material. We empirically evaluate the suitability of hashtags in representing the topics and found that, consistently, over 90% of the hashtags are good candidates for the topic labels.\nInference on Authors’ Topic Distributions In addition to inference on the topic distribution of each document, the TN topic model allows us to analyze the topic distribution of each author. Table 3 presents a summary of topics by different authors, where topics are obvious from the Twitter ID.\nAuthor Recommendation We illustrate the use of the TN topic model for author recommendation. On a new test dataset with 90451 tweets and 625 new authors, we predict the most similar and dissimilar authors for the new authors, based on the training model of 60370 tweets. We quantify the recommendation quality with the cosine similarities of the authors’ topic distributions for the recommended author pairs. We compare our new kernel function with the original kernel function (denoted as original) used in (Lloyd et al., 2012). Table 4 shows average cosine similarities between the recommended and not-recommended authors. This suggests that our kernel function is more appropriate. Additionally, we manually checked the recommended authors and we found that they usually belong to the same community, i.e., having tweets with similar topics.\nClustering and Topic Coherence We also evaluate the TN topic model against state-of-the-art LDA-based clustering techniques (Mehrotra et al., 2013). We find that the TN topic model outperforms the state-of-the-art in purity, normalized mutual information and pointwise mutual information (PMI). Due to space, the evaluation result is provided in the supplementary material."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We propose a full Bayesian nonparametric Twitter-Network (TN) topic model that jointly models tweets and the associated social network information. Our model employs a nonparametric Bayesian approach by using the PDP and GP, and achieves flexible modeling by performing inference on a\nnetwork of PDPs. Our experiments with Twitter dataset show that the TN topic model achieves significant improvement compared to existing baselines. Furthermore, our ablation study demonstrates the usefulness of each component of the TN model. Our model also shows interesting applications such as author recommendation, as well as providing additional informative inferences.\nWe also engineered a framework for rapid topic model development, which is important due to the complexity of the model. While we could have used Adaptor Grammars (Johnson et al., 2007), our framework yields more efficient computation for topic models.\nFuture work includes speeding up the posterior inference algorithm, especially for the network model, as well as incorporating other auxiliary information that is available in social media such as location, hyperlinks and multimedia contents. We also intend to explore other applications that can be addressed with the TN topic model, such as hashtag recommendation. It is also interesting to apply the TN topic model to other types of data such as blog and publication data."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We would like to thank the anonymous reviewers for their helpful feedback and comments.\nNICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program."
    } ],
    "references" : [ {
      "title" : "How noisy social media text, how diffrnt social media sources? IJCNLP",
      "author" : [ "T. Baldwin", "P. Cook", "M. Lui", "A. MacKinlay", "L. Wang" ],
      "venue" : null,
      "citeRegEx" : "Baldwin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Baldwin et al\\.",
      "year" : 2013
    }, {
      "title" : "Latent Dirichlet Allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "the Journal of machine Learning research, 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Bayesian networks on dirichlet distributed vectors",
      "author" : [ "W. Buntine", "L. Du", "P. Nurmi" ],
      "venue" : "pages 33–40.",
      "citeRegEx" : "Buntine et al\\.,? 2010",
      "shortCiteRegEx" : "Buntine et al\\.",
      "year" : 2010
    }, {
      "title" : "Hierarchical relational models for document networks",
      "author" : [ "J. Chang", "D.M. Blei" ],
      "venue" : "The Annals of Applied Statistics, 4(1):124–150.",
      "citeRegEx" : "Chang and Blei,? 2010",
      "shortCiteRegEx" : "Chang and Blei",
      "year" : 2010
    }, {
      "title" : "Sampling table configurations for the hierarchical Poisson-Dirichlet process",
      "author" : [ "C. Chen", "L. Du", "W. Buntine" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, pages 296–311. Springer.",
      "citeRegEx" : "Chen et al\\.,? 2011",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2011
    }, {
      "title" : "What to do about bad language on the internet",
      "author" : [ "J. Eisenstein" ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia. Association for Computational Linguistics.",
      "citeRegEx" : "Eisenstein,? 2013",
      "shortCiteRegEx" : "Eisenstein",
      "year" : 2013
    }, {
      "title" : "Interpolating between types and tokens by estimating power-law generators",
      "author" : [ "S. Goldwater", "T. Griffiths", "M. Johnson" ],
      "venue" : "Advances in neural information processing systems, 18:459.",
      "citeRegEx" : "Goldwater et al\\.,? 2006",
      "shortCiteRegEx" : "Goldwater et al\\.",
      "year" : 2006
    }, {
      "title" : "Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models",
      "author" : [ "M. Johnson", "T.L. Griffiths", "S. Goldwater" ],
      "venue" : "Advances in neural information processing systems, 19:641.",
      "citeRegEx" : "Johnson et al\\.,? 2007",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2007
    }, {
      "title" : "Topic-link LDA: joint models of topic and author community",
      "author" : [ "Y. Liu", "A. Niculescu-Mizil", "W. Gryc" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning, pages 665–672. ACM.",
      "citeRegEx" : "Liu et al\\.,? 2009",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Random function priors for exchangeable arrays with applications to graphs and relational data",
      "author" : [ "J. Lloyd", "P. Orbanz", "Z. Ghahramani", "D. Roy" ],
      "venue" : "Advances in Neural Information Processing Systems 25, pages 1007–1015.",
      "citeRegEx" : "Lloyd et al\\.,? 2012",
      "shortCiteRegEx" : "Lloyd et al\\.",
      "year" : 2012
    }, {
      "title" : "langid.py: An off-the-shelf language identification tool",
      "author" : [ "M. Lui", "T. Baldwin" ],
      "venue" : "In Proceedings of the ACL 2012 System Demonstrations,",
      "citeRegEx" : "Lui and Baldwin,? \\Q2012\\E",
      "shortCiteRegEx" : "Lui and Baldwin",
      "year" : 2012
    }, {
      "title" : "Improving LDA topic models for microblogs via tweet pooling and automatic labeling",
      "author" : [ "R. Mehrotra", "S. Sanner", "W. Buntine", "L. Xie" ],
      "venue" : "The 36th Annual ACM SIGIR Conference, page 4, Dublin/Ireland.",
      "citeRegEx" : "Mehrotra et al\\.,? 2013",
      "shortCiteRegEx" : "Mehrotra et al\\.",
      "year" : 2013
    }, {
      "title" : "Nonparametric latent feature models for link prediction",
      "author" : [ "K. Miller", "M.I. Jordan", "T.L. Griffiths" ],
      "venue" : "Advances in neural information processing systems, pages 1276–1284.",
      "citeRegEx" : "Miller et al\\.,? 2009",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2009
    }, {
      "title" : "Joint latent topic models for text and citations",
      "author" : [ "R. Nallapati", "A. Ahmed", "E.P. Xing" ],
      "venue" : "KDD.",
      "citeRegEx" : "Nallapati et al\\.,? 2008",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2008
    }, {
      "title" : "It is not just what we say, but how we say them: Lda-based behavior-topic model",
      "author" : [ "M. Qiu", "F. Zhu", "J. Jiang" ],
      "venue" : "2013 SIAM International Conference on Data Mining (SDM’13).",
      "citeRegEx" : "Qiu et al\\.,? 2013",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2013
    }, {
      "title" : "The author-topic model for authors and documents",
      "author" : [ "M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth" ],
      "venue" : "Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 487–494. AUAI Press.",
      "citeRegEx" : "Rosen.Zvi et al\\.,? 2004",
      "shortCiteRegEx" : "Rosen.Zvi et al\\.",
      "year" : 2004
    }, {
      "title" : "Topic models with power-law using pitman-yor process",
      "author" : [ "I. Sato", "H. Nakagawa" ],
      "venue" : "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’10, pages 673–682, New York, NY, USA. ACM.",
      "citeRegEx" : "Sato and Nakagawa,? 2010",
      "shortCiteRegEx" : "Sato and Nakagawa",
      "year" : 2010
    }, {
      "title" : "Hierarchical Bayesian nonparametric models with applications",
      "author" : [ "Y.W. Teh", "M.I. Jordan" ],
      "venue" : "Bayesian Nonparametrics: Principles and Practice. Cambridge University Press.",
      "citeRegEx" : "Teh and Jordan,? 2010",
      "shortCiteRegEx" : "Teh and Jordan",
      "year" : 2010
    }, {
      "title" : "A tag-topic model for blog mining",
      "author" : [ "F.S. Tsai" ],
      "venue" : "Expert Systems with Applications, 38(5):5330–5335.",
      "citeRegEx" : "Tsai,? 2011",
      "shortCiteRegEx" : "Tsai",
      "year" : 2011
    }, {
      "title" : "A biterm topic model for short texts",
      "author" : [ "X. Yan", "J. Guo", "Y. Lan", "X. Cheng" ],
      "venue" : "Proceedings of the 22nd international conference on World Wide Web, WWW ’13, pages 1445–1456, Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee.",
      "citeRegEx" : "Yan et al\\.,? 2013",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2013
    }, {
      "title" : "Patterns of temporal variation in online media",
      "author" : [ "J. Yang", "J. Leskovec" ],
      "venue" : "Proceedings of the fourth ACM international conference on Web search and data mining, pages 177–186. ACM.",
      "citeRegEx" : "Yang and Leskovec,? 2011",
      "shortCiteRegEx" : "Yang and Leskovec",
      "year" : 2011
    }, {
      "title" : "Comparing twitter and traditional media using topic models",
      "author" : [ "W.X. Zhao", "J. Jiang", "J. Weng", "J. He", "Lim", "E.-P.", "H. Yan", "X. Li" ],
      "venue" : "Proceedings of the 33rd European conference on Advances in information retrieval, ECIR’11, pages 338–349, Berlin, Heidelberg. Springer-Verlag.",
      "citeRegEx" : "Zhao et al\\.,? 2011",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a popular form of topic model.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : "Unfortunately, a direct application of LDA on tweets yields poor result as tweets are short and often noisy (Zhao et al., 2011), i.",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "In other text analysis applications, tweets are often ‘cleansed’ by NLP methods such as lexical normalization (Baldwin et al., 2013).",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "However, the use of normalization is also criticized (Eisenstein, 2013).",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "LDA is often extended for different types of data, some notable examples that use auxiliary information are the author-topic model (Rosen-Zvi et al., 2004), the tag-topic model (Tsai, 2011), and Topic-Link LDA (Liu et al.",
      "startOffset" : 131,
      "endOffset" : 155
    }, {
      "referenceID" : 18,
      "context" : ", 2004), the tag-topic model (Tsai, 2011), and Topic-Link LDA (Liu et al.",
      "startOffset" : 29,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : ", 2004), the tag-topic model (Tsai, 2011), and Topic-Link LDA (Liu et al., 2009).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "Twitter-LDA (Zhao et al., 2011) and the behavior-topic model (Qiu et al.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : ", 2011) and the behavior-topic model (Qiu et al., 2013) were designed to explicitly model tweets.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 19,
      "context" : "On the other hand, the biterm topic model (Yan et al., 2013) uses only the biterm co-occurrence to model tweets, discarding document level information.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "To sidestep the need of choosing the number of topics, (Teh and Jordan, 2010) proposed Hierarchical Dirichlet process (HDP) LDA, which utilizes the Dirichlet process (DP) as nonparametric prior.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "In natural languages, the distribution of word frequencies exhibits a power-law (Goldwater et al., 2006).",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "For topic models, replacing the Dirichlet distribution with the PDP can yield great improvement (Sato and Nakagawa, 2010).",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "Some recent work models text data with network information ((Liu et al., 2009; Chang and Blei, 2010; Nallapati et al., 2008)), however, these models are parametric in nature and can be restrictive.",
      "startOffset" : 60,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "Some recent work models text data with network information ((Liu et al., 2009; Chang and Blei, 2010; Nallapati et al., 2008)), however, these models are parametric in nature and can be restrictive.",
      "startOffset" : 60,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "Some recent work models text data with network information ((Liu et al., 2009; Chang and Blei, 2010; Nallapati et al., 2008)), however, these models are parametric in nature and can be restrictive.",
      "startOffset" : 60,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "(Miller et al., 2009) and Lloyd et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "(Lloyd et al., 2012) model network data directly with nonparametric priors, i.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "We emphasize that our treatment of the network model is different to that of (Lloyd et al., 2012).",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "We develop a framework to perform collapse Gibbs sampling generally on any Bayesian network of PDPs, built upon the work of (Buntine et al., 2010; Chen et al., 2011), which allows quick prototyping and development of new variants of topic model.",
      "startOffset" : 124,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "We develop a framework to perform collapse Gibbs sampling generally on any Bayesian network of PDPs, built upon the work of (Buntine et al., 2010; Chen et al., 2011), which allows quick prototyping and development of new variants of topic model.",
      "startOffset" : 124,
      "endOffset" : 165
    }, {
      "referenceID" : 20,
      "context" : "We use two tweets corpus for experiments, first is a subset of Twitter7 dataset2 (Yang and Leskovec, 2011), obtained by querying with certain keywords (e.",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "py (Lui and Baldwin, 2012) and filter authors who do not have network information and who authored less than 100 tweets.",
      "startOffset" : 3,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "Second tweets corpus is obtained from (Mehrotra et al., 2013), which contains a total of 781186 tweets.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "We compare our new kernel function with the original kernel function (denoted as original) used in (Lloyd et al., 2012).",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 11,
      "context" : "Clustering and Topic Coherence We also evaluate the TN topic model against state-of-the-art LDA-based clustering techniques (Mehrotra et al., 2013).",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "While we could have used Adaptor Grammars (Johnson et al., 2007), our framework yields more efficient computation for topic models.",
      "startOffset" : 42,
      "endOffset" : 64
    } ],
    "year" : 2016,
    "abstractText" : "Twitter data is extremely noisy – each tweet is short, unstructured and with informal language, a challenge for current topic modeling. On the other hand, tweets are accompanied by extra information such as authorship, hashtags and the user-follower network. Exploiting this additional information, we propose the Twitter-Network (TN) topic model to jointly model the text and the social network in a full Bayesian nonparametric way. The TN topic model employs the hierarchical Poisson-Dirichlet processes (PDP) for text modeling and a Gaussian process random function model for social network modeling. We show that the TN topic model significantly outperforms several existing nonparametric models due to its flexibility. Moreover, the TN topic model enables additional informative inference such as authors’ interests, hashtag analysis, as well as leading to further applications such as author recommendation, automatic topic labeling and hashtag suggestion. Note our general inference framework can readily be applied to other topic models with embedded PDP nodes.",
    "creator" : "LaTeX with hyperref package"
  }
}