{
  "name" : "1505.02425.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "mheilman@ets.org", "sagae@ict.usc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 5.\n02 42\n5v 1\n[ cs\n.C L\n] 1\n0 M\nay 2\n01 5\nFast Rhetorical Structure Theory Discourse Parsing\nMichael Heilman Educational Testing Service\nPrinceton, NJ, USA mheilman@ets.org\nKenji Sagae Institute for Creative Technologies University of Southern California\nLos Angeles, CA, USA sagae@ict.usc.edu"
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, There has been a variety of research on discourse parsing, particularly RST discourse parsing (Feng and Hirst, 2014; Li et al., 2014b; Ji and Eisenstein, 2014; Joty and Moschitti, 2014; Li et al., 2014a). Most of the recent work on RST parsing has focused on implementing new types of features or learning algorithms in order to improve accuracy, with relatively little focus on efficiency, robustness, or practical use. Also, most implementations are not widely available.\nHere, we describe an RST segmentation and parsing system that adapts models and feature sets from various previous work, as described below. Its accuracy is near state-of-the-art, and it was developed to be fast, robust, and practical. For example, it can process short documents such as news articles or essays in less than a second.\nThe system is written in Python and is publicly available at https://github. com/EducationalTestingService/ discourse-parsing."
    }, {
      "heading" : "2 Tasks and Data",
      "text" : "We address two tasks in this work: discourse segmentation and discourse parsing. Discourse segmentation is the task of taking a sequence of word and punctuation tokens as input and identifying boundaries where new discourse units begin. Discourse parsing is the task of taking a sequence of discourse units and identifying relationships between them. In our case, the set of these relationships form a tree.\nFor both, we follow the conventions encoded in the RST Discourse Treebank (Carlson et al., 2002). Here, we give a brief overview of the corpus. See Carlson et al. (2001) for more information.\nThe treebank uses a representation where discourse is represented as a tree, with labels on nodes indicating relationships between siblings. Most RST relationships have a nucleus, expressing the core content, and a satellite that contributes additional information to the nucleus. Probably the simplest example is the “attribution” relationship: attributed (e.g., quoted) text is labeled as the nucleus, and text indicating the source of the attributed text is labeled as the satellite, with an “attribution” subcategorization.\nThe leaves of the RST trees are “elementary discourse units” (EDUs), which are contiguous spans of tokens roughly similar to indepedent clauses. Most branching in RST trees is binary, with one satellite and one nucleus, though there are some relations that have multiple nuclei and no satellite (e.g., lists).\nThe RST corpus consists of a training set of 347 documents and a test set of 38 documents. The texts in the RST treebank are a subset of those in the Penn Treebank (Marcus et al., 1993). For this reason, we retrained the syntactic parser used in our system, ZPar (Zhang and Clark, 2011), on the subset of the Penn Treebank WSJ sections 2 to 21 texts not present in the RST treebank.\nFor development of the system, we split the training set into a smaller subset for model estimation and a development validation set similar in size (n = 40) to the RST treebank test set."
    }, {
      "heading" : "3 Discourse Segmenter Description",
      "text" : "In this section, we describe and evaluate the discourse segmentation component of the system. Our discourse segmenter is essentially a reimplementation of the baseline system from Xuan Bach et al. (2012). We do not implement their reranked model, which is more complex to implement and probably less efficient, and we use the ZPar parser (Zhang and Clark, 2011) for automatic syntactic parsing."
    }, {
      "heading" : "3.1 Segmenter Model and Features",
      "text" : "Following Xuan Bach et al. (2012), we model RST as a tagging problem. Specifically, for each token in a sentence, the system predicts whether that token is the beginning of a new EDU or the continuation of an EDU. For this task, we use a conditional random field (Lafferty et al., 2001) model with ℓ2 regularization, using the CRF++ implementation (https://crfpp.googlecode.com). Also, we assume that a new sentence always starts a new EDU, regardless of the CRF output.\nThe CRF uses simple word and POS features as well as syntactic features. The word and POS features are as follows (note that by “word”, we mean word or punctuation token):\n• the lowercased form of the current word\n• the part-of-speech (POS) of the current word\nThe syntactic features are based on automatic parses from ZPar (using a retrained model as discussed in §2). For each of the following nodes in the syntactic tree, there are two features, one for the nonterminal symbol and the head word (e.g., “VP, said”), and one for the nonterminal symbol and the head POS (e.g., “VP, VBD”). Note that these features will not be used for the last token in a sentence since there is no subsequent token.\n• Np: the first common ancestor of the current token and the subsequent word\n• the subtree of Np that contains the current word\n• the subtree of Np that contains the subsequent word\n• the parent of Np\n• the right sibling of Np\nAll of these features are extracted for the current word, the previous 2 words, and next 2 words in the sentence."
    }, {
      "heading" : "3.2 Segmenter Evaluation",
      "text" : "Following Xuan Bach et al. (2012), we evaluate segmentation performance using the gold standard EDUs from the RST treebank test set, using the F1 score for the tag indicating the beginning of a new EDU (“B-EDU”). Since new sentences always begin new EDUs, we exclude the first tag in the output (always “B-EDU”) for each sentence. We first tuned the CRF regularization parameter using grid search on the split of the training set used for development evaluations, using a grid of powers of 2 ranging from 1/64 to 64.\nThe results are shown in Table 1. For comparison, we include previous results, including humanhuman agreement, reported by Xuan Bach et al. (2012), using syntax from the Stanford Parser (Klein and Manning, 2003a) (it is not clear from the paper what parsing model was used). The “CRFSeg” results are for the system from Hernault et al. (2010).\nWe are uncertain as to the cause for the observed differences in performance, though we hypothesize that the differences are at least partially due to differences in syntactic parsing, which is a key step in feature computation."
    }, {
      "heading" : "4 Discourse Parser Description",
      "text" : "In this section, we describe our RST parser. It borrows extensively from previous work, especially Sagae (2009).1\n1Note that we do not include Sagae (2009) in our evaluations since only within-sentence parsing performance was reported in that paper."
    }, {
      "heading" : "4.1 Shift-Reduce Approach",
      "text" : "Following Sagae (2009) and Ji and Eisenstein (2014), we use an “arc standard” shift-reduce approach to RST discourse parsing."
    }, {
      "heading" : "4.2 Parsing Model",
      "text" : "The parser maintains two primary data structures: a queue containing the EDUs in the document that have not been processed yet, and a stack of RST subtrees that will eventually be combined to form a complete tree.\nInitially, the stack is empty and all EDUs are placed in the queue. Until a complete tree is found or no actions can be performed, the parser iteratively chooses to perform shift or reduce actions. The shift action creates a new subtree for the next EDU on the queue.\nReduce actions create new subtrees from the subtrees on the top of the stack. There are multiple types reduce actions. First, there are unary or binary versions of reduce actions, depending on whether the top 1 or 2 items on the stack will be included as children in the subtree to be created. Second, there are versions for each of the nonterminal labels (e.g., “satellite:attribution”).\nFollowing previous work, we collapse the full set of RST relations to 18 labels. Additionally, we binarize trees as described by Sagae and Lavie (2005).\nFollowing Sagae (2009) and Ji and Eisenstein (2014), we treat the problem of selecting the best parsing action given the current parsing state (i.e., the stack and queue) as a classification problem. We use multi-class logistic regression with an ℓ1 penalty, as implemented in the scikit-learn package, to estimate our classifier.\nThe parser supports beam search and k-best parsing, though we use simple greedy parsing (i.e., we set the beam size and k to 1) for the experiments described here."
    }, {
      "heading" : "4.3 Parsing Features",
      "text" : "To select the next shift or reduce action, the parsing model considers a variety of lexical, syntactic, and positional features adapted from various previous work on RST discourse parsing, such as that of Sagae (2009) and the systems we compare to in §5. The features are as follows:\n• the previous action (e.g., “binary reduce to satellite:attribution”)\n• the nonterminal symbols of the nth subtree on the stack (n = 0, 1, 2), and their combinations\n• the nonterminal symbols of the children of the nth subtree on the stack (n = 0, 1, 2)\n• the lowercased words (and POS tags) for the tokens in the head EDU for the nth subtree on the stack (n = 0, 1) and the first EDU on the queue\n• whether, for pairs of the top 3 stack subtrees and the 1st queue item, the distance (in EDU indices) between the EDU head is greater than n (n = 1, 2, 3, 4)\n• whether, for pairs of the top 3 stack subtrees and the 1st queue item, the head EDUs are in the same sentence\n• for the head EDUs of top 3 stack subtrees and the 1st queue item, the syntactic head word (lowercased), head POS, and the nonterminal symbol of the highest node in the subtree\n• syntactic dominance features between pairs of the top 3 stack items and 1st queue item, similar to (Soricut and Marcu, 2003)\n• for each of the first 3 stack items or 1st queue item, whether that item starts a new paragraph"
    }, {
      "heading" : "5 Parsing Experiments",
      "text" : "Following (Marcu, 2000, pp. 143–144) and other recent work, we evaluate our system according to the F1 score over labeled and unlabeled spans of discourse units in the RST treebank test set. This evaluation is analogous to the evalb bracket scoring program commonly used for constituency parsing (http://nlp.cs.nyu.edu/evalb/). For comparison with previous results, we use gold standard discourse segmentations (but automatic syntactic parses from ZPar).\nWe report F1 scores for agreement with the gold standard on unlabeled EDU spans (“span”), spans labeled only with nuclearity (“nuclearity”), and fully labeled spans that include relation information (“relation”).\nWe first tuned the ℓ1 regularization parameter using grid search on the split of the training set used\nfor development evaluations, using a grid of powers of 2 ranging from 1/16 to 16. We selected the setting that led to the highest F1 score for fully labeled spans (i.e., relation F1).\nWe compare to recently reported results from Ji and Eisenstein (2014) (their DPLP general +features model), Feng and Hirst (2014), Li et al. (2014b), Joty and Moschitti (2014), Li et al. (2014a), and Joty and Moschitti (2014).2 The results are shown in Table 2. The human agreement statistics were originally reported by Ji and Eisenstein (2014). For each system, the table indicates the source of POS tags and syntactic parse trees (“Penn Treebank” means that gold standard Penn Treebank trees and tags were used).\nWe observe that our system is relatively close to the others in terms of F1 scores. We hypothesize that the differences in performance are at least partially due to differences in syntactic parsing.\n2Joty and Moschitti (2014) and Joty and Moschitti (2014) do not explicitly state the source of syntactic parsers, but we infer from Joty et al. (2012) that the Charniak (2000) parser was used, with a model trained on a subset of the Penn Treebank that did not include the RST treebank test set."
    }, {
      "heading" : "5.1 The effect of automatic syntax parsing",
      "text" : "In order to show the effect of using automatic parsing, we report performance on the development set (§2), using either gold standard syntax trees from the Penn Treebank or the automatic syntax trees from our retrained ZPar model (§2) for computing features. The F1 scores are shown in Table 3 (note that we are reporting results using the optimal settings from grid search on the development set).\nIt appears that the performance difference between using automatic rather than gold standard syntax is about 1 to 2 points of F1 score."
    }, {
      "heading" : "5.2 Parsing Speed",
      "text" : "In this section, we evaluate the speed of the parser. Most previous papers on RST parsing do not report runtime experiments, and most systems are not widely available or easy to replicate.\nOur parser uses a shift-reduce parsing algorithm that has a worst-case runtime that is linear in the number of EDUs. For comparison, Li et al. (2014b) employ a quadratic time maximum spanning tree parsing approach. The approach from Joty et al. (2013) also uses a polynominal runtime algorithm.\nOther linear time parsers have been developed (Feng and Hirst, 2014; Ji and Eisenstein, 2014). However, feature computation can also be a per-\nformance bottleneck. Feng and Hirst (2014) report an average parsing time of 10.71 seconds for RST treebank test set documents (and 5.52 seconds for a variant) on a system with “four duo-core 3.0 GHz processors”, not including time for preprocessing or discourse segmentation. In contrast, our system takes less than half a second per test set document on average (mean = 0.40, S.D. = 0.40, min. = 0.02, max. = 1.85 seconds) on a 2013 MacBook Pro with an i7-4850HQ CPU at 2.30 GHz. Of course, these performance measurements are not completely comparable since they were run on different hardware. The preprocessing (ZPar) and segmentation (§3.1) steps are also similarly fast."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we have presented a fast shiftreduce RST discourse segmenter and parser. The parser achieves near state-of-the-art accuracy and processes Penn Treebank documents in less than a second, which is about an order of magnitude faster than recent results reported by Feng and Hirst (2014)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Dan Blanchard, Xinhao Wang, and Keelan Evanini for feedback about the paper. We would also like to thank Dan Blanchard, Diane Napolitano, Nitin Madnani, Aoife Cahill, Chong Min Lee, Michael Flor, and Keisuke Sakaguchi for initial help with and feedback about the implementation."
    } ],
    "references" : [ {
      "title" : "Building a discourse-tagged corpus in the framework of rhetorical structure theory",
      "author" : [ "Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski." ],
      "venue" : "Proceedings of the Second SIGdial Workshop on Discourse and Dialogue - Volume 16, SIGDIAL ’01, pages 1–10,",
      "citeRegEx" : "Carlson et al\\.,? 2001",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2001
    }, {
      "title" : "A maximum-entropy-inspired parser",
      "author" : [ "Eugene Charniak." ],
      "venue" : "1st Meeting of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Charniak.,? 2000",
      "shortCiteRegEx" : "Charniak.",
      "year" : 2000
    }, {
      "title" : "A lineartime bottom-up discourse parser with constraints and post-editing",
      "author" : [ "Vanessa Wei Feng", "Graeme Hirst." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 511–521, Baltimore,",
      "citeRegEx" : "Feng and Hirst.,? 2014",
      "shortCiteRegEx" : "Feng and Hirst.",
      "year" : 2014
    }, {
      "title" : "A sequential model for discourse segmentation",
      "author" : [ "Hugo Hernault", "Danushka Bollegala", "Mitsuru Ishizuka." ],
      "venue" : "Proceedings of the 11th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing’10, pages 315–326,",
      "citeRegEx" : "Hernault et al\\.,? 2010",
      "shortCiteRegEx" : "Hernault et al\\.",
      "year" : 2010
    }, {
      "title" : "Representation learning for text-level discourse parsing",
      "author" : [ "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13–24, Baltimore, Maryland, June. Association",
      "citeRegEx" : "Ji and Eisenstein.,? 2014",
      "shortCiteRegEx" : "Ji and Eisenstein.",
      "year" : 2014
    }, {
      "title" : "Discriminative reranking of discourse parses using tree kernels",
      "author" : [ "Shafiq Joty", "Alessandro Moschitti." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2049–2060, Doha, Qatar, October. Association",
      "citeRegEx" : "Joty and Moschitti.,? 2014",
      "shortCiteRegEx" : "Joty and Moschitti.",
      "year" : 2014
    }, {
      "title" : "A novel discriminative framework for sentence-level discourse analysis",
      "author" : [ "Shafiq Joty", "Giuseppe Carenini", "Raymond Ng." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
      "citeRegEx" : "Joty et al\\.,? 2012",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2012
    }, {
      "title" : "Combining intra- and multisentential rhetorical parsing for document-level discourse analysis",
      "author" : [ "Shafiq Joty", "Giuseppe Carenini", "Raymond Ng", "Yashar Mehdad." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Joty et al\\.,? 2013",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2013
    }, {
      "title" : "Accurate unlexicalized parsing",
      "author" : [ "Dan Klein", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan, July. Association for Computational Linguistics.",
      "citeRegEx" : "Klein and Manning.,? 2003a",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2003
    }, {
      "title" : "Fast exact inference with a factored model for natural language parsing",
      "author" : [ "Dan Klein", "Christopher D Manning." ],
      "venue" : "S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 3–10. MIT Press.",
      "citeRegEx" : "Klein and Manning.,? 2003b",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2003
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the 18th International Conference",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Recursive deep models for discourse parsing",
      "author" : [ "Jiwei Li", "Rumeng Li", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061– 2069, Doha, Qatar, October. Association for Compu-",
      "citeRegEx" : "Li et al\\.,? 2014a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Text-level discourse dependency parsing",
      "author" : [ "Sujian Li", "Liang Wang", "Ziqiang Cao", "Wenjie Li." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25–35, Baltimore, Maryland, June. As-",
      "citeRegEx" : "Li et al\\.,? 2014b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "The Theory and Practice of Discourse Parsing and Summarization",
      "author" : [ "Daniel Marcu." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Marcu.,? 2000",
      "shortCiteRegEx" : "Marcu.",
      "year" : 2000
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini." ],
      "venue" : "Computational Linguistics, 19(2):313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Maltparser: A language-independent system for data-driven dependency parsing",
      "author" : [ "J.J. Hall J. Nilsson A. Chanev G. Eryigit S. Kbler S. Marinov Nivre", "E. Marsi." ],
      "venue" : "13(2):95–135.",
      "citeRegEx" : "Nivre and Marsi.,? 2007",
      "shortCiteRegEx" : "Nivre and Marsi.",
      "year" : 2007
    }, {
      "title" : "A classifier-based parser with linear run-time complexity",
      "author" : [ "Kenji Sagae", "Alon Lavie." ],
      "venue" : "Proceedings of the Ninth International Workshop on Parsing Technology, pages 125–132, Vancouver, British Columbia, October. Association for Computational",
      "citeRegEx" : "Sagae and Lavie.,? 2005",
      "shortCiteRegEx" : "Sagae and Lavie.",
      "year" : 2005
    }, {
      "title" : "Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce parsing",
      "author" : [ "Kenji Sagae." ],
      "venue" : "Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 81–84, Paris, France, October. Association for Com-",
      "citeRegEx" : "Sagae.,? 2009",
      "shortCiteRegEx" : "Sagae.",
      "year" : 2009
    }, {
      "title" : "Sentence level discourse parsing using syntactic and lexical information",
      "author" : [ "Radu Soricut", "Daniel Marcu." ],
      "venue" : "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technol-",
      "citeRegEx" : "Soricut and Marcu.,? 2003",
      "shortCiteRegEx" : "Soricut and Marcu.",
      "year" : 2003
    }, {
      "title" : "A reranking model for discourse segmentation using subtree features",
      "author" : [ "Ngo Xuan Bach", "Nguyen Le Minh", "Akira Shimazu." ],
      "venue" : "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 160–168, Seoul, South",
      "citeRegEx" : "Bach et al\\.,? 2012",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2012
    }, {
      "title" : "Syntactic processing using the generalized perceptron and beam search",
      "author" : [ "Yue Zhang", "Stephen Clark." ],
      "venue" : "Computational Linguistics, 37(1).",
      "citeRegEx" : "Zhang and Clark.,? 2011",
      "shortCiteRegEx" : "Zhang and Clark.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "In recent years, There has been a variety of research on discourse parsing, particularly RST discourse parsing (Feng and Hirst, 2014; Li et al., 2014b; Ji and Eisenstein, 2014; Joty and Moschitti, 2014; Li et al., 2014a).",
      "startOffset" : 111,
      "endOffset" : 220
    }, {
      "referenceID" : 12,
      "context" : "In recent years, There has been a variety of research on discourse parsing, particularly RST discourse parsing (Feng and Hirst, 2014; Li et al., 2014b; Ji and Eisenstein, 2014; Joty and Moschitti, 2014; Li et al., 2014a).",
      "startOffset" : 111,
      "endOffset" : 220
    }, {
      "referenceID" : 4,
      "context" : "In recent years, There has been a variety of research on discourse parsing, particularly RST discourse parsing (Feng and Hirst, 2014; Li et al., 2014b; Ji and Eisenstein, 2014; Joty and Moschitti, 2014; Li et al., 2014a).",
      "startOffset" : 111,
      "endOffset" : 220
    }, {
      "referenceID" : 5,
      "context" : "In recent years, There has been a variety of research on discourse parsing, particularly RST discourse parsing (Feng and Hirst, 2014; Li et al., 2014b; Ji and Eisenstein, 2014; Joty and Moschitti, 2014; Li et al., 2014a).",
      "startOffset" : 111,
      "endOffset" : 220
    }, {
      "referenceID" : 11,
      "context" : "In recent years, There has been a variety of research on discourse parsing, particularly RST discourse parsing (Feng and Hirst, 2014; Li et al., 2014b; Ji and Eisenstein, 2014; Joty and Moschitti, 2014; Li et al., 2014a).",
      "startOffset" : 111,
      "endOffset" : 220
    }, {
      "referenceID" : 0,
      "context" : "For both, we follow the conventions encoded in the RST Discourse Treebank (Carlson et al., 2002). Here, we give a brief overview of the corpus. See Carlson et al. (2001) for more information.",
      "startOffset" : 75,
      "endOffset" : 170
    }, {
      "referenceID" : 14,
      "context" : "The texts in the RST treebank are a subset of those in the Penn Treebank (Marcus et al., 1993).",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "For this reason, we retrained the syntactic parser used in our system, ZPar (Zhang and Clark, 2011), on the subset of the Penn Treebank WSJ sections 2 to 21 texts not present in the RST treebank.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "We do not implement their reranked model, which is more complex to implement and probably less efficient, and we use the ZPar parser (Zhang and Clark, 2011) for automatic syntactic parsing.",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 19,
      "context" : "Our discourse segmenter is essentially a reimplementation of the baseline system from Xuan Bach et al. (2012). We do not implement their reranked model, which is more complex to implement and probably less efficient, and we use the ZPar parser (Zhang and Clark, 2011) for automatic syntactic parsing.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "For this task, we use a conditional random field (Lafferty et al., 2001) model with l2 regularization, using the CRF++ implementation (https://crfpp.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "Following Xuan Bach et al. (2012), we model RST as a tagging problem.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "Following Xuan Bach et al. (2012), we evaluate segmentation performance using the gold standard EDUs from the RST treebank test set, using the F1 score for the tag indicating the beginning of a new EDU (“B-EDU”).",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "(2012), using syntax from the Stanford Parser (Klein and Manning, 2003a) (it is not clear from the paper what parsing model was used).",
      "startOffset" : 46,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : "For comparison, we include previous results, including humanhuman agreement, reported by Xuan Bach et al. (2012), using syntax from the Stanford Parser (Klein and Manning, 2003a) (it is not clear from the paper what parsing model was used).",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "The “CRFSeg” results are for the system from Hernault et al. (2010).",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "borrows extensively from previous work, especially Sagae (2009).1",
      "startOffset" : 51,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "Note that we do not include Sagae (2009) in our evaluations since only within-sentence parsing performance was reported in that paper.",
      "startOffset" : 28,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "Following Sagae (2009) and Ji and Eisenstein (2014), we use an “arc standard” shift-reduce approach to RST discourse parsing.",
      "startOffset" : 10,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "Following Sagae (2009) and Ji and Eisenstein (2014), we use an “arc standard” shift-reduce approach to RST discourse parsing.",
      "startOffset" : 27,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "Additionally, we binarize trees as described by Sagae and Lavie (2005).",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "Following Sagae (2009) and Ji and Eisenstein (2014), we treat the problem of selecting the best parsing action given the current parsing state (i.",
      "startOffset" : 10,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "Following Sagae (2009) and Ji and Eisenstein (2014), we treat the problem of selecting the best parsing action given the current parsing state (i.",
      "startOffset" : 27,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "To select the next shift or reduce action, the parsing model considers a variety of lexical, syntactic, and positional features adapted from various previous work on RST discourse parsing, such as that of Sagae (2009) and the systems we compare to in §5.",
      "startOffset" : 205,
      "endOffset" : 218
    }, {
      "referenceID" : 18,
      "context" : "• syntactic dominance features between pairs of the top 3 stack items and 1st queue item, similar to (Soricut and Marcu, 2003)",
      "startOffset" : 101,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "1 Li et al. (2014a) Stanford 84.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "6 Joty et al. (2013) Charniak (retrained) 82.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "(2013) Charniak (retrained) 82.5 68.4 55.7 Joty and Moschitti (2014) Charniak (retrained) – – 57.",
      "startOffset" : 7,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "(2013) Charniak (retrained) 82.5 68.4 55.7 Joty and Moschitti (2014) Charniak (retrained) – – 57.3 Feng and Hirst (2014) Stanford 85.",
      "startOffset" : 7,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "(2013) Charniak (retrained) 82.5 68.4 55.7 Joty and Moschitti (2014) Charniak (retrained) – – 57.3 Feng and Hirst (2014) Stanford 85.7 71.0 58.2 Li et al. (2014b) Penn Treebank 82.",
      "startOffset" : 7,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "(2013) Charniak (retrained) 82.5 68.4 55.7 Joty and Moschitti (2014) Charniak (retrained) – – 57.3 Feng and Hirst (2014) Stanford 85.7 71.0 58.2 Li et al. (2014b) Penn Treebank 82.9 73.0 60.6 Ji and Eisenstein (2014) MALT 81.",
      "startOffset" : 7,
      "endOffset" : 217
    }, {
      "referenceID" : 9,
      "context" : "“syntax” indicates the source of POS tags and syntactic parse trees: “Stanford” refers to the Stanford parser (Klein and Manning, 2003b), “MALT” refers to Nivre and Marsi (2007), and “Charniak” refers to Charniak (2000).",
      "startOffset" : 110,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "“syntax” indicates the source of POS tags and syntactic parse trees: “Stanford” refers to the Stanford parser (Klein and Manning, 2003b), “MALT” refers to Nivre and Marsi (2007), and “Charniak” refers to Charniak (2000).",
      "startOffset" : 111,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : "“syntax” indicates the source of POS tags and syntactic parse trees: “Stanford” refers to the Stanford parser (Klein and Manning, 2003b), “MALT” refers to Nivre and Marsi (2007), and “Charniak” refers to Charniak (2000).",
      "startOffset" : 184,
      "endOffset" : 220
    }, {
      "referenceID" : 3,
      "context" : "We compare to recently reported results from Ji and Eisenstein (2014) (their DPLP general +features model), Feng and Hirst (2014), Li et al.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "We compare to recently reported results from Ji and Eisenstein (2014) (their DPLP general +features model), Feng and Hirst (2014), Li et al.",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "We compare to recently reported results from Ji and Eisenstein (2014) (their DPLP general +features model), Feng and Hirst (2014), Li et al. (2014b), Joty and Moschitti (2014), Li et al.",
      "startOffset" : 108,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "We compare to recently reported results from Ji and Eisenstein (2014) (their DPLP general +features model), Feng and Hirst (2014), Li et al. (2014b), Joty and Moschitti (2014), Li et al.",
      "startOffset" : 108,
      "endOffset" : 176
    }, {
      "referenceID" : 2,
      "context" : "We compare to recently reported results from Ji and Eisenstein (2014) (their DPLP general +features model), Feng and Hirst (2014), Li et al. (2014b), Joty and Moschitti (2014), Li et al. (2014a), and Joty and Moschitti (2014).",
      "startOffset" : 108,
      "endOffset" : 195
    }, {
      "referenceID" : 2,
      "context" : "We compare to recently reported results from Ji and Eisenstein (2014) (their DPLP general +features model), Feng and Hirst (2014), Li et al. (2014b), Joty and Moschitti (2014), Li et al. (2014a), and Joty and Moschitti (2014).2 The results are shown in Table 2.",
      "startOffset" : 108,
      "endOffset" : 226
    }, {
      "referenceID" : 2,
      "context" : "We compare to recently reported results from Ji and Eisenstein (2014) (their DPLP general +features model), Feng and Hirst (2014), Li et al. (2014b), Joty and Moschitti (2014), Li et al. (2014a), and Joty and Moschitti (2014).2 The results are shown in Table 2. The human agreement statistics were originally reported by Ji and Eisenstein (2014). For each system, the table indicates the source of POS tags and syntactic parse trees (“Penn Treebank” means",
      "startOffset" : 108,
      "endOffset" : 346
    }, {
      "referenceID" : 1,
      "context" : "(2012) that the Charniak (2000) parser was used, with a model trained on a subset of the Penn Treebank that did not include the RST treebank test set.",
      "startOffset" : 16,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "Other linear time parsers have been developed (Feng and Hirst, 2014; Ji and Eisenstein, 2014).",
      "startOffset" : 46,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "Other linear time parsers have been developed (Feng and Hirst, 2014; Ji and Eisenstein, 2014).",
      "startOffset" : 46,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "For comparison, Li et al. (2014b) employ a quadratic time maximum spanning tree parsing approach.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "The approach from Joty et al. (2013) also uses a polynominal runtime algorithm.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "Feng and Hirst (2014) report an average parsing time of 10.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "The parser achieves near state-of-the-art accuracy and processes Penn Treebank documents in less than a second, which is about an order of magnitude faster than recent results reported by Feng and Hirst (2014).",
      "startOffset" : 188,
      "endOffset" : 210
    } ],
    "year" : 2015,
    "abstractText" : "In recent years, There has been a variety of research on discourse parsing, particularly RST discourse parsing (Feng and Hirst, 2014; Li et al., 2014b; Ji and Eisenstein, 2014; Joty and Moschitti, 2014; Li et al., 2014a). Most of the recent work on RST parsing has focused on implementing new types of features or learning algorithms in order to improve accuracy, with relatively little focus on efficiency, robustness, or practical use. Also, most implementations are not widely available. Here, we describe an RST segmentation and parsing system that adapts models and feature sets from various previous work, as described below. Its accuracy is near state-of-the-art, and it was developed to be fast, robust, and practical. For example, it can process short documents such as news articles or essays in less than a second. The system is written in Python and is publicly available at https://github. com/EducationalTestingService/ discourse-parsing.",
    "creator" : "dvips(k) 5.991 Copyright 2011 Radical Eye Software"
  }
}