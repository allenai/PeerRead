{
  "name" : "1602.07614.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Model of Selective Advantage for the Efficient Inference of Cancer Clonal Evolution",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "UNIVERSITY OF MILANO BICOCCA Doctorate School of Sciences\nPh.D Program in Computer Science XXVIII Cycle\nA Model of Selective Advantage for the Efficient Inference of\nCancer Clonal Evolution\nDoctoral Thesis of: Daniele Ramazzotti\nAdvisor: Professor Marco Antoniotti Tutor: Professor Fabio Stella Supervisor: Professor Giancarlo Mauri Supervisor: Professor Bud Mishra\nReader: Ph.D Giulio Caravagna Reader: Ph.D Alex Graudenzi\nJanuary 2013 - December 2015\nTo my beloved Grandpas, whose memory and affection\nconstantly dwell with me\nACKNOWLEDGMENTS\nThe work presented in my PhD thesis has been possible because of so many great people I worked with in the last three years to which I owe a lot and I want to thank here.\nThe prospect of working on understanding cancer evolution was proposed to me for the first time by my PhD advisor Professor Marco Antoniotti and Professor Giancarlo Mauri at the very start of my PhD in November 2012. At that time, Marco’s group at the University of Milan Bicocca was starting a collaboration with Professor Bud Mishra at the New York University. This project turned out later on to be a great fit for me, changed my understanding of cancer and bioinformatics and made me a researcher. I want to thank for this and for the subsequent constant source of inspiration and guidance Marco, Giancarlo and Bud.\nThe “junior” components of the group arose from this joint collaboration deserve a special thank. In particular I thank Giulio Caravagna and Alex Graudenzi for their continuous daily presence, Loes Olde Loohuis for her priceless “remote” interactions especially in the first half of my PhD and, more recently, Luca De Sano whose technical skills and hard work have been crucial in the development of the algorithms. Giulio, Alex, Loes and Luca working with you paved the way for the achievements of this thesis and it has been for me a great possibility of growth.\nMoreover, I need to thank my PhD tutor, Professor Fabio Stella for his always ready guidance when needed.\nI also thank all my friends that always made me feel their presence even when I was on the other side of the world.\nLastly and foremost, I want to thank my family for the continuous support of all my choices and for their help through the last 3 years and especially my grandpas that were always in the front line in supporting me and whose memory will always be source of great energy.\nIt has been a great journey, thank you all for making this possible!\nABSTRACT\nRecently, there has been a resurgence of interest in rigorous and scalable algorithms for efficient inference of cancer progression using genomic patient data. The motivations are manifold: (i) rapidly growing NGS and single cell data from cancer patients, (ii) long-felt need for novel Data Science and Machine Learning algorithms well-suited for inferring models of cancer progression, and finally, (iii) a desire to understand the temporal and heterogeneous structure of tumor so as to tame its natural progression through most efficacious therapeutic intervention. This thesis presents a multi-disciplinary effort to algorithmically and efficiently model tumor progression involving successive accumulation of genetic alterations, each resulting populations manifesting themselves with a novel cancer phenotype.\nThe framework presented in this work along with efficient algorithms derived from it, represents a novel and versatile approach for inferring cancer progression, whose accuracy and convergence rates surpass other existing techniques. The approach derives its power from many insights from, and contributes to, several fields including algorithms in machine learning, theory of causality, and cancer biology. Furthermore, a versatile and modular pipeline to extract ensemble-level progression models from cross-sectional sequenced cancer genomes is also proposed. The pipeline combines state-of-the-art techniques for sample stratification, driver selection, identification of fitness-equivalent exclusive alterations and progression model inference.\nFurthermore, the results are rigorously validated using synthetic data created with realistic generative models, and empirically interpreted in the context of real cancer datasets; in the later case, biologically significant conclusions revealed by the reconstructed progressions are also highlighted. Specifically, it demonstrates also the pipeline’s ability to reproduce much of the current knowledge on colorectal cancer progression, as well as to suggest novel experimentally verifiable hypotheses. Lastly, it also proves that the proposed framework can be applied, mutatis mutandis, in reconstructing the evolutionary history of cancer clones in single patients, as illustrated by an example with multiple biopsy data from clear cell renal carcinomas.\nCONTENTS"
    }, {
      "heading" : "List of Figures V",
      "text" : ""
    }, {
      "heading" : "List of Tables VI",
      "text" : ""
    }, {
      "heading" : "List of Algorithms VII",
      "text" : ""
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 A model of cancer evolution . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Two facets of inferring cancer progression models . . . . . . . . . . . . . . 4 1.3 State of the art . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8"
    }, {
      "heading" : "2 Modeling cancer clonal evolution 10",
      "text" : "2.1 A probabilistic model of selective advantage . . . . . . . . . . . . . . . . . 10 2.2 Singleton prima facie topologies . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3 Co-occurence prima facie patterns . . . . . . . . . . . . . . . . . . . . . . 14 2.4 Generalization to conjunctive normal form . . . . . . . . . . . . . . . . . . 16"
    }, {
      "heading" : "3 Singleton models of cancer progression 18",
      "text" : "3.1 Problem setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.2 A probabilistic approach to selective advantage . . . . . . . . . . . . . . . 20 3.3 Results and discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.3.1 Extracting progression trees . . . . . . . . . . . . . . . . . . . . . . 22 3.3.2 Optimal shrinkage-like coefficient . . . . . . . . . . . . . . . . . . . 26 3.3.3 Performance measure and synthetic datasets . . . . . . . . . . . . 28 3.3.4 Case studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32"
    }, {
      "heading" : "4 More complex models of cancer progression 37",
      "text" : "4.1 Problem setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.2 A novel efficient framework . . . . . . . . . . . . . . . . . . . . . . . . . . 38\nI"
    }, {
      "heading" : "II CONTENTS",
      "text" : "4.3 Results and discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.3.1 Synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.3.2 Atypical Chronic Myeloid Leukemia (aCML) . . . . . . . . . . . . 46"
    }, {
      "heading" : "5 An R package for TRanslational ONCOlogy 48",
      "text" : "5.1 Package implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 5.2 Use case of TRONCO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 5.3 TCGA MSI/MSS colorectal tumors . . . . . . . . . . . . . . . . . . . . . . 70\n5.3.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 5.3.2 Import of the TCGA data in TRONCO - TCGA-import.R . . . . . 71 5.3.3 Preparing the training datasets - training-samples.R . . . . . . 74 5.3.4 Exclusivity groups in the datasets - training-exclusivity.R . . 76 5.3.5 Reconstruction of the models - training-reconstruction.R . . . 79 5.3.6 Preparing the models for test - validation-samples.R . . . . . . 84 5.3.7 Validation of the models - validation-samples.R . . . . . . . . . 87"
    }, {
      "heading" : "6 Evolution of colorectal cancer 91",
      "text" : "6.1 A pipeline to infer ensemble-level progression models . . . . . . . . . . . . 91 6.2 Clonal evolution of MSI/MSS colorectal tumors . . . . . . . . . . . . . . . 94 6.3 Inference of patient-specific clonal evolution . . . . . . . . . . . . . . . . . 99"
    }, {
      "heading" : "7 Conclusions 101",
      "text" : "7.1 Future works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\nBibliography 103"
    }, {
      "heading" : "A Foundations of causation 121",
      "text" : "A.1 Hume’s regularity theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 A.2 Probabilistic theories of causation . . . . . . . . . . . . . . . . . . . . . . . 123 A.3 Counterfactual theories of causation . . . . . . . . . . . . . . . . . . . . . 126\nA.3.1 Manipulability theories of causation . . . . . . . . . . . . . . . . . 128 A.4 A simplified framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129"
    }, {
      "heading" : "B Learning Bayesian Networks 130",
      "text" : "B.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 B.2 Approaches to learn the structure of a BN . . . . . . . . . . . . . . . . . . 131\nB.2.1 Constraint based approaches . . . . . . . . . . . . . . . . . . . . . 132 B.2.2 Score based approaches . . . . . . . . . . . . . . . . . . . . . . . . 133 B.2.3 Learning logically constrained networks . . . . . . . . . . . . . . . 135\nB.3 Bayesian interpretation of the proposed framework . . . . . . . . . . . . . 135"
    }, {
      "heading" : "C CAPRESE - Supplementary materials 136",
      "text" : "C.1 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 C.2 Synthetic data generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 C.3 Further results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\nII\nCONTENTS III"
    }, {
      "heading" : "D CAPRI - Supplementary materials 146",
      "text" : "D.1 Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\nD.1.1 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 D.1.2 Correctness and expressivity . . . . . . . . . . . . . . . . . . . . . 148\nD.2 Results: synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 D.2.1 Performance with different topologies and small datasets . . . . . . 152 D.2.2 Comparison with other reconstruction techniques . . . . . . . . . . 153 D.2.3 Reconstruction without hypotheses: disjunctive patterns . . . . . . 154 D.2.4 Reconstruction with hypotheses: synthetic lethality . . . . . . . . 154 D.2.5 Execution time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 D.3 Biological examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 D.3.1 Atypical Chronic Myeloid Leukemia . . . . . . . . . . . . . . . . . 156 D.3.2 Ovarian cancer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157"
    }, {
      "heading" : "E CRC Study - Supplementary materials 168",
      "text" : "E.1 TCGA COADREAD project data . . . . . . . . . . . . . . . . . . . . . . 168\nE.1.1 Driver events selection . . . . . . . . . . . . . . . . . . . . . . . . . 169 E.1.2 Mutual exclusivity groups of alterations . . . . . . . . . . . . . . . 170 E.1.3 CAPRI’s execution . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\nE.2 Single-cell synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 E.3 Supplementary tables and figures . . . . . . . . . . . . . . . . . . . . . . . 172"
    }, {
      "heading" : "F The causal structure of discrimination 183",
      "text" : "F.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 F.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 F.3 Suppes-Bayes Causal Network . . . . . . . . . . . . . . . . . . . . . . . . . 188\nF.3.1 Suppes’ constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 F.3.2 Network simplification . . . . . . . . . . . . . . . . . . . . . . . . . 189 F.3.3 Confidence score . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 F.3.4 Expressivity of a SBCN . . . . . . . . . . . . . . . . . . . . . . . . 193\nF.4 Discrimination discovery by random walks . . . . . . . . . . . . . . . . . . 194 F.4.1 Group discrimination and favoritism . . . . . . . . . . . . . . . . . 194 F.4.2 Indirect discrimination . . . . . . . . . . . . . . . . . . . . . . . . . 195 F.4.3 Genuine requirement . . . . . . . . . . . . . . . . . . . . . . . . . . 195 F.4.4 Individual and subgroup discrimination . . . . . . . . . . . . . . . 196 F.5 Experimental Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 F.5.1 Community detection on the SBCN . . . . . . . . . . . . . . . . . . 197 F.5.2 Group discrimination and favoritism . . . . . . . . . . . . . . . . . 199 F.5.3 Genuine requirement . . . . . . . . . . . . . . . . . . . . . . . . . . 200 F.5.4 Subgroup and Individual Discrimination . . . . . . . . . . . . . . . 200 F.5.5 Comparison with prior art . . . . . . . . . . . . . . . . . . . . . . . 201 F.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\nIII\nLIST OF FIGURES\n1.1 Problem statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Selectivity relation in tumor evolution . . . . . . . . . . . . . . . . . . . . 8\n2.1 Prima facie properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.2 Singleton prima facie topology . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3 Co-occurrence selectivity patterns . . . . . . . . . . . . . . . . . . . . . . . 15\n3.1 Prima facie topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.2 Optimal shrinkage-like coefficient for reconstruction performance . . . . . 27 3.3 Optimal λ with datasets of different size . . . . . . . . . . . . . . . . . . . 28 3.4 Comparison on noise-free synthetic data . . . . . . . . . . . . . . . . . . . 30 3.5 Example of reconstructed trees . . . . . . . . . . . . . . . . . . . . . . . . 31 3.6 Reconstruction with noisy synthetic data and λ = 1/2 . . . . . . . . . . . 31 3.7 Tree reconstruction of ovarian cancer progression . . . . . . . . . . . . . . 33 3.8 The setbp1-asxl1 relation in atypical Chronic Myeloid Leukemia . . . . 36\n4.1 Data processing pipeline for cancer progression inference . . . . . . . . . . 38 4.2 Comparative Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.3 Atypical Chronic Myeloid Leukemia . . . . . . . . . . . . . . . . . . . . . 46\n5.1 Oncoprint function in TRONCO . . . . . . . . . . . . . . . . . . . . . . . 60 5.2 Annotated oncoprint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 5.3 RAS oncoprint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.4 TET/IDH2 oncoprint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 5.5 Final dataset for CAPRI . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 5.6 Reconstruction by CAPRI . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 5.7 Reconstruction by CAPRI and Bootstrap . . . . . . . . . . . . . . . . . . 68 5.8 Reconstruction by CAPRESE and Bootstrap . . . . . . . . . . . . . . . . 69 5.9 MSS training tumors with selected 33 driver genes. . . . . . . . . . . . . . 77\nIV"
    }, {
      "heading" : "LIST OF FIGURES V",
      "text" : "5.10 MSI exclusivity groups detected with MUTEX. . . . . . . . . . . . . . . . . . 80\n6.1 MSI-HIGH colorectal tumors used for the inference . . . . . . . . . . . . . 95 6.2 Progression model of MSS colorectal tumors . . . . . . . . . . . . . . . . . 96 6.3 Progression of MSI-HIGH colorectal tumors . . . . . . . . . . . . . . . . . 97 6.4 Ensemble-level progression of individual patients . . . . . . . . . . . . . . 100\nA.1 Example of screening-off and of background context . . . . . . . . . . . . 124\nC.1 Reconstruction with noisy synthetic data and λ→ 0 . . . . . . . . . . . . 143 C.2 Reconstruction with CAPRESE compared to oncotrees and h-CBN with noisy synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 C.3 Performance of CAPRESE to reconstruct models with conjunctive parents\nand noisy data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\nD.1 Reconstruction of trees and forests with small datasets . . . . . . . . . . . 158 D.2 Reconstruction of DAGs with small datasets . . . . . . . . . . . . . . . . . 159 D.3 Co-occurrence patterns: performance ranking . . . . . . . . . . . . . . . . 160 D.4 Comparison with related works: structural algorithms . . . . . . . . . . . 161 D.5 Comparison with related works: likelihood-based algorithms . . . . . . . . 162 D.6 Comparison with related works: hybrid algorithms . . . . . . . . . . . . . 163 D.7 Reconstruction of disjunctive patterns with no hypotheses . . . . . . . . . 164 D.8 Reconstruction with hypotheses: synthetic lethality . . . . . . . . . . . . . 165 D.9 Progression models of aCML by various algorithms . . . . . . . . . . . . 166 D.10 Progression models of ovarian cancer by various algorithms . . . . . . . . 167\nE.1 CRC pipeline processing MSI/MSS tumors . . . . . . . . . . . . . . . . . 176 E.2 MSI/MSS samples in the test dataset . . . . . . . . . . . . . . . . . . . . 177 E.3 Groups of exclusive alterations for MSS tumors . . . . . . . . . . . . . . . 178 E.4 Selected data for MSS tumors . . . . . . . . . . . . . . . . . . . . . . . . . 179 E.5 MSS training tumors: bootstrap confidence . . . . . . . . . . . . . . . . . 180 E.6 MSI training tumors: bootstrap confidence . . . . . . . . . . . . . . . . . 181 E.7 Single-cell synthetic data: performance . . . . . . . . . . . . . . . . . . . . 182\nF.1 Example of reconstructed network . . . . . . . . . . . . . . . . . . . . . . 190 F.2 Distribution of the edge scores . . . . . . . . . . . . . . . . . . . . . . . . 198 F.3 Scatter plot for individual in the German credit dataset . . . . . . . . . . . 201 F.4 Individual discrimination scores . . . . . . . . . . . . . . . . . . . . . . . . 202 F.5 Individual discrimination scatter plot . . . . . . . . . . . . . . . . . . . . . 203 F.6 Subgroup discrimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204 F.7 The SBCN of the Berkeley Admission Data dataset . . . . . . . . . . . . . . 206\nV\nLIST OF TABLES\n3.1 Estimated confidence for progression model of ovarian cancer . . . . . . . 34\n5.1 p-values of MSI-HIGH with BIC as regulatizator . . . . . . . . . . . . . . 89 5.2 p-values of MSI-HIGH with AIC as regulatizator . . . . . . . . . . . . . . 90\nD.1 Comparison in the execution time . . . . . . . . . . . . . . . . . . . . . . 155\nE.1 COADREAD Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 E.2 MUTEX: parameters and results . . . . . . . . . . . . . . . . . . . . . . . 173 E.3 CAPRI formulas for MSI-HIGH . . . . . . . . . . . . . . . . . . . . . . . . 174 E.4 CAPRI formulas for MSS . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\nF.1 SBCN main characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 F.2 Berkeley Admission Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 F.3 Communities from the SBCN extracted from the Adult dataset . . . . . . 199 F.8 Contingency table for foreign worker . . . . . . . . . . . . . . . . . . . . . 204 F.9 Contingency table for race black . . . . . . . . . . . . . . . . . . . . . . . . 204 F.4 Top groups by discrimination score in Adult dataset . . . . . . . . . . . . 205 F.5 Top groups by discrimination score in German credit . . . . . . . . . . . . 205 F.6 Top groups by discrimination score in Census-income dataset . . . . . . . 205 F.7 Explainable discrimination fom the Adult dataset . . . . . . . . . . . . . . 206 F.10 Contingency table for female . . . . . . . . . . . . . . . . . . . . . . . . . . 206\nVI\nLIST OF ALGORITHMS\n1 CAncer PRogression Extraction with Single Edge (CAPRESE) . . . . . . . 24\n2 CAncer PRogression wenference (CAPRI) . . . . . . . . . . . . . . . . . . . 42\n3 TRONCO Data Import and Preprocessing . . . . . . . . . . . . . . . . . . 49 4 Pseudocode of the CAPRESE algorithm . . . . . . . . . . . . . . . . . . . 50 5 Pseudocode of the CAPRI algorithm . . . . . . . . . . . . . . . . . . . . . 51 6 Bootstrap Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n7 Learning the Suppes-Bayes Causal Network . . . . . . . . . . . . . . . . . . 192\nVII\nCHAPTER 1\nINTRODUCTION\nIn the near future, cancer research is likely to become much more data-centric, primarily because of the rapid growth and ready availability of vast amount of cancer patient data, as well as because of advances in single-molecule single-cell technologies. Nonetheless, it remains impractical to track the tumor progression in any single patient over time, thus limiting the methods to work with data collected from biopsies of untreated tumors, although emerging technology for noninvasive analysis of circulating tumor cells and cell free DNA (in blood and urine) is beginning to paint an incomplete, but useful, picture1. Armed with the insights derived from such analysis, it would be possible to optimize therapy design (see CHA [117]) based on techniques of supervisory control theory, as well as to contribute insights for prevention, prognosis, treatment and drug design in new and possibly, unforeseen manners.\nMotivated by the increased availability of genetic patient data, in this thesis we therefore focus on the problem of reconstructing progression models of cancer. In particular, we aim at inferring the plausible sequences of genomic alterations that, by a process of accumulation, selectively make a tumor fitter to survive, expand and diffuse (i.e., metastasize). Along the trajectories of progression, a tumor (monotonically) acquires or “activates” mutations in the genome, which, in turn, produce progressively more “viable” clonal subpopulations over the so-called cancer evolutionary landscape (see [128, 86, 186]). Knowledge of such progression models is very important for drug development and in therapeutic decisions. For example, it is known that for the same cancer type, patients in different stages of different progressions respond differently to different treatments.\nBefore moving on in the description of the framework proposed in this thesis, we now describe the adopted model of cancer evolution.\n1For the sake of simplicity, we will limit our algorithmic studies to the ones dealing with data coming from untreated patients, derived from their initial biopsies and/or during “watchful waiting”. For other datasets, the algorithm will require electronic medical records of treatments and tests. For the sake of a clear exposition we ignore these issues here.\n1"
    }, {
      "heading" : "2 CHAPTER 1. INTRODUCTION",
      "text" : ""
    }, {
      "heading" : "1.1 A model of cancer evolution",
      "text" : "Since the late seventies, evolutionary dynamics, with its interplay between variation and selection, has progressively provided the widely-accepted paradigm for the interpretation of cancer emergence and development [143, 52, 40]. Random alterations of an organism’s (epi)genome can sometimes confer a functional selective advantage to certain cells, in terms of adaptability and ability to survive and proliferate. Since the consequent clonal expansions are naturally constrained by the availability of resources (metabolites, oxygen, etc.), further mutations in the emerging heterogeneous tumor populations are necessary to provide additional fitness of different kinds that allow survival and proliferation in the unstable micro environment. Such further advantageous mutations will eventually allow some of their sub-clones to outgrow the competing cells, thus enhancing tumor’s heterogeneity as well as its ability to overcome future limitations imposed by the rapidly exhausting resources. Competition, predation, parasitism and cooperation are indeed often observed in co-existing cancer clones [128].\nIn the well-known vision of Hananah and Weinberg [77, 78], the phenotypic stages that characterize this multistep evolutionary process are called hallmarks. These can be acquired by cancer cells in many possible alternative ways, as result of a complex biological interplay at several spatio-temporal scales that is still only partially deciphered [86]. In this framework, we distinguish alterations driving the hallmark acquisition process (i.e., drivers) by activating oncogenes or inactivating tumor suppressor genes, from those that are transferred to sub-clones without increasing their fitness (i.e., passengers) [58]. Driver identification is a modern challenge of cancer biology, as distinct cancer types exhibit very different combinations of drivers, some cancers display mutations in hundreds of genes [186], and the majority of drivers is mutated at low frequencies (“long tail” distribution), not allowing their detection by examining the recurrence at the population-level [60]. One can also use the evolutionary models to characterize, what may be called, anti-hallmarks – the phenotypes that are possible by the variational processes, but rarely found to be selected. For instance, certain collections of driver mutations, whose individual members are often present in the patient genomes, are never seen jointly. These anti-hallmarks point to tumors’ vulnerabilities, and thus, novel targets for therapeutic interventions.\nCancer clones harbour distinct types of “alterations”. The somatic ones involve either few nucleotides or larger chromosomal regions, and are usually catalogued as mutations - i.e., Single Nucleotide Variants (SNVs) and Structural Variants (SVs) at multiple scales (insertions, deletions, inversions, translocations) – of which only some are detectable as Copy Number Alterations (CNAs), which appear to be most prevalent in many tumor types [195]. Also epigenetic alterations, such as DNA methylation and chromatin reorganization, play a key role in the process [9]. The overall picture is confounded by factors such as genetic instability [190], aneuploidy, tumor heterogeneity and tumor-microenvironment interplay [3], the latter involving stromal and immunesystem cells with strong influence on the final effect of mutations [68]. Furthermore, spatial organization and tissue specificity play an essential role on tumor progression as\n2\n1.1. A MODEL OF CANCER EVOLUTION 3\nwell [142]2. In this scenario, genomic alterations are related to the phenotypic properties of tumor cells via the structure and dynamics of functional pathways, in a process which has been only partially characterized [185, 191, 92, 147]. In general, in fact, as there exist many equivalent ways to disrupt signaling and regulatory pathways, many mutations can provide equivalent fitness to cancer cells, leading to alternative routes to selective advantage across a population of tumors [141]. Practically, if multiple genes are equally functional for the same biological process, when any of those is altered the selection pressure on the others is diminished or even nullified [8]. Such genes, e.g., apc/ctnnb1 in colorectal cancer [64], therefore show a trend of exclusivity across a cohort – with few cases of co-occurrent alterations. The same applies when disruptive alterations hit on the same gene, e.g., pten’s mutations and deletions in prostate cancer [67].\nAn immediate consequence of this state of affairs is the dramatic heterogeneity and temporality of cancer, both at the inter-tumor and at the intra-tumor levels [55]. The former manifests as different patients with the same cancer type can display few common alterations. This led to the development of techniques to stratify tumors into subtypes with different genomic signatures, prognoses and response to therapy [30]. The latter refers to the noteworthy genotypic and phenotypic variability among the cancer cells within a single neoplastic lesion, characterized by the coexistence of more than one cancer clones with distinct evolutionary histories [63].\nCancer heterogeneity poses a serious problem from the diagnostic and therapeutical perspective as, for instance, it is now acknowledged that a single biopsy might not be representative of other parts of the tumor, hindering the problem of devising effective treatment strategies [128]. Therefore, the quest for an extensive etiology of cancer heterogeneity and for the identification of cancer evolutionary trajectories is nowadays central to cancer research, and attempt to exploit the huge amount of sequencing data available through public projects such as The Cancer Genome Atlas (TCGA) [136].\nSuch projects involve an increasing number of cross-sectional (epi)genomic profiles collected via single biopsies of patients affected by various cancer types, which might be used to extract trends of cancer evolution across a population of samples. Higher resolution data such as multiple samples collected from the same tumor [63], as well as single-cell sequencing data [134], might be complementarily used to face the same problem within a specific patient. However, either the lack of public data or problems of accuracy and reliability, due to technical and technological issues, currently prevent a straightforward application [42].\nNonetheless, there is a serious conceptual gap in the understanding of how such temporal heterogeneous cancer data could be analyzed, since available Machine Learning algorithms are not well-suited for the purposes, primarily because of their stationarity assumptions regarding the underlying statistical distributions. To solve this problem\n2We mention that much attention has been recently casted on newly discovered cancer genes affecting global processes that are apparently not directly related to cancer development, such as cell signaling, chromatin and epigenomic regulation, RNA splicing, protein homeostasis, metabolism and lineage maturation [60].\n3"
    }, {
      "heading" : "4 CHAPTER 1. INTRODUCTION",
      "text" : "rigorously, we build our foundations on the sound theory of probabilistic causality, originally proposed by Suppes [172] (see Chapter §2), and devise a framework, which for the first time algorithmizes Suppes’ formulation, while taming it’s efficiency satisfactorily, even for many complex situations that are specifically important in cancer studies (e.g., synthetic lethality or oncogene addiction). While the contributions of this thesis are primarily methodological – strongly supported by empirical studies using synthetic as well as some experimental genomic data – it is hoped to attract other algorithmicists to this problem and catalyze new directions of explorations."
    }, {
      "heading" : "1.2 Two facets of inferring cancer progression models",
      "text" : "The aforementioned different perspectives regarding cancer progression lead to the different mathematical formulations of the problem of inferring a cancer progression model from genomic data, which we examine at length in this thesis [13]. Indeed, such models can either be focused to describe trends characteristics of a population, i.e. ensemblelevel, or clonal progression in a single-patient. In general, both problems deal with understanding the temporal ordering of somatic alterations accumulating during cancer evolution, but use orthogonal perspectives and different input data – see Figure §1.1.\nEnsemble-level cancer evolution. It may seem desirable to extract a probabilistic graphical model (PGM) explaining the statistical trend of accumulation of somatic alterations in a population of n cross-sectional samples collected from patients affected by a specific cancer. To make this problem independent of the experimental conditions in which tumors are gathered, we only consider the list of alterations detected per sample – thus, as 0/1 Bernoulli variables.\nMuch of the difficulty lies in estimating the true and unknown trends of selective advantage among genomic alterations in the data, from such observations. This hurdle is not unsurmountable, if we constrain the scope to only those alterations that are persistent across tumor evolution in all sub-clonal populations, since it yields a consistent model of a temporal ordering of mutations. Therefore, epigenetic and trascriptomic states, such as hyper and hypo-methylations or over and under expression, could only be used, provided that they are persistent thorough tumor development [159].\nHistorically, the linear colorectal progression by Vogelstein is an instance of a solution to the cancer progression modeling problem [184]. That approach was later generalized to accommodate tree-models of branched evolution [38, 39, 173, 12] and, later, generalized to the inference of directed acyclic graph (DAG) models by Beerenwinkel and others [10, 65, 131]. We contributed to this research program with two related algorithms: CAncer PRogression Extraction with Single Edges (CAPRESE, see [117] and §3) and CAncer PRogression Inference (CAPRI, see [158] and §4). Both techniques rely on Suppes’ theory of probabilistic causation to define estimators of selective advantage [172], are robust to the presence of noise in the data and perform well even with limited sample sizes. The former algorithm exploits shrinkage-like statistics to extract a tree model of progression, the latter combines bootstrap and maximum likelihood estimation with\n4"
    }, {
      "heading" : "1.2. TWO FACETS OF INFERRING CANCER PROGRESSION MODELS 5",
      "text" : "regularization to extract general directed acyclic graphs (DAGs) that capture branched, independent and confluent evolution. Both algorithms represent the current state-of-theart to approach this problem, as they outperform others in speed, scale and predictive accuracy.\n5"
    }, {
      "heading" : "6 CHAPTER 1. INTRODUCTION",
      "text" : "Clonal architecture in individual patients. At the time of this writing, technical and economical limitations of single-cell sequencing prevent a straightforward application of phylogeny inference algorithms to the reconstruction of the clonal evolutionary history of genomic alterations within a single tumor [135, 188]. Conversely, samples of cells collected from a single bulk tumor do not define an isogenic lineage [20] and most likely contain a large number of cells belonging to a collection of sub-clones resulting from the complex evolutionary history of the tumor, where the prevalence of a particular clone in time and its spatial distribution reflect its growth and proliferative fitness. To overcome hurdles such as this, many recent efforts have aimed at inferring the clonal signatures and prevalence in individual patients from sequencing data [63, 62].\nThe majority of attempts employ different strategies, usually based on Bayesian inference, to relate allelic imbalance to cellular prevalence, and benefits from multiple sample per patient, taken across time or space. In particular, most tools usually process a set of read counts from a high-coverage sequencing experiment to estimate Variant Allele Frequency (VAF). Some of them are based on the VAF analysis of specific SNVs [130, 164]. Recent algorithms attempt to minimize the error between the observed and inferred mutation frequencies with distinct optimization procedures [91, 122, 48]. Other approaches support explicitly short-read data and different types of data, such as CNAs, SNVs and B-allele fractions (BAFs) [54]. Distinct techniques, instead, use genome-wide segmented read-depth information to determine mixtures of subclonal CNA profiles [144, 145], while others use a generative approach to deconvolve sequencing data to clonal architectures [196]. Clearly, any of these approaches gains precision from highcoverage sequencing data, since high read counts yield high confidence estimate of allele frequency.\nAs already discussed, several datasets are currently available that aggregate diverse cancer-patient data and report in-depth mutational profiles, including e.g., structural changes (e.g., inversions, translocations, copy-number variations) or somatic mutations (e.g., point mutations, insertions, deletions, etc.), see [136]. These data, by their very nature, only give a snapshot of a given tumor sample, mostly from biopsies of untreated tumor samples at the time of diagnoses. As it still remains impractical to track the tumor progression in any single patient over time, thus limiting most analysis methods to work with cross-sectional data3.\nFor this reason, in this thesis we focus on the problem of reconstructing cancer progression models from cross-sectional data. As already stated, this problem is not new and, to the best of our knowledge, two threads of research starting in the late 90’s have addressed it. The first category of works examined mostly gene-expression data to reconstruct the temporal ordering of samples (see [121, 71]). The second category of works aimed at inferring cancer progression models of increasing model-complexity, starting from the simplest tree models (see [38]) to more complex graph models (see [65]); see the next section for an overview of the state-of-the-art. Building on the works described\n3Unlike longitudinal studies, these cross-sectional data are derived from samples that are collected at unknown time points, and can be considered as “static”.\n6"
    }, {
      "heading" : "1.2. TWO FACETS OF INFERRING CANCER PROGRESSION MODELS 7",
      "text" : "in [117, 158], we present a novel and comprehensive framework of the second category that addresses this problem.\nMoreover, the framework proposed in this thesis along with the described algorithms is part of the TRanslational ONCOlogy (TRONCO) package (see [4, 33, 5]). In summary (also see Chapter §2), starting from cross-sectional genomic data, such algorithms aim at reconstructing a probabilistic progression model by inferring “selectivity relations”, where a mutation in a gene A “selects” for a later mutation in a gene B. These relations are depicted in a combinatorial graph and resemble the way a mutation exploits its “selective advantage” to allow its host cells to expand clonally. Among other things, a selectivity relation implies a putatively invariant temporal structure among the genomic alterations (i.e., events) in a specific cancer type4. In addition, these relations are expected to also imply “probability raising” for a pair of events in the following sense: namely, a selectivity relation between a pair of events here signifies that the presence of the earlier genomic alteration (i.e., the upstream event) that is advantageous in a Darwinian competition scenario increases the probability with which a subsequent advantageous genomic alteration (i.e., the downstream event) appears in the clonal evolution of the tumor. Thus the selectivity relation captures the effects of the evolutionary processes, and not just correlations among the events and imputed clocks associated with them. As an example, we show in Figure §1.2 the selectivity relation connecting a mutation of egfr to the mutation of cdk, see §2.\nConsequently, an inferred selectivity relation suggests mutational profiles in which certain samples (early-stage patients) display specific alterations only (e.g., the alteration characterizing the beginning of the progression), while certain other samples (e.g., latestage patients) display a superset subsuming the early mutations (as well as alterations that occur subsequently in the progression).\nVarious kinds of genomic aberrations are suitable as input data, and include somatic point/indel mutations, copy-number alterations, etc., provided that they are persistent, i.e., once an alteration is acquired no other genomic event can restore the cell to the non-mutated (i.e., wild type) condition5.\nIn what follows and in the rest of the thesis, we use the notations described below. Atomic events, in general, are denoted by small Roman letters, such as a, b, c, . . .; when it is clear from the context that the event in the model is, in fact, a genomic mutational event, we may refer to it directly using the standard biological nomenclature, e.g., brca1, brca2, etc. – it would be especially true, in the sections describing applications to real data. Patterns over events are mostly denoted by Greek letters, and their logical connectives with the usual “and” (∧), “or” (∨) and “negation” (·) symbols. Standard operations on sets are used as well.\nWe are not employing distinct notations to denote observed probabilities and prob-\n4It has already been mentioned the known existence of various molecular subtypes within the same cancer.\n5For instance, epigenetic alterations such as methylation and alterations in gene expression are not directly usable as input data for the algorithm. Notice that the selection of the relevant events is beyond the scope of this work and requires a further upstream pipeline, such as that provided, for instance, in [177, 186].\n7"
    }, {
      "heading" : "8 CHAPTER 1. INTRODUCTION",
      "text" : "abilities in the model which we aim at inferring (i.e., the “theoretical probabilities”). Which quantity is being referred to, is made clear from the context. In the following, P(x) denotes the probability of x; P(x ∧ y), the joint probability of x and y, which is naturally extended to the notation P(x ∧ y1 ∧ . . . ∧ yn) for an arbitrary arity; and P(x | y), the conditional probability of x given y. Here x and y are patterns over events.\nAs with the discussion of selective advantage structures, we write c e, where c and e are events being modeled, in order to denote the selective advantage relation “c has a selective influence on e”. As we extend our presentation to general patterns, we generalize the notation to ϕ e with the meaning generalized mutatis mutandis6.\nBefore moving on, we now provide in the next Section an overview of the state-of-theart on the problem of cancer progression models as for what pictured in this Chapter."
    }, {
      "heading" : "1.3 State of the art",
      "text" : "For an extensive review on cancer progression model reconstruction we refer to the recent survey by [13]. In brief, progression models for cancer have been studied starting with\n6Note that the scope of this thesis is intentionally kept limited from further generalizing the “selective advantage patterns”; for instance, we are not dealing with any example of the form ϕi ϕj , where ϕ could be any general pattern (including a complex causal pattern or a temporal pattern). This choice is justified in view of complexity, practicality, applicability and expressiveness in the context of cancer progression driven by somatic evolution.\n8"
    }, {
      "heading" : "1.3. STATE OF THE ART 9",
      "text" : "the seminal work of [184] where, for the first time, cancer progression was described in terms of what could be interpreted as a directed path a directed path. [184] manually created a (colorectal) cancer progression from a genetic and clinical point of view. More rigorous and complex algorithmic and statistical automated approaches have appeared subsequently. As stated already, the earliest thread of research simply sought more generic progression models that could assume tree-like structures. The oncogenetic tree model captured evolutionary branches of mutations (see [38, 173]) by optimizing a correlation-based score. Another popular approach to reconstruct tree structures appears in [39]. Other general Markov chain models such as, e.g., [81] reconstruct more flexible probabilistic networks, despite a computationally expensive parameter estimation. Other results that extend tree representations of cancer evolution exploit mixture tree models, i.e., multiple oncogenetic trees, each of which can independently result in cancer development (see [12]). In general, all these methods are capable of modeling diverging temporal orderings of events in terms of branches, although the possibility of converging evolutionary paths is precluded.\nTo overcome this limitation, the most recent approaches tends to adopt Bayesian graphical models, i.e., Bayesian Networks (BN). In the literature, there have been two initial families of methods aimed at inferring the structure of a BN from data (see [101]). The first class of models seeks to explicitly capture all the conditional independence relations encoded in the edges and will be referred to as structural approaches; the methods in this family are inspired by the work on causal theories by Judea Pearl (see [150, 151, 171, 179]). The second class – likelihood approaches – seeks a model that maximizes the likelihood of the data (see [167, 79, 23]).\nA more recent hybrid approach to learn a BN which combines the two families above by (i) constraining the search space of the valid solutions and, then, (ii) fitting the model with likelihood maximization (see [10, 65, 131]). A further technique to reconstruct progression models from cross-sectional data was introduced in [6], in which the transition probabilities between genotypes are inferred by defining a Moran process that describes the evolutionary dynamics of mutation accumulation. In [25] this methodology was extended to account for pathway-based phenotypic alterations.\nThis thesis is structured as follow. Chapter §2 together with Appendices §A and §B describe the theoretical foundations on which the presented framework is based. In Chapters §3 and §4 two efficient algorithms to respectively reconstruct tree-alike and directed acyclic graph models of cancer progression are described. Chapter §5 presents an R package which implements the described algorithms together with a series of functionalities to support the researcher through all the steps of the analysis of cancer progression. This package is then adopted in Chapter §6 where a detailed analysis based on a structured pipeline is performed for colorectal cancer. All the supplementary materials concerning to the previous Chapters are reported in the remaining Appendices. Finally, Chapter §7 concludes the thesis.\n9\nCHAPTER 2\nMODELING CANCER CLONAL EVOLUTION\nBased on the discussion of §1, in this Chapter we will formalize the adopted framework used to model cancer clonal evolution. In particular, we will define the proposed model of selective advantage through different levels of complexity."
    }, {
      "heading" : "2.1 A probabilistic model of selective advantage",
      "text" : "Central to the model proposed in this thesis is Suppes’ notion of probabilistic causation [172], which can be stated in the following terms: a selectivity relation1 among two observables i and j is verified if (1) i occurs earlier than j – temporal priority (tp) – and (2) if the probability of observing i raises the probability of observing j, i.e., P(j | i) > P(j | i)2 – probability raising (pr), see §A.2 for a deeper discussion of the philosophical aspects of this theory.\nNote that the definition of probability raising subsumes positive statistical dependency and mutuality (see, e.g., [117, 158]). But, it should be emphasized that the resulting relation (also termed prima facie causality) is purely observational and remains agnostic of any possible mechanistic cause-effect relation involving i and j. When through this thesis we term any relation to be causal, we will use this word with such an interpretation, i.e., ignoring any mechanistic behaviour.\nWhile Suppes’ definition of probabilistic causation has known limitations in the context of general causality theory (see discussions in §A.2 and, e.g., [80, 98]), in the context of cancer evolution, this relation appropriately describes various features of selective advantage in somatic alterations that accumulate as tumor progresses.\nTherefore, in this framework, we implement the temporal priority among events – condition (1) – as P(i) > P(j), because it is intuitively sound to assume that the\n1Suppes describes such a relation in terms of causality; however, here we avoid this terminology as we build on just two of his many axioms, which give rise to the notion of prima-facie causality.\n2Please remind that we are considering cross-sectional data, hence without any explicit measurement of time which needs to be imputed.\n10"
    }, {
      "heading" : "2.1. A PROBABILISTIC MODEL OF SELECTIVE ADVANTAGE 11",
      "text" : "(cumulative) genomic events occurring earlier are the ones present in higher frequency in a dataset. In addition, condition (2) is implemented as is, that is by requiring that for each pair of observables i and j it holds that P(j | i) > P(j | i). Taken together, these conditions give rise to a natural ordering relation among events, written “i j” and read as “i has a selective influence on j”. This relation is a necessary but not sufficient condition to capture the notion of selective advantage, hence additional constraints need to be imposed to filter any spurious correlation (see the discussions in §A.2). Spurious correlations are both intrinsic to the definition (e.g., if i j w then also i w, which could be spurious) and to the model we aim at inferring, because of finite data as well as presence of observational noise.\nBuilding on this framework, we aim at devising inference algorithms that capture the essential aspects of heterogeneous cancer progressions: branching, independence and convergence – all combining in a progression model.\nFurthermore, the complexity of cancer requires modeling multiple non-trivial patterns of its progression: for a specific event, a pattern is defined as a specific combination of the closest upstream events that confers a selective advantage.\nAs an example, imagine a clonal subpopulation becoming fit – thus enjoying expansion and selection – once it acquires a further mutation of gene c, provided that it also has previously acquired a mutation in a gene in the upstream a/b pathway. In terms of progression, we would like to capture these evolutionary trajectories: either {a,¬b}, {¬a, b} or {a, b} precedes c (where ¬ denotes the absence of an event in the gene).\nTo formally take this into account, we augment our model of selection in a tumor with a language built from simple propositional logic formulas using the usual Boolean connectives: namely, “and” (∧), “or” (∨) and “xor” (⊕). These patterns can be described by formulæ in a propositional logical language, which can be rendered in Conjunctive Normal Form (CNF). A CNF formula ϕ has the following syntax: ϕ = c1 ∧ . . . ∧ cn, where each ci is a disjunctive clause ci = ci,1 ∨ . . .∨ ci,k over a set of literals, each literal representing an event or its negation.\nIn this framework, we aim at reconstructing probabilistic graphical models of cancer progression. Given the above premises, this problem reduces to the following: for each input event e, assess a set of selectivity patterns {ϕ1 e, . . . , ϕk e}, filter the spurious ones, and combine the rest in a direct acyclic graph (DAG)3, G = (V,E), where the nodes are the atomic events (augmented, eventually, with logical symbols) and the edges represent selective advantage relations. Notice that, while we broke down the progression extraction into a series of sub-tasks, the problem still remains complex: patterns are unknown, potentially spurious, and exponential in formula size; moreover, data are noisy and patterns must allow for “imperfect regularities”, rather than being strict4.\nTo summarize, in this setting we can model complex progression trajectories with\n3A DAG is formed by a set of nodes and oriented edges connecting one node to another, such that there are no directed loops among them.\n4This statement implies that there could be samples – i.e., patients or tumor cells – contradicting a pattern which still remains valid at a population level. For this reason a pattern x ∧ y z is sometimes called a “noisy and”.\n11"
    }, {
      "heading" : "12 CHAPTER 2. MODELING CANCER CLONAL EVOLUTION",
      "text" : "branches (i.e., events involved in various patterns), independent progressions (i.e., events without common ancestors) and convergence (via CNF formulas).\nFor the sake of clarity, in the next sections we develop the presentation in steps of successively increasing complexity of the selective advantage patterns: e.g., going from singleton (i.e., “atomic”) patterns, to co-occurrance patterns consisting of atomic events, up to patterns in Conjunctive Normal Forms (CNF) (e.g., [(‘burning cigarette’ ∧ ‘dried wood’ ) ∨ (‘lightning’ ∧ ‘no rain’) ‘forest fire’])5."
    }, {
      "heading" : "2.2 Singleton prima facie topologies",
      "text" : "When at most a single incoming edge is assigned to each event (i.e., an event has at most one unique parent: ∀e∈V ∃!c∈V c e), we term this causal structure singleton prima facie topology, a special and important case of the most general prima facie topology structures. Note once again that the general model can be represented as a direct acyclic graph (DAG) where each edge represents a prima facie relation between a parent and its child. In the special case of the singleton prima facie topology, such a graph is a tree or, more generally, a forest when there are disconnected components. Thus, each progression tree induces a distribution of observing a subset of the mutations in a cancer sample (see [117] for a detailed discussion).\nIn [117] the following propositions (summarized in Figure §2.1 and discussed in details in §3) were shown to hold for singleton prima facie topologies (see §3 and §C for a detailed description and the related proofs), and used to derive an algorithm to infer tree (forest) models of cancer progression.\nStatistical dependence. Whenever probability raising holds between two events c and e, then the events are statistically dependent in a positive sense, i.e.,\nP(e | c) > P(e | c) ⇐⇒ P(e ∧ c) > P(e)P(c) . (2.1)\nMutuality. If c is a probability raiser for e, then so is the converse, i.e.,\nP(e | c) > P(e | c) ⇐⇒ P(c | e) > P(c | e) . (2.2)\nNatural ordering. For any two events c and e such that c is a probability raiser for e, a “natural” ordering arises to disentangle a causality relation, i.e.,\nP(c) > P(e) ⇐⇒ P(e | c)P(e | c) > P(c | e) P(c | e) . (2.3)\n5The statement above, which is expressed for conveniency in Disjunctive Normal Form and could be automatically translated in CNF, may also be shortened as ‘burning cigarette’ ‘forest fire.’ The intended interpretation is that, ‘burning cigarette’ is an insufficient but non-redundant part of an unnecessary but sufficient causal condition (INUS) for ‘forest fire,’ as originally suggested by the philosopher J. Mackie.\n12"
    }, {
      "heading" : "2.2. SINGLETON PRIMA FACIE TOPOLOGIES 13",
      "text" : "Putting together all these properties, it is straightforward to derive the following characterization of singleton prima facie relations: c is said to be a singleton prima facie cause of e if c is a probability raiser of e, and it occurs more frequently, i.e.,\nc e ⇐⇒ P(e | c) > P(e | c) ∧ P(c) > P(e) . (2.4)\nConsequently to this definition, we observe that (see also the discussions in §A.2) it is necessary but not sufficient to identify the accumulative processes (path or branch) and, thus, to solve the considered problem. In fact, as it can be easily observed in the Figure §2.2, black arrows make this definition necessary, while red arrows (spurious, resulting, e.g., from transitivities, because of the singleton hypothesis) make the condition insufficient. We remark that red arrows will always be present to indicate potential genuine causes corresponding to actual selective advantage relations. Thus, a correct inferential algorithm will have to select the real relations among the potential genuine ones, within a subset of the prima facie causes.\nA further discussion about spurious connections is now warranted. As deeply described in §A.2, spurious causes may manifest through spurious correlation or chance. In the infinite sample size limit the “law of large numbers” eliminates the effect of chance; in other words, with large enough sample, chance by itself will not suffice to satisfy Suppes’ conditions. Instead, the former situation for spuriousness depends on the topology to be reconstructed, and might appear under observation like a prima-facie/genuine selective advantage relation in disguise, even with an infinite sample size (purple edges, for\n13\n14 CHAPTER 2. MODELING CANCER CLONAL EVOLUTION\nwhich the “temporal direction” has no causal interpretation, as it depends on the data and topology). For these reasons, a singleton prima facie topology asymptotically will not contain false negatives (i.e., all the actual selective advantage relations are modeled in the topology) but it might contain false positives (red or purple edges, as Suppes’ prima facie conditions are not sufficient)."
    }, {
      "heading" : "2.3 Co-occurence prima facie patterns",
      "text" : "We now denote by a Boolean conjunctive clause, a propositional formula composed of conjunctions of a set of literals: c = c1 ∧ · · · ∧ cn, which implies that n events c1, . . ., cn have occurred (in some unspecified order) so as to collectively lead to effect e (graphically pictured as in Figure §2.3), and we assume that each ci (1 ≤ i ≤ n) is an atomic event.\nSuppes’ notion of probabilistic causation can be naturally extended to such cooccurence patterns as in the following definition:\nDefinition 1 (Co-occurence probabilistic causation). For any conjunctive event c = c1 ∧ . . . ∧ cn and e, occurring respectively at times {tci | i = 1, . . . , n} and te, under the mild assumptions that 0 < P(ci),P(e) < 1, for any i, the conjunctive event c is a prima facie conjunctive cause of e (c e) if all of its components ci occur before the effect and their occurrences collectively raises the probability of the effect, i.e.,\nmax{tc1 , . . . , tcn} < te and P(e | c) > P(e | c) . (2.5)\nwhere { P(e | c) = P(e | c1 ∧ · · · ∧ cn) P(e | c) = P(e | c1 ∧ · · · ∧ cn) = P(e | c1 ∨ · · · ∨ cn) .\n14\n2.3. CO-OCCURENCE PRIMA FACIE PATTERNS 15\nThis extension simply follows the semantics of co-occurence patterns, which states that all causes must occur before their effect, thus justifying the choice of picking the latest event, in time, prior to e as a generalization: namely, the max{·} operation applied to the preceeding events. Clearly, this definition retains the semantics of singleton prima facie relations unchanged, as it is a special case with c = c and max{tci} = tc. Unfortunately, as before, it still has the same weakness and it is necessary but not sufficient to identify co-occurence patterns.\nThe properties of singleton prima facie topologies also extend appropriately to cooccurence topologies – see §C and §D for the related proofs, along with all the other properties and theorems that appear in this thesis.\nProposition 1. The properties of statistical dependence, mutuality and natural ordering for singleton patterns are still valid for co-occurence patterns.\nIn this case some caution must be exercised in distinguishing between prima facie singleton or co-occurance patterns. As shown in Figure §2.3, in fact, for any co-occurance pattern in the real world (a and b and c) the following conjunctive clauses\na ∧ b d a ∧ c d b ∧ c d\nas well as the singleton patterns a d, b d and c d, are prima facie. The singleton patterns can be spurious or transitive, as in Figure §2.2. But this time, we will also have spurious sub-formulas, i.e., the conjunctive clauses that are syntactically strictly subformulas of a∧b∧c d, that is the only genuine formula we would like to infer. Notice that as in branch processes, topology-dependent spurious causes might also appear because\n15"
    }, {
      "heading" : "16 CHAPTER 2. MODELING CANCER CLONAL EVOLUTION",
      "text" : "of spurious correlations; in the Figure §2.2, we have not shown other potential spurious relations, as what we depict to make it visualizable is just a one-level conjunctive network. These selective advantage relations could include general spurious formulas constituting of a sub-formula and any of its parents. Similarly, spurious relations due to chance will vanish asymptotically as sample size grows to infinity. Summarizing, we note that a co-occurence topology, just as in the singleton patterns framework, will not contain false negatives (i.e., all real world selective advantage relations will be modeled in the topology) but it might also contain, depending on the real world topology, false positives (red, green or purple edges).\nBefore concluding, we note that the total number of potential formulas and transitivities is exponential in the size of |G| = n, that is\nn−1∑ i=1 ( n− 1 i ) = 2n−1 − 1 .\nNotice that this is a lower bound accounting only for the level of the connective, and is expected to grow further when more complex real world accumulative processes are considered. Finally, as shown in Figure §2.2, the number of spurious relations due to the topology (purple edges), are quadratic in the formula size, being\n2\n( n− 1\n2\n) = (n− 1)(n− 2) .\nThis complexity hints at the fact that an exhaustive search of all the possible conjunctive formula is not feasible, in general."
    }, {
      "heading" : "2.4 Generalization to conjunctive normal form",
      "text" : "We consider next a formula in conjunctive normal form (CNF)\nϕ = c1 ∧ . . . ∧ cn,\nwhere each ci is a disjunctive clause ci = ci,1 ∨ . . . ∨ ci,k over a set of literals and each literal represents an event (a Boolean variable) or its negation. By following analogous arguments as the ones used before, we can define ϕ e as follows.\nDefinition 2 (CNF probabilistic causation). For any CNF formula ϕ and e, occurring respectively at times tϕ and te, under the mild assumptions that 0 < P(ϕ),P(e) < 1, ϕ is a prima facie cause of e if\ntϕ < te and P(e | ϕ) > P(e | ϕ) . (2.6)\nAs stated before, also this definition is necessary but not sufficient to identify selective advantage relations, hence lacking the power to solve the considered problem.\nClearly, in this case, the number of prima facie (including both genuine and spurious) relations grows combinatorially much more rapidly than the simplest case of a unique\n16"
    }, {
      "heading" : "2.4. GENERALIZATION TO CONJUNCTIVE NORMAL FORM 17",
      "text" : "conjunctive clause (see §2.3); this situation is rather alarming, since even the simplest case already produces an exponentially large set of prima facies causes in terms of the number of events. In this case, in fact, further causal relations emerge as a result of mixing events from all the clauses of ϕ. CNF formulas follow analogous properties as singleton and co-occurrence topologies, as shown below.\nProposition 2. The properties of statistical dependence, mutuality and natural ordering for singleton and co-occurence prima facie topologies also extend to CNF formulas mutatis mutandis.\nWe conclude this Section with two final comments about CNF formulas: their relation with background contexts (see §A.2), and the notion of timing in Definition §2.\nThe first comment concerns Cartwright’s idea of background contexts as a conjunction of independent factors §A.2. For illustrative purposes, consider the formula (a ∧ b) ∨ c d, which is in disjunctive normal form (DNF). If, for example, we were to evaluate the claim a d, the (unique) background context would be the atomic event c, being relevant to evaluate the claim a causes d. A symmetric situation holds, when we were to evaluate b d. In light of this discussion note that, if we convert the formula to its CNF analogue (a ∨ c) ∧ (b ∨ c) d, we need to correctly interpret the roles of sub-formulas a ∨ c and b ∨ c in identifying a background context, c. It follows immediately that, for any CNF formula, the atomic events of all the disjunctive clauses in the equivalent DNF formula provide all the possible background contexts à-la-Cartwright.\nThe second comment concerns timing in the real world. Consider the CNF formula above, denote it as ϕ and recall that Definition §2 requires tϕ < td. One might wonder whether a trivial time-ordering relation exists, whose complexity is linear with respect to all the operators in ϕ. Were it so, we would be able to parse ϕ into its constituents, and recursively express the temporal relations as a direct function of those relations that hold for its sub-formulas. Unfortunately, this appears not to be the case, except when the underlying syntax is restricted to certain specific operators (e.g., conjunctions). Thus appropriate care must be taken in implementing a model of time. Thus, an algorithm, working on the illustrative example of the previous paragraph, cannot conclude any ordering about ta∨c, tb∨c and td, solely by looking at the marginal probabilities of their atomic events – instead it must gather the correct information for certain sub-formulas at the level of their connective (the ∨ in this case). A general rule that avoids these difficulties and devises a correct and efficient timing-inference algorithms, may be stated as follows: it is always safe to model probabilistic causation in terms of whole formulas, while permitting compositional reasoning over sub-formulas is operable only when the syntax is restricted to certain Boolean connectives. Further related comments appear in §3 and §4, where we describe the algorithmic implementations of the described framework.\nNext we will describe efficient algorithmic implementations of the framework presented in this Chapter.\n17\nCHAPTER 3\nSINGLETON MODELS OF CANCER PROGRESSION\nIn this Chapter we will present an algorithm for the efficient inference of singleton models of cancer progression. As a reference, see [117]."
    }, {
      "heading" : "3.1 Problem setting",
      "text" : "Assuming that we have a set G of n mutations (events, in probabilistic terminology) and s samples, we represent a cross-sectional dataset as an s×n binary matrix in which an entry (k, l) = 1 if the mutation l was observed in sample k, and 0 otherwise. The problem we solve here is the one of extracting a set of edges E yielding a progression tree T = (G ∪ { }, E, ) from this matrix which, we remark, only implicitly provides information of progression timing. The root of T is modeled using a (special) event 6∈ G such that heterogenous progression paths or forests can be reconstructed. More precisely, we aim at reconstructing a rooted tree that satisfies: (i) each node has at most one incoming edge, (ii) the root has no incoming edges (iii) there are no cycles.\nEach progression tree induces a distribution of observing a subset of the mutations in a cancer sample that can be formalized as follows:\nDefinition 3 (Tree-induced distribution). Let T be a tree and α : E → [0, 1] a labeling function denoting the independent probability of each edge, T generates a distribution where the probability of observing a sample with the set of alterations G∗ ⊆ G is\nP(G∗) = ∏ e∈E′ α(e) · ∏\n(u,v)∈E u∈G∗,v 6∈G\n[ 1− α(u, v) ] (3.1)\nwhere all events in G∗ are assumed to be reachable from the root , and E′ ⊆ E is the set of edges connecting the root to the events in G∗.\n18"
    }, {
      "heading" : "3.1. PROBLEM SETTING 19",
      "text" : "We would like to emphasize two properties related to the tree-induced distribution. First, the distribution subsumes that, given any oriented edge (a→ b), an observed sample contains alteration b with probability P(a)P(b), that is the probability of observing b after a. For this reason, if a causes b, the probability of observing a will be greater than the probability of observing b accordingly to the temporal priority principle which states that all causes must precede, in time, their effects [160].\nSecond, the input dataset is a set of samples generated, ideally, from an unknown distribution induced by an unknown tree or forest that we aim at reconstructing. However, in some cases, it could be that no tree exists whose induced distribution generates exactly those input data. When this happens, the set of observed samples slightly diverges from any tree-induced distribution. To model these situations a notion of noise can be introduced, which depends on the context in which data are gathered. Adding noise to the model complicates the reconstruction problem.\nThe oncotree approach. In [38] Desper et al. developed a method to extract progression trees, named “oncotrees”, from static CNV data. In [173], Szabo et al. extended the setting of Desper’s reconstruction problem to account for both false positives and negatives in the input data. In the oncotrees, nodes represent alterations and edges correspond to possible progressions from one event to the next.\nThe reconstruction problem is exactly as described above, and each tree is rooted in the special event . The choice of which edge to include in a tree is based on the estimator\nwa→b = log [ P(a) P(a) + P(b) · P(a ∧ b) P(a)P(b) ] , (3.2)\nwhich assigns to each edge a → b a weight accounting for both the relative and joint frequencies of the events – thus measuring correlation. The estimator is evaluated after including to each sample of the dataset. In this definition the rightmost term is the (symmetric) likelihood ratio for a and b occurring together, while the leftmost is the asymmetric temporal priority measured by rate of occurrence. This implicit form of timing assumes that, if a occurs more often than b, then it likely occurs earlier, thus satisfying\nP(a) P(a) + P(b) > P(b) P(a) + P(b) .\nAn oncotree is the rooted tree whose total weight (i.e., sum of all the weights of the edges) is maximized, and can be reconstructed in O(|G|2) steps using Edmond’s algorithm [43]. By construction, the resulting graph is a proper tree rooted in : each event occurs only once, confluences are absent, i.e., any event is caused by at most one other event. This method has been used to derive progressions for various cancer datasets e.g., [93, 85, 157]), and even though several methods that extend this framework exists (e.g., [39, 12, 65]), to the best of our knowledge, it is currently the only method that aims to solve exactly the same problem as the one investigated here and thus provide a benchmark to compare against.\n19"
    }, {
      "heading" : "20 CHAPTER 3. SINGLETON MODELS OF CANCER PROGRESSION",
      "text" : ""
    }, {
      "heading" : "3.2 A probabilistic approach to selective advantage",
      "text" : "We briefly recall the approach to probabilistic causation, on which this method is based. For an extensive discussion on this topic we refer to our previous discussion in §2.\nIn his seminal work [172], Suppes proposed a set of conditions that are necessary for any causal claim, that is, for any pair of two events c and e, occurring respectively at times tc and te, under the mild assumptions that 0 < P(c),P(e) < 1, the event c is a prima facie cause of the event e if it occurs before the effect and the cause raises the probability of the effect, i.e.,\ntc < te and P(e | c) > P(e | c) .\nAs already discussed in §2, the above conditions are not, in general, sufficient to claim that event c is a cause of event e. In fact a prima facie cause is either genuine or spurious. In the latter case, the fact that the conditions hold in the observations is due either to coincidence or to the presence of a certain third confounding factor, related both to c and to e [172]. Genuine causes, instead, satisfy Suppe’s criteria and are not screened-off by any confounding factor (also see §C). However, they need not be direct causes. See Figure §3.1.\nNote that we consider cross-sectional data where no information about tc and te is available, hence in this reconstruction setting we are restricted to consider solely the probability raising (pr) property, i.e., P(e | c) > P(e | c), which makes it harder to discriminate among genuine and spurious causes. Now we review some of its properties.\n20"
    }, {
      "heading" : "3.2. A PROBABILISTIC APPROACH TO SELECTIVE ADVANTAGE 21",
      "text" : "Proposition 3 (Statistical dependence). Whenever the pr holds between two events a and b, then the events are statistically dependent in a positive sense, i.e.,\nP(b | a) > P(b | a) ⇐⇒ P(a ∧ b) > P(a)P(b) . (3.3)\nThis and the next proposition are well-known facts of the pr; their derivation as well as the proofs of all the results we discuss here, are shown in §C.\nNotice that the opposite implication holds as well: when the events a and b are still dependent but in a negative sense, i.e., P(a ∧ b) < P(a)P(b), the pr does not hold, i.e., P(b | a) < P(b | a).\nWe would like to use the asymmetry of the pr to determine whether a pair of events a and b satisfy a selective advantage relation so to place a before b in the progression tree but, unfortunately, the pr satisfies the following property.\nProposition 4 (Mutuality). P(b | a) > P(b | a) ⇐⇒ P(a | b) > P(a | b) .\nThat is, if a raises the probability of observing b, then b raises the probability of observing a too.\nNevertheless, in order to determine the order among pair of genetic events, we can use the degree of confidence in the estimate of probability raising to decide the direction of their relationship. In other words, if a raises the probability of b more than the other way around, then a is a more likely singleton prima facie cause of b than b of a. Notice that this is sound as long as each event has at most one cause; otherwise, frequent late events with more than one predecessor, which are rather common in biological progressive phenomena, should be treated differently. As mentioned, the pr is not symmetric, and the direction of probability raising depends on the relative frequencies of the events. We make this asymmetry precise in the following proposition.\nProposition 5 (Natural ordering). For any two events a and b such that the probability raising P(a | b) > P(a | b) holds, we have\nP(a) > P(b) ⇐⇒ P(b | a)P(b | a) > P(a | b) P(a | b) . (3.4)\nThat is, given that the pr holds between two events, a raises the probability of b more than b raises the probability of a, if and only if a is observed more frequently than b. Also notice that we use the ratio to assess the pr inequality. The proof of this proposition is technical and can be found in §C.\nFrom this result it follows that if we measure the timing of an event by the rate of its occurrence (that is, P(a) > P(b) implies that a happens before b), this notion of pr subsumes the same notion of temporal priority induced by a tree. We also remark that this is also the temporal priority made explicit in the coefficients of Desper’s method [38]. Given these results, we define the following notion of singleton prima facie causaluty.\nDefinition 4. We state that a is a prima facie cause of b if a is a probability raiser of b, and it occurs more frequently: P(b | a) > P(b | a) and P(a) > P(b).\n21"
    }, {
      "heading" : "22 CHAPTER 3. SINGLETON MODELS OF CANCER PROGRESSION",
      "text" : "We term prima facie topology a directed acyclic graph (over some events) where each edge represents a prima facie cause. When at most a single incoming edge is assigned to each event (i.e., an event has at most a unique cause, in the real world), we term this structure singleton prima facie topology. Intuitively, this last class of topologies correspond to the trees or, more generally forests when they have disconnected components, that we aim at reconstructing.\nBefore moving on to introducing the algorithm proposed here, let us once again recall the definition of selective advantage, its role in the definition of the reconstruction problem and some of its limitations. As already mentioned, it may be that for some prima facie cause c of an event e, there is a third event a prior to both, such that a causes c and ultimately c causes e. Alternatively, a may cause both c and e independently, and the selective advantage relationship observed from c to e is merely spurious. In the context of singleton topologies, namely when it is assumed that each event has at most a unique predecessor, the aim is to filter out the spurious edges from a general prima facie topology, so to extract a singleton prima facie structure (see Figure §3.1).\nProposition §5 summarizes Suppes basic notion of prima facie causality, while it is ignoring deeper discussions of causationthat aim at distinguishing between actual genuine and spurious causes (see §A), e.g. screening-off, background context, d-separation [22, 150, 80]. For our purposes however, the above definition is sufficient when (i) all the significant events are considered, i.e., all the genuine causes are observed as in a closedworld assumption, and (ii) we aim at extracting the order of progression among them (or determine that there is no apparent relation), rather than extracting causalities per se.\nFinally, we recall a few algebraic requirements necessary for our framework to be welldefined. First of all, the pr must be computable: every mutation a should be observed with probability strictly 0 < P(a) < 1. Moreover, we need each pair of mutations (a, b) to be distinguishable in terms of pr, that is, for each pair of mutations a and b, P(a | b) < 1 or P(b | a) < 1 similarly to the above condition. Any non-distinguishable pair of events can be merged as a single composite event. From now on, we will assume these conditions to be verified."
    }, {
      "heading" : "3.3 Results and discussion",
      "text" : "We will now present the method in details."
    }, {
      "heading" : "3.3.1 Extracting progression trees",
      "text" : "The CAPRESE reconstruction method is described in Algorithm §1. The algorithm is similar to Desper’s and Szabo’s algorithms, the main difference being an alternative weight function based on a shrinkage-like estimator.\nDefinition 5 (Shrinkage-like estimator). We define the shrinkage-like estimator ma→b of the confidence in the causationrelationship from a to b as\nma→b = (1− λ)αa→b + λβa→b , (3.5)\n22"
    }, {
      "heading" : "3.3. RESULTS AND DISCUSSION 23",
      "text" : "where 0 ≤ λ ≤ 1 and\nαa→b = P(b | a)− P(b | a) P(b | a) + P(b | a) βa→b = P(a ∧ b)− P(a)P(b) P(a ∧ b) + P(a)P(b) . (3.6)\nThis estimator is similar in spirit to a shrinkage estimator (see [45]) and, in fact, it combines a normalized version of the pr, i.e., the raw estimate α, with a correction factor β (which in this case is a correlation-based measure of temporal distance among events), to estimate the confidence of each selective advantage relationship. The parameter λ is the analogous of the shrinkage coefficient and can have a Bayesian interpretation in terms of a measure of the belief that a and b are causally relevant to one another and of the evidence that a raises the probability of b. In the absence of a closed form solution for the optimal value of λ, one may rely on cross-validation of simulated data. The power of shrinkage (and the shrinkage-like estimator) lies in the possibility of determining an optimal value for λ to balance the effect of the correction factor on the raw model estimate to ensure optimal performances on ill posed instances of the inference problem. A crucial difference, however, between our estimator and classical shrinkage, is that this estimator aims at improving the performance of the overall reconstruction process, not limited to the performance of the estimator itself as is the case in shrinkage. That is, the metric m induces an ordering to the events reflecting the confidence for their causation. Furthermore, since we make no assumption about the underlying distribution, we learn it empirically by cross-validation. In the next sections we show that the shrinkage-like estimator is an effective way to get such an ordering especially when data are noisy. In CAPRESE we use a pairwise matrix version of the estimator.\nThe raw estimator and the correction factor. By considering only the raw estimator α, we would include an edge (a → b) in the tree consistently in terms of (i) Definition §4 and (ii) if a is the best probability raiser for b. When P(a) = P(b) the events a and b are indistinguishable in terms of temporal priority, thus α is not sufficient to decide their causal relation, if any. This intrinsic ambiguity is unlikely in practice even if, in principle, it is possible. Notice that this formulation of α is a monotonic normalized version of the pr ratio.\nProposition 6 (Monotonic normalization). For any two events a and b we have\nP(a) > P(b) ⇐⇒ P(b | a)P(b | a) > P(a | b) P(a | b) ⇐⇒ αa→b > αb→a . (3.7)\nThis raw model estimator satisfies −1 ≤ αa→b, αb→a ≤ 1: when it tends to −1 the pair of events appear disjointly (i.e., they show an anti-causation pattern), when it tends to 0 no causation or anti-causation can be inferred and the two events are statistically independent and, when it tends to 1, the causation relationship between the two events is genuine. Therefore, α provides a quantification of the degree of confidence for a pr selective advantage relationship. In fact, for any given possible causation edge (a, b), the term P(b | a) gives an estimate of the error rate of b, therefore the numerator of the raw\n23"
    }, {
      "heading" : "24 CHAPTER 3. SINGLETON MODELS OF CANCER PROGRESSION",
      "text" : "Algorithm 1: CAPRESE: a tree-alike reconstruction with a shrinkage-like estimator\n1: consider a set of n genetic events G plus a special event , added to each sample of the dataset; 2: define a m× n matrix M where each entry contains the shrinkage-like estimator\nmi→j = (1− λ) · P(j | i)− P(j | i) P(j | i) + P(j | i) + λ · P(i ∧ j)− P(i)P(j) P(i ∧ j) + P(i)P(j)\naccording to the observed probability of the events i and j; 3: [pr causation] define a tree T = (G ∪ { }, E, ) where (i→ j) ∈ E for i, j ∈ G\nif and only if:\nmi→j > 0 and mi→j > mj→i and ∀i′ ∈ G, mi,j > mi′,j .\n4: [Independent progressions filter] define Gj = {x ∈ G | P(x) > P(j)}, replace edge (i→ j) ∈ E with edge ( → j) if, for all x ∈ Gj , it holds\n1 1 + P(j) > P(x) P(x) + P(j) P(x ∧ j) P(x)P(j) .\nmodel α provides an estimate of how often b is actually caused by a. The α estimator is then normalized to range between −1 and +1.\nHowever, α does not provide a general criterion to disambiguate among genuine causes of a given event. We show a specific case in which α is not a sufficient estimator. Let us consider, for instance, a causal linear path: a → b → c. In this case, when evaluating the candidate parents a and b for c we have: αa→c = αb→c = 1, so a and b are genuine causes of c, though we would like to select b, instead of a. Accordingly, we can only infer that ta < tc and tb < tc, i.e., a partial ordering, which does not help to disentangle the relation among a and b with respect to c.\nIn this case, the β coefficient can be used to determine which of the two genuine causes occurs closer, in time, to c (b, in the example above). In general, such a correction factor provides information on the temporal distance between events, in terms of statistical dependency. In other words, the higher the β coefficient, the closer two events are. Therefore, when dealing with noisy data and limited sample sizes, there are situations where, by using the α estimator alone, we could infer a wrong transitive edge to be the most likely cause even in the presence of the real cause. For this reason, we reduce the α estimator to the correction factor β, which, for each given edge (a, b), is normalized within −1 and (1−max[P(a),P(b)])/(1 + max[P(a),P(b)]) < +1.\nThe shrinkage-like estimator m then results in the combination of the raw pr estimator α and of the β correction factor, which respects the temporal priority induced by\n24\n3.3. RESULTS AND DISCUSSION 25\nα.\nProposition 7 (Coherence in dependency and temporal priority). The β correction factor is symmetrical and subsumes the same notion of dependency of the raw estimator α, that is\nP(a ∧ b) > P(a)P(b) ⇔ αa→b > 0⇔ βa→b > 0 and βa→b = βb→a . (3.8)\nThe independent progressions filter. As in Desper’s approach, we also add a root with P( ) = 1 in order to separate different progression paths, i.e., the different sub-trees rooted in . CAPRESE initially builds a unique tree by using the estimator; typically, the most likely event will be at the top of the progression even if there may be rare cases where more than one event has no valid parent, in these cases we would already be reconstructing a forest. In the reconstructed tree, all the edges represent the most confident prima facie cause, although some of those could still be spurious causes. Then the correlation-like weight between any node j and is computed as\nP( ) P( ) + P(j) P( ∧ j) P( )P(j) =\n1\n1 + P(j) .\nIf this quantity is greater than the weight of j with each upstream connected element i, we consider the best prima facie cause of j to be a spurious cause and we substitute the edge (i → j) with the edge ( → j). Note that in this work we are ignoring deeper discussions of probabilistic causationthat aim at distinguishing between actual genuine causes and spurious causes. Instead, we remove spurious causes by using a filter based on correlation because the probability raising of the omnipresent event is not well defined (see Methods). In addition, we remark that the evaluation for an edge to be a genuine or a spurious cause takes into account all the given events. Because of this, if events are added or removed from the dataset, the same edge can be defined to be genuine or spurious as the set of events included in the model is varied arbitrarily. However, since we do not consider the problem of selecting the set of progression events, we assume that all and only the relevant events for the problem at hand are already known a priori and included in the model.\nFinally, note that this filter is indeed implying a non-negative threshold for the shrinkage-like estimator, when an edge is valid.\nTheorem 1 (Independent progressions). Let G∗ = {a1, . . . , ak} ⊂ G a set of k prima facie causes for some b 6∈ G∗, and let a∗ = maxai∈G∗{mai→b}. The reconstructed tree by CAPRESE contains edge → b instead of a∗ → b if, for all ai ∈ G∗\nP(ai, b) < P(ai)P(b) 1 1 + P(b) + P(b)2 1 + P(b) . (3.9)\nThe proof of this theorem can be found in §C. What this theorem suggests is that, in principle, by examining the level of statistical dependency of each pair of events, it\n25"
    }, {
      "heading" : "26 CHAPTER 3. SINGLETON MODELS OF CANCER PROGRESSION",
      "text" : "would be possible to determine how many trees compose the reconstructed forest. Furthermore, the theorem suggests that CAPRESE could be defined by first processing the independent progressions filter, and then using m to build the independent progression trees in the forest.\nTo conclude, the algorithm reconstructs a well defined tree (or, more in general, forest).\nTheorem 2 (Algorithm correctness). CAPRESE reconstructs a well defined tree T without disconnected components, transitive connections and cycles.\nAdditionally, asymptotically with the number of samples, the reconstructed tree is the correct one.\nTheorem 3 (Asymptotic convergence). Let T = (G ∪ { }, E, ) be the forest to reconstruct from a set of s input samples, given as the input matrix D. If D is strictly sampled from the distribution induced by T and infinite samples are available, i.e., s→∞, CAPRESE with λ→ 0 correctly reconstructs T .\nThe proof of these Theorems are also in §C. These theorems considered datasets where the observed and theoretical probabilities match, because of s → ∞. However, data often contains false positives and negatives (i.e., data are noisy) and resistance to their effects is desirable in any inferential technique. With this in mind, we prove a corollary of the theorem analoguos to a result appearing in [173].\nCorollary 1 (Uniform noise). Let the input matrix D be strictly sampled from the distribution induced by T with sample size s → ∞, but let it be corrupted by noise levels of false positives + and false negatives −. Let pmin = minx∈G{P(x)}, CAPRESE correctly reconstructs T for λ→ 0 whenever\n+ < √ pmin(1− + − −)\nand + + − < 1.\nEssentially, this corollary states that CAPRESE (and so the estimator m) is robust against a noise affecting all samples equally. Also, the fact that it holds for λ → 0 is sound with the theory of shrinkage estimators for which, asymptotically, the corrector factor is not needed to regularize the ill posed problem."
    }, {
      "heading" : "3.3.2 Optimal shrinkage-like coefficient",
      "text" : "Theorem §3 and Corollary §1 state that with infinite samples and mild constraints on the false positive/negative rates we get optimal results with λ → 0. Precisely, for the uniform noise model that we applied to synthetic data, we have + = − = ν/2, thus the hypothesis required by Corollary §1 is\nν <\n√ pmin\n1/2 + √ pmin .\n26"
    }, {
      "heading" : "3.3. RESULTS AND DISCUSSION 27",
      "text" : "For pmin = 0.05, which we set in data generation (see §C), this inequality implies correct reconstruction for ν < 0.3 (a 15% error rate), with infinite samples. However, we are interested in performance and the optimal value of λ in situations in which we have finite sample sizes as well. Here, we empirically estimate the optimal λ value, both in the case of trees and forests, as a function of noise and sample size. In the next section, we assess performance of our algorithm empirically.\nIn Figure §3.2, we show the variation of the performance of CAPRESE as a function of λ, for datasets with 150 samples generated from tree topologies. The optimal value, i.e., lowest Tree Edit Distance (TED), for noise-free datasets (i.e., ν = 0) is obtained for λ→ 0, whereas for the noisy datasets a series of U-shaped curves suggests a unique optimum value for λ → 1/2, immediately observable for ν < 0.15. Identical results are obtained when dealing with forests (not shown here). In addition, further experiments with n varying around the typical sample size (n = 150) show that the optimal λ is largely insensitive to the dataset size (see Figure §3.3). Thus we have limited our analysis to datasets with the typical sample size that is characteristic of data currently available.\nSummarizing, Figures §3.2 and §3.3 suggest that for sample size below 250 without false positives and negatives the pr raw estimate α suffices to drive reconstruction to\n27\n28 CHAPTER 3. SINGLETON MODELS OF CANCER PROGRESSION\ngood results (TED is 0 with 250 samples), i.e.,\nma→b λ→0≈ αa→b (3.10)\nwhich is obtained by setting λ to a very small value, e.g. 10−2, in order to consider at least a small contribution of the correction factor too. Conversely, when ν > 0, the best performance is obtained by averaging the shrinkage-like effect, i.e.,\nma→b λ=1/2 = αa→b 2 + βa→b 2 . (3.11)\nThese results suggest that, in general, a unique optimal value for the shrinkage-like coefficient can be determined, even in situations not captured by Theorem §3 and Corollary §1."
    }, {
      "heading" : "3.3.3 Performance measure and synthetic datasets",
      "text" : "To evaluate the performance of the algorithm, named CAPRESE, i.e., CAncer PRogression Extraction with Single Edge, proposed here to infer singleton prima facie topologies, we made substantial use of synthetic data as a function of dataset size and the false positive and negative rates. Many distinct synthetic datasets were created for this purpose, as explained below. The algorithm’s performance was measured in terms of Tree Edit Distance (TED, [197]), i.e., the minimum-cost sequence of node edit operations (relabeling, deletion and insertion) that transforms the reconstructed trees into the ones generating the data. The choice of this evaluation measure is motivated by the fact that we are interested in the structure behind the progressive phenomenon of cancer evolution and, in particular, we are interested in a measure of the genuine causes that we miss and of the spurious causes that we fail to recognize (and eliminate). Also, since topologies with similar distributions can be structurally different we choose to measure\n28"
    }, {
      "heading" : "3.3. RESULTS AND DISCUSSION 29",
      "text" : "performance using structural distance rather than a distance in terms of distributions. Within the realm of ‘structural metrics’ however, we have also evaluated the performance with the Hamming Distance [76], another commonly used structural metric, and we obtained analogous results (see §C).\nSynthetic data generation and experimental setting. We generated synthetic data by sampling from various random trees constrained to have depth log(|G|), since wide branches are harder to reconstruct than straight paths, and by sampling event probabilities in [0.05, 0.95] (see §C).\nUnless explicitly specified, in all the experiments we used 100 distinct random trees (or forests, accordingly to the test to perform) of 20 events each. This seems a fairly reasonable number of events and is in line with the usual size of reconstructed trees, e.g., [166, 69, 116, 148]. The scalability of the techniques was tested against the number of samples by ranging |G| from 50 to 250, with a step of 50, and by replicating 10 independent datasets for each parameters setting (see the caption of the figures for details).\nWe included a form of noise in generating the datasets, in order to account for (i) the realistic presence of biological noise (such as the one provided by bystander mutations, genetic heterogeneity, etc.) and (ii) experimental errors. A noise parameter 0 ≤ ν < 1 denotes the probability that any event assumes a random value (with uniform probability), after sampling from the tree-induced distribution. Algorithmically this process generates, on average, |G|ν/2 random entries in each sample (e.g., with ν = 0.1 we have, on average, one error per sample). We wish to assess whether these noisy samples can mislead the reconstruction process, even for low values of ν. Notice that assuming a uniformly distributed noise may appear simplistic since some events may be more robust, or easy to measure, than others. However, introducing in the data both false positives (at rate + = ν/2) and negatives (at rate − = ν/2) makes the inference problem substantially harder, and was first investigated in [173].\nIn the next section, we refer to datasets generated with rate ν > 0 as noisy synthetic dataset. In the numerical experiments, ν is usually discretized by 0.025, (i.e., 2.5% noise).\nPerformance of CAPRESE compared to oncotrees\nAn analogue of Theorem §3 holds for Despers’s oncotrees (Theorem 3.3, [38]), and an analogue of Corollary §1 holds for Szabo’s extension with uniform noise (Reconstruction Theorem 1, [173]). Thus, with infinite samples both approaches reconstruct the correct trees/forests. With finite samples and noise, however, their performance may show different patterns, as speed of convergence may vary. We investigate this issue in the current section.\nIn Figure §3.4 we compare the performance of CAPRESE with oncotrees, for the case of noise-free synthetic data with the optimal shrinkage-like coefficient: λ→ 0, equation (§3.10). Since Szabo’s algorithm is equivalent to Desper’s without false negatives and\n29"
    }, {
      "heading" : "30 CHAPTER 3. SINGLETON MODELS OF CANCER PROGRESSION",
      "text" : "positives, we rely solely on Szabo’s implementation [173]. In Figure §3.5 we show an example of reconstructed tree where CAPRESE infers the correct tree while oncotrees mislead an edge.\nIn general, one can observe that the TED of CAPRESE is, on average, always bounded above by the TED of oncotrees, both in the case of trees and forests. For trees, with 50 samples the average TED of CAPRESE is around 6, whereas for Desper’s technique it is around 13. The performance of both algorithms improves as long as the number of samples is increased: CAPRESE has the best performance (i.e., TED ≈ 0) with 250 samples, while oncotrees have TED around 6. When forests are considered, the difference between the performance of the algorithms reduces slightly, but also in this case CAPRESE clearly outperforms oncotrees.\nNotice that the improvement due to the increase in the sample size seems to reach a plateau, and the initial TED for our estimator seems rather close to the plateau value. This empirical analysis suggests that CAPRESE has already good performances with few samples, a favorable adjoint to Theorem §3. This result has some important practical implications, particularly considering the scarcity of available biological data.\nIn Figure §3.6 we extend the comparison to noisy datasets. In this case, we used the optimal shrinkage-like coefficient: λ→ 1/2, equation (§3.11). The results confirm what observed without false positives and negatives, as CAPRESE outperforms oncotrees up to ν = 0.15, for all the sizes of the sample sets. In §C, similar plots for the noise-free case are shown.\nWe can thus draw the conclusion that our algorithm performs better with finite samples and noise, since less samples are required to get good performances and a higher resistance to false positives and negatives is shown.\n30"
    }, {
      "heading" : "3.3. RESULTS AND DISCUSSION 31",
      "text" : "31"
    }, {
      "heading" : "32 CHAPTER 3. SINGLETON MODELS OF CANCER PROGRESSION",
      "text" : "Performance of CAPRESE compared to Conjunctive Bayesian Networks\nInspired by Desper’s seminal work, Beerenwinkel and others developed methods to estimate the constraints on the order in which mutations accumulate during cancer progression, using a probabilistic graphical model called Conjuntive Bayesian Networks (CBN) [10, 65]. While the goal of this research was to reconstruct direct acyclic graphs and not trees per se, evidence presented in [72] suggests that, in the absence of noise, these models perform better than oncotrees even at reconstructing trees. For this reason, we performed experiments similar to the ones suggested above, comparing CAPRESE to the extension of CBN called hidden-CBN (h-CBN) that accounts for noisy genotype observations [65]. This method combines CBNs with a simulated annealing algorithm for structure search and a denoising of the genotypes via the maximum a posteriori estimates to compute the most likely progression. One aspect that complicates a comparison between CAPRESE and (h-)CBN is that the methods assume different models. For example, at the heart of CBN is a monotonicity assumption (i.e., an event can only occur if all its predecessors have occurred) not assumed by CAPRESE. Despite the differences between the model assumptions, we present a comparison between the methods in §C, indicating that CAPRESE not only outperform oncotrees, but h-CBNs as well. In particular, this suggests that CAPRESE converges much faster than h-CBNs with respect to the sample size, also in the presence of noise.\nWe also analyze the rate of false positives/negatives reconstructed by CAPRESE when (synthetic) data are sampled from DAGs ( §C). The rate of false positives goes to 0 as the sample size increases, implying that CAPRESE is capable of reconstructing a tree subsumed by the underlying causal DAG topology. In addition, the number of false negatives approaches a value proportional to the connectivity of the model from which the data was generated. This is expected, since CAPRESE will assign at most one cause to each considered event. However, it should be noted that further experiments with samples selected from a wider array of topologies should be performed to confirm these results and compare both methods in full. While not within the scope of this specific work."
    }, {
      "heading" : "3.3.4 Case studies",
      "text" : "In the next subsections we apply CAPRESE to real cancer data obtained by Comparative Genomic Hybridization (CGH) and Next Generation Sequencing (NGS). This shows the potential application of reconstruction techniques to various types of mutational profiles and various cancers."
    }, {
      "heading" : "Performance on cancer CGH datasets",
      "text" : "We test our reconstruction approach on a real ovarian cancer dataset made available within the oncotree package [38]. The data were collected through the public platform SKY/M-FISH [100], used to allow investigators to share molecular cytogenetic data. The data was obtained by using the CGH technique on samples from papillary serous cystade-\n32"
    }, {
      "heading" : "3.3. RESULTS AND DISCUSSION 33",
      "text" : "nocarcinoma of the ovary. This technique uses fluorescent staining to detect CNV data at the resolution of chromosome arms. While the recent emergence of NGS approaches make the dataset itself rather outdated, the underlying principles remain the same and the dataset provides avalid test-case for our approach. The seven most commonly occurring events are selected from the 87 samples, and the set of events are the following gains and losses on chromosomes arms G = {8q+, 3q+, 1q+, 5q−, 4q−, 8p−, Xp−} (e.g., 4q− denotes a deletion of the q arm of the 4th chromosome).\nIn Figure §3.7 we compare the trees reconstructed by the two approaches. Our technique differs from Desper’s by predicting the causal sequence of alterations\n8q+ → 8p− → Xp− , when used either λ→ 0 or λ = 1/2. Notice that among the samples in the dataset some are not generated by the distribution induced by the recovered tree, thus comparing the reconstruction for both λs is necessary.\nAt this point, we do not have a biological interpretation for this result. However, we do know that common cancer genes reside in these regions, e.g. the tumor suppressor gene Pdgfr on 5q and the oncogene Myc on 8q), and loss of heterozygosity on the short arm of chromosome 8 is quite common (see, e.g., http://www.genome.jp/kegg/). Recently, evidence has been reported that 8p contains many cooperating cancer genes [193].\nIn order to assign a confidence level to these inferences we applied both parametric and non-parametric bootstrapping methods to these results. Essentially, these tests\n33"
    }, {
      "heading" : "34 CHAPTER 3. SINGLETON MODELS OF CANCER PROGRESSION",
      "text" : "consist of using the reconstructed trees (in the parametric case), or the probability observed in the dataset (in the non-parametric case) to generate new synthetic datasets, and then reconstructs again the progressions (see, e.g., [46] for an overview of these methods and [174] for the use of bootstrap for evalutating the confidence of oncogenetic trees.). The confidence is given by the number of times the trees in Figure §3.7 are reconstructed from the generated data. A similar approach can be used to estimate the confidence of every edge separately. For oncotrees the exact tree is obtained 83 times out of 1000 non-parametric resamples, so its estimated confidence is 8.3%. For CAPRESE the confidence is 8.6%. In the parametric case with false positive and false negative error rates of 0.21 and 0.027, following [173], the confidence of oncotrees is 17% while the confidence of our method is much higher: 32%. When error rates are forced to 0 the confidence of oncotrees raises to 86.6% and 90.9% respectively.\nFor the non-parametric case, edges confidence is shown in Table §3.1. Most notably, our algorithm reconstructs the inference 8q+ → 8p− with high confidence (confidence 62%, and 26% for 5q− → 8p−), while the confidence of the edge 8q+ → 5q− is only 39%, almost the same as 8p− → 8q+ (confidence 40%). The confidences are similar with either λ→ 0 or λ = 1/2.\n34"
    }, {
      "heading" : "3.3. RESULTS AND DISCUSSION 35",
      "text" : "Analysis of other CGH datasets. We report the differences between the reconstructed trees also based on datasets of gastrointestinal and oral cancer ([69, 148] respectively). In the case of gastrointestinal stromal cancer, among the 13 CGH events considered in [69] (gains on 5p, 5q and 8q, losses on 14q, 1p, 15q, 13q, 21q, 22q, 9p, 9q, 10q and 6q), oncotrees identify the path progression\n1p− → 15q− → 13q− → 21q−\nwhile CAPRESE reconstructs the branch\n1p− → 15q− 1p− → 13q− → 21q − .\nIn the case of oral cancer, among the 12 CGH events considered in [148] (gains on 8q, 9q, 11q, 20q, 17p, 7p, 5p, 20p and 18p, losses on 3p, 8p and 18q), the reconstructed trees differ since oncotrees identifies the path\n8q+→ 20q+→ 20p+\nwhile our algorithm reconstructs the path\n3p− → 7p+→ 20q+→ 20p+ .\nThese examples show that CAPRESE provides important differences in the reconstruction compared to oncotrees."
    }, {
      "heading" : "Performance on cancer NGS datasets",
      "text" : "In this section we show the application of reconstruction techniques to the validation of a specific relation among recurrent mutations involved in atypical Chronic Myeloid Leukemia (aCML).\nIn [154] Piazza et al. used high-throughput exome sequencing technology to identity somatically acquired mutations in 64 aCML patients, and found a previously unidentified recurring missense point mutation hitting setbp1. By re-sequencing setbp1 in samples with aCML and other common human cancers, they found that around 25% of the aCML patients tested positive for setbp1, while most of the other types of tumors were negative. Assessing the possible relationship between setbp1 variants and the mutations in many driver aCML oncogenes such as (e.g., asxl1, tet2, kras, etc.) no significant association or mutual exclusion with setbp1 was found but for asxl1, which was frequently mutated together with setbp1, hinting at a potential relation among the events. In particular, asxl1 was presenting either a non-sense point or a indel type of somatic mutation.\nHence, we reconstructed aCML progression models from the datasets provided in [154], with the goal of assessing a potential selective advantage relation between mutated setbp1 and asxl1. A more extensive analysis is postponed, as we only seek to clearly illustrate the functionalities of the algorithm here.\n35"
    }, {
      "heading" : "36 CHAPTER 3. SINGLETON MODELS OF CANCER PROGRESSION",
      "text" : "As a first case (Figure §3.8, left), we treated the asxl1 missense point and indel mutations as indistinguishable, and we merged the two events in the dataset. Afterwards, we separated the two types of mutations for asxl1 (Figure §3.8, right).\nIn particular, it is interesting to notice that, when the asxl1 mutations are considered equivalent, the inference suggests that the mutations belong to two independent progression paths (i.e., the independent progression filters “breaks” every potential selective advantage relation among the events). Conversely, when the mutations are kept separate, the progression model suggests that: (i) the missense point mutation hitting setbp1 can cause a non-sense point mutation in asxl1 and (ii) the observed asxl1 mutations seems to be independent. Concerning edges confidence, as before assessed via non-parametric bootstrap, it is worth noting that the confidence in the indel asxl1 mutation being an early event raises consistently in the latter case.\nAll in all, it seems that a progression model allows to test the significance of the association firstly observed in [154] and also refines the knowledge by suggesting a specific causal and temporal relations among events. With this this in mind, ad-hoc sequencing experiments might be set up to assess these predictions, eventually providing a strong evidence that could be used to, e.g., synthesize a progression-specific aCML-effective drug.\n36\nCHAPTER 4\nMORE COMPLEX MODELS OF CANCER PROGRESSION\nIn this Chapter we will present an algorithm that is capable of efficiently inferring complex models of cancer progression such as directed acyclic graphs. As a reference, see [158]."
    }, {
      "heading" : "4.1 Problem setting",
      "text" : "We define a progression DAG as a directed acyclic graph D = (N, π), where N ⊆ U is the set of nodes (e.g., selected from a universe U of mutations or propositional formulas) and π : N → ℘(N) is a function, which associates with each node j its parents π(j) ⊆ N . We wish to study the cases where such a DAG can be seen as a model for the following classes of selectivity patterns, expressed in conjunctive normal form (CNF). The symbol stands for the selectivity relation.\nDefinition 6 (DAG patterns). A D = (N, π) is a model for models the patterns⋃ j∈N { (c1 ∧ . . . ∧ cn) j | π(j) = {c1, . . . , cn} } ,\nwhere c1 ∧ . . . ∧ cn is a CNF formula (each clause cj is one of two kinds: either an atomic event or a disjunction of events).\nEach DAG represents an induced distribution of observing a subset of the considered events in a set of samples (i.e., the probability of observing certain genetic alterations in a group of patients or cells representing their mutational profile).\nDefinition 7 (DAG-induced distribution). Let D = (N, π) be a DAG and α : N → [0, 1] a labeling function, D generates a distribution where the probability of observing N∗ ⊆ N events is\nP(N∗) = ∏ x∈N∗ α(x) · ∏ y∈N\\N∗ [ 1− α(y) ] (4.1)\n37"
    }, {
      "heading" : "38 CHAPTER 4. MORE COMPLEX MODELS OF CANCER PROGRESSION",
      "text" : "whenever x ∈ N∗, π(x) ⊂ N∗, and 0 otherwise.\nNotice that this definition, as expected, is equivalent to the one used in [10] and retains a tree-induced distribution such as those used in [117, 38, 173]. Further, notice that a sample which contains an event but not all of its parents has a zero probability, thus subsuming the conjunctive interpretation of DAGs, as the result of compositional reasoning to infer co-occurrence patterns. These kinds of samples, which represent “irregularities” with respect to D, might be generated when adding false positives/negatives to the sampling strategy."
    }, {
      "heading" : "4.2 A novel efficient framework",
      "text" : "Building on the framework described in the §2, we now describe the implementation of my framework for CAncer PRogression Inference (named CAPRI)’s building blocks. Notice that, in general, the inference of cancer progression models requires a complex data processing pipeline, as summarized in Figure §4.1; its architecture optimally exploits CAPRI’s efficiency.\n38"
    }, {
      "heading" : "4.2. A NOVEL EFFICIENT FRAMEWORK 39",
      "text" : "Assumptions. CAPRI relies on the following assumptions: i) Every pattern is expressible as a propositional CNF formula; ii) All events are persistent, i.e., an acquired mutation cannot disappear; iii) All relevant events in tumor progression are observable, with the observations describing the progressive phenomenon in an essential manner (i.e., closed world assumption, in which all events ‘driving’ the progression are detectable); iv) All the events have non-degenerate observed probability in (0, 1); v) All events are distinguishable, in the following sense: input alterations produce different profiles across input samples.\nAssumptions i-ii) relate to the framework derived in previous section, while iii) imposes an onerous burden on the experimentalists, who must select the relevant genomic events to model1. Assumption iv) relates instead to the statistical distinguishability of the input events (see the next section on CAPRI’s Data Input).\nTrading Complexity for Expressivity. To automatically extract the patterns that underly a progression model, one may try to adopt a brute-force method of enumerating and testing all possibilities. This strategy is computationally intractable, however, since the number of (distinct) (sub)formulæ grows exponentially with the number of events included in the model. Therefore, we need to exploit certain properties of the relation whenever possible, and trade expressivity for complexity in other cases, as explained below.\nNote that singleton and co-occurrence (∧) types of patterns are amenable to compositional reasoning : if i1∧. . .∧ik j then, for any p = 1, . . . , k, ip j. This observation leads to the following straightforward strategy of evaluating every conjunctive (and henceforth singleton) relation using a pairwise-test for the selectivity relation (see Figure §2.2).\nUnfortunately, it is easy to see that this reasoning fails to generalize for CNF patterns: e.g., when the pattern contains disjunctive operators (∨). As an example, consider pattern a ∨ b c, in a cancer where {a,¬b} progression to c is more prevalent than {¬a, b} and {a, b}. In this case, considering sub-formulas only we might find a c but miss b c because the probability of mutated b is smaller than that of c, thus invalidating condition (1) of relation . Notice that in extreme situations, when the data is very noisy, the algorithm may even “invert” the selectivity relation to c b.\nThis difficulty is not a peculiarity of my framework, but rather intrinsic to the problem of extracting complex “causal networks” (see, [151, 150, 98]). To handle this situation, CAPRI adapts a strategy that trades complexity for expressivity: the resulting inference procedure, Algorithm §2, can be executed in two modes: unsupervised and supervised. In the former, inferred patterns of confluent progressions are constrained to co-occurrence types of relations, in the latter CAPRI can test more complex patterns, i.e., disjunctive or “mutual exclusive” ones, provided they are given as prior hypotheses.\n1Theoretically, this assumption - common to other Bayesian learning problems - is necessary to prove CAPRI’s ability to extract the exact model in the optimal case of infinite samples. Practically, as all relevant events are hardly selectable a priori and sample size is finite, further statistics can be used to select the most relevant driver alterations – see also Section §4.3. Nonetheless, CAPRI can provide significant results even if this assumption is not or cannot be verified.\n39"
    }, {
      "heading" : "40 CHAPTER 4. MORE COMPLEX MODELS OF CANCER PROGRESSION",
      "text" : "In both cases, CAPRI’s complexity – studied in next sections – is quadratic both in the number of events and hypotheses.\nData Input (Step 1). CAPRI (sett, Algorithm §2) requires an input set G of n events, i.e., genomic alterations, and m cross-sectional samples, represented as a dataset in an m × n binary matrix D, in which an entry Di,j = 1 if the event j was observed in sample i, and 0 otherwise. Assumption iv) is satisfied when all columns in D differ - i.e., the alteration profiles yield different observations.\nOptionally, a set of k input hypotheses Φ = {ϕ1 e1, . . . , ϕk ek}, where each ϕi is a well-formed2 CNF formula. Note that we advise that the algorithm be used in the following regime 3: k + n m.\nData Preprocessing (Lifting, step 2). When input hypotheses are provided (e.g., by a domain expert), CAPRI first performs a lifting operation over D to permit direct inference of complex selectivity relations over a joint representation, which involve input events as well as the hypotheses. Lifting operation evaluates each input CNF formula – for all input hypotheses in Φ – and outputs a augmented matrix D(Φ) to be processed further as in step 1. As an example, consider hypothesis a ⊕ b c augmented input matrix D is:\nD(Φ) =  a b c a⊕ b c 1 1 1 1⊕ 1 = 0 1 0 1 1⊕ 0 = 1 0 1 0 0⊕ 1 = 1 1 0 1 1⊕ 0 = 1  .\nNote that the first row (profile {a, b, c}) contradicts the hypothesis, while all other rows support it.\nSelectivity Topology (steps 3, 4, 5). We exploit a compositional approach to test CNF hypotheses as follows: the disjunctive relations are grouped, and treated as if they were individual objects in G. For example, when a formula ϕ d where ϕ = (a ∨ b) ∧ c is considered, we assess ϕ d as whether (a ∨ b) d and c d hold – with the proviso that we treat (a ∨ b) as an individual event. Formally, with clauses (ϕ) we denote the disjunctive clauses in a CNF formula.\nNodes in the reconstruction are all input events together with all the disjunctive clauses of each input formula ϕ.\nEdges in the reconstructed DAG are patterns that satisfy both conditions (1) and (2) of the selectivity relation . Formally, CAPRI includes an edge between two nodes ϕ\n2Formally, we require that ϕi 6v ei, where v represents the usual syntactical ordering relation among atomic events and formulas, and disallows for example a ∨ b a.\n3In the current biomedical setting, the number of samples (m) is usually in the hundreds, while number of possible mutations (n) and hypotheses (k), absent any pre-processing, could be large, thus violating the assumption; in these cases, we rely on various commonly used pre-preprocessing filters to limit n to driver mutations, and k to simple hypotheses involving the driver mutations. However, in the future as the number of samples increases, we envision a more agnostic application.\n40"
    }, {
      "heading" : "4.2. A NOVEL EFFICIENT FRAMEWORK 41",
      "text" : "and j only if both Γϕ,j = P(ϕ)−P(j) and Λϕ,j = P(j | ϕ)−P(j | ϕ) are strictly positive. Note that ϕ can be both a disjunctive clause as well as a singleton event. A function π(·) assigns a parent to each node that is not an input formula. Note that this approach works efficiently by nature of the augmented representation of D. The reconstructed DAG contains all the true positive patterns, with respect to , plus spurious instances of which CAPRI subsequently removes in step 6 (see §D for a proof of this statement).\nNote that D can be readily interpreted as a probabilistic graphical model, once it is augmented with a labeling function α : N → [0, 1], where N is the set of nodes – i.e., the genetic alterations – such that α(i) is the independent probability of observing mutation i in a sample, whenever all of its parent mutations (i.e., π(i)) are observed (if any). Thus D induces a distribution of observing a subset of events in a set of samples (i.e., a probability of observing a certain mutational profile in a patient).\nMaximum Likelihood Fit (step 6). As the selectivity relation provides only a necessary condition, we must filter out all of its spurious instances that might have been included in D (i.e., the possible false positives).\nFor any selectivity relation, spurious claims contribute to a reduction in the likelihoodfit4 relative to true patterns. Thus, a standard maximum-likelihood fit can be used to select and prune the selectivity DAG (including a regularization term to avoid overfitting5). Here, we adopt the Bayesian Information Criterion (BIC), which implements Occam’s razor by combining log-likelihood fit with a penalty criterion proportional to the log of the DAG size via Schwarz Information Criterion (see [167]). The BIC score is defined as follows.\nbic (D, D(Φ)) = LL (D, D(Φ))− logm 2 dim(D). (4.2)\nHere, D(Φ) is the augmented input matrix, m denotes the number of samples and dim(D) is the number of parameters in the model D. Because, in general, dim(·) depends on the number of parents each node has, it is a good metric for model complexity. Moreover, since each edge added to D increases model complexity, the regularization term based on dim(·) favors graphs with fewer edges and, more specifically, fewer parents for each node.\nAt the end of this step, D and the labeling function are modified accordingly, based on the result of BIC regularization. By collecting all the incoming edges in a node it is possible to extract the patterns, which have been selected by CAPRI as the positive ones.\n4The maximum-likelihood estimation (MLE) is a method for estimating the parameters of a statistical model given data. In general, given a dataset and its underlying statistical model, the maximum likelihood estimation aim at selecting the set of values of the model parameters (in the setting of this thesis, a set of arcs of a graphical model) that maximizes the likelihood function. Intuitively, this maximizes the agreement of the selected model given the observed data. See also §B.\n5In principle other regularisation strategies common to Bayesian learning could be used, e.g., Akaike information criterion (see [23] and references therein). In this task, we prefer to work with BIC which, in general, trades model complexity to reduce false positives rate.\n41"
    }, {
      "heading" : "42 CHAPTER 4. MORE COMPLEX MODELS OF CANCER PROGRESSION",
      "text" : "Algorithm 2: CAncer PRogression wenference (CAPRI)\n1: Input: A set of events G = {g1, . . . , gn}, a matrix D ∈ {0, 1}m×n and k CNF causal claims Φ = {ϕ1 e1, . . . , ϕk ek} where, for any i, ei 6v ϕi and ei ∈ G; 2: [Lifting] Define the lifting of D to D(Φ) as the augmented matrix\nD(Φ) =  D1,1 . . . D1,n ϕ1(D1,·) . . . ϕk(D1,·)... . . . ... ... . . . ... Dm,1 . . . Dm,n ϕ1(Dm,·) . . . ϕk(Dm,·)  . by adding a column for each ϕi ci ∈ Φ, with ϕi evaluated row-by-row. Define then the coefficients Γi,j = P(i)− P(j) and Λi,j = P(j | i)− P(j | i) pairwise over D(Φ);\n3: [DAG nodes] Define the set of nodes N = G ∪ (⋃\nϕi clauses (ϕi)\n) which contains both input events\nand the disjunctive clauses in every input formula of Φ. 4: [DAG edges] Define a parent function π where π(j 6∈ G) = ∅ – avoid edges incoming in a formula 6–\nand\nπ(j ∈ G) = {i ∈ G | Γi,j ,Λi,j > 0} ∪ {clauses (ϕ) | Γϕ,j ,Λϕ,j > 0, ϕ j ∈ Φ} .\nSet the DAG to D = (N, π). 5: [DAG labeling] Define the labeling α as follows\nα(j) = { P(j), if π(j) = ∅ and j ∈ G; P(j | i1 ∧ . . . ∧ in), if π(j) = {i1, . . . , in}.\n6: [Likelihood fit ] Filter out all spurious causes from D by likelihood fit with the regularization BIC score and set α(j) = 0 for each removed edge. 7: Output: the DAG D and α;\nInference Confidence: Bootstrap and Statistical Testing. To infer confidence intervals of the selectivity relations , CAPRI employs bootstrap with rejection resampling by estimating a distribution of the marginal and joint probabilities as follows. For each event, (i) CAPRI samples with repetitions rows from the input matrix D (bootstrapped dataset), (ii) CAPRI next estimates the distributions from the observed probabilities, and finally, (iii) CAPRI rejects values which do not satisfy 0 < P(i) < 1 and P(i | j) < 1 ∨ P(j | i) < 1, and iterates restarting from (i). We stop when we have, for each distribution, at least K values (in this case K = 100). Any inequality (i.e., checking temporal priority and probability raising) is estimated using the non-\n6Although CAPRI is equipped with bootstrap testing it is still possible to encounter various degenerate situations. In particular, for some pair of events it could be that temporal priority cannot be satisfactorily resolved, i.e. there is no significant p-value for any edge orientation. Thus, loops might be present in the inferred prima facie topology. Nonetheless, some of these could be still disentangled by probability raising, while some might remain, albeit rarely. To remove such edges we suggest to proceed as follows: (i) sort these edges according to their p-value (considering both temporal priority and probability raising), (ii) scan the sorted list in decreasing order of confidence, (iii) remove an edge if it forms a loop.\n42"
    }, {
      "heading" : "4.3. RESULTS AND DISCUSSION 43",
      "text" : "parametric Mann-Whitney U test7 with p-values set to 0.05. We compute confidence p-values for both temporal priority and probability raising using this test, which need not assume Gaussian distributions for the populations.\nOnce a DAG D is inferred both parametric and non-parametric bootstrapping methods can be used to assign a confidence level to its respective pattern and to the overall model. Essentially, these tests consist of using the reconstructed model (in the parametric case), or the probabilities observed in the dataset (in the non-parametric case) to generate new synthetic datasets, which are then reused to reconstruct the progressions (see, e.g., [45] for an overview of these methods). The confidence is estimated by the number of times the DAG or any instance of is reconstructed from the generated data.\nComplexity, Correctness and Expressivity. CAPRI has the following asymptotic complexity (see §D): (i) Without input hypotheses the execution is self-contained and polynomial in the size of D. (ii) In addition to the above cost, CAPRI tests input hypotheses of Φ at a polynomial cost in the size of |Φ|. In this case, however, its complexity may range over many orders of magnitude depending on the structural complexity of the input set Φ consisting of hypotheses. An empirical analysis of the execution time of CAPRI and the competing techniques on synthetic datasets is provided in §D.\nCAPRI is a sound and complete algorithm, and its expressivity in terms of the inferred patterns is proportional to the hypothesis set Φ which, in turn, determines the complexity of the algorithm. With a proper set of input hypothesis, CAPRI can infer all (and only) the true patterns from the data, filtering out all the spurious ones (Theorem 2, §D). Without hypotheses, besides singleton and co-occurrence, no other patterns can be inferred (see Figure §2.2). Also, some of these claims might be spurious in general for more complex (and unverified) CNF formula (see §D)."
    }, {
      "heading" : "4.3 Results and discussion",
      "text" : "To determine CAPRI’s relative accuracy (true-positives and false-negatives) and performance compared to the state-of-the-art techniques for network inference, we performed extensive simulation experiments. From a list of potential competitors of CAPRI, we selected: Incremental Association Markov Blanket (IAMB, [179]), the PC algorithm (see [171]), Bayesian Information Criterion (BIC, [167]), Bayesian Dirichlet with likelihood equivalence (BDE, [79]) Conjunctive Bayesian Networks (CBN, [65]) and Cancer Progression Inference with Single Edges (CAPRESE, [117]). These algorithms constitute a rich landscape of structural methods (IAMB and PC), likelihood scores (BIC and BDE) and hybrid approaches (CBN and CAPRESE).\nAlso, we applied CAPRI to the analysis of an atypical Chronic Myeloid Leukemia dataset of somatic mutations with data based onsee [154].\n7The Mann-Whithney U test is a rank-based non-parametric statistical hypothesis test that can be used as an alternative to the Student’s t-test and is particularly useful if data are not normally distributed.\n43"
    }, {
      "heading" : "44 CHAPTER 4. MORE COMPLEX MODELS OF CANCER PROGRESSION",
      "text" : "44"
    }, {
      "heading" : "4.3. RESULTS AND DISCUSSION 45",
      "text" : ""
    }, {
      "heading" : "4.3.1 Synthetic data",
      "text" : "We performed extensive tests on a large number of synthetic datasets generated by randomly parametrized progression models with distinct key features, such as the presence/absence of: (1) branches, (2) confluences with patterns of co-occurrence, (3) independent progressions (i.e., composed of disjoint sub-models involving distinct sets of events). Accordingly, we distinguish four classes of generative models with increasing complexity and the following features:\ntrees forests connected DAGs disconnected DAGs\n(1) 3 3 3 3 (2) 7 7 3 3 (3) 7 3 7 3\nThe choice of these different type of topologies is not a mere technical exercise, but rather it is motivated, in this application of primary interest, by heterogeneity of cancer cell types and possibility of multiple cells of origin.\nTo account for biological noise and experimental errors in the data we introduce a parameter ν ∈ (0, 1) which represents the probability of each entry to be random in D, thus representing a false positive ( +) and a false negative rate ( −): + = − = ν/2 . The noise level complicates the inference problem, since samples generated from such topologies will likely contain sets of mutations that are correlated but causally irrelevant.\nTo have reliable statistics in all the tests, 100 distinct progression models per topology are generated and, for each model, for every chosen combination of sample set size m and noise rate ν, 10 different datasets are sampled (see §D for my synthetic data generation methods).\nAlgorithmic performance was evaluated using the metrics Hamming distance (HD), precision and recall, as a function of dataset size, + and −. HD measures the structural similarity among the reconstructed progression and the generative model in terms of the minimum-cost sequence of node edit operations (inclusion and exclusion) that transforms the reconstructed topology into the generative one8. Precision and recall are defined as follows: precision = TP/(TP + FP) and recall = TP/(TP + FN), where TP are the true positives (number of correctly inferred true patterns), FP are the false positives (number of spurious patterns inferred) and FN are the false negatives (number of true patterns that are not inferred). The closer both precision and recall are to 1, the better.\nIn Figure §4.2 we show the performance of CAPRI and of the competing techniques, in terms of Hamming distance, on datasets generated from models with 10 events and all the four different topologies. In particular, we show the performance: (i) in the case of noise-free datasets, i.e., ν = 0 and different values of the sample set size m and (ii) in the case of a fixed sample set size, m = 100 (size that is likely to be found in currently available cancer databases, such as TCGA (see [136])) and different values of the noise rate ν. As is evident from Figure §4.2 CAPRI outperforms all the competing techniques with respect to all the topologies and all the possible combinations of noise rate and\n8This measure corresponds to the sum of false positives and false negative and, for a set of n events, is bounded above by n(n−1) when the reconstructed topology contains all the false negatives and positives.\n45"
    }, {
      "heading" : "46 CHAPTER 4. MORE COMPLEX MODELS OF CANCER PROGRESSION",
      "text" : "sample set size, in terms of average Hamming distance (with the only exception of CAPRESE in the case of tree and forests, which displays a behavior closer to CAPRI’s). The analyses on precision and recall display consistent results (see §D). In other words, we demonstrate on the basis of extensive synthetic tests that CAPRI requires a much lower number of samples than the other techniques in order to converge to the real generative model and also that it is much more robust even in the presence of significant amount of noise in the data, irrespective of the underlying topology.\nSee §D for a more complete description of the performance evaluation for all the analyzed combinations of parameters. There, we have shown that CAPRI is highly effective when the co-occurrence constraint on confluences is relaxed to disjunctive patterns, even if no input hypotheses are provided, i.e., Φ = ∅. This result hints at CAPRI’s robustness to infer patterns with imperfect regularities. Finally, we also show that CAPRI is effective in inferring synthetic lethality relations in this case using the operator ⊕ as introduced in §2; when a combination of mutations in two or more genes leads to cell death, while separately, the mutations are viable. In this case, candidate relations are directly input as Φ."
    }, {
      "heading" : "4.3.2 Atypical Chronic Myeloid Leukemia (aCML)",
      "text" : "As a case study, we applied CAPRI to the mutational profiles of 64 aCML patients described in [154]. Through exome sequencing, the authors identify a recurring missense point mutation in the SET-binding protein 1 (setbp1) gene as a novel aCML marker.\n46"
    }, {
      "heading" : "4.3. RESULTS AND DISCUSSION 47",
      "text" : "Among all the genes present in the dataset by Piazza et al., we selected those either (i) mutated - considered any mutation type - in at least 5% of the input samples (3 patients), or (ii) hypothesised to be part of a functional aCML progression pattern in the literature 9. The input dataset with selected events is shown in Figure §4.3; notice that somatic mutations are categorised as indel, missense point and nonsense point as in [154]. In Figure §4.3 we show the model reconstructed by CAPRI (supervised mode, execution time ≈ 5 seconds) on this dataset, with confidence assessed via 1000 nonparametric bootstrap iterations. The model highlights several non trivial selectivity relations involving genomic events relevant to aCML development.\nFirst, CAPRI predicts a progression involving mutations in setbp1, asxl1 and cbl, consistently with the recent study by [126], in which these genes were shown to be highly correlated and possibly functioning in a synergistic manner for aCML progression. Specifically, CAPRI predicts a selective advantage relation between missense point mutations in setbp1 and nonsense point mutations in asxl1. This is in line with recent evidence from [89] suggesting that setbp1 mutations are enriched among asxl1-mutated myelodysplastic syndrome (MDS) patients, and in-vivo experiments point to a driver role of setbp1 for that leukemic progression. Interestingly, my model seems also to suggest a different role of asxl1 missense and nonsense mutation types in the progression, yet more extensive studies (e.g., prospective or systems biology explanation) are needed to corroborate this hypothesis.\nAmong the hypotheses given as input to CAPRI, the algorithm seems to suggest that the exclusivity pattern among asxl1 and sf3b1 mutations selects for cbl missense point mutations. The role of the asxl1/sf3b1 exclusivity pattern is consistent with the study of [115] which shows that, on a cohort of 479 MDS patients, mutations in sf3b1are inversely related to asxl1 mutations.\nAlso, in [2] it was recently shown that asxl1 mutations, in patients with MDS, myeloproliferative neoplasms (MPN) and acute myeloid leukemia, most commonly occur as nonsense and insertion/deletion in a clustered region adjacent to the highly conserved PHD domain (see [61]) and that mutations of any type eventually result in a loss of asxl1 expression. This observation is consistent with the exclusivity pattern among asxl1 mutations in the reconstructed model, possibly suggesting alternative trajectories of somatic evolution for aCML (involving either asxl1 nonsense or indel mutations).\nFinally, CAPRI predicts selective advantage relations among tet2 and ezh2 missense point and indel mutations. Even though the limited sample size does not allow to draw definitive conclusions on the ordering of such alterations, we can hypothesize that they may play a synergistic role in aCML progression. Indeed, [133] suggests that the concurrent loss of ezh2 and tet2 might cooperate in the pathogenesis of myelodysplastic disorders, by accelerating the overall tumor development, with respect to both MDSs and overlap disorders (MDS/MPN).\n9Two hard exclusivity patterns - i.e., mutual exclusivity with “xor” - were tested, involving the mutations of: (i) genes asxl1 and sf3b1 (see [115]), which is present in the inferred progression model in Figure §4.3, and (ii) genes tet2 and idh2 (see [53]). The syntax in which the patterns are expressed is in §D.\n47\nCHAPTER 5\nAN R PACKAGE FOR TRANSLATIONAL ONCOLOGY\nTRONCO is an R package for TRanslational ONCOlogy which provides a series of software utilities to support the user in each step of the pipeline described in this thesis (see Chapters §2, §3 and §4), i.e., from data import, through data visualization and, finally, to the inference of cancer progression models. Specifically, in the currenct version, TRONCO implements CAPRESE and CAPRI algorithms for cancer progression inference, which we extensively described in Chapter §3 and §4. As a reference, see [33, 5].\nThe core of the two algorithms is a simple quadratic loop1 that prunes arcs from an initially totally connected graph. Each pruning decision is based on the application of Suppes’ probabilistic causation criteria.\nThe pseudocode of the two implemented algorithms along with the procedure to evaluate the confidence of the arcs by bootstrap is summarized in Algorithms §3, §4, §5 and §6, which depict the data preparation step, the CAPRESE and CAPRI algorithms and finally the optional bootstrap step."
    }, {
      "heading" : "5.1 Package implementation",
      "text" : "In this Section we will review the structure and implementation of the TRONCO package. For the sake of clarity, we will structure the description through the following functionalities that are implemented in the package.\n• Data import. Functions for the importation of data both from flat files (e.g., MAF, GISTIC) and from Web querying (e.g., cbioportal).\n• Data exploration and correctness. Functions for the exploration and visualization of the imported data.\n1For CAPRI the size of the input actually depends on the structural complexity of the input “patterns”, i.e., of the boolean formulæ employed in the “lifting operation’\n48"
    }, {
      "heading" : "5.1. PACKAGE IMPLEMENTATION 49",
      "text" : "Algorithm 3: TRONCO Data Import and Preprocessing\nInput: a data set containing MAF or GISTIC scores, e.g., as obtained from cBio portal ([24, 15]). Result: a data structure containing boolean flags for “events”, relative frequencies and other metadata.\n1 From the dataset (depending on the data format) derive a Boolean matrix M , where each entry 〈i, j〉 is true if event i is “present” in sample/patient j.\n2 forall the events e do 3 Compute the frequency of the event e in the dataset and save it in a map F . 4 Compute the joint probability of co-occurrence of pair of events in the dataset\nand save it in a map C.\n5 end\n6 return A data structure comprising the Boolean matrix M , the maps F and C and other metadata.\n• Data editing. Functions for the preprocessing of the data in order to tidy them.\n• External utilities. Functions for the interaction with external tools for the analysis of cancer subtypes or groups of mutually exclusive genes.\n• Inference algorithms. In the current version of TRONCO, the CAPRESE and CAPRI algorithms are provided in a polinomial implementation.\n• Confidence estimation. Functions for the statistical estimation of the confidence of the reconstructed models.\n• Visualization. Functions for the visualization of both the input data and the results of the inference and of the confidence estimation."
    }, {
      "heading" : "Data import",
      "text" : "The starting point of TRONCO analysis pipeline, is a dataset of genomics alterations (i.e., somatic mutations and copy number variations) which need to be imported as a TRONCO compliant data structure, i.e., a list R structure containing the required data both for the inference and the visualization. The data import functions take as input such genomic data and from them create a TRONCO compliant data structure.\nThe core of the data import functionalities from flat files is the function called import.genotypes(geno, event.type = ‘‘variant’’, color = ‘‘Darkgreen’’).\nThis function imports a matrix of 0/1 alterations as a TRONCO compliant dataset. The input “geno” can be either a dataframe or a file name. In any case the dataframe or the table stored in the file must have a column for each altered gene and a rows for each sample. Colnames will be used to determine gene names, if data are loaded from file the first column will be assigned as rownames.\n49"
    }, {
      "heading" : "50 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "Algorithm 4: Pseudocode of the CAPRESE algorithm\nInput: a dataset of n events, i.e., genomic alterations, and m samples packed in a datastructure obtained from Algorithm 3. Result: a tree model representing all the relations of selective advantage.\nPruning based on Suppes’ criteria.\n1 Let G← a complete directed graph over the vertices n. 2 forall the arcs (a, b) in G do 3 Compute a score S(·) for the nodes a and b based on Suppes’ criteria. 4 if Suppes’ criteria are not met then 5 Remove the arc (a, b) from G 6 else if S(a) > S(b) and S(a) > 0 then 7 Keep (a, b) as edge. i.e., select a as “candidate parent”. 8 else if S(b) > S(a) and S(b) > 0 then 9 Keep (a, b) as edge. i.e., select b as “candidate parent”.\n10 end\nFit of the Prima Facie directed acyclic graph to the best tree model.\n11 Let T ← the best tree model obtained by Edmonds’ algorithm (see [43]). 12 return The resulting tree model T .\nBesides this function, TRONCO implements data import from other file format such as MAF and GISTIC files as wrappers of the function import.genotypes. Specifically, the function import.MAF(file, sep = ‘‘\\t’’, is.TCGA = TRUE) imports mutation profiles from a Manual Annotation Format (MAF) file. All mutations are aggregated as a unique event type labeled “Mutation” and are assigned a color accordingly to the default of function import.genotypes. If the input is in the TCGA MAF file format, the function also checks for multiple samples per patient and a warning is raised if any are found. Furthermore, the function import.GISTIC(x) transforms GISTIC scores for CNAs in a TRONCO compliant object. The input can be a matrix, with columns for each altered gene and rows for each sample; in this case colnames or rownames mut be provided. If the input is a character an attempt to load a table from file is performed. In this case the input table format should be constitent with TCGA data for focal CNA; there should hence be: one column for each sample, one row for each gene, a column Hugo Symbol with every gene name and a column Entrez Gene Id with every genes Entrez ID. A valid GISTIC score should be any value of: “Homozygous Loss” (-2), “Heterozygous Loss” (-1), “Low-level Gain” (+1), “High-level Gain” (+2).\nFinally, TRONCO also provides utilities for the query of genomic data from cbioportal. This is implemented in the function cbio.query(cbio.study = NA, cbio.dataset = NA, cbio.profile = NA, genes) which is a wrapper for the CGDS package. This can work either automatically, if one sets cbio.study, cbio.dataset and cbio.profile, or interactively. A list of genes to query with less than 900 entries should be provided.\n50"
    }, {
      "heading" : "5.1. PACKAGE IMPLEMENTATION 51",
      "text" : "Algorithm 5: Pseudocode of the CAPRI algorithm\nInput: a dataset of n variables, i.e., genomic alterations or patterns, and m samples. Result: a graphical model representing all the relations of “selective advantage”.\nPruning based on the Suppes’ criteria\n1 Let G← a directed graph over the vertices n 2 forall the arcs (a, b) ∈ G do 3 Compute a score S(·) for the nodes a and b in terms of Suppes’ criteria. 4 Remove the arc (a, b) if Suppes’ criteria are not met.\n5 end\nLikelihood fit on the Prima Facie directed acyclic graph\n6 Let M← the subset of the remaining arcs ∈ G, that maximize the log-likelihood of the model, computed as: LL(D | M)− ((logm)/2) dim(M), where D denotes the input data, m denotes the number of samples, and dim(M) denotes the number of parameters in M (see [101]). 7 return The resulting graphical model M.\nThis function returns a list with two dataframes: the required genetic profile along with clinical data for the cbio.study. The output is also saved to disk as Rdata file. See also the cbioportal page at http://www.cbioportal.org.\nThe function show(x, view = 10) prints to console a short report of a dataset “x”, which should be a TRONCO compliant dataset.\nAll the functions described in the following sections will assume as input a TRONCO compliant data structure."
    }, {
      "heading" : "Data exploration and correctness",
      "text" : "TRONCO provides a series of function to explore the imported data and the inferred models. All these functions are named with the “as.” prefix.\nConcerning the imported data, the function as.genotypes(x) returns the 0/1 genotypes matrix. This function can be used in combination with the function keysToNames(x, matrix) to translate colnames to event names given the input matrix with colnames or rownames which represent genotypes keys. Also, functions to get the list of genes, events (i.e., each columns in the genotypes matrix, it differs from genes as the same genes of different types are considered different events), alterations (i.e., genes of different types are merged as 1 unique event), samples (i.e., patients or also single cells) and alteration types. See the functions:\nas.genes(x, types = NA) as.events(x, genes = NA, types = NA) as.alterations(x, new.type = \"Alteration\", new.color = \"khaki\")\n51"
    }, {
      "heading" : "52 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "Algorithm 6: Bootstrap Procedure\nInput: a model T obtained from CAPRESE or a model M obtained from CAPRI, and the initial dataset. Result: the confidence in the inferred arcs.\n1 Let counter ← 0 2 Let nboot← the number of bootstrap sampling to be performed. 3 while counter < nboot do 4 Create a new dataset for the inference by random sampling of the input data. 5 Perform the reconstruction on the sampled dataset and save the results. 6 counter = counter + 1\n7 end\n8 Evaluate the confidence in the reconstruction by counting the number of times any arc is inferred in the sampled datasets.\n9 return The inferred model T or M augmented with an estimated confidence for each arc.\nas.samples(x) as.types(x, genes = NA))\nFunctions of this kind are also implemented to explore the results such as the models that have been inferred (see as.models(x, models = names(x$model))), the reconstructions (see the functions as.adj.matrix(x, events = as.events(x), models = names(x$model),type = \"fit\")), the considered patters (see as.patterns(x)) and the confidence (see as.confidence(x, conf)).\nSimilarly, a set of function to extract the cardinality of the compliant TRONCO data structure are defined (see nevents(x, genes = NA, types = NA), ngenes(x, types = NA), npatterns(x), nsamples(x) and ntypes(x)).\nFurthermore, functions to asses the correctness of the inputes are also provided. With function is.compliant(x, err.fun = \"[ERR]\", stage = !(all(is.null(x$stages)) || all(is.na(x$stages)))) it is possible to verify the TRONCO data structure to be compliant with the standards. The function consolidate.data(x, print = FALSE) verifies if the input data are consolidated, i.e., if there are events with 0 or 1 probability or indistinguishable in terms of observations. Any indistinguishable event is returned by the function duplicates(x).\nFinally, TCGA specific functions are provided. TCGA.multiple.samples(x) checks if there are multiple sample in the input, while TCGA.remove.multiple.samples(x) remove them accordingly to TCGA barcodes naming."
    }, {
      "heading" : "Data editing",
      "text" : "TRONCO provides a wide range of editing functions. We will describe some of them in the following, for a technical description we refer to the manual.\n52"
    }, {
      "heading" : "5.1. PACKAGE IMPLEMENTATION 53",
      "text" : "Removing and merging. A set of functions to remove items from the data is provided; such functions are named with the “delete.” prefix. Specifically, using these functions it is possible to remove genes (delete.gene(x, gene), events (delete.event(x, gene, type)), samples (delete.samples(x, samples)), types (delete.type(x, type)) as well as patterns (delete.pattern(x, pattern)) and inferred progression models (delete.model(x)). At the same time, functions to merge events (merge.events(x, ..., new.event, new.type, event.color)) and alterations types (merge.types(x, ..., new.type = \"new.type\", new.color = \"khaki\")) are also provided.\nBinding. The purpose of the binding functions is to combine different datasets. The function ebind(...) combines events from one or more datasets, whose events need be defined over the same set of samples while the function sbind(...) combines samples from one or more datasets, whose samples need to be defined over the same set of events. Samples and events of two dataset can also be intersected through the function intersect.datasets(x, y, intersect.genomes = TRUE).\nChanging and renaming. The two functions rename.gene(x, old.name, new.name) and rename.type(x, old.name, new.name) can be used respectively to rename genes or alterations types. The function change.color(x, type, new.color) can be used to change the color associated to the specified alteration type.\nSelecting and splitting. Genomics data usually involves a vast number of genes, the most of which is not relevant for cancer development (i.e., such as passenger mutations). For this reason, TRONCO implements the function events.selection(x, filter.freq = NA, filter.in.names = NA,filter.out.names = NA) which allows the selection of a set of genes to be analyzed. The selection can be performed by frequency and gene symbols. The 0 probability events can are removed by the function trim(x). Moreover, the functions samples.selection(x, samples) and ssplit(x, clusters, idx = NA) respectively filters a dataset based on selected samples id and splits the dataset into groups (i.e., groups). This latter function can be used to analyze specific subtypes within a tumor."
    }, {
      "heading" : "External utilities",
      "text" : "TRONCO permits the interaction with external tools to (i) reduce inter-tumor heterogeneity by cohort subtyping and (ii) detect fitness equivalent exclusive alterations. The first issue can be attacked by adopting clustering techniques to split the dataset in order to analyze each cluster subtype separately. Currently, TRONCO can export and inport data from [82] via the function export.nbs.input(x, map hugo entrez, file = \"tronco to nbs.mat\") and the previously described splitting functions.\nTo handle alterations with equivalent fitness, TRONCO implements the interaction with [7] through the functions export.mutex(x, filename = \"tronco to mutex\", filepath = \"./\", label.mutation = \"SNV\", label.amplification = list(\"High-level Gain\"),\n53"
    }, {
      "heading" : "54 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "label.deletion = list(\"Homozygous Loss\")) and import.mutex.groups(file, fdr = 0.2, display = TRUE). Such exclusivity groups can then be further added as patters (see the next Section)."
    }, {
      "heading" : "Inference algorithms",
      "text" : "In the current version of TRONCO are implemented the algorithms CAPRESE [117] and CAPRI [158].\nCAPRESE The CAPRESE algorithm [117] can be executed by the function\ntronco.caprese(data, lambda = 0.5, do.estimation = FALSE, silent = FALSE)\nwith “data” being a compliant TRONCO data structure. The parameter “lambda” can be used to tune the shrinkage-alike estimator adopted by CAPRESE, with the default being 0.5 as suggested in [117].\nCAPRI The CAPRI algorithm [158] can be executed by the function\ntronco.capri(data, command = \"hc\", regularization = c(\"bic\", \"aic\"),\ndo.boot = TRUE, nboot = 100, pvalue = 0.05, min.boot = 3, min.stat = TRUE, boot.seed = NULL, do.estimation = FALSE, silent = FALSE)\nwith “data” being a TRONCO compliant data structure. The parameters “command” and “regularization” allows respectively to choose the heuristic search to be performed to fit the network and the regularizer to be used in the likelihood fit (see [158]). CAPRI can be also executed without the bootstrap preprocessing step by the parameter “do.boot”; this is discouraged, but can speed up the execution with big input datasets.\nAs discussed in [158], CAPRI constrains the seach space using Suppes’ prima facie conditions which lead to a subset of possible valid selective advantage relations which are then evaluated by the likelihood fit. Although uncommon, it may so happen (especially when pattern are given as input) that such a resulting prima facie graphical structure may still contain cycles. When this happens, the cycles are removed through the heuristic algorithm implemented in remove.cycles(adj.matrix, weights.temporal.priority, weights.matrix, not.ordered, hypotheses = NA, silent). The function takes as input a set of weights in term of confidence for any selective advantage valid edge, ranks all the valid edges in increasing confidence levels and, starting from the less confident, goes through each edge and remove the ones that can break the cycles.\nPatterns CAPRI allows for the input of patterns, i.e., group of events which express possible selective advantage relations. Such patters are given as input with the function hypothesis.add(data, pattern.label, lifted.pattern, pattern.effect = \"*\", pattern.cause = \"*\"). This function is wrapped by hypothesis.add.homologous(x,\n54"
    }, {
      "heading" : "5.1. PACKAGE IMPLEMENTATION 55",
      "text" : "pattern.cause = \"*\", pattern.effect = \"*\", genes = as.genes(x), FUN = OR) and hypothesis.add.group(x, FUN, group, pattern.cause = \"*\", pattern.effect = \"*\", dim.min = 2, dim.max = length(group), min.prob = 0) which, respectively, allow addition of analogous patterns (i.e., patterns involving the same gene of different types) and patterns involving a specified group of genes. In the current version of TRONCO, the implemented possible patters are the boolean operators AND, OR and XOR (functions AND(...), OR(...) and XOR(...))."
    }, {
      "heading" : "Confidence estimation",
      "text" : "To asses the confidence of any selectivity relations TRONCO implements non-parametric and statistical bootstrap. For the non-parametric bootstrap, each event row is uniformly sampled with repetitions from the input genotype and then, on such an input, the inference algorithms are performed. The assesment concludes after K repetitions (e.g., K = 100). Similarly, for CAPRI, a statistical bootstrap is provided: in this case the input dataset is kept fixed, but different seeds for the statistical procedures are sampled (see, e.g., [46] for an overview of these methods). The bootstrap is implemented in the function tronco.bootstrap(reconstruction, type = \"non-parametric\", nboot = 100, verbose = FALSE) where “reconstruction” is a compliant object obtained by the inference by one of the implemented algorithms."
    }, {
      "heading" : "Visualization",
      "text" : "During the development of the TRONCO package, a lot of attention was paid to the visualization features which are crucial for the understanding of biological results. Shown below is an overview of the main features; for a detailed description of each function, please refer to the manual.\nOncoPrint. OncoPrints are compact means of visualizing distinct genomic alterations, including somatic mutations, copy number alterations, and mRNA expression changes across a set of cases. They are extremely useful for visualizing gene set and pathway alterations across a set of cases, and for visually identifying trends, such as trends in mutual exclusivity or co-occurence between gene pairs within a gene set. Individual genes are represented as rows, and individual cases or patients are represented as columns. See http://www.cbioportal.org/. The function oncoprint provides such visualizations with a TRONCO compliant data structure as input. The function oncoprint.cbio exports the input for the cbioportl visualization, see http://www.cbioportal.org/ public-portal/oncoprinter.jsp.\nIt is also possible to annotate a description (annotate.description(x, label)) and tumor stages (annotate.stages(x, stages, match.TCGA.patients = FALSE)) to any oncoprint.\nReconstruction. The inferred models can be displayed by the function tronco.plot. The features included in the plots are multiple, such as the choice of the regularizer(s)\n55"
    }, {
      "heading" : "56 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "to show, editing font of nodes and edges, scaling nodes’ size in terms of estimated marginal probabilities, annotating the pathway of each gene and displaying the estimated confidence of each edge. We refer to the manual for a detailed description.\nReports. Finally, report utilities are provided. The function genes.table.report(x, name, dir = getwd(), maxrow = 33, font = 10, height = 11, width = 8.5, fill = \"lightblue\") can be used to generate LaTeX code to be used as report, while genes.table.plot(x, name, dir = getwd()) generates reports histograms."
    }, {
      "heading" : "5.2 Use case of TRONCO",
      "text" : "In this Section, we will present a case study for the usage of the TRONCO package based on the work presented in [158]. A detailed discussion of the pipeline along with a complete analysis based on it is provided in [33, 5]."
    }, {
      "heading" : "Events selection",
      "text" : "We will start by loading the TRONCO package in R along with an example ”dataset” that comes within the package.\n> library(TRONCO) > data(aCML) > hide.progress.bar <<- TRUE\nWe then use the function show to get a short summary of the aCML dataset that has just been loaded.\n> show(aCML)\nDescription: CAPRI - Bionformatics aCML data. Dataset: n=64, m=31, |G|=23. Events (types): Ins/Del, Missense point, Nonsense Ins/Del, Nonsense point. Colors (plot): darkgoldenrod1, forestgreen, cornflowerblue, coral. Events (10 shown):\ngene 4 : Ins/Del TET2 gene 5 : Ins/Del EZH2 gene 6 : Ins/Del CBL gene 7 : Ins/Del ASXL1 gene 29 : Missense point SETBP1 gene 30 : Missense point NRAS gene 31 : Missense point KRAS gene 32 : Missense point TET2 gene 33 : Missense point EZH2 gene 34 : Missense point CBL\n56"
    }, {
      "heading" : "5.2. USE CASE OF TRONCO 57",
      "text" : ""
    }, {
      "heading" : "Genotypes (10 shown):",
      "text" : "gene 4 gene 5 gene 6 gene 7 gene 29 gene 30 gene 31 gene 32 gene 33 gene 34\npatient 1 0 0 0 0 1 0 0 0 0 0 patient 2 0 0 0 0 1 0 0 0 0 1 patient 3 0 0 0 0 1 1 0 0 0 0 patient 4 0 0 0 0 1 0 0 0 0 1 patient 5 0 0 0 0 1 0 0 0 0 0 patient 6 0 0 0 0 1 0 0 0 0 0\nUsing the function as.events, we can have a look at the events in the dataset.\n> as.events(aCML)\ntype event\ngene 4 \"Ins/Del\" \"TET2\" gene 5 \"Ins/Del\" \"EZH2\" gene 6 \"Ins/Del\" \"CBL\" gene 7 \"Ins/Del\" \"ASXL1\" gene 29 \"Missense point\" \"SETBP1\" gene 30 \"Missense point\" \"NRAS\" gene 31 \"Missense point\" \"KRAS\" gene 32 \"Missense point\" \"TET2\" gene 33 \"Missense point\" \"EZH2\" gene 34 \"Missense point\" \"CBL\" gene 36 \"Missense point\" \"IDH2\" gene 39 \"Missense point\" \"SUZ12\" gene 40 \"Missense point\" \"SF3B1\" gene 44 \"Missense point\" \"JARID2\" gene 47 \"Missense point\" \"EED\" gene 48 \"Missense point\" \"DNMT3A\" gene 49 \"Missense point\" \"CEBPA\" gene 50 \"Missense point\" \"EPHB3\" gene 51 \"Missense point\" \"ETNK1\" gene 52 \"Missense point\" \"GATA2\" gene 53 \"Missense point\" \"IRAK4\" gene 54 \"Missense point\" \"MTA2\" gene 55 \"Missense point\" \"CSF3R\" gene 56 \"Missense point\" \"KIT\" gene 66 \"Nonsense Ins/Del\" \"WT1\" gene 69 \"Nonsense Ins/Del\" \"RUNX1\" gene 77 \"Nonsense Ins/Del\" \"CEBPA\" gene 88 \"Nonsense point\" \"TET2\" gene 89 \"Nonsense point\" \"EZH2\" gene 91 \"Nonsense point\" \"ASXL1\" gene 111 \"Nonsense point\" \"CSF3R\"\n57"
    }, {
      "heading" : "58 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "These events account for alterations in the following genes.\n> as.genes(aCML)\n[1] \"TET2\" \"EZH2\" \"CBL\" \"ASXL1\" \"SETBP1\" \"NRAS\" \"KRAS\" \"IDH2\"\n\"SUZ12\" \"SF3B1\" \"JARID2\" \"EED\" \"DNMT3A\" \"CEBPA\" \"EPHB3\" \"ETNK1\" \"GATA2\" \"IRAK4\" \"MTA2\" \"CSF3R\"\n[21] \"KIT\" \"WT1\" \"RUNX1\"\nNow we take a look at the alterations of only the gene SETBP1 across the samples.\n> as.gene(aCML, genes=’SETBP1’)\nMissense point SETBP1\npatient 1 1 patient 2 1 patient 3 1 patient 4 1 patient 5 1 patient 6 1 patient 7 1 patient 8 1 patient 9 1 patient 10 1 patient 11 1 patient 12 1 patient 13 1 patient 14 1 patient 15 0 patient 16 0 patient 17 0 patient 18 0 patient 19 0 patient 20 0 patient 21 0 patient 22 0 patient 23 0 patient 24 0 patient 25 0 patient 26 0 patient 27 0 patient 28 0 patient 29 0 patient 30 0\n58"
    }, {
      "heading" : "5.2. USE CASE OF TRONCO 59",
      "text" : "patient 31 0 patient 32 0 patient 33 0 patient 34 0 patient 35 0 patient 36 0 patient 37 0 patient 38 0 patient 39 0 patient 40 0 patient 41 0 patient 42 0 patient 43 0 patient 44 0 patient 45 0 patient 46 0 patient 47 0 patient 48 0 patient 49 0 patient 50 0 patient 51 0 patient 52 0 patient 53 0 patient 54 0 patient 55 0 patient 56 0 patient 57 0 patient 58 0 patient 59 0 patient 60 0 patient 61 0 patient 62 0 patient 63 0 patient 64 0\nWe consider a subset of all the genes in the dataset to be involved in patters based on the support we found in the literature. See [158] as a reference.\n> gene.hypotheses = c(’KRAS’, ’NRAS’, ’IDH1’, ’IDH2’, ’TET2’, ’SF3B1’, ’ASXL1’)\nRegardless from which types of mutations we include, we select only the genes which appear alterated in at least 5% of the patients. Thus, we first transform the dataset into ”Alteration” (i.e., by collapsing all the event types for the same gene), and then we consider only these events from the original dataset.\n59"
    }, {
      "heading" : "60 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "> alterations = events.selection(as.alterations(aCML), filter.freq = .05)\n*** Aggregating events of type(s) Ins/Del, Missense point, Nonsense Ins/Del, Nonsense point in a unique event with label \"Alteration\". Dropping event types Ins/Del, Missense point, Nonsense Ins/Del, Nonsense point for"
    }, {
      "heading" : "23 genes. *** Binding events for 2 datasets. *** Events selection: #events=23, #types=1",
      "text" : "Filters freq|in|out = {TRUE, FALSE, FALSE} Minimum event frequency: 0.05 (3 alterations out of 64 samples). Selected 7 events.\nSelected 7 events, returning.\nWe now show a plot of the selected genes. Note that this plot has no title as by default the function events.selection does not add any. The resulting figure is shonw in 5.1.\n> dummy = oncoprint(alterations,font.row=12,cellheight=20,cellwidth=4)\n*** Oncoprint for \"\" with attributes: stage=FALSE, hits=TRUE Sorting samples ordering to enhance exclusivity patterns.\n60"
    }, {
      "heading" : "5.2. USE CASE OF TRONCO 61",
      "text" : ""
    }, {
      "heading" : "Adding Hypotheses",
      "text" : "We now create the dataset to be used for the inference of the progression model. We consider the original dataset and from it we select all the genes whose mutations are occurring at least 5% of the times together with any gene involved in any hypothesis. To do so, we use the parameter filter.in.names as shown below.\n> hypo = events.selection(aCML, filter.in.names=c(as.genes(alterations),\ngene.hypotheses))\n*** Events selection: #events=31, #types=4 Filters freq|in|out = {FALSE, TRUE, FALSE} [filter.in] Genes hold: TET2, EZH2, CBL, ASXL1, SETBP1 ... [10/14 found]. Selected 17 events, returning. > hypo = annotate.description(hypo, ’CAPRI - Bionformatics aCML data (selected events)’)\nWe show a new oncoprint of this latest dataset where we annotate the genes in gene.hypotheses in order to identify them 5.2. The sample names are also shown.\n> dummy = oncoprint(hypo, gene.annot = list(priors= gene.hypotheses),\nsample.id = T, font.row=12, font.column=5, cellheight=20, cellwidth=4)\n*** Oncoprint for \"CAPRI - Bionformatics aCML data (selected events)\" with attributes: stage=FALSE, hits=TRUE Sorting samples ordering to enhance exclusivity patterns. Annotating genes with RColorBrewer color palette Set1 .\nWe now also add the hypotheses that are described in CAPRI’s manuscript. Hypothesis of hard exclusivity (XOR) for NRAS/KRAS events (Mutation). This hypothesis is tested against all the events in the dataset.\n> hypo = hypothesis.add(hypo, ’NRAS xor KRAS’, XOR(’NRAS’, ’KRAS’))\nWe then try to include also a soft exclusivity (OR) pattern but, since its ”signature” is the same of the hard one just included, it will not be included. The code below is expected to result in an error.\n> hypo = hypothesis.add(hypo, ’NRAS or KRAS’, OR(’NRAS’, ’KRAS’))"
    }, {
      "heading" : "Error in hypothesis.add(hypo, \"NRAS or KRAS\", OR(\"NRAS\", \"KRAS\")) :",
      "text" : "[ERR] Pattern duplicates Pattern NRAS xor KRAS.\nTo better highlight the perfect (hard) exclusivity among NRAS/KRAS mutations, one can examine further their alterations. See Figure 5.3.\n61"
    }, {
      "heading" : "62 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "> dummy = oncoprint(events.selection(hypo, filter.in.names = c(’KRAS’, ’NRAS’)),\nfont.row=12, cellheight=20, cellwidth=4)\n*** Events selection: #events=18, #types=4 Filters freq|in|out = {FALSE, TRUE, FALSE} [filter.in] Genes hold: KRAS, NRAS ... [2/2 found]. Selected 2 events, returning. *** Oncoprint for \"\" with attributes: stage=FALSE, hits=TRUE Sorting samples ordering to enhance exclusivity patterns.\nWe repeated the same analysis as before for other hypotheses and for the same reasons, we will include only the hard exclusivity pattern.\n> hypo = hypothesis.add(hypo, ’SF3B1 xor ASXL1’, XOR(’SF3B1’, OR(’ASXL1’)), ’*’) > hypo = hypothesis.add(hypo, ’SF3B1 or ASXL1’, OR(’SF3B1’, OR(’ASXL1’)), ’*’)\n62"
    }, {
      "heading" : "5.2. USE CASE OF TRONCO 63",
      "text" : ""
    }, {
      "heading" : "Error in hypothesis.add(hypo, \"SF3B1 or ASXL1\", OR(\"SF3B1\", OR(\"ASXL1\")), :",
      "text" : "[ERR] Pattern duplicates Pattern SF3B1 xor ASXL1.\nFinally, we now repeat the same for genes TET2 and IDH2. In this case 3 events for the gene TET2 are present, that is ”Ins/Del”, ”Missense point” and ”Nonsense point”. For this reason, since we are not specifying any subset of such events to be considered, all TET2 alterations are used. Since the events present a perfect hard exclusivity, their patters will be included as an XOR. See Figure 5.4.\n> as.events(hypo, genes = ’TET2’)\ntype event\ngene 4 \"Ins/Del\" \"TET2\" gene 32 \"Missense point\" \"TET2\" gene 88 \"Nonsense point\" \"TET2\" > hypo = hypothesis.add(hypo, ’TET2 xor IDH2’, XOR(’TET2’, ’IDH2’), ’*’) > hypo = hypothesis.add(hypo, ’TET2 or IDH2’, OR(’TET2’, ’IDH2’), ’*’) > dummy = oncoprint(events.selection(hypo, filter.in.names = c(’TET2’, ’IDH2’)), font.row=12, cellheight=20,cellwidth=4) *** Events selection: #events=21, #types=4 Filters freq|in|out = {FALSE, TRUE, FALSE} [filter.in] Genes hold: TET2, IDH2 ... [2/2 found]. Selected 4 events, returning. *** Oncoprint for \"\" with attributes: stage=FALSE, hits=TRUE Sorting samples ordering to enhance exclusivity patterns.\nWe now finally add any possible group of homologous events. For any gene having more than one event associated we also add a soft exclusivity pattern among them.\n> hypo = hypothesis.add.homologous(hypo)\n63"
    }, {
      "heading" : "64 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "*** Adding hypotheses for Homologous Patterns\nGenes: TET2, EZH2, CBL, ASXL1, CSF3R Function: OR Cause: * Effect: *\nHypothesis created for all possible gene patterns.\nThe final dataset that will be given as input to CAPRI is now finally shown. See Figure 5.5.\n> dummy = oncoprint(hypo, gene.annot = list(priors= gene.hypotheses),\nsample.id = T, font.row=10, font.column=5, cellheight=15, cellwidth=4)\n*** Oncoprint for \"CAPRI - Bionformatics aCML data (selected events)\" with attributes: stage=FALSE, hits=TRUE Sorting samples ordering to enhance exclusivity patterns. Annotating genes with RColorBrewer color palette Set1 ."
    }, {
      "heading" : "Model reconstruction",
      "text" : "We next infer the model by running CAPRI algorithm with its default parameters: we use both AIC and BIC as regularizators, Hill-climbing as heuristic search of the solutions and exhaustive bootstrap (nboot replicates or more for Wilcoxon testing, i.e., more iterations can be performed if samples are rejected), p-value set at 0.05. We set the seed for the sake of reproducibility.\n> model = tronco.capri(hypo, boot.seed = 12345, nboot=10)\n*** Checking input events. *** Inferring a progression model with the following settings.\n64"
    }, {
      "heading" : "5.2. USE CASE OF TRONCO 65",
      "text" : "Dataset size: n = 64, m = 26. Algorithm: CAPRI with \"bic, aic\" regularization and \"hc\" likelihood-fit strategy. Random seed: 12345. Bootstrap iterations (Wilcoxon): 10. exhaustive bootstrap: TRUE. p-value: 0.05. minimum bootstrapped scores: 3. *** Bootstraping selective advantage scores (prima facie). Evaluating \"temporal priority\" (Wilcoxon, p-value 0.05) Evaluating \"probability raising\" (Wilcoxon, p-value 0.05) *** Loop detection found loops to break. Removed 26 edges out of 68 (38%) *** Performing likelihood-fit with regularization bic. *** Performing likelihood-fit with regularization aic.\n65"
    }, {
      "heading" : "66 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "The reconstruction has been successfully completed in 00h:00m:02s\nWe then plot the model inferred by CAPRI with BIC as a regolarizator and we set some parameters to get a good plot; the confidence of each edge is shown both in terms of temporal priority and probability raising (selective advantage scores) and hypergeometric testing (statistical relevance of the dataset of input). See Figure 5.6.\n> tronco.plot(model, fontsize = 13, scale.nodes = .6, regularization=\"bic\",\nconfidence = c(’tp’, ’pr’, ’hg’), height.logic = 0.25, legend.cex = .5, pathways = list(priors= gene.hypotheses), label.edge.size=5)\n*** Expanding hypotheses syntax as graph nodes: *** Rendering graphics Nodes with no incoming/outgoing edges will not be displayed. Annotating nodes with pathway information. Annotating pathways with RColorBrewer color palette Set1 . Adding confidence information: tp, pr, hg RGraphviz object prepared. Plotting graph and adding legends."
    }, {
      "heading" : "Bootstrapping data",
      "text" : "Finally, we perform non-parametric bootstrap as a further estimation of the confidence in the inferred results. See Figure 5.7.\n> model.boot = tronco.bootstrap(model, nboot=10)\nExecuting now the bootstrap procedure, this may take a long time... Expected completion in approx. 00h:00m:03s *** Using 7 cores via \"parallel\"\n*** Reducing results\nPerformed non-parametric bootstrap with 10 resampling and 0.05 as pvalue for the statistical tests.\n> tronco.plot(model.boot, fontsize = 13, scale.nodes = .6, regularization=\"bic\",\nconfidence=c(’npb’), height.logic = 0.25, legend.cex = .5, pathways = list(priors= gene.hypotheses), label.edge.size=10)\n*** Expanding hypotheses syntax as graph nodes: *** Rendering graphics Nodes with no incoming/outgoing edges will not be displayed.\n66"
    }, {
      "heading" : "5.2. USE CASE OF TRONCO 67",
      "text" : "Annotating nodes with pathway information. Annotating pathways with RColorBrewer color palette Set1 . Adding confidence information: npb RGraphviz object prepared. Plotting graph and adding legends.\nWe now conclude this analysis with an example of inference with the CAPRESE algorithm. As CAPRESE does not consider any patter as input, we use the dataset shown in Figure 5.2. These results are shown in Figure 5.8.\n> model.boot.caprese = tronco.bootstrap(tronco.caprese(hypo))\n*** Checking input events.\n67"
    }, {
      "heading" : "68 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "*** Inferring a progression model with the following settings. Dataset size: n = 64, m = 17. Algorithm: CAPRESE with shrinkage coefficient: 0.5. The reconstruction has been successfully completed in 00h:00m:00s Executing now the bootstrap procedure, this may take a long time... Expected completion in approx. 00h:00m:00s\nPerformed non-parametric bootstrap with 100 resampling and 0.5 as shrinkage parameter.\n> tronco.plot(model.boot.caprese, fontsize = 13, scale.nodes = .6,\nconfidence=c(’npb’), height.logic = 0.25, legend.cex = .5,\n68"
    }, {
      "heading" : "5.2. USE CASE OF TRONCO 69",
      "text" : "pathways = list(priors= gene.hypotheses), label.edge.size=10, legend.pos=\"top\")\n*** Expanding hypotheses syntax as graph nodes: *** Rendering graphics Nodes with no incoming/outgoing edges will not be displayed. Annotating nodes with pathway information. Annotating pathways with RColorBrewer color palette Set1 . Adding confidence information: npb RGraphviz object prepared. Plotting graph and adding legends.\n69"
    }, {
      "heading" : "70 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : ""
    }, {
      "heading" : "5.3 TCGA MSI/MSS colorectal tumors",
      "text" : "We conclude this Chapter with a more elaborate analysis performed with the TRONCO package. As a reference, see Chapter §6 and [21].\nWe report the acronyms that will be adopted in this Section.\nMSS Microsatellite Stable MSI Microsatellite Highly-Instable CNA Copy Number Alteration\nTCGA The Cancer Genome Atlas COADREAD Human Colon and Rectal Cancer (TCGA project)\nMUTEX Mutual Exclusivity (tool) CAPRI Cancer Progression Inference (algorithm) TRONCO Translational Oncology (tool)"
    }, {
      "heading" : "5.3.1 Summary",
      "text" : "The summary and motivation of this study are detailed in the original paper [21]. Here, we just mention that we will (i) use samples from the COADREAD project to implement a case/control study, (ii) select somatic mutations and focal CNAs in 33 driver genes manually annotated to 5 pathways by the Consortium, (iii) scan groups of exclusive alterations with MUTEX ([7]) and, finally, (iv) retrieve progression models by running the CAPRI algorithm ([158]) implemented in TRONCO.\nPrerequisites. Install the TRONCO package either from Bioconductor or its official webpage:\nhttp://bimib.disco.unimib.it\nIn any case, check that the installed version is updated; this document references TRONCO version 2.0, Mantis Shrimp. This study processes TCGA data originally archived at:\nhttps://tcga-data.nci.nih.gov/docs/publications/coadread 2012/\nFor conveniency, we have converted such files to formats easily manipulable with R. We host such files at TRONCO’s webpage, as of the download performed the 12 March 2015. The files are:\n• \"crc clinical sheet.txt\": clinical data file;\n• \"TCGA CRC Suppl Table2 Mutations 20120719.csv\": MAF mutations file, originally Excel file, now converted to csv format with semi-comma separator;\n• \"crc gistic.txt\": GISTIC file of focal CNAs;\n• \"TCGA-clusters.csv\": clustering results, including MSI/MSS status.\n70"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 71",
      "text" : "and the code that we provide you with takes care of downloading them autonomously. For clarity and modularity, this tutorial makes use of different files, reported in sections’ titles; the root file is named main.R, and starts by loading TRONCO, setting up some variables and folder, then downloading data.\nListing 1: Root file main.R\n# You might install TRONCO’s version from BIMIB Github as library(devtools) install_github(\"BIMIB-DISCo/TRONCO\") library(TRONCO)\n# Working directory workdir = \"TCGA-data/\" dir.create(workdir)\n# Data files datafile = ’TCGA-COADREAD-TRONCO.zip’ download.file(\n’https://github.com/BIMIB-DISCo/datasets/raw/master/TCGA-COADREAD-TRONCO.zip’, destfile=datafile, method=’curl’)\nunzip(datafile, exdir = workdir)\n# Name input files clinical.file = paste0(workdir, \"/Clinical/crc_clinical_sheet.txt\")"
    }, {
      "heading" : "MAF.file = paste0(workdir,",
      "text" : "\"/Mutations/TCGA_CRC_Suppl_Table2_Mutations_20120719.csv\")\nGISTIC.file = paste0(workdir, \"/CNV/crc_gistic.txt\") clusters.file = paste0(workdir, \"/Clusters/TCGA-clusters.csv\") MUTEX.msi.file = paste0(workdir, \"/MUTEX/msi_results.txt\") MUTEX.mss.file = paste0(workdir, \"/MUTEX/mss_results.txt\")\n# Then this files sources the other scripts source(’scripts/TCGA-import.R’, echo = TRUE) source(’scripts/training-samples.R’, echo = TRUE) source(’scripts/training-exclusivity.R’, echo = TRUE) source(’scripts/training-reconstruction.R’, echo = TRUE) source(’scripts/validation-samples.R’, echo = TRUE) source(’scripts/validation-pvalues.R’, echo = TRUE)\n5.3.2 Import of the TCGA data in TRONCO - TCGA-import.R\nWe begin by loading clinical attributes.\n71"
    }, {
      "heading" : "72 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "clinical.data = TCGA.map.clinical.data(\nfile = clinical.file, column.samples = ’patient’, column.map = ’tumor_stage’)\nhead(clinical.data)\nNow we process separately mutations and CNAs to prepare two TRONCO objects and save them as Rdata. One will be the set of all CNAs, and shall be used afterwards to prepare the control samples, the other will be the set of training samples with both CNAs and mutations data.\nProcessing mutations. We can import the MAF file which annotates mutations for the training samples, and augment each patient with its associated stage. We use flag is.TCGA to check if any patient has multiple samples associated as we eventually want to select only one sample per patient. These steps generates a TRONCO object, on which we use show function to get a simple report of the dataset.\n# Load MAF - use is.TCGA to match samples to patients"
    }, {
      "heading" : "MAF = import.MAF(",
      "text" : "file = MAF.file, is.TCGA = TRUE, sep = ’;’)\n# Add stage annotation - use match.TCGA.patients to match long/short barcodes MAF = annotate.stages(MAF, clinical.data, match.TCGA.patients = TRUE) show(MAF)\nAs COADREAD has multiple samples, we use TRONCO functions which implement TCGA aliquote disambiguation policies to select one sample per patient\n# Check for duplicated samples - we find them"
    }, {
      "heading" : "TCGA.multiple.samples(MAF)",
      "text" : "# Remove duplicated samples according to TCGA criteria, # shorten barcodes and add stages MAF = TCGA.remove.multiple.samples(MAF) MAF = TCGA.shorten.barcodes(MAF) MAF = annotate.stages(MAF, clinical.data)\nProcessing Copy Numbers. As done for mutations, we import CNAs from focal GISTIC profiles. Preprocessing is here minor, as GISTIC profiles are in a matrix which can be imported directly in TRONCO after transposition. Editing of the GISTIC information that we do not want to use, is done on the TRONCO object as we have functions for its easy manipulation\n72"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 73",
      "text" : "# Load as a plain table"
    }, {
      "heading" : "GISTIC = read.table(",
      "text" : "GISTIC.file, check.names = FALSE, stringsAsFactors = FALSE, header = TRUE)\n# Have a look at this table to remove useless information head(GISTIC[, 1:5])"
    }, {
      "heading" : "GISTIC $Entrez_Gene_Id = NULL rownames(GISTIC) = GISTIC $Hugo_Symbol GISTIC $Hugo_Symbol = NULL",
      "text" : "# Import all GISTIC data GISTIC = import.GISTIC( t(GISTIC) ) show(GISTIC)\n# We want to use only high-confidence scores from the GISTIC, # renamed as Amplification/Deletion GISTIC = delete.type(GISTIC, ’Heterozygous Loss’) # low-level deletions GISTIC = delete.type(GISTIC, ’Low-level Gain’) # low-level amplifications GISTIC = rename.type(GISTIC, ’Homozygous Loss’, ’Deletion’) GISTIC = rename.type(GISTIC, ’High-level Gain’, ’Amplification’) GISTIC = annotate.stages(GISTIC, clinical.data) show(GISTIC)\nSubsetting samples for training. We now want to select only samples with both somatic mutations and CNAs available - i.e., the intersection of the dataset GISTIC and MAF. Notice that the GISTIC contains only genes with at least 1 CNV in the dataset, so some well-known CRC genes such as pik3ca or fam123b are not annotated in there.\nc(\"PIK3CA\", \"FAM123B\") \\%in\\% as.genes(GISTIC) # Shall be FALSE\nThus, we want to intersect patients as we assume that every patient has at least one CNA, and make the union of all the genes annotated in MAF/GISTIC datasets. This shall yield a new dataset called MAF.GISTIC; this and GISTIC will be exported as Rdata.\n# We set intersect.genomes to FALSE to take the union of altered genes MAF.GISTIC = intersect.datasets(GISTIC, MAF, intersect.genomes = FALSE)\n# We remove events which have no observations in the dataset, # and annotate stages"
    }, {
      "heading" : "MAF.GISTIC = trim(MAF.GISTIC)",
      "text" : "73"
    }, {
      "heading" : "74 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "MAF.GISTIC = annotate.stages(MAF.GISTIC, clinical.data) show(MAF.GISTIC)\n# Export these datasets as Rdata save(MAF.GISTIC, file = paste0(workdir, ’MAF.GISTIC.Rdata’)) save(GISTIC, file = paste0(workdir, ’GISTIC.Rdata’))\n5.3.3 Preparing the training datasets - training-samples.R\nWe first create some folders where we will separate TRONCO output, then we load the files that we just created, and set some fancy colors.\n# Prepare folders dir.create(’./MSS’) dir.create(’./MSI’) sub.dir = c(’MUTEX’, ’Rdata-lifted’, ’Rdata-models’) sapply(paste0(’./MSS/’, sub.dir), dir.create) sapply(paste0(’./MSI/’, sub.dir), dir.create)\n# Load MAF.GISTIC file, set some fancy colors to get cute visualization load(paste0(workdir, ’/MAF.GISTIC.Rdata’)) MAF.GISTIC = change.color(MAF.GISTIC, ’Mutation’, ’darkolivegreen3’) MAF.GISTIC = change.color(MAF.GISTIC, ’Amplification’, ’coral’) MAF.GISTIC = change.color(MAF.GISTIC, ’Deletion’, ’cornflowerblue’)\nNow we load clustering assignments from TCGA, we define the clustering maps for patients to MSI/MSS status and use that to create the corresponding TRONCO objects for each dataset.\n# Load table data file = read.delim(clusters.file, sep = \";\") head(file)\n# Select just certain annotations, remove blank lines tab = file[, c(\"patient\", \"MSI_status\", \"sequenced\")] tab = tab[1:276, ] rownames(tab) = tab $patient\n# Filter out non-sequenced samples, and order them (for console visualization) tab = tab[tab $sequenced == 1, ] tab = tab[order(tab $MSI_status), ] print(tab)\n# Define the maps to split samples map.MSS = tab[tab $MSI_status == \"MSS\", , drop = FALSE]\n74"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 75",
      "text" : "map.MSI.H = tab[tab $MSI_status == \"MSI-H\", , drop = FALSE]\n# These are the samples that we actually use MSS.samples = rownames(map.MSS) MSI.H.samples = rownames(map.MSI.H)\n# Split is done by using samples.selection with appropriate vectors as input MSS = trim(samples.selection(MAF.GISTIC, MSS.samples)) MSI.H = trim(samples.selection(MAF.GISTIC, MSI.H.samples)) show(MSS) show(MSI.H)\nTo produce fancy figures, we shall also define some further groups and colors. Among these, there are the 33 relevant driver genes identified by the Consortium which we include now. Also, as we are not going to use any external clustering tool, we can immediately subset MSI/MSS tumors to include only alterations in those driver genes.\n# Plotting colors alteration.color = ’dimgray’ pathways.color = c(’firebrick1’, ’darkblue’, ’darkgreen’, ’darkmagenta’,\n’darkorange’)\n# Driver events - 33 genes mapped to 5 pathways by TCGA Wnt = c(\"APC\", \"CTNNB1\", \"DKK1\", \"DKK2\", \"DKK3\", \"DKK4\", \"LRP5\", \"FZD10\",\n\"FAM123B\", \"AXIN2\", \"TCF7L2\", \"FBXW7\", \"ARID1A\", \"SOX9\")\nRAS = c(\"ERBB2\", \"ERBB3\", \"NRAS\", \"KRAS\", \"BRAF\") PI3K = c(\"IGF2\", \"IRS2\", \"PIK3CA\", \"PIK3R1\", \"PTEN\") TGFb = c(\"TGFBR1\", \"TGFBR2\", \"ACVR1B\", \"ACVR2A\", \"SMAD2\", \"SMAD3\", \"SMAD4\")"
    }, {
      "heading" : "P53 = c(\"TP53\", \"ATM\")",
      "text" : "# Some variable which will be processed by TRONCO plotting functions pathway.genes = c(Wnt, RAS, PI3K, TGFb, P53) pathway.names = c(’Wnt’, ’RAS’, ’PI3K’, ’TGFb’, ’P53’) pathway.list = list(Wnt = Wnt, RAS = RAS, PI3K = PI3K, TGFb = TGFb, P53 = P53)\nFinally, we can create the two datasets which we shall use - these contain as driver events only alterations in the 33 TCGA genes. Also, we can use oncoprint function to start making plots out of the data - see Figure 5.9.\n# MSS tumors MSS = trim(events.selection(MSS, filter.in.names = pathway.genes)) MSS = annotate.description(MSS, ’MSS subtype’)\n# MSI-HIGH tumors MSI.H = trim(events.selection(MSI.H, filter.in.names = pathway.genes))\n75"
    }, {
      "heading" : "76 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : ""
    }, {
      "heading" : "MSI.H = annotate.description(MSI.H, ’MSI-HIGH subtype’)",
      "text" : "# We use TRONCO visualization function - oncoprint - to view these dataset w = oncoprint(MSS,\ntitle = ’MSS tumors - with all driver genes’,\nlegend.cex = .5, # Legend size for events type gene.annot = pathway.list, # List of mapping to pathways/groups gene.annot.color = pathways.color, # Mapping color sample.id = T) # Sample names\nw = oncoprint(MSI.H,\nlegend.cex = .5,\ngene.annot = pathway.list, gene.annot.color = pathways.color, sample.id = T)\n5.3.4 Exclusivity groups in the datasets - training-exclusivity.R\nTRONCO relies on external tools such as MUTEX or others to detect exclusivity groups. As the input/output formats of such tools is non-uniform, however, we do not support direct conversion from TRONCO objects to every tool input/output.\nIn this case, we use the MUTEX tool to detect exclusivity groups in MSI/MSS datasets and, inside TRONCO, there are input/output function to use with it, which we update consistently.\n# Export a file MSS.txt compliant to MUTEX input export.MUTEX(MSS,\nfilename = ’MSS/MUTEX/MSS.txt’, label.mutation = ’Mutation’, label.amplification = ’Amplification’, label.deletion = ’Deletion’\n)\nexport.MUTEX(MSI.H,\nfilename = ’MSI/MUTEX/MSI.H.txt’, label.mutation = ’Mutation’, label.amplification = ’Amplification’, label.deletion = ’Deletion’\n)\nThen we can run the Java MUTEX tool by following the routines explained at the following webpage:\nhttps://github.com/BIMIB-DISCo/mutex\n76"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 77",
      "text" : "77"
    }, {
      "heading" : "78 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "In this case we provide you with the results of running MUTEX with its default parameters; we shall then collect results - groups with score below .2 threshold, as usually done - in R’s runtime by using function import.MUTEX.groups.\nMSI.H.MUTEX = import.MUTEX.groups(MUTEX.msi.file) MSS.MUTEX = import.MUTEX.groups(MUTEX.mss.file)\nThe 3 groups detected for MSI tumors can be visualized by arranging GROBS returned by oncoprint and obtain the plot that we show in Figure 5.10.\n# Visualize groups via console print(MSI.H.MUTEX)\n# Then combine oncoprint via grid.arrange grid.arrange(\noncoprint(\n# Select only events for genes in group 1 events.selection(MSI.H, filter.in.names = MSI.H.MUTEX[[1]]),\ntitle = paste(\"MSI-H - MUTEX group 1\"),\nlegend.cex = .3, font.row = 6, ann.hits = FALSE, # Avoid annotating the hits for these groups cellheight = 10, silent = T, # Do not plot the oncoprint gene.annot = pathway.list, gene.annot.color = pathways.color,\n) $gtable, oncoprint(\nevents.selection(MSI.H, filter.in.names = MSI.H.MUTEX[[2]]),\ntitle = paste(\"MSI-H - MUTEX group 2\"), legend.cex = .3, silent = T, font.row = 6, ann.hits = FALSE, cellheight = 10, gene.annot = pathway.list, gene.annot.color = pathways.color,\n) $gtable, oncoprint(\nevents.selection(MSI.H, filter.in.names = MSI.H.MUTEX[[3]]),\ntitle = paste(\"MSI-H - MUTEX group 3\"), legend.cex = .3, silent = T, font.row = 6, ann.hits = FALSE,\n78"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 79",
      "text" : "cellheight = 10, gene.annot = pathway.list, gene.annot.color = pathways.color,\n) $gtable, ncol=1 # Display all plots in a single column\n)\nOther three groups will be used - in each subtype, in principle - according to both literature on CRC, and analysis carried out by the Consortium with the MEMO tool (Ciriello et al, Curr Protoc Bioinformatics. 2013 Mar;Chapter 8:Unit 8.17). We shall define the following variables.\n# Apriori CRC knowledge"
    }, {
      "heading" : "KNOWLEDGE.PRIOR.WNT = c(’APC’, ’CTNNB1’) KNOWLEDGE.PRIOR.RAF = c(’KRAS’, ’NRAS’, ’BRAF’)",
      "text" : "# MEMO group estimated by TCGA for the non-hypermutated tumors: TCGA.MEMO = c(’ERBB2’, ’IGF2’, ’PIK3CA’, ’PTEN’)\n5.3.5 Reconstruction of the models - training-reconstruction.R\nWe first implement a selection strategy of driver events. For any dataset we select:\n• all genes with an alteration frequency > 5%;\n• all genes involved in an exclusivity prior (MUTEX / MEMO / knowledge-based).\nWe thus define this function\nselect = function(x, min.freq, forced.genes) {\n# Collapse multiple events per gene in one unique event x.sel = as.alterations(x)\n# Get a list of those with minimum frequency > min.freq # but force inclusion of all events for genes in \"forced.genes\" x.sel = events.selection(x.sel, filter.freq = min.freq,\nfilter.in.names = forced.genes)\n# Subset input - select all events for any gene in \"x.sel\" x = events.selection(x, filter.in.names = as.genes(x.sel)) return(x)\n}\n# We set this as a variable MIN.FREQ = 0.05\n79"
    }, {
      "heading" : "80 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "80"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 81",
      "text" : "Which can be used to subset the datasets to define the input objects for CAPRI.\n# Selected MSS dataset"
    }, {
      "heading" : "MSS.select = select(MSS,",
      "text" : "MIN.FREQ, unique( # All group genes\nc(TCGA.MEMO, KNOWLEDGE.PRIOR.WNT, KNOWLEDGE.PRIOR.RAF, unlist(MSS.MUTEX)) )\n)"
    }, {
      "heading" : "MSS.select = annotate.description(MSS.select,",
      "text" : "’TCGA MSS colorectal tumors’)\n# Selected MSI dataset"
    }, {
      "heading" : "MSI.H.select = select(MSI.H,",
      "text" : "MIN.FREQ, unique(c(TCGA.MEMO,\nKNOWLEDGE.PRIOR.WNT, KNOWLEDGE.PRIOR.RAF, unlist(MSI.H.MSS.MUTEX) ) ))"
    }, {
      "heading" : "MSI.H.select = annotate.description(MSI.H.select,",
      "text" : "’TCGA MSI-HIGH colorectal tumors’)\nTo visualize the selected data, one can use again oncoprint function.\noncoprint(MSS.select, gene.annot = pathway.list,\ngene.annot.color = pathways.color)\nAs detailed in CAPRI’s definition, we are required to provide no pair of events (rows in the oncoprint) which have the same signature. If that happens, those events are statistically indistinguishable, and we prefer to eliminate one of them - being aware that the inferred model would be equivalent whatever event we decide to delete. Further, we also want that no event appears never (resp. always) the dataset - rows of all 0s or 1s. To detect if MSS.select or MSS.select has any of these inconsistencies we use a TRONCO function which outputs NULL if the dataset has no of these issues.\n# Selected MSS is ok cd = consolidate.data( MSS.select, T)\n# In MSI we need to manipulate data instead cd = consolidate.data( MSI.H.select, T)\n81"
    }, {
      "heading" : "82 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "# FBXW7 amplification and PTEN mutations are discarded"
    }, {
      "heading" : "MSI.H.select = delete.event(MSI.H.select, gene=’FBXW7’,",
      "text" : "type =’Amplification’)"
    }, {
      "heading" : "MSI.H.select = delete.event(MSI.H.select, gene=’PTEN’,",
      "text" : "type =’Mutation’)\nNow we really perform inference of selective advantage relations with CAPRI. We first define a function which prepares all formulas that we want to test; these will include exclusivity of genes with multiple alterations, MUTEX groups and other priors.\nTo create formulas from groups we use function hypothesis.add.group which creates all possible formulas from a group of genes, or generally alterations identified by their name. The number of alterations determines the combinatorial number of formulas to create, and if a gene in the group has multiple events associated to it, e.g., a mutation and a CNA, a sub-formula is created to combine them. According to the overlap between such events the formula will be written in either hard or soft exclusivity form. Notice also that we use function hypothesis.add.homologous which creates, if a gene has multiple events associated, the same sub-formula created by hypothesis.add.group, but regardless of the group.\nAfter the creation of formulas we can apply the algorithm CAPRI to the dataset using the function tronco.capri. In this case we set the seed to 12345 to get the same results because of bootstrap random shuffling. The obtained progression model is plotted using the function tronco.plot.\nrecon = function(x, folder, MUTEX, ...) {\nlift = x\n# 1. create formulas from every MUTEX group if(!is.null(MUTEX))\n# TRONCO function to create formulas from groups. # Notice that only 1 formula per group is created # by the dim.min constraint for (w in MUTEX) {\nlift = hypothesis.add.group(lift,\nFUN = OR, # formula of \"soft exclusivity\" (OR) group = w, # the group dim.min = length(w) # only 1 group is maximal )\n}\n# Now we subset Wnt groups to genes actually present in lift. # Notice that formulas for groups with dimension smaller than 2 # will not be created\n82"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 83",
      "text" : "KNOWLEDGE.PRIOR.WNT.subtype = KNOWLEDGE.PRIOR.WNT[\nKNOWLEDGE.PRIOR.WNT %in% as.genes(lift) ]\nlift = hypothesis.add.group(lift,\nFUN = OR, group = KNOWLEDGE.PRIOR.WNT.subtype, dim.min = length(KNOWLEDGE.PRIOR.WNT.subtype))\n# The same for the RAF group KNOWLEDGE.PRIOR.RAF.subtype = KNOWLEDGE.PRIOR.RAF[\nKNOWLEDGE.PRIOR.RAF %in% as.genes(lift) ]\nlift = hypothesis.add.group(lift,\nFUN = OR, group = KNOWLEDGE.PRIOR.RAF.subtype, dim.min = length(KNOWLEDGE.PRIOR.RAF.subtype))\n# And the MEMO group TCGA.MEMO.subtype = TCGA.MEMO[TCGA.MEMO %in% as.genes(lift)]\nlift = hypothesis.add.group(lift,\nFUN = OR, group = TCGA.MEMO.subtype, dim.min = length(TCGA.MEMO.subtype))\n# Homologous in soft exclusivity lift = hypothesis.add.homologous(lift, FUN = OR)\n# Save to file the PDF of the lifted dataset, and its Rdata lift = annotate.description(lift, as.description(x)) oncoprint(lift, file = paste0(folder, ’/Rdata-lifted/lifted.pdf’)) save(lift, file=paste0(folder, ’/Rdata-lifted/lifted.Rdata’))\n# CAPRI execution with seed set, default parameters model = tronco.capri(lift, boot.seed = 12345)\n# As takes long to bootstrap, we make a simple visualization first tronco.plot(model,\npathways = pathway.list, confidence = c(’tp’, ’pr’, ’hg’), # Display p-values ... )\n83"
    }, {
      "heading" : "84 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "# Example non-parametric and statistical bootstrap. # Note that 5 iterations is purely explanatory, # at least 100 should be performed for statistical significance model = tronco.bootstrap(model) model = tronco.bootstrap(model, type = \"statistical\", )\n# Save the Rdata save(model, file=paste0(folder, ’/Rdata-models/model.Rdata’))\n# Plot the TRONCO model tronco.plot(model,\npathways = pathway.list, edge.cex = 1.5,\nlegend.cex = .5, scale.nodes = .6, confidence = c(’tp’, ’pr’, ’hg’), # Display p-values pathways.color = pathways.color,\ndisconnected = F, height.logic = .3, file = paste0(folder, ’/Rdata-models/model.pdf’),\n... )\nreturn(model)\n}\nNow we can apply the function recon to MSS.select and MSI.H.select.\n# Work.. MSS.models = recon(x = MSS.select, folder = ’MSS’, MUTEX = MSS.MUTEX) MSI.models = recon(x = MSI.H.select, folder = ’MSI’, MUTEX = MSI.H.MUTEX)\n5.3.6 Preparing the models for test - validation-samples.R\nWe create now another folder where we will put TRONCO’s output for test dataset, then we load the file which contain the binary matrix with mutation data and we import it into TRONCO format using import.genotypes function.\n# Prepare folders dir.create(’./VALIDATION’) sub.dir = c(’MUTEX’, ’Rdata-lifted’, ’Rdata-models’) sapply(paste0(’./VALIDATION/’, sub.dir), dir.create)\n# Load mutations (binary matrix), process names\n84"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 85",
      "text" : "new.data = load(paste0(workdir, \"/Mutations/TCGA_fun.Rdata\")) head(tcga.f.ag) rownames(tcga.f.ag) = tcga.f.ag $gene tcga.f.ag $gene = NULL\ntcga.f.ag = t(tcga.f.ag) rownames(tcga.f.ag) = gsub(’_’, ’-’, rownames(tcga.f.ag))\n# Import in TRONCO format tcga.f.ag = import.genotypes(tcga.f.ag, ’Mutation’) show(tcga.f.ag)\nNow we separate the list of samples of the dataset just loaded into two sets, one for MSI-HIGH and one for MSS. The discrimination is based on the mutation rate.\n# Mutation rate to discriminate MSS and MSI-HIGH plot(sort(rowSums(as.genotypes(tcga.f.ag))), col = ’darkgreen’, lty = 3,\nxlab = ’Tumor sample’, ylab = ’Mutations (number)’)\ntitle(’Validation dataset - Mutations per sample’) abline(h = 500, col = ’red’, lty = ’dashed’) abline(v = 204, col = ’black’, lty = ’dashed’) text(x = 230, y = 10, ’MSI-H (Val.)’, cex = .8) text(x = 170, y = 10, ’MSS (Val.)’, cex = .8) text(x = 30, y = 600, ’500 mutations (cutoff)’, cex = .5)\n# MSS tumors tcga.f.ag.MSS = rownames(as.genotypes(tcga.f.ag))[\nwhich(rowSums(as.genotypes(tcga.f.ag)) < 500) ]\n# MSI-HIGH tumors tcga.f.ag.MSI = rownames(as.genotypes(tcga.f.ag))[\nwhich(rowSums(as.genotypes(tcga.f.ag)) > 500) ]\nThen, based on the two list of samples using the function samples.selection we produce two dataset: one for MSI-HIGH and one for MSS. After this we trim the new datasets, shorten the barcodes of samples and keep only the genes mapped to selected pathways. At the end of this operation we have two new dataset ready: tcga.f.ag.MSS and tcga.f.ag.MSI.\n# TRONCO objects for these tumors tcga.f.ag.MSS = trim(samples.selection(tcga.f.ag, tcga.f.ag.MSS)) tcga.f.ag.MSI = trim(samples.selection(tcga.f.ag, tcga.f.ag.MSI))\n85"
    }, {
      "heading" : "86 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "tcga.f.ag.MSS = TCGA.shorten.barcodes(tcga.f.ag.MSS) tcga.f.ag.MSI = TCGA.shorten.barcodes(tcga.f.ag.MSI) show(tcga.f.ag.MSS) show(tcga.f.ag.MSI)\n# Select just genes mapped to pathways tcga.f.ag.MSS = trim(events.selection(tcga.f.ag.MSS,\nfilter.in.names = pathway.genes))\ntcga.f.ag.MSI = trim(events.selection(tcga.f.ag.MSI,\nfilter.in.names = pathway.genes))\ntcga.f.ag.MSS = change.color(tcga.f.ag.MSS, ’Mutation’, ’darkolivegreen3’) tcga.f.ag.MSI = change.color(tcga.f.ag.MSI, ’Mutation’, ’darkolivegreen3’)\nNow we can load the GISTIC of CNA data, change the events color and keep only the events which are present in pathways. The file GISTIC.RData contains a dataset already in TRONCO format.\n# Load CNA data load(paste0(workdir, ’GISTIC.Rdata’)) show(GISTIC) GISTIC = change.color(GISTIC, ’Amplification’, ’coral’) GISTIC = change.color(GISTIC, ’Deletion’, ’cornflowerblue’) GISTIC = events.selection(GISTIC, filter.in.names = pathway.genes)\nWe have now the two dataset with mutations data and the dataset with CNA. The next operation we have to do is to merge rispectively mutations and CNA data of MSS and MSI-HIGH. We can do this using the function ebind which merge two datasets with the same samples list and different events. We start with MSS.\n# Join datasets with both mutations and CNAs: MSS tcga.f.ag.MSS =\nebind(\nsamples.selection(tcga.f.ag.MSS,\nintersect(\nas.samples(tcga.f.ag.MSS), as.samples(GISTIC))\n),\nsamples.selection(GISTIC,\nintersect(\nas.samples(tcga.f.ag.MSS), as.samples(GISTIC))\n)\n)\ntcga.f.ag.MSS = trim(tcga.f.ag.MSS)\n86"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 87",
      "text" : "tcga.f.ag.MSS = annotate.stages(tcga.f.ag.MSS, as.stages(GISTIC)) oncoprint(tcga.f.ag.MSS)\nAnd we can do the same for MSI-HIGH.\n# Join datasets with both mutations and CNAs: MSI-HIGH tcga.f.ag.MSI = ebind(\nsamples.selection(tcga.f.ag.MSI,\nintersect(\nas.samples(tcga.f.ag.MSI), as.samples(GISTIC))\n), samples.selection(GISTIC,\nintersect(\nas.samples(tcga.f.ag.MSI), as.samples(GISTIC))\n)\n) tcga.f.ag.MSI = trim(tcga.f.ag.MSI) tcga.f.ag.MSI = annotate.stages(tcga.f.ag.MSI, as.stages(GISTIC)) oncoprint(tcga.f.ag.MSI)\n5.3.7 Validation of the models - validation-samples.R\nAt this point we can use the previously created select function on tcga.f.ag.MSS and tcga.f.ag.MSI to subset data with the minimum frequency, as we did for training sets.\n# Selected MSS tumors tcga.f.ag.MSS.select = select(tcga.f.ag.MSS,\nMIN.FREQ, unique(c(TCGA.MEMO,\nKNOWLEDGE.PRIOR.WNT, KNOWLEDGE.PRIOR.RAF, unlist(MSS.MUTEX) ) ))\n# Mnemonic dataset title tcga.f.ag.MSS.select =\nannotate.description(tcga.f.ag.MSS.select,\n’(Test) MSS colorectal tumors’)\n# Selected MSI tumors\n87"
    }, {
      "heading" : "88 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "tcga.f.ag.MSI.select = select(tcga.f.ag.MSI,\nMIN.FREQ, unique(c(TCGA.MEMO,\nKNOWLEDGE.PRIOR.WNT, KNOWLEDGE.PRIOR.RAF, unlist(MSI.H.MUTEX) ) ))\n# Mnemonic dataset title tcga.f.ag.MSI.select =\nannotate.description(tcga.f.ag.MSI.select,\n’(Test) MSI-HIGH colorectal tumors’)\nAfter this, we can check the consistency of the dataset with the consolidate.data function already described before - you should find no output in this case.\nNow we can reconstruct the two models with the function recon defined before.\n# Reconstructions tcga.f.ag.MSS.models = recon(x = tcga.f.ag.MSS.select,\nfolder = ’VALIDATION’, MUTEX = MSS.MUTEX)\ntcga.f.ag.MSI.models = recon(x = tcga.f.ag.MSI.select,\nfolder = ’VALIDATION’, MUTEX = MSS.MUTEX)\nFinally we list the p-values for MSS training model to be matched to those for test.\n# P-values for MSS training for # temporal priority, probability raising and hypergeometric test as.selective.advantage.relations(MSS.models) # edges used for MLE as.selective.advantage.relations(MSS.models, type = ’pf’) # all edges as.selective.advantage.relations(tcga.f.ag.MSS.models) # edges used for MLE as.selective.advantage.relations(tcga.f.ag.MSS.models, type = ’pf’) # all edges\n# P-values for MSI-HIGH as.selective.advantage.relations(MSI.models) # edges used for MLE as.selective.advantage.relations(MSI.models, type = ’pf’) # all edges as.selective.advantage.relations(tcga.f.ag.MSI.models) # edges used for MLE as.selective.advantage.relations(tcga.f.ag.MSI.models, type = ’pf’) # all edges\nThe result of this operation is show for MSI-HIGH in Table §5.1 using BIC as a regularizator and in Table §5.2 using AIC.\n88"
    }, {
      "heading" : "5.3. TCGA MSI/MSS COLORECTAL TUMORS 89",
      "text" : "S E\nL E\nC T\nS S\nE L\nE C\nT E\nD O\nB S\nO B\nS T\nE M\nP P\nR IO\nR P\nR O\nB R\nA IS\nH Y\nP G\n1 D e l N R A S\nD e l A C V R 1 B\n2 1\n2 . 9 2 5 5 3 2 e - 2 0\n1 . 8 8 4 4 0 9 e - 4 0 0 2 M u t L R P 5 M u t T G F B R 2 7 2 8 . 3 2 2 4 2 2 e - 7 4 3 . 4 1 8 8 9 0 e - 8 4 0 3 M u t A P C D e l N R A S 1 0 2 1 . 0 6 1 6 0 0 e - 8 0 1 . 1 7 8 0 8 6 e - 8 2 0 4 M u t A P C M u t P I K 3 C A 1 0 5 2 . 5 2 8 3 5 4 e - 6 5 3 . 8 1 0 7 9 5 e - 7 2\n0 . 0 4 7 3 4 2\n5 M u t A P C\nM u t S M A D 3\n1 0\n2 9 . 1 0 8 5 4 2 e - 8 1\n8 . 6 1 5 1 5 5 e - 8 1 0 6 M u t A P C P a t X O R N R A S 1 0 3 2 . 5 1 1 3 9 6 e - 7 8 5 . 3 5 9 6 8 1 e - 8 9 0 7 M u t A R I D 1 A M u t D K K 2 1 1 5 3 . 6 2 3 1 8 3 e - 7 1 3 . 3 0 7 5 3 2 e - 7 6\n0 . 0 0 5 7 2 2\n8 M u t A X I N 2\nM u t T P 5 3\n5 4\n1 . 3 1 8 1 6 5 e - 0 7\n9 . 0 7 8 7 0 8 e - 6 9\n0 . 0 1 2 8 2 0\n9 M u t F A M 1 2 3 B\nM u t S M A D 4\n8 4\n4 . 1 2 4 4 2 3 e - 5 1\n1 . 9 7 7 0 4 3 e - 6 6\n0 . 0 0 3 9 8\n1 0\nM u t P I K 3 C A\nM u t D K K 4\n5 2\n4 . 8 2 8 2 8 9 e - 5 3\n2 . 6 4 9 3 7 0 e - 7 5\n0 . 0 2 8 4 9 0\n1 1\nM u t E R B B 2\nM u t N R A S\n4 1\n4 . 2 1 3 9 4 9 e - 5 6\n1 . 6 5 4 6 8 4 e - 5 8 0 1 2 M u t E R B B 2 M u t S M A D 2 4 2 3 . 8 6 9 1 8 1 e - 2 8 6 . 6 9 0 9 0 0 e - 6 8 0 1 3 M u t E R B B 3 M u t K R A S 7 7 0 . 3 6 2 0 1 6 4 . 1 8 4 3 4 2 e - 7 7\n0 . 0 0 4 6 5 1\n1 4\nP a t O R E R B B 2 . . .\nM u t L R P 5\n8 7\n5 . 0 6 3 7 1 4 e - 0 5\n2 . 7 7 5 6 0 5 e - 7 9\n0 . 0 1 1 3 9 1\nT a b\nle 5.\n1: p -v\na lu\ne s\no f\nM S\nIH\nIG H\nw it\nh B\nIC a s\nre g u la\nti z a to\nr.\n89"
    }, {
      "heading" : "90 CHAPTER 5. AN R PACKAGE FOR TRANSLATIONAL ONCOLOGY",
      "text" : "S E\nL E\nC T\nS E\nL E\nC T\nE D\nO B\nS O\nB S\nT E\nM P\nP R\nIO R\nP R\nO B\nR A\nIS H\nY P\nG\n1 D e l N R A S\nD e l A C V R 1 B\n2 1\n2 . 9 2 5 5 3 2 e - 2 0\n1 . 8 8 4 4 0 9 e - 4 0 0 2 M u t A C V R 2 A M u t T C F 7 L 2 5 4 2 . 7 7 7 5 1 8 e - 0 8 5 . 5 7 3 2 1 5 - 4 6\n0 . 0 1 2 8 2 0\n3 M u t B R A F\nM u t A C V R 1 B\n1 5\n6 6 . 9 8 1 9 4 3 e - 7 9\n6 . 6 2 2 4 4 0 e - 6 7\n0 . 1 3 8 6 4 7\n4 M u t B R A F\nM u t D K K 1\n1 5\n2 2 . 4 6 3 4 6 9 e - 8 1\n1 . 1 8 0 2 9 7 e - 7 9 0 5 M u t F B X W 7 D e l N R A S 1 4 2 2 . 2 2 6 3 5 1 e - 8 1 1 . 1 9 8 6 3 3 e - 8 2 0 6 M u t L R P 5 M u t T G F B R 2 7 2 8 . 3 2 2 4 2 2 e - 7 4 3 . 4 1 8 8 9 0 e - 8 4 0 7 M u t A P C D e l N R A S 1 0 2 1 . 0 6 1 6 0 0 e - 8 0 1 . 1 7 8 0 8 6 e - 8 2 0 8 M u t A P C M u t K R A S 1 0 7 8 . 5 3 2 8 4 9 e - 3 4 5 . 8 4 4 5 0 4 e - 7 5\n0 . 0 4 2 7 4 8\n9 M u t A P C\nM u t P I K 3 C A\n1 0\n5 2 . 5 2 8 3 5 4 e - 6 5\n3 . 8 1 0 7 9 5 e - 7 2\n0 . 0 4 7 3 4 2\n1 0\nM u t A P C\nM u t S M A D 3\n1 0\n2 9 . 1 0 8 5 4 2 e - 8 1\n8 . 6 1 5 1 5 5 e - 8 1 0 1 1 M u t A P C P a t O R E R B B 2 . . . 1 0 8 3 . 4 3 7 0 7 8 e - 1 8 1 . 9 2 5 9 8 6 e - 6 4\n0 . 0 9 0 9 9 0\n1 2\nM u t A P C\nP a t X O R N R A S\n1 0\n3 2 . 5 1 1 3 9 6 e - 7 8\n5 . 3 5 9 6 8 1 e - 8 9 0 1 3 M u t A R I D 1 A M u t D K K 2 1 1 5 3 . 6 2 3 1 8 3 e - 7 1 3 . 3 0 7 5 3 2 e - 7 6\n0 . 0 0 5 7 2 2\n1 4\nM u t A R I D 1 A\nM u t E R B B 2\n1 1\n4 3 . 1 5 4 8 6 6 e - 7 6\n5 . 0 1 4 4 6 3 e - 5 7\n0 . 1 6 9 2 3 0\n1 5\nM u t A R I D 1 A\nM u t T G F B R 1\n1 1\n4 2 . 5 3 6 6 9 8 e - 7 8\n3 . 0 1 7 1 3 8 e - 6 8\n0 . 1 6 9 2 3 0\n1 6\nM u t A X I N 2\nM u t T P 5 3\n5 4\n1 . 3 1 8 1 6 5 e - 0 7\n9 . 0 7 8 7 0 8 e - 6 9\n0 . 0 1 2 8 2 0\n1 7\nM u t K R A S\nM u t N R A S\n7 1\n5 . 1 4 5 7 5 0 e - 8 0\n1 . 9 5 6 3 5 1 e - 6 1 0 1 8 M u t F A M 1 2 3 B M u t S M A D 4 8 4 4 . 1 2 4 4 2 3 e - 5 1 1 . 9 7 7 0 4 3 e - 6 6\n0 . 0 0 3 9 8 8\n1 9\nM u t F A M 1 2 3 B\nM u t S M A D 2\n8 2\n6 . 3 1 6 3 2 9 e - 7 5\n8 . 0 3 5 9 0 1 e - 7 8 0 2 0 M u t P I K 3 C A M u t E R B B 2 5 4 3 . 1 7 8 1 6 6 e - 0 7 1 . 4 0 8 4 2 2 e - 4 0\n0 . 0 1 2 8 2 0\n2 1\nM u t P I K 3 C A\nM u t D K K 4\n5 2\n4 . 8 2 8 2 8 9 e - 5 3\n2 . 6 4 9 3 7 0 e - 7 5\n0 . 0 2 8 4 9 0\n2 2\nM u t A T M\nD e l I G F 2\n6 1\n4 . 6 8 7 6 4 0 e - 7 8\n1 . 1 1 2 8 0 0 e - 6 0\n0 . 2 2 2 2 2 2\n2 3\nM u t E R B B 2\nM u t N R A S\n4 1\n4 . 2 1 3 9 4 9 e - 5 6\n1 . 6 5 4 6 8 4 e - 5 8 0 2 4 M u t E R B B 2 M u t S M A D 2 4 2 3 . 8 6 9 1 8 1 e - 2 8 6 . 6 9 0 9 0 0 e - 6 8 0 2 5 M u t E R B B 3 M u t K R A S 7 7 0 . 3 6 2 0 1 6 4 . 1 8 4 3 4 2 e - 7 7\n0 . 0 0 4 6 5 1\n2 6\nP a t O R E R B B 2 . . .\nM u t L R P 5\n8 7\n5 . 0 6 3 7 1 4 e - 0 5\n2 . 7 7 5 6 0 5 e - 7 9\n0 . 0 1 1 3 9 1\n2 7\nP a t X O R A C V R 1 B\nM u t T G F B R 2\n7 2\n9 . 3 0 2 7 6 8 e - 7 3\n1 . 9 2 6 8 2 e - 8 3\n0\nT ab\nle 5.\n2: p -v\na lu\ne s\no f\nM S\nIH\nIG H\nw it\nh A\nIC a s\nre g u\nla ti\nz a to\nr.\n90\nCHAPTER 6\nEVOLUTION OF COLORECTAL CANCER\nIn this Chapter, we present the design, development and evaluation of an optimal, versatile and modular pipeline which exploits state-of-the-art tools to extract ensemble-level cancer progression models from cross-sectional data. We also show its applications in interpreting colorectal cancer data which, because of its high levels of heterogeneity, may be thought of as one of the most challenging case studies. As a reference, see [21]."
    }, {
      "heading" : "6.1 A pipeline to infer ensemble-level progression models",
      "text" : "We have devised a customizable pipeline to infer ensemble-level cancer progression models from cross-sectional data with CAPRI [158]. To increase the statistical quality of its predictions this pipeline pre-processes data to diminish the confounding effects of inter and intra-tumor heterogeneity. At a high-level, one shall thus identify: (i) biologically meaningful subtypes with similar molecular profiles via tumor stratification, (ii) the set of driver alterations and (iii) the groups of fitness-equivalent (i.e., exclusive) alterations.\nSpecifically, let us consider n tumors (n patients) and assume the relevant (epi)genetic data to be available. We do not put constraints on data gathering and selection, leaving the user to decide the appropriate “resolution” of the input mutational data. For instance, one might decide whether somatic mutations should be classified by type, or aggregated. Or, one might decide to lift focal CNAs to the wider resolution of cytobands or full arms. These choices depend on data and on the overall understanding of such alterations and their functional effects for the cancer under study, and no single all-encompassing rationale may be provided.\nGiven these premises, the pipelines consists in the following steps.\nStep 1: Reducing inter-tumor heterogeneity by cohort subtyping. One might wish to identify cancer subtypes in the heterogeneous mixture of input samples. In some cases the classification can benefit from clinical biomarkers, such as evidences of certain\n91"
    }, {
      "heading" : "92 CHAPTER 6. EVOLUTION OF COLORECTAL CANCER",
      "text" : "cell types [14], but in most cases we will have to rely on multiple clustering approaches at once, see, e.g., [137, 139].\nMany common approaches cluster expression profiles [119] by relying on non-negative matrix factorization techniques [59] or earlier approaches such as k-means, Gaussians mixtures or hierarchical/spectral clustering - see the review in [34]. For glioblastoma and breast cancer, for instance, mRNA expression subtypes provides good correlation with clinical phenotypes [138, 102, 161]. However, this is not always the case as, e.g., in colorectal cancer such clusters mismatch with survival and chemotherapy response [138]. Clustering of full exome mutation profiles or smaller panels of genes might be an alternative as it was shown for ovarian, uterine and lung cancers [82, 199].\nStep 2: selection of driver events. In subtypes detection, with more alterations available it becomes easier to find similarities across n samples, as features selection gains precision. In progression inference, instead, one wishes to focus on m n driver alterations, which ensure also an appropriate statistical ratio between sample size (n) and problem dimension (m).\nMultiple tools filter out driver from passenger mutations. MutSigCV identifies drivers mutated more frequently than background mutation rate, [106]. OncodriveFM, avoids such estimation but looks for functional mutations [66]. OncodriveCLUST scans mutations clustering in small regions of the protein sequence [176]. MuSiC uses multiple types of clinical data to establish correlations among mutation sites, genes and pathways [35]. Some other tools search for driver CNAs that affect protein expression [178]. All these approaches use different statistics to estimate signs of positive selection, and we suggest using them in an orchestrated way, as done in some platforms [70]. Notice that driver genes will likely differ across subtypes, mimicking the different molecular properties of each group of samples.\nStep 3: fitness equivalence of exclusive alterations. When at the ensemble-level, identification of “groups of equivalent but alternative” mutually exclusivity alterations is crucial, prior to progression inference [158]. A plethora of tools can be used; greedy approaches [194, 129] or their optimizations, such as MEMO, which constrain searchspace with network priors [28]. This strategy is further improved in MUTEX, which scans mutations and focal CNAs for genes with a common downstream effect in a curated signalling network, and selects only those genes that significantly contributes to the exclusivity pattern [8]. Other tools, instead, employ advanced statistics or generative approaches without priors [182, 198, 110, 112, 84, 175].\nIn the fitness equivalent groups, we distinguish between hard and soft exclusivity, the former assuming strict exclusivity among events, with random errors accounting for possible overlaps, the latter admitting co-occurrences. [8]. CAPRI is the only algorithm where relations among group of genes can be input as “testable hypotheses” via logical Boolean formulas. In this case, we can use logical connectives such as ⊕ (the logical “xor”) as a proxy for hard-exclusivity, and ∨ (the logical “disjunction”) as a proxy\n92"
    }, {
      "heading" : "6.1. A PIPELINE TO INFER ENSEMBLE-LEVEL PROGRESSION MODELS 93",
      "text" : "for soft-exclusivity1. For example, these can be used to test wether colorectal tumors “start” prevalently from β-catenin deregulation, i.e., apc ∨ ctnnb1 , and if they further progress exclusively (⊕) through kras or nras alterations. In general, as this testingfeature leaves the inference unbiased - see [158] - arbitrary hypotheses on significantly mutated subnetworks could be considered as well [111, 181].\nStep 4: progression inference and confidence estimation. Finally, CAPRI is used to reconstruct cancer progression models of each identified molecular subtype, provided that there exist a reasonable list of driver events and the groups of fitnessequivalent exclusive alterations.\nAs already discussed in §4 CAPRI’s input is a binary n × (m + k) matrix M with n samples, m driver alteration events (Bernoulli 0/1 variables) and k testable formulas. CAPRI first scans pairwise M to identify a set of S plausible selective advantage relations, which then reduces to the most relevant ones, S∗ ⊂ S.\nConstruction of S depends on the number of non-parametric bootstrap iterations and confidence p-values for estimating selective advantage among input events x and y. CAPRI postulates that “x selects for y” if it estimates that “x is earlier than y” and that “x’s presence increases the probability of observing y” [172]. These conditions are implemented with the following inequalities:\np(x) > p(y) p(y | x) > p(y | ¬x) (6.1) for which we get p-values by Mann-Withney U Testing. Here, p(·) is an empirical marginal probability, p(· | ·) is a conditional, and ¬x is the negation of x.\nOptimization of S is central to the tolerance to false positives and negatives in S∗. CAPRI’s implementation in TRONCO [4] selects from S a subset of relations by optimizing the score with regularization:\nS∗ = arg min Ŝ⊂S\n{ −2 log[L(Ŝ |M)] + θ|Ŝ| } , (6.2)\nwhere L(·) is the model likelihood; the estimated optimal solution is S∗. Different values of θ lead to different tolerance to errors in S∗, the Akaike Information Criterion (AIC) being for θ = 2, the Bayesian Information Criterion (BIC) for θ = log(n). Both scores are approximately correct; AIC is more prone to overfitting but likely to provide also good predictions from data and is better when false negatives are more misleading than positive ones. BIC is more prone to underfitting errors, thus is more parsimonious and better in opposite cases. As often done, we suggest to combine both approaches and distinguish which relations are selected by BIC or AIC.\nModel confidence can be estimated with non-parametric, parametric or statistical bootstrap [47]. These procedures re-sample datasets to provide a confidence to every selective advantage relation and to the overall model. Bootstrapped datasets are randomly\n1Logical disjunction of a set of operands is true if and only if one or more of its operands is true. For this reason, if we shall use that as a model of soft-exclusivity, we shall also check that the majority of observations indeed shows an exclusivity trend, meaning that few cases of co-occurent observations happen.\n93"
    }, {
      "heading" : "94 CHAPTER 6. EVOLUTION OF COLORECTAL CANCER",
      "text" : "generated by re-shuffling data and seed (non-parametric), just seed (statistical) or by sampling from the model (parametric). CAPRI’s other statistics include hypergeometric tests to assess how significant is the overlap between pairs of alterations.\nThus, this pipeline is similar in spirit to those implemented by consortia such as TCGA to analyze huge populations of cancer samples collected [137, 139]. One of the main novelties of this approach, which is only possible by the specific features of hypothesis-testing provided by CAPRI [158], is the exploitation of groups of exclusive alterations as a proxy to detect fitness-equivalent routes of cancer progression. Thus, CAPRI is seen as an ideal tool for the efficient and theoretically-grounded investigations in population-based studies on cancer genomics.\nThis approach allows one to produce a progression model for virtually every cancer subtype identified in the input cohort, which shall be characteristic of the population trends of cancer initiation and progression. In the following, we empirically characterize the efficacy of the approach in processing colorectal cancer data from TCGA project [137], demonstrating that we were able to re-discover most of the existing body of knowledge about colorectal tumor progression or to propose further experimentally verifiable hypotheses2."
    }, {
      "heading" : "6.2 Clonal evolution of MSI/MSS colorectal tumors",
      "text" : "It is common knowledge that colorectal cancer (CRC) is a heterogeneous disease comprising different molecular entities. Since similar tumors are most likely to behave in a similar way, grouping tumors with homogeneous characteristics may be useful to define personalized therapies. Indeed, it is currently accepted that colon tumors can be classified according to their global genomic status into two main types: microsatellite instable tumors (MSI) and microsatellite stable (MSS) tumors (also known as tumors with chromosomal instability). This taxonomy plays a significant role in determining pathologic, clinical and biological characteristics of CRC tumors [146]. Thus, MSS tumors are characterized by changes in chromosomal copy number and show worse prognosis [125, 187]. On the contrary, the less common MSI tumors (about 15% of sporadic CRC) are characterized by the accumulation of a high number of mutations and show predominance in females, proximal colonic localization, poor differentiation, tumor-infiltrating lymphocytes and a better prognosis [183]. In addition, MSS and MSI tumors exhibit different responses to chemotherapeutic agents [99, 189]. Regarding molecular progression, it is also well established that each subtype arises from a distinctive molecular mechanism. While MSS tumors generally follow the classical adenoma-to-carcinoma progression (sequential apc-kras-tp53 mutations) described in the seminal work by Vogelstein and Fearon [51], MSI tumors results from the inactivation of DNA mismatch repair genes like mlh-1 [183].\nWe adopted the discussed pipeline to process MSI and MSS colorectal tumors collected from the The Cancer Genome Atlas project “Human Colon and Rectal Cancer”\n2We remark that in-vitro and in-vivo experiments could provide an optimal validation for the newly suggested selective advantage relations and hypotheses, yet this is out of the scope of the current work.\n94"
    }, {
      "heading" : "6.2. CLONAL EVOLUTION OF MSI/MSS COLORECTAL TUMORS 95",
      "text" : "(COADREAD, [137]) – also see §E. Details on the implementation are available in §E as well as source code to replicate this study. COADREAD has enough samples to implement a training/test statistical validation of these findings - see §E. In brief, we split subtypes by the microsatellite status of each tumor, and select somatic mutations and focal CNAs in 33 driver genes manually annotated to 5 pathways in [137] - wnt, raf, tgf-β, pi3k and p53. Groups of exclusive alterations were scanned by MUTEX [8] (see §E), and fetched by [137] using the MEMO [28] tool; groups were used to create CAPRI’s formulas, see §E. The data for MSI-HIGH tumors are shown in Figure 6.1, for MSS tumors see §E. CAPRI was run, on each subtype, by selecting recurrent alterations from the pool of 33 pathway genes and using both AIC/BIC regularizators. The model inferred for MSS tumors is in Figure 6.2, the model for MSI-HIGH ones is in Figure 6.3. Each edge in the graph mirrors selective advantage among the upstream and downstream nodes, as estimated by CAPRI from training datasets (statistics: p < 0.05, 100 non-parametric bootstraps); only the minimum amount of edges is selected to maximize the likelihood of data (see Online Methods). In Figure 6.3, for MSI-HIGH tumors, we\n95\nshow how the model predicts alternative routes to somatic evolution for COADREAD samples. As statistical validation of these models, we mark those relations that display significant p-values in the test datasets, and rank them if they contribute (or otherwise) to max-likelihood. For some edges it is not possible to provide a validation, as some upstream or downstream event may be missing in the test dataset, while other edges do not show statistical evidence in the test datasets.\nMSS (Microsatellite Stable). In agreement with the known literature, the progression model identifies kras, tp53 and apc as primary events in the carcinogenesis, as well as nras and kras determining two independent evolution branches, the former being “selected by” tp53 mutations, i.e. being a downstream event in the model, the latter ”selecting for” pik3ca mutations. The leftmost portion of the model links many wnt genes, in agreement with the observation that multiple concurrent lesions affecting such pathway confer selective advantage. In this respect, the model predicts multiple routes to eventually select alterations in sox9"
    }, {
      "heading" : "6.2. CLONAL EVOLUTION OF MSI/MSS COLORECTAL TUMORS 97",
      "text" : "97"
    }, {
      "heading" : "98 CHAPTER 6. EVOLUTION OF COLORECTAL CANCER",
      "text" : "gene, a transcription factor known to be active in colon mucosa [1]. Its mutations are indeed selected by apc/ctnnb1 alterations - with apc mutations in 78% of the tumors - or by fbxw7, an early mutated gene that both directly and in a redundant way via ctnnb1 selects for its mutations or amplifications. sox family of transcription factors have emerged as modulators of canonical wnt/β-catenin signaling in many disease contexts, with evidences that multiple sox proteins physically interact with β-catenin and modulate the transcription of wnt-target genes, as well as with evidences of regulating of sox’s expression by wnt resulting in feedback regulatory loops that fine-tune cellular responses to β-catenin/tcf activity [103]. Also interestingly, fbxw7 has been previously reported to be involved in the malignant transformation from adenoma to carcinoma [114], and it was recently shown that SCFFbw7, a complex of ubiquitin ligase that contains such gene, targets several oncogenic proteins including sox9 for degradation [83]; this relation has high-confidence also in the test dataset. The rightmost part of the model involves genes from other pathways, and outlines the relation between kras and the pi3k pathway. We indeed find, consistently in the training and test, selection of pik3ca mutations by kras ones, as well as selection of the whole MEMO module, which is responsible for the activation of the pi3k pathway [137]. Further, Mutations in pten (in hard exclusivity with its deletions) were found to be a late event acquired by a clone with kras mutations directly, or indirectly via erbb2 mutations. smad4 deletions or mutations (in hard exclusivity) are selected from tumors harboring kras mutations. In a smaller group of tumors smad2 amplification (1%) or mutation (5%) selects for braf mutations. To highlight, a sub-group of tumors lacking apc, kras or tp53 but exhibiting mutations in fam123b (10%) or mutations in tcf7l2 (9%) that later converge in dkk2 or dkk4 mutations. Interestingly, these four genes are implicated in the wnt signalling pathway. It is also worth pointing that the model predicts a selection trend among sox9/arid1a and atm/fam123b; however, for the corresponding events, in the test, we find the opposite model’s direction, suggesting a potential confounding effect which can be inputed to these events having very similar frequencies in the training dataset.\nMSI (Microstaellite Instable). In agreement with the current literature, braf is the most commonly mutated gene in MSI tumors (55%) [95]. CAPRI also predicted convergent evolution of tumors harbouring fbxw7 (51%) or apc (37%) mutations towards deletions of nras gene, as well as selection of smad2 or smad4 mutations by fam123b mutations, for these tumors. Relevant to all MSI tumors seems again the role of the pi3k pathway. Indeed, a relation among apc and pik3ca mutations was inferred with a high confidence in both training and test datasets, consistent with recent experimental evidences pointing at a synergistic role of these mutations, which co-occurr in the majority of human colorectal cancers [36]. Similarly, we find consistently a selection trend among apc and the whole MEMO module. Interestingly, both mutations in apc and erbb3 select for kras mutations, which might point to interesting therapeutic implications. On the contrary, mutations in braf mostly selects for mutations in acvr1b, a receptor that once activated\n98\nphosphorylates smad proteins. It forms receptor complex with acvr2a, a gene mutated in these tumors (18%) that selects for tcf7l2 mutations (also implicated in the progression of MSS tumors). Tumors harbouring tp53 mutations are those selected by exhibit mutations in axin2, a gene implicated in wnt signalling pathway, and related to instable gastric cancer development [96]."
    }, {
      "heading" : "6.3 Inference of patient-specific clonal evolution",
      "text" : "We also discovered that the CAPRESE [117] algorithm can be used to successfully reconstruct the clonal architecture in individual patients. This result is indicative of the power of the selective advantage scores à-la-Suppes [172], even outside the scope of cross-sectional data. We performed the analysis on data from Gerlinger et al., which have recently used multi-region targeted exome sequencing (> 70x coverage) to resolve the genetic architecture and evolutionary histories of ten clear cell renal carcinomas [62].\nBesides quantification of intra-tumor heterogeneity, their work found that loss of the 3p arm and alterations of the Von Hippel-Lindau tumor suppressor gene vhl are the only events ubiquitous among their patients. In Figure 6.4 we show the clonal evolution estimated for one of those patients, RMH004, computed with CAPRESE (shrinkage coefficient λ = 0.5, time < 1 sec) from the Bernoulli 0/1 profiles provided in Supplementary\nTable 3 and Figure 4 of [62], with non-parametric bootstrap confidence (time < 6 sec). This model shall be compared to the one inferred by processing the region-specific VAF with a max-mini optimization of most parsimonious evolutionary trees [156], and performing selection-by-consensus when multiple optimal solutions exist - Supplementary\nFigure 9 in [62]. CAPRESE requires no arbitrarily defined curation criteria to select the optimal tree, as it constructively searches for a solution which, in this case, is analogous in suggesting parallel evolution of subclones via deregulation of the swi/snf chromatinremodeling complex – i.e., as may be noted from multiple clones with distinct pbmr1 mutations. Finally, the approach in [156], estimates also the number of non-synonymous mutations acquired on a certain edge of the tree. While this model is silent about this, it is very likely due to the limitations imposed by the lower-resolution and small sample size of the data – 9 events from 8 regions, and not the VAFs for all alleles.\nSingle-cell synthetic data. We estimate the efficiency of our approach to single-cell sequencing data, as if it was collected from patient RMH004 (synthetic data generated from the clonal phylogeny architecture of Figure 6.4). To mimic a poor reliability of this technology, to each sampled cell a noise model which accounts for false positives and negatives in the calls of their genomic alterations is applied. Performance is measured as the fraction of true-positive and negative ancestry relations inferred among cells (precision and recall), as a function of the number of sequenced cells and noise level. Results evidence a very good performance even with very small number of cells and reasonable noise levels, hinting at a promising application with this technology. Complete details for synthetic data generation and further performance measures are provided in §E.\n100 CHAPTER 6. EVOLUTION OF COLORECTAL CANCER\n100\nCHAPTER 7\nCONCLUSIONS\nThe reconstruction of cancer progression models is a pressing problem, as it promises to highlight important clues about the evolutionary dynamics of tumors and to help in better targeting therapy to the disease (see e.g., [118]). In the absence of large longitudinal datasets, progression extraction algorithms rely primarily on cross-sectional input data, thus complicating the statistical inference problem.\nIn this thesis, we explore the nature of somatic evolution in cancer. The proposed model of somatic evolution not only supports the heterogeneity and temporality seen in tumor clonal populations, but it also suggests a selectivity/causality relation that can be used in analyzing (epi)genomic data and exploited in therapy design.\nAlthough strongly supported by empirical studies using synthetic as well as experimental genomic data, the contributions of this thesis are primarily methodological, with the aim of attaching the problem of understanding cancer evolution rigorously, by means of strong foundations built upon the sound theory of probabilistic causality, originally proposed by Patrick Suppes [172] (see §2).\nWith these premises, we devise efficient approaches, namely CAPRESE and CAPRI algorithms, which for the first time algorithmize Suppes’ formulation to reconstruct respectively tree and directed acyclic graph models of cancer progression (see §3 and §4). These methods, while taming their efficiency satisfactorily, even for many complex situations that are specifically important in cancer studies (e.g., synthetic lethality or oncogene addiction), are kept computational efficient. Furthermore, the aforementioned algorithms along with a set of utilities to support the scientist in his effort of understanding cancer progression are implemented in the TRONCO R package (see §5).\nAdditionally, the framework is shown to be effective in extracting evolutionary trajectories for cancer progression both at the level of populations and individual patients. In the former case a pipeline to minimize the confounding effects imputable to tumor heterogeneity is proposed and applied to a highly-heterogeneous cancer such as colorectal. In the latter we have also shown how the framework can be readily applied to reconstruct clonal phylogeny from multi-sample data, with an application to clear renal\n101\n102 CHAPTER 7. CONCLUSIONS\ncell carcinoma (see §6)."
    }, {
      "heading" : "7.1 Future works",
      "text" : "Despite in the last two decades many specific genes and genetic mechanisms that are involved in different types of cancer have been identified, our understanding of cancer progression is still largely elusive and faces fundamental challenges. Hence, several future research directions are possible.\nIn particular, the various steps described in the pipeline need further improvements with specific reference to the clustering in subtypes: while a series of state-of-the-art techniques to tackle the problem exist, their results are not always satisfactory and, in fact, in most studies, the identification of subtypes is not clear or it is done manually (see e.g., [137]).\nMoreover, with the objective of anticipating the forthcoming technologies, a further development of the proposed framework could involve the introduction of timed data, in order to extend the techniques to settings where a temporal information on the samples is available, hence needs not be imputed.\nIn the current version, a progression model is reconstructed in terms of event (i.e., genetic alterations) at a lower grain than the actual progression in terms of functions (i.e., hallmarks). One further improvement would be to link the inferred models to hallmarks or pathways. This prior knowledge could also be exploited to improve the reconstructions themselves.\nAnother interesting research direction would be to exploit the reconstructions by CAPRESE or CAPRI for subsequent quantitative estimations of the progression of the disease such as the waiting time between the occurrence of difference alterations and the survival time of cancer patients.\nFinally, as observed multiple times through the thesis, the proposed framework is agnostic of the input data and, hence, it could be adopted also in different contexts. As briefly done in §F with the task of discrimination discovery from data, it would be interesting to explore how the framework proposed in this thesis can be used in different fields of data mining.\n102\nBIBLIOGRAPHY\n[1] R Abdel-Samad, H Zalzali, C Rammah, J Giraud, C Naudin, S Dupasquier, F Poulat, B Boizet-Bonhoure, S Lumbroso, K Mouzat, et al. Minisox9, a dominantnegative variant in colon cancer cells. Oncogene, 30(22):2493–2503, 2011.\n[2] Omar Abdel-Wahab, Mazhar Adli, Lindsay M LaFave, Jie Gao, Todd Hricik, Alan H Shih, Suveg Pandey, Jay P Patel, Young Rock Chung, Richard Koche, et al. Asxl1 mutations promote myeloid transformation through loss of prc2-mediated gene repression. Cancer cell, 22(2):180–193, 2012.\n[3] Adriana Albini and Michael B Sporn. The tumour microenvironment as a target for chemoprevention. Nature Reviews Cancer, 7(2):139–147, 2007.\n[4] Marco Antoniotti, Giulio Caravagna, Luca De Sano, Alex Gradenzi, Ilya Korsunsky, Longoni Mattia, Loes Olde Loohuis, Giancarlo Mauri, Bud Mishra, and Daniele Ramazzotti. The TRONCO package for translational oncology. https: //github.com/BIMIB-DISCo/TRONCO, 2014. Available on Bioconductor.\n[5] Marco Antoniotti, Giulio Caravagna, Luca De Sano, Alex Graudenzi, Giancarlo Mauri, Bud Mishra, and Daniele Ramazzotti. Design of the tronco package for translational oncology. Submitted. Available on bioRxiv.org., 2015.\n[6] Camille Stephan-Otto Attolini, Yu-Kang Cheng, Rameen Beroukhim, Gad Getz, Omar Abdel-Wahab, Ross L Levine, Ingo K Mellinghoff, and Franziska Michor. A mathematical framework to determine the temporal sequence of somatic genetic events in cancer. Proceedings of the National Academy of Sciences, 107(41):17604– 17609, 2010.\n[7] Özgün Babur, Mithat Gönen, Bülent Arman Aksoy, Nikolaus Schultz, Giovanni Ciriello, Chris Sander, and Emek Demir. Systematic identification of cancer driving signaling pathways based on mutual exclusivity of genomic alterations. bioRxiv, page 009878, 2014.\n103\n104 BIBLIOGRAPHY\n[8] Özgün Babur, Mithat Gönen, Bülent Arman Aksoy, Nikolaus Schultz, Giovanni Ciriello, Chris Sander, and Emek Demir. Systematic identification of cancer driving signaling pathways based on mutual exclusivity of genomic alterations. Genome Biology, 16(1), 2015.\n[9] Stephen B Baylin and Peter A Jones. A decade of exploring the cancer epigenomebiological and translational implications. Nature Reviews Cancer, 11(10):726–734, 2011.\n[10] Niko Beerenwinkel, Nicholas Eriksson, and Bernd Sturmfels. Conjunctive bayesian networks. Bernoulli, pages 893–909, 2007.\n[11] Niko Beerenwinkel, Moritz Gerstung, and Seth Sullivant. Hidden conjunctive bayesian networks. https://www1.ethz.ch/bsse/cbg/software/ct-cbn, 20111.\n[12] Niko Beerenwinkel, Jörg Rahnenführer, Rolf Kaiser, Daniel Hoffmann, Joachim Selbig, and Thomas Lengauer. Mtreemix: a software package for learning and using mixture models of mutagenetic trees. Bioinformatics, 21(9):2106–2107, 2005.\n[13] Niko Beerenwinkel, Roland F Schwarz, Moritz Gerstung, and Florian Markowetz. Cancer evolution: mathematical models and computational inference. Systematic biology, 64(1):e1–e25, 2015.\n[14] John M Bennett, Daniel Catovsky, Marie-Theregse Daniel, Georges Flandrin, DAG Galton, H Retal Gralnick, and Claude Sultan. Proposals for the classification of the acute leukaemias french-american-british (fab) co-operative group. British journal of haematology, 33(4):451–458, 1976.\n[15] Rameen Beroukhim, Gad Getz, Leia Nghiemphu, Jordi Barretina, Teli Hsueh, David Linhart, Igor Vivanco, Jeffrey C Lee, Julie H Huang, Sethu Alexander, et al. Assessing the significance of chromosomal aberrations in cancer: methodology and application to glioma. Proceedings of the National Academy of Sciences, 104(50):20007–20012, 2007.\n[16] Peter J Bickel, Eugene A Hammel, J William OConnell, et al. Sex bias in graduate admissions: Data from berkeley. Science, 187(4175):398–404, 1975.\n[17] Francesco Bonchi, Sara Hajian, Bud Mishra, and Daniele Ramazzotti. Exposing the probabilistic causal structure of discrimination. Submitted. Available on arXiv.org., 2015.\n[18] PP Bonissone, M Henrion, LJ Kanal, and JF Lemmer. Equivalence and synthesis of causal models. In Uncertainty in artificial intelligence, volume 6, page 255, 1991.\n[19] Eliot Brenner and David Sontag. Sparsityboost: A new scoring function for learning bayesian network structure. arXiv preprint arXiv:1309.6820, 2013.\n104\nBIBLIOGRAPHY 105\n[20] Rebecca A Burrell, Nicholas McGranahan, Jiri Bartek, and Charles Swanton. The causes and consequences of genetic heterogeneity in cancer evolution. Nature, 501(7467):338–345, 2013.\n[21] Giulio Caravagna, Alex Graudenzi, Daniele Ramazzotti, Rebeca Sanz-Pamplona, Luca De Sano, Giancarlo Mauri, Victor Moreno, Marco Antoniotti, and Bud Mishra. Algorithmic methods to infer the evolutionary trajectories in cancer progression. Submitted. Available on bioRxiv.org., 2015.\n[22] Nancy Cartwright. Causal laws and effective strategies. Nous, pages 419–437, 1979.\n[23] Alexandra M Carvalho. Scoring functions for learning bayesian networks. Inesc-id Tec. Rep, 2009.\n[24] Ethan Cerami, Jianjiong Gao, Ugur Dogrusoz, Benjamin E Gross, Selcuk Onur Sumer, Bülent Arman Aksoy, Anders Jacobsen, Caitlin J Byrne, Michael L Heuer, Erik Larsson, et al. The cbio cancer genomics portal: an open platform for exploring multidimensional cancer genomics data. Cancer discovery, 2(5):401–404, 2012.\n[25] Yu-Kang Cheng, Rameen Beroukhim, Ross L Levine, Ingo K Mellinghoff, Eric C Holland, Franziska Michor, et al. A mathematical methodology for determining the temporal order of pathway alterations arising during gliomagenesis. PLoS Comput Biol, 8(1):e1002337, 2012.\n[26] David Maxwell Chickering. Learning bayesian networks is np-complete. In Learning from data, pages 121–130. Springer, 1996.\n[27] David Maxwell Chickering, David Heckerman, and Christopher Meek. Largesample learning of bayesian networks is np-hard. The Journal of Machine Learning Research, 5:1287–1330, 2004.\n[28] Giovanni Ciriello, Ethan Cerami, Chris Sander, and Nikolaus Schultz. Mutual exclusivity analysis identifies oncogenic network modules. Genome research, 22(2):398–406, 2012.\n[29] 1000 Genomes Project Consortium et al. An integrated map of genetic variation from 1,092 human genomes. Nature, 491(7422):56–65, 2012.\n[30] Christina Curtis, Sohrab P Shah, Suet-Feung Chin, Gulisa Turashvili, Oscar M Rueda, Mark J Dunning, Doug Speed, Andy G Lynch, Shamith Samarajiwa, Yinyin Yuan, et al. The genomic and transcriptomic architecture of 2,000 breast tumours reveals novel subgroups. Nature, 486(7403):346–352, 2012.\n[31] BHM Custers, T Calders, BW Schermer, and TZ Zarsky. Discrimination and privacy in the information society. vol. 3 of studies in applied philosophy, epistemology and rational ethics, 2013.\n105\n106 BIBLIOGRAPHY\n[32] Marilyn Dabady, Rebecca M Blank, Constance F Citro, et al. Measuring racial discrimination. National Academies Press, 2004.\n[33] Luca De Sano, Giulio Caravagna, Daniele Ramazzotti, Alex Graudenzi, Giancarlo Mauri, Bud Mishra, and Marco Antoniotti. Tronco: An r package for the inference of cancer progression models from heterogeneous genomic data. Submitted. Available on bioRxiv.org., 2015.\n[34] Marcilio CP de Souto, Ivan G Costa, Daniel SA de Araujo, Teresa B Ludermir, and Alexander Schliep. Clustering cancer gene expression data: a comparative study. BMC bioinformatics, 9(1):497, 2008.\n[35] Nathan D Dees, Qunyuan Zhang, Cyriac Kandoth, Michael C Wendl, William Schierding, Daniel C Koboldt, Thomas B Mooney, Matthew B Callaway, David Dooling, Elaine R Mardis, et al. Music: identifying mutational significance in cancer genomes. Genome research, 22(8):1589–1598, 2012.\n[36] Dustin A Deming, Alyssa A Leystra, Laura Nettekoven, Chelsea Sievers, Devon Miller, Malisa Middlebrooks, Linda Clipson, Dawn Albrecht, Jeff Bacher, MK Washington, et al. Pik3ca and apc mutations are synergistic in the development of intestinal cancers. Oncogene, 33(17):2245–2254, 2014.\n[37] Mark A DePristo, Eric Banks, Ryan Poplin, Kiran V Garimella, Jared R Maguire, Christopher Hartl, Anthony A Philippakis, Guillermo Del Angel, Manuel A Rivas, Matt Hanna, et al. A framework for variation discovery and genotyping using next-generation dna sequencing data. Nature genetics, 43(5):491–498, 2011.\n[38] Richard Desper, Feng Jiang, Olli-P Kallioniemi, Holger Moch, Christos H Papadimitriou, and Alejandro A Schäffer. Inferring tree models for oncogenesis from comparative genome hybridization data. Journal of computational biology, 6(1):37– 51, 1999.\n[39] Richard Desper, Feng Jiang, Olli-P Kallioniemi, Holger Moch, Christos H Papadimitriou, and Alejandro A Schäffer. Distance-based reconstruction of tree models for oncogenesis. Journal of Computational Biology, 7(6):789–803, 2000.\n[40] Daniel L Dexter, Henryk M Kowalski, Beverly A Blazar, Zuzana Fligiel, Renee Vogel, and Gloria H Heppner. Heterogeneity of tumor cells from a single mouse mammary tumor. Cancer research, 38(10):3174–3181, 1978.\n[41] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 214–226. ACM, 2012.\n[42] James Eberwine, Jai-Yoon Sul, Tamas Bartfai, and Junhyong Kim. The promise of single-cell sequencing. Nature methods, 11(1):25–27, 2014.\n106\nBIBLIOGRAPHY 107\n[43] Jack Edmonds. Optimum branchings. Journal of Research of the National Bureau of Standards B, 71(4):233–240, 1967.\n[44] Ellery Eells. Probabilistic causality, volume 1. Cambridge University Press, 1991.\n[45] Bradley Efron. Large-scale inference: empirical Bayes methods for estimation, testing, and prediction, volume 1. Cambridge University Press, 2010.\n[46] Bradley Efron and B Efron. The jackknife, the bootstrap and other resampling plans, volume 38. SIAM, 1982.\n[47] Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC press, 1994.\n[48] Mohammed El-Kebir, Layla Oesper, Hannah Acheson-Field, and Benjamin J Raphael. Reconstruction of clonal trees and tumor composition from multi-sample sequencing data. Bioinformatics, 31(12):i62–i70, 2015.\n[49] Evelyn Ellis and Philippa Watson. EU anti-discrimination law. OUP Oxford, 2012.\n[50] Hossein Shahrabi Farahani and Jens Lagergren. Learning oncogenetic networks by reducing to mixed integer linear programming. 2013.\n[51] Eric R Fearon and Bert Vogelstein. A genetic model for colorectal tumorigenesis. Cell, 61(5):759–767, 1990.\n[52] Isaiah J Fidler. Tumor heterogeneity and the biology of cancer invasion and metastasis. Cancer research, 38(9):2651–2660, 1978.\n[53] Maria E Figueroa, Omar Abdel-Wahab, Chao Lu, Patrick S Ward, Jay Patel, Alan Shih, Yushan Li, Neha Bhagwat, Aparna Vasanthakumar, Hugo F Fernandez, et al. Leukemic idh1 and idh2 mutations result in a hypermethylation phenotype, disrupt tet2 function, and impair hematopoietic differentiation. Cancer cell, 18(6):553–567, 2010.\n[54] Andrej Fischer, Ignacio Vázquez-Garćıa, Christopher JR Illingworth, and Ville Mustonen. High-definition reconstruction of clonal composition in cancer. Cell reports, 7(5):1740–1752, 2014.\n[55] R Fisher, L Pusztai, and C Swanton. Cancer heterogeneity: implications for targeted therapeutics. British journal of cancer, 108(3):479–485, 2013.\n[56] Sheila R Foster. Causation in antidiscrimination law: Beyond intent versus impact. Hous. L. Rev., 41:1469, 2004.\n[57] David Freedman, Robert Pisani, and Roger Purves. Statistics (3rd edn), 1998.\n107\n108 BIBLIOGRAPHY\n[58] P Andrew Futreal, Lachlan Coin, Mhairi Marshall, Thomas Down, Timothy Hubbard, Richard Wooster, Nazneen Rahman, and Michael R Stratton. A census of human cancer genes. Nature Reviews Cancer, 4(3):177–183, 2004.\n[59] Yuan Gao and George Church. Improving molecular cancer class discovery through sparse non-negative matrix factorization. Bioinformatics, 21(21):3970–3975, 2005.\n[60] Levi A Garraway and Eric S Lander. Lessons from the cancer genome. Cell, 153(1):17–37, 2013.\n[61] Véronique Gelsi-Boyer, Virginie Trouplin, José Adéläıde, Julien Bonansea, Nathalie Cervera, Nadine Carbuccia, Arnaud Lagarde, Thomas Prebet, Meyer Nezri, Danielle Sainty, et al. Mutations of polycomb-associated gene asxl1 in myelodysplastic syndromes and chronic myelomonocytic leukaemia. British journal of haematology, 145(6):788–800, 2009.\n[62] Marco Gerlinger, Stuart Horswell, James Larkin, Andrew J Rowan, Max P Salm, Ignacio Varela, Rosalie Fisher, Nicholas McGranahan, Nicholas Matthews, Claudio R Santos, et al. Genomic architecture and evolution of clear cell renal cell carcinomas defined by multiregion sequencing. Nature genetics, 46(3):225–233, 2014.\n[63] Marco Gerlinger, Andrew J Rowan, Stuart Horswell, James Larkin, David Endesfelder, Eva Gronroos, Pierre Martinez, Nicholas Matthews, Aengus Stewart, Patrick Tarpey, et al. Intratumor heterogeneity and branched evolution revealed by multiregion sequencing. New England Journal of Medicine, 366(10):883–892, 2012.\n[64] Amy V Gerstein, Teresa Acosta Almeida, Guojing Zhao, Eric Chess, Ie-Ming Shih, Kent Buhler, Kenneth Pienta, Mark A Rubin, Robert Vessella, and Nickolas Papadopoulos. Apc/ctnnb1 (β-catenin) pathway alterations in human prostate cancers. Genes, Chromosomes and Cancer, 34(1):9–16, 2002.\n[65] Moritz Gerstung, Michael Baudis, Holger Moch, and Niko Beerenwinkel. Quantifying cancer progression with conjunctive bayesian networks. Bioinformatics, 25(21):2809–2815, 2009.\n[66] Abel Gonzalez-Perez and Nuria Lopez-Bigas. Functional impact bias reveals cancer drivers. Nucleic acids research, page gks743, 2012.\n[67] Catherine S Grasso, Yi-Mi Wu, Dan R Robinson, Xuhong Cao, Saravana M Dhanasekaran, Amjad P Khan, Michael J Quist, Xiaojun Jing, Robert J Lonigro, J Chad Brenner, et al. The mutational landscape of lethal castration-resistant prostate cancer. Nature, 487(7406):239–243, 2012.\n[68] Mel Greaves and Carlo C Maley. Clonal evolution in cancer. Nature, 481(7381):306–313, 2012.\n108\nBIBLIOGRAPHY 109\n[69] B Gunawan, A Von Heydebreck, B Sander, H-J Schulten, F Haller, C Langer, T Armbrust, M Bollmann, S Gašparov, D Kovač, et al. An oncogenetic tree model in gastrointestinal stromal tumours (gists) identifies different pathways of cytogenetic evolution with prognostic implications. The Journal of pathology, 211(4):463– 470, 2007.\n[70] Gunes Gundem, Christian Perez-Llamas, Alba Jene-Sanz, Anna Kedzierska, Abul Islam, Jordi Deu-Pons, Simon J Furney, and Nuria Lopez-Bigas. Intogen: integration and data mining of multidimensional oncogenomic data. Nature methods, 7(2):92–93, 2010.\n[71] Anupam Gupta and Ziv Bar-Joseph. Extracting dynamics from static cancer expression data. Computational Biology and Bioinformatics, IEEE/ACM Transactions on, 5(2):172–182, 2008.\n[72] Katrin Hainke, Jörg Rahnenführer, and Roland Fried. Cumulative disease progression models for cross-sectional data: A review and comparison. Biometrical Journal, 54(5):617–640, 2012.\n[73] Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination prevention in data mining. Knowledge and Data Engineering, IEEE Transactions on, 25(7):1445–1459, 2013.\n[74] Sara Hajian, Josep Domingo-Ferrer, Anna Monreale, Dino Pedreschi, and Fosca Giannotti. Discrimination-and privacy-aware patterns. Data Mining and Knowledge Discovery, pages 1–50, 2014.\n[75] John Burdon Sanderson Haldane. The causes of evolution. Princeton University Press, 1990.\n[76] Richard W Hamming. Error detecting and error correcting codes. Bell System technical journal, 29(2):147–160, 1950.\n[77] Douglas Hanahan and Robert A Weinberg. The hallmarks of cancer. cell, 100(1):57–70, 2000.\n[78] Douglas Hanahan and Robert A Weinberg. Hallmarks of cancer: the next generation. cell, 144(5):646–674, 2011.\n[79] David Heckerman, Dan Geiger, and David M Chickering. Learning bayesian networks: The combination of knowledge and statistical data. Machine learning, 20(3):197–243, 1995.\n[80] Christopher Hitchcock. Probabilistic causation. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Winter 2012 edition, 2012.\n[81] Marcus Hjelm, Mattias Höglund, and Jens Lagergren. New probabilistic network models and algorithms for oncogenesis. Journal of Computational Biology, 13(4):853–865, 2006.\n109\n110 BIBLIOGRAPHY\n[82] Matan Hofree, John P Shen, Hannah Carter, Andrew Gross, and Trey Ideker. Network-based stratification of tumor mutations. Nature methods, 10(11):1108– 1115, 2013.\n[83] Xuehui Hong, Wenyu Liu, Hiroyuki Inuzuka, Lianxin Liu, and Sharon R Pine. Negative regulation of sox9 by glycogen synthase kinase 3 beta phosphorylation and scffbw7-dependent ubiquitination in cancer. Cancer Research, 75(15 Supplement):1957–1957, 2015.\n[84] Xing Hua, Paula L Hyland, Jing Huang, Bin Zhu, Neil E Caporaso, Maria Teresa Landi, Nilanjan Chatterjee, Jianxin Shi, and Jianxin Shi. Megsa: A powerful and flexible framework for analyzing mutual exclusivity of tumor mutations. bioRxiv, page 017731, 2015.\n[85] Qiang Huang, Guo Pei Yu, Steven A McCormick, Juan Mo, Bhakti Datta, Manoj Mahimkar, Philip Lazarus, Alejandro A Schäffer, Richard Desper, and Stimson P Schantz. Genetic differences detected by comparative genomic hybridization in head and neck squamous cell carcinomas from different tumor sites: construction of oncogenetic trees for tumor progression. Genes, Chromosomes and Cancer, 34(2):224–233, 2002.\n[86] Sui Huang, Ingemar Ernberg, and Stuart Kauffman. Cancer attractors: a systems view of tumors from a gene network dynamics and developmental perspective. In Seminars in cell & developmental biology, volume 20, pages 869–876. Elsevier, 2009.\n[87] David Hume et al. An enquiry concerning human understanding. Alex Catalogue, 1965.\n[88] Phyllis McKay Illari, Federica Russo, and Jon Williamson. Causality in the Sciences. OUP Oxford, 2011.\n[89] Daichi Inoue, J Kitaura, H Matsui, HA Hou, WC Chou, A Nagamachi, Kimihito Cojin Kawabata, Katsuhiro Togami, Reina Nagase, Sayuri Horikawa, et al. Setbp1 mutations drive leukemic transformation in asxl1-mutated mds. Leukemia, 2014.\n[90] Glen Jeh and Jennifer Widom. Scaling personalized web search. In Proceedings of the 12th international conference on World Wide Web, pages 271–279. ACM, 2003.\n[91] Wei Jiao, Shankar Vembu, Amit G Deshwar, Lincoln Stein, and Quaid Morris. Inferring clonal evolution of tumors from single nucleotide somatic mutations. BMC bioinformatics, 15(1):35, 2014.\n[92] Siân Jones, Xiaosong Zhang, D Williams Parsons, Jimmy Cheng-Ho Lin, Rebecca J Leary, Philipp Angenendt, Parminder Mankoo, Hannah Carter, Hirohiko\n110\nBIBLIOGRAPHY 111\nKamiyama, Antonio Jimeno, et al. Core signaling pathways in human pancreatic cancers revealed by global genomic analyses. science, 321(5897):1801–1806, 2008.\n[93] Tommi Kainu, Suh-Hang Hank Juo, Richard Desper, Alejandro A Schäffer, Elizabeth Gillanders, Ester Rozenblum, Diana Freas-Lutz, Don Weaver, Dietrich Stephan, Joan Bailey-Wilson, et al. Somatic deletions in hereditary breast cancers implicate 13q21 as a putative novel breast cancer susceptibility locus. Proceedings of the National Academy of Sciences, 97(17):9603–9608, 2000.\n[94] Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1–33, 2012.\n[95] Jung Ho Kim and Gyeong Hoon Kang. Molecular and prognostic heterogeneity of microsatellite-unstable colorectal cancer. World journal of gastroenterology: WJG, 20(15):4230, 2014.\n[96] Min Sung Kim, Sung Soo Kim, Chang Hyeok Ahn, Nam Jin Yoo, and Sug Hyung Lee. Frameshift mutations of wnt pathway genes axin2 and tcf7l2 in gastric carcinomas with high microsatellite instability. Human pathology, 40(1):58–64, 2009.\n[97] Scott Kirkpatrick. Optimization by simulated annealing: Quantitative studies. Journal of statistical physics, 34(5-6):975–986, 1984.\n[98] Samantha Kleinberg. Causality, probability, and time. Cambridge University Press, 2012.\n[99] D Klingbiel, Z Saridaki, AD Roth, FT Bosman, M Delorenzi, and Sabine Tejpar. Prognosis of stage ii and iii colon cancer treated with adjuvant 5-fluorouracil or folfiri in relation to microsatellite status: results of the petacc-3 trial. Annals of Oncology, 26(1):126–132, 2015.\n[100] Turid Knutsen, Vasuki Gobu, Rodger Knaus, Hesed Padilla-Nash, Meena Augustus, Robert L Strausberg, Ilan R Kirsch, Karl Sirotkin, and Thomas Ried. The interactive online sky/m-fish & cgh database and the entrez cancer chromosomes search database: Linkage of chromosomal aberrations with the genome sequence. Genes, Chromosomes and Cancer, 44(1):52–64, 2005.\n[101] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.\n[102] Panagiotis A Konstantinopoulos, Dimitrios Spentzos, Beth Y Karlan, Toshiyasu Taniguchi, Elena Fountzilas, Nancy Francoeur, Douglas A Levine, and Stephen A Cannistra. Gene expression profile of brcaness that correlates with responsiveness to chemotherapy and with outcome in patients with epithelial ovarian cancer. Journal of Clinical Oncology, 28(22):3555–3561, 2010.\n111\n112 BIBLIOGRAPHY\n[103] Jay D Kormish, Débora Sinner, and Aaron M Zorn. Interactions between sox factors and wnt/β-catenin signaling in development and disease. Developmental Dynamics, 239(1):56–68, 2010.\n[104] Henry E Kyburg. Salmon’s paper. Philosophy of Science, pages 147–151, 1965.\n[105] Ben Langmead and Steven L Salzberg. Fast gapped-read alignment with bowtie 2. Nature methods, 9(4):357–359, 2012.\n[106] Michael S Lawrence, Petar Stojanov, Paz Polak, Gregory V Kryukov, Kristian Cibulskis, Andrey Sivachenko, Scott L Carter, Chip Stewart, Craig H Mermel, Steven A Roberts, et al. Mutational heterogeneity in cancer and the search for new cancer-associated genes. Nature, 499(7457):214–218, 2013.\n[107] Australian Legislation. Equal opportunity act–victoria state,(b) antidiscrimination act–queensland state, 2008.\n[108] US Federal Legislation. Us federal legislation, (a) equal credit opportunity act, (b) fair housing act, (c) intentional employment discrimination, (d) equal pay act, (e) pregnancy discrimination act, (f) civil right act., 2010.\n[109] European Parliament Legislative resolution. Eu legislation, (a) race equality directive, 2000/43/ec, 2000; (b) employment equality directive, 2000/78/ec, 2000; (c) equal treatment of persons, 2009.\n[110] Mark DM Leiserson, Dima Blokh, Roded Sharan, and Benjamin J Raphael. Simultaneous identification of multiple driver pathways in cancer. 2013.\n[111] Mark DM Leiserson, Fabio Vandin, Hsin-Ta Wu, Jason R Dobson, Jonathan V Eldridge, Jacob L Thomas, Alexandra Papoutsaki, Younhun Kim, Beifang Niu, Michael McLellan, et al. Pan-cancer network analysis identifies combinations of rare somatic mutations across pathways and protein complexes. Nature genetics, 47(2):106–114, 2015.\n[112] Mark DM Leiserson, Hsin-Ta Wu, Fabio Vandin, and Benjamin J Raphael. Comet: A statistical approach to identify combinations of mutually exclusive alterations in cancer. In Research in Computational Molecular Biology, pages 202–204. Springer, 2015.\n[113] David Lewis. Causation. The journal of philosophy, pages 556–567, 1973.\n[114] Lihua Li, Aaron L Sarver, Rohini Khatri, Praveensingh B Hajeri, Iris Kamenev, Amy J French, Stephen N Thibodeau, Clifford J Steer, and Subbaya Subramanian. Sequential expression of mir-182 and mir-503 cooperatively targets fbxw7, contributing to the malignant transformation of colon adenoma to adenocarcinoma. The Journal of pathology, 234(4):488–501, 2014.\n112\nBIBLIOGRAPHY 113\n[115] Chien-Chin Lin, Hsin-An Hou, Wen-Chien Chou, Yuan-Yeh Kuo, Shang-Ju Wu, Chieh-Yu Liu, Chien-Yuan Chen, Mei-Hsuan Tseng, Chi-Fei Huang, Fen-Yu Lee, et al. Sf3b1 mutations in patients with myelodysplastic syndromes: the mutation is stable during disease evolution. American journal of hematology, 89(8):E109– E115, 2014.\n[116] Thomas Longerich, Michael Martin Mueller, Kai Breuhahn, Peter Schirmacher, Axel Benner, and Christiane Heiss. Oncogenetic tree modeling of human hepatocarcinogenesis. International Journal of Cancer, 130(3):575–583, 2012.\n[117] Loes Olde Loohuis, Giulio Caravagna, Alex Graudenzi, Daniele Ramazzotti, Giancarlo Mauri, Marco Antoniotti, and Bud Mishra. Inferring tree causal models of cancer progression with probability raising. PLoS ONE, 9(12):e115570, 2014.\n[118] Loes Olde Loohuis, Andreas Witzel, and Bud Mishra. Cancer hybrid automata: model, beliefs and therapy. Information and Computation, 236:68–86, 2014.\n[119] Jun Lu, Gad Getz, Eric A Miska, Ezequiel Alvarez-Saavedra, Justin Lamb, David Peck, Alejandro Sweet-Cordero, Benjamin L Ebert, Raymond H Mak, Adolfo A Ferrando, et al. Microrna expression profiles classify human cancers. nature, 435(7043):834–838, 2005.\n[120] Binh Thanh Luong, Salvatore Ruggieri, and Franco Turini. k-nn as an implementation of situation testing for discrimination discovery and prevention. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 502–510. ACM, 2011.\n[121] Paul M Magwene, Paul Lizardi, and Junhyong Kim. Reconstructing the temporal ordering of biological samples using microarray data. Bioinformatics, 19(7):842– 850, 2003.\n[122] Salem Malikic, Andrew W McPherson, Nilgun Donmez, and Cenk S Sahinalp. Clonality inference in multiple tumor samples using phylogeny. Bioinformatics, 31(9):1349–1356, 2015.\n[123] Koray Mancuhan and Chris Clifton. Combating discrimination using bayesian networks. Artificial intelligence and law, 22(2):211–238, 2014.\n[124] Dimitris Margaritis. Learning Bayesian network model structure from data. PhD thesis, US Army, 2003.\n[125] Sanford D Markowitz and Monica M Bertagnolli. Molecular basis of colorectal cancer. New England Journal of Medicine, 361(25):2449–2460, 2009.\n[126] M Meggendorfer, U Bacher, T Alpermann, C Haferlach, W Kern, C GambacortiPasserini, T Haferlach, and S Schnittger. Setbp1 mutations occur in 9&percnt; of mds/mpn and in 4&percnt; of mpn cases and are strongly associated with atypical\n113\n114 BIBLIOGRAPHY\ncml, monosomy 7, isochromosome i (17)(q10), asxl1 and cbl mutations. Leukemia, 27(9):1852–1860, 2013.\n[127] Peter Menzies. Counterfactual theories of causation. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Spring 2014 edition, 2014.\n[128] Lauren MF Merlo, John W Pepper, Brian J Reid, and Carlo C Maley. Cancer as an evolutionary and ecological process. Nature Reviews Cancer, 6(12):924–935, 2006.\n[129] Christopher A Miller, Stephen H Settle, Erik P Sulman, Kenneth D Aldape, and Aleksandar Milosavljevic. Discovering functional modules by identifying recurrent and mutually exclusive mutational patterns in tumors. BMC medical genomics, 4(1):34, 2011.\n[130] Christopher A Miller, Brian S White, Nathan D Dees, Malachi Griffith, John S Welch, Obi L Griffith, Ravi Vij, Michael H Tomasson, Timothy A Graubert, Matthew J Walter, et al. Sciclone: inferring clonal architecture and tracking the spatial and temporal patterns of tumor evolution. 2014.\n[131] Navodit Misra, Ewa Szczurek, and Martin Vingron. Inferring the paths of somatic evolution in cancer. Bioinformatics, page btu319, 2014.\n[132] Tood K Moon. The expectation-maximization algorithm. Signal processing magazine, IEEE, 13(6):47–60, 1996.\n[133] Tomoya Muto, Goro Sashida, Motohiko Oshima, George R Wendt, Makiko Mochizuki-Kashio, Yasunobu Nagata, Masashi Sanada, Satoru Miyagi, Atsunori Saraya, Asuka Kamio, et al. Concurrent loss of ezh2 and tet2 cooperates in the pathogenesis of myelodysplastic disorders. The Journal of experimental medicine, 210(12):2627–2639, 2013.\n[134] Nicholas Navin, Jude Kendall, Jennifer Troge, Peter Andrews, Linda Rodgers, Jeanne McIndoo, Kerry Cook, Asya Stepansky, Dan Levy, Diane Esposito, et al. Tumour evolution inferred by single-cell sequencing. Nature, 472(7341):90–94, 2011.\n[135] Nicholas E Navin. Cancer genomics: one cell at a time. Genome Biol, 15:452, 2014.\n[136] NCI and the NHGRI. The cancer genome atlas. http://cancergenome.nih.gov/, 2005.\n[137] Cancer Genome Atlas Network et al. Comprehensive molecular characterization of human colon and rectal cancer. Nature, 487(7407):330–337, 2012.\n[138] Cancer Genome Atlas Research Network et al. Integrated genomic analyses of ovarian carcinoma. Nature, 474(7353):609–615, 2011.\n114\nBIBLIOGRAPHY 115\n[139] Cancer Genome Atlas Research Network et al. Genomic and epigenomic landscapes of adult de novo acute myeloid leukemia. The New England journal of medicine, 368(22):2059, 2013.\n[140] Sarah B Ng, Emily H Turner, Peggy D Robertson, Steven D Flygare, Abigail W Bigham, Choli Lee, Tristan Shaffer, Michelle Wong, Arindam Bhattacharjee, Evan E Eichler, et al. Targeted capture and massively parallel sequencing of 12 human exomes. Nature, 461(7261):272–276, 2009.\n[141] Martin A Nowak. Evolutionary dynamics. Harvard University Press, 2006.\n[142] Martin A Nowak, Franziska Michor, and Yoh Iwasa. The linear process of somatic evolution. Proceedings of the national academy of sciences, 100(25):14966–14969, 2003.\n[143] Peter C Nowell. The clonal evolution of tumor cell populations. Science, 194(4260):23–28, 1976.\n[144] Layla Oesper, Ahmad Mahmoody, and Benjamin J Raphael. Theta: inferring intra-tumor heterogeneity from high-throughput dna sequencing data. Genome Biol, 14(7):R80, 2013.\n[145] Layla Oesper, Gryte Satas, and Benjamin J Raphael. Quantifying tumor heterogeneity in whole-genome and whole-exome sequencing data. Bioinformatics, 30(24):3532–3540, 2014.\n[146] Shuji Ogino and Ajay Goel. Molecular classification and correlates in colorectal cancer. The Journal of Molecular Diagnostics, 10(1):13–27, 2008.\n[147] D Williams Parsons, Siân Jones, Xiaosong Zhang, Jimmy Cheng-Ho Lin, Rebecca J Leary, Philipp Angenendt, Parminder Mankoo, Hannah Carter, I-Mei Siu, Gary L Gallia, et al. An integrated genomic analysis of human glioblastoma multiforme. Science, 321(5897):1807–1812, 2008.\n[148] Swapnali Pathare, Alejandro A Schäffer, Niko Beerenwinkel, and Manoj Mahimkar. Construction of oncogenetic tree models reveals multiple pathways of oral cancer progression. International journal of cancer, 124(12):2864–2871, 2009.\n[149] Timothy M Pawlik, Chandrajit P Raut, and Miguel A Rodriguez-Bigas. Colorectal carcinogenesis: Msi-h versus msi-l. Disease markers, 20(4-5):199–206, 2004.\n[150] Judea Pearl. Causality: models, reasoning, and inference. Econometric Theory, 19:675–685, 2003.\n[151] Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, 2014.\n115\n116 BIBLIOGRAPHY\n[152] Dino Pedreschi, Salvatore Ruggieri, and Franco Turini. Integrating induction and deduction for finding evidence of discrimination. In Proceedings of the 12th International Conference on Artificial Intelligence and Law, pages 157–166. ACM, 2009.\n[153] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 560–568. ACM, 2008.\n[154] Rocco Piazza, Simona Valletta, Nils Winkelmann, Sara Redaelli, Roberta Spinelli, Alessandra Pirola, Laura Antolini, Luca Mologni, Carla Donadoni, Elli Papaemmanuil, et al. Recurrent setbp1 mutations in atypical chronic myeloid leukemia. Nature genetics, 45(1):18–24, 2013.\n[155] Pascal Pons and Matthieu Latapy. Computing communities in large networks using random walks. In Computer and Information Sciences-ISCIS 2005, pages 284–293. Springer, 2005.\n[156] Paul Walton Purdom Jr, Phillip G Bradford, Koichiro Tamura, and Sudhir Kumar. Single column discrepancy and dynamic max-mini optimizations for quickly finding the most parsimonious evolutionary trees. Bioinformatics, 16(2):140–151, 2000.\n[157] Michael D Radmacher, Richard Simon, Richard Desper, Raymond Taetle, Alejandro A Schäffer, and Mark A Nelson. Graph models of oncogenesis with an application to melanoma. Journal of theoretical biology, 212(4):535–548, 2001.\n[158] Daniele Ramazzotti, Giulio Caravagna, Loes Olde Loohuis, Alex Graudenzi, Ilya Korsunsky, Giancarlo Mauri, Marco Antoniotti, and Bud Mishra. Capri: Efficient inference of cancer progression models from cross-sectional data. Bioinformatics, 31(18):btv296, 2015.\n[159] Shyam Ramchandani, Sanjoy K Bhattacharya, Nadia Cervoni, and Moshe Szyf. Dna methylation is a reversible biological signal. Proceedings of the National Academy of Sciences, 96(11):6107–6112, 1999.\n[160] Hans Reichenbach and Maria Reichenbach. The direction of time, volume 65. Univ of California Press, 1991.\n[161] Jorge S Reis-Filho and Lajos Pusztai. Gene expression profiling in breast cancer: classification, prognostication, and prediction. The Lancet, 378(9805):1812–1823, 2011.\n[162] Andrea Romei and Salvatore Ruggieri. A multidisciplinary survey on discrimination analysis. The Knowledge Engineering Review, 29(05):582–638, 2014.\n[163] Isabelle Rorive. Proving discrimination cases: The role of situation testing. 2009.\n116\nBIBLIOGRAPHY 117\n[164] Andrew Roth, Jaswinder Khattra, Damian Yap, Adrian Wan, Emma Laks, Justina Biele, Gavin Ha, Samuel Aparicio, Alexandre Bouchard-Côté, and Sohrab P Shah. Pyclone: statistical inference of clonal population structure in cancer. Nature methods, 11(4):396–398, 2014.\n[165] Salvatore Ruggieri, Dino Pedreschi, and Franco Turini. Data mining for discrimination discovery. ACM Transactions on Knowledge Discovery from Data (TKDD), 4(2):9, 2010.\n[166] Emma Samuelson, Sara Karlsson, Karolina Partheen, Staffan Nilsson, Claude Szpirer, and Afrouz Behboudi. Bac cgh-array identified specific small-scale genomic imbalances in diploid dmba-induced rat mammary tumors. BMC cancer, 12(1):352, 2012.\n[167] Gideon Schwarz et al. Estimating the dimension of a model. The annals of statistics, 6(2):461–464, 1978.\n[168] Marco Scutari. Learning bayesian networks with the bnlearn r package. arXiv preprint arXiv:0908.3817, 2009.\n[169] Stephen T Sherry, M-H Ward, M Kholodov, J Baker, Lon Phan, Elizabeth M Smigielski, and Karl Sirotkin. dbsnp: the ncbi database of genetic variation. Nucleic acids research, 29(1):308–311, 2001.\n[170] Brian Skyrms. Causal Necessity: A Pragmatic Investigation of the Necessity of Laws. Yale University Press, 1980.\n[171] Peter Spirtes, Clark N Glymour, and Richard Scheines. Causation, prediction, and search, volume 81. MIT press, 2000.\n[172] Patrick Suppes. A probabilistic theory of causality. North-Holland Publishing Company Amsterdam, 1970.\n[173] Aniko Szabo and Kenneth Boucher. Estimating an oncogenetic tree when false negatives and positives are present. Mathematical biosciences, 176(2):219–236, 2002.\n[174] Aniko Szabo and Kenneth M Boucher. Oncogenetic trees. Handbook of cancer models with applications, pages 1–24, 2008.\n[175] Ewa Szczurek and Niko Beerenwinkel. Modeling mutual exclusivity of cancer mutations. In Research in Computational Molecular Biology, pages 307–308. Springer, 2014.\n[176] David Tamborero, Abel Gonzalez-Perez, and Nuria Lopez-Bigas. Oncodriveclust: exploiting the positional clustering of somatic mutations to identify cancer genes. Bioinformatics, 29(18):2238–2244, 2013.\n117\n118 BIBLIOGRAPHY\n[177] David Tamborero, Abel Gonzalez-Perez, Christian Perez-Llamas, Jordi Deu-Pons, Cyriac Kandoth, Jüri Reimand, Michael S Lawrence, Gad Getz, Gary D Bader, Li Ding, et al. Comprehensive identification of mutational cancer driver genes across 12 tumor types. Scientific reports, 3, 2013.\n[178] David Tamborero, Nuria Lopez-Bigas, and Abel Gonzalez-Perez. Oncodrive-cis: a method to reveal likely driver genes based on the impact of their copy number changes on expression. PloS one, 8(2):e55489, 2013.\n[179] Ioannis Tsamardinos, Constantin F Aliferis, Alexander R Statnikov, and Er Statnikov. Algorithms for large scale markov blanket discovery. In FLAIRS Conference, volume 2, 2003.\n[180] Dino Pedreschi Salvatore Ruggieri Franco Turini. Measuring discrimination in socially-sensitive decision records. 2009.\n[181] Fabio Vandin, Eli Upfal, and Benjamin J Raphael. Algorithms for detecting significantly mutated pathways in cancer. Journal of Computational Biology, 18(3):507– 522, 2011.\n[182] Fabio Vandin, Eli Upfal, and Benjamin J Raphael. De novo discovery of mutated driver pathways in cancer. Genome research, 22(2):375–385, 2012.\n[183] Eduardo Vilar and Stephen B Gruber. Microsatellite instability in colorectal cancerthe stable evidence. Nature reviews Clinical oncology, 7(3):153–162, 2010.\n[184] Bert Vogelstein, Eric R Fearon, Stanley R Hamilton, Scott E Kern, Ann C Preisinger, Mark Leppert, Alida MM Smits, and Johannes L Bos. Genetic alterations during colorectal-tumor development. New England Journal of Medicine, 319(9):525–532, 1988.\n[185] Bert Vogelstein and Kenneth W Kinzler. Cancer genes and the pathways they control. Nature medicine, 10(8):789–799, 2004.\n[186] Bert Vogelstein, Nickolas Papadopoulos, Victor E Velculescu, Shibin Zhou, Luis A Diaz, and Kenneth W Kinzler. Cancer genome landscapes. science, 339(6127):1546–1558, 2013.\n[187] Axel Walther, Elaine Johnstone, Charles Swanton, Rachel Midgley, Ian Tomlinson, and David Kerr. Genetic prognostic and predictive markers in colorectal cancer. Nature Reviews Cancer, 9(7):489–499, 2009.\n[188] Yong Wang, Jill Waters, Marco L Leung, Anna Unruh, Whijae Roh, Xiuqing Shi, Ken Chen, Paul Scheet, Selina Vattathil, Han Liang, et al. Clonal evolution in breast cancer revealed by single nucleus genome sequencing. Nature, 512(7513):155–160, 2014.\n118\n[189] Janindra Warusavitarne and Margaret Schnitzler. The role of chemotherapy in microsatellite unstable (msi-h) colorectal cancer. International journal of colorectal disease, 22(7):739–748, 2007.\n[190] Robert Weinberg. The biology of cancer. Garland Science, 2013.\n[191] Laura D Wood, D Williams Parsons, Siân Jones, Jimmy Lin, Tobias Sjöblom, Rebecca J Leary, Dong Shen, Simina M Boca, Thomas Barber, Janine Ptak, et al. The genomic landscapes of human breast and colorectal cancers. Science, 318(5853):1108–1113, 2007.\n[192] James Woodward. Causation and manipulability. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Winter 2013 edition, 2014.\n[193] Wen Xue, Thomas Kitzing, Stephanie Roessler, Johannes Zuber, Alexander Krasnitz, Nikolaus Schultz, Kate Revill, Susann Weissmueller, Amy R Rappaport, Janelle Simon, et al. A cluster of cooperating tumor-suppressor gene candidates in chromosomal deletions. Proceedings of the National Academy of Sciences, 109(21):8212–8217, 2012.\n[194] Chen-Hsiang Yeang, Frank McCormick, and Arnold Levine. Combinatorial patterns of somatic gene mutations in cancer. The FASEB Journal, 22(8):2605–2622, 2008.\n[195] Travis I Zack, Steven E Schumacher, Scott L Carter, Andrew D Cherniack, Gordon Saksena, Barbara Tabak, Michael S Lawrence, Cheng-Zhong Zhang, Jeremiah Wala, Craig H Mermel, et al. Pan-cancer patterns of somatic copy number alteration. Nature genetics, 45(10):1134–1140, 2013.\n[196] Habil Zare, Junfeng Wang, Alex Hu, Kris Weber, Josh Smith, Debbie Nickerson, ChaoZhong Song, Daniela Witten, C Anthony Blau, and William Stafford Noble. Inferring clonal composition from multiple sections of a breast cancer. 2014.\n[197] Kaizhong Zhang and Dennis Shasha. Simple fast algorithms for the editing distance between trees and related problems. SIAM journal on computing, 18(6):1245–1262, 1989.\n[198] Junfei Zhao, Shihua Zhang, Ling-Yun Wu, and Xiang-Sun Zhang. Efficient methods for identifying mutated driver pathways in cancer. Bioinformatics, 28(22):2940–2947, 2012.\n[199] Xue Zhong, Hushan Yang, Shuyang Zhao, Yu Shyr, and Bingshan Li. Networkbased stratification analysis of 13 major cancer types using mutations in panels of cancer genes. BMC genomics, 16(Suppl 7):S7, 2015.\n[200] Indre Zliobaite, Faisal Kamiran, and Toon Calders. Handling conditional discrimination. In Data Mining (ICDM), 2011 IEEE 11th International Conference on, pages 992–1001. IEEE, 2011.\nAPPENDIX A\nFOUNDATIONS OF CAUSATION\nIn this Chapter, we give an outline of the current state-of-the-art theories of causation, which enjoys a long and colorful history, starting with the work of Aristotle and, more recently, of Avicenna circa 1000 ad. However, we restrict our description only to the main ideas and limitations of these theories, as a more detailed discussion of various topics related to these theories is available elsewhere (see [88] or [80]).\nThe biological notion of causality proposed in this thesis is firmly grounded on the notions of Darwinian evolution: in that, it is about an ensemble of entities (e.g., population of cells, organisms, etc.). Within this ensemble, a causal event (say c) in a member entity may result in variations (changes in genotypic frequencies); such variations are exhibited in the phenotypic variations within the population, which is subject to Darwinian positive (and subsequently, Malthusian negative) selections, and sets the stage for a new effect event (say e) to be selected, should it occur next; we then conclude that “c e.”\nWhile there could be other meaningful extensions of this framework (see [75])1, we believe that it suffices in describing the causality relations implicit in the somatic evolution responsible for tumor progression. Note further that by its very statistical nature, we capture just those relations that only reflect “Type-level Causality”, and relegate “Token-level Causality”, – a more nuanced concept – to the future research. Thus, note that, while our framework can estimate for a population of cancer patients of a particular kind (say atypical Chronic Myeloid Leukemia, aCML, patients) whether and with what probability a mutation (such as setbp1) would cause certain other mutations (such as asxl1 single nucleotide variants or in-del) to occur, it will remain silent as to whether a particular asxl1 mutation in a particular patient was caused by an earlier setbp1 mutation.\nBased on the afore-mentioned biological framework, we will focus primarily on how to\n1Also see the debate between Fisher and Wright in response to Fisher’s fundamental theorem of genetics.\n121\n122 APPENDIX A. FOUNDATIONS OF CAUSATION\ndevise efficient and accurate algorithms for extracting causal relations from the patient genomic data; we leave it to the readers to intuit how an inferred causal relation may be verified/refuted by in vitro or in silico experiments and how it could be used in therapy design that would guide the clocks involved in cancer’s natural somatic evolution (more details are forthcoming)."
    }, {
      "heading" : "A.1 Hume’s regularity theory",
      "text" : "The modern study of causation begins with the Scottish philosopher David Hume (1711- 1776). According to Hume, a theory of causation could be defined axiomatically, using the following ingredients: temporal priority , implying that causes are invariably followed by their effects [87], augmented by various constraints, such as contiguity, constant conjunction2, etc. Theories of this kind, that try to analyze causation in terms of invariable patterns of succession, have been referred to as regularity theories of causation.\nNonetheless, the notion of causation has spawned far too many variants and has been a source of acerbic debates. All these theories present well-known limitations and confusion, but have led to a small number of modern versions of commonly accepted (at least among the philosophers) frameworks. See the theories discussed and studied by Suppes et al. §A.2, Lewis et al. §A.3, and Pearl et al. §A.3.1. One of the most prominent among these is Suppes’ probabilistic causation, whose axioms are expressible in probabilistic propositional modal logics, and amenable to algorithmic analysis. It is the framework upon which we build our analyses and algorithms.\nWe will momentarily discuss the main limitations of regularity theories [80], in order to better prepare the reader for the subsequent discussions of these theories and the algorithms to which they lead. Thus, the next three sections will focus on two issues: (i) how the state-of-the-art theories of causation have attempted formulating a sound and complete theory of causation, as well as (ii) what unsolved problems in this framework still remain open.\nImperfect regularities. In general, one cannot state that causes are invariably (i.e., without fail) followed by their effects. For example, while we may state that “smoking is a cause of lung cancer”, we do grant that there would be still some smokers who do not develop lung cancer.\nSituations such as these are referred to as imperfect regularities, and could arise for many different reasons. One of these – which is a very common situation in the context of cancer – involves the heterogeneity of the situations in which a cause resides. For example, some smokers may have a genetic susceptibility to lung cancer, while others do not; moreover, some non-smokers may be exposed to other carcinogens, while others are not. Thus, the fact that not all smokers develop lung cancer can be explained in these terms.\n2Some of these notions have been modernized with the introduction of the machinery from statistical inference, logic and model theory; but they have stayed more or less true to Hume’s programme.\n122\nA.2. PROBABILISTIC THEORIES OF CAUSATION 123\nIrrelevance. An event that is invariably followed by another, can be irrelevant to it. Consider the example in [104]: salt that has been hexed by a sorceror invariably dissolves when placed in water, but hexing does not cause the salt to dissolve. In fact, hexing is irrelevant for this outcome. Probabilistic theories of causation capture exactly this situation by requiring that causes alter the probabilities of their effects, see §A.2.\nAsymmetry. If we claim that an event c causes another event e, then, typically, we would anticipate being able to claim that e does not cause c, which would naturally follow from a strict temporal-priority-constraint: cause precedes effect temporally. In the context of the preceding example, smoking causes lung cancer, but lung cancer does not cause one to smoke.\nSpurious regularities. Consider a situation – not very uncommon – where a unique cause is regularly followed by two or more effects. As an example, suppose that one observes the height of the column of mercury in a particular barometer dropping below a certain level. Shortly afterwards, because of the drop in atmospheric pressure (the unobserved cause for falling barometer), a storm occurs. In this settings, a regularity theory could claim that the drop of the mercury column causes the storm when, indeed, it is only correlated to it. Following common terminologies, we will say that such situations are due to spurious correlations. There now exists an extensive literature discussing such subtleties that are important in understanding the philosophical foundations of causality theory; see [80]."
    }, {
      "heading" : "A.2 Probabilistic theories of causation",
      "text" : "In this Section we will introduce the notion of probabilistic causation. The basic idea behind these theories is that “causes alter the probabilities of their effects;” see [80, 98] for more details."
    }, {
      "heading" : "Suppes’ prima facie cause",
      "text" : "Patrick Suppes proposed the notion of a prima facie cause that represents the core of probabilistic causation and also provides the algorithmic foundations of our analysis.\nDefinition 8 (Probabilistic causation, [172]). For any two events c and e, occurring respectively at times tc and te, under the mild assumptions that 0 < P(c),P(e) < 1, the event c is called a prima facie cause of e if it occurs before and raises the probability of e, i.e.\ntc < te and P(e | c) > P(e | c) . (A.1)\nFrom now on, the first condition will be referred to as temporal priority, whereas the second as probability raising. This notion of causation has some advantages over the simplest version of a regularity theory of causation, e.g., it deals with various issues usually associated with imperfect regularities (§A.1).\n123\n124 APPENDIX A. FOUNDATIONS OF CAUSATION\nUnfortunately, however, prima facie causality is still not sufficient in capturing a causation relationship in its full generality. For instance, the problem of spurious regularities still remains, additionally requiring that prima facie causes be refined further into two classes: genuine and spurious. In the latter case, as discussed, we may observe a prima facie cause to be so labeled only because of spurious correlations. Also, as discussed extensively in the literature (see [80]), one may encounter certain situations, in which Suppes’ characterization fails to provide a necessary condition. In the next two paragraphs, we will briefly discuss an attempt to make Suppes’ conditions sufficient for any causal claims, and another to determine when it is not necessary.\nReichenbach’s screening-off\nIn [160], Reichenbach discussed the notion of screening-off to describe a particular type of probabilistic relationship. Consider, e.g., events a, c and e, and assume to observe P(e | a ∧ c) = P(e | c), then we say that c is screening a off from e. When P(e ∧ c) > 0, this is equivalent to stating that P(a ∧ e | c) = P(a | c) · P(e | c) – i.e., a and e happen to be probabilistically independent, when conditioned upon c. The preceding situation could occur in two cases, see Figure §A.1.\nIn the first case, c is a genuine cause of e while a is a genuine cause of c as well, and the correlations between a and e are only just manifestations of these known causal connections. For example, unprotected sex (a) appears to cause AIDS (e) only because of sexually transmitted HIV infection (c). Then, we would expect that among those who have already been infected with HIV, the probability of contacting AIDS would be the same regardless of whether one is engaged in unprotected sex or not. Here c is a proximate cause of e and an intermediate cause leading from a to e, i.e. an instance of causal transitivity. In the second case, c is a common cause of both a and e, that is exactly the situation of spurious correlation described in §A.1.\nBuilding upon this idea, Reichenbach formulated the so-called Common Cause Principle (CCP) to detect situations leading to “screening-off,” and so identify when a spu-\n124\nA.2. PROBABILISTIC THEORIES OF CAUSATION 125\nrious correlation can be explained in terms of a common cause. Unfortunately, there are situations where such a principle leads to computationally intractable criteria. Since, these issues are not germane to the context of this work, we will not discuss them further, other than pointing the interested readers to appropriate literature [80]. Nevertheless, the idea of screening-off has significantly influenced some of the most widely-used recent theories of causation, and has become central to the topic."
    }, {
      "heading" : "Simpson’s paradox and Cartwright’s background context",
      "text" : "Up to now, we have discussed the sufficiency (or lack of it) of the characterization for causality provided in the Reichenbach-Suppes framework. Conversely, we may also examine those situations where this framework also fails to give all the necessary conditions for a causal claim. For example, consider smoking as a cause of lung cancer. But, examine in details a situation where it so happens that smoking is highly correlated with living in the country: those who live in the country are much more likely to smoke than those who do not. Suppose now that city pollution is a second cause of lung cancer, which happens to be a much stronger cause than smoking. Consider now the problem of causal claims on the combination of these two heterogenous populations: including those who live in the country and those who do not. Then, an analysis of those two populations in combination may falsely lead to the conclusion that smokers are, over all, less likely to suffer from lung cancer than non-smokers. This example is an instance of the so-called Simpson’s Paradox, which has been discussed extensively by various philosophers (see Nancy Cartwright [22] and Brian Skyrms [170]).\nCartwright and Skyrms introduced the concept of background contexts to explain and correct this problem. Let us call the set of all the factors that are causes of the event e (a factor can be an atomic event but it can also be the composition of a set of events), but are not caused by the event c, the set of independent causes of e. A background context for a causal relationship from c to e is the maximal conjunction of factors, each of which is either an independent cause of e, or the negation of an independent cause of e (as shown in Figure §A.1). We will denote by variables b1, . . ., bn all the background contexts of a causal relationship. According to Cartwright then, c causes e if and only if P(e | c ∧ bi) > P(e | c ∧ bi), that is if c raises the probability of e in every background context bi ∈ B. Skyrms proposed a slightly weaker condition: a cause must raise the probability of its effect in at least one background context, without lowering it in any other."
    }, {
      "heading" : "Eells’ taxonomy",
      "text" : "Cartwright defined a cause in terms of raising the probability of its effect. But there are other possible probabilistic relations between c and e, as described, for instance, by Eells, who proposes the following taxonomy [44]: (i) c is a cause of e if and only if it raises its probability in every background context B, (ii) c is an inhibition for e when it lowers such a probability, (iii) c is causally irrelevant to e when it does not change it and, finally, (iv) c is a mixed cause of e, otherwise.\n125\n126 APPENDIX A. FOUNDATIONS OF CAUSATION\nThis thesis will adhere to the basic idea of a cause being a probability-raiser of its effect and ignore for the time being all other variants. According to Suppes’ probabilistic theories of causation, we can evaluate a causal claim in terms of Definition §8, further augmented by the ideas of screening-off and background contexts; the same algorithmic, inferential and logical tools that we propose here can be used mutatis mutandis, should a user wish to explore a variant framework leading to a different axiomatic formulation of causation – provided its expressivity is limited to a probabilistic propositional modal logic – as seems the case to be."
    }, {
      "heading" : "Issues of probabilistic causation",
      "text" : "Next we describe some thorny issues in the theory of probabilistic causation. We also briefly point out some unresolved problems, proposed plans of attack, and ensuing criticisms. For a deeper discussion see [80].\nPearl’s criticism. In [150], Pearl argues that the notion that causes “raise the probabilities” of their effects cannot be expressed in the language of probability theory . In particular, according to Pearl, the inequality P(e | c) > P(e | c) fails to capture the intuition behind probability raising, which must be manipulative or counterfactual . Because of this limit, Pearl argues that it is not possible to rigorously describe the intuitions behind the probability raising theory and, for this reason, the only way to properly assess a causal claim is exclusively by intervention. The methods described in this thesis are not negated by these arguments as our model reasons about an ensemble (tumor with heterogeneous cell-types) and type-level causality. Pearl’s theory is discussed further in §A.3.1.\nDetermining the background context. As described, the background contexts of a claim are all the factors causally relevant to the effect, but not to the cause. This assumption appears to prevent Cartwright’s theory from being a reductive analysis of causation. In fact, the theory appeals to causal relations to define a set of probabilistic constraints on the possible causal claims compatible with the observations in terms of probabilities. In any case, even if there is no reduction of causation to probability, in practice, it can be difficult (or algorithmically complex) to determine the background contexts without knowing the causal topology in advance. Unfortunately, this argument introduces an unavoidable circularity."
    }, {
      "heading" : "A.3 Counterfactual theories of causation",
      "text" : "Here we present a brief discussion of [127] theories of causation where the meaning of causal claims is explained in terms of a possible-world semantics and counterfactual conditionals of the form: had c not occurred, e would not have occurred either. For detailed discussions see [127].\n126\nA.3. COUNTERFACTUAL THEORIES OF CAUSATION 127"
    }, {
      "heading" : "Lewis’s counterfactuals",
      "text" : "The most complete known counterfactual theory of causation is due to David Lewis [113] and exploits a possible world semantics to state truth conditions for counterfactuals in terms of similarity among possible worlds: one possible world is closer to actuality than another, if it is more similar to the actual world.\nFollowing this idea, Lewis defined two important constraints on the resulting similarity relation: (i) similarity induces an ordering of worlds in terms of closeness to the actual world and (ii) the actual world is the closest possible world to actuality. Then, the evaluation of the counterfactual “if c were the case, e would be the case” is true just in case it is closer to actuality to make the first term true along with the second – as opposed to making it true without. Therefore, in terms of counterfactuals Lewis defines the following notion of causality: given c and e, whether e occurs or not depends on whether c occurs or not, and e causally depends on c if and only if, if c were not to occur e would not occur. Thus, the idea of cause is conceptually linked to the idea of something that makes a difference, and this concept in turn is naturally described in terms of counterfactuals. Lewis also characterized causation in terms of temporal direction by stating that the direction of causation is the direction of causal dependence and that, typically, events causally depend on earlier events but not on later ones.\nCausal Chains. In [113], Lewis states that causal dependence between events is sufficient but not necessary, i.e., it is possible to have causation without causal dependence. Consider, e.g., when c causes d, which in turn causes e; Lewis argues that c must cause e as well by means of a transitivity. However, since causal dependence is not transitive as would be the case for causation according to Suppes, the causal relation between c and e may not be evident. To overcome this problem, Lewis defines a causal chain as the finite sequence of events c, d and e and defines that c is a cause of e if and only if there exists a causal chain leading from c to e."
    }, {
      "heading" : "Issues of counterfactual causation",
      "text" : "We briefly describe some issues inherent to these theories; for a deeper discussion, see [127].\nContext-sensitivity. Lewis’s theory assumes that causation is an absolute relation, whose nature does not vary from one context to another. This approach has recently been criticized since it often leads to absurd results [127], as demonstrated by various easy-to-construct counter-examples.\nTransitivity and Preemption. As discussed above, Lewis incorporates transitivities in his notion of causation by defining them in terms of chains of causal dependence. The transitivity of causation is sound in some contexts, but a number of counter-examples has been shown to cast doubts on this interpretation of causation [127]; the debate surrounding the transitivity of causation is unlikely to be easily settled. Nevertheless,\n127\n128 APPENDIX A. FOUNDATIONS OF CAUSATION\nin this work we aim at inferring minimal models of causation, in which each cause is sufficient for its child to occur. For this reason, we have opted to remove transitivities.\nA.3.1 Manipulability theories of causation\nWe now briefly discuss the notion of intervention as propounded by Judea Pearl [150]; in general interventionist versions of manipulability theories can be seen as counterfactual theories. For a detailed discussion on this and manipulability theories of causation refer to [192].\nPearl characterizes his notion of intervention in terms of a primitive notion of causal mechanism. According to him, the world is organized in the form of stable mechanisms (i.e. physical laws) which are autonomous. Therefore, he states that we can change one of them, without changing all the others. Thus an intervention may imply that: if we manipulate c and nothing happens, then c cannot be cause of e, but if a manipulation of c leads to a change in e, then we know that c is a cause of e, although there might be other causes as well.\nIn other words, when among many events a causal relationship between some e and its parents (i.e. directed causes, say c1, . . ., cn) is present, the interventions will disrupt completely the relationships between e and c1, . . ., cn such that the value of e is determined by the intervention only. Thus, intervention is a surgical operation in the sense that no other causal relationship in the system are changed by it. Hence, Pearl’s assumption is that the other variables that change in values under this intervention will do so only because they are effects of e. Going back to the barometer example of §A.1: observing the drop of the mercury column increases the probability of a storm coming, but if we manipulate the drop of the mercury column by intervention such that its drop is caused by the intervention only, then we will be able to qualify barometer as a cause of storms instead of the drop itself. Pearl’s theory has been very influential among the computational causality theorists, and has generated state-of-the-art algorithms for causal network inference, which we shortly present in §B."
    }, {
      "heading" : "Issues of interventionist causation",
      "text" : "Next, we point the reader to some problems that can arise in practice, when applying intervention in the context of causal inference. For a deeper discussion we refer to [192].\nCircularity. An intervention on an event e leaves intact all the other causal mechanisms besides the ones involving c as a cause. Because of this, Pearl’s intervention could lead to circularity problems, i.e., it seems that the causal mechanisms need to be known in advance in order to asses them.\nPossible and impossible interventions. Causal claims are described in terms of counterfactuals of what would happen when applying intervention to a given causal relationship. Moreover, the notion of intervention is connected with the possibility of a human action to intervene in a system. In some contexts, however, it may be impossible\n128\nA.4. A SIMPLIFIED FRAMEWORK 129\nto evaluate what would happen by performing a surgical intervention. Thus, it should be clear that, regardless of the possible criticisms to Pearl’s framework, there are situations where, at least relative to the current human capabilities, it is very complicated, if not impossible, to perform intervention."
    }, {
      "heading" : "A.4 A simplified framework",
      "text" : "In conclusion, each of the existing theories faces various difficulties, which are rooted primarily in the attempt to construct a framework in its full generality: each theory aims to be both necessary and sufficient for any causal claim, in any context. In contrast, this thesis simplifies the problem by breaking the task into two: first, define a framework for Suppes’ prima facie notion though it admits some spurious causes, but then deal with spuriousness by using a combination of tools, e.g., Bayesian, empirical Bayesian, regularization, which we recall in §B. The framework is based on a set of conditions that are necessary even though not sufficient for a causal claim, and is used to refine a prima facie cause to either a genuine or a spurious cause (or even ambiguous ones, to be treated as plausible hypotheses which can be refuted/validated by other means), see §2.\nStatement of assumptions. Along with the described interpretation of causality, through out this document, we make following simplifying assumptions to guarantee the convergence of the method:\n(i) All causes involved in cancer can be expressed by monotonic Boolean formulas: i.e., all causes are positive and can be expressed in CNF where all literals occur only positively. The size of the formula and each clause therein are bounded by small constants.\n(ii) All events are persistent: i.e., once a mutation has occurred, it cannot disappear. Hence, we do not model situations where P(e | c) < P(e | c).\n(iii) Closed world: all the events which are causally relevant for the progression are observable and the observation can significantly describe the progressive phenomenon.\n(iv) Relevance to the progression: all the events have probability strictly in the real open interval (0, 1), i.e. it is possible to asses if they are relevant to the progression.\n(v) Distinguishability: no two events appear equivalent, i.e. they are neither both observed nor both missing simultaneously.\nWe conclude by observing and stressing that the assumptions of above are to be intended as theoretical and, given them, we can prove asymptotic convergence of the framework. Nevertheless, as described along the thesis, there can be practical examples were these assumptions may not strictly be true and, yet, the approach still provides valid insights.\n129\nAPPENDIX B\nLEARNING BAYESIAN NETWORKS\nIn this Chapter we briefly discuss the notion of Bayesian Network (BN) and how to learn both its parameters and structure ab initio, with no prior knowledge. For a detailed discussion on the topic, refer to [101, 151]. This Section is intended to be accessible to a non-technical audience, although citations are provided for technical resources on each algorithm discussed."
    }, {
      "heading" : "B.1 Preliminaries",
      "text" : "A BN is a statistical model that succinctly represents a joint distribution over n variables and encodes it in a direct acyclic graph over n nodes (one per variable)1. In BNs, the full joint distribution can be written as a product of conditional distributions on each variable. An edge between two nodes A and B denotes statistical dependence, P(A ∧B) 6= P(A)P(B), no matter on which other variables we condition on (i.e., for any other set of variables C it holds P(A ∧B | C) 6= P(A | C)P(B | C). In such a graph, the set of variables connected to a node X determines its set of “parent” nodes π(X). Note that a node cannot be both ancestor and descendant of another node, as this would cause a directed cycle.\nAlso, the joint distribution over all the variables can be written as ∏ X P(X | π(X)). Of course, if a node has no incoming edges (i.e., no parents), we simply use its marginal probability P(X). Thus, to compute the probability of any combination of values over the variables, we need only parameterize the conditional probabilities of each variable given its parents. If the variables are binary, the number of parameters in each conditional probability table is locally of exponential size: namely, 2|π(X)| − 1. Thus, the total number of parameters needed to compute the full joint distribution is only of size∑\nX 2 |π(X)| − 1, which is considerably less than 2n − 1 for sparse networks.\n1In the setting of this thesis, each variable is a modeled event and, for consistency with the BN notation, we will denote these as capital letters in this section.\n130\nB.2. APPROACHES TO LEARN THE STRUCTURE OF A BN 131\nA useful property of the graph structure is that we can define, for each variable, a set of nodes called the Markov blanket so that, conditioned on it, this variable is independent of all other variables in the system. It can be proven that for any BN, the Markov blanket consists of a node’s parents, children as well as the parents of the children.\nThe usage of the symmetrical notion of conditional dependence introduces important limitations of structure learning in BNs. In fact, note that edges A → B and B → A denote equivalent dependence between A and B, thus distinct graphs model the exact same set of independence and conditional independence relations. This yields the notion of Markov equivalence class as a partially directed acyclic graph, in which the edges that can take either orientation are left undirected. A theorem proves that two BNs are Markov equivalent when they have the same skeleton and the same v-structures, the former being the set of edges, ignoring their direction (e.g., A → B and B → A constitute a unique edge in the skeleton) and the latter being all the edge structures in which a variable has at least two parents, but those do not share an edge (e.g., A→ B ← C)2 [18].\nBNs have an interesting relation to canonical boolean logical operators ∧, ∨ and ⊕ and formulas over variables. In fact these formulas, which are “deterministic” in principle, in BNs are naturally softened into probabilistic relations to allow some degree of uncertainty or noise. This probabilistic approach to modeling logic allows representation of qualitative relationships among variables in a way that is inherently robust to small perturbations by noise. For instance, the phrase “in order to hear music when listening to an mp3, it is necessary and sufficient that the power is on and the headphones are plugged in” can be represented by a probabilistic conjunctive formulation that relates power, headphones and music, in which the probability that music is audible depends only on whether power and headphones are present. On the other hand, there is a small probability that the music will still not play (perhaps we forgot to load any songs into the device) even if both power and headphones are on, and there is small probability that we will hear music even without power or headphone (perhaps we are next to a concert and overhear that music).\nNote that in this review, we only consider the subset of networks that have discrete random variables that are visible. Networks with latent and continuous variables present their own challenges, although they share most of the mathematical foundations discussed here."
    }, {
      "heading" : "B.2 Approaches to learn the structure of a BN",
      "text" : "In the the literature, there have been two initial families of methods aimed at learning the structure of a BN from data. The methods belonging to the first family seek to explicitly capture all the conditional independence relations encoded in the edges, and will be referred to as constraint based approaches (§B.2.1). The second family, that of score based approaches (§B.2.2), seeks to choose a model that maximizes the likelihood of the data\n2In BN terminology, parent A and C are considered “unwed parents.” For this reason, the v-structure is often called an immorality or an unshielded collider.\n131\n132 APPENDIX B. LEARNING BAYESIAN NETWORKS\ngiven the model. Since both the approaches lead to intractability (NP-hardness) [26, 27], computing and verifying an optimal solution is impractical and, therefore, heuristic algorithms have to be used, which only sometimes guarantee optimality. Recently, a third class of learning algorithms that takes advantage of specialized logical relations (mentioned in the previous section) have been introduced (§B.2.3). In the rest of this section we describe in detail some of these approaches.\nB.2.1 Constraint based approaches\nWe present an intuitive explanation of several common algorithms used for structure discovery by explicitly considering conditional independence relations between variables. For more detailed explanations and analyses of complexity, correctness and stability, refer to the related references.\nThe basic idea behind all algorithms is to build a graph structure reflecting the independence relations in the observed data, thus matching as closely as possible the empirical distribution. The difficulty in this approach lies in the number of conditional pairwise independence tests that an algorithm would have to perform to test all possible relations. This is indeed exponential requiring to condition on a power set, when testing for the conditional independence between two variables. This inherent intractability requires the introduction of approximations.\nHere, we focus on two specific constraint based algorithms, the PC algorithm [171] and the Incremental Association Markov Blanket (IAMB, [179]), because of their proven efficiency and widespread usage. In particular, the PC algorithm solves the aforementioned approximation problem by conditioning on incrementally larger sets of variables, such that most sets of variables will never have to be tested, whereas the IAMB first computes the Markov blanket of all the variables and conditions only on members of the blankets. A few more details about these algorithms follow.\nThe PC algorithm. The PC algorithm [171] begins with a fully connected graph and, on the basis of pairwise independence tests, iteratively removes all the extraneous edges. It is based on the idea that if a separating set exists that makes two variables independent, we can remove the edge between them. To avoid an exhaustive search of separating sets, these are ordered to find the correct ones early in the search. Once a separating set is found, the search for that pair can end. The PC algorithm orders separating sets of increasing size l starting from 0, the empty set, and incrementing until l = n − 2. The algorithm stops when every variable has fewer than l − 1 neighbors, since it can be proven that all valid sets must have already been chosen. During the computation, the larger the value of l is, the larger number of separating sets must be considered. However, by the time l gets too large, the number of nodes with degree l or higher must have dwindled considerably. Thus, in practice, we need only consider a small subset of all the possible separating sets.\n132\nB.2. APPROACHES TO LEARN THE STRUCTURE OF A BN 133\nIncremental Association Markov Blanket algorithm. A different type of constraint based learning algorithms uses the Markov blankets to restrict the subset of variables to test for independence. Thus, when this knowledge is available in advance, we do not have to test a conditioning on all possible variables. A widely used and efficient algorithm for Markov blanket discovery is IAMB. In it, for each variable X, we keep track of a hypothesis set H(X). The goal is for H(X) to equal the Markov blanket of X, B(X), at the end of the algorithm. IAMB consists of a forward and a backward phase. During the forward phase, it adds all possible variables into H(X) that could be in B(X). In the backward phase, it eliminates all the false positive variables from the hypotheses set, leaving the true B(X). The forward phase begins with an empty H(X) for each X. Iteratively, variables with a strong association with X (conditioned on all the variables in H(X)) are added to the hypotheses set. This association can be measured by a variety of non-negative functions, such as mutual information. As H(X) grows large enough to include B(X), the other variables in the network will have very little association with X, conditioned on H(X). At this point, the forward phase is complete. The backward phase starts with H(X) that contains B(X) and false positives, which will have little conditional association, while true positives will associate strongly. Using this test, the backward phase is able to remove the false positives iteratively until all but the true positives are eliminated.\nB.2.2 Score based approaches\nThis approach to structural learning seeks to maximize the likelihood of a set of observed data. Since we assume that the data are independent and identically distributed, the likelihood of the data L(·) is simply the product of the probability of each observation. That is,\nL(D) = ∏ d∈D P(d)\nfor a set of observations D. Since we want to infer a model G that best explains the observed data, we define the likelihood of observing the data given a specific model G as\nLL(G, D) = ∏ d∈D P(d | G) .\nThe actual likelihood is not used in practice, as this quantity becomes very small and impossible to represent in a computer. Instead, the logarithm of the likelihood is used for three reasons. First, the log(·) function is monotonic. Second, the values that the loglikelihood takes do not cause the same numerical problems that likelihood does. Third, it is easy to compute because the log of a product is simply the sum of the logs (e.g., log(xy) = log x+ log y), and the likelihood for a Bayesian network is a product of simple terms.\nPractically, however, there is a problem in learning the network structure by maximizing log-likelihood alone. Namely, for any arbitrary set of data, the most likely graph is always the fully connected one (i.e. all edges are present), since adding an edge can\n133\n134 APPENDIX B. LEARNING BAYESIAN NETWORKS\nonly increase the likelihood of the data. To correct for this phenomenon, log-likelihood is almost always supplemented with a regularization term that penalizes the complexity of the model3. There are a plethora of regularization terms, some based on information theory and others on Bayesian statistics (see [23] and references therein), which all serve to promote sparsity in the learned graph structure, though different regularization terms are better suited for particular applications.\nAlso in this case we choose to describe a particularly relevant and known score, the Bayesian Information Criterion (BIC, [101]), which will be subsequently compared to the performance of our approach.\nThe Bayesian Information Criterion. BIC uses a score that consists of a loglikelihood term and a regularization term depending on a model G and data D\nbic(G, D) = LL(G, D)− logm 2 dim(G). (B.1)\nHere, D denotes the data, m denotes the number of samples and dim(G) denotes the number of parameters in the model. Because, in general, dim(·) depends on the number of parents each node has, it is a good metric for model complexity. Moreover, each edge added to G increases model complexity. Thus, the regularization term based on dim(·) favors graphs with fewer edges and, more specifically, fewer parents for each node. The term logm/2 essentially weighs the regularization term. The effect is that the higher the weight, the more sparsity will be favored over “explaining” the data through maximum likelihood.\nNote that the likelihood is implicitly weighted by the number of data points, since each point contributes to the score. As the sample size increases, both the weight of the regularization term and the “weight” of the likelihood increase. However, the weight of the likelihood increases faster than that of the regularization term4. Thus, with more data, likelihood will contribute more to the score, and we may trust our observations more and have less need for regularization. Statistically speaking, BIC is a consistent score [101]. In terms of structure learning, this observation implies that for sufficiently large sample sizes, the network with the maximum BIC score is I-equivalent5 to the true structure. Consequently, G contains the same independence relations as those implied by the true structure. As the independence relations are encoded in the edges of the graph, we are guaranteed to learn a Markov-equivalent network, with the same skeleton and the same v-structures as the true graph, though not necessarily with the correct orientations for each edge.\n3Note that more edges in a graph require more parameters in the conditional probability distributions, thus increasing model complexity. If it was known that the number of parameters for each node is fixed, then regularization is not necessary.\n4Specifically, the likelihood weight increases linearly, while the weight of the regularization term grows only logarithmically.\n5Two networks are I-equivalent if their structures encode the same independence statements.\n134\nB.3. BAYESIAN INTERPRETATION OF THE PROPOSED FRAMEWORK 135\nB.2.3 Learning logically constrained networks\nIn §B.1, we noted that an important class of BNs captures common binary logical operators, such as ∧, ∨, and ⊕. Although the learning algorithms mentioned above can be used to infer the structure of such networks, some algorithms employ knowledge of these logical constraints in the learning process.\nA widely used approach to learn a monotonic cancer progression network with a directed acyclic graph (DAG) structure and conjunctive events are Conjunctive Bayesian Networks (see CBNs, [10]). This model is a standard BN over Bernoulli random variables with the constraint that the probability of a node X taking the value 1 is zero if at least one of its parents has value 0. This defines a conjunctive relationship, in that all the parents of X must be 1 for X to possibly be 1. Thus, this model alone cannot represent noise, which is an essential part of any real data. In response to this shortcoming, hidden CBNs [65] were developed by augmenting the set of variables: a correspondence to a new variable Y that represents the observed state is assigned to each CBN variable X, which captures the “true” state. Thus, each new variable Y takes the value of the corresponding variable X with a high probability, and the opposite value with a low probability. In this model, the variables X are latent, i.e., they are not present in the observed data, and have to be inferred from the observed values for the new variables. Learning is performed using a maximum likelihood approach and is separated into multiple iterations of two steps. First, the parameters for the current hypothesized structure are estimated using the Expectation-Maximization algorithm [132] and the likelihood given those parameters is computed. Second, the structure is perturbed to nudge the hill climbing scheme used to maximize expectation off local maximum. In their work, the authors used the Simulated Annealing algorithm [97] for this step. These two steps are repeated until the score converges. However, the Expectation-Maximization algorithm only guarantees convergence to a likelihood local maximum and, thus, the overall procedure is not guaranteed to converge to the optimal structure.\nB.3 Bayesian interpretation of the proposed framework\nThe algorithms proposed in this thesis can be placed among the constrained approaches. In particular, the aim is the one of reconstructing Bayesian graphical models whose induced probability distributions are biased in terms of Suppes’ criteria as follows. For any pair of nodes for which we have a directed edge from b to a,{\nP(a | b) = θ P(a | b) ≤\nwhere θ, ∈ [0, 1] and θ . Specifically, θ represents the conditional probability of any effect to follow its preceding cause and models the probability of any noisy observation.\n135"
    }, {
      "heading" : "APPENDIX C",
      "text" : "CAPRESE - SUPPLEMENTARY MATERIALS\nIn this Chapter we will report the supplementary materials related to chapter §3."
    }, {
      "heading" : "C.1 Proofs",
      "text" : "Here the proofs of all the propositions and theorems follow."
    }, {
      "heading" : "Proof of Proposition §3 (Statistical dependence).",
      "text" : "Proof. For ⇒ write P(a ∧ b) = P(b)− P(a ∧ b), then write the pr as\nP(a ∧ b) P(a) > P(b)− P(a ∧ b) 1− P(a)\nand, since 0 < P(a) < 1, the proposition follows by simple algebraic arrangements of P(a ∧ b) · [1−P(a)] > P(a)P(b)−P(a ∧ b) · P(a). The derivations are analogous but in reverse order for the implication ⇐."
    }, {
      "heading" : "Proof of Proposition §4 (Mutuality).",
      "text" : "Proof. The proof follows by Property §3 and the subsequent implication:\nP(b | a) > P(b | a)⇔ P(a ∧ b) > P(a)P(b)⇔ P(a | b) > P(a | b) ."
    }, {
      "heading" : "Proof of Proposition §5 (Natural ordering).",
      "text" : "Proof. We first prove the forward direction ⇒. Let x = P(a ∧ b), y = P(a ∧ b) and z = P(a ∧ b). We have two assumptions we will use later on:\n136\nC.1. PROOFS 137\n1. P(a) > P(b) which implies P(a ∧ b) < P(a ∧ b), i.e., x < z.\n2. P(a | b) > P(a | b) which, when 0 < x + y < 1, implies by simple algebraic rearrangements the inequality\ny[1− x− y − z] > xz . (C.1)\nWe proceed by rewriting P(b | a)/P(b | a) > P(a | b)/P(a | b) as\nP(a ∧ b)P(a) P(a ∧ b)P(a) > P(a ∧ b)P(b) P(a ∧ b)P(b)\nwhich means that\nP(b | a) P(b | a) > P(a | b) P(a | b) ⇐⇒ P(a)P(a ∧ b)P(a) > P(b) P(a ∧ b)P(b) (C.2)\nWe can rewrite the right-hand side of (§C.2) by using x, y, z where P(a) = P(a, b) + P(a, b) = y + z and P(b) = P(a, b) + P(a, b) = x + y, and then do suitable algebraic manipulations. We have\n1− y − z x(y + z) > 1− x− y z(x+ y) ⇐⇒ yz − y2z − xz2 − yz2 > xy − x2y − x2z − xy2 (C.3)\nwhen x(y + z) 6= 0 and z(x + y) 6= 0. To check that the right side of (§C.3) holds we show that (xy − x2y − x2z − xy2)− (yz − y2z − xz2 − yz2) < 0 . First, we rearrange it as (x− z)[y − y2 − xz − y(x+ z)] < 0 to show that\n(x− z)[y(1− y − x− z)− zx] < 0 (C.4)\nis always negative. By observing that, by assumption 1 we have z > x and thus (x−z) < 0, and, by equation (§C.1) we have y(1− y − x− z)− zx > 0, we derive\nP(b | a) P(b | a) > P(a | b) P(a | b)\nwhich concludes the ⇒ direction. The other direction⇐ follows immediately by contraposition: assume that P(a | b) > P(a | b), P(b | a)/P(b | a) > P(a | b)/P(a | b) and P(b) ≤ P(a). We distinguish two cases:\n1. P(b) = P(a), then P(b | a)/P(b | a) = P(a | b)/P(a | b).\n2. P(b) < P(a), then by symmetry P(b | a) > P(b | a), and by the⇒ direction of the proposition it follows that P(b | a)/P(b | a) < P(a | b)/P(a | b).\nIn both cases we have a contradiction. This completes the proof.\n137\n138 APPENDIX C. CAPRESE - SUPPLEMENTARY MATERIALS"
    }, {
      "heading" : "Proof of Proposition §6 (Monotonic normalization).",
      "text" : "Proof. We prove the forward direction ⇒, the converse follows by a similar argument. Let us assume\nP(b | a) P(b | a) > P(a | b) P(a | b)\n(C.5)\nthen P(b | a)P(a | b) > P(a | b)P(b | a). Now, to show the righthand side of the implication, we will show that[ P(b | a)− P(b | a) ][ P(a | b) + P(a | b) ] > [ P(b | a) + P(b | a) ][ P(a | b)− P(a | b)\n] which reduces to show\nP(b | a)P(a | b)− P(b | a)P(a | b) > P(b | a)P(a | b)− P(b | a)P(a | b) . By (§C.5), two equivalent inequalities hold\nP(b | a)P(a | b)− P(b | a)P(a | b) > 0 P(b | a)P(a | b)− P(b | a)P(a | b) < 0\nand hence the implication holds.\nProof of Proposition §7 (Coherence in dependency and temporal priority).\nProof. We make two assumptions:\n1. P(b | a) > P(b | a) which implies αa→b > 0. 2. P(a, b) > P(a)P(b) which implies βa→b > 0.\nThe proof for dependency follows by Proposition §3 and its implication:\nP(b | a) > P(b | a)⇔ P(a ∧ b) > P(a)P(b)⇔ αa→b > 0⇔ βa→b > 0. Moreover, being β symmetric by definition, the proof for temporal priority follows directly by Proposition §4"
    }, {
      "heading" : "Proof of Theorem §1 (Independent progressions).",
      "text" : "Proof. For any ai ∈ G∗ it holds that P(ai) > P(b) mai→b > 0 being prima facie, also a∗ → b is the edge selected by CAPRESE being the max{·} over G∗. Thus, CAPRESE selects → b instead of a∗ → b if, for any ai, it holds\n1 1 + P(b) > P(ai) P(ai) + P(b) P(ai ∧ b) P(ai)P(b) .\nWith some algebraic manipulations we rewrite this as\nP(ai)P(b) + P(b)2 > P(ai ∧ b)(1 + P(b)) , which gives the inequality in the theorem statement.\n138\nC.1. PROOFS 139"
    }, {
      "heading" : "Proof of Theorem §2 (Algorithm correctness).",
      "text" : "Proof. It is clear that CAPRESE does not create disconnected components since, to each node in G, a unique parent is attached (either from G or ). For the same reason, no transitive connections can appear.\nThe absence of cycles results from Properties §5, §6 and §7. Indeed, suppose for contradiction that there is a cycle (a1, a2), (a2, a3), . . . , (an, a1) in E, then by the three propositions we have\nP(a1) > P(a2) > . . . > P(an) > P(a1)\nwhich is a contradiction."
    }, {
      "heading" : "Proof of Theorem §3 (Asymptotic convergence).",
      "text" : "Proof. For any u ∈ G, when s → ∞ the observed probability P(u) (evaluated from D) is equivalent to the product of the probabilities (in T ) obtained by traversing the forest from the root to u (Definition §3). Thus, P(u) ∈ (0, 1) since the traversal probabilities are in (0, 1) too, hence all events are distinguishable and Algorithm 1 reconstructs a tree with the same events set G of T .\nWe now observe that the distribution induced by T (Definition §3) respects a singlecause prima facie topology where to each event is assigned at most a single cause. In other words, Definition §8 holds for any edge (u, v) ∈ E:\n• by the event-persistence property usually assumed in cancer (fixating mutations are present in the progeny of a clone) the occurring times satisfy tu < tv which, in a frequentist sense, implies P(u) > P(v);\n• it holds by construction (Definition §3) that P(v ∧ u) = P(v), thus P(v | u) = P(v)/P(u) which is strictly positive since P(v) and P(u) are, and that P(v ∧ u) = 0, thus P(v | u) = 0.\nTo correctly reconstruct T we rely on the fact that our score mu→v is consistent with the prima facie probabilistic causation because of:\n• Proposition §5, which states that pr (embedded as αu→v in m) subsumes a good temporal priority model of occurring times, as stated above;\n• Proposition §6 and §7 which ensure the monotonicity and sign coherency among αu→v and βu→v in m.\nThus, m is consistent with a single-cause prima facie topology. We now show that Algorithm 1 reconstructs correctly a generic edge in E, and hence also T .\nConsider an event v ∈ G and edge (u, v) ∈ E. The set of its “candidate” parent events is G \\ {v}, we partition it in three disjoint sets G, S and N :\n• G, genuine: all the backward-reachable events, in G \\ {v}, from v;\n139\n140 APPENDIX C. CAPRESE - SUPPLEMENTARY MATERIALS\n• S, spurious (or ambiguous): all the events (but v ) in the sub-forest which includes the path from to v, which are not in G;\n• N , non prima facie: all other events, i.e., G \\ ({v, } ∪ S ∪ G);\nNotice that G = {v} ∪ G ∪ S ∪N , that u ∈ G and that all the effects of v are non prima facie to v because of the temporal priority, as shown below.\nv\nu\nT\ngenuine\ntransitivity\nspurious\nnon prima\nfacie\nnon prima\nfacie\n......\nThis way of partitioning events according to the structure of T subsumes a equivalent partitioning based on the score α ∈ [0, 1], which we use to prove correctness of our algorithm: for any x ∈ G it holds that αx→v = 1 if x ∈ G, 0 < αx→v < 1 if x ∈ S and αx→v < 0 if x ∈ N .\nWe now show that CAPRESE correctly selects u ∈ G:\n• a non prima facie event x ∈ N either satisfies (Proposition §3)\nP(x ∧ v) ≤ P(x)P(v)\nwhich means that αx→v < 0, βx→v < 0 (Proposition §7) and thus mx→v < 0, or it is a descendant of v, which means that P(x) < P(v). By construction, CAPRESE considers as candidate parents of v only not descendant events with positive score (see step 3);\n• a spurious event x ∈ S is prima facie to v but αx→v < 1 since:\n– P(x)P(v) < P(v ∧ x) < P(v), otherwise x would be backward reachable from x and thus in G;\n– 0 < P(v ∧ x) = P(v)− P(v ∧ x) which means that P(v ∧ x) < P(x)P(v); – by all of the above P(v | x) > 0 which implies that αx→v < 1.\n140\nC.1. PROOFS 141\nRecall now that λ→ 0, which means that mx→v ≈ αx→v < 1. CAPRESE will thus not select any of these events as cause of v if there exist an event with mx→v = 1, which is actually the case with genuine causes;\n• genuine causes are the real cause of v, u, plus all the transitive backward-reachable events. Any x of these has maximum score αx→v = 1 since:\n– P(x)P(v) < P(v ∧ x) = P(v) and 0 = P(v ∧ x); – by the above P(v | x) = 0 which implies αx→v = 1.\nThus, CAPRESE will pick an event from G, and not from S. We need to show that u is the event with maximum score.\nEnumerate the events in G as g1 (which is u), . . ., gk in a way that\nP(g1) < . . . < P(gk)\nand recall that this is a total ordering induced by the temporal priority, and that this is consistent with coefficient β, which means that\nβg1→v > . . . > βgk→v .\nThus, in the limit λ→ 0\nmax{mgi→v | gi ∈ G} λ→0≈ 1 + max{βgi→v | gi ∈ G} ≈ 1 + βg1→v ≈ mu→v\nis the event closer in time to v, with respect to β. This event, namely u, is chosen by the algorithm as the real cause of v.\nFinally, we show that the last step of the algorithm (the independent progression filter, step 4), does not invalidate the edge (u, v). In fact, the algorithm would replace such an edge with ( , v) if, for all nodes x backward-reachable from v (i.e., those in G∪S) it was\n1 1 + P(v) > P(x) P(x) + P(v) P(x ∧ v) P(x)P(v) .\nIt suffices thus to show that the above inequality is violated just by one of the backwardreachable nodes. We pick just u ∈ G and note that\nP(u) P(u) + P(v) P(u ∧ v) P(u)P(v) = P(u | v) P(u) + P(v) .\nAlso, we have that P(u) < 1, P(v) < 1 and, by construction, P(u | v) = 1 because all the instances of v are co-occurring with those of u (but not the converse). Thus, inequality\n1 1 + P(v) < 1 P(u) + P(v) ,\nis always true and ensures that edge (u, v) is maintained, which concludes the proof.\n141\n142 APPENDIX C. CAPRESE - SUPPLEMENTARY MATERIALS"
    }, {
      "heading" : "Proof of Corollary §1 (Uniform noise).",
      "text" : "Proof. As shown in [173], the uniform rates + and − affect the observed probabilities as follows\nP(i)∗ = P(i)(1− −) + (1− P(i)) + (C.6) P(i ∧ j)∗ = P(i ∧ j)(1− −)2 + [πij − P(i ∧ j)](1− −) + + (1− πij) 2+ , (C.7)\nwhere πij = P(i) + P(j)− P(i ∧ j). It is important to note (Lemma 1, [173]) that\nP(i) > P(j) =⇒ P(i)∗ > P(j)∗ ,\nnamely uniform noise is still implying temporal priority. Because of this, and since the raw estimate α is monotonic relative to temporal priority, all the derivations for Theorem §3 are still valid in this context, and the algorithm selects the correct genuine cause for each effect.\nTo guarantee that no valid connection is broken by the independent progressions filter, we again rely on Szabo’s result (Reconstruction Theorem 1, [173]). In particular, for any correctly selected edge (u, v) in our algorithm, since we implement Desper’s filter (or, analogously, Szabo’s) for independent progressions we do not mistake by deleting (u, v) unless also their algorithms do. Since this is not the case when + < √ pmin(1 −\n+ − −) the proof is concluded."
    }, {
      "heading" : "C.2 Synthetic data generation",
      "text" : "A set of random trees is generated to prepare synthetic tests. Let n be the number of considered events and let pmin = 0.05 = 1 − pmax, a single tree with maximum depth log(n) is generated as follows:\n1: pick an event r ∈ G as the tree root; 2: assign to each event but r an integer value in [2, log(n)] representing its depth in the\ntree, ensure that for each level there is at least one event (0 is reserved for , 1 for r); 3: for all events e 6= r do 4: let l be the level assigned to e; 5: assign a father to e selecting an event among those at which level l−1 was assigned;\n6: add the selected pair to the set of edges E; 7: end for 8: for all edges (i, j) ∈ E do 9: assign α((i, j)) a random value in [pmin, pmax];\n10: end for 11: return the generated tree;\nWhen a forest is to be generated, we repeat the above algorithm to create its constituent trees. These trees (or forests), in turn, are used to sample the input matrix for the reconstruction algorithms, with the parameters described in the main text.\n142\nC.3. FURTHER RESULTS 143"
    }, {
      "heading" : "C.3 Further results",
      "text" : "We show here the results of the experiments discussed but not presented in the main text.\nReconstruction of noisy synthetic data with λ → 0. Although we know that λ → 0 is not the optimal value of the shrinkage-like coefficient for noisy data, we show in Figure §C.1 the analogue of Figure §3.6 when the estimator is shrank by λ → 0, i.e., λ = 0.01. When compared to Figure §3.6 it is clear that a best performance of CAPRESE is obtained with λ ≈ 1/2, as suggested by Figure §3.2.\nComparison with hidden Conjunctive Bayesian Networks, h-CBNs. We here compare the performance of CAPRESE to hidden Conjunctive Bayesian Networks (hCBN) [65], as well as to oncotrees. The settings of the experiment are slightly different from those of the previous analyses: we used 100 distinct random trees of 10 events each. We ranged the number of samples available for reconstruction from 50 to 200, with a step size of 50. The settings used for running h-CBNs are relatively standard settings: we allowed for 50 annealing iterations with initial temperature equal to 1. Since h-CBNs reconstruct DAGs, it is not possible to quantify its performance using Tree Edit Distance, as we did in the comparison with oncotrees. Instead, we here adopt Hamming Distance (computed on the connection adjacency matrix), as a closely related and computationally feasible alternative for measuring performance [76].\n143\n144 APPENDIX C. CAPRESE - SUPPLEMENTARY MATERIALS\nHamming distance\nThe results of the experiment can be found in Figure §C.2, and show that CAPRESE clearly outperforms h-CBNs. In particular, it is possible to notice that, for all the analyzed values of noise and sample sizes, both CAPRESE and oncotrees display a (average) Hamming Distance between the reconstructed model and the original tree topology that is significantly lower than h-CBNs, with the largest differences observed in the noise-free case. This result would point at a much faster convergence of CAPRESE with respect to the number of samples, also in presence of moderate levels of noise.\nA few remarks are warranted about this experiment. First, in contrast to the comparison with oncotrees, we ran each experiment exactly once rather than averaging the results over 10 repetitions, and on relatively smaller trees. These limitations are a consequence of the extremely high time complexity of the simulated annealing step of h-CBNs. However, the comparison between CAPRESE and h-CBNs shows a so large difference in the performance that we do not expect this to be have significant impact. Second, the results obtained by h-CBNs are perhaps worse than expected based on results in the absence of noise presented in [72], which were however based on a unique tree topology. Yet, this outcome may have been potentially influenced by either the estimation procedure of the noise parameter in h-CBN, the adopted annealing procedure or by the used number of iterations. In future work we plan to extend our algorithm to extract more general topologies and to compare both methods in a greater detail.\nInference of models with multiple conjunctive parents. CAPRESE is specifically tailored to reconstruct models with independent progressions and a unique cause for each event (i.e., trees or forests), while other approaches such as CBNs can recon-\n144\nC.3. FURTHER RESULTS 145\nstruct models where multiple conjunctive parents co-occur to cause an effect (i.e., a ∧ b cause c). It is thus reasonable to use such conjunctive approaches to infer more complex model, in spite of CAPRESE.\nHowever, it is interesting to asses CAPRESE’s performance when (synthetic) data are sampled from a model with multiple parents and noise. By sampling input data from random directed acyclic graphs with 10 nodes and where each event is caused by at most 3 conjunctive events (randomly assigned), we assess the number of false positives and false negatives retrieved in the model reconstructed with CAPRESE. We show the results in Figure §C.3. Our results indicate that for increasing sample size, the number of false positives approaches 0. Thus, for sufficiently large number of samples, all the causal claims returned by CAPRESE are true. In addition, the number of false negatives is always higher and proportional to the connectivity of the target model. This is to be expected since CAPRESE assigns at most one parent (the cause) to every node.\n145\nAPPENDIXD\nCAPRI - SUPPLEMENTARY MATERIALS\nIn this Chapter we will report the supplementary materials related to chapter §4."
    }, {
      "heading" : "D.1 Theorems",
      "text" : "The statements and proofs of the theorems mentioned in the main text follow."
    }, {
      "heading" : "D.1.1 Complexity",
      "text" : "Let U denote the universe of all possible patterns over a set G of n events, as before. Since |U| is exponential in |G|, then the following theorem holds.\nTheorem 4 (Asymptotic complexity). Let |G| = n and D ∈ {0, 1}m×n where m n, and let N be the nodes in the DAG returned by CAPRI, the worst case time and space complexity (ignoring the cost of bootstrap) of building a selectivity topology is:\n• Θ(mn) time and Θ(n2) space, if Φ = ∅;\n• Θ(|Φ|mn) time and Θ(|Φ|m) space, if Φ ⊂ U and |N | m (i.e., there are sufficiently many samples to characterize the input hypotheses);\n• O(22n) time and space, if Φ = U .\nThus, the overall complexity of CAPRI is one of the above, as suitable in each case, plus the complexity of likelihood fit with regularization.\nProof. Recall that k = |Φ|, n = |G| and D ∈ {0, 1}m×n, thus D(Φ) has K = (n + k)m entries. We now analyze the complexity of CAPRI step-by-step.\n• The cost of lifting depends on the input set Φ, if Φ = ∅ it is O(1) both in time and space since D(∅) = D. For non-empty sets, it is necessary to evaluate k ·m\n146\nD.1. THEOREMS 147\nentries, after each hypothesis ϕ e is evaluated. Given that every ϕ has at worst n events included, its evaluation cost is at most O(n), even if lazy evaluation is performed. Thus, the cost of lifting is Θ(k ·m · n), for a single bootstrap, which amplifies the bootstrap cost, as discussed in the previous section, and does so in a multiplicative fashion. In terms of space, if Φ 6= ∅ the overhead is Θ(K) if one copies D in D(Φ), Θ(km) otherwise.\n• The cost of computing the parent function for the DAG requires a pair-wise calculation of the probabilistic scores, plus the cost of testing the v relation1. Let w = |N |, where N is the set of nodes in the DAG returned by CAPRI. The score matrices for temporal priority and probability raising are n×w, i.e., have columns for both atomic events and the disjunctive patterns in the formulas of Φ, since CAPRI disregards patterns of the form ϕi ϕj and a ϕ (differently, it would have been w×w). With the simplest membership test algorithm, checking whether an atomic event is present in a patterns is logarithmic in the size of the pattern, if we lexicographically order its atomic events, thus bounded from above by log n. Thus if we perform lazy evaluation for v the total number of comparison to select the parent function is at most\nn[(n− 1) + (w − n) log n],\nyielding a Θ(n2) cost in time and space, if w−n is small (it is 0 if Φ = ∅), O(n(w− n) log n) otherwise. In terms of space, the complexity is Θ(n[(n − 1) + (w − n)]), for a general Φ.\n• As explained in CAPRI’s definition, sometimes, albeit extremely rarely, a few extra operations might have to be performed when degenerate scores and loops are present. The procedure we suggested in CAPRI’s definition requires sorting plus scan, thus its worst-case time complexity is O(n log n). Clearly, as this term is omitted in the worst-case complexity analysis of the steps discussed above, this unlikely scenario does not alter the complexity of the algorithm.\n• Note that the cost of this analysis does not include the cost of BIC/likelihood - or any regularization strategy one might adopt, as spelled out in the theorem statement2.\nThe overall complexity follows, since:\n• Φ = ∅ then the major cost is that of evaluating P(·) since usually m n, thus mn > n2. With regard to space, the only cost is that of book-keeping the scores.\n1Relation v represents the usual syntactical ordering relation among atomic events, e.g., a, b, and formulas, e.g., a v (a ∨ b) ∨ c ∨ d.\n2Since in the current version of CAPRI, the likelihood fit is computed by a hill climbing heuristic algorithm, the overall cost of CAPRI is still polynomial.\n147\n148 APPENDIX D. CAPRI - SUPPLEMENTARY MATERIALS\n• Let m n and w − n > k, in this case since km n and, under the mild assumption that m > w and that k and log n are not relevant (in size) for m and w, then km (w − n) log n which is the cost of lifting; thus is Θ(kmn) in time. Similarly, it follows that mk n[(n− 1) + (w − n)].\n• By computations similar to those carried out, it is indeed possible to see that U , which is clearly finite since G is, grows double-exponentially in size with |G| (i.e. the number of n-ary boolean functions, defined over the atomic events in any pattern, possibly with negated literals). Thus the bound follows."
    }, {
      "heading" : "D.1.2 Correctness and expressivity",
      "text" : "Let W ⊆ U be the set of true patterns, which we seek to infer. Here, we investigate the relation between W and the patterns retrieved by CAPRI, as a function of sample size m and error present as false positives/negatives, which are assumed to occur at rates + and −.\nHereafter, Σ denotes the set of patterns, implicit in the DAG returned by our algorithm for an input set Φ and a matrix D; we write this fact as D(Φ) Σ. We prove the following theorems3.\nTheorem 5 (Soundness and completeness). Let the sample size m → ∞ and the data be uniformly randomly corrupted by false positives and negatives rates − = + ∈ [0, 1). If the given input is a superset of the true patterns, then CAPRI reconstructs exactly the true patterns in W, that is, W ⊂ Φ ⇒ D(Φ) W ∩ Φ.\nProof. We first prove the case with + = − = 0, that is, the case where data have no noise. Some notations, used below: (i) we denote with ϕ e true patterns (i.e. in W), and (ii) with ϕ∗ e false ones. We divide the proof into several steps:\n• First, we show that a selectivity DAG contains all the true patterns, which is\n∀ϕ e∈W π(e) = {ϕ} .\nBy the event-persistence property usually valid for cancer genomes (fixating mutations are present in the progeny of a clone) the occurring times satisfy tϕ < te which, in a frequentist sense, implies P(ϕ) > P(e). In addition, it holds by construction that P(ϕ ∧ e) = P(e) when + = − = 0, thus P(e | ϕ) = P(e)/P(ϕ), which is strictly positive since P(ϕ) and P(e) are, and that P(ϕ ∧ e) = 0 , thus P(e | ϕ) = 0. Notice that e 6v ϕ by hypothesis.\n• Now, we show that it might contain also spurious patterns, which is\n∃ϕ∗ e 6∈W π(e) ⊆ clauses (ϕ∗) ∪ {ϕ∗} . 3These results assume a BIC regularisation but hold for any convergent regularization score.\n148\nD.1. THEOREMS 149\nThese ϕ∗ e are of two types: sub-formulas spurious or topologically spurious (which include transitivities, as we may recall). For the former case note that\n∀ϕ e∈W ∀ϕ̂∗∈clauses(ϕ) ϕ̂∗ e 6∈ W,\nbut satisfies both temporal priority and probability raising. Also, consider any other ϕ̂∗? v ϕ̂∗ and note that even this might satisfy both temporal priority and probability raising. For the latter case, it might be that there exists some other ϕ∗ such that, it is positively statistically correlated to a real pattern, and that might satisfy Suppe’s conditions as well.\nThus, for any e ∈ G such that ϕ e ∈ W\nπ(e) = {ϕ} ∪ S,\nwhere S is a set of spurious patterns. We now examine the relation holding between the selectivity DAG and its modification performed via BIC. The derivations shown in the following hold regardless of the type of regularization which enjoys convergency.\nWe denote these DAGs as Dpf and DBIC.\n(i) First, we show that all true patterns in Dpf are in DBIC, i.e.\n∀ϕ e∈W πBIC(e) = {ϕ} .\nNote that, although in general P(a ∧ b) ≤ min{P(a),P(b)}, for the true patterns the following holds: P(ϕ ∧ e) = P(e), when + = − = 0; it is the maximum value for this joint probability, thus ensuring the maximum-likelihood fit. Thus the pattern is maintained in DBIC.\n(ii) Second, we need to show that if ∀ϕ∗ e 6∈ W but present in Dpf, there exists a pattern ϕ e ∈ W, which is present in Dpf and in DBIC and any ϕ∗ e is not in DBIC. Note that P(ϕ ∧ e) = P(e), as above. Instead, P(ϕ∗ ∧ e) < P(e) since it is spurious, hence P(ϕ ∧ ϕ∗ ∧ e) < P(ϕ ∧ e), thus the likelihood fit of ϕ e is maximal with respect to any of the patterns ϕ∗ e.\nTo extend the proof to + = − ∈ [0, 1) with uniform noise, it suffices to note that the marginal and joint probabilities change monotonically as a consequence of the assumption that the noise is uniform. Thus, all inequalities used in the preceding proof still hold, which concludes the proof.\nNotice that if it could be assumed that Φ characterizesW well, then all true patterns would be in Φ, and the corollaries below follows immediately.\nCorollary 2 (Exhaustivity). Assuming the same hypothesis as for the theorem above, D(U) W.\n149\n150 APPENDIX D. CAPRI - SUPPLEMENTARY MATERIALS\n2\nCorollary 3 (Least Fixed Point). W is the lfp of the monotonic transformation⊔ Φ D(Φ) ≡ D (⊔ Φ Φ ) W. 2.\nSince a direct application of this theorem incurs a prohibitive computational cost, it only serves to idealize the ultimate power of the framework we have proposed. That is, the theorem only states that CAPRI is able to select only the true patterns asymptotically (in the sample size), regardless of how the putative hypotheses size U grows, e.g., in the worst-case exponentially. It also clarifies that the algorithm is able to “filter out” all the spurious patterns (true negatives), and produces the true positives more and more reliably as a function of the computational and data resources.\nNow we restrict our attention to co-occurrence types of patterns so as to enable a fair comparison with [10]. We denote with C ⊂ U the set of all possible such patterns, and we prove the following.\nTheorem 6 (Inference of co-occurrence patterns). Suppose Φ = ∅; as before, let the sample size m → ∞ and let the data be uniformly corrupted by false positives and negatives rates − = + ∈ [0, 1). Then only co-occurrence patterns on atomic events are inferred, which are either true or spurious for general CNF formulas. That is: if D(∅) Σ then Σ ⊆ C. Furthermore,\n1. Σ ∩W are true patterns and\n2. For any other pattern α e ∈ (Σ \\ Σ ∩W) there exist β e ∈ W \\ C such that β screens off α from e.\nProof. Consider the proof of the previous theorem. In this case, we am dealing with formulas such that clauses (ϕ) ⊆ G, i.e., formulas do not have any disjunctive component. All the derivations for Theorem 2 can be carried out in this context, notice that: formulas considered in step (i) of such a proof are those which are purely conjunctive and correctly inferred. Similarly, formulas in (ii) are those that screen off the false patterns, but are incorrectly present in DBIC.\nThis theorem states that, even if one is neither willing to pay the cost of augmenting CAPRI’s input with patterns nor able to find any suitable one, the algorithm is still capable of inferring singleton and conjunctive instances of relation, whose members are either true or part of a more complex types of patterns that fall outside CAPRI’s scope. An immediate corollary of these two theorems is that CAPRI works as specified, when it is fed with all possible co-occurrence patterns.\nCorollary 4. Under the hypothesis of the above theorems, D(∅) Σ ⇐⇒ D(C) Σ. 2\nIn practice, this algorithm, though still exponential, is certainly less computationally intensive. For instance, when using C than with U , it can trade off computational complexity against expressivity of the inferred patterns.\n150\nD.2. RESULTS: SYNTHETIC DATA 151"
    }, {
      "heading" : "D.2 Results: synthetic data",
      "text" : "Setting for comparison. The performance of all the algorithms were evaluated empirically with four different types of topologies: (i) trees, (ii) forests, (iii) DAGs without disconnected components and (iv) DAGs with disconnected components. Irrespective of the topology considered, we exclusively used atomic events, which implies that either singleton or co-occurrence patterns were used in the experiments. Based on Corollary 3, it sufficed to run CAPRI with Φ = ∅. This strategy is consistent with the fact that our algorithm can infer more general formulas if an input “set of putative causes, Φ 6= ∅” is given in addition – a fact which, without the care taken, could have unfairly and favorably biased our analysis in the more general situation. For the sake of completeness, however, we also tested specific CNF formulas, as shown in the next sections.\nType (i − ii) topologies are DAGs constrained to have nodes with a unique parent; condition (i) further restricts such DAGs to have no disconnected components, meaning that all nodes are reachable from a starting root r. Practically, condition (i) satisfies |π(j)| = 1 for j 6= r, and π(r) = ∅, while in (ii) we allow more roots to be present. This kind of topologies can be reconstructed with either ad-hoc algorithms [117, 38, 173] or general DAG-inference techniques [171, 179, 10, 167, 79]. Type (iii− iv) topologies are DAGs which have either a unique starting node r, or a set of independent sub-DAGs. Similarly, condition (iii) satisfies |π(j)| ≥ 1 for j 6= r, and π(r) = ∅, while in (iv) we allow more roots to be present, as it was in (ii). This kind of topologies are not reconstructible with tree-specific algorithms, and thus only algorithms in [171, 179, 10, 167, 79] could be used for comparison. The algorithm for the synthetic data generation is described in the following paragraph.\nGenerating synthetic data. Let n be the number of events we want to include in a DAG and let pmin = 0.05, pmax = 0.95, pmin = 1 − pmax. A DAG without disconnected components (i.e. an instance of type (iv) topology) with maximum depth log n and where each node has at most w∗ parents (i.e. |π(j)| ≤ w∗, for j 6= r) is generated as follows:\n1: pick an event r ∈ G as the root of the DAG; 2: assign to each j 6= r an integer in the interval [2, dlog ne] representing its depth in\nthe DAG (1 is reserved for r), ensure that each level has at least one event; 3: for all events j 6= r do 4: let l be the level assigned to e; 5: pick |π(j)| uniformly over (0, w∗], and accordingly define π(j) with events selected among those at which level l − 1 was assigned; 6: end for 7: assign α(r) a random value in the interval [pmin, pmax]; 8: for all events j 6= r do 9: let y be a random value in the interval [pmin, pmax], assign\nα(j) = y ∏\nx∈π(j)\nα(x) ;\n151\n152 APPENDIX D. CAPRI - SUPPLEMENTARY MATERIALS\n10: end for 11: return the generated DAG;\nWhen an instance of type (iv) topology is to be generated, we repeat the above algorithm to create its constituent DAGs. In this case, if multiple DAGs are generated, each one with randomly sampled ni events we require that |G| = ∑ ni = n. When instances of type (i) topology are required w∗ = 1, and by iterating multiple independent sampling instances of type (ii) topology are generated. When required DAGs were sampled, these are used to generate an instance of the input matrix D for the reconstruction algorithms.\nD.2.1 Performance with different topologies and small datasets\nHere we estimate the performance of CAPRI for datasets with sizes that are likely to be found in currently available cancer databases, such as The Cancer Genome Atlas, TCGA [136], i.e. m ≈ 250 samples, and 15 events. The results are shown in Figure §D.1, for topologies (i) and (ii), and Figure §D.2, for topologies (iii) and (iv). There, we show all the results obtained by running the algorithm with bootstrap resampling, although results (data not shown) without this pre-processing leave the conclusions unaffected.\nResults suggest a trend, as to be expected: namely, performance degrades as noise increases and sample size diminishes. However, it is particularly interesting to notice that, in various settings, CAPRI almost converges to a perfect score even with these small datasets. This happens for instance with type (i − ii) topologies, where the Hamming distance almost drops to 0 for m ≥ 150. In general, it is also clear that reconstructing forests is easier than trees, when the same number of events n is considered. This is a consequence of the fact that, once n is fixed, forests are likely to have less branches since every tree in the forest has less nodes. When reconstructing type (iii − iv) topologies, instead, the convergence-speed of CAPRI to lower Hamming distance is slower, as one might reasonably expect. In fact, in those settings the distance never drops below 3, and more samples would be required to get a perfect score. We consider this to be a remarkable result, when compared to the worst-case Hamming distance value of 15 · 14 = 210. Panels of Figure §D.2 also suggest that disconnected DAGs are easier to reconstruct than connected ones, when a fixed number of events is considered. Similarly to the above, this could be credited to the fact that the size of the conjunctive claims is generally smaller, for fixed n. With respect to the precision and recall scores, one may note that CAPRI seems to be quite robust to noise, since the loss in the score-values appear nearly unaffected by any increase in the noise parameter.\n152\nD.2. RESULTS: SYNTHETIC DATA 153\nD.2.2 Comparison with other reconstruction techniques\nWe compare now with state-of-the-art approaches mentioned in the main text4, which we divide into three categories: structural - Incremental Association Markov Blanket (IAMB) and PC algorithm -, likelihood - Bayesian Information Criterion (BIC) and Bayesian Dirichlet (BDE) and hybrid - Conjunctive Bayesian Networks (CBN) and Cancer Progression Inference with Single Edges (CAPRESE). For all the algorithms we used their standard r implementations: for IAMB, BDE and BIC we used package scutari2009learning [168], for the PC algorithm we used package pcalg, for CAPRESE we used TRONCO [4] (first release) and for CBN we used h-cbn [11].\nClearly, other algorithms exist in the literature, but we selected those which satisfied at least one of the following criteria: earlier, they have proven to be more effective in inferring “causal” claims, i.e., they are considered the best algorithms to infer “causal networks” (i.e., IAMB and PC); they regularize the Bayesian over-fit (i.e., BDE and BIC); they assume a prior (i.e. BDE) or they were developed specifically for cancer progression inference (i.e., CBN and CAPRESE). Prominent among the ones absent in this study are the following: Grow and Shrink [124], which preliminary analysis have shown to be very similar to IAMB, and the DiProg algorithm [50], which unrealistically requires advanced knowledge of input error rate to reconstruct a model; note that this kind of information is not generally available a priori.\nNotice that we selected all the algorithms capable of inferring generic DAGs but CAPRESE [117], which can only be applied to infer trees or forests (i.e., type (i − ii) topologies). In the literature there exist other approaches specifically tailored for such topologies, e.g., [38, 173]; however, since in [117] it is shown that CAPRESE performs better than other approaches, we assume no loss of information in restricting our study. We place CAPRI in the Hybrid category, though we clearly compare its performance with all the other approaches in order to quantify its suitability for reconstruction of all classes of topologies, as defined earlier.\nThe general trend is summarized in Figure §D.3, where we rank all of these algorithms according to their median performance, estimated as a function of noise and sample size, and provide the parameters used for comparison. In Figure §D.4, we compare CAPRI with the structural approaches (IAMB and PC). In Figure §D.5, we compare it with the likelihood approaches (BIC and BDE) and, finally, in Figure §D.6, we compare it with the hybrid algorithms. We remark that, because of the high computational cost of running CBNs, which relies on a nested Expectation-Maximization algorithm with Simulated Annealing, the number of ensembles performed is limited to 100 for CBNs, while it is 1000 for all other algorithms. Though this strategy provides less robust statistics for CBNs (i.e., less “smooth” performance surfaces), it is still sufficiently accurate to indicate the general comparative trends and relative performance efficiency.\n4Classic versions of the IAMB and PC algorithm were further subjected to log-likelihood optimization to assign a direction to all of the computed non-oriented edges. This additional feature is necessary to permit a fair comparison against various structural approaches, which, otherwise, would be penalized with a worse Hamming distance, since these algorithms, in principle, can return non-oriented edges. Note that progression models, by their very nature, consist only of oriented structures.\n153\n154 APPENDIX D. CAPRI - SUPPLEMENTARY MATERIALS\nD.2.3 Reconstruction without hypotheses: disjunctive patterns\nRecall that our algorithm expects as input all the hypothesized patterns to infer more expressive logical formulas, i.e., hypotheses with pure CNF formulas or even disjunctive patterns over atomic events. Nonetheless, it is instructive to investigate its performance under two specific conditions, especially to clarify the robustness with respect to imperfect regularities (the, e.g, “noisy and”): namely, (i) without hypotheses (Φ = ∅) and (ii) for datasets sampled from topologies with disjunctive patterns.\nTo generate the input dataset, we have to modify the generative procedure used for the other tests, thus reflecting the switch from co-occurrence to disjunctive patterns. This task is actually rather simple, since we just change the labeling function α to account for the probability of picking any subset of the clauses in the disjunctive pattern, while omitting the others. We use DAGs with 10 events and disjunctive patterns with at most 3 atomic events involved, which is a reasonable size, given the events considered. Clearly, this setting is generally harder than the one shown in Figures §D.4– §D.6, thus we expect performance to be somewhat inferior.\nHere we compare CAPRI with all the algorithms used so far, and we show the result of this comparison in Figure §D.7, where Φ = ∅, as noted earlier. The plot clearly confirms the trends suggested by previous analyses: namely, CAPRI infers the correct patterns more often than the others. Note also that the performance is measured on the reconstructed topology only, since, without input hypotheses, the algorithm evaluates only co-occurrence types of patterns, and does not allow different types of relations (e.g. disjunctions) to be inferred automatically. However, as anticipated, observed performance improvement is now much lower, and the Hamming distance fails to rise above 4. Furthermore, convergence to optimal performance was not observed for m ≤ 1000, and it appears not to be reachable even for m 1000 (at least so, when no hypotheses are used). It is also possible that, as n and the number of maximum disjunctive patterns increase, the result could be an even less satisfactory speed of convergence.\nD.2.4 Reconstruction with hypotheses: synthetic lethality\nWe wondered whether CAPRI would be able to infer synthetic lethality relations, when these are directly hypothesized in the input set Φ. We started with a test of the simplest form: e.g., [a ⊕ b c], for a set of events G = {a, b, c}, where we force progression from a to c to be preferential, i.e. it appears with 0.7 probability, whereas b to c does so with only 0.3 probability, thus implying that samples involving (a ∧ b̄) will be more abundant than those involving (ā ∧ b). Despite this being the smallest possible synthetically lethal pattern, the goal was to estimate the probability of such a pattern being robustly inferable, when Φ = {a⊕b c}, and its dependence on the sample size and noise. We measured the performance of all the algorithms, with an input lifted according to the pattern so that all algorithms start with the same initial pieces of information. The performance metric estimates how likely an edge from a⊕ b to c could be found in the reconstructed structures.\nWe show the results of this comparison in Figure §D.8. We note that CAPRI suc-\n154\nD.2. RESULTS: SYNTHETIC DATA 155\nceeds in inferring the synthetic lethality relation more frequently than 93% of the times, irrespective of the noise and sample size used. More precisely, with m ≥ 60 the algorithm infers the correct pattern under any execution, thus suggesting that CAPRI, with the correct input hypotheses, is able to infer complicated structures, many of which could have high biological significance. Naturally, it would be reasonably expected that the performance of any of these algorithms would drop, were the target relations part of a bigger model."
    }, {
      "heading" : "D.2.5 Execution time",
      "text" : "We report in table §D.1 an evaluation of the execution time for all the algorithms we tested, but CBN - which computation time is more than one order of magnitude higher than the competing techniques. Two distinct settings of experiments were used: Setting (A): n = 10 events, m = 100 samples, ν = 0 noise; Setting (B): n = 10, m = 100, ν = .10. Results account for the average time of execution as of 100 randomly generated topologies (one dataset sampled per topology). Time unit is second and the test was performed on a MacBook with 2.3 GHz Intel i7 processor, 16 Gb of RAM and Yosemite 10.9 OS.\nTo allow a fair comparison of CAPRI against the other algorithms we both executed the algorithm with and without bootstrap preprocessing, in order to asses the prima facie condition (Mann-Withney U test being performed in the former case). Execution timings are sorted according to mean time.\n155\n156 APPENDIX D. CAPRI - SUPPLEMENTARY MATERIALS"
    }, {
      "heading" : "D.3 Biological examples",
      "text" : "In this Section two examples of analysis of genomic alterations will be presented."
    }, {
      "heading" : "D.3.1 Atypical Chronic Myeloid Leukemia",
      "text" : "Input hypotheses for CAPRI (supervised mode)\nBy fetching the literature we selected the following patterns to input as CAPRI’s hypotheses:\n(1) “exclusivity among asxl1 and sf3b1 mutations” [115]:\n(asxl1 Nonsense point ⊕ asxl1 Ins/del ) ⊕ sf3b1 Missense point\n(1) “exclusivity among tet2 and idh2 mutations” [53]:\n(tet2 Nonsense point ⊕ tet2 Missense point ⊕ tet2 Ins/del ) ⊕ idh2 Missense point\nThese patterns were used to build CAPRI’s hypotheses which were tested against all events which do not appear in the above pattern itself, e.g., pattern (1) was tested against all input events but those involving asxl1 and sf3b1 genes.\nAs shown in the main text, among all, the following hypothesis gets selected by CAPRI\n(asxl1 Nonsense point ⊕ asxl1 Ins/del ) ⊕ sf3b1 Missense point cbl Missense point\naCML progression model with different techniques\nIn Figure §D.9 one can find the progression models reconstructed on the the aCML dataset [154], with 3 different algorithms: (i) CAPRESE, (ii) BIC and (iii) IAMB. These three techniques were chosen for this comparative study because of the overall better performance on synthetic tests (see Section 3.2-3.4 of the SI). The reconstruction obtained with CAPRI can be found in Figure 5 in the main text. For a biological interpretation of the results please refer to Section 4.2 in the main text.\nNote that all the progression model share some specific selective advantage relations, yet being substantially different. Relations involving setbp1 and asxl1 and those involving tet2 and ezh2 are, in fact, inferred by all the four algorithms, yet with different confidences and, sometimes, edge direction. In addition, IAMB does not include cbl in the path involving setbp1 and asxl1, and none of the algorithms but CAPRI can infer the complex pattern involving asxl1 mutations of both types and sf3b1 (Figure 5 in the main text). Finally, note that IAMB and BIC are often not able to disambiguate the edge direction and this represent a major limit of these techniques with respect to CAPRI and CAPRESE.\n156\nD.3. BIOLOGICAL EXAMPLES 157"
    }, {
      "heading" : "D.3.2 Ovarian cancer",
      "text" : "Ovarian cancer progression model with different techniques\nWe analyzed an ovarian cancer dataset reporting chromosome-level amplifications and deletions detected via Comparative Genome Hybridization in [100]. Similar to the case of aCML, we used 4 different techniques to infer a progression models for events included in the dataset: CAPRI (unsupervised), CAPRESE, BIC and IAMB. Models and input dataset are shown in Figure §D.10. Like with aCML extraction, the progression models share only some of the inferred relations. Among the most relevant differences is the conjunctive pattern inferred by CAPRI between the loss on chromosome 5q (5q−) and the gain on chromosome 8q (8q+) which is predicted to select for a loss on 8p; note also the aforementioned limitation of BIC and IAMB in disambiguating the direction of some of the inferred relations. Note that CAPRI infers a co-occurrence pattern of selective advantage which is not input a priori as hypothesis - unsupervised execution. In summary, CAPRI displays a better overall confidence on the reconstructed model.\n157\nFigure D.1: Reconstruction of trees and forests with small datasets. Hamming distance, precision and recall of CAPRI for synthetic data generated by trees (i.e., models with a singleton pattern per event and a unique progression), in top panels, and by forests (i.e., models with a singleton pattern per event but multiple independent progressions), in bottom panels. In both cases n = 15 events are considered, m ranges from 50 to 250 and the noise rate ranges from 0% to 20%. To have a reliable statistics, for each type of topology, we generate 100 distinct progression models and, for each value of sample size and noise rate, we sample 10 datasets from each topology. Thus, every performance entry is the average of 1000 reconstruction results. Notice that Hamming distance almost drops to 0 for m ≥ 150 and that precision and recall decrease very little as noise increases.\nFigure D.2: Reconstruction of DAGs with small datasets. Hamming distance, precision and recall of CAPRI for synthetic data generated by connected DAGs (i.e., models with either a singleton or co-occurrence pattern per event and a unique progression), in top panels, and by disconnected DAGs (i.e., models with either a singleton or co-occurrence pattern per event and multiple progressions), in bottom panels. In both cases the same parameters as in Figure §D.1 are used (n = 15, 50 ≤ m ≤ 250, 0% ≤ ν ≤ 20% and every performance entry is the average of 1000 reconstructions). In this setting, which is harder than the one shown in Figure §D.1, Hamming distance does not reach values below 3 – a reasonably small number for our purposes – while precision and recall still suffer very little as noise increases.\nParameter values\nn number of events 10 m number of samples [50, 1000] ν rate of false positives + and negatives − [0, 0.2] (0%-20% noise rate) − ensemble size 1000 (100 for CBN)\nFigure D.3: Co-occurrence patterns: performance ranking. We rank the algorithms we compared in Figure §D.4, §D.5 and §D.6 according to their performance for the parameters in the table. Rankings are divided according to the topology type and sorted according to the median performance.\nFigure D.4: Comparison with related works: structural algorithms. We compare CAPRI, IAMB and the PC algorithm to infer trees, forests, connected DAGs and disconnected DAGs with the parameters described in Table §D.3. Average Hamming distance, precision and recall are shown.\nFigure D.5: Comparison with related works: likelihood-based algorithms. We compare CAPRI against likelihood-based methods optimizing BIC and BDE scores to infer trees, forests, connected DAGs and disconnected DAGs with the parameters described in Table §D.3. Average Hamming distance, precision and recall are shown.\nFigure D.6: Comparison with related works: hybrid algorithms. We compare CAPRI, CBNs and CAPRESE to infer trees, forests, connected and disconnected DAGs with the parameters of Table §D.3 but, because of the computational cost of running CBNs with 100 annealing steps, we reduced the number of ensembles performed as: 100 for CBNs, 1000 for CAPRESE and, for CAPRI, 100 for DAGs and 1000 otherwise. Average Hamming distance, precision and recall are shown.\nFigure D.7: Reconstruction of disjunctive patterns with no hypotheses. We compare CAPRI against all the algorithms to infer progressions with disjunctive patterns. In top panel we show IAMB as the best structural algorithm, and the BIC score as the best among likelihood-based methods, according to Table §D.3. In bottom panel we compare the other algorithms. No hypotheses (Φ = ∅) are given as input to CAPRI. Input data is generated by DAGs with 10 atomic events and disjunctive patterns with at most 3 atomic events involved. Sample size ranges from 50 to 1000, noise rate from 0% to 20% and 1000 ensembles are generated for each configuration of noise and sample size. This setting is generally harder than the one shown in Figures §D.4– §D.6. Hamming distance, precision and recall are shown and confirm that this type or pattern is harder than the co-occurent one to be inferred, hinting at the difficulty of modeling unbalanced confluent progressions.\nFigure D.8: Reconstruction with hypotheses: synthetic lethality. We show the average probability of inferring a claim a ⊕ b c (synthetic lethality), when this is provided in the input set Φ. We show such a probability for CAPRI, the likelihood-based algorithms with BIC and BDE scores, and the structural IAMB and PC Algorithm. Data is generated from the model in the upper left panel (unbalanced “exclusive or” with a preferential progression), samples size ranges from 30 to 120, noise rate from 0% to 20% and 1000 ensembles are generated for each configuration of noise and sample size. Results suggest that a threshold level on the number of samples exists such that CAPRI infers the correct claim when Φ = {a⊕ b c}. We executed all the algorithms with an input matrix lifted to contain the target claim.\nProgression model inferred by CAPRESE (nodes scaling factor .4)\nCSF3R\nSETBP1\nNRAS TET2EZH2\nCBL IDH2TET2 EZH2\nCSF3R\nCBL\nASXL1TET2\nEZH2 ASXL1\n11% 0.19 26% 0.17 38% < .01\n19% 0.134% < .0141% < .0130% 0.066% 0.13\n26% 0.1311% 0.1 5% 0.17\n14% < .01\nProgression model inferred by BIC (nodes scaling factor .45)\nSETBP1\nTET2 EZH2\nCBL\nTET2EZH2\nCBL\nASXL1\n20% < .01\n29% < .01\n42% < .0168% < .01\n8% < .01\nProgression model inferred by IAMB (nodes scaling factor .2)\nSETBP1 TET2 EZH2\nTET2CBLASXL1\n20% < .01 33% < .01 20% < .01 Events type\nIns/Del Missense point Nonsense point Events frequency 2% EZH2 (min) 22% SETBP1 (max) Sample size n = 64 m = 16 |G| = 9\nFigure D.9: CAPRESE, IAMB and BIC progression models of aCML. Progression models reconstructed from the aCML dataset described in the main text - taken from [154] - obtained with the following algorithms: CAPRESE, IAMB and BIC. The model inferred by CAPRI is shown in the Main Text. Confidence shown is assessed as for the CAPRI algorithm. Nodes are scaled differently to better layout the graphs reconstructed by every algorithm.\nOvarian (CGH) n = 87 m = 7 |G| = 7\n70% 8q+\n55% 3q+\n53% 5q−\n51% 4q−\n47% 8p−\n44% 1q+\n43% Xp−\nhits hits 7\n0\nCAPRI (scaling nodes factor .35)\n8q+\n3q+\n5q-\n4q-\n8p-\nXp-\n83% < .0196% < .01 63% < .01 62% < .01\n69% < .01\nEvents type\nGain Loss Patterns Co-occurence Events frequency 42% Xp- (min) 70% 8q+ (max) Sample size n = m = |G| = 0\nCAPRESE (scaling nodes factor .45)\n8q+\n3q+\n5q-\n4q-8p-\nXp-\n93% < .01 49% < .01 53% < .01\n62% < .01\nBIC (scaling nodes factor .45)\n8q+\n3q+\n5q-\n4q- 8p-\nXp-\n88% < .01\n76% < .01 52% < .01\n60% < .01 72% < .01\nIAMB (scaling nodes factor .2)\n8q+\n3q+\n5q-\n4q-8p-\nXp-\n84% < .01 60% < .01\n59% < .0155% < .0138% < .01\nFigure D.10: CAPRI, CAPRESE, IAMB and BIC inferred progression models of ovarian cancer. Progression models reconstructed from the ovarian cancer Comparative Genome Hybridization dataset shown in top [100]. Algorithms used to infer the models are CAPRI, CAPRESE, IAMB and BIC. Confidence is shown as non-parametric bootstrap and hypergeometric test (p-values). Nodes are scaled differently to better layout the graphs reconstructed by every algorithm."
    }, {
      "heading" : "APPENDIX E",
      "text" : "CRC STUDY - SUPPLEMENTARY MATERIALS\nHere we detail all the steps implemented to perform the pipeline widely discussed in Chapter §6. The source code to replicate this study is available for download along with the documentation detailing all the implementation at:\nhttp://bimib.disco.unimib.it/index.php/Tronco\nE.1 TCGA COADREAD project data\nCOADREAD provides genome-scale analysis of samples with exome sequence, DNA copy number, promoter methylation, messenger RNA and microRNA expression data which we used to define “training” and “control” datasets. In both validation and control datasets, only samples with both mutations and CNAs profiles were used. Table §E.1 details each of the four datasets.\nTraining dataset. Samples published in [137] were used as training; for these samples, TCGA provides somatic mutation profiles and high-resolution focal CNAs via GISTIC. These are obtained from TCGA data freeze as of 2 February 2012, downloaded on 12 March 2015, from the repository:\nhttps://tcga-data.nci.nih.gov/docs/publications/coadread 2012/\nThe following files were processed to produce the training data:\n• TCGA CRC Suppl Table2 Mutations 20120719.xlsx. Somatic mutations profiles obtained via whole-exome sequencing of 224 colorectal tumors1. All annotated mutations were considered for analysis;\n115995 mutations in 228 samples are annotated in a Manual Annotation Format (MAF). Samples were selected to univocally match the 224 patients as of the TCGA guidelines for aliquote disambiguation, see https://wiki.nci.nih.gov/display/TCGA/TCGA+barcode.\n168\nE.1. TCGA COADREAD PROJECT DATA 169\n• crc gistic.txt.zip. Focal Copy Number Alterations (CNAs) for 564 patients derived from whole-genome sequencing using the Illumina HiSeq platform. High-level gains and homozygous deletions were considered for analysis by selecting entries with GISTIC scores ±2;\n• crc clinical sheet.txt. Clinical data summary with patient stage and Micro Satellite Stabe/Instable (MSS/MSI) status being any of: MSS, MSI-high and MSIlow.\nThe list of patients used was first reduced to those having both CNAs and somatic mutation data, and then was split in two groups: MSI-HIGH and MSS. The training cohort has 152 MSS and 27 MSI-HIGH samples; samples flagged as low MSI were excluded from the study as they have not been shown to differ in their clinicopathologic features or in most molecular features from MSS tumors [149].\nValidation dataset. For samples collected afterwards the consortium provides raw sequencing data, and CNAs (also in the file crc clinical sheet.txt). Thus, reads were processed to produce mutation profiles.\nBowtie 2.0 software was used to align sequences over the human reference genome HG19 [105]. To refine the data, reads unmapped, reads with unmapped mate, not primary alignments, and reads that were PCR or optical duplicates were discarded from the study (http://picard.sourceforge.net/). We also executed a local realignment around indels defined in SnpDB [169] and 1000G [29]. Variant calling was executed with GATK software and low quality variants (mapping quality below 30 or read depth below 10), were discarded [37]. Germline variants were also removed, i.e., variants that were present in non-tumoral samples, and variants reported in the 1000G project. Finally, variants were annotated using the SeattleSeq Variant Annotation web tool [140].\nThe input cohort of patients was partitioned according to the number of mutations per sample r, see Figure §E.2; a curated cutoff of 500 mutations per sample was used to determine 203 samples with MSS status (r < 500) and 36 MSI-HIGH samples (r ≥ 500). The former group had mutations in 14580 genes, and the latter in 15071.\nE.1.1 Driver events selection\nWe have selected 33 genes annotated to 5 pathways as drivers of colorectal tumorigenesis [137]. These are well-known cancer genes, frequently reported as relevant to colorectal progression and to the major pathways involved in CRC. Driver events are alterations in:\n• wnt genes (14): apc, dkk-4, tcf7l2, ctnnb1, lrp5, fbxw7, dkk-1, fzd10, arid1a, dkk-2, fam123b, sox9, dkk-3 and axin2;\n• rtk/ras genes (5): erbb2, erbb3, nras, kras and braf;\n• tgf-β genes (5): tgfbr1, smad3, tgfbr2, smad4, acvr1b, acvr2a and smad2;\n169\n170 APPENDIX E. CRC STUDY - SUPPLEMENTARY MATERIALS\n• igf2/pi3k genes (5): igf2, irs2, pik3ca, pik3r1 and pten;\n• p53 genes (2): tp53 and atm.\nIn Chapter §6,rtk/ras and igf2/pi3k pathways are shortly denoted as ras and pi3k.\nE.1.2 Mutual exclusivity groups of alterations\nGroups of alterations showing a trend of mutual exclusivity were scanned with MUTEX and mutations and CNA hitting any of the 33 selected genes as input. MUTEX was run independently on MSS and MSI-HIGH groups (Supplementary Table §E.2, running times: approximately 6 and 3.5 hours, respectively, on a standard Desktop machine).\nWe selected only groups with score < 0.2, where the score is derived from p-values corrected for false discovery rate. 3 groups are found for MSI-HIGH tumors and 6 for MSS. For MSI-HIGH tumors, the three predicted groups consists of genes acvr1b, acvr2a, tp53 and erbb2, of genes braf, nras and tgfbr2, and of genes kras and braf.\nFurther groups of exclusive alterations were considered consistent with results reported in [137]. These include groups derived by consolidated knowledge of colorectal progression: the well-known Wnt alterations in apc/ctnnb1 [64], as well as RAS alterations in kras, nras and braf genes [194]. Similarly, we used also a group collected by scanning non-hypermutated tumors with the MEMO tool in [137] - this group includes pik3ca, pten, erbb2 and igf2 genes. These groups were restricted to account only for genes actually altered in a certain subtype, e.g., MSI-HIGH tumors lack ctnnb1 mutation, making the Wnt group irrelevant. Groups for MSS tumors are shown as Supplementary Figure §E.3."
    }, {
      "heading" : "E.1.3 CAPRI’s execution",
      "text" : "CAPRI was run, on each group of tumors, by selecting alterations from the pool of 33 pathway genes; every alteration on a gene x is included if any of these apply:\n• the alteration frequency of x - sum of mutation and CNA frequency - is greater than 5%;\n• x it is part of an exclusivity group.\nThe set of selected events for MSI-HIGH training tumors is shown in Chapter §6, the analogous for MSS tumors is shown in Figure §E.4.\nCAPRI was executed in its supervised mode by writing formula over groups and genes with multiple alterations associated, as explained in Chapter §6. For instance, for MSI-HIGH tumors with alterations in ras pathway we grouped hard exclusivity of nras mutations and deletions, with soft exclusivity of kras and braf mutations. Our aim was to account for a small subset of samples with concurrent kras and nras alterations (see Chapter §6). The list of all Boolean formulas written over groups is in Tables §E.3 and §E.4; this approach was adopted also when a gene harbors multiple\n170\nE.2. SINGLE-CELL SYNTHETIC DATA 171\nalterations in a subtype, e.g., erbb2 in MSS training samples which shows a trend of soft exclusivity between mutations and amplifications. We used both AIC and BIC scores to regularize inference after 100 non-parametric bootstrap iterations for estimation of the preliminary selective advantage relations. Statistical significance was determined in terms of p-values using the Mann-Whitney U test for CAPRI’s inequalities. In most cases these are orders of magnitude below significance threshold - exact values reported as Additional File CRC-pvalues.xslx. CAPRI’s models with such p-values and nonparametric bootstrap confidence are shown in Figures §E.5 and §E.6."
    }, {
      "heading" : "E.2 Single-cell synthetic data",
      "text" : "We sampled single-cell data from the clonal phylogeny tree (see Chapter §6). Such a tree reports the presence of the following clones in patient RMH004:\nclone signature\nc1 vhl frame-shift c2 vhl frame-shift, smarca4 SNV c3 vhl frame-shift, arid1a SNV c4 vhl, pten frame-shift c5 vhl, pten frame-shift, atm SNV c6 vhl, pten frame-shift, atm SNV, 6q deletion c7 vhl, pten frame-shift, atm SNV, msh6 stop codon c8 vhl, pten frame-shift, atm SNV, 2q amplification\nwhere when there are multiple undistinguishable alterations we report the first appearing in the plot, from left to right. Single-cell sampling consists in sampling any of these clones, where a clone-sampling probability is a function of the probability to sample each of its constituting alterations. The probabilities of sampling any of the considered alterations is given by the shown in Chapter §6, where marginal and conditional probabilities are estimated by the observed frequencies in the input data.\nSo, for instance, the probability of sampling clone c4 is\np(c4) =p(vhl:fs) sample the trunk event\n× p(pten:fs) and its downstream event, × ( 1− p(arid1a:SNV) )\ndo not sample private events of c2, × ( 1− p(smarca4:SNV) )\ndo not sample private events of c3, × ( 1− p(atm:SNV) )\ndo not sample private events of c5. (E.1)\nwhere ‘fs’ is a short-hand for frame-shift. In this way, we can easily transform a clone sample in a binary signature which can be inputed to CAPRESE’s reconstruction, being a 0/1 Bernoulli model. So, for instance a sample of clone c4 will be the binary vector:\n171\n172 APPENDIX E. CRC STUDY - SUPPLEMENTARY MATERIALS\nvhl smarca4 arid1a pten atm 6q msh6 2q\n1 0 0 1 0 0 0 0\nTo make the problem more realistic, we included a noise parameter which is the probability to assign a random value to any entry of such vector, regardless its actual value. This aims at mimicing the problem of missing coverage for variant calling or other artefacts such as error in measurements. In Chapter §6, we sampled datasets with number of cells n spanning from 5 to 200 (with discretization at different densities), with noise ranging from 0 to 20% (i.e., probability .2, discretization .05). For each of these settings we generate 100 independent datasets, and average the performance. In Figure §E.7, we show further results with n ≤ 1000 - to report method’s convergency for increased sample size. The performance of CAPRESE was measure as precision and recall, computed as standard, and Hamming distance, which provides a standard approach to measure similarity between the phylogeny tree reconstructed by running the algorithm on a single dataset, and the tree shown in Chapter §6."
    }, {
      "heading" : "E.3 Supplementary tables and figures",
      "text" : ""
    }, {
      "heading" : "Training dataset",
      "text" : "statistics alteration type\ncancer† n m |G| mutations amplifications deletions MSI-HIGH 27 16100 13798 11556 2888 1656 MSS 152 21317 16371 12417 6925 1975 † Samples were classified as MSI-HIGH/LOW and MSS by TCGA; see flag MSI status in clinical data available for the COADREAD project."
    }, {
      "heading" : "Validation dataset",
      "text" : "statistics alteration type\ncancer† n m |G| mutations amplifications deletions MSI-HIGH 36 16891 15779 15071 199 1621 MSS 202 24957 18158 14567 5846 4544 † Samples were classified as MSI-HIGH if they had more than 500 mutations, as MSS otherwise; see Figure §E.2.\nTable E.1: COADREAD Datasets. Data used in this study, derived from the TCGA COADREAD project [137].\n172\nE.3. SUPPLEMENTARY TABLES AND FIGURES 173\nMUTEX parameters\nParameter Value Description signalling-network - MUTEX network† max-group-size 5 maximum size of a result group first-level-random-iteration 10000 number of randomisation to estimate null distribution of member p-values in groups second-level-random-iteration 100 number of runs to estimate the null distribution of final scores fdr-cutoff - false-discovery-rate cutoff maximising the ex-\npected value of true positives - false positives is estimated from data\nsearch-on-signaling-network TRUE reduce the search space using the signalling network † Manually curated from Pathway Commons, SPIKE and SignaLink databases. Provided with the tool; available for download at https://code.google.com/p/mutex/.\nMUTEX groups with score < .2\nMSI-HIGH Groups score q-value\n1 kras, braf, 0.095 0.48 2 nras, braf, tgfbr1 0.1677 0.45 3 erbb2, tp53, acvr1b, acvr2a 0.1703 0.355\nMSS Groups score q-value\n1 tp53, atm, 0.051 0.34 2 arid1a, tp53 0.075 0.193 3 kras, nras, braf, 0.0864 0.1975 4 ctnnb1, apc, dkk2, 0.098 0.144 5 dkk1, tp53, atm, dkk2 0.1387 0.176 6 pik3ca, tp53, atm 0.164 0.207\nTable E.2: MUTEX: parameters and results. Top: Parameters used to run MUTEX on the training MSS/MSI-HIGH datasets with input CNA and somatic mutations in the pathway genes described in Text. Bottom: MUTEX identified 3 and 6 groups of alterations showing a trend of mutual exclusivity in these groups with score below the suggested cutoff of 0.2.\n173\n174 APPENDIX E. CRC STUDY - SUPPLEMENTARY MATERIALS\nC A\nP R\nI fo\nrm u\nla s\nin p\nu te\nd fo\nr te\nst in\ng †\nM S\nIH\nIG H\ntu m\no rs\nd e s c r i p t i o n\n1 ( N R A S : m ⊕\nN R A S : d ) ∨\nK R A S : m ∨\nB R A F : m\nR A F e x c l u s i v i t y\n2 P I K 3 C A : m ∨\nE R B B 2 : m ∨\nP T E N : m ∨\nI G F 2 : d\nM E M O g r o u p\n3 ( A C V R 1 B : m ⊕\nA C V R 1 B : d ) ∨\nA C V R 2 A : m ∨\nT P 5 3 : m ∨\nE R B B 2 : m\nM U T E X g r o u p\n4 ( N R A S : m ⊕\nN R A S : d ) ∨\nT G F B R 1 : m ∨\nB R A F : m\nM U T E X g r o u p\n5 K R A S : m ∨\nB R A F : m\nM U T E X g r o u p\n6 A C V R 1 B : m ⊕\nA C V R 1 B : a\nm u l t i p l e a l t e r a t i o n s\n7 N R A S : m ⊕\nN R A S : a\nm u l t i p l e a l t e r a t i o n s\n8 F B X W 7 : m ∨\nF B X W 7 : a\nm u l t i p l e a l t e r a t i o n s ‡\n† E v e n t s t y p e :\nm u t a t i o n ( m ) , d e l e t i o n ( d ) , a m p l i f i c a t i o n ( a ) .\nH a r d ( ⊕ ) a n d\ns o f t ( ∨ ) e x c l u s i v i t y . ‡ F o r m u l a n o t i n c l u d e d a s i t c r e a t e s a d u p l i c a t e d s i g n a t u r e i n t h e d a t a s e t .\nT a b\nle E\n.3 :\nC A\nP R\nI fo\nrm u\nla s\nfo r\nM S\nIH\nIG H\n. F\nor m\nu la\ns cr\nea te\nd fo\nr th\ne gr\no u\np s,\na n\nd in\np u te\nd to\nC A\nP R\nI fo r te st in g. T h es e ar e ei th er d er iv ed fr om ex cl u si v it y gr ou p s or fr om ge n es in vo lv ed in d iff er en t ty p es o f a lt er at io n s.\n174\nE.3. SUPPLEMENTARY TABLES AND FIGURES 175\nC A\nP R\nI fo\nrm u\nla s\nin p\nu te\nd fo\nr te\nst in\ng †\nM S\nS tu\nm o rs\nd e s c r i p t i o n\n1 ( A P C : m ⊕\nA P C : d ) ∨\nC T N N B 1 : m\nW N T e x c l u s i v i t y\n2 ( K R A S : m ∨\nK R A S : a ) ∨\n( N R A S : m ⊕\nN R A S : a ) ∨\n( B R A F : m ⊕\nB R A F : a )\nR A F e x c l a n d M E M O g r o u p\n3 P I K 3 C A : m ∨\n( E R B B 2 : m ∨\nE R B B 2 : a ) ∨\n( P T E N : m ⊕\nP T E N : d ) ∨\nI G F 2 : a\nM E M O g r o u p\n4 ( T P 5 3 : m ⊕\nT P 5 3 : d ) ∨\n( A T M : m ⊕\nA T M : d )\nM U T E X g r o u p\n5 ( T P 5 3 : m ⊕\nT P 5 3 : d ) ∨\nA R I D 1 A : m\nM U T E X g r o u p\n6 ( T P 5 3 : m ⊕\nT P 5 3 : d ) ∨\nA R I D 1 A : m\nM U T E X g r o u p\n7 ( A P C : m ⊕\nA P C : d ) ∨\nC T N N B 1 : m ∨\nD K K 2 : m\nM U T E X g r o u p\n8 ( T P 5 3 : m ⊕\nT P 5 3 : d ) ∨\n( A T M : m ⊕\nA T M : d ) ∨\nD K K 2 : m ∨\nD K K 1 : m\nM U T E X g r o u p\n9 ( T P 5 3 : m ⊕\nT P 5 3 : d ) ∨\n( A T M : m ⊕\nA T M : d ) ∨\nP I K 3 C A : m\nM U T E X g r o u p\n1 0\n( A P C : m ⊕\nA P C : d )\nm u l t i p l e a l t e r a t i o n s\n1 1\n( T P 5 3 : m ⊕\nT P 5 3 : d )\nm u l t i p l e a l t e r a t i o n s\n1 2\n( S M A D 4 : m ⊕\nS M A D 4 : d )\nm u l t i p l e a l t e r a t i o n s\n1 3\n( T C F 7 L 2 : m ⊕\nT C F 7 L 2 : d )\nm u l t i p l e a l t e r a t i o n s\n1 4\n( A T M : m ⊕\nA T M : d )\nm u l t i p l e a l t e r a t i o n s\n1 5\n( N R A S : m ⊕\nN R A S : d )\nm u l t i p l e a l t e r a t i o n s\n1 6\n( E R B B 2 : m ∨\nE R B B 2 : a )\nm u l t i p l e a l t e r a t i o n s\n1 7\n( P T E N : m ⊕\nP T E N : d )\nm u l t i p l e a l t e r a t i o n s\n1 8\n( S M A D 2 : m ⊕\nS M A D 2 : a )\nm u l t i p l e a l t e r a t i o n s\n1 9\n( D K K 4 : m ⊕\nD K K 4 : a )\nm u l t i p l e a l t e r a t i o n s\n2 0\n( S O X 9 : m ⊕\nS O X 9 : d )\nm u l t i p l e a l t e r a t i o n s\n2 1\n( B R A F : m ⊕\nB R A F : a )\nm u l t i p l e a l t e r a t i o n s\n† E v e n t s t y p e :\nm u t a t i o n ( m ) , d e l e t i o n ( d ) , a m p l i f i c a t i o n ( a ) .\nH a r d ( ⊕ ) a n d\ns o f t ( ∨ ) e x c l u s i v i t y . ‡ F o r m u l a n o t i n c l u d e d a s i t c r e a t e s a d u p l i c a t e d s i g n a t u r e i n t h e d a t a s e t .\nT a b\nle E\n.4 :\nC A\nP R\nI fo\nrm u\nla s\nfo r\nM S\nS .\nF or\nm u\nla s\ncr ea\nte d\nfo r\nth e\ngr ou\np s,\na n\nd in\np u\nte d\nto C\nA P\nR I\nfo r\nte st\nin g .\nT h\nes e\na re\nei th\ner d\ner iv\ned fr\no m\nex cl\nu si\nv it\ny gr\nou p\ns or\nfr om\nge n\nes in\nvo lv\ned in\nd iff\ner en\nt ty\np es\nof a lt\ner at\nio n\ns.\n175\n176 APPENDIX E. CRC STUDY - SUPPLEMENTARY MATERIALS\nFigure E.1: CRC pipeline processing MSI/MSS tumors. we process Microsatellite Stable and highly Instable tumors collected from the The Cancer Genome Atlas project “Human Colon and Rectal Cancer”. We implement a test/training study on selected somatic mutations and focal CNAs in 33 driver genes manually annotated to 5 pathways in the project. We scan groups of exclusive alterations with computational tools and from the project results, and we select which alterations we input to CAPRI. Then, inference is performed with various settings of regularization and confidence.\n176\nE.3. SUPPLEMENTARY TABLES AND FIGURES 177\n●●●●●●●● ●●●●●●●\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●\n●●●●●●●●●●●●● ●●●●●\n●● ● ●●●●\n●● ●●\n●● ●●\n●●● ●●\n●\n●\n●\n●\n0 50 100 150 200\n0 10\n00 20\n00 30\n00 40\n00 50\n00\nTumor sample\nM ut\nat io\nns (\nnu m\nbe r)\nValidation dataset − Mutations per sample\nMSI tumorsMSS tumors\n500 mutations (MSI / MSS cutoff)\nFigure E.2: MSI/MSS samples in the test dataset. Mutation rate per sample, r, in data collected for testing (which do not have clinical annotation from TCGA COADREAD project). To distinguish between tumors with MSS/MSI-HIGH status we set an empirical cutoff of 500 mutations to define two groups of 203 (r ≤ 500) and 36 (r > 500) samples respectively.\n177\n178 APPENDIX E. CRC STUDY - SUPPLEMENTARY MATERIALS\nExclusivity groups for MSS tumors\nevent\n59% TP53 7% ARID1A 1% TP53\nstage\n47% KRAS 9% NRAS 4% BRAF 1% NRAS 1% BRAF 1% KRAS\nstage\n78% APC 7% CTNNB1 1% DKK2 1% APC\nstage\n59% TP53 10% ATM 1% DKK2 1% DKK1 1% ATM 1% TP53\nstage\n59% TP53 15% PIK3CA 10% ATM 1% ATM 1% TP53\nstage\nstage Stage I Stage IIA Stage IIIA Stage IIIB Stage IIIC Stage IV NA\npathway\nnone\nDeletion\nAmplification\nMutation\nKnowledge-based priors: WNT exclusivity\n78%APC 7%CTNNB1 1%APC\nstage\n47%KRAS 9%NRAS 4%BRAF 1%NRAS 1%BRAF 1%KRAS\nstage Knowledge-based priors: RAF exclusivity\n15% PIK3CA 5% ERBB2 5% ERBB2 3% PTEN 3% PTEN 3% IGF2\nstage\nComputational priors: MEMO group\n59% TP53 10% ATM 1% ATM 1% TP53\nstage Computational priors: Mutex groups with score < 0.2\nRAS Wnt TGFb P53 PI3K\nFigure E.3: Groups of exclusive alterations for MSS tumors. Knowledge-based groups of exclusive alterations consist of: kras, nras and braf genes (raf pathway) and apc and ctnnb1 genes (wnt pathway). The MEMO[28] group identified in [137] in this cohort consists of genes pik3ca, erbb2, igf2 and pten. Finally, 6 groups are predicted by MUTEX [8] with score below .2, one of these is equivalent to the known exclusive alterations in raf pathway.\n178\nE.3. SUPPLEMENTARY TABLES AND FIGURES 179\nTCGA MSS colorectal tumors\nT C G A − A A − A 01K T C G A − A A − 3848 T C G A − A G − A 020 T C G A − A A − 3977 T C G A − A A − 3673 T C G A − A G − 3594 T C G A − A A − A 01Z T C G A − A A − 3681 T C G A − A A − 3558 T C G A − A G − 4005 T C G A − A G − A 02X T C G A − A G − 3896 T C G A − A G − 3726 T C G A − A G − A 032 T C G A − A F − 2691 T C G A − A G − A 01W T C G A − A A − 3979 T C G A − A G − 3902 T C G A − A G − 3605 T C G A − A A − 3975 T C G A − A A − 3556 T C G A − A G − 3999 T C G A − A A − 3561 T C G A − A A − 3814 T C G A − A A − 3851 T C G A − A A − A 01F T C G A − A G − 3581 T C G A − A G − 3602 T C G A − A G − 3611 T C G A − A G − 3909 T C G A − A G − A 00C T C G A − A G − A 014 T C G A − A G − 4015 T C G A − A G − A 011 T C G A − A G − 3898 T C G A − A 6− 3807 T C G A − A A − 3666 T C G A − A A − 3679 T C G A − A A − 3860 T C G A − A A − A 01T T C G A − A G − 3882 T C G A − A A − A 02F T C G A − A A − 3549 T C G A − A A − 3976 T C G A − A F − 3913 T C G A − A G − A 01L T C G A − A A − A 00L T C G A − A G − 3890 T C G A − A A − A 00D T C G A − A A − 3858 T C G A − A 6− 2674 T C G A − A G − 3893 T C G A − A A − 3519 T C G A − A A − 3542 T C G A − A A − 3552 T C G A − A A − 3562 T C G A − A A − 3685 T C G A − A A − 3693 T C G A − A A − 3856 T C G A − A A − A 00W T C G A − A A − A 02H T C G A − A A − A 02J T C G A − A F − 3400 T C G A − A G − 3587 T C G A − A G − 3593 T C G A − A G − 3598 T C G A − A G − 3609 T C G A − A G − 3612 T C G A − A G − A 01Y T C G A − A G − A 026 T C G A − AY − 4071 T C G A − A G − 3892 T C G A − A A − A 03F T C G A − A A − 3837 T C G A − A A − A 02O T C G A − A G − 3887 T C G A − A A − 3939 T C G A − A A − A 01I T C G A − A G − 3878 T C G A − A A − 3818 T C G A − A A − 3530 T C G A − A A − 3555 T C G A − A A − 3522 T C G A − A G − A 008 T C G A − A A − 3521 T C G A − A A − 3548 T C G A − A G − 3727 T C G A − A A − 3695 T C G A − A F − 2692 T C G A − A G − A 025 T C G A − A A − 3994 T C G A − A G − 3599 T C G A − A G − 3586 T C G A − A A − 3870 T C G A − A A − 3532 T C G A − A G − 3580 T C G A − A G − A 015 T C G A − A A − A 01V T C G A − A A − 3812 T C G A − A A − 3984 T C G A − A G − A 002 T C G A − A G − 3881 T C G A − A 6− 2677 T C G A − A A − 3527 T C G A − A G − A 02G T C G A − A G − 3584 T C G A − A A − 3678 T C G A − A G − 3894 T C G A − A A − A 017 T C G A − A A − A 00U T C G A − A A − 3684 T C G A − A A − 3989 T C G A − A A − 3872 T C G A − A G − 3883 T C G A − A A − 3544 T C G A − A A − 3952 T C G A − A A − 3538 T C G A − A A − 3846 T C G A − A A − A 00Z T C G A − A A − A 03J T C G A − A A − 3560 T C G A − A G − 3575 T C G A − A A − 3842 T C G A − A F − 2689 T C G A − A G − 4008 T C G A − A A − 3956 T C G A − A A − 3524 T C G A − A A − A 01D T C G A − A 6− 2678 T C G A − A G − A 016 T C G A − A A − 3869 T C G A − A A − 3955 T C G A − A A − A 00F T C G A − AY − 4070 T C G A − A G − 3578 T C G A − A 6− 2670 T C G A − A A − 3875 T C G A − A G − 3901 T C G A − A A − A 02Y T C G A − A G − A 00H T C G A − A A − 3696 T C G A − A A − 3986 T C G A − A A − A 00Q T C G A − A A − A 01X T C G A − A G − 3600 T C G A − A A − 3831 T C G A − A A − 3664 T C G A − A G − 3582 T C G A − A A − 3514 T C G A − A G − 3574 T C G A − A A − 3534 T C G A − A A − 3971\n78% APC 59% TP53 47% KRAS 15% PIK3CA 12% FBXW7 11% SMAD4 10% FAM123B 10% ATM 9% TCF7L2 9% NRAS 7% ARID1A 7% CTNNB1 5% SMAD2 5% SOX9 5% ERBB2 5% ERBB2 4% DKK4 4% BRAF 3% PTEN 3% PTEN 3% SMAD4 3% IGF2 2% DKK4 1% TCF7L2 1% DKK2 1% DKK1 1% APC 1% ATM 1% TP53 1% NRAS 1% BRAF 1% KRAS 1% SOX9 1% SMAD2\nhits stage\ng ro u p\nstage Stage I Stage IIA Stage IIIA Stage IIIB Stage IIIC Stage IV none hits 10\n2\ngroup Wnt P53 RAS PI3K TGFb\nnone\nDeletion\nAmplification\nMutation\n152 samples 34 events 21 genes 0 patterns\nFigure E.4: Selected data for MSS tumors. Colorectal tumors with Microsatellite Stable clinical status in the TCGA COADREAD project, restricted to 152 samples with both somatic mutations and CNA data available. 33 driver genes annotated to 5 pathways are selected as of the list published in [137] to automatically detect groups of mutually exclusive alterations. Events selected for reconstruction are those involving genes altered in at least 5% of the cases, or part of group of alterations showing an exclusivity trend (see Figure §E.3). This dataset is used to infer the set of selective advantage relations which constitute the MSS progression model presented in Chapter §6.\n179\n180 APPENDIX E. CRC STUDY - SUPPLEMENTARY MATERIALS\nTC G\nA M\nSS c\nol or\nec ta\nl t um\nor s\nB R A F 1% (1\n) D K K 4 4% (6\n)\nIG F2 3% (4\n)\nE R B B 2 5% (7 ) S O X 9 1% (1 )\nS M A D 2 1% (1 ) TP 53\n58 %\n(8 9)\nB R A F 4% (6\n)\nFB X W 7 12 %\n(1 9)\nA P C\n78 %\n(1 19\n)\nA R ID 1A 7% (1\n1)\nD K K 2 1% (2\n)\nS M A D 4 11 %\n(1 7)\nC TN N B 1 7% (1\n0) N R A S 8% (1\n3)\nK R A S\n47 %\n(7 1)\nFA M 12 3B 10 % (1 5)\nP IK 3C A 15 %\n(2 3)\nA TM\n10 %\n(1 5)\nE R B B 2 4% (7 )\nS M A D 2 5% (8 )\nS O X 9 5% (8\n) TC F7 L2 9% (1 4)\nP TE N 3% (5 ) D K K 4 2% (3 )\nA P C 1% (1\n)\nP TE N 3% (4 ) TC F7 L2 1% (2 )\nA TM 1% (1\n)\nTP 53 1% (1\n) S M A D 4 3% (4 )\n0. 64 0. 13 * < 0. 01\n0. 49 < 0. 01 < 0. 01\n0. 34 < 0. 01 < 0. 01\n0. 2 < 0. 01 < 0. 01\n0. 49 < 0. 01 < 0. 01\n0. 53 < 0. 01 < 0. 01\n0. 34 < 0. 01 < 0. 01\n0. 18 < 0. 01 < 0. 01\n0. 5 < 0. 01 < 0. 01\n0. 16 < 0. 01 < 0. 01\n0. 5 < 0. 01 < 0. 01\n0. 33 < 0. 01 < 0. 01\n0. 34 < 0. 01 < 0. 01\n0. 82 < 0. 01 < 0. 01\n0. 64 < 0. 01 < 0. 01\n0. 34 < 0. 01 < 0. 01\n0. 48 0. 46 * < 0. 01\n0. 34 < 0. 01 < 0. 01\n0. 81 < 0. 01 < 0. 01\n0. 02 < 0. 01 < 0. 01\n0. 17 < 0. 01 < 0. 01\n0. 36 < 0. 01 < 0. 01\n0. 32 < 0. 01 < 0. 01\n0. 01 < 0. 01 < 0. 01\n< 0. 01 < 0. 01 < 0. 01\nEv en\nts ty\npe\nD el et io n A m pl ifi ca tio n M ut at io n Pa tte rn s E xc lu si vi ty\n(s of t) E xc lu si vi ty (h ar d) Pa th w ay s W nt R A S P I3 K TG Fb P 53\nEd ge\nc on\nfid en ce N on P ar am et ric\nB oo\nts tra p Te m po ra l P rio rit y P ro ba bi lit y R ai si ng p < 0. 05 Sa m pl e si ze n = 15 2, m = 5 4 |G | = 2 1, |P | = 2 0 R eg ul ar iz at io n bi c ai c\nF ig\nu re\nE .5\n: M\nS S\ntr a in\nin g\ntu m\no rs\n: b\no o ts\ntr a p\nc o n\nfi d\ne n\nc e .\nP ro\ngr es\nsi on\nm o d\nel fo\nr M\nS S\ntu m\nor s\nw it\nh co\nn fi d\nen ce\nsh ow n as ed ge la b el s. T h e fi rs t la b el re p re se n ts th e re la ti on co n fi d en ce es ti m at ed w it h 10 0 n on -p ar am et ri c b o ot st ra p it er at io n s, th e se co n d an d th ir d ar e p -v a lu es fo r te m p or al p ri or it y an d p ro b ab il it y ra is in g. R ed p -v al u es ar e ab ov e th e m in im u m si gn ifi ca n e th re sh o ld of .0 0 5.\n180\nE.3. SUPPLEMENTARY TABLES AND FIGURES 181 TC G A M SI -H IG H c ol or ec ta l t um or s\nTP 53\n15 %\n(4 )\nA C V R 2A 18 % (5 )\nB R A F\n55 %\n(1 5)\nFB X W 7 51 %\n(1 4)\nLR P 5\n26 %\n(7 )\nA P C\n37 %\n(1 0)\nA R ID 1A 41 %\n(1 1)\nA X IN 2 18 %\n(5 )\nD K K 2\n18 %\n(5 )\nS M A D 4 15 % (4 )\nN R A S 4% (1\n) K R A S\n26 %\n(7 )\nA C V R 1B 22 % (6 )\nFA M 12 3B 30 % (8 )\nP IK 3C A 18 % (5 )\nTG FB R 2 7% (2 )\nA TM\n21 %\n(6 )\nE R B B 2 15 %\n(4 )\nS M A D 2 7% (2 )\nTG FB R 1 14 % (4 )\nE R B B 3 26 %\n(7 )\nTC F7 L2 15 % (4 )\nS M A D 3 8% (2 )\nD K K 4 7% (2\n) D K K 1 7% (2\n)\nIG F2 4% (1\n)\nA C V R 1B 4% (1 ) N R A S 8% (2 )\n0. 34 < 0. 01 < 0. 01\n0. 36 < 0. 01 < 0. 01\n0. 27 < 0. 01 < 0. 01\n0. 21 < 0. 01 < 0. 01\n0. 3 < 0. 01 < 0. 01\n0. 51 < 0. 01 < 0. 01\n0. 66 < 0. 01 < 0. 01\n0. 48 < 0. 01 < 0. 01\n0. 28 < 0. 01 < 0. 01\n0. 48 < 0. 01 < 0. 01\n0. 54 < 0. 01 < 0. 01\n0. 62 < 0. 01 < 0. 01\n0. 38 < 0. 01 < 0. 01\n0. 2 < 0. 01 < 0. 01\n0. 32 < 0. 01 < 0. 01\n0. 16 < 0. 01 < 0. 01\n0. 46 < 0. 01 < 0. 01\n0. 23 < 0. 01 < 0. 01\n0. 03 < 0. 01 < 0. 01\n0. 58 < 0. 01 < 0. 01\n0. 4 < 0. 01 < 0. 01\n0. 15 < 0. 01 < 0. 01\n0. 48 < 0. 01 < 0. 01\n0. 27 0. 36 * < 0. 01\n0. 48 < 0. 01 < 0. 01\n0. 17 < 0. 01 < 0. 01\n0. 26 < 0. 01 < 0. 01\nEv en\nts ty\npe\nD el et io n M ut at io n Pa tte rn s E xc lu si\nvi ty\n(s of t) E xc lu si vi ty (h ar d) Pa th w ay s W nt R A S P I3 K TG Fb P 53\nEd ge\nc on\nfid en ce N on P ar am et ric\nB oo\nts tra p Te m po ra l P rio rit y P ro ba bi lit y R ai si ng p < 0. 05 Sa m pl e si ze n = 27 , m = 3 5 |G | = 2 6, |P | = 7 R eg ul ar iz at io n bi c ai c\nF ig\nu re\nE .6\n: M\nS I\ntr a in\nin g\ntu m\no rs\n: b\no o ts\ntr a p\nc o n\nfi d\ne n\nc e .\nP ro\ngr es\nsi on\nm o d\nel fo\nr M\nS S\ntu m\no rs\nw it\nh co\nn fi\nd en\nce sh\now n\na s\ned g e\nla b\nel s.\nT h\ne fi\nrs t\nla b\nel re\np re\nse n ts\nth e\nre la\nti on\nco n\nfi d\nen ce\nes ti\nm at\ned w\nit h\n10 0\nn o n\n-p ar\na m\net ri\nc b\no o ts\ntr a p\nit er\nat io\nn s,\nth e\nse co\nn d\na n d\nth ir\nd ar\ne p\n-v a lu\nes fo\nr te\nm p\nor al\np ri\nor it\ny an\nd p\nro b\nab il\nit y\nra is\nin g.\nR ed\np -v\nal u\nes ar\ne a b\nov e\nth e\nm in\nim u\nm si\ngn ifi\nca n e th re sh o ld o f .0 0 5 .\n181\n182 APPENDIX E. CRC STUDY - SUPPLEMENTARY MATERIALS\nFigure E.7: Single-cell synthetic data: performance. Reconstruction performance with CAPRESE and n ≤ 1000 as sample size. Precision and recall are reported as well as Hamming Distance between the phylogeny tree which generated the data (see Chapter §6) and the inferred by the algorithm.\n182"
    }, {
      "heading" : "APPENDIX F",
      "text" : "THE CAUSAL STRUCTURE OF DISCRIMINATION\nDiscrimination discovery from data is an important task aiming at identifying patterns of illegal and unethical discriminatory activities against protected-by-law groups, e.g., ethnic minorities. While any legally-valid proof of discrimination requires evidence of causality, the state-of-the-art methods are essentially correlation-based, albeit, as it is well known, correlation does not imply causation.\nIn this Chapter we present how the framework proposed in this thesis can be also adopted to tackle the data mining problem of discrimination detection in databases. Following Suppes’ probabilistic causation theory, we define a method to extract, from a dataset of historical decision records, the causal structures existing among the attributes in the data. The result is a type of constrained Bayesian network, which we dub SuppesBayes Causal Network (SBCN). Next, we develop a toolkit of methods based on random walks on top of the SBCN, addressing different anti-discrimination legal concepts, such as direct and indirect discrimination, group and individual discrimination, genuine requirement, and favoritism. Finally experiments on real-world datasets confirm the inferential power of this approach in all these different tasks. As a reference for this work, see [17].\nF.1 Introduction\nThe importance of discrimination discovery. At the beginning of 2014, as an answer to the growing concerns about the role played by data mining algorithms in decision-making, USA President Obama called for a 90-day review of big data collecting and analysing practices. The resulting report1 concluded that “big data technologies can cause societal harms beyond damages to privacy”. In particular, it expressed concerns about the possibility that decisions informed by big data could have discriminatory\n1http://www.whitehouse.gov/sites/default/files/docs/big_data_privacy_report_may_1_\n2014.pdf\n183\n184 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\neffects, even in the absence of discriminatory intent, further imposing less favorable treatment to already disadvantaged groups.\nDiscrimination refers to an unjustified distinction of individuals based on their membership, or perceived membership, in a certain group or category. Human rights laws prohibit discrimination on several grounds, such as gender, age, marital status, sexual orientation, race, religion or belief, membership in a national minority, disability or illness. Anti-discrimination authorities (such as equality enforcement bodies, regulation boards, consumer advisory councils) monitor, provide advice, and report on discrimination compliances based on investigations and inquiries. A fundamental role in this context is played by discrimination discovery in databases, i.e., the data mining problem of unveiling discriminatory practices by analyzing a dataset of historical decision records.\nDiscrimination is causal. According to current legislation, discrimination occurs when a group is treated “less favorably” [107] than others, or when “a higher proportion of people not in the group is able to comply” with a qualifying criterion [109]. Although these definitions do not directly imply causation, as stated in [56] all discrimination claims require plaintiffs to demonstrate a causal connection between the challenged outcome and a protected status characteristic. In other words, in order to prove discrimination, authorities must answer the counterfactual question: what would have happened to a member of a specific group (e.g., nonwhite), if he or she had been part of another group (e.g., white)?\n“The Sneetches”, the popular satiric tale2 against discrimination published in 1961 by Dr. Seuss, describes a society of yellow creatures divided in two races: the ones with a green star on their bellies, and the ones without. The Star-Belly Sneetches have some privileges that are instead denied to Plain-Belly Sneetches. There are, however, Star-On and Star-Off machines that can make a Plain-Belly into a Star-Belly, and viceversa. Thanks to these machines, the causal relationship between race and privileges can be clearly measured, because stars can be placed on or removed from any belly, and multiple outcomes can be observed for an individual. Therefore, one could readily answer the counterfactual question, saying with certainty what would have happened to a PlainBelly Sneetch had he or she been a Star-Belly Sneetch.\nIn the real world however, proving discrimination episodes is much harder, as one cannot manipulate race, gender, or sexual orientation of an individual. This highlights the need to assess discrimination as a causal inference problem [32] from a database of past decisions, where causality can be inferred probabilistically. Unfortunately, the state of the art of data mining methods for discrimination discovery in databases does not properly address the causal question, as it is mainly based on correlation-based methods (surveyed in Section §F.2). Correlation is not causation. It is well known that correlation between two variables does not necessarily imply that one causes the other. Consider a unique cause X of two effects, Y and Z: if we do not take in account X, we might derive wrong conclusions because of the observable correlation between Y and Z. In this situation, X is said to\n2http://en.wikipedia.org/wiki/The_Sneetches_and_Other_Stories\n184\nF.1. INTRODUCTION 185\nact as a confounding factor for the relationship between Y and Z. Variants of the complex relation just discussed can arise even if, in the example, X is not the actual cause of either Y or Z, but it is only correlated to them, for instance, because of how the data were collected. Consider for instance a credit dataset where there exists high correlation between a variable representing low income and an other variable representing loan denial and let us assume that this is due to an actual legitimate causal relationship in the sense that, legitimately, a loan is denied if the applicant has low income. Let us now assume that high correlation between low income and being female is also observed, which, for instance, can be due to the fact that the women represented in the specific dataset in analysis, tend to be underpaid. Given these settings, in the data we would also observe high correlation between the variable gender being female and the variable representing loan denial, due to the fact that we do not account for the presence of the variable low income. Following common terminologies, we will say that such situations are due to spurious correlations.\nHowever, the picture is even more complicated: it could be the case, in fact, that being female is the actual cause of the low income and, hence, be the indirect cause of loan denial through low income. This would represent a causal relationship between the gender and the loan denial, that we would like to detect as discrimination.\nDisentangling these two different cases, i.e., female is only correlated to low income in a spurious way, or being female is the actual cause of low income, is at the same time important and challenging. This highlights the need for a principled causal approach to discrimination detection.\nAnother typical pitfall of correlation-based reasoning is expressed by what is known as Simpson’s paradox3 according to which, correlations observed in different groups might disappear when these heterogeneous groups are aggregated, leading to false positives cases of discrimination discovery. One of the most famous false-positive examples due to Simpson’s paradox occurred when in 1973 the University of California, Berkeley was sued for discrimination against women who had applied for admission to graduate schools. In fact, by looking at the admissions of 1973, it first appeared that men applying were significantly more likely to be admitted than women. But later, by examining the individual departments carefully, it was discovered that none of them was significantly discriminating against women. On the contrary, most departments had exercised a small bias in favor of women. The apparent discrimination was due to the fact that women tended to apply to departments with lower rates of admission, while men tended to apply to departments with higher rates [16]. Later in Section §F.5.5 we will use the dataset from this episode to highlight the differences between correlation-based and causationbased methods.\nSpurious correlations can also lead to false negatives (i.e., discrimination existing but not being detected) as is commonly seen in “reverse-discrimination”. The typical case is when authorities take affirmative actions, e.g., with compensatory quota systems, in order to protect a minority group from a potential discrimination. Such actions, while trying to erase the supposed discrimination (i.e., the spurious correlation), fail to address\n3http://en.wikipedia.org/wiki/Simpson’s_paradox\n185\n186 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\nthe real underlying causes for discrimination, potentially ending up denying individual members of a privileged group from access to their rightful shares of social goods. In the early 70’s, a case involving the University of California at Davis Medical School highlighted one such incident as the school’s admissions program reserved 16 of the 100 places in its entering class for “disadvantaged” applicants, thus unintentionally reducing the chances of admission for a qualified applicant.4\nThese are just few typical examples of the pitfalls of correlation-based reasoning in the discovery of discrimination. Later in Section §F.5.5 we show concrete examples from real-world datasets where correlation-based methods to discrimination discovery are not satisfactory.\nThe proposal and contributions. In this Chapter we take a principled causal approach to the data mining problem of discrimination detection in databases. Following Suppes’ probabilistic causation theory [80, 172] we define a method to extract, from a dataset of historical decision records, the causal structures existing among the attributes in the data.\nIn particular, we define the Suppes-Bayes Causal Network (SBCN), i.e., a directed acyclic graph (dag) where we have a node representing a Bernulli variable of the type 〈attribute = value〉 for each pair attribute-value present in the database. In this dag an arc (A,B) represents the existence of a causal relation between A and B (i.e., A causes B). Moreover, each arc is labeled with a score, representing the strength of the causal relation.\nThe SBCN is a constrained Bayesian network reconstructed by means of maximum likelihood estimation (MLE) from the given database, where we force the conditional probability distributions induced by the reconstructed graph to obey Suppes’ constraints: i.e., temporal priority and probability rising. Imposing Suppes’ temporal priority and probability raising we obtain what we call the prima facie causes graph [172], which might still contain spurious causes (false positives). In order to remove these spurious case we add a bias term to the likelihood score, favoring sparser causal networks: in practice we sparsify the prima facie causes graph by extracting a minimal set of edges which best explain the data. This regularization is done by means of the Bayesian Information Criterion (BIC) [167].\nThe obtained SBCN provides a clear summary, amenable to visualization, of the probabilistic causal structures found in the data. Such structures can be used to reason about different types of discrimination. In particular, we show how using several randomwalk-based methods, where the next step in the walk is chosen proportionally to the edge weights, we can address different anti-discrimination legal concepts. The experiments show that the measures of discrimination produced by the methods are very strong, almost binary, signals: the measures are very clearly separating the discrimination and the non-discrimination cases.\nTo the best of our knowledge this is the first proposal of discrimination detection in databases grounded in probabilistic causal theory.\n4http://en.wikipedia.org/wiki/Regents_of_the_University_of_California_v._Bakke\n186\nF.2. RELATED WORK 187\nRoadmap. The rest of the Chapter is organized as follows. Section §F.2 discusses the state of the art in discrimination detection in databases. In Section §F.3 we formally introduce the SBCN and we present the method for extracting such causal network from the input dataset. Once extracted the SBCN, in Section §F.4 we show how to exploit it for different concepts of discrimination detection, by means of random-walk methods. Finally Section §F.5 presents the experimental assessment and comparison with correlation-based methods on two real-world datasets."
    }, {
      "heading" : "F.2 Related work",
      "text" : "Discrimination analysis is a multi-disciplinary problem, involving sociological causes, legal reasoning, economic models, statistical techniques [31, 162]. Some authors [73, 94] study how to prevent data mining from becoming itself a source of discrimination. In this Chapter instead we focus on the data mining problem of detecting discrimination in a dataset of historical decision records, and in the rest of this section we present the most related literature.\nPedreschi et al. [153, 180, 165] propose a technique based on extracting classification rules (inductive part) and ranking the rules according to some legally grounded measures of discrimination (deductive part). The result is a (possibly large) set of classification rules, providing local and overlapping niches of possible discrimination. This model only deals with group discrimination.\nLuong et al. [120] exploit the idea of situation-testing [163] to detect individual discrimination. For each member of the protected group with a negative decision outcome, testers with similar characteristics (k-nearest neighbors) are considered. If there are significantly different decision outcomes between the testers of the protected group and the testers of the unprotected group, the negative decision can be ascribed to discrimination.\nZliobaite et al. [200] focus on the concept of genuine requirement to detect that part of discrimination which may be explained by other, legally grounded, attributes. In [41] Dwork et al. address the problem of fair classification that achieves both group fairness, i.e., the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a whole, and individual fairness, i.e., similar individuals should be treated similarly.\nThe above approaches assume that the dataset under analysis contains attributes that denote protected groups (i.e., direct discrimination). This may not be the case when such attributes are not available, or not even collectable at a micro-data level as in the case of the loan applicant’s race. In these cases we talk about indirect discrimination discovery. Ruggieri et al. [152] adopt a form of rule inference to cope with the indirect discovery of discrimination. The correlation information is called background knowledge, and is itself coded as an association rule.\nMancuhan and Clifton [123] propose Bayesian networks as a tool for discrimination discovery. Bayesian networks consider the dependence between all the attributes and use these dependencies in estimating the joint probability distribution without any strong assumption, since a Bayesian network graphically represents a factorization of the\n187\n188 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\njoint distribution in terms of conditional probabilities encoded in the edges. Although Bayesian networks are often used to represent causal relationships, this needs not be the case, in fact a directed edge from two nodes of the network does not imply any causal relation between them. As an example, let us observe that the two graphs A→ B → C and C → B → A impose exactly the same conditional independence requirements and, hence, any Bayesian network would not be able to disentangle the direction of any causal relationship among these events.\nThe work departs from this literature as (i) it is grounded in probabilistic causal theory instead of being based on correlation, (ii) it proposes a holistic approach able to deal with different types of discrimination in a single unifying framework, while the methods in the state of the art usually deal with one and only one specific type of discrimination. This is also the first work to adopt graph theory and social network analysis concepts, such as random-walk-based centrality measures and community detection, for discrimination detection. The proposed methods also have low computational cost compared to the existing approaches in the literature."
    }, {
      "heading" : "F.3 Suppes-Bayes Causal Network",
      "text" : "In order to study discrimination as a causal inference problem, we exploit the criteria defined in the theories of probabilistic causation [80]. In particular, we follow [172], where Suppes proposed the notion of prima facie causation that is at the core of probabilistic causation. Suppes’ definition is based on two pillars: (i) any cause must happen before its effect (temporal priority) and (ii) it must raise the probability of observing the effect (probability raising).\nIn the rest of this Section we introduce the method to construct, from a given relational table D, a type of causal Bayesian network constrained to satisfy the conditions dictated by Suppes’ theory, which we dub Suppes-Bayes Causal Network (SBCN).\nIn the literature many algorithms exist to carry out structural learning of general Bayesian networks and they usually fall into two families [101]. The first family, constraint based learning, explicitly tests for pairwise independence of variables conditioned on the power set of the rest of the variables in the network. These algorithms exploit structural conditions defined in various approaches to causality [80, 127, 192]. The second family, score based learning, constructs a network which maximizes the likelihood of the observed data with some regularization constraints to avoid overfitting. Several hybrid approaches have also been recently proposed [19].\nThe framework can be considered a hybrid approach exploiting constrained maximum likelihood estimation (MLE) as follows: (i) we first define all the possible causal relationship among the variables in D by considering only the oriented edges between events that are consistent with Suppes’ notion of probabilistic causation and, subsequently, (ii) we perform the reconstruction of the SBCN by a score-based approach (using BIC), which considers only the valid edges.\nThe idea of adopting Suppes’ theory to reconstruct the causal structure subsumed by a progression model is the main contribution of this thesis and, as already deeply\n188\nF.3. SUPPES-BAYES CAUSAL NETWORK 189\ndiscussed, it was introduced for the first time in [117, 158], albeit in a completely different context, i.e., modelling somatic evolution in cancer.\nWe next present in details the whole learning process."
    }, {
      "heading" : "F.3.1 Suppes’ constraints",
      "text" : "We start with an input relational table D defined over a set A of h categorical attributes and s samples. In case continuous numerical attributes exists in D, we assume they have been discretized to become categorical. From D, we derive D′, an m× s binary matrix representing m Bernoulli variables of the type 〈attribute = value〉, where an entry is 1 if we have an observation for the specific variable and 0 otherwise.\nTemporal priority. The first constraint, temporal priority, cannot be simply checked in the data as we have no timing information for the events. In particular, in this context the events for which we want to reason about temporal priority are the Bernoulli variables 〈attribute = value〉.\nThe idea here is that, e.g., income = low cannot be a cause of gender = female, because the time when the gender of an individual is determined is antecedent to that of when the income is determined. This intuition is implemented by simply letting the data analyst provide as input to the framework a partial temporal order r : A → N for the h attributes, which is then inherited from the m Bernoulli variables. Note that the learning technique requires the input order r to be correct and complete in order to guarantee its convergence. Nevertheless, if this is not the case, it is still capable of providing valuable insights about the underlying causal model, although with the possibility of false positive or false negative causal claims.\nBased on the input dataset D and the partial order r we produce the first graph G = (V,E) where we have a node for each of the Bernoulli variables, so |V | = m, and we have an arc (u, v) ∈ E whenever r(u) ≤ r(v). This way we will immediately rule out causal relations that do not satisfy the temporal priority constraint.\nProbability raising. Given the graph G = (V,E) built as described above the next step requires to prune the arcs which do not satisfy the second constraint, probability raising, thus building G′ = (V,E′), where E′ ⊆ E. In particular we remove from E each arc (u, v) such that P(v | u) ≤ P(v | ¬u). The graph G′ so obtained is called prima facie graph.\nAs previously proved in [117], we recall that the probability raising condition is equivalent to constraining for positive statistical dependence: in the prima facie graph we model all and only the positive correlated relations among the nodes already partially ordered by temporal priority, consistently with Suppes’ characterization of causality in terms of relevance."
    }, {
      "heading" : "F.3.2 Network simplification",
      "text" : "As proved in [158], Suppes’ conditions are necessary but not sufficient to evaluate causation: especially when the sample size is small, the model may have false positives (spurious causes), even after constraining for Suppes’ temporal priority and probability\n189\n190 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\nsex_Male\nworkclass_Self_emp_not_inc\n0.06\nmarital_status_Married_civ_spouse\n0.456\noccupation_Craft_repair\n0.157\noccupation_Protective_serv\n0.019\nrelationship_Husband\n0.604\nage_old\n0.11\n0.088\n0.88\neducation_Bachelors\n0.038\npositive_dec\n0.382\neducation_Assoc_voc\n0.006\neducation_Masters\n0.026\noccupation_Exec_managerial\n0.066\neducation_Prof_school\n0.019\nworkclass_Self_emp_inc\n0.043 education_Doctorate\n0.012\n0.069\n0.39\n0.371\nworkclass_Local_gov\n0.035\n0.085\n0.267\n0.018\n0.011 0.052\n0.015\nnative_country_Cuba\n0.002\n0.132\nworkclass_State_gov\n0.071\n0.028\noccupation_Prof_specialty\n0.151\n0.153\n0.208\n0.066 0.036\nworkclass_Never_worked\n0.001\n0.272 0.0760.159 0.6560.509 0.1220.059 0.141 0.06\n0.35\n0.178\n0.328\n0.172\n0.6110.492\nnative_country_India\n0.213\n0.129\nFigure F.1: One portion of the SBCN extracted from the Adult dataset. This subgraph corresponds to the C2 community reported later in Table §F.3 (Section §F.5) extracted by a community detection algorithm.\nraising criteria (which aim at removing false negatives). Consequently, although we expect all the statistically relevant causal relations to be modelled in G′, we also expect some spurious ones in it.\nIn this proposal, in place of other structural conditions used in various approaches to causality, (see e.g., [80, 127, 192]), we perform a network simplification (i.e., we sparsify the network by removing arcs) with a score based approach, specifically by relying on the Bayesian Information Criterion (BIC) as the regularized likelihood score [167].\nWe consider as inputs for this score the graph G′ and the dataset D′. Given these, we select the set of arcs E∗ ⊆ E′ that maximizes the score:\nscoreBIC(D′, G′) = LL(D′|G′)− log s 2 dim(G′).\nIn the equation, G′ denotes the graph, D′ denotes the data, s denotes the number of samples, and dim(G′) denotes the number of parameters in G′. Thus, the regularization term −dim(G′) favors graphs with fewer arcs. The coefficient log s/2 weighs the regularization term, such that the higher the weight, the more sparsity will be favored over “explaining” the data through maximum likelihood. Note that the likelihood is implicitly weighted by the number of data points, since each point contributes to the score.\nAssume that there is one true (but unknown) probability distribution that generates the observed data, which is, eventually, uniformly randomly corrupted by false positives and negatives rates (in [0, 1)). Let us call correct model, the statistical model which best approximate this distribution. The use of BIC on G′ results in removing the false\n190\nF.3. SUPPES-BAYES CAUSAL NETWORK 191\npositives and, asymptotically (as the sample size increases), converges to the correct model. In particular, BIC is attempting to select the candidate model corresponding to the highest Bayesian Posterior probability, which can be proved to be equivalent to the presented score and its log(s) penalization factor.\nWe denote with G∗ = (V,E∗) the graph that we obtain after this step. We note that, as for general Bayesian network, G∗ is a dag by construction."
    }, {
      "heading" : "F.3.3 Confidence score",
      "text" : "Using the reconstructed SBCN, we can represent the probabilistic relationships between any set of events (nodes). As an example, suppose to consider the nodes representing respectively income = low and gender = female being the only two direct causes (i.e., with arcs toward) of loan = denial. Given SBCN, we can estimate the conditional probabilities for each node in the graph, i.e., probability of loan = denial given income = low AND gender = female in the example, by computing the conditional probability of only the pair of nodes directly connected by an arc. For an overview of state-ofthe-art methods for doing this, see [101]. However, we expect to be mostly dealing with full data, i.e., for every directly connected node in the SBCN, we expect to have several observations of any possible combination attribute = value. For this reason, we can simply estimate the node probabilities by counting the observations in the data. Moreover, we will exploit such conditional probabilities to define the confidence score of each arc in terms of their causal relationship.\nIn particular, for each arc (v, u) ∈ E∗ involving the causal relationship between two nodes u, v ∈ V , we define a confidence score W (v, u) = P(u | v) − P(u | ¬v), which, intuitively, aims at estimating the observations where the cause v is followed by its effect u, that is P(u | v), and the ones where this is not observed, i.e., P(u | ¬v), because of imperfect causal regularities. We also note that, by the constraints discussed above, we require P(u | v) P(u | ¬v) and, for this reason, each weight is positive and no larger than 1, i.e., W : E∗ → (0, 1].\nCombining all of the concepts discussed above, we conclude with the following definition. Definition 9 (Suppes-Bayes Causal Network). Given an input dataset D′ of m Bernoulli variables and s samples, and given a partial order r of the variables, the Suppes-Bayes Causal Network SBCN = (V,E∗,W ) subsumed by D′ is a weighted dag such that the following requirements hold:\n• [Suppes’ constraints] for each arc (v, u) ∈ E∗ involving the causal relationship between nodes u, v ∈ V , under the mild assumptions that 0 < P(u),P(v) < 1:\nr(v) ≤ r(u) and P(u | v) > P(u | ¬v) .\n• [Simplification] let E′ be the set of arcs satisfying the Suppes’ constraints as before; among all the subsets of E′, the set of arcs E∗ is the one whose corresponding graph maximizes BIC:\nE∗ = arg max E⊆E′,G=(V,E) (LL(D′, |G)− log s 2 dim(G)) .\n191\n192 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\n• [Score] W (v, u) = P(u | v)− P(u | ¬v), ∀(v, u) ∈ E∗\nAn example of a portion of a SBCN extracted from a real-world dataset is reported in Figure §F.1. Algorithm §7 summarized the learning approach adopted for the inference of the SBCN .\nAlgorithm 7: Learning the Suppes-Bayes Causal Network\n1: Inputs: D′ an input dataset of m Bernoulli variables and s samples, and r a partial order of the variables 2: Output: SBCN(V,E∗,W ) as in Definition 2 3: [Suppes’ constraints] 4: for all the arcs (v, u) between each pair of the m Bernoulli variables do 5: if r(v) ≤ r(u) and P(u | v) > P(u | ¬v) then 6: Set to the arc (v, u) its weight, being W (v, u) = P(u | v)− P(u | ¬v). 7: Add the arc (v, u) to SBCN . 8: end if 9: end for\n10: [Simplification] 11: Consider G(V,E∗,W )fit = ∅. 12: while !StoppingCriterion() do 13: Let G(V,E∗,W )neighbors be the neighbor solutions of G(V,E\n∗,W )fit. 14: Remove from G(V,E∗,W )neighbors any solution whose arcs are not included in SBCN . 15: Consider a random solution Gcurrent in G(V,E\n∗,W )neighbors. 16: if scoreBIC(D′, Gcurrent) > scoreBIC(D′, Gfit) then 17: Gfit = Gcurrent. 18: Assign to the arcs of Gfit the related weights of SBCN . 19: end if 20: end while 21: SBCN = Gfit. 22: return SBCN .\nGiven D′ an input dataset over m Bernoulli variables and s samples, and r a partial order of the variables, Suppes’ constraints are verified (Lines 4-9) to construct a dag as described in Section §F.3.1. The likelihood fit is performed by hill climbing (Lines 12-21), an iterative optimization technique that starts with an arbitrary solution to a problem (in this case an empty graph) and then attempts to find a better solution by incrementally visiting the neighbourhood of the current one. If the new candidate solution is better than the previous one it is considered in place of it. The procedure is repeated until the stopping criterion is matched. The !StoppingCriterion occurs (Line 12) in two situations: (i) the procedure stops when we have performed a large enough number of iterations or, (ii) it stops when none of the solutions in Gneighbors is better than the current Gfit. Note that Gneighbors denotes all the solutions that are derivable from Gfit by removing or adding at most one edge.\n192\nF.3. SUPPES-BAYES CAUSAL NETWORK 193\nTime and space complexity. The computation of the valid dag according to Suppes’ constraints (Lines 4-10) requires a pairwise calculation of the probabilistic scores leading to a polynomial cost. After that, the likelihood fit by hill climbing (Lines 11-21) is performed5. Hence, let m denotes the number of the Bernoulli variables and s the number of records in D′, and l the maximum number of iterations required for the hill climbing, the total computational complexity of Algorithm §7 is O(s ·m) in time and m2 in space."
    }, {
      "heading" : "F.3.4 Expressivity of a SBCN",
      "text" : "We conclude this Section with a discussion on the causal relations that we model by a SBCN .\nLet us assume that there is one true (but unknown) probability distribution that generates the observed data whose structure can be modelled by a dag. Furthermore, let us consider the causal structure of such a dag and let us also assume each node with more then one cause to have conjunctive parents: any observation of the child node is preceded by the occurrence of all its parents. As before we call correct model, the statistical model which best approximate the distribution. On these settings, we can prove the following theorem.\nTheorem 1. Let the sample size s → ∞, the provided partial temporal order r be correct and complete and the data be uniformly randomly corrupted by false positives and negatives rates (in [0, 1)), then the SBCN inferred from the data is the correct model.\nProof. [Sketch] Let us first consider the case where the observed data have no noise. On such an input, we observe that the prima facie graph has no false negatives: in fact ∀[c→ e] modelling a genuine causal relation, P(e ∧ c) = P(e), thus the probability raising constraint is satisfied, so it is the temporal priority given that we assumed r to be correct and complete.\nFurthermore, it is know that the likelihood fit performed by BIC converges to a class of structures equivalent in terms of likelihood among which there is the correct model: all these topologies are the same unless the directionality of some edges. But, being the prima facie graph already ordered by temporal priority, we can conclude that in this case the SBCN coincides with the correct model.\nTo extend the proof to the case of data uniformly randomly corrupted by false positives and negatives rates (in [0, 1)), we note that the marginal and joint probabilities change monotonically as a consequence of the assumption that the noise is uniform. Thus, all inequalities used in the preceding proof still hold, which concludes the proof.\n5Note that being an heuristic, the computational cost of hill climbing depends on the sopping criterion. However, constraining by Suppes’ criteria tends to regularize the problem leading on average to a quick convergence to a good solution.\n193\n194 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\nIn the more general case of causal topologies where any cause of a common effect is independent from any other cause (i.e., we relax the assumption of conjunctive parents), the SBCN is not guaranteed to converge to the correct model but it coincides with a subset of it modeling all the edges representing statistically relevant causal relations (i.e., where the probability raising condition is verified).\nF.4 Discrimination discovery by random walks\nIn this Section we propose several random-walk-based methods over the reconstructed SBCN, to deal with different discrimination-detection tasks.\nF.4.1 Group discrimination and favoritism\nThe basic problem in the analysis of direct discrimination is precisely to quantify the degree of discrimination suffered by a given protected group (e.g., an ethnic group) with respect to a decision (e.g., loan denial). In contrast to discrimination, favoritism refers to the case of an individual treated better than others for reasons not related to individual merit or business necessity: for instance, favoritism in the workplace might result in a person being promoted faster than others unfairly. In the following we denote favoritism as positive discrimination in contrast with negative discrimination.\nGiven an SBCN we define a measure of group discrimination (either negative or positive) for each node v ∈ V . Recall that each node represents a pair 〈attribute = value〉, so it is essentially what we refer to as a group, e.g., 〈gender = female〉. The task is to assign a score of discrimination ds− : V → [0, 1] to each node, so that the closer ds−(v) is to 1 the more discriminated is the group represented by v.\nWe compute this score by means of a number n of random walks that start from v and reaches either the node representing the positive decision or the one representing the negative decision. In these random walks the next step is chosen proportionally to the weights of the out-going arcs. Suppose a random walk has reached a node u, and let degout(u) denote the set of outgoing arcs from u. Then the arc (u, z) is chosen with probability\np(u, z) = W (u, z)∑\ne∈degout(u)W (e) .\nWhen a random walk ends in a node with no outgoing arc before reaching either the negative or the positive decision, it is restarted from the source node v.\nDefinition 10 (Group discrimination score). Given a SBCN = (V,E∗,W ), let δ− ∈ V and δ+ ∈ V denote the nodes indicating the negative and positive decision, respectively. Given a node v ∈ V , and a number n ∈ N of random walks to be performed, we denote as rwv→δ− the number of random walks started at node v that reach δ\n− earlier than δ+. The discrimination score for the group corresponding to node v is then defined as\nds−(v) = rwv→δ−\nn .\n194\nF.4. DISCRIMINATION DISCOVERY BY RANDOM WALKS 195\nThis implicitly also defines a score of positive discrimination (or favoritism): ds+(v) = 1− ds−(v).\nTaking advantage of the SBCN we also propose two additional measures capturing how far a node representing a group is from the positive and negative decision respectively. This is done by computing the average number of steps that the random walks take to reach the two decisions: we denote these scores as as−(v) and as+(v).\nF.4.2 Indirect discrimination\nThe European Union Legislation [109] provides a broad definition of indirect discrimination as occurring “where an apparently neutral provision, criterion or practice would put persons of a racial or ethnic origin at a particular disadvantage compared with other persons”. In other words, the actual result of the apparently neutral provision is the same as an explicitly discriminatory one. A typical legal case study of indirect discrimination is concerned with redlining : e.g., denying a loan because of ZIP code, which in some areas is an attribute highly correlated to race. Therefore, even if the attribute race cannot be required at loan-application time (thus would not be present in the data), still race discrimination is perpetrated. Indirect discrimination discovery refers to the data mining task of discovering the attributes values that can act as a proxy to the protected groups and lead to discriminatory decisions indirectly [153, 180, 73].\nIn the considered setting, indirect discrimination can be detected by applying the same method described in Section §F.4.1."
    }, {
      "heading" : "F.4.3 Genuine requirement",
      "text" : "The legal concept of genuine requirement refers to detecting that part of the discrimination which may be explained by other, legally-grounded, attributes; e.g., denying credit to women may be explainable by the fact that most of them have low salary or delay in returning previous credits. A typical example in the literature is the one of the “genuine occupational requirement”, also called “business necessity” in [108, 49]. In the state of the art of data mining methods for discrimination discovery, it is also known as explainable discrimination [74] and conditional discrimination [200].\nThe task here is to evaluate to which extent the discrimination apparent for a group is “explainable” on a legal ground. Let v ∈ V be the node representing the group which is suspected of being discriminated, and ul ∈ V be a node whose causal relation with a negative or positive decision is legally grounded. As before, δ− and δ+ denote the negative and positive decision, respectively. Following the same random-walk process described in Section §F.4.1, we define the fraction of explainable discrimination for the group v:\nfed−(v) = rwv→ul→δ−\nrwv→δ− ,\ni.e., the fraction of random walks passing trough ul among the ones started in v and reaching δ− earlier than δ+. Similarly we define fed+(v), i.e., the fraction of explainable positive discrimination.\n195\n196 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\nF.4.4 Individual and subgroup discrimination\nIndividual discrimination requires to measure the amount of discrimination for a specific individual, i.e., an entire record in the database. Similarly, subgroup discrimination refers to discrimination against a subgroup described by a combination of multiple protected and non-protected attributes: personal data, demographics, social, economic and cultural indicators, etc. For example, consider the case of gender discrimination in credit approval: although an analyst may observe that no discrimination occurs in general, it may turn out that older women obtain car loans only rarely.\nBoth problems can be handled by generalizing the technique introduced in Section §F.4.1 to deal with a set of starting nodes, instead of only one. Given an SBCN = (V,E∗,W ) let v1, . . . , vn be the nodes of interest. In order to define a discrimination score for v1, . . . , vn, we perform a personalized PageRank [90] computation with respect to v1, . . . , vn. In personalized PageRank, the probability of jumping to a node when abandoning the random walk is not uniform, but it is given by a vector of probabilities for each node. In this case the vector will have the value 1n for each of the nodes v1, ..., vn ∈ V and zero for all the others. The output of personalized PageRank is a score ppr(u|v1, ..., vn) of proximity/relevance to {v1, ..., vn} for each other node u in the network. In particular, we are interested in the score of the nodes representing the negative and positive decision: i.e., ppr(δ−|v1, ..., vn) and ppr(δ+|v1, ..., vn) respectively.\nDefinition 11 (Generalized discrimination score). Given an SBCN = (V,E∗,W ), let δ− ∈ V and δ+ ∈ V denote the nodes indicating the negative and positive decision, respectively. Given a set of nodes v1, ..., vn ∈ V , we define the generalized (negative) discrimination score for the subgroup or the individual represented by {v1, ..., vn} as\ngds−(v1, ..., vn) = ppr(δ−|v1, ..., vn)\nppr(δ−|v1, ..., vn) + ppr(δ+|v1, ..., vn) .\nThis implicitly also defines a generalized score of positive discrimination: gds+(v1, ..., vn) = 1− gds−(v1, ..., vn)."
    }, {
      "heading" : "F.5 Experimental Evaluation",
      "text" : "This section reports the experimental evaluation of this approach on four datasets, Adult, German credit and census-income from the UCI Repository of Machine Learning Databases6, and Berkeley Admissions Data from [57]. These are well-known real-life datasets typically used in discrimination-detection literature.\nAdult: consists of 48,842 tuples and 10 attributes, where each tuple correspond to an individual and it is described by personal attributes such as age, race, sex, relationship, education, employment, etc. Following the literature, in order to define the decision attribute we use the income levels, ≤50K (negative decision) or >50K (positive decision). We use four levels in the partial order for temporal priority: age, race, sex, and native\n6http://archive.ics.uci.edu/ml\n196\nF.5. EXPERIMENTAL EVALUATION 197\ncountry are defined in the first level; education, marital status, and relationship are defined in the second level; occupation and work class are defined in the third class, and the decision attribute (derived from income) is the last level.\nGerman credit: consists of 1000 tuples with 21 attributes on bank account holders applying for credit. The decision attribute is based on repayment history, i.e., whether the customer is labeled with good or bad credit risk. Also for this dataset the partial order for temporal priority has four orders. Personal attributes such as gender, age, foreign worker are defined in the first level. Personal attributes such as employment status and job status are defined in the second level. Personal properties such as savings status and credit history are defined in the third level, and finally the decision attribute is the last level.\nCensus-income: consists of 299,285 tuples and 40 attributes, where each tuple correspond to an individual and it is described by demographic and employment attributes such as age, sex, relationship, education, employment, ext. Similar to Adult dataset, the decision attribute is the income levels and we define four levels in the partial order for temporal priority.\nThe main characteristics of the extracted SBCN are reported in Table §F.1, while the distribution of the edges scores W (e) is plotted in Figure §F.2.\nDataset |V | |A| avgDeg maxInDeg maxOutDeg Adult 92 230 2.5 7 19\nGerman credit 73 102 1.39 3 7 Census-income 386 1426 3.69 8 54\nTable F.1: SBCN main characteristics.\nAs discussed in the Introduction we also use the dataset from the famous 1973 episode at University of California at Berkeley, in order to highlight the differences between correlation-based and causation-based methods.\nBerkeley Admissions Data: consists of 4,486 tuples and three attributes, where each tuple correspond to an individual and it is described by the gender of applicants and the department that they apply for it. For this dataset the partial order for temporal priority has three orders. Gender is defined in the first level, department in the second level, and finally the decision attribute is the last level. Table §F.2 is a three-way table that presents admissions data at the University of California, Berkeley in 1973 according to the variables department (A, B, C, D, E), gender (male, female), and outcome (admitted, denied). The table is adapted from data in the text by Freedman, et al. [57].\nF.5.1 Community detection on the SBCN\nGiven that the SBCN is a directed graph with edge weight, as a first characterization we try to partition it using a random-walks-based community detection algorithm, called\n197\n198 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\n0.0 0.2 0.4 0.6 0.8 1.0\n0. 0\n0. 4\n0. 8\nedge scores\ncd f\nAdult German census−income\nFigure F.2: Distribution of the edge scores.\nMale Female Admitted Denied Admitted Denied Department 512 313 89 19 A 313 207 17 8 B 120 205 202 391 C 138 279 131 244 D 53 138 94 299 E 22 351 24 317 F\nTable F.2: Berkeley Admission Data\nWalktrap and proposed in [155], whose unique parameter is the maximum number of steps in a random walk (we set it to 8), and which automatically identifies the right number of communities. The idea is that short random walks tend to stay in the same community (densely connected area of the graph). Using this algorithm over the reconstructed SBCN from Adult dataset, we obtain 5 communities: two larger ones and three smaller ones (reported in Table §F.3). Interestingly, the two larger communities seem built around the negative (C1) and the positive (C2) decisions.\nFigure §F.1 in Section §F.3 shows the subgraph of the SBCN corresponding to C2 (that we can call, the favoritism cluster): we note that such cluster also contains nodes such as sex Male, age old, relationship Husband. The other large community C1, can be considered the discrimination cluster: beside the negative decision it contains other nodes representing disadvantaged groups such as sex Female, age young, race Black, marital status Never married. This good separability of the SBCN in the two main clusters of discrimination and favoritism, highlights the goodness of the causal structure captured\n198\nF.5. EXPERIMENTAL EVALUATION 199\nby the SBCN.\nC1 negative dec, wc:Private, ed:Some college, ed:Assoc acdm,\nms:Never married, ms:Divorced, ms:Widowed, ms:Married AF spouse, oc:Sales, oc:Other service,\noc:Priv house serv, re:Own child, re:Not in family, re:Wife, re:Unmarried, re:Other relative, ra:Black, oc:Armed Forces, oc:Handlers cleaners, oc:Tech support, oc:Transport moving,\ned:7th 8th, ed:10th, ed:12th, ms:Separated, ed:HS grad,ed:11th, nc:Outlying US Guam USVI etc,\nnc:Haiti, ag:young, sx:Female, ra:Amer Indian Eskimo, nc:Trinadad Tobago, nc:Jamaica, oc:Machine op inspct,\nms:Married spouse absent, oc:Adm clerical, C2\npositive dec, oc:Prof specialty, wc:Self emp not inc, ms:Married civ spouse, oc:Craft repair,oc:Protective serv,\nre:Husband, ed:Prof school, wc:Self emp inc, ag:old , wc:Local gov, oc:Exec managerial,\ned:Bachelors, ed:Assoc voc, ed:Masters, wc:Never worked, wc:State gov, ed:Doctorate, sx:Male, nc:India, nc:Cuba\nC3 oc:Farming fishing, wc:Without pay, nc:Mexico, nc:Canada,\nnc:Italy, nc:Guatemala, nc:El Salvador, ra:White, nc:Poland, ed:1st 4th, ed:9th,ed:Preschool, ed:5th 6th\nC4 nc:Iran, nc:Puerto Rico, nc:Dominican Republic,\nnc:Columbia, nc:Peru, nc:Nicaragua, ra:Other C5\nnc:Philippines, nc:Cambodia, nc:China, nc:South, nc:Japan, nc:Taiwan, nc:Hong, nc:Laos, nc:Thailand,\nnc:Vietnam, ra:Asian Pac Islander\nTable F.3: Communities found in the SBCN extracted from the Adult dataset by Walktrap[155]. In the table the attributes are shortened as in parenthesis: age (ag), education (ed), marital status (ms), native country (nc), occupation (oc), race(ra), relationship (re), sex (sx), workclass (wc).\nF.5.2 Group discrimination and favoritism\nWe next focus on assessing the discrimination score ds− we defined in Section §F.4.1, as well as the average number of steps that the random walks take to reach the negative and positive decisions, denote as−(v) and as+(v) respectively.\nTables §F.4, §F.5 and §F.6 report the top-5 and bottom-5 nodes w.r.t. the discrimination score ds−, for datasets Adult, German and Census-income, respectively. The first and most important observation is that this discrimination score provides a very clear signal, with some disadvantaged groups having very high discrimination score (equal to 1 or very close), and similarly clear signals of favoritism, with groups having ds−(v) = 0, or equivalently ds+(v) = 1. This is more clear in the Adult dataset, where the positive and negative decisions are artificially derived from the income attribute. In the German credit dataset, which is more realistic as the decision attribute is truly about credit, both discrimination and favoritism are less palpable. This is also due to the fact that\n199\n200 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\nGerman credit contains less proper causal relations, as reflected in the higher sparsity of the SBCN. A consequence of this sparsity is also that the random walks generally need more steps to reach one of the two decisions. In Census-income dataset, we observe favoritism with respect to married and asian pacific individuals."
    }, {
      "heading" : "F.5.3 Genuine requirement",
      "text" : "We next focus on genuine requirement (or explainable discrimination). Table §F.7 reports some examples of fraction of explainable discrimination (both positive and negative) on the Adult dataset. We can see how some fractions of discrimination against protected groups, can be “explained” by intermediate nodes such as having a low education profile, or a simple job. In the case these intermediate nodes are considered legally grounded, then one cannot easily file a discrimination claim.\nSimilarly, one can observe that the favoritism towards groups such as married man, is not just simple favoritism but it is explainable, to a large extent, by higher education and good working position, such as managerial or executive roles."
    }, {
      "heading" : "F.5.4 Subgroup and Individual Discrimination",
      "text" : "We next turn the attention to subgroup and individual discrimination discovery. Here the problem is to assign a score of discrimination not to a single node (a group), but to multiple nodes (representing the attributes of an individual or a subgroup of citizens). In Section §F.4.4 we have introduced based on the PageRank of the positive and negative decision, ppr(δ+) and ppr(δ−) respectively, personalized on the nodes of interest. Figure §F.3 presents a scatter plot of ppr(δ+) versus ppr(δ−) for each individual in the German credit dataset. One can observe the perfect separation between individuals corresponding to a high personalized PageRank with respect to the positive decision, and those associated with a high personalized PageRank relative to the negative decision.\nSuch good separation is also reflected in the generalized discrimination score (Definition 4) that we obtain by combining ppr(δ+) versus ppr(δ−).\nIn Figure §F.4 we report the distribution of the generalized discrimination score gds− for the population of the German credit dataset: one can make a note of the clear separation between the two subgroups of the population.\nIn the Adult dataset (Figure §F.5) we do not observe the same neat separation in two subgroups as in the German credit dataset, also due to the much larger number of points. Nevertheless, as expected, ppr(δ+) and ppr(δ−) still exhibit anticorrelation. In Figure §F.5 we also use colors to show two different groups: red dots are for age Young and blue dots are for age Old individuals. As expected we can see that the red dots are distributed more in the area of higher ppr(δ−).\nThe plots in Figure §F.6 have a threshold t ∈ [0, 1] on the X-axis, and the fraction of tuples having gds−() ≥ t on the Y-axis, and they show this for different subgroups. The first plot, from the Adult dataset, shows the group female, young, and young female. As we can see the individuals that are both young and female have a higher generalized discrimination score. Similarly, the second plot shows the groups old, single male, and\n200\nF.5. EXPERIMENTAL EVALUATION 201\n0.010 0.015 0.020 0.025\n0. 01\n0 0.\n01 4\n0. 01\n8 0.\n02 2\nppr(+)\npp r(\n− )\nFigure F.3: Scatter plot of ppr(δ+) versus ppr(δ−) for each individual in the German credit dataset.\nold single male from the German credit dataset. Here one can observe much lower rates of discrimination with only 1/5 of the corresponding populations having gds−() ≥ 0.5, while in the previous plot it was more than 85%.\nF.5.5 Comparison with prior art\nWe next discuss examples in which the causation-based method draws different conclusions from the correlation-based methods presented in [153, 180, 165] using the same datasets and the same protected groups.7\nThe first example involves the foreign worker group from German Credit dataset, whose contingency table is reported in Figure §F.8. Following the approachs of [153, 180, 165] the foreign worker group results strongly discriminated. In fact Figure §F.8 shows an RD value (risk difference) of 0.244 which is considered a strong signal: in fact RD > 0 is already considered discrimination [165].\nHowever, we can observe that the foreign worker group is per se not very significant, as it contains 963 tuples out of 1000 total. In fact the causal approach does not detect any discrimination with respect to foreign worker which appears as a disconnected node in the SBCN.\nThe second example is in the opposite direction. Consider the race black group from Adult dataset whose contingency table is shown in Figure §F.9. The causalitybased approach detects a very strong signal of discrimination (ds−() = 0.994), while the\n7We could not compare with [123] due to repeatability issues: we contacted the authors asking their help, but we didn’t get a reply.\n201\n202 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\ngds−\nF re\nqu en\ncy\n0.3 0.4 0.5 0.6 0.7\n0 20\n60 10\n0\nFigure F.4: Individual discrimination: histogram representing the distribution of the values of the generalized discrimination score gds− for the population of the German credit dataset.\napproachs of [153, 180, 165] fail to discover discrimination against black minority when the value of minimum support threshold used for extracting classification rules is more than 10%. On the other hand, when such minimum support threshold is kept lower, the number of extracted rules might be overwhelming. Moreover, the value of RD is not very strong, while in this method the discrimination reported is strong, regardless of the small size of the black population contained in the dataset.\nFigure §F.7 presents the SBCN extracted by this approach from Berkeley Admission Data. Interestingly, we observe that there is no direct edge between node sex Female and Admission No. And sex Female is connected to node Admission No through nodes of Dep C, Dep D, Dep E, and Dep F, which are exactly the departments that have lower admission rate. By running the random walk-based methods over SBCN we obtain the value of 1 for the score of explainable discrimination confirming that apparent discrimination in this dataset is due the fact that women tended to apply to departments with lower rates of admission.\nSimilarly, we observe that is no direct edge between node sex Male and Admission Yes. And sex Male is connected to node Admission Yes through nodes of Dep A, and Dep B, which are exactly the departments that have higher admission rate. By running the random walk-based methods over SBCN we obtain the value of 1 for the score of explainable discrimination confirming that apparent favoritism towards men is due to the fact that men tended to apply to departments with higher rates of admission.\n202\nF.6. CONCLUSIONS 203\n0.02 0.03 0.04 0.05 0.06 0.07 0.08\n0. 04\n0. 08\nppr(+)\npp r(\n− )\nFigure F.5: Individual discrimination: scatter plot of ppr(δ+) versus ppr(δ−) for each individual in the Adult dataset. Red dots are for age Young and blue dots are for age Old.\nHowever, following the approaches of [153, 180, 165], the contingency table shown in Figure §F.10 can be extracted from Berkeley Admission Data. As shown in Figure §F.10, the value of RD suggests a very strong signal of discrimination versus women. This highlights once more the pitfalls of correlation-based approaches to discrimination detection and the need for a principled causal approach."
    }, {
      "heading" : "F.6 Conclusions",
      "text" : "Discrimination discovery from databases is a fundamental task in understanding past and current trends of discrimination, in judicial dispute resolution in legal trials, in the validation of micro-data before they are publicly released. While discrimination is a causal phenomenon, and any discrimination claim requires to prove a causal relationship, the bulk of the literature on data mining methods for discrimination detection is based on correlation reasoning.\nIn this Chapter we propose the first discrimination detection method grounded in probabilistic causal theory. We first define a method to extract a graph representing the causal structures found in the database, and then we propose several random-walkbased methods over the causal structures, addressing a range of different discrimination problems.\nThe experimental assessment confirmed the great flexibility of the proposal in tackling different aspects of the discrimination detection task, and doing so with very clean signals, clearly separating discrimination cases.\n203\n204 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\n0.4 0.5 0.6 0.7 0.8\n0. 0\n0. 4\n0. 8\nt\nfemale young young female\n0.3 0.4 0.5 0.6 0.7\n0. 0\n0. 4\n0. 8\nt\nold single male old single male\nFigure F.6: Subgroup discrimination: plots reporting a threshold t ∈ [0, 1] on the X-axis, and the fraction of tuples having gds−() ≥ t on the Y-axis. The top plot is from Adult, while the bottom is from German credit.\ndecision - +\nforeign worker=yes 298 667 968 foreign worker=no 2 30 32\n300 700 1000\np1 = 298/968 = 0.307 p2 = 2/32 = 0.0625 RD = p1 − p2 = 0.244\nTable F.8: Contingency table for foreign worker in the German credit dataset.\ndecision - +\nrace=black 4119 566 4685 race 6=black 33036 11121 44157\n37155 11687 48842\np1 = 4119/4685 = 0.879 p2 = 33036/44157 = 0.748\nRD = p1 − p2 = 0.13\nTable F.9: Contingency table for race black in the Adult dataset.\n204\nF.6. CONCLUSIONS 205\nds−(v) as−(v) as+(v) relationship Unmarried 1 1.164 -\nmarital status Never married 0.996 1.21 2.14 age Young 0.995 2.407 3.857 race Black 0.994 2.46 4.4 sex Female 0.98 2.60 3.76\nds−(v) as−(v) as+(v) relationship Husband 0 - 2\nmarital status Married civ spouse 0 - 2.06 sex Male 0 - 3.002\nnative country India 0.002 4.0 3.25 age Old 0.018 2.062 2.14\nTable F.4: Top-5 and bottom-5 groups by discrimination score ds−(v) in Adult dataset.\nds−(v) as−(v) as+(v) residence since le 1d6 1 6.0 - residence since gt 2d8 1 2.23 -\nresidence since from 1d6 le 2d2 1 6.0 - age gt 52d6 0.86 3.68 4.0\npersonal status male single 0.791 5.15 5.0\nds−(v) as−(v) as+(v) job unskilled resident 0 - 2.39\npersonal status male mar or wid 0.12 8.0 4.4 age le 30d2 0.186 7.0 3.34\npersonal status female 0.294 6.48 4.4 div or sep or mar\nTable F.5: Top-5 and bottom-4 groups by discrimination score ds−(v) in German credit. We report only the bottom-4, because there are only 4 nodes in which ds+(v) > ds−(v).\nds−(v) as−(v) as+(v) MIGSAME Not in universe under 1 year old 0.71 4.09 8.82\nWKSWORK 94 5 inf 0.625 3.0 6.76 AWKSTAT Not in labor force 0.59 2.0 6.16\nVETYN 0 5 20 5 0.58 1.01 5.17 MARSUPWT 3188 455 4277 98 0.55 5.0 9.25\nds−(v) as−(v) as+(v) AHGA Doctorate degreePhD EdD 0 - 3.07\nAMARITL Married A F spouse present 0 - 4.49 AMJOCC Sales 0 - 2.0\nARACE Asian or Pacific Islander 0 - 6.47 VETYN 20 5 32 5 0 - 5.89\nTable F.6: Top-5 and bottom-5 groups by discrimination score ds−(v) in Census-income dataset.\n205\n206 APPENDIX F. THE CAUSAL STRUCTURE OF DISCRIMINATION\nSource node Intermediate fed−(v) race Amer Indian Eskimo education HS grad 0.481\nsex Female occupation Other service 0.310 age Young occupation Other service 0.193\nrelationship Unmarried education HS grad 0.107 race Black education 11th 0.083\nSource node Intermediate fed+(v) relationship Husband occupation Exec managerial 0.806 sex Male occupation Exec managerial 0.587 native country Iran education Bachelors 0.480 native country India education Prof school 0.415\nage Old occupation Exec managerial 0.39\nTable F.7: Fraction of explainable discrimination for some exemplar pair of nodes in the Adult dataset.\nMale\nDep_A\n0.252\nDep_B\n0.183\nAdmission_Yes\n0.33 0.254\nFemale\nDep_C\n0.201\nDep_D\n0.047\nDep_E\n0.142\nDep_F\n0.045\nAdmission_No\n0.039 0.052 0.15 0.378\nFigure F.7: The SBCN constructed from Berkeley Admission Data dataset.\ndecision - +\ngender=female 1278 557 1835 gender=male 1493 1158 2651\n2771 1715 4486\np1 = 1278/1835 = 0.696 p2 = 1493/2651 = 0.563 RD = p1 − p2 = 0.133\nTable F.10: Contingency table for female in the Berkeley Admission Data dataset.\n206"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Recently, there has been a resurgence of interest in rigorous and scalable algorithms for<lb>efficient inference of cancer progression using genomic patient data. The motivations<lb>are manifold: (i) rapidly growing NGS and single cell data from cancer patients, (ii)<lb>long-felt need for novel Data Science and Machine Learning algorithms well-suited for<lb>inferring models of cancer progression, and finally, (iii) a desire to understand the tem-<lb>poral and heterogeneous structure of tumor so as to tame its natural progression through<lb>most efficacious therapeutic intervention. This thesis presents a multi-disciplinary effort<lb>to algorithmically and efficiently model tumor progression involving successive accumu-<lb>lation of genetic alterations, each resulting populations manifesting themselves with a<lb>novel cancer phenotype.<lb>The framework presented in this work along with efficient algorithms derived from it,<lb>represents a novel and versatile approach for inferring cancer progression, whose accu-<lb>racy and convergence rates surpass other existing techniques. The approach derives its<lb>power from many insights from, and contributes to, several fields including algorithms<lb>in machine learning, theory of causality, and cancer biology. Furthermore, a versatile<lb>and modular pipeline to extract ensemble-level progression models from cross-sectional<lb>sequenced cancer genomes is also proposed. The pipeline combines state-of-the-art tech-<lb>niques for sample stratification, driver selection, identification of fitness-equivalent ex-<lb>clusive alterations and progression model inference.<lb>Furthermore, the results are rigorously validated using synthetic data created with<lb>realistic generative models, and empirically interpreted in the context of real cancer<lb>datasets; in the later case, biologically significant conclusions revealed by the recon-<lb>structed progressions are also highlighted. Specifically, it demonstrates also the pipeline’s<lb>ability to reproduce much of the current knowledge on colorectal cancer progression, as<lb>well as to suggest novel experimentally verifiable hypotheses. Lastly, it also proves that<lb>the proposed framework can be applied, mutatis mutandis, in reconstructing the evo-<lb>lutionary history of cancer clones in single patients, as illustrated by an example with<lb>multiple biopsy data from clear cell renal carcinomas.",
    "creator" : "LaTeX with hyperref package"
  }
}