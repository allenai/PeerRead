{
  "name" : "1503.02417.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Structured Prediction of Sequences and Trees using Infinite Contexts",
    "authors" : [ "Ehsan Shareghi", "Gholamreza Haffari", "Trevor Cohn", "Ann Nicholson" ],
    "emails" : [ "1ehsan.shareghi@monash.edu", "gholamreza.haffari@monash.edu", "ann.nicholson@monash.edu", "2t.cohn@unimelb.edu.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Markov models are widespread popular techniques for modelling the underlying structure of natural language, e.g., as sequences and trees. However local Markov assumptions often fail to capture phenomena outside the local Markov context, i.e., when the data generation process exhibits long range dependencies. A prime example is language modelling where only short range dependencies are captured by finite-order (i.e. n-gram) Markov models. However, it has been shown that going beyond finite order in a Markov model improves language modelling because natural language embodies a large\narray of long range depepndencies (Wood et al., 2009a). While infinite order Markov models have been extensively explored for language modelling (Gasthaus and Teh, 2010; Wood et al., 2011), this has not yet been done for structure prediction.\nIn this paper, we propose an infinite-order Markov model for predicting latent structures, namely tag sequences and trees. We show that this expressive model can be applied to various structure prediction tasks in NLP, such as syntactic parsing and part-ofspeech tagging. We propose effective algorithms to tackle significant learning and inference challenges posed by the infinite Markov model.\nMore specifically, we propose an unboundeddepth, hierarchical, Bayesian non-parametric model for the generation of linguistic utterances and their corresponding structure (e.g., the sequence of POS tags or syntax trees). Our model conditions each decision in a tree generating process on an unbounded context consisting of the vertical chain of their ancestors, in the same way that infinite sequence models (e.g.,∞-gram language models) condition on an unbounded window of linear context (Mochihashi and Sumita, 2007; Wood et al., 2009b).\nLearning in this model is particularly challenging due to the large space of contexts and corresponding data sparsity. For this reason predictive distributions associated with contexts are smoothed using distribtions for successively smaller contexts via a hierarchical Pitman-Yor process, organised as a trie. The infinite context makes it impossible to directly apply dynamic programing for structure prediction. We present two inference algorithms based on A* and Markov Chain Monte Carlo (MCMC) for predicting the best structure for a given input utterance.\nThe experiments show that our generative model\nar X\niv :1\n50 3.\n02 41\n7v 1\n[ cs\n.L G\n] 9\nM ar\n2 01\nobtains similar performance to the state-of-theart Stanford part-of-speech-tagger (Toutanova and Manning, 2000) for English and Swedish. For Danish, our model outperforms the Stanford tagger, which is impressive given the Stanford parser uses many more complex features and a discriminative training objective. Our experiments on parsing show that our unbounded-context tree model adapts itself to the data to effectively capture sufficient context to outperform both a PCFG baseline as well as Markov models with finite ancestor conditioning."
    }, {
      "heading" : "2 Background and related work",
      "text" : "The syntactic parse tree of an utterance can be generated by combining a set of rules from a grammar, such as a context free grammar (CFG). A CFG is a 4- tuple G = (T ,N , S,R), where T is a set of terminal symbols,N is a set of non-terminal symbols, S ∈ N is the distinguished root non-terminal and R is a set of productions (a.k.a., rewriting rules). A PCFG assigns a probability to each rule in the grammar, where ∑ B,C P (A → B C|A) = 1. The grammar rules are often in Chomsky Normal Form, taking either the form A → B C or A → a where A,B,C are syntactic cagegories (nonterminals), and a is a word (terminal).\nTag sequences can also be represented as a tree structure, without loss of generality, in which rules take the form A → B a or A → a where A,B are POS tags, and a is a word. Hence tagging models can be represented by restricted (P)CFGs. This unifies view to syntactic parsing and POS tagging will allow us to apply our model and inference algorithms to these problems with only minor refinements (see Figure 1).\nIn PCFG, a tree is generated by starting with the root symbol and rewriting (substituting) it with a grammar rule, then continuing to rewrite frontier non-terminals with grammar rules until there are no remaining frontier non-terminals. When making the decision about the next rule to expand a frontier non-terminal, the only conditioning context used from the partially generated tree is the frontier non-terminal itself, i.e., the rewrite rule is assumed independent from the remainder of the tree given the frontier non-terminal. Our model relaxes this strong independence assumptions by consider-\ning unbounded vertical history when making the next inference decision. This takes into account a wider context when making the next parsing decision.\nPerhaps the most relevant work is on unbounded history language models (Mochihashi and Sumita, 2007; Wood et al., 2009a). A prime work is Sequence Memoizer (Wood et al., 2011) which conditions the generation of the next word on an unbounded history of previously generated words. We build on these techniques to develop rich infinitecontext models for structured prediction, leading to additional complexity and challenges.\nFor syntactic parsing, several infinite extensions of probabilistic context free grammars (PCFGs) have been proposed (Liang et al., 2007; Finkel et al., 2007). These approaches achieve infinite grammars by allowing an unbounded set of non-terminals (hence grammar rules), but still make use of a bounded history when expanding each non-terminal. An alternative method allows for infinite grammars by considering segmentation of trees into arbitrarily large tree fragments, although only a limited history is used to conjoin fragments (Cohn et al., 2010; Johnson et al., 2006). Our work achieves infinite grammars by growing the vertical history needed to make the next parsing decision, as opposed to growing the number of rules, non-terminals or states horizontally, as done in prior work.\nEarlier work in syntactic parsing has also looked into growing both the history vertically and the rules horizontally, in a bounded setting. (Johnson, 1998) has increased the history for the parsing task by parent-annotation, i.e., annotating each non-terminal in the training parse trees by its parent, and then reading off the grammar rules from the resulting trees. (Klein and Manning, 2003) have considered vertical and horizontal markovization while using the head words’ part-of-speech tag, and showed that increasing the size of the vertical contexts consistently improves the parsing performance. (Petrov et al., 2006), (Petrov and Klein, 2007) and (Matsuzaki et al., 2005) have treated non-terminal annotations as latent variables and estimated them from the data. Likewise, finite-state hidden Markov models (HMMs) have been extended horizontally to have countably infinite number of states (Beal et al.,\n2001). Previous works on applying Markov models to part-of-speech tagging either considered finiteorder Markov models (Brants, 2000), or finite-order HMM (Thede and Harper, 1999). We differ from these works by conditioning both the emissions and transitions on their full contexts."
    }, {
      "heading" : "3 The Model",
      "text" : "Our model relaxes strong local Markov assumptions in PCFG to enable capturing phenomena outside of the local Markov context. The model conditions the generation of a rule in a tree on its unbounded vertical history, i.e., its ancestors on the path towards the root of the tree (see Figure 1). Thus the probability of a tree T is\nP (T ) = ∏\n(u,r)∈T\nG[u](r)\nwhere r denotes the rule and u its history, and G[u](.) is the probability of the next inference decision (i.e., grammar rule) conditioned on the context u. In other words, a tree T can be represented as a sequence of context-rule events {(u, r) ∈ T}.\nWhen learning such a model from data, a vector of predictive probabilities for the next rule G[u](.) given each possible vertical context u ∈ U must be learned, where depending on the problem U can denote the set of chains of non-terminals N ∗ or\nchains of rules R∗. As the context size increases, the number of events observed for such long contexts in the training data drastically decreases which makes parameter estimation challenging, particularly when generalising to unseen contexts. Assuming our unbounded-depth model, we need suitable smoothing techniques to estimate conditional rule probabilities for large (and possibly infinite depth) contexts. We achieve smoothing by placing a hierarchical Bayesian prior over the set of probability distributions {G[u]}u∈U . We smooth G[u] with a distribution conditioned on a shorter contextG[π(u)], where π(u) is the suffix of u containing all but the earliest event. This ties parameters of longer histories to their shorter suffixes in a hierarchical manner, and leads to sharing statistical strengths to overcome sparsity issues. Figure 1 shows our infiniteorder Markov model and the smoothing mechanism described here.\nMore specifically, we assume that a distribution with the full history G[u] is related to a distribution with the most recent history G[π(u)] through the Pitman-Yor process PY P (Wood et al., 2011):\nG[ε] | d[ε], c[ε], H ∼ PY P (d0, c0, H) G[u] | d|u|, c|u|, G[π(u)] ∼ PY P (d|u|, c|u|, G[π(u)])\nwhere H denotes the base (e.g. uniform) distribution, and ε denotes the empty context. The PitmanYor process PY P (d, c,H) is a distribution over dis-\ntributions, where d is the discount parameter, c is the concentration parameter, and H is the base distribution. Note that G[u] depends on G[π(u)] which itself depends on G[π(π(u))], etc. This leads to a hierarchical Pitman-Yor process prior where contextdependent distributions are hidden. The formulation of the hierarchical PYP over different length contexts is illustrated in Figure 2.\nFigure 3 demonstrates the property of PYP and how its behaviour depends on discount d, and concentration c parameters. Note that the PYP allows a good fit to data distribution compared to the Dirichlet Process (d = 0; as used in prior work) which cannot adequately represent the long tail of events."
    }, {
      "heading" : "4 Learning",
      "text" : "Given a training tree-bank, i.e., a collection of utterances and their trees, we are interested in the posterior distribution over {G[u]}u∈U . We make use of the approach developed in Wood et al. (2011) for learning such suffix-based graphical models when\nlearning infinite-depth language models. It makes use of Chinese Restaurant Process (CRP) representation of the Pitman-Yor process in order to marginalize out distributions G[u] (Teh, 2006) and learn the predictive probabilities P (r|u).\nUnder the CRP representation each context corresponds to a restaurant. As a new (u, r) is observed in the training data, a customer is entered to the restaurant, i.e., the trie node corresponding to u. Whenever a customer enters a restaurant, it should be decided whether to seat him on an existing table serving the dish r, or to seat him on a new table and sending a proxy customer to the parent node in the trie to order r (i.e., based on (π(u), r)). Fixing a seating arrangement S and PYP parameters θ for all restaurants (i.e., the collection of concentration and discount parameters), the predictive probability of a rule based on our infinite-context rule model is:\nP (r| ,S, θ) = H(r)\nP (r|u,S, θ) = nur. − d|u|tur n|u|.. + c|u|\n+ c|u| + d|u|t\nu .\nnu.. + c|u| P (r|π(u),S, θ)\nwhere d|u| and c|u| are the discount and concentration parameters, nurk is the number of customers at table k served the dish r in the restaurant u (accordingly nur. is the number of customers served the dish r and nu.. is the number of customers), and t u r is the number of tables serving dish r in the restaurant u (accordingly tu. is the number of tables).\nThe seating arrangements (the state of all restaurants including their tables and customers sitting on each table) are hidden, so they need to be marginalized out:\nP (r|u,D) = ∫ P (r|u,S, θ)P (S, θ|D)d(S, θ)\nwhere D is the training tree-bank. We approximate this integral by the so called “minimal assumption seating arrangement” and the MAP parameter setting θ which maximizes the corresponding data posterior. Based on the minimal assumption, a new table is created only when there is no table serving the desired dish in a restaurant u. That is, a proxy customer is created and sent to the parent node in\nthe trie π(u) for each unique dish type (sequence of events).\nThis approximation has been shown to recover interpolated Kneser-Ney smoothing, when applied to hierarchical Pitman-Yor process language model (Teh, 2006).\nThe parameter θ is learned by maximising the posterior, given the seating arrangement corresponding to the minimal assumption. We put the following prior distributions over the parameters: dm ∼ Beta(am, bm) and cm ∼ Gamma(αm, βm). The posterior is the prior multiplied by the following likelihood term:\n∏ r H(r)n 0 r. ∏ u [c|u|] tu. d|u| [c|u|] nu.. 1 ∏ r tu.∏ k=1 [1− d|u|] (nurk−1) 1\nwhere [a]cb denotes the generalised factorial function.1 We maximize the posterior with the constraints cm ≥ 0 and dm ∈ [0, 1) using the L-BFGSB optimisation method (Zhu et al., 1997), which results in the optimised discount and concentration values for each context size."
    }, {
      "heading" : "5 Prediction",
      "text" : "In this section, we propose algorithms for the challenging problem of predicting the highest scoring tree. The key ideas are to compactly represent the space of all possible trees for a given utterance, and then search for the best tree in this space in a topdown manner. By traversing the hyper-graph topdown, the search algorithms have access to the full history of grammar rules.\nIn the test time, we need to predict the tree structure of a given utterance w by maximizing the tree score:\nargmax T P (T |D,w) = argmax T ∏ (u,r)∈T P (r|u,D)\nThe unbounded context allowed by our model makes it infeasible to apply dynamic programming, e.g. CYK (Cocke and Schwartz, 1970), for finding the highest scoring tree. CYK is a bottom-up algorithm which requires storing in a dynamic programming table the score of each utterance’s sub-span conditioned on all possible contexts. Even truncating the\n1[a]0b = [a] −1 b = 1 and [a] b c = ∏c−1 i=0 (a+ ib).\ncontext size to bound this term may be insufficient to allow CYK for prediction, due to the unreasonable computational complexity.\nThe space of all possible trees for a given utterance can be compactly represented as a hyper-graph (Klein and Manning, 2001). Each hyper-graph node is labelled with a non-terminal and a sub-span of the utterance. There exists a hyper-edge from the nodes B[i, j] and C[j + 1, k] to the node A[i, k] if the rule A → B C belongs to the grammar (Figure 4). Starting from the top node S[0, N ], our prediction algorithms search for the highest scoring tree sub-graph that covers all of the utterance terminals in the hyper-graph. Our top-down prediction algorithms have access to the full history needed by our model when deciding about the next hyper-edge to be added to the partial tree."
    }, {
      "heading" : "5.1 A* Search",
      "text" : "This algorithm incrementally expands frontier nodes of the best partial tree until a complete tree is constructed. In the expansion step, all possible rules for expanding all frontier non-terminals are considered and the resulting partial trees are inserted into a priority queue (see Figure 4), sorted based on the following score:\nScore(T+) = logP (T ) + logGu(A→ B C) + h(T+, A→ B C, i, k, j|G′)\nwhere T+ is a partial tree after expanding a frontier non-terminal, P (T ) is the probability of the current\npartial tree, Gu(A→ B C) is the probability of expanding a non-terminal via a rule A → B C in the full context u, and h is the heuristic function (i.e., the estimate of the score for the best tree completing T+). We use various heuristic functions when expanding a node A[i, j] in the hypergraph via a hyperedge with tails B[i, k] and C[k + 1, j]:\n• Full Frontier: which estimates the completion cost by\nh(T+, A→ B C, i, k, j|G′) =∑ (A′,i′,j′)∈Fr(T+) logP (A′, i′, j′|G′)\nwhere Fr(T+) is the set of frontier nodes of the partial tree, and G′ is a simplified grammar admitting dynamic programming. Here we choose the PCFG used the base measure H in the root of the PYP hierarchy. Accordingly the logP terms can be computed cheaply using the PCFG inside probabilities.\n• Local Frontier: which only takes into account the completion of the following frontier nodes:\nh(T+, A→ B C, i, k, j|G′) = logP (B, i, k|G′) + logP (C, k + 1, j|G′)\nThis heuristic focuses on the completion cost of the sub-span using the selected rule.\nThe above heuristics functions are not admissible, hence the A* algorithm is not guaranteed to find the optimal tree. However the PCFG provides reasonable estimates of the completion costs, and accordingly with a sufficiently wide beam, search error is likely to be low."
    }, {
      "heading" : "5.2 MCMC Sampling",
      "text" : "We make use of Metropolis-Hastings (MH) algorithm, which is a Markov chain Monte Carlo (MCMC) method, for obtaining a sequence of random trees. We then combine these trees to construct the predicted tree.\nIn the MH algorithm, we use a PCFG as our proposal distributionQ and draw samples from it. Each sampled tree is then accepted/rejected using the following acceptance rate:\nα(T, T ′) = min { 1, P (T ′)Q(T )\nP (T )Q(T ′)\n}\nwhere T ′ is the sampled tree, T is the current tree, P (T ′) is the probability of the proposed tree under our model, and Q(T ′) is its probability under the proposal PCFG. Under some conditions, i.e., detailed balance and ergodicity, it is guarantheed that the stationary distribution of the underlying Markov chain (defined by the MH sampling) is the distribution that our model induces over the space of trees P . For each utterence, we sample a fresh tree for the whole utterance from a PCFG using the approach of (Johnson et al., 2007), which works by first computing the inside lattice under the proposal model (which can be computed once and reused), followed by top-down sampling to recover a tree. Finally the proposed tree is scored using the MH test, according to which the tree is randomly accepted as the next sample or else rejected in which case the previous sample is retained.\nOnce the sampling is finished, we need to choose a tree based on statistics of the sampled collection of trees. One approach is to select the most frequently sampled tree, however this does not work effectively in such large search spaces because of high sampling variance. Note that local Gibbs samplers might be able to address this problem, at least partly, through resampling subtrees instead of full tree sampling (as done here). Local changes would allow for more rapid mixing from trees with some high and low scoring subtrees to trees with uniformly high scoring sub-structures. We leave local sampling for future work, noting that the obvious local operation of resampling complete sub-trees or local tree fragments would compromise detailed balance, and thus not constitute a valid MCMC sampler (Levenberg et al., 2012).\nTo address this problem, we use a Minimum Bayes Risk (MBR) decoding method to predict the best tree (Goodman, 1996) as follows: For each pair of a nonterminal-span, we record the count in the collection of sampled trees. Then using the Viterbi algorithm, we select the tree from the hypergraph for which the sum of the induced pairs of nonterminalspan is maximized. Roughly speaking, this allows to make local corrections that result in higher accuracy compared to the best sampled trees."
    }, {
      "heading" : "6 Experiments",
      "text" : "In order to evaluate the proposed model and prediction algorithms, we performed two sets of experiments on tasks with different structural complexity. The statistics of the tasks and datasets are provided in Table 1."
    }, {
      "heading" : "6.1 Syntactic Parsing",
      "text" : "For syntactic parsing, we use the Penn. treebank (PTB) dataset (Marcus et al., 1993). We used the standard data splits for training and testing (train sec 2-21; validation sec 22; test sec 23). We followed Petrov et al. (2006) preprocessing steps by right-binarizing the trees and replacing words with count ≤ 1 in the training sample with generic unknown word markers representing the tokens’ lexical features and position. The results reported in Table 2 are produced by EVALB.\nThe results in Table 2 demonstrate the superiority of our model compared to the baseline PCFG. We note that the A* parser becomes less effective (even with a large beam size) for this task, which we attribute to the large search arising for the large grammar and long sentences. Our best results are achieved by MCMC, demonstrating the effectiveness of MCMC in large search spaces.\nAn interesting observation is how our results compare with those achieved by bounded vertical and horizontal Markovization reported in (Klein and Manning, 2003). Our binarization corresponds to one of their simpler settings for horizontal markovization, namely h = 0 in their terminology, and note also that we ignore the head information which is used in their models. Despite this we still manage to equal their results obtained using vertical context of size 3 (v = 3), with 76.7 F1 score.\nTheir best result, F1 = 79.74, was achieved with h ≤ 2, v = 3 (and tags for head words). We believe that our model would outperform theirs if we consider greater horizontal markovization and incorporate head word information. To facilitate a fair comparison with vertical markovization, we experimented with limiting the size of the vertical contexts to 2, 3 or 4 within our model. Using MCMC parsing we found that performance consistently improved as the size of the context was increased, scoring 68.1, 71.1, 75.0 F-measure respectively. This is below 76.7 F-measure of our unbounded-context model which adapts itself to data to effectively capture the right context.\nOverall our approach significantly outperforms the baseline PCFG, although note these results are well below the current state-of-the-art in parsing, which typically makes use of discriminative training with much richer features. We speculate that future enhancements could close the gap between our results and that of modern parsers, while offering the potential benefits of our generative model which allows further incorporation of different types of contexts (e.g., head words and n-gram lexical context)."
    }, {
      "heading" : "6.2 Part-of-Speech Tagging",
      "text" : "The part of speech (POS) corpora have been extracted from PTB (sections 0-18 for training and 22-24 for test) for English, and NAACL-HLT 2012 Shared task on Grammar Induction2 for Danish and Swedish (Gelling et al., 2012). We convert the sequence of part-of-speech tags for each sentence into a tree structure analogous to a Hidden Markov Model (HMM). For each POS tag we introduce\n2 http://wiki.cs.ox.ac.uk/InducingLinguisticStructure/\nSharedTask\na twin (e.g., ADJ’ for ADJ) in order to encode HMM-like transition and emission probabilities in the grammar. As shown in Figure 5, this representation guarantees that all the rules in the structures are either in the form of ti → tj t′j (transition) or t′ → word (emission).\nThe tagging results are reported in Table 3, including comparison with the baseline PCFG (≡ HMM) and the state-of-the-art Stanford POS Tagger (Toutanova and Manning, 2000), which we trained and tested on these datasets. As illustrated in Table 3, our model consistently improves the PCFG baseline. While for Danish we outperform the stateof-the-art tagger, the results for English and Swedish we are a little behind the Stanford Tagger. This is an promising result since our model is only based on the rules and their contexts, as opposed to the Stanford Tagger which uses complex hand-designed features and a complex form of discriminative training.\nNote the strong performance of MCMC sampling, which consistently outperforms A* search on the three tagging tasks."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "We have proposed a novel hierarchical model over linguistic trees which exploits global context by conditioning the generation of a rule in a tree on an unbounded tree context consisting of the vertical chain of its ancestors. To facilitate learning of such a large and unbounded model, the predictive distributions associated with tree contexts are smoothed in a recursive manner using a hierarchical Pitman-Yor process. We have shown how to perform prediction based on our model to predict the parse tree of a given utterance using various search algorithms, e.g. A* and Markov Chain Monte Carlo.\nThis consistently improved over baseline methods in two tasks, and produced state-of-the-art results for Danish part-of-speech tagging.\nIn future, we would like to consider sampling the seating arrangements and model hyperparameters, and seek to incorporate several different notions of context besides the chain of ancestors."
    } ],
    "references" : [ {
      "title" : "The infinite hidden markov model",
      "author" : [ "M.J. Beal", "Z. Ghahramani", "C.E. Rasmussen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Beal et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Beal et al\\.",
      "year" : 2001
    }, {
      "title" : "Tnt – A statistical part-of-speech tagger",
      "author" : [ "T. Brants" ],
      "venue" : "In Proceedings of the sixth conference on Applied natural language processing,",
      "citeRegEx" : "Brants.,? \\Q2000\\E",
      "shortCiteRegEx" : "Brants.",
      "year" : 2000
    }, {
      "title" : "Programming languages and their compilers : preliminary notes",
      "author" : [ "J. Cocke", "J.T. Schwartz" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Cocke and Schwartz.,? \\Q1970\\E",
      "shortCiteRegEx" : "Cocke and Schwartz.",
      "year" : 1970
    }, {
      "title" : "Inducing treesubstitution grammars",
      "author" : [ "T. Cohn", "P. Blunsom", "S. Goldwater" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Cohn et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cohn et al\\.",
      "year" : 2010
    }, {
      "title" : "The infinite tree",
      "author" : [ "J. Finkel", "T. Grenager", "C. Manning" ],
      "venue" : "In Proceedings of the 45th annual meeting of Association for Computational Linguistics,",
      "citeRegEx" : "Finkel et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Finkel et al\\.",
      "year" : 2007
    }, {
      "title" : "Improvements to the sequence memoizer",
      "author" : [ "J. Gasthaus", "Y.W. Teh" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Gasthaus and Teh.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gasthaus and Teh.",
      "year" : 2010
    }, {
      "title" : "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, chapter The PASCAL Challenge on Grammar Induction, pages 64–80",
      "author" : [ "D. Gelling", "T. Cohn", "P. Blunsom", "J. Graca" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Gelling et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gelling et al\\.",
      "year" : 2012
    }, {
      "title" : "Pcfg models of linguistic tree representations",
      "author" : [ "M. Johnson" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Johnson.,? \\Q1998\\E",
      "shortCiteRegEx" : "Johnson.",
      "year" : 1998
    }, {
      "title" : "Bayesian inference for pcfgs via markov chain monte carlo",
      "author" : [ "M. Johnson", "T.L. Griffiths", "S. Goldwater" ],
      "venue" : "In HLT-NAACL,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2007
    }, {
      "title" : "Parsing and hypergraphs",
      "author" : [ "D. Klein", "C.D. Manning" ],
      "venue" : "In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT-2001),",
      "citeRegEx" : "Klein and Manning.,? \\Q2001\\E",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2001
    }, {
      "title" : "Accurate unlexicalized parsing",
      "author" : [ "D. Klein", "C.D. Manning" ],
      "venue" : "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume",
      "citeRegEx" : "Klein and Manning.,? \\Q2003\\E",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2003
    }, {
      "title" : "A bayesian model for learning scfgs with discontiguous rules. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pages 223–232",
      "author" : [ "A. Levenberg", "C. Dyer", "P. Blunsom" ],
      "venue" : null,
      "citeRegEx" : "Levenberg et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Levenberg et al\\.",
      "year" : 2012
    }, {
      "title" : "The infinite PCFG using hierarchical Dirichlet processes",
      "author" : [ "P. Liang", "S. Petrov", "M. Jordan", "D. Klein" ],
      "venue" : "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-",
      "citeRegEx" : "Liang et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2007
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Probabilistic cfg with latent annotations",
      "author" : [ "T. Matsuzaki", "Y. Miyao", "J. Tsujii" ],
      "venue" : "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,",
      "citeRegEx" : "Matsuzaki et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Matsuzaki et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning and inference for hierarchically split PCFGs",
      "author" : [ "S. Petrov", "D. Klein" ],
      "venue" : "In Proceedings of the TwentySecond AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Petrov and Klein.,? \\Q2007\\E",
      "shortCiteRegEx" : "Petrov and Klein.",
      "year" : 2007
    }, {
      "title" : "Enriching the knowledge sources used in a maximum entropy part-ofspeech tagger",
      "author" : [ "K. Toutanova", "C.D. Manning" ],
      "venue" : "In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora,",
      "citeRegEx" : "Toutanova and Manning.,? \\Q2000\\E",
      "shortCiteRegEx" : "Toutanova and Manning.",
      "year" : 2000
    }, {
      "title" : "A stochastic memoizer for sequence data",
      "author" : [ "F. Wood", "C. Archambeau", "J. Gasthaus", "L. James", "Y.W. Teh" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Wood et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wood et al\\.",
      "year" : 2009
    }, {
      "title" : "A stochastic memoizer for sequence data",
      "author" : [ "F. Wood", "C. Archambeau", "J. Gasthaus", "L. James", "Y.W. Teh" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Wood et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Wood et al\\.",
      "year" : 2009
    }, {
      "title" : "The sequence memoizer",
      "author" : [ "F. Wood", "J. Gasthaus", "C. Archambeau", "L. James", "Y.W. Teh" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Wood et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wood et al\\.",
      "year" : 2011
    }, {
      "title" : "Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale boundconstrained optimization",
      "author" : [ "C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal" ],
      "venue" : "ACM Transactions on Mathematical Software (TOMS),",
      "citeRegEx" : "Zhu et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "While infinite order Markov models have been extensively explored for language modelling (Gasthaus and Teh, 2010; Wood et al., 2011), this has not yet been done for structure prediction.",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 19,
      "context" : "While infinite order Markov models have been extensively explored for language modelling (Gasthaus and Teh, 2010; Wood et al., 2011), this has not yet been done for structure prediction.",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 16,
      "context" : "obtains similar performance to the state-of-theart Stanford part-of-speech-tagger (Toutanova and Manning, 2000) for English and Swedish.",
      "startOffset" : 82,
      "endOffset" : 111
    }, {
      "referenceID" : 19,
      "context" : "A prime work is Sequence Memoizer (Wood et al., 2011) which conditions the generation of the next word on an unbounded history of previously generated words.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "For syntactic parsing, several infinite extensions of probabilistic context free grammars (PCFGs) have been proposed (Liang et al., 2007; Finkel et al., 2007).",
      "startOffset" : 117,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "For syntactic parsing, several infinite extensions of probabilistic context free grammars (PCFGs) have been proposed (Liang et al., 2007; Finkel et al., 2007).",
      "startOffset" : 117,
      "endOffset" : 158
    }, {
      "referenceID" : 3,
      "context" : "An alternative method allows for infinite grammars by considering segmentation of trees into arbitrarily large tree fragments, although only a limited history is used to conjoin fragments (Cohn et al., 2010; Johnson et al., 2006).",
      "startOffset" : 188,
      "endOffset" : 229
    }, {
      "referenceID" : 7,
      "context" : "(Johnson, 1998) has increased the history for the parsing task by parent-annotation, i.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 10,
      "context" : "(Klein and Manning, 2003) have considered vertical and horizontal markovization while using the head words’ part-of-speech tag, and showed that increasing the size of the vertical contexts consistently improves the parsing performance.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : ", 2006), (Petrov and Klein, 2007) and (Matsuzaki et al.",
      "startOffset" : 9,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : ", 2006), (Petrov and Klein, 2007) and (Matsuzaki et al., 2005) have treated non-terminal annotations as latent variables and estimated them from the data.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Previous works on applying Markov models to part-of-speech tagging either considered finiteorder Markov models (Brants, 2000), or finite-order HMM (Thede and Harper, 1999).",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "More specifically, we assume that a distribution with the full history G[u] is related to a distribution with the most recent history G[π(u)] through the Pitman-Yor process PY P (Wood et al., 2011):",
      "startOffset" : 178,
      "endOffset" : 197
    }, {
      "referenceID" : 17,
      "context" : "We make use of the approach developed in Wood et al. (2011) for learning such suffix-based graphical models when learning infinite-depth language models.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "1 We maximize the posterior with the constraints cm ≥ 0 and dm ∈ [0, 1) using the L-BFGSB optimisation method (Zhu et al., 1997), which results in the optimised discount and concentration values for each context size.",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "CYK (Cocke and Schwartz, 1970), for finding the highest scoring tree.",
      "startOffset" : 4,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "The space of all possible trees for a given utterance can be compactly represented as a hyper-graph (Klein and Manning, 2001).",
      "startOffset" : 100,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "For each utterence, we sample a fresh tree for the whole utterance from a PCFG using the approach of (Johnson et al., 2007), which works by first computing the inside lattice under the proposal model (which can be computed once and reused), followed by top-down sampling to recover a tree.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 11,
      "context" : "We leave local sampling for future work, noting that the obvious local operation of resampling complete sub-trees or local tree fragments would compromise detailed balance, and thus not constitute a valid MCMC sampler (Levenberg et al., 2012).",
      "startOffset" : 218,
      "endOffset" : 242
    }, {
      "referenceID" : 13,
      "context" : "treebank (PTB) dataset (Marcus et al., 1993).",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "treebank (PTB) dataset (Marcus et al., 1993). We used the standard data splits for training and testing (train sec 2-21; validation sec 22; test sec 23). We followed Petrov et al. (2006) preprocessing steps by right-binarizing the trees and replacing words with count ≤ 1 in the training sample with generic unknown word markers representing the tokens’ lexical features and position.",
      "startOffset" : 24,
      "endOffset" : 187
    }, {
      "referenceID" : 10,
      "context" : "An interesting observation is how our results compare with those achieved by bounded vertical and horizontal Markovization reported in (Klein and Manning, 2003).",
      "startOffset" : 135,
      "endOffset" : 160
    }, {
      "referenceID" : 6,
      "context" : "The part of speech (POS) corpora have been extracted from PTB (sections 0-18 for training and 22-24 for test) for English, and NAACL-HLT 2012 Shared task on Grammar Induction2 for Danish and Swedish (Gelling et al., 2012).",
      "startOffset" : 199,
      "endOffset" : 221
    }, {
      "referenceID" : 16,
      "context" : "The tagging results are reported in Table 3, including comparison with the baseline PCFG (≡ HMM) and the state-of-the-art Stanford POS Tagger (Toutanova and Manning, 2000), which we trained and tested on these datasets.",
      "startOffset" : 142,
      "endOffset" : 171
    } ],
    "year" : 2015,
    "abstractText" : "Linguistic structures exhibit a rich array of global phenomena, however commonly used Markov models are unable to adequately describe these phenomena due to their strong locality assumptions. We propose a novel hierarchical model for structured prediction over sequences and trees which exploits global context by conditioning each generation decision on an unbounded context of prior decisions. This builds on the success of Markov models but without imposing a fixed bound in order to better represent global phenomena. To facilitate learning of this large and unbounded model, we use a hierarchical PitmanYor process prior which provides a recursive form of smoothing. We propose prediction algorithms based on A* and Markov Chain Monte Carlo sampling. Empirical results demonstrate the potential of our model compared to baseline finite-context Markov models on part-of-speech tagging and syntactic parsing.",
    "creator" : "LaTeX with hyperref package"
  }
}