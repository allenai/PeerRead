{
  "name" : "1705.10201.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Machine Learned Learning Machines",
    "authors" : [ "Leigh Sheneman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n10 20\n1v 1\n[ cs\n.A I]\n2 9\nM ay\n2 01\n7"
    }, {
      "heading" : "Machine Learned Learning Machines",
      "text" : "Leigh Sheneman1,3 & Arend Hintze1,2,3\n1Department of Computer Science and Engineering, Michigan State University 2Department of Integrative Biology, Michigan State University 3BEACON-Center for the Study of Evolution in Action, Michigan State University\nThere are two common approaches for optimizing the performance of a machine: genetic algorithms and machine learning. A genetic algorithm is applied over many generations whereas machine learning works by applying feedback until the system meets a performance threshold. Though these are methods that typically operate separately, we combine evolutionary adaptation and machine learning into one approach. Our focus is on machines that can learn during their lifetime, but instead of equipping them with a machine learning algorithm we aim to let them evolve their ability to learn by themselves. We use evolvable networks of probabilistic and deterministic logic gates, known as Markov Brains, as our computational model organism. The ability of Markov Brains to learn is augmented by a novel adaptive component that can change its computational behavior based on feedback. We show that Markov Brains can indeed evolve to incorporate these feedback gates to improve their adaptability to variable environments. By combining these two methods, we now also implemented a computational model that can be used to study the evolution of learning.\nA broad range of computational methods including data mining, clustering, classification,\nand evolutionary computation 1 fall under the category of ”machine learning”. Typically, these\nmethods try to find a solution to a specific problem. In the case of supervised learning a reference or example data set is provided to learn from. If no extra data is provided, unsupervised learning methods try to find structures or clusters within the data. While genetic algorithms (GAs) typically fall into the category of supervised learning we don’t necessarily provide an example but use a fitness function to compare solutions with each other to ultimately find the best one. Most of the supervised learning methods like genetic algorithms, reinforcement learning, or back propagation work over many iterations, adapt the solution by small changes, and stop once a satisfying solution is found.\nIn nature adaptation pertains to two distinctly different processes, namely evolution and\nlifetime changes (aka plasticity). Darwinian Evolution, which typically takes many generations, shapes an organism to fit the environment better through inheritance, variation, and natural selection. Evolution also optimizes an organism’s ability to adapt during its lifetime to its specific circumstances. This adaptability can be accomplished in many different ways, but generally by increasing an organism’s plasticity. While plasticity occurs in many forms, here we focus on neural plasticity which means that an organism uses “experiences” to improve later decisions and behavior. Neural plasticity allows natural organisms to learn due to reinforcement of their behavior 2. However, in natural organisms learning is tied to specific neural mechanisms- working memory (WM), short-term memory (STM) and long-term memory (LTM). It is difficult to draw an analogy between these cognitive mechanisms and the processes that happen in artificial computational systems, because the boundaries of each mechanism are unclear.\nNature combines two optimization processes and uses evolution to improve neural plasticity\nwhich has a wide range of implications 3. It is this type of plasticity that we want in artificial intelligence (AI): making better decisions from experience. However, currently we struggle to construct systems with proper internal models to represent the world 4, 5. Even the most human competitive AI systems use machine learning to create models 6 but most of those models still lack plasticity, meaning that they can’t adapt to their environment over their lifetime. One approach to overcome this is to let the system choose to either evolve structures so that they are easier to adapt by machine learning methods, or use a learning mechanism to pre-select structures that can be evolved later 7–9. Similarly, neuromodulation in artificial neural networks can be evolved which improves performance 10–13. Even though the results were less promising, when evolving artificial neural networks with dynamic policies 14 (dynamic policies could be understood as a form of neuromodulation or even a learning algorithm). However, all these approaches use an explicit external stimulus informing the network whether or not its actions were appropriate or not. This is very different to what we observe in nature. There is no objective implicit feedback telling an organisms that its actions will lead to more offspring or not. Organisms are evolved to receive pain or feel hunger and to a certain degree can interpret if their actions were improving a situation or not. In other words, feedback is generated within the organisms, based on observations about the world.\nHere we show, how one can integrate both methods into a continuous process. We use\na genetic algorithm to evolve the brain of a virtual agent to not only perform well within the environment, but to also evolve the ability to apply feedback to dedicated components that can\nadapt to the given feedback. As a consequence we evolve machines that can adapt over their lifetime.\nThe AI system we use is Markov Brains (MB) 5, 15–22, which are networks of deterministic\nand probabilistic logic gates, encoded in such a way that Darwinian evolution can easily improve them. One can think of these MBs as artificial neural networks (ANN) 23 with an arbitrary topology that uses Boolean logic instead of logistic functions. Through sensors these networks receive information about their environment as zeros or ones, perform computations and typically act upon their environment through their outputs. We commonly refer to MBs that are embodied and through that embodiment24 interact with their environment as agents (others use the term animat which is synonymous).\nWhile no direct analogy can be drawn between nature and MBs, we can find similarities\nbetween the neural mechanisms used in learning and the inputs, logic gates and states of MBs. In nature, information that is currently processed by the brain is stored in working memory (WM) 25, 26. This is similar to the registers of a CPU, or in the case of MBs the zeros and ones used by the logic gates to perform computations. Information that a living organism needs to store for a moment is believed to reside in short term memory (STM) 26, 27, but how information transforms fromWM to STM is not fully understood 25, 27, 28. MBs can mimic the rehearsing used to maintain a STM by forming recurrent connections and thus keep information present in the brain. This information can be used in computations and collectively forms a representation of the environment 5. These recurrent connections can simply store information, very much like a switch can indefinitely\nstore whether it is ON or OFF. Natural systems, on the other hand, use their long term memory (LTM) if they want to keep information for much longer. Presumably, information from STM becomes reinforced and thus forms LTM, this sometimes referred to as consolidation 26, 27, 29. The reinforcement process takes time and therefore is less immediate than STM. In addition, memories can be episodic or semantic 26, 27, 30 and can later be retrieved to influence current decisions.\nHere we introduce what we call feedback gates, which allow MBs to use internal feedback\nto store information by changing their probabilistic logic gates. Using this feedback mechanism to change their internal structure is akin to learning and forming long term memories during the lifetime of an organism.\nTo this point we have focused on adaptivity within the life of an organism. However, when\nwe incorporate evolution we can not overlook the effects of structural changes that occur between generations. Beyond the four major neurobiological concepts mentioned above – WM, STM, LTM, and learning 27 – we also have the evolutionary process that not only shaped these neural mechanisms but is itself adaptive. Naı̈vely arguing that evolution happens across many generations while learning happens during the lifetime of an organism neglects the interdependence that exist between the two processes. Evolution changes the neural mechanisms an organism uses to learn, and how well an organism learns has an effect of the evolutionary pressures it experiences 3, 31. Historically in computer science, these two adaptive processes are either never coupled or used interchangeably.\nMany variations of machine learning have attempted to bridge this gap, but have fallen short.\nFor example, in Q-learning 32 Markov Decision Processes maximize performance in supervised learning environments by applying rewards at the end of a generation, but require large networks to converge 33, 34. Another option, back-propagation, can be used to train artificial neural network by comparing the results achieved to the desired outcome at each generation. However, this method is non-trivial, and changes to the neural weights are predefined 35–37. When a feature set is completely disclosed, the Baum-Welch algorithm aids in event detection through determining the maximum likelihood estimate of the parameters, but this is a luxury nature seldom provides 38, 39. Multiplicative weights algorithm strengthens or weakens connections in a neural network-based the consensus of a pool of experts 40, 41, although many times learning must occur in isolation, that is without the experts. In neither case an adaptive process is used to create an adaptive machine. Here, instead of adding yet another machine learning method or modification to an evolutionary algorithm, we combine both approaches and we will show how evolved feedback learning can promote behavioral plasticity."
    }, {
      "heading" : "1 Results",
      "text" : "In order to evolve agents to learn during their lifetime we use a navigation task. The environment is a 2D lattice (64x64 tile wide) where a single tile is randomly selected as the goal an agent must reach. The lattice is surrounded by a wall so agents can’t escape the boundary, and 1 7 of the lattice is filled with additional walls to make navigation harder. From the goal the Dijkstra’s path is computed so that each tile in the lattice can now indicate which of its neighboring tiles is the next closest to the goal. In cases where two neighbor tiles might have the same distance, one of these\ntiles is randomly selected as the next closest. For ease of illustration we can now say that a tile has an arrow pointing towards the tile that should be visited next to reach the goal in the shortest number of tiles.\nThe agent, controlled by a Markov Brain, is randomly placed on a tile that is 32 tiles away\nfrom the goal and facing in a random direction (north, west, south, or east). Agents can see the arrow of the tile they are standing on. The direction indicated by the tile is relative to the that of the agent, so that a tile indicating north, will only be perceived as a forward facing arrow if the agent also faces north. The agent has four binary sensors that are used to indicate in which relative direction the agent should go to reach the goal.\nThe agent can move over the lattice by either turning 90 degrees to the left or right, or by\nmoving forward. So far, in order to navigate perfectly, the agent would simply need to move forward when seeing a forward facing arrow, or turn accordingly. Instead of allowing the agent to directly pick a movement, it can choose one of four intermediate options (A,B,C,or D) at any given update. At the birth of an agent, these four possible options are mapped to four possible actions: move forward, turn left, turn right, do nothing. As a result, the complexity of the task increases when the agent has to learn which of the 24 possible option-to-action maps currently applies to navigate the environment properly. The agent is not given any direct feedback about its actions; a mechanism must evolve to discern the current mapping and this is rather difficult.\nIn prior experiments 5, 15–22, MBs were made from deterministic or probabilistic logic gates\nthat use a logic table to determine the output given a particular input. Deterministic gates have\none possible output for each input, while probabilistic gates use a linear vector of probabilities to determine the likelihood for any of the possible outputs to occur. To enable agents to form long term memory and learn during their lifetime we introduce a new type of gate: feedback gate. These gates are different from other probabilistic gates, in that they can change their probability distribution during their lifetime based on feedback (for a detailed description, see below). This allows for permanent changes which are akin to long term memory. While Markov Brains could already retain information by using hidden states, now they can also change ”physically”. Markov Brains now have to evolve to integrate these new gates into their network of other gates and find a way to supply feedback appropriately.\nFeedback Gate Usage. To test if the newly introduced feedback gates help evolution and increase performance, we compare three different evolutionary experimental conditions. Agents were evolved over 500,000 generations that could use only deterministic logic gates, deterministic and probabilistic logic gates, or all three types of gates–deterministic, probabilistic, and feedback gates to solve the task.\nWhen analyzing the line of decent (LOD; see materials and methods), we find a strong dif-\nference in performance across the three evolutionary conditions (see supplementary information Figure 1). None of the 300 agents that were evolved using deterministic together with probabilistic logic gates were capable of reaching the goal in any of the 24 mappings. The agents that were allowed to use only deterministic logic gates failed to reach the goal in 225 of the 300 experiments, but the remaining 75 agents never reached the goal more than five times. Agents allowed to use all\ngates including feedback gates only failed to reach the goal in 75 experiments and in the remaining 225 experiments they reached the goal on average 5 times, with the best performer reaching the goal on average 9 times.\nIt is not entirely surprising to us that agents using only probabilistic gates struggle in this\ntask, because agents using probabilistic gates generally evolve slower, which might explain the effect. However, to our surprise, we found a couple of agents who were only allowed to use deterministic gates that evolved to solve the task at least a couple of times. In the group that could use all three types of gates, we found 3 agents that could reach the goal on average 5 times using only probabilistic gates . This shows two things: the task can be solved using only the gate inputs (i.e. WM) and providing agents with feedback gates during evolution allows them to reach the goal more often. This is an important control because from a computational point of view, there is no qualitative difference between WM and LTM as both methods allow for recall of the past.\nAgents allowed to use feedback gates quickly evolve the ability to reach the goal in any of the\n24 possible environments. The variance of their performance supports the same idea, that agents do not become better by just performing well in one environment, but instead evolve the general ability to learn the mapping each environment presents (See Figure 1 panel B).\nNow that we have shown that the agents with feedback gates are capable of evolving a so-\nlution to navigate in this environment, we have to ask if they actually utilize the feedback gates. For that, all agents on the LOD were tested again but their feedback gates were kept from changing their probability tables. Comparing these results with the agents performance when using the\nfeedback gates regularly reveals that the agents rely heavily on their feedback gates (see 1 panel A). As a comparison we evolved LSTM artificial neural networks 42 (see Supplementary Figure 9 for more details). These LSTM are recurrent artificial neural networks and have been widely used to solve numerous problems. As mentioned earlier, this task does not allow us to use back propagation since the expected output distribution is unknown. To circumvent the need for back propagation we use a GA with optimized the weights of the LSTM over 500.000 generations. All the LSTMs behaved similarly so 100 replicates were sufficient. We find that the LSTMs have on average an inferior performance than Markov Brains (see Figure 1 panel A dashed line), and while they perform well on certain mappings, they struggle greatly when it comes to generalization. As a result they learn a few mappings well but are incapable of reaching the goal on the majority of the other maps (see Figure 1 panel B dashed line).\nFeedback gates change over time We find that the probability tables modified by feedback become specifically adapted to each of the 24 possible mappings the agents get tested in. See Figure 2 as an example of the best performing agent using only one feedback gate. Some rows in the probability tables converge to having a single high value that is specific to the environment the agent experienced (for more details see supplementary information Figures 2-3). This shows, that indeed feedback gates become specifically adapted to the environment the agent experiences. It also indicates, that agents change their computational machinery according to their environment and do not rely solely on WM to perform their task.\nThe change to the feedback gates’ probability tables can be quantified by measuring the mu-\ntual information each table conveys at birth and after the agent completes their task. We find that the mutual information is generally lower at birth ( ˜0.25) and higher at the end of the task (0̃.8), signifying that the agents have more information about the environment at death then they did at birth, as expected. We then compute the difference between both measurements, (∆̄), which quantifies the increase of mutual information over the lifetime of the agent. When binning these values for different levels of performance, we find a strong correlation (0.922) between performance and increase in mutual information (see Figure: 3). This shows that agents who perform better increase information stored in their feedback gates over their lifetime.\nDifferences between agents using feedback gates and those who do not We wanted to test if feedback gates improve the agents ability to learn. However, we find agents that don’t utilize them, and instead are made from only deterministic gates. This either suggests that feedback gates are not necessary, or that they don’t provide enough of an selective advantage to be used every time. It is also possible that there is more than one algorithmic solution to perform well in this navigation task. All of these points suggest that a more thorough analysis and comparison of the independently evolved agents is necessary.\nWe find that agents that do not use feedback gates require a much greater number of logic\ngates than those who do (see Figure 4). This seems intuitive, since feedback gates can store information in their probability tables, whereas agents that do not use them, need to store all information in their WM. This suggests that there might be a difference in the evolved strategy between those agents that use feedback gates and those who do not. When observing the behavior of the differently evolved agents, we get the impression that there are two types of strategies (see supplementary information Figures 4-5 for details). Agents that evolved brains that do not contain feedback gates use a simple heuristic that makes them repeat their last action when the arrow they stand on points into the direction they are standing on. Otherwise, they start rotating until they move off a tile, which often results in them standing again on a tile that points into the direction they are facing, which makes them repeat the last action.\nAgents that evolved to use feedback gates on the other hand, appear to actually behave as\nif they learn how to turn properly. They make several mistakes in the beginning, but after some\nlearning period perform flawlessly. Of the 300 replicate evolutionary experiments where agents were allowed to use all types of logic gates, 56 did not end up using feedback gates. Comparing the actions those 56 agents take, with the remaining 244 which do use feedback gates, we first find that as expected the group using feedback gates reached the goal more often on average (5.33 times, versus 4.89 times for those agents not using feedback gates) which suggests a difference in behavior. The usage of actions is also drastically different even during evolution (see Figure 5). Agents using feedback gates reduce the instances where they do nothing, minimize turns and maximize moving forward. Agents not using feedback gates are less efficient because they rely on forward movements while minimizing times where they do nothing and turns. In conjunction with the observations made before, we conclude that indeed agents not using feedback gates, use some form of heuristic with a minimal amount of memory, while agents using feedback gates simply\nlearn to navigate the environment properly.\n1.0.1 Comparison to Q-learning\nSo far, we explored feedback learning in the context of evolutionary adaptation. To illustrate the difference between traditional feedback learning and our approach we compare it to Q-learning 43, 44. While it is easy to learn a single mapping the actual task would require agents to Q-learn all possible 24 mappings at the same time. The other option in the machine learning domain would be back propagation on an artificial neural network 1. While it would be easy to use an ANN to control the agent, back propagation requires us to know the desired outputs of the networks which\neliminates this option. For the sake of argument, let us consider the simplest case of learning only one map at a time using Q-learning.\nQ-learning uses a probability matrix, Q, that controls both the agent and when rewards are\napplied to Q. In this task the agent can be in four possible states (defined by the four possible inputs that the agent can read) at any given time and for each state the agent can choose one of four possible actions based on a 4x4 probability matrix (Q). The Q-matrix is initialized with uniform probabilities. The agent is then placed in the same navigation environment as the agents controlled by MBs. Ideally every action of the agent should be rewarded, but since this environment does not provide an explicit feedback this can not be done. Instead, the MB agents had to evolve the mechanism that updates the probability table of their feedback gates in order to receive this information. To substitute for that, we define the moment of reaching the goal a success which becomes rewarded. Once the goal is reached agent is reset to a valid start position.\nUpdates to the Q-matrix use a predetermined reward, R, whose magnitude controls the rate\nof learning. Both Q-learning and feedback gates have to use a process of trial-and-error to obtain feedback when a new environment is encountered, resulting in a random walk. Each time an agent reaches a goal using this process all actions taken are reinforced. Having a high R (R > 0.01) in this stage causes Q not to converge on a solution and prevents the agents from ever initially reaching the goal (data not shown). Instead we needed to reduce the initial R until it was as low as 0.0001. However, this value is so too low for an effective learning to occur, so we increase it by 0.00001 every time the goal was reached resulting in a convergence of Q. This is similar to\nthe policy gradient method 45, except that we use a strict temporal regime instead of updating the policy dynamically. This may lead to less optimal learning, but for our comparison the optimality is irrelevant.\nWe performed 500 independent Q-learning trials, tracking the number of updates it took on\naverage to arrive at the goal. As expected, we find that Q-learning is indeed capable of optimizing the performance of an agent on one specific mapping (see Figure 6 A). After the agent reaches the goal approximately ten times, performance continues to improve until the agent reaches a near optimal efficiency of on average of 42 updates to reach the goal. As mentioned earlier, there is no fair comparison between Q-learning and our approach, mostly because agents are evolved to learn any of the 24 possible mappings during their lifetime while we only use Q-learning to learn one environment. When learning a single environment, MBs reach the same performance level in less than 400 generations (see supplementary information Figure 8).\nQ-learning spends most of its time performing poorly, then suddenly converges on the so-\nlution. Watkins 32 discusses the optimality of Q-learning, and one of the key assumptions is that exploration must be cheap in order for learning to evolve. Q-learning in this context, spends most of the time on a random walk, potentially reaching the goal a couple of times. This process results in a biases to the Q-matrix that allows for a higher learning rate leading to optimal performance. On the other hand, Evolution quickly identifies agents that perform somewhat well in all 24 environments before optimizing performance (see Figure 6 B). Since evolution competes agents against each other, and those agents that reach the goal faster get selected. This makes the cost of explo-\nration relative since it applies to all competitors equally. Consequently, evolution selects for agents who minimize time spent on exploration.\nIn comparison evolved agents reach the goal 10 times on average no matter which of the 24\nfor mappings they are placed in; meaning agents take approximately 51 updates each time they reach the goal. MB agents take an average of 118 times steps to learn the environment and reach the first goal, 54 time steps to the second, and for remaining goals take the near optimal average of 50 time steps to reach the goal. Since it takes more time to reach the first few goals the average is positively skewed (see supplementary information Figure 7).\nThis scenario suggests that the evolved agents are extremely effective when compared to Q-\nlearning: Within the first 118 time steps of agents learn to navigate in their specific environment. In this 118 steps, agents deploy random actions, compare the effect indicated through their sensors and update their feedback gates to continually improve their ability to execute the correct actions. From Figure 6 B we know that Q-learning needs about 85,000 generations of 512 updates to get to the same performance in a single mapping as opposed to an arbitrary set of 24 possible mappings. The evolved agents incorporate and optimize the reward mechanism in their cognitive architecture, while the Q-learning algorithm has to wait for a delayed reward, R, to adapt to the environment.\nQ-learning of multiple mappings at the same time Using Q-learning to adapt an agent to navigate only one environment allowed us to show the extent to which an agent controlled by a Markov Brain using feedback gates outperforms this reinforcement algorithm. However, the task the evolved agent faces is many orders of magnitude more difficult. The agents evolved the ability\nto explore the environment, and learn any of the 24 possible mappings (similar to contexts 46). To accommodate for that complexity, we expand the Q matrix to include additional states. As a consequence the Q-matrix grows exponentially. Imagine we want to enable the agent to encode two possible mappings map0 and map1. The agent is not in four possible states {L,R, F,B} (left, right, forward, backward) anymore but in eight:{L0, R0, F0, B0, L1, R1, F1, B1}. This also expands the number of actions from four {A,B,C,D} to eight {A0, B0, C0, D0, A1, B1, C1, D1}. If, for example, the agent assumes to be in map0 but performs and gets rewarded for the action A1 it would start preforming the actions found onmap1. This effectively adds hidden states to the Markov Decision Process, and it is known that these hidden states are notoriously hard to learn 1, 47–49. Various different techniques have been developed to reduce the memory or time requirements 50–52 or changes to the hidden Markov Process have been proposed 53, 54.\nExtending the Q-matrix as shown above allows us to explore how Q-learning can improve\nan agents performance when confronted with multiple mappings. Instead of just finding a new start location for an agent when it reaches the goal, we now also change the current mapping. Feedback is applied when the agent experienced all possible mappings, which are drawn randomly and without replacement from a predefined set. Applying feedback immediately every time the goal was reached resulted in less adapted agents in the end.\nEvery time an agent reaches the goal the reward is increased as before. Agents were allowed\nto explore the environment for 109 time steps and each experiment was repeated 100 times. This time-frame is sufficient for the solutions to become stable. From those 100 samples we take the\naverage of the top 10 best performers to find examples where Q-learning worked optimally. We find that agents that had to learn to navigate 4 different mappings needed 4 hidden states to perform optimally, while agents exposed to 8 different mappings performed best using 16 hidden states (See Figure 7 A and B). When given more or less states their performance declined, however with an increasing number of hidden states the feedback needed to be lower (See Figure 7 C and D).\nWhileQ-learning improved the agents performance greatly compared to the untrained agent,\nthe ultimately attained performance was inferior to the evolved agents. The best evolved agents need about 40 time steps to reach a goal while those agents that evolved a heuristic need about 100 time steps regardless of mapping. Q-learned agents on 4 mappings need about 128 steps and agents confronted with 8 possible mappings need up to 9500 time steps. This confirms that using reinforcement learning struggles with situations that have hidden states, and emphasizes the advantage of using a genetic algorithm in conjunction with elements that can be feedback learned."
    }, {
      "heading" : "Discussion",
      "text" : "MBs have been used in different contexts to study the evolution of behavior 5, 15–22 using volatile hidden states for memory, but never an explicit learning mechanism. However, in nature we know that WM is different from STM or LTM and that learning occurs when STM transitions into LTM. To allow for this functionality we provide feedback gates that can change their probabilities when a positive or negative feedback signal is applied. Agents successfully incorporated these feedback gates when challenged with a navigation task that required them to learn how their choices affect\ntheir movement.\nAgents also evolved the ability to interpret the environment and provide appropriate feedback\nto their feedback gates. They incorporate information about the environment over their lifetime, and thus change their behavior appropriately to navigate properly. In addition, we find agents to be able to solve the same task using only deterministic gates. However, their performance is on average poorer, and instead of learning how their choices effect their movement they deploy a complex heuristic that helps them find the goal.\nThe central contribution of this work is that the agents must develop a learning rule and\nevolve how the feedback should be applied as opposed to be given direct feedback. This becomes most apparent when we ask: What did the agents evolve to do? The answer is that they evolved to apply feedback in the right moment while exploring their options in a meaningful way. Machine learning methods typically optimize towards an objective function, and predefine how feedback is obtained and applied. Here the genetic algorithm ultimately selects individuals who learn better based on a global fitness function we defined. Further the agents have to evolve an internal mechanism that applies feedback when appropriate to improve performance.\nThe differences between traditional learning algorithms like Q-learning and the MB agents\nmake them hard to compare. The navigation environment never directly informs the agent about progress or performance so MB agents evolve methods to recognize when progress is being made. When comparing the performance of Q-learning to this task we find two very striking differences. First,Q-learning is very capable of learning one of the 24 possible mappings, though not as quickly\nas a MB is able to learn a single map, but struggles to learn when presented with 4 or more mappings. Additionally, once Markov Brains are evolved to interpret the environment they learn each particular environment much faster than a Q-learner does on one map. Machine learning methods have a plethora of applications, and will remain an important tool. However, the very simple environment we designed here provides a realistic challenge, that in our opinion can not be solved conveniently by a pure learning algorithm or deep learning. Reinforcement learning struggles when the environments or agent that needs to understand the environment has hidden states. It seems questionable how these algorithms can extend their capabilities to such problems, when it comes to building more adaptive machines. Evolution on the other hand can find a way to not only optimize the learning algorithm, but also could solve the hidden state problem."
    }, {
      "heading" : "Methods",
      "text" : "Markov Brains are networks of probabilistic and deterministic logic gates encoded by a genome. The genome contains genes and each gene specifies one logic gates, the logic it performs and how it is connected to sensors and motors and to other gates 5, 15, 22. A new type of gate, the feedback gate, has been added to theMarkov Brain framework (https://github.com/lsheneman/PROJECTREPO)),and this framework has been used to run all the evolutionary experiments. The Markov Brain framework has since been updated to MABE 55. See below for a detailed description of each component:\nEnvironment The environment the agents have to navigate is a 2D spatial grid of 64x64 squares. Squares are either empty or contain a solid block that can not be traversed. The environment is surrounded by those solid blocks to prevent the navigating agent to leave that space. At the beginning\nof each agent evaluation a new environment is generated and 1 7 of the squares are randomly filled with a solid block. The randomness of the environments maintains a complex maze-like structure across environments, but no two agents will see the exact same environment.\nA target is randomly placed in the environment, and Dijkstra’s algorithm is used to compute\nthe distance from all empty squares to the target block. These distances are used to label each empty block so that it has an arrow facing to the next closest block to the target. When there is ambiguity (two adjacent blocks have the same distance) a random block of the set of closest blocks is chosen. At birth agents are randomly placed in a square that has a Dijkstra’s number of 32 and face a random direction (up, right, down, or left). Due to the random placement of blocks it is possible that the goal is blocked so that there is no tile that is 32 tiles away, in which case a new environment is created, which happens only very rarely.\nAgents are now allowed to move around the environment for 512 updates. If they are able\nto reach the target, a new random start orientation and location with a Dijkstra’s number of 32 is selected. Agents use two binary outputs from the MB to indicate their actions– 00, 01, 10, or 11. Each output is translated using a mapping function to one of four possible actions- move forward, do nothing, turn left, or turn right. This results in 24 different ways to map the four possible outputs of the MB to the four possible actions moving the agent. The input sensors give information about the label of the tile the agent stands on. Observe that the agent itself has an orientation and this the label is interpreted relative to the direction the agent faces. The four possible arrows the agent can see– forward, right, backward, or left– and are encoded as four binary inputs, one for each possible direction. Beyond the four input and two outputs nodes, agents can use 10 hidden nodes\nto connect their logic gates. Performance (or fitness) is calculated by exposing the agent to all 24 mappings and testing how often it reaches the goal within the 512 updates it is allowed to explore the world. At every update agents are rewarded proportional to their distance to the goal (d), and receive a bonus (b) every time they reach the goal, thus the fitness function becomes:\nW = n<24∏\nn=0\n(( t<512∑\nt=0\n1\n1 + d ) + b) (1)\nSelection After all agents in a population were tested on all 24 action-to-behavior mappings at each generation, the next generation was selected using tournament selection where organisms where individuals are randomly selected and the one with the best fitness transmits offspring into the next generation56. The tournament size is set to five.\nMutation Genomes for organisms in the first generation are generated randomly with a length of 5000 and 12 start codons are inserted coding for deterministic, probabilistic, and feedback gates. Each organism propagated into the next generation inherits the genome of it’s ancestor. The genome has at least 1000 and at most 20, 000 sites. Each site has a 0.003 chance to be mutated. If the genome hasn’t reached it’s maximum size stretches of a randomly selected length between 128 and 512 nucleotides get copied and inserted at random locations with a 0.02 likelihood. This allows for gene duplications. If the genome is above 1000 nucleotides, there is a 0.02 chance for a stretch of a randomly selected length between 128 and 255 nucleotides to be deleted at a random location.\nFeedback Gates At every update of a probabilistic gate, an input i results in a specific output o. To encode the mapping between all possible inputs and outputs of a gate we use a probability\nmatrix P . Each element of this matrix Pio defines the probability that given the input i the output o occurs. Observe that for each i the sum over all o must be 1.0 to define a probability:\n1.0 = O∑\no=0\nPio (2)\nwhere O defines the maximum number of possible outputs of each gate.\nA feedback gate uses this mechanism to determine it’s output at every given update. How-\never, at each update we consider the probability Pio that resulted in the gates output to be causally responsible. If that input-output mapping for that update was appropriate in future updates that probability should be higher. If the response of the gate at that update had negative consequences, the probability should be lower. As explained above, the sum over all probabilities for a given input must sum to one. Therefore, a single probability can not change independently, and thus, if a probability is changed, the other probabilities are normalized so that equation 2 remains true.\nBut where is the feedback coming from that defines, whether or not the action of that gate\nwas negative or positive? Feedback gates posses two more inputs, one for a positive, and one for a negative signal. These inputs can come from any of the nodes the Markov Brain has at it’s disposal, and are genetically encoded. Therefore, the feedback can be a sensor, or any output of another gate. Receiving a 0 from another gate to either of the two positive or negative feedback inputs has no effect, whereas reading a 1 triggers the feedback.\nIn the simplest case of feedback a random number in the range [0, δ] is applied to the prob-\nability Pio that was used in the last update of the gate. In case of positive feedback the value is increased, in the case of negative feedback the value is decreased. The probabilities are limited to\nnot exceed 0.99 or drop below 0.01. The rest of the probabilities are then normalized.\nThe effects of the feedback gate are immediately accessible to the MB. However, because\nMBs are networks, the signal that a feedback gate generates might need time to be relayed to the outputs via other gates. It is also possible that there is a delay between an agents actions and the time it takes to receive new sensorial inputs that give a clue about the situation being improved or not. Thus, allowing feedback to occur only on the last action is not sufficient. Therefore, feedback gates can evolve the depth of a buffer that stores prior Pio up to a depth of 4 and when feedback is applied all the probabilities identified by the elements in the cue are altered. The δ is determined by evolution and can be different for each element in the cue.\nLine of decent An agent is selected at random from the final generation to determine the line of decent (LOD) by tracing the ancestors to the first generation 57. During this process the most recent common ancestor (MRCA) is quickly found. Observe that all mutations that swept the population can be found on the LOD, and the LOD contains all evolutionary changes that mattered.\nQ-learning The Q-learning environment used is the same the evolved agents experiences. The agents are controlled by a Q-matrix, and all state action pairs are recorded (st, at). When needed all Q(st, at) are rewarded. The reward applied to the Q-matrix can vary according to different experiments, but the reward increases after ever update as defined by each experiment.\n1. Russell, S. J. & Norvig, P. Artificial intelligence: a modern approach (3rd edition) (Prentice\nHall, 2009).\n2. Hebb, D. O. The organization of behavior: A neuropsychological theory (Psychology Press,\n2005).\n3. Baldwin, J. M. A new factor in evolution. The american naturalist 30, 441–451 (1896).\n4. Brooks, R. A. Intelligence without representation. Artificial intelligence 47, 139–159 (1991).\n5. Marstaller, L., Hintze, A. & Adami, C. The Evolution of Representation in Simple Cognitive\nNetworks. Neural Computation 25, 2079–2107 (2013).\n6. Silver, D. et al. Mastering the game of go with deep neural networks and tree search. Nature\n529, 484–489 (2016).\n7. Greve, R. B., Jacobsen, E. J. & Risi, S. Evolving neural turing machines. In Neural Informa-\ntion Processing Systems: Reasoning, Attention, Memory Workshop (2015).\n8. Greve, R. B., Jacobsen, E. J. & Risi, S. Evolving neural turing machines for reward-based\nlearning. In Proceedings of the 2016 on Genetic and Evolutionary Computation Conference, 117–124 (ACM, 2016).\n9. Lüders, B., Schläger, M. & Risi, S. Continual learning through evolvable neural turing ma-\nchines (2016).\n10. Soltoggio, A., Bullinaria, J. A., Mattiussi, C., Dürr, P. & Floreano, D. Evolutionary advantages\nof neuromodulated plasticity in dynamic, reward-based scenarios. In Proceedings of the 11th International Conference on Artificial Life (Alife XI), LIS-CONF-2008-012, 569–576 (MIT Press, 2008).\n11. Tonelli, P. &Mouret, J.-B. On the relationships between synaptic plasticity and generative sys-\ntems. In Proceedings of the 13th annual conference on Genetic and evolutionary computation, 1531–1538 (ACM, 2011).\n12. Risi, S. & Stanley, K. O. A unified approach to evolving plasticity and neural geometry. In\nNeural Networks (IJCNN), The 2012 International Joint Conference on, 1–8 (IEEE, 2012).\n13. Coleman, O. J. & Blair, A. D. Evolving plastic neural networks for online learning: review\nand future directions. In Australasian Joint Conference on Artificial Intelligence, 326–337 (Springer, 2012).\n14. Stanley, K. O., Bryant, B. D. & Miikkulainen, R. Evolving adaptive neural networks with and\nwithout adaptive synapses. In Evolutionary Computation, 2003. CEC’03. The 2003 Congress on, vol. 4, 2557–2564 (IEEE, 2003).\n15. Edlund, J. A. et al. Integrated Information Increases with Fitness in the Evolution of Animats.\nPLoS Comput Biol 7, e1002236 (2011).\n16. Joshi, N. J., Tononi, G. & Koch, C. The minimal complexity of adapting agents increases with\nfitness. PLoS Comput Biol (2013).\n17. Chapman, S., Knoester, D. B., Hintze, A. & Adami, C. Evolution of an artificial visual cortex\nfor image recognition. ECAL 1067–1074 (2013).\n18. Olson, R. S., Hintze, A., Dyer, F. C., Knoester, D. B. & Adami, C. Predator confusion is suf-\nficient to evolve swarming behaviour. Journal of The Royal Society Interface 10, 20130305– 20130305 (2013).\n19. Albantakis, L., Hintze, A., Koch, C., Adami, C. & Tononi, G. Evolution of Integrated Causal\nStructures in Animats Exposed to Environments of Increasing Complexity. PLoS Comput Biol 10, e1003966–19 (2014).\n20. Hintze, A. et al. Evolution of Autonomous Hierarchy Formation and Maintenance. In Arti-\nficial Life 14: Proceedings of the Fourteenth International Conference on the Synthesis and Simulation of Living Systems, 366–367 (The MIT Press, 2014).\n21. Kvam, P., Cesario, J., Schossau, J., Eisthen, H. & Hintze, A. Computational evolution of\ndecision-making strategies. arXiv preprint arXiv:1509.05646 (2015).\n22. Schossau, J., Adami, C. & Hintze, A. Information-Theoretic Neuro-Correlates Boost Evolu-\ntion of Cognitive Systems. Entropy 18, 6–22 (2016).\n23. Russell, S. & Norvig, P. Ai a modern approach. Learning 2, 4 (2005).\n24. Clark, A. Being there: Putting brain, body, and world together again (MIT press, 1998).\n25. Ma, W. J., Husain, M. & Bays, P. M. Changing concepts of working memory. Nature Neuro-\nscience 17, 347–356 (2014).\n26. Nadel, L. & Hardt, O. Update onMemory Systems and Processes. Neuropsychopharmacology\n36, 251–273 (2010).\n27. Kandel, E. R., Dudai, Y. & Mayford, M. R. The Molecular and Systems Biology of Memory.\nCell 157, 163–186 (2014).\n28. Squire, L. R. & Wixted, J. T. The Cognitive Neuroscience of Human Memory Since H.M.\nAnnual Review of Neuroscience 34, 259–288 (2011).\n29. Abraham, W. C. & Robins, A. Memory retention – the synaptic stability versus plasticity\ndilemma. Trends in Neurosciences 28, 73–78 (2005).\n30. McKenzie, S. & Eichenbaum, H. Consolidation and Reconsolidation: Two Lives of Memo-\nries? Neuron 71, 224–233 (2011).\n31. Sznajder, B., Sabelis, M. & Egas, M. How adaptive learning affects evolution: reviewing\ntheory on the baldwin effect. Evolutionary biology 39, 301–310 (2012).\n32. Watkins, C. J. C. H. Learning from delayed rewards. Ph.D. thesis, University of Cambridge\nEngland (1989).\n33. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A. & Veness, J. Human-level control through\ndeep reinforcement learning. Nature 518, 529–533 (2015).\n34. Wang, Y. et al. Neural Control of a Tracking Task via Attention-gated Reinforcement Learn-\ning for Brain-Machine Interfaces. IEEE Transactions on Neural Systems and Rehabilitation Engineering 1–1 (2015).\n35. Schmidhuber, J. Deep learning in neural networks: An overview. Neural Networks 61, 85–117\n(2015).\n36. Zhang, H., Wu, W. & Yao, M. Boundedness and convergence of batch back-propagation\nalgorithmwith penalty for feedforward neural networks. Neurocomputing 89, 141–146 (2012).\n37. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444 (2015).\n38. Kaleh, G. K. & Vallet, R. Joint parameter estimation and symbol detection for linear or\nnonlinear unknown channels. IEEE Trans. Communications () 42, 2406–2413 (1994).\n39. Baggenstoss, P. M. A modified Baum-Welch algorithm for hidden Markov models with mul-\ntiple observation spaces. IEEE Transactions on Speech and Audio Processing 9, 411–416 (2001).\n40. Arora, S., Hazan, E. &Kale, S. TheMultiplicativeWeights UpdateMethod: aMeta-Algorithm\nand Applications. Theory of Computing (2012).\n41. Freund, Y. & Schapire, R. E. Adaptive game playing using multiplicative weights. Games and\nEconomic Behavior 29, 79–103 (1999).\n42. Hochreiter, S. & Schmidhuber, J. Long short-termmemory. Neural computation 9, 1735–1780\n(1997).\n43. Watkins, C. J. C. H. Learning from delayed rewards. Ph.D. thesis, University of Cambridge\nEngland (1989).\n44. Watkins, C. J. & Dayan, P. Q-learning. Machine learning 8, 279–292 (1992).\n45. Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y. et al. Policy gradient methods for\nreinforcement learning with function approximation. In NIPS, vol. 99, 1057–1063 (1999).\n46. Hallak, A., Di Castro, D. & Mannor, S. Contextual markov decision processes. arXiv preprint\narXiv:1502.02259 (2015).\n47. Lin, L.-J. Reinforcement learning for robots using neural networks. Ph.D. thesis, Fujitsu\nLaboratories Ltd (1993).\n48. Lin, L.-J. & Mitchell, T. M. Reinforcement learning with hidden states. From animals to\nanimats 2, 271–280 (1993).\n49. Mongillo, G., Shteingart, H. & Loewenstein, Y. The misbehavior of reinforcement learning.\nProceedings of the IEEE 102, 528–541 (2014).\n50. Kaelbling, L. P., Littman, M. L. & Moore, A. W. Reinforcement learning: A survey. Journal\nof artificial intelligence research 4, 237–285 (1996).\n51. McCallum, R. A. Hidden state and reinforcement learning with instance-based state identifica-\ntion. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 26, 464–473 (1996).\n52. Lee, H.-Y., Kamaya, H. & Abe, K.-i. Labeling q-learning in hidden state environments. Arti-\nficial Life and Robotics 6, 181–184 (2002).\n53. Sondik, E. J. The optimal control of partially observable markov processes. Tech. Rep., DTIC\nDocument (1971).\n54. Smallwood, R. D. & Sondik, E. J. The optimal control of partially observable markov pro-\ncesses over a finite horizon. Operations research 21, 1071–1088 (1973).\n55. Hintze, A. & Bohm, C. Mabe. https://github.com/ahnt/MABE (2016).\n56. Blickle, T. & Thiele, L. A comparison of selection schemes used in evolutionary algorithms.\nEvolutionary Computation 4, 361–394 (1996).\n57. Lenski, R. E., Ofria, C., Pennock, R. T. & Adami, C. The evolutionary origin of complex\nfeatures. Nature 423, 139–144 (2003).\nAcknowledgements Put acknowledgements here.\nCompeting Interests The authors declare that they have no competing financial interests.\nCorrespondence Correspondence and requests for materials should be addressed to Arend Hintze (email: hintze@msu.edu)."
    } ],
    "references" : [ {
      "title" : "Artificial intelligence: a modern approach (3rd edition",
      "author" : [ "S.J. Russell", "P. Norvig" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "The organization of behavior: A neuropsychological theory (Psychology Press",
      "author" : [ "D.O. Hebb" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Intelligence without representation",
      "author" : [ "R.A. Brooks" ],
      "venue" : "Artificial intelligence 47,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1991
    }, {
      "title" : "The Evolution of Representation in Simple Cognitive Networks",
      "author" : [ "L. Marstaller", "A. Hintze", "C. Adami" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search",
      "author" : [ "D Silver" ],
      "venue" : "Nature 529,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Evolving neural turing machines",
      "author" : [ "R.B. Greve", "E.J. Jacobsen", "S. Risi" ],
      "venue" : "In Neural Information Processing Systems: Reasoning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Evolving neural turing machines for reward-based learning",
      "author" : [ "R.B. Greve", "E.J. Jacobsen", "S. Risi" ],
      "venue" : "In Proceedings of the 2016 on Genetic and Evolutionary Computation Conference,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Continual learning through evolvable neural turing machines",
      "author" : [ "B. Lüders", "M. Schläger", "S. Risi" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Evolutionary advantages of neuromodulated plasticity in dynamic, reward-based scenarios",
      "author" : [ "A. Soltoggio", "J.A. Bullinaria", "C. Mattiussi", "P. Dürr", "D. Floreano" ],
      "venue" : "In Proceedings of the 11th International Conference on Artificial Life (Alife XI),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "On the relationships between synaptic plasticity and generative systems",
      "author" : [ "P. Tonelli", "Mouret", "J.-B" ],
      "venue" : "In Proceedings of the 13th annual conference on Genetic and evolutionary computation,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "A unified approach to evolving plasticity and neural geometry",
      "author" : [ "S. Risi", "K.O. Stanley" ],
      "venue" : "In Neural Networks (IJCNN), The 2012 International Joint Conference on,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Evolving plastic neural networks for online learning: review and future directions",
      "author" : [ "O.J. Coleman", "A.D. Blair" ],
      "venue" : "In Australasian Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Evolving adaptive neural networks with and without adaptive synapses",
      "author" : [ "K.O. Stanley", "B.D. Bryant", "R. Miikkulainen" ],
      "venue" : "In Evolutionary Computation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2003
    }, {
      "title" : "Integrated Information Increases with Fitness in the Evolution of Animats",
      "author" : [ "Edlund", "J. A" ],
      "venue" : "PLoS Comput Biol 7,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "The minimal complexity of adapting agents increases with fitness",
      "author" : [ "N.J. Joshi", "G. Tononi", "C. Koch" ],
      "venue" : "PLoS Comput Biol",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Evolution of an artificial visual cortex for image recognition",
      "author" : [ "S. Chapman", "D.B. Knoester", "A. Hintze", "C. Adami" ],
      "venue" : "ECAL 1067–1074",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Predator confusion is sufficient to evolve swarming behaviour",
      "author" : [ "R.S. Olson", "A. Hintze", "F.C. Dyer", "D.B. Knoester", "C. Adami" ],
      "venue" : "Journal of The Royal Society Interface",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Evolution of Integrated Causal Structures in Animats Exposed to Environments of Increasing Complexity",
      "author" : [ "L. Albantakis", "A. Hintze", "C. Koch", "C. Adami", "G. Tononi" ],
      "venue" : "PLoS Comput Biol 10,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Evolution of Autonomous Hierarchy Formation and Maintenance",
      "author" : [ "A Hintze" ],
      "venue" : "In Artificial Life 14: Proceedings of the Fourteenth International Conference on the Synthesis and Simulation of Living Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Computational evolution of decision-making strategies",
      "author" : [ "P. Kvam", "J. Cesario", "J. Schossau", "H. Eisthen", "A. Hintze" ],
      "venue" : "arXiv preprint arXiv:1509.05646",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Information-Theoretic Neuro-Correlates Boost Evolution of Cognitive Systems",
      "author" : [ "J. Schossau", "C. Adami", "A. Hintze" ],
      "venue" : "Entropy 18,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Being there: Putting brain, body, and world together again",
      "author" : [ "A. Clark" ],
      "venue" : "(MIT press,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1998
    }, {
      "title" : "Changing concepts of working memory",
      "author" : [ "W.J. Ma", "M. Husain", "P.M. Bays" ],
      "venue" : "Nature Neuroscience 17,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Update onMemory Systems and Processes",
      "author" : [ "L. Nadel", "O. Hardt" ],
      "venue" : "Neuropsychopharmacology 36,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "The Cognitive Neuroscience of Human Memory",
      "author" : [ "L.R. Squire", "J.T. Wixted" ],
      "venue" : "Since H.M. Annual Review of Neuroscience",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Memory retention – the synaptic stability versus plasticity dilemma",
      "author" : [ "W.C. Abraham", "A. Robins" ],
      "venue" : "Trends in Neurosciences",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2005
    }, {
      "title" : "Consolidation and Reconsolidation: Two Lives of Memories",
      "author" : [ "S. McKenzie", "H. Eichenbaum" ],
      "venue" : "Neuron 71,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2011
    }, {
      "title" : "How adaptive learning affects evolution: reviewing theory on the baldwin effect",
      "author" : [ "B. Sznajder", "M. Sabelis", "M. Egas" ],
      "venue" : "Evolutionary biology 39,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2012
    }, {
      "title" : "Learning from delayed rewards",
      "author" : [ "Watkins", "C.J.C. H" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1989
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness" ],
      "venue" : "Nature 518,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Neural Control of a Tracking Task via Attention-gated Reinforcement Learning for Brain-Machine Interfaces",
      "author" : [ "Y Wang" ],
      "venue" : "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2015
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Networks",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2015
    }, {
      "title" : "Boundedness and convergence of batch back-propagation algorithmwith penalty for feedforward neural networks",
      "author" : [ "H. Zhang", "W. Wu", "M. Yao" ],
      "venue" : "Neurocomputing 89,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2012
    }, {
      "title" : "Joint parameter estimation and symbol detection for linear or nonlinear unknown channels",
      "author" : [ "G.K. Kaleh", "R. Vallet" ],
      "venue" : "IEEE Trans. Communications",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1994
    }, {
      "title" : "A modified Baum-Welch algorithm for hidden Markov models with multiple observation spaces",
      "author" : [ "P.M. Baggenstoss" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2001
    }, {
      "title" : "TheMultiplicativeWeights UpdateMethod: aMeta-Algorithm and Applications",
      "author" : [ "S. Arora", "E. Hazan", "S. Kale" ],
      "venue" : "Theory of Computing",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2012
    }, {
      "title" : "Adaptive game playing using multiplicative weights",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "Games and Economic Behavior",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1999
    }, {
      "title" : "Learning from delayed rewards",
      "author" : [ "Watkins", "C.J.C. H" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1989
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y Mansour" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1999
    }, {
      "title" : "Contextual markov decision processes",
      "author" : [ "A. Hallak", "D. Di Castro", "S. Mannor" ],
      "venue" : "arXiv preprint arXiv:1502.02259",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning for robots using neural networks",
      "author" : [ "Lin", "L.-J" ],
      "venue" : "Ph.D. thesis, Fujitsu Laboratories Ltd",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 1993
    }, {
      "title" : "Reinforcement learning with hidden states. From animals to animats",
      "author" : [ "Lin", "L.-J", "T.M. Mitchell" ],
      "venue" : null,
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1993
    }, {
      "title" : "The misbehavior of reinforcement learning",
      "author" : [ "G. Mongillo", "H. Shteingart", "Y. Loewenstein" ],
      "venue" : "Proceedings of the IEEE 102,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2014
    }, {
      "title" : "Reinforcement learning: A survey",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "A.W. Moore" ],
      "venue" : "Journal of artificial intelligence research",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1996
    }, {
      "title" : "Hidden state and reinforcement learning with instance-based state identification",
      "author" : [ "R.A. McCallum" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1996
    }, {
      "title" : "Labeling q-learning in hidden state environments",
      "author" : [ "Lee", "H.-Y", "H. Kamaya", "Abe", "K.-i" ],
      "venue" : "Artificial Life and Robotics",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2002
    }, {
      "title" : "The optimal control of partially observable markov processes",
      "author" : [ "E.J. Sondik" ],
      "venue" : "Tech. Rep., DTIC Document",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 1971
    }, {
      "title" : "The optimal control of partially observable markov processes over a finite horizon",
      "author" : [ "R.D. Smallwood", "E.J. Sondik" ],
      "venue" : "Operations research 21,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 1973
    }, {
      "title" : "A comparison of selection schemes used in evolutionary algorithms",
      "author" : [ "T. Blickle", "L. Thiele" ],
      "venue" : "Evolutionary Computation",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 1996
    }, {
      "title" : "The evolutionary origin of complex features",
      "author" : [ "R.E. Lenski", "C. Ofria", "R.T. Pennock", "C. Adami" ],
      "venue" : "Nature 423,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2003
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "There are two common approaches for optimizing the performance of a machine: genetic algorithms and machine learning. A genetic algorithm is applied over many generations whereas machine learning works by applying feedback until the system meets a performance threshold. Though these are methods that typically operate separately, we combine evolutionary adaptation and machine learning into one approach. Our focus is on machines that can learn during their lifetime, but instead of equipping them with a machine learning algorithm we aim to let them evolve their ability to learn by themselves. We use evolvable networks of probabilistic and deterministic logic gates, known as Markov Brains, as our computational model organism. The ability of Markov Brains to learn is augmented by a novel adaptive component that can change its computational behavior based on feedback. We show that Markov Brains can indeed evolve to incorporate these feedback gates to improve their adaptability to variable environments. By combining these two methods, we now also implemented a computational model that can be used to study the evolution of learning.",
    "creator" : "LaTeX with hyperref package"
  }
}