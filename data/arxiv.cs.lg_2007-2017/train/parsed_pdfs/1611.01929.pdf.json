{
  "name" : "1611.01929.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Reinforcement Learning with Averaged Target DQN",
    "authors" : [ "Oron Anschel", "Nir Baram", "Nahum Shimkin" ],
    "emails" : [ "{oronanschel@campus,", "nirb@campus,", "shimkin@ee}.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In Reinforcement Learning (RL) an agent seeks to find an optimal policy for a sequential decision problem (Sutton and Barto, 1998). One of the most popular algorithm for RL problems is the Q-learning algorithm (Watkins and Dayan, 1992). Here the agent learns sequential estimates of the optimal state-action value function:\nQ∗(s, a) , max π Eπ[ ∞∑ t=0 γtRt|s0 = s, a0 = a]\nFor small problems the Q-learning algorithm can be implemented using a table containing current value estimates of all possible state-action pairs, and will benefit from convergence guarantees under mild assumptions. In practice, most of the real-world problems are far too complex for a tabular implementation and require the use of function approximation.\nOne of the most impressive early works, bringing together RL and neural networks presented a worldclass backgammon agent (Tesauro, 1992). Nevertheless, combined directly, divergence was shown even on simple problems (Boyan and Moore, 1995), and until recent breakthroughs the progress in the field was slow.\nThe DQN algorithm (Mnih et al. (2013)) combines Q-learning with the recent progress in neural network techniques. DQN was the first successful attempt to learn control policies directly from a high-dimensional sensory input, and led the way for many subsequent developments in the field. Two significant modifications were introduced in the DQN work that help to stabilize the learning process: First, the network is trained using samples from an experience-buffer. Second, the target values are calculated using a target network whose parameters are frozen for a fixed number of iterations at a time.\nAlthough DQN proved to be successful in most of the benchmark problems (Mnih et al., 2013), in some of them it seems to fail. One of the reasons for the failure of DQN is argued to be insufficient exploration strategy, such as in the Atari game MONTEZUMA’S REVENGE. Several works suggested\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n01 92\n9v 1\n[ cs\n.A I]\n7 N\nov 2\n01 6\nexploration techniques for high-dimensional state space to remedy this problem (Bellemare et al., 2016; Osband et al., 2016).\nAnother possible reason for the DQN to fail in Atari games such as WIZARD OF WOR and ASTERIX is systematic errors caused by the use of Q-learning combined with function approximation. Essentially, high variance in target values combined with the max operator might lead to a positive bias, which at the extreme might leads to divergence. In this work, we propose the ADQN algorithm as a solution to this problem. Additionally, we analyze and show how a straightforward approach which we denote as Ensemble DQN is inferior in variance reduction.\nThe rest of the paper is constructed as follows. In Section 2 we discuss relevant related work. Section 3 elaborates on the Q-learning and DQN algorithms. In section 4 we present the ADQN algorithm. In Section 5 we discuss and analyze errors bias and variance induced by the combination of function approximation with Q-learning. Section 6 provides an empirical evaluation of the ADQN algorithm both in a toy problem and in several of the Atari 2600 games from the Arcade Learning Environment (Bellemare et al., 2013). Section 7 closes the paper with our conclusions and possible future direction."
    }, {
      "heading" : "2 Related Work",
      "text" : "Thrun and Schwartz (1993) is an early theoretical work analyzing the effects of combining function approximation and RL techniques. This work modeled the generalization noise induced by the use of approximator as zero-mean independent random variables and showed that systematic overestimation errors could occur due to the utilization of the max operator in the standard Q-learning update rule.\nDouble Q-learning (Hasselt, 2010) was suggested as a solution for this problem. The max operator was divided into selection and evaluation parts, which replaced the positive bias of the original predictor with a negative one. Later the Double-DQN (DDQN) algorithm that was introduced in Van Hasselt et al. (2015) provided a DQN implementation of this algorithm.\nSince the publication of the DQN work, many variants and modification of the standard algorithm have been developed. Schaul et al. (2015) showed improved results by employing smart policies for sampling from the experience buffer; Wang et al. (2015) introduced a modified Q-network architecture; Mnih et al. (2016) implemented asynchronous version of the DQN which does not require the use of an experience-buffer and showed impressive performance.\nAnother recent work, the Bootstrapped DQN algorithm (Osband et al., 2016) implemented Deep Exploration with the use of several Q-networks learning in parallel. Bootstrapped DQN is similar to our work by the utilization of an ensemble of several Q-networks. However, the primary purpose of Bootstrapped DQN is exploration whereas our objective is variance reduction. We note two additional differences between the Bootstrapped DQN and this work: (1) In Bootstrapped DQN the different Q-networks are used for gathering shared experience during the learning process, while in our algorithm they are combined continuously in order to construct target values; (2) In Bootstrapped DQN the networks learn in parallel while in our work there is a single learning network.\nWe note that the variants and modification mentioned above are essentially orthogonal to our work, so that combining them with our proposed scheme is likely to yield further performance improvements."
    }, {
      "heading" : "3 Deep Q Networks (DQN)",
      "text" : "In classical Q-learning (Watkins and Dayan (1992)) the update rule is given by\nQt+1(s, a)← (1− α)Qt(s, a) + α(r(s, a) + γmax a′ Qt(s ′, a′)) (1)\nwhere s′ is the resulting state after applying action a in the current state s, γ is the MDP’s discount factor and α is a time varying learning rate. Under mild assumption for finite state and action spaces, it can be shown that the Q-learning algorithm converges to the optimal action-value function, as long as every state action pair is updated infinitely often. The update of the Q-function estimate can be done in an online or offline manner.\nThe above Q-learning update rule can be directly implemented in a neural network. Since no direct assignment of Q-values can be performed as if we had a table of state action pairs, a loss function\nmust be introduced (Riedmiller, 2005). In case the squared error is taken as a loss function (Eq. 3), gradient descent takes the form of Eq. 4:\nyQt = Es′∼B [ r(s, a) + γmax\na′ Q(s′, a′; θ−)|s, a\n] (2)\nL(θt) = E(s,a)∼B [ (yQt −Q(s, a; θt))2 ] (3)\nθt+1 ← θt + ηE(s,a)∼B [ (yQt −Q(s, a; θt))∇θQ(s, a; θt) ] (4)\nHere η is a scalar learning rate, yQt is the target value with θ − = θt, and B is an experience buffer which normally contains one tuple of experience B = {st, at, st+1, rt}. The DQN algorithm (Mnih et al., 2013) introduced two important modifications for the algorithm above. First, the algorithm samples from a large experience buffer B which contains many experience tuples. Second, the algorithm uses a target network with parameters θ− that are copied from the learning network θ− ← θt only every τ iterations. These last modifications enable the Q-learning algorithm to operate in a semi off-policy manner, breaking the task of learning the optimal state-action values into smaller supervised learning problems."
    }, {
      "heading" : "4 Averaged Target DQN (ADQN)",
      "text" : "We introduce here the ADQN algorithm. ADQN maintains a target networks buffer which contains the parameters of K past learned networks. Also, an additional learning rate parameter α ∈ [0, 1) is required. Essentially the DQN algorithm can be viewed as a special case of the ADQN algorithm with a target networks buffer of size one. The only algorithmic difference from the DQN algorithm is the different choice of target value as in Eq. 5. The loss function (Eq. 6) remains the same as in DQN and so does the parameter update equation (Eq. 7):\nyAt = Es′∼B\n[ r(s, a) + γmax\na′ K−1∑ k=0 wkQ(s ′, a′; θτ(bt/τc−k))|s, a\n] (5)\nL(θt) = E(s,a)∼B [ (yAt −Q(s, a; θt))2 ] (6)\nθt+1 ← θt + ηE(s,a)∼B [ (yAt −Q(s, a; θt))∇θQ(s, a; θt) ] (7)\nwhere wk = (1− α)k/Cw and Cw = ∑K−1 k=0 wk is a normalization constant. Every τ iterations, the parameters of the learning networks are copied to the target networks buffer and the oldest parameters are removed. In Eq. 5 the summation is performed over the K networks in the target buffer. The learned Q-function at time t is the averaged network:\nQA(s, a; θ̄t) = K−1∑ k=0 wkQ(s, a; θτ(bt/τc−k)) (8)\nThe learning rate parameter α trades between variance reduction and valuing newer Q estimates. In general, α can be a function of time and even the state space."
    }, {
      "heading" : "5 Estimation Errors in RL",
      "text" : "Combining function approximation and RL methods introduces difficulties that do not exist for each separately. Thrun and Schwartz (1993) provided a first theoretical analysis for combing Q-learning and function approximations. In this section, we will discuss their results and suggest further analysis of variance propagation along trajectories."
    }, {
      "heading" : "5.1 Overestimation",
      "text" : "Thrun and Schwartz (1993) considered an additive zero-mean random variable, a generalization error model for the Q-function. That is, Q(s′, a′; θ−) = Qtarget(s′, a′) + Zs′,a′ in Eq. 2. Due to the max operator in the equation, up to γ n−1n+1 overestimation in each target action-value y Q t (s, a) can be seen on average (where Zs,a is uniformly distributed in an interval [− , ], and n is the number of applicable actions in state s). Systematic errors occur due to the nature of the Q-learning update equations, where along trajectories of the current greedy policy, each state can be overestimated up to γ n−1n+1 plus the bias of the next state in the trajectory. Hasselt (2010) argued that even in a tabular setting environment noise can cause overestimations and suggested the Double Q-learning algorithm as a solution.\nWe observe two adverse effects of large variance in the estimated Q-function: First, evidently, the learned Q-function is likely to deviate from the required one; Second, large variance in Q causes positive bias in the target values yQt as well as variance which might lead to divergence."
    }, {
      "heading" : "5.2 Variance Reduction due to Averaging",
      "text" : "In the following illustrative example, we analyze the noise variance propagation along trajectories, from the use of ADQN, DQN and an Ensemble technique that will be introduced later in this section.\nLet 0, 1, . . . , t denote the indices of time instants in which parameters are copied from the learned network to the target buffer (DQN maintains a buffer of size one). We model the noise in the learned Q-function, in Eq. 2 or 5 as Q(s′, a′; θt) = yt−1(s′, a′) + Zs′,a′ , where yt−1 is the previous time step target value. The noise Zs,a is modeled as a zero-mean random variable with variance σ2z , independent of the state. Also, we assume that τ , the number of iterations between time t and t+ 1 is large enough so that Zts,a and Z t+1 s,a are not correlated. The Q-networks themselves are not stochastic, and after the network parameters are copied to the target buffer the noise is also fixed. We are interested in calculating the variance of the learned policy which is defined as Q̄(s, a) , Q(s, a; θt) for DQN, and Q̄(s, a) , QA(s, a; θ̄t) for ADQN (Eq. 8).\nConsider a two states, single action MDP such as in Figure 1. The process starts at state s2, then moves to a terminal state s1. A zero reward is obtained in both states.\nObviously, for all pairs (s, a), the optimal action-value function Q∗(s, a) = 0, and therefore Ez [ (Q̄(s, a)−Q∗(s, a))2 ] = Ez [ (Q̄(s, a))2 ] . The analysis here reflects variance analysis for ADQN or DQN, assuming that the estimated Q-values are different enough along a current greedy policy trajectory so that the max operator in Eq. 2 can be ignored. The results are summarized in Table 1 (calculations are detailed in Appendix A).\nADQN takes an average over past learned Q-functions. One can consider a more direct approach to reducing variance. To reduce noise variance in the learned Q-networks we can learn several\nindependent estimates from the same target, and then average out the noise. This leads to an architecture of K independent Q-learners with distinct simulators and experience buffers, sharing only the learned policy while constructing target values. Denote the k-th network parameters at time t as θkt . The learned Q-function in this architecture is the average over the K target networks estimates:\nQE(s2, a; θ̄t) , 1\nK K−1∑ k=0 Q(s′, a′; θkt )\nand the target values for the k-th leaner at time t are computed as:\nyE,kt (s, a) , Es′∼Bk\n[ r(s, a) + γmax\na′\n1\nK K−1∑ k=0 Q(s′, a′; θkt−1)|s, a ] Surprisingly, the above described architecture which we refer to as Ensemble DQN is not only computationally demanding (learning K networks in parallel instead of one), but also leads to inferior variance reduction as summarized in Table 1 (calculations are detailed in Appendix B).\nThe above calculations can be generalized to an L state, single action MDP as in Figure 2 (see Appendix B). ADQN’s performance in variance reduction improves as the trajectory gets longer.\nThis is due to the time averaging which leads to average over averages and can be observed in the calculations. We denote by uK a discrete rectangle function (uKi = 1{i ∈ (0, 1, . . . ,K − 1)}), and by UK,DFT its Discrete Fourier Transform (DFT). Also, we define DK,L , 1N ∑N−1 n=0 |UK,DFTn |2L (where N is the length of UK,DFT ). Results, and the behaviour of the ADQN variance reduction coefficient DK,L/K2L, are summarized in Table 2 and Figure 3."
    }, {
      "heading" : "6 Experiments",
      "text" : "We use two different environments to evaluate the performance of the ADQN algorithm and some assumptions made in previous sections. First, we will show experiments on a toy problem of Gridworld where the optimal value function can be computed exactly with a tabular state space representation. Second, we will demonstrate and compare performance using the Arcade Learning Environment (Bellemare et al. (2013)), which is the most commonly used benchmark environment for RL algorithms."
    }, {
      "heading" : "6.1 Gridworld",
      "text" : "In this experiment on the problem of 2D Gridworld, the state space contains pairs of points from a 2D discrete grid (S = {(x, y)}x,y∈0,...,N ). The algorithm interacts with the MDP through raw pixel features with a 2D one-hot feature map φ(st) := (1{(x, y) = st})x,y∈0,...,N . There are four actions corresponding steps in each compass direction, a reward of r = 1 if st = (N,N) (else the reward is zero) and a discount factor of γ = 0.9.\nFigure 4 shows the mean value estimation over all states for the ADQN algorithm with different size of target networks buffer. Also, we have plotted a baseline of the true optimal mean value V ∗. On the left plot, we can see a negative effect of the slowdown in convergence in ADQN, and this is since newer estimation of the state-action values are more likely to be better than older ones. On the right, we have zoomed in to see how systematic overestimations cause positive bias, and how adding target networks reduces both the variance between the models and the overall bias. For the experiments, we have used the ADAM optimizer (Kingma and Ba (2014)), updated the target network parameters each τ = 300 iterations, and α = 0.01 was set as the ADQN learning rate.\nIn Figure 5 we have used the same setup and hyper-parameters as mentioned above. We have compared ADQN with K = 20 target networks to the Ensemble DQN method mentioned in Section 5.2. The Ensemble DQN learns in parallel 20 Q-networks for τ = 300 iterations and then constructs the next target and test Q-network by averaging over the 20 networks that were learned. On the left plot, we can see that Ensemble DQN converges as fast as DQN, and ADQN suffers from a slowdown in convergence due to the use of a fixed small learning rate α. On the right plot we see that although Ensemble DQN is much more demanding computationally, ADQN converged to a lower bias faster and with less variance between successive models."
    }, {
      "heading" : "6.2 Arcade Learning Environment",
      "text" : "We have evaluated the ADQN algorithm on several Atari games on the Arcade Learning Environment (Bellemare et al. (2013)). The Arcade Learning Environment is the most commonly used benchmark to evaluate DQN variants and adaptations, and this is due to the diverse set of games that need to be solved with a single set of architecture and hyper-parameters, directly from raw pixels input and\na reward signal. We followed the setup of hyper-parameters and network architecture as in (Mnih et al. (2013)). Additionally, we have used moderate computation and memory architecture for ADQN with K = 10 target networks buffer and α = 0.01. This configuration of the ADQN presents a 35% slowdown from the standard DQN, and the slowdown is due to the need of K times more forward computation (compared to DQN) to construct the next target values.\nFigure 6 presents a visual comparison. ADQN shows improvement in the learned policies over DQN, the learning progress is more stable, and the value estimation is considerably more conservative. Also, the spikes phenomena (e.g., Wizard of Wor value estimate plot) that occurs in value estimates of DQN (and in Double-DQN) does not take place in the ADQN algorithm (which could be an indication of remedy to the systematic overestimation that occurs in the DQN algorithm).\n0 20 40 60 80 100 120 140 frames [millions]\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\nAv er\nag ed\ns co\nre p\ner e\npi so\nde\nBreakout\nDQN Alpha-DQN\n0 20 40 60 80 100 120 140 frames [millions]\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nAv er\nag ed\ns co\nre p\ner e\npi so\nde\nWizard of Wor\nDQN Alpha-DQN\n0 20 40 60 80 100 120 140 frames [millions]\n0\n2000\n4000\n6000\n8000\n10000\n12000\nAv er\nag ed\ns co\nre p\ner e\npi so\nde\nSeaquest\nDQN Alpha-DQN\n0 20 40 60 80 100 120 140 frames [millions]\n10-1\n100\n101\nVa lu\ne Es\ntim at\ne\nBreakout\nAlpha-DQN DQN\n0 20 40 60 80 100 120 140 frames [millions]\n10-1\n100\n101\n102\n103\n104\n105\n106\nVa lu\ne Es\ntim at\ne\nWizard of Wor\nAlpha-DQN DQN\n0 20 40 60 80 100 120 140 frames [millions]\n10-1\n100\n101\n102\nVa lu\ne Es\ntim at\ne\nSeaquest\nAlpha-DQN DQN\nFigure 6: The top row shows the average reward per episode achieved by the algorithm during training evaluated with -greedy policy with = 0.05 for 125,000 steps. The bottom row shows the average value estimate on a held-out set of states taken from the experience buffer. The stability of the ADQN, as well as the improved result and conservative value estimates can be seen."
    }, {
      "heading" : "7 Conclusions and Future Directions",
      "text" : "In this work, we have presented the ADQN algorithm, an efficient variance reduction technique for the DQN family of algorithms. We have analyzed and empirically showed why a time-average approach is more beneficial for variance reduction, compared to a direct averaging approach such as the Ensemble DQN mentioned above. Using the Arcade Learning Environment we have shown how the learning process is stabilized along with improvement in performance, and the spikes phenomena that occur in both DQN and DDQN for some games, does not take place anymore.\nIn future work, it may be interesting to see how α, the ADQN learning rate parameter can be learned as a function of the states or features, and to what extent combining the proposed averaging scheme with other techniques will prove beneficial. Furthermore, as our variance analysis predicts, the ADQN technique is likely to be beneficial in value estimation schemes, and further investigation of the subject is required. Finally, the average is just one of the ensemble techniques that could be considered. Other include a majority vote, median, etc., over the learned Q-functions, and in the target values computations."
    }, {
      "heading" : "B Trajectory Calculations for Section 5.2",
      "text" : "We use the following notation for the Discrete Fourier Transform (DFT) in our calculations: For a vector x, XDFTk = ∑K−1 j=0 xjW jk K−1 where WK−1 = e −2πi/(K−1) and i = √ −1. We denote the discrete rectangle function as uK which is uKn = 1{n ∈ (0, . . . ,K − 1)} and its DFT as UDFT,K . Since Zt are assumed to be zero-mean and independent random variables, it is easy\nto check that E [(∑K−1\nkL=0,...,k2=0,k1=0 Zk1−k2...−kL )2] = σ2z ∑ |u1 ∗ u2 . . . ∗ uL|2 where ∗ is a\ncircular convolutional and ul are all discrete rectangle functions with support of K. Using Parseval’s theorem we get that ∑ |u1 ∗ u2 . . . ∗ uL|2 = ∑N−1 n=0 |UDFT,Kn |2L/N where N is the actual length of the vector u that might need to be zero padded (Nyquist condition)."
    } ],
    "references" : [ {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research, 47:253–279.",
      "citeRegEx" : "Bellemare et al\\.,? 2013",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Unifying count-based exploration and intrinsic motivation",
      "author" : [ "M.G. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos" ],
      "venue" : "arXiv preprint arXiv:1606.01868.",
      "citeRegEx" : "Bellemare et al\\.,? 2016",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2016
    }, {
      "title" : "Generalization in reinforcement learning: Safely approximating the value function",
      "author" : [ "J. Boyan", "A.W. Moore" ],
      "venue" : "Advances in neural information processing systems, pages 369–376.",
      "citeRegEx" : "Boyan and Moore,? 1995",
      "shortCiteRegEx" : "Boyan and Moore",
      "year" : 1995
    }, {
      "title" : "Double Q-learning",
      "author" : [ "H.V. Hasselt" ],
      "venue" : "Lafferty, J. D., Williams, C. K. I., Shawe-Taylor, J., Zemel, R. S., and Culotta, A., editors, Advances in Neural Information Processing Systems 23, pages 2613–2621. Curran Associates, Inc.",
      "citeRegEx" : "Hasselt,? 2010",
      "shortCiteRegEx" : "Hasselt",
      "year" : 2010
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv: 1412.6980.",
      "citeRegEx" : "Kingma and Ba,? 2014",
      "shortCiteRegEx" : "Kingma and Ba",
      "year" : 2014
    }, {
      "title" : "Asynchronous methods for deep reinforcement learning",
      "author" : [ "V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1602.01783.",
      "citeRegEx" : "Mnih et al\\.,? 2016",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1312.5602.",
      "citeRegEx" : "Mnih et al\\.,? 2013",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep exploration via bootstrapped dqn",
      "author" : [ "I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy" ],
      "venue" : "arXiv preprint arXiv:1602.04621.",
      "citeRegEx" : "Osband et al\\.,? 2016",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method",
      "author" : [ "M. Riedmiller" ],
      "venue" : "European Conference on Machine Learning, pages 317–328. Springer.",
      "citeRegEx" : "Riedmiller,? 2005",
      "shortCiteRegEx" : "Riedmiller",
      "year" : 2005
    }, {
      "title" : "Prioritized experience replay",
      "author" : [ "T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver" ],
      "venue" : "arXiv preprint arXiv:1511.05952.",
      "citeRegEx" : "Schaul et al\\.,? 2015",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement Learning: An Introduction, volume 1",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT Press Cambridge.",
      "citeRegEx" : "Sutton and Barto,? 1998",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Practical issues in temporal difference learning",
      "author" : [ "G. Tesauro" ],
      "venue" : "Reinforcement Learning, pages 33–53. Springer.",
      "citeRegEx" : "Tesauro,? 1992",
      "shortCiteRegEx" : "Tesauro",
      "year" : 1992
    }, {
      "title" : "Issues in using function approximation for reinforcement learning",
      "author" : [ "S. Thrun", "A. Schwartz" ],
      "venue" : "Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum.",
      "citeRegEx" : "Thrun and Schwartz,? 1993",
      "shortCiteRegEx" : "Thrun and Schwartz",
      "year" : 1993
    }, {
      "title" : "Deep reinforcement learning with double Q-learning",
      "author" : [ "H. Van Hasselt", "A. Guez", "D. Silver" ],
      "venue" : "arXiv preprint arXiv: 1509.06461.",
      "citeRegEx" : "Hasselt et al\\.,? 2015",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2015
    }, {
      "title" : "Dueling network architectures for deep reinforcement learning",
      "author" : [ "Z. Wang", "N. de Freitas", "M. Lanctot" ],
      "venue" : "arXiv preprint arXiv:",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Q-learning",
      "author" : [ "C.J. Watkins", "P. Dayan" ],
      "venue" : "Machine Learning, 8(3-4):279–292.",
      "citeRegEx" : "Watkins and Dayan,? 1992",
      "shortCiteRegEx" : "Watkins and Dayan",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "1 Introduction In Reinforcement Learning (RL) an agent seeks to find an optimal policy for a sequential decision problem (Sutton and Barto, 1998).",
      "startOffset" : 121,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : "One of the most popular algorithm for RL problems is the Q-learning algorithm (Watkins and Dayan, 1992).",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "One of the most impressive early works, bringing together RL and neural networks presented a worldclass backgammon agent (Tesauro, 1992).",
      "startOffset" : 121,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "Nevertheless, combined directly, divergence was shown even on simple problems (Boyan and Moore, 1995), and until recent breakthroughs the progress in the field was slow.",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "Although DQN proved to be successful in most of the benchmark problems (Mnih et al., 2013), in some of them it seems to fail.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "Nevertheless, combined directly, divergence was shown even on simple problems (Boyan and Moore, 1995), and until recent breakthroughs the progress in the field was slow. The DQN algorithm (Mnih et al. (2013)) combines Q-learning with the recent progress in neural network techniques.",
      "startOffset" : 79,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : "exploration techniques for high-dimensional state space to remedy this problem (Bellemare et al., 2016; Osband et al., 2016).",
      "startOffset" : 79,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "exploration techniques for high-dimensional state space to remedy this problem (Bellemare et al., 2016; Osband et al., 2016).",
      "startOffset" : 79,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "Section 6 provides an empirical evaluation of the ADQN algorithm both in a toy problem and in several of the Atari 2600 games from the Arcade Learning Environment (Bellemare et al., 2013).",
      "startOffset" : 163,
      "endOffset" : 187
    }, {
      "referenceID" : 3,
      "context" : "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem.",
      "startOffset" : 18,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Another recent work, the Bootstrapped DQN algorithm (Osband et al., 2016) implemented Deep Exploration with the use of several Q-networks learning in parallel.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem. The max operator was divided into selection and evaluation parts, which replaced the positive bias of the original predictor with a negative one. Later the Double-DQN (DDQN) algorithm that was introduced in Van Hasselt et al. (2015) provided a DQN implementation of this algorithm.",
      "startOffset" : 19,
      "endOffset" : 313
    }, {
      "referenceID" : 3,
      "context" : "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem. The max operator was divided into selection and evaluation parts, which replaced the positive bias of the original predictor with a negative one. Later the Double-DQN (DDQN) algorithm that was introduced in Van Hasselt et al. (2015) provided a DQN implementation of this algorithm. Since the publication of the DQN work, many variants and modification of the standard algorithm have been developed. Schaul et al. (2015) showed improved results by employing smart policies for sampling from the experience buffer; Wang et al.",
      "startOffset" : 19,
      "endOffset" : 500
    }, {
      "referenceID" : 3,
      "context" : "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem. The max operator was divided into selection and evaluation parts, which replaced the positive bias of the original predictor with a negative one. Later the Double-DQN (DDQN) algorithm that was introduced in Van Hasselt et al. (2015) provided a DQN implementation of this algorithm. Since the publication of the DQN work, many variants and modification of the standard algorithm have been developed. Schaul et al. (2015) showed improved results by employing smart policies for sampling from the experience buffer; Wang et al. (2015) introduced a modified Q-network architecture; Mnih et al.",
      "startOffset" : 19,
      "endOffset" : 612
    }, {
      "referenceID" : 3,
      "context" : "Double Q-learning (Hasselt, 2010) was suggested as a solution for this problem. The max operator was divided into selection and evaluation parts, which replaced the positive bias of the original predictor with a negative one. Later the Double-DQN (DDQN) algorithm that was introduced in Van Hasselt et al. (2015) provided a DQN implementation of this algorithm. Since the publication of the DQN work, many variants and modification of the standard algorithm have been developed. Schaul et al. (2015) showed improved results by employing smart policies for sampling from the experience buffer; Wang et al. (2015) introduced a modified Q-network architecture; Mnih et al. (2016) implemented asynchronous version of the DQN which does not require the use of an experience-buffer and showed impressive performance.",
      "startOffset" : 19,
      "endOffset" : 677
    }, {
      "referenceID" : 15,
      "context" : "In classical Q-learning (Watkins and Dayan (1992)) the update rule is given by Qt+1(s, a)← (1− α)Qt(s, a) + α(r(s, a) + γmax a′ Qt(s ′, a′)) (1) where s′ is the resulting state after applying action a in the current state s, γ is the MDP’s discount factor and α is a time varying learning rate.",
      "startOffset" : 25,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "must be introduced (Riedmiller, 2005).",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "The DQN algorithm (Mnih et al., 2013) introduced two important modifications for the algorithm above.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "Thrun and Schwartz (1993) provided a first theoretical analysis for combing Q-learning and function approximations.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "1 Overestimation Thrun and Schwartz (1993) considered an additive zero-mean random variable, a generalization error model for the Q-function.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "Hasselt (2010) argued that even in a tabular setting environment noise can cause overestimations and suggested the Double Q-learning algorithm as a solution.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "Second, we will demonstrate and compare performance using the Arcade Learning Environment (Bellemare et al. (2013)), which is the most commonly used benchmark environment for RL algorithms.",
      "startOffset" : 91,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "For the experiments, we have used the ADAM optimizer (Kingma and Ba (2014)), updated the target network parameters each τ = 300 iterations, and α = 0.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "2 Arcade Learning Environment We have evaluated the ADQN algorithm on several Atari games on the Arcade Learning Environment (Bellemare et al. (2013)).",
      "startOffset" : 126,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : "We followed the setup of hyper-parameters and network architecture as in (Mnih et al. (2013)).",
      "startOffset" : 74,
      "endOffset" : 93
    } ],
    "year" : 2016,
    "abstractText" : "The commonly used Q-learning algorithm combined with function approximation induces systematic overestimations of state-action values. These systematic errors might cause instability, poor performance and sometimes divergence of learning. In this work, we present the AVERAGED TARGET DQN (ADQN) algorithm, an adaptation to the DQN class of algorithms which uses a weighted average over past learned networks to reduce generalization noise variance. As a consequence, this leads to reduced overestimations, more stable learning process and improved performance. Additionally, we analyze ADQN variance reduction along trajectories and demonstrate the performance of ADQN on a toy Gridworld problem, as well as on several of the Atari 2600 games from the Arcade Learning Environment.",
    "creator" : "LaTeX with hyperref package"
  }
}