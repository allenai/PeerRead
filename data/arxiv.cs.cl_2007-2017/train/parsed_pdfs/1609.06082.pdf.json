{
  "name" : "1609.06082.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Robust Representations of Text",
    "authors" : [ "Yitong Li", "Trevor Cohn", "Timothy Baldwin" ],
    "emails" : [ "yitongl4@student.unimelb.edu.au,", "tcohn@unimelb.edu.au", "tbaldwin@unimelb.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 9.\n06 08\n2v 1\n[ cs\n.C L\n] 2\n0 Se"
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argued that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization.\n1Implementation available at https://github.com/lrank/Robust-Representation.\nFawzi et al. (2015) provided a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and also showed linear models are usually not robust to adversarial noise.\nIn this work, we present a regularization method which makes deep learning models more robust to noise, inspired by Rifai et al. (2011). The intuition behind the approach is to stabilize predictions by minimizing the ability of features to perturb predictions, based on high-order derivatives. Rifai et al. (2011) introduced contractive autoencoders based on similar ideas, using the Frobenius norm of the Jacobian matrix as a penalty term to extract robust features. Also related, Martens (2010) investigated a second-order optimization method based on Hessian-free approach for training deep auto-encoders. Where our proposed approach differs is that we train models using first-order derivatives of the training loss as part of a regularization term, necessitating second-order derivatives for computing the gradient. We empirically demonstrate the effectiveness of the model over text corpora with increasing amounts of artificial masking noise, using a range of sentiment analysis datasets (Pang and Lee, 2008) with a convolutional neural network model (Kim, 2014). In this, we show that our method is superior to dropout (Srivastava et al., 2014) and a baseline method using MAP training."
    }, {
      "heading" : "2 Training for Robustness",
      "text" : "Our method introduces a regularization term during training to ensure model robustness. We develop our approach based on a general class of parametric\nmodels, with the following structure. Let x be the input, which is a sequence of (discrete) words, represented by a fixed-size vector of continuous values, h. A transfer function takes h as input and produces an output distribution, ypred. Training proceeds using stochastic gradient descent to minimize a loss function L, measuring the difference between ypred and the truth ytrue.\nThe purpose of our work is to learn neural models which are more robust to strange or invalid inputs. When small perturbations are applied on x, we want the prediction ypred to remain stable. Text can be highly variable, allowing for the same information to be conveyed with different word choice, different syntactic structures, typographical errors, stylistic changes, etc. This is a particular problem in transfer learning scenarios such as domain adaptation, where the inputs in distinct domains are drawn from related, but different, distributions. A good model should be robust to these kinds of small changes to the input, and produce reliable and stable predictions.\nNext we discuss methods for learning models which are robust to variations in the input, before providing details of the neural network model used in our experimental evaluation."
    }, {
      "heading" : "2.1 Conventional Regularization and Dropout",
      "text" : "Conventional methods for learning robust models include l1 and l2 regularization (Ng, 2004), and dropout (Srivastava et al., 2014). In fact, Wager et al. (2013) showed that the dropout regularizer is first-order equivalent to an l2 regularizer applied after scaling the features. Dropout is also equivalent to “Follow the Perturbed Leader” (FPL) which perturbs exponential numbers of experts by noise and then predicts with the expert of minimum perturbed loss for online learning robustness (van Erven et al., 2014). Given its popularity in deep learning, we take dropout to be a strong baseline in our evaluation.\nThe key idea behind dropout is to randomly zero out units, along with their connections, from the network during training, thus limiting the extent of coadaptation between units. We apply dropout on the representation vector h, denoted ĥ = dropoutβ(h), where β is the dropout rate. Similarly to our proposed method, training with dropout requires gradi-\nent based search for the minimizer of the loss L. We also use dropout to generate noise in the test data as part of our experimental simulations, as we will discuss later."
    }, {
      "heading" : "2.2 Robust Regularization",
      "text" : "Our method is inspired by the work on adversarial training in computer vision (Goodfellow et al., 2014). In image recognition tasks, small distortions that are indiscernible to humans can significantly distort the predictions of neural networks (Szegedy et al., 2014). An intuitive explanation of our regularization method is, when noise is applied to the data, the variation of the output is kept lower than the noise. We adapt this idea from Rifai et al. (2011) and develop the Jacobian regularization method.\nThe proposed regularization method works as follows. Conventional training seeks to minimise the difference between ytrue and ypred. However, in order to make our model robust against noise, we also want to minimize the variation of the output when noise is applied to the input. This is to say, when perturbations are applied to the input, there should be as little perturbation in the output as possible. Formally, the perturbations of output can be written as py = M(x+px)−M(x), where x is the input, px is the vector of perturbations applied to x, M expresses the trained model, py is the vector of perturbations generated by the model, and the output distribution y = M(x). Therefore\nlim px→0 py = lim px→0\n( M(x + px)−M(x) ) = ∂y\n∂x · px ,\nand distance\n(\nlim px→0 py/px,0\n)\n=\n∥ ∥ ∥ ∥ ∂y\n∂x\n∥ ∥ ∥ ∥\nF\n.\nIn other words, minimising local noise sensitivity is equivalent to minimising the Frobenius norm of the Jacobean matrix of partial derivatives of the model outputs wrt its inputs.\nTo minimize the effect of perturbation noise, our method involves an additional term in the loss function, in the form of the derivative of loss L with respect to hidden layer h. Note that while in principle we could consider robustness to perturbations in the input x, the discrete nature of x adds additional mathematical complications, and thus we defer this\nsetting for future work. Combining the elements, the new loss function can be expressed as\nL = L+ λ ·\n∥ ∥ ∥ ∥ ∂L\n∂h\n∥ ∥ ∥ ∥\n2\n, (1)\nwhere λ is a weight term, and distance takes the form of the l2 norm. The training objective in Equation (1) supports gradient optimization, but note that it requires the calculation of second-order derivatives of L during back propagation, arising from the ∂L/∂h term. Henceforth we refer to this method as robust regularization."
    }, {
      "heading" : "2.3 Convolutional Network",
      "text" : "For the purposes of this paper, we focus exclusively on convolutional neural networks (CNNs), but stress that the method is compatible with other neural architectures and other types of parametric models (not just deep neural networks). The CNN used in this research is based on the model proposed by Kim (2014), and is outlined below.\nLet S be the sentence, consisting of n words {w1, w2, · · · , wn}. A look-up table is applied to S, made up of word vectors ei ∈ Rm corresponding to each word wi, where m is the word vector dimensionality. Thus, sentence S can be represented as a matrix ES ∈ Rm×n by concatenating the word vectors ES = ⊕n i=1 ewi .\nA convolutional layer combined with a number of wide convolutional filters is applied to ES. Specifically, the k-th convolutional filter operator filterk involves a weight vector wk ∈ Rm×t, which works on every tk-sized window of ES, and is accompanied by a bias term b ∈ R. The filter operator is followed by the non-linear function F , a rectified linear unit, ReLU, followed by a max-pooling operation, to generate a hidden activation hk = MaxPooling(F (filterk(ES;wk, b)). Multiple filters with different window sizes are used to learn different local properties of the sentence. We concatenate all the hidden activations hk to form a hidden layer h, with size equal to the number of filters. Details of parameter settings can be found in Section 3.2.\nThe feature vector h is fed into a final softmax layer with a linear transform to generate a probability distribution over labels\nypred = softmax(w · h + b) ,\nwhere w and b are parameters. Finally, the model minimizes the loss of the cross-entropy between the ground-truth and the model prediction, L = CrossEntropy(ytrue,ypred), for which we use stochastic gradient descent."
    }, {
      "heading" : "3 Datasets and Experimental Setups",
      "text" : "We experiment on the following datasets,2 following Kim (2014):\n• MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy."
    }, {
      "heading" : "3.1 Noisifying the Data",
      "text" : "Different to conventional evaluation, we corrupt the test data with noise in order to evaluate the robustness of our model. We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013). To simulate this, we apply word-level dropout noise to each document, by randomly replacing words by a unique sentinel symbol.6 This is applied to each word with probability α ∈ {0, 0.1, 0.2, 0.3}.\nWe also experimented with adding different levels of Gaussian noise to the sentence embeddings ES, but found the results to be largely consistent with those for word dropout noise, and therefore we have omitted these results from the paper.\nTo directly test the robustness under a more realistic setting, we additionally perform cross-domain\n2For datasets where there is no pre-defined training/test split, we evaluate using 10-fold cross validation. Refer to Kim (2014) for more details on the datasets.\n3 https://www.cs.cornell.edu/people/pabo/movie-review4http://www.cs.uic.edu/˜liub/FBS/sentiment-analysis.html 5http://nlp.stanford.edu/sentiment/ 6This was to avoid creating new n-grams which would oc-\ncur when symbols are deleted from the input. Masking tokens instead results in partially masked n-grams as input to the convolutional filters.\nevaluation, where we train a model on one dataset and apply it to another. For this, we use the pairing of MR and CR, where the first dataset is based on movie reviews and the second on product reviews, but both use the same label set. Note that there is a significant domain shift between these corpora, due to the very nature of the items reviewed."
    }, {
      "heading" : "3.2 Word Vectors and Hyper-parameters",
      "text" : "To set the hyper-parameters of the CNN, we follow the guidelines of Zhang and Wallace (2015), setting word embeddings to m = 300 dimensions and initialising based on word2vec pre-training (Mikolov et al., 2013). Words not in the pre-trained vector table were initialized randomly by the uniform distribution U([−0.25, 0.25)m). The window sizes of filters (t) are set to 3, 4, 5, with 128 filters for each size, resulting in a hidden layer dimensionality of 384 = 128× 3. We use the Adam optimizer (Kingma and Ba, 2015) for training."
    }, {
      "heading" : "4 Results and Discussions",
      "text" : "The results for word-level dropout noise are presented in Table 1. In general, increasing the wordlevel dropout noise leads to a drop in accuracy for all four datasets, however the relative dropoff in accuracy for Robust Regularization is less than for Word Dropout, and in 15 out of 16 cases (four noise levels across the four datasets), our method achieves the best result. Note that this includes the case of α = 0, where the test data is left in its original form, which shows that Robust Regularization is also an effective means of preventing overfitting in the model.\nFor each dataset, we also evaluated based on the combination of Word Dropout and Robust Regularization using the fixed parameters β = 0.5 and λ = 10−2, which are overall the best individual settings. The combined approach performs better than either individual method for the highest noise levels tested across all datasets. This indicates that Ro-\nbust Regularization acts in a complementary way to Word Dropout.\nTable 2 presents the results of the cross-domain experiment, whereby we train a model on MR and test on CR, and vice versa, to measure the robustness of the different regularization methods in a more real-world setting. Once again, we see that our regularization method is superior to word-level dropout and the baseline CNN, and the techniques combined do very well, consistent with our findings for synthetic noise."
    }, {
      "heading" : "4.1 Running Time",
      "text" : "Our method requires second-order derivatives, and thus is a little slower at training time. Figure 1 is a plot of the training and test accuracy at varying points during training over SST.\nWe can see that the runtime till convergence is only slightly slower for Robust Regularization than standard training, at roughly 30 minutes on a twocore CPU (one fold) with standard training vs. 35– 40 minutes with Robust Regularization. The convergence time for Robust Regularization is comparable to that for Word Dropout."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we present a robust regularization method which explicitly minimises a neural model’s sensitivity to small changes in its hidden representation. Based on evaluation over four sentiment analysis datasets using convolutional neural networks, we found our method to be both superior and complementary to conventional word-level dropout under varying levels of noise, and in a cross-domain evalu-\nation. For future work, we plan to apply our regularization method to other models and tasks to determine how generally applicable our method is. Also, we will explore methods for more realistic linguistic noise, such as lexical, syntactic and semantic noise, to develop models that are robust to the kinds of data often encountered at test time."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are grateful to the anonymous reviewers for their helpful feedback and suggestions. This work was supported by the Australian Research Council (grant number FT130101105). Also, we would like to thank the developers of Tensorflow (Abadi et al., 2015), which was used for the experiments in this paper."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng." ],
      "venue" : "Technical report, Google Research.",
      "citeRegEx" : "Wattenberg et al\\.,? 2015",
      "shortCiteRegEx" : "Wattenberg et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Non-linear text regression with a deep convolutional neural network",
      "author" : [ "Zsolt Bitvai", "Trevor Cohn." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Bitvai and Cohn.,? 2015",
      "shortCiteRegEx" : "Bitvai and Cohn.",
      "year" : 2015
    }, {
      "title" : "What to do about bad language on the internet",
      "author" : [ "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 359–369.",
      "citeRegEx" : "Eisenstein.,? 2013",
      "shortCiteRegEx" : "Eisenstein.",
      "year" : 2013
    }, {
      "title" : "Analysis of classifiers’ robustness to adversarial perturbations",
      "author" : [ "Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard." ],
      "venue" : "arXiv preprint arXiv:1502.02590.",
      "citeRegEx" : "Fawzi et al\\.,? 2015",
      "shortCiteRegEx" : "Fawzi et al\\.",
      "year" : 2015
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton." ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6645–6649.",
      "citeRegEx" : "Graves et al\\.,? 2013",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Lexical normalisation of short text messages: Makn sens a #twitter",
      "author" : [ "Bo Han", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368–378.",
      "citeRegEx" : "Han and Baldwin.,? 2011",
      "shortCiteRegEx" : "Han and Baldwin.",
      "year" : 2011
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168–177.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655–665.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1746–1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems 25, pages 1097–1105.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Deep learning via Hessian-free optimization",
      "author" : [ "James Martens." ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning, pages 735–742.",
      "citeRegEx" : "Martens.,? 2010",
      "shortCiteRegEx" : "Martens.",
      "year" : 2010
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Feature selection, L1 vs",
      "author" : [ "Andrew Y. Ng." ],
      "venue" : "L2 regularization, and rotational invariance. In Proceedings of the Twenty-first International Conference on Machine Learning.",
      "citeRegEx" : "Ng.,? 2004",
      "shortCiteRegEx" : "Ng.",
      "year" : 2004
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "Anh Nguyen", "Jason Yosinski", "Jeff Clune." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "Nguyen et al\\.,? 2015",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 115–124.",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Foundations and Trends in Information Retrieval, 2(1-2):1–135.",
      "citeRegEx" : "Pang and Lee.,? 2008",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2008
    }, {
      "title" : "Contractive autoencoders: Explicit invariance during feature extraction",
      "author" : [ "Salah Rifai", "Pascal Vincent", "Xavier Muller", "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning, pages 833–840.",
      "citeRegEx" : "Rifai et al\\.,? 2011",
      "shortCiteRegEx" : "Rifai et al\\.",
      "year" : 2011
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on Empiri-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research, 15:1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "A sensitivity anal",
      "author" : [ "Ye Zhang", "Byron Wallace" ],
      "venue" : null,
      "citeRegEx" : "Zhang and Wallace.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhang and Wallace.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Deep learning has achieved state-of-the-art results across a range of computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al.",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : ", 2012), speech recognition (Graves et al., 2013) and natural language processing tasks (Bahdanau et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : ", 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015).",
      "startOffset" : 46,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : ", 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015).",
      "startOffset" : 46,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : ", 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015).",
      "startOffset" : 46,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016).",
      "startOffset" : 118,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : ", 2013) and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). However, deep models are often overconfident for noisy test instances, making them susceptible to adversarial attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). Goodfellow et al. (2014) argued that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature, due to neural models being intentionally designed to behave in a mostly linear manner to facilitate optimization.",
      "startOffset" : 47,
      "endOffset" : 330
    }, {
      "referenceID" : 4,
      "context" : "Fawzi et al. (2015) provided a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and also showed linear models are usually not robust to adversarial noise.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 18,
      "context" : "We empirically demonstrate the effectiveness of the model over text corpora with increasing amounts of artificial masking noise, using a range of sentiment analysis datasets (Pang and Lee, 2008) with a convolutional neural network model (Kim, 2014).",
      "startOffset" : 174,
      "endOffset" : 194
    }, {
      "referenceID" : 10,
      "context" : "We empirically demonstrate the effectiveness of the model over text corpora with increasing amounts of artificial masking noise, using a range of sentiment analysis datasets (Pang and Lee, 2008) with a convolutional neural network model (Kim, 2014).",
      "startOffset" : 237,
      "endOffset" : 248
    }, {
      "referenceID" : 21,
      "context" : "In this, we show that our method is superior to dropout (Srivastava et al., 2014) and a baseline method using MAP training.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "In this work, we present a regularization method which makes deep learning models more robust to noise, inspired by Rifai et al. (2011). The intuition behind the approach is to stabilize predictions by minimizing the ability of features to perturb predictions, based on high-order derivatives.",
      "startOffset" : 72,
      "endOffset" : 136
    }, {
      "referenceID" : 13,
      "context" : "In this work, we present a regularization method which makes deep learning models more robust to noise, inspired by Rifai et al. (2011). The intuition behind the approach is to stabilize predictions by minimizing the ability of features to perturb predictions, based on high-order derivatives. Rifai et al. (2011) introduced contractive autoencoders based on similar ideas, using the Frobenius norm of the Jacobian matrix as a penalty term to extract robust features.",
      "startOffset" : 72,
      "endOffset" : 314
    }, {
      "referenceID" : 12,
      "context" : "Also related, Martens (2010) investigated a second-order optimization method based on Hessian-free approach for training deep auto-encoders.",
      "startOffset" : 14,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "Conventional methods for learning robust models include l1 and l2 regularization (Ng, 2004), and dropout (Srivastava et al.",
      "startOffset" : 81,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "Conventional methods for learning robust models include l1 and l2 regularization (Ng, 2004), and dropout (Srivastava et al., 2014).",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "Conventional methods for learning robust models include l1 and l2 regularization (Ng, 2004), and dropout (Srivastava et al., 2014). In fact, Wager et al. (2013) showed that the dropout regularizer is first-order equivalent to an l2 regularizer applied after scaling the features.",
      "startOffset" : 31,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "Our method is inspired by the work on adversarial training in computer vision (Goodfellow et al., 2014).",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Our method is inspired by the work on adversarial training in computer vision (Goodfellow et al., 2014). In image recognition tasks, small distortions that are indiscernible to humans can significantly distort the predictions of neural networks (Szegedy et al., 2014). An intuitive explanation of our regularization method is, when noise is applied to the data, the variation of the output is kept lower than the noise. We adapt this idea from Rifai et al. (2011) and develop the Jacobian regularization method.",
      "startOffset" : 79,
      "endOffset" : 464
    }, {
      "referenceID" : 10,
      "context" : "The CNN used in this research is based on the model proposed by Kim (2014), and is outlined below.",
      "startOffset" : 64,
      "endOffset" : 75
    }, {
      "referenceID" : 18,
      "context" : "We experiment on the following datasets,2 following Kim (2014): • MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al.",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "We experiment on the following datasets,2 following Kim (2014): • MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al.",
      "startOffset" : 147,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "We experiment on the following datasets,2 following Kim (2014): • MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al.",
      "startOffset" : 199,
      "endOffset" : 217
    }, {
      "referenceID" : 20,
      "context" : "We experiment on the following datasets,2 following Kim (2014): • MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al., 2013)5 In each case, we evaluate using classification accuracy.",
      "startOffset" : 287,
      "endOffset" : 308
    }, {
      "referenceID" : 9,
      "context" : "We experiment on the following datasets,2 following Kim (2014): • MR: Sentence polarity dataset (Pang and Lee, 2008)3 • Subj: Subjectivity dataset (Pang and Lee, 2005)3 • CR: Customer review dataset (Hu and Liu, 2004)4 • SST: Stanford Sentiment Treebank, using the 3-class configuration (Socher et al.",
      "startOffset" : 52,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013).",
      "startOffset" : 176,
      "endOffset" : 217
    }, {
      "referenceID" : 3,
      "context" : "We assume that when dealing with short text such as Twitter posts, it is common to see unknown words due to typos, abbreviations and sociolinguistic marking of different types (Han and Baldwin, 2011; Eisenstein, 2013).",
      "startOffset" : 176,
      "endOffset" : 217
    }, {
      "referenceID" : 10,
      "context" : "Refer to Kim (2014) for more details on the datasets.",
      "startOffset" : 9,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "To set the hyper-parameters of the CNN, we follow the guidelines of Zhang and Wallace (2015), setting word embeddings to m = 300 dimensions and initialising based on word2vec pre-training (Mikolov et al., 2013).",
      "startOffset" : 188,
      "endOffset" : 210
    }, {
      "referenceID" : 11,
      "context" : "We use the Adam optimizer (Kingma and Ba, 2015) for training.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "To set the hyper-parameters of the CNN, we follow the guidelines of Zhang and Wallace (2015), setting word embeddings to m = 300 dimensions and initialising based on word2vec pre-training (Mikolov et al.",
      "startOffset" : 71,
      "endOffset" : 93
    } ],
    "year" : 2016,
    "abstractText" : "Deep neural networks have achieved remarkable results across many language processing tasks, however these methods are highly sensitive to noise and adversarial attacks. We present a regularization based method for limiting network sensitivity to its inputs, inspired by ideas from computer vision, thus learning models that are more robust. Empirical evaluation over a range of sentiment datasets with a convolutional neural network shows that, compared to a baseline model and the dropout method, our method achieves superior performance over noisy inputs and out-of-domain data.1",
    "creator" : "LaTeX with hyperref package"
  }
}