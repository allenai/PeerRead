{
  "name" : "1612.00959.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "apacuk@mimuw.edu.pl", "sank@mimuw.edu.pl", "k.wegrzycki@mimuw.edu.pl", "a.witkowski@mimuw.edu.pl", "wygos@mimuw.edu.pl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 2.\n00 95\n9v 1\n[ cs\n.A I]\n3 D\nec 2\n01 6\nWe present the Mim-Solution’s approach to the RecSys Challenge 2016, which ranked 2nd. The goal of\nthe competition was to prepare job recommendations for the users of the website Xing.com.\nOur two phase algorithm consists of candidate selection followed by the candidate ranking. We ranked the candidates by the predicted probability that the user will positively interact with the job offer. We have used Gradient Boosting Decision Trees as the regression tool."
    }, {
      "heading" : "1 Introduction",
      "text" : "The Recsys Challenge is an annual competition of recommender systems. The 2016th edition1 was based on data provided by xing.com – a platform for business networking. On Xing, users search for job offers that could fit them. Each user has a profile containing information such as: place of living, industry branch and experience (in years). Job offers are described by a related set of properties. There are various ways in which a user can interact with a job offer (called item): by clicking is, as well as bookmarking interesting ones, replying to offers and finally by deleting a recommendation.\nThe task in the challenge was to predict for a given XING user 30 items that this user will positively interact with (click, bookmark or reply to).\nDataset The dataset consisted of properties of users and items, interactions of the users (which items user clicked, bookmarked, replied to or deleted) and impressions (items shown to users by XING recommender system). The interactions and impressions were gathered from a 3 month period.\nAll data was anonymized by changing all properties to numerical values and adding an unknown number of artificial users, items, interactions and impressions.\nWe were also given a set of both user and item properties. Common attributes of users and items were: career level, discipline id, industry id, country and region. Besides that users had following attributes: jobroles, experience n entries class, experience years experience, experience years in current, edu degree, edu fieldofstudies. Items had attributes: title, latitude, longitude, employment, created at and active during test2. Values of those attributes we will denote by, e.g., career level(i) for a given item i.\nEach impression is a tuple containing: user ID, item ID, number of the week in which impression occurred. Each interaction contains: user ID, item ID, interaction type (click, bookmark, reply, delete) and timestamp of the interaction. We will denote the positive interactions of a user u by Intu, the negative interactions by Delu and impressions by Impu.\n10% of users were target users: users which we needed to compute the predictions for. The predicted items had to come from a subset (24%) of all items (these were the job offers open during the period). Exact datasets sizes are presented in Table 1.\nEvaluation measure The ground truth is a mapping that assigns to each user, the set of items he interacted positively with during the test week. Let T be the set of target users, I be the set of all items, and G : T → 2I the ground truth. We will also denote the sequence of items predicted for a user by pred (u). The quality of recommendations was measured by a function:\nscore(pred, G) = ∑\nu∈T\nuserScore(pred(u), G(u)), (1)\n∗Institute of Informatics, University of Warsaw, Poland 1http://2016.recsyschallenge.com/ 2 Amore detailed specification of the dataset can be found on https://github.com/recsyschallenge/2016/blob/master/TrainingDataset.md\nwhere:\nand for a sequence ā = a1, a2, . . . , a30, a set B and a natural number k:\np(ā, B, k) = |{a1, a2, . . . , ak} ∩B|/k\nr(ā, B) = |{a1, a2, . . . , a30} ∩B|/min(1, |B|)\nus(ā, B) = min(1, |{a1, a2, . . . , a30} ∩B|).\nNote that: p(ā, B, k) is the precision for the first k elements of a, r(ā, B) is the recall and us(ā, B) is user success (1 if if we predicted at least one item for this user correctly, 0 otherwise).\nSolutions were evaluated by the submission system and contestants received instant feedback with score value calculated on a sample of 13 of target users."
    }, {
      "heading" : "2 Our solution",
      "text" : "Our solution consists of two parts:\n1. for each user we calculate a set of candidate items, much smaller than the whole set I (Only those candidates were considered when creating a submission),\n2. learn for each (user, candidate) pair (u, i) the probability P [i ∈ G(u)] that the user will interact with this item.\nWe submitted, for each user, 30 items with the highest predicted probability of interaction, excluding items that the user deleted. Limiting the number of considered items per user allowed us to substantially decrease the time required to train a model and prepare a submission. Examining all possible user-item pairs (150.000 × 327.000 pairs) was infeasible considering our resources. To learn the probabilities P [i ∈ G(u)], we have used Gradient Boosting Decision Trees (GBDT) [1]3, optimizing the logloss measure. We learned the probabilities instead of directly optimizing the score function since it does not give results for a single user-item pair. The schema of our solution is presented in Figure 1.\nOur solution was ranked 2nd in the competition, scoring 675985.03 and 2035964.16 points on the public and private leaderboard respectively. To put this into perspective, submitting for each user u, sorted from most recent Intu but without items from Delu (adding Impu if there were less than 30 unique interactions) achieved a score of 495k on the public leaderboard. All of the computation were performed on 12 cores (24 threads), 64 GB RAM Linux server."
    }, {
      "heading" : "2.1 Training set",
      "text" : "In this problem, there was no clearly defined training set and the first challenge was to create it. Since our task was to predict users’ interactions in the week following the end of the available data, we trained our model on all the data except the last available week and then used this last week data to compute the training ground truth G to evaluate the model. This way, we could calculate the score and determine if we are making progress without sending an official submission (every team was allowed max. 5 submissions per day).\nThere was an overlap between the training data and the test data (the full dataset). Both candidates and features were computed separately for the training set and the full dataset.\n3https://github.com/dmlc/xgboost"
    }, {
      "heading" : "2.2 Candidate items selection",
      "text" : "Since there were 150k target users and more than 300k items, making a prediction for each user-item pair would take too long. To address this issue, we chose for each user u a set of promising items, which we called candidates.\nTo define candidates, we will need a few notions of similarity. The Jaccard coefficient between two sets A and B is J(A,B) ≡ |A∩B||A∪B| . Interactions (impressions) similarity between two users u, u ′, denoted Int-sim(u, u′) (Imp-sim(u, u′)), is the Jaccard coefficient between the sets of items from their positive interactions (impressions). For example Int-sim(u, u′) = J(Intu, Intu′).\nFor items i, i′, we will denote common-tags(i, i′) ≡ |tags(i)∩tags(i′)| and common-title(i, i′) ≡ |title(i)∩title(i′)|. For an user u, the candidates were:\n1. Intu sorted by the week of occurrence (most recent first) and the number of interactions,\n2. Impu sorted the same way as 1, 3. Intu′ for users u ′ with large Int-sim(u, u′), 4. Impu′ of users u ′ with large Imp-sim(u, u′), 5. items i with large maxi′∈Intu common-tags(i, i ′), similarly for maxi′∈Intu common-title(i, i\n′), max |title(i′) ∩ tags(i)| and max |title(i) ∩ tags(i′)|,\n6. same as 5, just with max taken over i′ ∈ Impu,\n7. items i with large |jobroles(u) ∩ tags(i)|,\n8. items i with large |jobroles(u) ∩ title(i)|,\n9. the most popular items (globally, this list was the same for all users).\nThe popularity of an item was measured by the number of interactions of all users with this item. From each category, we took 60 best candidates for each user. This approach gave us 43M (user,item) pairs, on average just short of 300 candidates per user. On the training set, candidates chosen this way covered 37% of the training ground truth.\nOf course, we could have chosen a different notions of similarity between users/items. In particular we considered similarity based on user and item properties such as region, industry, etc. Candidates based on those measures of similarity did not improve the score sufficiently, probably due to anonymization of the data."
    }, {
      "heading" : "2.3 Learning the probabilities",
      "text" : "We wanted to construct a model that given a (u, i) pair and values of features for this pair will compute the probability P [i ∈ G(u)]. In order to estimate the probabilities, we have used XGBoost4, a machine learning library implementing GBDT.\n4https://github.com/dmlc/xgboost\nFor training the ranking model, we split all the users with at least one item in the training ground truth into two sets of equal size. The users from the first set were used for training XGBoost model, and the users from the second set were used for validation of the model. The training file (and validation file) contained for each user:\n• all the training candidate items, which occurred in the training ground truth (positive candidates)\n• and up to 5 training candidate items, which did not occur in the training ground truth (negative candidates).\nJust after deadline for submitting solutions we observed, that training a model on all users with all positive and 1/4 of negative candidates improves our score by 6.5k points on unofficial public leaderboard5.\nEvaluation We used maximum likelihood as the objective function optimized by GBDT. Additionally we verified models by computing the score function based on the validation part of the training ground truth. Major improvements in this validation score translated to comparable improvements on the score achieved via the submission system. We measured on training ground truth that our way of ordering previously selected candidate items achieved score which was 77.5% of best possible result based only on those candidates.\nParameters tuning We found that the optimal XGBoost parameters for our task were:\n• maximum depth of a tree (max depth) in range [4, 6],\n• minimum weight of node to be splitted (min child weight) in range [4, 6],\n• learning rate (eta) = 0.1,\n• minimum loss reduction to make a node partition (gamma) = 1.0,\n• number of rounds (num round) = 1000 (validation logloss did not improve after 1000 rounds)."
    }, {
      "heading" : "2.4 Features",
      "text" : "Each feature is a function that for user u and item i maps the pair (u, i) to some real number. We ended up with 273 features. Many of these features were highly correlated, however, we observed that redundant features do not reduce the quality of the model. Many features differ only in:\n• used different events source: impressions instead of interactions, only positive/negative interactions, only interactions of one type,\n• instead of Jaccard coefficient we used size of sets intersection |A ∩B| and vice-versa,\n• used various aggregate functions: maximum, minimum, sum, average, count or unique count (count without duplicate entries).\nBecause of this we will only describe the important groups of features. There are 12 such groups Table 2 summarizes the importance of the features we used. Ideas for features were inspired by papers of previous RecSys Challenge [2, 3] and similar competitions hosted on Kaggle platform6. Event based features are percentages of items from Intu that had some property (e.g., item’s career level) equal to item’s i corresponding property. Dually we also used the percentages of users from Int-users(i) (i.e., users who interact a given item i) that had some attribute equal to user u attribute. From this group of features, the best were those based on item tags and item title fields. Item global popularity is the number of times item i was clicked by any user. Additionally we computed the trend of popularity: clicks count in the last week divided by the clicks count in the previous last week. Another way to observe week trend was to compute trend based on events from last and previous last Mondays, Tuesdays, . . . , Sundays. Colaborative filtering most similar are features that measure similarity between the item i and the items from Intu using I -sim, and dually between the user u and users that interacted with i, using Int -sim. Formally, these are given by formulas:\n• maxi′∈Intu I-sim(i, i ′), • maxu′∈Int-users(i) Int-sim(u, u ′).\n5 Note that difference between 1st and 2nd place was 5.7k, so extending training file earlier could result in winning a competition\nby our team. 6www.kaggle.com\nUser total events is just |Intu| both with repetitions of items and without. We also used this feature limited to the user’s last week of activity. Seconds from last user activity are the differences, in seconds, between:\n• the last time u clicked i and the maximal timestamp in data,\n• the last time u clicked i and the last time u clicked any item,\n• the last time u clicked any item and the maximal timestamp in data.\nAnalogous features based on impressions were timed in weeks. Max common tags with clicked item are features that return the maximum number of common tags between item i and items from Intu: maxi′∈Intu common-tags(i, i\n′). We used those features with common-title(i, i′) instead of common-tags(i, i′) and with impressions instead of interactions. Position on the candidates list is position number of item i on user’s u candidates list. There was separate feature for each candidate algorithm (see Subsection 2.2). User-item events count in last week is the count of how many times user u clicked item i in the last week of this users activity, or in the last week from the dataset. Item properties are values of item’s i attributes. Content based user-item similarity is a group of features based only on properties of u and i. The features were:\n• career level(i)− career level(u),\n• |jobroles(u) ∩ title(i)|,\n• |jobroles(u) ∩ tags(i)|.\n• 1 if attr(u) = attr(i) else 0, for the rest of matching attributes.\nDistance to the closest clicked item is the Euclidean distance between the location of item i and the location of the closest item from Intu. Item cluster is a boolean feature, true if the item i is in a cluster of some item from Intu, where i ∈ cluster(i\n′) if i 6= i′ and there exists a user u that clicked both i and i′ within 10 minutes. This feature is another variation of the similarity between items. This time, we say that two items are similar if a user clicked both of them in a similar time. The motivation is simple: if both those items were interesting for some user, then they probably have something in common that appeals to this user.\nSome of the features (especially time-related) had to be calculated separately and with care for the training dataset and the full dataset. For example, we had a feature “timestamp of the last interaction”. This feature on training instance should be shifted in order to cover the same range of values as on the full dataset."
    }, {
      "heading" : "2.5 Blending",
      "text" : "In a final step in the construction of our solution we merged our best models. Since all of them were similar and XGBoost based, we took for each (u, i) pair, the arithmetic mean of probabilities P [i ∈ G(u)] calculated by those models. Finally, for each target user, we sorted candidate items by these averaged probabilities and selected the top 30."
    }, {
      "heading" : "3 Conclusions",
      "text" : "We have presented Mim-Solution’s approach to the 2016th RecSys Challenge. We used XGBoost to predict probabilities that a user will be interested in a job offer, but only for preselected offers. This allowed to efficiently train and evaluate the model as well as use complex and robust features.\nEven tough our score was good (28.5 % of best possible score measured on our train data), there was definitely a lot of room to improve. One easy improvement, mentioned already in the paper, is increasing the size of the training set. There was also some place for improvements in ordering of candidate items, but we suspect that twice bigger room for improvement was in expanding the set of candidates (we achieved 77.5% and 37% of best possible results in the layer of sorting and selecting candidates respectively).\nThis work is supported by ERC project PAAl-POC 680912."
    } ],
    "references" : [ {
      "title" : "Xgboost: A scalable tree boosting system",
      "author" : [ "T. Chen", "C. Guestrin" ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Recsys challenge 2015: Ensemble learning with categorical features",
      "author" : [ "P. Romov", "E. Sokolov" ],
      "venue" : "In Proceedings of the 2015 International ACM Recommender Systems Challenge, RecSys ’15 Challenge,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Two-stage approach to item recommendation from user sessions",
      "author" : [ "M. Volkovs" ],
      "venue" : "In Proceedings of the 2015 International ACM Recommender Systems Challenge, RecSys ’15 Challenge,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "To learn the probabilities P [i ∈ G(u)], we have used Gradient Boosting Decision Trees (GBDT) [1], optimizing the logloss measure.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "Ideas for features were inspired by papers of previous RecSys Challenge [2, 3] and similar competitions hosted on Kaggle platform.",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Ideas for features were inspired by papers of previous RecSys Challenge [2, 3] and similar competitions hosted on Kaggle platform.",
      "startOffset" : 72,
      "endOffset" : 78
    } ],
    "year" : 2016,
    "abstractText" : "We present the Mim-Solution’s approach to the RecSys Challenge 2016, which ranked 2nd. The goal of the competition was to prepare job recommendations for the users of the website Xing.com. Our two phase algorithm consists of candidate selection followed by the candidate ranking. We ranked the candidates by the predicted probability that the user will positively interact with the job offer. We have used Gradient Boosting Decision Trees as the regression tool.",
    "creator" : "LaTeX with hyperref package"
  }
}