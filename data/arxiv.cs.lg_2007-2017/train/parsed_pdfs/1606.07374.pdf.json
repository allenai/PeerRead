{
  "name" : "1606.07374.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Gabriele Cirulli" ],
    "emails" : [ "icwu@csie.nctu.edu.tw)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "difference (TD) learning together with n-tuple networks for playing the game 2048. However, we observed a phenomenon that the programs based on TD learning still hardly reach large tiles. In this paper, we propose multi-stage TD (MS-TD) learning, a kind of hierarchical reinforcement learning method, to effectively improve the performance for the rates of reaching large tiles, which are good metrics to analyze the strength of 2048 programs. Our experiments showed significant improvements over the one without using MS-TD learning. Namely, using 3-ply expectimax search, the program with MS-TD learning reached 32768-tiles with a rate of 18.31%, while the one with TD learning did not reach any. After further tuned, our 2048 program reached 32768-tiles with a rate of 31.75% in 10,000 games, and one among these games even reached a 65536-tile, which is the first ever reaching a 65536-tile to our knowledge. In addition, MS-TD learning method can be easily applied to other 2048-like games, such as Threes. Based on MS-TD learning, our experiments for Threes also demonstrated similar performance improvement, where the program with MS-TD learning reached 6144-tiles with a rate of 7.83%, while the one with TD learning only reached 0.45%.\nKeywords—Stochastic Puzzle Game, 2048, Threes, Temporal\nDifference Learning, Expectimax.\nI. INTRODUCTION\necently, 2048-like games, single-player stochastic games including 2048 2 , 1024 3 and Threes 4 , have been very popular over the Internet, especially for 2048. Both 2048 and 1024 were actually originated from Threes. Gabriele Cirulli [15], the author of 2048, claimed his estimation: the aggregated time of playing the game online by players during the first three weeks after release was over 3,000 years. The game is intriguing and even addictive to human players, because it is hard to win the game despite the simple rules. For the same reason, the game also attracted many programmers to develop artificial intelligence (AI) programs to play it. In [22], the authors also thought that the game was an interesting testbed for studying AI methods.\nMany methods were proposed to design AI programs for 2048 and Threes in the past. Most commonly used methods\n1 This work was supported in part by the Ministry of Science and Technology of the Republic of China (Taiwan) under Contracts MOST 104-2221-E-009-127-MY2 and 104-2221-E-009-074-MY2. The authors are with the Department of Computer Science, National Chiao Tung University, Hsinchu 30050, Taiwan. (e-mail of the correspondent: icwu@csie.nctu.edu.tw)\n2 Game 2048. [Online]. Available: http://gabrielecirulli.github.io/2048/ 3 Game 1024. [Online]. Available: http://1024game.org/ 4 Game Threes. [Online]. Available: http://asherv.com/threes\nmethod for two-player games, and expectimax search [2][13][19], a common game search method for single-player stochastic games. Recently, Szubert and Jaśkowski [22] proposed Temporal Difference (TD) learning together with n-tuple networks for 2048. They successfully used it to reach a win rate (the rate of reaching 2048-tiles) of 97%, and obtain the average score 100,178 with maximum score 261,526. However, we observed a phenomenon: the TD learning method tends to maximize the average scores, but becomes less motivated to reach large tiles, such as 16384 or 32768, even with expectimax search incorporated.\nTo cope with this problem, we propose multi-stage TD (MS-TD) learning, a kind of hierarchical reinforcement learning method. In MS-TD, we separate the training into multiple stages. Our experiments showed significant improvements over the one without using MS-TD learning for 2048 and Threes, especially in the reaching rates of large tiles, which are good metrics to analyze the performance. We consider the 32768-tile reaching rate for 2048 and the 6144-tile reaching rate for Threes.\nOur experiments also showed improvements when further incorporating expectimax search. For 2048, the program incorporating MS-TD with 3-ply expectimax search reached 32768-tiles with a probability of 18.31%, while the program with TD learning only did not reach any 32768-tile. After more improvements together with expectimax search, our 2048 program reached 32768-tiles with a probability of 31.75% in 10,000 games. Interestingly, one among these games reached a 65536-tile, the first ever reaching a 65536-tile to our knowledge.\nSimilarly for Threes, the experiments also showed significant improvement of the Threes program based on MS-TD. Namely, it reached 6144-tiles with a probability of 7.83%, while the program with TD learning only reached 6144-tiles with a much lower probability of 0.45%.\nNote that the preliminary version of this paper [32] did not include the following: MS-TD applied to Threes, more splitting strategies for both 2048 and Threes, and a demonstration of combining MS-TD with other techniques to improve 2048 and Threes programs.\nThis paper is organized as follows. Section II reviews background knowledge. Section III proposes MS-TD learning. Section IV does experiments and analysis for MS-TD learning for 2048, and Section V for Threes. Section VI makes concluding remarks.\nR"
    }, {
      "heading" : "II. BACKGROUND",
      "text" : "This section introduces the rules of 2048 and Threes in Subsection II.A, describes game tree search in Subsection II.B, reviews TD learning in Subsection II.C, and discusses n-tuple networks for 2048 proposed in [22] in Subsection II.D.\nA. 2048 and Threes\nThe game 2048 can be played on web pages and mobile devices with a 4x4 board, where each cell is either empty or placed with a tile labeled with a value which is a power of two. Let \uD835\uDC63-tile denote a tile with value \uD835\uDC63. Initially, two of 2-tiles or 4-tiles are placed on the board at random. In each turn, the player makes a move and then the game generates a new 2-tile with a probability of 9/10 or 4-tile with a probability of 1/10 on an empty cell chosen at random.\nTo make a move, the player chooses one of the four directions, up, down, left and right. Upon choosing a direction, all the tiles move in that direction as far as they can until they reach the border or there is already a different-label tile next to it. When sliding a tile, say \uD835\uDC63-tile, if the tile next to it is also a \uD835\uDC63-tile, then the two tiles will be merged into a larger tile, 2\uD835\uDC63-tile. At the same time, the player gains 2\uD835\uDC63 more points in the score. A move is legal if at least one tile can be moved.\nConsider an example, in which an initial board is shown in Fig. 1 (a). After making a move to right, the board becomes the one shown in Fig. 1 (b). Then, a new 2-tile is randomly generated as shown in Fig. 1 (c). The player repeatedly makes moves in this way.\nA game ends when the player cannot make any legal move. The final score is the points accumulated during the game. The objective of the game is to accumulate as many points as possible. The game claims that the player wins when a 2048-tile is created, but still allows players to continue playing optionally.\nIt is observed that the game 2048 has a phenomenon, also called survival phenomenon in this paper. A 2048 game often ends when nearly reaching a large tile, say 16384-tile, due to being blocked by some other smaller large tiles, such as 8192-tiles and 4096-tiles. However, once the game successfully reaches a 16384-tile, most of these smaller large tiles are gone usually. The game survives in the sense that it can usually continue to be played without being blocked by these large tiles for a while, and thus obtain a much higher score. So, the score gap between the games reaching 16384-tiles and the games not reaching the tile is usually high. From the above observations, the reaching rates of large tiles are a good metric to stand for playing strength. In this paper, the 32768-tile reaching rate is used.\nIn the game Threes, the board size is 4x4 as well. When the game starts, every cell either is empty or has one \uD835\uDC63-tile, where \uD835\uDC63\nis 1, 2 or 3. The rule of tile moving is slightly different from 2048. The sliding distance is at most one. In the rules of merging, a 1-tile and a 2-tile can be merged into a 3-tile, and two \uD835\uDC63-tiles can be merged into a 2\uD835\uDC63-tile like the game 2048, where 2 < \uD835\uDC63 < 6144. Note that 6144-tiles cannot be merged anymore. The rules of generating new tiles are much more complex than those for 2048 [8]. Initially, there is a bag of 12 tiles composed of equal amount of 1, 2, and 3-tiles. A normal random tile is randomly selected from the bag until the bag is empty and then refilled. Let \uD835\uDC63\uD835\uDC5A\uD835\uDC4E\uD835\uDC65-tile denote the tile labeled with the largest value \uD835\uDC63\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 on the current board. If \uD835\uDC63\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 ≥ 48, a new tile could be a bonus random tile, namely a \uD835\uDC63-tile with \uD835\uDC63 >3, which is generated with a probability of 1/21 while other tiles from the bag are generated with 20/21. A bonus random tile ranges from a 6-tile to a \uD835\uDC63-tile, where \uD835\uDC63 = \uD835\uDC63\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 ∗ (1/8), with equal probability. For example, if \uD835\uDC63\uD835\uDC5A\uD835\uDC4E\uD835\uDC65 = 192, then the bonus random tile is one of 6-tile, 12-tile and 24-tile with equal probabilities.\nA Threes game ends similarly when the player is no longer able to make any legal move. The final score is the sum of scores of all \uD835\uDC63-tiles with \uD835\uDC63 ≥ 3, using the formula: score =\n3\uD835\uDC59\uD835\uDC5C\uD835\uDC542( \uD835\uDC63 3 )+1 . The objective of this game is to achieve as much\nscore as possible. The game does not define a win like 2048. The above survival phenomenon also exists in Threes. In this paper, the 6144-tile reaching rate is used for Threes.\nIn a 2048-bot tournament [23], contests for both 2048 and Threes were held separately. For the 2048 contest, all the 2048-bot participants played 100 games. Their performances were graded in a formula (described in [23]) from the following criteria, the win rates, the average scores, the maximum scores, and the reaching rates of large tiles. The Threes contest was similar except that reaching 192-tiles was defined as a win."
    }, {
      "heading" : "B. Game Tree Search",
      "text" : "A common game tree search algorithm used in 2048 programs is expectimax search [2][13][19]. Like most game tree search, the leaves are evaluated with values calculated by heuristic functions. An expectimax search tree contains two different kinds of nodes, max nodes and chance nodes. At a max node, its value is the highest value of its children, if any. At a chance node, its value is the expected value of its children, if any, weighted by the probabilities of children. An expectimax search tree is illustrated in Fig. 2.\nSeveral kinds of features were used in heuristic functions for 2048 programs that use game tree search [14][30]. The first is the monotonicity of a board. Most high-ranked players tend to play 2048 with tiles arranged decreasingly in several ways, as described in [19]. The second is the number of empty tiles on a\nboard. The more empty tiles, the less likely for the game to end in a few steps. The third is the number of pairs of mergeable tiles, since it is a measurement of the ability to create empty tiles by merging tiles. A pair of mergeable tiles are two neighboring tiles which can be merged, e.g., two tiles with the same labels in 2048.\nA transposition table is a technique to avoid searching the same positions redundantly and therefore speed up the search. One common implementation is based on Zobrist hashing [33]. In Zobrist hashing, for each cell, each kind of possible tile is assigned a unique random number as a key. When looking up table, the hash value used to access the transposition table is calculated by doing an exclusive-or operation on the 16 keys for all 4x4 cells."
    }, {
      "heading" : "C. Temporal Difference (TD) Learning",
      "text" : "Reinforcement learning is an important technique in training an agent to learn how to respond to a given environment [21]. A Markov decision process (MDP) is a model commonly used in reinforcement learning, modeling the problems in which an agent interacts with the given environment through a sequence of actions according to the change of the state and the rewards, if any. In terms of MDP, an AI program for a 2048-like game thus can be regarded as such an agent, which makes actions (legal moves) on board states and gets points as rewards.\nTemporal difference (TD) learning [20][21], a kind of reinforcement learning, is a method for adjusting state values from the subsequent evaluations. This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28]. Among the above, TD learning was demonstrated to improve some world class game-playing programs, e.g., Chinook [17], and TD-Gammon [24]. Since 2048-like games can be easily modeled as MDP, TD learning can be naturally applied to AI programs for these games.\nIn TD(0), the value function \uD835\uDC49(\uD835\uDC60) is used to approximate the expected return of a state \uD835\uDC60. The error between states \uD835\uDC60\uD835\uDC61 and \uD835\uDC60\uD835\uDC61+1 is \uD835\uDEFF\uD835\uDC61 = \uD835\uDC5F\uD835\uDC61 + \uD835\uDC49(\uD835\uDC60\uD835\uDC61+1) − \uD835\uDC49(\uD835\uDC60\uD835\uDC61) , where \uD835\uDC5F\uD835\uDC61 is the reward at turn \uD835\uDC61. The value of \uD835\uDC49(\uD835\uDC60\uD835\uDC61) in TD(0) is expected to be adjusted by the following value difference ∆\uD835\uDC49(\uD835\uDC60\uD835\uDC61),\n∆\uD835\uDC49(\uD835\uDC60\uD835\uDC61) = \uD835\uDEFC\uD835\uDEFF\uD835\uDC61 = \uD835\uDEFC(\uD835\uDC5F\uD835\uDC61 + \uD835\uDC49(\uD835\uDC60\uD835\uDC61+1) − \uD835\uDC49(\uD835\uDC60\uD835\uDC61)) (1)\nwhere \uD835\uDEFC is a step-size parameter to control the learning rate. Note that in [21] \uD835\uDC49(\uD835\uDC60\uD835\uDC61+1) is weighted by a discount factor, which is ignored in this paper for simplicity. For the general TD(\uD835\uDF06) as in [20][21][31], we adopted the forward view of value difference as mentioned (c.f. Section 7.2 of [21]):\n∆\uD835\uDC49(\uD835\uDC60\uD835\uDC61) = \uD835\uDEFC (\uD835\uDC45\uD835\uDC61 \uD835\uDF06 − \uD835\uDC49(\uD835\uDC60\uD835\uDC61)), (2)\n\uD835\uDC45\uD835\uDC61 \uD835\uDF06 = (1 − \uD835\uDF06) ∑ \uD835\uDF06\uD835\uDC5B−1\uD835\uDC45\uD835\uDC61 \uD835\uDC5B + \uD835\uDF06\uD835\uDC47−\uD835\uDC61−1\uD835\uDC45\uD835\uDC61 \uD835\uDC47−\uD835\uDC61\uD835\uDC47−\uD835\uDC61−1 \uD835\uDC5B=1 , (3)\n\uD835\uDC45\uD835\uDC61 \uD835\uDC5B = ∑ \uD835\uDC5F\uD835\uDC61+\uD835\uDC58 \uD835\uDC5B−1 \uD835\uDC58=0 + \uD835\uDC49(\uD835\uDC60\uD835\uDC61+\uD835\uDC5B), (4)\nwhere \uD835\uDC47 is the ending time step. In this paper, TD(0) is investigated, unless specified.\nIn most applications, the evaluation function of states \uD835\uDC49(\uD835\uDC60) can be viewed as a function of features, such as the monotonicity, the number of empty tiles, and the number of mergeable tiles for 2048 [14], mentioned in Subsection II.B. The function is usually modified into a linear combination of\nfeatures [31] for TD learning, that is, \uD835\uDC49(\uD835\uDC60) = \uD835\uDF11(\uD835\uDC60) ∙ \uD835\uDF03, where \uD835\uDF11(\uD835\uDC60) denotes a vector of feature occurrences in \uD835\uDC60, and \uD835\uDF03 denotes a vector of feature weights.\nIn order to correct the value \uD835\uDC49(\uD835\uDC60\uD835\uDC61) by the difference ∆\uD835\uDC49(\uD835\uDC60\uD835\uDC61), we adjust the feature weights \uD835\uDF03 by a difference ∆\uD835\uDF03 based on the gradient ∇\uD835\uDF03\uD835\uDC49(\uD835\uDC60\uD835\uDC61) , which is \uD835\uDF11(\uD835\uDC60\uD835\uDC61) for linear TD(0) learning. Thus, the difference ∆\uD835\uDF03 is\n∆\uD835\uDF03 = ∆\uD835\uDC49(\uD835\uDC60\uD835\uDC61)\uD835\uDF11(\uD835\uDC60\uD835\uDC61) = \uD835\uDEFC\uD835\uDEFF\uD835\uDC61\uD835\uDF11(\uD835\uDC60\uD835\uDC61). (5)\nIn [22], Szubert and Jaśkowski proposed TD learning for 2048. A transition from turn \uD835\uDC61 to \uD835\uDC61 + 1 is illustrated in Fig. 3. They also proposed three kinds of methods to evaluate values for training and learning as follows. 1. Evaluate actions. This method is to evaluate the function\n\uD835\uDC44(\uD835\uDC60, \uD835\uDC4E), which stands for the expected values of taking an action \uD835\uDC4E on a state \uD835\uDC60. For 2048, an action \uD835\uDC4E is one of the four directions, up, down, left, and right. This is so-called Q-learning.\n2. Evaluate states to play. This method is to evaluate the value\nfunction \uD835\uDC49(\uD835\uDC60\uD835\uDC61) on state \uD835\uDC60\uD835\uDC61, the player to move. 3. Evaluate states after an action. This method is to evaluate\nthe value function \uD835\uDC49(\uD835\uDC60\uD835\uDC61 ′) on state \uD835\uDC60\uD835\uDC61 ′, a state after an action, also called an afterstate in [22].\nIn [22], their experiments showed that the third method clearly outperformed the other two. In the rest of this paper, we only consider the third, evaluating afterstates, and let states refer to afterstates for simplicity.\nD. N-Tuple Network\nIn [22], they also proposed to use n-tuple networks for TD learning in 2048. In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26]. An n-tuple network\nconsists of \uD835\uDC5A \uD835\uDC5B\uD835\uDC56-tuples, where \uD835\uDC5B\uD835\uDC56 is the size of the \uD835\uDC56-th tuple. As shown in Fig. 4 (a), one 4-tuple covers four cells marked in red dots and one 6-tuple covers six cells marked in blue dots. Each tuple contributes a large number of features, each for one distinct occurrence of tiles on the covered cells. For example, the leftmost 4-tuple in Fig. 4 (a) includes 164 features, assuming that a cell has 16 occurrences, empty or 2-tile to 215-tile. The output of a network is a linear summation of feature weights for all occurring features. For each tuple, since one and only one feature occurs at a time, the feature weight can be easily accessed by looking up a table. If an n-tuple network includes \uD835\uDC5A different tuples, we need \uD835\uDC5A lookups.\nIn [22], they used the tuples shown in Fig. 4 (a) as well as all of their rotated and mirrored tuples. All the rotated and mirrored tuples can share the same feature weights. Thus, the total number of features was roughly 2x154+2x156, about 23 millions. Their experiments in [22] showed an average score of 100,178 and a maximum score of 261,526. In this paper, we use the n-tuple network, as shown in Fig. 4 (b), by changing 4-tuples (1x4 lines in red) in Fig. 4 (a) to 6-tuples in knife-shaped ones in Fig. 4 (b). Apparently, the new 6-tuples cover all the original 4-tuples while still not covering the 6-tuples in 2x3. The number of features in total increases to 4x166, about triple5 of the one used in [22] only."
    }, {
      "heading" : "III. MULTI-STAGE TD LEARNING ALGORITHM",
      "text" : "From above, TD learning is intrinsically to train and learn to obtain average (or expected) scores as high as possible. The experiments in [22] also demonstrated this. However, TD learning does not necessarily lead to other criteria such as high maximum scores, or the reaching rates of large tiles, though it does often.\nFrom the experience of playing 2048, we observed that it was hard to push to higher maximum scores, or raise the reaching rates of 32768-tiles based on the original TD learning, even with expectimax search. However, obtaining the maximum scores as well as reaching large tiles is a kind of goal or achievement for most players, and was also one of the criteria of the 2048-bot tournament [23] as described in Subsection II.A.\nIn order to address this issue, we propose multi-stage TD (MS-TD) learning for 2048-like games. In this method, we divide the learning process into multiple stages, each of which has its own learning agent and subgoal, e.g., reaching 8192-tiles or 16384-tiles, as in hierarchical reinforcement learning (HRL) [3]. In HRL, different agents use their own sets of feature weights. The concept of using different feature weights in multiple stages was also mentioned in the work [6] to evaluate game states and select different features according to different game stages, but not for reinforcement learning. In our method, different feature weights are trained and used in different stages to improve the original TD learning.\nThe method using a simple 3-stage strategy for 2048 is illustrated as follows. We divide the process into three stages with two splitting times, marked as \uD835\uDC4716\uD835\uDC58 and \uD835\uDC4716+8\uD835\uDC58, in games. \uD835\uDC4716\uD835\uDC58 denotes the first time when a 16384-tile is created on the board, and \uD835\uDC4716+8\uD835\uDC58 denotes the first time when both 16384-tile and 8192-tile are created.\n5 Our n-tuple networks contain 32768-tiles, while those in [22] do not.\nThe learning process in the three stages is described respectively as follows. In the first stage, use TD learning to train the feature weights for millions of training games, until the learning is saturated, namely the average scores in every 1,000 games gradually stabilize without further significant improvement. Usually, we train a certain number of games, say 5 million games, to saturation. The set of trained feature weights are called Stage-1 feature weights. After saturation, the program keeps playing games to collect a number of boards, say 100,000 samples, at \uD835\uDC4716\uD835\uDC58 , and their scores, which become the initial boards and scores for the training in the next stage. Note that Stage-1 feature weights remain unchanged during collection.\nIn the second stage, use TD learning to train another new set of feature weights starting from these collected boards in the first stage. The collected boards are repeatedly used in a round-robin manner until millions of training games are trained. After finishing the training, the set of trained feature weights, called Stage-2 feature weights, are saved. Then, collect boards at \uD835\uDC4716+8\uD835\uDC58 for the third stage in a similar way as the first stage. In the third stage, use TD learning to train another new set of feature weights starting from those collected boards in the second stage. Again, the set of trained feature weights, called Stage-3 feature weights, are saved. For simplicity, feature weights are all initialized to be zero.\nWhen playing a game, we also divide the process into three stages in the following way. 1. Before \uD835\uDC4716\uD835\uDC58, use Stage-1 feature weights to play. 2. After \uD835\uDC4716\uD835\uDC58 and before \uD835\uDC4716+8\uD835\uDC58, use Stage-2 feature weights\nto play.\n3. After \uD835\uDC4716+8\uD835\uDC58, use Stage-3 feature weights to play.\nThe idea behind using more stages is to make learning more accurate for all states during the second and the third stages, based on the following observation. The feature weights learned from the first stage (the same as the original TD learning) tend to perform well in the first stage, but may not in the rest of stages. In these stages, large tiles such as 16384-tile and 8192-tile increase the difficulty of playing the game, since these large tiles are more difficult to be merged into a larger one. Therefore, the feature weights may not accurately reflect the expected scores with the difficulty. Hence, we use different sets of feature weights in different stages in order to make the feature weights reflect the difficulty in the expected scores.\nThe effectiveness of MS-TD learning is justified in the experiments in Section IV and Section V, with significant improvements for 2048 and Threes."
    }, {
      "heading" : "IV. EXPERIMENTS FOR 2048",
      "text" : "In this section, experiments are done to analyze the performances of MS-TD learning for 2048, on machines equipped with AMD Opteron 6174 x4, 128GB RAM, Linux. First, in Subsection IV.A, we modify the n-tuple network from that by Szubert and Jaśkowski [22] for subsequent experiments. Second, experiments for the above simple 3-stage strategy (as illustrated in Section III) are done and analyzed in Subsection IV.B, and expectimax search for the strategy is described in Subsection IV.C. More splitting strategies are experimented and analyzed in Subsection IV.D. Subsection IV.E discusses more methods to further improve MS-TD learning."
    }, {
      "heading" : "A. New N-Tuple Network and Feature",
      "text" : "In our experiments, we used (i) the tuples shown in Fig. 4 (b) as well as all of their rotated and mirrored tuples, and (ii) large-tile features, which is a set of features representing all the combinations of large tiles, namely \uD835\uDC63-tiles, where \uD835\uDC63 ≥ 2048. Namely, the large-tile feature for one game state is indexed by the tuple: (\uD835\uDC5B2048, \uD835\uDC5B4096, \uD835\uDC5B8\uD835\uDC58, \uD835\uDC5B16\uD835\uDC58, \uD835\uDC5B32\uD835\uDC58), denoting the numbers of 2048-tile(s), 4096-tile(s), 8192-tile(s), 16384-tile(s) and 32768-tile(s), respectively. The features in (ii) were used to indicate difficulty due to large tiles. In our implementation, a state is evaluated as the sum of all occurring features’ weights. We tried all combinations for both (i) and (ii). The n-tuple network with both (i) and (ii) outperformed others in terms of both average and maximum scores, as shown in Fig. 5 and Fig. 6 respectively. Note that the step sizes for both were set to \uD835\uDEFC = 0.0025, the same as that in [22]. In these figures, the number of training games is 2 millions and average/maximum scores in y-axis are sampled every 1,000 games. For simplicity of analysis, we use the n-tuple network as shown in Fig. 4 (b) and additional features described above in the rest of this paper."
    }, {
      "heading" : "B. MS-TD Learning for the Simple 3-Stage Strategy",
      "text" : "In the experiment for MS-TD learning, 5 million training games were run in each stage, and 100,000 game boards were collected for the next stage. All feature weights of each stage were initialized to zero.\nFig. 7 (below) shows the learning curves of average scores (sampled every 100,000 games) in the three stages. The curve for Stage 1 is depicted like Fig. 5. For fair comparison, we used the following method to depict other curves. For illustration, we first consider the curve for Stage 2. In Stage 2, we trained games starting from the splitting points (\uD835\uDC4716\uD835\uDC58) of the 100,000 games\ncollected from Stage 1. The scores for games trained in Stage 2 included the scores accumulated up to \uD835\uDC4716\uD835\uDC58 in Stage 1. To fairly compare the performances of Stage-1 feature weights with Stage-2 feature weights, we calculated the average score of the 100,000 collected games (in Stage 1), which included all scores after \uD835\uDC4716\uD835\uDC58. The average score, shown as a (blue) dashed line in Fig. 7, is called the normalized score of Stage 1 with respect to Stage 2, or called the normalized 1-2 score. The comparison between the curve of Stage 2 and the normalized score is fair in the sense that both include all the scores accumulated before and after T16k. The curve for Stage 3 and the normalized 2-3 score were derived similarly.\nIn Fig. 7, the learning curve of Stage 2 grows higher than normalized 1-2 after about two million games. Similarly, the learning curve of Stage 3 grows higher than normalized 2-3 after about 1.5 million games. The results showed that the learning in the second stage did slightly improve over the first stage in terms of average scores, and the same for the third over the second.\nFig. 8 shows the curves of maximum scores (sampled every 100,000 games) in the three stages. For fair comparison, we obtained maximum scores in the second and the third stages as follows. For example, in the experiments, if 30% of games in the first stage can reach the second, the maximum scores for Stage 2 were only retrieved from 30,000 randomly selected games of every 100,000 games, namely the first 30,000 games in this paper. The maximum scores for the third stage were obtained similarly. In this figure, the curve for the third stage does go up to 500,000 often, which actually indicates to reach 32768-tiles.\nIn contrast, the curves for the first and the second stages rarely reach 500,000 points. This demonstrated that maximum scores can be significantly improved by using MS-TD learning."
    }, {
      "heading" : "C. MS-TD Learning Together with Expectimax Search",
      "text" : "Expectimax search fits afterstates evaluation well. As described in Subsection II.B, max nodes are the states, where players are allowed to move, and chance nodes are the afterstates, where new tiles are generated after moves. For TD learning, the learned afterstate values can be used as the heuristic values of leaves. Thus, choosing the maximum afterstate values can be viewed as 1-ply expectimax search. Fig. 2 shows an example of 2-ply expectimax search.\nTable 1 shows the results of running 10,000 games for the original TD learning for 1-ply to 3-ply expectimax search, respectively, while Table 2 shows those for MS-TD learning with three stages. Maximum scores in the two tables were\nderived as follows. First, calculate the maximum score for every 100 games. Then, derive the average score of these maximum scores. Besides, all the data in the two tables include errors with 95% confidence intervals. Speeds are the total numbers of moves divided by the total time of generating moves in 10,000 games. For fairer comparison, we ran 15 million training games for TD learning, while running 5 million training games for each of the three stages of MS-TD learning.\nFrom the two tables, when incorporating expectimax search, the performance for MS-TD learning was clearly improved in terms of the 32768-tile reaching rates and the maximum scores. Particularly, with 3-ply search, MS-TD learning significantly improved the maximum score from 374,458 to 599,645, and the reaching rate of 32768-tiles from 0% to 13.78%. Generally, the performance was better for deeper search, but the computation time was longer.\nOne may notice that for MS-TD learning the reaching rates for smaller \uD835\uDC63-tiles, where \uD835\uDC63 ≤ 16384, were lower than those for the original TD learning. The reason is: before \uD835\uDC4716\uD835\uDC58, the original TD learning trained 15 million games, much larger than 5 million training games by MS-TD learning. In addition, the speed for the program trained from the original TD learning was slightly faster than that from MS-TD, since MS-TD used more tables of feature weights that incurred some overhead."
    }, {
      "heading" : "D. More Splitting Strategies",
      "text" : "In this subsection, we investigate the issue of splitting strategies for MS-TD learning. Let \uD835\uDC478\uD835\uDC58 be the first time when an 8192-tile is created, \uD835\uDC4716+8+4\uD835\uDC58 the first time when a 16384-tile, an 8192-tile and a 4096-tile are created, and similarly for \uD835\uDC4716+8+4+2\uD835\uDC58 and \uD835\uDC4716+8+4+2+1\uD835\uDC58 . We tried the following four more splitting strategies. 1. Split into four stages at times \uD835\uDC478\uD835\uDC58, \uD835\uDC4716\uD835\uDC58, and \uD835\uDC4716+8\uD835\uDC58. 2. Split into four stages at times \uD835\uDC4716\uD835\uDC58, \uD835\uDC4716+8\uD835\uDC58, and \uD835\uDC4716+8+4\uD835\uDC58. 3. Split into five stages at times \uD835\uDC4716\uD835\uDC58, \uD835\uDC4716+8\uD835\uDC58, \uD835\uDC4716+8+4\uD835\uDC58, and\n\uD835\uDC4716+8+4+2\uD835\uDC58. 4. Split into six stages at times \uD835\uDC4716\uD835\uDC58 , \uD835\uDC4716+8\uD835\uDC58 , \uD835\uDC4716+8+4\uD835\uDC58 ,\n\uD835\uDC4716+8+4+2\uD835\uDC58, and \uD835\uDC4716+8+4+2+1\uD835\uDC58.\nFig. 10. Maximum scores in 2048 for Strategy 1.\nStrategy 1 adds one more splitting time at \uD835\uDC478\uD835\uDC58 to the above simple 3-stage strategy. Fig. 9 and Fig. 10 show its learning curves for the average scores and maximum scores, respectively. Similarly, both show the improvement, especially for the maximum scores. However, when compared with Fig. 7 and Fig. 8, Fig. 9 and Fig. 10 show that Strategy 1 did not perform better, that is, adding one more splitting at \uD835\uDC478\uD835\uDC58 did not help. Thus, the other three strategies did not split at \uD835\uDC478\uD835\uDC58.\n0\n100\n200\n300\n400\n500\n600\n0 10 20 30 40 50\nStage 1 Stage 2 Stage 3 Stage 4\nM ax\nm u\nm s\nco re\n( x\n1 ,0\n0 0\n)\nTraining games ( x 100,000)\nStrategy 2 adds one more at \uD835\uDC4716+8+4\uD835\uDC58 to the above simple 3-stage strategy, Strategy 3 adds one more at \uD835\uDC4716+8+4+2\uD835\uDC58 to Strategy 2, and Strategy 4 adds one more at \uD835\uDC4716+8+4+2+1\uD835\uDC58 to Strategy 3. For the three strategies, the average scores are shown in Fig. 11, Fig. 13 and Fig. 15, respectively, and the maximum scores are shown in Fig. 12, Fig. 14 and Fig. 16, respectively. These figures show that MS-TD learning indeed improved the performances. Particularly, the average scores in Strategy 4 were close to 450,000. And the maximum scores in Strategy 3 and Strategy 4 reached 500,000 very often and some were close to 600,000.\nTable 3 shows the experiments of playing 10,000 games using 3-ply expectimax search for the above four strategies. Note that the reaching rates of 2048-tile to 8192-tile are not shown since they are all the same. First, in Strategy 1, adding \uD835\uDC478\uD835\uDC58 did not improve much in 3-ply expectimax search either. Second, Strategy 4 was the best in terms of average score, while Strategy 3 was the best in terms of maximum score and 32768-tile reaching rate. The 32768-tile reaching rate of Strategy 3 went up to 18.31% but became 16.82% in Strategy 4. This showed that the benefit of splitting extra stage was diminishing. In any case, the results showed that the 32768-tile reaching rates of using MS-TD learning were much higher than that for TD, which was 0% in Table 1. To sum up, the version with Strategy 3 performed the best among all the splitting strategies."
    }, {
      "heading" : "E. Further Improvements",
      "text" : "In general, we can further improve the performance in the following ways. (a) Add more features. (b) Adjust the step-size parameter \uD835\uDEFC to a smaller value in order to make MS-TD learning more accurate. (c) Use TD(\uD835\uDF06) instead of TD(0) for each stage.\nIn (a), we added features, including the number of empty tiles, the number of distinct tiles, the number of the pairs of mergeable tiles and the number of pairs of neighboring cells with \uD835\uDC63-tile and 2\uD835\uDC63-tile. Again, a state is evaluated as the sum of all features’ weights. In (b), we adjusted \uD835\uDEFC = 0.00025 after the improvement is saturated for \uD835\uDEFC = 0.0025 . We consider the learning is saturated if the average scores of 1,000 games do not make significant improvement as described in Section III. Finally in (c), we used a variant of TD( \uD835\uDF06 ), \uD835\uDF06 = 0.5 , and consider only up to the five-step returns (from \uD835\uDC60\uD835\uDC61+1 to \uD835\uDC60\uD835\uDC61+5) instead of all returns (from \uD835\uDC60\uD835\uDC61+1 to \uD835\uDC60\uD835\uDC47):\n\uD835\uDC45\uD835\uDC61 \uD835\uDF06 = 0.5\uD835\uDC45\uD835\uDC61 1 + 0.25\uD835\uDC45\uD835\uDC61 2 + 0.125\uD835\uDC45\uD835\uDC61 3 + 0.0625\uD835\uDC45\uD835\uDC61 4\n+ 0.0625\uD835\uDC45\uD835\uDC61 5.\n(6)\nIn Formula (6), the sum of the weights of \uD835\uDC5B-step returns (\uD835\uDC5B =1 to 5) is still 1. We used the variant without eligibility traces [20][21] for the following reasons. First, using eligibility traces needs extra effort to deal with the update of sparse feature vectors, especially for those n-tuple networks that represent millions of features. Second, it often takes thousands of moves to finish a game, thus updating from all returns is hence less efficient. For the above reasons, we simply recorded all the state sequences for whole games, and performed updates after the games finished. We leave it open for better variants of TD(\uD835\uDF06) [12][28].\nWe experimented incrementally with the first stage training; that is, added (a), (b), and then (c) one at a time to demonstrate the improvement with these three techniques. These improvements are shown in Fig. 17, and can be summarized as follows. First, (a) improved the training in terms of average score. Second, while (a) was applied, (b) helped to raise the average score significantly. Third, (c) helped reach to a higher average score after (b) was applied. Hence, we applied Strategy 3 (splitting into 5 stages) MS-TD to the above three techniques along with some other fine tunes, and experimented with 3-ply expectimax search. We obtained the following results for this new version in 10,000 games: a 32768-tile reaching rate of 31.75%, the average score 443,526, the maximum score 793,835, and a speed of 500 moves/sec. Note that the maximum score here is the average of maximum scores as described in Subsection IV.C. Interestingly, one among these games even reached a 65536-tile, the first ever reaching a 65536-tile to our knowledge. This version is available openly at [27].\nIn the past, the one developed by nneonneo and xificurk [14] obtained competitive results to ours. We ran their program for 2,000 games and obtained a 32768-tile reaching rate of 28.25%, the average score 432,557, and the maximum score 814,759 (calculated in the same way as above). However, since their program heavily relied on deep search with tuned heuristics, it ran slowly, about 4 moves/sec, 125 times slower than ours."
    }, {
      "heading" : "V. EXPERIMENTS FOR THREES",
      "text" : "The experiments for Threes were similar to those for 2048. We used the same n-tuple network in Fig. 4 (b) and the features including those mentioned in Section IV.A, and some specific features for Threes, such as the hint tiles. Experiments for a simple 3-stage strategy splitting at \uD835\uDC471536 and \uD835\uDC473072 are shown and analyzed in Subsection V.A. Then, more splitting strategies and further improvements are experimented and analyzed in Subsection V.B."
    }, {
      "heading" : "A. MS-TD Learning for a Simple 3-Stage Strategy",
      "text" : "In our experiments for Threes, similar to those for 2048, 5 million training games were run in each stage, and average scores and maximum scores were sampled every 100,000 games. Fig. 18 and Fig. 19 depict the learning curves of average scores and maximum scores in the three stages, respectively. The curves are depicted in the same way as those described in Subsection IV.B. Both figures show that the learning in Stage 2 and Stage 3 did improve, especially for maximum scores. In Fig. 19, the curve for Stage 3 does appear more often around 250,000 than Stage 1 and Stage 2.\nTable 4 (below) shows the results of running 10,000 games for the original TD learning together with 1-ply to 3-ply expectimax search, respectively, while Table 5 shows those for MS-TD learning with the simple 3-stage strategy. For simplicity, both tables for reaching rates only include those of large \uD835\uDC63-tiles, namely 3072-tile and 6144-tile.\nThe results showed that the performance of MS-TD learning clearly outperformed that of the original TD learning. Especially, when using 3-ply expectimax search, the one with MS-TD learning significantly improved the 6144-tile reaching rates from 0.45% to 7.83%. For this one, its average score and maximum score were 220,393 and 700,181, respectively.\nSimilar to the results in 2048, the speed for the program trained from the original TD learning was slightly faster since some overheads were incurred upon accessing multiple tables of feature weights for MS-TD learning."
    }, {
      "heading" : "B. More Splitting Strategies",
      "text" : "In this subsection, different splitting strategies for Threes are studied first. Strategy 1 adds one more splitting time at \uD835\uDC47768 to the above simple 3-stage strategy. Fig. 20 and Fig. 21 (below) show its learning curves for the average scores and maximum scores, respectively. Strategy 2 adds one more splitting time at \uD835\uDC473072+1536, instead. Fig. 22 and Fig. 23 show its learning curves for the average scores and maximum scores, respectively. Table 6 (below) shows the experimental results of playing 10,000 games using 3-ply expectimax search for the above two strategies.\nFig. 20 to Fig. 23 demonstrate the improvements of MS-TD learning for the learning curves. However, when incorporated into expectimax search, these strategies did not perform better compared with the simple 3-stage strategy, especially in terms\nof 6144-tile reaching rates, shown in Table 6. This also showed that the benefit of splitting extra stages was diminishing, as observed in 2048. From above, the version with the simple 3-stage strategy performed the best among all the splitting strategies. This version is available openly at [7].\nWe further applied the techniques (a), (b) and (c) in IV.E to Threes. Fig. 24 shows the improvement over un-tuned TD(0) training during the first stage. We extended this version with the simple 3-stage strategy, and obtained the following results in 10,000 games: a 6144-tile reaching rate of 8.91%, the average score 234,490, the maximum score 709,712, and a speed of 288 moves/sec."
    }, {
      "heading" : "VI. CONCLUSIONS",
      "text" : "This paper proposes multi-stage TD (MS-TD) learning, a kind of hierarchical reinforcement learning method, which improves the performance of 2048-like games effectively, especially for maximum scores and the reaching rates of large tiles. Our experiments in Sections IV and V demonstrated significant improvements when using MS-TD learning for both 2048 and Threes. For 2048, when 3-ply expectimax search was used, the 32768-tile reaching rate of using MS-TD learning with Strategy 3 was 18.31%, much higher than that for TD, which was 0%. After further improvements, our 2048 program reached 32768-tiles with a probability of 31.75% in 10,000 games and even reached a 65536-tile, the first ever reaching a 65536-tile to our knowledge. For Threes, when 3-ply expectimax search was used, the 6144-tile reaching rate of using MS-TD learning with the simple 3-stage strategy was 7.83%, much higher than that for TD, which was 0.45%.\nApparently, MS-TD learning can be easily applied to other 2048-like games. It is interesting and still open whether the MS-TD method can be combined with better variants of TD(\uD835\uDF06) learning, applied to other non-deterministic games and effectively used with expectimax search."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This research was supported by NOVATEK Fellowship, Ministry of Science and Technology of the Republic of China (Taiwan) under the contract numbers MOST 104-2221-E-009-127-MY2 and 104-2221-E-009-074-MY2, and the National Center for High-performance Computing (NCHC) for computer time and facilities."
    } ],
    "references" : [ {
      "title" : "and W",
      "author" : [ "S. Bagheri", "M. Thill", "P. Koch" ],
      "venue" : "Konen, “Online adaptable learning rates for the game Connect-4,” IEEE Trans. on Computational Intelligence and AI in Games, vol. PP, no. 99, pp. 1–1",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The *-minimax search procedure for trees containing chance nodes,",
      "author" : [ "B.W. Ballard" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1983
    }, {
      "title" : "Recent advances in hierarchical reinforcement learning,",
      "author" : [ "A.G. Barto", "S. Mahadevan" ],
      "venue" : "Discrete Event Dynamic Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Learning to play chess using temporal differences,",
      "author" : [ "J. Baxter", "A. Tridgell", "L. Weaver" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "First results from using temporal difference learning in Shogi,",
      "author" : [ "D.F. Beal", "M.C. Smith" ],
      "venue" : "Proceedings of the First International Conference on Computers and Games (CG’98),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Experiments with multi-probcut and a new high-quality evaluation function for Othello,",
      "author" : [ "M. Buro" ],
      "venue" : "Proceedings of a Workshop on Game-Tree Search: Games in AI Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Systematic n-tuple networks for position evaluation: Exceeding 90% in the Othello league,",
      "author" : [ "W. Jaśkowski" ],
      "venue" : "http://arxiv.org/abs/1406.1509, Poznan University of Technology, Tech. Rep. RA-06/2014,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "An analysis of alpha-beta pruning,",
      "author" : [ "D.E. Knuth", "R.W. Moore" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1975
    }, {
      "title" : "Learning to play Othello with n-tuple systems,",
      "author" : [ "S.M. Lucas" ],
      "venue" : "Australian Journal of Intelligent Information Processing,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Fast Online Q(λ),",
      "author" : [ "W. Marco", "S. Jürgen" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1998
    }, {
      "title" : "Optimal strategy in games with chance nodes,",
      "author" : [ "E. Melkó", "B. Nagy" ],
      "venue" : "Acta Cybernetica,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "The solution for the branching factor of the alpha-beta pruning algorithm and its optimality,",
      "author" : [ "J. Pearl" ],
      "venue" : "Communications of ACM,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1982
    }, {
      "title" : "Temporal difference learning applied to a high-performance game-playing program,",
      "author" : [ "J. Schaeffer", "M. Hlynka", "V. Jussila" ],
      "venue" : "Proceedings of the 17th International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2001
    }, {
      "title" : "Reinforcement learning and simulation-based search in computer Go,",
      "author" : [ "D. Silver" ],
      "venue" : "Ph.D. dissertation, Dept. Comput. Sci., Univ. Alberta,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Learning to predict by the methods of temporal differences,",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1988
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "Cambridge, MA, USA: MIT Press",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "TD-Gammon, a self-teaching Backgammon program, achieves master-level play,",
      "author" : [ "G. Tesauro" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1994
    }, {
      "title" : "Temporal difference learning with eligibility traces for the game Connect-4,",
      "author" : [ "M. Thill", "S. Bagheri", "P. Koch", "W. Konen" ],
      "venue" : "IEEE Conference on Computational Intelligence and Games (CIG),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Reinforcement learning with n-tuples on the game Connect-4,",
      "author" : [ "M. Thill", "P. Koch", "W. Konen" ],
      "venue" : "Proceedings of the 12th International Conference on Parallel Problem Solving from Nature - Volume Part I",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Temporal difference learning in Chinese chess,",
      "author" : [ "T.B. Trinh", "A.S. Bashi", "N. Deshpande" ],
      "venue" : "Proceedings of the 11th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems: Tasks and Methods in Applied Artificial Intelligence",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1998
    }, {
      "title" : "Temporal difference learning for Connect6,",
      "author" : [ "I.-C. Wu", "H.-T. Tsai", "H.-H. Lin", "Y.-S. Lin", "C.-M. Chang", "P.-H. Lin" ],
      "venue" : "Proceedings of the 13th International Conference on Advances in Computer Games (ACG),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Multi-stage temporal difference learning for 2048,",
      "author" : [ "I.-C. Wu", "K.-H. Yeh", "C.-C. Liang", "C.-C. Chang", "H. Chiang" ],
      "venue" : "Proceedings of the 19th International Conference on Technologies and Applications of Artificial Intelligence (TAAI),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "com/threes were alpha-beta search [10][16][19], a traditional game search method for two-player games, and expectimax search [2][13][19], a common game search method for single-player stochastic games.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "com/threes were alpha-beta search [10][16][19], a traditional game search method for two-player games, and expectimax search [2][13][19], a common game search method for single-player stochastic games.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "com/threes were alpha-beta search [10][16][19], a traditional game search method for two-player games, and expectimax search [2][13][19], a common game search method for single-player stochastic games.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "com/threes were alpha-beta search [10][16][19], a traditional game search method for two-player games, and expectimax search [2][13][19], a common game search method for single-player stochastic games.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "Note that the preliminary version of this paper [32] did not include the following: MS-TD applied to Threes, more splitting strategies for both 2048 and Threes, and a demonstration of combining MS-TD with other techniques to improve 2048 and Threes programs.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "A common game tree search algorithm used in 2048 programs is expectimax search [2][13][19].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "A common game tree search algorithm used in 2048 programs is expectimax search [2][13][19].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "Reinforcement learning is an important technique in training an agent to learn how to respond to a given environment [21].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "Temporal difference (TD) learning [20][21], a kind of reinforcement learning, is a method for adjusting state values from the subsequent evaluations.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "Temporal difference (TD) learning [20][21], a kind of reinforcement learning, is a method for adjusting state values from the subsequent evaluations.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 8,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 17,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 18,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 19,
      "context" : "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 12,
      "context" : ", Chinook [17], and TD-Gammon [24].",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 16,
      "context" : ", Chinook [17], and TD-Gammon [24].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "Note that in [21] V(st+1) is weighted by a discount factor, which is ignored in this paper for simplicity.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 14,
      "context" : "For the general TD(λ) as in [20][21][31], we adopted the forward view of value difference as mentioned (c.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "For the general TD(λ) as in [20][21][31], we adopted the forward view of value difference as mentioned (c.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "For the general TD(λ) as in [20][21][31], we adopted the forward view of value difference as mentioned (c.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : "2 of [21]):",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 20,
      "context" : "The function is usually modified into a linear combination of features [31] for TD learning, that is, V(s) = φ(s) ∙ θ, where φ(s) denotes a vector of feature occurrences in s, and θ denotes a vector of feature weights.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 18,
      "context" : "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : ", reaching 8192-tiles or 16384-tiles, as in hierarchical reinforcement learning (HRL) [3].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "The concept of using different feature weights in multiple stages was also mentioned in the work [6] to evaluate game states and select different features according to different game stages, but not for reinforcement learning.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "We used the variant without eligibility traces [20][21] for the following reasons.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "We used the variant without eligibility traces [20][21] for the following reasons.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 9,
      "context" : "We leave it open for better variants of TD(λ) [12][28].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "We leave it open for better variants of TD(λ) [12][28].",
      "startOffset" : 50,
      "endOffset" : 54
    } ],
    "year" : 2016,
    "abstractText" : "Szubert and Jaśkowski successfully used temporal difference (TD) learning together with n-tuple networks for playing the game 2048. However, we observed a phenomenon that the programs based on TD learning still hardly reach large tiles. In this paper, we propose multi-stage TD (MS-TD) learning, a kind of hierarchical reinforcement learning method, to effectively improve the performance for the rates of reaching large tiles, which are good metrics to analyze the strength of 2048 programs. Our experiments showed significant improvements over the one without using MS-TD learning. Namely, using 3-ply expectimax search, the program with MS-TD learning reached 32768-tiles with a rate of 18.31%, while the one with TD learning did not reach any. After further tuned, our 2048 program reached 32768-tiles with a rate of 31.75% in 10,000 games, and one among these games even reached a 65536-tile, which is the first ever reaching a 65536-tile to our knowledge. In addition, MS-TD learning method can be easily applied to other 2048-like games, such as Threes. Based on MS-TD learning, our experiments for Threes also demonstrated similar performance improvement, where the program with MS-TD learning reached 6144-tiles with a rate of 7.83%, while the one with TD learning only reached 0.45%. Keywords—Stochastic Puzzle Game, 2048, Threes, Temporal Difference Learning, Expectimax.",
    "creator" : "Microsoft® Word 2013"
  }
}