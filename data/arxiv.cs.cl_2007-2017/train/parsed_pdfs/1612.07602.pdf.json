{
  "name" : "1612.07602.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Jointly Extracting Relations with Class Ties via Effective Deep Ranking",
    "authors" : [ "Hai Ye", "Wenhan Chao", "Zhunchen Luo" ],
    "emails" : [ "chaowenhan}@buaa.edu.cn,", "zhunchenluo@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Relation extraction (RE) aims to classify the relations between two given named entities from natural-language text. Traditional supervised machine learning methods require numerous labeled data to work well, which brings much humanlabor at the same time. With the rapid growth of volume of relation types, traditional methods can not keep up with the step for the limitation of labeled data. In order to narrow down the gap of data sparsity, [Mintz et al., 2009] propose distant supervision(DS) for relation extraction, which automatically generates training data by aligning a knowledge facts database (ie. freebase [Bollacker et al., 2008] ) with texts.\nClass ties means the connections between relations of one entity tuple in DS based relation extraction. In general, we conclude that class ties can have two types: weak class ties and strong class ties. Weak class ties mainly involve the cooccurence of relations such as place of birth and place lived, CEO of and founder of. On the contrary, strong class ties mean relations have latent logic entailments. Take the two\nrelations of capital of and city of for example, if one entity tuple has the relation of capital of, it must express the relation fact of city of, because the two relations have the entailment of capital of ⇒ city of. Obviously the opposite induction is not correct. Further take the following sentence for example,\nJonbenet told me that her mother [Patsy Ramsey]e1 never left [Atlanta]e2 since she was born.\nThis sentence expresses two relations that are place of birth and place lived. However, the word “born” is a strong bios to extract place of birth, so it may not be easy to predict the relation of place lived, but if we can incorporate the weak ties between the two relations, extracting place of birth will provide evidence for prediction of place lived.\nDS based relation extraction suffers the challenge of relation overlapping which one entity tuple may have multiple relation facts ([Hoffmann et al., 2011; Surdeanu et al., 2012]). However, overlapping relations may accompany the property of class ties among them. In the widely used dataset of [Riedel et al., 2010], we find that 98.9% of entity tuples with overlapping relations have a characteristic that their relations derive from a same root class and most of them have weak class ties, so incorporating class ties is necessary for relation extraction.\nHowever, most previous works on DS based relation extraction ignore to leverage the property of class ties between relations. [Hoffmann et al., 2011; Surdeanu et al., 2012] propose a multi-instance multi-label learning framework to jointly extract relations, but they only incorporate information from sentences and can not model relation ties. [Han and Sun, 2016] use rule-based Markov logic model to capture consistency between relation labels, but it is hard for human to define exact rules instead bringing errors to extraction.\nDifferent from [Han and Sun, 2016] using defined rules, we propose a general pairwise ranking framework under two variants combined with a convolutional neural network (CNN) to automatically learn the connections between relations by joint extraction. In this paper we focus on exploiting weak class ties. Similar to [Santos et al., 2015; Lin et al., 2016], we use class embeddings to represent relation classes. We first use CNN to embed sentences of one entity tuple and then combine the embedded sentences into one bag representation vector with two variant methods to aggregate information across sentences [Lin et al., 2016; Zheng et al., 2016]. Then we measure the similarity between\nar X\niv :1\n61 2.\n07 60\n2v 1\n[ cs\n.A I]\n2 2\nD ec\n2 01\n6\nbag representation and relation class in real-valued space. Our deep ranking aims to jointly rank positive classes higher than negative ones and in this way, the aggregated information across sentences can be propagated to all positive classes and the loss of positive classes will be propagated back to sentences, so the connections between relations can be captured. Negative sampling will bring much noise data expressing NR (not relation) to dataset. In dataset of [Riedel et al., 2010], almost training and test data is about NR. It will be really hard to train the model for data extreme imbalance ([He and Garcia, 2009]). To relieve this problem, we cut down loss propagation from NR class during training. Our experimental results on dataset of [Riedel et al., 2010] are evident that: (1) Our model is much more effective than the baselines; (2) Joint extraction for relations is better than separated extraction by leveraging class ties; (3) Our method is effective to relieve the impact of NR on training; (4) Wrong labeling problem can be primely solved by our ranking method.\nOur contributions in this paper can be encapsulated as follows: • To incorporate class ties to enhance relation extraction, we propose two novel pairwise raking loss functions combined with CNN, which can effectively learn the connections between relations by joint extraction; •Our model is effective to relieve the impact of data imbalance from NR and can primely deal with the wrong labeling problem in DS scenario. • Our method achieves state-of-the-art performance."
    }, {
      "heading" : "2 Related Work",
      "text" : "We summarize related works on two main aspects: distant supervised relation extraction and deep learning to rank."
    }, {
      "heading" : "2.1 Distant Supervised Relation Extraction",
      "text" : "Traditional supervised methods for relation extraction suffer from the problem of limited labeled data which needs much human labor to get. To overcome the sparsity of labeled data, [Mintz et al., 2009] propose distant supervision for relation extraction, which align the relation facts to texts. To relieve the wrong labeling problem, [Riedel et al., 2010] introduce multi-instance learning but can not deal with the relation overlapping problem. Afterwards, [Hoffmann et al., 2011; Surdeanu et al., 2012] model this problem by multi-instance multi-label learning to extract overlapping relations.\nRecent years, deep learning has achieved remarkable success in computer vision and natural language processing ([LeCun et al., 2015]). In relation extraction, deep learning has been applied to automatically learn the features of sentences ([Zeng et al., 2014; Zeng et al., 2015; Santos et al., 2015; Lin et al., 2016]), the specific deep learning architecture can be CNN ([Zeng et al., 2014]), RNN ([Zhou et al., 2016]), etc. [Zeng et al., 2015] propose a piecewise convolutional neural network in multi-instance learning framework for DS based relation extraction, which improves the precision and recall significantly. Afterwards, [Lin et al., 2016] introduce the mechanism of attention ([Luong et al., 2015; Bahdanau et al., 2014]) to select the sentences to relieve the wrong labeling problem and use all the information across sentences."
    }, {
      "heading" : "2.2 Deep Learning to Rank",
      "text" : "Learning to rank (LTR) is an important technique in information retrieval (IR) ([Liu, 2009]). The methods to train a LTR model includes pointwise, pairwise and listwise. We apply pairwise LTR in our paper.\nDeep learning to rank has been widely used in multi-label classification problem. In image retrieval, [Zhao et al., 2015] apply deep semantic ranking for multi-label image retrieval. In text matching, [Severyn and Moschitti, 2015] adopt learning to rank combined with deep CNN for short text pairs matching. In traditional supervised relation extraction, [Santos et al., 2015] design a pairwise loss function based on CNN for single label extraction. Based on the advantage of deep learning to rank in multi-label classification, we adopt this method in our model aiming to jointly extract multiple relations. [Zheng et al., 2016] also deal with this problem by learning to rank. However, they apply listwise LTR instead of pairwise LTR; their model is feature-based model that uses NLP tools to get the features of sentences, which will bring much noise, but we apply CNN to automatically encode the sentences."
    }, {
      "heading" : "3 Methodology",
      "text" : "We define the relation classes as L = {1, 2, · · · , C}, entity tuples as T = {ti}Mi=1 and mentions as X = {xi}Ni=1. Dataset is constructed as follows: for entity tuple ti ∈ T and its relation class sets Li ⊆ L, we collect all the mentions Xi that contain ti, the dataset we use is D = {(ti, Li, Xi)}Hi=1."
    }, {
      "heading" : "3.1 Convolutional Neural Network for Sentence Embedding",
      "text" : "Words Representations The raw sentences in format of natural-language text can not be directly encoded by the neural network, so we have transform words in sentences from raw text to real-valued vectors. Firstly, the words will be represented by pre-trained word embeddings. Then we add position embeddings to word embeddings in order to indicate the distances from words to entities in the sentence ([Zeng et al., 2014]). •Word Embedding Given a word embedding matrix V ∈ Rlw×d1 where lw is the size of word dictionary and d1 is the dimension of word embedding, the words of a mention x = {w1, w2, · · · , wn} will be represented by real-valued vectors from V . • Position Embedding [Zeng et al., 2014; Zeng et al., 2015] have shown that adding the word position embedding can improve the performance. The position embedding of a word measures the distance from the word to entities in a mention. For example, in mention “Steve Jobs is the CEO of Apple”, the distance from word CEO to entity Steve Jobs is 3 and to entity Apple is 2, then the distances are transformed into position embeddings. We apply position embeddings into words representations appending position embedding to word embedding for every word. Given a position embedding matrix P ∈ Rlp×d2 where lp is the number of distances and d2 is the dimension of position embeddings, the dimension of words representations becomes dw = d1 + d2 × 2.\nConvolution, Piecewise max-pooling Convolutional neural network is suited for encoding sentences for following two reasons. Firstly, the mechanism of pooling can embed sentences with variable lengths into fixed representations, which relieves the challenge that naturallanguage texts are usually length-variable. Secondly, applying a number of kernels will automatically capture multiple features of sentences without relying on NLP tools.\nAfter transforming words in x to real-valued vectors, we get the sentence q ∈ Rdw×n. The set of kernels K = {Ki}d s i=1 where d s is the number of kernels. Define the window size as dwin and given one kernel Kk ∈ Rd w×dwin , the convolution operation is defined as follows:\nm[i] = q[i:i+dwin−1] Kk + b[k] (1)\nwhere m is the vector after conducting convolution along q for n − dwin + 1 times and b ∈ Rds is the bias vector. For these vectors whose indexes out of range of [1, n], we replace them with zero vectors.\n[Zeng et al., 2014] propose a variable CNN called PCNN that do max pooling on segmentations of sentence not on all the sentence. When pooling, the sentence is divided into three parts: m[p0:p1], m[p1:p2] and m[p2:p3] (p1 and p2 are the positions of entities, p0 is the beginning of sentence and p3 is the end of sentence). This piecewise max-pooling is defined as follows:\nz[i] = max(m[pi−1:pi]) (2)\nwhere z ∈ R3 is the result of mention x processed by kernel Kk; 1 ≤ i ≤ 3. Given the set of kernels K, following the above steps, the mention x can be embedded to o where o ∈ Rd s∗3.\nNon-Linear Layer, Regularization To learn high-level features of mentions, we apply a nonlinear layer after pooling layer. After that, a dropout layer is applied to prevent over-fitting. We define the final fixed sentence representation as s ∈ Rdf (df = ds ∗ 3).\ns = g(o) ◦ h (3)\nwhere g(·) is a non-linear function and we use tanh(·) in this paper; h is a Bernoulli random vector with probability p to be 1."
    }, {
      "heading" : "3.2 Learning to Rank",
      "text" : "We start to introduce how to combine the information across the sentences for joint extraction then we discuss our pairwise loss ranking function.\nGiven a data (tk, Lk, Xk) ∈ {(ti, Li, Xi)}Hi=1, the sentence embeddings of Xk encoded by CNN are defined as Sk = {si}|Xk|i=1 and we use class embedding W ∈ R|L|×d to represent the relation classes.\nCombining Information across Sentences Previous works have shown that aggregating information from all the sentences will benefit relation extraction because it will provide more evidence to predict relations. We propose two options to combine information across the sentences.\n• AVE (Variant-1) The first option is average method. This method regards all the sentences equally and directly average the values in all dimensions of sentences embedding. This AVE function is defined as follows:\ns = 1\nn ∑ si∈Sk si (4)\nwhere s is the representation vector combining all sentence embeddings. The drawback of this function is that it may bring much noise data because it weights the importance of sentences equally ignoring the wrong labeling problem.\n• ATT (Variant-2) The second one is a sentence-level attention algorithm used by [Lin et al., 2016] to measure the importance of sentences aiming to relieve the wrong labeling problem. For every sentence, ATT will calculate a weight by comparing the sentence to one relation. We first calculate the similarity between one sentence embedding and relation class as follows:\nej = a ·W[c] · sj (5) where ej is the similarity between sentence embedding sj and relation class c and a is a bias factor. In this paper, we set a as 0.5. Then we apply Softmax to rescale e (e = {ei}|Xk|i=1 ) to [0, 1]. We get the weight αi for si as follows:\nαi = exp(ei)∑\nej∈e exp(ej) (6)\nso the function to merge s with ATT is as follows:\ns = |Xk|∑ i=1 αi · si (7)\nPairwise Loss Ranking • Score Function We use dot function to product score for s to be predicted as relation c. The score function is as follows:\nF(s, c) =W[c] · s (8)\nThere are other options for score function. In [Wang et al., 2016], they propose distance function that measures the similarity between s andW[c] by distance. Because score function is not an important issue in our model, we adopt dot function, also used by [Santos et al., 2015; Lin et al., 2016], as our score function.\nPairwise ranking aims to learn the score function F(s, c) that ranks positive classes higher than negative ones. This goal can be summarized as follows:\n∀c+ ∈ Lk,∀c− ∈ L − Lk : F(s, c+) > F(s, c−) + β\nwhere β is a margin factor which controls the minimum margin between the positive scores and negative scores.\nAccording to the variant options combining information across sentences, we propose two novel pairwise loss functions:\n• with AVE We define the margin-based loss function with option of AVE as follows:\nL[ave] = ∑\nc+∈Lk\nρ[0, σ+ −F(s, c+)]+\n+ρ|Lk|[0, σ− + F(s, c−)]+ (9) where [·]+ = max(0, ·); ρ is the rescale factor, σ+ is positive margin and σ− is negative margin. Similar to [Santos et al., 2015; Wang et al., 2016], this loss function is designed to rank positive classes higher than negative ones controlled by the margin of σ+ − σ−. In reality, F(s, c+) will get higher than σ+ and F(s, c−) will be lower than σ−. In our work, we set ρ as 2.5, σ+ as 3 and σ− as 1.\nSimilar to [Weston et al., 2011; Santos et al., 2015], we update one negative class at every training round but to balance the loss between positive classes and negative ones, we multiply |Lk| before the right term in function (9) to expand the negative loss. We apply mini-bach based stochastic gradient descent (SGD) to minimize the loss function. The negative class is chosen as the one with highest score among all negative classes ([Santos et al., 2015]), i.e.:\nc− = argmax c∈L−Lk F(s, c) (10)\n• with ATT Now we define the loss function for the option of ATT as follows: L[att] = ∑\nc+∈Lk\n(ρ[0, σ+ −F(sc + , c+)]+\n+ρ[0, σ− + F(sc + , c−)]+) (11) where sc means the attention weights of representation s are merged by comparing sentence embeddings with relation class c and c− is chosen by the following function:\nc− = argmax c∈L−Lk F(sc + , c) (12)\nAccording to this loss function, we can see that: for each class c+ ∈ Lk, it will capture the most related information from sentences to merge sc +\n, then rank F(sc+ , c+) higher than all negative scores which each is F(sc+ , c−) (c− ∈ L− Lk). We use the same update algorithm to minimize this loss.\nDiscussion There are two procedures to promise our model can capture the connections between relations, which one is combining information across sentences and another one is combining the loss of positive classes. From down to top, aggregating all sentence embeddings to one representation will capture enough information for joint extraction. Then the combined information is propagated to the ranking function to jointly extract relations, making all positive relations share the same information from sentences, in this way, the positive class embeddings will be jointly updated. From top to down, combining loss of all positive relations will capture the connections among relations and the connections will be backpropagated to be learned by CNN. Specifically, if relation c1 and c2 usually occur together, our model will be trained to learn the connection between them via jointly tuning CNN with their aggregated loss."
    }, {
      "heading" : "3.3 Relieving Impact of NR",
      "text" : "In relation extraction, the dataset will always contain certain negative samples which do not express relations classified as NR (not relation). In traditional supervised relation extraction, [Santos et al., 2015] propose to omit the NR class embedding to relieve the problem. In DS based supervised relation extraction, the problem is more severe. Table 2 presents the proportion of NR samples in SemEval-2010 Task 8 dataset1 ([Erk and Strapparava, 2010]) and [Riedel et al., 2010] dataset, which shows almost data is about NR in [Riedel et al., 2010] dataset. Data imbalance will severely affect the model training and cause the model only sensitive to classes with high proportion.\nIn order to relieve the impact of NR in DS based relation extraction, we cut the propagation of loss from NR, which means if relation c is NR, we set its loss as 0. Our method is similar to [Santos et al., 2015] with sight variance. [Santos et al., 2015] directly omit the NR class embedding, but we keep it. If we use ATT method to combine information across sentences, we can not omit NR class embedding according to function (6) and (7), on the contrary, it will be updated from the negative classes loss.\nIn Algorithm 1, we give out the pseudocodes of merging loss with ATT of combining sentences, jointly considering reliving the impact of NR.\nAlgorithm 1: Loss function with option ATT input : L, (tk, Lk, Xk) and Sk; output: L[att]; 1 L[att] ← 0; 2 for c+ ∈ Lk do 3 Merge representation sc +\nby function (5), (6), (7); 4 if c+ is not NR then 5 L[att] ← L[att] + ρ[0, σ+ −F(sc + , c+)]+;\n6 c− ← argmaxc∈L−Lk F(s c+ , c); 7 L[att] ← L[att] + ρ[0, σ− + F(sc + , c−)]+;\n8 return L[att];"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset and Evaluation Criteria",
      "text" : "We conduct our experiments on a widely used dataset, developed by [Riedel et al., 2010] and has been used by ([Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015;\n1This is a dataset for relation extraction in traditional supervision framework.\nLin et al., 2016]). The dataset aligns Freebase relation facts with the New York Times corpus, in which training mentions are from 2005-2006 corpus and test mentions from 2007.\nFollowing [Mintz et al., 2009], we adopt held-out evaluation framework in all experiments. Aggregate precision/recall curves are drawn and precision@N (P@N) is reported to illustrate the model performance."
    }, {
      "heading" : "4.2 Experimental Settings",
      "text" : "Word Embeddings We use a word2vec tool that is gensim2 to train word embeddings on NYT corpus. Similar to [Lin et al., 2016], we keep the words that appear more than 100 times to construct word dictionary and use ”UNK” to represent the other ones.\nHyper-parameter Settings Three-fold validation on the training dataset is adopted to tune the parameters following [Surdeanu et al., 2012]. We use grid search to determine the optimal hyper-parameters. We select word embedding size from {50, 100, 150, 200, 250, 300}. Batch size is tuned from {80, 160, 320, 640}. We determine learning rate among {0.01, 0.02, 0.03, 0.04}. The window size of convolution is tuned from {1, 3, 5}. We keep other hyper-parameters same as [Zeng et al., 2015] because they make little effect on model performance. Table 2 shows the detailed parameter settings."
    }, {
      "heading" : "4.3 Model Evaluation",
      "text" : "We evaluate our model from the following three aspects:\nImpact of Joint Extraction This experiment aims to see whether joint extraction can improve the model performance and further to reveal the ability of our model to learn class ties between relations. Because the option of aggregating sentence embeddings has little effect on achieving the goal of this experiment, we only use ATT (Variant-2) to merge s and draw the P/R curves under two settings, in which one is relieving the impact of NR and one is not. The results are shown in Figure 1, which tells us that joint extraction is better than separated extraction under two experimental settings. Joint extraction can both improve precision and recall, which in some extent proves that our model can effectively learn the class ties between relations.\nImpact of NR Relation The goal of this experiment is to inspect how much relation of NR can affect the model performance. Same as the former experiment, we only use ATT to combine sentence embeddings and report the P/R curves under the settings of joint extraction and separated extraction.\n2http://radimrehurek.com/gensim/models/word2vec.html\nThe results are shown in Figure 2, from which we can see that our method can effectively deal with the negative impact of NR. After relieving the impact of NR, our model significantly improves both precision and recall.\nThen we further evaluate the impact of NR for convergence behavior of our model. Under the setting of using AVE and applying joint extraction, in each iteration, we record the highest value of f-measure to represent the model performance at current epoch. Model parameters are tuned for 15 times and the convergence curves are shown in Figure 3. From the result, we can find out: in the first six epoches, “+NR” converges quite quicker than “-NR” and arrives to the final score at the ninth epoch. In general “-NR” converges more smoothly and will achieve better performance than “+NR” in the end.\nImpact of Combing Information across Sentences Previous work has proven combining information across all the sentences will improve the performance ([Lin et al., 2016; Zheng et al., 2016]), so we do not repeat experiment to verify this in this paper. In our approach mentioned above, we propose two variants that are AVE (Variant-1) and ATT (Variant2) to aggregate the sentence embeddings. In the work of [Lin et al., 2016], ATT (Variant-2) can improve precision and recall significantly comparing to AVE. In this experiment, we re-compare the two options aiming to know which option is better for our model. We conduct experiments under the setting of reliving impact of NR and joint extraction with vari-\nants of ATT and AVE. We draw the P/R curves and report the top N (100, 200, · · · , 500) precisions to compare the two options. Figure 4 shows the responding P/R curves and Table 3 reports the top N precisions. From the two results, we can see that ATT can not improve the model performance and even hurt the performance which does not correspond to the results in [Lin et al., 2016]. This surprising result shows sentencelevel attention does not work when combining with ranking loss functions but it instead proves our model can effectively relieve the wrong labeling problem and can make full use of information across sentences only applying a simple method (AVE) to aggregate sentences."
    }, {
      "heading" : "4.4 Model Comparison",
      "text" : "From the evaluations above, we can conclude that the best experimental setting for our model is using AVE (Variant-1) to combine information across sentences, making joint extraction and relieving NR impact. We compare our model with\nbaseline methods in the best experimental setting. Baseline We compare our model with the following baselines: • Mintz ([Mintz et al., 2009]) the original distant supervised model. •MultiR ([Hoffmann et al., 2011]) a multi-instance learning based graphical model which aims to address overlapping relation problem. • MIML ([Surdeanu et al., 2012]) also solving overlapping relations in a multi-instance multi-label framework. • PCNN+ATT ([Lin et al., 2016]) the state-of-the-art model in dataset of [Riedel et al., 2010] which applies sentence-level attention to measure contributions of sentences and combines all the mentions for training and prediction.\nWe re-implement the above baseline models by using their published codes. P/R curves of the baselines and our model are reported for comparison.\nResults and Discussion The performances of our model and the baselines are shown in Figure 5. From the results we can conclude that: (1) Our model outperforms all the baselines, achieving the best performance; (2) Comparing the neural based methods (our model and PCNN + ATT) with the traditional models, the former improves performance significantly with higher precision and higher recall, further demonstrating deep learning can effectively capture the features of sentence; (3) Comparing with PCNN + ATT, in all range of recall, our model achieves much higher precision by around 15%. Besides, our model exhibits sightly higher recall than it."
    }, {
      "heading" : "4.5 Case Study",
      "text" : "Joint and Sep. Extraction We randomly select an entity tuple (Cuyahoga County, Cleveland) from test set to see its scores for every relation class under joint extraction and separated extraction settings. This entity tuple have two relations: /location/us county/county seat and /location/location/contains, which derive from the same root class and we can regard that they have weak class ties for they all relating to topic of location. We rescale the scores by adding value 10. The results are shown in Figure 6, from which we can see that: under joint extraction setting, the two gold relations have the highest scores among the other\nrelations but under separated extraction setting, only /location/location/contains can be distinguished from the negative relations, which demonstrates that joint extraction is better than separated extraction by capturing the class ties between relations."
    }, {
      "heading" : "5 Conclusion and Feature Works",
      "text" : "In this paper, we propose two novel pairwise ranking loss functions combined with CNN to incorporate class ties by joint extraction. Besides, an effective method is introduced to relieve the impact of NR. Experimental results show that our model outperforms the baselines significantly. Besides, we find that sentence-level attention may not be suitable to combine with ranking based frameworks.\nOur model can not only be applied to relation extraction, but also can be transformed to other multi-label classification problems such as multi-category text categorization and multi-label image categorization, especially negative samples have much impact on model training and test."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al.. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "Bollacker et al", "2008] Kurt D. Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor" ],
      "venue" : "In Proceedings of KDD",
      "citeRegEx" : "al. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2008
    }, {
      "title" : "editors",
      "author" : [ "Katrin Erk", "Carlo Strapparava" ],
      "venue" : "Proceedings of SemEval. The Association for Computer Linguistics,",
      "citeRegEx" : "Erk and Strapparava. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "In Proceedings of AAAI",
      "author" : [ "Xianpei Han", "Le Sun. Global distant supervision for relation extraction" ],
      "venue" : "pages 2950–2956,",
      "citeRegEx" : "Han and Sun. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Knowl",
      "author" : [ "Haibo He", "Edwardo A. Garcia. Learning from imbalanced data. IEEE Trans" ],
      "venue" : "Data Eng., 21(9):1263–1284,",
      "citeRegEx" : "He and Garcia. 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Knowledge-based weak supervision for information extraction of overlapping relations",
      "author" : [ "Hoffmann et al", "2011] Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld" ],
      "venue" : "In Proceedings of ACLHLT,",
      "citeRegEx" : "al. et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2011
    }, {
      "title" : "Nature",
      "author" : [ "Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton. Deep learning" ],
      "venue" : "521(7553):436– 444,",
      "citeRegEx" : "LeCun et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "volume 1",
      "author" : [ "Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun. Neural relation extraction with selective attention over instances. In Proceedings of ACL" ],
      "venue" : "pages 2124–2133,",
      "citeRegEx" : "Lin et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Foundations and Trends in Information Retrieval",
      "author" : [ "Tie-Yan Liu. Learning to rank for information retrieval" ],
      "venue" : "3(3):225–331,",
      "citeRegEx" : "Liu. 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "In Proceedings of EMNLP",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning. Effective approaches to attention-based neural machine translation" ],
      "venue" : "pages 1412–1421,",
      "citeRegEx" : "Luong et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "pages 1003–1011",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky. Distant supervision for relation extraction without labeled data. In Proceedings of ACL-IJCNLP" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Mintz et al.. 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "pages 148–163",
      "author" : [ "Sebastian Riedel", "Limin Yao", "Andrew McCallum. Modeling relations", "their mentions without labeled text. In Proceedings of ECML-PKDD" ],
      "venue" : "Springer,",
      "citeRegEx" : "Riedel et al.. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Classifying relations by ranking with convolutional neural networks",
      "author" : [ "Cicero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou" ],
      "venue" : "pages 626–634,",
      "citeRegEx" : "Santos et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "pages 373–382",
      "author" : [ "Aliaksei Severyn", "Alessandro Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of SIGIR" ],
      "venue" : "ACM,",
      "citeRegEx" : "Severyn and Moschitti. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "pages 455–465",
      "author" : [ "Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning. Multiinstance multi-label learning for relation extraction. In Proceedings of EMNLP" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Surdeanu et al.. 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "In Proceedings of ACL",
      "author" : [ "Linlin Wang", "Zhu Cao", "Gerard de Melo", "Zhiyuan Liu. Relation classification via multi-level attention cnns" ],
      "venue" : "Volume 1: Long Papers,",
      "citeRegEx" : "Wang et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "WSABIE: scaling up to large vocabulary image annotation",
      "author" : [ "Jason Weston", "Samy Bengio", "Nicolas Usunier" ],
      "venue" : "Proceedings of IJCAI, pages 2764– 2770,",
      "citeRegEx" : "Weston et al.. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Relation classification",
      "author" : [ "Zeng et al", "2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "In Proceedings of EMNLP",
      "author" : [ "Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao. Distant supervision for relation extraction via piecewise convolutional neural networks" ],
      "venue" : "pages 17–21,",
      "citeRegEx" : "Zeng et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "In Proceedings of CVPR",
      "author" : [ "Fang Zhao", "Yongzhen Huang", "Liang Wang", "Tieniu Tan. Deep semantic ranking based hashing for multi-label image retrieval" ],
      "venue" : "pages 1556–1564,",
      "citeRegEx" : "Zhao et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Aggregating intersentence information to enhance relation extraction",
      "author" : [ "Hao Zheng", "Zhoujun Li", "Senzhang Wang", "Zhao Yan", "Jianshe Zhou" ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Zheng et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "In Proceeding of ACL",
      "author" : [ "Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu. Attentionbased bidirectional long short-term memory networks for relation classification" ],
      "venue" : "page 207,",
      "citeRegEx" : "Zhou et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In order to narrow down the gap of data sparsity, [Mintz et al., 2009] propose distant supervision(DS) for relation extraction, which automatically generates training data by aligning a knowledge facts database (ie.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "DS based relation extraction suffers the challenge of relation overlapping which one entity tuple may have multiple relation facts ([Hoffmann et al., 2011; Surdeanu et al., 2012]).",
      "startOffset" : 132,
      "endOffset" : 178
    }, {
      "referenceID" : 11,
      "context" : "In the widely used dataset of [Riedel et al., 2010], we find that 98.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "[Hoffmann et al., 2011; Surdeanu et al., 2012] propose a multi-instance multi-label learning framework to jointly extract relations, but they only incorporate information from sentences and can not model relation ties.",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "[Han and Sun, 2016] use rule-based Markov logic model to capture consistency between relation labels, but it is hard for human to define exact rules instead bringing errors to extraction.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "Different from [Han and Sun, 2016] using defined rules, we propose a general pairwise ranking framework under two variants combined with a convolutional neural network (CNN) to automatically learn the connections between relations by joint extraction.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "Similar to [Santos et al., 2015; Lin et al., 2016], we use class embeddings to represent relation classes.",
      "startOffset" : 11,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "Similar to [Santos et al., 2015; Lin et al., 2016], we use class embeddings to represent relation classes.",
      "startOffset" : 11,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "We first use CNN to embed sentences of one entity tuple and then combine the embedded sentences into one bag representation vector with two variant methods to aggregate information across sentences [Lin et al., 2016; Zheng et al., 2016].",
      "startOffset" : 198,
      "endOffset" : 236
    }, {
      "referenceID" : 20,
      "context" : "We first use CNN to embed sentences of one entity tuple and then combine the embedded sentences into one bag representation vector with two variant methods to aggregate information across sentences [Lin et al., 2016; Zheng et al., 2016].",
      "startOffset" : 198,
      "endOffset" : 236
    }, {
      "referenceID" : 11,
      "context" : "In dataset of [Riedel et al., 2010], almost training and test data is about NR.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "It will be really hard to train the model for data extreme imbalance ([He and Garcia, 2009]).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "Our experimental results on dataset of [Riedel et al., 2010] are evident that: (1) Our model is much more effective than the baselines; (2) Joint extraction for relations is better than separated extraction by leveraging class ties; (3) Our method is effective to relieve the impact of NR on training; (4) Wrong labeling problem can be primely solved by our ranking method.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : "To overcome the sparsity of labeled data, [Mintz et al., 2009] propose distant supervision for relation extraction, which align the relation facts to texts.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "To relieve the wrong labeling problem, [Riedel et al., 2010] introduce multi-instance learning but can not deal with the relation overlapping problem.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "Afterwards, [Hoffmann et al., 2011; Surdeanu et al., 2012] model this problem by multi-instance multi-label learning to extract overlapping relations.",
      "startOffset" : 12,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "Recent years, deep learning has achieved remarkable success in computer vision and natural language processing ([LeCun et al., 2015]).",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 18,
      "context" : "In relation extraction, deep learning has been applied to automatically learn the features of sentences ([Zeng et al., 2014; Zeng et al., 2015; Santos et al., 2015; Lin et al., 2016]), the specific deep learning architecture can be CNN ([Zeng et al.",
      "startOffset" : 105,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "In relation extraction, deep learning has been applied to automatically learn the features of sentences ([Zeng et al., 2014; Zeng et al., 2015; Santos et al., 2015; Lin et al., 2016]), the specific deep learning architecture can be CNN ([Zeng et al.",
      "startOffset" : 105,
      "endOffset" : 182
    }, {
      "referenceID" : 7,
      "context" : "In relation extraction, deep learning has been applied to automatically learn the features of sentences ([Zeng et al., 2014; Zeng et al., 2015; Santos et al., 2015; Lin et al., 2016]), the specific deep learning architecture can be CNN ([Zeng et al.",
      "startOffset" : 105,
      "endOffset" : 182
    }, {
      "referenceID" : 21,
      "context" : ", 2014]), RNN ([Zhou et al., 2016]), etc.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "[Zeng et al., 2015] propose a piecewise convolutional neural network in multi-instance learning framework for DS based relation extraction, which improves the precision and recall significantly.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "Afterwards, [Lin et al., 2016] introduce the mechanism of attention ([Luong et al.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : ", 2016] introduce the mechanism of attention ([Luong et al., 2015; Bahdanau et al., 2014]) to select the sentences to relieve the wrong labeling problem and use all the information across sentences.",
      "startOffset" : 46,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : ", 2016] introduce the mechanism of attention ([Luong et al., 2015; Bahdanau et al., 2014]) to select the sentences to relieve the wrong labeling problem and use all the information across sentences.",
      "startOffset" : 46,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "Learning to rank (LTR) is an important technique in information retrieval (IR) ([Liu, 2009]).",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "In image retrieval, [Zhao et al., 2015] apply deep semantic ranking for multi-label image retrieval.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "In text matching, [Severyn and Moschitti, 2015] adopt learning to rank combined with deep CNN for short text pairs matching.",
      "startOffset" : 18,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : "In traditional supervised relation extraction, [Santos et al., 2015] design a pairwise loss function based on CNN for single label extraction.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 20,
      "context" : "[Zheng et al., 2016] also deal with this problem by learning to rank.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 18,
      "context" : "• Position Embedding [Zeng et al., 2014; Zeng et al., 2015] have shown that adding the word position embedding can improve the performance.",
      "startOffset" : 21,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "• ATT (Variant-2) The second one is a sentence-level attention algorithm used by [Lin et al., 2016] to measure the importance of sentences aiming to relieve the wrong labeling problem.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "In [Wang et al., 2016], they propose distance function that measures the similarity between s andW[c] by distance.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "Because score function is not an important issue in our model, we adopt dot function, also used by [Santos et al., 2015; Lin et al., 2016], as our score function.",
      "startOffset" : 99,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : "Because score function is not an important issue in our model, we adopt dot function, also used by [Santos et al., 2015; Lin et al., 2016], as our score function.",
      "startOffset" : 99,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "Similar to [Santos et al., 2015; Wang et al., 2016], this loss function is designed to rank positive classes higher than negative ones controlled by the margin of σ − σ−.",
      "startOffset" : 11,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "Similar to [Santos et al., 2015; Wang et al., 2016], this loss function is designed to rank positive classes higher than negative ones controlled by the margin of σ − σ−.",
      "startOffset" : 11,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : "Similar to [Weston et al., 2011; Santos et al., 2015], we update one negative class at every training round but to balance the loss between positive classes and negative ones, we multiply |Lk| before the right term in function (9) to expand the negative loss.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "Similar to [Weston et al., 2011; Santos et al., 2015], we update one negative class at every training round but to balance the loss between positive classes and negative ones, we multiply |Lk| before the right term in function (9) to expand the negative loss.",
      "startOffset" : 11,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "The negative class is chosen as the one with highest score among all negative classes ([Santos et al., 2015]), i.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "In traditional supervised relation extraction, [Santos et al., 2015] propose to omit the NR class embedding to relieve the problem.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "Table 2 presents the proportion of NR samples in SemEval-2010 Task 8 dataset1 ([Erk and Strapparava, 2010]) and [Riedel et al.",
      "startOffset" : 79,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "Table 2 presents the proportion of NR samples in SemEval-2010 Task 8 dataset1 ([Erk and Strapparava, 2010]) and [Riedel et al., 2010] dataset, which shows almost data is about NR in [Riedel et al.",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : ", 2010] dataset, which shows almost data is about NR in [Riedel et al., 2010] dataset.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "Our method is similar to [Santos et al., 2015] with sight variance.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "[Santos et al., 2015] directly omit the NR class embedding, but we keep it.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "We conduct our experiments on a widely used dataset, developed by [Riedel et al., 2010] and has been used by ([Hoffmann et al.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "Following [Mintz et al., 2009], we adopt held-out evaluation framework in all experiments.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "Similar to [Lin et al., 2016], we keep the words that appear more than 100 times to construct word dictionary and use ”UNK” to represent the other ones.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Hyper-parameter Settings Three-fold validation on the training dataset is adopted to tune the parameters following [Surdeanu et al., 2012].",
      "startOffset" : 115,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "We keep other hyper-parameters same as [Zeng et al., 2015] because they make little effect on model performance.",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "Impact of Combing Information across Sentences Previous work has proven combining information across all the sentences will improve the performance ([Lin et al., 2016; Zheng et al., 2016]), so we do not repeat experiment to verify this in this paper.",
      "startOffset" : 149,
      "endOffset" : 187
    }, {
      "referenceID" : 20,
      "context" : "Impact of Combing Information across Sentences Previous work has proven combining information across all the sentences will improve the performance ([Lin et al., 2016; Zheng et al., 2016]), so we do not repeat experiment to verify this in this paper.",
      "startOffset" : 149,
      "endOffset" : 187
    }, {
      "referenceID" : 7,
      "context" : "In the work of [Lin et al., 2016], ATT (Variant-2) can improve precision and recall significantly comparing to AVE.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "From the two results, we can see that ATT can not improve the model performance and even hurt the performance which does not correspond to the results in [Lin et al., 2016].",
      "startOffset" : 154,
      "endOffset" : 172
    }, {
      "referenceID" : 10,
      "context" : "Baseline We compare our model with the following baselines: • Mintz ([Mintz et al., 2009]) the original distant supervised model.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "• MIML ([Surdeanu et al., 2012]) also solving overlapping relations in a multi-instance multi-label framework.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "• PCNN+ATT ([Lin et al., 2016]) the state-of-the-art model in dataset of [Riedel et al.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : ", 2016]) the state-of-the-art model in dataset of [Riedel et al., 2010] which applies sentence-level attention to measure contributions of sentences and combines all the mentions for training and prediction.",
      "startOffset" : 50,
      "endOffset" : 71
    } ],
    "year" : 2016,
    "abstractText" : "In distant supervised relation extraction, the connection between relations of one entity tuple, which we call class ties, is common. Exploiting this connection may be promising for relation extraction. However, this property is seldom considered by previous work. In this work, to leverage class ties, we propose to make joint relation extraction with a unified model that integrates convolutional neural network with a general pairwise ranking framework, in which two novel ranking loss functions are introduced. Besides, an effective method is proposed to relieve the impact of relation NR (not relation) for model training and test. Experimental results on a widely used dataset show that: (1) Our model is much more superior than the baselines, achieving state-of-the-art performance; (2) Leveraging class ties, joint extraction is indeed better than separated extraction; (3) Relieving the impact of NR will significantly boost our model performance; (4) Our model can primely deal with wrong labeling problem.",
    "creator" : "LaTeX with hyperref package"
  }
}