{
  "name" : "1611.06824.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "BUDGETED REINFORCE", "Aurélia Léon", "Ludovic Denoyer" ],
    "emails" : [ "aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Reinforcement Learning (RL) is one of the key problem in machine learning , and the interest of the research community has been recently renewed with the apparition of models mixing classical reinforcement learning techniques and deep neural networks. These new methods include for example the DQN algorithm (Mnih et al., 2015) and its variants (Van Hasselt et al., 2015), the use of recurrent architectures with policy gradient models (Wierstra et al., 2010), or even approaches like Guided Policy Search (Levine & Koltun, 2013) or actor-critic algorithms (Konda & Tsitsiklis, 1999).\nResearch in cognitive science based on the study of human or animal behavior have long emphasized that the internal policy of such agents can be seen as a hierarchical process where solving a task is obtained by sequentially solving sub-tasks, each sub-task being treated by choosing a sequence of primitive actions (Botvinick et al., 2009). In the computer science domain, these researches have been echoed during the last decade with the apparition of the hierarchical reinforcement learning paradigm (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) and its generalization to options (Sutton et al., 1999). The underlying idea is to define a policy at two different levels: a level which goal is to choose between options, and a level which will select the actions to apply to the environment based on the current option. Informally, in a maze an option can correspond to an order like go to the door, while the actions are primitive moves (up, down, left, right). In the literature, the catalog of available options is usually specified manually which is not satisfactory.\nWe propose a new architecture called BONN (Budgeted Options Neural Network) able to simultaneously discover options and to learn how and when to use them. It is based on the idea that a good policy is a trade-off between policy efficiency and cognitive effort: a system will learn relevant options if these options allow it to reduce the cognitive effort for solving the task, without decreasing the quality of the solution. This idea is implemented here through a budgeted learning problem that encourages the BONN model to learn to acquire as few information as possible. Note that BONN\n1code available here: https://github.com/aureliale/BONN-model\nar X\niv :1\n61 1.\n06 82\n4v 1\n[ cs\n.L G\n] 2\n1 N\nov 2\nrelies on a new framework called Bi-POMDP that extends POMDP2, making our model usable in both classical RL problems.\nThe contributions of the paper are: (i) We propose the Bi-POMDP framework as an extension of the POMDP framework. (ii) On top of this framework, we propose the BONN model able to discover options based on a Budgeted Reinforcement Learning problem where information acquisition has a cost, each option being a continuous vector in an learned latent option space. (iii) We propose a discrete variant of BONN (D-BONN) where a discrete set of options is learned, each option corresponding to a particular embedding in the latent option space. (iv) The model is tested on different RL tasks and exhibits interesting properties and a strong ability to capture relevant options.\nThe paper is organized as follows: we present the background in RL and on policy gradients methods in Section 2. The Bi-POMPD formalism, and the BONN model are presented in Sections 3.1 and 3.2 while the budgeted learning problem is described in Section 3.3. The variant of BONN able to extract a discrete set of options is given in Section 3.4. At last, experiments are proposed in Section 4 while the related works are presented in Section 5."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 (PO-) MARKOV DECISION PROCESSES AND REINFORCEMENT LEARNING",
      "text" : "Let us denote a MDP as a set of states S, a discrete set of possible actionsA, a transition distribution P (st+1|st, at) and a reward function r(s, a) ∈ R+. We consider that each state st is associated with an observation xt ∈ Rn, and that xt is a partial view of st (i.e POMDP), n being the size of the observation space. Moreover, we denote PI(s) the probability distribution over the possible initial states of the MDP.\nGiven a current trajectory x1, a1, x2, a2, ...., xt, a policy is defined by a probability distribution such that π(x1, a1, x2, a2, ...., xt, a) = P (a|x1, a1, x2, a2, ...., xt) which is the probability of each possible action a at time t, knowing the history of the agent."
    }, {
      "heading" : "2.2 LEARNING WITH POLICY GRADIENT",
      "text" : "Let us denote γ the discount factor, and R(s1, a1, s2, a2, ...., sT ) the discounted reward corresponding to trajectory (s1, a1, s2, a2, ...., sT ) such that:\nR(s1, a1, s2, a2, ...., sT ) = T−1∑ t=1 γt−1r(st, at)\nwith T the size of the trajectories sampled by the policy3. We can define the reinforcement learning problem as the optimization problem such that the optimal policy π∗ is computed by maximizing the expected reward J(π):\nJ(π) = Es1≈PI(s1),a1,....,aT≈π [R(s1, a1, s2, a2, ...., sT )] (1)\nwhere s1 is sampled following PI and the actions are sampled based on π.\nDifferent learning algorithms aim at maximizing J(π). In the case of policy gradient techniques, if we consider that, for sake of simplicity, π also denotes the set of parameters of the policy, the gradient of the objective can be computed as:\n∇πJ(π) = 1\nM M∑ m=1 T∑ t=1 (∇π log π(at|x1, a1, ..., xt) (R(st, at, ...., sT )− bt)) (2)\nwhere M is the number of sampled trajectories used for approximating the gradient using Monte Carlo sampling techniques and bt is a variance reduction term estimated during learning.\n2Partially Observed Markov Decision Processes 3We describe finite-horizon problems where T is the size of the horizon and γ ≤ 1, but the approach can\nalso be applied to infinite horizon problems with discount factor γ < 1"
    }, {
      "heading" : "3 BUDGETED OPTION NEURAL NETWORK",
      "text" : ""
    }, {
      "heading" : "3.1 BI-OBSERVATIONS POMDP",
      "text" : "We consider a slightly different version of Partially Observed Markov Decision Processes where an additional observation is available at each time step. In addition to xt, the agent can ask for yt which will bring to him a supplementary information that will help the agent to decide which action to choose. This generic definition corresponds to many practical cases: for example a robot that acquires information through its camera (xt) but can sometimes decide to make a complete scan of the room (yt); a user driving a car (using xt) but who decides to consult its map or GPS (yt); a virtual agent taking decisions in a virtual world (based on xt) but that can ask instructions from a human (yt), etc. Note that the Bi-POMPD generalize the POMDP case either by choosing yt = ∅ or by choosing yt = xt and xt = ∅ (see Section 4)."
    }, {
      "heading" : "3.2 THE BONN ARCHITECTURE",
      "text" : "We now describe the budgeted option neural network able to learn a policy in Bi-POMDP. This model is composed of three components. The underlying idea is that the first component will use the additional observations yt to compute which option to use, while the second component will use the basic observations xt and the lastly chosen option to sample primitive actions; the third component being used to decide when to switch between options. A new option will thus be computed each time yt is acquired. Let us now describe how each component works: (i) The first one (or option model) aims at choosing which option to apply depending on the observations yt collected over the states of the process. In our model, an option is represented by a vector denoted ot ∈ RO, O being the size of the options representation space. (ii) Choosing a new option ot will then initialize the second component (or actor model). During the next time steps, the actor model will sequentially choose actions based on observations xt and update its state until a new option is generated. (iii) The acquisition model denoted σt ∈ {0; 1} will decide if the model has to acquire yt or not. To better understand this two-levels architecture, we provide the inference pseudo-code in Algorithm 2 and the architecture of BONN in Figure 1. We now describe the resulting components (the details are given in the Appendix).\nOption model: The option model will be denoted f such that f(yt)→ ot generates an option ot as a latent vector in a latent space RO, O being the dimension of this space. Note that the option model is a deterministic model where the option is computed based on the current observation (see Appendix). Recurrent and/or stochastic versions of the option model will be studied in a future work.\nActor Model: The state of the actor model is represented by a vector zt ∈ RZ , Z being the size of the latent space of the actor model. At each time step, the distribution over the possible set\nAlgorithm 2 The pseudo code of the inference algorithm for the BONN model. 1: procedure INFERENCE(s1) . s1 is the initial state 2: initialize z0 with the empty option= (0, 0, ..., 0) ∈ RO 3: for t = 1 to T do 4: acquisition model: Compute σt based on zt−1, at−1 and xt 5: if σt == 1 then 6: option level: Acquire yt and generate a new option ot 7: actor level: Initialize the actor zt = r(ot, xt) 8: else 9: actor level: Update the actor state zt = h(zt−1, at−1, xt) 10: end if 11: actor level: Choose the action at w.r.t zt 12: Execute the chosen action 13: end for 14: end procedure\nof actions is computed by the function d such that P (at|zt) ≈ d(zt, at). Note that d is typically based on a soft-max function mapping action scores to action probabilities (see Appendix). If a new option ot is computed by the option model, then the state zt is re-initialized with zt = p(ot, xt). p is a reset function which aims at choosing the ”initial” state of the actor for each new option. If a new option is not generated, the actor state is updated with a classical recurrent mechanism i.e zt+1 = g(zt, at, xt+1)\nAcquisition Model: The acquisition model aims at deciding if a new option has to be generated. It is a stochastic process such that σt = 1 (new option) or σt = 0 (keep the same option) and is computed over the state of the actor and the new observation xt: P (σt+1 = 1) = h(zt, at, xt+1). In our case, this probability is based on a Bernoulli distribution over a sigmoid-based h function (see Appendix)."
    }, {
      "heading" : "3.3 BUDGETED LEARNING FOR OPTIONS DISCOVERY",
      "text" : "The way options emerge in a hierarchical reinforcement learning system has been the topic of many different works in both reinforcement learning and cognitive science. Most of these techniques associate the problem of option discovery with the problem of sub-goals discovery where different strategies are used to discover the sub-goals – see Botvinick et al. (2009) for a review on links between cognitive research and hierarchical reinforcement learning. The BONN model is based on a different approach, where we consider that the discovery of options will result in learning a good trade-off between policy efficiency and the cognitive effort generated by such a policy. The underlying idea is that a system will learn relevant options if these options allow to reduce the cognitive effort that is generated when solving the task, without decreasing the quality of the solution. Note that the reduction of the cognitive effort has already been studied in cognitive science (Kool & Botvinick, 2014), and very recently in the RL context (Bacon & Precup) but defined differently.\nIn the Bi-POMDP framework, the cognitive effort is associated with the acquisition of the additional information yt, this additional information (and its computation) being considered costly but crucial for discovering a good policy: an agent only using the observations xt would be unable to solve\nthe task, but using yt at each time step would be ”too expensive”. Let us denote C = T∑ t=1 σt the acquisition cost for a particular episode: by reducingC, we will encourage the agent to learn relevant options that will be used during many time steps, the model extracting relevant sub-policies. We propose to integrate the acquisition cost C (or cognitive effort) in the learning objective, relying on the budgeted learning paradigm already explored in different RL-based applications (Contardo et al., 2016; Dulac-Arnold et al., 2012). We define an augmented reward r∗ that includes the generated cost: r∗(st, at, σt) = r(st, at)− λσt (3) where λ controls the trade-off between the task efficiency and the cognitive charge. The resulting discounted reward denoted R∗ will be used as the objective to maximize instead of the classical\ndiscounted reward R, resulting in the following policy gradient update rule:\nπ ← π−γ T−1∑ t=1 (∇π logP (at|zt) +∇π logP (σt|zt−1, at−1, xt)) (R∗(st, at, σt, ...., sT )− b∗t ) (4)\nwhere γ is the learning rate. Note that this rule now updates both the probabilities of the chosen actions at, but also the probabilities of the σt that can be seen as internal actions and that decide if a new option has to be computed or not, b∗t being the new resulting variance reduction term."
    }, {
      "heading" : "3.4 DISCOVERING A DISCRETE SET OF OPTIONS",
      "text" : "In the previous sections, we considered that the option ot generated by the option model is a vector in a latent space RO. This is slightly different than the classical option definition which usually considers that an agent has a given ”catalog” of possible sub-routines i.e the set of options is a finite discrete set. We propose here a variant of the model where the model learns a finite discrete set of options.\nLet us denote K the (manually-fixed) number of options one wants to discover. Each option will be associated with a (learned) embedding denoted ok. The option model will store the different possible options and choose which one to use each time an option is required. In that case, the option model will be considered as a stochastic model able to sample one option index denoted it in {1, 2, ...,K} by using a multinomial distribution on top of a softmax computation. In that case, as the option model computes some stochastic choices, the policy gradient update rule will integrate these additional internal actions with: π ← π−γ T−1∑ t=1 (∇ logP (at|zt) +∇ logP (σt|zt−1, at−1, xt) +∇ logP (it|yt)) (R∗(st, at, σt, ...., sT )− bt) (5) By considering that P (it|yt) is computed based on a softmax over a scoring function P (it|yt) ≈ `(oit , yt) where ` is a differentiable function, the learning will update both the ` function and the options embedding ok."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "The complete details of the architecture used for the experiments are provided in the Appendix. We have tested this architecture on 3 different types of environments and compared it to a Recurrent Policy Gradient algorithm using a GRU-based neural network (R-PG):\nCartPole: This is the classical cart-pole environment as implemented in the OpenAI Gym4 platform where observations are (position, angle, speed, angularspeed), and actions are right or left. The reward is +1 for every time step without failure. For BONN, the observation is only used by the option model i.e yt = (position, angle, speed, angularspeed), the actor model receiving an empty observation xt = ∅ at each time step. Lunar Lander: This environment corresponds to the Lunar Lander environment proposed in OpenAI Gym where observations describe the position, velocity, angle of the agent and if he is in contact with the ground or not, and actions are do nothing, fire left engine, fire main engine, fire right engine. The reward is +100 if landing, +10 for each leg on the ground, -100 if crashing and -0.3 each time the main engine is fired. As for the cart-pole, the observation is only acquired by the option model, the actor model receiving an empty observation xt = ∅ at each time step. Multi-room Maze: The Multi-room Maze is the most interesting environment since it is a BiPOMDP. It corresponds to a maze composed of k × k rooms (k = 2 or k = 3), with doors between them (see Figure 3). The agent always starts at the upper-left corner, while the goal position is chosen randomly at each episode: it can be in any room, in any position and its position changes at each episode. The reward function is -1 when moving and +20 when reaching the goal, while 4 different actions are possible: (up, down, left, right). We consider two variants: MAZE1 where xt = ∅ is the empty observation and the agent must learn when to acquired the more informative\n4https://gym.openai.com/\nobservation yt which contains the observed doors, the agent position and the goal position if the goal is in the same room than the agent (i.e the agent only observes the current room). In the MAZE2 world, xt is the agent position in the room, while yt corresponds to the description of the room and the goal position if the goal is in the same room (i.e contrary to MAZE1, the agent always observes his position). The R-PG baseline has access to all these information (doors, position, goal) at each time step. Note that this environment is much more difficult than other 4-rooms problems (introduced by Sutton et al. (1999)) in others RL works, where there is only one or two goal(s), and that, in a more realistic way, the agent only observes the room he is in.\nFor all the environments, we consider different levels of stochasticity such that the action chosen by the agent is applied with probability 1 − while a random action is chosen with probability . The higher epsilon is, the more the environment is stochastic."
    }, {
      "heading" : "4.1 QUANTITATIVE ANALYSIS",
      "text" : "We illustrate the quality of BONN with cost/reward curves (see Figure 2) where the X-axis corresponds to the number of times an option is computed (normalized w.r.t the size of the episode) while the Y-axis corresponds to the overall reward R = ∑ r(st, at) obtained on each task, for different levels of stochasticity . Note that cost/reward curves are generated by computing the Pareto front over all the learned models at different cost levels λ. These curves have been obtained by first learn-\ning our model with a zero cost λ = 0 and then by progressively increasing this cost, forcing the model to acquire yt less frequently and to discover options5.\nFirst, one can see that even at low cost values (with only a few options computed), the BONN model is able to keep the same performance than the R-PG model, even if R-PG uses all the information contained in both xt and yt at each time step. Some specific cost/reward values are given in Table 1 for different environments and different values of λ, confirming that BONN is able to keep a high performance level while discovering relevant options. Note that if the cost of computing a new option is too expensive, the BONN model is not able to find a good policy since it is not allowed to switch between options.\nWe can also see that the obtained reward decreases when the environments are more stochastic, which seems intuitive since stochasticity makes the tasks harder. Figure 4a compares the results obtained on the MAZE1 environment and the MAZE2 environment when λ = 0.25. We note that the drop of performance in MAZE2 happens at a lower cost than the one in MAZE1. Indeed, in MAZE2, the agent has access to its position at each time step and is more able to ”compensate” the stochasticiy of the environment than in the MAZE1 case, where the position is only available through yt. This experiment shows that using Bi-POMDP seems to be a good way to learn powerful hierarchical policies, even in complex environments.\nFigures 3b and 3a illustrates trajectories generated by the agent in theMAZE2 environment, and the positions where the options are generated. We can see that the agent learns to observe yt only once in each room and that the agent uses the resulting option until it reaches another room (thus the agent deducts from yt if he must move to another room, or reach the goal if it is in the current room). Note that the agent cannot find the shortest path to the goal’s room because, having no information about the position of the goal in another room, it learns to explore the maze in a ”particular” order until reaching the goal’s room. We have visualized the options latent vectors using the t-SNE algorithm (Figure 4b). The green color corresponds to options that are chosen when the agent is in the same room than the goal (and thus when the agent knows the position of the goal), and the red color when the goal is not in the same room and the agent has to explore the maze. The 4 colored clusters in this figure correspond to the options chosen for reaching one of the 4 possible doors, showing that the latent option space has captured a particular structure (one cluster for each door position). Analyzing this latent structure will be the topic of a future research.\nThe D-BONN model has been experimented on the MAZE1 2 × 2, and an example of generated trajectories is given in Figure 3c. Each color corresponds to one of the learned discrete options. One can see that the model is still able to learn a good policy, but the constraint over the fixed number of discrete options clearly decreases the quality of the obtained policy. It seems thus more interesting to use continuous options instead of discrete ones, the continuous options being regrouped in smooth clusters as illustrated in Figure 4b.\n5Learning separate models for many λ values is time-consuming and does not significantly improve the obtained results"
    }, {
      "heading" : "5 RELATED WORK",
      "text" : "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks. Many different models have been proposed where subtasks are a priori known like Dietterich (1998) which proposes the MAXQ method. The concept of option has been introduced by Sutton et al. (1999). In this architecture, each option consists of an initiation set, its own policy (over primitive actions or other options), and a termination function which defines the probability of ending the option given a certain state.\nThis concept of options is at the core of many recent articles, for example in Kulkarni et al. (2016), the Deep Q-Learning framework is extended to integrate hierarchical value functions using intrinsic motivation to learn the option policies. But in these different models, the options have to be manually chosen a priori and are not discovered during the learning process. Still in the option framework, Daniel et al. learns options (both policies and termination probabilities) without supervision using the Expectation Maximization algorithm. More recently, Bacon & Precup (2015) does the same with an architecture close to an actor-critic algorithm where options are discrete. The closest work to our seems to be Bacon & Precup but the model is also based on a discrete set of options in the POMDP framework. Note that this article also introduces the cognitive effort concept. Some models are focused on the problem of learning macro-actions (Hauskrecht et al., 1998; Mnih et al., 2016). In that case a given state is mapped to a sequence of actions (i.e macro-actions). But macro-actions are more restricted than options since the sequence of actions is fixed.\nThe BONN architecture is different from these works on two main aspects. The first one is that, in our case, options are latent vectors, the model being able to learn a manifold of possible options – even if a discrete version has been also proposed, with less convincing performance. The second difference is that our model is based on Bi-POMDP which are more general than POMDP.\nOutside reinforcement learning, our work is also in relation with the Hierarchical Multiscale Recurrent Neural Networks (Chung et al., 2016) that discover hierarchical structure in sequences."
    }, {
      "heading" : "6 CONCLUSION AND PERSPECTIVES",
      "text" : "We have proposed a new model for learning options in POMDP and in Bi-POMDP where the agent can choose to acquire a more informative observation at each time step. The model is learned in a budgeted learning setting where the acquisition of an additional information, and thus the use of a new option, has a cost. The learned policy is a trade-off between the efficiency and the cognitive effort of the agent. In our setting, the options are handled through learned latent representations, and we have also proposed a discrete version of BONN where the number of options is kept constant. Experimental results show that the model is able to extract relevant options in complex environments. This work opens different research directions. One is to study if BONN can be applied in multi-task reinforcement learning problems (the environment MAZE, since the goal position is randomly chosen at each episode, can be seen as a particular simple multitask RL problem). Another question would be to study n-POMDP where many different observations can be acquired by the agent at different costs - e.g many different sensors on a robot.\nACKNOWLEGMENTS\nThis work has been supported within the Labex SMART supported by French state funds managed by the ANR within the Investissements d’Avenir programme under reference ANR-11-LABX-65."
    } ],
    "references" : [ {
      "title" : "The option-critic architecture",
      "author" : [ "Pierre-Luc Bacon", "Doina Precup" ],
      "venue" : "In NIPS Deep Reinforcement Learning Workshop,",
      "citeRegEx" : "Bacon and Precup.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bacon and Precup.",
      "year" : 2015
    }, {
      "title" : "Hierarchically organized behavior and its neural foundations: A reinforcement-learning perspective",
      "author" : [ "Matthew Botvinick", "Yael Niv", "Andrew C. Barto" ],
      "venue" : null,
      "citeRegEx" : "Botvinick et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Botvinick et al\\.",
      "year" : 2009
    }, {
      "title" : "Hierarchical multiscale recurrent neural networks",
      "author" : [ "Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1609.01704,",
      "citeRegEx" : "Chung et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural networks for adaptive feature acquisition",
      "author" : [ "Gabriella Contardo", "Ludovic Denoyer", "Thierry Artières" ],
      "venue" : "In International Conference on Neural Information Processing,",
      "citeRegEx" : "Contardo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Contardo et al\\.",
      "year" : 2016
    }, {
      "title" : "Feudal reinforcement learning. In Advances in neural information processing systems, pp. 271–271",
      "author" : [ "Peter Dayan", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Dayan and Hinton.,? \\Q1993\\E",
      "shortCiteRegEx" : "Dayan and Hinton.",
      "year" : 1993
    }, {
      "title" : "The maxq method for hierarchical reinforcement learning",
      "author" : [ "Thomas G Dietterich" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Dietterich.,? \\Q1998\\E",
      "shortCiteRegEx" : "Dietterich.",
      "year" : 1998
    }, {
      "title" : "Sequential approaches for learning datum-wise sparse representations",
      "author" : [ "Gabriel Dulac-Arnold", "Ludovic Denoyer", "Philippe Preux", "Patrick Gallinari" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Dulac.Arnold et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dulac.Arnold et al\\.",
      "year" : 2012
    }, {
      "title" : "Hierarchical solution of markov decision processes using macro-actions",
      "author" : [ "Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier" ],
      "venue" : "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "Hauskrecht et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Hauskrecht et al\\.",
      "year" : 1998
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Actor-critic algorithms. In Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December",
      "author" : [ "Vijay R. Konda", "John N. Tsitsiklis" ],
      "venue" : null,
      "citeRegEx" : "Konda and Tsitsiklis.,? \\Q1999\\E",
      "shortCiteRegEx" : "Konda and Tsitsiklis.",
      "year" : 1999
    }, {
      "title" : "A labor/leisure tradeoff in cognitive control",
      "author" : [ "Wouter Kool", "Matthew Botvinick" ],
      "venue" : "Journal of Experimental Psychology: General,",
      "citeRegEx" : "Kool and Botvinick.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kool and Botvinick.",
      "year" : 2014
    }, {
      "title" : "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
      "author" : [ "Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum" ],
      "venue" : "arXiv preprint arXiv:1604.06057,",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Strategic attentive writer for learning macro-actions",
      "author" : [ "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1606.04695,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning with hierarchies of machines",
      "author" : [ "Ronald Parr", "Stuart Russell" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Parr and Russell.,? \\Q1998\\E",
      "shortCiteRegEx" : "Parr and Russell.",
      "year" : 1998
    }, {
      "title" : "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "Richard S Sutton", "Doina Precup", "Satinder Singh" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Deep reinforcement learning with double qlearning",
      "author" : [ "Hado Van Hasselt", "Arthur Guez", "David Silver" ],
      "venue" : "CoRR, abs/1509.06461,",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent policy gradients",
      "author" : [ "Daan Wierstra", "Alexander Förster", "Jan Peters", "Jürgen Schmidhuber" ],
      "venue" : "Logic Journal of the IGPL,",
      "citeRegEx" : "Wierstra et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Wierstra et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "These new methods include for example the DQN algorithm (Mnih et al., 2015) and its variants (Van Hasselt et al.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : ", 2015), the use of recurrent architectures with policy gradient models (Wierstra et al., 2010), or even approaches like Guided Policy Search (Levine & Koltun, 2013) or actor-critic algorithms (Konda & Tsitsiklis, 1999).",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "Research in cognitive science based on the study of human or animal behavior have long emphasized that the internal policy of such agents can be seen as a hierarchical process where solving a task is obtained by sequentially solving sub-tasks, each sub-task being treated by choosing a sequence of primitive actions (Botvinick et al., 2009).",
      "startOffset" : 316,
      "endOffset" : 340
    }, {
      "referenceID" : 5,
      "context" : "In the computer science domain, these researches have been echoed during the last decade with the apparition of the hierarchical reinforcement learning paradigm (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) and its generalization to options (Sutton et al.",
      "startOffset" : 161,
      "endOffset" : 223
    }, {
      "referenceID" : 15,
      "context" : "In the computer science domain, these researches have been echoed during the last decade with the apparition of the hierarchical reinforcement learning paradigm (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) and its generalization to options (Sutton et al., 1999).",
      "startOffset" : 258,
      "endOffset" : 279
    }, {
      "referenceID" : 1,
      "context" : "Most of these techniques associate the problem of option discovery with the problem of sub-goals discovery where different strategies are used to discover the sub-goals – see Botvinick et al. (2009) for a review on links between cognitive research and hierarchical reinforcement learning.",
      "startOffset" : 175,
      "endOffset" : 199
    }, {
      "referenceID" : 3,
      "context" : "We propose to integrate the acquisition cost C (or cognitive effort) in the learning objective, relying on the budgeted learning paradigm already explored in different RL-based applications (Contardo et al., 2016; Dulac-Arnold et al., 2012).",
      "startOffset" : 190,
      "endOffset" : 240
    }, {
      "referenceID" : 6,
      "context" : "We propose to integrate the acquisition cost C (or cognitive effort) in the learning objective, relying on the budgeted learning paradigm already explored in different RL-based applications (Contardo et al., 2016; Dulac-Arnold et al., 2012).",
      "startOffset" : 190,
      "endOffset" : 240
    }, {
      "referenceID" : 15,
      "context" : "Note that this environment is much more difficult than other 4-rooms problems (introduced by Sutton et al. (1999)) in others RL works, where there is only one or two goal(s), and that, in a more realistic way, the agent only observes the room he is in.",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks.",
      "startOffset" : 36,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "Some models are focused on the problem of learning macro-actions (Hauskrecht et al., 1998; Mnih et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Some models are focused on the problem of learning macro-actions (Hauskrecht et al., 1998; Mnih et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "Outside reinforcement learning, our work is also in relation with the Hierarchical Multiscale Recurrent Neural Networks (Chung et al., 2016) that discover hierarchical structure in sequences.",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks. Many different models have been proposed where subtasks are a priori known like Dietterich (1998) which proposes the MAXQ method.",
      "startOffset" : 59,
      "endOffset" : 387
    }, {
      "referenceID" : 4,
      "context" : "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks. Many different models have been proposed where subtasks are a priori known like Dietterich (1998) which proposes the MAXQ method. The concept of option has been introduced by Sutton et al. (1999). In this architecture, each option consists of an initiation set, its own policy (over primitive actions or other options), and a termination function which defines the probability of ending the option given a certain state.",
      "startOffset" : 59,
      "endOffset" : 485
    }, {
      "referenceID" : 4,
      "context" : "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks. Many different models have been proposed where subtasks are a priori known like Dietterich (1998) which proposes the MAXQ method. The concept of option has been introduced by Sutton et al. (1999). In this architecture, each option consists of an initiation set, its own policy (over primitive actions or other options), and a termination function which defines the probability of ending the option given a certain state. This concept of options is at the core of many recent articles, for example in Kulkarni et al. (2016), the Deep Q-Learning framework is extended to integrate hierarchical value functions using intrinsic motivation to learn the option policies.",
      "startOffset" : 59,
      "endOffset" : 812
    }, {
      "referenceID" : 4,
      "context" : "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks. Many different models have been proposed where subtasks are a priori known like Dietterich (1998) which proposes the MAXQ method. The concept of option has been introduced by Sutton et al. (1999). In this architecture, each option consists of an initiation set, its own policy (over primitive actions or other options), and a termination function which defines the probability of ending the option given a certain state. This concept of options is at the core of many recent articles, for example in Kulkarni et al. (2016), the Deep Q-Learning framework is extended to integrate hierarchical value functions using intrinsic motivation to learn the option policies. But in these different models, the options have to be manually chosen a priori and are not discovered during the learning process. Still in the option framework, Daniel et al. learns options (both policies and termination probabilities) without supervision using the Expectation Maximization algorithm. More recently, Bacon & Precup (2015) does the same with an architecture close to an actor-critic algorithm where options are discrete.",
      "startOffset" : 59,
      "endOffset" : 1294
    } ],
    "year" : 2016,
    "abstractText" : "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) 1 able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.",
    "creator" : "LaTeX with hyperref package"
  }
}