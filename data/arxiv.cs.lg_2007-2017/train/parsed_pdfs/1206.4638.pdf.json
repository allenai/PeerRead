{
  "name" : "1206.4638.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Euclidean Projections onto the Intersection of Norm Balls",
    "authors" : [ "Hao Su", "Wei Yu", "Li Fei-Fei" ],
    "emails" : [ "haosu@cs.stanford.edu", "wyu@cs.hku.hk", "feifeili@cs.stanford.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The sparse-inducing norms have been powerful tools for learning robust models with limited data in highdimensional space. By imposing such norms as the\n* Indicates equal contributions.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nconstraints to the optimization task, one could bias the model towards learning sparse solutions, which in many cases has been proven to be statistically effective. Typical sparse-inducing norms include the ℓ1 norm and ℓ1,q norm (defined in Sec 3) (Liu & Ye, 2010); the former encourages element-wise sparsity and the latter encourages group-wise sparsity. In a variety of contexts, the two types of sparsity pattern exist simultaneously. For example, in the multi-task learning setting, the index set for features of each task may be sparse and there might be a large overlap of features across multiple tasks (Jalali et al., 2010). One natural approach is to formalize an optimization problem constrained by ℓ1 norm and ℓ1,q norm together, so that ℓ1 norm induces the sparsity in features of each task and ℓ1,q norm couples the sparsity across tasks(Friedman et al., 2010).\nProjection-based algorithms such as Projected Gradient (Bertsekas, 1999), Nesterov’s optimal first-order method (Beck & Teboulle, 2009; Nesterov, 2007) and Projected Quasi-Newton (Schmidt et al., 2009) are major approaches to minimize a convex function with constraints. These algorithms minimize the objective function iteratively. By invoking the projection operation, the point of each iteration is guaranteed to be in the constraint set. Therefore, the projection serves as a key building block for such type of method.\nIn this paper, we study the problem of projecting a point in a high-dimensional space into the constraint set formed by the ℓ1 and ℓ1,q norms simultaneously, in particular, q = 2 or ∞. We choose q = 2 or ∞ because these two types of norms are the most widely used group-sparsity inducing norms(Bach et al., 2011). Our Euclidean projection operator Pτ1,τ2(1,q)+1(c) can be formulated as\nPτ1,τ2(1,q)+1(c) = argminx {∥x− c∥ 2 2 | ∥x∥1,q ≤ τ1, ∥x∥1 ≤ τ2}.\n(1)\nwhere c is the point to be projected, ∥x∥1,q and ∥x∥1\nare the ℓ1,q norm and ℓ1 norm of x (Sec 3).\nWe formalize the projection as a convex optimization problem and show that the solution can be parameterized by the dual variables and the parametrization has an intuitive geometrical interpretation. Since the dual problem optimizes a concave objective, intuitively we can solve the dual variables through 2-D grid search. However, this method is costly and inaccurate. Further inspection reveals that seeking the optimal dual variable can be associated with finding the unique root of a 1-D auxiliary function. As a main result of this paper, we prove that this function is piecewise smooth and strictly monotonic. Therefore, it is sufficient to adopt a bisection algorithm to handle it efficiently. We then theoretically analyze its time complexity, which are O(n+ g log g) for q = 2 and O(n log n) for q = ∞.\nHaving obtained an efficient projection algorithm, we can embed it in the projection-based optimization methods to efficiently find the “simultaneous sparse” solution to the following problem:\nmin w\nf(w) s.t. ∥w∥1,q ≤ τ1, ∥w∥1 ≤ τ2 (2)\nwhere f(w) is a convex function. We illustrate this point by experimentally solving regression problems with the above constraints.\nThe main contribution of this paper can be summarized as follows. Firstly, we are the first to propose a specific method that is highly efficient in time and memory usage for the composite norm projection problem. Secondly, we derive a bound to the time complexity of our algorithm and theoretically show that the algorithm enjoys fast convergence rate. This result is supported by the experiments using synthetic data and real data."
    }, {
      "heading" : "2. Related Work",
      "text" : "There has been a lot of research on efficient projection operators to norm balls. Projection onto the ℓ2 ball is straightforward since we only need to rescale the point towards the origin. Linear time projection algorithms for ℓ1 norm and ℓ1,2 norm are proposed by (Duchi et al., 2008; Liu & Ye, 2009) and (Schmidt et al., 2009), respectively. For the ℓ1,∞ norm, (Quattoni et al., 2009) proposes a method with O(n log n) complexity and (Sra, 2010) introduces a method with weak linear time complexity.\nThe problem of projecting a point onto the intersection of convex sets has been studied since decades ago. In particular, various alternating direction methods have been proposed(Han, 1988; Perkins, 2002; Schmidt & Murphy, 2010). For example, Dykstra’s algorithm (Dykstra, 1983) and ADMM\n(Gabay & Mercier, 1976) are variants of the alternating projection algorithm which successively projects the point onto the convex sets until convergence. Another approach to solve the projection problem is by modeling it as a Second-Order Cone Programming (SOCP) problem and solve it by the Interior Point solver. Although these algorithms could be applied, empirical results reveal their slow convergence rate and poor scalability. Recently, (Gong et al., 2011) solves the projection onto the Intersection of Hyperplane and a Halfspace by PRF method in linear time via root finding in 1-D space. However, the problem is quite different from ours and their method could not be trivially applied. Consequently, a specific method tailored for our problem is needed."
    }, {
      "heading" : "3. Notations and Definitions",
      "text" : "We start by introducing the notation and definitions that will be used throughout the paper.\nGiven c ∈ Rn, the ℓ1, ℓ2 and ℓ∞ norms of c are defined as ∥c∥1 = ∑n i=1 |ci|, ∥c∥2 = √∑n i=1 c 2 i and ∥c∥∞ = max1≤i≤n{|ci|} respectively.\nIn addition, the indices of c are divided into g disjoint groups. Thus c can be written as c = [c̃1; . . . ; c̃g], where each c̃i is a subvector of c. We define the ℓ1,qnorm of c as ∥c∥1,q ≡ ∑g i=1 ∥c̃i∥q. The (ℓ1+ℓ1,q)-norm ball is defined as the intersection of ℓ1-norm ball and ℓ1,q-norm ball.\nThe Euclidean projections onto the ℓ1, ℓ1,q and (ℓ1 + ℓ1,q) norm balls are denoted as Pτ21 (·),P τ1 (1,q)(·) and Pτ1,τ2(1,q)+1(·), respectively.\nFinally, we introduce three functions, SGN(c) = c ∥c∥2 , MAX(c,d) = [max(c1, d1); . . . ;max(cn, dn)], and MIN(c,d) = [min(c1, d1); . . . ;min(cn, dn)], where ci, di is the ith element of c and d."
    }, {
      "heading" : "4. Euclidean Projection on the",
      "text" : "(ℓ1 + ℓ1,q)-norm Ball\nIn this section, we will introduce our approach of Euclidean projection onto the (ℓ1 + ℓ1,2)-norm ball and (ℓ1 + ℓ1,∞)-norm ball. Due to space constraints, we leave most proofs in the appendix except Theorem 1."
    }, {
      "heading" : "4.1. Euclidean Projection on the",
      "text" : "(ℓ1 + ℓ1,2)-norm Ball\nIn this section, we first formulate the projection on the (ℓ1 + ℓ1,2)-norm ball as a convex optimization problem (Sec 4.1.1). We then parameterize the solution by dual variables and provide an intuitive geometrical interpretation (Sec 4.1.2). Finally, we determine\nthe optimal dual variable values by finding the unique zero point of a monotonic function with a bisection algorithm (Sec 4.1.3)."
    }, {
      "heading" : "4.1.1. Problem Formulation",
      "text" : "The Euclidean projection in Rn for the (ℓ1+ℓ1,2)-norm ball can be formalized as\nmin x\n1 2 ∥c− x∥22 s.t. ∥x∥1,2 ≤ τ1, ∥x∥1 ≤ τ2 (3)\nwhere τ1 (τ2) specifies the radius of ℓ1,2 (ℓ1) norm ball.\nUsing the following proposition, we can reflect the point c to the positive orthant by simply setting ci := |ci| and later recover the original optimizer by setting x∗i := sign(ci) · x∗i . Therefore, we simply assume ci ≥ 0, i = 1, 2, ..., n from now on.\nProposition 1 Let x∗ be the optimizer of problem (3), then x∗i ci ≥ 0, i = 1, 2, ..., n."
    }, {
      "heading" : "4.1.2. Parameterizing the Solution by Optimal Dual Variables",
      "text" : "We can parameterize the solution x∗ by the optimal dual variables λ∗1 and λ ∗ 2 as shown in the following lemma, so that the KKT system (Sec A.1 in the appendix) is satisfied (Bach et al., 2011).\nProposition 2 Suppose x∗, λ∗1 and λ ∗ 2 are the primal and dual solutions respectively, then\nx̃∗k =SGN(MAX(c̃k − λ∗2ẽk, 0̃k)) ·max(∥MAX(c̃k − λ∗2ẽk, 0̃k)∥2 − λ∗1, 0)\n(4)\nwhere ẽk is a vector of all 1s which has the same dimension as c̃k.\nThe solution has an intuitive geometrical interpretation. x̃∗k is obtained by first translating c̃k by MIN(c̃k, λ ∗ 2ẽ) units towards the origin and then shrinking by a factor of max(∥MAX(c̃k−λ∗2ẽk, 0̃k)∥2−λ∗1, 0). The geometrical interpretation is illustrated for the simple case where n = 2, g = 1 in Figure 1. According to Proposition 1, it is sufficient to consider the projection in the positive orthant. We divide the region outside the constraint set into three sets (Region I, II and III in Figure 1). The projection in Region I corresponds to the degenerated case when λ∗2 = 0 and thus x∗ = Pτ21 (c) (A1 is projected to B1). The projection in Region II corresponds to the degenerated case when λ∗1 = 0, and thus x\n∗ = Pτ11,2(c) (A2 is projected to B2). The projection in Region III corresponds to the case when λ∗1 > 0 and λ ∗ 2 > 0, where we should employ P(1,2),1 (A3 is projected to B3). In this simple setting with only one group, we assume\nci−λ∗2 > 0, i = 1, 2, ∥c̃1−λ∗2ẽ1∥2 > λ∗1, and thus (4) is reduced to x̃∗1 = SGN(c̃1−λ∗2ẽ1) · (∥c̃1−λ∗2ẽ1∥2−λ∗1). One can find that x̃∗1 (OB3 in Figure 1) is actually a contraction of the translated unit vector SGN(c̃1 − λ∗2ẽ1) by a factor (∥c̃1 − λ∗2ẽ1∥2 − λ∗1). Hence the projection path is separated into two segments. The first segment is called Translation Path, which is c̃1 −λ∗2ẽ1 (A3C3) of height λ ∗ 2 in Figure 1. The second segment is called Stretch Path, which is C3B3 of length λ ∗ 1 in Figure 1."
    }, {
      "heading" : "4.1.3. Determining the Dual Variables",
      "text" : "So far, we have transformed the Euclidean projection problem into determining the optimal dual variables λ∗1 and λ ∗ 2. In this section, we discuss how to determine the variables case by case. We first consider the trivial cases when at least one of λ∗1 and λ ∗ 2 equals to zero.\nCase 1: λ∗1 = 0 and λ ∗ 2 = 0. This is the case when c\nis already in the (ℓ1 + ℓ1,2)-norm ball (the shaded area in Figure 1) and no projection is needed. We can test this case by checking the ℓ1 and ℓ1,2 norms of c. Case 2: λ∗1 > 0 and λ ∗ 2 = 0. This is the case when\nx∗ = Pτ11,2(c) (Region II in Figure 1). We can test this case by checking whether Pτ11,2(c) lies in the ℓ1-norm ball. Case 3: λ∗1 = 0 and λ ∗ 2 > 0. This is the case when\nx∗ = Pτ21 (c) (Region I in Figure 1). We can test\nthis case by checking whether Pτ21 (c) lies in the ℓ1,2-norm ball. Now we discuss the non-trivial case when λ∗1 > 0 and λ∗2 > 0 (Region III in Figure 1). According to the complementary-slackness condition of the KKT system (See (3) and (4) in the appendix), the solution satisfies\ng∑ i=1 ∥x̃∗i ∥2 = τ1, n∑ j=1 |x∗j | = τ2. (5)\nSubstitute (4) into (5) and we get the two equations of λ1 and λ2:\nτ1 = ∑\ni∈Sλ1,λ2\n[∥MAX(c̃i − λ2ẽi, 0̃i)∥2 − λ1] (6)\nτ2 = ∑\ni∈Sλ1,λ2\n(1− λ1 ∥MAX(c̃i − λ2ẽi, 0̃i)∥2 )\n· ∑\nj∈Siλ2\n(ci,j − λ2) (7)\nwhere Sλ1,λ2 = {i | ∥MAX(c̃i − λ2ẽi, 0̃i)∥2 > λ1} and Siλ2 = {j | ci,j > λ2}, i = 1, 2, . . . , g. Now the task is to find a pair (λ∗1, λ ∗ 2) which satisfies (6) and (7) simultaneously.\n(6) implicitly defines a function λ1(λ2) and use this fact we obtain the following equation:\nλ1(λ2) =\n∑ i∈Sλ1,λ2\n∥MAX(c̃i − λ2ẽi, 0̃i)∥2 − τ1 |Sλ1,λ2 | (8)\nNote that (8) does not define an explicit function λ1(λ2) since λ1 also appears on the right side of (8). For a detailed proof that λ1 is an implicit function of λ2, please check Lemma 2 and Lemma 3 in the appendix.\nBy substituting λ1(λ2) into (7), it is easy to see that solving the equation system (6) and (7) is equivalent to finding the zero point of the following function:\nf(λ2) = ∑\ni∈Sλ1(λ2),λ2\n(1− λ1(λ2) ∥MAX(c̃i − λ2ẽi, 0̃i)∥2 )\n· ∑\nj∈Siλ2\n(ci,j − λ2)− τ2 (9)\nThe following theorem states that f(λ2) is continuous, piece-wise smooth and monotone. The fact immediately leads to a bisection algorithm to efficiently find the zero point.\nTheorem 1 1) f is a continuous piece-wise smooth function in (0,max{ci,j}); 2) f is monotonically decreasing and it has a unique root in (0,max{ci,j}).\nWe leave the proof of the continuity and piecewise smooth property in the appendix (Lemma 5 in the appendix). Here we just prove the monotonicity of f(λ2).\nProof: Because f(λ2) is continuous and piecewise smooth in (0,max{ci,j}), it is sufficient to prove that f ′(λ2) ≤ 0 for λ2 ∈ R+\\E , where E is a set containing finite points as defined in Lemma 4 in the appendix. For such points, by Lemma 4, we can always find an interval (a, b) where Sλ1,λ2 and Siλ2 do not change, hence we can denote S1 = Sλ1,λ2 and Si2 = Siλ2 here for simplicity.\nDenote ∥ · ∥i1 = ∑\nj∈Si2 (ci,j − λ2) and ∥ · ∥i2 =√∑\nj∈Si2 (ci,j − λ2)2. Within the interval, we assume\n∥ · ∥i2 ≥ λ1(λ2). Therefore,\nf ′(λ2) = − ∑ i∈S1 |Si2|+ 1 |S1| ( ∑ i∈S1 ∥ · ∥i1 ∥ · ∥i2 )2\n+λ1(λ2) ∑ i∈S1 |Si2| − ( ∥·∥i1 ∥·∥i2 )2 ∥ · ∥i2\n≤ − ∑ i∈S1 |Si2|+ 1 |S1| ( ∑ i∈S1 ∥ · ∥i1 ∥ · ∥i2 )2\n+min{∥ · ∥i2} ∑ i∈S1 |Si2| − ( ∥·∥i1 ∥·∥i2 )2 min{∥ · ∥i2}\n≤ |S1|[( 1 |S1| ∑ i∈S1 ∥ · ∥1 ∥ · ∥2 )2 − 1 |S1| ∑ i∈S1 ( ∥ · ∥1 ∥ · ∥2 )2] ≤ 0\nSince f(0) > 0 and f(max{ci,j}) < 0 and E is finite, there exists one and only one root in [0,max{ci,j}]. Given the theorem above, it is sufficient to apply a bisection algorithm to f(λ2) to find its unique root. Note that it is non-trivial to evaluate λ1(λ2) since (8) is not a definition of the function, as we discussed before. Now we introduce an algorithm FindLambda1 to tackle it. More specifically, we first sort all the groups MAX(c̃k − λ2ẽk, 0̃k), k = 1, 2, ...g in ascending order w.r.t their ℓ2-norms. Then we repeatedly add the group indexes one at a time to the active group set Sλ1,λ2 , calculating the corresponding λ1 and checking its validity. This process stops as soon as λ1, λ2 and Sλ1,λ2 are all consistent.\nComplexity Analysis: Given the tolerance ϵ, the bisection projection algorithm converges after no more than ⌈log2[maxi(ci)/ϵ]⌉ outermost iterations (lines 4- 21 in Algorithm 1). In each iteration, FindLambda1 dominates the complexity. In Algorithm 2, line 4 costs O(n) flops. While additional O(n) flops are needed for calculating the ℓ2-norm of each group, the sort-\nAlgorithm 1 ℓ1 + ℓ1,2 Projection\n1: Input: c, group Index of each ci, τ1, τ2, ϵ. 2: Output: λ1, λ2, x. 3: left = 0; right = max{ci,j}; 4: while true do 5: λ2 = (left+ right)/2; 6: [λ1,isLambda1Found]=FindLambda1(c, λ2, group\nIndex of each ci); 7: if isLambda1Found == true then 8: Evaluate f(λ2) according to (9); 9: if |f(λ2)| < ϵ then 10: break; 11: else 12: if f(λ2) < −ϵ then 13: right = λ2; 14: else 15: left = λ2; 16: end if 17: end if 18: else 19: right = λ2; 20: end if 21: end while 22: Calculate x according to (4);\ning in line 5 takes O(g log g) flops. Finally, the complexity of line 7 to line 14 is O(g). Therefore, the overall time complexity for the projection algorithm is ⌈log2[maxi(ci)/ϵ]⌉ ·O(n+ g log g)."
    }, {
      "heading" : "4.2. Euclidean Projection on the",
      "text" : "(ℓ1 + ℓ1,∞)-norm Ball\nThe projection onto the (ℓ1 + ℓ1,∞)-norm ball could also be addressed by a bisection algorithm. We first introduce a variable d, and then give an equivalent formulation of the projection problem (1) for q = ∞ as follows:\nmin x,d\n1 2 ∥c− x∥22\ns.t. xi,j ≤ di(i = 1, 2, ...g), g∑\ni=1\ndi ≤ τ1,\n∥x∥1 ≤ τ2, xi,j ≥ 0, di ≥ 0.\n(10)\nNote that the formulation above differs from (Quattoni et al., 2009) in the additional term ∥x∥1 ≤ τ2. Similar to Sec 4.1.2, given the KKT system (in the appendix), we can parameterize the solution x∗ using d∗ and optimal dual variables λ∗1 and λ ∗ 2:\nProposition 3 Suppose x∗, d∗ and λ∗1, λ ∗ 2 are the primal and dual solution respectively, then\nx∗i,j = min(max(ci,j − λ∗2, 0), d∗i ),\nd∗i =\n∑ j∈Si,1\nλ∗2\n(ci,j − λ∗2)− λ∗1\n|Si,1λ∗2 | ,\nλ∗1 =\n∑ i∈Sλ∗2\n∑ j∈Si,1\nλ∗2\n(ci,j−λ∗2)−τ1\n|Si,1 λ∗2 |∑ i∈Sλ∗2 1\n|Si,1 λ∗2 |\n,\nwhere Siλ∗2 ≡ {j|ci,j − λ ∗ 2 > 0} = S i,1 λ∗2 ∪ Si,2λ∗2 , S i,1 λ∗2 = {j|ci,j − λ∗2 > di} and S i,2 λ∗2 = {j|0 < ci,j − λ∗2 ≤ di}.\nThe proposition above reveals that x∗i,j , d ∗ and λ∗1 can all be viewed as functions of λ∗2. We can substitute the above equations into the KKT system and show that λ∗2 is the zero point of the following function\nh(λ2) = ∑ i ∑ j min(max(ci,j − λ2, 0), di(λ2))− τ2\nWe can prove that h(λ2) is a strictly monotonically decreasing function:\nTheorem 2 1) h is a continuous piece-wise smooth function in (0,max{ci,j}); 2) h is monotonically decreasing and it has a unique root in (0,max{ci,j}).\nComplexity Analysis: Based upon the above theorem, we can determine λ∗2 using the bisection Algorithm 3. For a given λ2, ∀i, j,max(ci,j − λ2, 0) is determined, (Quattoni et al., 2009) shows that λ∗1 and d ∗ can be solved with time complexity O(n log n). Therefore, the total time complexity of the bisection algorithm is ⌈log2[maxi,j(ci,j)/ϵ]⌉ ·O(n log n)."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section, we demonstrate the efficiency and effectiveness of the proposed projection algorithm in experiments using synthetic and real-world data. Due to the space limitation, we only show the result of ℓ1 + ℓ1,2, and the case of ℓ1 + ℓ1,∞ is shown in the appendix."
    }, {
      "heading" : "5.1. Efficiency of the Proposed Projection Algorithms",
      "text" : "We first compare our methods to Interior Point (IP) method and alternating projection methods which are also applicable in solving the Euclidean projection problem. For IP, we use a highly optimized commercial software MOSEK1. For alternating projection methods, as there is no algorithm specifically tailored for our problem, we compare with two widely used representative algorithms – Dykstra’s algorithm and ADMM algorithm. Both algorithms generate a sequence of points whose limit is the orthogonal projection onto the intersection of convex sets 2.\n1We reformulate the problem as an SOCP problem. Please refer to the appendix for the SOCP formulation.\n2Check Sec G in Appendix for more details.\nNote that all methods in the comparison apply Pτ21 (·) in Region I and Pτ11,2(·) in Region II. Therefore, we first show the time cost for the shared modules Pτ11,q(·) and Pτ21 (·), and then compare the different projection methods in Region III. To estimate the expected running time of each method, we estimate the volume of the three regions by Monte Carlo method with uniform sampling distribution.\nWe generate synthetic data with different number of groups g and dimensions n. Specifically, each dimension of a point is sampled uniformly in [−103, 103], and τ (2) 1 = 5, τ (2) 2 = 6, τ (∞) 1 = 5, τ (∞) 2 = 10. Each method is run for 10 times to calculate the average running time and standard deviation. To estimate the area of each region, we sampled 10,000 i.i.d points.\nAlgorithm 2 FindLambda1\n1: Input: c, λ2, group Index of each ci. 2: Output: λ1, isLambda1Found. 3: isLambda1Found = false; 4: for each i do xi = max(ci − λ2, 0); 5: Sort the groups of x̃ in ascending order, w.r.t their ℓ2-\nnorms; 6: sum = 0; 7: for i = g down to 1 do 8: sum = sum+ ∥x̃i∥2; 9: λ1 = (sum− τ1)/(g − i+ 1); 10: if (i > 1 and ∥x̃i−1∥2 < λ1 ≤ ∥x̃i∥2) or (i == 1 and 0 < λ1 ≤ ∥x̃1∥2) then 11: isLambda1Found = true; 12: break; 13: end if 14: end for\nAlgorithm 3 ℓ1 + ℓ1,∞ Projection\n1: Input: c, group Index of each ci, τ1, τ2, ϵ. 2: Output: λ2, λ1,x. 3: Initialization; 4: while |h(λ2)| > ϵ do 5: if h(λ2) > ϵ then 6: left = λ2; 7: else 8: right = λ2; 9: end if 10: λ2 = (left+ right)/2; 11: for each i, j do xi,j = max(ci,j − λ2, 0); 12: [x,d, λ2, λ1] = Pτ1(1,∞)(x); 13: Evaluate h(λ2); 14: end while\nIn our proposed projection method, as in each step the bisection algorithm halves the range of λ2, we stop when the range is smaller than 10−9. For Dykstra’s algorithm and ADMM algorithm, we stop when the ℓ2-\nnorm of two consecutive projected points falls below 10−9. For IP, we stop it if the duality gap is below 10−9. Table 1 summarizes the results for q = 2.\nWe can observe from the table that: 1) our proposed technique is significantly faster than IP and alternating projection methods (Dykstra and ADMM) in Region III; 2) our method scales well to large problems.\nCompared with IP, which is the runner-up algorithm in speed, our algorithm is more memory efficient since very little extra space is needed, whereas IP introduces several groups of auxiliary variables.\nCompared with Dykstra’s algorithm and ADMM algorithm, our method takes much less iterations to converge. As we discussed before, the number of iterations of our algorithm is bounded by ⌈log2[maxi(ci)/ϵ]⌉. When n = 1000 and g = 100, it can be calculated that our algorithm takes no more than 30 iterations. On the other hand, empirical study shows that Dykstra’s algorithm takes 692±247 iterations to converge in Region III. A closer study also shows that Dykstra’s algorithm suffers from a high variance in iterations, which may be related to the order of projections. For example, 692 ± 247 iterations are taken if we first project onto the ℓ1-norm ball, whereas 3460 ± 565 iterations are taken if we first project onto the ℓ1,2-norm ball.\nWe also analyze the volume of each region by Monte Carlo sampling method. Table 2 indicates that the probability for a point falling in Region III may be very high. Because our algorithm runs much faster than competitors in Region III, its expected running time can be much shorter in general."
    }, {
      "heading" : "5.2. Efficiency of the Projection-based Methods in Linear Regression with Composite Norm Constraints",
      "text" : "In this section, we show that embedded with our proposed projection operator, various projection-based method can efficiently optimize the composite norm constrained linear regression problem. These projection-based methods significantly outperform the baseline method in speed.\nWe embed our projection operator into three types of projection-based optimization framework, including Projected Gradient method (PG), Nesterov’s optimal first-order method (Nesterov) and Projected QuasiNewton method (PQN).\nWe synthesize a small-sized data and a medium-sized data. For the small-sized data, we adopt the experiment setting in (Friedman et al., 2010). We create the coefficient vector w ∈ R100 divided in ten blocks of ten, where the last four blocks are set to all ze-\nros and the first six blocks have 10, 8, 6, 4, 2 and 1 non-zero entries respectively. All the non-zero coefficients are randomly chosen as ±1. Then we generate N = 200 observations y ∈ R200 by setting y = Xw+ϵ, where X ∈ R200×100 denotes the synthetic data points of standard Gaussian distribution with correlation 0.2 within a group and zero otherwise and ϵ ∈ R200 is a Gaussian noise vector with standard deviation 4.0 in each entry. For the medium-sized data, the data generation process is similar except that the first six blocks have 100, 80, 60, 40, 20 and 10 non-zero entries respectively for 4000 observations.\nSince the generated w exhibits sparsity in both group level and individual level, it is natural to recover w by solving the following constrained linear regression problem (q = 2 or ∞):\nmin 1\n2 ∥y −Xw∥22 s.t. ∥w∥1,q ≤ τ (q) 1 , ∥w∥1 ≤ τ (q) 2\nWe choose Interior Point method as the baseline to solve the above problem for its efficiency. Embedded with our efficient projection operator, all projectionbased algorithms (PG, Nesterov and PQN) take much less time and memory than IP to converge to the same accuracy (10−9) (see Table 3 for time used). We note that, projection-based methods usually take much more iterations to converge than IP (see Figure 2, and the projection operator may be invoked several times per iteration. Hence, the efficiency of the projection operator greatly impact the performance.\n0 10 20 30 40 50 60 70 80 90 100 10\n−10\n10 −8\n10 −6\n10 −4\n10 −2\n10 0\n10 2\n10 4\nL 1 + L 1,2\nIteration\nf − f *\nIP PQN Nesterov\nPG } USE OUR ALGO\nFigure 2. Number of iterations on linear regression with (ℓ1 + ℓ1,2)-norm constraint for different methods. With our efficient projection operator, all three projection-based methods converge faster than IP even though they take more iterations."
    }, {
      "heading" : "5.3. Classification Performance in Multi-task Learning",
      "text" : "In this experiment, we show that using our efficient projection operator, with limited additional time cost, composite norm regularizer outperforms single norm regularizer in multi-task learning. In the multiple-task learning setting, there are r > 1 response variables (each corresponding to a task) and a common set of p features. We hypothesize that, if the relevant features for each task are sparse and there is a large overlap of these relevant features across tasks, combining ℓ1,q norm and ℓ1 norm will recover both the sparsity of each task and the sparsity shared across tasks.\nWe use handwritten digits recognition as test case. The input data are features of handwritten digits (0-9) extracted from a collection of Dutch utility maps(Asuncion & Newman, 2007). This dataset has been used by a number of papers as a reliable dataset for handwritten recognition algorithms(Jalali et al., 2010). There are r = 10 tasks, and each sample consists of p = 649 features. We use logistic regression as the classifier and constrain the classifier by ℓ1 norm, ℓ1,q norm and ℓ1+ℓ1,q norm, respectively. PQN method is used to optimize the objective function.\nWe compare the running time and classification performance of each method. The classification performance is measured by the mean and standard deviation of the classification error. Results are obtained from ten ran-\ndom samples of training and testing data with parameters chosen via cross-validation in all methods. Using our projection operator, (ℓ1+ℓ1,q)-norm yields the best classification result with similar running time to ℓ1,q-norm (Table 4). We also test replacing our projection algorithm by the runner-up algorithm in Table 1, which is IP for q = 2 and Dykstra for q = ∞. Unfortunately, using these projection operators, PQN could not converge within 30 minutes. These results show that a more structured yet complicated regularizer is more effective in a multiple-task learning problem and our efficient projection algorithms make it feasible."
    } ],
    "references" : [ {
      "title" : "Convex optimization with sparsity-inducing norms",
      "author" : [ "Bach", "Francis", "Jenatton", "Rodolphe", "Mairal", "Julien", "Obozinski", "Guillaume" ],
      "venue" : "Optimization for Machine Learning,",
      "citeRegEx" : "Bach et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2011
    }, {
      "title" : "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems",
      "author" : [ "Beck", "Amir", "Teboulle", "Marc" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "Beck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient projections onto the l1-ball for learning in high dimensions",
      "author" : [ "Duchi", "John C", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Duchi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2008
    }, {
      "title" : "An algorithm for restricted least squares regression",
      "author" : [ "Dykstra", "Richard L" ],
      "venue" : "JASA, 78(384):837–842,",
      "citeRegEx" : "Dykstra and L.,? \\Q1983\\E",
      "shortCiteRegEx" : "Dykstra and L.",
      "year" : 1983
    }, {
      "title" : "A note on the group lasso and a sparse group lasso",
      "author" : [ "Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert" ],
      "venue" : "Arxiv preprint arXiv10010736,",
      "citeRegEx" : "Friedman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2010
    }, {
      "title" : "A dual algorithm for the solution of nonlinear variational problems via finite element approximation",
      "author" : [ "Gabay", "Daniel", "Mercier", "Bertrand" ],
      "venue" : "Computers & Mathematics with Applications,",
      "citeRegEx" : "Gabay et al\\.,? \\Q1976\\E",
      "shortCiteRegEx" : "Gabay et al\\.",
      "year" : 1976
    }, {
      "title" : "Efficient euclidean projections via piecewise root finding and its application in gradient",
      "author" : [ "Gong", "Pinghua", "Gai", "Kun", "Zhang", "Changshui" ],
      "venue" : "projection. Neurocomputing,",
      "citeRegEx" : "Gong et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2011
    }, {
      "title" : "A successive projection method",
      "author" : [ "Han", "Shih-Ping" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Han and Shih.Ping.,? \\Q1988\\E",
      "shortCiteRegEx" : "Han and Shih.Ping.",
      "year" : 1988
    }, {
      "title" : "A dirty model for multi-task learning",
      "author" : [ "Jalali", "Ali", "Ravikumar", "Pradeep D", "Sanghavi", "Sujay", "Ruan", "Chao" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Jalali et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jalali et al\\.",
      "year" : 2010
    }, {
      "title" : "Efficient euclidean projections in linear time",
      "author" : [ "Liu", "Jun", "Ye", "Jieping" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Liu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient l1/lq Norm Regularization",
      "author" : [ "Liu", "Jun", "Ye", "Jieping" ],
      "venue" : "Arxiv preprint arXiv:1009.4766,",
      "citeRegEx" : "Liu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Nesterov", "Yu" ],
      "venue" : "ReCALL,",
      "citeRegEx" : "Nesterov and Yu.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nesterov and Yu.",
      "year" : 2007
    }, {
      "title" : "A convergence analysis of dykstra’s algorithm for polyhedral sets",
      "author" : [ "Perkins", "Chris" ],
      "venue" : "SIAM J. Numerical Analysis,",
      "citeRegEx" : "Perkins and Chris.,? \\Q2002\\E",
      "shortCiteRegEx" : "Perkins and Chris.",
      "year" : 2002
    }, {
      "title" : "An efficient projection for l1,∞ regularization",
      "author" : [ "Quattoni", "Ariadna", "Carreras", "Xavier", "Collins", "Michael", "Darrell", "Trevor" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Quattoni et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Quattoni et al\\.",
      "year" : 2009
    }, {
      "title" : "Convex structure learning in log-linear models: Beyond pairwise potentials",
      "author" : [ "Schmidt", "Mark W", "Murphy", "Kevin P" ],
      "venue" : "In AISTATS, pp",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2010
    }, {
      "title" : "Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton",
      "author" : [ "Schmidt", "Mark W", "van den Berg", "Ewout", "Friedlander", "Michael P", "Murphy", "Kevin P" ],
      "venue" : null,
      "citeRegEx" : "Schmidt et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2009
    }, {
      "title" : "Generalized proximity and projection with norms and mixed-norms",
      "author" : [ "Sra", "Suvrit" ],
      "venue" : "In Technique Report,",
      "citeRegEx" : "Sra and Suvrit.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sra and Suvrit.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "For example, in the multi-task learning setting, the index set for features of each task may be sparse and there might be a large overlap of features across multiple tasks (Jalali et al., 2010).",
      "startOffset" : 172,
      "endOffset" : 193
    }, {
      "referenceID" : 4,
      "context" : "One natural approach is to formalize an optimization problem constrained by l1 norm and l1,q norm together, so that l1 norm induces the sparsity in features of each task and l1,q norm couples the sparsity across tasks(Friedman et al., 2010).",
      "startOffset" : 217,
      "endOffset" : 240
    }, {
      "referenceID" : 15,
      "context" : "Projection-based algorithms such as Projected Gradient (Bertsekas, 1999), Nesterov’s optimal first-order method (Beck & Teboulle, 2009; Nesterov, 2007) and Projected Quasi-Newton (Schmidt et al., 2009) are major approaches to minimize a convex function with constraints.",
      "startOffset" : 179,
      "endOffset" : 201
    }, {
      "referenceID" : 0,
      "context" : "We choose q = 2 or ∞ because these two types of norms are the most widely used group-sparsity inducing norms(Bach et al., 2011).",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "Linear time projection algorithms for l1 norm and l1,2 norm are proposed by (Duchi et al., 2008; Liu & Ye, 2009) and (Schmidt et al.",
      "startOffset" : 76,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : ", 2008; Liu & Ye, 2009) and (Schmidt et al., 2009), respectively.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "For the l1,∞ norm, (Quattoni et al., 2009) proposes a method with O(n log n) complexity and (Sra, 2010) introduces a method with weak linear time complexity.",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "Recently, (Gong et al., 2011) solves the projection onto the Intersection of Hyperplane and a Halfspace by PRF method in linear time via root finding in 1-D space.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "1 in the appendix) is satisfied (Bach et al., 2011).",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "Note that the formulation above differs from (Quattoni et al., 2009) in the additional term ∥x∥1 ≤ τ2.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "For a given λ2, ∀i, j,max(ci,j − λ2, 0) is determined, (Quattoni et al., 2009) shows that λ1 and d ∗ can be solved with time complexity O(n log n).",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "For the small-sized data, we adopt the experiment setting in (Friedman et al., 2010).",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "This dataset has been used by a number of papers as a reliable dataset for handwritten recognition algorithms(Jalali et al., 2010).",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : "Our performance is on par with state-of-the-art (Jalali et al., 2010).",
      "startOffset" : 48,
      "endOffset" : 69
    } ],
    "year" : 2012,
    "abstractText" : "Using sparse-inducing norms to learn robust models has received increasing attention from many fields for its attractive properties. Projection-based methods have been widely applied to learning tasks constrained by such norms. As a key building block of these methods, an efficient operator for Euclidean projection onto the intersection of l1 and l1,q norm balls (q = 2 or ∞) is proposed in this paper. We prove that the projection can be reduced to finding the root of an auxiliary function which is piecewise smooth and monotonic. Hence, a bisection algorithm is sufficient to solve the problem. We show that the time complexity of our solution is O(n + g log g) for q = 2 and O(n log n) for q = ∞, where n is the dimensionality of the vector to be projected and g is the number of disjoint groups; we confirm this complexity by experimentation. Empirical study reveals that our method achieves significantly better performance than classical methods in terms of running time and memory usage. We further show that embedded with our efficient projection operator, projectionbased algorithms can solve regression problems with composite norm constraints more efficiently than other methods and give superior accuracy.",
    "creator" : " TeX output 2012.05.19:1810"
  }
}