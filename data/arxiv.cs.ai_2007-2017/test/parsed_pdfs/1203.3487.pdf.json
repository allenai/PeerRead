{
  "name" : "1203.3487.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "BEEM : Bucket Elimination with External Memory",
    "authors" : [ "Kalev Kask", "Rina Dechter" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "A major limitation of exact inference algorithms for probabilistic graphical models is their extensive memory usage, which often puts real-world problems out of their reach. In this paper we show how we can extend inference algorithms, particularly Bucket Elimination, a special case of cluster (join) tree decomposition, to utilize disk memory. We provide the underlying ideas and show promising empirical results of exactly solving large problems not solvable before."
    }, {
      "heading" : "1 Introduction",
      "text" : "Exact inference algorithms for graphical models broadly fall into two categories: 1) Inference-based algorithms (e.g. Bucket Elimination (BE), Cluster Tree Elimination (CTE)); and 2) Search-based (e.g. best- rst, depth- rst branch and bound) [3, 4, 5, 2]. Inference-based algorithms are time exponential in the induced width and also require space exponential in the induced-width. While brute force search algorithms can work in linear space (e.g., depth rst search), more advanced search schemes that use and/or search spaces require memory exponential in the induced-width as well. Consequently, both classes of algorithms are feasible only when the induced-width (or treewidth) is small due to memory limitations.\nNot surprisingly, various algorithms have been proposed that can work with bounded memory at the expense of additional time [2, 8, 9, 10]. In this paper we aim to push the boundary of memory-intensive algorithms further, allowing a more e ective tradeo or in some cases eliminating the compromise altogether. To do so, we extend the memory available to the algorithm to include external (disk) memory.\nIn comparison to main/virtual memory, external mem-\nory is seemingly unlimited. So an algorithm that e ectively utilizes external memory should, in principle, be able to tackle problems with very large induced widths. However, the additional space does not come without cost, as access of disk memory is typically orders of magnitude slower than main memory. Nonetheless, it has been demonstrated in the context of (A*) heuristic search that algorithms can be designed to mitigate such e ects [6, 7], yielding powerful schemes that can be applied to previously unsolvable problems.\nTo make the realm of problems solvable by inference algorithms using external memory more concrete, consider a Bayesian network (BN) comprised of binary variables (k = 2) and having induced width, w? = 20. The largest table in this model has 219 = 524, 288 entries. Assuming that a double-precision oating point number requires 8 bytes, this problem requires about 4MB of memory and easily ts into main memory. However, if each variable in the model is ternary (k = 3) rather than binary, the largest table requires about 9GB of memory. While 9 GB is more main memory than most computers have, such a problem would t comfortably into external disk, which can exceed several Tera-bytes in size.\nIn the remainder of this paper, we describe how a speci c inference-based algorithm, BE [3], can be modi ed to use external memory. We demonstrate the performance of this new algorithm, named Bucket Elimination with External Memory (BEEM), for computing the probability of evidence on a class of large networks for which exact computations had not previously been made, and otherwise show that it is faster then some of the best algorithms that trade space for time."
    }, {
      "heading" : "2 Background",
      "text" : "In this section we present some necessary preliminaries on graphical models and Bucket Elimination.\nDefinition 1. Graphical Model - A Graphical model R is a 4-tuple R = 〈X,D,F,⊗〉, where:\n1. X = {X1, ..., Xn} is a set of variables;\n2. D = {D1, ..., Dn} is the set of their respective - nite domains of values;\n3. F = {f1, ..., fr} is a set of real valued functions de ned over a subset of variables Si ⊆ X. The scope of function fi, denoted scope(fi), is its set of arguments, Si;\n4. ⊗i fi ∈ { ∏ i fi, ∑\ni fi, ./i fi} is a combination operator.\nThe graphical model represents the combination of all its functions: ⊗ri=1fi.\nThe primal graph of a graphical model associates a node with each variable and connect any two nodes whose variables appear in the same scope.\nDefinition 2. Induced Width - An ordered graph is a pair (G, d), where G is an undirected graph and d = X(1), ..., X(n) is an ordering of the nodes (X(i) means the ith node in the ordering). The width of a node is the number of the node's neighbors that precede it in the ordering. The width of an ordering d is the maximum width over all nodes. The induced width of an ordered graph w?(d) is the width obtained when nodes are processed from last to rst, such that when node X(i) is processed, all of its preceding neighbors (X(j) for j < i) are connected. The induced width of a graph, w?, is the minimal induced width over all possible orderings.\nBucket Elimination (BE) is a special case of cluster tree elimination in which the tree-structure upon which messages are passed is determined by the variable elimination order used [3]. In BE terminology, the nodes of the tree-structure are referred to as buckets and each bucket is associated with a variable to be eliminated. Each bucket contains a set of functions, either the original functions (e.g. Conditional Probability Tables (CPTs) in a BN), or functions generated by the algorithm. Each bucket is processed by BE in two steps. First, all functions in the bucket are combined (by multiplication in the case of BNs ). Then the variable associated with the bucket is eliminated from the combined function (by summation in case of Belief Updating in BNs). The function resulting from the combination and elimination steps is then passed to the parent of the current bucket. Processing occurs in this fashion, from the leaves of the tree to the root, one node (bucket) at a time as illustrated in Figure 1. It is important to note that the bucket-tree induces a partial order on the variables in which child nodes (variables) are processed prior to their parents.\nFormal de nitions of BE data-structures are given next for completeness. A formal description of the BE algorithm is also presented in Figure 2.\nDefinition 3. Bucket Let Bx1 , ..., Bxn be a set of buckets, one for each variable and let d be ordering of these variables. Each bucket Bxi contains those functions in F whose latest variable in d is Xi. Definition 4. Bucket Tree Let G?d be the induced graph along an ordering d of a reasoning problem whose primal graph is G. The vertices of the buckettree are the n buckets, which are denoted by their respective variables. Each vertex BX points to BY (or, BY is the parent of BX) if Y is the latest neighbor of X that appear before X in G?d. The degree of bucket B, denoted degB, is the number of neighbors of bucket B in the bucket-tree.\nDefinition 5. Input-Output Functions Given a directed bucket-tree T , for any bucket B, the output function of B is the function that B sends to its parent, and the input functions of B are functions that B receives from its children."
    }, {
      "heading" : "3 Bucket Elimination with External Memory",
      "text" : "Processing a bucket in a bucket-tree involves two operations: 1) Combining functions in the bucket; and 2) Eliminating the bucket's variable1. In many problems, the function resulting from these operations is too large to t into memory. For example, assume we are computing a function fXp , by eliminating a variable Xp from a function h. Normal BE (as de ned in Figure 2) cannot operate if the function h does not t into main memory in its entirety. Since computers have orders of magnitude more disk space than main memory, a straightforward modi cation of BE would be to divide large tables (such as h) into blocks that t into main memory and store these blocks to disk.\n1In the remainder of this paper, we assume that functions take a tabular form.\nThe function fXp can then be computed by loading required blocks from hard disk. In an extreme case, one could compute fXp one entry at a time, each time loading the relevant entries of the input function h from disk, while saving entries of fXp as they are computed.\nThe performance of this naive, entry by entry, algorithm would be extremely poor because:\n• While main memory has bandwidth (data transfer rate) of a few GB/second, disk memory typically has sequential transfer rate of 100 MB/second, and much worse non-sequential transfer rate. • Main memory has 0 seek time (since it allows random access), while disk memory has a seek time of about 10 ms.\nAs a result, this naive algorithm would spend most of its time waiting for table entries to be loaded and then saving them (i.e. in disk I/O), rather than performing actual computations. The time spent on disk I/O has a linear component that depends on block size and a xed component given by the seek time. In most applications the xed component dominates, suggesting that the primary goal in designing a disk-based adaptation of any algorithm is to minimize the number of reads/writes from hard disk.\nThe challenge of minimizing disk reads/writes is further compounded in a multi-threaded environment. Since individual entries of a function table are independent, function blocks can be processed in parallel. Thus, while one thread is waiting for data to be loaded from disk, other threads can carry out computation on their assigned blocks. This parallelism o ers the potential for algorithmic speed-up, while at the same\ntime introducing a new scheduling challenge. Speci - cally, one now has to schedule disk I/O so as to minimize the amount of time that each thread waits for data to be loaded/saved.\nIn this paper, we address the two, potentially conicting goals of minimizing reads/writes and limiting thread starvation, by decomposing the challenge into two tasks: 1) Function table indexing; and 2) Blocksize computation. The block size computation task involves dividing the function tables into blocks that are as large as possible. The function table indexing task involves arranging the entries within a table (and block) so as to minimize the number of reads/writes to disk. Both of these tasks must be addressed within the constraints imposed by the bucket-tree. The following two subsections describe our approach to these two design aspects."
    }, {
      "heading" : "3.1 Function Table Indexing",
      "text" : "When processing a table, all of the table's entries are ordered, assigned an index consistent with that order, and then processed one-by-one. For example, if f(X1, X2) is a table of ternary variables, the entries are ordered as: < 0, 0 > (index 0), < 0, 1 > (index 1), < 0, 2 > (index 2), < 1, 0 > (index 3), etc. The ordering of variables in the scope of a function thus dictates where an entry is located within that function's table. Since we are considering functions (i.e. fXp) that are broken into blocks because they do not t into memory, the ordering of variables can also impact the number of reads/writes to disk.\nIn the following section, we illustrate how the ordering of variables within a scope can impact the performance of our algorithm. We then show how some of these ine ciencies are remedied by the scope ordering imposed by the bucket-tree structure. Since processing a bucket and generating its output function involves two steps - combination and elimination - we analyze the ordering constraints imposed by these two steps."
    }, {
      "heading" : "3.1.1 Ordering Constraints due to Elimination",
      "text" : "Assume we are computing a function f(X1, X2), by eliminating variable Y from the function h(Y,X2, X1) (as shown in Figure 3). Furthermore, assume that we are computing entry 1 (corresponding to value combination < 0, 1 >) of f . Since f(X1 = 0, X2 = 1) =∑\nY h(Y,X2 = 1, X1 = 0), we need entries 3, 12, 21 from table h (corresponding to argument combinations < 0, 1, 0 >, < 1, 1, 0 >, < 2, 1, 0 >, respectively). As mentioned in the previous section, data is loaded in predetermined size blocks, each stored as a separate le. If the blocks of h were only 8 entries in size,\nthe required entries (3, 12, 21) would reside in di erent blocks and three block load operations would be required. When processing the next entry of f (corresponding to value combination < 0, 2 >), we then require entries 6, 15, 24, which also reside in di erent blocks. Worse yet, since only one block per table is kept in memory at a time, the blocks of h containing entries 6, 15, 24 were in memory when entry 1 of f was computed, but then unloaded to make room for subsequent blocks. Thus, many unnecessary block loads/unloads are performed due to the poor ordering of variables in the scope. Our goal is to minimize the number of times any block is loaded.\nMore generally, computing a function f by eliminating a variable from function h involves maintaining two lists of table entries. First, we enumerate the entries of f and then we identify the entries of h required by each entry of f . A condition su cient to guarantee that every block is loaded no more than once is that the enumeration of entries of h be monotonically organized. This condition was not satis ed in the previous example, since to compute entries 1 and 2 of f we needed entries 3, 12, 21 and 6, 15, 24 of h, respectively.\nDefinition 6 (monotone computation of h relative to f). If f is obtained from h by eliminating X (f =∑\nx h), we say that h is monotone relative to f i : 1. the computation of a single entry of f requires only consecutive entries of h, 2. the computation of successive entries in f requires successive collections of consecutive entries in h.\nClearly the non-monotoncity stems from the fact that the order of variables in the scopes of f and h do not agree. By rearranging the scope of h to< X1, X2, Y >, we can make the enumeration of hmonotonic wrt f . In particular, the entries of h needed to compute f(X1 = 0, X2 = 1) are at indices 3, 4, 5, and the entries needed\nto compute f(X1 = 0, X2 = 2) are at indices 6, 7, 8. Under this ordering, no redundant loading/unloading of blocks of h are required. The monotonicity of h relative to f can thus be achieved by the following two ordering constraints:\nProposition 1. Let f be obtained from h by eliminating X,\n1. If X is the last variable in the scope of h, and\n2. If the order of the remaining variables in the scope of h agrees with the order of the variables in the scope of f ,\nthen h is monotone relative to f .\nProof. The rst condition guarantees that the entries of the input function h needed for the computation of any entry of the output function f are consecutive in the table of h. The second condition ensures the second requirement of monotonicity.\nIt turns out that the constraints for monotonicity can be satis ed over all buckets, simultaneously. In fact, the topological, partial ordering dictated by the bucket tree (from leaves to root) can be shown to guarantee monotonicity.\nProposition 2. Given a directed bucket-tree T = (V,E), where V are the bucket variables and E are the directed edges, any partial order along the tree and, in particular the order of bucket-elimination execution, yields monotonic processing of buckets relative to elimination.\nSince the variables in the scope of each function correspond to nodes in the bucket-tree that have yet to be processed, we can infer the following.\nProposition 3. The partial order along the buckettree yields a strict ordering within each function scope.\nAlgorithm Scope Ordering takes as input a buckettree and orders the scopes of functions in all buckets, according to the topological ordering of the given bucket-tree. It works by traversing the bucket tree from the root in a breadth- rst manner and at each bucket ordering the scopes of the input functions wrt the bucket's output function."
    }, {
      "heading" : "3.1.2 Ordering Constraints due to Combination",
      "text" : "Our discussion thus far has focused on the elimination step. During the combination step, a new set of ordering constraints are needed to ensure monotonicity. To illustrate, consider the following example. Assume we are computing a function f(X1, X2, X3), by combining and eliminating variable Y from functions\nh1(X1, X2, Y ), h2(X1, X3, Y ), h3(X2, X3, Y ). Further assume that the domain size of all variables is 3. From the previous section, we know that Y must be the last variable in the scope of the combined function. Thus, when computing f(X1 = 1, X2 = 1, X3 = 2), we need to access entries h2(X1 = 1, X3 = 2, Y = 0, 1, 2), corresponding to indices 15, 16 and 17 in table h2. The next entry of f(X1 = 1, X2 = 2, X3 = 0), requires entries h2(X1 = 1, X3 = 0, Y = 0, 1, 2), corresponding to indices 9, 10 and 11. Finally, the entry f(X1 = 1, X2 = 2, X3 = 1) requires entries h2(X1 = 1, X3 = 1, Y = 0, 1, 2), corresponding to indices 12, 13 and 14. This enumeration of h2 is nonmontonic, even though it is consistent with the constraints imposed by elimination.\nThe problem occurs because the scope of f has a 'gap' with respect to the scope of h2. Speci cally, f has a variable X2 between variables X1 and X3 that is not in the scope of h2. In this example, there is no ordering of the variables that will avoid such a 'gap'. In many situations, such gaps occur due to ordering constraints imposed by the elimination order.2"
    }, {
      "heading" : "3.2 Block Computation",
      "text" : "As discussed earlier, we assume that all tables of intermediate functions are handled (i.e. loaded, computed, saved) as blocks. Clearly, loading/saving one table entry at a time is ine cient. Intuitively, then, we should divide function tables into blocks that are as large as possible. However, block size is limited by several factors. First, our algorithm is operating in a shared memory setup and each thread requires memory to operate. In addition, processing a bucket requires enough space in memory for the output function block and the blocks of each input function. Our goal is thus to determine how to divide function tables into blocks that minimize unutilized memory.\nTo simplify this problem we make the following design assumptions:\n1. We assume the original functions occupy little space and can be stored in memory at all times; 3\n2. We assume that each thread uses the same amount of memory and that the memory allocated to a thread remains xed throughout the algorithm's execution;\n3. We assume that each bucket's output function table is broken into equally sized blocks; and\n2The removal of 'gaps' is untreated at this point and we are currently exploring a variety of approaches for dealing with this issue.\n3This assumption is for simplicity. The original function could also be broken into blocks and stored on the disk.\n4. When computing a block of an output function, a thread requires enough space in memory for the output function block and the necessary blocks from each input function. That is, to compute a block of fXp = ∑ Xp ∏ f∈BXp f , we assume that\nthe needed blocks of f ∈ BXp are in memory.\nThe rst two assumptions imply that the amount of memory available to each thread, denoted by Mpt, is Mpt = (M − O)/m, where M is the total memory available, O is the memory occupied by the original functions and m is the number of threads. It is worth noting that the third design restriction does not imply that all block sizes are the same size.\nUnder the above design restrictions, a workable and simple upper bound on block size of all the functions residing in a bucket BX is Mpt/degBX - i.e. allocate the memory equally among the output function of a block and the output functions of its children in the bucket tree. Since each block is used twice - once as an output block and once as input block - and the degree of the buckets operating on a block may (and most likely will) be di erent, we need to coordinate this upper-bound between adjacent buckets.\nTo illustrate this issue, consider the function fu sent by bucket u to bucket v in Figure 4. When function fu is computed, its bucket imposes an upper bound on its block size as Mpt/3, since its degree is 3. However, when fu is used by parent bucket, v, its block size is bounded by Mpt/6, since bucket v's degree is 6. Therefore, setting the block size of fu toMpt/3 equally among all the bucket's function block would violate the fourth design restriction when bucket v is processed.\nThe limitations on block sizes can be captured more formally by the following set of simultaneous constraints. Given a bucket tree and the root node of the bucket tree, the block size of each bucket must satisfy\nMpt = i + bi + ∑\nj∈C(Xi)\nbj for each bucket i (1)\nwhere bi is the block size for bucket i, C(Xi) is the set of children of bucket i and i is the unutilized space in the computation of blocks for bucket i.\nWith the block size constraints in place, the problem of minimizing the amount of unutilized memory can be formalized as\n{b?1, ..., b?n} = argmin{b1,...,bn} n∑\ni=1\ni (2)\ns.t. Eqn.1, bi ≥ 0, i ≥ 0, ∀i\nwhere Mpt in Eqn.1 is a constant and the children of each node are governed by the underlying bucket-tree structure.\nEqn. 2 is a standard constraint satisfaction problem. In Figure 5 we provide a greedy algorithm that provides a feasible, though possibly suboptimal, solution to this problem. The algorithm computes block sizes starting with the buckets of highest degree (since they are the most constrained) and continues processing buckets in decreasing order of degree. At each bucket the remaining memory (i.e. the memory not already allocated to functions in that bucket) is divided equally between the undetermined functions in that bucket.\nProposition 4. The complexity of the Block Size Computation algorithm is O(n · log(n)), where n is the number of variables.\nIt is worth noting that many di erent block sizes can satisfy the constraints in Eqn.1 depending on which variable is chosen as the root of the tree. This is why we require the bucket-tree structure to be xed in advance. It is also worth noting that bi, the block size for bucket i, is not a continuous variable; rather it is some multiple of the operating system's cluster size. However, in practice we have found this relaxation to be non-problematic."
    }, {
      "heading" : "3.3 The BEEM Algorithm",
      "text" : "We have developed a new algorithm, called BEEM, that incorporates the design ideas and algorithms presented in this section. The basic outline of the BEEM algorithm is given in Figure 6.\nThere is a 1-to-1 mapping between blocks and les. A le name is a concatenation of the bucket's variable (that generated the function that the block belongs to) and the block index (wrt the function table). For example, if the function generated by bucket BX is split into 5 blocks, the 5 les that contain the data will be named \"X-1\", \"X-2\", \"X-3\", \"X-4\", \"X-5\". When a particular block is needed, the program looks for and loads/saves a le with the appropriate name. A block is an array of double-precision oating-point numbers, and entire block/ le can be loaded/saved with a single command.\nA basic step of the algorithm is computing a function table block. First we pick the block to compute, from an eligible bucket (Computation.1 ). A block is computed by enumerating all entries as described in Computation.2. In particular, we determine the indices of the entries in each input table and output table as described in the Function Table Indexing section. Based on these indices, we can then determine which block from each input table is needed to carry out the computation.\nOur scope ordering heuristic guarantees that, for any function f(X1, ..., Xk, Y ), where Y is the variable being eliminated, given any assignment of values to X1, ..., Xk, all entries of the table corresponding to all di erent assignments to Y reside within the same block. This implies that when a thread computes an entry in the table of a bucket's output function, it needs exactly one block from each of the input tables (see Computation.2.d).\nProposition 5. Algorithm BEEM, given in Figure 6, is correct in terms of computing P (e). It performs the same amount of work, O(n·kw∗), as regular BE, where n is the number of variables, k is variable domain size and w∗ is the induced width."
    }, {
      "heading" : "4 Experimental Evaluation",
      "text" : "To evaluate the BEEM algorithm, we compared its performance with that of two other algorithms on computing the probability of evidence on probabilistic networks (Bayesian and Markov). In our comparison, we used a set of problems (called linkage/pedigree problems) derived from genetics. These problems were used\nin the solver competition held at the UAI-2008 conference4. Many of these problems were not solved in that competition and, in addition, we also consider a class of problems - type 4 linkage - with high induced width and large numbers of variables.\nThe two algorithms we used for comparison were: 1) VEC (Variable Elimination and Conditioning) [3]; and ACE[1]. Both VEC and ACE participated in the UAI2008 solver competition. In their class (exact solvers for probabilistic networks, solving the P(e) problem) VEC/ACE were the two best solvers at the UAI-2008 competition, so a comparison with them seems warranted. A brief description of these algorithms follows in the next section.\nOur experiments were carried out on a PC with an Intel quad-core processor, using 4 X 2TB hard disks in RAID-5 con guration (a total of 6TB of disk space). BEEM was con gured so that m = 4 worker threads. As a space-saving measure, we deleted all input functions to a bucket after its output function was computed. All algorithms were given 1GB of RAM (main) memory and both VEC and BEEM were given the same variable orderings."
    }, {
      "heading" : "4.1 VEC",
      "text" : "VEC is an algorithm that uses conditioning to produce sub-problems with induced widths small enough to be solved by an elimination algorithm5. A basic outline of VEC is as follows:\n• As a pre-processing step, VEC reduces variable domains by converting all 0-probabilities to a SAT problem F and checks for each assignment X = a whether (F andX = a) is consistent. Inconsistent assignments are pruned from the domain of X.\n• Repeatedly, remove conditioning variables from the problem until the remaining problem ts within 1GB of main memory.\n• Enumerate all value combinations of the conditioning variables. For each assignment, solve the remaining problem using variable elimination. Combine conditioned subproblem solutions to yield a solution to the entire problem."
    }, {
      "heading" : "4.2 ACE",
      "text" : "ACE is a software package for performing exact inference on Bayesian networks developed in the Au-\n4For a report on the results of the competition, see http://graphmod.ics.uci.edu/uai08/Evaluation/Report.\n5see http://graphmod.ics.uci.edu/group/Software for more detail on VEC\ntomated Reasoning Group at UCLA6. ACE operates by compiling a Bayesian network into an Arithmetic Circuit (AC) and then using this AC to execute queries. Compiling into an AC occurs by rst encoding the Bayesian network into Conjunctive Normal Form (CNF) and then extracting the AC from the factored CNF [1]. Encoding a network in this way e ciently exploits determinism, allowing ACE to answer queries on large networks in the UAI-08 solver competition."
    }, {
      "heading" : "4.3 Results",
      "text" : "Preliminary results from running the three algorithms on a single class of problems are shown in Tables 1 and 2. In these tables N indicates the number of variables, w? is an upper bound on the induced width (determined experimentally using several min- ll orderings) and K is the maximum domain size. The run time is presented in hh:mm:ss format and '>24 h' indicates the algorithm failed to compute p(e) in 24 hours, while 'OOM' indicates the algorithm exceeded the allotted 1 GB of RAM.\nTable 1 contains results on pedigree problems with a few hundred to a thousand variables. Table 2 contains results from a set of problems with several thousand variables. On both sets of problems, we observe a few interesting phenomena. First, if a problem has w? small enough that the problem ts into memory, all three algorithms compute p(e) very rapidly. In such situations, VEC and ACE may actually outperform BEEM because of the overhead associated with multithreading. However, only BEEM and VEC are capable of solving problems that do not t into RAM. In such situations, we see that the cost associated with reading and writing to hard disk is far less than the cost of conditioning. Finally, BEEM successfully computed p(E) for problems 7, 9, 13, 31, 34, 41, 50 and 51 for which an exact solution is not known."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We proposed an extension of the Bucket Elimination algorithm that utilizes external disk space for storing intermediate function tables. Extending the BE algorithm in this manner and also parallelizing computation is a non-trivial matter. In this paper we identied and addressed a number of key issues, including the decomposition of functions into appropriately sized blocks and processing to minimize access to hard disk.\nWhile the performance of our algorithm is not fully optimized, it has shown very promising results on a class of large probabilistic networks. The algorithm demonstrates improved scalability, allowing exact computa-\n6http://reasoning.cs.ucla.edu/\ntion of p(e) on problems not before solved by a generalpurpose algorithm. To better understand its performance, we plan to run BEEM on several additional classes of problems. In addition to further improving the table decomposition and computation schemes, we also plan to extend BEEM for belief updating on variables other than the root and to handle more general tree decompositions. As illustrated in this paper, such modi cations will inevitably impact the way in which tables are decomposed and processed."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part by the NSF under award number IIS-0713118 and by the NIH grant R01HG004175-02."
    } ],
    "references" : [ {
      "title" : "Compiling bayesian networks with local structure",
      "author" : [ "M. Chavira", "A. Darwiche" ],
      "venue" : "Proc. of 19th Intl. Joint Conf. on AI, pages 1306 1312",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Recursive conditioning",
      "author" : [ "A. Darwiche" ],
      "venue" : "Arti cial Intelligence, 126(1-2):5 41",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Bucket elimination: A unifying framework for probabilistic inference",
      "author" : [ "R. Dechter" ],
      "venue" : "pages 211 219",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "And/or search spaces for graphical models",
      "author" : [ "R. Dechter", "R. Mateescu" ],
      "venue" : "Artif. Intell., 171(2- 3):73 106",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Unifying cluster-tree decompositions for reasoning in graphical models",
      "author" : [ "K. Kask" ],
      "venue" : "Arti cial Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Linear-time disk-based implicit graph search",
      "author" : [ "R.E. Korf" ],
      "venue" : "J. ACM, 55(6):1 40",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Minimizing disk i/o in two-bit breadthrst search",
      "author" : [ "R.E. Korf" ],
      "venue" : "Proc. of 23rd Natl. Conf. on AI, pages 317 324. AAAI Press",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Boosting search with variable elimination in constraint optimization and constraint satisfaction problems",
      "author" : [ "J. Larrosa", "R. Dechter" ],
      "venue" : "Constraints, 8(3):303 326",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : "Morgan Kaufmann",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Global conditioning for probabilistic inference in belief networks",
      "author" : [ "R.D. Shachter", "S.K. Andersen" ],
      "venue" : "Proc. 10th Conf. on Uncert. in AI, pages 514 522",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "best- rst, depth- rst branch and bound) [3, 4, 5, 2].",
      "startOffset" : 40,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "best- rst, depth- rst branch and bound) [3, 4, 5, 2].",
      "startOffset" : 40,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "best- rst, depth- rst branch and bound) [3, 4, 5, 2].",
      "startOffset" : 40,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "best- rst, depth- rst branch and bound) [3, 4, 5, 2].",
      "startOffset" : 40,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "Not surprisingly, various algorithms have been proposed that can work with bounded memory at the expense of additional time [2, 8, 9, 10].",
      "startOffset" : 124,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "Not surprisingly, various algorithms have been proposed that can work with bounded memory at the expense of additional time [2, 8, 9, 10].",
      "startOffset" : 124,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "Not surprisingly, various algorithms have been proposed that can work with bounded memory at the expense of additional time [2, 8, 9, 10].",
      "startOffset" : 124,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "Not surprisingly, various algorithms have been proposed that can work with bounded memory at the expense of additional time [2, 8, 9, 10].",
      "startOffset" : 124,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "Nonetheless, it has been demonstrated in the context of (A*) heuristic search that algorithms can be designed to mitigate such e ects [6, 7], yielding powerful schemes that can be applied to previously unsolvable problems.",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "Nonetheless, it has been demonstrated in the context of (A*) heuristic search that algorithms can be designed to mitigate such e ects [6, 7], yielding powerful schemes that can be applied to previously unsolvable problems.",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 2,
      "context" : "In the remainder of this paper, we describe how a speci c inference-based algorithm, BE [3], can be modi ed to use external memory.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Bucket Elimination (BE) is a special case of cluster tree elimination in which the tree-structure upon which messages are passed is determined by the variable elimination order used [3].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "Figure 2: The Bucket Elimination Algorithm [3]",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "The two algorithms we used for comparison were: 1) VEC (Variable Elimination and Conditioning) [3]; and ACE[1].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "The two algorithms we used for comparison were: 1) VEC (Variable Elimination and Conditioning) [3]; and ACE[1].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Compiling into an AC occurs by rst encoding the Bayesian network into Conjunctive Normal Form (CNF) and then extracting the AC from the factored CNF [1].",
      "startOffset" : 149,
      "endOffset" : 152
    } ],
    "year" : 2010,
    "abstractText" : "A major limitation of exact inference algorithms for probabilistic graphical models is their extensive memory usage, which often puts real-world problems out of their reach. In this paper we show how we can extend inference algorithms, particularly Bucket Elimination, a special case of cluster (join) tree decomposition, to utilize disk memory. We provide the underlying ideas and show promising empirical results of exactly solving large problems not solvable before.",
    "creator" : "TeX"
  }
}