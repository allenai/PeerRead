{
  "name" : "1503.03535.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Using Monolingual Corpora in Neural Machine Translation",
    "authors" : [ "Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation is a novel approach to machine translation that has shown promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014). Until recently, the application of neural networks to machine translation\n? Equal contribution † Work done while author was at Université de Montréal\nwas restricted to extend standard machine translation tools for rescoring translation hypotheses or reranking n-best lists (see, e.g., Schwenk, 2012, 2007a). The neural machine translation approach, however, showed that it is possible to build a competitive end-to-end neural network-based translation system for English-French and EnglishGerman (Sutskever et al., 2014; Jean et al., 2014) (also see Section 2).\nA large part of the recent success of neural machine translation systems for these language pairs is due to the availability of very large amounts of high quality, sentence aligned corpora. For the majority of language pairs however, this is not the case since large refined corpora are in general scarce. This can be a problem even in the case of high-resource languages (such as ChineseEnglish) where in a domain specific setting there can also be a lack of parallel corpora. In this work, we focus on such low resource language pairs and domain specific translation problems which is a common scenario in machine translation. In the case of a lack of data, applicability of neural machine translation to these low-resource translation tasks can be restricted.\nOn the other hand, monolingual data is almost always both universally available and abundant. Despite being “unlabeled”, monolingual corpora still exhibit rich linguistic structure that may be useful for translation tasks. This presents an opportunity to leverage large available monolingual corpora to give hints to the neural machine translation system. One way to incorporate monolingual corpora in any translation system is to integrate a language model (LM) trained on the monolingual dataset (target language) into the translation system. This approach has been traditionally used heavily in machine translation (Hoang et al., 2007) and speech recognition system (Young et al., 1996).\nIn this paper, we explore methods for integrat-\nar X\niv :1\n50 3.\n03 53\n5v 1\n[ cs\n.C L\n] 1\n1 M\nar 2\n01 5\ning a language model into neural machine translation systems. In particular, we focus on two challenging, low-resource translation tasks in TurkishEnglish (Tr-En) and Chinese-English (Zh-En). Available parallel corpora for these two pairs are significantly smaller than those available for language pairs used in previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) such as English-French. For instance, the OpenMT’15 English-Chinese bilingual corpus (the largest dataset we used in this work) has approximately 40 times fewer examples than the commonly used subset of the WMT’14 English-French dataset.\nIn addition to the lack of large parallel corpora, both Tr-En and Zh-En translation tasks pose unique challenges arising from the linguistic properties of Turkish and Chinese. The morphology of Turkish, which is an agglutinative language, is richer than that of many Indo-European languages (e.g., English and French). This leads to the exploding vocabulary problem without proper segmentation (Oflazer et al., 1994). Chinese sentences, on the other hand, do not utilize blank spaces to separate words, making it difficult to build a vocabulary of word tokens which are often the unit of translation in the traditional statistical machine translation (SMT) system (Koehn, 2010). In our work, we empirically show that the neural machine translation system performs on these challenging language pairs as well as, or better than the existing SMT systems.\nIn Section 2, we review recent work in neural machine translation. We present our basic model architecture in Sec. 3 and describe our shallow and deep fusion approaches in Sec. 4. Finally, we describe our main experimental results in Sec. 6."
    }, {
      "heading" : "2 Background: Neural Machine Translation",
      "text" : "SMT systems maximize the conditional probability p(y | x) of a correct target translation y given a source sentence x. This is done by maximizing separately a language model p(y) and the (reverse) translation model p(x | y) component by using Bayes’ rule:\np(y | x) ∝ p(x | y)p(y).\nThis decomposition into a language model and translation model is meant to make full use of available corpora: monolingual corpora for fitting\nthe language model and parallel corpora for the translation model. In reality, however, SMT systems tend to model log p(y | x) directly by linearly combining multiple features by using a socalled log-linear model:\nlog p(y | x) = ∑ j fj(x,y) + C, (1)\nwhere fj is the j-th feature based on both or either of the source and target sentences, and C is a normalization constant which is often ignored. These features include, for instance, pair-wise statistics between two sentences/phrases. The log-linear model is fitted to data, in most cases, by maximizing an automatic evaluation metric other than an actual conditional probability, such as BLEU.\nNeural machine translation, on the other hand, aims at directly optimizing log p(y | x) including the feature extraction as well as the normalization constant by a single neural network. This is typically done under the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) consisting of neural networks. The first network encodes the source sentence x into a continuous-space representation from which the decoder decodes the target translation sentence. By using RNN architectures equipped to learn long term dependencies such as Gated Recurrent Units (GRU)/Long Short-Term Memory (LSTM), the whole system can be trained a end-to-end fashion (Cho et al., 2014; Sutskever et al., 2014).\nOnce the model learns the conditional distribution or translation model, given a source sentence we can find a translation that approximately maximizes the conditional probability using, for instance, a beam search algorithm."
    }, {
      "heading" : "3 Model Description",
      "text" : "We used the model proposed recently by Bahdanau et al. (2014) that learns to jointly (soft)align and translate as a baseline neural machine translation system in this paper. Here, we describe in detail this model to which we refer as “NMT”.\nThe encoder of the NMT is a bidirectional RNN which consists of forward and backward RNNs (Schuster and Paliwal, 1997). The forward RNN reads the input sequence/sentence x = (x1, . . . , xT ) in a forward direction, resulting in a sequence of hidden states ( −→ h 1, . . . , −→ h T ). The backward RNN reads x in an opposite direction\nand outputs ( ←− h 1, . . . , ←− h T ). We concatenate a pair of hidden states at each timestep to build a sequence of annotation vectors (h1, . . . ,hT ), where\nhj = [ ←− h j−→ h j ] .\nEach annotation vector hj encodes information about the j-th word with respect to all the other surrounding words in the sentence.\nIn our decoder, which we construct with a single layer RNN, at each time step t a soft-alignment mechanism first decides on which annotation vectors are most relevant. The relevance weight αtj of the j-th annotation vector for the t-th target word is computed by a feedforward neural network f that takes as input hj and the previous decoder’s hidden state st−1:\netj = f(st−1,hj).\nThe outputs etj are normalized over the sequence of the annotation vectors so that the they sum to 1:\nαtj = exp(etj)∑T k=1 exp(etk) ,\nand we call αtj a relevance score, or an alignment weight, of the j-th annotation vector.\nThe relevance scores are used to get the context vector ct of the t-th word in the translation:\nct = T∑ j=1 αtjhj ,\nThen, the decoder’s hidden state sTMt at time t is computed based on the previous hidden state sTMt−1, the context vector ct and the previously translated word yt−1:\nst = fr(sTMt−1,yt−1, ct), (2)\nwhere fr is the gated recurrent unit (Cho et al., 2014).\nWe use a deep output layer (Pascanu et al., 2014) to compute the conditional distribution over words:\np(yt|y<t,x) ∝ exp(y>t (Wofo(s TM t ,yt−1, ct) + bo)), (3)\nwhere yt is an one-hot encoded vector indicating one of the words in the target vocabulary. Wo is a learned weight matrix and bo is a bias. fo\nis a single-layer feedforward neural network with a 2-way maxout non-linearity (Goodfellow et al., 2013).\nThe whole model, including both the encoder and decoder, is jointly trained to maximize the (conditional) log-likelihood of the bilingual training corpus:\nmax θ\n1\nN N∑ n=1 log pθ(y (n)|x(n)),\nwhere the training corpus is a set of (x(n),y(n))’s, and θ denotes a set of all the tunable parameters."
    }, {
      "heading" : "4 Integrating Language Model into the Decoder",
      "text" : "In this paper, we propose two alternatives to integrating a language model into a neural machine translation system which we refer as shallow fusion (Sec. 4.1) and deep fusion (Sec. 4.2). Without loss of generality, we use a language model based on recurrent neural networks (RNNLM, Mikolov et al., 2011) which is equivalent to the decoder described in the previous section except that it is not biased by a context vector (i.e., ct = 0 in Eqs. (2)– (3)).\nIn the sections that follow, we assume that both a NMT model (on parallel corpora) as well as a RNNLM (on larger monolingual corpora) have been pre-trained separately before being integrated. We denote the hidden state at time t of the RNNLM with sLMt ."
    }, {
      "heading" : "4.1 Shallow Fusion",
      "text" : "Shallow fusion is analogous to how language models are used in the decoder of a usual SMT system. At each time step, the translation model proposes a set of candidate words. The candidates are then scored according to the weighted sum of the scores given by the translation model and the language model.\nIn more detail, at each time step t, the translation model (in this case, the NMT) will compute the score of every possible next word for each hypothesis in a set of hypotheses { y (i) ≤t−1 } . Each score is the summation of the score of the hypothesis and the score given by the NMT to the word. All these new hypotheses (a hypothesis from the previous timestep with a next word appended at the end) are then sorted according to their respec-\ntive scores, and the topK ones are selected as candidates { ŷ (i) ≤t } i=1,...,K .\nWe then rescore these hypotheses with the weighted sum of the scores by the NMT and RNNLM, where we only need to recompute the score of the “new word” at the end of each candidate hypothesis. The score of the new word is computed by\nlog p(yt = k) = log pTM(yt = k)\n+ β log pLM(yt = k), (4)\nwhere β is a hyper-parameter that needs to be tuned to maximize the translation performance on a development set.\nSee Fig. 1 (b) for illustration."
    }, {
      "heading" : "4.2 Deep Fusion",
      "text" : "In deep fusion, we integrate the RNNLM and the decoder of the NMT by concatenating their hidden states next to each other (see Fig. 1 (b)). The model is then finetuned to use the hidden states from both of these models when computing the output probability of the next word (see Eq. (3)). Unlike the vanilla NMT (without any language model component), the hidden layer of the deep output takes as input the hidden state of the RNNLM in addition to that of the NMT, the previous word and the context such that\np(yt|y<t,x) ∝ exp(y>t (Wofo(s LM t , s TM t ,yt−1, ct) + bo)),\n(5)\nwhere again we used the superscripts LM and TM to distinguish the hidden states of the RNNLM and NMT.\nDuring the finetuning of the model, we tune only the parameters that were used to parameterize the output (5). This is to ensure that the structure learned by the LM from monolingual corpora is not overwritten. It is possible to use monolingual corpora as well while finetuning all the parameters, but in this paper, we altered only the output parameters in the stage of finetuning."
    }, {
      "heading" : "4.2.1 Balance between LM and TM",
      "text" : "Intuitively, some words in the translated sentence are decided based mostly on the source sentence (i.e., by the TM), but some other words will be mostly based on the language model (LM). For example, there are no Chinese word that corresponds\nto articles in English, and in the case of Chinese to English translation, the decoder will need to insert a correct/appropriate article based mainly on the signal from the LM. On the other hand, if a noun is to be translated, it may be better to ignore any signal from the LM, as it may prevents the decoder from choosing the correct translation.\nIn order for the decoder to flexibly balance between the signal from the LM and TM, we augment the decoder with a so-called controller. The controller is implemented as a function taking as input the hidden state of the LM and computing\ngt = σ ( v>g s LM t + bg ) , (6)\nwhere σ is a logistic sigmoid function. vg and bg are learned parameters.\nThe output of the controller is then multiplied to the hidden state of the LM. This means that we let the decoder use the signal from the TM fully, while the controller controls the magnitude of the LM signal.\nFrom our experiments, we empirically found that it is better to initialize the bias bg to a small, negative number. This allows the decoder to decide the importance of the LM only when it is deemed necessary."
    }, {
      "heading" : "5 Datasets",
      "text" : "We evaluate the proposed approaches to incorporating monolingual data in neural machine translation in two tasks: Chinese to English (Zh-En) and Turkish to English (Tr-En). We describe each of these in more detail below."
    }, {
      "heading" : "5.1 Parallel Corpora",
      "text" : ""
    }, {
      "heading" : "5.1.1 Zh-En: OpenMT’15",
      "text" : "We used the parallel corpora made available as a part of the NIST OpenMT’15 Challenge. Sentence-aligned pairs from three domains are combined to form a training set: (1) SMS/CHAT and (2) conversational telephone speech (CTS) from DARPA BOLT Project, and (3) newsgroups/weblogs from DARPA GALE Project. In total, the training set consisted of 430K sentence pairs (see Table 1 for the detailed statistics). We trained models with this training set and the development set (the concatenation of the provided development and tune sets from the challenge), and eval-\nIn all our experiments, we set bg = −1 to ensure that gt is initially 0.2 on average.\nuated them on the test set. The domain of the development and test sets was restricted to CTS.\nPreprocessing Importantly, we did “not segment” the Chinese sentences and considered each character as a symbol, unlike a more traditional approach of using a separate segmentation tool to segment the Chinese characters into words (Devlin et al., 2014). Any consecutive non-Chinese characters (e.g latin characters) were, however, considered as an individual word. Lastly, we removed any HTML/XML tags from the corpus, chose only the intended meaning word if both intended and literal translations are available and ignored any indicator of, e.g., typos. The only preprocessing we did on the English side of the corpus was a simple tokenization using the tokenizer from Moses.\nhttps://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/tokenizer.perl"
    }, {
      "heading" : "5.1.2 Tr-En: IWSLT’14",
      "text" : "We used the WIT parallel corpus (Cettolo et al., 2012) and SETimes parallel corpus made available as a part of the IWSLT’14 (machine translation track). The corpus consists of the sentence-aligned subtitles of TED and TEDx talks, and we concatenated dev2010 and tst2010 to form a development set, and tst2011, tst2012, tst2013 and tst2014 to form a test set. See Table 1 for the detailed statistics of the parallel corpora.\nPreprocessing As done with the case of ZhEn, we removed any special symbols from the corpora initially and tokenized the Turkish side with the tokenizer provided by Moses. In order to overcome the exploding vocabulary due to the rich inflections and derivations in Turkish, we segmented each Turkish sentence into a sequence of sub-word units using Zemberek followed by morphological disambiguation on the morphological\nhttps://github.com/ahmetaa/ zemberek-nlp\nanalysis (Sak et al., 2007). We removed any nonsurface morphemes corresponding to, for instance, part-of-speech tags."
    }, {
      "heading" : "5.2 Monolingual Corpora",
      "text" : "For language modeling, the English Gigaword corpus by the Linguistic Data Consortium, which consists of newswire documents, was allowed for both OpenMT-15 and IWSLT-15 challenges. The tokenized Gigaword corpus was directly used without additional preprocessing steps to train an external neural language model for English side of both Zh-En and Tr-En neural machine translation systems. It should be noted that the additional monolingual corpus was out of domain for both OpenMT-15 and IWSLT-15."
    }, {
      "heading" : "6 Settings",
      "text" : ""
    }, {
      "heading" : "6.1 Training Procedure",
      "text" : ""
    }, {
      "heading" : "6.1.1 Neural Machine Translation",
      "text" : "The input and output to the network were sequences of one-hot vectors whose dimensionality corresponds to the sizes of the source and target vocabularies, respectively. We constructed the vocabularies with the most common words in the parallel corpora. The vocabularies sizes for Chinese, Turkish and English, were 10K, 30K and 40K respectively. Each word was projected into the continuous space of 620-dimensional Euclidean space first to reduce the dimensionality, on both the encoder and the decoder. We chose the size of the recurrent units for Zh-En and Tr-En to be 1200 and 1000.\nEach model was optimized using Adadelta (Zeiler, 2012) with a minibatch of 80 samples. At each update, we used gradient clipping such that if the L2 norm of the gradient exceeds 5, we renormalized it back to 5 (Pascanu et al., 2013). For the non-recurrent layers, we used dropout (see Eq. (3)) and added Gaussian noise (std. dev. 0.001) to each parameter to prevent overfitting (Graves, 2011). Training was early-stopped to maximize the performance on the development set measured by BLEU. We initialized all recurrent weight matrices as random orthogonal matrices.\nWe compute the BLEU score using the multi-blue.perl script from Moses on tokenized sentence pairs."
    }, {
      "heading" : "6.1.2 Language Model",
      "text" : "We trained two recurrent neural network language model (RNNLM) using 2400 LSTM units on English Gigaword Corpus using respectively the vocabularies constructed from Zh-En and Tr-En corpora. Any sentence with more than 10% words unknown was discarded. We used early stopping when training each of these RNNLM’s on the corresponding development set’s perplexity."
    }, {
      "heading" : "6.2 Shallow and Deep Fusion",
      "text" : ""
    }, {
      "heading" : "6.2.1 Shallow Fusion",
      "text" : "The hyperparameter β in Eq. 4 was tuned to maximize the translation performance on the development set, from the range between 0.001 and 0.1. In our experiment, we found it important to renormalize the softmax of the LM without the end of sequence sentence and out of vocabulary symbol (i.e β = 0 in (4)). We suspect this is due to domain different and also a bias towards longer sentences in the LM’s training domain."
    }, {
      "heading" : "6.2.2 Deep Fusion",
      "text" : "We finetuned the parameters of the deep output layer (Eq. (5)) as well as the controller (see Eq. (6) using the Adam optimizer (Kingma and Ba, 2014) for Zh-En and RMSProp with momentum on TrEn. During the finetuning, the dropout probability and the standard deviation of the weight noise were set to 0.56 and 0.005, based on preliminary experiments, which are reduced after the first 10K updates."
    }, {
      "heading" : "7 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "7.1 Zh-En: OpenMT’15",
      "text" : "In addition to NMT-based systems, we also trained a phrase-based as well as hierarchical phrasebased SMT systems (Koehn et al., 2003; Chiang, 2005) with/without re-scoring by an external neural language model (CSLM, Schwenk, 2007b). We present the results in Table 2.\nWe observed that integrating an additional LM by deep fusion (see Sec. 4.2) helped the models achieving better performance in general, except in the case of the CTS task. Interestingly, we saw that the NMT-based models, regardless of whether the LM was integrated or not, outperformed the more traditional phrase-based SMT systems.\nGigaword corpus is prepared by John Hopkins University Human Language Technology Center and provided by LDC."
    }, {
      "heading" : "7.2 Tr-En: IWSLT’14",
      "text" : "In Table 3, we present the results on Tr-En. Compared to Zh-En, we saw a greater performance improvement (up to +1.19 BLEU points) from the basic NMT to the NMT integrated with the LM via the proposed method of deep fusion. Furthermore, by incorporating the LM via deep fusion, the NMT systems were able to significantly outperform the previously reported best result (Yılmaz et al., 2013) (up to +1.96 BLEU points) on all of the separate test sets without ensemble learning."
    }, {
      "heading" : "7.3 Analysis: Effect of Language Model Performance",
      "text" : "Performance improvements from integrating language model are heavily dependent on the degree of similarity between the domain of monolingual corpora and the target domain of translation. The results we reported in this paper reflect this dependency clearly.\nIn the case of Zh-En, intuitively, we can tell that the style of writing in both SMS/CHAT as well as the conversational speech will be significantly different from that of news articles (which constitutes the majority of the English Gigaword corpus). This is clearly supported by the high per-\nplexity on the development set with our LM (see the column Zh-En of Table 4). This explains the marginal improvement we observed in Sec. 7.1.\nOn the other hand, in the case of Tr-En, the similarity between the domain of the monolingual corpus and that of translation is higher (see the column Tr-En of Table 4). Accordingly, we were able to gain a significant improvement in translation performance by integrating the external language model.\nThis further hints at the importance of the controller (see Sec. 4.2.1) and explains why the proposed shallow fusion did not help, and sometimes hurt, the overall translation performance. In other words, the fusion works only if the final model has a capability of selectively using the additional LM which may well contain useless or adversarial information (according to the task at hand). The controller mechanism embedded in deep fusion implements this capability, and thus makes the model more robust to dissimilarities in the domains."
    }, {
      "heading" : "8 Conclusion and Future Work",
      "text" : "In conclusion, despite recent successes, neural machine translation (NMT) suffers from the fact that it is not able to use the vast amount of available monolingual data. The ability of incorporating monolingual corpus is especially important for the common case of low-resource language pairs, which may be the reason why the recent success of NMT has largely been for pairs of European languages (En-Fr and En-De).\nIn this paper, we proposed two alternative approaches to this problem which allow NMT models leverage monolingual data: shallow fusion and deep fusion. We empirically evaluated these approaches on two low-resource language pairs: EnZh (SMS/Chat and conversational speech) and EnTr (TED/TEDx Subtitles). We observed a significant performance improvement when the NMT model was integrated with the external language model (LM) using the proposed deep fusion. Furthermore, on both language pairs, the NMT models trained with deep fusion were able to achieve better results than the existing phrase-based statistical machine translation systems (up to +1.96 BLEU points on En-Tr).\nCareful analysis also revealed that the performance improvement from incorporating an external LM was highly dependent on the domain similarity between the monolingual corpus and the target task. This also explained why deep fusion, which implements an adaptive mechanism for modulating information from the integrated LM, works better than shallow fusion. This analysis also suggests that in the future, domain adaption of the language model may further improve the performance."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We acknowledge the support of the following organizations for research funding and computing support: NSERC, Samsung, Calcul Québec, Compute Canada, the Canada Research Chairs and CIFAR. O.F. is funded by TUBITAK 2214-A Program. The authors of this paper from LIUM are funded by DARPA BOLT."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : "Submited to the Deep Learning and Unsuper-",
      "citeRegEx" : "Bastien et al\\.,? 2012",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "D. Farley", "Y. Bengio" ],
      "venue" : "Proceedings of the Python for Scientific Computing Conference (SciPy).",
      "citeRegEx" : "Farley and Bengio,? 2010",
      "shortCiteRegEx" : "Farley and Bengio",
      "year" : 2010
    }, {
      "title" : "Wit3: Web inventory of transcribed and translated talks",
      "author" : [ "M. Cettolo", "C. Girardi", "M. Federico" ],
      "venue" : "Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT), pages 261–268.",
      "citeRegEx" : "Cettolo et al\\.,? 2012",
      "shortCiteRegEx" : "Cettolo et al\\.",
      "year" : 2012
    }, {
      "title" : "A hierarchical phrase-based model for statistical machine translation",
      "author" : [ "D. Chiang" ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270. Association for Computational Lin-",
      "citeRegEx" : "Chiang,? 2005",
      "shortCiteRegEx" : "Chiang",
      "year" : 2005
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio" ],
      "venue" : "In Proceedings of the Empiri-",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast and robust neural network joint models for statistical machine translation",
      "author" : [ "J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul" ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Devlin et al\\.,? 2014",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2014
    }, {
      "title" : "Maxout networks",
      "author" : [ "I. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio" ],
      "venue" : "Proceedings of The 30th International Conference on Machine Learning, pages 1319–1327.",
      "citeRegEx" : "Goodfellow et al\\.,? 2013",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2013
    }, {
      "title" : "Practical variational inference for neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2348– 2356.",
      "citeRegEx" : "Graves,? 2011",
      "shortCiteRegEx" : "Graves",
      "year" : 2011
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.2007.",
      "citeRegEx" : "Jean et al\\.,? 2014",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "N. Kalchbrenner", "P. Blunsom" ],
      "venue" : "Proceedings of the ACL Conference on Empirical Meth-",
      "citeRegEx" : "Kalchbrenner and Blunsom,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom",
      "year" : 2013
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "arXiv:1412.6980 [cs.LG].",
      "citeRegEx" : "Kingma and Ba,? 2014",
      "shortCiteRegEx" : "Kingma and Ba",
      "year" : 2014
    }, {
      "title" : "Statistical Machine Translation",
      "author" : [ "P. Koehn" ],
      "venue" : "Cambridge University Press, New York, NY, USA.",
      "citeRegEx" : "Koehn,? 2010",
      "shortCiteRegEx" : "Koehn",
      "year" : 2010
    }, {
      "title" : "Statistical phrase-based translation",
      "author" : [ "P. Koehn", "F.J. Och", "D. Marcu" ],
      "venue" : "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-",
      "citeRegEx" : "Koehn et al\\.,? 2003",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2003
    }, {
      "title" : "Rnnlm-recurrent neural network language modeling toolkit",
      "author" : [ "T. Mikolov", "S. Kombrink", "A. Deoras", "L. Burget", "J. Cernocky" ],
      "venue" : "Proc. of the 2011 ASRU Workshop, pages 196–201.",
      "citeRegEx" : "Mikolov et al\\.,? 2011",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2011
    }, {
      "title" : "An outline of turkish morphology",
      "author" : [ "K. Oflazer", "E. Göçmen", "E. Gocmen", "C. Bozsahin" ],
      "venue" : null,
      "citeRegEx" : "Oflazer et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Oflazer et al\\.",
      "year" : 1994
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "R. Pascanu", "T. Mikolov", "Y. Bengio" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML 2013).",
      "citeRegEx" : "Pascanu et al\\.,? 2013",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "How to construct deep recurrent neural networks",
      "author" : [ "R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio" ],
      "venue" : "Proceedings of the Second International Conference on Learning Representations (ICLR 2014).",
      "citeRegEx" : "Pascanu et al\\.,? 2014",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2014
    }, {
      "title" : "Morphological disambiguation of turkish text with perceptron algorithm",
      "author" : [ "H. Sak", "T. Güngör", "M. Saraçlar" ],
      "venue" : "Computational Linguistics and Intelligent Text Processing, pages 107–118. Springer.",
      "citeRegEx" : "Sak et al\\.,? 2007",
      "shortCiteRegEx" : "Sak et al\\.",
      "year" : 2007
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "M. Schuster", "K.K. Paliwal" ],
      "venue" : "Signal Processing, IEEE Transactions on, 45(11), 2673– 2681.",
      "citeRegEx" : "Schuster and Paliwal,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal",
      "year" : 1997
    }, {
      "title" : "Continuous space language models",
      "author" : [ "H. Schwenk" ],
      "venue" : "Comput. Speech Lang., 21(3), 492– 518.",
      "citeRegEx" : "Schwenk,? 2007a",
      "shortCiteRegEx" : "Schwenk",
      "year" : 2007
    }, {
      "title" : "Continuous space language models",
      "author" : [ "H. Schwenk" ],
      "venue" : "Computer Speech & Language, 21(3), 492–518.",
      "citeRegEx" : "Schwenk,? 2007b",
      "shortCiteRegEx" : "Schwenk",
      "year" : 2007
    }, {
      "title" : "Continuous space translation models for phrase-based statistical machine translation",
      "author" : [ "H. Schwenk" ],
      "venue" : "M. Kay and C. Boitet, editors, Proceedings of the 24th International Conference on Computational Linguistics (COLIN),",
      "citeRegEx" : "Schwenk,? 2012",
      "shortCiteRegEx" : "Schwenk",
      "year" : 2012
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q. Le" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS 2014).",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Tubitak turkishenglish submissions for iwslt 2013",
      "author" : [ "E. Yılmaz", "I.D. El-Kahlout", "B. Aydın", "Z.S. Ozil", "C. Mermer" ],
      "venue" : "Proceedings of the 10th International Workshop on Spoken Language Translation (IWSLT), pages 152–",
      "citeRegEx" : "Yılmaz et al\\.,? 2013",
      "shortCiteRegEx" : "Yılmaz et al\\.",
      "year" : 2013
    }, {
      "title" : "ADADELTA: An adaptive learning rate method",
      "author" : [ "M.D. Zeiler" ],
      "venue" : "arXiv:1212.5701",
      "citeRegEx" : "Zeiler,? 2012",
      "shortCiteRegEx" : "Zeiler",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Neural machine translation is a novel approach to machine translation that has shown promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014).",
      "startOffset" : 103,
      "endOffset" : 200
    }, {
      "referenceID" : 23,
      "context" : "Neural machine translation is a novel approach to machine translation that has shown promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014).",
      "startOffset" : 103,
      "endOffset" : 200
    }, {
      "referenceID" : 5,
      "context" : "Neural machine translation is a novel approach to machine translation that has shown promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014).",
      "startOffset" : 103,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "Neural machine translation is a novel approach to machine translation that has shown promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014).",
      "startOffset" : 103,
      "endOffset" : 200
    }, {
      "referenceID" : 23,
      "context" : "The neural machine translation approach, however, showed that it is possible to build a competitive end-to-end neural network-based translation system for English-French and EnglishGerman (Sutskever et al., 2014; Jean et al., 2014) (also see Section 2).",
      "startOffset" : 188,
      "endOffset" : 231
    }, {
      "referenceID" : 9,
      "context" : "The neural machine translation approach, however, showed that it is possible to build a competitive end-to-end neural network-based translation system for English-French and EnglishGerman (Sutskever et al., 2014; Jean et al., 2014) (also see Section 2).",
      "startOffset" : 188,
      "endOffset" : 231
    }, {
      "referenceID" : 10,
      "context" : "Available parallel corpora for these two pairs are significantly smaller than those available for language pairs used in previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) such as English-French.",
      "startOffset" : 135,
      "endOffset" : 214
    }, {
      "referenceID" : 23,
      "context" : "Available parallel corpora for these two pairs are significantly smaller than those available for language pairs used in previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) such as English-French.",
      "startOffset" : 135,
      "endOffset" : 214
    }, {
      "referenceID" : 0,
      "context" : "Available parallel corpora for these two pairs are significantly smaller than those available for language pairs used in previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) such as English-French.",
      "startOffset" : 135,
      "endOffset" : 214
    }, {
      "referenceID" : 15,
      "context" : "This leads to the exploding vocabulary problem without proper segmentation (Oflazer et al., 1994).",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "Chinese sentences, on the other hand, do not utilize blank spaces to separate words, making it difficult to build a vocabulary of word tokens which are often the unit of translation in the traditional statistical machine translation (SMT) system (Koehn, 2010).",
      "startOffset" : 246,
      "endOffset" : 259
    }, {
      "referenceID" : 10,
      "context" : "This is typically done under the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) consisting of neural networks.",
      "startOffset" : 59,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "This is typically done under the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) consisting of neural networks.",
      "startOffset" : 59,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "This is typically done under the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014) consisting of neural networks.",
      "startOffset" : 59,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "By using RNN architectures equipped to learn long term dependencies such as Gated Recurrent Units (GRU)/Long Short-Term Memory (LSTM), the whole system can be trained a end-to-end fashion (Cho et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 188,
      "endOffset" : 230
    }, {
      "referenceID" : 23,
      "context" : "By using RNN architectures equipped to learn long term dependencies such as Gated Recurrent Units (GRU)/Long Short-Term Memory (LSTM), the whole system can be trained a end-to-end fashion (Cho et al., 2014; Sutskever et al., 2014).",
      "startOffset" : 188,
      "endOffset" : 230
    }, {
      "referenceID" : 19,
      "context" : "The encoder of the NMT is a bidirectional RNN which consists of forward and backward RNNs (Schuster and Paliwal, 1997).",
      "startOffset" : 90,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "We used the model proposed recently by Bahdanau et al. (2014) that learns to jointly (soft)align and translate as a baseline neural machine translation system in this paper.",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "where fr is the gated recurrent unit (Cho et al., 2014).",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "We use a deep output layer (Pascanu et al., 2014) to compute the conditional distribution over words:",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "fo is a single-layer feedforward neural network with a 2-way maxout non-linearity (Goodfellow et al., 2013).",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "Preprocessing Importantly, we did “not segment” the Chinese sentences and considered each character as a symbol, unlike a more traditional approach of using a separate segmentation tool to segment the Chinese characters into words (Devlin et al., 2014).",
      "startOffset" : 231,
      "endOffset" : 252
    }, {
      "referenceID" : 3,
      "context" : "We used the WIT parallel corpus (Cettolo et al., 2012) and SETimes parallel corpus made available as a part of the IWSLT’14 (machine translation track).",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "analysis (Sak et al., 2007).",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 25,
      "context" : "Each model was optimized using Adadelta (Zeiler, 2012) with a minibatch of 80 samples.",
      "startOffset" : 40,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "At each update, we used gradient clipping such that if the L2 norm of the gradient exceeds 5, we renormalized it back to 5 (Pascanu et al., 2013).",
      "startOffset" : 123,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "001) to each parameter to prevent overfitting (Graves, 2011).",
      "startOffset" : 46,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "(6) using the Adam optimizer (Kingma and Ba, 2014) for Zh-En and RMSProp with momentum on TrEn.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "In addition to NMT-based systems, we also trained a phrase-based as well as hierarchical phrasebased SMT systems (Koehn et al., 2003; Chiang, 2005) with/without re-scoring by an external neural language model (CSLM, Schwenk, 2007b).",
      "startOffset" : 113,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "In addition to NMT-based systems, we also trained a phrase-based as well as hierarchical phrasebased SMT systems (Koehn et al., 2003; Chiang, 2005) with/without re-scoring by an external neural language model (CSLM, Schwenk, 2007b).",
      "startOffset" : 113,
      "endOffset" : 147
    }, {
      "referenceID" : 24,
      "context" : "Furthermore, by incorporating the LM via deep fusion, the NMT systems were able to significantly outperform the previously reported best result (Yılmaz et al., 2013) (up to +1.",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 1,
      "context" : "The authors would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).",
      "startOffset" : 57,
      "endOffset" : 102
    } ],
    "year" : 2017,
    "abstractText" : "Recent works on end-to-end neural network-based architectures for machine translation have shown promising results for English-French and English-German translation. Unlike these language pairs, however, in the majority of scenarios, there is a lack of high quality parallel corpora. In this work, we focus on applying neural machine translation to challenging/low-resource languages Turkish and low-resource domains such as parallel corpora of Chinese chat messages. In particular, we investigated how to leverage abundant monolingual data for these low-resource translation tasks. Without the use of external alignment tools, we obtained up to a 1.96 BLEU score improvement with our proposed method compared to the previous best result in Turkish-to-English translation on the IWLST 2014 dataset. On Chinese-toEnglish translation by using the OpenMT 2015 dataset, we were able to obtain up to a 1.59 BLEU score improvement over phrase-based and hierarchical phrase-based baselines.",
    "creator" : "LaTeX with hyperref package"
  }
}