{
  "name" : "1301.6687.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Loglinear models for first-order probabilistic reasoning",
    "authors" : [ "James Cussens" ],
    "emails" : [ "jc@cs." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Recent work on loglinear models in probabilistic constraint logic programming is applied to first order probabilistic reasoning. Probabilities are defined directly on the proofs of atomic formu lae, and by marginalisation on the atomic formu lae themselves. We use Stochastic Logic Pro grams (SLPs) composed of labelled and unla belled definite clauses to define the proof prob abilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and ran dom variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning.\nKeywords: loglinear models, constraint logic program ming, inductive logic programming\n1 Introduction\nA framework which merges first-order logical and prob abilistic inference in a theoretically sound and applicable manner promises many benefits. We can benefit from the compact knowledge representation of logic, and still rep resent and reason about the uncertainty found in most ap plications. Here we propose a conservative extension to the logic programming framework by defining probabilities di rectly on proofs and hence indirectly on atomic formulae. Our conservatism allows us to tie probabilistic and logical concepts very closely. Table I lists the linkages which the proposed approach establishes.\nThis paper is laid out as follows. We begin in Section 2, with a brief overview of logic programming concepts. Sec tion 3 forms the core of the paper where we introduce the loglinear model and Stochastic Logic Programs. Section 4\nthen presents SLPs which represent Markov nets and then more complex models. Section 5 discusses the role of ILP in learning the structure of the loglinear model, focusing on the work of Dehaspe. We discuss related work in Section 6 and briefly mention future work in Section 7.\n2 Logic programming essentials\nWe give a very brief overview of logic programming. For more details, the reader can consult any standard textbook on logic programming, e.g. (Lloyd, 1987). In this pa per we will consider only definite logic programs. Defi nite (logic) programs consist of a set of definite clauses, where each definite clause is a disjunctive first-order for mula such as p(X, Y) V --,q(X, Z) V ..,r(Z) ¢} p(X, Y) + q(X, Z) 1\\ r(Z). All variables are implicitly universally quantified (we will denote variables by names starting with upper-case letters). A literal is an atomic formula (briefly atom) or the negation of an atom. Definite clauses consist of exactly one positive literal (p(X, Y)) in our example) and zero or more negative literals (such as q(X, Z) and r(Z)). The positive literal is the head of the clause and the negative literals are the body.\nA goal or query is of the form +- Atom1 1\\ Atom2 1\\ · · · 1\\ Atomn. A substitution, such as(} = {X fa, Y/Z} is a mapping from variables to first-order terms. If a substi tution maps variables to terms which do not include vari-\nLoglinear models for first-order probabilistic reasoning 127\nabies, we will call it an instantiation. A substitution (} uni fies two atoms Atom1, Atom2 if Atom1(} ((} applied to Atom1) is identical to Atom29. Resolution is an infer ence rule that takes an atom Atom selected from a goal +- Atomtll· · · /\\Atom /\\· · ·IIAtomn. unifies Atom with the head H of a clause H +- B using a substitution (} and returns ( +- Atom1 II · · · II B II · · · II Atomn )(} as a new goal. Note that B may be empty. With Prolog the selected atom is always the leftmost atom. An SW-refutation of a goal G is a sequence of resolution steps which produce the empty goal. The SW-tree for a goal G is a tree of goals, with G as root node, and such that the children of any node/goal G' are goals produced by one resolution step using G' (the empty goal has no children). Branches of the SLD-tree ending in the empty goal are success branches corresponding to successful refutations. The success set for a definite program is the set of all ground atoms Atom such that +- Atom has an SLD-refutation. The success set for an n-a rity predicate pjn, denoted SS(pjn), is all those atoms in the program's success set that have pin as their predicate symbol. The most general goal for a predicate pjn is of the form+- p(X1,X2, • • • ,Xn) where the X; are distinct variables. The computed answer for a goal is a substitution for the variables in G produced by an SLD refutation of G. We will sometimes use Pro log notation, where p(X, Y) +- q(X, Z) II r(Z) is represented thus: p ( X, Y) : - q ( X, Z), r ( Z)., and+- q(X, a) is represented thus:\n- q ( X, a).\n3 Loglinear models for first-order probabilistic reasoning\nA loglinear probability distribution on a set fl is of the fol lowing form. For w E fl:\np(w) = z-i exp ( � Ad;(w)) (I) where the /; are the features of the distribution, the A; are the model parameters and Z is a normalising constant.\n3.1 Probabilistic Constraint Logic Programming\n(Riezler, 1997) develops (Abney, 1997) by defining a log linear model on the proofs of formulae with some con straint logic program. This requires defining features on these proofs (the/;) and defining the model parameters (the A;).\nThe essentials of this approach can be given by using the logic programming framework. This is a special case of constraint logic programming where the only constraints allowed are equational constraints between terms. We will stay with the standard logic programming framework for simplicity. Consider the logic program LPl given in Fig I.\ns ( X) : - p ( X,Y), q ( Y). p ( a, b). p ( a,a). p ( a, c). p ( d,b). q ( b). q ( c).\nFigure I: LP1, a simple logic program\nFig 2 shows the SLD-tree generated by the query+- s(X) (with empty goals omitted). This shows three refutations of+- s(X) which amount to two proofs of s(a) and one proof of s(d).\nWe can now define a loglinear distribution on refutations of +- s(X). Firstly, we define the features of the distri bution. Consider two features of refutations, /1 and /2. For any refutation R, ft (R) = n if the goal+- q(b) ap pears n times in the R, and h(R) = m if+- q(c) ap pears m times in R. Let A1 = 0.2 and A2 = 0.4, then the leftmost proof of p(a) has probability z-l exp(0.2 X 1 + 0.4 x 0) and the one further to the right has probabil ity z-i exp(0.2 X 0 + 0.4 X 1) . The probability of the single proof of p(d) is z-l exp(0.2 X 1 + 0.4 X 0). Z is simply exp(0.2 x 1 + 0.4 x 0) + exp(0.2 x 0 + 0.4 x 1) + exp(0.2 x 1 + 0.4 x 0) = 2e0·2 + e0·4 ::::: 3.9, so the three probabilities are 0.31, 0.38, 0.31 respectively. Having defined probabilities p on the proofs of these atomic formu lae, it is now trivial to define a distribution p' on the formu lae themselves: p'(Atom) = LR is a proof of AtomP(R). We have p'(s(a)) = 0.69 and p'(s(b)) = 0.31, which is a distribution on S S ( s I 1), the success set for s I 1. This approach applies very naturally to natural language processing (NLP). In NLP, a proof that a sentence belongs to a language amounts to a parse of that sentence, and the loglinear model can be used to find the most likely parse of any particular sentence. Riezler extends the improved iterative scaling algorithm of (Pietra et al., 1997) to induce features and parameters for a loglinear model from incom plete data. Incomplete data here consists of just atoms, rather than the proofs of those atoms. In an NLP context this means having a corpus of sentences rather than sen tences annotated with their correct parses, the former being a considerably cheaper resource.\n128 Cussens\n3.2 Stochastic Logic Programs\nRiezler's framework allows arbitrary features of SLD-trees, and recent experiments have used features \"indicating the number of argument-nodes or adjunct-nodes in the tree, and features indicating complexity, parallelism or branching behaviour\" (Stefan Riezler, personal communication).\nHere we concentrate on a special case of Riezler' s frame work, where the clauses used in a proof are the features defining the probability of that proof, with clause labels de noting the parameters. (Eisele, 1994) examined this ap proach from an NLP perspective. (Muggleton, 1995) intro duced Stochastic Logic Programs, approaching the issue from a general logic programming perspective, with a view to applications in Inductive Logic Programming.\nIn these cases, Stochastic Context-Free Grammars (SCFGs) were \"lifted\" to stochastic feature grammars (SFGs) and stochastic logic programs (SLPs) respectively. SCFGs are CFGs where each production is labelled, such that the labels for a particular non-terminal sum to one. The probability of a parse is then simply the product of the la bels of all production rules used in that parse. Sentence probabilities are given by the sum of all parses of a sen tence. The distributions so defined are special cases of log linear models where the grammar rules define the features f; and their labels are the parameters A;. Z is guaranteed to be one. This is because the labels for each non-terminal sum to one and because the context-freeness ensures that we never fail, and hence never have to backtrack, when generating a sentence from a SCFG-a production rule can always be applied to a nonterminal. Because of this a num ber of techniques (such as the inside-outside algorithm for parameter estimation (Lari and Young, 1990)) can be ap plied to SCFGs, but cannot be lifted to SFGs or SLPs. (See (Abney, 1997) for a demonstration of this.)\nWe define a stochastic logic program (SLP) as follows. An SLP is a logic program where some of the clauses are la belled with a non-negative number, and which satisfies the following constraints:\nConstraint 1 If there is a refutation of the most general goal for a predicate that uses a labelled clause, then the predicate is distribution-defining. It is required that the computed answer substitutions for any unit goal where the predicate is distribution-defining be ground.\nConstraint 2 The potential'ljJ(R) of any refutation R is the product of all the clause labels of the clauses used in R. If none of the clauses used in R have labels, then 1/!(R) is undefined. The potential 'I/!( G) of a goal is L,REref(G) 1/!(R), where ref( G) is the set of all refu tations R of G such that 1/J(R) is defined. If 1/J(R) is undefined for all refutations R of a goal G, then 1/J (G) is also undefined. We require that all goal potentials be finite.\nConstraint 3 For every distribution-defining predicate, the potential of its most general goal must be positive.\nConstraint 1 can be met by requiring SLPs to be range restricted: every variable appearing in the head of a clause must also appear in the body. The second condition is triv ially met by any SLP where there is a bound on the depth of any refutation, e.g. non-recursive SLPs, and can also be met by requiring the clause labels for the clauses defining any given predicate to sum to at most one. Our definition generalises that found in (Muggleton, 1995), where Mug gleton requires SLPs to be range-restricted and with labels for the same predicate summing exactly to one. Also, Mug gleton does not use SLPs to define a loglinear model as we do here.\nAn SLP defines a distribution for every distribution defining predicate in the SLP. Suppose r /3 were a distribution-defining predicate, then we have a loglinear distribution over refutations R of the most general goal for this predicate+- r(Xt, X2, X3), as follows:\np,(R) = z;1 exp ( � Jog(A ; )/(R , i)) (2) z-! II AJ( R,i) r • (3)\nwhere A; is the label of clause C; and f ( R, i) is the number of times the clause C; is used in R. So we have a loglinear model where the labelled clauses define features and the logs of the clause labels are the model parameters.\nZ, is simply the appropriate normalising constant, which can be found by simply summing the potentials of all refu tations of+- r(X1, X2, X3). By definition, this sum is the potential of the goal +- r(Xt, X2, X3). We have that Z, = 1/!( +- r(A, B, C)) = L,AtomESS(r/3) 1/!( +- Atom). This last equation holds because Constraint 1 ensures that every refutation of+- r(Xt, X2, X3) finds a member of SS(r/3), and all elements of SS(r/3) can be found this way. Constraints 2 and 3 ensure that 0 < Z, < oo, so Z;1 is always defined.\nWe get a marginal distribution over SS(r/3): any ground atom A has probability p�(A) = L,REref(<-A) Pr(R). Now consider the X; in+- r(X1, X2, X3) . Each atom in SS(r /3) defines a joint instantiation of the X; and there fore the distribution on atoms defines a three-dimensional joint probability distribution over (Dt, D2, D3) , where D; is the domain of X; which is both a logical and random variable. D; is just the set of values found for X; in SS(r/3). In a standard logic program the D; will be fi nite or countably infinite.\nWe have used an example predicate r /3 to concreteness, but all of the above applies to predicates of any arity. We can use the logical structure of SLPs to define complex\nLoglinear models for first-order probabilistic reasoning 129\nmulti-dimensional joint distributions. The next section de scribes presents some SLPs, beginning with the simplest SLPs which represent Markov nets.\n4 SLPmodels\n4.1 SLPs and Markov nets\nMarkov nets (or undirected Bayes nets) are representations of graphical models, a special case of loglinear models. Fig 3 shows the \"Asia\" Markov net used as a running ex ample in (Lauritzen and Spiegelhalter, 1988).\nFigure 3: \"Asia\" Markov net\nIn general, let V c be the set of cliques of a Markov net G. A potential representation consists of evidence poten tials 1/JA. defined on the cliques. Potentials are real-valued non-negative functions depending only on the states of the variables in each clique. The evidence potentials define a joint distribution on the nodes V of the net as follows:\np(V) = z-l II 1/JA (4) A EVe\nwhere\nz = L II 1/JA (5) V AEVc\nis a normalising constant. If Z = 0 then pis undefined. (Z will always be finite.) Consider the Markov net in Fig 3, which has six cliques. Each of the random variables in this net is binary, taking values t or f. Table 2 gives a potential function defined on the clique {A, T}.\nMarkov nets consist of a structure with associated param eters. Both can be represented easily using SLPs. Clique\npotentials are represented as tables of SLP ground facts. Figure 4 gives an SLP representation of the clique poten tial defined in Table 2.\nWe can then use a single unlabelled clause to represent the structure of a Markov net. The net in Fig 3 is represented by the unlabelled clause shown in Fig 5. Let us call SLPs that represent Markov nets in this fashion Markov net SLPs. Each ground goal has one refutation, so the probability of any ground atom is a normalised product; it is clear that the SLP and the Markov net represent the same loglinear dis tribution. Since they represent Markov nets, probabilistic inference based on such SLPs can use any of the standard algorithms for Markov nets.\nasia ( A,B,D,E,L,S,T,X) c6 ( E,X), c5 ( E,B,D), c4 ( L,B,X), c3 ( L, E, B), c2 ( E,L,T), c1 ( A,T).\nConsider the SLP in Figure 6 which represents a simple linear Markov net. We have that A is independent of C given B (A .L CIB). This conditional independence phe nomenon is central to probabilistic graphical models such as Markov nets. But note that A is independent of C given any value of B. Sometimes we may not be justified in mak ing such a strong assumption. It may be that A is only independent of C given particular values of B. This con ditional conditional independence1 or context-specific in dependence between A and C crops up often in applica tions and has been investigated by a number of workers, e.g. (Boutilier et a!., 1996).\nTo represent context-sensitive independence, we need to be able to differentiate between these two sorts of values of B. Let us assume we have two predicates, strong I 1 and weak/ 1 defined to be mutually exclusive which achieves\n1conditional on a variable and conditional on values of that variable\nthis. We can then define the SLP in Fig 7 that defines an appropriate mixture model. A neater alternative might be to use negation to differentiate, using strong (B) and \\ + strong (B) 2, but the use of negation in SLPs has yet to be properly investigated, hence our current restriction to definite clauses. Mixture models for context-specific inde pendence are investigated in (Thiessen et al., 1997), where learning of such models is considered. (One can view ta bles defining discrete distributions as in Fig 4, as mixtures of degenerate distributions, but we will not do so.)\nContext-sensitivity occurs whenever backtracking (due to unification failure) is a possibility in the search for refu tations, and is ubiquitous in (real) natural language gram mars. Fig 8 shows an SLP defining a distribution over the non-context-free language { anbncn : n � 0}. Note that we can define distributions using structured terms, not just constants, and that the domain of this distribution is count ably infinite.\nanbncn(A) :- build(A-B, B-C, C-[]).\n0.3: build(A-A, B-B, C-C). 0.7: build( [a i AJ-Ap, [b i BJ-Bp, [c i CJ-Cp)\nbuild(A-Ap, B-Bp, C-Cp).\nFigure 8: Stochastic non context-free grammar defined with an SLP\n2\\ + is ISO Pro log notation for not.\n4.3 Inference in SLP models\nMarkov nets, mixtures of Markov nets and context sensitive stochastic grammars are all models that have been investigated in previous work, as have corresponding algo rithms for inference and learning. Our aim here is to use SLPs as a common framework which can bring out useful connections and contrasts between these different models and algorithms.\nA basic probabilistic inference problem in SLPs is to take a query, e.g. f.- t(X1,a,X3) and returnPrt(XI,X31Xz = a), where Prt is the distribution assocated with the predi cate t/3. The simple naive approach to inference in SLPs is to look for all refutations of f.- t(X1, a, X3), record their potentials and find Prt (X1, X3IX2 = a) by marginalising. Although this could be used where we know that goals will have few refutations, in general it will be very inefficient and will not even terminate for goals with infinitely many refutations.\nWe do not have efficient general purpose algorithms for SLPs, so here we just sketch an approach. For a given query, find all clauses which have heads which unify with the goal, then apply the unifying substitution to the clause body, and then attempt to refute the subgoal composed of all the literals in the body that are not distribution-defining. For each clause body, and for each successful refutation, we have a remaining subgoal involving only distribution defining predicates. Some of the variables in this remain ing subgoal may be instantiated, so the subgoal represents a partially instantiated Markov net, but one where the func tions defined on the cliques may not be represented by ta bles. When they are, we can use standard Markov net in ference algorithms. When they are not, one possibility is to call our sketch algorithm recursively, if the SLP is so de fined to guarantee termination. Note that we will only be interested in the distribution over the variables that appear in the head of the clause. These distributions can then be mixed according to the relevant clause labels to produce the final distribution.\n5 Using ILP for feature construction\nSince we use clauses to define the structural features of our distribution, it is natural to look to ILP for techniques which induce such structural features from data. (Dehaspe, 1997) does just this using the MACCENT algorithm which con structs a log-linear model using boolean clausal constraints as features. Dehaspe uses the \"learning from interpreta tions\" ILP setting where each example is represented as a Prolog database. Dehaspe applies MACCENT to classi fication, using a simple animal classification task to illus trate his approach. To bring out the connections between Dehaspe's approach and that presented here, we can re write Dehaspe's clausal constraints as labelled clauses as\nLoglinear models for first-order probabilistic reasoning 131\nin Fig 9. Dehaspe uses negation which is safe here since it is assumed that all queries are ground.\nThere are techniques for learning the structure of Bayes nets which start from an unconnected net and incremen tally add arcs. Such techniques are strongly related to ILP\nLl : p ( X,fish) : - searches (likeDehaspe's) where we start from a maximally \\+ has_legs (X), habitat (X, water). general clause e.g. p(X, Y, Z) +-and refine it by adding L2 : p ( X, reptile) literals to the body until a 'best' (however defined) clause\n\\ + has_ covering ( X, hair) , \\ + has_legs ( X) . is found. p(X, Y, Z) +- corresponds to an totally uncon-\nFigure 9: Dehaspe's clausal constraints as labelled clauses\nDehaspe associates (modulo our rewrite) boolean features with each labelled clause, defined on (I, Class) pairs, where I denotes an, as yet, unclassified instance.\n/j(I, Class) = { � if B,Ci f- p(I,Class) otherwise\nB is background knowledge represented by a logic pro gram. This defines a distribution over pairs (I, Class),\nPr(I,Class) = z-1exp ( �.Xifi(I,Class)) and hence, with suitable normalisation, conditional distri butions Pr(Classll). We have a bijection between proofs ofp/2 atoms and p/2 clauses, since each proof uses exactly one p/2 clause. This allows Dehaspe to treat each proof as a featu re, where the parameter associated with each fea ture (=proof) is the label of the p/2 clause used in that proof. These features are then used to define a probabil ity on atoms directly.\nThis contrasts with the SLP and PFG approach where each proof has features (e.g. the set of labelled clauses used in the proof), and these are used to define a probability ove r proofs. To get an (unnormalised) probability on an atom with SLPs we have to sum up the probabilities of the proofs of that atom.\nDehaspe's approach allows a more direct definition of a distribution over atoms, but relies on each proof passing through exactly one labelled clause. SLPs are not so re stricted. Also with SLPs, the probability of an atom always increases with the number of proofs of that atom, which seems desirable. Following Dehaspe's approach this may not always be the case.\nDehaspe exploits the lattice structure of clauses and ap plies ILP techniques to guide the search for suitable con straints, searching for clauses with a general-to-specific beam search using the DLAB declarative bias language for malism. Dehaspe, like Riezler, keeps all the old parameters fixed when searching for the next constraint (= clause).\nnected Markov net with three nodes. Refining this, to, say, p(X, Y, Z) +- q(X, Y) corresponds adding an arc between the X and Y nodes. Further exploration of this connection may well yield valuable cross-fertilisation between ILP and Bayes net structure learning.\n6 Related work\nWe do not give anything like a comprehensive survey of the work on connecting logic and probability that can be found in the U AI, philosophical, statistical and logical literature. Instead we will contrast the approach presented here with a few examples of particularly closely related work.\nThis translation of the clique functions of a Markov net to a generalised relational database is essentially the same as that of (Wong et a!., 1995). Wong eta/ translate many of the graphical operations used with Markov nets to database op erations: product distributions are constructed using joins, conditional distributions by projection, and marginals by database operations which mimic the standard approach in the Markov net literature. Wong et a/'s argument is that since the operations required for effective use of Markov nets are defined on tables-for example, tables defining marginal probability distributions-one should use opti mised methods developed by the database community for manipulating tables. The current work seeks to extend that of Wong et a/ by moving from a relational database setting to the logic programming setting.\nIn Knowledge-based model construction (KBMC) (Ngo and Haddaway, 1997; Koller and Pfeffer, 1997; Had daway, 1 999) first-order rules with associated probabili ties are used to generate Bayesian networks for particular queries. As in SLD-resolution queries are matched to the heads of rules, but in KBMC this results in nodes repre senting ground facts being added to a growing (directed) Bayesian network. A context is defined using normal first order rules, perhaps explicitly as a logic program (Ngo and Haddaway, 1 997), which specifies logical conditions for labelled rules to be used. The ground facts are seen as boolean variables (either t rue or false). Once the Bayesian network is built it is then used to compute the probability that the query is true.\nIn KBMC, as in much of the work connecting logic and probability, parameterised first-order rules a : c(X) + a(X) are connected to conditional probability statements such as p(c(b)ia(b)) = a. Also the objective is to compute the probability that an atom is true. In the current paper, we\n132 Cussens\nfocus on undirected representations, so that>. : p(X, Y) +- q(X, Y), r(Y) forms part of the definition of a binary dis tribution associated with p/2 defined in terms of distribu tions associated with q/2 and r fl. We make no attempt to model causality.\nSecondly, we do not use a labelled rule >. : p( X, Y) +- q(X, Y), r(Y) to define the probability that some ground atom p( a, b) is true as in KBMC, or to provide bounds on the probability that p(a, b) is true as in (Shapiro, 1983; Ng and Subrahmanian, 1 992). Instead, we have a binary distri bution associated with p(X, Y) which defines the probabil ity of instantiations such as {X fa, Yfb }. In order to reason about the probability of the truth of atoms, we simply aug ment atoms by introducing an extra logical-random vari able to represent the truth value of unaugmented atoms, and then treat this logical-random variable exactly as any other. This is in keeping with our conservative approach-if we are interested in the truth value of an atom as it varies across different \"possible worlds\" -then we model this variation in the standard way: with a random variable.\nConsider genotype (P,G) : - (0. 5) parent (P,Q), genotype{Q,G). an example from (Koller and Pfeffer, 1 997), where the \"rule says that when a person's parent has a gene, the per son will inherit it with probability 0.5''. We would en code such \"degree of belief\" probability in an SLP with a boolean truth-value variable as in Fig 1 0.\nTo find the probability that genotype( bob, big..ears) is true we are required to use the SLP to com pute the probabilities of genotype(bob, big ..ears, 1) and genotype( bob, big ..ears, 0). (In fact, all we need are un normalised potentials for these, which is simpler.) This amounts to demanding arguments (=proofs) for the truth of genotype(bob, big ..ears) and for its falsity. We then effec tively balance the strength of these proofs when deciding on the probability of truth.\nDespite these differences in approach there are clear simi larities between KBMC query-specific Bayes net construc tion and the query-specific exploration of an SLD-tree by Pro log which deserve further investigation.\nAnother approach to relational probabilistic reasoning are the relational Bayesian networks of (Jaeger, 1 997). Here whole interpretations are the nodes of a Bayesian net. It is\nconceivable that such networks could be implemented as an SLP, using some suitable object-level representation of an interpretation, but it is likely that they would be unwieldy in practice. Since, at the end of the day, we are interested in the truth-values of atoms, it seems easier to deal with these directly, perhaps resorting to quite complex SLPs to model complex interactions between degrees of belief.\nFinally, SLPs are very closely related to the stochastic (functional) programs of (Koller et al., 1 997). Stochastic execution of the functional program defines a distribution over outputs of the program. As we have done here, Koller et al show how Bayesian nets and SCFGs can be repre sented in their richer formalism. They base their repre sentation of directed Bayes nets on \"the observation that each node in a Bayes net is a stochastic function of its parent's values.\" They also show how their formalism can exploit context-sensitive independence. Unlike the present paper, they also provide details of an efficient algorithm for probabilistic inference in their formalism, which mim ics standard efficient algorithms for Bayesian networks. (Koller et al., 1 997), does not discuss methods for inducing stochastic functional programs, but it seems highly likely that ILP techniques could be applied.\n7 Open questions and future work\nWe have shown how various properties of SLPs (shared variables, multi-clause programs, unification failure and existential variables) correspond to various existing mod els (graphical models, mixture models, context-sensitive models and marginalisation) and argued that existing algo rithms for these models can hence be used for inference and learning in SLPs. This work remains to be done. It is likely that suitable algorithms will mimic algorithms those used in Koller eta/'s stochastic programs. Work on the imple mentation of randomised algorithms in logic programming is likely to be relevant too (Angelopoulos et al., 1 998). We also expect techniques from logic programming and com putational linguistics, such as Earley deduction and pro gram transformation to be useful. For example, when learn ing the parameters of SLPs, Riezler's approach of storing proofs in a chart using Earley deduction makes a lot more sense than continually re-refuting goals.\nProbabilistic inference and learning by Markov Chain Monte Carlo is also attractive for SLPs. For example, in a Gibbs sampling approach, all except one argument of a goal would be ground on each iteration. Such constrained goals generally have few refutations which might lead to an efficient method.\nFinally, we hope that the current framework will stimu late further research into statistical ILP, and that such re search will benefit from and contribute to related work on inducing models from data in computational linguistics and Bayesian networks.\nLoglinear models for first-order probabilistic reasoning 133\nAcknowledgements\nMany thanks to Sara-Jayne Fanner for weeding out vari ous errors and omissions. Thanks to Stephen Muggleton for useful discussions on the role of normalisation in SLPs and to Stefan Riezler for clarifying his method. Thanks also Gillian, Jane and Robert Higgins for putting up with me. Finally thanks to L uc. de Raedt for encouraging me to investigate first-order Bayesian nets.\nReferences\nAbney, S. ( 1997). Stochastic attribute-value grammars. Computational Linguistics, 23( 4 ):597-618.\nAngelopoulos, N., Pierro, A. D., and Wicklicky, H. ( 1998). Implementing randomised algorithms in constraint logic programming. In Proc. of the Joint International Conference and Symposium on Logic Programming, JICSLP'98, Manchester, UK. MIT Press.\nBoutilier, C., Friedman, N., Goldszmidt, M., and Koller, D. ( 1 996). Context-specific independence in Bayesian networks. In Proceedings of the Twelfth Annual Con ference on Uncertainty in Artificial Intelligence ( UAI96), pages 1 15- 123, Portland, Oregon.\nDehaspe, L . ( 1997). Maximum entropy modeling with clausal constraints. In Inductive Logic Programming: Proceedings of the 7th International Workshop (ILP97). LNAI 1297, pages 109- 124. Springer.\nEisele, A. ( 1994 ). Towards probabilistic extensions of constraint-based grammars. Contribution to DYANA2 Deliverable R l.2B, DYANA-2 project.\nHaddaway, P. ( 1999). An overview of some recent devel opments in Bayesian problem solving techniques. AI Magazine.\nJaeger, M. ( 1997). Relational Bayesian networks. In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UA/-97), pages 266-273, San Francisco, CA. Morgan Kaufmann Pub lishers.\nKoller, D., McAllester, D., and Pfeffer, A. (1997). Effec tive Bayesian inference for stochastic programs. In Proc. of the Fourteenth National Conference on Arti ficial Intelligence (AAAI-97), pages 740-747, Provi dence, Rhode Island, USA.\nKoller, D. and Pfeffer, A. ( 1997). L earning probabilities for noisy first-order rules. In Proceedings of the Fif teenth International Joint Conference on Artificial In telligence ( /JCA/-97), Nagoya, Japan.\nL ari, K. and Young, S. ( 1990). The estimation of stochastic context-free grammars using the Inside-Outside algo rithm. Computer Speech and Language, 4:35-56.\nL auritzen, S. and Spiegelhalter, D. ( 1988). L ocal compu tations with probabilities on graphical structures and their applications to expert systems. Journal of the Royal Statistical Society A, 50(2): 1 57-224.\nL loyd, J. ( 1987). Foundations of Logic Programming. Springer, Berlin, second edition.\nMuggleton, S. ( 1 995). Stochastic logic programs. In De Raedt, L ., editor, Proceedings of the 5th In ternational Workshop on Inductive Logic Program ming, page 29. Department of Computer Science, Katholieke Universiteit L euven.\nNg, R. and Subrahmanian, V. ( 1992). Probabilistic logic programming. Information and Computation, 10 1(2): 150-20 1 .\nNgo, L . and Haddaway, P. ( 1997). Answering queries from context-sensitive probabilistic knowledge bases. The oretical Computer Science, 171:147- 1 7 1. Special is sue on uncertainty in databases and deductive systems.\nPietra, S.D., Pietra, V. D., and L afferty, J. ( 1997). Inducing features of random fields. IEEE Transactions on Pat tern Analysis and Machine Intelligence, 1 9(4):380- 393.\nRiezier, S. ( 1997). Probabilistic constraint logic program ming. Arbeitsberichte des SFB 340 Bericht Nr. 1 17, Universitiit Tiibingen.\nShapiro, E. ( 1 983 ). L ogic programs with uncertainties: A tool for implementing rule-based systems. In Proc. /JCA/-83, pages 529-532.\nThiesson, B., Meek, C., Chickering, D., and Hecker man, D. ( 1997). L earning mixtures of DAG mod els. Technical Report MSR-TR-97-30, Microsoft Re search. Revised May 1998.\nWong, S. K. M., Butz, C. J., and Xiang, Y. ( 1995). A method for implementing a probabilistic model as a relational database. In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intel ligence (UA/-95), pages 556-564, Montreal, Quebec, Canada."
    } ],
    "references" : [ {
      "title" : "Stochastic attribute-value grammars",
      "author" : [ "S. Abney" ],
      "venue" : null,
      "citeRegEx" : "Abney,? \\Q1997\\E",
      "shortCiteRegEx" : "Abney",
      "year" : 1997
    }, {
      "title" : "Implementing randomised algorithms in constraint logic programming",
      "author" : [ "H. Wicklicky" ],
      "venue" : "In Proc. of the Joint International Conference and Symposium on Logic Programming,",
      "citeRegEx" : "D. and Wicklicky,? \\Q1998\\E",
      "shortCiteRegEx" : "D. and Wicklicky",
      "year" : 1998
    }, {
      "title" : "Maximum entropy modeling with clausal constraints",
      "author" : [ "L Dehaspe" ],
      "venue" : "In Inductive Logic Programming: Proceedings of the 7th International Workshop (ILP97). LNAI",
      "citeRegEx" : "Dehaspe,? \\Q1997\\E",
      "shortCiteRegEx" : "Dehaspe",
      "year" : 1997
    }, {
      "title" : "An overview of some recent devel­ opments in Bayesian problem solving techniques",
      "author" : [ "P. Haddaway" ],
      "venue" : "AI Magazine",
      "citeRegEx" : "Haddaway,? \\Q1999\\E",
      "shortCiteRegEx" : "Haddaway",
      "year" : 1999
    }, {
      "title" : "Relational Bayesian networks",
      "author" : [ "M. Jaeger" ],
      "venue" : "In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Jaeger,? \\Q1997\\E",
      "shortCiteRegEx" : "Jaeger",
      "year" : 1997
    }, {
      "title" : "Effec­ tive Bayesian inference for stochastic programs",
      "author" : [ "D. Koller", "D. McAllester", "A. Pfeffer" ],
      "venue" : "In Proc. of the Fourteenth National Conference on Arti­ ficial Intelligence",
      "citeRegEx" : "Koller et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Koller et al\\.",
      "year" : 1997
    }, {
      "title" : "L earning probabilities for noisy first-order rules",
      "author" : [ "D. Koller", "A. Pfeffer" ],
      "venue" : "In Proceedings of the Fif­ teenth International Joint Conference on Artificial In­",
      "citeRegEx" : "Koller and Pfeffer,? \\Q1997\\E",
      "shortCiteRegEx" : "Koller and Pfeffer",
      "year" : 1997
    }, {
      "title" : "The estimation of stochastic context-free grammars using the Inside-Outside algo­ rithm",
      "author" : [ "JCA", "Nagoya", "K. Japan. L ari", "S. Young" ],
      "venue" : "Computer Speech and Language,",
      "citeRegEx" : "JCA..97. et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "JCA..97. et al\\.",
      "year" : 1990
    }, {
      "title" : "L ocal compu­ tations with probabilities on graphical structures and their applications to expert systems",
      "author" : [ "S. L auritzen", "D. Spiegelhalter" ],
      "venue" : "Journal of the Royal Statistical Society A,",
      "citeRegEx" : "auritzen and Spiegelhalter,? \\Q1988\\E",
      "shortCiteRegEx" : "auritzen and Spiegelhalter",
      "year" : 1988
    }, {
      "title" : "Probabilistic logic programming",
      "author" : [ "R. Ng", "V. Subrahmanian" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Ng and Subrahmanian,? \\Q1992\\E",
      "shortCiteRegEx" : "Ng and Subrahmanian",
      "year" : 1992
    }, {
      "title" : "Answering queries from context-sensitive probabilistic knowledge bases. The­ oretical Computer Science, 171:147- 1 7 1. Special is­ sue on uncertainty in databases and deductive systems",
      "author" : [ "Ngo", "P. Haddaway" ],
      "venue" : null,
      "citeRegEx" : "Ngo et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Ngo et al\\.",
      "year" : 1997
    }, {
      "title" : "Inducing features of random fields",
      "author" : [ "J. L afferty" ],
      "venue" : "IEEE Transactions on Pat­ tern Analysis and Machine Intelligence,",
      "citeRegEx" : "D. and afferty,? \\Q1997\\E",
      "shortCiteRegEx" : "D. and afferty",
      "year" : 1997
    }, {
      "title" : "Probabilistic constraint logic program­",
      "author" : [ "S. Riezier" ],
      "venue" : "ming. Arbeitsberichte des SFB 340 Bericht Nr",
      "citeRegEx" : "Riezier,? \\Q1997\\E",
      "shortCiteRegEx" : "Riezier",
      "year" : 1997
    }, {
      "title" : "A method for implementing a probabilistic model as a relational database",
      "author" : [ "S.K.M. Wong", "C.J. Butz", "Y. Xiang" ],
      "venue" : "In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intel­ ligence (UA/-95),",
      "citeRegEx" : "Wong et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Wong et al\\.",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "(Riezler, 1997) develops (Abney, 1997) by defining a log­ linear model on the proofs of formulae with some con­ straint logic program.",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "(See (Abney, 1997) for a demonstration of this.",
      "startOffset" : 5,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "(Dehaspe, 1997) does just this using the MACCENT algorithm which con­ structs a log-linear model using boolean clausal constraints as features.",
      "startOffset" : 0,
      "endOffset" : 15
    } ],
    "year" : 2011,
    "abstractText" : "Recent work on loglinear models in probabilistic constraint logic programming is applied to first­ order probabilistic reasoning. Probabilities are defined directly on the proofs of atomic formu­ lae, and by marginalisation on the atomic formu­ lae themselves. We use Stochastic Logic Pro­ grams (SLPs) composed of labelled and unla­ belled definite clauses to define the proof prob­ abilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and ran­ dom variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}