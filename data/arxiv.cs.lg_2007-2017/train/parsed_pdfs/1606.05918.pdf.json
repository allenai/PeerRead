{
  "name" : "1606.05918.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Slack and Margin Rescaling as Convex Extensions of Supermodular Functions",
    "authors" : [ "Matthew B. Blaschko" ],
    "emails" : [ "matthew.blaschko@esat.kuleuven.be" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Slack and margin rescaling are variants of the structured output SVM. They define convex surrogates to task specific loss functions, which, when specialized to nonadditive loss functions for multi-label problems, yield extensions to increasing set functions. We demonstrate in this paper that we may use these concepts to define polynomial time convex extensions of arbitrary supermodular functions. We further show that slack and margin rescaling can be interpreted as dominating convex extensions over multiplicative and additive families, and that margin rescaling is strictly dominated by slack rescaling. However, we also demonstrate that, while the function value and gradient for margin rescaling can be computed in polynomial time, the same for slack rescaling corresponds to a non-supermodular maximization problem."
    }, {
      "heading" : "1 Introduction",
      "text" : "It is known that submodular minimization is strongly polynomial time solvable while non-submodular minimization is NP-hard even to find an O(n1−ε) approximation. However, NP-hardness is a worst case analysis over a set of problems. We therefore take a more refined view and seek to characterize a class of piecewise-linear convex extensions of set functions that minimizes (within a certain class) the probability of a non-integral solution to the corresponding LP relaxation when problem instances are sampled from a given distribution. We do so through the notion of a dominating convex extension, i.e. one that is closer to the convex closure at all points within the unit cube.\nIn this work, we explore analogous strategies analogous to margin and slack rescaling in structured output support vector machines [8] for set function minimization in general, and substantially advance the theory of supermodular set function minimization. In particular, we advance the theory of convex extensions of supermodular extensions by showing an explicit form for the closest convex extension to the convex closure within a specific multiplicative family (Proposition 6). We further show that this multiplicative family dominates a related polynomial time computable additive family (i.e. that the closed form extension shown previously is closer to the convex closure than any member of this additive family) (Proposition 11). Subsequently, we prove that computation of an extension operator derived from a modular upper bound on submodular functions introduced by Jegelka & Bilmes does not correspond with a supermodular maximization (Proposition 15). This in general suggests a tradeoff between the tightness of the extension and the tractability of its computation, with only margin rescaling demonstrated to be polynomial time computable despite being dominated by slack rescaling. A summary of key theoretical results is given in Table 1. We may additionally summarize key theoretical contributions through a partial ordering over (convex) extensions of set\nar X\niv :1\n60 6.\n05 91\n8v 1\n[ cs\n.L G\n] 1\n9 Ju\nfunctions considered in this paper using the notion of restricted operator inequality (Definition 5):\nS1 ≤+ S+ ≤G+S ≤G C ≤ V (1) Mγ ≤GS (2)\nin which V denotes the (non-convex) multi-linear extension due to Vondrák [9] (Appendix B).\nRemark 1. If a fixed finite set of poly-time convex extensions does not have a total ordering, the maximum of these extensions is a poly-time convex extension that dominates them all."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Supermodular minimization (submodular maximization) has been widely studied both in the context of relaxations as well as for fully combinatorial approximation algorithms [5]. Notable contributions include the multilinear extension [9, 10] (mentioned above and in Appendix B), which, although non-convex, has been used in an approximate minimization framework.\nA large amount of work has been done on relaxations approximate energy minimization in the context of random field models with low maximal clique size [6], including roof duality [4], or by making additional assumptions on the structure of the graph such as balancedness [11].\nThe connection between the structured output SVM [8] and convex extensions of set functions seems to be due to [12]. The comparative difficulty of optimizing slack rescaling vs. margin rescaling is known, and an interesting approach to the optimization of slack rescaling via multiple margin rescaling operations can be found in [1]."
    }, {
      "heading" : "2 Mathematical Preliminaries",
      "text" : "Definition 1 (Set function). A set function ` is a mapping from the power set of a base set V to the reals:\n` : P(V ) 7→ R. (3)\nWe will assume w.l.o.g. that `(∅) = 0 for all set functions in the sequel. Definition 2 (Extension). An extension of a set function is an operator that yields a continuous function over the p-dimensional unit cube (where p = |V |) such that the function value on each vertex x of the unit cube is equal to `(A) for A such that xi = 1 ⇐⇒ i ∈ A. Definition 3 (Submodular set function). A set function f is said to be submodular if for all A ⊆ B ⊂ V and x ∈ V \\B,\nf(A ∪ {x})− f(A) ≥ f(B ∪ {x})− f(B). (4)\nA set function is said to be supermodular if its negative is submodular, and a function is said to be modular if it is both submodular and supermodular. We denote the set of all submodular functions S , the set of all supermodular functions G := {g| − g ∈ S}, and G+ := {g|g(S) ≥ 0, ∀S ⊆ V } ∩ G denotes the set of all non-negative supermodular functions. Additional properties of submodular functions can be found in [2].\nLemma 1. Any g ∈ G such that g({x}) ≥ 0, ∀x ∈ V is an increasing function, i.e. g(A ∪ {x}) ≥ g(A), ∀A ⊂ V, x ∈ V \\A.\nProof. We have that g(∅) = 0 and g({x}) ≥ 0, ∀x ∈ V by assumption. From the supermodularity of g, ∀A ⊂ V, x ∈ V \\A,\ng(A) + ≥0︷ ︸︸ ︷ g({x}) ≤g(A ∪ {x}) + =0︷ ︸︸ ︷ g(A ∩ {x}) (5)\n=⇒ g(A) ≤g(A ∪ {x}) (6)\nCorollary 1. Any g ∈ G+ is an increasing function. Definition 4 (Convex closure). The convex closure of a set function C` is defined for all x ∈ [0, 1]|V | as\nC`(x) := min α∈R2|V | ∑ A⊆V αA`(A), s.t. ∑ A⊆V αA1A = x, ∑ A⊆V αA = 1, αA ≥ 0 ∀A ⊆ V. (7)\nIn general, the convex closure is not polynomial time solvable, with the notable exception of the Lovász extension [2, 7]. Definition 5 (Restricted operator inequality). For two convex extensions A and B, we write A < B (analogously A ≤ B) if A`(x) < B`(x) ∀`, x ∈ [0, 1]p. We may subscript the inequality sign with a class of set functions (e.g. B ≤S L) if the inequality holds for all set functions in that class. Proposition 1 (Transitivity of restricted operator inequality). For two function sets A and B and three operators F, G, and H,\nF ≤A G ≤B H =⇒ F ≤A∩B H. (8)\nProposition 2. B ≤ C for all convex extensions B.\nProof. Assume that there exists some convex extension B` such that B`(x) > C`(x) for some x in the p-dimensional unit cube. If we map all A ⊆ V to vertices of the p-dimensional unit cube, and identify ` with 2p distinct points in {0, 1}p × R, then the convex closure is the lower hull of these points taken with respect to the dimension corresponding to the function value. As each of the 2p points corresponding to the function are vertices of this lower hull, any B` that has a point greater than a corresponding point of C` is not a lower hull and cannot contain all of these vertices, which contradicts the definition of a set function extension.\nOne of the key applications of set function extensions is their application to (approximate) minimization of set functions through LP relaxations. In Proposition 3 we formalize the unsurprising but important result that dominating convex extensions result in integral solutions to LP relaxations with higher probability. For some set S ⊆ Rd, we denote int(S) := [S ∩ {0, 1}d 6= ∅], where we have used Iverson bracket notation on the rhs of the definition. Proposition 3. For two operators that yield convex extensions for all set functions ` ∈ A, F ≤A G ≤A C, and for all distributions p : A 7→ R+,\nE`∼p { int ( arg min\nx∈[0,1]|V | F`(x)\n)} ≤ E`∼p { int ( arg min\nx∈[0,1]|V | G`(x)\n)} , (9)\nwhere arg min is defined as a map to the set of minimizers of an expression.\nProof. It is sufficient to demonstrate that( arg min\nx∈[0,1]|V | F`(x)\n) ∩ {0, 1}|V | = ∅ ⇐= ( arg min\nx∈[0,1]|V | G`(x)\n) ∩ {0, 1}|V | = ∅. (10)\nDenote x∗ an (integral) optimum of arg minx∈[0,1]|V | C`(x), As C`(x) is total dual integral, we have that C`(x∗) = minA⊆V `(A). If `(x∗) = G`(x∗) 6= minx∈[0,1]|V | G`(x), there must be a negative directional gradient ∇vG`(x∗) for some v ∈ R|V | pointing into the unit cube from x∗. From the definition of set function extensions and the fact that F ≤A G, we have that 0 > ∇vG`(x∗) ≥ ∇vF`(x∗) and therefore x∗ /∈ arg minx∈[0,1]|V | F`(x)."
    }, {
      "heading" : "3 Slack and margin rescaling",
      "text" : ""
    }, {
      "heading" : "3.1 Slack rescaling",
      "text" : "Definition 6 (Slack rescaling for increasing set functions [8, 12, 13]). For an increasing set function `,\nS1`(x) = max A⊆V `(A)\n( 1− 2|A|+ 2\n∑ i∈A xi\n) . (11)\nThe derivation of this extension from the related convex surrogate is shown in Appendix A. We note that the factor 2 in the above definition is an artifact of the origin of the extension as a convex surrogate. We show below that we may instead use a tighter variant in the sense that S1 ≤+ S+, where ≤+ denotes the inequality holds for all increasing functions. Definition 7 (A tighter slack rescaling variant for increasing set functions). For an increasing set function `,\nS+`(x) = max A⊆V `(A)\n( 1− |A|+\n∑ i∈A xi\n) . (12)\nProposition 4. Computation of S+`(x) corresponds to a non-supermodular maximization problem for x ∈ [0, 1]|V | and supermodular increasing `.\nThe proof is given in Appendix D. Proposition 5. Definition 7 yields a convex extension for all increasing `.\nProof. Equation (12) is convex: The r.h.s. is a maximum over linear functions of x, and is therefore convex in x.\nEquation (12) is an extension: We will use the notation set : {0, 1}|V | 7→ P(V ) to denote the set associated with an indicator vector. We must have that S+`(x) = `(A) whenever x ∈ {0, 1}|V | and set(x) = A. For set(x) = A we have that\n`(A) 1− |A|+ =|A|︷ ︸︸ ︷∑ i∈A xi  = `(A). (13) We now must show that when set(x) = A\n`(A) ≥ `(B) ( 1− |B|+\n∑ i∈B xi\n) = `(B) (1− |B|+ |A ∩B|) . (14)\nFor any B * A: ≥0︷︸︸︷ `(B) ≤0︷ ︸︸ ︷ (1− |B|+ |A ∩B|) ≤ 0 ≤ `(A), (15)\nand for any B ⊆ A ≤`(A)︷︸︸︷ `(B) =1︷ ︸︸ ︷ (1− |B|+ |A ∩B|) ≤ `(A). (16)\nAn example of the convex extension achieved with Definition 7 applied to the set function `(∅) = 0, `({a}) = 0.5, `({b}) = 1.5, `({a, b}) = 4 is shown in Figure 1. We can see that Definition 7 is an instance of a general strategy for multiplicative extensions of increasing functions:\nB`(x) = max A⊆V\n`(A) · h×(x,A) (17)\nwhere h× : [0, 1]|V | × P(V ) 7→ R is a function convex in x. We require h×(x,A) ≤ `(set(x))`(A) for integral x and all A in order to ensure that `(A)h×(x,A) ≤ `(set(x)), which is the general form of the inequality in Equation (14). This strategy is quite general if h× can have a dependency on ` as the convex closure is recovered simply by setting h×(x,A) = C`(x) `(A) ; In Proposition 6 we will limit ourselves to h× that have no explicit dependency on ` beyond that it is an increasing function. For increasing `, `(set(x))`(A) is bounded below by 1 for all A ⊆ set(x) and is bounded below by 0 for all A * set(x). If we additionally assume that ` ∈ G+, these bounds remain unchanged. If we take these bounds with respect to all increasing `, or indeed all g ∈ G+, the bounds are sharp. Proposition 6. h×(x,A) = ( 1− |A|+ ∑ i∈A xi ) +\nis the largest possible convex function over the unit cube satisfying the constraints that for integral x, h×(x,A) ≤ 1 ∀A ⊆ set(x), h×(x,A) ≤ 0 ∀A * set(x), and h×(x, set(x)) = 1, where (·)+ = max(0, ·).\nProof. Each constraint is an equality constraint or an upper bound, and the values of the function are constrained only at the vertices of the unit cube, so the maximum convex function satisfying the bounds must be the lower hull of the constraint points, that is h×(x,A) = C[A ⊆ ·](x), where we have employed Iverson bracket notation in our definition of the set function to which we apply C:\nC[A ⊆ ·](x) = min α∈R2|V | ∑ B⊆V αB [A ⊆ B] = min α∈R2|V | ∑ B⊇A αB (18)\ns.t. ∑ B⊆V αB1B = x, ∑ B⊆V αB = 1, αB ≥ 0 ∀B. (19)\nFor all |V | ∈ Z+ and A ⊆ V , we have that C[A ⊆ ·](x) = 0, ∀x ∈ conv ( {y ∈ {0, 1}|V || set(y) + A} ) . (20)\nFor x ∈ conv ( {y ∈ {0, 1}|V || set(y) ⊇ A ∨ ∃i ∈ V, set(y) ∪ {i} ⊇ A} ) , we must have a linear\nfunction that interpolates between 0 at all integral points x such that set(x) + A and 1 at points such that set(x) ⊇ A. This linear function is exactly 1− |A|+ ∑ i∈A xi.\nIn practice, we do not need to threshold h× at zero (i.e. we do not need to apply the (·)+ operation above) as this is redundant with the maximization in Equation (17) being achieved by A = ∅. Remark 2. The constraints in Proposition 6 are necessary and sufficient conditions on h for this multiplicative family to yield a convex extension for all increasing `. They are also necessary and sufficient to yield a convex extension for all g ∈ G+.\nCorollary 2 (S+ dominates the multiplicative family of extensions). For all B that can be written as in Equation (17) and h having no oracle access to `,\nB ≤+ S+. (21) Definition 8 (m g). For a set function g, we may construct a modular function m such that\nm({x}) = g({x}), ∀x ∈ V, (22) and denote this function m g.\nFor non-increasing supermodular g, we may extend the definition of slack rescaling as follows: Definition 9 (Slack rescaling for all supermodular g). Assume g ∈ G. For a modular function m, we will denote vec(m) ∈ R|V | the vectorization of m, and we define\nSg(x) = 〈vec(m g), x〉+ max A⊆V (g(A)−m g(A))\n( 1− |A|+\n∑ i∈A xi\n) . (23)\nExamples of the convex extension achieved by Definition 9 are given in Figure 2. We now show that not only does S yield an extension for non-increasing supermodular functions, it yields a tighter extension for increasing supermodular functions: Proposition 7. S+ ≤G+ S.\nProof. For g ∈ G+,\nSg(x)− S+g(x) = max A⊆V\n( 〈vec(m g), x〉+ (g(A)−m g(A)) ( 1− |A|+\n∑ i∈A xi\n)) (24)\n−max A⊆V g(A)\n( 1− |A|+\n∑ i∈A xi\n) .\nTo show that this difference is greater than zero, we will demonstrate that the difference of each element indexed by A ⊆ 0 is greater than zero.\n〈vec(m g), x〉+ (g(A)−m g(A)) ( 1− |A|+\n∑ i∈A xi\n) − g(A) ( 1− |A|+\n∑ i∈A xi\n)\n= ∑ i∈V g({i})xi − ∑ i∈A g({i}) 1 +∑ j∈A (xj − 1)  (25) = ∑ i∈V g({i})xi − ∑ i∈A g({i}) + ∑ i∈A (1− xi)g({i}) (26)\n+ =ξ≥0︷ ︸︸ ︷∑ i∈A ∑ j∈A g({i})(1− xj)− ∑ i∈A (1− xi)g({i})\n= ∑ i∈V \\A g({i})xi + ξ ≥ 0 (27)"
    }, {
      "heading" : "3.2 Margin rescaling",
      "text" : "We may similarly define the following convex extension based on margin rescaling: Definition 10 (Margin rescaling for supermodular g [8, 12, 13]).\nMg(x) := 〈vec(m), x〉+ max A⊆V g(A)−m(A)− |A|+ ∑ i∈A xi. (28)\nAnalogous to the development of S from S1, we have removed a factor of 2. It has been demonstrated that up to a strictly positive scale factor, margin rescaling yields an extension in the convex surrogate loss setting [12, Proposition 2]. We prove this next and in doing so show necessary and sufficient conditions on the scaling of the loss function for M to yield an extension. Proposition 8 (Proposition 2 of [12]). For every increasing function `, there exists a scalar γ > 0 such that γ−1M(γ`) is an extension of `. We will denote Mγ := γ−1M(γ ·) for short.\nProof. Wlog, we may assume that `({i}) = 0, ∀i ∈ V . For Mγ to yield an extension, we need that for all A,B ⊆ V , set(x) = A,\nγg(A) ≥ γg(B)− |B|+ ∑ i∈B xi = γg(B)− |B|+ |A ∩B|. (29)\nγ(g(A)− g(B)) + |B| − |A ∩B| ≥ 0 (30)\n⇐⇒ γ ≤ |B| − |A ∩B| g(B)− g(A) ∀(A,B) s.t. g(A) < g(B) (31)\nwhere the restriction to g(A) < g(B) is due to the fact that g(B) < g(A) reverses the direction of the inequality as we multiply both sides by 1g(B)−g(A) . As g is increasing, we cannot simultaneously have that |A ∩ B| = |B| and g(A) < g(B) so the numerator is strictly positive and the ratio is therefore strictly positive.\nProposition 9. For 0 < γ1 < γ2 both satisfying the conditions in Equation (31), Mγ1 ≤+ Mγ2 .\nProof. Wlog, we will assume that g ∈ G+ and ignore the modular transformation in Definition 10.\n0 ≤ 1 γ2 max A⊆V\n( γ2g(A)− |A|+\n∑ i∈A xi ) − 1 γ1 max A⊆V ( γ1g(A)− |A|+ ∑ i∈A xi ) (32)\n⇐= 0 ≤ 1 γ2\n( γ2g(A)− |A|+\n∑ i∈A xi ) − 1 γ1 ( γ1g(A)− |A|+ ∑ i∈A xi ) (33)\n=\n( 1\nγ2 − 1 γ1 ) ︸ ︷︷ ︸\n<0\n( −|A|+\n∑ i∈A xi ) ︸ ︷︷ ︸\n≤0\n(34)\nWe next demonstrate that margin rescaling is dominated by slack rescaling, and the latter is therefore closer to the convex closure. Proposition 10. Mγ ≤G S, for all γ satisfying Equation (31).\nProof. For g ∈ G+ and γ satifying Equation (31),\n0 ≤Sg(x)−Mγg(x) (35)\n⇐= 0 ≤ ( g(A)− g(A)|A|+ g(A)\n∑ i∈A xi\n) − ( g(A)− 1\nγ |A|+ 1 γ ∑ i∈A xi\n) (36)\n=\n( 1\nγ − g(A)\n) ≥0︷ ︸︸ ︷( |A| −\n∑ i∈A xi\n) . (37)\nFrom Equation (31),\n1 γ ≥ g(A)− g(B) |A| − |A ∩B| ≥ g(A) (38)\nfor some B ⊂ A. The second inequality follows as the numerator is less than or equal to g(A) as g is increasing and the denominator is at least 1. The desired result follows.\nAs in Equation (17), we may view margin rescaling as a special case of an additive strategy for computing convex extensions in which\nB`(x) = 1\nγ max A⊆V γ`(A) + h+(x,A) (39)\nwhere as in Equation (17), h+ is some convex function. We have seen from Proposition 8 that γ is necessary to guarantee that the operator yields an extension for members of this family. We again restrict ourselves to h+ that do not have access to ` except the assumption that it is an increasing function satisfying some scaling constraints that we may enforce by setting γ as a function of `. Allowing γ to depend on ` in a way that may require an exponential number of oracle accesses to ` to compute is extremely generous, but we will see in Proposition 11 that even with this degree of freedom the optimal multiplicative extension dominates all additive extensions. Proposition 11. Slack rescaling, which is the dominating member of the multiplicative family of extensions following Equation (17) (Corollary 2), dominates the family of additive extensions following Equation (39), where h+(x,A) has no oracle access to `.\nProof. For Equation (39) to yield an extension, we require that γ`(A) + h+(x,A) ≥ γ`(set(x)) for integral x ∈ [0, 1]|V |, which is satisfied for h+(x,A) ≥ γ(`(set(x))− `(A)). For increasing `, γ(`(set(x))− `(A)) is bounded below by 0 for all A ⊆ set(x), and for A * set(x) we have from Equation (29) that γ(`(set(x))−`(A)) ≥ −|A|+| set(x)∩A|. These yield a set of constraints similar to those in Proposition 6 but shifted downwards by one, and without thresholding to zero. In Proposition 6 the maximal convex function satisfying the constraints had the form ( 1− |A|+ ∑ i∈A xi ) +\n, and we may conclude that the optimal function for the additive family of extensions is achieved by h+(x,A) = −|A|+ ∑ i∈A xi.\nThis indicates that Mγ` with maximal γ satisfying Equation (31) (Proposition 9) is the additive convex extension closest to the convex closure of `. From Proposition 10, Mγ is dominated by S+ for all γ such that Mγ yields an extension."
    }, {
      "heading" : "4 Conclusions",
      "text" : "In this work, we have formally analyzed slack and margin rescaling as convex extensions of supermodular set functions. Although they were originally developed for empirical risk minimization of (increasing) functions, we show that supermodularity can be exploited to first transform a set function into an increasing function with a linear number of accesses to the loss function. We have\nshown that, while slack rescaling dominates margin rescaling in the sense that it is strictly closer to the convex closure, computation of slack rescaling for a supermodular function corresponds to a non-supermodular maximization problem. Margin rescaling, by contrast, remains tractable. We have further shown that margin and slack rescaling correspond to optimal additive and multiplicative extensions, respectively, given a computational budget of one oracle access to the loss function."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is funded by Internal Funds KU Leuven, FP7-MC-CIG 334380, and the Research Foundation - Flanders (FWO) through project number G0A2716N."
    }, {
      "heading" : "A Slack and Margin Rescaling",
      "text" : "In this section, we show how we arrive at Definition 6 and Definition 10 from slack and margin rescaling [12, Equations (13) and (12)]. Slack and margin rescaling were introduced in the context of convex surrogate loss functions, and so the notation is defined with respect to a loss function ∆(·, ·) that compares a correct ground truth labeling y ∈ {−1, 1}p with a (potentially incorrect) prediction\nỹ ∈ {−1, 1}p. We identify this function with a set function, where the input is the set of entries for which y and ỹ differ [12, Equation (6)]. The convex surrogate is defined with respect to axes gj · yj , where g ∈ Rp is a vector of predictions. To translate from coordinates for a convex surrogate loss function to those for an extension of a set function, we must take the mapping xj = 1−gj ·yj (where xj is this paper’s notation). From the definition of the slack rescaling operator [12, Equation (13)]:\nmax ỹ∈Y ∆(y, ỹ) (1 + 〈g, ỹ〉 − 〈g, y〉) = max A⊆V `(A) 1 +∑ i∈A gi =−yi︷︸︸︷ ỹi + ∑ j∈V \\A gj =yj︷︸︸︷ ỹj − ∑ k∈V gkyk  (40)\n= max A⊆V `(A) 1−∑ i∈A (1− xi) + ∑ j∈V \\A (1− xj)− ∑ k∈V (1− xk)  (41)\n= max A⊆V `(A)\n( 1− 2\n∑ i∈A (1− xi) ) = max A⊆V `(A) ( 1− 2|A|+ 2 ∑ i∈A xi ) .\n(42)\nThe derivation for margin rescaling is analogous."
    }, {
      "heading" : "B Multi-linear Extension",
      "text" : "The multi-linear extension is due to Vondrák [9].\nDefinition 11 (Multi-linear extension [9]). V`(x) := ∑ A⊆V `(A) ∏ i∈A xi ∏ j∈V \\A (1− xj) (43)\nWe note that V does not yield a convex extension in general, which is implied by Proposition 2, the fact that V 6= C, and the following proposition: Proposition 12.\nC ≤ V. (44)\nProof. We can see that the definition of V is an expectation over Boolean random variables of the function value of ` for the sets defined by those random variables. Consequently the multi-linear extension lies entirely within the convex hull of the points defined in the proof of Proposition 2, and the values of the multi-linear extension must therefore be pointwise greater than the values of the lower hull, which is equivalent to the convex closure."
    }, {
      "heading" : "C Extensions through modular upper bounds on submodular functions",
      "text" : "In this section, we show that a known and frequently employed modular upper bound on submodular functions [3] does in fact define a convex extension to a supermodular function, but that computation of the extension does not correspond to a supermodular maximization in general.\nDefinition 12 (Modular upper bound to a submodular function (Lemma 1 [3])). For a submodular function f and an arbitrary B ⊆ V , define the modular function\njB,f (A) := f(B) + ∑ i∈A\\B (f(B ∪ {i})− f(B))− ∑ i∈B\\A (f(V )− f(V \\ {i})) . (45)\nLemma 2. For all f ∈ S and all A ⊆ V\njA,f (A) = f(A). (46)\nProof.\njA,f (A) := f(A) + =0︷ ︸︸ ︷∑ i∈A\\A (f(A ∪ {i})− f(A))− =0︷ ︸︸ ︷∑ i∈A\\A (f(V )− f(V \\ {i})) . (47)\nProposition 13. For modular m, jB,f+m(A) = jB,f (A) +m(A)\nProof. jB,f+m(A) =f(B) +m(B) + ∑ i∈A\\B (f(B ∪ {i}) +m(B ∪ {i})− f(B)−m(B))\n− ∑ i∈B\\A (f(V ) +m(V )− f(V \\ {i})−m(V \\ {i})) (48)\n=jB,f (A) + =m(B∩A)︷ ︸︸ ︷ m(B)−m(B \\A) +m(A \\B) = jB,f (A) +m(A) (49)\nAs this provides an upper bound to any submodular function, we may also use it to provide a lower bound to a supermodular function by taking −jB,−g(·). We may now define a convex extension based on this modular lower bound.\nProposition 14. Given g ∈ G,\nJg = max B⊆V\nC (−jB,−g(·)) (50)\nis a convex extension.\nProof. Convexity is immediate as Cm is linear for any modular m.\nJg(x) = g(A) for all set(x) = A: From Lemma 2, it suffices to show that jB,f (A) ≥ jA,f (A) = f(A) for all B ⊆ V , which is immediatly true as jB,f upper bounds f for all B. Taking the negative along with the fact that g ≥ g̃ =⇒ Cg ≥ Cg̃ yields the desired result for supermodular functions.\nProposition 15. For x ∈ [0, 1]|V | and g ∈ G, maxB⊆V C (−jB,−g(·)) is not guaranteed to be a supermodular maximization problem.\nProof. The convex closure of a modular function is linear time computable. As we are showing the result for arbitrary g ∈ G, the only relevant fact about −g is that it is submodular. The maximization in Equation (50) is supermodular if for all f ∈ S, ∀A ⊆ B ⊂ V , k ∈ V \\B,\n−C ( jf,B∪{k}(·) ) (x) + C (jf,B(·)) (x) ≥ −C ( jf,A∪{k}(·) ) (x) + C (jf,A(·)) (x), (51)\nwhere we make use of the fact that C(−m) = −Cm for modular m. For any B C (jB,f (·)) (x) = f(B) + ∑\ni∈V \\B xi (f(B ∪ {i})− f(B))− ∑ i∈B (1− xi) (f(V )− f(V \\ {i})) .\n(52)\nA numerical counterexample to this corresponding to a supermodular maximization is g(A) = 0,∀A ∈ P(V ) \\ {V, V \\ {a}}, g(V ) = 1, g(V \\ {a}) = 13 , and xi = 2 3 ,∀i. In this setting, maximizing Equation (52) w.r.t. B is neither submodular nor supermodular."
    }, {
      "heading" : "D Proof of Proposition 4",
      "text" : "Proof. To show that the optimization is not in general a supermodular maximization, we may consider the following counterexample: V = {a, b}, `(∅) = `({a}) = `({b}) = 0, `({a, b}) = ε. For ε > 0 this is strictly supermodular. Denote\n˜̀(A) := `(A) ( 1− |A|+\n∑ i∈A xi\n) (53)\nFor x = 0, we have that ˜̀(∅) = ˜̀({a}) = ˜̀({b}) = 0 and ˜̀({a, b}) = ε(1− 2 + 0) = −ε, but this indicates that ` = −˜̀and they cannot both be supermodular."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Slack and margin rescaling are variants of the structured output SVM. They define<lb>convex surrogates to task specific loss functions, which, when specialized to non-<lb>additive loss functions for multi-label problems, yield extensions to increasing set<lb>functions. We demonstrate in this paper that we may use these concepts to define<lb>polynomial time convex extensions of arbitrary supermodular functions. We further<lb>show that slack and margin rescaling can be interpreted as dominating convex<lb>extensions over multiplicative and additive families, and that margin rescaling is<lb>strictly dominated by slack rescaling. However, we also demonstrate that, while the<lb>function value and gradient for margin rescaling can be computed in polynomial<lb>time, the same for slack rescaling corresponds to a non-supermodular maximization<lb>problem.",
    "creator" : "TeX"
  }
}