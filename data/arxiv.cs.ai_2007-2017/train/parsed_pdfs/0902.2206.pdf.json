{
  "name" : "0902.2206.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Feature Hashing for Large Scale Multitask Learning",
    "authors" : [ "Kilian Weinberger", "Anirban Dasgupta", "Josh Attenberg", "John Langford", "Alex Smola" ],
    "emails" : [ "KILIAN@YAHOO-INC.COM", "ANIRBAN@YAHOO-INC.COM", "JOSH@CIS.POLY.EDU", "JL@HUNCH.NET", "ALEX@SMOLA.ORG" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :0\n90 2.\n22 06\nv1 [\ncs .A\nI] 1\n2 Fe"
    }, {
      "heading" : "1. Introduction",
      "text" : "Kernel methods use inner products as the basic tool for comparisons between objects. That is, given objects x1, . . . , xn ∈ X for some domain X, they rely on\nk(xi, xj) := 〈φ(xi), φ(xj)〉 (1)\nto compare the features φ(xi) of xi and φ(xj) of xj respectively.\nEq. (1) is often famously referred to as the kernel-trick. It allows the use of inner products between very high dimensional feature vectors φ(xi) and φ(xj) implicitly through the definition of a positive semi-definite kernel matrix k without ever having to compute a vector φ(xi) directly. This can be particularly powerful in classification settings where the original input representation has a non-linear decision boundary. Often, linear separability can be achieved in a high dimensional feature space φ(xi).\nPreliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.\nIn practice, for example in text classification, researchers frequently encounter the opposite problem: the original input space is almost linearly separable (often because of the existence of handcrafted non-linear features), yet, the training set may be prohibitively large in size and very high dimensional. In such a case, there is no need to map the input vectors into a higher dimensional feature space. Instead, limited memory makes storing a kernel matrix infeasible.\nFor this common scenario several authors have recently proposed an alternative, but highly complimentary variation of the kernel-trick, which we refer to as the hashing-trick: one hashes the high dimensional input vectors x into a lower dimensional feature space Rm with φ : X → Rm (Langford et al., 2007; Shi et al., 2009). The parameter vector of a classifier can therefore live in Rm instead of in Rn with kernel matrices or Rd in the original input space, where m ≪ n and m ≪ d. Different from random projections, the hashing-trick preserves sparsity and introduces no additional overhead to store projection matrices.\nTo our knowledge, we are the first to provide exponential tail bounds on the canonical distortion of these hashed inner products. We also show that the hashing-trick can be particularly powerful in multi-task learning scenarios where the original feature spaces are the cross-product of the data, X, and the set of tasks, U . We show that one can use different hash functions for each task φ1, . . . , φ|U| to map the data into one joint space with little interference.\nWhile many potential applications exist for the hashingtrick, as a particular case study we focus on collaborative email spam filtering. In this scenario, hundreds of thousands of users collectively label emails as spam or notspam, and each user expects a personalized classifier that reflects their particular preferences. Here, the set of tasks,\nFeature Hashing for Large Scale Multitask Learning\nU , is the number of email users (this can be very large for open systems such as Yahoo MailTMor GmailTM), and the feature space spans the union of vocabularies in multitudes of languages.\nThis paper makes four main contributions: 1. In section 2 we introduce specialized hash functions with unbiased inner-products that are directly applicable to a large variety of kernel-methods. 2. In section 3 we provide exponential tail bounds that help explain why hashed feature vectors have repeatedly lead to, at times surprisingly, strong empirical results. 3. Also in section 3 we show that the interference between independently hashed subspaces is negligible with high probability, which allows large-scale multi-task learning in a very compressed space. 4. In section 5 we introduce collaborative email-spam filtering as a novel application for hash representations and provide experimental results on large-scale real-world spam data sets."
    }, {
      "heading" : "2. Hash Functions",
      "text" : "We introduce a variant on the hash kernel proposed by (Shi et al., 2009). This scheme is modified through the introduction of a signed sum of hashed features whereas the original hash kernels use an unsigned sum. This modification leads to an unbiased estimate, which we demonstrate and further utilize in the following section.\nDefinition 1 Denote by h a hash function h : N → {1, . . . ,m}. Moreover, denote by ξ a hash function ξ : N → {±1}. Then for vectors x, x′ ∈ ℓ2 we define the hashed feature map φ and the corresponding inner product as\nφ (h,ξ) i (x) =\n∑\nj:h(j)=i\nξ(i)xi (2)\nand 〈x, x′〉φ := 〈 φ(h,ξ)(x), φ(h,ξ)(x′) 〉 . (3)\nAlthough the hash functions in definition 1 are defined over the natural numbers N, in practice we often consider hash functions over arbitrary strings. These are equivalent, since each finite-length string can be represented by a unique natural number.\nUsually, we abbreviate the notation φ(h,ξ)(·) by just φ(·). Two hash functions φ and φ′ are different when φ = φ(h,ξ) and φ′ = φ(h ′,ξ′) such that either h′ 6= h or ξ 6= ξ′. The purpose of the binary hash ξ is to remove the bias inherent in the hash kernel of (Shi et al., 2009).\nIn a multi-task setting, we obtain instances in combination with tasks, (x, u) ∈ X × U . We can naturally extend our definition 1 to hash pairs, and will write φu(x) = φ(x, u)."
    }, {
      "heading" : "3. Analysis",
      "text" : "The following section is dedicated to theoretical analysis of hash kernels and their applications. In this sense, the present paper continues where (Shi et al., 2009) falls short: we prove exponential tail bounds. These bounds hold for general hash kernels, which we later apply to show how hashing enables us to do large-scale multitask learning efficiently. We start with a simple lemma about the bias and variance of the hash kernel. The proof of this lemma appears in appendix A.\nLemma 2 The hash kernel is unbiased, that is Eφ[〈x, x′〉φ] = 〈x, x′〉. Moreover, the variance is σ2x,x′ = 1 m ( ∑ i6=j x 2 i x ′ j 2 + xix ′ ixjx ′ j ) , and thus, for ‖x‖2 = ‖x′‖2 = 1, σ2x,x′ = O ( 1 m ) .\nThis suggests that typical values of the hash kernel should be concentrated within O( 1√\nm ) of the target value. We use\nChebyshev’s inequality to show that half of all observations are within a range of √ 2σ. This, together with Talagrand’s convex distance inequality, enables us to construct exponential tail bounds."
    }, {
      "heading" : "3.1. Concentration of Measure Bounds",
      "text" : "In this subsection we show that under a hashed feature-map the length of each vector is preserved with high probability. Talagrand’s inequality (Ledoux, 2001) is a key tool for the proof of the following theorem (detailed in the appendix B).\nTheorem 3 Let ǫ < 1 be a fixed constant and x be a given instance. Let η = ‖x‖∞‖x‖2 . Under the assumptions above, the hash kernel satisfies the following inequality\nPr\n{\n| ‖x‖2φ − ‖x‖ 2 2 |\n‖x‖22 ≥\n√ 2σx,x + ǫ\n}\n≤ exp ( − √ ǫ\n4η\n)\n.\nNote that an analogous result would also hold for the original hash kernel of (Shi et al., 2009), the only modification being the associated bias terms. The above result can also be utilized to show a concentration bound on the inner product between two general vectors x and x′.\nCorollary 4 For two vectors x and x′, let us define\nσ := max(σx,x, σx′,x′, σx−x′,x−x′)\nη := min (‖x‖∞ ‖x‖2 , ‖x′‖∞ ‖x′‖2 , ‖x− x′‖∞ ‖x− x′‖2 ) .\nAlso let ∆ = ‖x‖2 + ‖x′‖2 + ‖x− x′‖2. Under the assumptions above, we have that\nPr [ | 〈x, x′〉φ−〈x, x′〉 |>( √ 2σ+ǫ)∆/2 ] <3e− √ ǫ 4η .\nFeature Hashing for Large Scale Multitask Learning\nThe proof for this corollary can be found in appendix C. We can also extend the bound in Theorem 3 for the maximal canonical distortion over large sets of distances between vectors as follows:\nCorollary 5 Denote by X = {x1, . . . , xn} a set of vectors which satisfy ‖xi − xj‖∞ ≤ η ‖xi − xj‖2 for all pairs i, j. In this case with probability 1− δ we have for all i, j\n| ‖xi − xj‖2φ − ‖xi − xj‖ 2 2 |\n‖xi − xj‖22 ≤\n√\n2 m + 64η2 log2 n2δ .\nThis means that the number of observations n (or correspondingly the size of the un-hashed kernel matrix) only enters logarithmically in the analysis.\nProof We apply the bound of Theorem 3 to each distance individually. Note the bound σ2 ≤ 1m for all normalized vectors. Also, since we have n(n−1)2 pairs of distances the union bound yields a corresponding factor. Solving δ ≥ n(n−1)2 e − √ ǫ\n4η for ǫ and easy inequalities proves the claim."
    }, {
      "heading" : "3.2. Multiple Hashing",
      "text" : "Note that the tightness of the union bound in Corollary 5 depends crucially on the magnitude of η. In other words, for large values of η, that is, whenever some terms in x are very large, even a single collision can already lead to significant distortions of the embedding. This issue can be amended by trading off sparsity with variance. A vector of unit length may be written as (1, 0, 0, 0, . . .), or as (\n1√ 2 , 1√ 2 , 0, . . .\n)\n, or more generally as a vector with c\nnonzero terms of magnitude c− 1 2 . This is relevant, for instance whenever the magnitudes of x follow a known pattern, e.g. when representing documents as bags of words since we may simply hash frequent words several times. The following corollary gives an intuition as to how the confidence bounds scale in terms of the replications:\nLemma 6 If we let x′ = 1√ c (x, . . . , x) then:\n1. It is norm preserving: ‖x‖2 = ‖x′‖2 .\n2. It reduces component magnitude by 1√ c = ‖x′‖∞ ‖x‖∞ .\n3. Variance increases to σ2x′,x′ = 1 cσ 2 x,x+ c−1 c 2 ‖x‖ 4 2 .\nApplying Lemma 6 to Theorem 3, a large magnitude can be decreased at the cost of an increased variance."
    }, {
      "heading" : "3.3. Approximate Orthogonality",
      "text" : "For multitask learning, we must learn a different parameter vector for each related task. When mapped into the same hash-feature space we want to ensure that there is little interaction between the different parameter vectors. Let U be a set of different tasks, u ∈ U being a specific one. Let w be a combination of the parameter vectors of tasks in U \\ {u}. We show that for any observation x for task u, the interaction of w with x in the hashed feature space is minimal. For each x, let the image of x under the hash feature-map for task u be denoted as φu(x) = φ(ξ,h)((x, u)).\nTheorem 7 Let w ∈ Rm be a parameter vector for tasks in U \\ {u}. In this case the value of the inner product 〈w, φu(x)〉 is bounded by\nPr {|〈w, φu(x)〉| > ǫ} ≤ 2e − ǫ\n2/2\nm−1‖w‖2 2 ‖x‖2 2 +ǫ‖w‖∞‖x‖∞/3\nProof We use Bernstein’s inequality (Bernstein, 1946), which states that for independent random variables Xj , with E [Xj ] = 0, if C > 0 is such that |Xj| ≤ C, then\nPr\n\n\nn ∑\nj=1\nXj>t\n ≤exp ( − t 2/2\n∑n j=1 E [ X2j ] + Ct/3\n)\n. (4)\nWe have to compute the concentration property of 〈w, φu(x)〉 = ∑\nj xjξ(j)wh(j). Let Xj = xjξ(j)wh(j). By the definition of h and ξ, Xj are independent. Also, for each j, since w depends only on the hash-functions for U \\ {u}, wh(j) is independent of ξ(j). Thus, E[Xj] = E(ξ,h) [ xjξ(j)wh(j) ]\n= 0. For each j, we also have |Xj | < ‖x‖∞ ‖w‖∞ =: C. Finally, ∑ j E[X 2 j ] is given by\nE\n\n\n∑\nj\n(xjξ(j)wh(j)) 2\n\n = 1m\n∑\nj,ℓ\nx2jw 2 ℓ = 1 m ‖x‖ 2 2 ‖w‖ 2 2\nThe claim follows by plugging both terms and C into the Bernstein inequality (4).\nTheorem 7 bounds the influence of unrelated tasks with any particular instance. In section 5 we demonstrate the realworld applicability with empirical results on a large-scale multi-task learning problem."
    }, {
      "heading" : "4. Applications",
      "text" : "The advantage of feature hashing is that it allows for significant storage compression for parameter vectors: storing w in the raw feature space naively requires O(d) numbers, when w ∈ Rd. By hashing, we are able to reduce this to\nFeature Hashing for Large Scale Multitask Learning\nO(m) numbers while avoiding costly matrix-vector multiplications common in Locally Sensitive Hashing. In addition, the sparsity of the resulting vector is preserved.\nThe benefits of the hashing-trick leads to applications in almost all areas of machine learning and beyond. In particular, feature hashing is extremely useful whenever large numbers of parameters with redundancies need to be stored within bounded memory capacity.\nPersonalization One powerful application of feature hashing is found in multitask learning. Theorem 7 allows us to hash multiple classifiers for different tasks into one feature space with little interaction. To illustrate, we explore this setting in the context of spam-classifier personalization.\nSuppose we have thousands of users U and want to perform related but not identical classification tasks for each of the them. Users provide labeled data by marking emails as spam or not-spam. Ideally, for each user u ∈ U , we want to learn a predictor wu based on the data of that user solely. However, webmail users are notoriously lazy in labeling emails and even those that do not contribute to the training data expect a working spam filter. Therefore, we also need to learn an additional global predictorw0 to allow data sharing amongst all users.\nStoring all predictors wi requires O(d × (|U | + 1)) memory. In a task like collaborative spam-filtering, d, the number of users can be in the hundreds of thousands and the size of the vocabulary is usually in the order of millions. The naive way of dealing with this is to eliminate all infrequent tokens. However, spammers target this memory-vulnerability by maliciously misspelling words and thereby creating highly infrequent but spam-typical tokens that “fall under the radar” of conventional classifiers. Instead, if all words are hashed into a finite-sized feature vector, infrequent but class-indicative tokens get a chance to contribute to the classification outcome. Further, large scale spam-filters (e.g. Yahoo MailTMor GMailTM) typically have severe memory and time constraints, since they have to handle billions of emails per day. To guarantee a finite-size memory footprint we hash all weight vectors w0, . . . , w|U| into a joint, significantly smaller, feature space Rm with different hash functions φ0, . . . , φ|U|. The resulting hashed-weight vector wh ∈ Rm can then be written as:\nwh = φ0(w0) + ∑\nu∈U φu(wu). (5)\nNote that in practice the weight vector wh can be learned directly in the hashed space. All un-hashed weight vectors never need to be computed. Given a new document/email x of user u ∈ U , the prediction task now consists of calculating 〈φ0(x) + φu(x), wh〉. Due to hashing we have two\nsources of error – distortion ǫd of the hashed inner products and the interference with other hashed weight vectors ǫi. More precisely:\n〈φ0(x) + φu(x), wh〉 = 〈x,w0 + wu〉+ ǫd + ǫi. (6)\nThe interference error consists of all collisions between φ0(x) or φu(x) with hash functions of other users,\nǫi= ∑\nv∈U,v 6=0 〈φ0(x), φv(wv)〉+\n∑\nv∈U,v 6=u 〈φu(x), φv(wv)〉 . (7)\nTo show that ǫi is small with high probability we can apply Theorem 7 twice, once for each term of (7). We consider each user’s classification to be a separate task, and since ∑\nv∈U,v 6=0 wv is independent of the hashfunction φ0, the conditions of Theorem 7 apply with w = ∑\nv 6=0 wv and we can employ it to bound the second term, ∑\nv∈U,v 6=0 〈φu(x), φu(wv)〉. The second application is identical except that all subscripts “0” are substituted with “u”. For lack of space we do not derive the exact bounds.\nThe distortion error occurs because each hash function that is utilized by user u can self-collide:\nǫd = ∑\nv∈{u,0} | 〈φv(x), φv(wv)〉 − 〈x,wv〉 |. (8)\nTo show that ǫd is small with high probability, we apply Corollary 4 once for each possible values of v.\nIn section 5 we show experimental results for this setting. The empirical results are stronger than the theoretical bounds derived in this subsection—our technique outperforms a single global classifier on hundreds thousands of users. We discuss an intuitive explanation in section 5.\nMassively Multiclass Estimation We can also regard massively multi-class classification as a multitask problem, and apply feature hashing in a way similar to the personalization setting. Instead of using a different hash function for each user, we use a different hash function for each class.\n(Shi et al., 2009) apply feature hashing to problems with a high number of categories. They show empirically that joint hashing of the feature vector φ(x, y) can be efficiently achieved for problems with millions of features and thousands of classes.\nCollaborative Filtering Assume that we are given a very large sparse matrix M where the entry Mij indicates what action user i took on instance j. A common example for actions and instances is user-ratings of movies (Bennett & Lanning, ). A successful method for finding common factors amongst users and instances for predicting unobserved\nFeature Hashing for Large Scale Multitask Learning\nNEU Votre\nApotheke ...\n1 0 -1 0 0 -1 0 1 0 ...\ntext document (email) bag of words hashed,\nsparse vector\nx NEU USER123_NEU\nVotre USER123_Votre\nApotheke USER123_Apotheke\n...\nφ\nφ0(x)+φu(x)\nbag of words (personalized)\nFigure 1. The hashed personalization summarized in a schematic layout. Each token is duplicated and one copy is individualized (e.g. by concatenating each word with a unique user identifier). Then, the global hash function maps all tokens into a low dimensional feature space where the document is classified.\nactions is to factorize M into M = U⊤W . If we have millions of users performing millions of actions, storing U and W in memory quickly becomes infeasible. Instead, we may choose to compress the matrices U and W using hashing. For U,W ∈ Rn×d denote by u,w ∈ Rm vectors with\nui = ∑\nj,k:h(j,k)=i\nξ(j, k)Ujk and wi = ∑\nj,k:h′(j,k)=i\nξ′(j, k)Wjk .\nwhere (h, ξ) and (h′, ξ′) are independently chosen hash functions. This allows us to approximate matrix elements Mij = [U ⊤W ]ij via\nMφij := ∑\nk\nξ(k, i)ξ′(k, j)uh(k,i)wh′(k,j).\nThis gives a compressed vector representation of M that can be efficiently stored."
    }, {
      "heading" : "5. Results",
      "text" : "We evaluated our algorithm in the setting of personalization. As data set, we used a proprietary email spamclassification task of n = 3.2 million emails, properly anonymized, collected from |U | = 433167 users. Each email is labeled as spam or not-spam by one user in U . After tokenization, the data set consists of 40 million unique words.\nFor all experiments in this paper, we used the Vowpal Wabbit implementation1 of stochastic gradient descent on a square-loss. In the mail-spam literature the misclassification of not-spam is considered to be much more harmful than misclassification of spam. We therefore follow the convention to set the classification threshold during test time such that exactly 1% of the not − spam test data is classified as spam Our implementation of the personalized hash functions is illustrated in Figure 1. To obtain a personalized hash function φu for user u, we concatenate a unique\n1http://hunch.net/∼vw/\nuser-id to each word in the email and then hash the newly generated tokens with the same global hash function.\nThe data set was collected over a span of 14 days. We used the first 10 days for training and the remaining 4 days for testing. As baseline, we chose the purely global classifier trained over all users and hashed into 226 dimensional space. As 226 far exceeds the total number of unique words we can regard the baseline to be representative for the classification without hashing. All results are reported as the amount of spam that passed the filter undetected, relative to this baseline (eg. a value of 0.80 indicates a 20% reduction in spam for the user)2.\nFigure 2 displays the average amount of spam in users’ inboxes as a function of the number of hash keys m, relative to the baseline above. In addition to the baseline, we evaluate two different settings.\nThe global-hashed curve represents the relative spam catch-rate of the global classifier after hashing 〈φ0(w0), φ0(x)〉. At m = 226 this is identical to the baseline. Early convergence at m = 222 suggests that at this point hash collisions have no impact on the classification error and the baseline is indeed equivalent to that obtainable without hashing.\nIn the personalized setting each user u ∈ U gets her own classifier φu(wu) as well as the global classifier φ0(w0). Without hashing the feature space explodes, as the cross product of u = 400K users and n = 40M tokens results in 16 trillion possible unique personalized features. Figure 2 shows that despite aggressive hashing, personalization results in a 30% spam reduction once the hash table is indexed by 22 bits.\n2As part of our data sharing agreement, we agreed not to include absolute classification error-rates.\nFeature Hashing for Large Scale Multitask Learning\nUser clustering One hypothesis for the strong results in Figure 2 might originate from the non-uniform distribution of user votes — it is possible that using personalization and feature hashing we benefit a small number of users who have labeled many emails, degrading the performance of most users (who have labeled few or no emails) in the process. In fact, in real life, a large fraction of email users do not contribute at all to the training corpus and only interact with the classifier during test time. The personalized version of the test email Φu(xu) is then hashed into buckets of other tokens and only adds interference noise ǫi to the classification.\nIn order to show that we improve the performance of most users, it is therefore important that we not only report averaged results over all emails, but explicitly examine the effects of the personalized classifier for users depending on their contribution to the training set. To this end, we place users into exponentially growing buckets based on their number of training emails and compute the relative reduction of uncaught spam for each bucket individually. Figure 3 shows the results on a per-bucket basis. We do not compare against a purely local approach, with no global component, since for a large fraction of users—those without training data—this approach cannot outperform random guessing.\nIt might appear rather surprising that users in the bucket with none or very little training emails (the line of bucket [0] is identical to bucket [1]) also benefit from personalization. After all, their personalized classifier was never trained and can only add noise at test-time. The classifier improvement of this bucket can be explained by the subjective definition of spam and not-spam. In the personalized setting the individual component of user labeling is absorbed by the local classifiers and the global classifier represents the common definition of spam and not-spam. In other words, the global part of the personalized classi-\nfier obtains better generalization properties, benefiting all users."
    }, {
      "heading" : "6. Related Work",
      "text" : "A number of researchers have tackled related, albeit different problems.\n(Rahimi & Recht, 2008) use Bochner’s theorem and sampling to obtain approximate inner products for Radial Basis Function kernels. (Rahimi & Recht, 2009) extend this to sparse approximation of weighted combinations of basis functions. This is computationally efficient for many function spaces. Note that the representation is dense.\n(Li et al., 2007) take a complementary approach: for sparse feature vectors, φ(x), they devise a scheme of reducing the number of nonzero terms even further. While this is in principle desirable, it does not resolve the problem of φ(x) being high dimensional. More succinctly, it is necessary to express the function in the dual representation rather than expressing f as a linear function, where w is unlikely to be compactly represented: f(x) = 〈φ(x), w〉. (Achlioptas, 2003) provides computationally efficient randomization schemes for dimensionality reduction. Instead of performing a dense d·m dimensional matrix vector multiplication to reduce the dimensionality for a vector of dimensionality d to one of dimensionality m, as is required by the algorithm of (Gionis et al., 1999), he only requires 13 of that computation by designing a matrix consisting only of entries {−1, 0, 1}. The work most closely related ot that presented here is the CountMin sketch of (Cormode & Muthukrishnan, 2004) which stores counts in a number of replicates of a hash table. This leads to good concentration inequalities for range and point queries.\n(Shi et al., 2009) propose a hash kernel to deal with the issue of computational efficiency by a very simple algorithm: high-dimensional vectors are compressed by adding up all coordinates which have the same hash value — one only needs to perform as many calculations as there are nonzero terms in the vector. This is a significant computational saving over locality sensitive hashing (Achlioptas, 2003; Gionis et al., 1999).\nSeveral additional works provide motivation for the investigation of hashing representations. For example, (Ganchev & Dredze, 2008) provide empirical evidence that the hashing trick can be used to effectively reduce the memory footprint on many sparse learning problems by an order of magnitude via removal of the dictionary. Our experimental results validate this, and show that much more radical compression levels are achievable. In addition, (Langford et al., 2007) released the Vowpal Wabbit fast online learn-\nFeature Hashing for Large Scale Multitask Learning\ning software which uses a hash representation similar to that discussed here."
    }, {
      "heading" : "7. Conclusion",
      "text" : "In this paper we analyze the hashing-trick for dimensionality reduction theoretically and empirically. As part of our theoretical analysis we introduce unbiased hash functions and provide exponential tail bounds for hash kernels. These give further inside into hash-spaces and explain previously made empirical observations. We also derive that random subspaces of the hashed space are likely to not interact, which makes multitask learning with many tasks possible.\nOur empirical results validate this on a real-world application within the context of spam filtering. Here we demonstrate that even with a very large number of tasks and features, all mapped into a joint lower dimensional hashspace, one can obtain impressive classification results with finite memory guarantee."
    }, {
      "heading" : "A. Mean and Variance",
      "text" : "Proof [Lemma 2] To compute the expectation we expand\n〈x, x′〉φ = ∑\ni,j\nξ(i)ξ(j)xix ′ jδh(i),h(j). (9)\nSince Eφ[〈x, x′〉φ] = Eh[Eξ[〈x, x′〉φ]], taking expectations over ξ we see that only the terms i = j have nonzero value, which shows the first claim. For the variance we compute Eφ[〈x, x′〉2φ]. Expanding this, we get:\n〈x, x′〉2φ = ∑\ni,j,k,l\nξ(i)ξ(j)ξ(k)ξ(l)xix ′ jxkx ′ lδh(i),h(j)δh(k),h(l).\nThis expression can be simplified by noting that:\nEξ [ξ(i)ξ(j)ξ(k)ξ(l)] = δijδkl + [1− δijkl ](δikδjl + δilδjk).\nPassing the expectation over ξ through the sum, this allows us to break down the expansion of the variance into two terms.\nEφ[〈x, x′〉2φ] = ∑\ni,k\nxix ′ ixkx ′ k +\n∑ i6=j x2ix ′ j 2 Eh [ δh(i),h(j) ]\n+ ∑\ni6=j xix\n′ ixjx ′ jEh [ δh(i),h(j) ]\n= 〈x, x′〉2 + 1 m\n\n\n∑ i6=j x2ix ′ j 2 + ∑ i6=j xix ′ ixjx ′ j\n\n\nby noting that Eh [ δh(i),h(j) ] = 1m for i 6= j. Using the fact that σ2 = Eφ[〈x, x′〉2φ]−Eφ[〈x, x′〉φ]2 proves the claim.\nFeature Hashing for Large Scale Multitask Learning"
    }, {
      "heading" : "B. Concentration of Measure",
      "text" : "Our proof uses Talagrand’s convex distance inequality. We first define a weighted Hamming distance function between two hash-function φ and φ′ as follows.\nd(φ, φ′) = sup ‖α‖\n2 ≤1\n∑\ni\nαiI(h(i) 6= h′(i) or ξ(i) 6= ξ′(i))\n= √ | {i : h(i) 6= h′(i) or ξ(i) 6= ξ′(i)} |\nNext denote by d(φ,A) the distance between a hash function and a set A of hash functions, that is d(φ,A) = infφ′∈A d(φ, φ′). In this case Talagrand’s convex distance inequality (Ledoux, 2001) holds. If Pr(A) denotes the total probability mass of the set A, then\nPr {d(φ,A) ≥ s} ≤ [Pr(A)]−1 e−s2/4. (10)\nProof [Theorem 3] Without loss of generality assume that ‖x‖2 = 1. We can then easily generalize to the general x case. From Lemma 2 it follows that the variance of ‖x‖2φ is given by σ2x,x = 2 N [1− ‖x‖ 4 4] and E(‖x‖ 2 φ) = 1.\nChebyshev’s inequality states that P (|X − E(X)| ≤√ 2σ) ≥ 12 . We can therefore denote\nA := { φ where ∣ ∣\n∣ ‖x‖2φ − 1\n∣ ∣ ∣ ≤ √ 2σx,x } .\nand obtain Pr(A) ≥ 12 . From Talagrand’s inequality (10) we know that Pr({φ : d(φ,A) ≥ s}) ≤ 2e−s2/4. Now assume that we have a pair of hash functions φ and φ′, with φ′ ∈ A. Let us define the difference of their hashed innerproducts as δ := ‖x‖2φ − 〈x, x〉φ′ . By the triangle inequality and because φ′ ∈ A, we can state that\n∣ ∣ ∣‖x‖2φ − 1 ∣ ∣ ∣ ≤ ∣ ∣ ∣‖x‖2φ − 〈x, x〉φ′ ∣ ∣ ∣+ ∣ ∣ ∣〈x, x〉φ′ − 1 ∣ ∣ ∣\n≤ |δ|+ √ 2σ. (11)\nLet us now denote the coordinate-wise difference between the hashed features as vi := φ′i(x) − φi(x). With this definition, we can express δ in terms of v: δ = ∑\ni φi(x) 2 −\nφ′i(x) 2 = −2 〈φ′(x), v〉 + ‖v‖22. By applying the CauchySchwartz inequality to the inner product 〈φ′(x), v〉, we obtain |δ| ≤ 2‖φ′(x)‖2‖v‖2 + ‖v‖22. Plugging this into (11) leads us to\n∣ ∣ ∣‖x‖2φ − 1 ∣ ∣ ∣ ≤ 2‖φ′(x)‖2‖v‖2 + ‖v‖22 + √ 2σx,x. (12)\nNext, we bound ‖v‖2 in terms of d(φ, φ′). To do this, expand vi = ∑ j xj(ξ ′ jδh′(j)i− ξjδh(j)i). As ξj ∈ {+1,−1}, we know that |ξj − ξ′j | ≤ 2. Further, xj ≤ ‖x‖∞ and we can write\n|vi| ≤ 2 ‖x‖∞ ∑\nj\nδh(j)i + δh′(j)i. (13)\nWe can now make two observations: First note that ∑\ni\n∑\nj δh(j)i+δh′(j)i is at most 2t where t = |{j : h(j) 6= h′(j)}|. Second, from the definition of the distance function, we get that d(φ, φ′) ≥ √ t. Putting these together,\n∑\ni\n|vi| ≤ 4 ‖x‖∞ t ≤ 4 ‖x‖∞ d2(φ, φ′)\n‖vi‖22 = ∑\ni\n|vi|2 ≤ 16 ‖x‖2∞ d4(φ, φ′).\n(The last inequality holds because in the worst case all mass is concentrated in a single entry of vi.) As a next step we will express ‖φ′(x)‖2 in terms of σx,x. Because φ′ ∈ A, we obtain that\n‖φ′(x)‖2 = √ 〈x, x〉φ′ ≤ (1+ √ 2σx,x) 1/2 ≤ 1+σx,x/ √ 2. To simplify our notation, let us define β = 1 + σx,x/ √ 2. Plugging our upper bounds for ‖v‖2 and ‖φ′(x)‖2 into (12) leads to ∣ ∣\n∣‖x‖2φ−1 ∣ ∣ ∣≤8 ‖x‖∞βd2(φ, φ′)(β+2 ‖x‖∞d2(φ, φ′))+ √ 2σ.\nAs we have not specified our particular choice of φ′, we can now choose it to be the closest vector to φ within A, ie such that d(φ, φ′) = d(φ,A). By Talagrand’s inequality, we know that with probability at least 1−2e−s2/4 we obtain d(φ,A) ≤ s and therefore with high probability:\n∣ ∣ ∣‖x‖2φ − 1 ∣ ∣ ∣ ≤ 8 ‖x‖∞ βs2 + 16 ‖x‖ 2 ∞ s 4 + √ 2σ.\nA change of variables s2 = √\nβ2+ǫ−β 4‖x‖∞ gives us that ∣ ∣\n∣‖x‖2φ − 1 ∣ ∣ ∣ ≤ √ 2σ + ǫ w.p. 1 − 2e−s2/4. Noting that s2 = ( √ β2 + ǫ − β)/4 ‖x‖∞ ≥ √ ǫ/4 ‖x‖∞, lets us obtain our final result ∣\n∣ ∣‖x‖2φ − 1 ∣ ∣ ∣ ≤ √ 2σ + ǫ w.p. 1− 2e− √ ǫ/4‖x‖∞ .\nFinally, for a general x, we can derive the above result for y = x‖x‖2 . Replacing ‖y‖∞ = ‖x‖∞ ‖x‖2 we get the following version for a general x,\nPr\n{\n| ‖x‖2φ − ‖x‖ 2 2 |\n‖x‖22 ≥\n√ 2σx,x + ǫ\n}\n≤ exp ( − √ ǫ‖x‖2\n4‖x‖∞\n)\nC. Inner Product\nProof [Corollary 4] We have that 2 〈x, x′〉φ = ‖x‖ 2 φ + ‖x′‖2φ−‖x− x′‖ 2 φ. Taking expectations, we have the standard inner product inequality. Thus,\n|2 〈φu(x), φu(x)〉 − 2 〈x, x〉 | ≤ | ‖φu(x)‖2 − ‖x‖2 | + | ‖φu(x′)‖2 − ‖x′‖2 |+ | ‖φ(x− x′)‖2 − ‖x− x′‖2 |\nFeature Hashing for Large Scale Multitask Learning\nUsing union bound, with probability 1 − 3 exp ( − √ ǫ\n4η\n)\n,\neach of the terms above is bounded using Theorem 3. Thus, putting the bounds together, we have that, with probability 1− 3 exp ( − √ ǫ\n4η\n)\n,\n|2 〈φu(x), φu(x)〉 − 2 〈x, x〉 | ≤ ≤ ( √ 2σ + ǫ)(‖x‖2 + ‖x′‖2 + ‖x− x′‖2)\nThis figure \"global_results.jpg\" is available in \"jpg\" format from:\nhttp://arxiv.org/ps/0902.2206v1\nThis figure \"results.jpg\" is available in \"jpg\" format from:\nhttp://arxiv.org/ps/0902.2206v1\nThis figure \"schema.jpg\" is available in \"jpg\" format from:\nhttp://arxiv.org/ps/0902.2206v1\nThis figure \"schema.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/0902.2206v1"
    } ],
    "references" : [ {
      "title" : "Database-friendly random projections: Johnson-lindenstrauss with binary coins",
      "author" : [ "D. Achlioptas" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Achlioptas,? \\Q2003\\E",
      "shortCiteRegEx" : "Achlioptas",
      "year" : 2003
    }, {
      "title" : "The Netflix Prize",
      "author" : [ "J. Bennett", "S. Lanning" ],
      "venue" : "Proceedings of KDD Cup and Workshop",
      "citeRegEx" : "Bennett and Lanning,? \\Q2007\\E",
      "shortCiteRegEx" : "Bennett and Lanning",
      "year" : 2007
    }, {
      "title" : "The theory of probabilities. Moscow: Gastehizdat Publishing House",
      "author" : [ "S. Bernstein" ],
      "venue" : null,
      "citeRegEx" : "Bernstein,? \\Q1946\\E",
      "shortCiteRegEx" : "Bernstein",
      "year" : 1946
    }, {
      "title" : "An improved data stream summary: The count-min sketch and its applications",
      "author" : [ "G. Cormode", "M. Muthukrishnan" ],
      "venue" : "LATIN: Latin American Symposium on Theoretical Informatics",
      "citeRegEx" : "Cormode and Muthukrishnan,? \\Q2004\\E",
      "shortCiteRegEx" : "Cormode and Muthukrishnan",
      "year" : 2004
    }, {
      "title" : "Small statistical models by random feature mixing",
      "author" : [ "K. Ganchev", "M. Dredze" ],
      "venue" : "Workshop on Mobile Language Processing, Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Ganchev and Dredze,? \\Q2008\\E",
      "shortCiteRegEx" : "Ganchev and Dredze",
      "year" : 2008
    }, {
      "title" : "Similarity search in high dimensions via hashing",
      "author" : [ "A. Gionis", "P. Indyk", "R. Motwani" ],
      "venue" : "Proceedings of the 25th VLDB Conference (pp. 518–529)",
      "citeRegEx" : "Gionis et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Gionis et al\\.",
      "year" : 1999
    }, {
      "title" : "Vowpal wabbit online learning project (Technical Report)",
      "author" : [ "J. Langford", "L. Li", "A. Strehl" ],
      "venue" : null,
      "citeRegEx" : "Langford et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2007
    }, {
      "title" : "The concentration of measure phenomenon",
      "author" : [ "M. Ledoux" ],
      "venue" : null,
      "citeRegEx" : "Ledoux,? \\Q2001\\E",
      "shortCiteRegEx" : "Ledoux",
      "year" : 2001
    }, {
      "title" : "Conditional random sampling: A sketch-based sampling technique for sparse",
      "author" : [ "P. Li", "K. Church", "T. Hastie" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2007
    }, {
      "title" : "Random features for largescale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : null,
      "citeRegEx" : "Rahimi and Recht,? \\Q2008\\E",
      "shortCiteRegEx" : "Rahimi and Recht",
      "year" : 2008
    }, {
      "title" : "Randomized kitchen sinks",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : null,
      "citeRegEx" : "Rahimi and Recht,? \\Q2009\\E",
      "shortCiteRegEx" : "Rahimi and Recht",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "For this common scenario several authors have recently proposed an alternative, but highly complimentary variation of the kernel-trick, which we refer to as the hashing-trick: one hashes the high dimensional input vectors x into a lower dimensional feature space R with φ : X → R (Langford et al., 2007; Shi et al., 2009).",
      "startOffset" : 280,
      "endOffset" : 321
    }, {
      "referenceID" : 7,
      "context" : "Talagrand’s inequality (Ledoux, 2001) is a key tool for the proof of the following theorem (detailed in the appendix B).",
      "startOffset" : 23,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "Proof We use Bernstein’s inequality (Bernstein, 1946), which states that for independent random variables Xj , with E [Xj ] = 0, if C > 0 is such that |Xj| ≤ C, then",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : "(Li et al., 2007) take a complementary approach: for sparse feature vectors, φ(x), they devise a scheme of reducing the number of nonzero terms even further.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "(Achlioptas, 2003) provides computationally efficient randomization schemes for dimensionality reduction.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "Instead of performing a dense d·m dimensional matrix vector multiplication to reduce the dimensionality for a vector of dimensionality d to one of dimensionality m, as is required by the algorithm of (Gionis et al., 1999), he only requires 1 3 of that computation by designing a matrix consisting only of entries {−1, 0, 1}.",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 0,
      "context" : "This is a significant computational saving over locality sensitive hashing (Achlioptas, 2003; Gionis et al., 1999).",
      "startOffset" : 75,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "This is a significant computational saving over locality sensitive hashing (Achlioptas, 2003; Gionis et al., 1999).",
      "startOffset" : 75,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "In addition, (Langford et al., 2007) released the Vowpal Wabbit fast online learn-",
      "startOffset" : 13,
      "endOffset" : 36
    } ],
    "year" : 2017,
    "abstractText" : "Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case — multitask learning with hundreds of thousands of tasks.",
    "creator" : null
  }
}