{
  "name" : "1604.04383.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Composition of Deep and Spiking Neural Networks for Very Low Bit Rate Speech Coding",
    "authors" : [ "Milos Cernak", "Alexandros Lazaridis", "Afsaneh Asaei", "Philip N. Garner" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we propose a novel VLBR speech coding framework based on neural networks (NNs) for end-to-end speech analysis and synthesis without HMMs. The speech coding framework relies on phonological (sub-phonetic) representation of speech, and it is designed as a composition of deep and spiking NNs: a bank of phonological analysers at the transmitter, and a phonological synthesizer at the receiver, both realised as deep NNs, and a spiking NN as an incremental and robust encoder of syllable boundaries for coding of continuous fundamental frequency (F0). A combination of phonological features defines much more sound patterns than phonetic features defined by HMM-based speech coders, and the finer analysis/synthesis code contributes into smoother encoded speech. Listeners significantly prefer the NN-based approach due to fewer discontinuities and speech artefacts of the encoded speech. A single forward pass is required during the speech encoding and decoding. The proposed VLBR speech coding operates at bit rate about 360 bits/sec.\nIndex Terms—Very low bit rate speech coding, deep neural networks, spiking neural networks, continuous F0 coding\nI. INTRODUCTION\nThe ITU-T standardisation effort for speech coders operated below 4 k bits-per-second (bps) began in 1994 [1], but it has been shown to be difficult to achieve toll-quality performance in all conditions, such as intelligibility, quality, speaker recognizability, communicability, language independence and complexity. All these conditions are exposed even more in speech coders operated at VLBR speech coding of the order of hundreds of bps.\nTo achieve VLBR of 200–500 bps, parametric speech coding based on recognition/synthesis paradigm has been proposed. Approaches following this paradigm can be classified into two categories: corpus-based, e.g., [2], [3], and hidden Markov model (HMM) based, e.g., [4]–[6]. This classification follows trends in speech synthesis, where popular unitselection methods are replaced by HMM-based parametric speech synthesis methods. The parametric methods benefit\nAuthors are with Idiap Research Institute, Centre du Parc, Rue Marconi 19, 1920 Martigny, Switzerland, contact details: http://people.idiap.ch/mcernak.\nfrom better adaptation properties and lower footprint. However, most current HMM-based VLBR systems have complex designs. The phonetic encoder — automatic speech recognition (ASR) — consists of acoustic HMMs and language models and an incremental search module; similarly the phonetic decoder requires acoustic HMMs, including a streaming/performative HMM-based speech synthesis system and an incremental speech vocoder.\nOur recent work [7] also focused on HMM-based VLBR speech coding, designing a coder operating with an acceptable communication delay for real-time speech communication, with a view to be exploited in military and tactical communication systems.\nCloser analysis of our HMM-based encoder — a phoneme ASR system — revealed that it sometimes creates bursts of segmental errors when the recognition fails. This is well known; phoneme ASR misrecognition rates tend to increase with longer words [8]. In addition, the coder [7] detects syllable boundaries from recognised phoneme sequences based on the sonority sequencing principle, and thus phoneme misrecognition is further propagated to the syllable boundary estimation that may result in wrongly detected syllables. The HMM coder also uses voiced/unvoiced detection for parametrization of the F0 signal in the voiced regions. This F0 encoding plugged-in to the VLBR system creates additional speech discontinuities and unnatural speech sound artefacts. Moreover, canonical phone indexes transmitted from a transmitter to a receiver compress all phonetic variability to the order of tens of phonetic categories. Increasing the number of categories increases the bit rate.\nIn this paper, we aim to address the above limitations of HMM-based recognition/synthesis very low bit rate speech coding. First, we propose to replace HMMs by deep NNs speech analysis and synthesis based on a phonological speech representation. The phonological speech representation extends encoded phonetic variability to the order of hundreds or even thousands of sound categories. Second, we propose to use a spiking NN as a neuromorphic incremental and highly noiserobust syllable boundary detector, used for syllable-based continuous F0 signal coding. Using continuous F0 modelling should also alleviate speech re-synthesis discontinuities caused by erroneous detection of unvoiced segments. This end-to-end NN based speech coding (called NN speech coder hereinafter) should significantly reduce current design (and hopefully also computational) complexity – the speech encoding and decoding would be realised as a single NN forward pass.\nIn this work, we consider the three phonological systems\nar X\niv :1\n60 4.\n04 38\n3v 1\n[ cs\n.S D\n] 1\n5 A\npr 2\n01 6\n2 defined in Appendix A of [9]: (i) the Government Phonology (GP) [10], [11], (ii) the Sound Pattern of English (SPE) [12] and (iii) the extended SPE system (eSPE) [13], [14]. Each phoneme is represented by its sub-phonetic attributes, or phonological classes. A vector of all phonological class probabilities is referred to as phonological posterior. There are only very few phonological classes comprising a short term speech signal; hence, the phonological posterior is a sparse vector, and allow very few combinations that can be stored in a codebook for VLBR speech coding [15]. The temporal span of phonological features is wider than the span of phonetic features and thus the frame shift could be higher, i.e., fewer frames are transmitted yielding lower bit rates.\nThe rest of the paper is organized as follows. Section II contains a review of usage of NNs for speech coding. In section III an open-source experimental framework used in this work is introduced, and in Section IV the results are presented of a comparison of HMM-based and NN-based VLBR speech coders. Finally, conclusions are drawn in Section VI."
    }, {
      "heading" : "II. NEURAL NETWORKS FOR SPEECH CODING",
      "text" : ""
    }, {
      "heading" : "A. Background",
      "text" : "Recently, in the speech recognition/analysis field, a shift has been observed from HMM-based approaches towards the use of deep neural networks (DNNs) [16]. Even though the concept of using neural networks in recognition is not new [17], the increase of available speech resources and of computational power, and the use of graphics processing units, have led to a major research interest in DNNs. Additionally, deep architectures with multiple layers can overcome the limitations in the representational capability of HMMs, which are incapable of modelling multiple interacting source streams [18]. Furthermore, DNNs are able to create non-linear mappings between the input and output features which cannot be achieved by using Gaussian mixture models (GMMs) in HMM-based approaches, making them more appropriate for modelling the speech signal [19]. As a result, DNN-based approaches have managed to show superior performance in comparison to HMM-based ones in recognition tasks [19], [20].\nThe same shift has also been observed in the text-to-speech (TTS). There are various limitations and drawbacks which occur in HMM-based TTS [21], e.g., inefficiency to express complex dependencies in the feature space [22], which leads to decision trees becoming exceedingly large, hence inefficient and data hungry. Also, the use of decision trees leads to the fragmentation of the training data, linking specific parts of them to each terminal node of the tree [23]. These limitations led to the introduction of DNNs in the field of parametric speech synthesis as well, constantly outperforming HMMbased speech synthesis systems [21], [24], [25].\nIn the context of speech coding, NNs are used in waveformapproximating coders, a family of coders originated in [26], [27], to address either improving quality or reducing computational complexity. The former usage aims to improve linear prediction (of speech samples or parameters of the excitation signal) with a non-linear prediction based usually on\nmultilayer perceptrons [28]–[32]. Recently, regression-based packet loss concealment was proposed using DNNs [33]. The latter usage aims to reduce the complexity of the codebook search process or gain prediction [34] using, for example, recurrent NNs [35], [36].\nSpeech coding based on speech modelling is known as parametric coding, where the parameters of the speech models are transmitted, such as in a multiband excitation vocoder [37]. Similarly, as in waveform coding, a multilayer perceptron was proposed to decrease the computation complexity of the codebook of line spectral frequencies in the 800 bps multiband excitation speech coding [38].\nWhile NNs have been used in previous speech coding approaches only as isolated modules targeting some particular computation, we propose end-to-end VLBR NN-based coder, aiming to replace HMM-based speech analysis and synthesis by deep and spiking NNs."
    }, {
      "heading" : "B. Coding of phonetic and phonological information",
      "text" : "Encoding of segmental information starts with analysis by converting a segment of speech samples into a sequence of acoustic features X = {~x1, . . . , ~xn, . . . , ~xN} where N denotes the number of segments in the utterance. Conventional cepstral coefficients can be used as acoustic features. Encoding can be done on a phonetic or phonological (sub-phonetic) level.\nIn the former case, the acoustic feature observation sequence X is converted into a parameter sequence ~zn = [z1n, . . . , z p n, . . . , z P n ] > where the n-th frame consists of posterior probabilities zpn = p(cp|xn) of P classes (phonemes), and .> stands for the transpose operator. The a posteriori estimates p(cp|xn) are 0 ≤ p(cp|xn) ≤ 1,∀p and ∑P p=1 p(cp|xn) = 1. All the phonemes have to be recognised to access higher semantic levels (words and utterances), hence, using the phone posterior probabilities can be considered in a sequential sense. The phonetic vocoding, where only one encoded segment (a phoneme) is transmitted each time, is an example.\nIn the latter case, the acoustic feature observation sequence X is converted into a parameter sequence ~zn = [p(c1|xn), . . . , p(ck|xn), . . . , p(cK |xn)]> that consists of K phonological class-conditional posterior probabilities, where ck denotes the phonological class. The phonological posteriors are computed by a bank of parallel DNNs, each estimating the posteriors zkn as probabilities that the k-th phonological feature occurs (versus does not occur). The a-posteriori estimates p(ck|xn) are also 0 ≤ p(ck|xn) ≤ 1,∀k, but max ∑K k=1 p(ck|xn) = K. Only very few classes are active\nduring a short term signal, ∑K\nk=1 p(ck|xn) K, resulting in a sparse vector ~zn. Using the phonological posterior probabilities can be considered a parallel scheme via K different phonological classes.\nWhile erroneous phone posterior estimation leads to a possible failure of the higher semantic segment recognition, erroneous phonological posterior estimation leads to a failure only at a sub-phonetic feature level, and this partial error does not necessarily lead to misrecognition of the whole recognized segment.\nDecoding of segmental information is realised as a DNN that learns the highly-complex mapping of the parameter\n3 sequence Z to the speech parameters [39]. It consists of two computational steps. The first step is a DNN forward pass that generates the speech parameters (the LPC speech parameters described in Section III-C4); the second one is generation of the speech samples from the speech parameters."
    }, {
      "heading" : "C. Coding of prosodic information",
      "text" : "Speech analysis results in discrete units while moving from segmental to suprasegmental (prosodic) level of speech representation. The discrete information can be estimated directly from the speech signal. For example, Probabilistic Amplitude Demodulation method proposed in [40] may robustly estimate the syllable and stress amplitude modulations as a representation of electrophysiological recordings of auditory cortex. The work of [41] proves that phase relations of the amplitude modulations, known as hierarchical phase locking and nesting or synchronization across different temporal granularity [42], is a good indication of the syllable stress.\nPhonological posteriors have several interesting properties. Even though they are segmental features by definition, they convey prosodic information about lexical stress and prosodic accent, embedded in their support (index) of active coefficients [43]. In the context of parametric speech coding it could be interpreted that we do not need to encode this kind of prosodic information explicitly. Rather, in our current approach, we focus just on coding of the continuous F0 signal. Modelling continuous F0 has been shown to be more effective in achieving natural synthesised speech [44], [45], and can be effectively used with noisy speech [46]. We hypothesise that continuous F0 modelling could improve recognition/synthesis VLBR coding as well.\nEffective encoding of the F0 signal can be realised by curve fitting done on syllable level. We thus propose to encode the continuous F0 signal using the discrete (Legendre) orthogonal polynomial (DLOP), similarly as in [7]. To estimate syllable boundaries from the speech signal, a neuromorphic oscillatory device is used based on modelling brain neural oscillations at syllable frequency, resulting in highly noise robust incremental syllable boundary detection [47]. It is built around an interconnected network composed of 10 excitatory and 10 inhibitory leaky integrate-and-fire neurons. The spiking NN declares a putative syllable boundary for each inhibitory spike burst. We selected this approach to alleviate segmental error propagation to the suprasegmental information coding, and also keep an end-to-end neural network design for the VLBR system.\nIn the original proposal of syllable-based F0 parametrization for speech coding [48], unvoiced syllables were not parametrized (and not transmitted), and the pitch coding operated at a very low 40–60 bps. To achieve similar transmission rates with continuous F0 coding, we further linearly quantized the DLOP parameters. Thus, 3-bit quantized second order DLOP (linear) F0 signal stylization is used for coding of the original F0 signal."
    }, {
      "heading" : "D. Transmission scheme",
      "text" : "Segmental features — phonological posteriors — have values mostly concentrated very close to either 1 or 0, and these\nbinary patterns allow very efficient use of 1-bit quantization: the probabilities above 0.5 are normalized to 1 and the probabilities less than 0.5 are forced to zero.\nFigure 1 illustrates a demonstration sample of the transmission scheme. Figure 1a shows the speech signal. Figure 1b shows binary values of the three basic resonance phonological primes of the GP system commonly labelled as A, U, I, denoting the peripheral vowel qualities [a], [u] and [i] respectively. Other vowels are defined by a composition of the basic ones; for example, [e] results from fusing the I and A primes. In addition to these ‘vocalic’ primes, GP also proposes the “consonantal” primes that are omitted in the picture for simplicity.\nFigure 1c shows an original continuous log F0 signal, and linear curve fitting “DLOP2” using the syllable boundaries from the SNN. Each syllable is parametrized by 2 floating point numbers, that are further quantized using linear 3-bit codebooks, drawn as “DLOP2q3”. 6 bits are thus needed to parametrize the first DLOP parameter (the F0 mean) and the second DLOP parameter (the F0 slope) of the transmitted syllable-based prosodic code."
    }, {
      "heading" : "III. OPEN-SOURCE EXPERIMENTAL FRAMEWORK",
      "text" : ""
    }, {
      "heading" : "A. Composition of neural networks",
      "text" : "Figure 2 shows the design of the NN codec. The encoder shown in Figure 2a is based on a bank of DNNs performing segmental speech analysis of conventional acoustic features,\n4 and a parallel spiking NN detecting syllable boundaries of the continuous F0 signal. The decoder shown in Figure 2b is based on a DNN performing synthesis of speech cepstral parameters from transmitted segmental and prosodic information.\nThe outputs of segmental speech analysis are phonological posteriors where all unique patterns of the training data create the segmental codebook. The number of unique binary patterns, the size of the segmental codebook, is a small fraction of the whole permissible patterns (for example, for the eSPE phonological system, it is about 0.5%) The binary patterns are often repeated frame by frame. The segmental code thus consists of an index of the codebook, along with the duration of the code. The output of syllable analysis spiking NN is stylized using 3-bit quantization of second order DLOP parameters. All stylized F0 mean and F0 slope values create the prosodic codebooks, and the prosodic code consists of the two indexes of the F0 mean and slope codebooks, along with the duration of the transmitted syllable.\nThe prosodic code is extracted by spiking NN and DLOP parametrization independently of the segmental code. That means that the prosodic code is encoded asynchronously to the segmental one (both codes might have different start, end, and duration). Both segmental and prosodic codes are transmitted in parallel.\nTo confirm the feasibility of the proposed NN speech coder, we created an experimental framework. The BSD-licensed open-source platform is based on:\n• phonological vocoding1 performing speech analysis and synthesis using phonological posteriors, • LPC vocoding of the Speech Signal Processing (SSP) toolkit2, and • syllable onset detection and DLOP-based parametriza-\n1https://github.com/idiap/phonvoc 2https://github.com/idiap/ssp\ntion3."
    }, {
      "heading" : "B. Data",
      "text" : "The DNN encoder is trained on the si tr s 284 set of the Wall Street Journal (WSJ0 and WSJ1) continuous speech recognition corpora [49], and the SNN is trained on a subset of the TIMIT corpus [50] (the 10 sentences for speakers indexed 1–100).\nThe DNN decoder is trained on the Nancy database provided by the Blizzard Challenge4. The speaker is known as “Nancy”, and she is a US English native female speaker. The database consisted of 16.6 hours of high quality recordings of natural expressive human speech recorded in an anechoic chamber at a 96 kHz sampling rate during 2007 and 2008. The database comprised of around 12k utterances, and the following split was used: • the training set, utterances from 1 to 10k, • the cross-validation set, utterances from 10k to 11k, • the test set, the remaining 1095 utterances.\nThe text was processed by a conventional and freely available TTS front-end [51], and the resulting phonetic labels were used for training of the synthesis DNN.\nThe same data is also used to train a baseline HMM-based VLBR speech coding system."
    }, {
      "heading" : "C. Training",
      "text" : "1) Baseline HMM-based system: We trained the baseline HMM-based system as described in [7]. For HMM analysis models, we trained three-state, cross-word triphone models with the HTS variant [52] of the HTK toolkit on the WSJ training set. We tied triphone models with decision tree state clustering based on the minimum description length (MDL) criterion. The MDL criterion allows an unsupervised determination of the number of states. In this study, we obtained 12,685 states each modelled with a GMM consisting of 16 Gaussians. We used mel-frequency cepstral coefficients as acoustic features. The phoneme set comprising of 40 phonemes (including “sil”, representing silence) was defined by the CMU pronunciation dictionary.\nFor building the HMM synthesis models, the implementation of training from the EMIME project [53] was used. Fivestate, left-to-right, no-skip HSMMs were used. The speech parameters which were used for training the HSMMs were 39- order cepstral coefficients, log-F0 and 21-band aperiodicities, along with their delta and delta-delta features, framed by 25-ms windows, extracted every 5 ms. Cepstral instead of mel-cepstral features were used, as re-synthesis without melwarping was almost two times faster.\n2) Phonological analysis DNNs: First, we aligned the WSJ training data usinf HMM acoustic models trained for the baseline system. Then, considering the three different phonological systems, GP, SPE and eSPE, the three different banks of DNNs were trained on the 90% subset of the training set, and the remaining 10% subset was used for cross-validation.\n3https://github.com/mcernak/parsyll 4http://www.cstr.ed.ac.uk/projects/blizzard/2011/lessac blizzard2011\n5 For all systems, the labels of phonemes were mapped to the respective phonological classes. In total, K DNNs (12 for GP, 15 for SPE and 21 for eSPE) were trained as phonological analyzers using the short segment (frame) alignment, with two output labels indicating whether the k-th phonological class exists for the aligned phoneme or not. The architecture of the DNNs was 351×1024×1024×1024×2 neurons, determined empirically. The input vectors were 39-order MFCC features with a temporal context of 9 successive frames.\nThe training was initialized using deep belief network pretraining done by the single-step contrastive divergence (CD-1) procedure of [54]. The DNNs with the softmax output function were then trained using a mini-batch based stochastic gradient descent algorithm with the cross-entropy cost function of the Kaldi toolkit [55]. Tables I, II and III list the phonological classes and detection accuracy for GP, SPE and eSPE respectively. The DNNs outputs for individual phonological classes determine the phonological posterior probabilities.\n3) Prosodic analysis SNN: The prosodic analysis SNN is based on an interconnected network composed of 10 excitatory and 10 inhibitory leaky integrate-and-fire neurons. Its principles are based on findings on the role of slow neural oscillations in the auditory cortex for natural speech parsing [56].\nThe 13-dim cepstral input vectors are first transformed to unidimensional time series. We use weighting to allow for some channels/frequencies to have higher importance, e.g. frequencies around formants likely to provide more information about syllable boundaries because of the vocalisation process. Syllable boundaries are characterised by local minima of the weighted signal, that can be generalised to a convolution of the temporal kernel and the weighted signal [47].\nTraining of the parameters of the spiking NN is based on minimising the syllabic distance of actual syllable boundaries and those produced by the convolution, over the 1000 sentences of the training set. The syllabification program tsylb2 [57] was used to convert phonetician-labelled phonemes and phoneme boundaries into syllables and syllable boundaries.\nThe output of the spiking NN is stylized using 3-bit quantization of 2nd order DLOP parameters. To create the codebooks, the logarithm of the continuous pitch of the all syllables of the Nancy training data was parametrized by the DLOP parameters. The values were linearly spaced between the µ−3σ and µ+3σ boundaries, where µ is the mean and σ is the standard deviation of all the measurements of the DLOP parameters. One codebook was thus created for the 1st order DLOP parameter, and one for the 2nd one.\n4) Phonological synthesis DNNs: The speech signals from the training and cross-validation sets of the Nancy database, down-sampled to 16 kHz, framed by 25 ms windows with the three different frame shifts: 10, 16 and 20 ms, were used for extracting both DNN input and output features. The input features, phonological posteriors ~zn, were generated by the phonological analysers. The temporal context of 11 successive frames resulted in input features of 12× 11× 1 = 132, 15× 11 × 1 = 165 and 21 × 11 × 1 = 231 dimensions, for the GP, SPE and eSPE schemes, respectively. The output features, the LPC speech parameters, were extracted by the SSP: ~pn - Line Spectral Pairs (LSPs) of 24th order plus gain, log(~rn) - a Harmonic-To-Noise (HNR) ratio, and ~tn, log(~mn) - two glottal model parameters [58], angle t and magnitude log(m) of a glottal pole, respectively. Thus, we used static speech parametrization of 28th order along with its dynamic features, altogether of 84th order.\nCepstral mean normalisation of the output features was applied before DNN training. Altogether, 6 DNNs were trained for 3 different phonological schemes and 3 time shifts. The DNN were initialised using (K ∗ 11)× 1024× 1024× 1024× 1024×84 pre-training and DNNs with a linear output function were then trained by Kaldi with mean square error cost function."
    }, {
      "heading" : "IV. EVALUATION",
      "text" : "In this section, we present an evaluation of the VLBR phonetic and phonological NN speech coder, and a comparison\n6 to the baseline HMM-based system. We encoded and decoded 1095 utterances of the Nancy database test set. In following sections, we present results focusing on:\n1) Phonetic NN speech coder, comparing the phonetic and phonological coding in Section IV-A. 2) Phonological NN speech coder, quantifying an impact of different phonological schemes and frame shifts on speech quality and transmission rates in Section IV-B. 3) HMM vesus NN speech coders, comparison of VLBR HMM-based and NN speech coding in Section IV-C."
    }, {
      "heading" : "A. Phonetic NN speech coder",
      "text" : "Segmental information of the NN speech coder may consist of either phonetic or phonological posteriors (cf. Section II-B), transmitted frame by frame. Therefore, we started evaluation by comparison of speech quality of the phonetic and phonological speech coding. To measure the impact of the phonetic and phonological posteriors only, the F0 encoding was bypassed with the original F0 signals. To achieve VLBR, binary posteriors have to be used, hence we were interested which of binary phonetic or phonological posteriors perform better.\nWe normalised the phonetic and phonological posteriors to the binary values (the probabilities above 0.5 are normalized to 1 and the probabilities less than 0.5 are forced to zero) and used Mel Cepstral Distortion (MCD) [59] between original and encoded speech samples as an objective metric to compare overall speech quality. Lower MCD values indicate higher speech quality of the encoded speech samples. The segmental information in this experiment consists of 10 ms framed vectors of either phone posterior probabilities or eSPE phonological-class posterior probabilities.\nTable IV reports the results. All results are statistically significant (p < 0.01 of t-test). The first row shows speech distortion caused by the LPC vocoding. The second and third rows report the distortions of phonetic and phonological vocoding done on the top of the LPC vocoding. We can conclude that the majority of the distortion comes from the parametric vocoder; the distortion of phonetic/phonological vocoding with continuous features is about 1.4 dB. While the performance of continuous phonetic and phonological posteriors is similar, feature normalization has higher negative impact on the binary phonetic posteriors. The reason why binary phonological posteriors outperform the phonetic ones could be in the parallel nature of phonological vocoding. Also, maximum a posteriori classification of phonological posteriors is far more accurate than phonetic posteriors. Therefore, we conclude that phonological posteriors are more suitable for the NN coder, and we selected the binary phonological features in further evaluation."
    }, {
      "heading" : "B. Phonological NN speech coder",
      "text" : "We used the binary phonological posterior features, and created a codebook from the unique patterns for each phonological scheme. Linear 3-bit quantized syllable-based F0 parametrization is used in this experiment. The 3-bit quantization degrades speech quality by only about 0.1 dB.\nLower bit-rates can be achieved by using a lower dimensional phonological scheme. Figure 3 shows a linear dependence of the codebook size on the number of phonological classes. Increasing the frame shift slightly decreases the number of unique patterns. We speculate that the number of unique binary phonological posteriors is related to the number of clustered senones (tied context-dependent HMM states) used in ASR and TTS acoustic modelling. Then we could interpret a vector of phonological posterior as a senone.\nRecall that we consider three different phonological systems in this work. Each system defines different set of features, with the dimensions 12, 15 and 21, for the GP, SPE and eSPE phonological systems respectively. We discuss their meaning and exact definition in [9].\nFigure 4 shows the objective evaluation of the NN speech coding. The transmission rate depends on two variables: the phonological scheme that results in smaller or larger codebooks, and the frame shift. The eSPE codebook consists of more than 10k unique binary phonological patterns; 14 bits are thus required to transmit the segmental code. Fewer bits are required for the SPE and GP codebook, 12 and 10 bits, respectively. We can see from the figure that the difference in quality degradation lies in the range of about 0.2 dB, therefore we identified the GP system as the most suitable for the VLBR system. The frame shift has bigger impact on the bit rate; for the GP system, increasing the frame shift from 10ms to 16ms, the degradation increases by 0.3 dB.\nThe overall quality of the NN speech coding was evaluated subjectively using the Degradation Category Rating (DCR) procedure [60] quantifying the Degradation Mean Opinion Score (DMOS). The aim was to estimate speech encoding quality variations based on the different frame-shift. The test consisted of 20 randomly selected utterances from the Nancy test data, at least 2 seconds long. 37 listeners were asked to rate the degradation of re-synthesised signals, compared with\n7 350 400 450 500 550 600\nEstimated transmission rate [bps]\n6.6\n6.8\n7\n7.2\n7.4 7.6 M e l c e p s tr a l D is to rt io n [ d B ] GP based SPE based eSPE based 20ms frame shift\n10ms frame shift\n16ms frame shift\nFig. 4. Estimated transmission rates of the speech coding.\nreference signals, based on their overall perception. Figure 4 shows the result that NN speech coding operating at 360–370 bps achieves above 2.3 MOS. The figure also shows higher uncertainty of listeners (higher standard deviation of MOS) when increasing the frame shift of the DNN coder. Although similar DMOS is achieved with both the 16 ms and 20 ms frame shifts, speech coding with the 20 ms frame shift has much higher standard deviation of subjective testing. Thus, we selected the GP-based 16 ms frame shift system as an optimal VLBR NN speech coder for a qualitative comparison with the HMM coder.\nHMM-based VLBR system operating in an asynchronous mode achieves 2.3 MOS as well [7]."
    }, {
      "heading" : "C. HMM versus NN coding",
      "text" : "Finally, we were interested in qualitative comparison of the VLBR HMM and NN speech coders. We employed a 5-point scale ABX subjective evaluation listening test [61], suitable for comparing two different systems. In this test, listeners were\npresented with pairs of samples produced by two systems (A and B) and for each pair they indicated their preference or strong preference for A, B, or both samples sound the same (X). The material for the test consisted of 20 pairs of sentences such that one member of the pair was generated using the GPbased 16 ms frame shift NN speech coder (system A) and the other member was generated using the HMM-based speech coder (system B). Random utterances from the test set of the Nancy database were used to generate the encoded speech. We chose this GP system as a compromise between the speech quality and the low bit rate. The subjects for the ABX test were 32 native English listeners, roughly equally pooled from experts in speech processing on the one hand, and completely naive subjects on the other hand. The subjects were presented with pairs of sentences in a random order with no indication of which system they represented. They were asked to listen to these pairs of sentences (as many times as they wanted), and choose between them in terms of their overall quality. Additionally, the option X, i.e. both samples sound the same, was available if they had no preference for either of them.\nAs can be seen in Figure 6, the NN speech coder significantly outperforms the HMM-based one. The strong preference and preference choices of NN coder achieve 30.62% and 54.22% (sum up to 84.84%) over 0.63% and 4.53% (sum up to 5.16%) respectively for the HMM-based one. In addition the “no preference” choice achieved a 10%.\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\n5.16 10.00 84.84\nHMM coder Sound the same NN coder\nFig. 6. ABX subjective evaluation test of HMM and DNN/SNN coders.\nThe HMM coder creates bursts of segmental errors when phoneme ASR fails. It also uses voiced/unvoiced detection for parametrization of the F0 signal only in voiced regions. This prosodic encoding plugged-in to the VLBR system creates additional speech discontinuities and unnatural speech artefacts. On the other hand, the NN speech coder transmits information per frame, and parameterizes continuous F0, that altogether significantly reduces discontinuities during speech reconstruction at the receiver."
    }, {
      "heading" : "V. BIT ALLOCATION",
      "text" : "The test set contained about 36k syllables in 5240 seconds of speech including silences. On average, there were 6.8 syllables/sec. The transmission code included the index of the binary pattern along with its duration, the segmental code, and two indexes of quantized mean and slope of syllablebased F0 codebooks along with the syllable duration, and the supra-segmental code. Both segmental and suprasegmental information is transmitted asynchronously, i.e., the segmental blocks and syllables have different start, end and duration.\nAs an example of the bit allocation, Table V shows the details for the GP system used in the ABX test. Closer analysis of repeated patterns of the segmental code reveals that the number of blocks is less than 46% of the total number of frames and 2 bits is sufficient to transmit the number of\n8\nrepeated codes. That amounts to 0.46 × 56 × 10 = 257 bps transmission rate for the GP codes, and 0.46 × 56 × 2 = 52 bps transmission rate for the segmental code duration. Because around 10% of the transmitted speech is detected as silence (valid for our training data), we obtain the effective speech frame-rate for the 16 ms frame shift as 62.5× 0.1 = 56 bps. The transmission rate of supra-segmental code is constant for all tested NN-based speech coding systems, as the number of syllables is always constant. It consists of an index of the F0 mean codebook (3 × 6 = 18 bps), and index of the F0 slope codebook (3× 6 = 18 bps), and the syllable duration (4 bps). Effective syllable rate (leading and trailing syllables removed) of our data was 6 syllables per second. Duration of syllables is encoded by 4 bits (covering duration up to 256 ms) that results in encoding supra-segmental information at a constant 60 bps. Altogether, the estimated bit rate for the NN-based speech coding using the GP phonological scheme and the 16 ms frame shift is 369 bps.\nThe average syllable duration, including leading, trailing and short pause silences, was 150 ms. As both speech encoding and decoding processing (forward passes of the NNs) were faster than real-time, we consider the average syllable duration as an algorithmic latency of 150 ms of the proposed coder. According to the G.114, the users are “very satisfied” as long as latency does not exceed 200 ms [62]."
    }, {
      "heading" : "VI. CONCLUSIONS",
      "text" : "VLBR speech coding based on a recognition/synthesis paradigm is either corpus-based (using unit-selection approach), or HMM-based (using HMM-based ASR or TTS). We have designed and presented a NN-based speech coding composed of deep NNs and spiking NN; the solution represents an end-to-end neural network based VLBR speech coding.\nWe have compared phonetic and phonological NN coding; given the binary nature of the phonological posteriors, they outperform the binary phonetic posteriors. Further, we have compared three different phonological systems, and we conclude that an optimal NN speech coder can be designed by using the phonological posteriors defined by the Government Phonology classes. By selecting a frame shift of 16 ms, the NN coder operates on 369 bps with a latency of 150 ms.\nListener preference evaluation of HMM and NN-based speech coders showed that NN speech coder with continuous F0 modelling is significantly preferred by (85% of) the listeners. Speech quality evaluation of the VLBR speech coding has showed that both HMM and NN-based speech coders achieve\nsimilar (2.3 MOS) speech quality. As we have used an opensource experimental framework with a rather standard LPC vocoder, there is potential for higher speech quality of NN speech coding in future by improved parametric vocoding. Table IV shows that more than 77% of all degradation comes from the parametric vocoding.\nThe design of the proposed coder is simplified just to the three kinds of neural networks. Our future work will be focused on investigations of computational complexity of the NN speech coding. Following recent research on complexity reduction of NNs (for example [63]–[65]), we believe that this coding approach becomes more feasible for a broad range of computation platforms that may be used in telecommunication networks."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "This work has been conducted with the support of the Swiss NSF under grant CRSII2 141903: Spoken Interaction with Interpretation in Switzerland (SIWIS), and under SP2: the SCOPES Project on Speech Prosody. The wor was also partly supported under the RECOD project by armasuisse, the Procurement and Technology Center of the Swiss Federal Department of Defence, Civil Protection and Sport.\nAfsaneh Asaei has been supported by SNSF project on “Parsimonious Hierarchical Automatic Speech Recognition (PHASER)” grant agreement number 200021-153507."
    } ],
    "references" : [ {
      "title" : "A very low bit rate speech coder based on a recognition/synthesis paradigm,",
      "author" : [ "K.-S. Lee", "R. Cox" ],
      "venue" : "IEEE Trans. on Audio, Speech, and Language Processing,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Corpus based very low bit rate speech coding,",
      "author" : [ "G.V. Baudoin", "F. El Chami" ],
      "venue" : "in Proc. of ICASSP,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "A phonetic vocoder,",
      "author" : [ "J. Picone", "G.R. Doddington" ],
      "venue" : "Proc. of ICASSP",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1989
    }, {
      "title" : "A very low bit rate speech coder using HMM-based speech recognition/synthesis techniques,",
      "author" : [ "K. Tokuda", "T. Masuko", "J. Hiroi", "T. Kobayashi", "T. Kitamura" ],
      "venue" : "in Proc. of ICASSP,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Multisensor very lowbit rate speech coding using segment quantization,",
      "author" : [ "A. McCree", "K. Brady", "T.F. Quatieri" ],
      "venue" : "in Proc. of ICASSP",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "Linguistic dissection of switchboardcorpus automatic speech recognition systems,",
      "author" : [ "G. Greenberg", "S. Chang" ],
      "venue" : "Proc. of ITRW on Automatic Speech Recognition: Challenges for the new Millenium, Paris, France,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Speech vocoding for laboratory phonology,",
      "author" : [ "M. Cernak", "S. Benus", "A. Lazaridis" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "The elements of phonological representation",
      "author" : [ "J. Harris", "G. Lindsey" ],
      "venue" : "Harlow, Essex: Longman,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1995
    }, {
      "title" : "Boosting attribute and phone estimation accuracies with deep neural networks for detectionbased speech recognition,",
      "author" : [ "D. Yu", "S. Siniscalchi", "L. Deng", "C.-H. Lee" ],
      "venue" : "in Proc. of ICASSP. IEEE SPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Experiments on Cross-Language Attribute Detection and Phone Recognition With Minimal Target-Specific Training Data,",
      "author" : [ "S.M. Siniscalchi", "D.-C. Lyu", "T. Svendsen", "C.-H. Lee" ],
      "venue" : "IEEE Trans. on Audio, Speech, and Language Processing, vol. 20,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "On Compressibility of Neural Network Phonological Features for Low Bit Rate Speech Coding,",
      "author" : [ "A. Asaei", "M. Cernak", "H. Bourlard" ],
      "venue" : "in Proc. of Interspeech,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Learning deep architectures for AI,",
      "author" : [ "Y. Bengio" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Connectionist Speech Recognition: A Hybrid Approach",
      "author" : [ "H. Bourlard", "N. Morgan" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1994
    }, {
      "title" : "Deep belief networks for phone recognition,",
      "author" : [ "A. Mohamed", "G.E. Dahl", "G.E. Hinton" ],
      "venue" : "in NIPS’22 workshop on deep learning for speech recognition,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,",
      "author" : [ "G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury" ],
      "venue" : "Signal Processing Magazine, IEEE,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Context-dependent pre-trained deep neural networks for large vocabulary speech recognition,",
      "author" : [ "G. Dahl", "D. Yu", "L. Deng", "A. Acero" ],
      "venue" : "IEEE Transactions on Audio, Speech, and Language Processing (receiving 2013 IEEE SPS Best Paper Award),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Statistical parametric speech synthesis using Deep Neural Networks,",
      "author" : [ "H. Zen", "A. Senior", "M. Schuster" ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Anytime learning of decision trees,",
      "author" : [ "S. Esmeir", "S. Markovitch", "C. Sammut" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "On the training aspects of deep neural network (DNN) for parametric tts synthesis,",
      "author" : [ "Y. Qian", "Y. Fan", "W. Hu", "F. Soong" ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "DNN-based speech synthesis: Importance of input features and training data,",
      "author" : [ "A. Lazaridis", "B. Potard", "P.N. Garner" ],
      "venue" : "in International Conference on Speech and Computer, SPECOM 2015, ser. Lecture Notes in Computer Science,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "A new model of LPC excitation for producing natural-sounding speech at low bit rates,",
      "author" : [ "B.S. Atal", "J.R. Remde" ],
      "venue" : "in Proc. of ICASSP,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1982
    }, {
      "title" : "Code-excited linear prediction(CELP): High-quality speech at very low bit rates,",
      "author" : [ "M. Schroeder", "B. Atal" ],
      "venue" : "in Proc. of ICASSP,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1985
    }, {
      "title" : "Speech coding based on a multi-layer neural network,",
      "author" : [ "S. Morishima", "H. Harashima", "Y. Katayama" ],
      "venue" : "in Communications,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1990
    }, {
      "title" : "Prediction in speech coding: the modification of the coding of LPC parameters and nonlinear estimation technique by using ANN,",
      "author" : [ "Y. Zhen" ],
      "venue" : "in Signal Processing,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1996
    }, {
      "title" : "A nonlinear adaptive predictor for speech compression,",
      "author" : [ "S. Hunt" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1996
    }, {
      "title" : "Discriminative coding with predictive neural networks,",
      "author" : [ "C. Chavy", "B. Gas", "J.L. Zarader" ],
      "venue" : "Artificial Neural Networks,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1999
    }, {
      "title" : "Engineering Applications of Bio-Inspired Artificial Neural Networks: International Work-Conference on Artificial and Natural Neural Networks, IWANN’99",
      "author" : [ "M. Faúndez-Zanuy" ],
      "venue" : "Proceedings, Volume II. Berlin,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1999
    }, {
      "title" : "Packet Loss Concealment Based on Deep Neural Networks for Digital Speech Transmission,",
      "author" : [ "B.-K. Lee", "J.-H. Chang" ],
      "venue" : "IEEE/ACM Trans. on Audio, Speech, and Language Processing,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2016
    }, {
      "title" : "Garoucy, “Complexity Reduction of LD-CELP Speech Coding in Prediction of Gain Using Neural Networks,",
      "author" : [ "M. Sheikhan", "V.T. Vakili" ],
      "venue" : "World Applied Sciences Journal,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2009
    }, {
      "title" : "A CELP codebook and search technique using a Hopfield net,",
      "author" : [ "M.G. Easton", "C.C. Goodyear" ],
      "venue" : "in Proc. of ICASSP. IEEE,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1991
    }, {
      "title" : "Fully vector-quantized neural network-based code-excited nonlinear predictive speech coding,",
      "author" : [ "L. Wu", "M. Niranjan", "F. Fallside" ],
      "venue" : "IEEE Trans. on Audio, Speech, and Language Processing,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1994
    }, {
      "title" : "Multiband excitation vocoder,",
      "author" : [ "D.W. Griffin", "J.S. Lim" ],
      "venue" : "IEEE Trans. on Audio, Speech, and Language Processing,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1988
    }, {
      "title" : "A robust 800 bps MBE coder with VQ and MLP,",
      "author" : [ "H. Cui", "H. Jiang" ],
      "venue" : "Communication Technology Proceedings,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1998
    }, {
      "title" : "Phonological vocoding using artificial neural networks,",
      "author" : [ "M. Cernak", "B. Potard", "P.N. Garner" ],
      "venue" : "in Proc. of ICASSP",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2015
    }, {
      "title" : "Demodulation as Probabilistic Inference,",
      "author" : [ "R.E. Turner", "M. Sahani" ],
      "venue" : "IEEE Trans. on Audio, Speech, and Language Processing,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2011
    }, {
      "title" : "A role for amplitude modulation phase relationships in speech rhythm perception.",
      "author" : [ "V. Leong", "M.A. Stone", "R.E. Turner", "U. Goswami" ],
      "venue" : "J. Acoust. Soc. Am.,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2014
    }, {
      "title" : "An oscillatory hierarchy controlling neuronal excitability and stimulus processing in the auditory cortex.",
      "author" : [ "P. Lakatos", "A.S. Shah", "K.H. Knuth", "I. Ulbert", "G. Karmos", "C.E. Schroeder" ],
      "venue" : "Journal of neurophysiology,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2005
    }, {
      "title" : "From Discontinuous To Continuous F0 Modelling In HMM-based Speech Synthesis,",
      "author" : [ "K. Yu", "B. Thomson", "S. Young", "T. Street" ],
      "venue" : "in Proc. ISCA SSW7. Kyoto, Japan: ISCA,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2010
    }, {
      "title" : "Continuous F0 Modeling for HMM Based Statistical Parametric Speech Synthesis,",
      "author" : [ "K. Yu", "S. Young" ],
      "venue" : "IEEE Trans. on Audio, Speech, and Language Processing,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2011
    }, {
      "title" : "Using Noisy Speech to Study the Robustness of a Continuous F0 Modelling Method in HMM-based Speech Synthesis,",
      "author" : [ "K.U. Ogbureke", "J.P. Cabral", "J. Carson-Berndsen" ],
      "venue" : "in Proc. Speech Prosody, Shanghai,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2012
    }, {
      "title" : "Neuromorphic Based Oscillatory Device for Incremental Syllable Boundary Detection,",
      "author" : [ "A. Hyafil", "M. Cernak" ],
      "venue" : "in Proc. of Interspeech,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2015
    }, {
      "title" : "Syllable-Based Pitch Encoding for Low Bit Rate Speech Coding with Recognition/Synthesis Architecture,",
      "author" : [ "M. Cernak", "X. Na", "P.N. Garner" ],
      "venue" : "in Proc. of Interspeech,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2013
    }, {
      "title" : "The design for the wall street journalbased CSR corpus,",
      "author" : [ "D.B. Paul", "J.M. Baker" ],
      "venue" : "Proceedings of the workshop on Speech and Natural Language, ser. HLT ’91. Stroudsburg, PA, USA: Association for Computational Linguistics,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1992
    }, {
      "title" : "TIMIT Acoustic-Phonetic Continuous Speech Corpus,",
      "author" : [ "L.D. Consortium" ],
      "venue" : null,
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1993
    }, {
      "title" : "The Festival Speech Synthesis System,",
      "author" : [ "A. Black", "P. Taylor", "R. Caley" ],
      "venue" : "Human Communication Research Centre,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1997
    }, {
      "title" : "The HMM-based Speech Synthesis System Version 2.0,",
      "author" : [ "H. Zen", "T. Nose", "J. Yamagishi", "S. Sako", "T. Masuko", "A. Black", "K. Tokuda" ],
      "venue" : "in Proc. of ISCA SSW6,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2007
    }, {
      "title" : "Speaker adaptation and the evaluation of speaker similarity in the EMIME speech-to-speech translation project,",
      "author" : [ "M. Wester", "J. Dines", "M. Gibson", "H. Liang", "Y.-J. Wu", "L. Saheer", "S. King", "K. Oura", "P.N. Garner", "W. Byrne", "Y. Guan", "T. Hirsimäki", "R. Karhila", "M. Kurimo", "M. Shannon", "S. Shiota", "J. Tian", "K. Tokuda", "J. Yamagishi" ],
      "venue" : null,
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2010
    }, {
      "title" : "A Fast Learning Algorithm for Deep Belief Nets,",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.W. Teh" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2006
    }, {
      "title" : "The kaldi speech recognition toolkit,",
      "author" : [ "D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely" ],
      "venue" : "in Proc. of ASRU. IEEE SPS,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2011
    }, {
      "title" : "Speech encoding by coupled cortical theta and gamma oscillations,",
      "author" : [ "A. Hyafil", "L. Fontolan", "C. Kabdebon", "B. Gutkin", "A.-L. Giraud", "H. Brownell" ],
      "venue" : "eLife,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2015
    }, {
      "title" : "A simple continuous excitation model for parametric vocoding,",
      "author" : [ "P.N. Garner", "M. Cernak", "B. Potard" ],
      "venue" : "in Proc. of ICASSP",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2015
    }, {
      "title" : "Mel-cepstral distance measure for objective speech quality assessment,",
      "author" : [ "R.F. Kubichek" ],
      "venue" : "Proc. of ICASSP,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 1993
    }, {
      "title" : "Speech Quality Assessment,",
      "author" : [ "V. Grancharov", "W. . B. Kleijn" ],
      "venue" : "Handbook of Speech Processing,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2008
    }, {
      "title" : "Distilling the Knowledge in a Neural Network,",
      "author" : [ "G. Hinton", "O. Vinyals", "J. Dean" ],
      "venue" : null,
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2015
    }, {
      "title" : "Compressing Deep Neural Networks using a Rank-Constrained Topology,",
      "author" : [ "P. Nakkiran", "R. Alvarez", "R. Prabhavalkar", "C. Parada" ],
      "venue" : "in Proc. of Interspeech,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2015
    }, {
      "title" : "Structured Transforms for Small-Footprint Deep Learning,",
      "author" : [ "V. Sindhwani", "T.N. Sainath", "S. Kumar" ],
      "venue" : null,
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", [2], [3], and hidden Markov model (HMM) based, e.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 1,
      "context" : ", [2], [3], and hidden Markov model (HMM) based, e.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 2,
      "context" : ", [4]–[6].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 4,
      "context" : ", [4]–[6].",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "This is well known; phoneme ASR misrecognition rates tend to increase with longer words [8].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "defined in Appendix A of [9]: (i) the Government Phonology",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "(GP) [10], [11], (ii) the Sound Pattern of English (SPE) [12] and (iii) the extended SPE system (eSPE) [13], [14].",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : "(GP) [10], [11], (ii) the Sound Pattern of English (SPE) [12] and (iii) the extended SPE system (eSPE) [13], [14].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "(GP) [10], [11], (ii) the Sound Pattern of English (SPE) [12] and (iii) the extended SPE system (eSPE) [13], [14].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : "There are only very few phonological classes comprising a short term speech signal; hence, the phonological posterior is a sparse vector, and allow very few combinations that can be stored in a codebook for VLBR speech coding [15].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 11,
      "context" : "Recently, in the speech recognition/analysis field, a shift has been observed from HMM-based approaches towards the use of deep neural networks (DNNs) [16].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : "Even though the concept of using neural networks in recognition is not new [17], the increase of available speech resources and of computational power, and the use of graphics processing units, have led to a major research interest in DNNs.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "Additionally, deep architectures with multiple layers can overcome the limitations in the representational capability of HMMs, which are incapable of modelling multiple interacting source streams [18].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 14,
      "context" : "Furthermore, DNNs are able to create non-linear mappings between the input and output features which cannot be achieved by using Gaussian mixture models (GMMs) in HMM-based approaches, making them more appropriate for modelling the speech signal [19].",
      "startOffset" : 246,
      "endOffset" : 250
    }, {
      "referenceID" : 14,
      "context" : "As a result, DNN-based approaches have managed to show superior performance in comparison to HMM-based ones in recognition tasks [19], [20].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 15,
      "context" : "As a result, DNN-based approaches have managed to show superior performance in comparison to HMM-based ones in recognition tasks [19], [20].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 16,
      "context" : "occur in HMM-based TTS [21], e.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : ", inefficiency to express complex dependencies in the feature space [22], which leads to decision trees becoming exceedingly large, hence inefficient and data hungry.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : "led to the introduction of DNNs in the field of parametric speech synthesis as well, constantly outperforming HMMbased speech synthesis systems [21], [24], [25].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 18,
      "context" : "led to the introduction of DNNs in the field of parametric speech synthesis as well, constantly outperforming HMMbased speech synthesis systems [21], [24], [25].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 19,
      "context" : "led to the introduction of DNNs in the field of parametric speech synthesis as well, constantly outperforming HMMbased speech synthesis systems [21], [24], [25].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "approximating coders, a family of coders originated in [26], [27], to address either improving quality or reducing computational complexity.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "approximating coders, a family of coders originated in [26], [27], to address either improving quality or reducing computational complexity.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "The former usage aims to improve linear prediction (of speech samples or parameters of the excitation signal) with a non-linear prediction based usually on multilayer perceptrons [28]–[32].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 26,
      "context" : "The former usage aims to improve linear prediction (of speech samples or parameters of the excitation signal) with a non-linear prediction based usually on multilayer perceptrons [28]–[32].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 27,
      "context" : "Recently, regression-based packet loss concealment was proposed using DNNs [33].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "The latter usage aims to reduce the complexity of the codebook search process or gain prediction [34] using, for example, recurrent NNs [35], [36].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 29,
      "context" : "The latter usage aims to reduce the complexity of the codebook search process or gain prediction [34] using, for example, recurrent NNs [35], [36].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 30,
      "context" : "The latter usage aims to reduce the complexity of the codebook search process or gain prediction [34] using, for example, recurrent NNs [35], [36].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 31,
      "context" : "Speech coding based on speech modelling is known as parametric coding, where the parameters of the speech models are transmitted, such as in a multiband excitation vocoder [37].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 32,
      "context" : "Similarly, as in waveform coding, a multilayer perceptron was proposed to decrease the computation complexity of the codebook of line spectral frequencies in the 800 bps multiband excitation speech coding [38].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 33,
      "context" : "sequence Z to the speech parameters [39].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 34,
      "context" : "For example, Probabilistic Amplitude Demodulation method proposed in [40] may robustly estimate the syllable and stress amplitude modulations as a representation of electrophysiological recordings of auditory cortex.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 35,
      "context" : "The work of [41] proves that phase relations of the amplitude modulations, known as hierarchical phase locking and nesting or synchronization across different temporal granularity [42], is a good indication of the syllable stress.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 36,
      "context" : "The work of [41] proves that phase relations of the amplitude modulations, known as hierarchical phase locking and nesting or synchronization across different temporal granularity [42], is a good indication of the syllable stress.",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 37,
      "context" : "Modelling continuous F0 has been shown to be more effective in achieving natural synthesised speech [44], [45], and can be effectively used with noisy speech [46].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 38,
      "context" : "Modelling continuous F0 has been shown to be more effective in achieving natural synthesised speech [44], [45], and can be effectively used with noisy speech [46].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 39,
      "context" : "Modelling continuous F0 has been shown to be more effective in achieving natural synthesised speech [44], [45], and can be effectively used with noisy speech [46].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 40,
      "context" : "To estimate syllable boundaries from the speech signal, a neuromorphic oscillatory device is used based on modelling brain neural oscillations at syllable frequency, resulting in highly noise robust incremental syllable boundary detection [47].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 41,
      "context" : "In the original proposal of syllable-based F0 parametrization for speech coding [48], unvoiced syllables were not parametrized (and not transmitted), and the pitch coding oper-",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 42,
      "context" : "The DNN encoder is trained on the si tr s 284 set of the Wall Street Journal (WSJ0 and WSJ1) continuous speech recognition corpora [49], and the SNN is trained on a subset of the TIMIT corpus [50] (the 10 sentences for speakers indexed 1–100).",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 43,
      "context" : "The DNN encoder is trained on the si tr s 284 set of the Wall Street Journal (WSJ0 and WSJ1) continuous speech recognition corpora [49], and the SNN is trained on a subset of the TIMIT corpus [50] (the 10 sentences for speakers indexed 1–100).",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 44,
      "context" : "The text was processed by a conventional and freely available TTS front-end [51], and the resulting phonetic labels were used for training of the synthesis DNN.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 45,
      "context" : "For HMM analysis models, we trained three-state, cross-word triphone models with the HTS variant [52] of the HTK toolkit on the WSJ training set.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 46,
      "context" : "For building the HMM synthesis models, the implementation of training from the EMIME project [53] was used.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 47,
      "context" : "The training was initialized using deep belief network pretraining done by the single-step contrastive divergence (CD-1) procedure of [54].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 48,
      "context" : "The DNNs with the softmax output function were then trained using a mini-batch based stochastic gradient descent algorithm with the cross-entropy cost function of the Kaldi toolkit [55].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 49,
      "context" : "Its principles are based on findings on the role of slow neural oscillations in the auditory cortex for natural speech parsing [56].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 40,
      "context" : "Syllable boundaries are characterised by local minima of the weighted signal, that can be generalised to a convolution of the temporal kernel and the weighted signal [47].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 50,
      "context" : "The output features, the LPC speech parameters, were extracted by the SSP: ~ pn Line Spectral Pairs (LSPs) of 24th order plus gain, log(~rn) - a Harmonic-To-Noise (HNR) ratio, and ~tn, log(~ mn) - two glottal model parameters [58], angle t and magnitude log(m) of a glottal pole, respectively.",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 51,
      "context" : "5 are forced to zero) and used Mel Cepstral Distortion (MCD) [59] between original and encoded speech samples as an objective metric to compare overall speech quality.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "We discuss their meaning and exact definition in [9].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 52,
      "context" : "We employed a 5-point scale ABX subjective evaluation listening test [61], suitable for comparing two different systems.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 53,
      "context" : "Following recent research on complexity reduction of NNs (for example [63]–[65]), we believe that this coding approach becomes more feasible for a broad range of computation platforms that may be used in telecommunication networks.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 55,
      "context" : "Following recent research on complexity reduction of NNs (for example [63]–[65]), we believe that this coding approach becomes more feasible for a broad range of computation platforms that may be used in telecommunication networks.",
      "startOffset" : 75,
      "endOffset" : 79
    } ],
    "year" : 2017,
    "abstractText" : "Most current very low bit rate (VLBR) speech coding systems use hidden Markov model (HMM) based speech recognition/synthesis techniques. This allows transmission of information (such as phonemes) segment by segment that decreases the bit rate. However, the encoder based on a phoneme speech recognition may create bursts of segmental errors. Segmental errors are further propagated to optional suprasegmental (such as syllable) information coding. Together with the errors of voicing detection in pitch parametrization, HMM-based speech coding creates speech discontinuities and unnatural speech sound artefacts. In this paper, we propose a novel VLBR speech coding framework based on neural networks (NNs) for end-to-end speech analysis and synthesis without HMMs. The speech coding framework relies on phonological (sub-phonetic) representation of speech, and it is designed as a composition of deep and spiking NNs: a bank of phonological analysers at the transmitter, and a phonological synthesizer at the receiver, both realised as deep NNs, and a spiking NN as an incremental and robust encoder of syllable boundaries for coding of continuous fundamental frequency (F0). A combination of phonological features defines much more sound patterns than phonetic features defined by HMM-based speech coders, and the finer analysis/synthesis code contributes into smoother encoded speech. Listeners significantly prefer the NN-based approach due to fewer discontinuities and speech artefacts of the encoded speech. A single forward pass is required during the speech encoding and decoding. The proposed VLBR speech coding operates at bit rate about 360 bits/sec.",
    "creator" : "TeX"
  }
}