{
  "name" : "1307.3824.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Fundamental Learning Problem that Genetic Algorithms with Uniform Crossover Solve Efficiently and Repeatedly As Evolution Proceeds",
    "authors" : [ "Keki M. Burjorjee" ],
    "emails" : [ "kekib@cs.brandeis.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We recently hypothesized [2] that an efficient form of computational learning underlies generalpurpose, non-local, noise-tolerant optimization in genetic algorithms with uniform crossover (UGAs). The hypothesized computational efficiency, implicit concurrent multivariate effect evaluation— implicit concurrency for short—is broad and versatile, and carries significant implications for efficient large-scale, general-purpose global optimization in the presence of noise, and, in turn, large-scale machine learning. In this paper, we describe implicit concurrency and explain how it can power general-purpose, non-local, noise-tolerant optimization. We then establish that implicit concurrency is a bonafide form of efficient computational learning by using it to obtain close to optimal bounds on the query and time complexity of an algorithm that solves a constrained version of a problem from the computational learning literature: learning parities with a noisy membership query oracle [18, 6].\n1http://bit.ly/YtwdST\nar X\niv :1\n30 7.\n38 24\nv1 [\ncs .N\nE ]\n1 5"
    }, {
      "heading" : "2 Implicit Concurrenct Multivariate Effect Evaluation",
      "text" : "First, a brief primer on schemata and schema partitions [13]: Let S = {0, 1}n be a search space consisting of binary strings of length n and let I be some set of indices between 1 and n, i.e. I ⊆ {1, . . . , n}. Then I represents a partition of S into 2|I| subsets called schemata (singular schema) as in the following example: Suppose n = 5, and I = {1, 2, 4}, then I partitions S into eight schemata:\n00*0* 00*1* 01*0* 01*1* 10*0* 10*1* 11*0* 11*1*\n00000 00010 01000 01000 01010 10000 10010 11010 00001 00011 01001 01001 01011 10001 10011 11011 00100 00110 01100 01100 01110 10100 10110 11110 00101 00111 01101 01101 01111 10101 10111 11111\nwhere the symbol ∗ stands for ’wildcard’. Partitions of this type are called schema partitions. As we’ve already seen, schemata can be expressed using templates, for example, 10 ∗ 1∗. The same goes for schema partitions. For example ## ∗#∗ denotes the schema partition represented by the index set {1, 2, 4}; the one shown above. Here the symbol # stands for ’defined bit’. The fineness order of a schema partition is simply the cardinality of the index set that defines the partition, which is equivalent to the number of # symbols in the schema partition’s template (in our running example, the fineness order is 3). Clearly, schema partitions of lower fineness order are coarser than schema partitions of higher fineness order.\nWe define the effect of a schema partition to be the variance2 of the average fitness values of the constituent schemata under sampling from the uniform distribution over each schema in the partition. So for example, the effect of the schema partition ## ∗#∗ = {00 ∗ 0∗ , 00 ∗ 1∗ , 01 ∗ 0∗, 01 ∗ 1∗, 10 ∗ 0∗ , 10 ∗ 1∗ , 11 ∗ 0∗ , 11 ∗ 1∗} is\n1\n8 1∑ i=0 1∑ j=0 1∑ k=0 (F (ij ∗ k∗)− F (∗ ∗ ∗∗))2\nwhere the operator F gives the average fitness of a schema under sampling from the uniform distribution.\nLet JIK denote the schema partition represented by some index set I. Consider the following intuition pump [4] that illuminates how effects change with the coarseness of schema partitions. For some large n, consider a search space S = {0, 1}n, and let I = [n]. Then JIK is the finest possible partition of S; one where each schema in the partition has just one point. Consider what happens to the effect of JIK as we remove elements from I. It is easily seen that the effect of JIK decreases monotonically. Why? Because we are averaging over points that used to be in separate partitions. Secondly, observe that the number of schema partitions of order k is ( n k ) . Thus, when k n, the number of schema partitions with fineness order k will grow very fast with k (sub-exponentially to be sure, but still very fast). For example, when n = 106, the number of schema partitions of fineness order 2, 3, 4 and 5 are on the order of 1011, 1017, 1022, and 1028 respectively.\nThe point of this exercise is to develop the following intuition: when n is large, a search space will have vast numbers of coarse schema partitions, but most of them will have negligible\n2We use variance because it is a well known measure of dispersion. Other measures of dispersion may well be substituted here without affecting the discussion\neffects due to averaging. In other words, while coarse schema partitions are numerous, ones with non-negligible effects are rare. Implicit concurrent multivariate effect evaluation is a capacity for scaleably learning (with respect to n) small numbers of coarse schema partitions with non-negligible effects. It amounts to a capacity for efficiently performing vast numbers of concurrent effect/noeffect multivariate analyses to identify small numbers of interacting loci."
    }, {
      "heading" : "2.1 Use (and Abuse) of Implicit Concurrency",
      "text" : "Assuming implicit concurrency is possible, how can it be used to power efficient general-purpose, non-local, noise-tolerant optimization? Consider the following heuristic: Use implicit concurrency to identify a coarse schema partition JIK with a significant effect. Now limit future search to the schema in this partition with the highest average sampling fitness. Limiting future search in this way amounts to permanently setting the bits at each locus whose index is in I to some fixed value, and performing search over the remaining loci. In other words, picking a schema effectively yields a new, lower-dimensional search space. Importantly, coarse schema partitions in the new search space that had negligible effects in the old search space may have detectable effects in the new space. Use implicit concurrency to pick one and limit future search to a schema in this partition with the highest average sampling fitness. Recurse.\nSuch a heuristic is non-local because it does not make use of neighborhood information of any sort. It is noise-tolerant because it is sensitive only to the average fitness values of coarse schemata. We claim it is general-purpose firstly, because it relies on a very weak assumption about the distribution of fitness over a search space—the existence of staggered conditional effects [2]; and secondly, because it is an example of a decimation heuristic, and as such is in good company. Decimation heuristics such as Survey Propagation [12, 10] and Belief Propagation [11], when used in concert with local search heuristics (e.g. WalkSat [17]), are state of the art methods for solving large instances of a number of NP-Hard combinatorial optimization problems close to their solvability/unsolvability thresholds.\nThe hyperclimbing hypothesis [2] posits that by and large, the heuristic described above is the abstract heuristic that UGAs implement—or as is the case more often than not, misimplement (it stands to reason that a “secret sauce” computational efficiency that stays secret will not be harnessed fully). One difference is that in each “recursive step”, a UGA is capable of identifying multiple (some small number greater than one) coarse schema partitions with non-negligible effects; another difference is that for each coarse schema partition identified, the UGA does not always pick the schema with the highest average sampling fitness."
    }, {
      "heading" : "2.2 Needed: The Scientific Method, Practiced With Rigor",
      "text" : "Unfortunately, several aspects of the description above are not crisp. (How small is small? What constitutes a “negligible” effect?) Indeed, a formal statement of the above, much less a formal proof, is difficult to provide. Evolutionary Algorithms are typically constructed with biomimicry, not formal analyzability, in mind. This makes it difficult to formally state/prove complexity theoretic results about them without making simplifying assumptions that effectively neuter the algorithm or the fitness function used in the analysis.\nWe have argued previously [2] that the adoption of the scientific method [15] is a necessary\nand appropriate response to this hurdle. Science, rigorously practiced, is, after all, the foundation of many a useful field of engineering. A hallmark of a rigorous science is the ongoing making and testing of predictions. Predictions found to be true lend credence to the hypotheses that entail them. The more unexpected a prediction (in the absence of the hypothesis), the greater the credence owed the hypothesis if the prediction is validated [15, 14].\nThe work that follows validates a prediction that is straightforwardly entailed by the hyperclimbing hypothesis, namely that a UGA that uses a noisy membership query oracle as a fitness function should be able to efficiently solve the learning parities problem for small values of k, the number of essential attributes, and non-trivial values of η, where 0 < η < 1/2 is the probability that the oracle makes a classification error (returns a 1 instead of a 0, or vice versa). Such a result is completely unexpected in the absence of the hypothesis."
    }, {
      "heading" : "2.3 Implicit Concurrency 6= Implicit Parallelism",
      "text" : "Given its name and description in terms of concepts from schema theory, implicit concurrency bears a superficial resemblance to implicit parallelism, the hypothetical “engine” presumed, under the beleaguered building block hypothesis [7, 16], to power optimization in genetic algorithms with strong linkage between genetic loci. The two hypothetical phenomena are emphatically not the same. We preface a comparison between the two with the observation that strictly speaking, implicit concurrency and implicit parallelism pertain to different kinds of genetic algorithms—ones with tight linkage between genetic loci, and ones with no linkage at all. This difference makes these hypothetical engines of optimization non-competing from a scientific perspective. Nevertheless a comparison between the two is instructive for what it reveals about the power of implicit concurrency.\nThe unit of implicit parallel evaluation is a schema h belonging to a coarse schema partition JIK that satisfies the following adjacency constraint : the elements of I are adjacent, or close to adjacent (e.g. I = {439, 441, 442, 445}). The evaluated characteristic is the average fitness of samples drawn from h, and the outcome of evaluation is as follows: the frequency of h rises if its evaluated characteristic is greater than the evaluated characteristics of the other schemata in JIK.\nThe unit of implicit concurrent evaluation, on the other hand, is the coarse schema partition JIK, where the elements of I are unconstrained. The evaluated characteristic is the effect of JIK, and the outcome of evaluation is as follows: if the effect of JIK is non negligible, then a schema in this partition with an above average sampling fitness goes to fixation, i.e. the frequency of this schema in the population goes to 1.\nImplicit concurrency derives its superior power viz-a-viz implicit parallelism from the absence of an adjacency constraint. For example, for chromosomes of length n, the number of schema partitions with fineness order 7 is ( n 7 ) ∈ Ω(n7) [3]. The enforcement of the adjacency constraint, brings this number down to O(n). Schema partition of fineness order 7 contain a constant number of schemata (128 to be exact). Thus for fineness order 7, the number of units of evaluation under implicit parallelism are in O(n), whereas the number of units of evaluation under implicit concurrency are in O(n7)."
    }, {
      "heading" : "3 The Learning Model",
      "text" : "For any positive integer n, let [n] denote the set {1, . . . , n}. For any set K such that |K| < n and any binary string x ∈ {0, 1}n, let πK(x) denote the string y ∈ {0, 1}|K| such that for any i ∈ [ |K| ], yi = xj iff j is the ith smallest element of K (i.e. πK strips out bits in x whose indices are not in K). An essential attribute oracle with random classification error is a tuple φ = 〈k, f, n,K, η〉 where n and k are positive integers, such that |K| = k and k < n, f is a boolean function over {0, 1}k (i.e. f : {0, 1}k → {0, 1}), and η, the random classification error parameter, obeys 0 < η < 1/2. For any input bitstring x,\nφ(x) = { f(πK(x)) with probability 1− η ¬f(πK(x)) with probability η\nClearly, the value returned by the oracle depends only on the bits of the attributes whose indices are given by the elements of K. These attributes are said to be essential. All other attributes are said to be non-essential. The concept space C of the learning model is the set {0, 1}n and the target concept given the oracle is the element c∗ ∈ C such that for any i ∈ [n], c∗i = 1 ⇐⇒ i ∈ K. The hypothesis space H is the same as the concept space, i.e. H = {0, 1}n.\nDefinition 1 (Approximately Correct Learning). Given some positive integer k, some boolean function f : {0, 1}k → {0, 1}, and some random classification error parameter 0 < η < 1/2, we say that the learning problem 〈k, f, η〉 can be approximately correctly solved if there exists an algorithm A such that for any oracle φ = 〈n, k, f,K, η〉 and any 0 < < 1/2, Aφ(n, ) returns a hypothesis h ∈ H such that P (h 6= c∗) < , where c∗ ∈ C is the target concept."
    }, {
      "heading" : "4 Our Result and Approach",
      "text" : "Let ⊕k denote the parity function over k bits. For k = 7 and η = 1/5, we give an algorithm that approximately correctly learns 〈k , ⊕k , η〉 in O(log1.585 n) queries and O(n log1.585 n) time. Our argument relies on the use of hypothesis testing to reject two null hypotheses, each at a Bonferroni adjusted significance level of 10−100/2. In other words, we rely on a hypothesis testing based rejection of a global null hypothesis at the 10−100 level of significance. In layman’s terms, our result is based on conclusions that have a 1 in 10100 chance of being false.\nWhile approximately correct learning lends itself to straightforward comparisons with other forms of learning in the computational learning literature, the following, weaker, definition of learning more naturally captures the computation performed by implicit concurrency.\nDefinition 2 (Attributewise -Approximately Correct Learning). Given some positive integer k, some boolean function f : {0, 1}k → {0, 1}, some random classification error parameter 0 < η < 1/2, and some 0 < < 1/2, we say that the learning problem 〈k, f, η〉 can be attributewise -approximately correctly solved if there exists an algorithm A such that for any oracle φ = 〈n, k, f,K, η〉, Aφ(n) returns a hypothesis h ∈ H such that for all i ∈ [n], P (hi 6= c∗i ) < , where c∗ ∈ C is the target concept.\nOur argument is comprised of two parts. In the first part, we rely on a symmetry argument and the rejection of two null hypotheses, each at a Bonferroni adjusted significance level of 10−100/2,\nto conclude that a UGA can attributewise -approximately correctly solve the learning problem 〈k = 7, f = ⊕7, η = 1/5〉 in O(1) queries and O(n) time.\nIn the second part (relegated to Appendix A) we use recursive three-way majority voting to show that for any 0 < < 18 , if some algorithm A is capable of attributewise -approximately correctly solving a learning problem in O(1) queries and O(n) time, then A can be used in the construction of an algorithm capable of approximately correctly solving the same learning problem in O(log1.585 n) queries and O(n log1.585 n) time. This part of the argument is entirely formal and does not require knowledge of genetic algorithms or statistical hypothesis testing."
    }, {
      "heading" : "5 Symmetry Analysis Based Conclusions",
      "text" : "For any integer m ∈ Z+, let Dm denote the set {0, 1m , 2 m , . . . , m−1 m , 1}. Let V be some genetic algorithm with a population of size m of bitstring chromosomes of length n. A hypothetical population is shown in Figure 1.\nWe define the 1-frequency of some locus i ∈ [n] in some generation τ to be the frequency of the bit 1 at locus i in the population of V in generation τ (in other words the number of ones in the population of V at locus i in generation τ divided by m, the size of the population). For any i ∈ {1, . . . , n}, let oneFreq(V,τ,i) denote the discrete probability distribution over the domain Dm that gives the 1-frequency of locus i in generation τ of V.3\n3Note: oneFreq(V,τ,i) is an unconditional probability distribution in the sense that for any i ∈ {1, . . . , n} and any generation τ , if X0 ∼ oneFreq(V,0,i), . . . , Xτ ∼ oneFreq(V,τ,i) are random variables that give the 1-frequency of\nLet φ = 〈n, k, f,K, η〉 be an oracle such that the output of the boolean function f is invariant to a reordering of its inputs (i.e., for any π : [n] → [n] and any element x ∈ {0, 1}, f(x1, . . . , xk) = f(xπ(1), . . . , xπ(k))) and let W be the genetic algorithm described in algorithm 1 with some population size m, some mutation rate pm, and that uses φ as a fitness function. An appreciation of the algorithmic symmetries in effect (for the purpose at hand, one essential locus is no different from any other essential locus, and a nonessential locus is no different from any other nonessential locus; see Appendix B) yields the following conclusions:\nConclusion 3. ∀τ ∈ Z+0 , ∀i, j such that i ∈ K, j ∈ K,oneFreq(W,τ,i) = oneFreq(W,τ,j)\nConclusion 4. ∀τ ∈ Z+0 , ∀i, j such that i 6∈ K, j 6∈ K,oneFreq(W,τ,i) = oneFreq(W,τ,j)\nFor any i ∈ K, j 6∈ K, and any generation τ we define essential(W,τ) and nonessential(W,τ) to be the probability distributions oneFreq(W,τ,i) and oneFreq(W,τ,j) respectively. Given the conclusions reached above, these distributions are well defined. A further appreciation of the algorithmic symmetries in effect (the location of the k essential loci is immaterial, each non-essential locus is just “along for the ride” and can be spliced out without affecting the 1-frequency dynamics at other loci) yields the following conclusion:\nConclusion 5. ∀τ ∈ Z+0 , essential(W,τ) and nonessential(W,τ) are invariant to n and K"
    }, {
      "heading" : "6 Statistical Hypothesis Testing Based Conclusions",
      "text" : "Fact 6. Let D be some discrete set. For any subset S ⊆ D, and any independent and identically distributed random variables X1, . . . , XN , the following statements are equivalent:\n1. ∀i ∈ [N ], P (Xi ∈ D\\S) ≥\n2. ∀i ∈ [N ], P (Xi ∈ S) < 1−\n3. P (X1 ∈ S ∧ . . . ∧XN ∈ S) < (1− )N\nLet ⊕7 denote the parity function over seven bits, let ψ = 〈n = 5, k = 7, f = ⊕7 , K = [7], η = 1/5〉 be an oracle, and let Gψ be the genetic algorithm described in Algorithm 1 with population size 1500 and mutation rate 0.004 that treats ψ as a fitness function. Figures 2a and 2b show the 1-frequency of the first and last loci of Gψ over 800 generations in each of 3000 runs. Let D∗m be the set {x ∈ Dm | 0.05 < x < 0.95}. Consider the following two null hypotheses:\nHessential0 : ∑\nx∈D∗1500\nessential(Gψ ,800)(x) ≥ 1\n8\nHnonessential0 : ∑\nx∈(D1500\\D∗1500)\nnonessential(Gψ ,800)(x) ≥ 1\n8\nAlgorithm 1: Pseudocode for a simple genetic algorithm with uniform crossover. The population is stored in an m by n array of bits, with each row representing a single chromosome. Rand() returns a number drawn uniformly at random from the interval [0,1] and rand(a, b) < c denotes an a by b array of bits such that for each bit, the probability that it is 1 is c.\nInput: m: population size Input: n: number of bits in a bitstring chromosome Input: τ : number of generations Input: pm: probability that a bit will be flipped during mutation\n1 pop ← rand(m,n) < 0.5 2 for t ← 1 to τ do 3 fitnessVals ← evaluate-fitness(pop) 4 for i ←1 to m do 5 totalFitness ← totalFitness + fitnessVals[i] 6 end 7 cumNormFitnessVals[1] ← fitnessVals[1] 8 for i ←2 to m do 9 cumNormFitnessVals[i] ← cumNormFitnessVals[i− 1] +\n10 (fitnessVals[i]/totalFitness)\n11 end 12 for i ← 1 to 2m do 13 k ← rand() 14 ctr ← 1 15 while k > cumNormFitnessVals[ctr] do 16 ctr ← ctr + 1 17 end 18 parentIndices[i] ← ctr 19 end 20 crossOverMasks ← rand(m,n) < 0.5 21 for i ← 1 to m do 22 for j ← 1 to n do 23 if crossMasks[i,j]= 1 then 24 newPop[i, j]← pop[parentIndices[i],j] 25 else 26 newPop[i, j]← pop[parentIndices[i+m],j] 27 end\n28 end\n29 end 30 mutationMasks ← rand(m,n) < pm 31 for i ← 1 to m do 32 for j ← 1 to n do 33 newPop[i,j]← xor(newPop[i, j], mutMasks[i, j]) 34 end\n35 end 36 pop←newPop 37 end\nwhere essential and nonessential are as described in the previous section.\nWe seek to reject Hessential0 ∨ Hnonessential0 at the 10−100 level of significance. If Hessential0 is true, then for any independent and identically distributed random variables X1, . . . , X3000 drawn from distribution essential(Gψ ,800), and any i ∈ [3000], P (Xi ∈ D∗1500) ≥ 1/8. By Lemma 6, and the observation that D1500\\(D1500\\D∗1500) = D∗1500, the chance that the 1-frequency of the first locus of Gψ will be in D1500\\D∗1500 in generation 800 in all 3000 runs, as seen in Figure 2a, is less than (7/8)3000. Which gives us a p-value less than 10−173 for the hypothesis Hessential0 .\nLikewise, if Hnonessential0 is true, then for any independent and identically distributed random variables X1, . . . , X3000 drawn from distribution nonessential(Gφ,800), and any i ∈ [3000], P (Xi ∈ D1500\\D∗1500) ≥ 1/8. So by Lemma 6, the chance that the 1-frequency of the last locus of Gφ will be in D∗1500 in generation 800 in all 3000 runs, as seen in Figure 2b, is less than (7/8)\n3000. Which gives us a p-value less than 10−173 for the hypothesis Hnonessential0 . Both p-values are less than a Bonferroni adjusted critical value of 10−100/2, so we can reject the global null hypothesesHessential0 ∨ Hnonessential0 at the 10 −100 level of significance. We are left with the following conclusions: Conclusion 7.∑ x∈D∗1500 essential(Gφ,800)(x) < 1 8 Conclusion 8.∑ x∈(D1500\\D∗1500) nonessential(Gφ,800)(x) < 1 8\nAlgorithm 2: Gφ 1 pop ← population of Gφ after 800 generations 2 for i← 1 to n do 3 if 0.05 < 1-frequency of locus i in pop < 0.95 then 4 x[r][i] = 0 5 else 6 x[r][i] = 1 7 end\n8 end\nConclusion 9. The learning problem 〈k = 7, f = ⊕7, η = 1/5〉 can be attributewise 18 -approximately correctly solved in O(1) queries and O(n) time.\nArgument. For any oracle φ = 〈n, k = 7, f = ⊕7,K, η = 1/5〉 with target concept c, let h be the hypothesis returned by the algorithm Gφ shown in Algorithm 2. It is easily seen that Gφ runs in O(1) queries and O(n) time; by Conclusions 7 and 8, for any i ∈ [n], P (ci 6= hi) < 18\nThat 〈k = 7, f = ⊕7, η = 1/5〉 is approximately correctly learnable in O(log1.585 n) queries and O(n log1.585 n) time follows from the conclusion above and from Theorem 14 in the Appendix.\nlocus i in generations 0, . . . , τ of V in some run. Then P (Xτ |Xτ−1, . . . , X0) = P (Xτ )"
    }, {
      "heading" : "7 Discussion and Conclusion",
      "text" : "We used an empirico symmetry analytic proof technique with a 1 in 10100 chance of error to conclude that a genetic algorithm with uniform crossover can be used to solve the noisy learning parities problem 〈k = 7, f = ⊕7, η = 1/5〉 in O(log1.585 n) queries and O(n log1.585 n) time. These bounds are marginally higher than the known optimal bounds for a more difficult version of the problem in which the oracle is queried non-adaptively : O(log n) and O(n) respectively [6, 8]. Tighter bounds on the query complexity than the one obtained can be achieved using recursive majority voting with a higher branching factor (e.g. 5 instead of 3). However, for our purposes the bounds obtained suffice. The finding that a genetic algorithm with uniform crossover can straightforwardly be used to obtain close-to-optimal bounds on the queries and running time required to solve to solve 〈7,⊕7, 1/5〉 is wholly unexpected and lends support to the hypothesis that implicit concurrent multivariate effect evaluation powers general-purpose, non-local, noise-tolerant optimization in genetic algorithms with uniform crossover."
    }, {
      "heading" : "A Formal Analysis",
      "text" : "Algorithm 3: Recursive-3-Way-Maj(`, (x1, . . . , x3`))\n1 if ` = 1 then 2 return M(x1, x2, x3) 3 else 4 y1 = Recursive-3-Way-Maj(`− 1, (x1, . . . , x3`−1)) 5 y2 = Recursive-3-Way-Maj(`− 1, (x3`−1+1, . . . , x2×3`−1)) 6 y3 = Recursive-3-Way-Maj(`− 1, (x2×3`−1+1, . . . , x3`)) 7 return M(y1, y2, y3) 8 end\nLet M : {0, 1}3 → {0, 1} denote the three way majority function that returns the mode of its three arguments. So, M(1, 1, 1) = M(0, 1, 1) = M(1, 0, 1) = M(1, 1, 0) = 1, and M(0, 0, 0) = M(1, 0, 0) = M(0, 1, 0) = M(0, 0, 1) = M(0, 0, 0) = 0.\nLemma 10. Let X1, X2, X3 be independent binary random variables such that for any i ∈ {1, 2, 3}, P (Xi = 0) < . Then P (M(X1, X2, X3) = 0) < 4 2\nProof :\nP (M(X1, X2, X3) = 0) =P (X1 = 0 ∧X2 = 0 ∧X3 = 0)+ P (X1 = 1 ∧X2 = 0 ∧X3 = 0)+ P (X1 = 0 ∧X2 = 1 ∧X3 = 0)+ P (X1 = 0 ∧X2 = 0 ∧X3 = 1)\n< 3 + 3 2\n< 4 2\nConsider Algorithm 3.\nTheorem 11 (Recursive 3-way majority voting). For any > 0 and any ` ∈ Z+, let X1, . . . , X3` be independent binary random variables such that ∀i ∈ [3`], P (Xi = 0) < . Then P (Recursive3-Way-Maj(`, (X1, . . . , X3`)) = 0) < 4 2`−1 2 `\nProof. The proof is by induction on `. The base case, when ` = 1, follows from lemma 10. We assume the inductive hypothesis for ` = k and prove it for ` = k + 1. Let Y1, . . . , Y3 be random binary variables that give the values of y1, y2, y3 in the first (top-level/non-recursive) pass of Algorithm. Three applications of the inductive hypothesis for ` = k gives us that ∀i ∈ {1, 2, 3}, P (Yi = 0) < 4 2k−1 2 k . Since X1, . . . , X3k+1 are independent, Y1, . . . , Y3 are also independent. So,\nby Lemma 10,\nP (Recursive-3-Way-Maj(`, (X1, . . . , X3k+1)) = 0) < 4 ( 42 k−1 2 k )2\n= 4 ( 42 k−1 )2 ( 2 k )2\n= 4 ( 42(2 k−1) )( 2(2 k) )\n= 4 ( 42 k+1−2 )( 2 k+1 )\n= 42 k+1−1 2 k+1\nCorollary 12. For any ` ∈ Z+, let X1, . . . , X3` be independent binary random variables such that for any i ∈ [3`], P (Xi = 0) < 18 . Then, P (Recursive-3-Way-Majority(`, (X1, . . . , X3`)) = 0) < 1/2 2`\nProof. Follows from Theorem 12 and the observation that\n42 `−1 ( 1\n8\n)2` = 42 `−1 ( 1\n4 )2` (1 2 )2` = 1\n4\n( 1\n2 )2` < 1\n22`\nNoting that 0 and 1 are just labels in the above gives us the following:\nCorollary 13. For any ` ∈ Z+, let X1, . . . , X3` be independent binary random variables such that for all i ∈ [3`], P (Xi = 1) < 18 . Then, P (Recursive-3-Way-Majority(`, (X1, . . . , X3`)) = 1) < 1/2 2`\nTheorem 14. For any 0 < < 1/8, if the learning problem 〈k, f, η〉 can be attributewise - approximately correctly solved in O(1) queries and O(n) time, then 〈k, f, η〉 can be approximately correctly solved in O(log1.585 n) queries and O(n log1.585 n) time\nProof Let A be an algorithm that attributewise -approximately correctly solves 〈k, f, η〉 in O(1) queries and O(n) time. Let φ = 〈n, k, f,K, η〉 be some oracle. Consider Algorithm 4 that executes ` = dlog2(log2 n + log2 1 )e runs of the algorithm A\nφ. Let c be the target concept, and for any i ∈ [n], let Hi be a binary random variable that gives the value of h[i] (set in line 6), and let H be the random variable that give the value of h.\nClaim 15. For all i ∈ [n], P (Hi 6= ci) < 1/22 ` .\nAlgorithm 4: Bφ(n, ) Input: n Input: 1 `← dlog2(log2 n+ log2 1 )e+ 1 2 for r ← 1 to 3` do 3 x[r] = Aφ(n) 4 end 5 for i← 1 to n do 6 h[i] = Recursive-3-Way-Majority(`, (x[0][i], . . . , x[3`][i])) 7 end 8 return h\nProof of Claim 15. For any r ∈ [3`], let X(r,i) be a binary random variable that gives the value of x[r][i] where x[r], set in line 3, is the hypothesis returned by A in run r. Clearly, X(1,i), . . . , X(3`,i) are independent. The claim follows from Corollaries 12 and 13, and the premise that 〈7,⊕7, 1/5〉 is attributewise 18 -approximately correctly learnable by A.\nClaim 16. If n/22 ` < , then P (H 6= c) <\nProof of Claim 16. The claim follows from Claim 15 and the union bound.\nClaim 17. n/22 ` <\nProof of Claim 17.\nn\n22` =\nn\n22 dlog2(log2 n+log2\n1 )e+1\n< n\n22 log2(log2 n+log2\n1 )\n= n\n2log2 n+log2 1\n= n\nn/\n=\nBy claims 16 and 17, B approximately correctly solves the learning problem 〈7,⊕7, 1/5〉. We now consider the query complexity of B. Let qA(n), qB(n) give the number of queries made by algorithms A and B respectively. Likewise, let tA(n), tB(n) give the running time of algorithms A and B respectively. The following two claims complete the proof.\nClaim 18. qB(n) ∈ O(log1.585 n):\nClaim 19. tB(n) ∈ O(n log1.585 n):\nProof of Claim 18. By the premise of the theorem, there exist constants n0, cA such that for all n ≥ n0, qA(n) ≤ cA. Thus, for all n ≥ n0,\nqB(n) ≤ cA.3`\n= cA.3 dlog2(log2 n+log2 1 )e+1 ≤ cA.3log2(log2 n+log2 1 )+2\n= 9cA.3 log2(log2 n+log2\n1 )\nTaking logs to the base 2 on both sides gives\nlog2(qB(n)) ≤ log2(9cA) + log2(log2 n+ log2 1 ). log2 3\n⇔ qB(n) ≤ 9cA.(log2 n+ log2 1 )log2 3\n⇔ qB(n) ≤ 9cA.(log2 n+ log2 1 )1.585\nProof of Claim 19. Let τ1(n), τ2(n) be the time taken to execute lines 1—4 and one iteration of the for loop in lines 5–8 of B, respectively. Clearly, tB(n) = τ1(n) + n.τ2(n).\nSub Claim 20. τ1(n) ∈ O(n log1.585 n)\nProof of Sub Claim 20. The proof closely mirrors the proof of Claim 18. We include it for the sake of completeness. By the premise of the theorem, there exist constants n0, cA such that for all n ≥ n0, tA(n) ≤ cA. Thus, for all n ≥ n0,\nτ1(n) ≤ cA.n.3`\n= cA.n.3 dlog2(log2 n+log2 1 )e+1 ≤ cA.n.3log2(log2 n+log2 1 )+2\n= 9cA.n.3 log2(log2 n+log2\n1 )\nTaking logs to the base 2 on both sides gives\nlog2(τ1(n)) ≤ log2(9cA.n) + log2(log2 n+ log2 1 ). log2 3\n⇔ τ1(n) ≤ 9cA.n.(log2 n+ log2 1 )log2 3\n⇔ τ1(n) ≤ 9cA.n.(log2 n+ log2 1 )1.585\nSub Claim 21. τ2(n) ∈ O(log1.5 n)\nProof of Sub Claim 21. Observe that τ2(n) ≤ T (3log2(log2 n+log2 1 )+2), where T is given by the following recurrence relation:\nT (x) =\n{ 3T (x/3) + x if x > 3\n1 if x = 3\nA simple inductive argument (omitted) gives us T (x) ∈ O(x log x). Thus,\nτ2(n) ∈ O(3log2(log2 n+log2 1 )+2(log2(log2 n+ log2\n1\n) + 2))\n= O(3log2(log2 n+log2 1 )(log2(log2 n+ log2\n1\n) + 2))\n= O(3log2(log2 n+log2 1 ) log2(log2 n+ log2\n1\n))\n= O((log2 n+ log2 1 )(log2(log2 n+ log2 1 )))\n⊂ O((log2 n+ log2 1 )1.5)\nWhere the last equation follows from the observation that log(x) < √ x for all positive reals.\nClaim 19 follows from the observation that\ntB(n) = τ1(n) + τ2(n)⇒ tB(n) ∈ O(n log1.585 n+ n log1.5 n) ⇒ tB(n) ∈ O(n log1.585 n)\nWhere the first implication follows from Sub Claims 20 and 21, and the fact that for any functions f1 ∈ O(g1), f2 ∈ O(g2), we have that f1 + f2 ∈ O(|g1|+ |g2|) and f1.f2 ∈ O(g1.g2)"
    }, {
      "heading" : "B On Our Use of Symmetry",
      "text" : "A homologous crossover operation between two chromosomes of length ` can be modeled by a vector of ` random binary variables 〈X1, . . . , X`〉 from which a crossover mask is sampled. Likewise, a mutation operation can be modeled by a vector of ` random binary variables 〈Y1, . . . , Y`〉 from which a mutation mask is sampled. Only in the case of uniform crossover are the random variables X1, . . . , X` independent and identically distributed. This absence of positional bias [5] in uniform crossover constitutes a symmetry. Essentially, permuting the bits of all chromosomes using some permutation π before crossover, and permuting the bits back using π−1 after crossover has no effect on the dynamics of a UGA. If, in addition, the random variables Y1, . . . , Y` that model the mutation operator are independent and identically distributed (which is typical), and (more crucially) independent of the value of `, then in the event that the values of chromosomes at some locus i are immaterial during fitness evaluation, the locus i can be “spliced out” without affecting allele dynamics at other loci. In other words, the dynamics of the UGA can be coarse-grained [1].\nThese conclusions flow readily from an appreciation of the symmetries induced by uniform crossover and length independent mutation. While the use of symmetry arguments is uncommon in EC research, symmetry arguments form a crucial part of the foundations of physics and chemistry. Indeed, according to the theoretical physicist E. T. Jaynes “almost the only known exact results in atomic and nuclear structure are those which we can deduce by symmetry arguments, using the methods of group theory” [9, p331-332]. Note that the conclusions above hold true regardless of the selection scheme (fitness proportionate, tournament, truncation, etc), and any fitness scaling that may occur (sigma scaling, linear scaling etc). “The great power of symmetry arguments lies just in the fact that they are not deterred by any amount of complication in the details”, writes Jaynes [9, p331]. An appeal to symmetry, in other words, allows one to cut through complications that might hobble attempts to reason within a formal axiomatic system.\nOf course, symmetry arguments are not without peril. However, when used sparingly and only in circumstances where the symmetries are readily apparent, they can yield significant insight at low cost. It bears emphasizing that the goal of foundational work in evolutionary computation is not pristine mathematics within a formal axiomatic system, but insights of the kind that allow one to a) explain optimization in current evolutionary algorithms on real world problems, and b) design more effective evolutionary algorithms."
    } ],
    "references" : [ {
      "title" : "Sufficient conditions for coarse-graining evolutionary dynamics",
      "author" : [ "Keki Burjorjee" ],
      "venue" : "In Foundations of Genetic Algorithms 9 (FOGA IX),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Explaining optimization in genetic algorithms with uniform crossover",
      "author" : [ "Keki M. Burjorjee" ],
      "venue" : "In Proceedings of the twelfth workshop on Foundations of genetic algorithms XII, FOGA XII",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Introduction to Algorithms",
      "author" : [ "T.H. Cormen", "C.H. Leiserson", "R.L. Rivest" ],
      "venue" : "McGraw-Hill",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Intuition Pumps and Other Tools for Thinking",
      "author" : [ "Daniel C Dennett" ],
      "venue" : "WW Norton & Company,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Biases in the crossover landscape",
      "author" : [ "L.J. Eshelman", "R.A. Caruana", "J.D. Schaffer" ],
      "venue" : "Proceedings of the third international conference on Genetic algorithms table of contents, pages 10–19",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Attribute-efficient and non-adaptive learning of parities and dnf expressions",
      "author" : [ "Vitaly Feldman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Genetic Algorithms in Search, Optimization & Machine Learning",
      "author" : [ "David E. Goldberg" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1989
    }, {
      "title" : "An application of codes to attribute-efficient learning",
      "author" : [ "Thomas Hofmeister" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1999
    }, {
      "title" : "Probability Theory: The Logic of Science",
      "author" : [ "E.T. Jaynes" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Survey propagation revisited",
      "author" : [ "Lukas Kroc", "Ashish Sabharwal", "Bart Selman" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "A new look at survey propagation and its generalizations",
      "author" : [ "Elitza Maneva", "Elchanan Mossel", "Martin J. Wainwright" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "Analytic and algorithmic solution of random satisfiability problems",
      "author" : [ "M. Mézard", "G. Parisi", "R. Zecchina" ],
      "venue" : "Science, 297(5582):812–815",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "An Introduction to Genetic Algorithms",
      "author" : [ "Melanie Mitchell" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1996
    }, {
      "title" : "Conjectures and Refutations",
      "author" : [ "Karl Popper" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "The Logic Of Scientific Discovery",
      "author" : [ "Karl Popper" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Genetic Algorithms: Principles and Perspectives: a Guide to GA Theory",
      "author" : [ "C.R. Reeves", "J.E. Rowe" ],
      "venue" : "Kluwer Academic Publishers",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Local search strategies for satisfiability testing",
      "author" : [ "B. Selman", "H. Kautz", "B. Cohen" ],
      "venue" : "Cliques, coloring, and satisfiability: Second DIMACS implementation challenge, 26:521–532",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Identification of partial disjunction, parity, and threshold functions",
      "author" : [ "Uehara", "Tsuchida", "Wegener" ],
      "venue" : "TCS: Theoretical Computer Science,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "We recently hypothesized [2] that an efficient form of computational learning underlies generalpurpose, non-local, noise-tolerant optimization in genetic algorithms with uniform crossover (UGAs).",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "We then establish that implicit concurrency is a bonafide form of efficient computational learning by using it to obtain close to optimal bounds on the query and time complexity of an algorithm that solves a constrained version of a problem from the computational learning literature: learning parities with a noisy membership query oracle [18, 6].",
      "startOffset" : 340,
      "endOffset" : 347
    }, {
      "referenceID" : 5,
      "context" : "We then establish that implicit concurrency is a bonafide form of efficient computational learning by using it to obtain close to optimal bounds on the query and time complexity of an algorithm that solves a constrained version of a problem from the computational learning literature: learning parities with a noisy membership query oracle [18, 6].",
      "startOffset" : 340,
      "endOffset" : 347
    }, {
      "referenceID" : 12,
      "context" : "First, a brief primer on schemata and schema partitions [13]: Let S = {0, 1}n be a search space consisting of binary strings of length n and let I be some set of indices between 1 and n, i.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "Consider the following intuition pump [4] that illuminates how effects change with the coarseness of schema partitions.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "We claim it is general-purpose firstly, because it relies on a very weak assumption about the distribution of fitness over a search space—the existence of staggered conditional effects [2]; and secondly, because it is an example of a decimation heuristic, and as such is in good company.",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "Decimation heuristics such as Survey Propagation [12, 10] and Belief Propagation [11], when used in concert with local search heuristics (e.",
      "startOffset" : 49,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "Decimation heuristics such as Survey Propagation [12, 10] and Belief Propagation [11], when used in concert with local search heuristics (e.",
      "startOffset" : 49,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "Decimation heuristics such as Survey Propagation [12, 10] and Belief Propagation [11], when used in concert with local search heuristics (e.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "WalkSat [17]), are state of the art methods for solving large instances of a number of NP-Hard combinatorial optimization problems close to their solvability/unsolvability thresholds.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 1,
      "context" : "The hyperclimbing hypothesis [2] posits that by and large, the heuristic described above is the abstract heuristic that UGAs implement—or as is the case more often than not, misimplement (it stands to reason that a “secret sauce” computational efficiency that stays secret will not be harnessed fully).",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "We have argued previously [2] that the adoption of the scientific method [15] is a necessary",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "We have argued previously [2] that the adoption of the scientific method [15] is a necessary",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "The more unexpected a prediction (in the absence of the hypothesis), the greater the credence owed the hypothesis if the prediction is validated [15, 14].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : "The more unexpected a prediction (in the absence of the hypothesis), the greater the credence owed the hypothesis if the prediction is validated [15, 14].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "Given its name and description in terms of concepts from schema theory, implicit concurrency bears a superficial resemblance to implicit parallelism, the hypothetical “engine” presumed, under the beleaguered building block hypothesis [7, 16], to power optimization in genetic algorithms with strong linkage between genetic loci.",
      "startOffset" : 234,
      "endOffset" : 241
    }, {
      "referenceID" : 15,
      "context" : "Given its name and description in terms of concepts from schema theory, implicit concurrency bears a superficial resemblance to implicit parallelism, the hypothetical “engine” presumed, under the beleaguered building block hypothesis [7, 16], to power optimization in genetic algorithms with strong linkage between genetic loci.",
      "startOffset" : 234,
      "endOffset" : 241
    }, {
      "referenceID" : 2,
      "context" : "For example, for chromosomes of length n, the number of schema partitions with fineness order 7 is ( n 7 ) ∈ Ω(n7) [3].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "Let ⊕7 denote the parity function over seven bits, let ψ = 〈n = 5, k = 7, f = ⊕7 , K = [7], η = 1/5〉 be an oracle, and let Gψ be the genetic algorithm described in Algorithm 1 with population size 1500 and mutation rate 0.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "Rand() returns a number drawn uniformly at random from the interval [0,1] and rand(a, b) < c denotes an a by b array of bits such that for each bit, the probability that it is 1 is c.",
      "startOffset" : 68,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "5 2 for t ← 1 to τ do 3 fitnessVals ← evaluate-fitness(pop) 4 for i ←1 to m do 5 totalFitness ← totalFitness + fitnessVals[i] 6 end 7 cumNormFitnessVals[1] ← fitnessVals[1] 8 for i ←2 to m do 9 cumNormFitnessVals[i] ← cumNormFitnessVals[i− 1] + 10 (fitnessVals[i]/totalFitness) 11 end 12 for i ← 1 to 2m do 13 k ← rand() 14 ctr ← 1 15 while k > cumNormFitnessVals[ctr] do 16 ctr ← ctr + 1 17 end 18 parentIndices[i] ← ctr 19 end 20 crossOverMasks ← rand(m,n) < 0.",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "5 2 for t ← 1 to τ do 3 fitnessVals ← evaluate-fitness(pop) 4 for i ←1 to m do 5 totalFitness ← totalFitness + fitnessVals[i] 6 end 7 cumNormFitnessVals[1] ← fitnessVals[1] 8 for i ←2 to m do 9 cumNormFitnessVals[i] ← cumNormFitnessVals[i− 1] + 10 (fitnessVals[i]/totalFitness) 11 end 12 for i ← 1 to 2m do 13 k ← rand() 14 ctr ← 1 15 while k > cumNormFitnessVals[ctr] do 16 ctr ← ctr + 1 17 end 18 parentIndices[i] ← ctr 19 end 20 crossOverMasks ← rand(m,n) < 0.",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "004, and ψ is the oracle 〈n = 8, k = 7, f = ⊕7,K = [7], η = 1/5〉.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "These bounds are marginally higher than the known optimal bounds for a more difficult version of the problem in which the oracle is queried non-adaptively : O(log n) and O(n) respectively [6, 8].",
      "startOffset" : 188,
      "endOffset" : 194
    }, {
      "referenceID" : 7,
      "context" : "These bounds are marginally higher than the known optimal bounds for a more difficult version of the problem in which the oracle is queried non-adaptively : O(log n) and O(n) respectively [6, 8].",
      "startOffset" : 188,
      "endOffset" : 194
    } ],
    "year" : 2013,
    "abstractText" : "This paper establishes theoretical bonafides for implicit concurrent multivariate effect evaluation—implicit concurrency for short—a broad and versatile computational learning efficiency thought to underlie general-purpose, non-local, noise-tolerant optimization in genetic algorithms with uniform crossover (UGAs). We demonstrate that implicit concurrency is indeed a form of efficient learning by showing that it can be used to obtain close-to-optimal bounds on the time and queries required to approximately correctly solve a constrained version (k = 7, η = 1/5) of a recognizable computational learning problem: learning parities with noisy membership queries. We argue that a UGA that treats the noisy membership query oracle as a fitness function can be straightforwardly used to approximately correctly learn the essential attributes in O(log n) queries and O(n log n) time, where n is the total number of attributes. Our proof relies on an accessible symmetry argument and the use of statistical hypothesis testing to reject a global null hypothesis at the 10−100 level of significance. It is, to the best of our knowledge, the first relatively rigorous identification of efficient computational learning in an evolutionary algorithm on a non-trivial learning problem.",
    "creator" : "LaTeX with hyperref package"
  }
}