{
  "name" : "1708.08985.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 8.\n08 98\n5v 1\n[ cs\n.C V\n] 1\n6 A\nug 2\ning with various applications, including data compression and signal restoration. Training methods for such systems focus on the generality of the network given limited amount of training data. A less researched type of techniques concerns generation of only a single type of input. This is useful for applications such as constraint handling, noise reduction and anomaly detection. In this paper we present a technique to limit the generative capability of the network using negative learning. The proposed method searches the solution in the gradient direction for the desired input and in the opposite direction for the undesired input. One of the application can be anomaly detection where the undesired inputs are the anomalous data. In the results section we demonstrate the features of the algorithm using MNIST handwritten digit dataset and latter apply the technique to a real-world obstacle detection problem. The results clearly show that the proposed learning technique can significantly improve the performance for anomaly detection."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Generative networks can learn to generate high-dimensional data from lower dimensional embeddings. Most of the applications require the generative models to generalize given a limited amount of training data. As a consequence, even the signals that are far from the training data distribution can be generated fairly well. Controlling this generalization property of the generative networks can increase their efficiency in the domains where we need to separate one kind of data from the other. Some of the applications of system with limited generative ability include noise reduction and anomaly detection.\nIn this paper, we present a method to control the generative capabilities of a system in such a way that it can only reconstruct a limited range of input signal space. The technique can be used with different network structures and training algorithms. We will explain the proposed method by focusing on anomaly detection in higher dimensional spaces (e.g. images etc.) using a kind of generative neural networks called autoencoder. Using the proposed technique, generative models can be trained in a way to learn a latent representation\nthat can only encode the input distributions of non-anomalous data. After decoding the latent space back to the signal space, the reconstruction similarity can be used to judge if the input signal contains an anomaly or not.\nAnomaly detection is a key and usually the first requirement in many signal-processing applications pipeline [3, 10]. Generative models have previously been applied to anomaly detection [11, 1] and noise reduction [14]. In anomaly detection the task is to find if the input distribution is normal or has an anomaly. It is a one-class classification problem where the training data consists mostly of the non-anomalous class. We argue that due to the generalization property the classic training methods are not ideal for anomaly detection using generative models (as shown in Section 5).\nThe main contribution of this paper is a new approach to limit the reconstruction capability of the generative networks by learning conflicting objectives for the normal and anomaly data. The technique can use the limited real or synthetic anomalous data by using a negative learning phase in the training. For example, in case of anomaly detection on the road [1], any non-road object (e.g. vehicles, bushes etc.) can be treated as the anomalous data. Some anomaly data is available in most of the anomaly detection applications. The anomaly data might be gathered over time automatically or by human intervention. For instance, in case of a misclassification by a radar based target detection system, the human operator can label the sample correctly for future use. Instead of ignoring this anomaly data, the proposed method uses this data to improve the future detections.\nThe rest of the paper is organized as follows. Related work is given in Section 2. We formally define the problem in Section 3. The specificities of our approach are detailed in Section 4. Quantitative analysis of the technique are presented in Section 5. Finally, we conclude the paper with some directions for future work in Section 6."
    }, {
      "heading" : "2. RELATED WORK",
      "text" : "There are a large number of literatures on noise reduction and anomaly detection using generative models. M.N. Schmidt et al. [13] uses non-negative sparse coding to reduce the wind noise in speech data. They rely on a system that have the\nsource model for the wind noise but not for the speech to reduce the noise. The work done on denoising autoencoder by V. Pascal et al. [15] is also very important in this area. L. Gondara [4] presents an application of such denoising system to remove noise from medical images. These techniques can reduce the noise the input data but they do not limit the generative capabilities of the network. Due to the generalization property of such networks, they can also generate the data that is very different from the data shown during training.\nSimilar to the method proposed in this paper, for anomaly detection, the machine learns a model to represent normality and then use the model to detect anomalous data. B. Saleh et al. [12] proposed a method to model a normality of a particular class of object using visual attributes. The attributes [2] are handcrafted and mainly based on the appearance of the input data, i.e. shape, texture and color. A generative model is then trained and used to reason about normal and anomalous data. Recent trend tends to replace these handcrafted attributes with a deep feature representation. W. Lawson et al. [9] uses deep visual features obtained from AlexNet [8] to represent objects and associated them with a scene to define type of objects that can be found in the certain environment. D. Xu et al. [17] used stacked denoising autoencoders to learn the deep features in an unsupervised fashion and use them to represent both appearance and motion of the scene. Anomalous data is in turn detected by a multiple one-class SVM classifiers. These approaches are more likely to suffer from the imbalance between normal and anomalous data which are the common characteristic of an anomaly detection problem, The proposed method try to solve this problem by effectively using the anomaly data.\nOur proposed method uses a similar approach to C. Creusot and A. Munawar [1]. They use an extremely compressive Restricted Boltzmann Machine (RBM) to form a deep feature representation. But rather than training a classifier in the feature space, anomaly detection is performed by reconstructing the data back to the original image space and use conventional image difference as a metric. The extreme compression in autoencoders can severely effect the reconstruction of input appearance in case the non-anomalous data have a non-trivial appearance."
    }, {
      "heading" : "3. PROBLEM STATEMENT",
      "text" : "In this section, we will formally describe the problem of limiting the generative network to learn a single type of input distribution. Consider two random variables X and Y representing instances of two input distributions in same signal space (e.g. image space). Lets assume we have K and J number of samples from each distribution, X = {x1, ..., xK} and Y = {y\n1 , ..., yJ}. X is the input distribution we want the\nnetwork to reconstruct as well as possible, let the reconstruction be called X̂. On the other hand, Y is the distribution that we do not want to the network to reconstruct. Let its recon-\nstructed space be represented by Ŷ. In order to achieve this objective, we need to maximize\npθ(X̂|X)− pθ(Ŷ|Y) =\nK ∑\ni=1\nlog pθ(x̂i|xi)−\nJ ∑\ni=1\nlog pθ(ŷi|yi)\n(1)\nBy maximizing the probability of reconstruction for X and minimizing it for Y, the generative properties of the model can be controlled in the desired way. It is important to note that usually the data for the distribution X is available in plenty while the data for Y is available scarcely (K ≫ J)."
    }, {
      "heading" : "4. PROPOSED METHOD",
      "text" : "In this section we discuss the proposed approach to maximize Equation 1. Generative models can be used in a variety of settings and configurations. In this paper we deal with the generative models that encode the input distribution into a latent feature spaceL and then reconstruct it back in the original signal space. Such generative systems are also known as autoencoders. We use the word “autoencoder” for any kind of generative neural network structure including but not limited to RBM, variational autoencoders and Convolutional Neural Network based autoencoders.\nThe problem is to learn latent representation L such that it can learn to encode and decode X fairly well but fails to do the same for Y distribution. In order to formally define the autoencoder like generative models, let us consider a network with input vector of size N , a latent space or hidden layer of size H . As the network will learn to reconstruct the input the output of the network will also be of size N . Given the training data X, a function F can transform this input signal to the hidden layer while a function G can reconstruct the image from the latent space. The network parameters for encoder and decoder are represented by α and β respectively. We want to find the optimal parameters θ = {α, β} to minimize the reconstruction error. When presented with an input vector x, the network produces a hidden vector ℓ = F(x;α) and an output vector x̂ = G(ℓ;β). The goal of the learning is to minimize an error or energy function E\nmin E(F,G) = min F,G\nK ∑\ni=1\n∆(x̂i, xi)\n= min F,G\nK ∑\ni=1\n∆(G(F(xi;α);β)), xi)\n(2)\nwhere, ∆ is a distance or dissimilarity measure. We can use any dissimilarity measure, in this paper we use mean square error: ∆ = ∑K\ni=1 (x̂i − xi) 2 . Optimum set of parameters θ∗\ncan be found by\nAlgorithm 1: Training algorithm with a negative learning phase.\nData: X; Non-anomalous data. Data: Y; Anomalous data. Input: Q; Number of iteration of a negative learning\nphase.\nResult: An autoencoder that can reconstruct X\nproperly, but poorly on Y.\n1 ———START———\n/* Perform a positive learning with\nX. */\n2 POSITIVELEARNING(X)\n/* Loop until the termination\ncriteria is satisfied. */\n3 while Not Satisfying the Termination Criteria do\n/* Multiple iterations of negative\nlearning are performed; in order to balance the ratio of X and Y. */\n4 for i = 0 to Q do /* Perform a negative learning\nwith Y. */\n5 NEGATIVELEARNING(Y)\n/* Perform a positive learning\nwith X. */\n6 POSITIVELEARNING(X)\n7 ———END———\nθ∗ = arg min θ\nK ∑\ni=1\n(x̂i − xi) 2\n(3)\nIn order to create an interesting representation of the data, usually the size of hidden layer is kept smaller then the input size H < N . However, H ≥ N can also be used with additional sparsity constraints to see very interesting behaviors for some applications.\nIn this paper, we propose using any real or synthetic anomalous data Y to limit the reconstruction capability of the autoencoder. This is done by incorporating a negative learning phase in the training. System parameters learned during the training allow reconstructing a wide variety of input patterns. Negative training adjusts the system parameters in a way that the anomalous patterns cannot be reconstructed well. In terms of neural networks, the connections that are used to reconstruct anomalies are weakened during the negative learning. The negative learning can formally be defined as\nθ∗ = arg max θ\nJ ∑\ni=1\n(ŷi − yi) 2\n(4)\nUsing Equation 3 and Equation 4, the model can be controlled to reconstruct non-anomalous data better than the anomalies. It is important to note that both the equations are optimizing the same set of parameters θ with conflicting objectives. Going along the gradient for non-anomalous data and against the gradient for negative makes the system go towards a minimum where it can reconstruct only non-anomalous data.\nAlgorithm 1 introduces the negative learning phase. The strategy is to use all the non-anomalous data and finish one epoch of positive learning in which the system learns to reconstruct the non-anomalous training data X. Then in the negative learning step the available anomalous data Y is used to make the system unlearn the reconstruction capability of the anomalies. Positive and negative learning steps are repeated until the termination criteria is met. This enables the system to learn only the reconstruction of non-anomalous signals. We show that the benefits of using the negative learning approach are significant even when the size of anomalous training samples is much smaller than the non-anomalous signal data.\nIt is important to keep a balance between the negative and the positive learning. If the size of anomalous data is very small compared to non-anomalous data, a single positive learning iteration should be followed by multiple iterations of negative learning. An adaptive approach can be used to compute optimal number of iterations for the negative learning. This adaptive algorithm is out of the scope of this paper."
    }, {
      "heading" : "5. EXPERIMENTAL RESULTS",
      "text" : "In order to explain the working of the algorithm, the initial experiments are conducted with the MNIST digits dataset. The later part of this section uses actual highway data to show the validity of the approach for real world problems."
    }, {
      "heading" : "5.1. Evaluation using MNIST",
      "text" : "For this experiment we have used a single layer RBM based autoencoder. 28 × 28 gray-scale images of MNIST digit dataset are used to train a fully connected autoencoder of size 784−500−784. Sigmoid was used as the activation function. Termination criteria, maximum number of epochs was set to 200. Batch size was 50. The network was trained by using single-step contrastive divergence (CD-1) [5].\n∆wij = ζ [ ǫ(〈vihj〉data − 〈vihj〉recon) ]\n(5)\nwhere v represents visible layer, h is the hidden layer, ǫ represents the expected value and ζ is the sign. For positive learning stage the change in weights are updated by Equation 5 with ζ = 1, and for negative stage (going against the gradient) the same equation is used with ζ = −1.\nFigure 1(a) shows images fromMNIST test dataset. Digit 3 and 5 are considered to be the anomalies; hence, the autoen-\ncoder trained with the proposed method should not be able to reconstruct these digits well.\nFigure 1(b) shows the results of reconstruction using a conventional autoencoder, trained using CD-1 with 200 epochs and batch size of 50. The training data for conventional training method contains all the digits except the images of digit 3 and 5. It can be clearly seen that that even though the system knows nothing about digits 3 and 5, it is able to reconstruct them fairly accurately. This property of autoencoders is not desirable for anomaly detection.\nFigure 1(c) shows the reconstruction results using the proposed approach. Digits 3 and 5 are no more reconstructed\nproperly; rather they are converted to the closest point in the non-anomalous signal space. From the shapes point of view, digits 3 can be thought of as a part of digit 8 and similarly digit 5 is a part of digit 6. Yet the system trained with the proposed approach was able to reconstruct digits 8 and 6 but failed to reconstruct 3 and 5.\nFigure 2 shows the frequency distribution of dissimilarity measure for normal and anomaly data. For the conventional autoencoder, we can observe a huge overlap between the curves, making it difficult to select a suitable threshold to decide anomalies. The proposed autoencoder shifts and spreads the curve of the anomaly data horizontally while the curve for the non-anomalous data largely remain unaffected.\nTo simulate the case where the anomaly data is much smaller than the non-anomalous data, another experiment is conducted using only 1, 000 anomalous images (the first 500 images of 3 and 5), and roughly 50, 000 non-anomalous images for training. In order to create a balance between the positive and negative learning phase, five iterations of negative learning are performed after each positive learning phase (number of iterations for negative learning can be computed adaptively, this however is out of the scope of current paper). As shown in Figure 2(c), even for this experiment where the anomalous data size is 50 times smaller then regular data, there is still a major improvement as compared to the results of conventional autoencoder given in Figure 2(a). Similar results were achieved using other digits as anomaly.\nAnother very interesting results are shown in Figure 3. Figure 3(b) shows that a conventional autoencoder trained solely using the images of digits in MNIST dataset can also reconstruct random shapes. However, as visible in Figure 3(c), even though only digit 3 and 5 were used as anomalous images during the training process, the system failed to reconstruct anomalies that it has never seen before. This proves that knowing the appearance of all the anomalies is not important."
    }, {
      "heading" : "5.2. Evaluation on obstacle detection",
      "text" : "In the second experiment, we used the 4K highway video on Japan highways [16]. It consists in a 1h40m sequence of Japan highway recorded from the car dashboard with a Panasonic GH4 camera in 4K resolution (3840× 2160). We considered the video between frame 105360 to frame 114360. These frames were selected as they have a good view of the\nroadwithout any vehicle occluding the road. The images were converted to gray-scale and then resized to 25% of the original size. We then selected a fix mask of 170× 170 in the center road area. 24, 800 gray-scale road patches images of size 32× 32were extracted with random strides from the rescaled video as non-anomalous data. During the dataset creating all featureless road patches were ignored. In this experiment we used just 500 gray-scale CIFAR-10 images [7] as anomaly data (as shown in Figure 4). Randomly selected 70% of the data was used for training while the remaining data was used for testing. Mean and standard deviation is computed for all the images in the training data. Training and test data is then\nnormalized by subtracting this mean from each image and dividing each image by the computed standard deviation. The network was of size 1024−512−1024 in size. Adam [6] was used as the optimizer to verify the validity of the proposed technique for different learning methods. Parameters used for Adam were, α = 0.001, β1 = 0.9, β2 = 0.999 and ǫ = 10 −8. The termination criteria was number of epochs that was set at 100. Batch size was selected as 32.\nArea under the receivers operating characteristics curve (AUROC) was used for quantitative measure. Figure 5 shows that with conventional training of autoencoder using only the road data can still reconstruct the CIFAR images fairly well. The value of AUROC for conventional autoencoders stays around 0.875. As the system learns to reconstruct the road better it becomes equally good at reconstructing the CIFAR data. For the proposed technique the AUROC tends to reach 0.96 as the number of epochs increases (for the proposed method one epoch means one iteration of positive and negative learning). For this experiment the amount of negative learning to perform was computed adaptively by observing the maximum gain in AUROC by increasing or decreasing the size of data and iterations for negative learning. Details on adaptive control of negative learning are out of the scope of this paper. Figure 4 shows the reconstruction quality for the road and CIFAR images by the conventional and the proposed approach. It is clear that after 100 epochs the conventional method can reconstruct the anomaly images much better then the proposed approach. This results in lowering the AUROC for the conventional approach.\nIn Figure 6 we have compared the ROC of our system with classical two class classifiers. In this case a mask was used to locate the road in the video and anything outside the mask was treated as anomaly. We captured a video on Japan highways in similar conditions to [16]. However, while [16] use a relative wide angle 12-35mm lens, we used 70-150mm lens. 50, 000 road patches of size 16 × 16 were extracted, while only 1, 000 patches were extracted for anomaly data. The size of anomaly data was 50 times less than that of the road data. 70% data was used as training and the remaining for testing.\nThe data was normalized in a manner similar to the previous experiment. The AUROC for SVM and LDA classifier is 0.8326 and 0.7526 respectively. The AUROC for the anomaly detection technique proposed in this paper reaches 0.9636 in 100 epochs. The network was of size 256−200−256. The batch size was kept at 32. Vanilla stochastic gradient descent algorithm was used with a learning rate of 0.001. A significant improvement in AUROC clearly shows the benefit of the proposed approach."
    }, {
      "heading" : "5.3. Limitations",
      "text" : "This techniqueworks for the cases where non-anomalous data compared to anomaly data is confined in the input space. In the above experiment treating CIFAR as normal and road as anomaly will not produce expected results. This assumption is generally true for anomaly detection applications where normal operation is more or less predictable and uniform."
    }, {
      "heading" : "6. CONCLUSIONS",
      "text" : "We proposed a novel method to train generative models namely autoencoders for anomaly detection. Anomaly is determined by considering the similarity measure of the input and the reconstructed signal. Conventional training methods allow the reconstruction of the signal space far beyond the training data. The proposed method ensures that the autoencoder only learns to reconstruct the signals that are similar to the training distribution. This makes it easier to separate a normal signal from an anomalous signal. The core idea of this research is the introduction of a negative learning phase, in which the system unlearns the reconstruction of anomalous signals. The balance of positive learning with negative learning phases help to move the frequency distribution curves for dissimilarity of regular and anomalous data away from each other.\nAs for a future direction, we are currently working on adding the notion of time to this approach. In this case the system will only be able to predict the road feature that should appear next. By matching prediction with the actual observation we can reveal anomalies."
    }, {
      "heading" : "7. REFERENCES",
      "text" : "[1] C. Creusot and A. Munawar. Real-time small obstacle\ndetection on highways using compressive rbm road reconstruction. In IEEE Intelligent Vehicles Symposium, Seoul, Korea, 2015.\n[2] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. De-\nscribing objects by their attributes. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1778–1785, June 2009.\n[3] D. Forslund and J. Bjrkefur. Night vision animal de-\ntection. In 2014 IEEE Intelligent Vehicles Symposium Proceedings, pages 737–742, June 2014.\n[4] L. Gondara. Medical image denoising using convo-\nlutional denoising autoencoders. In 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW), pages 241–246, Dec 2016.\n[5] G. E. Hinton. A Practical Guide to Training Restricted\nBoltzmann Machines, pages 599–619. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012.\n[6] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. CoRR, abs/1412.6980, 2014.\n[7] A. Krizhevsky and G. Hinton. Learning multiple layers\nof features from tiny images. 2009.\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Ima-\ngenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran Associates, Inc., 2012.\n[9] W. Lawson, L. Hiatt, and K. Sullivan. Detecting anoma-\nlous objects on mobile platforms. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2016.\n[10] A. Munawar, P. Vinayavekhin, and G. D. Magistris.\nSpatio-temporal anomaly detection for industrial robots through prediction in unsupervised feature space. In IEEE Winter Conference on Applications of Computer Vision (WACV), Santa Rosa, USA, 2017.\n[11] M. Sakurada and T. Yairi. Anomaly detection using au-\ntoencoders with nonlinear dimensionality reduction. In Proceedings of the MLSDA 2014 2Nd Workshop on Machine Learning for Sensory Data Analysis, MLSDA’14, pages 4:4–4:11, New York, NY, USA, 2014. ACM.\n[12] B. Saleh, A. Farhadi, and A. Elgammal. Object-centric\nanomaly detection by attribute-based reasoning. In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR ’13, pages 787– 794, Washington, DC, USA, 2013. IEEE Computer Society.\n[13] M. N. Schmidt, J. Larsen, and F. T. Hsiao. Wind noise\nreduction using non-negative sparse coding. In 2007 IEEE Workshop on Machine Learning for Signal Processing, pages 431–436, Aug 2007.\n[14] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Man-\nzagol. Extracting and composing robust features with denoising autoencoders. InW.W. Cohen, A. McCallum, and S. T. Roweis, editors, Proceedings of the Twentyfifth International Conference on Machine Learning (ICML’08), pages 1096–1103. ACM, 2008.\n[15] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A.\nManzagol. Stacked denoising autoencoders: Learning\nuseful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 11:3371–3408, Dec. 2010.\n[16] Wataken777. Youtube. tokyo express way gh4\n4k. https://www.youtube.com/watch?v=UQgj3zkh8zk. (3840 x 2160), 2014.\n[17] D. Xu, E. Ricci, Y. Yan, J. Song, and N. Sebe. Learn-\ning deep representations of appearance and motion for anomalous event detection. In X. Xie, M. W. Jones, and G. K. L. Tam, editors, Proceedings of the British Machine Vision Conference 2015, BMVC 2015, Swansea, UK, September 7-10, 2015, pages 8.1–8.12. BMVA Press, 2015."
    } ],
    "references" : [ {
      "title" : "Real-time small obstacle detection on highways using compressive rbm road reconstruction",
      "author" : [ "C. Creusot", "A. Munawar" ],
      "venue" : "In IEEE Intelligent Vehicles Symposium, Seoul,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Describing objects by their attributes",
      "author" : [ "A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Night vision animal detection",
      "author" : [ "D. Forslund", "J. Bjrkefur" ],
      "venue" : "IEEE Intelligent Vehicles Symposium Proceedings,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Medical image denoising using convolutional denoising autoencoders",
      "author" : [ "L. Gondara" ],
      "venue" : "IEEE 16th International Conference on Data Mining Workshops (ICDMW),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "A Practical Guide to Training Restricted Boltzmann Machines, pages 599–619",
      "author" : [ "G.E. Hinton" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Detecting anomalous objects on mobile platforms",
      "author" : [ "W. Lawson", "L. Hiatt", "K. Sullivan" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Spatio-temporal anomaly detection for industrial robots through prediction in unsupervised feature space",
      "author" : [ "A. Munawar", "P. Vinayavekhin", "G.D. Magistris" ],
      "venue" : "In IEEE Winter Conference on Applications of Computer Vision (WACV), Santa Rosa,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2017
    }, {
      "title" : "Anomaly detection using autoencoders with nonlinear dimensionality reduction",
      "author" : [ "M. Sakurada", "T. Yairi" ],
      "venue" : "In Proceedings of the MLSDA 2014 2Nd Workshop on Machine Learning for Sensory Data Analysis,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Object-centric anomaly detection by attribute-based reasoning",
      "author" : [ "B. Saleh", "A. Farhadi", "A. Elgammal" ],
      "venue" : "In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Wind noise reduction using non-negative sparse coding",
      "author" : [ "M.N. Schmidt", "J. Larsen", "F.T. Hsiao" ],
      "venue" : "IEEE Workshop on Machine Learning for Signal Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol" ],
      "venue" : "Proceedings of the Twentyfifth International Conference on Machine Learning",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Stacked denoising autoencoders: Learning  useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Learning deep representations of appearance and motion for anomalous event detection",
      "author" : [ "D. Xu", "E. Ricci", "Y. Yan", "J. Song", "N. Sebe" ],
      "venue" : "Proceedings of the British Machine Vision Conference",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Anomaly detection is a key and usually the first requirement in many signal-processing applications pipeline [3, 10].",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "Anomaly detection is a key and usually the first requirement in many signal-processing applications pipeline [3, 10].",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "Generative models have previously been applied to anomaly detection [11, 1] and noise reduction [14].",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "Generative models have previously been applied to anomaly detection [11, 1] and noise reduction [14].",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "Generative models have previously been applied to anomaly detection [11, 1] and noise reduction [14].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "For example, in case of anomaly detection on the road [1], any non-road object (e.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "[13] uses non-negative sparse coding to reduce the wind noise in speech data.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] is also very important in this area.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "Gondara [4] presents an application of such denoising system to remove noise from medical images.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "[12] proposed a method to model a normality of a particular class of object using visual attributes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "The attributes [2] are handcrafted and mainly based on the appearance of the input data, i.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "[9] uses deep visual features obtained from AlexNet [8] to represent objects and associated them with a scene to define type of objects that can be found in the certain environment.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[9] uses deep visual features obtained from AlexNet [8] to represent objects and associated them with a scene to define type of objects that can be found in the certain environment.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "[17] used stacked denoising autoencoders to learn the deep features in an unsupervised fashion and use them to represent both appearance and motion of the scene.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Munawar [1].",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 4,
      "context" : "The network was trained by using single-step contrastive divergence (CD-1) [5].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "In this experiment we used just 500 gray-scale CIFAR-10 images [7] as anomaly data (as shown in Figure 4).",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "Adam [6] was used as the optimizer to verify the validity of the proposed technique for different learning methods.",
      "startOffset" : 5,
      "endOffset" : 8
    } ],
    "year" : 2017,
    "abstractText" : "Generative models are widely used for unsupervised learning with various applications, including data compression and signal restoration. Training methods for such systems focus on the generality of the network given limited amount of training data. A less researched type of techniques concerns generation of only a single type of input. This is useful for applications such as constraint handling, noise reduction and anomaly detection. In this paper we present a technique to limit the generative capability of the network using negative learning. The proposed method searches the solution in the gradient direction for the desired input and in the opposite direction for the undesired input. One of the application can be anomaly detection where the undesired inputs are the anomalous data. In the results section we demonstrate the features of the algorithm using MNIST handwritten digit dataset and latter apply the technique to a real-world obstacle detection problem. The results clearly show that the proposed learning technique can significantly improve the performance for anomaly detection.",
    "creator" : "LaTeX with hyperref package"
  }
}