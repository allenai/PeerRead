{"conference": "ICLR 2017 conference submission", "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks", "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert human performance on Labyrinth.", "reviews": [{"is_meta_review": true, "comments": "This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features.\nThey propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features. They also propose a supervised regression task, predict immediate reward following a sequence of events. The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance.\nSuch agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations. \n\nThis work contrasts with traditional \"passive\" unsupervised or model-based learning. Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.\n\nTo me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the \"possibility\" of control that an agent has over the environment.\nThe proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful. For example L_PC should be explicitly mentioned, before reaching the appendix. Otherwise this work is clear and easily understandable by readers familiar with Deep RL.\nThe methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found.\nI think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "There was broad consensus by the reviewers that this paper should be accepted. There was also a good deal of discussion about detailed aspects of the paper. I think the directions in which this paper points are going to be of interest to many in the community. As one reviewer put is, the idea here seems to involve taking advantage of the \"possibility\" of control that an agent may have over the environment. This is formulated in terms of auxiliary control and auxiliary prediction tasks, which share an underlying CNN and LSTM representation.\n \n The revision posted by the author's addresses a number of the questions, suggestions and concerns of reviewers.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Response to reviewers", "OTHER_KEYS": "Volodymyr Mnih", "comments": "We thank the reviewers and everyone else who provided comments for their feedback. The most recent revision of the paper incorporates a number of the suggestions and aims to address the most serious concerns.\n\nAnonReviewer3\n\n- Can authors comment about the computational resources needed to train the UNREAL agent?\n\nPlease see the answer to a previous question below.\n\n- The overall architecture is quite complicated. Are the authors willing to release the source code for their model?\n\nThe UNREAL architecture was quite straightforward to implement. We will add pseudocode to the appendix of the camera ready version to clarify how the model works. We don\u2019t currently have plans to release the code but we\u2019re happy to assist any open source reimplementation effort.\n\nAnonReviewer4\n\n- I believe you should say something about the extra computational cost of optimizing these auxiliary tasks. How much do you lose in terms of training speed? Which are the most costly components?\n\nPlease see the answer to a previous question below. The wall clock speedup of UNREAL vs A3C is currently about 8x.\n\n- If possible, please try to make it clearer in the abstract / intro that the agent is learning different policies for each task. When I read in the abstract that the agent \"also maximises many other pseudo-reward functions simultaneously by reinforcement learning\", my first understanding was that it learned a single policy to optimize all rewards together, and I realized my mistake only when reaching eq. 1.\n\nThanks for pointing this out. We\u2019ve tried to clarify this in the abstract.\n\n- The \"feature control\" idea is not validated empirically (the preliminary experiment in Fig. 5 is far from convincing as it only seems to help slightly initially). I like that idea but I am worried by the fact the task is changing during learning, since the extracted features are being modified. There might be stability / convergence issues at play here.\n\nThat is a fair point. We have updated the paper with improved results for feature control. We used a target network to make the features being controlled change less frequently during training. Feature control now works roughly as well as pixel control.\n\n- Since as you mentioned, \"the performance of our agents is still steadily improving\", why not keep them going to see how far they go? (at least the best ones)\n\nWe haven\u2019t done this because we were following the training protocol for A3C. It would be interesting to see where the scores saturate.\n\n- Why aren't the auxiliary tasks weight parameters (the lambda_*) hyperparameters to optimize? Were there any experiments to validate that using 1 was a good choice?\n\nWe do include the lambda for pixel control in the hyperparameter search. The parameter ranges are given in the Appendix.\n\n- Please mention the fact that auxiliary tasks are not trained with \"true\" Q-Learning since they are trained off-policy with more than one step of empirical rewards (as discussed in the OpenReview comments)\n\nWe do say that they are trained with n-step Q-learning. Do you mean that it is not a true off-policy learning method?\n\nAnonReviewer5\n\n- I think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.\n\nWe have noticed some qualitative differences between the learned policies of the UNREAL and A3C agents. The policies learned by UNREAL tend to \u201cpay attention\u201d to where they are going, for example they look straight ahead when moving forward while agents trained with A3C sometimes look in less informative directions (e.g. looking at the sky). They also seem to lead to agents which prefer to stay in the middle of corridors, bump much less frequently into walls.", "IS_META_REVIEW": false, "DATE": "17 Jan 2017", "is_meta_review": false}, {"TITLE": "Question about loss functions", "OTHER_KEYS": "(anonymous)", "comments": "Dear authors,\n\ncongratulations on this very interesting paper; using auxiliary tasks is a great idea that opens a lot of future research.\n\nHowever I'm having troubles understanding the exact objective of the UNREAL agent. In 3.4, you state that the agent optimises \"a single combined loss function\" (equation (2)) but that \"in practise, the loss is broken down into separate components\". So when exactly is which part optimised? \n\nFrom how I understand the paper now, it seems that in practise, the individual loss functions on the right-hand side of equation (2) are updated separately, i.e., during training, you switch between loss functions and this way optimise them \"simultaneously in parallel\" (p.4). Do I understand this correctly? Because then I don't understand how the lambdas come into play; in what way do they influence the learning (or evaluation) of the agent? Do you ever actually look at L_UNREAL, or evaluate how the agent performs on the auxiliary tasks? In figure 3, you're evaluating \"performance\", so I guess that's the score received while playing the game (which indirectly corresponds to L_A3C for the UNREAL agent)?", "IS_META_REVIEW": false, "DATE": "04 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "is_meta_review": false, "comments": "This paper proposes a way of adding unsupervised auxiliary tasks to a deep RL agent like A3C. Authors propose a bunch of auxiliary control tasks and auxiliary reward tasks and evaluate the agent in Labyrinth and Atari. Proposed UNREAL agent performs significantly better than A3C and also learns faster. This is definitely a good contribution to the conference. However, this is not a surprising result since adding additional auxiliary tasks that are relevant to the goal should always help in better and faster feature shaping. This paper is a proof of concept for this idea.\n\nThe paper is well written and easy to follow by any reader with deep RL expertise.\n\nCan authors comment about the computational resources needed to train the UNREAL agent?\n\nThe overall architecture is quite complicated. Are the authors willing to release the source code for their model?\n\n--------------------------------------------------------\nAfter rebuttal:\nNo change in the review.", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "21 Dec 2016 (modified: 24 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "A solid submission", "is_meta_review": false, "comments": "This paper is about improving feature learning in deep reinforcement learning, by augmenting the main policy's optimization problem with terms corresponding to (domain-independent) auxiliary tasks. These tasks are about control (learning other policies that attempt to maximally modify the state space, i.e. here the pixels), immediate reward prediction, and value function replay. Except for the latter, these auxiliary tasks are only used to help shape the features (by sharing the CNN+LSTM feature extraction network). Experiments show the benefits of this approach on Atari and Labyrinth problems, with in particular much better data efficiency than A3C.\n\nThe paper is well written, ideas are sound, and results pretty convincing, so to me this is a clear acceptance. At high level I only have few things to say, none being of major concern:\n- I believe you should say something about the extra computational cost of optimizing these auxiliary tasks. How much do you lose in terms of training speed? Which are the most costly components?\n- If possible, please try to make it clearer in the abstract / intro that the agent is learning different policies for each task. When I read in the abstract that the agent \"also maximises many other pseudo-reward functions simultaneously by reinforcement learning\", my first understanding was that it learned a single policy to optimize all rewards together, and I realized my mistake only when reaching eq. 1.\n- The \"feature control\" idea is not validated empirically (the preliminary experiment in Fig. 5 is far from convincing as it only seems to help slightly initially). I like that idea but I am worried by the fact the task is changing during learning, since the extracted features are being modified. There might be stability / convergence issues at play here.\n- Since as you mentioned, \"the performance of our agents is still steadily improving\", why not keep them going to see how far they go? (at least the best ones)\n- Why aren't the auxiliary tasks weight parameters (the lambda_*) hyperparameters to optimize? Were there any experiments to validate that using 1 was a good choice?\n- Please mention the fact that auxiliary tasks are not trained with \"true\" Q-Learning since they are trained off-policy with more than one step of empirical rewards (as discussed in the OpenReview comments)\n\nMinor stuff:\n- \"Policy gradient algorithms adjust the policy to maximise the expected reward, L_pi = -...\" => that's actually a loss to be minimized\n- In eq. 1 lambda_c should be within the sum\n- Just below eq. 1 r_t^(c) should be r_t+k^(c)\n- Figure 2 does not seem to be referenced in the text, also Figure 1(d) should be referenced in 3.3\n- \"the features discovered in this manner is shared\" => are shared\n- The text around eq. 2 refers to the loss L_PC but that term is not defined and is not (explicitly) in eq. 2\n- Please explain what \"Clip\" means for dueling networks in the legend of Figure 3\n- I would have liked to see more ablated versions on Atari, to see in particular if the same patterns of individual contribution as on Labyrinth were observed\n- In the legend of Figure 3 the % mentioned are for Labyrinth, which is not clear from the text.\n- In 4.1.2: \"Figure 3 (right) shows...\" => it is actually the top left plot of the figure. Also later \"This is shown in Figure 3 Top\" should be Figure 3 Top Right.\n- \"Figure 5 shows the learning curves for the top 5 hyperparameter settings on three Labyrinth navigation levels\" => I think it is referring to the left and middle plots of the figure, so only on two levels (the text above might also need fixing)\n- In 4.2: \"The left side shows the average performance curves of the top 5 agents for all three methods the right half shows...\" => missing a comma or something after \"methods\"\n- Appendix: \"Further details are included in the supplementary materials.\" => where are they?\n- What is the value of lambda_PC? (=1 I guess?)\n\n[Edit] I know some of my questions were already answered in Comments, no need to re-answer them", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "TITLE": "Review", "is_meta_review": false, "comments": "This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features.\nThey propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features. They also propose a supervised regression task, predict immediate reward following a sequence of events. The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance.\nSuch agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations. \n\nThis work contrasts with traditional \"passive\" unsupervised or model-based learning. Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.\n\nTo me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the \"possibility\" of control that an agent has over the environment.\nThe proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful. For example L_PC should be explicitly mentioned, before reaching the appendix. Otherwise this work is clear and easily understandable by readers familiar with Deep RL.\nThe methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found.\nI think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 8, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Evaluation metric", "OTHER_KEYS": "(anonymous)", "comments": "The authors report \"performance over last 100 episodes of the top-3 jobs at every point in training.\" While there is no information on how many jobs total are used, this does not seem to be a statistically sound evaluation metric, since it is not measuring average case performance. The paper essentially reports a max over a large number of stochastic runs of the algorithm. In this case, wouldn't an algorithm that simply injects noise into the gradients eventually attain the best result given enough jobs? A more generous interpretation is that the authors reported the top-3 jobs for *entire* training run, but even that seems not particularly sound, since the claim is that the proposed method achieves *faster* learning -- in that case, shouldn't you also factor in the cost of all the other jobs that did not succeed?", "IS_META_REVIEW": false, "DATE": "12 Dec 2016", "is_meta_review": false}, {"TITLE": "A few questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer5", "comments": "", "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "is_meta_review": false}, {"TITLE": "n-step Q-Learning for auxiliary tasks", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "", "IS_META_REVIEW": false, "DATE": "25 Nov 2016", "is_meta_review": false}, {"TITLE": "Equation 1 and supplementary material", "OTHER_KEYS": "(anonymous)", "comments": "Very interesting paper! It is quite stunning to see these improvements over A3C.\n\nPerhaps the choice of \\lambda_c in equation 1 in front of the sum is somewhat confusing, because the use of a lower case c seems to suggest that \\lambda_c is actually different for each task (although then it should not be in front of the sum). If I understood things correctly (also regarding equation 2), there is only a single weighting \\lambda parameter for the entire set of auxiliary control tasks. So wouldn't \\lambda_\\mathcal{C} be a better name for this parameter or am I misinterpreting things?\n\nAlso, the appendix mentions 'Further details are included in the supplementary materials'. Perhaps I missed something, but if not, when/where will they be available?  ", "IS_META_REVIEW": false, "DATE": "25 Nov 2016 (modified: 27 Nov 2016)", "is_meta_review": false}, {"TITLE": "Misleading first line of abstract", "OTHER_KEYS": "(anonymous)", "comments": "Hi, I believe the first line of the abstract: \"Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward\" is slightly misleading. Due to reward clipping done by most DRL algorithms, what is maximized is discounted rewards frequency and not discounted cumulative rewards.", "IS_META_REVIEW": false, "DATE": "20 Nov 2016", "is_meta_review": false}, {"TITLE": "Questions", "OTHER_KEYS": "Karthik R Narasimhan", "comments": "Hi, \nVery nice work, with a neat jump in performance! Just had a couple questions:\n1. I understand the idea behind 'pixel control' but I don't quite get the motivation behind adding the 'feature control' task. Could you provide some intuition as to why maximizing the hidden unit activations would help learning? Also, from figure 5(c), it looks like this doesn't help much?\n2. Have you tried playing with the skewed sampling parameter (0.5)? We had experimented with a very similar scheme, prioritized sampling (Narasimhan et al., 2015), and found that tuning this parameter did give us some gains. \n\nReferences:\nKarthik Narasimhan, Tejas Kulkarni, Regina Barzilay. Language Understanding for Text-based Games Using Deep Reinforcement Learning. Proceedings of EMNLP, 2015 ", "IS_META_REVIEW": false, "DATE": "18 Nov 2016", "is_meta_review": false}], "SCORE": 8, "authors": "Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu", "accepted": true, "id": ""}
