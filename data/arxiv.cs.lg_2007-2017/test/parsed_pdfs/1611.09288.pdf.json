{
  "name" : "1611.09288.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dense Prediction on Sequences with Time-Dilated Convolutions for Speech Recognition",
    "authors" : [ "Tom Sercu", "Vaibhava Goel" ],
    "emails" : [ "Tom.Sercu1@ibm.com", "vgoel@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years. Many computer vision problems fall into one of two problem types: the first is classification, where a single label is produced per image, the second dense pixelwise prediction, where a label is produced for each pixel in the image. Examples of dense prediciton are semantic segmentation, depth map prediction, optical flow, surface normal prediction, etc. Efficient convolutional architectures allow to produce a full image sized output rather than predicting the values for each pixel separately from a small patch centered around the pixel. In this paper we argue that we should look at acoustic modeling in speech as a dense prediction task on sequences. This is in contrast to the the usual viewpoint of “framewise classification”, indicating the cross-entropy training stage where a context-window is used as input and the network predicts only for the center frame. However, during all other stages, we want the acoustic model to be applied to a sequence, and produce a sequence of predictions. This is the case during sequence training, test time, or in an end-to-end training setting. Similar to convolutional architectures for dense prediction in computer vision, we focus our efforts on convolutional architectures that process an utterance at once and produce a sequence of labels as output, rather than “splicing” up the utterance, i.e. labeling each frame independently from a small window around it.\nThere are four main advantages to convolutional architectures that allow efficient evaluation of full utterance (without need of splicing) in this dense prediction viewpoint:\n• Computational efficiency: processing a spliced utterance requires window_size times more floating point operations.\n• Batch normalization can easily be adopted during sequence training (or end to end training), which we will show gives strong improvements (as outlined in [8]).\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 1.\n09 28\n8v 2\n[ cs\n.C L\n] 1\n4 D\nec 2\n• The main architectural novelty of this paper is that we can allow for strided pooling in time. In the next two sections, we will adopt a recent technique from dense prediction, named dilated convolutions, for CNN acoustic models to enable strided pooling in time. Experiments and results for this new model are in section 4.\n• We will show a unifying viewpoint with Stacked Bottleneck Networks, and discuss the relevance for end-to-end models with convolutional layers in section 5."
    }, {
      "heading" : "2 Related work: Pooling in CNNs for dense prediction on images",
      "text" : "Pooling with stride is an essential ingredient of any classification CNN, allowing to access more context on higher feature maps, while reducing the spatial resolution before it is absorbed into the fully connected layers. However, for dense pixelwise prediction tasks, it is less straightforward how to deal with downsampling: on the one hand downsampling allows for a “global view” by having large receptive fields at low resolution, on the other hand we also need detail on a small scale, i.e. we need the high resolution information.\nTo incorporate both global and local information, downsampling pooling has been incorporated in dense prediction networks in several ways. Firstly, many methods involve upsampling lower resolution feature maps, usually combined with some higher resolution feature maps. In [9], an image is processed at three different scales with three different CNNs, after which the output feature maps are merged. The Fully Convolutional Networks (FCNs) from [10] use a VGG classification network as basis, introducing skip connections to merge hi-res lower layers with upsampled low-res layers from deeper in the network. SegNet [11] uses a encoder-decoder structure, in which upsampling is done with max-unpooling [12], i.e. by remembering the max location of the encoder’s pooling layers. A second way of using CNNs with strided pooling for dense prediction was proposed in [3]: at every pooling layer with stride s× s, the input is duplicated s× s times, but shifted with offset (∆x,∆y) ∈ [0 . . . s− 1]× [0 . . . s− 1]. After the convolutional stages, the output is then interleaved to recover the full resolution. A third way (which we will use) is called spatial dilated convolutions, which keeps the feature maps in their original resolution. The idea is to replace the pooling with stride s by pooling with stride 1, then dilate all convolutions with a factor s, meaning that s−1s values get skipped. This was called filter rarefaction in [10], introduced as “d-regularly sparse kernels” in [13], and dubbed spatial dilated convolutions in [14]. It was noted [3, 10] that this method is equivalent to shift-and-interleave, though more intuitive. The recent WaveNet work [15] uses dilated convolutions for a generative model of audio."
    }, {
      "heading" : "3 Time-dilated convolutions",
      "text" : "Previous work on CNNs for acoustic modeling [5, 6] eliminated the possibility of strided pooling in time because of the downsampling effect. Recent work [7, 8] shows a significant performance boost by using pooling in time during cross-entropy training, however sequence training is prohibitively expensive since an utterance has to be spliced into uttLen independent windows. By adapting the notion of dense prediction, we propose to allow pooling in time while maintaining efficient full-utterance processing, by using an asymmetric version of spatial dilated convolution with dilation in the time direction but not in the frequency direction, which we appropriately call time-dilated convolutions.\nThe problem with strided pooling in time is that the length of the output sequence is shorter than the length of the input sequence with a factor 2(p), assuming p pooling layers with stride 2. For recurrent end-to-end networks typically a factor 4 size reduction is accepted [16, 17] which limits the number of pooling layers to 2, while in the hybrid NN/HMM framework, pooling is not acceptable. Essentially we need a way to do strided pooling in time, while keeping the resolution. We tackle this problem with a 1D version of sparse kernels [13], or equivalently spatial dilated convolutions [14].\nConsider the simple toy CNN (conv3, pool2-s2, conv3) in Figure 1 (a), which takes in a context window of 8 frames and produces a single output. Let’s consider applying this CNN to a full utterance of length 10 (padded to length 16), as in figure (b). The top row of blue outputs is downsampled with factor 2 because of the strided pooling, so the output sequence length does not match the number of targets (i.e. input size). The solution of this problem is visualized in Figure 1 (c). First, we pool without stride, which preserves the resolution after pooling. However, now our consecutive convolutional layer needs to be modified; specifically the kernel has to skip every other value, in order to ignore the new (dark blue) values which came between the values. This is dilation (or sparsification) of the kernel with a factor 2 in the time direction. Formally a 1-D discrete convolution ∗l with dilation l which convolves signal F with kernel k with size r is defined as (F ∗l k)(p) = ∑ s+lt=p F (s)k(t), t ∈ [−r, r].\nIn general, the procedure to change a CNN with time-pooling from the cross-entropy training (classification) to dense prediction stage for sequence training and testing is as follows. Change pooling layers from stride s to stride 1, and multiply the dilation factor of all following convolutions with factor s. After this, any convolution coming after p pooling layers with original stride s, will have the dilation factor sp. Fully connected layers are equivalent to, and can be trivially replaced by, convolutional layers with kernel 1× 1 (except the first convolution which has kernel size matching the output of the conv stack before being flattened for the fully connected layers). This dilating procedure is how a VGG classification network is adapted for semantic segmentation [13, 14].\nUsing time-dilated convolutions, the feature maps and output can keep the full resolution of the input, while pooling with stride. With pooling, the receptive field in time of the CNN can be larger than the same network without pooling. This allows to combine the performance gains of pooling [7], while maintaining the computational efficiency and ability to apply batch normalization [8]."
    }, {
      "heading" : "4 Experiments and results",
      "text" : "We trained a VGG style CNN [4] in the hybrid NN/HMM setting on the 2000h Switchboard+Fisher dataset. The architecture and training method is similar to our earlier papers [7, 8], and is based on the setup described in [21]. Our input features are VTLN-warped logmel with ∆,∆∆, the outputs are 32k tied CD states from forced alignment. Table 1 fully specifies the CNN when training on windows and predicting the center frame. Corresponding to the observations in [8], we do not pad in time,\nLayer Output: fmaps × f × T Input window 3× 64× 48 conv 7× 7 64× 64× 42 pool 2× 1 64× 32× 42 conv 3× 3 64× 32× 40 conv 3× 3 64× 32× 38 conv 3× 3 64× 32× 36 pool 2× 1 64× 16× 36 conv 3× 3 128×16×34 conv 3× 3 128×16×32 conv 3× 3 128×16×30 pool 2× 1 128× 8× 30 conv 3× 3 256× 8× 28 conv 3× 3 256× 8× 26 conv 3× 3 256× 8× 24 pool 2× 2 256× 4× 12 conv 3× 3 512× 4× 10 conv 3× 3 512× 4× 8 conv 3× 3 512× 4× 6 pool 2× 2 512× 2× 3 3× FC 2048 FC 1024 FC 32000\nTable 1: CNN architecture.\nSWB CH XE ST XE ST\nClassic 512 CNN [18] 12.6 10.4 IBM 2016 RNN+VGG+LSTM [19] 8.6 † 14.4 † MSR 2016 ResNet * [20] 8.9 MSR 2016 LACE * [20] 8.6 MSR 2016 BLSTM * [20] 8.7 VGG (pool, inefficient) [19] 10.2 9.4 16.3 16.0 VGG (no pool) [8] 10.8 9.7 17.1 16.7 VGG-10 + BN (no pool) [8] 10.8 9.5 17.0 16.3 VGG-13 + BN (no pool) 10.3 9.0 16.5 16.4 VGG-13 + BN + pool 9.5 8.5 15.1 15.4 VGG-13 + BN + pool (uncouple CH acwt) 14.8 15.2\nTable 2: Results with small LM (4M n-grams)\nSWB CH\nIBM 2015 DNN+RNN+CNN [21] 8.8 † 15.3 † IBM 2016 RNN+VGG+LSTM [19] 7.6 † 13.7 † MSR 2016 ResNet [20] 8.6 14.8 MSR 2016 LACE [20] 8.3 14.8 MSR 2016 BLSTM [20] 8.7 16.2 VGG-13 + BN (no pool) 8.1 15.9 VGG-13 + BN + pool 7.7 14.5 VGG-13 + BN + pool (uncouple CH acwt) 14.4\nTable 3: Results with big LM (36M n-grams)\nthough we do pad in the frequency direction. Training followed the standard two-stage scheme, with first 1600M frames of cross-entropy training (XE) followed by 310M frames of Sequence Training (ST). XE training was done with SGD with nesterov acceleration, with learning rate decaying from 0.03 to 9e−4 over 600M frames. We use the data balancing from [7] with exponent γ = 0.8. We report results on Hub5’00 (SWB and CH part) after decoding using the standard small 4M n-gram language model with a 30.5k word vocabulary. We saw slight improvement in results when decoding with exponent γ on the prior lower than what is used during training. As mentioned in section 3, we use batch normalization in our network, where the mean and variance statistics are accumulated over both the feature maps and the frequency direction. The selection of models, decoding prior and acoustic weight happened by decoding on rt02 as heldout set.\nThe result after XE and ST are presented in Tables 2 and 3. Baseline with * from personal communication with the authors. Baseline with † means system combination. Note that the baselines from [20] use slightly smaller LMs: 3.4M n-grams for small LM (table 2) and 16M n-grams for big LM (table 3). We note that one typically does subsequent rescoring with more advanced language models like RNN or LSTM LMs; this way in [22] a single model performance of 6.6 is achieved, starting from 8.6. With just n-gram decoding, this result is to our knowledge the best published single model."
    }, {
      "heading" : "5 Relation to other models",
      "text" : "Stacked bottleneck networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are a influential acoustic model in hybrid NN/HMM speech recognition. SBNs are typically seen as two consecutive DNNs, each stage separately trained discriminatively with a bottleneck (small hidden layer). The first DNN sees the input features, while the second DNN gets the bottleneck features from the first DNN as input. Typically, the second DNN gets 5 bottleneck features with stride 5, i.e. features from position {−10,−5, 0, 5, 10} relative to the center [25]. In [24], it was pointed out that this SBN is convolutional and one can backpropagate through both stages together.\nIn fact this multi-stage SBN architecture is a special case of a CNN with time-dilated convolution. Specifically, the DNN is equivalent to a CNN with a large first kernel followed by all 1× 1 kernels. The second DNN is exactly equivalent to a CNN with the first kernel having size 5 and dilation factor 5 in the time direction. The layers after the bottleneck in the first DNN form an auxilary classifier. This realization prompts a number of directions in which the SBNs can be extended. Firstly, by avoiding the large kernel in the first convolutional layer, it is possible to keep time and frequency structure in the internal representations in future layers, enabling increased depth. Secondly, rather than increasing the time-dilation factor to 5 at once, it seems more natural to gradually increase the time-dilation factor throughout the depth of the network.\nConvolutional networks are also used in end-to-end models for speech recognition. Both the CLDNN architecture [17] and Deep Speech 2 (DS2) [16] combine a convolutional network as first stage with LSTM and fully connected (DNN) output layers. In Wav2Letter [27], a competitive end-to-end model is presented which is fully convolutional. Both DS2 and Wav2Letter do a certain amount of downsampling through pooling or striding, which can be accepted when training with a CTC (or AutoSeg [27]) criterion since it doesn’t require the output to be the same length as the input. However, DS2 does report a degradation on English, which they work around using grapheme bigram targets.\nThe time-dilated convolutions we introduced, could improve these end to end models in two ways: either, one could allow the same amount of pooling while keeping a higher resolution. Alternatively, one could keep the same resolution, but expand the receptive field by adding more time-dilated convolution layers, which gives access to a broader context in the CNN layers. In conclusion, this work is both relevant to end-to-end models and to hybrid HMM/NN models."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We drew the parallel between dense prediction in computer vision and framewise sequence labeling, both in the HMM/NN and end-to-end setting. This provided us with the tool (time-dilated convolutions) to adopt pooling in time to CNN acoustic models, while maintaining efficient processing and batch normalization on full utterances. On Hub5’00 we brought down the WER from 9.4% in previous work to 8.5%, a 10% relative improvement. With a big (36M n-gram) language model, we achieve 7.7% WER, the best single model single-pass performance reported so far."
    } ],
    "references" : [ {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 1097–1105.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "Pierre Sermanet", "David Eigen", "Xiang Zhang", "Michaël Mathieu", "Rob Fergus", "Yann LeCun" ],
      "venue" : "arXiv:1312.6229, 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "CoRR arXiv:1409.1556, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition",
      "author" : [ "Ossama Abdel-Hamid", "Abdel-rahman Mohamed", "Hui Jiang", "Gerald Penn" ],
      "venue" : "Proc. ICASSP, 2012.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep convolutional neural networks for lvcsr",
      "author" : [ "Tara N Sainath", "Abdel-rahman Mohamed", "Brian Kingsbury", "Bhuvana Ramabhadran" ],
      "venue" : "Proc. ICASSP, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Very deep multilingual convolutional neural networks for lvcsr",
      "author" : [ "Tom Sercu", "Christian Puhrsch", "Brian Kingsbury", "Yann LeCun" ],
      "venue" : "Proc. ICASSP, 2016.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Advances in very deep convolutional neural networks for lvcsr",
      "author" : [ "Tom Sercu", "Vaibhava Goel" ],
      "venue" : "Proc. Interspeech, 2016.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Scene parsing with multiscale feature learning, purity trees, and optimal covers",
      "author" : [ "Clément Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun" ],
      "venue" : "Proc. ICML, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Jonathan Long", "Evan Shelhamer", "Trevor Darrell" ],
      "venue" : "CVPR, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Segnet: A deep convolutional encoder-decoder architecture for image segmentation",
      "author" : [ "Vijay Badrinarayanan", "Alex Kendall", "Roberto Cipolla" ],
      "venue" : "arXiv:1511.00561, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adaptive deconvolutional networks for mid and high level feature learning",
      "author" : [ "Matthew D Zeiler", "Graham W Taylor", "Rob Fergus" ],
      "venue" : "2011 International Conference on Computer Vision. IEEE, 2011, pp. 2018–2025.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Highly efficient forward and backward propagation of convolutional neural networks for pixelwise classification",
      "author" : [ "Hongsheng Li", "Rui Zhao", "Xiaogang Wang" ],
      "venue" : "arXiv:1412.4526, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-scale context aggregation by dilated convolutions",
      "author" : [ "Fisher Yu", "Vladlen Koltun" ],
      "venue" : "proc ICLR, 2016.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Wavenet: A generative model for raw audio",
      "author" : [ "Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu" ],
      "venue" : "arXiv:1609.03499, 2016.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "author" : [ "Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos" ],
      "venue" : "CoRR arXiv:1512.02595, 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Convolutional, long short-term memory, fully connected deep neural networks",
      "author" : [ "Tara N Sainath", "Oriol Vinyals", "Andrew Senior", "Haşim Sak" ],
      "venue" : "proc. ICASSP, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Joint training of convolutional and non-convolutional neural networks",
      "author" : [ "Hagen Soltau", "George Saon", "Tara N Sainath" ],
      "venue" : "to Proc. ICASSP, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The ibm 2016 english conversational telephone speech recognition system",
      "author" : [ "George Saon", "Tom Sercu", "Steven Rennie", "Hong-Kwang J Kuo" ],
      "venue" : "Proc. Interspeech, 2016.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The microsoft 2016 conversational speech recognition system",
      "author" : [ "W Xiong", "J Droppo", "X Huang", "F Seide", "M Seltzer", "A Stolcke", "D Yu", "G Zweig" ],
      "venue" : "arXiv:1609.03528, 2016.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The ibm 2015 english conversational telephone speech recognition system",
      "author" : [ "George Saon", "Hong-Kwang J Kuo", "Steven Rennie", "Michael Picheny" ],
      "venue" : "Proc. Interspeech, 2015.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Achieving human parity in conversational speech recognition",
      "author" : [ "W Xiong", "J Droppo", "X Huang", "F Seide", "M Seltzer", "A Stolcke", "D Yu", "G Zweig" ],
      "venue" : "arXiv:1610.05256, 2016.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Investigation into bottle-neck features for meeting speech recognition",
      "author" : [ "Frantisek Grezl", "Martin Karafiát", "Lukas Burget" ],
      "venue" : "Proc. Interspeech, 2009.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Convolutive bottleneck network features for lvcsr",
      "author" : [ "Karel Veselỳ", "Martin Karafiát", "František Grézl" ],
      "venue" : "ASRU, 2011.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Adaptation of multilingual stacked bottle-neck neural network structure for new language",
      "author" : [ "Frantisek Grézl", "Martin Karafiát", "Karel Veselỳ" ],
      "venue" : "Proc. ICASSP, 2014.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Hierarchical bottle neck features for lvcsr",
      "author" : [ "Christian Plahl", "Ralf Schlüter", "Hermann Ney" ],
      "venue" : "Interspeech, 2010, pp. 1197–1200.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Wav2letter: an end-to-end convnet-based speech recognition system",
      "author" : [ "Ronan Collobert", "Christian Puhrsch", "Gabriel Synnaeve" ],
      "venue" : "arXiv:1609.03193, 2016. 5",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.",
      "startOffset" : 84,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.",
      "startOffset" : 84,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.",
      "startOffset" : 84,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "• Batch normalization can easily be adopted during sequence training (or end to end training), which we will show gives strong improvements (as outlined in [8]).",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "In [9], an image is processed at three different scales with three different CNNs, after which the output feature maps are merged.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "The Fully Convolutional Networks (FCNs) from [10] use a VGG classification network as basis, introducing skip connections to merge hi-res lower layers with upsampled low-res layers from deeper in the network.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "SegNet [11] uses a encoder-decoder structure, in which upsampling is done with max-unpooling [12], i.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "SegNet [11] uses a encoder-decoder structure, in which upsampling is done with max-unpooling [12], i.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "A second way of using CNNs with strided pooling for dense prediction was proposed in [3]: at every pooling layer with stride s× s, the input is duplicated s× s times, but shifted with offset (∆x,∆y) ∈ [0 .",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "This was called filter rarefaction in [10], introduced as “d-regularly sparse kernels” in [13], and dubbed spatial dilated convolutions in [14].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "This was called filter rarefaction in [10], introduced as “d-regularly sparse kernels” in [13], and dubbed spatial dilated convolutions in [14].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "This was called filter rarefaction in [10], introduced as “d-regularly sparse kernels” in [13], and dubbed spatial dilated convolutions in [14].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "It was noted [3, 10] that this method is equivalent to shift-and-interleave, though more intuitive.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "It was noted [3, 10] that this method is equivalent to shift-and-interleave, though more intuitive.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "The recent WaveNet work [15] uses dilated convolutions for a generative model of audio.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "Previous work on CNNs for acoustic modeling [5, 6] eliminated the possibility of strided pooling in time because of the downsampling effect.",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "Previous work on CNNs for acoustic modeling [5, 6] eliminated the possibility of strided pooling in time because of the downsampling effect.",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "Recent work [7, 8] shows a significant performance boost by using pooling in time during cross-entropy training, however sequence training is prohibitively expensive since an utterance has to be spliced into uttLen independent windows.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "Recent work [7, 8] shows a significant performance boost by using pooling in time during cross-entropy training, however sequence training is prohibitively expensive since an utterance has to be spliced into uttLen independent windows.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "For recurrent end-to-end networks typically a factor 4 size reduction is accepted [16, 17] which limits the number of pooling layers to 2, while in the hybrid NN/HMM framework, pooling is not acceptable.",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 16,
      "context" : "For recurrent end-to-end networks typically a factor 4 size reduction is accepted [16, 17] which limits the number of pooling layers to 2, while in the hybrid NN/HMM framework, pooling is not acceptable.",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "We tackle this problem with a 1D version of sparse kernels [13], or equivalently spatial dilated convolutions [14].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "We tackle this problem with a 1D version of sparse kernels [13], or equivalently spatial dilated convolutions [14].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "This dilating procedure is how a VGG classification network is adapted for semantic segmentation [13, 14].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 13,
      "context" : "This dilating procedure is how a VGG classification network is adapted for semantic segmentation [13, 14].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "This allows to combine the performance gains of pooling [7], while maintaining the computational efficiency and ability to apply batch normalization [8].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "This allows to combine the performance gains of pooling [7], while maintaining the computational efficiency and ability to apply batch normalization [8].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "We trained a VGG style CNN [4] in the hybrid NN/HMM setting on the 2000h Switchboard+Fisher dataset.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "The architecture and training method is similar to our earlier papers [7, 8], and is based on the setup described in [21].",
      "startOffset" : 70,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "The architecture and training method is similar to our earlier papers [7, 8], and is based on the setup described in [21].",
      "startOffset" : 70,
      "endOffset" : 76
    }, {
      "referenceID" : 20,
      "context" : "The architecture and training method is similar to our earlier papers [7, 8], and is based on the setup described in [21].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "Corresponding to the observations in [8], we do not pad in time,",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 17,
      "context" : "SWB CH XE ST XE ST Classic 512 CNN [18] 12.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "4 IBM 2016 RNN+VGG+LSTM [19] 8.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 19,
      "context" : "4 † MSR 2016 ResNet * [20] 8.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : "9 MSR 2016 LACE * [20] 8.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "6 MSR 2016 BLSTM * [20] 8.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "7 VGG (pool, inefficient) [19] 10.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "0 VGG (no pool) [8] 10.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "7 VGG-10 + BN (no pool) [8] 10.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 20,
      "context" : "Table 2: Results with small LM (4M n-grams) SWB CH IBM 2015 DNN+RNN+CNN [21] 8.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "3 † IBM 2016 RNN+VGG+LSTM [19] 7.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "7 † MSR 2016 ResNet [20] 8.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "8 MSR 2016 LACE [20] 8.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "8 MSR 2016 BLSTM [20] 8.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "We use the data balancing from [7] with exponent γ = 0.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "Note that the baselines from [20] use slightly smaller LMs: 3.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : "We note that one typically does subsequent rescoring with more advanced language models like RNN or LSTM LMs; this way in [22] a single model performance of 6.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 22,
      "context" : "Stacked bottleneck networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are a influential acoustic model in hybrid NN/HMM speech recognition.",
      "startOffset" : 34,
      "endOffset" : 46
    }, {
      "referenceID" : 23,
      "context" : "Stacked bottleneck networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are a influential acoustic model in hybrid NN/HMM speech recognition.",
      "startOffset" : 34,
      "endOffset" : 46
    }, {
      "referenceID" : 24,
      "context" : "Stacked bottleneck networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are a influential acoustic model in hybrid NN/HMM speech recognition.",
      "startOffset" : 34,
      "endOffset" : 46
    }, {
      "referenceID" : 25,
      "context" : "Stacked bottleneck networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are a influential acoustic model in hybrid NN/HMM speech recognition.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "features from position {−10,−5, 0, 5, 10} relative to the center [25].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 23,
      "context" : "In [24], it was pointed out that this SBN is convolutional and one can backpropagate through both stages together.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "Both the CLDNN architecture [17] and Deep Speech 2 (DS2) [16] combine a convolutional network as first stage with LSTM and fully connected (DNN) output layers.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "Both the CLDNN architecture [17] and Deep Speech 2 (DS2) [16] combine a convolutional network as first stage with LSTM and fully connected (DNN) output layers.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : "In Wav2Letter [27], a competitive end-to-end model is presented which is fully convolutional.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 26,
      "context" : "Both DS2 and Wav2Letter do a certain amount of downsampling through pooling or striding, which can be accepted when training with a CTC (or AutoSeg [27]) criterion since it doesn’t require the output to be the same length as the input.",
      "startOffset" : 148,
      "endOffset" : 152
    } ],
    "year" : 2016,
    "abstractText" : "In computer vision pixelwise dense prediction is the task of predicting a label for each pixel in the image. Convolutional neural networks achieve good performance on this task, while being computationally efficient. In this paper we carry these ideas over to the problem of assigning a sequence of labels to a set of speech frames, a task commonly known as framewise classification. We show that dense prediction view of framewise classification offers several advantages and insights, including computational efficiency and the ability to apply batch normalization. When doing dense prediction we pay specific attention to strided pooling in time and introduce an asymmetric dilated convolution, called time-dilated convolution, that allows for efficient and elegant implementation of pooling in time. We show results using time-dilated convolutions in a very deep VGG-style CNN with batch normalization on the Hub5 Switchboard-2000 benchmark task. With a big n-gram language model, we achieve 7.7% WER which is the best single model single-pass performance reported so far.",
    "creator" : "LaTeX with hyperref package"
  }
}