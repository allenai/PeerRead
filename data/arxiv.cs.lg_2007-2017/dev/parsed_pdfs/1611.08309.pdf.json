{
  "name" : "1611.08309.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Human Intellect and Machine Failures: Troubleshooting Integrative Machine Learning Systems",
    "authors" : [ "Besmira Nushi", "Ece Kamar", "Eric Horvitz", "Donald Kossmann" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Advances in machine learning have enabled the design of integrative systems that perform sophisticated tasks via the execution of analytical pipelines of components. Despite the widespread adoption of such systems, current applications lack the ability to understand, diagnose, and fix their own mistakes which consequently reduces users’ trust and limits future improvements. Therefore, the problem of understanding and troubleshooting failures of machine learning systems is of particular interest in the community (Sculley et al. 2015). Our work studies component-based machine learning systems composed of specialized components that are individually trained for solving specific problems and work altogether for solving a single complex task. We analyze how the special characteristics of these integrated learning systems, including continuous (non-binary) success measures, entangled component design, and non-monotonic error propagation, make it challenging to assign blame to individual components. These challenges hinder future system improvements as designers lack an understanding of how different potential fixes on components may improve the overall system output. Approach. We introduce a troubleshooting methodology which relies on crowdworkers to identify and fix mistakes\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nin existing systems. Human intervention is crucial to the approach as human fixes simulate improved component output that cannot be produced otherwise without significant system development efforts. Figure 1 shows the main flow of our approach. First, workers evaluate the system output without any fix to analyze the current system state. To simulate a component fix, the input and output of the component accompanied with the fix description are sent to a crowdsourcing platform as microtasks. Once workers apply the targeted fixes for a component, the fixed output is integrated back into the running system, which thereby generates an improved version of the component. The system is executed as a simulation with the injected fix and the output is evaluated again via crowdworkers. The overall process collects a valuable set of log data on system failures, human fixes, and their impact on the final output. This data can then be analyzed to identify the most effective combination of component improvements to guide future development efforts. Case study. We apply our methodology to a state-of-the-art integrative learning system developed to automatically caption images (Fang et al. 2015). The system involves three machine learning components in a pipeline, including visual detection, language generation, and caption ranking. The multimodal nature of this case study allows us to demonstrate the applicability of the approach for components processing different forms of data and carrying out various tasks. The methodology reveals new insights on the error dynamics previously unknown to the designers, and offers recommendations on how to best improve the system. For example, in contrast to what system designers had assumed, improving the Reranker is more effective than improving the ar X iv :1 61 1. 08 30 9v 1\n[ cs\n.L G\n] 2\n4 N\nov 2\n01 6\nVisual Detector. Experiments highlight the benefits of making informed decisions about component fixes as their effectiveness varies greatly (18%, 3% and 27% for the three components respectively)."
    }, {
      "heading" : "Background and Problem Characterization",
      "text" : "We now define the problem of troubleshooting componentbased machine learning systems. We start by describing the image captioning system as a running example and then continue with the problem formalization."
    }, {
      "heading" : "Case Study: An Image Captioning System",
      "text" : "The system (Fang et al. 2015) that we use as a case study automatically generates captions as textual descriptions for images. This task has emerged as a challenge problem1 in artificial intelligence as it involves visual and language understanding and has multiple real-world applications, including the provision of descriptions for assisting visually impaired people. Figure 2 shows the system architecture, consisting of three machine learning components. The first and third components leverage convolutional neural networks combined with multiple instance learning (Zhang, Platt, and Viola 2005). The second one is a maximum-entropy language model (Berger, Pietra, and Pietra 1996). Visual Detector. The first component takes an image as an input and detects a list of words associated with recognition scores. The detector recognizes only a restricted vocabulary of the 1000 most common words in the training captions. Language Model. This component is a statistical model that generates likely word sequences as captions based on the words recognized from the Visual Detector, without having access to the input image. The set of the 500 most likely image captions and the respective log-likelihood scores are forwarded to the Caption Reranker. Caption Reranker. The task of the component is to rerank the captions generated from the Language Model and select the best match for the image. The Reranker uses multiple features among which the similarity between the vector rep-\n1http://mscoco.org/dataset/#captions-challenge2015\nresentations of images and captions. The caption with the highest ranking score is selected as the final best caption. Dataset. All components are individually trained on the MSCOCO dataset (Lin et al. 2014) which was built as an image captioning training set and benchmark. It contains 160,000 images as well as five human-generated captions for all images. We use images randomly sampled from the validation dataset to evaluate our approach."
    }, {
      "heading" : "Problem Characterization",
      "text" : "Problem context. This work studies machine learning systems consisting of several components designed to carry out specific tasks in the system. The system takes a set of data as system input and the individual components work together to produce a final system output. We assume that the system architecture is provided to the methodology by system designers by specifying:\n1. The set of system components along with the component input and output data types. 2. A set of directed communication dependencies between components denoting the input / output exchange. The whole set of dependencies defines the system execution workflow. In our methodology, we only handle acyclic dependencies but allow for branching.\nProblem definition. Troubleshooting of component-based machine learning systems can be decoupled to answering the following two questions: Question 1: How does the system fail? The system designer is interested in identifying and measuring the different types of system failures and their frequencies as well as the failures of individual components in the system context.\nQuestion 2: How to improve the system? System failures can be addressed by various potential fixes applicable to individual components. To guide future efforts on improving the system, the system designer is interested in knowing the effects of component fixes on: (i) specific input instances, and (ii) the overall system output quality. The first requirement is relevant to fine-grained instance-based debugging that a troubleshooting methodology should implement, while the second one provides guidance on future efforts to improve the whole system.\nThe first question aims at analyzing the current state of the system. The second question explores future opportunities and investigates the efficiency of different strategies to improve the system. Next, we examine the special characteristics of component-based machine learning systems that make the problem of troubleshooting challenging. These characteristics differentiate this problem from previous work on troubleshooting and motivate our methodology. Continuous quality measures. Uncertainty is inherent in machine learning components. When these components work together to solve complex tasks, the measure of quality for individual components and the system as a whole is no longer binary, rather it spans a wide spectrum. Therefore, the evaluation of these systems needs to go beyond accuracy metrics to deeper analysis of system behavior. For\ninstance, Figure 3 shows a few examples from image captioning where the system and component output varies in quality and the types of mistakes. Troubleshooting in this quality continuum where all components are only partially correct is non-trivial. Complex component entanglement. In component-based machine learning systems, components have complex influences on each other as they may be tightly coupled or the boundaries between their responsibilities may not be clear. When the quality of a component depends on the output of previous components, blame cannot be assigned to individual components without decoupling imperfection problems in component inputs. Figure 4 illustrates a typical scenario of component entanglement in the image captioning system. The final caption is clearly unsatisfactory as it mentions a non-existing object (blender). However, the Visual Detector assigns a low score to the word blender (0.57), which makes the detector only partially responsible for the mistake. The Language Model is also partially responsible as it creates a sentence with low commonsense awareness. Finally, the caption reranker chooses as the best caption a sentence that includes a word with a low score. In this example, errors from all components are interleaved and it is not possible to disentangle their individual impact on the final error. Non-monotonic error. We note that improving the outputs of components does not guarantee system improvement. On the contrary, doing so may lead to quality deterioration. For example, when components are tuned to suppress erroneous behavior of preceding components, applying fixes to the ear-\nlier ones may result to unknown failures. Figure 5 shows an example of non-monotonic error behavior. Here, the Visual Detector makes a mistake by including computer in the list. The initial assumption would be that if the list is fixed so that it contains only the prominent words, then the quality of the caption should increase. In reality, the caption after the fix is more erroneous than the original. Since the language model finds a teddy bear wearing glasses unlikely, it creates a caption that mentions a person instead."
    }, {
      "heading" : "Human-in-the-loop Methodology",
      "text" : "Due to these problem characteristics, blame assignment is challenging in integrative systems and analyzing only the current state of the system is not sufficient to develop strategies for system improvement. As shown in Figure 1, our methodology overcomes these challenges by introducing humans in the loop for: (i) simulating component fixes, and (ii) evaluating the system before and after fixes to directly measure the effect of future system improvements. Methodology setup. The troubleshooting methodology is applicable to systems that follow the assumptions of: (i) system modularity with clearly defined component inputs and outputs, and (ii) human interpretability of the component input / output. To apply the methodology to a new system, the system designer provides the set of components and their input/outputs within the system execution workflow. After identifying a list of component fixes that can potentially improve the system, the system designer formulates corresponding crowdsourcing tasks for these fixes and the over-\nall system evaluation. Both types of tasks should describe the high-level goal of the system, the context in which it operates as well as its requirements (e.g. an image captioning system designed to assist users with visual impairments). In addition, component fixing tasks should be appropriately formulated so that their expected output matches the output of implementable fixes that the system designer plans to test. Troubleshooting steps. The execution of the methodology is guided by the fix workflow, which is a combination of various component fixes to be evaluated. The system designer chooses which fix workflows to execute and evaluate for the purpose of troubleshooting. For a given fix workflow, the steps of our methodology are as follows: 1. Current system evaluation — workers assess the final\noutput of the current system on various quality measures. 2. Component fix simulation — for each fix in the workflow,\nworkers complete the respective micro-task for examining and correcting the component output. 3. Fix workflow execution — executing a fix workflow involves integrating the corrected outputs of each component into the system execution. 4. After-fix system evaluation — workers re-evaluate the new system output after incorporating component fixes. When a workflow includes fixes for multiple components, steps 2 and 3 need to be repeated so that the fixes of earlier components are reflected on the inputs of later components. Troubleshooting outcomes. Applying human fix workflows simulates improved component states and helps system designers to observe the effect of component fixes on system performance, overcoming the challenges raised by the problem characteristics. 1. Continuous quality measures — Comparing the system\nquality before and after various fix workflow executions not only can quantify the current quality of system and component output, but it can also isolate and quantify the effect of individual component fixes. For example, if many components are partially failing and are possibly responsible for a specific error, the system designer can test the respective fixes, systematically understand their impact, and decide which are the most promising ones. 2. Non-monotonic error — Non-monotonic error propagation can be disclosed when the overall system quality drops after a component fix. When such a behavior is observed, the system designer can conclude that although these fixes may improve the internal component state, they are not advisable to be implemented in the current state of the system as they produce negative artifacts in the holistic system. 3. Complex component entanglement — Entanglement detection requires the execution of workflows with different combinations of fixes to measure the individual and the joint effect of component fixes. For example, if two consecutive components are entangled, individual fixes in either one of them may not improve the final output. However, if both components are fixed jointly, this may trigger a significant improvement. The designer could also use this information to detect entanglement and potentially correct the system architecture in future versions."
    }, {
      "heading" : "Troubleshooting the Image Captioning System",
      "text" : "We now describe the customized crowdsourcing tasks for our case study for both system evaluation and component fixes. Table 1 lists all component fixes specifically designed for this case study. The task design is an iterative process in collaboration with system designers so that human fixes can appropriately simulate implementable improvements. System evaluation. The system evaluation task is designed to measure different quality metrics associated with captions as well as the overall human satisfaction. The task shows workers an image-caption pair and asks them to evaluate the following quality measures: accuracy (1-5 Likert scale), detail (1-5 Likert scale), language (1-5 Likert scale), commonsense (0-1), and general evaluation. For each measure, we provided a detailed description along with representative examples. However, we intentionally did not instruct workers for the general evaluation to prevent biasing them on which quality measure is more important (e.g. accuracy vs. detail). Visual Detector fixes. We designed two different tasks for fixing object and activity detections respectively represented by nouns and verbs in the word list. The object fix shows workers the input image with the list of nouns present in the visual detector output. Workers are asked to correct the list by either removing objects or adding new ones. The activity fix has a similar design. We intentionally group addition and removal fixes to simulate improvements of the Visual Detector in precision and recall as potential implementable fixes in this component. The result of any Visual Detector fix is a new word list which is passed to the Language Model together with the worker agreement scores (e.g. majority vote). Language Model fixes. These fixes are designed for removing sentences that are either not commonsense or not fluent. The tasks do not share the input image with the worker, as the Language Model itself does not have access to the image. In the commonsense fix, workers mark whether a caption describes a likely situation that makes sense in the real world. For example, the caption A cat playing a video game has no commonsense awareness in a general context. In the language fix, the goal is to evaluate the language fluency of captions in a 1-5 Likert scale. In addition, workers highlight problematic parts of the sentence which they think would make the caption fluent if fixed appropriately. The resulting list of problematic segments is a reusable resource to filter out captions that contain the same patterns.\nBoth fixes simulate improved versions of the Language Model that generate commonsense sentences in a fluent language. To integrate Language Model fixes in the system execution we exclude the noncommonsense and the non-fluent captions from the list forwarded to the Caption Reranker.\nCaption Reranker fixes. This task shows an image together with the corresponding top 10 captions from the Reranker (in random order), and asks workers to pick up to 3 captions that they think fit the image best. The answers are then aggregated via majority vote and the caption with the highest agreement is selected as the new system output."
    }, {
      "heading" : "Experimental Evaluation",
      "text" : "The evaluation of the captioning system with our methodology uses an Evaluation dataset of 1000 images randomly selected from the MSCOCO validation dataset. All experiments were performed on Amazon Mechanical Turk. We report the system quality based on human assessments. An additional analysis using automatic machine translation scores can be found in the appendix."
    }, {
      "heading" : "Current System State",
      "text" : "First, we evaluate the current system state as shown in Table 2. To gain a deeper understanding of the system performance, we divide the Evaluation data in two datasets: Satisfactory and Unsatisfactory based on the general evaluation score collected from the system evaluation task. We consider every answer in 1-3 as an unsatisfactory evaluation, and every other answer in 4-5 as satisfactory. All instances whose original caption reaches a majority agreement on being satisfactory belong to the Satisfactory dataset. The rest is classified as Unsatisfactory. Result: Only 57.8% of the images in the Evaluation dataset have a satisfactory caption. The comparison between the Satisfactory and Unsatisfactory partitions shows that the highest discrepancies happen for the accuracy and detail measures, highlighting the correlation of accuracy and detail with the overall satisfaction."
    }, {
      "heading" : "Current Component State",
      "text" : "An additional functionality of a human-assisted troubleshooting methodology is to evaluate the quality of the existing individual system components. Visual Detector Figure 6 shows the precision and recall of the Visual Detector for both objects and activities when compared to the human-curated lists created from the majority vote aggregation of multiple workers’ fixes. In the same figure, we also show the size of the lists before and after applying human fixes. Result: The Visual Detector produces longer lists for objects than for activities but with lower precision and recall.\nCommonsense fix\nLanguage fix Both fixes\nTop1-Eval. 8.0% 22.9% 25.0% Top10-Eval. 8.6% 21.7% 27.1% Top1-Val. 3.0% 15.2% 16.1% Top10-Val. 2.6% 14.2% 14.9%"
    }, {
      "heading" : "Component Fixes and System Improvement",
      "text" : "Visual Detector fixes. Table 4 shows results from applying the four types of fixes on the Visual Detector. These fixes increase the number of satisfactory captions in the dataset up to 17.6% compared to the initial state of the system. Object fixes are more effective than the activity ones for two reasons. First, the precision of the Visual Detector is originally significantly lower for objects than for activities (0.44\nNo fix Commonsense Language All fixes Accuracy 3.674 3.698 3.696 3.712 Detail 3.563 3.583 3.590 3.602 Language 4.509 4.575 4.618 4.632 Csense. 0.957 0.973 0.974 0.982 General 3.517 3.546 3.557 3.572 %Sat. 57.8% 58.5% 59.2% 59.3%\nComplete fix workflow. Table 7 shows the improvements from each component and the complete fix workflow which sequentially applies all component fixes. Figure 7 decouples the results for the Satisfactory and Unsatisfactory partitions of the data set.\nNo fix Reranking (All fixes) Accuracy 3.674 4.145 Detail 3.563 3.966 Language 4.509 4.626 Csense. 0.957 0.988 General 3.517 3.973 %Sat. 57.8% 73.6%"
    }, {
      "heading" : "Examples of Fix Integration",
      "text" : "Figure 8 presents examples of different ways fix workflows affect the system output. Figure 8(a) is an example of fixes to the Visual Detector resulting in a satisfactory system output. In this example, workers removed the erroneous object kite and added umbrella which propagated the improvement to the final caption. In the larger dataset, successful propagations of individual component fixes to the final output are also observed for activity fixes, commonsense fixes, and caption refinement fixes. Figure 8(b) shows an example of fixes having a limited improvement on the\nfinal caption due to the commonsense barrier of the Language Model. In this example, the word horse was present in both the original and the fixed word list. However, none of the sentences generated by the Language Model could depict the situation in the image as it was not found to be likely. This example is not unique, the Unsatisfactory dataset contains a few more images of the same nature which describe an unlikely situation that are (at the moment) hard to be described by a statistical automated system. Figure 8(c) is an example in which improvements from fixes are hindered by the limited size of the dictionary. Since the word guitar is not in the dictionary, the final caption fails to provide a satisfactory description."
    }, {
      "heading" : "Discussion",
      "text" : "Quality control and methodology cost. For all crowdsourcing experiments we applied the following quality control techniques: (i) spam detection based on worker disagreement, (ii) worker training via representative examples and detailed instructions, (iii) periodical batching to prevent worker overload and keep them engaged. These techniques ensured high-quality data and enabled us to rely on the majority vote aggregation. Depending on the human computation task, specialized label aggregation techniques can also be leveraged to further improve data quality.\nThe cost of human-in-the loop troubleshooting depends on the number of components, the number of fixes, the fix workload, and the size of the dataset to be investigated. Our analysis covered various fix workflows on all components in the 1000 images Evaluation dataset which showed to be a good representative of the Validation dataset. The total cost of the complete fix workflow (the most expensive one) was $1,850, respectively spent in system evaluation ($250), Visual Detector fixes ($450), Language Model fixes ($900), and Caption Reranker fixes ($250). For a more specialized troubleshooting, the system designer can guide the process towards components that are prime candidates for improvement or on errors to which users are most sensitive. We provide further details on quality control and cost aspects in the appendix.\nOur analysis shows that even with a reasonably small subset of data, our human-in-the loop methodology can efficiently characterize system failure and identify potential component fixes. Alternative improvement methods (i.e. re-\ntraining) require a larger amount of data and oftentimes do not ensure significant improvements. This can happen due inherent learning barriers in the system or slow learning curves of underlying algorithms. System improvement. The results from the methodology provide guidance on next steps to improve the captioning system. First, improving the Reranker emerges as the most promising direction to pursue. Second, due to entanglement issues, improvements on the Visual Detector are suppressed by the shortcomings of the Language Model. Therefore, Visual Detector fixes need to be accompanied with a more capable and commonsense Language Model. Note these insights cannot be revealed via other methodologies that do not involve human computation as it is challenging to automatically simulate improved components without significant engineering effort.\nThere are multiple ways how human input collected from simulating component fixes can help with permanently implementing component fixes. Human fixes on the Visual Detector reveal that the majority of mistakes are false detections. This issue can be addressed by improving model precision. Moreover, the data collected from language and commonsense fixes can be used for training better language models. or for immediately filtering out phrases and sentences flagged by workers. Finally, since a common type of reranking error occurs when the component chooses sentences with words scored low by the Detector, the Reranker can be improved by increasing the weight of the imagecaption similarity score. Generalizability. The general methodology we presented can be applied to a broad range of component-based systems that are designed to be modular and their component input / output is human-interpretable. Even in systems in which these assumptions do not hold in the functional component design, a new structure can be discovered by logically separating components in boundaries where data dependencies are guaranteed and the exchanged data can be analyzed by humans.\nApplying the methodology to a new system requires the designer to customize the methodology by identifying component fixes, defining system quality measures and designing human computation tasks for evaluation and for simulating component fixes. In addition to the captioning system, we conceptually applied our methodology to two other sys-\ntems: (i) question answering with knowledge bases and web search (Yih et al. 2015), and (ii) an email-based reminder for a personal assistant. Our feasibility analysis showed that both applications are compatible with the methodology and highlighted that the most crucial aspect of customization is providing careful and non-ambigious training to crowdsourcing workers tailored to the system context. Given the novelty of the proposed human interventions, the resulting crowdsourcing tasks are expected to be different from the typical labeling tasks frequently encountered in today’s platforms. Such problems are usually more interesting and engaging to crowdsourcing workers. Alternative use cases. In this work, we discuss the benefits of using a human-in-the-loop methodology for troubleshooting integrative systems by simulating fixes within the existing individual components of a system. The methodology can additionally be used for further troubleshooting use cases as follows: • Component prototyping — An interesting application of\nhuman interventions is to completely simulate the output of a component if building it is currently difficult or too expensive. This would allow system designers to make feasibility studies before developing a new component. • Architectural fixes — Our goal in this work was to study single-component fixes that precisely follow the current given architecture. Besides individual component fixes, a system designer may be interested to explore fixes with architectural modifications (e.g. exposing the image to the Language Model), which is useful if designers are not bound to the initial architecture. Some examples include merging, dividing, or introducing new components. In these cases, the task design for human fixes needs to be adjusted according to the new architectural specification and the data exchange flow between components. • Imperfect fixes — In our evaluation, we analyzed fixes that simulate perfect component states. However, building flawless components is not always feasible. Therefore, it is realistic to leverage the methodology to test imperfect fixes by only partially incorporating the humangenerated corrections."
    }, {
      "heading" : "Related Work",
      "text" : "Human input for Machine Learning. The contribution of crowdsourcing to machine learning has been mostly limited to the creation of offline data sets for learning (e.g., (Lin et al. 2014; Sheng, Provost, and Ipeirotis 2008)), with recent interest in active crowd participation to the development of machine learning algorithms (Cheng and Bernstein 2015; Zou, Chaudhuri, and Kalai 2015; Chang, Kittur, and Hahn 2016). However, there has been only limited work on understanding and diagnosing errors of such systems. On debugging a single classifier, researchers have developed techniques for a domain expert to interact with the machine learning process (Chakarov et al. 2016; Kulesza et al. 2010; Patel et al. 2010; Attenberg, Ipeirotis, and Provost 2011). Our work contributes to this line of literature by studying the diagnosis of component-based systems, rather than individual predictive components, by leveraging the crowd input.\nCrowdsourcing for Image Captioning. Crowdsourcing is heavily used for collecting data for object recognition and image captioning (Lin et al. 2014; Fang et al. 2015). In terms of evaluating the performance of a component-based system, previous work explored replacing different components with human input to measure the effect of component imperfections on system performance (Parikh and Zitnick 2011b; Parikh and Zitnick 2011a; Yao et al. 2015). This approach provides information on the current system but does not offer guidance on system improvements. Our work differs from these studies in that it evaluates the effect of different component fixes on system performance to guide decisions on future improvements. Troubleshooting and Blame Assignment. The problems of error diagnosis and blame assignment have been studied for systems whose components are not machine learned and the state of components is binary through rule-based and model-based diagnosis approaches (Darwiche 2000). Breese and Heckerman developed Bayesian networks for predicting the state of each component and for making decisions about the next repair action to take (Breese and Heckerman 1996). In recent work, Sculley et. al., overviewed the challenges of maintaining and improving real-world machine learning systems highlighting error diagnosis as a critical task in particular for component-based systems (Sculley et al. 2015). An alternative way of improving machine learning algorithms is active learning (Settles 2010). Current techniques are applicable to single components (e.g. classifiers) but not to integrative systems with multiple components which is the focus of this work."
    }, {
      "heading" : "Conclusion and Future Work",
      "text" : "We reviewed our efforts for troubleshooting componentbased machine learning systems. The proposed methodology highlights the benefits of deeper integration of crowd input on troubleshooting and improving these integrative systems. Future work directions include the exploration of models that learn from the log data of our methodology to predict which fixes are most likely to improve the system quality for a given input. Such models can enable algorithms to query human fixes during system execution for improved system output. Finally, there is an opportunity to develop generalizable pipelines for automating human-in-the-loop troubleshooting of machine learning systems with reusable crowdsourcing task templates. Such a pipeline would provide valuable insights on system development and create a feedback loop in support of continuous improvement."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research was initiated during an internship of Besmira Nushi at Microsoft Research. The work was supported in part by the Swiss National Science Foundation. The authors would like to thank Xiadong He, Margaret Mitchell, and Larry Zitcnik for their support in understanding and modifying the execution of the image captioning system, as well as Paul Koch for his help while building the necessary project infrastructure. Our thanks extend to Dan Bohus for the valuable insights in current problems in integrative systems and Jaime Teevan for helpful discussions on this work."
    }, {
      "heading" : "A. Experimental evaluation: automatic scores",
      "text" : "The automatic scores for evaluating image captioning systems are adapted from automatic machine translation scores where the automatically translated text is compared to human-generated translations. Similarly, in image captioning, an automatic caption is compared to the five image captions retrieved from crowdsourcing workers available in the MSCOCO dataset. While this evaluation is generally cheaper than directly asking people to report their satisfaction, it does not always correlate well with human satisfaction. More specifically, studies show that often what people like is not necessarily similar to what people generate (Vedantam, Lawrence Zitnick, and Parikh 2015). This phenomena happens due to the fact that an image description can be expressed in many different ways from different people. However, designing good automatic scores for image captioning (and machine learning tasks in general) is an active research area (Yang et al. 2011; Vedantam, Lawrence Zitnick, and Parikh 2015; Banerjee and Lavie 2005; Papineni et al. 2002), as it facilitates systematic and large-scale evaluation of systems.\nIn our evaluation, we experimented with CIDEr, Bleu4, Bleu1, ROUGEL, and METEOR. The Bleu scores (Papineni et al. 2002) compute n-gram precision similarities of the candidate caption with respect to the reference captions. For example, Bleu4 scores higher those captions that have patterns of 4 consecutive words in common with the human captions. However, Bleu scores do not explicitly model recall which in this case would measure how well does the automatic caption cover the content of the human captions. ROUGE (Lin 2004) instead, is a recall-based score that was proposed for content summarization which makes it suitable for image captioning as the caption is in fact a textual summary of the visual content in the image. METEOR (Banerjee and Lavie 2005) and CIDEr (Vedantam, Lawrence Zitnick, and Parikh 2015) take into account multiple versions of captions independently and have shown to correlate better with human satisfaction than the other methods.\nAutomatic Scores — Evaluation dataset No fix Reranking (All fixes)\nCIDEr 0.974 1.087 Bleu4 0.294 0.320 Bleu1 0.693 0.720 ROUGEL 0.521 0.543 METEOR 0.248 0.261\nComponent fixes. (Tables 9, 10, 11, 12) Among all automatic scores, CIDEr, Bleu4, and METEOR preserve the same trends as the human satisfaction score. For example, they confirm that object and removal fixes are more effective than respectively the activity and addition fixes. The non-monotonic error propagations are also reflected in the automatic score analysis as previously concluded from the human satisfaction evaluation. In overall, Caption Reranker fixes remain the most effective ones, followed by the Visual Detector, and the Language Model.\nImprovements for the ROUGEL, METEOR, and Bleu4 are lower as the metrics require a more complete coverage of the human captions which is challenging to achieve in a sentence with limited length. For example, ROUGEL and METEOR require high recall, while Bleu4 relies on exact matches of 4-gram patterns."
    }, {
      "heading" : "B. Quality control and methodology cost",
      "text" : "Quality control. For all crowdsourcing experiments we applied the following quality control techniques: Worker training — Providing accurate instructions to crowdsourcing workers is a crucial aspect in applying our methodology to new machine learning systems. We paid special attention to the task setup by reiterating over the user interface design, providing multiple examples of correct solutions, and giving online feedback to workers on their work quality. Detailed task instructions are necessary for workers to understand the goal of the system and the role of their fixes. We also noticed that workers get more engaged if they understand how their work contributes to improving the presented application. Spam detection — Due to the multimodal nature of the case study, the set of crowdsourcing tasks we used in our methodology is rich and versatile both in terms of content (e.g. images, words, sentences) and design (e.g. yes / no questions, word list correction etc.). Therefore, each task required specific spam detection techniques. Given the lack of ground truth, we used worker disagreement as the main criterion to distinguish low-quality work. However, due to potential subjectivity in our tasks, we used worker disagreement only as an initial indication of low quality and further analyzed distinct cases to decide upon work acceptance. Batching — Large crowdsourcing batches are exposed to low-quality work risk as workers may get tired or reinforce repetitive and wrong biases over time. To avoid such effects, we performed experiments in small-sized batches (e.g. 250 image-caption pairs) which were published in periodical intervals. This allowed us to analyze the batch data offline and give constructive feedback to specific workers on how they can improve their work in the future. The strategy also helped to keep workers engaged and motivated. Methodology cost. Table 13 shows the cost of each crowdsourcing task used in our methodology for the captioning system. Based on this table, system designers can estimate the cost of any future fix workflow. The last column corresponds to the total cost for all instances in the Evaluation dataset. Note that the data collected for the Language Model fixes is reusable as the sentences pruned in one workflow can be safely pruned in other workflows as well. Once we executed the tasks for the workflow that fixes only the Language Model, we observed that due to self-repetition, the number of new sentences generated from other workflows is at least 4 times lower. This means that the cost of fixes for the Language Model continuously decreases over time.\nFor a new machine learning sytem, the associated cost to human-in-the loop troubleshooting depends on the number of components, the number of fixes per component, the fix workload, and the size of the dataset to be investigated."
    } ],
    "references" : [ {
      "title" : "F",
      "author" : [ "J. Attenberg", "P.G. Ipeirotis", "Provost" ],
      "venue" : "J.",
      "citeRegEx" : "Attenberg. Ipeirotis. and Provost 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "and Lavie",
      "author" : [ "S. Banerjee" ],
      "venue" : "A.",
      "citeRegEx" : "Banerjee and Lavie 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "S",
      "author" : [ "A.L. Berger", "V.J.D. Pietra", "Pietra" ],
      "venue" : "A. D.",
      "citeRegEx" : "Berger. Pietra. and Pietra 1996",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "and Heckerman",
      "author" : [ "J.S. Breese" ],
      "venue" : "D.",
      "citeRegEx" : "Breese and Heckerman 1996",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Debugging machine learning",
      "author" : [ "Chakarov" ],
      "venue" : null,
      "citeRegEx" : "Chakarov,? \\Q2016\\E",
      "shortCiteRegEx" : "Chakarov",
      "year" : 2016
    }, {
      "title" : "J",
      "author" : [ "Chang" ],
      "venue" : "C.; Kittur, A.; and Hahn, N.",
      "citeRegEx" : "Chang. Kittur. and Hahn 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "M",
      "author" : [ "J. Cheng", "Bernstein" ],
      "venue" : "S.",
      "citeRegEx" : "Cheng and Bernstein 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "J",
      "author" : [ "H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Dollár", "J. Gao", "X. He", "M. Mitchell", "Platt" ],
      "venue" : "C.; et al.",
      "citeRegEx" : "Fang et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Explanatory debugging: Supporting end-user debugging of machine-learned programs",
      "author" : [ "Kulesza" ],
      "venue" : "In VL/HCC,",
      "citeRegEx" : "Kulesza,? \\Q2010\\E",
      "shortCiteRegEx" : "Kulesza",
      "year" : 2010
    }, {
      "title" : "2014",
      "author" : [ "T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollár", "C. L Zitnick" ],
      "venue" : "Microsoft coco: Common objects in context. In ECCV",
      "citeRegEx" : "Lin et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out",
      "author" : [ "Lin", "C.-Y" ],
      "venue" : "Proceedings of the ACL-04 workshop,",
      "citeRegEx" : "Lin and C..Y.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lin and C..Y.",
      "year" : 2004
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Papineni" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Papineni,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni",
      "year" : 2002
    }, {
      "title" : "Finding the weakest link in person detectors",
      "author" : [ "Parikh", "D. Zitnick 2011b] Parikh", "C.L. Zitnick" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Parikh et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2011
    }, {
      "title" : "A",
      "author" : [ "K. Patel", "N. Bancroft", "S.M. Drucker", "J. Fogarty", "Ko" ],
      "venue" : "J.; and Landay, J.",
      "citeRegEx" : "Patel et al. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Hidden technical debt in machine learning systems",
      "author" : [ "Sculley" ],
      "venue" : null,
      "citeRegEx" : "Sculley,? \\Q2015\\E",
      "shortCiteRegEx" : "Sculley",
      "year" : 2015
    }, {
      "title" : "P",
      "author" : [ "V.S. Sheng", "F. Provost", "Ipeirotis" ],
      "venue" : "G.",
      "citeRegEx" : "Sheng. Provost. and Ipeirotis 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Lawrence Zitnick Vedantam", "R. Parikh 2015] Vedantam", "C. Lawrence Zitnick", "D. Parikh" ],
      "venue" : null,
      "citeRegEx" : "Vedantam et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "C",
      "author" : [ "Yang, Y.", "Teo" ],
      "venue" : "L.; Daumé III, H.; and Aloimonos, Y.",
      "citeRegEx" : "Yang et al. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "J",
      "author" : [ "L. Yao", "N. Ballas", "K. Cho", "Smith" ],
      "venue" : "R.; and Bengio, Y.",
      "citeRegEx" : "Yao et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Semantic parsing via staged query graph generation: Question answering with knowledge base",
      "author" : [ "Yih" ],
      "venue" : null,
      "citeRegEx" : "Yih,? \\Q2015\\E",
      "shortCiteRegEx" : "Yih",
      "year" : 2015
    }, {
      "title" : "P",
      "author" : [ "C. Zhang", "J.C. Platt", "Viola" ],
      "venue" : "A.",
      "citeRegEx" : "Zhang. Platt. and Viola 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A",
      "author" : [ "J.Y. Zou", "K. Chaudhuri", "Kalai" ],
      "venue" : "T.",
      "citeRegEx" : "Zou. Chaudhuri. and Kalai 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "designing good automatic scores for image captioning (and machine learning tasks in general) is an active research area (Yang et al",
      "author" : [ "Vedantam", "Lawrence Zitnick", "Parikh", "Banerjee", "Lavie", "Papineni" ],
      "venue" : null,
      "citeRegEx" : "Zitnick. et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Zitnick. et al\\.",
      "year" : 2002
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We study the problem of troubleshooting machine learning systems that rely on analytical pipelines of distinct components. Understanding and fixing errors that arise in such integrative systems is difficult as failures can occur at multiple points in the execution workflow. Moreover, errors can propagate, become amplified or be suppressed, making blame assignment difficult. We propose a human-in-the-loop methodology which leverages human intellect for troubleshooting system failures. The approach simulates potential component fixes through human computation tasks and measures the expected improvements in the holistic behavior of the system. The method provides guidance to designers about how they can best improve the system. We demonstrate the effectiveness of the approach on an automated image captioning system that has been pressed into real-world use.",
    "creator" : "LaTeX with hyperref package"
  }
}