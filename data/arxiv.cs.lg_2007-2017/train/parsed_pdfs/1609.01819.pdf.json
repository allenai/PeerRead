{
  "name" : "1609.01819.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Semantic Video Trailers",
    "authors" : [ "Harrie Oosterhuis", "Sujith Ravi", "Michael Bendersky" ],
    "emails" : [ "HARRIE.OOSTERHUIS@STUDENT.UVA.NL", "SRAVI@GOOGLE.COM", "BEMIKE@GOOGLE.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In recent years, the availability of video content online has been growing rapidly. YouTube alone has over a billion users, and every day people watch hundreds of millions of hours on YouTube (Youtube Blog Statistics, 2008). With the rapid growth of available content and the rising popularity of online video platforms, accessibility and discoverability become increasingly important. Specifically, in the video search scenario, it is crucial that the platforms enable effective discovery of relevant video content.\nPrevious research, indeed, has dedicated a great deal of attention to video retrieval (Over et al., 2015), a task that is much harder than document retrieval due to the seman-\n1Work done at Google.\ntic mismatch between the keyword queries and the video frames. Therefore, video classification has been a prominent research topic (Karpathy et al., 2014; Brezeale & Cook, 2008), as well as detecting semantic concepts within video material (Jiang et al., 2007). Both video categories and semantic concepts can be used for relevance matching between the query and parts of the video (Snoek & Worring, 2008).\nIn this paper, we extend this existing research, and propose a system for query-based video summarization. Our system creates a brief, visually attractive trailer, which captures the parts of the video (or a collection of videos) that are most relevant to the user-issued query. For instance, for a query Istanbul, and a video describing a trip to Istanbul, our system will construct an informative trailer, highlighting points of interest (Hagia Sophia, Blue Mosque, Grand Bazaar), and skipping non-relevant content (shots of the tour bus, hotel room interior, etc.).\nThe applications for such a system are numerous, as such trailer skips the extraneous parts of a video, thus enhancing the user experience and saving time. For instance, it can better inform user decisions, and save time and money for services where users pay per view or pay for mobile data consumption. A trailer can also serve as an alternative to the standard thumbnail, a still image that represents a video in the query result list. It could potentially better capture the relevant contents of the full video than a single thumbnail image.\nThe query-based summarization done by our system has two main objectives. First, the trailer will capture a semantic match between the query and the video frames that goes beyond simple entity matching. For instance, for a query racecar, a frame containing a car driving on a racetrack will be more relevant than a frame containing a stationary car. We achieve this semantic match via the use of entity embeddings (Levy & Goldberg, 2014). Second,\nar X\niv :1\n60 9.\n01 81\n9v 1\n[ cs\n.L G\n] 7\nS ep\nthe trailer will be visually attractive. For instance, we will prefer frames containing visually prominent, clear depictions of relevant content. We will also prefer summaries that have smooth contiguous frame transitions, similar to human-edited movie trailers.\nThe overall approach – combining semantic match and visual similarities – is outlined in Figure 1. In summary, the main contributions of this paper are:\n1. A robust approach for semantically matching keyword queries to video frames, using entity embeddings trained on non-video corpora.\n2. A scalable method for detecting prominent visual clusters within videos based on label propagation.\n3. An efficient and effective graph-based approach that combines semantic and visual signals to construct trailers, which are both relevant and visually appealing.\n4. Detailed empirical evaluation of the proposed method with comparison to several baseline systems."
    }, {
      "heading" : "2. Related Work",
      "text" : "Previous work on video summarization has taken many different approaches to the problem and interpretations of the task. The task of summarizing a video can be interpreted as creating a textual description, a story board, a graphical representation or a video skim that captures the content of a video appropriately (Money & Agius, 2008). In this study we address the task of constructing a video skim, which is done by taking the video and skipping all unimportant parts. Thus all content in the resulting skim comes from the video and is played in the same chronological order. The main difference from this prior work is that our summaries are query-based.\nApproaches to computing the prominence of a video fragment are widely varied. Some use only visual features, e.g. the model only adds a fragment if it is visually distinct from already added fragments (Zhao & Xing, 2014;\nAlmeida et al., 2013). Others cluster all the frames in the video based on their visual similarity (Carvajal et al., 2014), and subsequently compose a summary by including a single fragment from each cluster. All of these approaches attempt to capture a video by covering all of its visually distinct parts.\nConversely, (Gong et al., 2014) propose a supervised system that learns from human created summaries. Furthermore, by using a collection of videos belonging to a very narrow category one could train a model to recognize the fragments that are the most characteristic of their category (Potapov et al., 2014). Moreover, if no such videos are available, the model can be trained on web images of the same category (Khosla et al., 2013). Our method contrasts with these approaches, as we incorporate a semantic interpretation of the video segments, as well as use the visual information of the fragments. In addition, our approach scales much better, as it is not restricted to a specific video category.\nExisting work has also looked into using higher level concepts to construct summaries. For instance, recognizing events summaries can better address user issued event queries (Wang et al., 2012). In the same vein, detected events can be used to infer causality and construct a storybased summary (Lu & Grauman, 2013).\nMore similar to our method is previous work which recognizes ontology concepts in sports videos. A rule based method is then used to detect and include the meaningful events within the video in the summary (Ouyang & Liu, 2013). Comparable to these methods, our system computes a semantic interpretation of the video content, however we use entity embeddings, which avoids the limitation of rigid event ontologies.\nAlthough not used for summarization, semantic embeddings have been trained for video frames. These can embody a temporal aspect as the embedding of a frame can also based on the preceding and following frames (Ramanathan et al., 2015). Similar embeddings have been used for thumbnail detection where embeddings can be used to find the frame that is the most characteristic of the video’s\ncontent (Liu et al., 2015). The novelty of our approach is that it uses embeddings to find the most relevant segments with respect to a keyword query and uses them for video summarization. Additionally, it is expected to create visually appealing summaries, by including visual features.\nLastly, text-based summarization methods for documents and other textual content have been long studied in the natural language processing literature. However, all these methods have primarily focused on summarizing text documents or user generated written content (Dasgupta et al., 2013; Wang et al., 2014). Graph-based methods have also been used in the past for summarization (Ganesan et al., 2010), but in a very different context. For a detailed survey on existing text summariation techniques, see (Nenkova & McKeown, 2012)."
    }, {
      "heading" : "3. Method",
      "text" : "In this section we propose two models for semantic querybased video summarization, the first only uses semantic information of the video whereas the second incorporates both semantic and visual information.\nBoth models take as input a query q and a video V ; the query has been issued by a user and the video is judged to be relevant by a video retrieval algorithm. Each input video is first divided into one second segments, these are eventually used to compose the trailer summary. Working with these segments makes the final summary more comprehensible, as a second is enough time for the viewer to perceive an included clip. Furthermore, it makes the systems more scalable, as computationally expensive operations only have to be run every second instead of once for every frame in the full video. Both systems rank all the segments of a single video based on the segment content and the user query. The summary is then generated by taking the top k = 20 ranked segments and stitching them together in order of chronological appearance in the full video. By keeping the ordering of the original video the resulting trailer is expected to be more coherent, additionally the generated summary is the equivalent of a video skim."
    }, {
      "heading" : "3.1. Query Representation",
      "text" : "All our models are based on the intuition that segments capturing the same semantic content as the query should be included. Thus, the model estimates how similar the content in the query and the segment are, and ranks them accordingly. The first step in similarity estimation is to process the query q and map it to a universal representation of entities eq ∈ Eq (and their corresponding confidence scores weq ), extracted from a knowledge base such as Wikipedia."
    }, {
      "heading" : "3.2. Direct Matching",
      "text" : "Given the entities Eq in the query, a straightforward approach is to use an image-processing model to recognize the given entities in the frame image, e.g. a deep learning architecture for concept detection in images (Szegedy et al., 2015; He et al., 2015). Then, the query-segment matching is simply a confidence of the concept detection model in detecting the query entities in the segment. However, this direct matching approach has several major drawbacks.\nFirst, the number of concepts that a state of the art detection model can recognize is limited to 22,000 by the largest publicly available corpus (Russakovsky et al., 2015), an extremely small subset of the entities a query can express. Moreover, processing the dataset of query-video pairs gathered for our experiments in section 4.2 which contains over 34,000 pairs revealed that 57% had no entity overlap.\nSecond, many summaries should contain segments that do not directly display the entities in the query but are relevant nonetheless. For instance a good summary for the entity turkey could contain a segment of turkey stuffing being prepared, despite that visually no turkey is actually present. However, direct detection models are not robust enough to recognize such related concepts.\nTherefore, since direct matching models cannot be applied to majority of the summarization cases, instead we focus our attention on more advanced approaches in the rest of the paper. We present two such methods next."
    }, {
      "heading" : "3.3. Semantic Matching",
      "text" : "As in the previous method, we first apply the Inception model (Szegedy et al., 2015) – state-of-the-art deep neural network architecture, that is trained to detect a large number of concepts in images – on each frame Fi in the segment. The model outputs a set of entity concepts EFi with confidence scores wef for how certain the system is that each concept ef ∈ EFi is present in the segment Fi.\nHowever, instead of directly matching concepts between the sparse entity mappings EFi and Eq , we compute a dense semantic embedding representation for both the query q and a given video frame Fi using their entity mappings. In other words, we replace each concept e with its pre-computed semantic embeddings vector Se . Then, a semantic representation of the segment Fi is given by\nSFi = 1 |EFi | ∑\nef∈EFi\nwefSef\nSimilarly, we represent the query q, by weighted average of embeddings for its entities to create a semantic representation Sq .\nSemantic embeddings at the entity level are computed\nusing the recent approach from Mikolov et al. (2013), and trained on a large corpus of text documents from Wikipedia. The embedding model can be learned in an unsupervised manner, thus the amount of training data can be acquired at magnitudes greater than labeled data available for training visual recognition systems. This allows the embedding model to be applicable for a substantially larger number of entities. Recent work reports 175,000 embeddings can be trained from only using the English Wikipedia (Levy & Goldberg, 2014).\nFinally, the similarity between the query q and segment Fi can be estimated using the cosine similarity of their associated embeddings Sq,SFi as follows:∑\neq∈Eq ∑ ef∈EFi weqwef cosine(Seq ,Sef )\n= cosine(Sq,SFi)\nThe ranking of segments Fi is based on the estimated semantic similarity to q, where the most similar segment is added first to the summary."
    }, {
      "heading" : "3.4. Graph-Based Matching",
      "text" : "The semantic matching approach provides a robust method of estimating the relevance of segments, however it only considers semantic similarity and treats all the segments independently. Next, we introduce a second graph-based approach that models the intuition that content visually prominent in a video must be relevant to the topic it covers. In other words, besides the semantic similarity between the query and segments, the prominence of the content in a segment should also be used to estimate its relevance. We estimate prominence using visual information, thus if large parts of the video look visually similar we will assume they cover relevant content.\nTo effectively combine the semantic and visual signals in our system, we use Expander, an efficient graph-based learning framework based on label propagation (Ravi & Diao, 2016). The framework is typically used for semisupervised learning scenarios over graph structures (Bengio et al., 2006; Ravi & Diao, 2016; Wendt et al., 2016). Usually, the weight of the edge between two nodes indicate their similarity, and true labels are known for only a subset of the nodes. The approach relies on the assumption that nodes that are very similar are also very likely to have the same labels. Accordingly the model iterates over the graph several times, at each iteration all nodes acquire the labels of the nodes they are connected to. Each node keeps a confidence score for every label based on how strongly it is connected to the nodes it acquired it from and their corresponding confident scores. In this manner, the labels are propagated through the graph at each iteration until a stable distribution of labels is reached. The typical use of this method is considered semi-supervised, as only a fraction of the true labels need to be known and the remaining are not learned from training data but directly inferred from the graph structure.\nOur model uses a graph for each query-video pair (q, V ) to be summarized, each segment Fi extracted from the video V is represented by a node in the graph, finally there is a node representing the query q. The values of the edges between the query node and the segment nodes are computed using the semantic matching approach, thus these edges represent their semantic similarity cosine(Sq,SFi). The edges between the segments on the other hand are computed by their visual similarity, this is done sampling a frame from each segment and calculating their resemblance cosine(VFi ,VFj ), where VFi corresponds to a visual embedding corresponding to the frame Fi which is computed using a hidden layer representation of the frame image within the deep learning network described earlier. A diagram of the resulting graph is displayed in Figure 2.\nWe learn a label assignment L̂ on this graph that minimizes the following convex objective function:\nC(L̂) = ∑ Fi∈V wqFi ||L̂q − L̂Fi || 2 2\n+ ∑\nFi,Fj∈V\nwij ||L̂Fi − L̂Fj || 2 2\n+ ∑ Fi∈V ||LFi − L̂Fi || 2 2 (1)\nwhere wqFi , wij represent the semantic and visual similarity scores as defined above; L̂ is the learned label distribution for query and segment nodes in the graph; and LFi is the seed label (i.e., identity) on the video segment nodes.\nThe segment nodes are each assigned a unique “seed” label (i.e., their identity). We optimize the above objective function using the iterative streaming algorithm described in (Ravi & Diao, 2016), then after running label propagation the confidence scores of the labels acquired by the query node L̂q are considered. The segments are ranked corresponding to how strongly their corresponding labels were propagated to the query node. In other words, the output label scores on the query node L̂q indicate how well the segments are connected to the query in the graph. A segment can be strongly connected because it is semantically similar to the query or it is visually similar to other segments that are strongly connected. Note that contrary to the typical usage of label propagation, our approach is in fact unsupervised as the initial labels can automatically be assigned. The streaming Expander algorithm permits efficient scaling to thousands or millions of frames for long videos while maintaining constant space complexity.\nPresumably we could ignore the semantic edges in the graph completely and propagate only the frame-ids over the visual edges. This is equivalent to performing visual clustering, we do not consider this model here because it ignores the query and therefore is unsuited for this task. Similarly the edges could be weighted so that the model values the either semantic or visual signals more. We can also easily incorporate diversity among ranked results, as in traditional summarization approaches, by simply converting the visual similarity signal into a distance metric.2 Furthermore the generic setup of the method allows it to be easily extended with novel signals in the future.\nThough the intuition behind the previous graph construction is reasonable, preliminary results revealed some practical problems with this model. Namely many videos contain visuals that often recur in the video but are not relevant for a summary. For instance, news shows or documentaries can feature a presenter who talks periodically throughout the video. These segments will be very similar visually\n2Different graph configurations were tried but are not included in to maintain brevity.\ndespite being the least interesting parts to include in a summary. Moreover this problem can be extremely prevalent in online video content, since they often feature an almost static outro where users are asked to leave favorable feedback and watch more videos. Because these outros usually consist of text on a near static background, they form very strong clusters in the graph which boost these segments into the summary.\nTo counter these issues, we change the model to instead only consider the hundred highest semantically similar segments, thereby yielding a graph-based reranking model. The nodes representing the other segments and their edges are completely disregarded, as can be seen in the Figure 2. The intuition behind this reranking model is that content prominent among the relevant parts of a video are expected to be good additions to a summary and the irrelevant frames are automatically discarded."
    }, {
      "heading" : "4. Experiments",
      "text" : "In this section, we detail our experiments designed to evaluate the performance of our models. Section 4.1 introduces two baselines for comparison, subsequently we discuss the data used for evaluation and our experimental setup in Section 4.2 and Section 4.3 respectively."
    }, {
      "heading" : "4.1. Baselines",
      "text" : "To properly investigate the performance of the models introduced in Section 3 we introduce the uniform baseline model for comparison. Similar to the models, the uniform baseline also uses one second segments, however instead of judging their relevance the method selects segments according to a uniform distribution. As a result each segment is equally likely to appear in the generated summary. Because the uniform sampling covers all parts of the video equally, the summary is expected to capture all parts of the video. Since the video is selected using a state-of-the-art retrieval method, its content is expected to be very relevant to the topic. Thus the resulting summary is expected to be just as relevant to the query. However since it does not take into account the content of the video nor the query, it is expected to fail on videos that spend disproportionate time on some topics or contain cover material unrelated to the query. Both of these are unlikely if a strong retrieval model was used or if it was a short video.\nAdditionally, we introduce a second baseline: the first twenty seconds model (first-20). This baseline creates a summary of a video by taking its first twenty seconds. This simple model is based on two intuitions. Firstly the generated summaries keep the coherency of the original video because each summary is an unaltered clip where no film cuts were introduced. Secondly, many videos start with an\nintroduction of their topic usually to gain the viewers attention. Accordingly, this baseline attempts to select a single clip that gives an overview of the video."
    }, {
      "heading" : "4.2. Dataset",
      "text" : "Since our proposed system uses a query and a matching video, we make use of YouTube to collect these queryvideo pairs. Because YouTube receives millions of user queries per day and has a large variety of content, we consider it a good fit to test the effectiveness of our system. We sampled 1800 of the most commonly issued queries, for each query twenty matching videos were sampled uniformly from the top hundred search results. Subsequently the summarization system was then applied to the resulting 34,725 videos, note that some videos are matched to multiple queries.\nSampling of videos was limited to those with a running length greater than ten minutes. This makes sure that summarization is not a trivial task. In addition, video-query pairs which had an overlap in extracted entities were discarded as well. We chose to discard these videos to test the robustness of our system, since this limitation makes the direct match approach (described in Section 3.2) impossible. As a result the data only contains instances where the semantic similarity between segments and the query cannot be computed directly. As described in Section 3.3 our system can handle these entity mis-matches by using semantic embeddings. We believe this focus on the mismatching cases is warranted, as we consider wide applicability as more important than good performance on a particular video subset.\nLastly since the system was evaluated using crowdsourcing we were unable to use the entire set of summarized queryvideo pairs. Instead a subset of 127 query-video pairs was used for the crowdsourced evaluation."
    }, {
      "heading" : "4.3. Experimental Setup",
      "text" : "The quality of a summary is difficult be judged objectively. Consequently we used the Amazon Turk platform to perform a crowdsourced experiment, with three raters per task. Our comparison of models and baselines is based on the crowdsourced assessments of generated summaries.\nHowever the task of judging a single summary proved to be very hard for most people, instead we found asking for preferences between summaries is a more comprehensible task. Accordingly the task consisted of a single question: “Someone is looking for a video about [query], which of the following two 20 second videos is best to show?” followed by two side-by-side summary trailers: one generated by a model, and another by a baseline, their order randomized.\nA judgement was collected for the combination of each\nquery-video pair, model and baseline, giving us a total of 508 judgments. However we noticed that some users disregarded the task to quickly optimize on the money incentive. For this reason we disregarded any judgement made within less than 30 seconds, bringing the number of judgements down to 449. Significance testing of the preferences between the systems was done by applying a two sided Wilcoxon sign test."
    }, {
      "heading" : "5. Results",
      "text" : "In this section, we present the results of our experiment described in Section 4, provide several example summaries and evaluate our proposed summarization method."
    }, {
      "heading" : "5.1. Experimental results",
      "text" : "The results of our crowdsourcing experiment are displayed in Table 1. A clear preference of both models over the first twenty seconds baseline is visible. Since they are statistically significant (p < 0.01) we conclude that both our models create better summaries than this baseline. When compared to the uniform baseline though, the graph-based approach yields more favorable summaries compared to the semantic-only model. However, overall preference % for the two models compared to the uniform baseline are not as high. There could be several reasons for this, e.g., the task is not easy for people who are not familiar with video summarization.\nFurthermore, the videos may not be appropriate for summarization; to further investigate this judgements were split\nbased on video-category and length. Table 1 shows the preferences for videos in the Gaming and Animation category (29% of videos) and all others. These categories were chosen as they are prevalent on YouTube and are expected to be less suited for summarization. The results show us that both models perform better for Non Gaming and Animation categories when compared to the uniform baseline. Additionally, results split by video length are also displayed in Table 1, we chose to split on 20 minutes as close to half (44%) are under 20 minutes. Here we see that the semantic model performs substantially better on videos over 20 minutes with a 13% difference compared to the uniform model, though graph-based performs almost the same with a 1% difference. These results suggest that certain types of videos are more suited for auto-generating summary trailers.\nIn addition to the previous experiment, we performed a more detailed study on a smaller video dataset to better understand the differences between models. This experiment was also crowdsourced and showed judges a single summary together with a multi-choice questionnaire; videos were sampled and judgements were gathered for their summaries created by the uniform baseline and the graph-based model. In total 60 judgements were collected, the questions and results are displayed in Table 2, answers ranged from 1 (most negative) to 5 (most positive). The questionnaire shows us a clear signal that the graph-based method creates summary trailers that are visually more attractive than the uniform baseline."
    }, {
      "heading" : "5.2. Example summaries",
      "text" : "To further investigate the effects of using different models we display example summaries in Figure 3 which are the result of applying different models to the same three queryvideo pairs. For this illustration the uniform baseline, semantic model and graph-based model were applied, the first twenty seconds baseline was dismissed as it performs significantly worse according to the results in Section 5.1. Three videos were sampled from different categories to illustrate robustness and diversity, the selected query-video\npairs are: frogs, an animal documentary; salmon pasta, an amateur cooking video; volvo P1800, an informational video regarding a famous car model3.\nThe uniform summaries cover the videos passably, however\n3Videos are available under the Creative Commons licence at: youtu.be/w-AItfioqlw, youtu.be/tR9ZtaGtCAM and youtu.be/FwCjOakOMKE\nthe summaries contain many shots unrelated to the query. Most notably all uniform summaries contain shots of people who are presenting the video but are not relevant to the query. In contrast, the semantic summaries only contain shots related to the query. For the first video we see that the semantic model has only included shots containing frogs, for the salmon pasta video only shots of fish are included, and for the volvo P1800 video the summary consists of only shots that clearly display cars. Therefore we conclude that the semantic model can recognize semantic similarity robustly, as it found relevant shots effectively despite the fact that no direct annotations of the query were available in the video.\nLastly we have the graph-based summaries, as expected they are very similar to those of the semantic model. The differences are important though: the frogs summary displays more shots of more different frogs, which adds diversity to the video. The model picked up on shots where the frogs are less directly recognizable (for instance due to camouflage or displaying the head) due to their visual similarity to semantically relevant shots. In the salmon pasta summary shots of the vegetable sauce are included, the model inferred their relevance due to their prominence in the video. The semantic model did not include these as salmon pasta is defined by its fish, however with respect to the cooking video this seems to be a good inclusion. Finally, the volvo P1800 summary displays more shots showing the outside of the car. The model picked up on interesting shots by their prominence and the result is a more visually appealing summary.\nThese examples show a clear difference between the uniform baseline and our models. This contrasts with some of the results in Section 5.1, where the preference differences between our models and the uniform model were not as pronounced. This suggests that the query-based video summarization task is a difficult one, and visual summary evaluation is an interesting direction for future work."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We presented a system for query-based video summarization that effectively combines semantic interpretations and visual signals of the video to construct summary trailers.\nDespite the difficulties of evaluating for this complex task, we show that the new approach outperforms other baselines in terms of summarization quality as judged by human raters. We also show several examples which demonstrate that the approach of combining embeddings with frame annotations allows for robust semantic detection of relevant segments.\nMoreover, our proposed graph-based model is able to recognize parts of the video that are both relevant to the query\nand visually prominent in the video. Future research could expand this approach by applying the graph-based model over several related videos to find latent topics using their visual similarity or to create multiple summary views per video each focused on a different topic. Finally, the usage of query-based summaries as dynamic thumbnails seems a promising direction for research."
    } ],
    "references" : [ {
      "title" : "Online video summarization on compressed domain",
      "author" : [ "Almeida", "Jurandy", "Leite", "Neucimar J", "Torres", "Ricardo da S" ],
      "venue" : "Journal of Visual Communication and Image Representation,",
      "citeRegEx" : "Almeida et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Almeida et al\\.",
      "year" : 2013
    }, {
      "title" : "Label propagation and quadratic criterion",
      "author" : [ "Bengio", "Yoshua", "Delalleau", "Olivier", "Le Roux", "Nicolas" ],
      "venue" : "Semi-Supervised Learning,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2006
    }, {
      "title" : "Automatic video classification: A survey of the literature. Systems, Man, and Cybernetics, Part C: Applications and Reviews",
      "author" : [ "Brezeale", "Darin", "Cook", "Diane J" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "Brezeale et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Brezeale et al\\.",
      "year" : 2008
    }, {
      "title" : "Summarisation of short-term and long-term videos using texture and colour",
      "author" : [ "Carvajal", "Johanna", "McCool", "Chris", "Sanderson", "Conrad" ],
      "venue" : "In Applications of Computer Vision (WACV),",
      "citeRegEx" : "Carvajal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Carvajal et al\\.",
      "year" : 2014
    }, {
      "title" : "Summarization through submodularity and dispersion",
      "author" : [ "Dasgupta", "Anirban", "Kumar", "Ravi", "Sujith" ],
      "venue" : "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Dasgupta et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2013
    }, {
      "title" : "Opinosis: A graph-based approach to abstractive summarization of highly redundant opinions",
      "author" : [ "Ganesan", "Kavita", "Zhai", "ChengXiang", "Han", "Jiawei" ],
      "venue" : "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),",
      "citeRegEx" : "Ganesan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ganesan et al\\.",
      "year" : 2010
    }, {
      "title" : "Diverse sequential subset selection for supervised video summarization",
      "author" : [ "Gong", "Boqing", "Chao", "Wei-Lun", "Grauman", "Kristen", "Sha", "Fei" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Gong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2014
    }, {
      "title" : "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "author" : [ "He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "He et al\\.,? \\Q1916\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 1916
    }, {
      "title" : "Large-scale video summarization using web-image priors",
      "author" : [ "Khosla", "Aditya", "Hamid", "Raffay", "Lin", "Chih-Jen", "Sundaresan", "Neel" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Khosla et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Khosla et al\\.",
      "year" : 2013
    }, {
      "title" : "Multi-task deep visual-semantic embedding for video thumbnail selection",
      "author" : [ "Liu", "Wu", "Mei", "Tao", "Zhang", "Yongdong", "Che", "Cherry", "Luo", "Jiebo" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Story-driven summarization for egocentric video",
      "author" : [ "Lu", "Zheng", "Grauman", "Kristen" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Lu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Video summarisation: A conceptual framework and survey of the state of the art",
      "author" : [ "Money", "Arthur G", "Agius", "Harry" ],
      "venue" : "Journal of Visual Communication and Image Representation,",
      "citeRegEx" : "Money et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Money et al\\.",
      "year" : 2008
    }, {
      "title" : "A survey of text summarization techniques",
      "author" : [ "Nenkova", "Ani", "McKeown", "Kathleen" ],
      "venue" : "Mining Text Data,",
      "citeRegEx" : "Nenkova et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nenkova et al\\.",
      "year" : 2012
    }, {
      "title" : "Ontology reasoning scheme for constructing meaningful sports video summarisation",
      "author" : [ "Ouyang", "Jian-quan", "Liu", "Renren" ],
      "venue" : "Image Processing, IET,",
      "citeRegEx" : "Ouyang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ouyang et al\\.",
      "year" : 2013
    }, {
      "title" : "Trecvid 2015 – an overview of the goals, tasks, data, evaluation mechanisms and metrics",
      "author" : [ "Over", "Paul", "Awad", "George", "Michel", "Martial", "Fiscus", "Jonathan", "Kraaij", "Wessel", "Smeaton", "Alan F", "Quenot", "Georges", "Ordelman", "Roeland" ],
      "venue" : "Proceedings of TRECVID",
      "citeRegEx" : "Over et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Over et al\\.",
      "year" : 2015
    }, {
      "title" : "Category-specific video summarization",
      "author" : [ "Potapov", "Danila", "Douze", "Matthijs", "Harchaoui", "Zaid", "Schmid", "Cordelia" ],
      "venue" : "In Computer Vision–ECCV",
      "citeRegEx" : "Potapov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Potapov et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning temporal embeddings for complex video analysis",
      "author" : [ "Ramanathan", "Vignesh", "Tang", "Kevin", "Mori", "Greg", "FeiFei", "Li" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision, pp",
      "citeRegEx" : "Ramanathan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ramanathan et al\\.",
      "year" : 2015
    }, {
      "title" : "Large scale distributed semi-supervised learning using streaming approximation",
      "author" : [ "Ravi", "Sujith", "Diao", "Qiming" ],
      "venue" : "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Ravi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ravi et al\\.",
      "year" : 2016
    }, {
      "title" : "Concept-based video retrieval",
      "author" : [ "Snoek", "Cees GM", "Worring", "Marcel" ],
      "venue" : "Foundations and Trends in Information Retrieval,",
      "citeRegEx" : "Snoek et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2008
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew" ],
      "venue" : "In CVPR 2015,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Query-focused opinion summarization for usergenerated content",
      "author" : [ "Wang", "Lu", "Raghavan", "Hema", "Cardie", "Claire", "Castelli", "Vittorio" ],
      "venue" : "In Proceedings of the 25th International Conference on Computational Linguistics (COLING),",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Event driven web video summarization by tag localization and keyshot identification",
      "author" : [ "Wang", "Meng", "Hong", "Richang", "Li", "Guangda", "Zha", "ZhengJun", "Yan", "Shuicheng", "Chua", "Tat-Seng" ],
      "venue" : "Multimedia, IEEE Transactions on,",
      "citeRegEx" : "Wang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2012
    }, {
      "title" : "Quasi real-time summarization for consumer videos",
      "author" : [ "Zhao", "Bin", "Xing", "Eric" ],
      "venue" : "https://www.youtube.com/yt/press/statistics.html,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Previous research, indeed, has dedicated a great deal of attention to video retrieval (Over et al., 2015), a task that is much harder than document retrieval due to the seman-",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "the model only adds a fragment if it is visually distinct from already added fragments (Zhao & Xing, 2014; Almeida et al., 2013).",
      "startOffset" : 87,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "Others cluster all the frames in the video based on their visual similarity (Carvajal et al., 2014), and subsequently compose a summary by including a single fragment from each cluster.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "Conversely, (Gong et al., 2014) propose a supervised system that learns from human created summaries.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "Moreover, if no such videos are available, the model can be trained on web images of the same category (Khosla et al., 2013).",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "For instance, recognizing events summaries can better address user issued event queries (Wang et al., 2012).",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "These can embody a temporal aspect as the embedding of a frame can also based on the preceding and following frames (Ramanathan et al., 2015).",
      "startOffset" : 116,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "content (Liu et al., 2015).",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "However, all these methods have primarily focused on summarizing text documents or user generated written content (Dasgupta et al., 2013; Wang et al., 2014).",
      "startOffset" : 114,
      "endOffset" : 156
    }, {
      "referenceID" : 21,
      "context" : "However, all these methods have primarily focused on summarizing text documents or user generated written content (Dasgupta et al., 2013; Wang et al., 2014).",
      "startOffset" : 114,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "Graph-based methods have also been used in the past for summarization (Ganesan et al., 2010), but in a very different context.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "a deep learning architecture for concept detection in images (Szegedy et al., 2015; He et al., 2015).",
      "startOffset" : 61,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "As in the previous method, we first apply the Inception model (Szegedy et al., 2015) – state-of-the-art deep neural network architecture, that is trained to detect a large number of concepts in images – on each frame Fi in the segment.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "using the recent approach from Mikolov et al. (2013), and trained on a large corpus of text documents from Wikipedia.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "The framework is typically used for semisupervised learning scenarios over graph structures (Bengio et al., 2006; Ravi & Diao, 2016; Wendt et al., 2016).",
      "startOffset" : 92,
      "endOffset" : 152
    } ],
    "year" : 2016,
    "abstractText" : "Query-based video summarization is the task of creating a brief visual trailer, which captures the parts of the video (or a collection of videos) that are most relevant to the user-issued query. In this paper, we propose an unsupervised label propagation approach for this task. Our approach effectively captures the multimodal semantics of queries and videos using state-of-the-art deep neural networks and creates a summary that is both semantically coherent and visually attractive. We describe the theoretical framework of our graph-based approach and empirically evaluate its effectiveness in creating relevant and attractive trailers. Finally, we showcase example video trailers generated by our system.",
    "creator" : "LaTeX with hyperref package"
  }
}