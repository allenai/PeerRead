{
  "name" : "1209.3129.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Analog readout for optical reservoir computers",
    "authors" : [ "A. Smerieri", "F. Duport", "Y. Paquot", "B. Schrauwen", "M. Haelterman", "S. Massar" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The term “reservoir computing” encompasses a range of similar machine learning techniques, independently introduced by H. Jaeger [1] and by W. Maass [2]. While these techniques differ in implementation details, they share the same core idea: that one can leverage the dynamics of a recurrent nonlinear network to perform computation on a time dependent signal without having to train the network itself. This is done simply by adding an external, generally linear readout layer and training it instead. The result is a powerful system that can outperform other techniques on a range of tasks (see for example the ones reported in [3, 4]), and is significantly easier to train than recurrent neural\nar X\niv :1\n20 9.\n31 29\nv1 [\ncs .E\nnetworks. Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].\nOne great advantage of this technique is that it places almost no requirements on the structure of the recurrent nonlinear network. The topology of the network, as well as the characteristics of the nonlinear nodes, are left to the user. The only requirements are that the network should be of sufficiently high dimensionality, and that it should have suitable rich dynamics. The last requirement essentially means that the dynamics allows the exploration of a large number of network states when new inputs come in, while at the same time retaining for a finite time information on the previous inputs [11]. For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].\nOptical reservoir computers are particularly promising, as they can provide an alternative path to optical computing. They could leverage the inherent high speeds and parallelism granted by optics, without the need for strong nonlinear interaction needed to mimic traditional electronic components. Very recently, optoelectronic reservoir computers have been demonstrated by different research teams [10, 9], conjugating good computational performances with the promise of very high operating speeds. However, one major drawback in these experiments, as well as all preceding ones, was the absence of readout mechanisms: reservoir states were collected on a computer and post-processed digitally, severely limiting the processing speeds obtained and hence the applicability.\nAn analog readout for experimental reservoirs would remove this major bottleneck, as pointed out in [13]. The modular characteristics of reservoir computing imply that hardware reservoirs and readouts can be optimized independently and in parallel. Moreover, an analog readout opens the possibility of feeding back the output of the reservoir into the reservoir itself, which in turn allows the use of different training techniques [14] and to apply reservoir computing to new categories of tasks, such as pattern generation [15, 16].\nIn this paper we present a proposal for the readout mechanism for optoelectronic reservoirs, using an optoelectronic intensity modulator. The design that we propose will drastically cut down their operation time, specially in the case of long input sequences. Our proposal is suited to optoelectronic or alloptical reservoirs, but the concept can be easily extended to any experimental time-multiplexed reservoir computer. The mechanism has been tested experimentally using the experimental reservoir reported in [10], and compared to a digital readout. Although the results are preliminary, they are promising: while not as good as those reported in [10], they are however already better than non-reservoir methods for the same task [16]."
    }, {
      "heading" : "2 Reservoir computing and time multiplexing",
      "text" : ""
    }, {
      "heading" : "2.1 Principles of Reservoir Computing",
      "text" : "The main component of a reservoir computer (RC) is a recurrent network of nonlinear elements, usually called “nodes” or “neurons”. The system typically works in discrete time, and the state of each node at each time step is a function of the input value at that time step and of the states of neighboring nodes at the previous time step. The network output is generated by a readout layer - a set of linear nodes that provide a linear combination of the instantaneous node states with fixed coefficients.\nThe equation that describes the evolution of the reservoir computer is\nxi(n) = f(αmiu(n) + β N∑ j=1 wijxj(n− 1)) (1)\nwhere xi(n) is the state of the i-th node at discrete time n, N is the total number of nodes, u(n) is the reservoir input at time n, mi and wij are the connection coefficients that describe the network topology, α and β are two parameters that regulate the network’s dynamics, and f is a nonlinear function. One generally tunes α and β to have favorable dynamics when the input to be treated is injected in the reservoir. The network output y(n) is then constructed using a set of readout weights Wi and a bias weight Wb, as\ny(n) = N∑ i=1 Wixi(n) +Wb (2)\nTraining a reservoir computer only involves the readout layer, and consists in finding the best set of readout weights Wi and bias Wb that minimize the error between the desired output and the actual network output. Unlike conventional recurrent neural networks, the strength of connections mi and wij are left untouched. As the output layer is made only of linear units, given the full set of reservoir states xi(n) for all the time steps n, the training procedure is a basic, regularized linear regression."
    }, {
      "heading" : "2.2 Time multiplexing",
      "text" : "The number of nodes in a reservoir computer determines an upper limit to the reservoir performance [17]; this can be an obstacle when designing physical implementations of RCs, which should contain a high number of interconnected nonlinear units. A solution to this problem proposed in [7, 8], is time multiplexing: the xi(n) are computed one by one by a single nonlinear element, which receives a combination of the input u(n) and a previous state xj(n − 1). In addition an input mask mi is applied to the input u(n), to enrich the reservoir dynamics. The value of xi(n) is then stored in a delay line to be used at a later time step n + 1. The interaction between different neurons can be provided\nby either having a slow nonlinear element which couples state xi to the previous states xi−1, xi−2, ... [8], or by using an instantaneous nonlinear element and desynchronizing the input with respect to the delay line [10]."
    }, {
      "heading" : "2.3 Hardware RC with digital readout",
      "text" : "The hardware reservoir computer we use in the present work is identical to the one reported in [10] (see also [9]). It uses the time-multiplexing with desynchronisation technique described in the previous paragraph. We give a brief description of the experimental system, represented in the left part of Figure 1. It uses a LiNbO3 Mach-Zehnder (MZ) modulator, operating on a constant power 1560 nm laser, as the nonlinear component. A MZ modulator is a voltage controlled optoelectronic device; the amount of light that it transmits is a sine function of the voltage applied to it. The resulting state xi(n) is encoded in a light intensity level at the MZ output. It is then stored in a spool of optical fiber, acting as delay line of duration T = 8.5µs, while all the subsequent states xi(n) are being computed by the MZ modulator. When a state xi(n) reaches the end of the fiber spool it is converted into a voltage by a photodiode.\nThe input u(n) is multiplied by the input mask mi and encoded in a voltage level by an Arbitrary Waveform Generator (AWG). The two voltages corresponding to the state xi(n) at the end of the fiber spool and the input miu(n) are added, amplified, and the resulting voltage is used to drive the MZ modulator, thereby producing the state xj(n+ 1), and so on for all values of n.\nIn the experiment reported in [10] a portion of the light coming out of the MZ is deviated to a second photodiode (not shown in Figure 1), that converts it\ninto a voltage and sends it to a digital oscilloscope. The Mach-Zehnder output can be represented as “steps” of light intensities of duration θ (see Figure 2a), each one representing the value of a single node state xi at discrete time n. The value of each xi(n) is recovered by taking an average of the measured voltage for each state at each time step. The optimal readout weights Wi and bias Wb are then calculated on a computer from a subset (training set) of the recorded states, using ridge regression [18], and the output y(n) is then calculated using equation 2 for all the states collected. The performance of the reservoir is then calculated by comparing the reservoir output y(n) with the desired output ŷ(n)."
    }, {
      "heading" : "3 Analog readout",
      "text" : "Readout scheme Developing an analog readout for the reservoir computer described in section 2 means designing a device that multiplies the reservoir states shown in Figure 2a by the readout weights Wi, and that sums them together in such a way that the reservoir output y(n) can be retrieved directly from its output. However, this is not straightforward to do, since obtaining good performance requires positive and negative readout weights Wi. In optical implementations [10, 9] the states xi are encoded as light intensities which are always positive, so they cannot be subtracted one from another. Moreover, the summation over the states must include only the values of xi pertaining to the same discrete time step n and reject all other values. This is difficult in time-multiplexed reservoirs, where the states xN (n) and x1(n+ 1) follow seamlessly.\nHere we show how to resolve both difficulties using the scheme depicted in the right panel of Figure 1. Reservoir states encoded as light intensities in the optical reservoir computer and represented in Figure 2a are fed to the input of a second MZ modulator with two outputs. A second function generator governs the bias of the second Mach-Zehnder, providing the modulation voltage V (t). The modulation voltage controls how much of the input light passing through the readout Mach-Zehnder is sent to each output, keeping constant the sum of the two output intensities. The two outputs are connected to the two inputs of a balanced photodiode, which in turn gives as output a voltage level proportional to the difference of the light intensities received at its two inputs1. This allows us to multiply the reservoir states by both positive and negative weights.\nThe time average of the output voltage of the photodiode is obtained by using a capacitor. The characteristic time of the analog integrator τ is proportional to the capacity C.2 The role of this time scale is to include in the readout output all the pertinent contributions and exclude the others. The final output of the\n1A balanced photodiode consists of two photodiodes which convert the two light intensities into two electric currents, followed by an electronic circuit which produces as output a voltage proportional to the difference of the two currents\n2In the case where the impedance of the coaxial cable R = 50Ω is matched with the output impedance of the photodiode, we have τ = RC\n2\nreservoir is the voltage across the capacitor at the end of each discretized time n.\nWhat follows is a detailed description of the readout design.\nMultiplication by arbitrary weights The multiplication of the reservoir states by arbitrary weights, positive or negative, is realized by the second MZ modulator followed by the balanced photodiode. The modulation voltage V (t) that drives the second Mach Zehnder is piecewise constant, with a step duration equal to the duration θ of the reservoir states; transitions in voltages and in reservoir states are synchronized. The modulation voltage is also a periodic function of period θN , so that each reservoir state xi(n) is paired with a voltage level Vi that doesn’t depend on n. The light intensities O1(t) and O2(t) at the two outputs of the Mach-Zehnder modulator are\nO1(t) = I(t) 1 + cos((V (t) + Vbias)\nπ Vπ + ϕ)\n2 , O2(t) = I(t) 1− cos((V (t) + Vbias) πVπ + ϕ) 2 ,\n(3) where I(t) is the light intensity coming from the reservoir, Vbias is a constant voltage that drives the modulator, ϕ is an arbitrary, constant phase value, and Vπ is the half-wave voltage of the modulator. Neglecting the effect of any bandpass filter in the photodiode, and choosing Vbias appropriately, the output P (t) from the photodiode can be written as\nP (t) = G(O1(t)−O2(t)) = I(t)(G sin( V (t)π\nVπ )) = I(t)W (t) (4)\nwith G a constant gain factor. In other words, by setting the right bias and driving the modulator with a voltage V (t), we multiply the signal I(t) by an arbitrary coefficientW (t). Note that, if V (t) is piecewise constant, thenW (t) is as well. This allows us to achieve the multiplication of the states xi(n), encoded in the light intensity I(t), by the weights Wi, just by choosing the right voltage V (t), as shown in Figure 2b.\nSummation of weighted states To achieve the summation over all the states pertaining to the same discrete time step n, which according to equation 2 will give us the reservoir output minus the bias Wb, we use the capacitor at the right side of the Output layer in Figure 1. The capacitor provides the integration of the photodiode output given by eq. 4 with an exponential kernel and time constant τ . If τ is significantly less than the amount of time θN needed for the system to process all the nodes relative to a single time step, we can minimize the crosstalk between node states relative to different time steps.\nLet us consider the input I(t) of the readout, and let t = 0 be the instant where the state of the first node for a given discrete time step n begins to\nbe encoded in I(t) . Using equation 4, we can write the voltage Q(t) on the capacitor at time θN as\nQ(θN) = Q(0)e− θN τ + ˆ θN 0 I(s)W (s)e− θN−s τ ds (5)\nFor 0 < t < θN , we have\nI(t) = xi(n),W (t) = wi, for θ(i− 1) < t < θi (6)\nIntegrating equation 5 yields\nQ(θN) = Q(0)e− θN τ + N∑ i=1 xi(n)ηiwi, ηi = e − θ(N−i)τ (1− e− θτ )τ (7)\nEquation 7 shows that, at time θN , the voltage on the capacitor is a linear combination of the reservoir states for the discrete time n, with node-dependent coefficients ηiwi, plus a residual of the voltage at time 0, multiplied by an extinction coefficient e− θN τ . At time 2θN the voltage on the capacitor would be a linear combination of the states for discrete time n+ 1, multiplied by the same coefficients, plus a residual of the voltage at time θN , and so on for all values of n and corresponding multiples of θN .\nA simple procedure would encode the weights wi = Wiηi onto the voltage V (t) that drives the modulator , provide an external, constant bias Wb, and have the output y(n) of the reservoir, defined by equation 2, effectively encoded on the capacitor. This simple procedure would however be unsatisfactory because unavoidably some of the ηi would be very small, and therefore the wi would be large, spanning several orders of magnitude. This is undesirable, as it requires a very precise control of the modulation voltage V (t) in order to recreate all the\nwi values, leaving the system vulnerable to noise and to any non-ideal behavior of the modulator itself.\nTo mitigate this, we adapt the training algorithm based on ridge regression to our case. We redefine the reservoir states as ξi(n) = xi(n)ηi; we then calculate the weights ωi that, applied to the states ξi, give the best approximation to the desired output ŷ(n). The advantage here is that ridge regression keeps the norm of the weight vector to a minimum; by redefining the states, we can take the ηi into account without having big values of wi that force us to be extremely precise in generating the readout weights.\nA sample trace of the voltage on the capacitor is shown in Figure 2c.\nHardware implementation To implement the analog readout, we started from the experimental architecture described in Section 2, and we added the components depicted in the right part of Figure 1. For the weight multiplication, we used a second Mach-Zehnder modulator (Photline model MXDO-LN-10 with bandwidth in excess of 10GHz and Vπ = 5.9V ), driven by a Tabor 2074 Arbitrary Waveform Generator (maximum sampling rate 200 MSamples/s). The two outputs of the modulator were fed into a balanced photodiode (Terahertz technologies model 527 InGaAs balanced photodiode, bandwidth set to 125MHz, response set to 1000V/W), whose output was read by the National Instruments PXI digital acquisition card (sampling rate 200 MSamples/s).\nIn most of the experimental results described here, the capacitor at the end of the circuit was simulated and not physically inserted into the circuit: this allowed us to quickly cycle in our experiments through different values of τ without taking apart the circuit every time. The external biasWb to the output, introduced in equation 2, was also provided after the readout. The reasoning behind these choices is that both these implementations are straightforward, while the use of a modulator and a balanced photodiode as a weight generator is more complex: we chose to focus on the latter issue for now, as our goal is to validate the proposed architecture."
    }, {
      "heading" : "4 Results",
      "text" : "As a benchmark for our analog readout, we use a wireless channel equalization task, introduced in 1994 [19] to test adaptive bilinear filtering and subsequently used by Jaeger [16] to show the capabilities of reservoir computing. This task is becoming a standard benchmark task in the reservoir computing community, and has been used for example in [20]. It consists in recovering a sequence of symbols transmitted along a wireless channel, in presence of multiple reflections, noise and nonlinear distortion; a more detailed description of the task can be found in the Appendix. The performance of the reservoir is usually measured in Symbol Error Rate (SER), i.e. the rate of misinterpreted symbols, as a function of the amount of noise in the wireless channel.\nFigure 3 shows the performance of the experimental setup of [10] for a network of 28 nodes and one of 64 nodes, for different amounts of noise. For each noise level, three quantities are presented. The first is the performance of the reservoir with a digital readout (blue triangles), identical to the one used in [10]. The second is the performance of a simulated, ideal analog readout, which takes into account the effect of the ηi coefficients introduced in equation 7, but no other imperfection. It produces as output the discrete sum ωb + ∑N i=1 ξiωi (red squares). This is, roughly speaking, the goal performance for our experimental readout. The third and most important is the performance of the reservoir as calculated on real data taken from the analog reservoir with the analog output, with the effect of the continuous capacitive integration computed in simulation (black circles).\nAs can be seen from the figure, the performance of the analog readout is fairly close to its ideal value, although it is significantly worse than the performance of the digital readout. However, it is already better than the non-reservoir methods reported in [19] and used by Jaeger as benchmarks in [16]. It can also handle higher signal-to-noise ratios. As expected, networks with more nodes have better performance; it should be noted, however, that in experimental reservoirs the number of nodes cannot be raised over a certain threshold. The reason is that the total loop time θN is determined by the experimental hardware (specifically, the length of the delay line); as N increases, the length θ of each node must decrease. This leaves the experiment vulnerable to noise and bandpass effect, that may lead, for example, to an incorrect discretization of the xi(n) values, and an overall worse performance.\nWe did test our readout with a 70nF capacitor, with a network of 28 nodes, to prove that the physical implementation of our concept is feasible: the performance of this setup is shown in the left panel of Figure 3. The results are\ncomparable to those obtained in simulation, even if, at low levels of noise in the input, the performance of the physical setup is slightly worse.\nThe rightmost panel of figure 3 shows the effects of the choice of the capacitor at the end of the circuit, and therefore of the value of τ . The plot represents the performance at 28 dB SNR for a network of 64 nodes, for different values of the ratio τ/θN , obtained by averaging the results of 10 tests. It is clear that the choice of τ has a complicated effect on the readout performance; however, some general rules may be inferred. Too small values of τ mean that the contribution from the very first nodes is vanishingly small, effectively decreasing the reservoir dimensionality, which has a strong impact on the performance both of the ideal and the experimental reservoir. On the other hand, larger values of τ impact the performance of the experimental readout, as the residual term in equation 7 gets larger. A compromise value of τ/θN = 0.222 seems to give the best result, corresponding in our case to a capacity of about 70 nF."
    }, {
      "heading" : "5 Discussion",
      "text" : "To our knowledge, the system presented here is the first analog readout for an experimental reservoir computer. While the results presented here are preliminary, and there is much optimization of experimental parameters to be done, the system already outperforms non-reservoir methods. We expect to extend easily this approach to different tasks, already studied in [9, 10], including a spoken digit recognition task on a standard dataset.3\nFurther performance improvements can reasonably be expected from finetuning of the training parameters: for instance the amount of regularization in the ridge regression procedure, that here is left constant at 1 · 10−4, should be tuned for best performance. Adaptive training algorithms, such as the ones mentioned in [21], could also take into account nonidealities in the readout components. Moreover the choice of τ, as Figure 3 shows, is not obvious and a more extensive investigation could lead to better performance.\nThe architecture proposed here is simple and quite straightforward to realize. It is very modular, meaning that it can be added at the output of any preexisting time multiplexing reservoir with minimal effort, whether it is based on optics or electronics. The capacitor at the end of the circuit could probably be substituted with a more complicated, active electronic circuit performing the summation of the incoming signal before resetting itself. This would eliminate the problem of residual voltages, and allow better performance at the cost of increased complexity of the readout.\nThe main interest of the analog readout is that it allows optoelectronic reservoir computers to fully leverage their main characteristic, which is the speed of operation. Indeed, removing the need for slow, offline postprocessing is indicated in [13] as one of the major challenges in the field. Once the training is finished, optoelectronic reservoirs can process millions of nonlinear nodes per\n3Texas Instruments-Developed 46-Word Speaker-Dependent Isolated Word Corpus (TI46), September 1991, NIST Speech Disc 7-1.1 (1 disc) (1991).\nsecond [10]; however, in the case of a digital readout, the node states must be recovered and postprocessed to obtain the reservoir outputs. It takes around 1.6 seconds for the digital readout in our setup to retrieve and digitize the states generated by a 9000 symbol input sequence. The analog readout removes the need for postprocessing, and can work at a rate of about 8.5 µs per input symbol, five orders of magnitude faster than the electronic reservoir reported in [8].\nFinally, having an analog readout opens the possibility of feedback - using the output of the reservoir as input or part of an input for the successive time steps. This opens the way for different tasks to be performed [15] or different training techniques to be employed [14]."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by the Interuniversity Attraction Poles program of the Belgian Science Policy Office, under grant IAP P7-35 «photonics@be»"
    }, {
      "heading" : "Appendix: Nonlinear Channel Equalization task",
      "text" : "What follows is a detailed description of the channel equalization task used to test the reservoir performance. The goal for this task is to reconstruct a sequence d(n) of symbols taken from {−3,−1, 1, 3}. The symbols in d(n) are mixed together in a new sequence q(n) given by\nq(n) = 0.08d(n+ 2)− 0.12d(n+ 1) + d(n) + 0.18d(n− 1)− 0.1d(n-2) (8) +0.091d(n− 3)-0.05d(n− 4) + 0.04d(n− 5) + 0.03d(n− 6) + 0.01d(n-7)\nwhich models a wireless signal reaching a receiver through different paths with different traveling times. A noisy, distorted version u(n) of the mixed signal q(n), simulating the nonlinearities and the noise sources in the receiver, is created by having u(n) = q(n) + 0.036q(n)2 − 0.011q(n)3 + ν(n), where ν(n) is an i.i.d. Gaussian noise with zero mean adjusted in power to yield signal-to-noise ratios ranging from 12 to 32 dB. The sequence u(n) is then fed to the reservoir as an input; the output of the readout R(n) is rounded off to the closest value among {−3,−1, 1, 3}, and then compared to the desired symbol d(n). The performance is usually measured in Signal Error Rate (SER), or the rate of misinterpreted symbols."
    } ],
    "references" : [ {
      "title" : "The \"echo state\" approach to analysing and training recurrent neural networks",
      "author" : [ "H. Jaeger" ],
      "venue" : "Technical report, Technical Report GMD Report 148, German National Research Center for Information Technology,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "Real-time computing without stable states: A new framework for neural computation based on perturbations",
      "author" : [ "W. Maass", "T. Natschlager", "H. Markram" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "An overview of reservoir computing: theory, applications and implementations",
      "author" : [ "B. Schrauwen", "D. Verstraeten", "J. Van Campenhout" ],
      "venue" : "In Proceedings of the 15th European Symposium on Artificial Neural Networks,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Reservoir computing approaches to recurrent neural network training",
      "author" : [ "M. Lukosevicius", "H. Jaeger" ],
      "venue" : "Computer Science Review,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Pattern recognition in a bucket",
      "author" : [ "C. Fernando", "S. Sojakka" ],
      "venue" : "Advances in Artificial Life,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Edge of chaos computation in mixed-mode vlsi - a hard liquid",
      "author" : [ "F. Schurmann", "K. Meier", "J. Schemmel" ],
      "venue" : "In In Proc. of NIPS. MIT Press,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Reservoir computing: a photonic neural network for information processing. volume 7728, page 77280B",
      "author" : [ "Y. Paquot", "J. Dambre", "B. Schrauwen", "M. Haelterman", "S. Massar" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Information processing using a single dynamical node as complex system",
      "author" : [ "L. Appeltant", "M.C. Soriano", "G. Van der Sande", "G. Danckaert", "S. Massar", "J. Dambre", "B. Schrauwen", "C.R. Mirasso", "I. Fischer" ],
      "venue" : "Nature Communications,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Photonic information processing beyond Turing: an optoelectronic implementation of reservoir computing",
      "author" : [ "L. Larger", "M.C. Soriano", "D. Brunner", "L. Appeltant", "J.M. Gutierrez", "L. Pesquera", "C.R. Mirasso", "I. Fischer" ],
      "venue" : "Optics Express,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Optoelectronic reservoir computing",
      "author" : [ "Y. Paquot", "F. Duport", "A. Smerieri", "J. Dambre", "B. Schrauwen", "M. Haelterman", "S. Massar" ],
      "venue" : "Scientific reports,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "What makes a dynamical system computationally powerful",
      "author" : [ "R. Legenstein", "W. Maass" ],
      "venue" : "New Directions in Statistical Signal Processing: From Systems to Brain. MIT Press,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Photonic reservoir computing: A new approach to optical information processing",
      "author" : [ "K. Vandoorne", "M. Fiers", "D. Verstraeten", "B. Schrauwen", "J. Dambre", "P. Bienstman" ],
      "venue" : "In 2010 12th International Conference on Transparent Optical Networks,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Optical computing: Photonic neural networks",
      "author" : [ "D. Woods", "T.J. Naughton" ],
      "venue" : "Nature Physics,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Generating coherent patterns of activity from chaotic neural networks. Neuron",
      "author" : [ "D. Sussillo", "L.F. Abbott" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Optimization and applications of echo state networks with leaky-integrator neurons",
      "author" : [ "H. Jaeger", "M. Lukosevicius", "D. Popovici", "U. Siewert" ],
      "venue" : "Neural networks : the official journal of the International Neural Network Society,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication",
      "author" : [ "H. Jaeger", "H. Haas" ],
      "venue" : "Science, 304(5667):78–80,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "Memory versus nonlinearity in reservoirs",
      "author" : [ "D. Verstraeten", "J. Dambre", "X. Dutoit", "B. Schrauwen" ],
      "venue" : "In The 2010 International Joint Conference on Neural Networks (IJCNN),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Stable output feedback in reservoir computing using ridge regression",
      "author" : [ "F. Wyffels", "B. Schrauwen" ],
      "venue" : "Artificial Neural Networks-ICANN,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Adaptive algorithms for bilinear filtering",
      "author" : [ "J. Mathews. V" ],
      "venue" : "Proceedings of SPIE,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1994
    }, {
      "title" : "Minimum complexity echo state network",
      "author" : [ "A. Rodan", "P. Tino" ],
      "venue" : "IEEE transactions on neural networks,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "A rewardmodulated hebbian learning rule can explain experimentally observed network reorganization in a brain control task",
      "author" : [ "R. Legenstein", "S.M. Chase", "A.B. Schwartz", "W. Maass" ],
      "venue" : "The Journal of neuroscience : the official journal of the Society for Neuroscience,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Jaeger [1] and by W.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 1,
      "context" : "Maass [2].",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "The result is a powerful system that can outperform other techniques on a range of tasks (see for example the ones reported in [3, 4]), and is significantly easier to train than recurrent neural",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "The result is a powerful system that can outperform other techniques on a range of tasks (see for example the ones reported in [3, 4]), and is significantly easier to train than recurrent neural",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].",
      "startOffset" : 59,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].",
      "startOffset" : 59,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].",
      "startOffset" : 59,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].",
      "startOffset" : 203,
      "endOffset" : 213
    }, {
      "referenceID" : 8,
      "context" : "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].",
      "startOffset" : 203,
      "endOffset" : 213
    }, {
      "referenceID" : 9,
      "context" : "Furthermore it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that hardware implementations with performance comparable to digital implementations have been reported [8, 9, 10].",
      "startOffset" : 203,
      "endOffset" : 213
    }, {
      "referenceID" : 10,
      "context" : "The last requirement essentially means that the dynamics allows the exploration of a large number of network states when new inputs come in, while at the same time retaining for a finite time information on the previous inputs [11].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 0,
      "context" : "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].",
      "startOffset" : 119,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].",
      "startOffset" : 119,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].",
      "startOffset" : 119,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].",
      "startOffset" : 119,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].",
      "startOffset" : 198,
      "endOffset" : 211
    }, {
      "referenceID" : 7,
      "context" : "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].",
      "startOffset" : 198,
      "endOffset" : 211
    }, {
      "referenceID" : 8,
      "context" : "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].",
      "startOffset" : 198,
      "endOffset" : 211
    }, {
      "referenceID" : 9,
      "context" : "For this reason, the reservoir computers appearing in literature use widely different nonlinear units, see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed in [7, 8, 9, 10].",
      "startOffset" : 198,
      "endOffset" : 211
    }, {
      "referenceID" : 9,
      "context" : "Very recently, optoelectronic reservoir computers have been demonstrated by different research teams [10, 9], conjugating good computational performances with the promise of very high operating speeds.",
      "startOffset" : 101,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "Very recently, optoelectronic reservoir computers have been demonstrated by different research teams [10, 9], conjugating good computational performances with the promise of very high operating speeds.",
      "startOffset" : 101,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "An analog readout for experimental reservoirs would remove this major bottleneck, as pointed out in [13].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 13,
      "context" : "Moreover, an analog readout opens the possibility of feeding back the output of the reservoir into the reservoir itself, which in turn allows the use of different training techniques [14] and to apply reservoir computing to new categories of tasks, such as pattern generation [15, 16].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 14,
      "context" : "Moreover, an analog readout opens the possibility of feeding back the output of the reservoir into the reservoir itself, which in turn allows the use of different training techniques [14] and to apply reservoir computing to new categories of tasks, such as pattern generation [15, 16].",
      "startOffset" : 276,
      "endOffset" : 284
    }, {
      "referenceID" : 15,
      "context" : "Moreover, an analog readout opens the possibility of feeding back the output of the reservoir into the reservoir itself, which in turn allows the use of different training techniques [14] and to apply reservoir computing to new categories of tasks, such as pattern generation [15, 16].",
      "startOffset" : 276,
      "endOffset" : 284
    }, {
      "referenceID" : 9,
      "context" : "The mechanism has been tested experimentally using the experimental reservoir reported in [10], and compared to a digital readout.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "Although the results are preliminary, they are promising: while not as good as those reported in [10], they are however already better than non-reservoir methods for the same task [16].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "Although the results are preliminary, they are promising: while not as good as those reported in [10], they are however already better than non-reservoir methods for the same task [16].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 16,
      "context" : "The number of nodes in a reservoir computer determines an upper limit to the reservoir performance [17]; this can be an obstacle when designing physical implementations of RCs, which should contain a high number of interconnected nonlinear units.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "A solution to this problem proposed in [7, 8], is time multiplexing: the xi(n) are computed one by one by a single nonlinear element, which receives a combination of the input u(n) and a previous state xj(n − 1).",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "A solution to this problem proposed in [7, 8], is time multiplexing: the xi(n) are computed one by one by a single nonlinear element, which receives a combination of the input u(n) and a previous state xj(n − 1).",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "[8], or by using an instantaneous nonlinear element and desynchronizing the input with respect to the delay line [10].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[8], or by using an instantaneous nonlinear element and desynchronizing the input with respect to the delay line [10].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "The hardware reservoir computer we use in the present work is identical to the one reported in [10] (see also [9]).",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "The hardware reservoir computer we use in the present work is identical to the one reported in [10] (see also [9]).",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "In the experiment reported in [10] a portion of the light coming out of the MZ is deviated to a second photodiode (not shown in Figure 1), that converts it",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : "The optimal readout weights Wi and bias Wb are then calculated on a computer from a subset (training set) of the recorded states, using ridge regression [18], and the output y(n) is then calculated using equation 2 for all the states collected.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "In optical implementations [10, 9] the states xi are encoded as light intensities which are always positive, so they cannot be subtracted one from another.",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "In optical implementations [10, 9] the states xi are encoded as light intensities which are always positive, so they cannot be subtracted one from another.",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "As a benchmark for our analog readout, we use a wireless channel equalization task, introduced in 1994 [19] to test adaptive bilinear filtering and subsequently used by Jaeger [16] to show the capabilities of reservoir computing.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "As a benchmark for our analog readout, we use a wireless channel equalization task, introduced in 1994 [19] to test adaptive bilinear filtering and subsequently used by Jaeger [16] to show the capabilities of reservoir computing.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 19,
      "context" : "This task is becoming a standard benchmark task in the reservoir computing community, and has been used for example in [20].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "Figure 3 shows the performance of the experimental setup of [10] for a network of 28 nodes and one of 64 nodes, for different amounts of noise.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "The first is the performance of the reservoir with a digital readout (blue triangles), identical to the one used in [10].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : "However, it is already better than the non-reservoir methods reported in [19] and used by Jaeger as benchmarks in [16].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "However, it is already better than the non-reservoir methods reported in [19] and used by Jaeger as benchmarks in [16].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "We expect to extend easily this approach to different tasks, already studied in [9, 10], including a spoken digit recognition task on a standard dataset.",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "We expect to extend easily this approach to different tasks, already studied in [9, 10], including a spoken digit recognition task on a standard dataset.",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "Adaptive training algorithms, such as the ones mentioned in [21], could also take into account nonidealities in the readout components.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "Indeed, removing the need for slow, offline postprocessing is indicated in [13] as one of the major challenges in the field.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "second [10]; however, in the case of a digital readout, the node states must be recovered and postprocessed to obtain the reservoir outputs.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 7,
      "context" : "5 μs per input symbol, five orders of magnitude faster than the electronic reservoir reported in [8].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "This opens the way for different tasks to be performed [15] or different training techniques to be employed [14].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 13,
      "context" : "This opens the way for different tasks to be performed [15] or different training techniques to be employed [14].",
      "startOffset" : 108,
      "endOffset" : 112
    } ],
    "year" : 2012,
    "abstractText" : "Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers.",
    "creator" : "LaTeX with hyperref package"
  }
}