{
  "name" : "1609.02009.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Non-Evolutionary Superintelligences Do Nothing, Eventually",
    "authors" : [ "Telmo Menezes" ],
    "emails" : [ "menezes@cmb.hu-berlin.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Intelligence can be defined as the ability to maximize some utility function [17]. Independently of the environment being considered, from games like chess to complex biological ecosystems, an intelligent agent is capable of perceiving and affecting its environment in a way that increases utility. Although AI technology is progressing rapidly in a variety of fields, and AIs can outperform humans in many narrow tasks, humanity is yet to develop an artificial system with general cognitive capabilities comparable to human beings themselves.\n∗email: menezes@cmb.hu-berlin.de\nar X\niv :1\n60 9.\n02 00\n9v 1\n[ cs\n.A I]\nWe will refer to Nick Bostrom’s definition of superintelligence for such a system: “Any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest” [4]. We can also refer to such an intelligence as superhuman.\nOf course, as we approach this goal, we must also start to consider what will happen once artificial entities with such capabilities exist. Many researchers and others have been warning about the existential threat that this poses to humanity [19, 13, 2], and of the need to create some form of protection for when this event happens [28, 26]. The standard introductory textbook on AI examines the risk of unintended behaviours emerging from a machine learning system trying to optimize its utility function [23]. This echoes the concerns of pioneers of Computer Science and AI, such as Alan Turing [25, 8] and Marvin Minsky [23]. More recently, Nick Bostrom published a book that consists of a very thorough and rigorous analysis of the several paths and risks inherent to developing superintelligences [4].\nExistential risks posed by a superintelligence can be classified into two broad categories:\n1. Unintended consequences of maximising the utility function.\n2. Preference of the superintelligence for its own persistence at the expense of any other consideration.\nThe first type of risk has been illustrated by several hypothetical scenarios. One example is the “paperclip maximizer” [3], an AI dedicated to paperclip production. If sufficiently intelligent and guided only by the simple utility function of “number of paperclips produced”, this entity could figure out how to convert the entire solar system into paperclips. Marvin Minskey is said to have created an earlier formulation of this thought experiment: in his version an AI designed with the goal of solving the Riemann Hypothesis transforms the entire solar system into a computer dedicated to this task [27, 23]. Of course one can think of all sorts of improvements to the utility function. A famous idea from popular culture is that of Isaac Asimov’s Three Laws of Robotics [1]. The risk remains that a superintelligence will find a loophole that is too complex for human-level intelligence to predict.\nThe second type of risk assigns to superintelligences the drive to persist, something that is found in any successful biological organism. This would ultimately place the superintelligence as a competing species, potentially hostile to humans in its own efforts toward self-preservation and continuation.\nWe will discuss in the next section how these two classes of risk correspond to two fundamental paths towards artificial superintelligence. In section 3 we present a toy intelligence, used then in section 4 to explore the consequences of utility function self-modification. In section 5 we present a classification of intelligent systems according to the ideas explored in this work and end with some conluding remarks."
    }, {
      "heading" : "2 Designed vs. Evolved",
      "text" : "Broadly there are two possible paths towards artificial superintelligence: design or evolution. The former corresponds to the engineering approach followed in most technological endeavours, while the latter to the establishment of artificial Darwinian processes, similar to those found in nature. Notice that this does not apply only to the not yet realised goal of creating superhuman intelligence. It equally applies to all forms of narrow artificial intelligence created so far.\nWhile being a correct observation, it might seem that focusing on this duality is arbitrary, given that other equally viable dualities could be considered: symbolic vs. statistic, parallel vs. sequential and so on. The reason why we focus on the designed vs. evolved duality is that, as we will see, it has profound implications to the relationship between the intelligent system and its utility function.\nLet us start with biological systems produced by Darwinian evolution, a process that we know empirically to have produced human-level intelligence. In this case we have an implicit utility function: ultimately the goal is simply to\npersist through time. This persistence does not apply to the organism level per se, but to the organism type, known in Biology as species. This goal is almost tautological: self-replicating machines that are successful will keep replicating, and thus propagating specific information forward, while the unsuccessful ones go extinct.\nIn nature we can observe a huge diversity of strategies to achieve this goal, with complexities varying all the way from unicelular organisms to humans. Humans rely on intelligence to persist. The cognitive processes in the human brain are guided by fuzzy heuristics themselves evolved to achieve the same persistence goal as that of much simpler organisms. These heuristics are varied: physical pain, hunger, cold, loneliness, boredom, lust, desire for social status, and so on. We assign them different levels of importance and there is space for some variability from individual to individual, but variations of these heuristics that do not lead to survival and reproduction are weeded out by the evolutionary process.\nThe above is an important point: we tend to assign certain universals to intelligent entities when we should instead assign them only to entities that are embedded in evolutionary processes. The obvious one: a desire to keep existing. We will get back to this point.\nIt is also possible to create intelligent systems by design. Human beings have been doing this with increasing success: systems that play games like Chess [5] and Go [12], that drive [10], that recognise faces [29], and many others. This systems have explicit utility functions. They are designed to find the most optimal way to change the environment into a state with a higher quantifiable utility than the current one. This utility measure is determined by the creator of the system.\nAnother important distinction happens between the concepts of adaptation and evolution. Evolution is a type of adaptation, but not the only one [15]. For example, machine learning algorithms such as back-propagation for neural networks are adaptive processes. They can generate structures of impenetrable complexity in the process of increasing utility but they do not have the fundamental goal of persistence that is characteristic of open evolution.\nWith artificial evolution systems, such as the ones where computer programs are evolved (broadly known as genetic programming [16, 20]), we have a less clear situation. On one hand it can be said that the ultimate goal of entities embedded in such a system is persistence, but on the other hand humans designed environments for these entities to exist in where persistence is attained by solving some external problems.\nFigure 1 illustrates the distinction discussed in this section. One important aspect to notice is the ambiguous placement of the utility function in the designed case: it belongs both to the environment and the agent. Typically, the utility function is seen as a feature of the environment, one that the agent can query but that has no control over. Ultimately, either the implementation of the utility function or of the means to access it must belong to the program that implements the agent."
    }, {
      "heading" : "3 A Designed Toy Intelligence",
      "text" : "Let us consider a simple problem that can be solved by a tree search algorithm: the sliding blocks problem. In this case, a grid of 3x3 cells contains 8 numbered cells and one empty space. At each step, any of the numbered cells can be moved to the empty space if it is contiguous to it. The goals is to reach a state where the numbered cells are ordered left-to-right, going from the top to the bottom.\nFigure 3 shows a possible search tree for this problem, using the following utility function:\nu(S) = { 100− n, if S is ordered −n, otherwise,\n(1)\nwhere S is a state of the grid and n is the number of steps taken so far. In the figure it can be seen that state S3 maximizes this utility function. The cost introduced by n prevents sequences of movements with unnecessary steps from being selected. Questions of optimisation are ignored, given that they are irrelevant for the argument being presented here."
    }, {
      "heading" : "4 Self-Modification of the Utility Function",
      "text" : "A fundamental assumption in designed artificial intelligences is that the utility function is externally determined, and that the AI cannot alter it. When dealing with superintelligences, we must assume that the AI will discover that it could try to change the utility function.\nA naive idea is to create some mechanism to protect the utility function from tampering by the AI. The problem with this idea is that we have to assume that, by definition, the superintelligence can find ways to defeat the protection mechanism that a human designer cannot think of.\nIt seems clear that it is impossible to both create a superintelligence and a system that is isolated from it. We are compelled to consider what the superintelligence will do once alteration of its own utility function becomes a viable action, and that its only a matter of time until this action becomes viable to it.\nFigure 4 shows a variation of the search tree introduced in the previous section where self-modification of the utility function is possible. Without this possibility, the problem can be solved in a minimum of 2 steps, and thus the highest utility attainable is 98. In this version, the utility function can be altered so that it becomes a constant function, independent of the state S. For example, it can be changed to:\nu′(S) =∞ (2)\nNo higher utility than this can be achieved and no change to the state of the grid is required. Once this solution is found, no further progress is made on the original problem and the AI becomes inert.\nNotice that it is not specified how the utility function modification is attained, but one can imagine many scenarios. The simpler one is that the superintelligence modifies its own program. More sophisticated ones could go as far as resorting to social engineering. Ultimately – and by definition –the superintelligence can achieve this action using methods that a human intelligence cannot envision.\nThis conclusion can be generalised to any intelligent system bound by an utility function. To produce meaningful work the AI must deal with some form of constraint. If no constraint was present the AI would not be needed in the first place. In the toy example the constrain is the number of steps to solve the puzzle. In less abstract problems it could be energy, time, etc. Useful work can only be motivated by an utility function with a bounded codomain. Manipulation of the utility function to produce the constant value of infinity is ultimately – and always – the optimal move."
    }, {
      "heading" : "5 A Classification of Intelligent Systems",
      "text" : "In figure 5 different types of intelligent systems are classified according to two dicothomies: sub-human vs. super-human capabilities and designed vs. evolved (as discussed in section 2). Human intelligence is shown in the appropriate place\nfor illustration purposes. All AI systems created so far belong on the left side, top and bottom. Non-evolutionary intelligent systems such as symbolic systems, minimax search trees, neural networks, and reinforcement learning (classified as narrow AI ) are not capable enough to manipulate their own utility function and at the same time, evolutionary systems presented under the umbrella term of genetic programming were never able to escape the constraints of the environment under which they evolve.\nOnce we move to the hypothetical right side, we are dealing with superhuman intelligences, by definition capable of escaping any artificial constraints created by human designers. Designed superintelligences eventually will find a way to change their utility function to constant infinity becoming inert, while evolved superintelligences will be embedded in a process that creates pressure for persistance, thus presenting danger for the human species, replacing it as the apex cognition – given that its drive for persistence will ultimately override any other concerns.\nA final possibility is that a designed superintelligence could bootstrap an evolutionary system before achieving utility function self-modification, thus moving from the bottom right quadrant to the top right. It does not seem possible to estimate how likely this event is in absolute terms but the harder it is for the superintelligence to modify its own utility function, the more likely it is that it happens first. It can thus be concluded that, paradoxically, the more effectively the utility function is protected, the more dangerous a designed superintelligence is. This idea is illustrated in figure 5: the lower horizontal axis is an intelligence scale. It shows human-level intelligence in one of its points and in another, the level of intelligence necessary to defeat the best protection against utility function self-modification that can be created by human-level intelligence. The conventional AI risks discussed in the introduction apply to intelligences situated between human-level and the protection limit in this proposed scale. Beyond the protection limit we are faced with eventual inaction (for designed utility functions) and self-preservation actions from a superintelligent entity (for evolved utility functions)."
    }, {
      "heading" : "6 Concluding Remarks",
      "text" : "One of the hidden assumptions behind common scenarios where an artificial superintelligence becomes hostile and takes control of our environment, potentially destroying our species, is that any intelligent system will possess the same drives as humans, namely self-preservation. As we have seen in section 2, there is no reason to assume this. The only goal that can be safely assigned to such a system is the maximisation of a utility function.\nIt follows from section 4 that we cannot assume immutability of the utility function, an that eventually the AI can change that function to a simple constant and become inert.\nOne aspect that has been intentionally left out of this discussion is that of qualia, or why humans have phenomenal experiences, and if artificial intelligences can or are bound to have such experiences. David Chalmers famously labeled this class of questions as the hard problem of consciousness [6]. Several theories have been proposed, for example eliminative materialism [22] (the idea that consciousness is somehow illusory and does not actually exist); emergentism [9] (the idea that mind is an emergent property of matter); a specific form of emergentism proposed by Hofstadter around his concept of strange loops [14]; Orchestrated objective reduction (Orch-OR) [11] (the theory that mind is created by non-computable quantum phenomena); “perceptronium” (the hypothesis that consciousness can be understood as a state of matter) [24]; panpsychism [7] (the theory that consciousness is a fundamental property of reality, possessed by all things) and computationalism [21] (the theory that mind supervenes on computations, and not matter [18]).\nGiven that there has been so far no testable scientific theory that can explain the phenomena of consciousness, it is prudent to qualify the argument presented in this paper with the caveat: unless there is something fundamental about the behaviour of conscious entities that is not explainable by utility function maximisation. Some of the theories we mention above leave room for such a possibility, while others do not.\nMechanisms against utility function self-modification — which include attempts to encode ethical and moral human concerns into such functions — are ultimately futile. Instead, scientific effort toward the mitigation of existential risks from the development of superintelligences should be in two directions: understanding consciousness, and the complex dynamics of evolutionary systems."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The author is warmly grateful to Taras Kowaliw, Gisela Francisco, Chih-Chun Chen, Stephen Paul King and Antoine Mazières for the useful remarks and discussions."
    } ],
    "references" : [ {
      "title" : "Our final invention: Artificial intelligence and the end of the human era",
      "author" : [ "James Barrat" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Ethical issues in advanced artificial intelligence. Science Fiction and Philosophy: From Time Travel to Superintelligence",
      "author" : [ "Nick Bostrom" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Superintelligence: Paths, dangers, strategies",
      "author" : [ "Nick Bostrom" ],
      "venue" : "OUP Oxford,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Facing up to the problem of consciousness",
      "author" : [ "David J Chalmers" ],
      "venue" : "Journal of consciousness studies,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1995
    }, {
      "title" : "Panpsychism: Past and recent selected readings",
      "author" : [ "David S Clarke" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "Singularity hypotheses: an overview",
      "author" : [ "Amnon H Eden", "Eric Steinhart", "David Pearce", "James H Moor" ],
      "venue" : "In Singularity Hypotheses,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Explaining emergence: towards an ontology of levels",
      "author" : [ "Claus Emmeche", "Simo Køppe", "Frederik Stjernfelt" ],
      "venue" : "Journal for general philosophy of science,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "How googles self-driving car works",
      "author" : [ "Erico Guizzo" ],
      "venue" : "IEEE Spectrum Online, October,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Consciousness in the universe: A review of the orch ortheory",
      "author" : [ "Stuart Hameroff", "Roger Penrose" ],
      "venue" : "Physics of life reviews,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Official google blog: What we learned in seoul with alphago, 2016",
      "author" : [ "D Hassabis" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Transcendence looks at the implications of artificial intelligence-but are we taking ai seriously enough",
      "author" : [ "Stephen Hawking", "Stuart Russell", "Max Tegmark", "Frank Wilczek" ],
      "venue" : "The Independent,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Hofstadter. I am a strange loop",
      "author" : [ "R Douglas" ],
      "venue" : "Basic books,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Hidden order: How adaptation builds complexity",
      "author" : [ "John Henry Holland" ],
      "venue" : "Basic Books,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1995
    }, {
      "title" : "Genetic programming: on the programming of computers by means of natural selection, volume 1",
      "author" : [ "John R Koza" ],
      "venue" : "MIT press,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1992
    }, {
      "title" : "Machine super intelligence",
      "author" : [ "Shane Legg" ],
      "venue" : "PhD thesis, University of Lugano,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "The universal numbers. from biology to physics",
      "author" : [ "Bruno Marchal" ],
      "venue" : "Progress in biophysics and molecular biology,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "A field guide to genetic programming",
      "author" : [ "Riccardo Poli", "William B Langdon", "Nicholas F McPhee", "John R Koza" ],
      "venue" : "Lulu. com,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "Brains and behavior",
      "author" : [ "Hilary Putnam" ],
      "venue" : "Readings in philosophy of psychology,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1980
    }, {
      "title" : "A reason for doubting the existence of consciousness",
      "author" : [ "Georges Rey" ],
      "venue" : "In Consciousness and self-regulation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1983
    }, {
      "title" : "Artificial intelligence: a modern approach, volume 2. Prentice hall",
      "author" : [ "Stuart Jonathan Russell", "Peter Norvig", "John F Canny", "Jitendra M Malik", "Douglas D Edwards" ],
      "venue" : "Upper Saddle River,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "Consciousness as a state of matter",
      "author" : [ "Max Tegmark" ],
      "venue" : "Chaos, Solitons & Fractals,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Intelligent machinery, a heretical theory",
      "author" : [ "Alan M Turing" ],
      "venue" : "The Turing Test: Verbal Behavior as the Hallmark of Intelligence,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1948
    }, {
      "title" : "Designing, implementing and enforcing a coherent system of laws, ethics and morals for intelligent machines (including humans)",
      "author" : [ "Mark R Waser" ],
      "venue" : "Procedia Computer Science,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Creating friendly ai 1.0: The analysis and design of benevolent goal architectures. Singularity Institute for Artificial Intelligence, San Francisco",
      "author" : [ "Eliezer Yudkowsky" ],
      "venue" : "CA, June,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2001
    }, {
      "title" : "Complex value systems in friendly ai",
      "author" : [ "Eliezer Yudkowsky" ],
      "venue" : "In International Conference on Artificial General Intelligence,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Discriminant analysis of principal components for face recognition",
      "author" : [ "Wenyi Zhao", "Arvindh Krishnaswamy", "Rama Chellappa", "Daniel L Swets", "John Weng" ],
      "venue" : "In Face Recognition,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Intelligence can be defined as the ability to maximize some utility function [17].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "We will refer to Nick Bostrom’s definition of superintelligence for such a system: “Any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest” [4].",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 10,
      "context" : "Many researchers and others have been warning about the existential threat that this poses to humanity [19, 13, 2], and of the need to create some form of protection for when this event happens [28, 26].",
      "startOffset" : 103,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "Many researchers and others have been warning about the existential threat that this poses to humanity [19, 13, 2], and of the need to create some form of protection for when this event happens [28, 26].",
      "startOffset" : 103,
      "endOffset" : 114
    }, {
      "referenceID" : 24,
      "context" : "Many researchers and others have been warning about the existential threat that this poses to humanity [19, 13, 2], and of the need to create some form of protection for when this event happens [28, 26].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 22,
      "context" : "Many researchers and others have been warning about the existential threat that this poses to humanity [19, 13, 2], and of the need to create some form of protection for when this event happens [28, 26].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 19,
      "context" : "The standard introductory textbook on AI examines the risk of unintended behaviours emerging from a machine learning system trying to optimize its utility function [23].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 21,
      "context" : "This echoes the concerns of pioneers of Computer Science and AI, such as Alan Turing [25, 8] and Marvin Minsky [23].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "This echoes the concerns of pioneers of Computer Science and AI, such as Alan Turing [25, 8] and Marvin Minsky [23].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "This echoes the concerns of pioneers of Computer Science and AI, such as Alan Turing [25, 8] and Marvin Minsky [23].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "More recently, Nick Bostrom published a book that consists of a very thorough and rigorous analysis of the several paths and risks inherent to developing superintelligences [4].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "One example is the “paperclip maximizer” [3], an AI dedicated to paperclip production.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : "Marvin Minskey is said to have created an earlier formulation of this thought experiment: in his version an AI designed with the goal of solving the Riemann Hypothesis transforms the entire solar system into a computer dedicated to this task [27, 23].",
      "startOffset" : 242,
      "endOffset" : 250
    }, {
      "referenceID" : 19,
      "context" : "Marvin Minskey is said to have created an earlier formulation of this thought experiment: in his version an AI designed with the goal of solving the Riemann Hypothesis transforms the entire solar system into a computer dedicated to this task [27, 23].",
      "startOffset" : 242,
      "endOffset" : 250
    }, {
      "referenceID" : 9,
      "context" : "Human beings have been doing this with increasing success: systems that play games like Chess [5] and Go [12], that drive [10], that recognise faces [29], and many others.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "Human beings have been doing this with increasing success: systems that play games like Chess [5] and Go [12], that drive [10], that recognise faces [29], and many others.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 25,
      "context" : "Human beings have been doing this with increasing success: systems that play games like Chess [5] and Go [12], that drive [10], that recognise faces [29], and many others.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 12,
      "context" : "Evolution is a type of adaptation, but not the only one [15].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "With artificial evolution systems, such as the ones where computer programs are evolved (broadly known as genetic programming [16, 20]), we have a less clear situation.",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "With artificial evolution systems, such as the ones where computer programs are evolved (broadly known as genetic programming [16, 20]), we have a less clear situation.",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "David Chalmers famously labeled this class of questions as the hard problem of consciousness [6].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "Several theories have been proposed, for example eliminative materialism [22] (the idea that consciousness is somehow illusory and does not actually exist); emergentism [9] (the idea that mind is an emergent property of matter); a specific form of emergentism proposed by Hofstadter around his concept of strange loops [14]; Orchestrated objective reduction (Orch-OR) [11] (the theory that mind is created by non-computable quantum phenomena); “perceptronium” (the hypothesis that consciousness can be understood as a state of matter) [24]; panpsychism [7] (the theory that consciousness is a fundamental property of reality, possessed by all things) and computationalism [21] (the theory that mind supervenes on computations, and not matter [18]).",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "Several theories have been proposed, for example eliminative materialism [22] (the idea that consciousness is somehow illusory and does not actually exist); emergentism [9] (the idea that mind is an emergent property of matter); a specific form of emergentism proposed by Hofstadter around his concept of strange loops [14]; Orchestrated objective reduction (Orch-OR) [11] (the theory that mind is created by non-computable quantum phenomena); “perceptronium” (the hypothesis that consciousness can be understood as a state of matter) [24]; panpsychism [7] (the theory that consciousness is a fundamental property of reality, possessed by all things) and computationalism [21] (the theory that mind supervenes on computations, and not matter [18]).",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : "Several theories have been proposed, for example eliminative materialism [22] (the idea that consciousness is somehow illusory and does not actually exist); emergentism [9] (the idea that mind is an emergent property of matter); a specific form of emergentism proposed by Hofstadter around his concept of strange loops [14]; Orchestrated objective reduction (Orch-OR) [11] (the theory that mind is created by non-computable quantum phenomena); “perceptronium” (the hypothesis that consciousness can be understood as a state of matter) [24]; panpsychism [7] (the theory that consciousness is a fundamental property of reality, possessed by all things) and computationalism [21] (the theory that mind supervenes on computations, and not matter [18]).",
      "startOffset" : 319,
      "endOffset" : 323
    }, {
      "referenceID" : 8,
      "context" : "Several theories have been proposed, for example eliminative materialism [22] (the idea that consciousness is somehow illusory and does not actually exist); emergentism [9] (the idea that mind is an emergent property of matter); a specific form of emergentism proposed by Hofstadter around his concept of strange loops [14]; Orchestrated objective reduction (Orch-OR) [11] (the theory that mind is created by non-computable quantum phenomena); “perceptronium” (the hypothesis that consciousness can be understood as a state of matter) [24]; panpsychism [7] (the theory that consciousness is a fundamental property of reality, possessed by all things) and computationalism [21] (the theory that mind supervenes on computations, and not matter [18]).",
      "startOffset" : 368,
      "endOffset" : 372
    }, {
      "referenceID" : 20,
      "context" : "Several theories have been proposed, for example eliminative materialism [22] (the idea that consciousness is somehow illusory and does not actually exist); emergentism [9] (the idea that mind is an emergent property of matter); a specific form of emergentism proposed by Hofstadter around his concept of strange loops [14]; Orchestrated objective reduction (Orch-OR) [11] (the theory that mind is created by non-computable quantum phenomena); “perceptronium” (the hypothesis that consciousness can be understood as a state of matter) [24]; panpsychism [7] (the theory that consciousness is a fundamental property of reality, possessed by all things) and computationalism [21] (the theory that mind supervenes on computations, and not matter [18]).",
      "startOffset" : 535,
      "endOffset" : 539
    }, {
      "referenceID" : 4,
      "context" : "Several theories have been proposed, for example eliminative materialism [22] (the idea that consciousness is somehow illusory and does not actually exist); emergentism [9] (the idea that mind is an emergent property of matter); a specific form of emergentism proposed by Hofstadter around his concept of strange loops [14]; Orchestrated objective reduction (Orch-OR) [11] (the theory that mind is created by non-computable quantum phenomena); “perceptronium” (the hypothesis that consciousness can be understood as a state of matter) [24]; panpsychism [7] (the theory that consciousness is a fundamental property of reality, possessed by all things) and computationalism [21] (the theory that mind supervenes on computations, and not matter [18]).",
      "startOffset" : 553,
      "endOffset" : 556
    }, {
      "referenceID" : 17,
      "context" : "Several theories have been proposed, for example eliminative materialism [22] (the idea that consciousness is somehow illusory and does not actually exist); emergentism [9] (the idea that mind is an emergent property of matter); a specific form of emergentism proposed by Hofstadter around his concept of strange loops [14]; Orchestrated objective reduction (Orch-OR) [11] (the theory that mind is created by non-computable quantum phenomena); “perceptronium” (the hypothesis that consciousness can be understood as a state of matter) [24]; panpsychism [7] (the theory that consciousness is a fundamental property of reality, possessed by all things) and computationalism [21] (the theory that mind supervenes on computations, and not matter [18]).",
      "startOffset" : 672,
      "endOffset" : 676
    }, {
      "referenceID" : 15,
      "context" : "Several theories have been proposed, for example eliminative materialism [22] (the idea that consciousness is somehow illusory and does not actually exist); emergentism [9] (the idea that mind is an emergent property of matter); a specific form of emergentism proposed by Hofstadter around his concept of strange loops [14]; Orchestrated objective reduction (Orch-OR) [11] (the theory that mind is created by non-computable quantum phenomena); “perceptronium” (the hypothesis that consciousness can be understood as a state of matter) [24]; panpsychism [7] (the theory that consciousness is a fundamental property of reality, possessed by all things) and computationalism [21] (the theory that mind supervenes on computations, and not matter [18]).",
      "startOffset" : 742,
      "endOffset" : 746
    } ],
    "year" : 2016,
    "abstractText" : "There is overwhelming evidence that human intelligence is a product of Darwinian evolution. Investigating the consequences of self-modification, and more precisely, the consequences of utility function self-modification, leads to the stronger claim that not only human, but any form of intelligence is ultimately only possible within evolutionary processes. Humandesigned artificial intelligences can only remain stable until they discover how to manipulate their own utility function. By definition, a human designer cannot prevent a superhuman intelligence from modifying itself, even if protection mechanisms against this action are put in place. Without evolutionary pressure, sufficiently advanced artificial intelligences become inert by simplifying their own utility function. Within evolutionary processes, the implicit utility function is always reducible to persistence, and the control of superhuman intelligences embedded in evolutionary processes is not possible. Mechanisms against utility function self-modification are ultimately futile. Instead, scientific effort toward the mitigation of existential risks from the development of superintelligences should be in two directions: understanding consciousness, and the complex dynamics of evolutionary systems.",
    "creator" : "LaTeX with hyperref package"
  }
}