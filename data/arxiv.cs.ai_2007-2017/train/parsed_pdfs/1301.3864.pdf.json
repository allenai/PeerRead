{
  "name" : "1301.3864.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probabilistic Arc Consistency: A Connection between Constraint Reasoning and Probabilistic Reasoning",
    "authors" : [ "Michael C. Horsch", "William S. Havens" ],
    "emails" : [ "mhorsch@cs.sfu.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We document a connection between constraint reasoning and probabilistic reasoning. We present an algorithm, called probabilistic arc consistency, which is both a generalization of a well known algorithm for arc consistency used in constraint reasoning, and a specialization of the belief updating algorithm for singly-connected networks. Our algorithm is exact for singly connected constraint problems, but can work well as an approximation for arbitrary problems. We briefly discuss some empirical results, and re lated methods.\n1 INTRODUCTION\nConstraint reasoning is about finding configurations which satisfy constraints, possibly optimizing the configurations according to some objective function. One of the most im portant tools in constraint reasoning is the process of arc consistency, which reduces the configuration space to those configurations which meet minimal local consistencies and their immediate consequences.\nIn this paper we present an algorithm, called probabilis tic arc consistency (pAC), which was developed to com pute solution probabilities, i.e., the frequency with which a variable takes on a particular value in all solutions, in constraint satisfaction problems. This information can be used as a heuristic to guide constructive search algorithms: for a given variable, choose the value which appears in the most solutions. Similar proposals for counting solu tions or estimating solution probabilities have been made [6, 14, 4, 11, 16, 15] . Solution probabilities are orthogonal to preference over solutions (e.g., [3, 1]), or probabilistic constraints (e.g.[1]) in which there is uncertainty regarding whether a constraint applies.\nThe main purpose of this paper is to document a connec tion between constraint reasoning and probabilistic reason-\ning. We show that pAC algorithm is a generalization of\nthe basic arc consistency algorithm AC-3 [10], and is also a specialization of the belief propagation algorithm [8] for singly-connected Bayesian networks. However, since the value of our method must be established empirically, we will briefly describe some of our empirical results in Sec tion 6. We feel our results are positive: we can report a dra matic decrease in search costs, i.e., number of backtracks, using pAC as compared to related methods for counting solutions. A detailed description of our results is found in [7].\nThis paper is organized as follows. In Section 2, we will provide a brief description of the belief propagation al gorithm for singly connected Bayesian networks, so that the comparison between algorithms can be self-contained. Section 2.1 presents an independence assumption which al lows CSPs to be represented compactly, and show how this assumption changes the belief propagation algorithm. Sec tion 3 will give a brief overview of constraint satisfaction problems, and show that the arc consistency algorithm is a specialization of belief propagation. In Section 4, we present the pAC algorithm itself, give a formal statement of correctness, and show how it generalizes the arc consis\ntency algorithm, and specializes the belief propagation al gorithm. In Section 5 we discuss the relationship between pAC and similar methods in the literature: Section 6 pro\nvides a summary of our empirical evaluation. In Section 7 we close with a discussion of these results."
    }, {
      "heading" : "2 BELIEF PROPAGATION IN BAYESIAN NETWORKS",
      "text" : "Kim and Pearl [8, 13] developed a polynomial-time al gorithm for singly-connected Bayesian networks. The method is based on message passing, and there are two message types: causal messages, denoted by the symbol 1r, are passed along the direction of the arcs in the DAG; diagnostic messages, denoted by the symbol A, are passed against the direction of the arcs in the DAG.\nThe posterior probability of a variable X given evidence E\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 283\nis computed by combining the messages it receives from its parents and children:\nP(X = xiE) = o:.A(x)1r(x) where o: is a normalization constant.\nThe effect of evidence in the descendants of X is summa rized by:\n.A(x) = II ,\\yi(x) j\nwhere the quantity ,\\yi(x) summarizes the effect of evi dence through child }j.\nThe effect of evidence in the ancestors of X is summarized by:\n1r(x) = L P(x lu1, ... , un) II 7rx(ui) (1) Ut, ... ,Un\nHere, the sum is over the space of assignments to X's par ent variables, and the product is the product of all messages received from any parents. The quantity 1r x ( ui) summa rizes the evidence through parent ui. Once X has determined its own posterior probability given the evidence \"mentioned\" in the messages received by X, it passes messages to its neighbours, with care taken to avoid double-counting of evidence. The message X sends to its parent Uk reflects all evidence seen by X except for the evidence already seen by uk:\nwhere o: is a normalization constant. Notice that in this equation, Ax ( Uk ) omits any information received from uk. The message X sends to its child }j reflects all evidence seen by X except for the evidence already seen by }j:\nwhere o: is a normalization constant. The first product in the above expression is over all children except the one to which X is sending the message; the outgoing 1r-message includes information received by all parents, as collected in 7r( X) . The initial conditions of the algorithm are as follows: if a node X has no parents, 1r(x) = P(X = x ). If a variable X has no children, then .A( xt) = 1 for all values Xt E S\"h. If X= x is given as evidence, then .A(x) = 1 and .A(xt ) = 0 for all values Xt E Ox, Xt # x. The correctness of this algorithm is guaranteed by the fact that the sources of evidence are independent. The complex ity of the algorithm, in terms of the number of messages sent, is linear in the number of nodes in the network. These results are proven in [ 13].\n2.1 A SIMPLE MODEL OF CAUSAL INDEPENDENCE\nConditional independence is a simplifying assumption based on structure in the factorization of a joint probabil ity distribution. This assumption is the basis for modelling joint distributions using a Bayesian network: a variable is conditionally independent of its non-descendants, given an assignment of values to the variable's parents.\nCausal independence is a simplifying assumption based on structure in the factorization of a conditional probability distribution. The idea behind causal independence is that, for a given configuration of a subset of its parents, the prob ability distribution of a variable may be independent of the remainder of its parents. To take an example from a di agnostic model, the proposition that a car starts when the ignition key is turned may depend directly on factors such as the amount of gas in the tank, the state of the battery, etc .. The conditional probability table allows for arbitrarily complex interactions among the parent values. However, the domain expert may judge that a car cannot start if the battery is dead, no matter how much gas is in the tank. On the other hand, if the battery voltage is low or high, the probability of starting may depend on the quantity of gas in the tank.\nThere have been many formalizations of causal indepen dence (e.g., [13, 17]), which allow for many interesting and important variations. The assumption we will use here is very simple. Suppose a variable X has parents {U1, ... , Un}. We will assume that the conditional proba bility table P(XIU1, ... , Un) can be factored as follows:\nn P(XIU1, ... , Un) = 0: II P(XIUi) i=l\nwhere o: is a normalizing constant. We will make this as sumption for every variable in a singly-connected Bayesian network.\nUnder this assumption, the 1r-terms in the polytree algo rithm are composed as follows:\n1r(x) = o: II L P(x lui)7rx(u;) (2) i ui The 1r-message sent from X to child }j is unchanged, once 1r(x) is computed.\nThe .A-message sent to parent uk is as follows:\n.Ax(uk) = o: L .A(x) II L P(x lui)7rx(ui) X k-:j:i U;\nNote that the .A-message is sent to Ui by summing over X's values. This is done because the algorithm assumes that the conditional distribution P(XIU1, .. . , Un) is stored locally for X, but is not stored locally for Uk.\n284 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nGiven our assumption of causal independence for ev ery variable in the network, the low dimensional ity of P(XIUi), and the fact that P(XIU;)P(U;) = P(U; IX)P(X), it is not unreasonable to allow U to store local information P(U; IX), even when X stores P(XIU;). We assume that this information is available. In this case, X need not sum out its own values before sending the .A-message; it can let U; sum out over X's values, using P(U; IX) . Thus, we can simplify the .A-messages by send ing the following A' -message from X to U;:\n.Au, (x) = a.A(x) II L P(xlu;)7rx ( u;) k:j:i U;\nThis message must be \"interpreted\" by U; by summing out the values of X. Under our causal independence assump tion, the original .A-message can be reconstructed by U; us ing the following:\n.Ax(u;) =a L P(udx).Au,(x) xEX\nNote that this is exactly how 1r-messages are treated, ac cording to the causal independence assumption. This im plies that under our assumption, the parent-child relation ship is symmetric, and we need only pass one kind of mes sage (to distinguish this method from the more general case where our causal independence assumption does not hold, we will call them 1-messages).\nThe message to neighbour V; is simply the product of all the information received by X except for the information received from V; itself. We define\nBel(x) =a II .Av, (x) i\nAs above, .Av, (x) describes the information about X re ceived from V;, but is not available directly. It is computed from the 1-message sent from V; to X as follows:\n.Av, (x) =a L P(x lv)lx (v) vEV;\nThe outgoing message to all neighbours is:\n/U,(x) =a IILP(xlv;).Av,(x) k:j:i V;\nWe have seen how a simple assumption of causal indepen dence leads to a simplification of the polytree belief propa gation algorithm. We have replaced 1r and .A messages with 1 messages. The determination of the local quantity .A v used above is derived from /V, and it represents the infor mation received by X from neighbour V.\n3 CONSTRAINT SATISFACTION\nPROBLEMS\nA constraint satisfaction problem (CSP) is posed as a set of variables, a domain for each variable, and a set of con-\nstraints over tuples of domain values (readers who would like more background on CSPs than can be presented in here are referred to [9]). The problem is to find an as signment of values to all variables which satisfies all the constraints. We will denote variables X;, 0 ::; i < n, and the domains D;. We will focus on binary CSPs, in which all the constraints are sets of tuples from pairs of domains, C;j C D; x Dj . As well, we will limit our discussion to domains which are finite and discrete.\nOne approach to solving CSPs is to use constructive search . Values are assigned to variables in some sequence, and af ter each assignment, the assignment is checked for consis tency with the constraints. If the assignment is consistent, the search continues recursively. If an assignment is incon sistent, a different value is chosen, and if no values for the current variable in the sequence are consistent, the search backtracks to the previously assigned variable, and contin ues the search. Heuristic information can be used to or der the values and the variables in an attempt to speed up search.\nThere is a well-documented problem with simple construc tive search, namely that there are values, and combinations of values, which never appear in any solution. If the con structive search method tries one of these assignments, it is guaranteed to have to backtrack. Furthermore, unless spe cial care is taken in the search algorithm, these assignments may be tried often during the search. These problems have been well-studied [10], and several pre-processing algo rithms have been devised to limit their effects. In partic ular, an algorithm called AC-3 reduces the domains of the variables in a CSP, by removing those values which are not consistent with some value of its neighbouring variables. We present AC-3 in more detail in the following section.\n3.1 ARC CONSISTENCY\nArc consistency can be defined as follows [10]. An arc (or edge) Cxy in a constraint graph for a binary CSP is arc consistent iff for all x E D x, there is a value y E Dy such that ( x, y) E C XY. In this definition, we assume that there are no unary constraints which may rule out values in D x or Dy. Clearly, CSPs need not be arc consistent, but if the domains of the CSP variables were reduced such that each arc was arc consistent, the cost of search could be reduced. When all arcs are arc consistent, we say the problem is arc consistent.\nAn algorithm for computing arc consistency is given in Fig ure 1 and is due to [10]. The underlying idea is to cycle through the variables in some order, and reduce the domain of each variable such that only values which are arc consis tent remain. The effects of reducing a domain are propa gated to neighbouring variables.\nThere are two key aspects of the AC-3 algorithm. First,\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 285\nin Revise, a domain element is removed if there is some neighbour that does not support the element. Equivalently, a domain element is supported if there is a supporting ele ment in all neighbours' domains. Let F; C D; be the set of supported values for variable i.\nF; ={xED; l'v'j EN;, 3y E Dj : (x, y) E C;j 1\\ y E Fj}\nwhere N; are the neighbours of X;, i.e., variables which share a constraint with i, and Fj C Dj is the set of sup ported values for variable j. We will write F; ( x) to rep resent the proposition x E F;, and similarly C;j(x, y) to represent (x, y) E C;j. Thus we have the following equiv alence\nThe use of'v' and 3 is purely by convention; using x and+ to represent 1\\ and V, resp. and using L and TI to represent boolean sums and products, we can rewrite this equivalence as follows:\nF;(x) = IT L (C;j(x, y) x Fj(Y)) (3) jEN; yEDj\nThis is the domain update rule applied to each variable dur ing the propagation of arc consistency.\nThe second key element in AC-3 is the mechanism for cy cling through the variables to ensure soundness of the al gorithm. As shown, a queue of pairs of constrained vari ables is maintained, and pairs are removed and added to the queue.\nThe purpose of the queue is to guarantee that changes to the domain of one variable are propagated to its neighbours. In other words, selecting edge ( k, m) from the queue results in a call to Revise(k, m), which updates Fk, as in Equa tion 3 above, using F m. The queue contains and orders messages between variables, and messages are processed in a sequential manner. The propagation mechanism could equally well be expressed as a distributed algorithm: af ter each change in the domain of a variable, the variable's neighbours are made aware of the changes, and change their domains accordingly.\nNote also that when the processing of edge ( k, m) results in a domain reduction (i.e., Revise(k, m) returns true), arcs ( i, k) are added to the queue, but the arc ( m, k) is explic itly omitted from the queue. That is, when a variable's do main is reduced due to information received from a neigh bour, it does not cause the neighbour to be checked for re vision. This is a matter of efficiency only, because, due to the symmetric nature of a binary constraint, the neigh bour's domain cannot be reduced if the arc were included in the queue.\nThus we have shown informally that AC-3 can be expressed as a special case of belief propagation in singly-connected\nprocedure Revise(i, j): begin\ndelete := false for each x E D; do\nif there is no y E D j such that ( x, y) E C;j then remove x from D; delete := true\nend if end for return delete\nend\nprocedure AC-3 begin\nQ := {(i,j)l(i,j) E arcs( G), if j} while Q not empty do\nselect and delete any arc ( k, m) from Q; if Revise(k,m) then\nQ := Q U{(i, k)l(i, k) E arcs( G), if k, if m} end if\nend while end\nFigure 1: AC-3. An algorithm for achieving arc consis tency.\nnetworks. To make the case explicit, we first observe that in a binary CSP, the neighbours of a node are causally independent, in the sense described in the previous sec tion. Second, we replace the conditional probability dis tributions P(X;IXj), i # j with boolean arrays represent ing C;j. Finally, the operations + and x are replaced by boolean operations V and /\\, respectively."
    }, {
      "heading" : "4 PROBABILISTIC ARC CONSISTENCY",
      "text" : "In this section we present the pAC algorithm, which we use to compute (or approximate, as we will see) the solution probabilities for every variable in a CSP. This information can be used as a heuristic to guide constructive search al gorithms: e.g., for a given variable, choose the value which appears in the most solutions.\nDefinition 1 Let < V, B > be a binary constraint satis faction problem with variables V, and binary constraints B, such that the constraint graph G =< V, B > is singly connected. For every X E V, let Dx be the domain of X, i.e., D x = { x 1, . . . , x m}. For every pair of variables (X, Y) such that there is a binary constraint Cxy E B, define the following:\nCxy(i, j) { � if\nm\n(x;, Yi) E Cxy otherwise\nL Cxy(i, j)M��(j) j=l\n286 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nF�)(i) a II s�l(i) {YI(X,Y)EB}\nm\nwhere a is such that L F�) ( i) = 1 { F�)(i) s�l(i)\n0\ni=l\nif s�l(i) > o\notherwise\nThe interpretation of the quantities s�t(i), F�)(i), M�¢1l(i) is as follows. The binary constraints in B are expressed numerically in Cxy, where Cxy(i,j) = 1 iff the pair ( x;, Yi) satisfies the binary constraint. The inter pretation of the remaining quantities is the subject of the following theorems (the proofs appear in [7]).\nLemma 1 Let X, Y be any constrained pair of variables in a CSP G whose constraint graph is singly-connected. Let G ' be the subproblem constructed in the following way: Include in V' only those variables from V which are k steps away from Y in the constraint graph of G , and which are separated from Y by X. Likewise, include in B' only those constraintsCxy E Bfor which bothX E V' andY E V'. M�t (i) is proportional to the number of times value x; E Dx is used in all solutions of the sub-problem G '.\nLemma 2 Let X be any variable in a CSP G whose con straint graph is singly connected. F�) ( i) is the relative frequency of the use of x; E Dx in all solutions of the sub-problem including only those variables in G which are distance k or less from X. If G is unsatisfiable, F�)(i) = 0 for all i.\nTheorem 3 Let G = < V, B > be a CSP such that the constraint graph for G is singly-connected, with diameter d. For any variable X E V, the relative frequency that value x; E Dx is used in all solutions of G is F�) (i).\nThe pAC equations can be expressed as a distributed pro cedure. Each variable X; is initialized to have uniform dis tributions. At each time step, each variable saves its previ ous distribution (F), and prepares to handle incoming mes sages. Messages (M) from neighbouring variables are pro cessed (S), and the results are stored locally, so that mes sages need not be sent to all neighbours if no changes were made in the distribution. The new distribution is computed by forming the product of all information stored from the most recent message received from all neighbours. Finally, if the variable's distribution has changed significantly, a new message (M) is sent to all neighbours, taking care not to double count.\nThe pAC equations require arbitrary precision floating point numbers. In our implementation, we use 64 bit float-\ning point numbers, at the risk of a non-trivial loss of pre cision when the CSPs get very large. In our experimental work, we can observe this phenomenon in about 5% of the large problems we have tried.\nThe pAC equations generalize arc consistency. If boolean values are used instead of probabilities, and boolean opera tors \"and\" and \"or\" instead of floating point multiplication and addition, the algorithm computes arc consistency. In singly-connected CSPs, the arc consistent domain values are used in some solution, and therefore have non-zero so lution probability.\nThe pAC equations are also a special case of Pearl & Kim's belief propagation algorithm for singly-connected belief networks. To prove this informally, it is sufficient to observe the following correspondences: C XY ( i, j) can be represented as P(X = x;IY = Yi), under the causal independence assumption given in Section 2.1; s�t (i), M�t(i), and F�)(i) correspond to ..\\y(x;), /Y(x;), and Bel(x;), resp., after k messages were received by X.\nThe guarantee of correctness only holds in CSPs with singly connected constraint graphs. For more general CSPs, the equations can be used to approximate solution probabilities for these problems, by iterating the equations some number of times. This approach has parallels with relaxation methods for belief propagation in Bayesian net works [ 12], and decoding turbo-codes [5]. The method is not guaranteed to converge to a stable set of probability distributions, and if it does, there is no guarantee that the approximations are useful. Thus the value of the method is an empirical question, which we explore elsewhere [7], and summarize in Section 6.\nTo limit computation costs, we use two parameters, c and Maxlter, to detect convergence or non-convergence. Iter ation continues while both of the following conditions are true:\nm_;x L(F�+l)(i)- F�)(i))2 > c (4) i\nk < Maxlter (5)\nWhen the change in solution probability is less than a given c, the process is declared to have converged; when the number of iterations exceeds Maxlter, the process is halted without convergence.\nThe approximate solution probabilities computed using pAC iteratively can be used to guide constructive search. For example, approximate solution probabilities can be computed before each assignment, and the probability used to induce a variable ordering, i.e., choose the variable whose maximum probability domain value is maximum over all unassigned variables, as well as a value ordering i.e., choose the most likely domain value for the variable.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 287\n5 RELATED WORK\nProbabilistic arc consistency is strongly related to tech\nniques proposed in the literature. Dechter and Pearl's [4] single spanning tree method (SST) determines an exact so lution count in singly-connected CSPs. The variables in a CSP are given an arbitrary ordering, in which Xj is cho sen as the root. The number of solutions in which Xj takes value Vt is computed recursively by the following:\nwhere\nN(Xj = x) = IIL N(Xc = y) C' D'\nC' { ciXc is a child of Xj} D' {y E Dc l(x, y) E Cjc}\nThe leaf variables X1 in the ordering have N ( X1 = v) = 1 for all v E D1. This method must be repeated for each variable if solution counts are necessary for all variables. For more general CSPs, the method is applied to the sub problem consisting of the tightest constraints in the original problem. The method is efficient, but can result in over optimistic approximations for the number of solutions in the original problem.\nMeisels et al.'s [11] universal propagation method (UP) uses a Bayesian network to compute solution counts for a designated variable, making the same independence as sumptions in Section 2.1. The Bayesian network is the constraint graph whose constraints are directed according to a pre-established variable ordering. The goal of the computation is to find the marginal probability distribu tion P(Xj) for the designated sink variable Xj. Under the same assumption of causal independence as described in Section 2.1:\nP(Xj = x) =a II L P(Xj = xiXc = y)P(Xc = y)\nwhere\nC' D'\nC' { ciXc is a parent of Xj} D' {y E De}\nand where P(Xj IX c) represents the constraint between Xj and Xc, and a is a normalization constant. When applied to CSPs whose constraint graph is singly-connected, the method reduces to that of[4], with the addition of a normal ization constant, and computes exact solution probabilities for a designated variable. For more general CSPs, the same method is applied, producing approximate solution counts, which can be over-optimistic.\nVemooy and Havens [16] method extends [4] by construct ing a forest of spanning trees from the original CPS's con straint graph. Using either of the previous methods, the\nsolution counts for each tree are determined. An approxi mate solution count for the original CSP is determined by combining the results from each tree, under the assumption\nthat the solutions to each tree are independent. Using this assumption, an approximation is made for P(X;), that is, the distribution of solutions for variable X; in the original CSP, by normalizing the product of the distributions from all trees:\nP(X; = x) =a II p(k)(X; = x) k\nwhere a is a normalization constant, and p(k)(X; = x) is the solution probability derived from the kth spanning tree. Vemooy and Havens explore this multiple spanning tree method (MST) as both a static and dynamic heuristic.\nPeleg [14] develops a probabilistic relaxation method in tended to find a satisfying assignment to the variables in a CSP. Although different in intent, the algorithm itself is\nsimilar to the algorithms discussed above. The difference, apart from the motivation, is a single factor in the expres sion for F�) ( i) in the pAC algorithm. Peleg's formula uses the previous iteration's estimate to modify the current esti\nmate:\nII {YI(X,Y)EB}\nIt should be noted that in this interpretation, F�) ( i) is no longer a solution probability, in the sense used in this paper. Rather, Peleg's method is intended to converge to a solution to the CSP, i.e., a distribution in which every variable has only one value with non-zero \"probability.\"\nThe work of Shazeer et al. [ 15] on computing probabilistic preferences over solutions to CSPs develops an algorithm which is essentially the same as the pAC algorithm. The main difference is that pAC uses the constraint graph struc ture itself, whereas the proposal by Shazeer et al. unrolls the constraint graph to form a large singly-connected struc\nture in which the nodes and arcs are repeated some fixed number of times. A minor difference is that pAC assumes a uniform prior over all values in a domain, so that the re sulting distributions are solution probabilities; the proposal by Shazeer et al. allows arbitrary probability distributions over the domain values to express more or less likely val ues. Thus these distributions are not strictly solution prob abilities, in the sense used in this paper.\n6 RESULTS\nWe have conducted an extensive investigation into the per formance of pAC as a method for approximating the solu tion frequencies of binary CSPs [7]. We evaluated the ac curacy of the approximation computed by pAC, and exam ined its effectiveness as a heuristic in constructive search.\n288 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nWe were able to show that there was a high correlation between the exact solution probabilities (as computed by\nexhaustive search) and the approximations computed by\npAC. We were also able to show that when used as a vari able ordering heuristic, the approximate solution probabil ities computed by pAC substantially reduce backtracking in constructive search, by up to two orders of magnitude. Space constraints prohibit a complete presentation of these results here.\nOur experiments were performed on randomly constructed CSPs, which can be described using four parameters: the number of variables, n, the number of values m, the con straint density p1 (the probability that any two variables share a constraint), and the constraint tightness, P2 (the probability that any pair of values is disallowed in a given constraint). It is well known that random CSPs vary greatly in difficulty, across the parameter space for these problems.\nWe evaluated the accuracy of the approximation by direct comparison to exact solution counts, computed by exhaus tive search. We applied pAC to 3 sets of random CSPs of varying topology and constrainedness, with n = 20, m = 10. The difficulty of these problems varied from very sim ple (having very many solutions, or being obviously over constrained), to very difficult (having only only a few solu tions). The convergence criteria were set fairly high, with f = 10-5 and Maxlter= 1000.\nOf the problems with fewer than 1 million solutions and at least one solution, the average correlation between the ex act solution probability and the approximation determined by pAC ranged between 0.83 for for Pl = 0.2 (sparse graphs), through 0.78 for Pl = 1.0 (complete graphs). As well, pAC was able to identify the majority of over constrained problems very quickly; this is what should be expected of a method which generalizes arc consistency.\nFor about 10% of the problems, pAC failed to converge. In some cases, the convergence was just very slow, and in other cases, the values computed by pAC oscillated be tween two distinct points in the distribution space. When pAC does not converge, any degree of accuracy is possible.\nWe also evaluated the effectiveness of the heuristic as used in constructive search. We compared pAC to the methods of Dechter and Pearl (SST) [4], Meisels et al. (UP) [11], Vemooy and Havens (SMST) [16] and Peleg [ 14]. Each of these methods was used as a static value ordering heuris tic (i.e., the heuristics were computed as a preprocessing step, but not updated as assignments were made). The MST method of Vemooy and Havens was also used dynamically (DMST), i.e., the solution probabilities were recomputed after every assignment. A random value ordering strategy was included in this comparison to provide a baseline.\nWe applied simple constructive search to 2 18 random prob lems, using the various heuristics as a value ordering\nheuristic. These problems were selected from a larger set of random problems, from which we discarded the over constrained problems. They were constructed with 20 vari ables and 10 values each, and p1 = 1.0 (complete graphs), and P2 E [0.1 , 0.23]. Some of these problems were very easy, but many were much more difficult.\nFigure 2 shows the results. The horizontal axis shows search costs, in terms of the number of backtracks; the ver tical axis indicates the cumulative fraction of the problems solved. Each curve shows the cumulative fraction of prob lems solved using a given number of backtracks. For ex ample, the median number of backtracks for each method can be read from the graph on the horizontal line through the 0.5 mark.\nThe graph clearly shows that pAC is superior to all the methods, except for Peleg's method, which was able to converge on a solution for roughly 66% of the problems, requiring no backtracking whatsoever. However, Peleg's method is not much of an improvement over random value orders for some problems. We repeated this experiment for random problems with different constraint densities, with very similar results.\nWe also evaluated the approximate solution probabilities computed by pAC when used as a dynamic variable and value ordering heuristic, i.e., after each assignment, the so lution probabilities were recomputed. We found that the reduction in search costs is dramatic, up to two orders of magnitude smaller than First Fail [6] or Least-Constrained [2].\nThe price for this success is the cost of computing the heuristic values: even for relatively large f, say f = 0.1, and few iterations, e.g., Maxiter= 50, the cost of perform ing all the floating point operations is such that search re quires almost an order of magnitude more time using ap proximate solution probabilities than the First Fail or Least Constrained heuristics.\n7 DISCUSSION AND FUTURE WORK\nIn this paper we have presented three algorithms, belief propagation (BP) in singly-connected Bayesian networks, arc consistency (AC-3) for binary constraint satisfaction problems, and probabilistic arc consistency (pAC) for bi nary CSPs. We have shown that probabilistic arc consis tency is a special case of the belief propagation algorithm, under an assumption of causal independence. We have also shown that the arc consistency algorithm is a special case of probabilistic arc consistency, specialized to perform of boolean arithmetic.\nIn the case of singly-connected topologies, all three algo rithms are exact, and run in polynomial time. The BP al gorithm assumes conditional independence of a variable's parents for correctness. Likewise, when a CSP's constraint\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 289\ngraph is singly-connected, the pAC algorithm is also exact, correctly computing solution probabilities for all variables. Finally, AC-3 is exact for all topologies and runs in poly nomial time. However, when the constraint graph is singly connected, constructive search over the reduced domains is backtrack-free. This property is not true for AC-3 on binary CSPs of arbitrary topology.\nThe BP algorithm has been suggested and studied as an approximation method for more general topologies (e.g., [13, 12]). It is almost startling that the propagation method, designed for singly connected graphs, often converges on arbitrary graphs to a distribution which is a reasonable approximation of the exact distribution. Obviously, the method will work well in graphs in which the conditional dependence is weak.\nSince approximation in Bayesian networks of arbitrary topology is NP-hard, it is expected that it might take a long time for BP to converge in at least a few instances. This has been observed in empirical studies, and in some cases, the algorithm has been observed to oscillate indefinitely [12].\nThe pAC algorithm can also be used to approximate solu tion probabilities for constraint problems of arbitrary topol ogy. Although pAC has stopping conditions, it is not guar anteed that the process will converge. Again, as solving binary CSPs is an NP-hard problem, it was expected that convergence of PAC might be slow in some cases. In some\ncases, as with the BP algorithm, the iterative process oscil lates.\nIf the process converges, it usually converges to a good ap proximation to the solution probabilities. However, when pAC is oscillating, any arbitrary cutoff point is likely to provide no better than random choice heuristic information. Fortunately, the oscillation problem can disappear when values are assigned; this means that oscillation will incur some extra search costs, but these seem to be small. Un fortunately, the reverse is true as well: making assignments can also induce oscillation on the remaining variables.\nWe have found that if pAC oscillates, it does so between two \"poles,\" that is, two distributions which favour dramat ically different configurations. We were able to construct a problem with 5 variables and two \"loops,\" on which pAC demonstrates oscillatory behaviour. The problem in this ex ample is that messages circulate around the loop, and due to loops having different lengths, arrive at loop junctures \"out of phase.\"\nThe relationship between belief propagation, arc consis tency and probabilistic arc consistency has demonstrated to be useful for constraint reasoning. In hindsight, it should have been obvious that the relationship would lead to fairly accurate approximations to solution probabilities, based on the experience of applying belief propagation to arbitrary Bayesian networks. The relationship should also prove use-\n290 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nful for probabilistic reasoning in ways that may not yet be obvious.\nWe are currently investigating a number of interesting is sues. Computing approximate solution probabilities using pAC as described in this paper is an expensive operation. There are many variations of the scheme which could be explored. The goal would be to reduce computational costs, and maintain the effectiveness of the heuristic. Obviously, the convergence criteria can be modified to reduce the num ber of iterations. Propagation could be limited to some sub set of the constraints (the work of [ 4, 16] limit propagation to tree structures, but other subsets are possible). We are\nalso investigating techniques to reason explicitly about the trade-offs between propagating solution probabilities, and search costs.\nRecently, a theory of so-called \"Semiring CSPs\" [1] has unified several variations of constraint problems, including satisfaction of classical CSPs, and optimization in proba bilistic, fuzzy and valued CSPs. As described in [14, 15], the propagation of solution probabilities can perform some kinds of optimization by giving domain elements non uniform a priori weights. We are looking at using prop agation techniques to provide heuristic information for op\ntimization of different kinds of objective functions, extend ing both [3, 1].\nAcknowledgements\nThanks to Matt Vemooy for providing the data from his thesis, and to the anonymous referees for their helpful com ments.\nReferences\n[1] S. Bistarelli, U. Montanari, F. Rossi, T. Schieux, G. Verfaille, and H. Fargier. Semiring-Based CSPs\nand Valued CSPs: Frameworks, Properties and Com parison. Constraints, 4(3):199-240, 1999.\n[2] D. Brelaz. New methods to color the verticies of a\ngraph. JACM, 22(4):251-256, 1979.\n[3] Rina Dechter, Avi Dechter, and Judea Pearl. Opti\nmization in constraint networks. In Influence Dia grams, Belief Nets and Decision Analysis, pages 411- 425. John Wiley and Sons Ltd, 1990.\n[4] Rina Dechter and Judea Pearl. Network-based heuris tics for constraint-satisfaction problems. Artificial In telligence, 34:1-34, 1988.\n[5] Brendan J. Frey and David J. C. MacKay. A revo lution: Belief propagation in graphs with cycles. In\nAdvances in Neural Information Processing Systems, pages 479-485, 1998.\n[6] Richard M. Haralick and Gordon L. Elliot. Increasing tree search efficiency for constraint satisfaction prob\nlems. Artificial Intelligence, 14(3):263-313, 1980.\n[7] Michael C. Horsch and William S. Havens. How to\ncount Solutions to CSPs. Technical report, School of Computing Science, Simon Fraser University, 2000.\n[8] Jin H. Kim and Judea Pearl. A computational model\nfor causal and diagnostic reasoning in inference sys tems. In Proceedings of the Eighth Intemationalloint Conference on Artificial Intelligence, pages 190-193, 1983.\n[9] Vipin Kumar. Algorithms for constraint-satisfaction\nproblems: A survey. AI Magazine, 13(1):32-44, 1992.\n[10] Alan K. Mackworth. Consistency in networks of re lations. Artificiallntelligence, 8(1):99-118, 1977.\n[11] Arnnon Meisels, Solomon Ehal Shimonoy, and Gadi Solotorevsky. Bayes networks for estimating the\nnumber of solutions to a csp. In Proceedings of the Fourteenth National Conference on Artificial Intelli gence, 1997.\n[12] Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.\nLoopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, pages 467-475, 1999.\n[13] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Reasoning. Morgan Kaufmann Publishers, Los Altos, 1988.\n[ 14] Shmuel Peleg. A new probabilistic relaxation method.\nIEEE Transactions on Pattern Matching and Machine Intelligence, 2(4):362-369, 1980.\n[15] Noam M. Shazeer, Michael L. Littman, and Greg A.\nKeirn. Constraint satisfaction with probabilistic vari able values. Technical Report CS-99-03, Duke Uni versity, Department of Computer Science, 1999.\n[16] Matt Vemooy and William S. Havens. An examina\ntion of probabilistic value-ordering heuristics. In Pro ceedings of the 12th Australian Joint Conference on Artificial Intelligence, 1999.\n[17] Nevin Lianwen Zhang and David Poole. Exploit ing Causal Independence in Bayesian Network In\nference. Journal of Artificial Intelligence Research, 5:301-328, 1996."
    } ],
    "references" : [ {
      "title" : "Semiring-Based CSPs and Valued CSPs: Frameworks",
      "author" : [ "S. Bistarelli", "U. Montanari", "F. Rossi", "T. Schieux", "G. Verfaille", "H. Fargier" ],
      "venue" : "Properties and Com­ parison. Constraints,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1999
    }, {
      "title" : "New methods to color the verticies of a graph",
      "author" : [ "D. Brelaz" ],
      "venue" : "JACM, 22(4):251-256,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1979
    }, {
      "title" : "Opti­ mization in constraint networks. In Influence Dia­ grams, Belief Nets and Decision Analysis, pages",
      "author" : [ "Rina Dechter", "Avi Dechter", "Judea Pearl" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1990
    }, {
      "title" : "Network-based heuris­ tics for constraint-satisfaction problems",
      "author" : [ "Rina Dechter", "Judea Pearl" ],
      "venue" : "Artificial In­ telligence,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1988
    }, {
      "title" : "A revo­ lution: Belief propagation in graphs with cycles",
      "author" : [ "Brendan J. Frey", "David J.C. MacKay" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Increasing tree search efficiency for constraint satisfaction prob­ lems",
      "author" : [ "Richard M. Haralick", "Gordon L. Elliot" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1980
    }, {
      "title" : "How to count Solutions to CSPs",
      "author" : [ "Michael C. Horsch", "William S. Havens" ],
      "venue" : "Technical report,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2000
    }, {
      "title" : "A computational model for causal and diagnostic reasoning in inference sys­ tems",
      "author" : [ "Jin H. Kim", "Judea Pearl" ],
      "venue" : "In Proceedings of the Eighth Intemationalloint Conference on Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1983
    }, {
      "title" : "Algorithms for constraint-satisfaction problems: A survey",
      "author" : [ "Vipin Kumar" ],
      "venue" : "AI Magazine,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1992
    }, {
      "title" : "Consistency in networks of re­",
      "author" : [ "Alan K. Mackworth" ],
      "venue" : "lations. Artificiallntelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1977
    }, {
      "title" : "Bayes networks for estimating the number of solutions to a csp",
      "author" : [ "Arnn  on Meisels", "Solomon Ehal Shimonoy", "Gadi Solotorevsky" ],
      "venue" : "In Proceedings of the Fourteenth National Conference on Artificial Intelli­ gence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1997
    }, {
      "title" : "Loopy belief propagation for approximate inference: An empirical study",
      "author" : [ "Kevin P. Murphy", "Yair Weiss", "Michael I. Jordan" ],
      "venue" : "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Reasoning",
      "author" : [ "Judea Pearl" ],
      "venue" : "IEEE Transactions on Pattern Matching and Machine Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Similar proposals for counting solu­ tions or estimating solution probabilities have been made [6, 14, 4, 11, 16, 15] .",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "Similar proposals for counting solu­ tions or estimating solution probabilities have been made [6, 14, 4, 11, 16, 15] .",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "Similar proposals for counting solu­ tions or estimating solution probabilities have been made [6, 14, 4, 11, 16, 15] .",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : ", [3, 1]), or probabilistic constraints (e.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : ", [3, 1]), or probabilistic constraints (e.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "[1]) in which there is uncertainty regarding whether a constraint applies.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "the basic arc consistency algorithm AC-3 [10], and is also a specialization of the belief propagation algorithm [8] for singly-connected Bayesian networks.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "the basic arc consistency algorithm AC-3 [10], and is also a specialization of the belief propagation algorithm [8] for singly-connected Bayesian networks.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "A detailed description of our results is found in [7].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "Kim and Pearl [8, 13] developed a polynomial-time al­ gorithm for singly-connected Bayesian networks.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "Kim and Pearl [8, 13] developed a polynomial-time al­ gorithm for singly-connected Bayesian networks.",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "These results are proven in [ 13].",
      "startOffset" : 28,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : ", [13, 17]), which allow for many interesting and important variations.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 8,
      "context" : "A constraint satisfaction problem (CSP) is posed as a set of variables, a domain for each variable, and a set of constraints over tuples of domain values (readers who would like more background on CSPs than can be presented in here are referred to [9]).",
      "startOffset" : 248,
      "endOffset" : 251
    }, {
      "referenceID" : 9,
      "context" : "These problems have been well-studied [10], and several pre-processing algo­ rithms have been devised to limit their effects.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "Arc consistency can be defined as follows [10].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "An algorithm for computing arc consistency is given in Fig­ ure 1 and is due to [10].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "The inter­ pretation of the remaining quantities is the subject of the following theorems (the proofs appear in [7]).",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 11,
      "context" : "This approach has parallels with relaxation methods for belief propagation in Bayesian net­ works [ 12], and decoding turbo-codes [5].",
      "startOffset" : 98,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "This approach has parallels with relaxation methods for belief propagation in Bayesian net­ works [ 12], and decoding turbo-codes [5].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "Thus the value of the method is an empirical question, which we explore elsewhere [7], and summarize in Section 6.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Dechter and Pearl's [4] single spanning tree method (SST) determines an exact so­ lution count in singly-connected CSPs.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "'s [11] universal propagation method (UP) uses a Bayesian network to compute solution counts for a designated variable, making the same independence as­ sumptions in Section 2.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "When applied to CSPs whose constraint graph is singly-connected, the method reduces to that of[4], with the addition of a normal­ ization constant, and computes exact solution probabilities for a designated variable.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Vemooy and Havens [16] method extends [4] by construct­ ing a forest of spanning trees from the original CPS's con­ straint graph.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 6,
      "context" : "We have conducted an extensive investigation into the per­ formance of pAC as a method for approximating the solu­ tion frequencies of binary CSPs [7].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "We compared pAC to the methods of Dechter and Pearl (SST) [4], Meisels et al.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "(UP) [11], Vemooy and Havens (SMST) [16] and Peleg [ 14].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "We found that the reduction in search costs is dramatic, up to two orders of magnitude smaller than First Fail [6] or Least-Constrained [2].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "We found that the reduction in search costs is dramatic, up to two orders of magnitude smaller than First Fail [6] or Least-Constrained [2].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 12,
      "context" : ", [13, 12]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 11,
      "context" : ", [13, 12]).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 11,
      "context" : "This has been observed in empirical studies, and in some cases, the algorithm has been observed to oscillate indefinitely [12].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Propagation could be limited to some sub­ set of the constraints (the work of [ 4, 16] limit propagation to tree structures, but other subsets are possible).",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "Recently, a theory of so-called \"Semiring CSPs\" [1] has unified several variations of constraint problems, including satisfaction of classical CSPs, and optimization in proba­ bilistic, fuzzy and valued CSPs.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "timization of different kinds of objective functions, extend­ ing both [3, 1].",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "timization of different kinds of objective functions, extend­ ing both [3, 1].",
      "startOffset" : 71,
      "endOffset" : 77
    } ],
    "year" : 2011,
    "abstractText" : "We document a connection between constraint reasoning and probabilistic reasoning. We present an algorithm, called probabilistic arc consistency, which is both a generalization of a well known algorithm for arc consistency used in constraint reasoning, and a specialization of the belief updating algorithm for singly-connected networks. Our algorithm is exact for singly­ connected constraint problems, but can work well as an approximation for arbitrary problems. We briefly discuss some empirical results, and re­ lated methods.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}