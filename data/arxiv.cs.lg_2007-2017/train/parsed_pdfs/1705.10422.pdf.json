{
  "name" : "1705.10422.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation",
    "authors" : [ "Guan-Horng Liu", "Avinash Siravuru", "Sai Prabhakar", "Manuela Veloso", "George Kantor" ],
    "emails" : [ "guanhorl@andrew.cmu.edu", "avinashs@andrew.cmu.edu", "spandise@andrew.cmu.edu", "mmv@cs.cmu.edu,", "kantor@ri.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "One of the key challenges to building accurate and robust autonomous navigation systems is to develop a strong intelligence pipeline that is able to efficiently gather incoming sensor data and take suitable control actions with good repeatability and fault-tolerance. In the past, this was addressed in a modular fashion, where specialized algorithms were developed for each sub-system and finally integrated with some fine tuning. More recently, there is a great interest in end-to-end approach that can learn complex mappings that go directly from the input to the output by leveraging the availability of a large volume of task specific data.\nThis end-to-end approaches have become more appealing thanks to the extension of deep learning’s success to robotics, and has been applied in visuomotor policies for autonomous driving [2, 1, 42]. However, the traditional deep supervised learning-based driving requires a great deal of human annotation and may not be able to deal with the problem of accumulating errors [30]. On the other hand, deep reinforcement learning (DRL) offers a better formulation that allows policy improvement with feedback, and has been shown to achieve human-level performance on challenging game environments [22, 23, 28].\nIn this work, we present an end-to-end controller that uses multi-sensor input to learn an autonomous navigation policy in a physics-based gaming environment called TORCS [41] without needing any pretraining. To show the effectiveness of multimodal perception, we pick two popular continuous action DRL algorithms namely Normalized Advantage Function (NAF) [8] and Deep Deterministic Policy Gradient (DDPG) [20], and augment them to accept multimodal input. We limit our objective to only achieving autonomous navigation without any obstacles or other cars. This problem is kept simpler deliberately to focus our attention more on analyzing the performance of the proposed multimodal configurations using extensive quantitative and qualitative testing.\nar X\niv :1\n70 5.\n10 42\n2v 1\n[ cs\n.R O\n] 3\n0 M\nay 2\nMoreover, we also observe that a multimodal sensor policy might rely heavily on all the inputs to the extent that it may fail completely even if a single sensor broke down fully or partially. This undesirable consequence renders sensor redundancy useless. To ensure that the learned policy does not succumb to such over-fitting, we apply a novel stochastic regularization method called Sensor Dropout during training. Our approach reduces the policy sensitivity to a particular sensor subset, and make it capable of functioning even in the face of partial sensor failure. Based on the Sensor Dropout, we further augment the standard DRL loss with an auxiliary loss that help reduce the action variations of the trained policy. As far as we know, we are the first to address the multimodal sensor policy learning in terms of sensor fusion."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multimodal DRL aims to leverage the availability of multiple, potentially imperfect, sensor inputs to improve learned policy. Most autonomous driving vehicles have been equipped with an array of sensors like GPS, Lidar, Camera, and Odometer, etc [10]. While one would offer a long range noisy estimate, the other would offer a shorter range accurate one. When combined, the resulting observer will have a good and reliable estimate of the environment. It is important to note that some of these sensors, like GPS and odometers, are readily available but unfortunately seldom included in these end-to-end learning models [1].\nPrevious works in DRL predominantly learned policies based on a single input modality, i.e., either low-dimensional physical states, or high-dimensional pixels. For autonomous driving where enhancing safety and accuracy to the maximum possible extent is top priority, developing policies that operate with multiple inputs is the need of the hour. In fact, multimodal perception was an integral part of autonomous navigation solutions and even played a critical role in their success [38] before the advent of end-to-end deep learning based approaches. Sensor fusion offers several advantages, namely robustness to individual sensor noise/failure, improving object classification and tracking [6, 3, 4], etc. In this light, several recent works in DRL have tried to solve the complex robotics tasks such as human-robot-interaction [29], manipulation [17] and maze navigation [21] with multimodal sensor inputs. In principle, [21] is similar to this work as it uses image, velocity, and depth information to navigate through a maze. However, the robot evolves with simpler dynamics when compared to autonomous road navigation. Additionally, depth is only used as an auxiliary loss, i.e. it is not an input to the trained policy but is only used to improve the learning outcomes.\nMultimodal deep learning, in general, is an active area of research in other domains like audiovisual systems [27], gesture recognition [26], text/speech and language models [12, 34], etc. However, Multi-modal learning is conspicuous by its absence in the modern end-to-end autonomous navigation literature. Another challenge in multimodal learning is the specific case of over-fitting where instead of learning the underlying latent object representation using multiple diverse observations, the model instead learns a complex representation in the original space itself, rendering the whole point of multimodal sensing useless and computationally burdensome. An illustrative example for this case is a car navigating when all sensors remain functional but fails to navigate completely even if one fails or is partially corrupted. This kind of behavior is detrimental and suitable regularization measures should be setup during training to avoid it.\nStochastic regularization is an active area of research in deep learning made popular by the success of, Dropout [35]. Following this landmark paper, numerous extensions were proposed to further generalize this idea ([25, 39, 13, 7]). In the similar vein, two interesting techniques have been proposed for specialized regularization in the multimodal setting namely ModDrop [26] and ModOut [18]. Given a much wider set of sensors to choose from, ModOut attempts to identify which sensors are actually needed to fully observe the system behavior. Here, we assume that all the sensors are critical and we only focus on improving the state information based on inputs from multiple observers. ModDrop is much closer in spirit to the proposed Sensor Dropout (SD). However, unlike ModDrop, pretraining with individual sensor inputs using separate loss functions is not required. A network can be directly constructed in an end-to-end fashion and Sensor Dropout can be directly applied at the sensor fusion layer just like Dropout. Its appeal lies in its simplicity during implementation and is designed to be applicable even to the DRL setting. We show extensive simulation results to validate the net improvement in robustness and generalization with Sensor Dropout."
    }, {
      "heading" : "3 Multimodal Deep Reinforcement Learning",
      "text" : ""
    }, {
      "heading" : "3.1 Deep Reinforcement Learning (DRL) Brief Review",
      "text" : "We consider a standard Reinforcement Learning (RL) setup, where an agent operates in an environment E. At each discrete time step t, the agent observes a state st ∈ S, picks an action at ∈ A, and receives a scalar reward r(st, at) ∈ R from the environment. The return Rt = ∑T i=t γ\n(i−t)r(si, ai) is defined as total discounted future reward at time step t, with γ being a discount factor ∈ [0, 1]. The objective of the agent is to learn a policy that eventually maximizes the expected return. The learned policy, π, can be formulated as either stochastic π(a|s) = P(a|s), or deterministic a = µ(s). The value function V π and action-value function Qπ describe the expected return for each state and state-action pair upon following a policy π. Finally, an advantage function Aπ(st, at) is defined as the additional reward or advantage that the agent will have for executing some action at at state st and it is given by Aπ(st, at) = Qπ(st, at)− V π(st). In high dimensional state/action space, these functions are usually approximated by a suitable parametrization. Accordingly, we define θQ, θV , θA, θπ , and θµ as the parameters for approximating Q, V , A, π, and µ functions, respectively. It was generally believed that using non-linear function approximators would lead to unstable learning in practice. Recently, Mnih et al. [22] applied two novel modifications, namely replay buffer and target network, to stabilize the learning with deep nets. Later, several variants were introduced that exploited deep architectures and extended to learning tasks with continuous actions [20, 24, 8, 31].\nTo exhaustively analyze the effect of multi-sensor input and the new stochastic regularization technique, we pick two algorithms in this work namely DDPG and NAF. It is worth noting that the two algorithms are very different, with DDPG being an off-policy actor-critic method and NAF an off-policy value-based one. By augmenting these two algorithms, we highlight that any DRL algorithm, modified appropriately, can benefit from using multi-sensor inputs. Due to space constraint, we list the formulation of the two algorithms in Supplementary Material."
    }, {
      "heading" : "3.2 Multimodal Sensor Policy Architecture",
      "text" : "We denote a set of observations composed from M sensors as, S = [S(1) S(2) .. S(M)]T , where S(i) stands for observation from ith sensor. In the multimodal network, each sensory signal is pre-processed along independent path. Each path has a feature extraction module that can be either pure identity function (modality 1), or convolution-based layer (modality 2→M ). The modularized feature extraction stage naturally allows for independent extraction of salient information that is transferable (with some tuning if needed) to other applications . The outputs of feature extraction modules are eventually flattened and concatenated to form the multimodal state. The schematic illustration of modularized multimodal policy is shown in Fig. 1."
    }, {
      "heading" : "4 Augmenting MDRL",
      "text" : "n this section, we propose two methods to improve training of a multi-sensor policy. We first introduce a new stochastic regularization called Sensor Dropout, and explain its advantages over the standard Dropout for this problem. Later, we propose an additional unsupervised auxiliary loss function to reduce the policy variance."
    }, {
      "heading" : "4.1 Sensor Dropout (SD) for Robustness",
      "text" : "The Sensor Dropout is a variant of the vanilla Dropout [35] that maintains dropping configurations on each sensor module instead of individual neuron. Though both methods share a similar motivation on stochastic regularization, SD is better-motivated for training the multimodal sensor policy. By randomly dropping the sensor block during training, the policy network is encouraged to exploit the modularized structure among each sensor stream. In the application to the complex robotics system, SD has advantages on handling imperfect sensing conditions such as latency, noises, and even partial sensor failure. As shown in Fig.1, consider the multimodal state S̃, obtained from feature extraction and given by S̃ = [S̃(1) S̃(2) .. S̃(M)]T , where S̃(i) = [X̃(i)1 X̃ (i) 2 .. X̃ (i) Ki ]T . The dropping configuration is defined as a M -dimensional vector c = [δ(1)c δ (2) c .. δ (M) c ]T , where each element δ (i) c ∈ {0, 1} represents the on/off indicator for the ith sensor modality. We now detail the two main differences between original Dropout and SD along with their interpretations.\nFirstly, note that the dimension of the dropping vector c is much lower than the one in the standard Dropout ( ∑M i=1Ki). As a consequence, the probability of the event where all sensors are dropped out (i.e. c0 = [0(1) 0(2) .. 0(M)]T ) is not negligible in SD. To explicitly remove c0, we slightly depart from [35] in modeling the SD layer. Instead of modeling SD as random process where any sensor block S̃(i) is switched on/off with a fixed probability p, we define the random variable as the dropping configuration c itself. Since there are N = 2M − 1 possible states for c, we accordingly sample from an N -state categorical distribution P. The categorical distribution not only offers convenience in analysis and interpretation over standard Bernoulli on sensor blocks, but is better-motivated in that it can be adaptive to the current sensor reliability during run-time. We denote the probability of a dropping configuration cj occurring with pj , where the subscript j ranges from 1 to N . The corresponding pseudo-Bernoulli 1 distribution for switching on a sensor block S̃(i) can be calculated as p(i) = ∑N j=1 δ (i) cj pj .\nAnother difference from the standard Dropout is the rescaling process. Unlike the standard Dropout which preserves a fixed scaling ratio after dropping neurons, the rescaling ratio in SD is formulated as a function of the dropping configuration and sensor dimensions. The intuition is to keep the weighted summations equivalent among different dropping configurations in order to activate the later hidden layers. The scaling ratio is calculated as αcj = ∑M i=1Ki∑M\ni=1 δ (i) cj Ki .\nIn summary, the output of SD for the kth feature in ith sensor block (i.e. S̃(i)) given a dropping configuration cj can be shown as Ŝ (i) cj ,k =M(i)cj X̃ (i) k , whereM (k) cj = αcjδ (i) cj is an augmented mask encapsulating both dropout and re-scaling."
    }, {
      "heading" : "4.2 Auxiliary Loss for Variance Reduction",
      "text" : "An alternative interpretation of the SD-augmented policy is that sub-policy induced by each sensor combination are jointly optimized during training. Denote the ultimate SD-augmented policy and sub-policy induced by each sensor combination as µc∼P and µcj , respectively. The final output maintains a geometric mean over N different actions.\nDespite the expectation of the total policy gradients for each sub-policy is the same, SD provides no guarantees on the consistency of these actions. To encourage the policy network to extract salient\n1 We wish to point out that p(i) is pseudo-Bernoulli as we restrict our attention to cases where at least one sensor block is switched on at any given instant in the layer. This implies that, while the switching on of any sensor block S̃(i) is independent of the other, switching off is not. So the distribution is no longer fully independent.\nfeatures from each sensor that can be embedded with similar representations on the latent space, we further augment an auxiliary loss that penalizes the inconsistency among µcj . This additional penalty term provides an alternative gradient that reduces the variation of the ultimate policy, i.e. V ar [µc∼P].\nThe mechanism is motivated from the recent successes [9, 21, 14, 5] that use the auxiliary tasks to improve both agent’s performance and convergence rate. However, unlike most previous works that design the auxiliary tasks carefully from the ground truth environment, we formulate the target action from the policy network itself. Under the standard actor-critic architecture, the target action is defined as the output action of the sub-policy in target actor network µ̃c∼P that maximizes the target critic values Q̃.\nLaux = λ N∑ i=1 (µcj (si)− µ̃c∗(si))2 (1)\nwhere c∗ = argmax cj∼P N∑ i=1 Q̃(si, µ̃cj (si)) (2)\nHere, λ is an additional hyperparameter that indicates the ratio between the two losses, and N is the batch size for off-policy learning."
    }, {
      "heading" : "5 Evaluation Results",
      "text" : ""
    }, {
      "heading" : "5.1 Platform Setup",
      "text" : "TORCS Simulator The proposed approach is verified on TORCS [41], a popular open-source car racing simulator that is capable of simulating physically realistic vehicle dynamics as well as multiple sensing modalities [43] to build sophisticated AI agents. In order to make the learning problem representative of the real-world setting, we use the following sensing modalities for our state description: (1) We define Sensor 1 as a hybrid state containing physical-based information such as odometry and simulated GPS signal. (2) Sensor 2 consists of 4 consecutive laser scans (i.e., at time t, we input scans from times t, t−1, t−2 & t−3). Finally, as Sensor 3, we supply 4 consecutive color images capturing the car’s front-view. These three representations are used separately to develop our baseline uni-modal sensor policies. The multi-modal state on the other hand has access to all sensors at any given point. When Sensor Dropout (SD) is applied, agent will randomly lose access to a strict subset of sensors. The categorical distribution is initialized with a uniform distribution among total 7 possible combinations of sensor subset, and the best learned policy is reported here. The action space is a continuous vector in R2, whose elements represent steering angle, and acceleration. Experiment details such as exploration strategy, network architectures of each model, and sensor dimensionality are shown in the Supplementary Material (Section B)."
    }, {
      "heading" : "5.2 Results",
      "text" : "Training Summary: The training performances, for all the proposed models and their corresponding baselines, are shown in Fig. 2. For DDPG, using high dimensional sensory input directly impacts convergence rate of the policy. Note that the Images uni-policy (orange line) has a much larger dimensional state space compared with Multi policies (purple and green lines). Counter-intuitively,\nTable 1: Performance of Policy\nPOLICY W/O NOISE W/ NOISE PERFORMANCE DROP\nMULTI UNI-MODAL W/ META CONTROLLER 1.51 ± 0.57 0.73 ± 0.40 51.7 % MULTIMODAL W/ SD 2.54 ± 0.08 2.29 ± 0.60 9.8 %\nNAF\nTrain Env.\nNAF\nTest Env.\nDDPG\nTrain Env.\nDDPG\nTest Env.\n0\n20\n40\n60\n80\n100\nR e w\na rd\n/ S\nte p\nw/o SD w/ Dropout w/ SD w/ SD + aux\nFigure 3: Policy performance when facing random sensor failure.\nTable 2: Results of the sensitivity metric.\nTRAINING TESTING ENV. ENV.\nNAF W/O SD 1.651 1.722 W/ SD 1.284 1.086\nDDPG W/O SD 1.458 1.468 W/ SD 1.168 1.171\nNAF performs a nearly linear improvement over training steps, and is relatively insensitive to the dimensionality of the state space. However, adding Sensor Dropout (SD) dramatically increases the convergence rate. For both algorithms, the final performance for multimodal sensor policies trained with SD is slightly lower than training without SD, indicating that SD has a regularization effect similar to original Dropout.\nComparison with Uni-modal Policies + Meta Controller: One of the intuitive baseline for the multi-sensor problem is to train each uni-modal sensor policy separately. Once individual policies are learned, we can train an additional meta controller that select which policy to follow given the current state. For this, we follow the setup in [19] by training a meta controller that takes the processed states from each uni-modal policy, and outputs a 3DOF softmax layer as the probability of choosing which sub-policy to perform. Note that, we assumed perfect sensing during the training. However, to test performance in a more realistic scenario, we simulate mildly imperfect sensing by adding Gaussian noise. Policy performance with and without noise are summarized in Table 1. The performance of the baseline policy drops dramatically once noise is introduced, which implies that the uni-modal policy is prone to over-fitting without any regularization. In fact, the performance drop is sometimes severe in physical-based or laser-based policy. In comparison, the policy trained with SD reaches a higher score in both scenarios, and the drop when noise is introduced is almost negligible.\nPolicy Robustness Analysis: In this part, we show that SD reduces the learned policy’s acute dependence on a subset of sensors in a multimodal sensor setting. First, we considered a scenario when malfunction of a sensor has been detected by the system, and the agent must rely on the remaining sensors to make navigation decisions. To simulate this setting during testing, we randomly block out some sensor modules, and scale the rest using the same rescaling mechanism as proposed in Section 4.1. Fig. 3 reports the averaging normalized reward of each model. A naive multimodal policy without any stochastic regularization (blue bar) performs poorly in the face of partial sensor failure and transfer tasks. Adding original Dropout makes the policy more generalized, yet the performance is not comparable with SD. Interestingly, by reducing the variance of the multimodal sensor policy with auxiliary loss, policy tends to have a better generalization among other environments.\nPolicy Sensitivity Analysis: To monitor the extent to which the learned policy depends on each sensor block, we measure the gradient of the policy output w.r.t a subset block S̃(i). The technique is motivated from the salient map analysis [33], which has also been applied to DRL study recently [40]. To better analyze the effects of SD, we report on a the smaller subset by implementing SD layer to drop either (1) (physical, laser) or (2) vision. Consequently, the sensitivity metric is formulated as the relative sensitivity of the policy on two sensor subsets. If the ratio increases, the agent’s dependence shifts toward the sensor block in the numerator and vice versa. Assuming the fusion-of-interest is between the above-mentioned two subsets, we show in Table 2 that, using SD, the metric get closer to 1.0, indicating nearly equal importance to both the sensing modalities. The\nsensitivity metric is calculated as T 12 = 1M ∑ i (∣∣∣∣∇S̃(1)i µ(S̃|θµ)∣∣∣Si ∣∣∣∣)(∣∣∣∣∇S̃(2)i µ(S̃|θµ)∣∣∣Si ∣∣∣∣)−1.\nEffect of Auxiliary Loss: In this experiment we verify how the auxiliary loss helps reshape the multimodal sensor policy and reduce the action variance. We extract the representations of the last hidden layer assigned by the policy network throughout an fixed episode. At every time step, the representation induced by each sensor combination is collected. Our intuition is that this latent space represents how the policy network interprets the incoming sensor stream for reaction. Based on this assumption, an ideal multimodal sensor policy should map different sensor streams to an similar distribution as long as the information provided by each combination is representative to lead to the same output action.\nAs shown in Fig. 4, the naive multimodal sensor policy has a scattered distribution over the latent space, indicating that representative information from each sensor is treated very differently. In comparison, the policy trained with SD has a concentrated distribution, yet it is still distinguishable w.r.t. different sensors. Adding the auxiliary training loss encourages the true sensor fusion as the distribution becomes more integrated. During training, the policy is not only forced to explicitly make decisions under each sensor combination, but also penalized with the disagreements among multimodal sensor policies. In fact, as shown in Fig. 5, the concentration of the latent space directly affect the action variance induced by each sub-policy. Note that in practice, the covariance of the third principle components contain around 85%. We provide the actual covariances for each component and the actual action variance values in the Supplementary Material."
    }, {
      "heading" : "6 Discussion",
      "text" : "Full Sub-Policy Analysis: The performance of each sub-policy is summarized in Fig. 6. As shown in the first and third column, the performances of the naive multimodal sensor policy (red) and the policy trained with standard Dropout (blue) drop dramatically as the policies lose access to image, which shares 87.9% of the total multimodal state. Though Dropout increases the performance of the policy in the testing environment, the generalization is limited to using full multimodel state as input. On the other hand, SD generalizes the policy across sensor module, making the sub-policies successfully transfer to the testing environment. It is worth mentioning that the policies trained with SD is capable to operate even when both laser and image sensor are blocked.\nVisualize Policy Attention Region: The average gradient in the policy sensitivity section can also be used to visualize the regions among each sensor where the policy network pays attentions. As shown in Fig. 7(a), we observe that polices trained with SD have higher gradients on neurons corresponding to the corner inputs of the laser sensor, indicating that a more sparse and meaningful policy is learned. These corner inputs corresponded to the laser beams that are oriented perpendicularly to the vehicle’s direction of motion, and give an estimate of its relative position on the track. To look for similar patterns in Fig. 7(b), image pixels with higher gradients are marked to interpret the policy’s view of the world. We pick two scenarios, 1) straight track and 2) sharp left turn, depicted by the first and second rows in the figure. Note that though policies trained without SD tend to focus more on the road, those areas are in plain color and offer little salient information. In conclusion, policies trained with SD are more sensitive to features such as road boundary, which is crucial for long horizon planning. In comparison, network trained without SD has a relatively low and unclear gradients over both laser and image sensor state space."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "In this work, we introduce a new stochastic regularization technique called Sensor Dropout to promote an effective fusing of information from multiple sensors. The variance of the resulting policy can be further reduced by introducing an auxiliary loss during training. We show that the aid of SD reduces the policy sensitivity to a particular sensor subset, and make it capable of functioning even in the face of partial sensor failure. Moreover, the policy network is able to automatically infer and weight locations providing salient information. For future work, we wish to extend the framework to other environments such as real robotics systems, and other algorithms like GPS[16], TRPO [31], and Q-Prop [9], etc.. Secondly, systematic investigation into the problems such as how to augment the reward function for other important driving tasks like collision avoidance, and lane changing, and how to adaptively adjust the SD distribution during training are also interesting avenues that merit further study."
    }, {
      "heading" : "A Continuous Action Space Algorithms",
      "text" : "A.1 Normalized Advantage Function (NAF)\nQ-learning [36] is an off-policy model-free algorithm, where agent learns an approximatedQ function, and follows a greedy policy µ(s) = argmaxaQ(s, a) at each step. The objective function J = Esi,ri∼E ,ai∼π[R1], can be reached by minimizing the square loss Bellman error L = 1N ∑N i (yi − Q(si, ai|θQ))2, where target yi is defined as r(si, ai) + γQ(si+1, µ(si+1)). Recently, [8] proposed a continuous variant of Deep Q-Learning by a clever network construction. The Q network, which they called Normalized Advantage Function (NAF), parameterized the advantage function quadratically over the action space, and is weighted by non-linear feature of states.\nQ(s, a|θQ) = A(s, a|θµ, θL) + V (s|θV ) (3)\nA(s, a|θµ, θL) = −1 2 (a− µ(s|θµ))TP (s|θL)\n(a− µ(s|θµ)) (4) P (s|θL) = L(s|θL)TL(s|θL) (5)\nDuring run-time, the greedy policy can be performed by simply taking the output of sub-network a = µ(s|θµ). The data flow at forward prediction and back-propagation steps are shown in Fig. 8 (a) and (b), respectively.\nA.2 Deep Deterministic Policy Gradient (DDPG)\nAn alternative approach to continuous RL tasks was the use of an actor-critic framework, which maintains an explicit policy function, called actor, and an action-value function called as critic. In [32], a novel deterministic policy gradient (DPG) approach was proposed and it was shown that deterministic policy gradients have a model-free form and follow the gradient of the action-value function.\n∇θµJ = E[∇aQ(s, a|θQ)∇aµ(s)] (6)\n[32] proved that using the policy gradient calculated in (6) to update model parameters leads to the maximum expected reward.\nBuilding on this result, [20] proposed an extension of DPG with deep architecture to generalize their prior success with discrete action spaces [23] onto continuous spaces. Using the DPG, an off-policy algorithm was developed to estimate the Q function using a differentiable function approximator. Similar techniques as in [23] were utilized for stable learning. In order to explore the full state and action space, an exploration policy was constructed by adding Ornstein-Uhlenbeck noise process [37]. The data flow for prediction and back-propagation steps are shown in Fig. 8 (c) and (d), respectively."
    }, {
      "heading" : "B Experiment Details",
      "text" : "B.1 Exploration and Reward\nAn exploration strategy is injected adding an Ornstein-Uhlenbeck process noise [37] to the output of the policy network. The choice of reward function is slightly different from [20] and [24] as an additional penalty term to penalize side-ways drifting along the track was added. In practice, this modification leads to more stable policies during training [15].\nB.2 Network Architecture\nFor laser feature extraction module, we use two 1D convolution layers with 4 filters of size 4× 1, while image feature extraction is composed of three 2D convolution layers: one layer of 16 filters of size 4× 4 and striding length 4, followed by two layers each with 32 filters of size 2× 2 and striding length 2. Batch normalization is followed after every convolution layer. All these extraction modules are fused and are later followed up with two fully-connected layers of 200 hidden units each. All hidden layers have relu activations. The final layer of the critic network use leaner activation, while the output of the actor network are bounded using tanh activation. We use sigmoid activation for the output of L network in NAF. In practice, it leads to a more stable training for high dimensional state space. We trained with minibatch size of 16.\nWe used Adam [11] for learning the network parameters. For DDPG, the learning rates for actor and critic are 10−4 and 10−3, respectively. We allow the actor and critic to maintain its own feature extraction module. In practice, sharing the same extraction module can lead to unstable training. Note that the NAF algorithm maintains three separate networks, which represent the value function (V (s|θV )), policy network (µ(s|θµ)), and the state-dependent covariance matrix in the action space (P (s|θL)), respectively. In order to maintain a similar experiment setting and avoid unstable training, we maintain two independent feature extraction modules for θµ, and both θV and θL. In a similar vein, we apply a learning rate of 10−4 for θµ, and 10−3 for both θµ and θV .\nB.3 Simulated Sensor Detail\nAs shown in Fig. 9, the physical state is a 10 DOF hybrid state, including 3D velocity (3 DOF), position and orientation with respect to track center-line (2 DOF), and finally rotational speed of 4 wheels (4 DOF) and engine (1 DOF). Each laser scan is composed of 19 readings spanning a 180° field-of-view in the the front of car. Finally, camera provides RGB channels with resolution 64× 64."
    }, {
      "heading" : "C More Experimental Results",
      "text" : "C.1 Effect of Auxiliary Loss\nThe covariance of PCA and the actual action variance is summarized in Table 4 and 5, respectively."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Sensor fusion is indispensable to improve accuracy and robustness in an<lb>autonomous navigation setting. However, in the space of end-to-end sensorimotor<lb>control, this multimodal outlook has received limited attention. In this work,<lb>we propose a novel stochastic regularization technique, called Sensor Dropout,<lb>to robustify multimodal sensor policy learning outcomes. We also introduce<lb>an auxiliary loss on policy network along with the standard DRL loss that help<lb>reduce the action variations of the multimodal sensor policy. Through empirical<lb>testing we demonstrate that our proposed policy can 1) operate with minimal<lb>performance drop in noisy environments, 2) remain functional even in the face<lb>of a sensor subset failure. Finally, through the visualization of gradients, we<lb>show that the learned policies are conditioned on the same latent input distribution<lb>despite having multiple sensory observations spaces a hallmark of true sensor-<lb>fusion. This efficacy of a multimodal policy is shown through simulations on<lb>TORCS, a popular open-source racing car game. A demo video can be seen here:<lb>https://youtu.be/HC3TcJjXf3Q.",
    "creator" : "LaTeX with hyperref package"
  }
}