{
  "name" : "1704.07657.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Decision Stream: Cultivating Deep Decision Trees",
    "authors" : [ "Dmitry Ignatov", "Andrey Ignatov" ],
    "emails" : [ "ignatov.dmitry@huawei.com,", "andrey.ignatoff@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n07 65\n7v 1\n[ cs\n.L G\n] 2\n5 A\npr 2\n01 7\nKeywords: Decision tree · Data fusion · Two-sample test statistics · Distributed machine learning"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the recent growth of data amount available for analysis and exploration, there is an inevitable need of comprehensive and automated methods for intellectual data processing. Decision tree (DT) is one of the most popular techniques in this area, and due to robustness and efficiency this prediction model became a standard tool for many machine learning and big data problems. The idea behind this method is to separate one complex decision rule into a union of several primitive rules, which leads to another crucial property — DT can be easily interpreted by humans compared to many other machine learning techniques.\nThe DT construction is performed by recursive data partitioning. At each stage the best splitting criterion is determined, and data from the current node is divided into child nodes according to selected criterion. The same procedure is recursively applied to all new nodes in the obtained tree until the stopping condition is met. While being a fast and clear way of data splitting, the geometrical reduction of data quantity in the nodes leads to their exhaustion and causes poor generalization ability and data overfitting. Since multiple partitioning generates many nodes with the same or similar label distribution (especially in the lower layers), it looks quite natural to merge such nodes to diminish the problem of data exhaustion and to continually increase the purity of the separated samples.\nIn this paper, we propose a novel method for regression and classification tasks — a Decision Stream (DS). Unlike the classical decision tree algorithm, the proposed method builds a directed acyclic graph with higher degree of connectivity (Fig. 1) by merging statistically indistinguishable nodes, which leads to reduction of the model size and better generalization due to more representative data samples. The split and merge operations are combined in this approach and repeated at each step of the iterative learning process. The performed experiments demonstrate that the proposed method achieves notably better results compared to the standard decision tree approach, at the same time showing high computational performance during training in distributed systems.\nThe rest of the paper is organized as follows. Sect. 2 gives an overview of the related works. Sect. 3 presents in details the proposed approach, and Sect. 4 provides the experimental results\nobtained on the real-world problems as well as on synthetic data. Sect. 5 summarizes our conclusions."
    }, {
      "heading" : "2 Related Work",
      "text" : "Decision trees have been extensively studied, and a large number of their modifications were developed during the past years. The proposed methods include the Iterative Dichotomiser 3 and its successor — C4.5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31]. Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].\nThe majority of these algorithms consider only node partitioning for decision tree construction, or use node merging as an auxiliary procedure that has no significant effect on the tree structure. For instance, C4.5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations. QUEST algorithm merges several classes into two superclasses to produce one binary split [20]. In [6], the number of terminal nodes is reduced by fusing the leaves with similar predictions after the training is finished. The CHAID algorithm merges data samples within a node, which is equivalent to using a modified splitting criterion. Data samples are fused based on the significance of their similarity estimated by test statistics: Chi-squared [17] for categorical label and F-test [22] for continuous. In this work, we present an algorithm that combines the classical approach with a new procedure — merging nodes from the same and/or different levels of decision stream structure, that leads to a new directed acyclic graph architecture."
    }, {
      "heading" : "3 Decision Stream",
      "text" : "In this section, we describe the proposed Decision Stream algorithm. The main concept of this method consists in merging similar nodes after each splitting iteration. The similarity is estimated using two-sample test statistics that is applied to compare the distribution of labels in each pair of nodes. The nodes are merged if the difference is statistically insignificant. This procedure eliminates the classical problem of decision trees — extreme decrease of data quantity in the leaf nodes, and produces a more general structure — a directed acyclic graph (Fig. 1). A more detailed explanation of the algorithm is provided below."
    }, {
      "heading" : "3.1 Node Merging with Two-Sample Test Statistics",
      "text" : "The overview of the merging operation is illustrated in the Fig. 2. After the classical decision tree branching, the merging algorithm takes as an input all leaf nodes that are presented in the tree at this stage (Fig. 2(a)) and fuses statistically similar nodes (Fig. 2(b-c)) using an input parameter — significance threshold Plim. Since the nodes are merged based on the similarity of their label distributions, the merging procedure is similar to the statistically-based label clustering.\nMerging Algorithm 1 consists of an outer and inner loop. In the outer loop the leaves are sorted in ascending order according to the number of associated samples. The inner loop consists of the following three steps:\n1. Leaf nodet is picked up from the head of the sorted collection. 2. For each (nodet, nodei) pair we compute the similarity of these two nodes and then take the\nleaf nodex that corresponds to its highest value. The similarity is calculated using the statistical function Sst(nodei, nodej) that returns the significance level p representing the probability that the mean values of labels associated with these two nodes are identical. 3. If the obtained significance level p is above the threshold Plim, the leaves nodet and nodex are merged into a new leaf with parents obtained by uniting the parents of the merged nodes."
    }, {
      "heading" : "3.2 Decision Stream Training",
      "text" : "The whole DS training procedure is described in Algorithm 2, where each learning iteration consists of two steps. At the first step, DS grows using the classical decision tree branching operation — new nodes are created by splitting all current non-terminal leaves [3,11,19]. At the second step, the leaves are merged using the procedure described in Algorithm 1. A leaf is marked as terminal if it cannot be split into statistically different child nodes. The pair of splitting and merging steps is iteratively performed till the stopping criterion is met. If all leaves are terminal or the prediction accuracy is not improved, the DS training is finished and Algorithm 2 returns the reference to the root node of the generated DS. To estimate the prediction accuracy, we use a cross-node Gini impurity measure calculated for K leaf nodes and J classes:\nIG(nodes) =\nK ∑\nk=1\nnk\nN\nJ ∑\nj=1\nfjk(1− fjk) , (1)\nAlgorithm 1: Merging the Leaf Nodes\nData: A set A = {node0, node1, . . . , noden} of Decision Stream leaves Input: Plim Result: A set of merged leaves\n1 do 2 A′ ← ∅ 3 sort nodes of A by number of samples in ascending order 4 do 5 nodet ← A.poll() 6 (nodet, nodex) ← argmax\nnodei∈A Sst(nodet, nodei)\n7 p ← Sst(nodet, nodex) 8 if p > Plim then\n9 A.remove(nodex) 10 A′.add(mergePair(nodet, nodex)) 11 else 12 A′.add(nodet)"
    }, {
      "heading" : "13 while A 6= ∅",
      "text" : "14 A ← A′ 15 while size of A is decreased 16 return A\nAlgorithm 2: Decision Stream Training\nData: A set Gin = {s0, s1, . . . , sn} of labeled training data samples Input: Significance threshold Plim Result: Decision Stream\n1 rootNode ← node(Gin) 2 A ← {rootNode} 3 do 4 A′ ← ∅ 5 foreach node ∈ A do 6 if node is terminal then 7 A′.add(node) 8 else\n9 children ← bestSplit(node, Plim) ⊳ Algorithm 3 or 4 10 if #children ≤ 1 then 11 mark node as terminal 12 A′.add(node)\n13 else 14 A′.addAll(children)\n15 A ← merge(A′, Plim) ⊳ Algorithm 1 16 while ∃ non-terminal node ∈ A AND IG(A) is decreased 17 return rootNode\nwhere N and nk is the number of samples in all leaves and leaf node k, respectively; fjk is the fraction of samples of class j in leaf k."
    }, {
      "heading" : "3.3 Splitting/Merging Criteria",
      "text" : "The splitting and merging operations are performed according to significance threshold Plim. We take as the null hypothesis that labels from two nodes are from the same distribution and have the same mean value. The null hypothesis is rejected at the significance level Plim, and in case of rejection we consider that the nodes are statistically different. The similarity is estimated by two-sample test statistics. We use Z-test/Student’s t-test for labels with presumably normal distribution. The choice between the tests is determined according to rule [29]: Z-test is applied if the size of both data samples is greater than 30, Student’s t-test — otherwise. For labels with nonnormal distribution we use Kolmogorov-Smirnov/Mann-Whitney U tests: the first one is applied if the size of data samples is greater than 2, the second — otherwise. We prefer Kolmogorov-Smirnov over Mann-Whitney U test since it is more sensitive to variations of both location and shape of the empirical cumulative distribution function [7], and provides better prediction accuracy in our experiments.\nWe propose two different versions of the split function bestSplit: one for relatively small datasets, where a precise selection of the split is crucial; and one for large-scale datasets where a trade-off between the accuracy and running time is important due to big amount of training samples."
    }, {
      "heading" : "3.4 Node Splitting for Non-Distributed Data",
      "text" : "For non-distributed datasets the splitting is performed according to Algorithm 3, which takes as an input the significance threshold Plim and a particular node. Firstly, binary splits of the data within the node is generated for each unique value of every feature. Then the similarity function Sst is calculated for each split, and the one with the lowest significance of similarity is selected. If this significance is smaller than the input threshold Plim, the selected best split is returned, otherwise — splitting is rejected and the node becomes terminal. Though this method is rather computationally expensive, it provides the best split quality and is reasonable for compact datasets.\nAlgorithm 3: Spitting the Non-Distributed Data\nData: A node of Decision Stream Input: Significance threshold Plim Result: Child nodes\n1 splits ← all binary splits of data for all features within the node 2 (child0, child1) ← argmin\n(child j 0 ,child j 1 )∈splits\nSst(child j 0, child j 1)\n3 p ← Sst(child0, child1) 4 if p < Plim then 5 return {child0, child1} 6 else 7 return ∅"
    }, {
      "heading" : "3.5 Node Splitting for Distributed Data",
      "text" : "Using the above algorithm for large-scale datasets is infeasible in most cases, thus in this paper we propose a different way of split selection designed for big data solutions. Instead of the greedy search, we perform data splitting based on the feature that is most correlated with label within a particular node [27]. Another difference of the proposed method is that it attempts to produce multiple leaves for each node as shown in Fig. 3, so far as the large number of samples presumes the robustness of such split.\nAlgorithm 4 demonstrates the body of the method. The procedure starts with function corr that selects the feature that is most correlated with the label. The obtained feature is then used to split the samples in the current node. If the feature is categorical, the samples are split by\nAlgorithm 4: Splitting the Distributed Data\nData: A node of Decision Stream Input: Significance threshold Plim Result: Child nodes\n1 feature ← argmax ftj∈features corr(ftj , lable) 2 if feature is categorical then 3 leaves ← split node samples with regard to categories of feature 4 else 5 n ← √# samples in the node 6 leaves ←split node samples with regard to n ranges of feature 7 do 8 leaves ← merge adjacent leaves with Plim ⊳ Algorithm 1 9 while quantity of leaves is decreased\n10 leaves ← merge(leaves, Plim) ⊳ Algorithm 1 11 return leaves\nAlgorithm 5: Estimation of Correlation Strength\nData: Variables x and y Result: Strength of the correlation between variables x and y\n1 if x is categorical then 2 return η2y|x 3 else if y is categorical then 4 return η2x|y\n5 else 6 return r2x,y\nits categories, each one forming a leaf node. If the feature is continuous, all samples are firstly sorted according to values of the feature and then divided into √ N ranges, where N is a number of samples in the node. Samples from the same range are then associated with one leaf node (Fig. 3(a)). At the next step, the adjacent leaves are merged using Algorithm 1 with threshold Plim until all neighboring nodes are statistically distinguishable (Fig. 3(b-c)). Finally, as soon as splitting with regard to categorical or continuous feature is finished, the obtained leaf nodes are merged again (this time not only adjacent ones) and the leaves providing statistically different predictions are returned.\nThe strength of correlation between the feature and label is estimated by function corr as described in Algorithm 5: if the feature and label are continuous, the correlation strength is calculated as coefficient of determination:\nr2xy =\n(\n∑n\ni=1 (xi − x̄)(yi − ȳ) √\n∑n i=1 (xi − x̄)2 √ ∑n i=1 (yi − ȳ)2\n)2\n, (2)\notherwise it is computed as correlation ratio:\nη2y|x =\n∑\nx nx(ȳx − ȳ)2 ∑\nx,i (yx,i − ȳ)2 . (3)\nSince both coefficients measure the same characteristics in discrete and continuous cases, we can compare the values obtained for different types of features to select the best one."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we describe the experiments conducted to evaluate the performance of the proposed Decision Stream algorithm. The solution was tested on five common machine learning problems, and on large-scale synthetic classification/regression data. To compare the obtained results, we applied the decision tree algorithm to the same training and test data to get the baseline accuracy. We use the scikit-learn implementation of decision trees3 that is based on the CART algorithm. The detailed classification and regression results for these methods are provided below."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "The predictive methods were tested on the following datasets:\n• Credit scoring4 — classification problem, 2 classes, 10 features, 100K training and 20K test samples. • Twitter sentiment analysis5 — classification problem, 3 classes (positive, negative, neutral), 500 features,\n6500 training and 824 test samples. Features were generated using the bag-of-words model.\n• F16 aircraft control problem (Ailerons)6 — regression problem, 40 features, 7154 training and 6596 test samples.\n3 http://scikit-learn.org/ 4 https://www.kaggle.com/c/GiveMeSomeCredit/data 5 http://alt.qcri.org/semeval2015/task10/ 6 http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html\n• MNIST handwritten digits classification7 — 10 classes, 784 features, 60K training and 10K test samples. • CIFAR-10 image classification8 — 10 classes, 1K features, 50K training and 10K test samples. Features\nwere extracted from the last convolutional layer of the pre-trained ResNet-18 [14] CNN.\nAdditionally, the algorithms were tested on large-scale synthetic classification and regression data generated on the fly by Spark Performance Tests Suite9. Each generated sample consisted of 500 features (125 binary, 125 categorical with 20 categories and 250 continuous within interval [0, 1]) and represented binary classification and regression problems.\nTo tune model parameters, the training data for each problem was split into training (90%) and validation (10%) subsets. The same data was used for training and testing both decision tree and Decision Stream algorithms."
    }, {
      "heading" : "4.2 Tuning the Significance Threshold",
      "text" : "Significance threshold is the key parameter of our algorithm, and in the first experiment our goal was to estimate its optimal value for each problem. The level of Plim was tuned as follows: for each dataset we varied it between 10−4 and 0.5 and for each value estimated the accuracy of DS on the validation set. For synthetic data the similarity of labels was estimated by unpaired two-sample Z-test and Students t-test, for all other datasets — by Kolmogorov-Smirnov and Mann-Whitney U nonparametric tests. For classification problems we use the standard accuracy metric, for regression tasks — the weighted absolute percentage error:\nLM (X,Y) = 100 ∣\n∣ ∣\n∑N\ni=1 yi\n∣ ∣ ∣\nN ∑\ni=1\n|yi − F (xi)| , (4)\nwhere X and Y are validation samples and corresponding labels, N is X quantity, yi and F (xi) are respectively label and prediction for sample xi.\nThe results of the experiment are presented in Fig. 4. The best accuracy was achieved at the significance threshold Plim that is equal to 0.005 for credit scoring, 0.05 for tweets, 0.02 for aileron control, 0.005 for MNIST, 0.01 for CIFAR-10 and 0.001 for synthetic data. The obtained values were used for DS training in the following experiments."
    }, {
      "heading" : "4.3 Classification and Regression Results for Non-Distributed Data",
      "text" : "This section presents the results obtained using a Decision Stream implementation for non-distributed data. Along with the single DS and DT models, we train their ensembles generated using five methods: random forest [15], extremely randomized trees [12], gradient boosting [10] and bagging [2]. Table 1 shows the results for single DS, DT and DS−merge models, where the last one denotes a DS with disabled merging phase. We should note that DS−merge is not equivalent to DT since in this\n7 http://yann.lecun.com/exdb/mnist/ 8 https://www.cs.toronto.edu/~kriz/cifar.html 9 https://github.com/databricks/spark-perf/ (v. 1.6)\nversion node splitting is performed only if the resulting child nodes are statistically distinguishable. The results demonstrate that this change leads to substantially different accuracy — while on the first three relatively small datasets it prevents overfitting, for larger MNIST and CIFAR datasets this results in an oversimplified tree model. Enabling the merging operation changes the situation: the growth doesn’t stop on the stage of simple predictive model that has many similar leaf nodes — merging operation fuses them and thus forces the training procedure to continue that can result in very deep trees. Fig. 5 illustrates this oscillating behavior: the merge operation is performed till no more statistically distinguishable nodes can be produced. Table 1 demonstrates that this leads to significantly higher accuracy compared to the standard decision tree architecture: the error on the first four datasets is reduced by 34%, 14%, 35% and 17%, respectively.\nFig. 6 illustrates the dependency between the size and the predictive error of ensembles constructed from decision trees and Decision Streams. The best results for all datasets are summarized in Table 2. As one can see, in all cases the best performance of Decision Stream ensemble was obtained when using the extremely randomized trees algorithm. The explanation of this effect is the following: in contrast to decision trees, the construction of Decision Streams involves a large number of recombinations caused by continuously repeating splitting and merging operations. The chances that DS will find the optimal solution are therefore higher compared to DT, but at the same time the resulting Decision Streams tend to provide less diverse results. The power of ensemble significantly depends on the diversity of predictors, which is thus lower in case of Decision Streams. Extremely randomized trees method partially solves this problem by using random features for training the DS, and therefore it tends to provide better final results compared to other methods.\nIn almost all cases the best results for Decision Stream are achieved with ensembles of size 500. The only exception is the sentiment analysis problem: in this case the classification error for a single DS is lower compared to all other methods. The greatest advantage of DS over the DT is obtained on the credit scoring and aileron control tasks: a single DS outperforms all DT ensembles. Overall, the Decision Stream based methods have shown the best results on four out of five datasets with an average advantage of 16%.\nfor classification (left) and regression (right) tasks. Decision tree metrics: — information gain, — Gini impurity, — variance reduction, — variance reduction and Gini impurity for continuous and categorical features, respectively."
    }, {
      "heading" : "4.4 Classification and Regression Results for Large-Scale Data",
      "text" : "The next set of experiments is conducted using Apache Spark-based10 distributed implementation of Decision Stream and decision tree algorithms. For the last one an open-source implementation from MLlib machine learning library is used. To perform the distributed computations, the models were running on a computer cluster with 4 nodes, 50 GB of RAM and 12 cores per node. The algorithms were trained on synthetic data generated by Spark Performance Tests Suite for classification and regression problems.\nFig. 7 shows the classification accuracy, the weighted absolute percentage error (Eq. 4) and the training time for DT with a depth ranging from 3 to 15 levels, and DS which depth is regulated automatically. According to the results, decision trees trained with variance reduction metric and depth restriction of 5 levels demonstrate the best accuracy in both classification and regression tasks and so are used in our further experiments. The prediction error of Decision Stream algorithm ( ) is about 9 — 48 times lower than the error obtained by DT. The explanation of this significant difference lies in the fact that the generated synthetic data had a distribution that was close to normal, thus the used pair of Z-test/t-test was especially effective in this case. Another reason is that better accuracy was also obtained at the expense of higher running time of DS algorithm. (Fig. 8).\nTo find the time that is required for DS and DT to provide the same accuracy, and to compare the accuracy after corresponding training periods, the experiments with different quantity\n10 http://spark.apache.org/ (v. 1.6).\nof training data and number of models in ensembles were carried out According to the empirical results presented in Table 3, it takes significantly lower amount of data and less training time for DS to provide the same quality of prediction as for DT in both classification and regression tasks; for comparable training time Decision Stream demonstrates significantly better accuracy. Gradient boosting and random forest ensembles improve DT performance, though the minimal error of ensembles even with 30 decision trees is still higher than the corresponding error of 30 Decision Streams: the difference reaches 46 — 48 times for classification and 5.9 — 8.3 times for regression tasks. Thus, the proposed modification of Decision Stream for large-scale data demonstrates faster training and better accuracy on both regression and classification tasks compared to DT algorithm."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper we presented a novel decision tree based algorithm — a Decision Stream, which avoids the problems of data exhaustion and formation of unrepresentative data samples in decision tree nodes by merging the leaves from the same and/or different levels of the model structure. By increasing the number of samples in each node and reducing the tree width, the proposed algorithm preserves statistically representative data and allows extremely deep graph architecture that can consist of hundreds of levels. The main parameter of the algorithm — significance threshold, determines the results of each split/merge operation and automatically defines the depth of the Decision Stream model.\nThe experiments demonstrated that Decision Stream algorithm shows a strong advantage over the standard decision tree method on both regression and classification tasks in both versions: non-distributed for relatively small datasets, where a precise selection of the best data splits is crucial; and distributed, where a balance between the accuracy and computational performance should be maintained."
    } ],
    "references" : [ {
      "title" : "Random forests and gradient boosting for wind energy prediction",
      "author" : [ "Á. Alonso", "A. Torres", "J.R. Dorronsoro" ],
      "venue" : "HAIS",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bagging predictors",
      "author" : [ "L. Breiman" ],
      "venue" : "Mach. Learn. 24, 123–140",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Classification and regression trees",
      "author" : [ "L. Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone" ],
      "venue" : "Wadsworth, Belmont",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Investigating the use of gradient boosting machine, random forest and their ensemble to predict skin flavonoid content from berry physical-mechanical characteristics in wine grapes",
      "author" : [ "Brillante" ],
      "venue" : "Comput. Electron. Agric. 117, 186–193",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Machine learning approach for distinction of ADHD and OSA",
      "author" : [ "K.C. Chu", "H.J. Huang", "Y.S. Huang" ],
      "venue" : "ASONAM",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "RECPAM: a computer program for recursive partition amalgamation for censored survival data and other situations frequently occurring in biostatistics",
      "author" : [ "A. Ciampi", "J. Thiffault", "U. Sagman" ],
      "venue" : "Comput. Meth. Prog. Bio. 30, 283–296",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Nonparametric statistics: a step-by-step approach",
      "author" : [ "G.W. Corder", "D.I. Foreman" ],
      "venue" : "Wiley, New York",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Measuring firm performance using financial ratios: a decision tree approach",
      "author" : [ "D. Delen", "C. Kuzey", "A. Uyar" ],
      "venue" : "Expert Syst. Appl. 40, 3970–3983",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Random forests and stochastic gradient boosting for predicting tree canopy cover: comparing tuning processes and model performance",
      "author" : [ "E.A. Freeman", "G.G. Moisen", "J.W. Coulston", "B.T. Wilson" ],
      "venue" : "Can. J. For. Res. 46, 323–339",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Greedy function approximation: a gradient boosting machine",
      "author" : [ "J.H. Friedman" ],
      "venue" : "Ann. Stat. 29, 1189– 1232",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "CART versus CHAID behavioral biometric parameter segmentation analysis",
      "author" : [ "I.R. Glăvan", "D. Petcu", "E. Simion" ],
      "venue" : "SECITC",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Extremely randomized trees",
      "author" : [ "P. Geurts", "D. Ernst", "L. Wehenkel" ],
      "venue" : "Mach. Learn. 63, 3–42",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning to rank with extremely randomized trees",
      "author" : [ "P. Geurts", "G. Louppe" ],
      "venue" : "JMLR 14, 49–61",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "CVPR",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Random decision forests",
      "author" : [ "T.K. Ho" ],
      "venue" : "ICDAR",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A cross-platform evaluation of various decision tree algorithms for prognostic analysis of breast cancer data",
      "author" : [ "S. Jhajharia", "S. Verma", "R. Kumar" ],
      "venue" : "ICICT",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "An exploratory technique for investigating large quantities of categorical data",
      "author" : [ "G.V. Kass" ],
      "venue" : "Appl. Stat. 29, 119–127",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "A hybrid classification algorithm by subspace partitioning through semi-supervised decision tree",
      "author" : [ "K. Kyoungok" ],
      "venue" : "Pattern Recogn. 60, 157–163",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Fifty years of classification and regression trees",
      "author" : [ "Loh", "W.-Y." ],
      "venue" : "Intern. Stat. Review 82, 329–348",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Split selection methods for classification trees",
      "author" : [ "Loh", "W.-Y.", "Shih", "Y.-S." ],
      "venue" : "Stat. Sin. 7, 815–840",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Detection of independent associations in a large epidemiologic dataset: a comparison of random forests, boosted regression trees, conventional and penalized logistic regression for identifying factors associated with H1N1pdm influenza infections",
      "author" : [ "Y. Mansiaux", "F. Carrat" ],
      "venue" : "BMC Med. Res. Methodol. 14, 99",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Handbook of statistical analysis and data mining applications",
      "author" : [ "R. Nisbet", "J. Elder", "G. Miner" ],
      "venue" : "Academic Press, Canada",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An outlook in some aspects of hybrid decision tree classification approach: a survey",
      "author" : [ "A. Panhalkar", "D. Doye" ],
      "venue" : "ICDECT",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Machine learning techniques for predicting hospital length of stay in Pennsylvania Federal and Specialty hospitals",
      "author" : [ "P.C. Pendharkar", "H. Khurana" ],
      "venue" : "IJACSA 11, 45–56",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Comparing decision tree algorithms to estimate intercity trip distribution",
      "author" : [ "C.S. Pitombo", "A.D. Souza", "A. Lindner" ],
      "venue" : "Transport. Res. C-EMER 77, 16–32",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Programs for machine learning",
      "author" : [ "Quinlan", "J.R.: C" ],
      "venue" : "Mach. Learn. 16,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1994
    }, {
      "title" : "Correlation based splitting criterion in multi-branch decision tree",
      "author" : [ "N. Salehi", "H. Yazdi", "H. Poostchi" ],
      "venue" : "Cent. Eur. J. Comp. Sci. 2, 205–220",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A decision tree based approach with sampling techniques to predict the survival status of poly-trauma patients",
      "author" : [ "J. Sanz", "J. Fernandez", "H. Bustince", "C. Gradin", "M. Fortun", "T. Belzunegui" ],
      "venue" : "IJCIS 10, 440–455",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Basic statistical analysis",
      "author" : [ "R.C. Sprinthall" ],
      "venue" : "Pearson Education, Boston",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Using Kaplan-Meier analysis together with decision tree methods (C&RT, CHAID, QUEST, C4.5 and ID3) in determining recurrence-free survival of breast cancer patients",
      "author" : [ "M. Ture", "F. Tokatli", "I. Kurt" ],
      "venue" : "Expert Syst. Appl",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2009
    }, {
      "title" : "A cost sensitive decision tree algorithm based on weighted class distribution with batch deleting attribute mechanism",
      "author" : [ "H. Zhao", "X. Li" ],
      "venue" : "Inform. Sciences 378, 303–316",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 17,
      "context" : "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].",
      "startOffset" : 216,
      "endOffset" : 232
    }, {
      "referenceID" : 18,
      "context" : "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].",
      "startOffset" : 216,
      "endOffset" : 232
    }, {
      "referenceID" : 22,
      "context" : "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].",
      "startOffset" : 216,
      "endOffset" : 232
    }, {
      "referenceID" : 27,
      "context" : "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].",
      "startOffset" : 216,
      "endOffset" : 232
    }, {
      "referenceID" : 30,
      "context" : "5 [26], Classification and Regression Tree (CART) [3], Chi-squared Automatic Interaction Detector (CHAID) [17], Quick, Unbiased, Efficient, Statistical Tree (QUEST) [19] and various modifications of these algorithms [18,19,23,28,31].",
      "startOffset" : 216,
      "endOffset" : 232
    }, {
      "referenceID" : 4,
      "context" : "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 10,
      "context" : "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 15,
      "context" : "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 23,
      "context" : "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 24,
      "context" : "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 29,
      "context" : "Despite the essential difference in the training procedure, they usually tend to show similar performance on many real-world regression and classification tasks [5,8,11,16,24,25,30].",
      "startOffset" : 161,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.",
      "startOffset" : 53,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.",
      "startOffset" : 53,
      "endOffset" : 69
    }, {
      "referenceID" : 22,
      "context" : "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.",
      "startOffset" : 53,
      "endOffset" : 69
    }, {
      "referenceID" : 27,
      "context" : "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.",
      "startOffset" : 53,
      "endOffset" : 69
    }, {
      "referenceID" : 30,
      "context" : "5 and CART algorithms as well as their modifications [18,19,23,28,31] perform only node splitting based on the selected features without any merging or fusion operations.",
      "startOffset" : 53,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "QUEST algorithm merges several classes into two superclasses to produce one binary split [20].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "In [6], the number of terminal nodes is reduced by fusing the leaves with similar predictions after the training is finished.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 16,
      "context" : "Data samples are fused based on the significance of their similarity estimated by test statistics: Chi-squared [17] for categorical label and F-test [22] for continuous.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : "Data samples are fused based on the significance of their similarity estimated by test statistics: Chi-squared [17] for categorical label and F-test [22] for continuous.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "At the first step, DS grows using the classical decision tree branching operation — new nodes are created by splitting all current non-terminal leaves [3,11,19].",
      "startOffset" : 151,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "At the first step, DS grows using the classical decision tree branching operation — new nodes are created by splitting all current non-terminal leaves [3,11,19].",
      "startOffset" : 151,
      "endOffset" : 160
    }, {
      "referenceID" : 18,
      "context" : "At the first step, DS grows using the classical decision tree branching operation — new nodes are created by splitting all current non-terminal leaves [3,11,19].",
      "startOffset" : 151,
      "endOffset" : 160
    }, {
      "referenceID" : 28,
      "context" : "The choice between the tests is determined according to rule [29]: Z-test is applied if the size of both data samples is greater than 30, Student’s t-test — otherwise.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "We prefer Kolmogorov-Smirnov over Mann-Whitney U test since it is more sensitive to variations of both location and shape of the empirical cumulative distribution function [7], and provides better prediction accuracy in our experiments.",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 26,
      "context" : "Instead of the greedy search, we perform data splitting based on the feature that is most correlated with label within a particular node [27].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "Features were extracted from the last convolutional layer of the pre-trained ResNet-18 [14] CNN.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Each generated sample consisted of 500 features (125 binary, 125 categorical with 20 categories and 250 continuous within interval [0, 1]) and represented binary classification and regression problems.",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "Along with the single DS and DT models, we train their ensembles generated using five methods: random forest [15], extremely randomized trees [12], gradient boosting [10] and bagging [2].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "Along with the single DS and DT models, we train their ensembles generated using five methods: random forest [15], extremely randomized trees [12], gradient boosting [10] and bagging [2].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "Along with the single DS and DT models, we train their ensembles generated using five methods: random forest [15], extremely randomized trees [12], gradient boosting [10] and bagging [2].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "Along with the single DS and DT models, we train their ensembles generated using five methods: random forest [15], extremely randomized trees [12], gradient boosting [10] and bagging [2].",
      "startOffset" : 183,
      "endOffset" : 186
    } ],
    "year" : 2017,
    "abstractText" : "Various modifications of decision trees have been extensively used during the past years due to their high efficiency and interpretability. Selection of relevant features for spitting the tree nodes is a key property of their architecture, at the same time being their major shortcoming: the recursive nodes partitioning leads to geometric reduction of data quantity in the leaf nodes, which causes an excessive model complexity and data overfitting. In this paper, we present a novel architecture — a Decision Stream, — aimed to overcome this problem. Instead of building an acyclic tree structure during the training process, we propose merging nodes from different branches based on their similarity that is estimated with two-sample test statistics. To evaluate the proposed solution, we test it on several common machine learning problems — credit scoring, twitter sentiment analysis, aircraft flight control, MNIST and CIFAR image classification, synthetic data classification and regression. Our experimental results reveal that the proposed approach significantly outperforms the standard decision tree method on both regression and classification tasks, yielding a prediction error decrease up to 35%.",
    "creator" : "LaTeX with hyperref package"
  }
}