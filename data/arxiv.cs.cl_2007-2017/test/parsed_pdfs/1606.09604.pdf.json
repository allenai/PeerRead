{
  "name" : "1606.09604.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "SnapToGrid: From Statistical to Interpretable Models for Biomedical Information Extraction",
    "authors" : [ "Marco A. Valenzuela-Escárcega", "Gus Hahn-Powell", "Dane Bell", "Mihai Surdeanu" ],
    "emails" : [ "msurdeanu}@email.arizona.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Due to the deluge of unstructured data, information extraction (IE) systems, which aim to translate this data to structured information, have become ubiquitous. For example, applications of IE range from parsing literature (Iyyer et al., 2016) to converting thousands of cancer research publications into complex proteins signaling pathways (Cohen, 2015).\nBy and large, in academia most of these approaches are implemented using machine learning (ML). This choice is warranted: generally, ML approaches, where the machine learns directly from\nthe data, perform better than approaches where human domain experts encode the structure to be extracted manually. For example, the top systems in the BioNLP event extraction shared tasks have consistently been ML-based approaches (Kim et al., 2009; Kim et al., 2013). However, this is only part of the story: most of these models cannot be easily understood by their users, and, by and large, cannot be modified without retraining. This “technical debt” of ML (Sculley et al., 2014) is better understood in industry: Chiticariu et al. (2013) report that 67% of large commercial vendors of natural language processing (NLP) software focus on rule-based IE, and an additional 17% on hybrid systems that combine rule-based and ML approaches.\nIn this paper we focus on interpretable models for information extraction, i.e., models that: (a) can be understood by human users, and (b) can be directly edited and improved by these users. In particular, we focus on deterministic, rule-based models. Here, we introduce a novel approach to generate such models, which maintains both the advantages of ML such as learning from data, and the benefits of interpretability such as allowing human domain experts to directly edit and improve these models. Specifically, our contributions are:\n(1) We introduce a simple strategy that converts statistical models for IE to rule-based models. We call the proposed algorithm SnapToGrid. Our approach works in three steps. First, we train a statistical model for the task at hand. Here we experiment with logistic regression, but the proposed method is, in principle, independent of the underlying statistical model. Further, our strategy can operate over multiple classifiers that are part of the same IE system (e.g., one classifier to identify event triggers, and another to identify event arguments). Second, we convert features\nar X\niv :1\n60 6.\n09 60\n4v 1\n[ cs\n.C L\n] 3\n0 Ju\nn 20\n16\nto rules implemented in Odin, a modern declarative rule language (Valenzuela-Escarcega et al., 2016; Valenzuela-Escarcega et al., 2015). We also discard most of the statistical information acquired previously, by converting feature weights to discrete votes, which guarantees interpretability (hence the SnapToGrid name). Third, human domain experts inspect and manually improve the generated model, under certain time constraints.\n(1) We evaluate our approach on the BioNLP 2009 core event extraction task, and demonstrate that the resulting interpretable model has similar performance to the statistical model that served as starting point."
    }, {
      "heading" : "2 Approach",
      "text" : "Our motivation for this work is to keep the human domain expert in the loop when building IE systems. We show in Section 3 that this is beneficial, even when the domain experts have limited time to work on the task and no access to data other than the model itself. To achieve this “human in the loop” goal we propose the following three-step algorithm:\n1. Train a statistical model for the IE task at hand (Section 2.1). The model may consist of several statistical classifiers. For example, for the BioNLP event extraction task, the most common approach involves two classifiers: one to identify event triggers, and a following classifier to identify event participants. One restriction is that these classifiers be feature-based classifiers, e.g., logistic regression, rather than the classifiers based on latent representations, e.g., neural networks.\n2. Convert the statistical model into an interpretable, rule-based model (Section 2.2):\n(a) First, we convert the features to rules in the Odin language.\n(b) Then, we assign to each rules “votes” for a given class, by “snapping to grid”, i.e., converting to discrete values, the weights computed by the above statistical model.\n3. Domain experts edit the produced rule-based model directly, aiming to improve its quality with respect to both coverage and precision (Section 2.3).\nWe detail this process in the rest of this section, focusing on the BioNLP core event extraction task as the domain of interest."
    }, {
      "heading" : "2.1 Step 1: Build Statistical Model",
      "text" : "Our statistical model is inspired by the top performing approach at the 2009 evaluation (Björne et al., 2009). The approach is summarized in Figure 1. Similar to (Björne et al., 2009), our approach consists of two classifiers: the first classifier detects and labels event trigger words in the input text; the second classifiers extracts and labels relations between event triggers and potential event participants, which can be either Protein entities or other event triggers. Both classifiers are implemented using multi-class logistic regression (LR), but our conversion process (Steps 2 and 3) is independent of the underlying statistical model, so, in principle, other feature-based classifiers that assign explicit weights to features could be used, e.g., perceptron, or linear support vector machines."
    }, {
      "heading" : "The Trigger Classifier",
      "text" : "The first classifier sequentially labels each word in the input text as a trigger for a specific BioNLP event class, or as Nil otherwise. We implemented the following features:\nSurface features: These features include the original and lemmatized words, and the presence of the word in a gazetteer of known event triggers (constructed automatically from the training data). These features are generated for the word being classified, as well as the words surrounding it inside a window of n tokens. We used two windows in our experiments, with n = 1 and n = 4. Further, bag-of-words features are generated for the windows and for the sentence as a whole.\nSyntactic features: These features capture the syntactic dependencies (both incoming and outgoing) directly connected to the token. All syntactic information was represented using Stanford dependencies (De Marneffe and Manning, 2008), and was generated using the CoreNLP toolkit (Manning et al., 2014). For each of these paths, we generate two different versions: one containing just the label and direction of the syntactic dependencies, and another including also the destination words.\nEntity features: These features encode the number of other entities surrounding the token, both inside a window and in the sentence as a whole."
    }, {
      "heading" : "The Event Participant Classifier",
      "text" : "This classifier pairs all the triggers detected by the previous classifier with other named entities (Proteins in this case) or event triggers that occur in the same sentence. These pairs are then classified into one of the possible participant relations, or Nil indicating that there is no relation between the pair. This classifier uses the following features:\nSyntactic features: These features are based on the shortest path connecting the two mentions (trigger and candidate participant) in the Stanford syntactic dependency graph. Two versions of the shortest path are used: a lexicalized one (capturing the words along the path), and an unlexicalized one.\nSurface features: These features include: the order of the two mentions in text, their distance in terms of tokens, the number of entities and triggers in the sentence, the parts of speech and words of the mentions, and the number of triggers and entities between the mentions.\nConsistency features: These features encode the labels of the two mentions jointly, as well as the labels of their superclasses. For example, the features <Regulation, Phosphorylation> and <Regulation, Event> are generated for a relation between a Regulation event trigger and a Phosphorylation trigger as its theme. These feature capture selectional preferences for arguments, e.g., the Theme of a regulation event should be another event.\nGraph features: The parent, children, and siblings of the mentions in the syntactic dependency graph."
    }, {
      "heading" : "Limitations",
      "text" : "Not all of the above features can be represented as rules in the current implementation of the chosen rule language. Currently1, Odin rules capture paths (over sequences or directed graphs) that are anchored at both ends (e.g., from an event trigger to an event argument) (Valenzuela-Escarcega et al., 2015; Valenzuela-Escarcega et al., 2016). Because of this, Odin cannot represent the following information: bag-of-word features, syntactic paths that are not anchored at both ends (such as dependencies connected only to event trigger candidates), and features that count occurrences of tokens or entities in text. In Section 3 we analyze the performance drop when such features are removed from the model."
    }, {
      "heading" : "2.2 Step 2: Convert the Statistical Model to a Rule-based Model",
      "text" : "Once the statistical model is constructed, we employ the lossy process below to convert it to an interpretable one."
    }, {
      "heading" : "Converting Features to Rules",
      "text" : "First, we convert the features encoded in the statistical model to rules in the Odin language (Valenzuela-Escarcega et al., 2015; Valenzuela-Escarcega et al., 2016). In general, the features previously introduced consist of conjunctions of information bits, each of which corresponds to a different rule fragment. For example, for the classification of event participants, one such conjunction captures the type of the expected trigger (e.g., Phosphorylation), combined\n1As of June 2016\n- name: phospho_event label: Phosphorylation pattern: | trigger:Phosphorylation\ntheme:Protein = >nsubjpass\n1\"Figure 2: Example of a rule for event participant classification that is built from a single feature. The feature captures the passive nominal subject (nsubjpass) outgoing (>) from a Phosphorylation trigger and landing on a Protein. The bold font indicates the rule output, i.e., the nominal subject is the theme of a Phosphorylation event.\nwith the syntactic path that connects the trigger with the participant candidate (e.g., an outgoing passive nominal subject – nsubjpass), and a semantic constraint for the type of named entity of the participant (e.g., Protein). These are immediately translatable to Odin rules, as illustrated in Figure 2.\nImportantly, the rules encode output information as well, e.g., the recognized event participant serves as a theme for a Phosphorylation event in Figure 2. At this stage, this information is exhaustively generated from all possible classifier labels (e.g., for the classification of event participants these labels are the cartesian product of {theme, cause} and possible event labels {Phosphorylation, Binding, . . . }). Of course, some of these outputs do not apply. For example, it is highly unlikely that the rule shown in\nFigure 2 produces the cause of a Regulation event. We quantify the confidence in these outputs in the next stage of the algorithm."
    }, {
      "heading" : "Converting Weights to Votes",
      "text" : "Feature weights are unbounded continuous values that are difficult to interpret and manually modify. For this reason, we would ideally prefer to exclude them completely from the interpretable model. Conceptually, this is simple: we could use the weights to choose the most likely output label for a rule (from the options generated previously), and discard them afterwards. However, our early experiments demonstrated that this performs poorly, because it forces the algorithm to ignore the inherent ambiguity of language, which is captured by the statistical model through weights. For example, the trigger classifier learns that “recruits” serves as trigger for two different events, Binding and Localization, and, consequently, assigns different weights to the two labels based on\nTrigger features\nWeights\nF re\nqu en\ncy\n−5 0 5 10\n0 20\n0 60\n0 10\n00\nRelation features\nWeights F\nre qu\nen cy\n−6 −4 −2 0 2 4\n0 20\n0 40\n0 60\n0 80\n0\nFigure 3: Weights of the two classifiers converted to votes (trigger classifier – top, participant classifier – bottom). Each histogram bin receives a number of votes (positive or negative) equal to its offset from 0.\nthe amount of evidence seen in training. During inference, the most likely class is chosen by aggregating the weights of all features that apply.\nGiven this observation, we chose to preserve the weights, but convert them from the original unbounded continuous values to discrete “votes” (positive or negative) that are then used during inference to resolve conflicts. This achieves two things. First, we increase the interpretability of the model: humans can now interpret these discrete votes, which mimic a Likert scale (Likert, 1932). Second, by keeping and using these discretized votes, we preserve some of the statistical power of the model. We show in Section 3 that some performance is indeed lost in this conversion, but the loss is small and the gain in interpretability compensates for that.\nThe conversion from continuous weights to discrete votes is a process similar to choosing the bins in a histogram. In our case, we first construct a histogram of all feature weights. Then, each histogram bin receives a number of votes equal to its offset (positive or negative) from 0. For example, all the weights in the second bin to the left of 0 receive two negative votes. Several methods have been proposed for selecting the number of bins in\na histogram, for example (Sturges, 1926; Doane, 1976; Freedman and Diaconis, 1981). Here, we use the formula proposed by (Scott, 1979):\nh = 3.5σ̂n−1/3 (1)\nwhere h is the estimated bin width, n is the sample size, and σ̂ is the estimated standard deviation. We chose this formula because it gives a good compromise between retaining most of the information in the weights while minimizing the number of bins. The resulting binned weights for trigger and relation features (generated using the BioNLP 2009 training corpus) are shown in Figure 3."
    }, {
      "heading" : "2.3 Step 3: Edit the Rule-based Model",
      "text" : "The output of the previous two steps is a model consisting of a set of rules. The association between rules and output classes is measured through votes that each matching rule gives to each output label. The last step in our proposed approach is to let human domain experts improve this model by directly editing it. The experts had complete freedom in the operations they were allowed to do. For example, they could improve the syntactic paths captured by the rules, or increase/decrease the number of votes assigned to a specific rule. The only constraints were: (a) they were not allowed to look only at the learned rules and not at the training data, and (b) they had to complete the process within one hour. This setting is of course artificial and unrealistic. We enforced it in this work to demonstrate the interpretability of the generated model."
    }, {
      "heading" : "3 Empirical Results",
      "text" : "We analyze the performance of our approach on the core event extraction dataset from the BioNLP 2009 shared task (Kim et al., 2009). All the results reported in this section were measured on the development partition of the dataset, which was not used at all during training.2 To minimize overfitting, we did not implement any feature selection or other hyper parameter tuning process.\nTable 2 lists the results of the complete statistical model, i.e., using all features introduced in Section 2.1, trained using L2-regularized LR. This configuration generated 1,190,029 features with non-zero weights. The table shows that this model\n2The online scoring website, which would have allowed us to also obtain scores on the official test partition, was down due to updates during the development of this work.\nachieved an overall F1 score of over 40 points, which likely puts it in the top 5 or 6 (out of 24) systems that participated in the actual challenge.3 The performance of this system could be further improved by adding more features proposed in other event extraction approaches (Miwa et al., 2010), feature selection, hyper parameter tuning, etc.\nFor a fair comparison, we next trained the same model but using only features that can be converted to rules. As discussed, the features that were removed include bag-of-word features and features that count occurrences of tokens or entities in text. These results, summarized in Table 2, show that the overall F1 score drops 4 points. This suggests that rule languages need to be extended if they are to have the same representational power as feature-based models. Given that the focus of\n3(Kim et al., 2009) report results on the official test partition, which are not directly comparable with our results. However, in the authors’ experience, the difference in scores between the development and test partitions in this dataset tend to be small. Since the 2009 evaluation, several works have improved upon these results, with performance reaching 58 F1 points, but using more complex methods, including joint inference, coreference resolution, and domain adaptation (Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014).\nthis work is not on the design of rule-based languages for IE, we will use this latter model as the starting point of our approach, ignoring (for now) the performance penalty observed above.\nImportantly, a system with more than 1 million features is not interpretable. To address this, we trained the same system using L1 regularization as a form of feature selection. This reduced the number of features with non-zero weights by two orders of magnitude: from over 1 million to 10,926. The performance of this model is shown in Table 3. The results demonstrate that this drastic reduction in the number of useful features came with a small performance cost, of less than 1 F1 point.\nGiven this successful compression of the feature space, we next convert this L1-regularized model to rules, using the approach discussed in Section 2.2. The performance of the rule-based model (before expert intervention!) is summarized in Table 4. The table shows that the overall cost of “snapping to grid” the statistical model is approximately 3 F1 points, which come from a drop in recall. This happens because many feature weights associated with specific labels (such as specific event triggers) have low values (due to sparsity), and, after the discretization process, the model can no longer prioritize these labels over the Nil class. Interestingly, the same process yielded a small increase in precision from 59% to 62%.\nAll in all, we consider a drop of 3 F1 points for the gain of interpretability an acceptable tradeoff. To empirically demonstrate the value of interpretability, we let two Linguistics PhD students edit the generated rule-based model for one hour, aiming to improve its generalization, robustness to syntactic errors, and readability. The students were familiar with the Odin language (ValenzuelaEscarcega et al., 2015) so they could “read” the\nmodel, and had a high-level understanding of the BioNLP shared task (although they did not participate in it). To guarantee that their recommendations came from understanding the model rather than other external factors, they were not given access to the BioNLP dataset. Given the large number of rules at this point, the students tended to randomly sample the rules in the model attempting to find repeated mistakes, rather than linearly inspect the list of rules. Table 5 summarizes the experts’ recommendations. As shown, several of the experts’ suggestions involved removing or collapsing rules, which reduced the number of rules from 10,926 to 8,868.\nTable 6 lists the performance of the resulting model, after implementing the experts’ recommendations. The table shows that most of the F1 loss has been recovered: the overall F1 score for this system approaches 35 F1 points, and is less than 1 F1 point behind the L1-regularized LR statistical model. In addition of reducing the number of rules in the model, the experts’ recommendations increased recall by over 4%, which is more than what was lost during the conversion to rules. However, the precision of this configuration decreased by 11%, which we blame on the experts’ limited familiarity with the BioNLP task, and the strict settings of the experiment (no access to data, limited time). However, all in all, this experiment demonstrates that the rule-based model produced by the proposed approach is interpretable: the experts understood the model, and were able to improve it, both with respect to its generalization power and its readability.\nLastly, Figure 4 shows a learning curve for the statistical model and the corresponding rule-based model (before expert intervention). The curve shows that the rule-based model follows closely\nthe behavior of its statistical counterpart, with a small penalty of 1-2 F1 points throughout. As discussed before, this performance loss can be mitigated through interventions by domain experts."
    }, {
      "heading" : "4 Related Work",
      "text" : "Most of the biomedical IE systems in academia rely on supervised machine learning. This includes the top performing system at the BioNLP 2009 shared task (Björne et al., 2009), as well as several following approaches that improve upon its performance (Miwa et al., 2010; McClosky et al., 2012; Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014).\nHowever, rule-based approaches (Appelt et al.,\n1993; Cunningham et al., 2002; Piskorski et al., 2004; Li et al., 2011; Chang and Manning, 2014) are preferable when the corresponding systems have to be deployed for long periods of time, during which they have to be maintained and improved. This has been recognized in industry (Chiticariu et al., 2013).\nWe bring together these two diverging directions by combining the advantages of ML with the interpretability of rule-based approaches. By representing the model as a collection of declarative rules, experts can directly edit the model, thus guaranteeing that the desired changes are actually applied. This is in contrast with methods such as active learning, in which the learning algorithm\npresents the “human in the loop” with new examples to annotate (Thompson et al., 1999). Although active learning may require less domain expertise than our proposal, it generally does not guarantee that the examples provided are actually propagated in the model (the learning algorithm may choose to override them with other data)."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "We have proposed a simple approach that marries the advantages of machine learning models for information extraction (such as learning directly from data) with the benefits of rule-based approaches (interpretability, easier maintainability). Our approach starts by training a featurebased statistical model, then converts this model to a rule-based variant by converting its features to rules and its feature weights to discrete votes. In doing so, our proposal learns from data similar to\nother machine learning approaches, but produces an interpretable rule-based model that can be directly edited by experts. Using the BioNLP 2009 event extraction task as a test bed, we show that while there is a small performance penalty when converting the statistical model to rules, the gain in interpretability compensates for that.\nIn this work, we focused on building upon feature-based classifiers, in particular logistic regression, due to their potential extensions to distant supervision (DS), where training data is generated automatically by aligning a knowledge base (KB) of known examples (e.g., known drug-gene interactions) with text (e.g., scientific publications). Distant supervision has obvious applications to bioinformatics (Craven et al., 1999), but it generally suffers from noise in the automaticallygenerated annotations (Riedel et al., 2010). In future work, we plan to combine our work with distant supervision by adapting our proposal to logistic regression variants that are robust to the noise introduced in DS (Surdeanu et al., 2012). This extension would make it possible to generate rules even when no annotated examples are available, as long as a suitable KB of known examples exists.\nAnother planned extension of this work focuses on reducing the number of generated rules by merging/collapsing similar paths into a single pattern. This can be achieved by constructing a minimal deterministic acyclic finite-state automaton (DAFSA) (Daciuk et al., 2000) with the paths that are similar, and then converting the DAFSA into a single pattern (Neumann, 2005). For example, such approaches would collapse the two patterns: dobj and dobj nn, into a single one: dobj nn?. This is fundamental for the long-term maintainability of the rule-based model, because the human experts would have to maintain considerably fewer rules.\nLastly, we plan to improve the “snap to grid” algorithm. Currently, the conversion of weights to votes is implemented using Scott’s rule (Scott, 1979), which is one method among several available to choose a histogram’s bin size. Scott’s method assumes that all bins have the same size, which may not be the best solution if interpretability is the goal. A potentially better approach is to select the bin divisions in a way that retains as much of the information contained in the weights as possible, while minimizing the number of bins."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was funded by the Defense Advanced Research Projects Agency (DARPA) Big Mechanism program under ARO contract W911NF-141-0395."
    } ],
    "references" : [ {
      "title" : "Fastus: A finite-state processor for information extraction from real-world text",
      "author" : [ "Jerry R. Hobbs", "John Bear", "David Israel", "Mabry Tyson" ],
      "venue" : "In Proceedings of the International Conferences on Artificial Intelligence",
      "citeRegEx" : "Appelt et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Appelt et al\\.",
      "year" : 1993
    }, {
      "title" : "Extracting complex biological events with rich graph-based feature sets",
      "author" : [ "Björne et al.2009] Jari Björne", "Juho Heimonen", "Filip Ginter", "Antti Airola", "Tapio Pahikkala", "Tapio Salakoski" ],
      "venue" : "In Proceedings of the Workshop on Current Trends",
      "citeRegEx" : "Björne et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Björne et al\\.",
      "year" : 2009
    }, {
      "title" : "A robust approach to extract biomedical events from literature",
      "author" : [ "Bui", "Sloot2012] Quoc-Chinh Bui", "Peter MA Sloot" ],
      "venue" : null,
      "citeRegEx" : "Bui et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bui et al\\.",
      "year" : 2012
    }, {
      "title" : "TokensRegex: Defining cascaded regular expressions over tokens",
      "author" : [ "Chang", "Manning2014] Angel X. Chang", "Christopher D. Manning" ],
      "venue" : "Technical Report CSTR 2014-02,",
      "citeRegEx" : "Chang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2014
    }, {
      "title" : "Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems",
      "author" : [ "Yunyao Li", "R. Reiss", "Frederick" ],
      "venue" : "In Proceedings of 2013 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Chiticariu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chiticariu et al\\.",
      "year" : 2013
    }, {
      "title" : "DARPA’s Big Mechanism program",
      "author" : [ "Paul R. Cohen" ],
      "venue" : "Physical Biology,",
      "citeRegEx" : "Cohen.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cohen.",
      "year" : 2015
    }, {
      "title" : "Constructing biological knowledge bases by extracting information from text sources",
      "author" : [ "Craven et al.1999] Mark Craven", "Johan Kumlien" ],
      "venue" : "In ISMB,",
      "citeRegEx" : "Craven and Kumlien,? \\Q1999\\E",
      "shortCiteRegEx" : "Craven and Kumlien",
      "year" : 1999
    }, {
      "title" : "A framework and graphical development environment for robust nlp tools and applications",
      "author" : [ "Diana Maynard", "Kalina Bontcheva", "Valentin Tablan" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Cunningham et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Cunningham et al\\.",
      "year" : 2002
    }, {
      "title" : "Incremental construction of minimal acyclic finite-state automata",
      "author" : [ "Daciuk et al.2000] Jan Daciuk", "Stoyan Mihov", "Bruce W Watson", "Richard E Watson" ],
      "venue" : null,
      "citeRegEx" : "Daciuk et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Daciuk et al\\.",
      "year" : 2000
    }, {
      "title" : "Stanford typed dependencies manual",
      "author" : [ "De Marneffe", "Christopher D Manning" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Marneffe et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2008
    }, {
      "title" : "Aesthetic frequency classifications",
      "author" : [ "David P Doane" ],
      "venue" : "The American Statistician,",
      "citeRegEx" : "Doane.,? \\Q1976\\E",
      "shortCiteRegEx" : "Doane.",
      "year" : 1976
    }, {
      "title" : "On the histogram as a density estimator: l2 theory. Probability theory and related fields, 57(4):453–476",
      "author" : [ "Freedman", "Diaconis1981] David Freedman", "Persi Diaconis" ],
      "venue" : null,
      "citeRegEx" : "Freedman et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Freedman et al\\.",
      "year" : 1981
    }, {
      "title" : "Feuding Families and Former Friends: Unsupervised Learning for Dynamic Fictional Relationships",
      "author" : [ "Iyyer et al.2016] Mohit Iyyer", "Anupam Guha", "Jordan Boyd-Graber" ],
      "venue" : "In Proceedings of the 15th Annual Conference of the North American Chap-",
      "citeRegEx" : "Iyyer et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Overview of BioNLP09 Shared Task on Event Extraction",
      "author" : [ "Kim et al.2009] Jin-Dong Kim", "Tomoko Ohta", "Sampo Pyysalo", "Yoshinobu Kano", "Junichi Tsujii" ],
      "venue" : "In Proceedings of the Workshop on BioNLP: Shared Task",
      "citeRegEx" : "Kim et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2009
    }, {
      "title" : "The Genia Event Extraction Shared Task, 2013 Edition - Overview",
      "author" : [ "Kim et al.2013] Jin-Dong Kim", "Yue Wang", "Yamamoto Yasunori" ],
      "venue" : "In Proceedings of the BioNLP Shared Task 2013 Workshop",
      "citeRegEx" : "Kim et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2013
    }, {
      "title" : "Systemt: A declarative information extraction system",
      "author" : [ "Li et al.2011] Yunyao Li", "Frederick R Reiss", "Laura Chiticariu" ],
      "venue" : "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies:",
      "citeRegEx" : "Li et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky" ],
      "venue" : "In ACL (System Demonstrations),",
      "citeRegEx" : "Manning et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Combining joint models for biomedical event extraction",
      "author" : [ "Sebastian Riedel", "Mihai Surdeanu", "Andrew McCallum", "Christopher D. Manning" ],
      "venue" : "BMC Bioinformatics,",
      "citeRegEx" : "McClosky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2012
    }, {
      "title" : "Event extraction with complex event classification using rich features",
      "author" : [ "Miwa et al.2010] Makoto Miwa", "Rune Sætre", "JinDong Kim", "Jun’ichi Tsujii" ],
      "venue" : "Journal of bioinformatics and computational biology,",
      "citeRegEx" : "Miwa et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Miwa et al\\.",
      "year" : 2010
    }, {
      "title" : "Boosting automatic event",
      "author" : [ "Miwa et al.2012] Makoto Miwa", "Paul Thompson", "Sophia Ananiadou" ],
      "venue" : null,
      "citeRegEx" : "Miwa et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Miwa et al\\.",
      "year" : 2012
    }, {
      "title" : "Shallow processing with unification and typed feature structures–foundations and applications",
      "author" : [ "Ulrich Schäfer", "Feiyu Xu" ],
      "venue" : "Knstliche Intelligenz,",
      "citeRegEx" : "Piskorski et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Piskorski et al\\.",
      "year" : 2004
    }, {
      "title" : "Modeling relations and their mentions without labeled text",
      "author" : [ "Limin Yao", "Andrew McCallum" ],
      "venue" : "In Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Riedel et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2010
    }, {
      "title" : "On optimal and data-based histograms",
      "author" : [ "David W Scott" ],
      "venue" : null,
      "citeRegEx" : "Scott.,? \\Q1979\\E",
      "shortCiteRegEx" : "Scott.",
      "year" : 1979
    }, {
      "title" : "The choice of a class interval",
      "author" : [ "Herbert A Sturges" ],
      "venue" : "Journal of the american statistical association,",
      "citeRegEx" : "Sturges.,? \\Q1926\\E",
      "shortCiteRegEx" : "Sturges.",
      "year" : 1926
    }, {
      "title" : "Multi-instance multi-label learning for relation extraction",
      "author" : [ "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Surdeanu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Surdeanu et al\\.",
      "year" : 2012
    }, {
      "title" : "Active learning for natural language parsing and information extraction",
      "author" : [ "Mary Elaine Califf", "Raymond J Mooney" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Thompson et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Thompson et al\\.",
      "year" : 1999
    }, {
      "title" : "Description of the Odin Event Extraction Framework and Rule Language",
      "author" : [ "A. Valenzuela-Escarcega", "Gustave Hahn-Powell", "Mihai Surdeanu." ],
      "venue" : "arXiv:1509.07513.",
      "citeRegEx" : "Valenzuela.Escarcega et al\\.,? 2015",
      "shortCiteRegEx" : "Valenzuela.Escarcega et al\\.",
      "year" : 2015
    }, {
      "title" : "Odin’s Runes: A Rule Language for Information Extraction",
      "author" : [ "A. Valenzuela-Escarcega", "Gustave Hahn-Powell", "Mihai Surdeanu." ],
      "venue" : "Proceedings of the 10th edition of the Language",
      "citeRegEx" : "Valenzuela.Escarcega et al\\.,? 2016",
      "shortCiteRegEx" : "Valenzuela.Escarcega et al\\.",
      "year" : 2016
    }, {
      "title" : "Relieving the computational bottleneck: Joint inference for event extraction with high-dimensional features",
      "author" : [ "Chen Chen", "Vibhav Gogate", "Vincent Ng" ],
      "venue" : null,
      "citeRegEx" : "Venugopal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Venugopal et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "For example, applications of IE range from parsing literature (Iyyer et al., 2016) to converting thousands of cancer research publications into complex proteins signaling pathways (Cohen, 2015).",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : ", 2016) to converting thousands of cancer research publications into complex proteins signaling pathways (Cohen, 2015).",
      "startOffset" : 105,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "For example, the top systems in the BioNLP event extraction shared tasks have consistently been ML-based approaches (Kim et al., 2009; Kim et al., 2013).",
      "startOffset" : 116,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "For example, the top systems in the BioNLP event extraction shared tasks have consistently been ML-based approaches (Kim et al., 2009; Kim et al., 2013).",
      "startOffset" : 116,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : ", 2014) is better understood in industry: Chiticariu et al. (2013)",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "ative rule language (Valenzuela-Escarcega et al., 2016; Valenzuela-Escarcega et al., 2015).",
      "startOffset" : 20,
      "endOffset" : 90
    }, {
      "referenceID" : 26,
      "context" : "ative rule language (Valenzuela-Escarcega et al., 2016; Valenzuela-Escarcega et al., 2015).",
      "startOffset" : 20,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "Our statistical model is inspired by the top performing approach at the 2009 evaluation (Björne et al., 2009).",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "Similar to (Björne et al., 2009), our approach consists of two classifiers: the first clas-",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : "Manning, 2008), and was generated using the CoreNLP toolkit (Manning et al., 2014).",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 26,
      "context" : ", from an event trigger to an event argument) (Valenzuela-Escarcega et al., 2015; Valenzuela-Escarcega et al., 2016).",
      "startOffset" : 46,
      "endOffset" : 116
    }, {
      "referenceID" : 27,
      "context" : ", from an event trigger to an event argument) (Valenzuela-Escarcega et al., 2015; Valenzuela-Escarcega et al., 2016).",
      "startOffset" : 46,
      "endOffset" : 116
    }, {
      "referenceID" : 26,
      "context" : "First, we convert the features encoded in the statistical model to rules in the Odin language (Valenzuela-Escarcega et al., 2015; Valenzuela-Escarcega et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 164
    }, {
      "referenceID" : 27,
      "context" : "First, we convert the features encoded in the statistical model to rules in the Odin language (Valenzuela-Escarcega et al., 2015; Valenzuela-Escarcega et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 164
    }, {
      "referenceID" : 22,
      "context" : "Here, we use the formula proposed by (Scott, 1979):",
      "startOffset" : 37,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "We analyze the performance of our approach on the core event extraction dataset from the BioNLP 2009 shared task (Kim et al., 2009).",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 18,
      "context" : "3 The performance of this system could be further improved by adding more features proposed in other event extraction approaches (Miwa et al., 2010),",
      "startOffset" : 129,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "(Kim et al., 2009) report results on the official test partition, which are not directly comparable with our results.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 19,
      "context" : "Since the 2009 evaluation, several works have improved upon these results, with performance reaching 58 F1 points, but using more complex methods, including joint inference, coreference resolution, and domain adaptation (Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014).",
      "startOffset" : 220,
      "endOffset" : 284
    }, {
      "referenceID" : 28,
      "context" : "Since the 2009 evaluation, several works have improved upon these results, with performance reaching 58 F1 points, but using more complex methods, including joint inference, coreference resolution, and domain adaptation (Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014).",
      "startOffset" : 220,
      "endOffset" : 284
    }, {
      "referenceID" : 1,
      "context" : "This includes the top performing system at the BioNLP 2009 shared task (Björne et al., 2009), as well as several following approaches that improve upon its performance (Miwa et al.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 18,
      "context" : ", 2009), as well as several following approaches that improve upon its performance (Miwa et al., 2010; McClosky et al., 2012; Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014).",
      "startOffset" : 83,
      "endOffset" : 189
    }, {
      "referenceID" : 17,
      "context" : ", 2009), as well as several following approaches that improve upon its performance (Miwa et al., 2010; McClosky et al., 2012; Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014).",
      "startOffset" : 83,
      "endOffset" : 189
    }, {
      "referenceID" : 19,
      "context" : ", 2009), as well as several following approaches that improve upon its performance (Miwa et al., 2010; McClosky et al., 2012; Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014).",
      "startOffset" : 83,
      "endOffset" : 189
    }, {
      "referenceID" : 28,
      "context" : ", 2009), as well as several following approaches that improve upon its performance (Miwa et al., 2010; McClosky et al., 2012; Miwa et al., 2012; Bui and Sloot, 2012; Venugopal et al., 2014).",
      "startOffset" : 83,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "However, rule-based approaches (Appelt et al., 1993; Cunningham et al., 2002; Piskorski et al., 2004; Li et al., 2011; Chang and Manning, 2014) are preferable when the corresponding systems have to be deployed for long periods of time, during which they have to be maintained and improved.",
      "startOffset" : 31,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "However, rule-based approaches (Appelt et al., 1993; Cunningham et al., 2002; Piskorski et al., 2004; Li et al., 2011; Chang and Manning, 2014) are preferable when the corresponding systems have to be deployed for long periods of time, during which they have to be maintained and improved.",
      "startOffset" : 31,
      "endOffset" : 143
    }, {
      "referenceID" : 20,
      "context" : "However, rule-based approaches (Appelt et al., 1993; Cunningham et al., 2002; Piskorski et al., 2004; Li et al., 2011; Chang and Manning, 2014) are preferable when the corresponding systems have to be deployed for long periods of time, during which they have to be maintained and improved.",
      "startOffset" : 31,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "However, rule-based approaches (Appelt et al., 1993; Cunningham et al., 2002; Piskorski et al., 2004; Li et al., 2011; Chang and Manning, 2014) are preferable when the corresponding systems have to be deployed for long periods of time, during which they have to be maintained and improved.",
      "startOffset" : 31,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "This has been recognized in industry (Chiticariu et al., 2013).",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "presents the “human in the loop” with new examples to annotate (Thompson et al., 1999).",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 21,
      "context" : "generally suffers from noise in the automaticallygenerated annotations (Riedel et al., 2010).",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "introduced in DS (Surdeanu et al., 2012).",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "(DAFSA) (Daciuk et al., 2000) with the paths that are similar, and then converting the DAFSA into a single pattern (Neumann, 2005).",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 22,
      "context" : "Currently, the conversion of weights to votes is implemented using Scott’s rule (Scott, 1979), which is one method among several available to choose a histogram’s bin size.",
      "startOffset" : 80,
      "endOffset" : 93
    } ],
    "year" : 2016,
    "abstractText" : "We propose an approach for biomedical information extraction that marries the advantages of machine learning models, e.g., learning directly from data, with the benefits of rule-based approaches, e.g., interpretability. Our approach starts by training a feature-based statistical model, then converts this model to a rule-based variant by converting its features to rules, and “snapping to grid” the feature weights to discrete votes. In doing so, our proposal takes advantage of the large body of work in machine learning, but it produces an interpretable model, which can be directly edited by experts. We evaluate our approach on the BioNLP 2009 event extraction task. Our results show that there is a small performance penalty when converting the statistical model to rules, but the gain in interpretability compensates for that: with minimal effort, human experts improve this model to have similar performance to the statistical model that served as starting point.",
    "creator" : "LaTeX with hyperref package"
  }
}