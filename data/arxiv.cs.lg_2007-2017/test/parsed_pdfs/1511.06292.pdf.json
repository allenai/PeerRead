{
  "name" : "1511.06292.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "FOVEATION-BASED MECHANISMS ALLEVIATE ADVERSARIAL EXAMPLES",
    "authors" : [ "Yan Luo", "Xavier Boix", "Gemma Roig", "Tomaso Poggio", "Qi Zhao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We show that adversarial examples, i.e., the visually imperceptible perturbations that result in Convolutional Neural Networks (CNN) fail, can be alleviated with a mechanism based on foveations — applying the CNN in a different image region. To see this, first, we report results in ImageNet that lead to a revision of the hypothesis that adversarial perturbations are a consequence of CNNs acting too linearly: a CNN acts locally linearly, only to changes in the receptive fields with objects recognized by the CNN, otherwise the CNN acts non-linearly. Then, we corroborate the hypothesis that when the neural responses are in the linear region, applying the foveation mechanism to the adversarial example tends to reduce the effect of the perturbation. This is because CNNs in ImageNet are robust to changes produced by the foveation (scale and translation of the recognized objects), but this property does not generalize to transformations of the perturbation."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "The generalization properties of the CNNs have been recently questioned. Szegedy et al. (2014) showed that images which are correctly classified by a CNN can be perturbed slightly to result in misclassification, and surprisingly, the perturbations are so small that are imperceptible to humans. The misclassified images with these perturbations are so-called adversarial examples.\nFurther, when the norm of the perturbation is not constrained to be small, it has been found that there are perturbations that produce a “synthetic” image, which does not resemble any object, but that a CNN classifies with high confidence as an object category (Nguyen et al., 2015; Erhan et al., 2009; Simonyan et al., 2013). In this case, CNNs are not expected to generalize, since these are images that a human would classify as synthetic. Yet, a CNN is expected to generalize for adversarial examples.\nAdversarial examples exist because, hypothetically, CNNs act as high-dimensional linear classifiers, and these classifiers are not robust to small perturbations of the input, c.f. (Goodfellow et al., 2015; Fawzi et al., 2015). The high-dimensionality of the linear classifier allows that a perturbation well aligned with the classifier is spread among many dimensions, which results in a perturbation with small spatial density, i.e., imperceptible.\nThis hypothesis of CNNs acting too linearly is supported by quantitative experiments in datasets where the target object is always centered and fixed to the same scale and orientation, namely, MNIST (LeCun et al., 1998) and CIFAR (Krizhevsky, 2009). Can experiments for CNNs trained to recognize objects in unconstrained conditions, such as the CNNs used for ImageNet, challenge the current hypothesis?\nIn this paper, we analyze adversarial examples using several common CNNs architectures for ImageNet (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015), and the experiments show that the hypothesis of the CNNs’ linearity needs a revision. The CNN is locally linear to changes in the receptive fields where there is an object recognized by the CNN, otherwise the CNN acts non-linearly. We also show that the adversarial perturbations that are generated with a CNN different from the CNN used for evaluation, produce a much smaller decrease of the accuracy compared to the CNN architectures used for the datasets in previous works (Szegedy et al., 2014; Goodfellow et al., 2015).\nar X\niv :1\n51 1.\n06 29\n2v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nA second observation is that adversarial examples can be alleviated by applying a mechanism similar to a foveation, i.e., a mechanism that selects a region of the image to apply the CNN, discarding information from the other regions. From the aforementioned linearity hypothesis, follows that the neural responses of a foveated object in the adversarial example can be decomposed into two independent terms, one that comes from the foveated clean object without perturbation, and another from the foveated perturbation. We introduce the hypothesis — supported by a series of experiments — that the term of the clean object is not negatively affected by the foveation, as CNNs are trained to be robust to scale and location transformations of the object, and even to partial occlusions. Yet, this property does not generalize to transformations of the perturbation, and the effect of perturbation on the classification score after the foveation is lower than before.\nPrevious works to alleviate adversarial perturbations learn CNNs with adversarial examples in the training set (Gu & Rigazio, 2015; Goodfellow et al., 2015). It will become clear at the end of the sequel, why these methods fail to generalize to adversarial examples that are not used in the training set."
    }, {
      "heading" : "2 EXPERIMENTAL SET-UP",
      "text" : "In this section we introduce the experimental set-up that we use in the rest of the paper.\nDataset We use ImageNet (ILSVRC) (Deng et al., 2009), which is a large-scale dataset for object classification and detection. We report our results on the validation set of ILSVRC 2012 dataset, which contains 50′000 images. Each image is labeled with one of the 1′000 possible object categories, and the ground truth bounding box around the object is also provided. We evaluate the accuracy performance with the standard protocol (Russakovsky et al., 2015), taking the top 5 prediction error.\nCNN Architectures We report results using 3 CNN which are representative of the state-of-the-art, namely AlexNet (Krizhevsky et al., 2012), GoogLeNet (Szegedy et al., 2015) and VGG (Simonyan & Zisserman, 2015). We use the publicly available pre-trained models provided by the Caffe library (Jia et al., 2014). We use the whole image as input to the CNN, resized to 227× 227 pixels in AlexNet, and 224 × 224 in VGG and GoogLeNet. In order to improve the accuracy at test time, a commonly used strategy in the literature is to use multiple crops of the input image and then combine their classification scores. We do not use this strategy unless we explicitly mention it.\nGeneration of Adversarial Perturbations The adversarial perturbation consist on finding the perturbation with minimum norm that produces misclassification, under the top-5 criterion. This is an\nNP-hard problem, and the algorithms to compute the minimum perturbation are approximate. To generate the adversarial examples we use the two algorithms available in the literature. Namely, the algorithm by Szegedy et al. (2014), that is based on an optimization with L-BFGS, and we also use the algorithm by Goodfellow et al. (2015), that uses the sign of the gradient of the loss function. We call them BFGS and Sign, respectively. For more details about the generation of the perturbation we refer to Sec. B, and to (Szegedy et al., 2014; Goodfellow et al., 2015).\nSeveral qualitative example of the perturbations are shown in Fig. 1. Observe that for BFGS, the perturbation is concentrated on the location of the object, while for Sign, the perturbation is spread through the image. Also, note that for each CNN architecture the perturbation is different.\nWhen are Adversarial Perturbations Perceptible? The perceptibility of adversarial examples is subjective. We show examples of the same adversarial perturbation varying the L1 norm per pixel, Fig. 14, 16, 18 in Sec. C, and the L∞ norm, Fig. 15, 17, 19 in Sec. C. We can see that after the norm of the perturbation reaches a certain value, the perturbation becomes perceptible, and it starts occluding parts of the target object. We give an estimate of the minimum value of the norm that makes the adversarial perturbations perceptible. For BFGS, we can see that when the L1 norm per pixel is higher than 15 the perturbation has become slightly visible in almost all cases, and for L∞ this value is 100. This difference between the two values is because the density of BFGS is not the same through all image, as commented before. For Sign, the threshold for both norms to make the perturbation visible is about 15 (here the value coincides for both because this perturbation is spread evenly through all the image).\nLocalization of the Target Object. We use the localization of the target object in the image as a tool to take into account the perturbation’s position in the image. We use the ground truth bounding box around the object provided in the ILSVRC 2012 dataset, which is manually annotated by a human."
    }, {
      "heading" : "3 THE LINEARITY OF CNNS FOR IMAGENET",
      "text" : "We now introduce the analysis of adversarial examples in CNNs for ImageNet. In this dataset, objects can be at different scales and locations, and as we will see, this difference with previous works leads to a revision of the hypotheses introduced in these works.\nLet x be an image that contains an object whose object category is of class label `, and we denote as f(x) the mapping of the CNN from the input image to the classification scores. We use to denote a perturbation that produces a misclassification when added to the input image, i.e., ` 6∈ C(f(x+ )) in which C(·) maps the classification scores to class labels. The set of all perturbations that produce a misclassification is denoted as Ex = { | ` 6∈ C(f(x+ ))}. Also, we define ? as the perturbation in Ex with minimal norm, i.e., ? = argmin ∈Ex ‖ ‖, which may be imperceptible. The causes of adversarial examples have been studied by Goodfellow et al. (2015); Fawzi et al. (2015), that showed that small perturbations can affect linear classifiers, and CNNs may act as a high-dimensional linear classifier (of the same dimensionality as the image size). The learned parameters of the CNN make that the CNN acts as a linear classifier, bypassing the non-linearities in the architecture. In this way, f(x+ ?) can be rewritten as w′x+w′ ?, where w′ is the transpose of a high-dimensional linear classifier, and it yields a classifier approximately equivalent to f(·). Observe that the high-dimensionality of the classifier allows that even when w′ ? is high, the average per pixel value of ? may be low, since the perturbation can be spread among all the pixels, and make it imperceptible.\nA direct consequence of the linearity of the CNN is that multiplying the adversarial perturbation by a constant value larger than 1 also produces misclassification, since cw′ ? is a factor c times larger than before. Thus, the set of adversarial examples, Ex, is not only ?, but rather a dense set of perturbations around ?. Another phenomenon that can be explained through the linearity of the CNN is that adversarial examples generated for a specific CNN model produce misclassification in other CNNs (Szegedy et al., 2014). This may be because the different CNNs act as a linear classifier, that are very similar among them.\nIn the following, we review the linearity of CNNs in ImageNet. Then, we analyze the influence of the location of the perturbation in the image, since in ImageNet the objects are neither centered nor uniformly scaled. Our results lead to a new hypothesis.\nReview of the Properties of Adversarial Examples We now revisit the aforementioned properties of the set of adversarial examples, Ex, that can be derived from CNNs acting too linearly. To analyze that multiplying adversarial perturbations by a factor also leads to misclassification, we measure the classification accuracy when we vary the norm of the perturbation. We use the L1 norm per pixel and also the L∞ norm, as BFGS and Sign perturbations are optimized for each of these norms respectively. At the same time, we also analyze the effect of adversarial perturbations generated on a CNN architecture different from the CNN we evaluate.\nResults can be seen in Fig. 2 for L1 norm per pixel. We additionally provide the accuracy for a perturbation generated by adding to each pixel a random value uniformly distributed in the range [−1, 1]. We observe a clear tendency that when we increase the norm of the perturbation, it results in more misclassification on average, which is in accordance to the hypothesis that CNNs are linear (Goodfellow et al., 2015). In Fig. 7 in Sec. A.1, the same conclusions can be extracted for L∞. Also, we observe that for the L1 norm per pixel, BFGS produces more misclassification on average than Sign, and the opposite is true if we evaluate with L∞. This is not surprising as the BFGS optimizes the L1 norm of the perturbation, and the Sign optimizes the maximum. Thus, in the rest of the experiments of the paper we only evaluate BFGS with the L1 norm per pixel, and Sign with the L∞ norm.\nInterestingly, we observe that there is not an imperceptible adversarial perturbation for every image. Recall that an L1 norm per pixel higher than 15, and L∞ also higher than 15 produce a perturbation slightly perceptible (Sec.2). Thus, the results in Fig. 2 show that the accuracy is between 10% and 20%, depending on the CNN, when the adversarial perturbations are constrained to be no more than slightly perceptible.\nFinally, we see that for a particular CNN architecture, the perturbation that results in more misclassification with lower norm is in all cases the one that is generated using the same CNN architecture. Observe that the perturbation generated for a CNN architecture, does not affect so severely the other CNN architectures as reported in MNIST, c.f. (Szegedy et al., 2014). When the adversarial perturbations are constrained to be no more than a bit perceptible, the accuracy is between 40% and 60% in the worst case, depending on the CNN. Following the reasoning of Goodfellow et al. (2015), the differences among the learned classifiers for the CNNs in ImageNet are bigger than the differences among the classifiers learned in MNIST.\nRole of the Target Object Location in the Perturbation We now analyze the effect of the perturbation at different locations of the image. We report the accuracy performance that yields the perturbation when it is located only on the target object, or when it is only on the background. Specifically, we create an image mask using the ground truth bounding box (denoted as Object Masked in the figures), where the mask’s pixels are 1 inside the bounding box, and 0 otherwise. Before the perturbation is added to the image, we multiply (pixel-wise) the perturbation by the mask, such that the resulting perturbation is only located inside the bounding box. To analyze the perturbation that is in the background, we also create the inverted mask, denoted as Background Masked, in which the pixels outside the bounding box are 1, and 0 for the pixels inside.\nIn Fig. 3 we report the accuracy performance for BFGS and Sign for Object Masked and Background Masked, and also for the perturbation applied in the full image generated for each CNN architecture (minimum perturbation (MP)). We observe that the perturbation localized only on the object decrease the accuracy performance in a similar way as MP. When adding the perturbation only on the background, the accuracy decreases significantly less than for Object Masked (there is a difference between them of about 40% of accuracy). Note that this result is also clear for small values of the perturbation norm, i.e., the values that are imperceptible.\nFrom the hypothesis that the CNN acts as a linear classifier, the result of this experiment suggests that the perturbation, ?, is aligned with the classifier, w, on the same location of the target object, otherwise there is not an alignment between them. However, if the CNN is linear, to produce misclassification, ? could be aligned with w in any location, because w′ ? only needs to surpass the value of w′x, independently of the image, x. This begs the question why there is such a clear relationship between ? and x. An answer could be that for the algorithms that generate the adversarial perturbation (BFGS and Sign), it is easier to find perturbations on the location of the object than in other locations, and hypothetically, we could also find adversarial perturbations that are aligned with the classifier in locations different from the object. Yet, this needs to be verified. The next experiment clarifies this point.\nCNNs act Locally Linearly on the Location of a Recognized Object We test if CNNs act as a linear classifier by evaluating whether f(x + ?) = f(x) + f( ?) holds in practice. We evaluate f( ?) and f(x + ?) − f(x) for each image, varying the norm of ?, for the ground truth object category. We remove from the CNNs the last soft-max layer, because this layer may distort the results of evaluating the linearity, and the accuracy of the CNN at test time is the same (the soft-max preserves the ranking of the classification scores). In Fig. 4, we show an example for BFGS for the image of Fig. 1. We indicate the point at which the perturbation produces the misclassification, ?. We show more examples for BFGS and Sign in Fig. 9, 10, 11 in Sec. A.1. We can see that f( ?) is completely unrelated to f(x+ ?)− f(x). Thus, the hypothesis that the CNNs behave too linearly needs to be reviewed in order to reconcile the hypothesis with these results.\nWe introduce the hypothesis that a CNN is locally linear to changes on the receptive fields that contain objects recognized by the CNN, otherwise the CNN acts non-linearly. We use the expression “locally linear” to denote that the linearity of the CNN only holds for perturbations close to x. Also, note that this linearity is only for perturbations on the regions of x that cause the CNN to recognize an object. This view of the CNN also suggest that adversarial examples are a consequence of CNNs acting as a high-dimensional linear classifier, since the hypothesis shows that CNNs work as a linear classifier in the vicinity of x, where there is the adversarial example. The hypothesis also explains why the perturbation is always aligned with the classifier on the location of the recognized object. This is because in a different location from the recognized objects the CNN does not behave linearly. Also, now we can explain the result in Fig. 4 about why f(x + ?) is not equal to f(x) + f( ?). Since for f( ?) the input of the CNN is not in the vicinity of an object that can be recognized by the CNN, f( ?) is not computed in the linear region of the CNN.\nNote that our hypothesis can be motivated from the non-linearities induced by the linear rectifier units (ReLUs) (Krizhevsky et al., 2012), which are used in all CNNs we tested. ReLUs are applied to the neural responses after the convolutional layers of the CNNs. They have a linear behavior\nonly for the active neural responses, otherwise they output a constant value equal to 0. Thus, if the perturbation is added in a location where the neural responses are active, the CNN tends to behave more linearly than when the neural responses are not active. The linearity of the CNN is local because it holds until the perturbation brings a ReLU to the non-linear region. Thus, when we increase the norm of the perturbation, the effect of the perturbation to the final classification score, f(x+ ?)−f(x), stops increasing at the same linear pace, because the number of ReLUs that return a 0 value is higher than before increasing the norm of the perturbation. This can be seen in Fig. 4, and Fig. 9, 10, 11 in Sec. A.1, when we increase the norm of the perturbation.\nFinally, we provide quantitative results to show that the linearity hypothesis reasonably holds in the vicinity of x. We approximate the alignment of the perturbation with the hypothetical linear classifier, i.e., w′ , and we evaluate how much this hypothetical linear classifier deviates from the real classification score, f(x+ )− f(x). To do the approximation of w′ we trace a line between f(x) and f(x + 2 ?), which gives w′ in the direction of ? (see the yellow line in Fig. 4). If the CNN would approximately act as a linear classifier between x and x + 2 ?, then the error between w′ ? and f(x+ ?)− f(x) would be 0. In Fig. 12 in Sec. A.1 we report the cumulative histogram of the number of images with an error smaller than a threshold, and we compare it with the error that yields calculating f(x+ ?)− f(x) as f( ?), which comes from the assumption that the CNN is always a linear classifier. We can see that the error derived from our hypothesis is much smaller than from the hypothesis that the CNN is always a linear classifier."
    }, {
      "heading" : "4 FOVEATION-BASED MECHANISMS ALLEVIATE ADVERSARIAL EXAMPLES",
      "text" : "In this section we introduce the hypothesis that supports that a mechanism based on applying foveations reduces the effect of adversarial examples. We define a foveation as a transformation of the image that selects a region in which the CNN is applied, discarding the information from the other regions. The foveation provides an input to the CNN that always has the same size, even if the size of the selected region varies. Thus, the foveation may produce a change of the location and scale of the objects in the input of the CNN. We assume that the foveation mechanism does not introduce non-linearities, e.g.,it does not modify pixel values from the original image, and the interpolations for re-sizing the image are linear. Without loss of generality, we use a crop of a region as the foveation mechanism.\nLet T (x) be the image after the foveation. Recall the linearity hypothesis introduced in the previous section, and recall the linearity of T (·). If the perturbation is applied in the same location of an object recognized by the CNN, we have that hypothetically, f(T (x + ?)) is equal to w′T (x) + w′T ( ?) for small perturbations. It is is well-known that the representations learned by the CNNs are robust to changes of scale and location of the objects, such as the transformations produced by the foveation. Thus, the term without the perturbation after the foveation, w′T (x), is not expected to have a lower accuracy than before, w′x. In addition, since T (·) may remove clutter from the image, applying the foveation could even improve the accuracy when there is not the perturbation, if T (x) does not remove any part of the target object.\nWe introduce the hypothesis that the aforementioned robustness of the CNNs to changes of scale and locations of the objects, does not generalize to the perturbations. Thus, the foveation mechanism reduces the alignment between the classifier and the perturbation, w′T ( ?), but does not negatively affect the alignment between the classifier and the image without perturbation, w′T (x). Note that CNNs are trained with objects at many different positions and scales, that produce representations robust to the transformations of the object, such as the transformation produced by T (·). However, objects are visually very different from the adversarial perturbations, and the robustness of CNNs to transformations of objects do not generalize to the perturbations.\nIn the following, we show experimental evidence that supports the hypothesis, and then, we use it to analyze previous works that learn CNNs using adversarial examples in the training set.\nExperiments that Support the Hypothesis We compare the accuracy after and before the foveation using two different set-ups, that are useful to analyze the effect of the foveation when removing the clutter, or when not. In the first set-up, we use the adversarial example used in previous experiments (MP), and in the second set-up, we generate the adversarial example with minimum norm, ?, for the image produced by cropping the target object with the ground truth bounding box (denoted as MP-Object). Note that MP-Object is the adversarial example with minimum norm when there is no clutter in the image. Next, we introduce the foveations we use for each set-up, and then, we report the results.\n— Experiments with the MP set-up: We test different foveation mechanisms based on cropping a region. We use as foveation mechanism the crop of the target object using the ground truth bounding box (Object Crop MP). If there are multiple bounding boxes in the image, because there are multiple target objects, we crop each of them, and average the classification scores. Note that this foveation mechanism guarantees that no part of the object is removed, and it almost completely removes the clutter. Also, it produces a change in scale and translation of the image in the location of the object. Note the difference between Object Crop MP and the adversarial perturbation for the second setup, MP-Object. Object Crop MP is T (x + ?) where ? is calculated for x, and MP-Object is the minimum perturbation, ?, for T (x).\nAnother foveation strategy we evaluate is using the 10 crops that come already implemented in the Caffe library we use (Jia et al., 2014). The crops are done on large regions of the image ensuring that in most cases the target object is contained in the crops (each crop discards about 21% of the area of the image). Then, the classification accuracy is computed by averaging the confidences from the multiple crops. We denote it as 10 Crop MP. The crops are always of the same size, and there are 5 of them (4 clamped at each corner of the image, and one in the center of the image). Then, the images resulting from these 5 crops are flipped, which makes a total of 10 crops.\nA third foveation mechanism we evaluate for MP is based on using a state-of-the-art saliency model to select 3 regions to crop from the most salient locations of the image, denoted as Saliency Crop MP. The crops are of the same size as the crops in 10 Crop MP, but they are generated selecting three centroids of the saliency map. We use the SALICON saliency map, which extracts the saliency map using the same CNNs we test (Huan et al., 2015). We observed that these saliency maps are robust to the adversarial perturbations, since these are generated to produce misclassification for object recognition, but not to affect the saliency prediction. To compare Saliency Crop MP with 10 Crop MP we use the same number of crops, i.e., we use 10 Crop MP but only with 3 random crops selected among the 10 crops (3 Crop MP). Note that 3 Crop MP, 10 Crop MP, and Saliency Crop MP do not guarantee that all the clutter is removed, and also, they may remove part of the target object.\n— Experiments with MP-Object set-up: In this set-up any foveation mechanism will not remove clutter, as there is only the cropped target object in the image. We apply the foveation based on the 10 crops of Caffe we introduced before, and we call it 10 Crop MP-Object. Each of the 10 crops are a shifted version of the target object, that remove part of the object and uses part of the background to do the crop (the foveation shifts a bit to the background, and it yields a cropped region with 21% of background). We set the size of the crops such that they do not modify the original scale of the object. Another foveation mechanism based on this, consists on selecting 1 random crop from the 10 crops (1 Crop MP-Object), which we will use to control how much imporvement comes from averaging the results of multiple crops.\n— Results: In Fig. 5 we report the accuracy using different values of the norm of the perturbation before the foveation. All the foveation mechanisms we introduce improve the accuracy between 30% to 40% in both set-ups, MP and MP-Object, when the perturbations are imperceptible or almost imperceptible. Object Crop MP produces the biggest improvement over all foveation mechanisms, probably because it is the only foveation mechanism that guarantees that parts of the target object are not removed. Also, for similar reasons, Saliency Crop MP has a better accuracy than 3 Crop MP. Table 1 in Sec. A.2 shows the increase of the norm of the perturbation to produce misclassification after the foveation mechanisms. We can see that for the different foveations this is about 5 times for BFGS, and between 5 to 8 times for Sign.\nIn Fig. 5, we can also see that the foveation slightly improves the accuracy in the absence of adversarial perturbation, when we crop the bounding box, Object Crop MP, or average over multiple crops, 3 and 10 Crop MP. In Table 2 in Sec. A.2 we show the exact values of the accuracy in Fig. 5, which are about 5% of improvement of the accuracy by using a foveation when there is no perturbation. This suggests that removing clutter and averaging multiple crops improves the accuracy. However, this can not be the main reason of the improvement of the accuracy by the foveation, since in the set-up of MP-Object, in which there is no clutter, the accuracy also significantly improves after the foveation. In this set-up, we do not show 1 Crop MP-Object in Fig. 5 because it is on the same curve as 10 Crops MP-Object (as shown in Table 2 in Sec. A.2).\nFig. 5 also shows that the accuracy of the adversarial examples in both set-ups, MP and MP-Object, are very similar. Since MP-Object is the perturbation generated using the cropped target object, and the accuracy is similar to MP, it shows that a foveation mechanism does not improve the norm of the minimum adversarial perturbation. Note that our hypothesis does not say that this should be the case. In applications, the perturbation may be generated to affect a specific foveation, e.g., MP-Object is the adversarial example for the foveation Object Crop MP. Yet, we can use a different foveation to alleviate the perturbation, e.g., 1 Crop MP-Object improves over MP-Object.\nFinally, Fig. 5 also shows that when we increase the value of the norm of the perturbation, the accuracy decreases in all cases. This is not surprising, since large perturbations bring the CNN to the non-linear region (Sec 3), and our hypothesis of the foveations assumes that the CNN is working in the linear region. Thus, when we increase the norm of the perturbation, the foveations are not effective since the reason of the misclassification is an occlusion of the object, and not an alignment of the perturbation with the linear classifier.\nEffect of T (·) in the Classification Scores of the Perturbation and the Target Object: To provide more evidence to support our hypothesis, we evaluate the classification scores for the ground truth object category, before and after the foveation (without the last soft-max layer, as in Sec. 3). We evaluate the image and the perturbation independently, in order to see if the foveation affects in a different way each of them, as our hypothesis predicts. Thus, we evaluate f(x) − f(T (x)), and (f(x+ ?)− f(x)) − (f(T (x+ ?))− f(T (x))) for the two set-ups, MP and MP-Obj, and the two perturbations, BFGS and Sign. In Fig. 6, we show the cumulative histogram of the number of images with a change of classification score smaller than the indicated in the horizontal axis. We can see that the alignment between the perturbation and the classifier decreases after the foveation. Yet, the term of the object without perturbation is not affected as much as the perturbation (BFGS\nand Sign are compared independently). Note that the result in Fig. 6 is for the scores of the ground truth object category, and does not tell much about the final classification accuracy. Yet, it strongly suggests that the robustness of the CNNs to the transformations of the foveation does not generalize to perturbations.\nComparison to Previous Works to Alleviate Adversarial Examples Previous works to alleviate adversarial perturbations are based on training a CNN that learns to classify adversarial examples generated at the training phase (Gu & Rigazio, 2015; Goodfellow et al., 2015). Yet, new adversarial examples can be created for the CNN during testing, and a new re-training of the CNN would be needed to take into account the new adversarial examples. Our hypothesis gives a plausible explanation for this phenomenon: CNNs do not generalize to new adversarial perturbations because they are not robust to transformations of the perturbation. If CNNs would have some non-linearity that produces some kind of invariance to transformations for the perturbation, the sample complexity to learn to classify with perturbations would decrease.\nFinally, observe that if we calculate the adversarial perturbation taking into account which foveation mechanism will be applied to the image, we can find an adversarial perturbation that will misclassify the CNN that uses the foveation. However, we can apply a second foveation mechanism different from the first one, that will transform the adversarial perturbation in a different way. At testing time, a different foveation can always be applied, but we can not re-train the CNN."
    }, {
      "heading" : "5 CONCLUSIONS",
      "text" : "Adversarial examples are a consequence of CNNs acting as a high-dimensional linear classifier in the vicinity of the images with objects recognized by the CNN. Also, the transformation of the image produced by a foveation decreases the effect of the adversarial perturbation to the classification scores, because the robustness of the CNNs to transformations of objects does not generalize to the perturbations.\nThis suggests that a system similar to Mnih et al. (2014), which integrates information from multiple fixations for object recognition, may be more robust to the phenomenon of the adversarial examples. Note that this system, which is based on integrating information from several image regions, is more similar to human vision than current CNN architectures. Yet, a remaining puzzle is the robustness of human perception to the perturbations. Some hints towards an answer could be that humans fixate their eyes on salient regions, and the eccentricity dependent resolution of the retina help eliminate the background outside the eye fixation."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work was supported by the Singapore Defense Innovative Research Program 9014100596, Ministry of Education Academic Research Fund Tier 2 MOE2014-T2-1-144, and also, by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216."
    }, {
      "heading" : "A RESULTS",
      "text" : "A.1 THE LINEARITY OF CNNS FOR IMAGENET\nReview of the Properties of Adversarial Examples in ImageNet In Fig. 7, we extend the results of Fig. 2 in the paper with the results for Sign perturbation using L∞ norm. We observe the same tendency than in Fig. 2.\nRole of the Target Object Location in the Perturbation In Fig. 8 we report the accuracy performance of the different CNN architectures for Sign perturbation and using L∞ norm, when changing the norm of the perturbation that is inside the mask, or the inverted mask, and the result for the perturbation applied in the full image (denoted as minimum perturbation (MP)). We make the same observations as Fig. 3 in the paper.\nCNNs act Locally Linear on the Location of a Recognized Object In Fig. 9, 10, 11 we show more examples as the one in Fig. 4 in the paper, for the images in Fig. 14, 16 and 18, in Sec. C, respectivelly. We can see that f( ?) is completely unrelated to f(x + ?) − f(x), and we need to review the hypothesis that the CNNs behave too linearly in order to reconcile the hypothesis with these results.\nIn Fig. 12, we approximate the alignment of the perturbation with the hypothetical linear classifier, i.e., w′ , and we evaluate how much this hypothetical linear classifier deviates from the real classification score, f(x + ) − f(x). To do the approximation of w′ we trace a line between f(x) and f(x + 2 ?), which gives w′ in the direction of ? (see the yellow line in Fig. 9). If the CNN would approximately act as a linear classifier between x and x + 2 ?, then the error between w′ ? and f(x + ?) − f(x) would be 0. We report the cumulative histogram of the number of images with an error smaller than a threshold, and we compare it with the error that yields calculating f(x + ?) − f(x) as f( ?), from the assumption that the CNN is always a linear classifier. We can see that the error derived from our hypothesis is much smaller than from the hypothesis that the CNN is always a linear classifier.\nA.2 FOVEATION-BASED MECHANISMS ALLEVIATE ADVERSARIAL EXAMPLES\nExperiments that Support the Hypothesis In Fig. 13 we report the accuracy for Sign and L∞ norm as in Fig. 5 of the paper. We can extract the same conclusions an in Fig. 5 in the paper.\nIn Table 1, we show the exact values of the improvements by Crop MP and Crop MP-Obj. We show how many times the perturbation needs to be increased to produce misclassification when we use the foveation. We can see that it is about 5 time more for BFGS, and between 8 to 5 times more for Sign depending on the CNN and the foveation mechanism. The Table also shows the control experiment that the 10 crops of Crop MP-Obj just produce a small improvement over doing one single crop.\nIn Table 2 we show the exact value of the accuracy of the results in Fig. 5 and Fig. 13."
    }, {
      "heading" : "B GENERATION OF THE ADVERSARIAL PERTURBATION",
      "text" : "Before introducing the generation of the perturbation, we introduce a more specific mathematical notation, that later will serve to clarify the details about the perturbation generation. Recall that x ∈ RN is an image of size N pixels. This image contains an object whose object category is of class label ` ∈ {1 . . . k}, where k is the number of object categories (e.g., k = 1000 in ImageNet). f : RN → Rk is the mapping from the input image to the classification scores returned by the CNN. Note that f(x) ∈ Rk is a vector that contains the classification scores for all object categories. We use e(f(x), `) to denote a function to evaluate the error of the classification scores, where ` is the label of the ground truth object category. In ImageNet, this error function is based on the top-5 scores (Russakovsky et al., 2015). Thus, e(f(x), l) returns 0 when ` corresponds to one of the five highest scores in f(x), otherwise e(f(x), l) returns 1.\nWe use ∈ RN to denote the perturbation image that produce a misclassification when added to the input image, i.e., e(f(x + ), l) = 1. The image x + is the adversarial example. The set of all perturbation images that produce a misclassification of an image can be grouped together. We use Ex ⊂ RN to denote such set, Ex = { ∈ RN | e(f(x+ ), l) = 1 } . Then, we define ? as the perturbation with minimal norm to produce misclassification of the image, i.e., ? = argmin ∈Ex ‖ ‖. Observe that the optimal perturbation depends on the norm we choose to minimize. We will analyze the L1 and L∞ norms, since the perturbations we analyze optimize one of these two norms.\nBFGS perturbation. As in (Szegedy et al., 2014), we approximate the perturbation ? by using a box-constrained L-BFGS 1. It consists on minimizing the L1 norm of the perturbation, ‖ ‖1, that produces a misclassification of the image using the top-5 accuracy criteria, i.e., e(f(x+ ), `) = 1. To do so, we minimize the L1 norm plus a loss function, which we denote as loss( , {x, `}). Let η be a constant that weights the L1 norm with respect to the loss function.\nSince the minimization should produce a misclassification, the loss function is based on the accuracy. Thus, the loss could be equal to (1 − e(f(x + ), `)). Since directly minimizing the top-5 accuracy is difficult, because the derivative of e(f(x + ) is 0 for any except in the boundary of producing a misclassification, we use a hinge loss. Thus, loss( , {x, `}) is equal to 0 when the image is misclassified (which corresponds to the final objective), otherwise the loss function takes the value of the classification score of the class that we aim to misclassify, which can be expressed as I(`)T f(x+ ) where I(`) ∈ Rk is an indicator vector that is 1 in the entry corresponding to the class ` and 0 otherwise.\nThe minimization of η‖ ‖1+ loss( , {x, `}) is done with L-BFGS. This uses the gradient of the loss with respect to , which can be computed using the back-propagation routines used during training of the CNN. In order to further minimize the norm returned by L-BFGS, we do a line search of the norm given the perturbation by L-BFGS, i.e., ̃ = α /‖ ‖, where α is a scalar factor. In the experiments, we set η = 10−6 because using this constant L-BFGS could find a perturbation that produces a misclassification in the majority of the images, except for approximately the 0.5% of the images. In all the CNNs tested, these images had a classification score higher than 0.9. To obtain a perturbation that produces a misclassification in these images, we apply the line-search method with the perturbation returned after stopping L-BFGS after one iteration.\nSign perturbation. It was introduced in Goodfellow et al. (2015). The perturbation of the Sign perturbation is generated using sign(∇loss( , {x, `})), which can be computed using back-propagation. Then, we use the line-search method to minimize the norm of the perturbation to misclassify all images.\n1http://www.cs.ubc.ca/˜schmidtm/Software/minFunc.html"
    }, {
      "heading" : "C PERCEPTIBILITY OF THE ADVERSARIAL PERTURBATION",
      "text" : "In the following pages we show examples of the perceptibility of the perturbations of the adversarial examples.\nIn Fig. 14, 16 and 18, we show examples of the same adversarial perturbation varying the L1 norm per pixel, and the L∞ norm, in Fig. 15, 17, 19. We can see that from a certain factor, the perturbation becomes clearly perceptible, and it occludes parts of the target object. This helps us approximately determine at what point the adversarial perturbations become perceptible. For BFGS, we can say that when the L1 norm per pixel is higher than 15 the perturbation becomes visible, and for L∞ the threshold is 100. This difference is because the density of BFGS is not the same through all image, it is mainly in the same location of the object, as shown in Fig. 1. For Sign, the threshold for both norms to make the perturbation visible is about 15, and it is the same for both norms because this perturbation is spread evenly through all the image. We use these values for the analysis."
    } ],
    "references" : [ {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Deng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Visualizing higher-layer features of a deep network",
      "author" : [ "Erhan", "Dumitru", "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal" ],
      "venue" : "Dept. IRO, Université de Montréal, Tech. Rep,",
      "citeRegEx" : "Erhan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2009
    }, {
      "title" : "Fundamental limits on adversarial robustness",
      "author" : [ "Fawzi", "Alhussein", "Omar", "Frossard", "Pascal" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Fawzi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fawzi et al\\.",
      "year" : 2015
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards deep neural network architectures robust to adversarial examples",
      "author" : [ "Gu", "Shixiang", "Rigazio", "Luca" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Gu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2015
    }, {
      "title" : "SALICON: Reducing the semantic gap in saliency prediction by adapting neural networks",
      "author" : [ "Huan", "Xun", "Schen", "Chengyao", "Boix", "Xavier", "Zhao", "Qi" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Huan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huan et al\\.",
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor" ],
      "venue" : "In MM,",
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Krizhevsky", "Alex" ],
      "venue" : "Tech. Rep,",
      "citeRegEx" : "Krizhevsky and Alex.,? \\Q2009\\E",
      "shortCiteRegEx" : "Krizhevsky and Alex.",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "LeCun", "Yann", "Bottou", "Léon", "Bengio", "Yoshua", "Haffner", "Patrick" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Mnih", "Volodymyr", "Hees", "Nicolas", "Graves", "Alex", "Kavukcuoglu", "Koray" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "Nguyen", "Anh", "Yosinski", "Jason", "Clune", "Jeff" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael S", "Berg", "Alexander C", "FeiFei", "Li" ],
      "venue" : null,
      "citeRegEx" : "Russakovsky et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russakovsky et al\\.",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Simonyan", "Karen", "Zisserman", "Andrew" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep inside convolutional networks: Visualizing image classification models and saliency maps",
      "author" : [ "Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2013
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian J", "Fergus", "Rob" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "The perturbation of the Sign perturbation is generated using sign(∇loss",
      "author" : [ "Goodfellow" ],
      "venue" : "Sign perturbation",
      "citeRegEx" : "Goodfellow,? \\Q2015\\E",
      "shortCiteRegEx" : "Goodfellow",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Further, when the norm of the perturbation is not constrained to be small, it has been found that there are perturbations that produce a “synthetic” image, which does not resemble any object, but that a CNN classifies with high confidence as an object category (Nguyen et al., 2015; Erhan et al., 2009; Simonyan et al., 2013).",
      "startOffset" : 261,
      "endOffset" : 325
    }, {
      "referenceID" : 1,
      "context" : "Further, when the norm of the perturbation is not constrained to be small, it has been found that there are perturbations that produce a “synthetic” image, which does not resemble any object, but that a CNN classifies with high confidence as an object category (Nguyen et al., 2015; Erhan et al., 2009; Simonyan et al., 2013).",
      "startOffset" : 261,
      "endOffset" : 325
    }, {
      "referenceID" : 14,
      "context" : "Further, when the norm of the perturbation is not constrained to be small, it has been found that there are perturbations that produce a “synthetic” image, which does not resemble any object, but that a CNN classifies with high confidence as an object category (Nguyen et al., 2015; Erhan et al., 2009; Simonyan et al., 2013).",
      "startOffset" : 261,
      "endOffset" : 325
    }, {
      "referenceID" : 3,
      "context" : "(Goodfellow et al., 2015; Fawzi et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "(Goodfellow et al., 2015; Fawzi et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "This hypothesis of CNNs acting too linearly is supported by quantitative experiments in datasets where the target object is always centered and fixed to the same scale and orientation, namely, MNIST (LeCun et al., 1998) and CIFAR (Krizhevsky, 2009).",
      "startOffset" : 199,
      "endOffset" : 219
    }, {
      "referenceID" : 8,
      "context" : "Can experiments for CNNs trained to recognize objects in unconstrained conditions, such as the CNNs used for ImageNet, challenge the current hypothesis? In this paper, we analyze adversarial examples using several common CNNs architectures for ImageNet (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015), and the experiments show that the hypothesis of the CNNs’ linearity needs a revision.",
      "startOffset" : 253,
      "endOffset" : 328
    }, {
      "referenceID" : 16,
      "context" : "Can experiments for CNNs trained to recognize objects in unconstrained conditions, such as the CNNs used for ImageNet, challenge the current hypothesis? In this paper, we analyze adversarial examples using several common CNNs architectures for ImageNet (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015), and the experiments show that the hypothesis of the CNNs’ linearity needs a revision.",
      "startOffset" : 253,
      "endOffset" : 328
    }, {
      "referenceID" : 15,
      "context" : "We also show that the adversarial perturbations that are generated with a CNN different from the CNN used for evaluation, produce a much smaller decrease of the accuracy compared to the CNN architectures used for the datasets in previous works (Szegedy et al., 2014; Goodfellow et al., 2015).",
      "startOffset" : 244,
      "endOffset" : 291
    }, {
      "referenceID" : 3,
      "context" : "We also show that the adversarial perturbations that are generated with a CNN different from the CNN used for evaluation, produce a much smaller decrease of the accuracy compared to the CNN architectures used for the datasets in previous works (Szegedy et al., 2014; Goodfellow et al., 2015).",
      "startOffset" : 244,
      "endOffset" : 291
    }, {
      "referenceID" : 7,
      "context" : "Szegedy et al. (2014) showed that images which are correctly classified by a CNN can be perturbed slightly to result in misclassification, and surprisingly, the perturbations are so small that are imperceptible to humans.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Previous works to alleviate adversarial perturbations learn CNNs with adversarial examples in the training set (Gu & Rigazio, 2015; Goodfellow et al., 2015).",
      "startOffset" : 111,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "Dataset We use ImageNet (ILSVRC) (Deng et al., 2009), which is a large-scale dataset for object classification and detection.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "We evaluate the accuracy performance with the standard protocol (Russakovsky et al., 2015), taking the top 5 prediction error.",
      "startOffset" : 64,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "CNN Architectures We report results using 3 CNN which are representative of the state-of-the-art, namely AlexNet (Krizhevsky et al., 2012), GoogLeNet (Szegedy et al.",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 16,
      "context" : ", 2012), GoogLeNet (Szegedy et al., 2015) and VGG (Simonyan & Zisserman, 2015).",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 6,
      "context" : "We use the publicly available pre-trained models provided by the Caffe library (Jia et al., 2014).",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "B, and to (Szegedy et al., 2014; Goodfellow et al., 2015).",
      "startOffset" : 10,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "B, and to (Szegedy et al., 2014; Goodfellow et al., 2015).",
      "startOffset" : 10,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "Namely, the algorithm by Szegedy et al. (2014), that is based on an optimization with L-BFGS, and we also use the algorithm by Goodfellow et al.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "(2014), that is based on an optimization with L-BFGS, and we also use the algorithm by Goodfellow et al. (2015), that uses the sign of the gradient of the loss function.",
      "startOffset" : 87,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "Another phenomenon that can be explained through the linearity of the CNN is that adversarial examples generated for a specific CNN model produce misclassification in other CNNs (Szegedy et al., 2014).",
      "startOffset" : 178,
      "endOffset" : 200
    }, {
      "referenceID" : 2,
      "context" : "The causes of adversarial examples have been studied by Goodfellow et al. (2015); Fawzi et al.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "(2015); Fawzi et al. (2015), that showed that small perturbations can affect linear classifiers, and CNNs may act as a high-dimensional linear classifier (of the same dimensionality as the image size).",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "We observe a clear tendency that when we increase the norm of the perturbation, it results in more misclassification on average, which is in accordance to the hypothesis that CNNs are linear (Goodfellow et al., 2015).",
      "startOffset" : 191,
      "endOffset" : 216
    }, {
      "referenceID" : 15,
      "context" : "(Szegedy et al., 2014).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "We observe a clear tendency that when we increase the norm of the perturbation, it results in more misclassification on average, which is in accordance to the hypothesis that CNNs are linear (Goodfellow et al., 2015). In Fig. 7 in Sec. A.1, the same conclusions can be extracted for L∞. Also, we observe that for the L1 norm per pixel, BFGS produces more misclassification on average than Sign, and the opposite is true if we evaluate with L∞. This is not surprising as the BFGS optimizes the L1 norm of the perturbation, and the Sign optimizes the maximum. Thus, in the rest of the experiments of the paper we only evaluate BFGS with the L1 norm per pixel, and Sign with the L∞ norm. Interestingly, we observe that there is not an imperceptible adversarial perturbation for every image. Recall that an L1 norm per pixel higher than 15, and L∞ also higher than 15 produce a perturbation slightly perceptible (Sec.2). Thus, the results in Fig. 2 show that the accuracy is between 10% and 20%, depending on the CNN, when the adversarial perturbations are constrained to be no more than slightly perceptible. Finally, we see that for a particular CNN architecture, the perturbation that results in more misclassification with lower norm is in all cases the one that is generated using the same CNN architecture. Observe that the perturbation generated for a CNN architecture, does not affect so severely the other CNN architectures as reported in MNIST, c.f. (Szegedy et al., 2014). When the adversarial perturbations are constrained to be no more than a bit perceptible, the accuracy is between 40% and 60% in the worst case, depending on the CNN. Following the reasoning of Goodfellow et al. (2015), the differences among the learned classifiers for the CNNs in ImageNet are bigger than the differences among the classifiers learned in MNIST.",
      "startOffset" : 192,
      "endOffset" : 1698
    }, {
      "referenceID" : 8,
      "context" : "Note that our hypothesis can be motivated from the non-linearities induced by the linear rectifier units (ReLUs) (Krizhevsky et al., 2012), which are used in all CNNs we tested.",
      "startOffset" : 113,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "Another foveation strategy we evaluate is using the 10 crops that come already implemented in the Caffe library we use (Jia et al., 2014).",
      "startOffset" : 119,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "We use the SALICON saliency map, which extracts the saliency map using the same CNNs we test (Huan et al., 2015).",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "Comparison to Previous Works to Alleviate Adversarial Examples Previous works to alleviate adversarial perturbations are based on training a CNN that learns to classify adversarial examples generated at the training phase (Gu & Rigazio, 2015; Goodfellow et al., 2015).",
      "startOffset" : 222,
      "endOffset" : 267
    }, {
      "referenceID" : 10,
      "context" : "This suggests that a system similar to Mnih et al. (2014), which integrates information from multiple fixations for object recognition, may be more robust to the phenomenon of the adversarial examples.",
      "startOffset" : 39,
      "endOffset" : 58
    } ],
    "year" : 2015,
    "abstractText" : "We show that adversarial examples, i.e., the visually imperceptible perturbations that result in Convolutional Neural Networks (CNN) fail, can be alleviated with a mechanism based on foveations — applying the CNN in a different image region. To see this, first, we report results in ImageNet that lead to a revision of the hypothesis that adversarial perturbations are a consequence of CNNs acting too linearly: a CNN acts locally linearly, only to changes in the receptive fields with objects recognized by the CNN, otherwise the CNN acts non-linearly. Then, we corroborate the hypothesis that when the neural responses are in the linear region, applying the foveation mechanism to the adversarial example tends to reduce the effect of the perturbation. This is because CNNs in ImageNet are robust to changes produced by the foveation (scale and translation of the recognized objects), but this property does not generalize to transformations of the perturbation.",
    "creator" : "LaTeX with hyperref package"
  }
}