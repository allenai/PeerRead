{
  "name" : "1610.03585.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Paradigm for Situated and Goal-Driven Language Learning",
    "authors" : [ "Jon Gauthier", "Igor Mordatch" ],
    "emails" : [ "jon@gauthiers.net", "mordatch@openai.com" ],
    "sections" : [ {
      "heading" : "A Paradigm for Situated and Goal-Driven Language Learning",
      "text" : "Jon Gauthier1,2\njon@gauthiers.net\nIgor Mordatch1,3\nmordatch@openai.com\n1OpenAI 2Stanford NLP Group 3UC Berkeley\nA distinguishing property of human intelligence is the ability to flexibly use language in order to communicate complex ideas with other humans in a variety of contexts. Research in natural language dialogue should focus on designing communicative agents which can integrate themselves into these contexts and productively collaborate with humans.\nIn this abstract, we propose a general situated language learning paradigm which is designed to bring about robust language agents able to cooperate productively with humans. This dialogue paradigm is built on a utilitarian definition of language understanding. Language is one of multiple tools which an agent may use to accomplish goals in its environment. We say an agent “understands” language only when it is able to use language productively to accomplish these goals. Under this definition, an agent’s communication success reduces to its success on tasks within its environment.\nThis setup contrasts with many conventional natural language tasks, which maximize linguistic objectives derived from static datasets. Such applications often make the mistake of reifying language as an end in itself. The tasks prioritize an isolated measure of linguistic intelligence (often one of linguistic competence, in the sense of Chomsky (1965)), rather than measuring a model’s effectiveness in real-world scenarios.1 Our utilitarian definition is motivated by recent successes in reinforcement learning methods. In a reinforcement learning setting, agents maximize success metrics on real-world tasks, without requiring direct supervision of linguistic behavior."
    }, {
      "heading" : "The environment",
      "text" : "We propose an end-to-end learning environment with multiple language-enabled agents, each with the capacity to define their own internal goals and plans to reach those goals. Each agent may also have different capacities to observe or act in this environment. Their goals are grounded non-linguistic objectives: for example, to reach a desired location, manipulate objects in the environment, or transmit a piece of information.2 (We address the issue of non-linguistic grounding at length in the next section.) Some fixed-language agents in the environment speak an existing conventional language (e.g. English), and other learning agents are tasked with jointly learning this language while also solving other goals in the environment. The agents are assigned difficult (possibly distinct) tasks which\n1Our motivation here is similar to that of Dagan et al. (2006), who recognized the negative effects of a community fragmented across different isolated application-specific tasks, and suggested the unified task of recognizing textual entailment (RTE) as a solution.\n2A related line of work in evolutionary linguistics constructs a similar language learning scenario entirely without fixed-language agents (Smith et al., 2003; Steels, 2012; Kirby et al., 2014). All of the agents in these environments construct a novel language simultaneously to accomplish some shared task. This is an interesting separate line of research, but ultimately a separate task from the understanding and acquisition problems discussed in this abstract.\nar X\niv :1\n61 0.\n03 58\n5v 1\n[ cs\n.C L\n] 1\n2 O\nct 2\n01 6\nrequire them to cooperate by communicating via their language channel.3\nIn the simplest instance of the paradigm above, a single fixed-language parent agent interacts with a learning child agent. The parent cooperates with the child, but only when prompted through language. As a result, the child acquires the language of the parent in order to accomplish its task. This specific scenario is designed to train single communicative agents which can accomplish tasks in their environments with the help of human or simulated counterparts.\nIt is possible to include fixed-language agents in these environments without requiring human involvement. We have developed several instantiations of this paradigm in which the fixed-language agents speak some simplified hard-coded English or an artificial language (e.g. a programming language or query language). Near the limit of artificial complexity, agents might take advantage of the Internet to access world knowledge and synthesize responses. As our algorithms for learning in these environments improve, however, we expect that involving human agents in the environment would be an extremely effective way to train language-enabled artificial agents."
    }, {
      "heading" : "Environment grounding",
      "text" : "Our language-learning agents crucially need to be grounded in a world which is not only linguistic. This grounding may be physical — for example, agents which are embodied in the real world — or virtual. This grounding is what allows us to evaluate the agents in some way that does not prioritize language as the only objective. Grounding imposes additional responsibilities on the agent, such as competently sensing and acting in the world, but this should be seen as an opportunity rather than a problem. As we discuss in the following paragraphs, a grounded agent may use its experience in its environment to build better models of language, and likewise use its language to better reason about its environment.\nThis grounded environment is designed to bring about agents with comprehensive predictive models of the world, which combine linguistic knowledge with more general intelligent behavior. By design, we do not separate the activity of language model construction from many other intelligent predictive activities — whether physical (predicting physical behavior of objects), psychological (modeling the beliefs and intentions of other agents), or social (understanding group membership and group-level action) (Lake et al., 2016; Clark, 2013). While early instantiations of this environment will limit the complexity necessary for the agents to model, we expect all of these different factors to eventually be relevant in a comprehensive learning environment.\nMikolov et al. (2015) make a similar argument for grounding, but arrive at an environment in which perception and action is mediated only through a linguistic channel. While this textual interface certainly has the potential to simplify the problem, we believe there are several issues with this setup.\nTextual interfaces impose a lossy representation of an agent’s actual environment (Brooks,"
    }, {
      "heading" : "3 Much of the recent novel work in dialogue-based learning (Fleischman and Roy, 2005; Vogel et al., 2014;",
      "text" : "Wang et al., 2016; Weston, 2016) and multi-agent communication (Lazaridou et al., 2016; Andreas and Klein,"
    }, {
      "heading" : "2016; Foerster et al., 2016; Sukhbaatar et al., 2016) can be fit into the paradigm described so far. This paper is",
      "text" : "concerned with designing a general paradigm for language-learning which contains these experiments, and picking out properties of the learning environment which are important for future research.\n1991), and necessarily bias the focus of an agent’s observations. Consider the following text-only interaction in the style of Mikolov et al. (2015) between a learning agent A and omniscient agent B, situated in some simulated physical world:\nB: You are carrying a box of eggs and need to set them down. A: Is there a table nearby? B: There is a table to your left. A: I put the box on the table. B: The box slides off of the table and the eggs break open. [The table is missing a leg and tilted when the box was laid down.]\nThe agent could have avoided this disaster if it had queried to find out that the table was missing a leg before placing weight on it. But we cannot expect an agent to exhaustively query its environment via text in general. (Should it also ask whether the table is made of solid material, or whether the table is on fire?) By contrast, a lossless visual observation does not remove possibly relevant information about the properties of tables, and it is up to the agent to learn what to attend to in such an environment and how to interpret these visual percepts.\nFor environments involving pre-programmed fixed-language agents, generating textual environment descriptions also places a significant implementational burden. It is relatively easy to generate physical motion and visual renderings of three-legged tables or toppling events, but much more difficult to describe these concepts in natural language.\nOf course, even if we claim that text data is not enough, we must settle for some level of abstraction. A similar argument to the one above could be made for including sound-wave speech data in the agent’s observations rather than only providing text and visual data. While this argument is likely also correct, we have to settle for a level of representation which is computationally tractable. Given that we want to make near-term progress on language comprehension and production, it seems reasonable to work with messages in text representation for the time being. It is likewise tractable to work in a virtual environment with simulated physics and visual inputs."
    }, {
      "heading" : "Conclusion",
      "text" : "In this abstract, we outlined a paradigm for grounded and goal-driven language learning in artificial agents. The paradigm is centered around a utilitarian definition of language understanding, which equates language understanding with the ability to cooperate with other language users in real-world environments. This position demotes language from its position as a separate task to be solved to one of several communicative tools agents might use to accomplish their real-world goals.\nWhile this paradigm does already capture a small amount of recent work in dialogue, on the whole it has not received the focus it deserves in the research communities of natural language processing and machine learning. We hope this paper brings focus to the task of situated language learning as a way forward for research in natural language dialogue."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors wish to thank their colleagues at OpenAI and Stanford for their useful comments and critiques."
    } ],
    "references" : [ {
      "title" : "Reasoning About Pragmatics with Neural Listeners and Speakers",
      "author" : [ "Jacob Andreas", "Dan Klein" ],
      "venue" : "[cs],",
      "citeRegEx" : "Andreas and Klein.,? \\Q2016\\E",
      "shortCiteRegEx" : "Andreas and Klein.",
      "year" : 2016
    }, {
      "title" : "Intelligence without representation",
      "author" : [ "Rodney Brooks" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Brooks.,? \\Q1991\\E",
      "shortCiteRegEx" : "Brooks.",
      "year" : 1991
    }, {
      "title" : "Whatever next? predictive brains, situated agents, and the future of cognitive science",
      "author" : [ "Andy Clark" ],
      "venue" : "Behavioral and Brain Sciences, 36(03):181–204,",
      "citeRegEx" : "Clark.,? \\Q2013\\E",
      "shortCiteRegEx" : "Clark.",
      "year" : 2013
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini" ],
      "venue" : "In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment,",
      "citeRegEx" : "Dagan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), chapter Intentional Context in Situated Natural Language Learning, pages 104–111",
      "author" : [ "Michael Fleischman", "Deb Roy" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Fleischman and Roy.,? \\Q2005\\E",
      "shortCiteRegEx" : "Fleischman and Roy.",
      "year" : 2005
    }, {
      "title" : "Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks",
      "author" : [ "Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson" ],
      "venue" : "[cs],",
      "citeRegEx" : "Foerster et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2016
    }, {
      "title" : "Iterated learning and the evolution of language",
      "author" : [ "Simon Kirby", "Tom Griffiths", "Kenny Smith" ],
      "venue" : "Current Opinion in Neurobiology,",
      "citeRegEx" : "Kirby et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kirby et al\\.",
      "year" : 2014
    }, {
      "title" : "Building machines that learn and think like people",
      "author" : [ "Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman" ],
      "venue" : "CoRR, abs/1604.00289,",
      "citeRegEx" : "Lake et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards Multi-Agent Communication-Based Language Learning",
      "author" : [ "Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni" ],
      "venue" : "[cs],",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2016
    }, {
      "title" : "A Roadmap towards Machine Intelligence",
      "author" : [ "Tomas Mikolov", "Armand Joulin", "Marco Baroni" ],
      "venue" : "[cs],",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2015
    }, {
      "title" : "Iterated learning: A framework for the emergence of language",
      "author" : [ "Kenneth Smith", "Simon Kirby", "Henry Brighton" ],
      "venue" : "Artificial Life, 9(4):371–386,",
      "citeRegEx" : "Smith et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2003
    }, {
      "title" : "Grounding language through evolutionary language games. In Language Grounding in Robots, pages 1–22",
      "author" : [ "Luc Steels" ],
      "venue" : "URL http://link.springer.com/ chapter/10.1007/978-1-4614-3064-3_1",
      "citeRegEx" : "Steels.,? \\Q2012\\E",
      "shortCiteRegEx" : "Steels.",
      "year" : 2012
    }, {
      "title" : "Learning Multiagent Communication with Backpropagation",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus" ],
      "venue" : "[cs],",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to reason pragmatically with cognitive limitations",
      "author" : [ "Adam Vogel", "Andrés Gómez Emilsson", "Michael C. Frank", "Dan Jurafsky", "Christopher Potts" ],
      "venue" : "In Proceedings of the 36th Annual Meeting of the Cognitive Science Society,",
      "citeRegEx" : "Vogel et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Vogel et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning language games through interaction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2368–2378, Berlin, Germany, August 2016",
      "author" : [ "Sida I. Wang", "Percy Liang", "Christopher D. Manning" ],
      "venue" : "Association for Computational Linguistics. URL http://www.aclweb. org/anthology/P16-1224",
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Dialog-based language learning",
      "author" : [ "Jason Weston" ],
      "venue" : "arXiv preprint arXiv:1604.06045,",
      "citeRegEx" : "Weston.,? \\Q2016\\E",
      "shortCiteRegEx" : "Weston.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "A related line of work in evolutionary linguistics constructs a similar language learning scenario entirely without fixed-language agents (Smith et al., 2003; Steels, 2012; Kirby et al., 2014).",
      "startOffset" : 138,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "A related line of work in evolutionary linguistics constructs a similar language learning scenario entirely without fixed-language agents (Smith et al., 2003; Steels, 2012; Kirby et al., 2014).",
      "startOffset" : 138,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : "A related line of work in evolutionary linguistics constructs a similar language learning scenario entirely without fixed-language agents (Smith et al., 2003; Steels, 2012; Kirby et al., 2014).",
      "startOffset" : 138,
      "endOffset" : 192
    }, {
      "referenceID" : 3,
      "context" : "Our motivation here is similar to that of Dagan et al. (2006), who recognized the negative effects of a community fragmented across different isolated application-specific tasks, and suggested the unified task of recognizing textual entailment (RTE) as a solution.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "By design, we do not separate the activity of language model construction from many other intelligent predictive activities — whether physical (predicting physical behavior of objects), psychological (modeling the beliefs and intentions of other agents), or social (understanding group membership and group-level action) (Lake et al., 2016; Clark, 2013).",
      "startOffset" : 321,
      "endOffset" : 353
    }, {
      "referenceID" : 2,
      "context" : "By design, we do not separate the activity of language model construction from many other intelligent predictive activities — whether physical (predicting physical behavior of objects), psychological (modeling the beliefs and intentions of other agents), or social (understanding group membership and group-level action) (Lake et al., 2016; Clark, 2013).",
      "startOffset" : 321,
      "endOffset" : 353
    }, {
      "referenceID" : 1,
      "context" : ", 2016; Clark, 2013). While early instantiations of this environment will limit the complexity necessary for the agents to model, we expect all of these different factors to eventually be relevant in a comprehensive learning environment. Mikolov et al. (2015) make a similar argument for grounding, but arrive at an environment in which perception and action is mediated only through a linguistic channel.",
      "startOffset" : 8,
      "endOffset" : 260
    }, {
      "referenceID" : 4,
      "context" : "3 Much of the recent novel work in dialogue-based learning (Fleischman and Roy, 2005; Vogel et al., 2014; Wang et al., 2016; Weston, 2016) and multi-agent communication (Lazaridou et al.",
      "startOffset" : 59,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "3 Much of the recent novel work in dialogue-based learning (Fleischman and Roy, 2005; Vogel et al., 2014; Wang et al., 2016; Weston, 2016) and multi-agent communication (Lazaridou et al.",
      "startOffset" : 59,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : "3 Much of the recent novel work in dialogue-based learning (Fleischman and Roy, 2005; Vogel et al., 2014; Wang et al., 2016; Weston, 2016) and multi-agent communication (Lazaridou et al.",
      "startOffset" : 59,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "3 Much of the recent novel work in dialogue-based learning (Fleischman and Roy, 2005; Vogel et al., 2014; Wang et al., 2016; Weston, 2016) and multi-agent communication (Lazaridou et al.",
      "startOffset" : 59,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : ", 2016; Weston, 2016) and multi-agent communication (Lazaridou et al., 2016; Andreas and Klein, 2016; Foerster et al., 2016; Sukhbaatar et al., 2016) can be fit into the paradigm described so far.",
      "startOffset" : 52,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : ", 2016; Weston, 2016) and multi-agent communication (Lazaridou et al., 2016; Andreas and Klein, 2016; Foerster et al., 2016; Sukhbaatar et al., 2016) can be fit into the paradigm described so far.",
      "startOffset" : 52,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : ", 2016; Weston, 2016) and multi-agent communication (Lazaridou et al., 2016; Andreas and Klein, 2016; Foerster et al., 2016; Sukhbaatar et al., 2016) can be fit into the paradigm described so far.",
      "startOffset" : 52,
      "endOffset" : 149
    }, {
      "referenceID" : 12,
      "context" : ", 2016; Weston, 2016) and multi-agent communication (Lazaridou et al., 2016; Andreas and Klein, 2016; Foerster et al., 2016; Sukhbaatar et al., 2016) can be fit into the paradigm described so far.",
      "startOffset" : 52,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "Consider the following text-only interaction in the style of Mikolov et al. (2015) between a learning agent A and omniscient agent B, situated in some simulated physical world:",
      "startOffset" : 61,
      "endOffset" : 83
    } ],
    "year" : 2016,
    "abstractText" : "A distinguishing property of human intelligence is the ability to flexibly use language in order to communicate complex ideas with other humans in a variety of contexts. Research in natural language dialogue should focus on designing communicative agents which can integrate themselves into these contexts and productively collaborate with humans. In this abstract, we propose a general situated language learning paradigm which is designed to bring about robust language agents able to cooperate productively with humans. This dialogue paradigm is built on a utilitarian definition of language understanding. Language is one of multiple tools which an agent may use to accomplish goals in its environment. We say an agent “understands” language only when it is able to use language productively to accomplish these goals. Under this definition, an agent’s communication success reduces to its success on tasks within its environment. This setup contrasts with many conventional natural language tasks, which maximize linguistic objectives derived from static datasets. Such applications often make the mistake of reifying language as an end in itself. The tasks prioritize an isolated measure of linguistic intelligence (often one of linguistic competence, in the sense of Chomsky (1965)), rather than measuring a model’s effectiveness in real-world scenarios. Our utilitarian definition is motivated by recent successes in reinforcement learning methods. In a reinforcement learning setting, agents maximize success metrics on real-world tasks, without requiring direct supervision of linguistic behavior.",
    "creator" : "LaTeX with hyperref package"
  }
}