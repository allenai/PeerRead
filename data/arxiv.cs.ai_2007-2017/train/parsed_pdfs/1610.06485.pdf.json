{
  "name" : "1610.06485.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Multidimensional Cascade Neuro-Fuzzy System with Neuron Pool Optimization in Each Cascade",
    "authors" : [ "Yevgeniy V. Bodyanskiy", "Daria S. Kopaliani" ],
    "emails" : [ "bodya@kture.kharkov.ua", "}@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION\nToday artificial neural networks (ANNs) and neuro-fuzzy systems (NFSs) are successfully used in a wide range of data processing problems (when data can be presented either in the form of “object-property” tables or in the form of time series, often produced by non-stationary nonlinear stochastic or chaotic systems). The advantages ANNs and NFSs have over other existing approaches derive from their universal approximating capabilities and learning capacities.\nConventionally “learning” is defined as a process of adjusting synaptic weights using an optimization procedure that involves searching for the extremum of a given learning criterion. The learning process quality can be improved by adjusting a network topology along with its synaptic weights [1, 2]. This idea is the foundation of evolving computational intelligence systems [3, 4]. One of the most successful implementations of this approach is cascadecorrelation neural networks [5–8] due to their high degree of efficiency and learning simplicity of both synaptic weights and a network topology). Such a network starts off with a simple architecture consisting of a pool (ensemble) of neurons which are trained independently (the first cascade). Each neuron in the pool can have a different activation function and a different learning algorithm. The neurons in the pool do not interact with each other while they are trained. After all the neurons in the pool of the first cascade have had their weights adjusted, the best neuron with respect to a learning criterion forms the first cascade and its synaptic weights can no longer be adjusted. Then the second cascade is formed usually out of similar neurons in the training pool. The only difference is that neurons which are trained in the pool of the second cascade have an additional input (and therefore an additional synaptic weight) which is an output of the first cascade. Similar to the first cascade, the second cascade will eliminate all but one neuron showing the best performance whose synaptic weights will thereafter be fixed.\nNeurons of the third cascade have two additional inputs, namely the outputs of the first and second cascades. The evolving network continues to add new cascades to its architecture until it reaches the desired quality of problem solving over the given training set.\nAuthors of the most popular cascade neural network, CasCorLA, S. E. Fahlman and C. Lebiere, used elementary Rosenblatt perceptrons with traditional sigmoidal activation functions and adjusted synaptic weights using the Quickprop-algorithm (a modification of the  -learning rule). Since the outgoing signal of such neurons is nonlinearly dependent on its synaptic weights, the learning rate cannot be increased for such neurons. In order to avoid multi-epoch learning [9–16], different types of neurons (with outputs that depend linearly on synaptic weights) should be used as network nodes. This would allow the use of optimal learning algorithms in terms of speed and process data as it is an input to the network. However, if the network is learning in an online mode, it is impossible to determine the best neuron in the pool. While working with non-stationary objects, one neuron of the training pool can be identified as the best for one part of the training set, but not for the others. Thus we suggest that all neurons\nretain in the training pool and a certain optimization procedure (generated according to a general network quality criterion) is used to determine an output of the cascade.\nIt should be noticed that the well-known cascade neural networks implement non-linear mapping 1nR R , i.e. they are a systems with a single output. At the same time, many problems (which are solved with the help of ANNs and NFSs) require the multidimensional mapping n gR R implementation, which leads to the fact that g times more neurons should be trained in each cascade comparing to a conventional neural network, which makes such a system too cumbersome. Therefore, it seems appropriate to use as the cascade network nodes specialized multidimensional neuron structures with multiple outputs instead of traditional neurons like elementary Rosenblatt perceptrons.\nThe remainder of this paper is organized as follows: Section 2 gives an optimized multidimensional cascade neural network architecture. Section 3 describes training neo-fuzzy neurons in the network. Section 4 describes output signals’ optimization of the multidimensional neo-fuzzy neuron pool. Section 5 presents experiments and evaluation. Conclusions and future work are given in the final section.\nII. AN OPTIMIZED MULTIDIMENSIONAL CASCADE NEURAL NETWORK ARCHITECTURE\nAn input of the network (the so-called “receptive layer”) is a vector signal         1 2, , ,  Tnx k x k x k x k , where 1, 2, k is either the quantity of samples in the “object-property” table or the current discrete time. These\nsignals are sent to the inputs of each neuron [ ]mjMN in the network ( 1,2, , j q is the quantity of neurons in a training pool, 1,2, m is a cascade number), that produces vector outputs\n        [ ] [ ] [ ] [ ]1 2ˆ ˆ ˆ ˆ, , ,  Tm j m j m j m jgdy k y k y k y k , 1, 2, , d g . These outputs are later combined with a generalizing neuron [ ]mGMN which generates an optimal vector output  *[ ]ˆ my k of the m th cascade. While the input of the neurons in the first cascade is  x k , neurons in the second cascade have g additional inputs for the generated signal  *[1]ŷ k , neurons in the third cascade have 2g additional inputs    *[1] *[2]ˆ ˆ,y k y k , neurons in the m th cascade have  1m g additional inputs      *[1] *[2] *[ 1]ˆ ˆ ˆ, , ,  my k y k y k . New cascades become a part of the network during a training process when it becomes clear that the current cascades do not provide the desired quality of information processing.\nIII. TRAINING NEO-FUZZY NEURONS IN THE MULTIDIMENSIONAL CASCADE NEURO-FUZZY NETWORKS\nThe low learning rate of the Rosenblatt perceptrons (which are widely used in traditional cascade ANNs) along with the difficulty in interpreting results (inherent to all ANN in general) encourages us to search for alternative approaches to the synthesis of evolving systems in general and cascade neural networks in particular. High interpretability and transparency along with good approximation capabilities and ability to learn are the main features of neuro-fuzzy systems [17, 18], which are the foundation of hybrid computational intelligence systems. In [10,11,13] hybrid cascade systems were introduced which used neo-fuzzy neurons [19-21] as network nodes, allowing one to significantly increase the speed of synaptic weight adjustment. A neo-fuzzy neuron (NFN) is a nonlinear system having the following mapping\n  1\nˆ n\ni i i y f x   (1)\nwhere ix is the i th input  1,2, , i n , ŷ is the neo-fuzzy neuron’s output. Structural units of the neo-fuzzy neuron are nonlinear synapses iNS which transform the i th input signal in the following way\n    1\nh\ni i li li i l f x w x   (2)\nwhere liw is the l th synaptic weight of the i th nonlinear synapse, 1, 2, l h is the total quantity of synaptic weights and therefore membership functions  li ix in this nonlinear synapse. In this way iNS implements the fuzzy inference [18]\ni li liIF x IS X THEN THE OUTPUT IS w (3)\nwhere liX is a fuzzy set with a membership function li , liw is a singleton (a synaptic weight in a consequent). It can be seen that, in fact, a nonlinear synapse implements Takagi-Sugeno zero-order fuzzy inference.\nThe j th neo-fuzzy neuron  1, 2, , j q of the d th output  1, 2, , d g of the first cascade (according to the network topology) can be written\n       \n   \n[1] [1] [1] [1]\n1 1 1 [1]\nˆ ,\n. 4\nn n h j j j j\ni id di dli dli i i l\nj j i li dli\ny k f x k w x k\nIF x k IS X THEN THE OUTPUT IS w\n   \n       \nAuthors of the neo-fuzzy neuron [19–21] used traditional triangular constructions meeting the conditions of Ruspini partitioning (unity partitioning) as membership functions [22]:\n \n[1] , 1, [1] [1]\n, 1,[1] [1] , 1,\n[1] , 1,[1] [1] [1]\n, 1,[1] [1] , 1,\n, , ,\n, , ,\n0,\nj i d l i j j\ni d l i dlij j dli d l i\nj id l ij j j\ni idli dli d l ij j d l i dli\nx c if x c c\nc c\nc x x if x c c\nc c\notherwise\n\n \n\n \n\n       \n           \n(5)\nwhere [1] jdlic are relatively arbitrarily chosen (usually evenly distributed) center parameters of membership functions\nover the interval  0,1 where, naturally, 0 1 ix . This choice of membership functions ensures that the input signal ix only activates two neighboring membership functions, and their sum is always equal to 1 which means that\n   [1] [1], 1, 1  j j i idli d l ix x  (6)\nand\n     [1] [1] [1] [1] [1], 1, , 1, .   j j j j j i i idi dli dli d l i d l if x w x w x  (7)\nIt is clear that other constructions such as polynomial and harmonic functions, wavelets, orthogonal functions, etc. can be used as membership functions in nonlinear synapses. It is still unclear which of the functions provide the best results, which is why the idea of using not one neuron, but a pool of neurons with different membership and activation functions seems promising.\nSimilar to (4) we can determine outputs for the remaining cascades: outputs of the neurons in the second cascade:\n \n \n[2] [2] [2]\n1 1\n[2] [2] *[1] , 1 , 1\n1 1\nˆ\nˆ 1, 2, , ;\n \n   \n \n  \n\n \n\nn h j j j\nid dli dli i l\ng h j j\ndl n dl n d d l\ny w x\nw y d g\n\n (8)\noutputs of the neurons in the m th cascade:\n \n \n[ ] [ ] [ ]\n1 1 1\n[ ] [ ] *[ ]\n1 1 1\nˆ\nˆ 1,2, , .\n   \n\n   \n \n  \n\n   \nn h m j m j m j\nid dli dli i l\ng n m h m j m j p n\ndlp dlp d d p n l\ny w x\nw y d g\n\n\n(9)\nThus, the cascade network formed with neo-fuzzy neurons consisting of m cascades contains 1\n1\n\n          m p gh n p\nparameters. Introducing the vector of membership functions for the j th NFN of the d th output in the m th cascade\n                               [ ] [ ] [ ] [ ] [ ] [ ] 1 1 2 211 1 12 2 [ ] [ ] [ ] [ ]*[1] *[1] *[ 1] 11, 1 , , 1 , , , , , , , , , ˆ ˆ ˆ, , , , , m j m j m j m j m j m j id d dh d dh dli Tm j m j m j m j m n g gdhn d n dh n h dh g n m k x k x k x k x k x k x k y k y k y k                     \nand a corresponding synaptic weights’ vector\n   [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]11 1 12 2 1, 1 , , 1, , , , , , , , , , , , , , , Tm j m j m j m j m j m j m j m j m j m j d d dh d dh dli dhn d n dh n h dh g n mw w w w w w w w w w         \nthe output signal can be finally written down in a compact form\n     [ ] [ ] [ ]ˆ . Tm j m j m jd d dy k w k (10) Since this signal is linearly dependent on the synaptic weights, any adaptive identification algorithm [23-25] can be used for training the network neo-fuzzy-neurons, for example, the exponentially-weighted least-squares method in a recurrent form\n                      \n                      \n[ ] [ ] [ ]\n1[ ] [ ] [ ]\n[ ] [ ] [ ]\n[ ] [ ] [ ]\n1[ ] [ ] [ ] [ ]\n[ ] [ ]\n1 1\n1 1\n1 1 ,\n11 1\n1 1\n1 11\nTm j m j m j d d d\nm j m j m j d d d d\nTm j m j m j d d d\nTm j m j m j d d d\nm j m j m j m j d d d d\nTm j m j d d\nw k w k k\nP k k P k y k\nw k k k\nP k P k k\nP k k P k k\nk P k\n \n\n \n  \n \n\n\n\n                                   \n     \n(here  1 , 1,2, ,  dy k d g an external learning signal, 0 1   a forgetting factor) or the gradient learning algorithm with both tracking and filtering properties [26]\n                 \n     \n1[ ] [ ] [ ]\n[ ] [ ] [ ]\n2[ ] [ ] [ ]\n1 1 1\n1 1 , 12\n1 1 , 0 1.\nm j m j m j d d d d\nTm j m j m j d d d\nm j m j m j d d d\nw k w k r k y k\nw k k k\nr k r k k\n \n  \n                   \nAn architecture of a typical neo-fuzzy neuron which was discussed earlier as a part of the multidimensional neuron [1] gMN of the cascade system is redundant, since a vector of input signals  x k (the first cascade) is fed to one-type\nnonlinear synapses [1] jdiNS of the neo-fuzzy neurons, each neuron of which generates at its output a signal  [1]ˆ , 1, 2, , jdy k d g . As a result, the output vector components         [1] [1][1] [1]1 2ˆ ˆ ˆ ˆ, , ,  Tj jj jgy k y k y k y k are calculated independently. This can be avoided by considering a multidimensional neo-fuzzy neuron [27], which is a modification of the system proposed in [28]. Structure nodes are composite nonlinear synapses [1] jiMNS , each\nsynapse contains h membership functions [1] jli and gh adjustable synaptic weights [1] j dliw . Thus, a multidimensional neo-fuzzy neuron of the first cascade contains ghn synaptic weights, but only hn membership functions that makes it g times less than if the cascade would have been formed of conventional neo-fuzzy neurons.\nTaking into consideration a  1 hn membership functions’ vector\n                 [1] [1] [1][1] 1 1 111 21 1 [1] [1] , , , , , , ,     j j jj h Tj j i nli hn k x k x k x k x k x k       and a   g hn synaptic weights’ matrix\n[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1]\n[1] 211 212 2 2\n[1] [1] [1] [1] 11 12                             j j j j li hn j j j j j li hn j j j j g g gli ghn w w w w w w w w W w w w w ,\nthe output signal [1]jMN can be written down at the k th time moment in the form of\n   [1] [1] [1]ˆ .j j jy k W k (13)\nMultidimensional neo-fuzzy-neuron learning can be implemented with the help of a matrix modification of the exponentially-weighted recurrent least squares method (11) in the form of\n                      \n                     \n[1] [1] [1] [1]\n1[1] [1] [1]\n[1] [1]\n[1] [1] [1]\n1[1] [1] [1] [1]\n[1] [1]\n1 1\n1 1 1\n1 ,\n11 1\n1 1\n1 ,0 1 14\nTj j j j\nj j j\nTj j\nTj j j\nj j j j\nTj j\nW k W k k P k\nk y k W k k\nk P k\nP k P k k\nP k k P k k\nk P k\n \n \n\n  \n \n \n\n\n                                    \nor in the form of a multidimensional version of the algorithm (12) [29]:\n                 \n     \n1[1] [1] [1]\n[1] [1] [1]\n2[1] [1] [1]\n1 1 1\n1 1 , 15\n1 1 , 0 1\nj j j\nTj j j\nj j j\nW k W k r k y k\nW k k k\nr k r k k\n \n  \n                  \nhere         1 21 1 , 1 , , 1 .     Tgy k y k y k y k The remaining cascades are trained in a similar manner, while the m th cascade membership functions’ vector\n [ ] 1m j k increases its dimensionality by  1m g components which are formed by the previous cascades’ outputs.\nIV. OUTPUT SIGNALS’ OPTIMIZATION OF THE MULTIDIMENSIONAL NEO-FUZZY NEURON POOL\nThe output signals of all the neurons [ ]mdMN of the pool in each cascade are joined by a neuron generalizer [ ]mGMN , the output signals         *[ ] *[ ]*[ ] *[ ]1 2ˆ ˆ ˆ ˆ, , ,  Tm mm mgy k y k y k y k of which should surpass the accuracy of any signal  [ ]ˆ mjy k . This problem can be solved by using undetermined Lagrange multipliers and adaptive multidimensional generalized prediction [30, 31].\nLet’s introduce a neuron output signal [ ]mGMN in the form of\n     [ ] [ ]*[ ] [ ] [ ] 1\nˆ ˆ ˆ q\nm mm m m j j\nj y k c y k y k c    (16)\nwhere         [ ] [ ][ ] [ ]1 2ˆ ˆ ˆ ˆ, , , m mm mqy k y k y k y k   g q a matrix,    [ ] 1  mc k q a generalization coefficients’ vector meeting the unbiasedness conditions\n[ ] [ ]\n1 1,   \nq m T m j\nj c E c (17)\n   1,1, ,1 1    TE q a vector that consists of ones. Entering a training criterion\n               2[ ] [ ] [ ] [ ] [ ] [ ] [ ] 1 ˆ ˆˆ k Tm m m m m m mE k y y c Tr Y k Y k I c Y k Y k I c               (18)\n(here           1 , 2 , ,    TT T TY k y y y k k s an observations’ matrix,\n \n     \n     \n     \n[ ] [ ] [ ] 1 2 [ ] [ ] [ ]\n[ ] 1 2\n[ ] [ ] [ ] 1 2\nˆ ˆ ˆ1 1 1\nˆ ˆ ˆ2 2 2ˆ\nˆ ˆ ˆ                         m T m T m T q m T m T m T m q m T m T m T q y y y y y y Y k y k y k y k a   k gq matrix, I an identity   g g matrix,  a\ntensor product symbol). Let’s write down the Lagrange function taking into consideration the constraint (6)\n           \n                  \n2[ ] [ ] [ ] [ ] [ ] [ ]\n1\n[ ] [ ] [ ] [ ] [ ]\n[ ] [ ] [ ]\nˆ1 1\nˆ ˆ 1\n1\nk m m T m m m T m\nTm m m m T m\nm T m T m\nL k E k E c y y c E c\nTr Y k Y k I c Y k Y k I c E c\nTr V k V k E c\n    \n\n\n        \n        \n  \n\n(19)\nwhere        [ ] [ ] [ ]ˆ     m m mV k Y k Y k I c k g an innovations’ matrix. The Karush-Kuhn-Tucker equations’ system solution\n \n  [ ]\n[ ]\n[ ]\n0,\n0       \n m\nm c\nm\nL k\nL k \n(20)\nleads to an obvious result\n        \n11 1[ ] [ ] [ ]\n1[ ]\n,\n2\nm m T m\nT m\nc R k E E R k E\nE R k E\n \n           \n(21)\nwhere      [ ] [ ] [ ] .m m T mR k V k V k\nThus, an optimal union of all neurons’ pool outputs of each cascade can be organized. It is clear that not only the multidimensional neo-fuzzy neurons can be used as such neurons, but also any other structures that implement the nonlinear mapping  1  n m g gR R .\nV. EXPERIMENT AND ANALYSIS\nTo demonstrate the efficiency of the proposed adaptive neuro-fuzzy system and its learning procedures (19) and (21), we have implemented a simulation test based on solving the chaotic Lorenz attractor identification. The Lorenz attractor is a fractal structure corresponding to the long-term behavior of Lorenz oscillator. The Lorenz oscillator is a 3-dimensional dynamical system that exhibits chaotic flow, well-known for its lemniscate shape. The map shows how a dynamical system state (three variables of a three-dimensional system) evolves over time in a complex nonrepeating pattern.\nThe Lorenz attractor is described by the differential equation in the form\n    ,\n, .\n            x y x y x r z y z xy bz \n(22)\nWe can rewrite (22) in the recurrent form:\n                                 1 , 1 , 1            \n   \nx i x i y i x i dt\ny i y i rx i x i z i y i dt\nz i z i x i y i bz i dt\n\n(23)\nwhere parameter values are: 810, 28, , 0.0013   r b dt .\nThe data sample was obtained using (23) which consists of 10000 samples, where 7000 samples form a training set, 3000 samples form a checking set.\nFig.1 and Fig.2 present a time series output and a prediction value for 500 and 10000 iterations respectively (a dark color stays for a time series value, a light color stays for a prediction value).\nSymmetric mean absolute percentage error (SMAPE) and mean square error (MSE), used for results evaluation, are shown in Fig.3 and Fig.4.\nFig.1 presents a time series output and a prediction value (a dark color stays for a time series value, a light color stays for a prediction value).\nFig.1. A visual representation of the predicted values (500 iterations)\nFig.2. A visual representation of the predicted values (10000 iterations)\nFig.3. Symmetric mean absolute percentage error (SMAPE)\nFig.4. Mean squared error (SME)\nBy the 10000th iteration the evolving system consists of 8-9 cascades and, as one can see in Fig.3 and Fig.4, the error curve is almost parallel to the Y-axis, which means the highest possible prediction accuracy, conditioned by the form of membership functions, is reached.\nVI. CONCLUSION\nThe new cascade system type of computational intelligence is proposed in the paper, in which nodes are multidimensional neo-fuzzy neurons that implement the multidimensional Takagi-Sugeno-Kang fuzzy reasoning. The adaptive learning algorithm for a multidimensional neo-fuzzy neuron is proposed that has both tracking and filtering properties. The distinguishing feature of the proposed system is that each cascade is formed by a set of neurons, and their outputs are combined with an optimization procedure of a special type. Thus, each cascade generates an optimal accuracy output signal. The proposed system which is essentially an evolving system of computational intelligence, makes it possible to process incoming data in an online mode unlike other traditional systems. This feature allows to process multidimensional non-stationary stochastic and chaotic signals under a priori and current uncertainty conditions as well as to provide the optimal forecast accuracy.\nREFERENCES\n[1] A. Cichocki, R. Unbehauen, Neural Networks for Optimization and Signal Processing. Stuttgart: Teubner, 1993. [2] S. Haykin, Neural Networks: A Comprehensive Foundation. Upper Saddle River, New Jersey: Prentice Hall, 1999. [3] N. Kasabov, Evolving Connectionist Systems. London: Springer-Verlag, 2003. [4] E. Lughofer, Evolving Fuzzy Systems – Methodologies, Advanced Concepts and Applications. Berlin-Heidelberg: Springer-\nVerlag, 2011. [5] S.E. Fahlman and C. Lebiere, “The cascade-correlation learning architecture,” in Advances Neural Information Processing\nSystems, San Mateo, CA: Morgan Kaufman, 1990, pp. 524-532. [6] L. Prechelt, “Investigation of the CasCor family of learning algorithms,” in Neural Networks, vol. 10, 1997, pp. 885-896. [7] R.J. Schalkoff, Artificial Neural Networks, New York: The McGraw-Hill Comp., 1997. [8] E.D. Avedjan, G.V. Bаrkаn and I.K. Lеvin, “Cascade neural networks,” in J. Avtomatika i Telemekhanika, vol. 3, 1999, pp.\n38-55. [9] Ye. Bodyanskiy, A. Dolotov, I. Pliss and Ye. Viktorov, “The cascaded orthogonal neural network,” in Int. J. Information\nScience and Computing, vol. 2, Sofia: FOI ITHEA, 2008, pp. 13-20. [10] Ye. Bodyanskiy and Ye. Viktorov, “The cascaded neo-fuzzy architecture and its on-line learning algorithm,” in Int. J.\nIntelligent Processing, vol. 9, Sofia: FOI ITHEA, 2009, pp. 110-116.\n[11] Ye. Bodyanskiy and Ye. Viktorov, “The cascaded neo-fuzzy architecture using cubic-spline activation functions,” in Int. J. Information Theories and Applications, vol. 16, no. 3, 2009, pp. 245-259. [12] Ye. Bodyanskiy, Ye. Viktorov and I. Pliss, “The cascade growing neural network using quadratic neurons and its learning algorithms for on-line information processing,” in Int. J. Intelligent Information and Engineering Systems, vol. 13, RzeszovSofia: FOI ITHEA, 2009, pp. 27-34. [13] V. Kolodyazhniy and Ye. Bodyanskiy, “Cascaded multi-resolution spline-based fuzzy neural network,” Proc. Int. Symp. on Evolving Intelligent Systems, pp. 26-29, 2010. [14] Ye. Bodyanskiy, O. Vynokurova and N. Teslenko, “Cascaded GMDH-wavelet-neuro-fuzzy network,” Proc 4th Int. Workshop on Inductive Modelling, pp. 22-30, 2011. [15] Ye. Bodyanskiy, O. Kharchenko and O. Vynokurova, “Hybrid cascaded neural network based on wavelet-neuron,” in Int. J. Information Theories and Applications, vol. 18, no. 4, 2011, pp. 35-343. [16] Ye. Bodyanskiy, P. Grimm and N. Teslenko, “Evolving cascaded neural network based on multidimensional Epanechnikov’s kernels and its learning algorithm,” in Int. J. Information Technologies and Knowledge, vol. 5, no. 1, 2011, pp. 25-30. [17] J-S.R. Jang, C.T. Sun and E. Mizutani, Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and Machine Intelligence, New Jersey: Prentice Hall, 1997. [18] S. Wadhawan, G. Goel and S. Kaushik, “Data Driven Fuzzy Modelling for Sugeno and Mamdani Type Fuzzy Model using Memetic Algorithm,” in Int. J. Information Technology and Computer Science, vol. 5, no. 8, 2013, pp. 24-37, doi: 10.5815/ijitcs.2013.08.03. [19] T. Yamakawa, E. Uchino, T. Miki and H. Kusanagi, “A neo fuzzy neuron and its applications to system identification and prediction of the system behavior,” Proc. 2nd Int. Conf. on Fuzzy Logic and Neural Networks, pp. 477-483, 1992. [20] E. Uchino and T. Yamakawa, “Soft computing based signal prediction, restoration and filtering,” in Intelligent Hybrid Systems: Fuzzy Logic, Neural Networks and Genetic Algorithms, Boston: Kluwer Academic Publisher, 1997, pp. 331-349. [21] T. Miki and T. Yamakawa, “Analog implementation of neo-fuzzy neuron and its on-board learning,” in Computational Intelligence and Applications, Piraeus: WSES Press, 1999, pp. 144-149. [22] M. Barman and J.P. Chaudhury, “A Framework for Selection of Membership Function Using Fuzzy Rule Base System for the Diagnosis of Heart Disease,” in Int. J. Information Technology and Computer Science, vol. 5, no.11, 2013, pp. 62-70, doi: 10.5815/ijitcs.2013.11.07. [23] B. Widrow and Jr.M.E. Hoff, “Adaptive switching circuits,” URE WESCON Convention Record, vol. 4, pp. 96-104, 1960. [24] S. Kaczmarz, “Approximate solution of systems of linear equations,” in Int. J. Control, vol. 53, 1993, pp. 1269-1271. [25] L. Ljung, System Identification: Theory for the User, New York: Prentice-Hall, 1999. [26] Ye. Bodyanskiy, I. Kokshenev and V. Kolodyazhniy, “An adaptive learning algorithm for a neo-fuzzy neuron,” Proc. 3rd\nInt. Conf. of European Union Soc. for Fuzzy Logic and Technology, pp. 375-379, 2003. [27] Ye. Bodyanskiy, O. Tyshchenko and D. Kopaliani, “Multidimensional non-stationary time-series prediction with the help of\nan adaptive neo-fuzzy model,” in Visnyk Nacionalnogo universytety “Lvivska politehnika”, vol. 744, 2012, pp. 312-320. [28] W.M. Caminhas, S.R. Silva, B. Rodrigues and R.P. Landim, “A neo-fuzzy-neuron with real time training applied to flux\nobserver for an induction motor,” Proceedings 5th Brazilian Symposium on Neural Networks, pp. 67-72, 1998. [29] Ye.V. Bodyanskiy, I.P. Pliss and T.V. Solovyova, “Multistep optimal predictors of multidimensional non-stationary\nstochastic processes,” in Doklady AN USSR, vol. A(12), 1986, pp. 47-49. [30] Ye.V. Bodyanskiy, I.P. Pliss and T.V. Solovyova, “Adaptive generalized forecasting of multidimensional stochastic\nsequences,” in Doklady AN USSR, vol. A(9), 1989, pp.73-75. [31] Ye. Bodyanskiy and I. Pliss, “Adaptive generalized forecasting of multivariate stochastic signals,” Proc. Latvian Sign.\nProc. Int. Conf., vol. 2, pp. 80-83, 1990."
    } ],
    "references" : [ {
      "title" : "Neural Networks for Optimization and Signal Processing",
      "author" : [ "A. Cichocki", "R. Unbehauen" ],
      "venue" : "Stuttgart: Teubner",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Neural Networks: A Comprehensive Foundation",
      "author" : [ "S. Haykin" ],
      "venue" : "Upper Saddle River, New Jersey: Prentice Hall",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Evolving Connectionist Systems",
      "author" : [ "N. Kasabov" ],
      "venue" : "London: Springer-Verlag",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Evolving Fuzzy Systems – Methodologies",
      "author" : [ "E. Lughofer" ],
      "venue" : "Advanced Concepts and Applications. Berlin-Heidelberg: Springer- Verlag",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The cascade-correlation learning architecture,",
      "author" : [ "S.E. Fahlman", "C. Lebiere" ],
      "venue" : "Advances Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1990
    }, {
      "title" : "Investigation of the CasCor family of learning algorithms,",
      "author" : [ "L. Prechelt" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Artificial Neural Networks",
      "author" : [ "R.J. Schalkoff" ],
      "venue" : "New York: The McGraw-Hill Comp.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Bаrkаn and I.K",
      "author" : [ "G.V.E.D. Avedjan" ],
      "venue" : "Lеvin, “Cascade neural networks,” in J. Avtomatika i Telemekhanika,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1999
    }, {
      "title" : "Viktorov, “The cascaded orthogonal neural network,",
      "author" : [ "Ye. Bodyanskiy", "A. Dolotov", "I. Pliss", "Ye" ],
      "venue" : "in Int. J. Information Science and Computing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Viktorov, “The cascaded neo-fuzzy architecture and its on-line learning algorithm,",
      "author" : [ "Ye. Bodyanskiy", "Ye" ],
      "venue" : "in Int. J. Intelligent Processing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Viktorov, “The cascaded neo-fuzzy architecture using cubic-spline activation functions,",
      "author" : [ "Ye. Bodyanskiy", "Ye" ],
      "venue" : "in Int. J. Information Theories and Applications,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "The cascade growing neural network using quadratic neurons and its learning algorithms for on-line information processing,",
      "author" : [ "Ye. Bodyanskiy", "Ye. Viktorov", "I. Pliss" ],
      "venue" : "in Int. J. Intelligent Information and Engineering Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Bodyanskiy, “Cascaded multi-resolution spline-based fuzzy neural network,",
      "author" : [ "V. Kolodyazhniy", "Ye" ],
      "venue" : "Proc. Int. Symp. on Evolving Intelligent Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Vynokurova, “Hybrid cascaded neural network based on wavelet-neuron,",
      "author" : [ "Ye. Bodyanskiy", "O.O. Kharchenko" ],
      "venue" : "in Int. J. Information Theories and Applications,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Evolving cascaded neural network based on multidimensional Epanechnikov’s kernels and its learning algorithm,",
      "author" : [ "Ye. Bodyanskiy", "P. Grimm", "N. Teslenko" ],
      "venue" : "in Int. J. Information Technologies and Knowledge,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and Machine Intelligence",
      "author" : [ "J-S.R. Jang", "C.T. Sun", "E. Mizutani" ],
      "venue" : "New Jersey: Prentice Hall",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "G",
      "author" : [ "S. Wadhawan" ],
      "venue" : "Goel and S. Kaushik, “Data Driven Fuzzy Modelling for Sugeno and Mamdani Type Fuzzy Model using Memetic Algorithm,” in Int. J. Information Technology and Computer Science, vol. 5, no. 8",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "T",
      "author" : [ "T. Yamakawa", "E. Uchino" ],
      "venue" : "Miki and H. Kusanagi, “A neo fuzzy neuron and its applications to system identification and prediction of the system behavior,” Proc. 2nd Int. Conf. on Fuzzy Logic and Neural Networks, pp. 477-483",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Soft computing based signal prediction",
      "author" : [ "E. Uchino", "T. Yamakawa" ],
      "venue" : "restoration and filtering,” in Intelligent Hybrid Systems: Fuzzy Logic, Neural Networks and Genetic Algorithms, Boston: Kluwer Academic Publisher",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Analog implementation of neo-fuzzy neuron and its on-board learning,",
      "author" : [ "T. Miki", "T. Yamakawa" ],
      "venue" : "Computational Intelligence and Applications, Piraeus: WSES Press,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1999
    }, {
      "title" : "A Framework for Selection of Membership Function Using Fuzzy Rule Base System for the Diagnosis of Heart Disease,",
      "author" : [ "M. Barman", "J.P. Chaudhury" ],
      "venue" : "in Int. J. Information Technology and Computer Science, vol. 5,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Adaptive switching circuits,",
      "author" : [ "B. Widrow", "Jr.M.E. Hoff" ],
      "venue" : "URE WESCON Convention Record,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1960
    }, {
      "title" : "Approximate solution of systems of linear equations,",
      "author" : [ "S. Kaczmarz" ],
      "venue" : "in Int. J. Control,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1993
    }, {
      "title" : "System Identification: Theory for the User",
      "author" : [ "L. Ljung" ],
      "venue" : "New York: Prentice-Hall",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "An adaptive learning algorithm for a neo-fuzzy neuron,",
      "author" : [ "Ye. Bodyanskiy", "I. Kokshenev", "V. Kolodyazhniy" ],
      "venue" : "Proc. 3rd Int. Conf. of European Union Soc. for Fuzzy Logic and Technology,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2003
    }, {
      "title" : "Kopaliani, “Multidimensional non-stationary time-series prediction with the help of an adaptive neo-fuzzy model,",
      "author" : [ "Ye. Bodyanskiy", "D.O. Tyshchenko" ],
      "venue" : "Visnyk Nacionalnogo universytety “Lvivska politehnika”,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "B",
      "author" : [ "W.M. Caminhas", "S.R. Silva" ],
      "venue" : "Rodrigues and R.P. Landim, “A neo-fuzzy-neuron with real time training applied to flux observer for an induction motor,” Proceedings 5th Brazilian Symposium on Neural Networks, pp. 67-72",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Solovyova, “Multistep optimal predictors of multidimensional non-stationary stochastic processes,",
      "author" : [ "Ye.V. Bodyanskiy", "T.V.I.P. Pliss" ],
      "venue" : "Doklady AN USSR, vol",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1986
    }, {
      "title" : "Solovyova, “Adaptive generalized forecasting of multidimensional stochastic sequences,",
      "author" : [ "Ye.V. Bodyanskiy", "T.V.I.P. Pliss" ],
      "venue" : "Doklady AN USSR, vol",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1989
    }, {
      "title" : "Adaptive generalized forecasting of multivariate stochastic signals,",
      "author" : [ "Ye. Bodyanskiy", "I. Pliss" ],
      "venue" : "Proc. Latvian Sign. Proc. Int. Conf.,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The learning process quality can be improved by adjusting a network topology along with its synaptic weights [1, 2].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "The learning process quality can be improved by adjusting a network topology along with its synaptic weights [1, 2].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "This idea is the foundation of evolving computational intelligence systems [3, 4].",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "This idea is the foundation of evolving computational intelligence systems [3, 4].",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "One of the most successful implementations of this approach is cascadecorrelation neural networks [5–8] due to their high degree of efficiency and learning simplicity of both synaptic weights and a network topology).",
      "startOffset" : 98,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "One of the most successful implementations of this approach is cascadecorrelation neural networks [5–8] due to their high degree of efficiency and learning simplicity of both synaptic weights and a network topology).",
      "startOffset" : 98,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "One of the most successful implementations of this approach is cascadecorrelation neural networks [5–8] due to their high degree of efficiency and learning simplicity of both synaptic weights and a network topology).",
      "startOffset" : 98,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "One of the most successful implementations of this approach is cascadecorrelation neural networks [5–8] due to their high degree of efficiency and learning simplicity of both synaptic weights and a network topology).",
      "startOffset" : 98,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "In order to avoid multi-epoch learning [9–16], different types of neurons (with outputs that depend linearly on synaptic weights) should be used as network nodes.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "In order to avoid multi-epoch learning [9–16], different types of neurons (with outputs that depend linearly on synaptic weights) should be used as network nodes.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "In order to avoid multi-epoch learning [9–16], different types of neurons (with outputs that depend linearly on synaptic weights) should be used as network nodes.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "In order to avoid multi-epoch learning [9–16], different types of neurons (with outputs that depend linearly on synaptic weights) should be used as network nodes.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "In order to avoid multi-epoch learning [9–16], different types of neurons (with outputs that depend linearly on synaptic weights) should be used as network nodes.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : "In order to avoid multi-epoch learning [9–16], different types of neurons (with outputs that depend linearly on synaptic weights) should be used as network nodes.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "In order to avoid multi-epoch learning [9–16], different types of neurons (with outputs that depend linearly on synaptic weights) should be used as network nodes.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "While the input of the neurons in the first cascade is   x k , neurons in the second cascade have g additional inputs for the generated signal   *[1] ŷ k , neurons in the third cascade have 2g additional inputs     *[1] *[2] ˆ ˆ , y k y k , neurons in the  m th cascade have   1  m g additional inputs       *[1] *[2] *[ 1] ˆ ˆ ˆ , , ,   m y k y k y k .",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "While the input of the neurons in the first cascade is   x k , neurons in the second cascade have g additional inputs for the generated signal   *[1] ŷ k , neurons in the third cascade have 2g additional inputs     *[1] *[2] ˆ ˆ , y k y k , neurons in the  m th cascade have   1  m g additional inputs       *[1] *[2] *[ 1] ˆ ˆ ˆ , , ,   m y k y k y k .",
      "startOffset" : 224,
      "endOffset" : 227
    }, {
      "referenceID" : 1,
      "context" : "While the input of the neurons in the first cascade is   x k , neurons in the second cascade have g additional inputs for the generated signal   *[1] ŷ k , neurons in the third cascade have 2g additional inputs     *[1] *[2] ˆ ˆ , y k y k , neurons in the  m th cascade have   1  m g additional inputs       *[1] *[2] *[ 1] ˆ ˆ ˆ , , ,   m y k y k y k .",
      "startOffset" : 229,
      "endOffset" : 232
    }, {
      "referenceID" : 0,
      "context" : "While the input of the neurons in the first cascade is   x k , neurons in the second cascade have g additional inputs for the generated signal   *[1] ŷ k , neurons in the third cascade have 2g additional inputs     *[1] *[2] ˆ ˆ , y k y k , neurons in the  m th cascade have   1  m g additional inputs       *[1] *[2] *[ 1] ˆ ˆ ˆ , , ,   m y k y k y k .",
      "startOffset" : 327,
      "endOffset" : 330
    }, {
      "referenceID" : 1,
      "context" : "While the input of the neurons in the first cascade is   x k , neurons in the second cascade have g additional inputs for the generated signal   *[1] ŷ k , neurons in the third cascade have 2g additional inputs     *[1] *[2] ˆ ˆ , y k y k , neurons in the  m th cascade have   1  m g additional inputs       *[1] *[2] *[ 1] ˆ ˆ ˆ , , ,   m y k y k y k .",
      "startOffset" : 332,
      "endOffset" : 335
    }, {
      "referenceID" : 0,
      "context" : "While the input of the neurons in the first cascade is   x k , neurons in the second cascade have g additional inputs for the generated signal   *[1] ŷ k , neurons in the third cascade have 2g additional inputs     *[1] *[2] ˆ ˆ , y k y k , neurons in the  m th cascade have   1  m g additional inputs       *[1] *[2] *[ 1] ˆ ˆ ˆ , , ,   m y k y k y k .",
      "startOffset" : 337,
      "endOffset" : 341
    }, {
      "referenceID" : 15,
      "context" : "High interpretability and transparency along with good approximation capabilities and ability to learn are the main features of neuro-fuzzy systems [17, 18], which are the foundation of hybrid computational intelligence systems.",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : "High interpretability and transparency along with good approximation capabilities and ability to learn are the main features of neuro-fuzzy systems [17, 18], which are the foundation of hybrid computational intelligence systems.",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : "In [10,11,13] hybrid cascade systems were introduced which used neo-fuzzy neurons [19-21] as network nodes, allowing one to significantly increase the speed of synaptic weight adjustment.",
      "startOffset" : 3,
      "endOffset" : 13
    }, {
      "referenceID" : 10,
      "context" : "In [10,11,13] hybrid cascade systems were introduced which used neo-fuzzy neurons [19-21] as network nodes, allowing one to significantly increase the speed of synaptic weight adjustment.",
      "startOffset" : 3,
      "endOffset" : 13
    }, {
      "referenceID" : 12,
      "context" : "In [10,11,13] hybrid cascade systems were introduced which used neo-fuzzy neurons [19-21] as network nodes, allowing one to significantly increase the speed of synaptic weight adjustment.",
      "startOffset" : 3,
      "endOffset" : 13
    }, {
      "referenceID" : 17,
      "context" : "In [10,11,13] hybrid cascade systems were introduced which used neo-fuzzy neurons [19-21] as network nodes, allowing one to significantly increase the speed of synaptic weight adjustment.",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "In [10,11,13] hybrid cascade systems were introduced which used neo-fuzzy neurons [19-21] as network nodes, allowing one to significantly increase the speed of synaptic weight adjustment.",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : "In [10,11,13] hybrid cascade systems were introduced which used neo-fuzzy neurons [19-21] as network nodes, allowing one to significantly increase the speed of synaptic weight adjustment.",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "In this way i NS implements the fuzzy inference [18]",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "              [1] [1] [1] [1] 1 1 1 [1] ˆ ,",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "              [1] [1] [1] [1] 1 1 1 [1] ˆ ,",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "              [1] [1] [1] [1] 1 1 1 [1] ˆ ,",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "              [1] [1] [1] [1] 1 1 1 [1] ˆ ,",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "              [1] [1] [1] [1] 1 1 1 [1] ˆ ,",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "Authors of the neo-fuzzy neuron [19–21] used traditional triangular constructions meeting the conditions of Ruspini partitioning (unity partitioning) as membership functions [22]:",
      "startOffset" : 32,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "Authors of the neo-fuzzy neuron [19–21] used traditional triangular constructions meeting the conditions of Ruspini partitioning (unity partitioning) as membership functions [22]:",
      "startOffset" : 32,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "Authors of the neo-fuzzy neuron [19–21] used traditional triangular constructions meeting the conditions of Ruspini partitioning (unity partitioning) as membership functions [22]:",
      "startOffset" : 32,
      "endOffset" : 39
    }, {
      "referenceID" : 20,
      "context" : "Authors of the neo-fuzzy neuron [19–21] used traditional triangular constructions meeting the conditions of Ruspini partitioning (unity partitioning) as membership functions [22]:",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "  [1] , 1, [1] [1] , 1, [1] [1] , 1, [1] , 1, [1] [1] [1] , 1, [1] [1] , 1, , , ,",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "where [1] j dli c are relatively arbitrarily chosen (usually evenly distributed) center parameters of membership functions over the interval   0,1 where, naturally, 0 1   i x .",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "This choice of membership functions ensures that the input signal i x only activates two neighboring membership functions, and their sum is always equal to 1 which means that     [1] [1] , 1, 1    j j i i dli d l i x x   (6) and       [1] [1] [1] [1] [1] , 1, , 1, .",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 0,
      "context" : "This choice of membership functions ensures that the input signal i x only activates two neighboring membership functions, and their sum is always equal to 1 which means that     [1] [1] , 1, 1    j j i i dli d l i x x   (6) and       [1] [1] [1] [1] [1] , 1, , 1, .",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 0,
      "context" : "This choice of membership functions ensures that the input signal i x only activates two neighboring membership functions, and their sum is always equal to 1 which means that     [1] [1] , 1, 1    j j i i dli d l i x x   (6) and       [1] [1] [1] [1] [1] , 1, , 1, .",
      "startOffset" : 250,
      "endOffset" : 253
    }, {
      "referenceID" : 0,
      "context" : "This choice of membership functions ensures that the input signal i x only activates two neighboring membership functions, and their sum is always equal to 1 which means that     [1] [1] , 1, 1    j j i i dli d l i x x   (6) and       [1] [1] [1] [1] [1] , 1, , 1, .",
      "startOffset" : 254,
      "endOffset" : 257
    }, {
      "referenceID" : 0,
      "context" : "This choice of membership functions ensures that the input signal i x only activates two neighboring membership functions, and their sum is always equal to 1 which means that     [1] [1] , 1, 1    j j i i dli d l i x x   (6) and       [1] [1] [1] [1] [1] , 1, , 1, .",
      "startOffset" : 258,
      "endOffset" : 261
    }, {
      "referenceID" : 0,
      "context" : "This choice of membership functions ensures that the input signal i x only activates two neighboring membership functions, and their sum is always equal to 1 which means that     [1] [1] , 1, 1    j j i i dli d l i x x   (6) and       [1] [1] [1] [1] [1] , 1, , 1, .",
      "startOffset" : 262,
      "endOffset" : 265
    }, {
      "referenceID" : 0,
      "context" : "This choice of membership functions ensures that the input signal i x only activates two neighboring membership functions, and their sum is always equal to 1 which means that     [1] [1] , 1, 1    j j i i dli d l i x x   (6) and       [1] [1] [1] [1] [1] , 1, , 1, .",
      "startOffset" : 266,
      "endOffset" : 269
    }, {
      "referenceID" : 1,
      "context" : "  [2] [2] [2] 1 1 [2] [2] *[1] , 1 , 1 1 1 ˆ",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "  [2] [2] [2] 1 1 [2] [2] *[1] , 1 , 1 1 1 ˆ",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "  [2] [2] [2] 1 1 [2] [2] *[1] , 1 , 1 1 1 ˆ",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 1,
      "context" : "  [2] [2] [2] 1 1 [2] [2] *[1] , 1 , 1 1 1 ˆ",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "  [2] [2] [2] 1 1 [2] [2] *[1] , 1 , 1 1 1 ˆ",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "  [2] [2] [2] 1 1 [2] [2] *[1] , 1 , 1 1 1 ˆ",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "                                         [ ] [ ] [ ] [ ] [ ] [ ] 1 1 2 2 11 1 12 2 [ ] [ ] [ ] [ ] *[1] *[1] *[ 1] 1 1, 1 , , 1 , , , , , , , , ,",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "                                         [ ] [ ] [ ] [ ] [ ] [ ] 1 1 2 2 11 1 12 2 [ ] [ ] [ ] [ ] *[1] *[1] *[ 1] 1 1, 1 , , 1 , , , , , , , , ,",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "                                         [ ] [ ] [ ] [ ] [ ] [ ] 1 1 2 2 11 1 12 2 [ ] [ ] [ ] [ ] *[1] *[1] *[ 1] 1 1, 1 , , 1 , , , , , , , , ,",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 21,
      "context" : " T m j m j m j d d d y k w k  (10) Since this signal is linearly dependent on the synaptic weights, any adaptive identification algorithm [23-25] can be used for training the network neo-fuzzy-neurons, for example, the exponentially-weighted least-squares method in a recurrent form",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : " T m j m j m j d d d y k w k  (10) Since this signal is linearly dependent on the synaptic weights, any adaptive identification algorithm [23-25] can be used for training the network neo-fuzzy-neurons, for example, the exponentially-weighted least-squares method in a recurrent form",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : " T m j m j m j d d d y k w k  (10) Since this signal is linearly dependent on the synaptic weights, any adaptive identification algorithm [23-25] can be used for training the network neo-fuzzy-neurons, for example, the exponentially-weighted least-squares method in a recurrent form",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 24,
      "context" : "      (here   1 , 1,2, ,     d y k d g an external learning signal, 0 1     a forgetting factor) or the gradient learning algorithm with both tracking and filtering properties [26]                            1 [ ] [ ] [ ]",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 0,
      "context" : "                           An architecture of a typical neo-fuzzy neuron which was discussed earlier as a part of the multidimensional neuron [1] g MN of the cascade system is redundant, since a vector of input signals   x k (the first cascade) is fed to one-type nonlinear synapses [1] j di NS of the neo-fuzzy neurons, each neuron of which generates at its output a signal   [1] ˆ , 1, 2, ,   j d y k d g .",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 0,
      "context" : "                           An architecture of a typical neo-fuzzy neuron which was discussed earlier as a part of the multidimensional neuron [1] g MN of the cascade system is redundant, since a vector of input signals   x k (the first cascade) is fed to one-type nonlinear synapses [1] j di NS of the neo-fuzzy neurons, each neuron of which generates at its output a signal   [1] ˆ , 1, 2, ,   j d y k d g .",
      "startOffset" : 313,
      "endOffset" : 316
    }, {
      "referenceID" : 0,
      "context" : "                           An architecture of a typical neo-fuzzy neuron which was discussed earlier as a part of the multidimensional neuron [1] g MN of the cascade system is redundant, since a vector of input signals   x k (the first cascade) is fed to one-type nonlinear synapses [1] j di NS of the neo-fuzzy neurons, each neuron of which generates at its output a signal   [1] ˆ , 1, 2, ,   j d y k d g .",
      "startOffset" : 409,
      "endOffset" : 412
    }, {
      "referenceID" : 0,
      "context" : "As a result, the output vector components           [1] [1] [1] [1] 1 2 ˆ ˆ ˆ ˆ , , ,   T j j j j g y k y k y k y k are calculated independently.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "As a result, the output vector components           [1] [1] [1] [1] 1 2 ˆ ˆ ˆ ˆ , , ,   T j j j j g y k y k y k y k are calculated independently.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "As a result, the output vector components           [1] [1] [1] [1] 1 2 ˆ ˆ ˆ ˆ , , ,   T j j j j g y k y k y k y k are calculated independently.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "As a result, the output vector components           [1] [1] [1] [1] 1 2 ˆ ˆ ˆ ˆ , , ,   T j j j j g y k y k y k y k are calculated independently.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : "This can be avoided by considering a multidimensional neo-fuzzy neuron [27], which is a modification of the system proposed in [28].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 26,
      "context" : "This can be avoided by considering a multidimensional neo-fuzzy neuron [27], which is a modification of the system proposed in [28].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "Structure nodes are composite nonlinear synapses [1] j i MNS , each synapse contains h membership functions [1] j li  and gh adjustable synaptic weights [1] j dli w .",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "Structure nodes are composite nonlinear synapses [1] j i MNS , each synapse contains h membership functions [1] j li  and gh adjustable synaptic weights [1] j dli w .",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "Structure nodes are composite nonlinear synapses [1] j i MNS , each synapse contains h membership functions [1] j li  and gh adjustable synaptic weights [1] j dli w .",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "Taking into consideration a   1   hn membership functions’ vector                        [1] [1] [1] [1] 1 1 1 11 21 1 [1] [1] , , , ,",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "Taking into consideration a   1   hn membership functions’ vector                        [1] [1] [1] [1] 1 1 1 11 21 1 [1] [1] , , , ,",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "Taking into consideration a   1   hn membership functions’ vector                        [1] [1] [1] [1] 1 1 1 11 21 1 [1] [1] , , , ,",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "Taking into consideration a   1   hn membership functions’ vector                        [1] [1] [1] [1] 1 1 1 11 21 1 [1] [1] , , , ,",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "Taking into consideration a   1   hn membership functions’ vector                        [1] [1] [1] [1] 1 1 1 11 21 1 [1] [1] , , , ,",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "Taking into consideration a   1   hn membership functions’ vector                        [1] [1] [1] [1] 1 1 1 11 21 1 [1] [1] , , , ,",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1] [1] 211 212 2 2",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1] [1] 211 212 2 2",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1] [1] 211 212 2 2",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1] [1] 211 212 2 2",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1] [1] 211 212 2 2",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1] [1] 211 212 2 2",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1] [1] 211 212 2 2",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1] [1] 211 212 2 2",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 111 112 1 1 [1] [1] [1] [1] [1] 211 212 2 2",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 11 12                              j j j j li hn j j j j j li hn",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 11 12                              j j j j li hn j j j j j li hn",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 11 12                              j j j j li hn j j j j j li hn",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1] [1] 11 12                              j j j j li hn j j j j j li hn",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "the output signal [1] j MN can be written down at the  k th time moment in the form of     [1] [1] [1] ˆ .",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "the output signal [1] j MN can be written down at the  k th time moment in the form of     [1] [1] [1] ˆ .",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "the output signal [1] j MN can be written down at the  k th time moment in the form of     [1] [1] [1] ˆ .",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "the output signal [1] j MN can be written down at the  k th time moment in the form of     [1] [1] [1] ˆ .",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : " j j j y k W k  (13) Multidimensional neo-fuzzy-neuron learning can be implemented with the help of a matrix modification of the exponentially-weighted recurrent least squares method (11) in the form of                                                     [1] [1] [1] [1]",
      "startOffset" : 313,
      "endOffset" : 316
    }, {
      "referenceID" : 0,
      "context" : " j j j y k W k  (13) Multidimensional neo-fuzzy-neuron learning can be implemented with the help of a matrix modification of the exponentially-weighted recurrent least squares method (11) in the form of                                                     [1] [1] [1] [1]",
      "startOffset" : 317,
      "endOffset" : 320
    }, {
      "referenceID" : 0,
      "context" : " j j j y k W k  (13) Multidimensional neo-fuzzy-neuron learning can be implemented with the help of a matrix modification of the exponentially-weighted recurrent least squares method (11) in the form of                                                     [1] [1] [1] [1]",
      "startOffset" : 321,
      "endOffset" : 324
    }, {
      "referenceID" : 0,
      "context" : " j j j y k W k  (13) Multidimensional neo-fuzzy-neuron learning can be implemented with the help of a matrix modification of the exponentially-weighted recurrent least squares method (11) in the form of                                                     [1] [1] [1] [1]",
      "startOffset" : 325,
      "endOffset" : 328
    }, {
      "referenceID" : 0,
      "context" : "1 [1] [1] [1]",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 0,
      "context" : "1 [1] [1] [1]",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "1 [1] [1] [1]",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "[1] [1]",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] [1]",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1]",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1]",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1]",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "1 [1] [1] [1] [1]",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 0,
      "context" : "1 [1] [1] [1] [1]",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "1 [1] [1] [1] [1]",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "1 [1] [1] [1] [1]",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] 1 1",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] 1 1",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 27,
      "context" : "                                                 or in the form of a multidimensional version of the algorithm (12) [29]:                            1 [1] [1] [1]",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "                                                 or in the form of a multidimensional version of the algorithm (12) [29]:                            1 [1] [1] [1]",
      "startOffset" : 233,
      "endOffset" : 236
    }, {
      "referenceID" : 0,
      "context" : "                                                 or in the form of a multidimensional version of the algorithm (12) [29]:                            1 [1] [1] [1]",
      "startOffset" : 237,
      "endOffset" : 240
    }, {
      "referenceID" : 0,
      "context" : "                                                 or in the form of a multidimensional version of the algorithm (12) [29]:                            1 [1] [1] [1]",
      "startOffset" : 241,
      "endOffset" : 244
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1]",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1]",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "[1] [1] [1]",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "2 [1] [1] [1] 1 1 1",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 0,
      "context" : "2 [1] [1] [1] 1 1 1",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "2 [1] [1] [1] 1 1 1",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 28,
      "context" : "This problem can be solved by using undetermined Lagrange multipliers and adaptive multidimensional generalized prediction [30, 31].",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 29,
      "context" : "This problem can be solved by using undetermined Lagrange multipliers and adaptive multidimensional generalized prediction [30, 31].",
      "startOffset" : 123,
      "endOffset" : 131
    } ],
    "year" : 2016,
    "abstractText" : "A new architecture and learning algorithms for the multidimensional hybrid cascade neural network with neuron pool optimization in each cascade are proposed in this paper. The proposed system differs from the well-known cascade systems in its capability to process multidimensional time series in an online mode, which makes it possible to process non-stationary stochastic and chaotic signals with the required accuracy. Compared to conventional analogs, the proposed system provides computational simplicity and possesses both tracking and filtering capabilities.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}