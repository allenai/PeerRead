{
  "name" : "1708.07280.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Generalized Reactive Policies using Deep Neural Networks",
    "authors" : [ "Edward Groshev" ],
    "emails" : [ "eddiegroshev@berkeley.edu", "avivt@berkeley.edu", "siddharths@asu.edu", "pabbeel@cs.berkeley.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In order to help with day to day chores such as organizing a cabinet or arranging a dinner table, robots need to be able plan: to reason about the best course of action that could lead to a given objective. Unfortunately, planning is known to be a challenging computation problem. The plan existence problem for deterministic, fully observable environments is PSPACE-complete when expressed using rudimentary propositional representations [2]. Such results have inspired the learning for planning paradigm: learning, or reusing the knowledge acquired while planning across multiple problem instances (in the form of triangle tables [6], learning control knowledge for planning [31], and constructing generalized plans [25], among other approaches) with the goal of faster plan computation in a new problem instance.\n∗Some of the work was done while this author was at United Technologies Research Center\nar X\niv :1\n70 8.\n07 28\n0v 1\nOne challenge in learning for planning, however, is how to select a good representation for the learning problem, and prior approaches (e.g., [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].\nRecently, deep neural networks (DNNs) have been used to automatically extract expressive features from data, leading to state-of-the-art learning results in image classification [13], natural language processing [26], and control [16], among other domains. The phenomenal success of DNNs across various disciplines motivates us to investigate whether DNNs can learn useful representations in the learning for planning setting as well.\nIn this work, we present an imitation learning (IL) approach for learning a generalized reactive policy (GRP) – a policy that mimics a planner, by drawing upon past plan executions for similar problems. Our learned GRP captures a reactive policy in the form of a DNN that predicts the action to be taken under any situation, given an observation of the planning domain and the current state. Our approach can also be used to automatically generate heuristic functions for a given domain, to be used in arbitrary directed search algorithms such as A∗ [22].\nImitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4]. In this work, we investigate whether IL can be used to learn tasks that require longer-horizon reasoning, such as demonstrated by state-of-the-art planners.\nFor the purpose of this paper, we restrict our attention to planning problems for which plan execution can be accurately captured as a sequence of images. This category captures a number of problems of interest in household robotics including setting the dinner table. In particular, we focus on the Sokoban domain (see Figure 1), which has been described as the most challenging problem in the literature on learning for planning [5]: “ Sokoban has been demonstrated to be a very challenging domain for AI search and planning algorithms, even when significant human domain knowledge is provided [10]. This domain is more complex than the others.”2\nOur experiments reveal that several architectural components are required to make IL work efficiently in learning for planning:\n1. A deep network. We attribute this finding to the network having to learn some form of planning computation, which cannot be captured with a shallow architecture.\n2. Structuring the network to receive as input pairs of current state and goal observations. This allows us to ‘bootstrap’ the data, by training with all pairs of states in a demonstration trajectory.\n3. Predicting plan length as an auxiliary training signal can improve IL performance. In addition, the plan length can be effectively exploited as a heuristic by standard planners.\nWe believe that these observations are general, and will hold for many domains. For the particular case of Sokoban, using these insights, we were able to demonstrate a 97% success rate in one object domains, and an 87% success rate in two object domains. In Figure 1 we show an example test domain, and a non-trivial solution produced by our learned DNN.\n2This quote is in reference to the general Sokoban domain, containing multiple objects and goals.\nThe performance of reactive policies learned by our method for a given domain is consistent across a number of test problems generated at random from the same domain. We also show that the learned heuristic function significantly improves upon standard hand-designed heuristics for Sokoban."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "The interface of planning and learning has been investigated extensively in the past.\nWithin the learning for planning literature [5], several studies considered learning a reactive policy, which is similar to our imitation learning approach. The works of Khadron [11], Martin and Geffner [15], and Yoon et al. [30] learn policies represented as decision lists on the logical problem representation, which needs to be hand specified. Our approach requires as input only a set of successful plans and their executions—our neural network architecture is able to learn a reactive policy that predicts the best action to execute based on an image of the current state of the environment without any additional representational expressions. Our approach thus offers two major advantages over prior efforts: (1) in situations where successful plan executions can be observed, e.g. by observing humans solving problems, our approach can eliminate the effort required in designing domain representations; (2) in situations where guarantees of success are required, and domain representations are available, our approach can automatically generate a representation-independent heuristic function which can be used with arbitrary directed search algorithms.\nWorks in imitation learning have mainly focused on learning skills from human demonstrations, such as driving and obstacle avoidance [20, 21], and robot table tennis and rope manipulation [17, 18]. Pfeiffer et al. [19] recently applied IL to learning obstacle avoidance from a motion planner. These skills do not require learning a planning computation, in the sense that the difference between the train and test environments is mostly in the observation (e.g., the visual driving conditions), but not in the task goal (e.g., stay on the lane and avoid obstacles). Indeed, the recent work of Tamar et al. [27] demonstrated the difficulty of generalizing goal directed behavior from demonstration. The recent work of Duan et al. [4] showed learning of various block stacking tasks where the goal was specified by an additional execution trace. From a planning perspective, the Sokoban domain considered here is considerably more challenging than block stacking or navigation between obstacles. The ‘one-shot’ techniques in [4], however, are complimentary to this work. The impressive Alpha-Go [23] program learned a DNN strategy for Go, using a combination of IL, and reinforcement learning through self-play. Extending our work to reinforcement learning is a direction for future research.\nRecently, several authors considered DNN architectures that are suitable for learning a planning computation. In [27], a value iteration planning computation was embedded within the network structure, and demonstrated successful learning on 2D gridworld navigation. Due to the curse of dimensionality, it is not clear how to extend that work to planning domains with much larger state spaces, such as the Sokoban domain considered here. The predictron architecture [24] uses ideas from temporal difference learning to design a DNN for reward prediction. Their architecture uses a recurrent network to internally simulate the future state transitions and predict future rewards. At present, it is also not clear how to use the predictron for learning a policy, as we do here. Concurrently with our work, Weber et at. [29] proposed a DNN architecture that combines model based planning with model free components for reinforcement learning, and demonstrated results on the Sokoban domain. In comparison, our IL approach requires significantly less training instances of the planning problem (over 3 orders of magnitude) to achieve similar performance in Sokoban."
    }, {
      "heading" : "2 Background",
      "text" : "In this section we present our formulation and preliminaries.\nPlanning: We focus on fully observable, deterministic task planning problems described in the formal language PDDL [7]. Such planning problems are defined as a tuple Π = 〈E ,F , I, G,A〉, with the following definitions:\nE : a set of entities in the domain (e.g. individual objects or locations). F : a set of binary fluents that describe relations between entities: ObjAt(obj3, loc1), InGrip-\nper(obj2), etc. I ∈ 2F : a conjunction of fluents that are initially true. G ∈ 2F : a conjunction of fluents that characterizes the set of goal states.\nA: a set of actions that describe the ways in which an agent can alter the world. Each action is characterized by: preconditions, a set of fluents that describes the set of states where the action is applicable and effects, a set fluents that change after the action is carried out.\nFor clarity, we will describe both preconditions and effects as a conjunctive lists of fluents. As an example, the discrete move action could be represented as follows:\nMove(loc1, loc2) : { pre : RobotAt(loc1),\neff : ¬RobotAt(loc1), RobotAt(loc2).\nWe introduce several additional notations to the planning problem, to make the connection with imitation learning clearer. We denote by S = 2F the state space of the planning problem. A state s ∈ S corresponds to the values of each fluent in F . The initial state s0 is defined by I , and a goal state sg is defined by G. The task in planning is to find a sequence of actions – the so called plan – that, when consecutively applied to the initial state, results in a goal state. In addition, we denote by o(Π, s) the observation for a problem Π when the state is s. For example, o can be an image of the current game state, as depicted in Figure 1 for Sokoban. We let τ = {s0, o0, a0, s1, . . . , sg, og} denote the state-observation-action trajectory implied by the plan. The plan length is the number of states in τ .\nLearning for Planning: In the learning for planning setting, we are given a set Dtrain of Ntrain problem instances {Π1, . . . ,ΠNtrain}, which will be used for learning a model that can improve and/or replace a planner, and a set Dtest of Ntest problem instances that will be used for evaluating the learned model. We assume that the training and test domains are similar in some sense, so that relevant knowledge can be extracted from the training set to improve performance on the test set. Concretely, both training and test domain instances come from the same distribution.\nImitation Learning: In imitation learning (IL), demonstrations of an expert performing a task are given in the form of observation-action trajectories Dimitation = {o0, a0, o1, . . . , oT , aT }. The goal is to find a policy – a mapping from observation to actions a = µ(o), which imitates the expert. A straightforward IL approach is behavioral cloning [20], in which standard supervised learning is used to learn µ from the data."
    }, {
      "heading" : "3 Imitation Learning in Learning for Planning",
      "text" : "We define a generalized reactive policy (GRP) as a function that maps <problem instance, state> to action, thus generalizing the concept of a policy. In this work, similar to [27], we assume that the problem instance and state are given as an image observation. Such a representation is suitable for many robotic problems3.\nIn this section we describe our approach for learning GRPs. Our approach is comprised of two stages: a data generation stage and a policy training stage.\nData generation: given the training domains Dtrain we generate a data set for imitation learning Dimitation. For each Π ∈ Dtrain, we run an off-the-shelf planner to generate a plan and corresponding trajectory τ , and then add the observations and actions in τ to Dimitation. In our experiments we used the Fast-Forward (FF) planner [9], though any other PDDL planner can be used instead.\nPolicy training: Given the generated data Dimitation, we use IL to learn a policy µ. The learned policy µ maps an observation to action, and therefore can be readily deployed to any test problem in Dtest. If the observation o(Π, s) contains sufficient information about Π and s, then potentially, the policy µ can represent the decision making of the planning algorithm used to generate the data. Moreover, if there is a shared structure between the domains in Dtrain, such as subgoals, or simple decision rules in certain situations, a good learning algorithm has the potential to learn the shared structure.\nOne may wonder why such a naive approach would even learn to produce the complex decision making ability that is required to solve unseen instances in Dtest. Indeed, as we show in our experiments, naive behavioral cloning with standard shallow neural networks fails on this task. One\n3It is also possible to extend our work to graph representations using convolutions on graphs [27, 3]. We defer this to future work.\nof the contributions of this work is the investigation of DNN representations that make this simple approach succeed."
    }, {
      "heading" : "4 Network Architecture",
      "text" : "In this section, we present our DNN architecture for learning a GRP. In particular, we propose two design choices that aid in learning long-horizon planning behavior."
    }, {
      "heading" : "4.1 Goal Based Policy for Data Bootstrapping",
      "text" : "In the IL literature (e.g., [20, 21]), the policy is typically structured as a mapping from observation to action a = µ(o). In order for this policy to represent goal directed behavior, as in the planning domains we consider, the observation o at each state must contain information about the goal state sg .\nRecall that our training data Dimitation consists of Ntrain trajectories composed of observation-action pairs. This means that the number of training samples for a policy a = µ(o) is equal to the number of observation-action pairs in the training data.\nWe propose instead, to structure the policy as a mapping from both a current observation and goal observation to the current action a = µ(o, og). We term such a policy structure a goal based policy. Our reasoning for such a structure is based on the following fact:\nProposition 1. For a planning problem Π with initial state s0 and goal state sg, let τopt = {s0, s1, . . . , sg} denote the shortest plan from s0 to sg. Let µopt(s) denote an optimal policy for Π in the sense that executing it from s0 generates the shortest path τopt to sg . Then, µopt is also optimal for a problem Π with the initial and goal states replaced with any two states si, sj ∈ τopt such that i < j.\nProposition 1 underlies classical planning methods such as triangle tables [6]. Here, we exploit it to design our DNN based on the following observation: if we structure the DNN to take as input both the current observation and a goal observation, for each observation-action trajectory in our data Dimitation, any pair of successive observations oi, oj can be used as a sample for training the policy. We term this bootstrapping the data. For a given trajectory of length T , the bootstrap can potentially increase the number of training samples from T to (T − 1)2/2. In practice, for each trajectory τ ∈ Dimitation, we uniformly sample nbootstrap pairs of observations from τ . In each pair, the first observation is treated as the current observation, while the last observation is treated as the goal observation. This results in nbootstrap + T training samples for each trajectory τ , which are added to a bootstrap training set Dbootstrap to be used instead of Dimitation for training the policy. 4"
    }, {
      "heading" : "4.2 Network Structure",
      "text" : "We propose a general structure for a network that can learn a GRP from visual execution traces.\nOur network is depicted in Figure 2. The current state and goal state observations are passed through several layers of convolution which are shared between the action prediction network and the plan length prediction network. There are also skip connections from the input layer to to every convolution layer.\nThe shared representation is motivated by the fact that both the actions and the overall plan length are integral parts of a plan. Having knowledge of the actions makes it easy to determine plan length and vice versa, knowledge about the plan length can act as a template for determining the actions. The skip connections are motivated by the fact that several planning algorithms can be seen as applying a repeated computation, based on the planning domain, to a latent variable. For example, greedy search expands the current node based on the possible next states, which are encoded in the domain; value iteration is a repeated modification of the value given the reward and state transitions, which are also encoded in the domain. Since the network receive no other knowledge about the domain, other than what’s present in the observation, we hypothesize that feeding the observation to every conv-net layer can facilitate the learning of similar planning computations. We note that in value iteration networks [27], similar skip connections were used in an explicit neural network implementation of value iteration.\n4Note that for the Sokoban domain, goal observations in the test set (i.e., real goals) do not contain the robot position, while the goal observations in the bootstrap training set include the robot position. However, this inconsistency had no effect in practice, which we verified by explicitly removing the robot from the observation."
    }, {
      "heading" : "4.3 Generalization to Different Problem Sizes",
      "text" : "A primary challenge in learning for planning is finding representations that can generalize across different problem sizes. For example, we expect that a good policy for Sokoban should work well on the instances it was trained on, 9× 9 domains for example, as well as on larger instances, such as 12× 12 domains. While the convolution layers can be applied to any image size, the number of inputs to the fully connected layer is strictly tied to the image size. This means that the network architecture described above is fixed to a particular domain size. To remove this dependency, we employ a trick used in fully convolutional networks [14], and keep only a k × k window of the last convolution layer, centered around the current agent position. This modification makes our DNN applicable to any domain size."
    }, {
      "heading" : "5 Experiments",
      "text" : "Here we report our experiments on learning for planning with DNNs. Our goal is to answer the following questions:\n1. What makes a good DNN architecture for learning planning behavior?\n2. Is the DNN plan length prediction a useful planning heuristic?\n3. Can DNN-based policies and heuristics generalize to changes in the size of domain?\nWe consider the Sokoban domain, as described on Figure 1, with a 9×9 grid and two difficulty levels: moving a single object, and a harder task of moving two objects. We generated training data using a random level generator5 for Sokoban. Note that in Sokoban, the last observation in a trajectory contains the agent’s final position, which reveals information about the plan. Since our networks take a goal observation as input, we added an additional observation at the end of each trajectory with the agent removed, such that we can feed in a goal observation without revealing additional information.\nFor imitation learning, we represent the policy with the DNNs described in Section 4 and optimize using Adam [12] (step size 0.001). When training using the bootstrap method of Section 4.1, we selected nbootstrap = T for generating Dbootstrap. Unless stated otherwise, the training set used in all experiments was comprised of 45k observation-action trajectories.\nWe use two metrics to evaluate policy performance on the set of test domains Dtest. The first one is execution success rate. Starting from the initial state we execute the policy and track whether or not the goal state is reached. The second metric is classification error on the next action, whether or not it matches what the planner would have done. For evaluating the accuracy of plan length prediction, we measure the average `1 loss (absolute difference).\n5We did not use the Sokoban data from the learning for planning competition as it only contains 60 training domains, which is not enough samples for training DNNs. Our generator works as follows: we assume the room dimensions are a multiple of 3 and partition the grid into 3x3 blocks. Each block is filled with a randomly chosen and randomly rotated pattern from a predefined set of 17 different patterns. To make sure the generated levels are not too easy and not impossible, we discard the ones containing open areas greater than 3x4 and discard the ones with disconnected floor tiles. For more details we refer the reader to Taylor et al. [28]."
    }, {
      "heading" : "5.1 Evaluating Network Structure",
      "text" : "In this section, based on several ablation experiments, we aim to tease out the important ingredients for a successful GRP.\nIn Figure 3 we plot the success rate on two-object Sokoban, for different network depths, and with or without skip connections. The results suggest that deeper networks perform better, with skip connections resulting in a consistent advantage. To further establish this claim, in Table 1 we compare deep networks with shallow and wide networks that have the same number of parameters. The improved results for the deeper networks suggest that for learning the planning based reasoning in this data – the deeper the network the better. We note a related observation in the context of a DNN representation of the value iteration planning algorithm in [27]. However, in our experiments the performance levels off after 14 layers. We attribute this to the general difficulty of training deep DNNs due to gradient propagation, as evident in the failure of training the 14 layer architecture without skip connections, Figure 3.\nWe also investigated the benefit of having a shared representation for both action and plan length prediction, compared to predicting each with a separate network. The ablation results are presented in Table 2. Interestingly, the plan length prediction improves the accuracy of the action prediction."
    }, {
      "heading" : "5.2 Evaluating Bootstrap Performance",
      "text" : "Here we evaluate the bootstrapping approach of Section 4.1. In Table 2 we show the success rate and plan length prediction error for architectures with and without the bootstrapping. As can be observed, the bootstrapping resulted in better use of the data, and led to improved results.\nWe also investigated the performance of bootstrapping with respect to the size of the training dataset. We observed that for smaller datasets, a non-uniform sampling strategy for the bootstrap data performed better. For each τ ∈ Dimitation, we sampled an observation ô from a distribution that is linearly increasing in time, such that observations near the goal have higher probability. We also noted that in Sokoban, all goal observations in the test set have the objects placed at goal positions.\nThus, bootstrapped goal observations in which the objects are not at a goal position would be visually different from the test data, and thus less effective for learning. We therefore made a modification to the bootstrapped observations. For a trajectory τ̂ starting from the initial observation in τ and ending at the sampled observation ô, we update the goal location6 to be the object position in ô. Thus, the objects positions in ô are visually modified to look as goals for the trajectory leading to ô. Both τ and τ̂ are added to a bootstrap training set Dbootstrap which is used for training instead of Dimitation. The performance of this bootstrapping strategy is shown in Figure 4. As should be expected, the performance improvement due to data augmentation is more significant for smaller data sets."
    }, {
      "heading" : "5.3 Evaluating GRP Performance",
      "text" : "The learned GRP in the best performing architecture (14 layers, with bootstrapping and a shared representation) can solve one-object Sokoban with 97% success rate, and two-object Sokoban with 87% success rate. In Figure 1 we plot a trajectory that the policy predicted in a challenging one-object domain. The two-object trajectories are harder to visualise in images, and we provide a video demonstration at https://sites.google.com/site/learn2plannips/. We observed that the policy learned to predict actions that avoid dead ends that happen far in the future, as Figure 1 demonstrates. The learned policy can be deployed in a new planning problem instead of running a planner."
    }, {
      "heading" : "5.4 DNN as a Heuristic Generator",
      "text" : "In terms of computation speed, running a forward pass of the DNN is generally faster than a planner. However, with some nonzero probability, the policy will fail to accomplish the task. In this section, we show that the plan length predicted by the DNN can also be used as a heuristic within standard planners. This approach can guarantee a successful completion of the task. The advantage, as we demonstrate, is that the learned DNN plan length can significantly improve upon an off-the-shelf heuristic that does not make use of the training data.\n6We note that this procedure requires us to make a change to the observations, which limits our approach to domains where such a modification is feasible. In Sokoban, performing this modification is straightforward, and we believe the same should hold for many other domains. We also note that a related idea was recently suggested in the context of reinforcement learning [1].\nIn particular, we investigated using the DNN as a heuristic in greedy search and A∗ search [22]. In Table 3 we evaluate 3 planning performance measures: number of nodes generated during search, number of nodes explored during search, and length of the plan that was found. The first two measures indicate the planning speed, where evaluating less nodes translates to faster planning. The total plan length is used as a measure of plan quality. We evaluate planning performance on the test set. As can be seen, in terms of the number of nodes explored, the learned NN heuristic significantly outperforms the Manhattan heuristic7 in both greedy search and A* search. Even though the NN heuristic is not guaranteed to be admissible, when used in conjunction with A*, plan quality is very close to optimal. We also add a comparison to two state-of-the-art planners: Fast Forward (FF, [9]) and Fast Downward (FD, [8]). FD uses an anytime algorithm, so we constrained the planning time to be no more than 5 minutes per domain. For the 9x9 domains, FD always found the optimal solution. We note that A* with our learned heuristic dramatically outperformed both planners in terms of the number of nodes explored. In terms of plan length, A* with our heuristic outperforms FF and is comparable with FD.\nThe utility of faster planning, exploring less nodes, comes into play when the robot performs similar planning tasks over and over again – in general, reduced performance latency tends to increase user satisfaction."
    }, {
      "heading" : "5.5 DNN Heuristic Generalization",
      "text" : "We also evaluate generalization using our GRP. In Table 4 and Figure 5, we evaluate a DNN heuristic trained on 9× 9 domains, and evaluated on larger domains. During training, we chose the window size k = 1, to influence learning a domain-invariant policy.\n7Note that the Manhattan heuristic is only admissible in the 1-object case. We also tried other heuristics such as Euclidean distance, Hamiltonian distance, and Max over the three. Hamiltonian distance took long to compute. Overall, Manhattan distance gave the best performance."
    }, {
      "heading" : "5.6 Analysis of Failure Modes",
      "text" : "In this section we investigate the failure modes of the learned GRP. We noticed that there were two primary failure modes. The first failure mode is due to cycles in the policy, and is a consequence of using a deterministic policy. For example, when the agent is between two objects a deterministic policy may oscillate, moving back and fourth between the two. We found that a stochastic policy significantly reduces this type of failure. However, stochastic policies have some non-zero probability of choosing actions that lead to a dead end (e.g., pushing the box directly up against a wall), which can lead to different failures. The second failure mode was the inability of our policy to foresee long term dependencies between the two objects. An example of such a case is shown in Figure 6 (f-h), where deciding which object to move first requires a look-ahead of more than 20 steps. A possible explanation for this failure is that such scenarios are not frequent in the training data.\nAdditionally, we investigated whether the failure cases can be related to specific features in the task. Specifically, we considered the task plan length (computed using FD), the number of walls in the domain, and the planning time with the FD planner (results are similar with other planners). Intuitively, these features are expected to correlate with the difficulty of the task. In Figure 6 (a-c) we plot the success rate vs. the features described above. As expected, success rate decreases with plan length. Interestingly, however, several domains that required a long time for FD were ‘easy’ for the learned policy, and had a high success rate. Further investigation revealed that these domains had large open areas, which are ‘hard’ for planners to solve due to a large branching factor, but admit a simple policy. An example of one such domain is shown in Figure 6 (d-e). We also note that the number of walls had no visible effect on success rate – it is the configuration of the walls that matters, and not their quantity."
    }, {
      "heading" : "5.7 Comparison to Value Iteration Networks",
      "text" : "Value iteration networks (VINs; [27]) are DNNs designs that have the capacity to perform a value iteration planning computation. While value iteration can easily be applied to small state space problems such as 2D navigation, as shown in [27], the state space in Sokoban is much larger, as it involves the interaction of the agent with the movable objects, and not only the obstacles. Here we demonstrate that indeed VINs cannot solve this domain. We first trained a VIN on Sokoban with no movable objects, which is equivalent to a navigation task. As expected, the VIN learned a successful policy with a 97.5% success rate. However, for 1 object Sokoban, VIN performance dropped to 53.5% and for 2 objects it dropped to 8.5%. This shows that VINs (at least as implemented in [27]) are not suitable for learning the complex planning behavior in Sokoban."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work we explored a simple yet powerful approach in learning for planning, based on imitation learning from visual execution traces of a planner. We used deep neural networks for learning a policy, and proposed several network designs that improve learning performance in this setting. In addition, we proposed networks that can be used to learn a heuristic for off-the-shelf planners, which led to significant improvements over standard heuristics that do not leverage learning.\nOur results on the challenging Sokoban domain suggest that DNNs have the capability to extract powerful features from observations, and the potential to learn the type of ‘visual thinking’ that makes some planning problems easy to humans but very hard for automatic planners.\nThere is still much to explore in employing deep networks for planning. While representations for images based on deep conv-nets have become standard, representations for other modalities such as graphs and logical expressions are not yet mature (although see [3] for recent developments). We believe that the results presented here will motivate future research in representation learning for planning."
    } ],
    "references" : [ {
      "title" : "Hindsight experience replay",
      "author" : [ "Marcin Andrychowicz", "Filip Wolski", "Alex Ray", "Jonas Schneider", "Rachel Fong", "Peter Welinder", "Bob McGrew", "Josh Tobin", "Pieter Abbeel", "Wojciech Zaremba" ],
      "venue" : "arXiv preprint arXiv:1707.01495,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2017
    }, {
      "title" : "The computational complexity of propositional strips planning",
      "author" : [ "Tom Bylander" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1994
    }, {
      "title" : "Learning combinatorial optimization algorithms over graphs",
      "author" : [ "Hanjun Dai", "Elias B Khalil", "Yuyu Zhang", "Bistra Dilkina", "Le Song" ],
      "venue" : "arXiv preprint arXiv:1704.01665,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "One-shot imitation learning",
      "author" : [ "Yan Duan", "Marcin Andrychowicz", "Bradly Stadie", "Jonathan Ho", "Jonas Schneider", "Ilya Sutskever", "Pieter Abbeel", "Wojciech Zaremba" ],
      "venue" : "arXiv preprint arXiv:1703.07326,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2017
    }, {
      "title" : "The first learning track of the international planning competition",
      "author" : [ "Alan Fern", "Roni Khardon", "Prasad Tadepalli" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Learning and executing generalized robot plans",
      "author" : [ "Richard E Fikes", "Peter E Hart", "Nils J Nilsson" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1972
    }, {
      "title" : "PDDL2. 1: An extension to PDDL for expressing temporal planning domains",
      "author" : [ "Maria Fox", "Derek Long" ],
      "venue" : "J. Artif. Intell. Res.(JAIR),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "The fast downward planning system",
      "author" : [ "Malte Helmert" ],
      "venue" : "Journal of Artificial Intelligence (JAIR),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "FF: The fast-forward planning system",
      "author" : [ "Jörg Hoffman" ],
      "venue" : "AI Magazine,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Sokoban: Enhancing general single-agent search methods using domain knowledge",
      "author" : [ "Andreas Junghanns", "Jonathan Schaeffer" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2001
    }, {
      "title" : "Learning action strategies for planning domains",
      "author" : [ "Roni Khardon" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1999
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Jonathan Long", "Evan Shelhamer", "Trevor Darrell" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Learning generalized policies in planning using concept languages",
      "author" : [ "Mario Martin", "Hector Geffner" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2000
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Learning to select and generalize striking movements in robot table tennis",
      "author" : [ "Katharina Mülling", "Jens Kober", "Oliver Kroemer", "Jan Peters" ],
      "venue" : "The International Journal of Robotics Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Combining self-supervised learning and imitation for vision-based rope manipulation",
      "author" : [ "Ashvin Nair", "Dian Chen", "Pulkit Agrawal", "Phillip Isola", "Pieter Abbeel", "Jitendra Malik", "Sergey Levine" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2018
    }, {
      "title" : "From perception to decision: A data-driven approach to end-to-end motion planning for autonomous ground robots",
      "author" : [ "Mark Pfeiffer", "Michael Schaeuble", "Juan Nieto", "Roland Siegwart", "Cesar Cadena" ],
      "venue" : "arXiv preprint arXiv:1609.07910,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Alvinn: An autonomous land vehicle in a neural network",
      "author" : [ "Dean A Pomerleau" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1989
    }, {
      "title" : "A reduction of imitation learning and structured prediction to no-regret online learning",
      "author" : [ "Stéphane Ross", "Geoffrey J Gordon", "Drew Bagnell" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "The predictron: End-toend learning and planning",
      "author" : [ "David Silver", "Hado van Hasselt", "Matteo Hessel", "Tom Schaul", "Arthur Guez", "Tim Harley", "Gabriel Dulac-Arnold", "David Reichert", "Neil Rabinowitz", "Andre Barreto" ],
      "venue" : "arXiv preprint arXiv:1612.08810,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "Directed search for generalized plans using classical planners",
      "author" : [ "Siddharth Srivastava", "Neil Immerman", "Shlomo Zilberstein", "Tianjiao Zhang" ],
      "venue" : "In Proceedings of the International Conference on Automated Planning and Scheduling,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Value iteration networks",
      "author" : [ "Aviv Tamar", "Sergey Levine", "Pieter Abbeel", "YI WU", "Garrett Thomas" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Procedural generation of sokoban levels",
      "author" : [ "Joshua Taylor", "Ian Parberry" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Imagination-augmented agents for deep reinforcement learning",
      "author" : [ "Théophane Weber", "Sébastien Racanière", "David P Reichert", "Lars Buesing", "Arthur Guez", "Danilo Jimenez Rezende", "Adria Puigdomènech Badia", "Oriol Vinyals", "Nicolas Heess", "Yujia Li" ],
      "venue" : "arXiv preprint arXiv:1707.06203,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2017
    }, {
      "title" : "Inductive policy selection for first-order MDPs",
      "author" : [ "SungWook Yoon", "Alan Fern", "Robert Givan" ],
      "venue" : "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2002
    }, {
      "title" : "Learning control knowledge for forward search planning",
      "author" : [ "Sungwook Yoon", "Alan Fern", "Robert Givan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "The plan existence problem for deterministic, fully observable environments is PSPACE-complete when expressed using rudimentary propositional representations [2].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "Such results have inspired the learning for planning paradigm: learning, or reusing the knowledge acquired while planning across multiple problem instances (in the form of triangle tables [6], learning control knowledge for planning [31], and constructing generalized plans [25], among other approaches) with the goal of faster plan computation in a new problem instance.",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 29,
      "context" : "Such results have inspired the learning for planning paradigm: learning, or reusing the knowledge acquired while planning across multiple problem instances (in the form of triangle tables [6], learning control knowledge for planning [31], and constructing generalized plans [25], among other approaches) with the goal of faster plan computation in a new problem instance.",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 23,
      "context" : "Such results have inspired the learning for planning paradigm: learning, or reusing the knowledge acquired while planning across multiple problem instances (in the form of triangle tables [6], learning control knowledge for planning [31], and constructing generalized plans [25], among other approaches) with the goal of faster plan computation in a new problem instance.",
      "startOffset" : 274,
      "endOffset" : 278
    }, {
      "referenceID" : 10,
      "context" : ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 29,
      "context" : ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 23,
      "context" : ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : ", [11, 15, 30, 31, 25]) have relied upon hand-written domain descriptions and feature sets based on languages such as PDDL [7].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "Recently, deep neural networks (DNNs) have been used to automatically extract expressive features from data, leading to state-of-the-art learning results in image classification [13], natural language processing [26], and control [16], among other domains.",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 24,
      "context" : "Recently, deep neural networks (DNNs) have been used to automatically extract expressive features from data, leading to state-of-the-art learning results in image classification [13], natural language processing [26], and control [16], among other domains.",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 15,
      "context" : "Recently, deep neural networks (DNNs) have been used to automatically extract expressive features from data, leading to state-of-the-art learning results in image classification [13], natural language processing [26], and control [16], among other domains.",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 19,
      "context" : "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].",
      "startOffset" : 165,
      "endOffset" : 177
    }, {
      "referenceID" : 20,
      "context" : "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].",
      "startOffset" : 165,
      "endOffset" : 177
    }, {
      "referenceID" : 25,
      "context" : "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].",
      "startOffset" : 165,
      "endOffset" : 177
    }, {
      "referenceID" : 16,
      "context" : "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 17,
      "context" : "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 3,
      "context" : "Imitation learning has been previously used with DNNs to learn policies for tasks that involve short horizon reasoning such as path following and obstacle avoidance [20, 21, 27], focused robot skills [17, 18], and recently block stacking [4].",
      "startOffset" : 238,
      "endOffset" : 241
    }, {
      "referenceID" : 4,
      "context" : "In particular, we focus on the Sokoban domain (see Figure 1), which has been described as the most challenging problem in the literature on learning for planning [5]: “ Sokoban has been demonstrated to be a very challenging domain for AI search and planning algorithms, even when significant human domain knowledge is provided [10].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 9,
      "context" : "In particular, we focus on the Sokoban domain (see Figure 1), which has been described as the most challenging problem in the literature on learning for planning [5]: “ Sokoban has been demonstrated to be a very challenging domain for AI search and planning algorithms, even when significant human domain knowledge is provided [10].",
      "startOffset" : 327,
      "endOffset" : 331
    }, {
      "referenceID" : 4,
      "context" : "Within the learning for planning literature [5], several studies considered learning a reactive policy, which is similar to our imitation learning approach.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "The works of Khadron [11], Martin and Geffner [15], and Yoon et al.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "The works of Khadron [11], Martin and Geffner [15], and Yoon et al.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "[30] learn policies represented as decision lists on the logical problem representation, which needs to be hand specified.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "Works in imitation learning have mainly focused on learning skills from human demonstrations, such as driving and obstacle avoidance [20, 21], and robot table tennis and rope manipulation [17, 18].",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 20,
      "context" : "Works in imitation learning have mainly focused on learning skills from human demonstrations, such as driving and obstacle avoidance [20, 21], and robot table tennis and rope manipulation [17, 18].",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "Works in imitation learning have mainly focused on learning skills from human demonstrations, such as driving and obstacle avoidance [20, 21], and robot table tennis and rope manipulation [17, 18].",
      "startOffset" : 188,
      "endOffset" : 196
    }, {
      "referenceID" : 17,
      "context" : "Works in imitation learning have mainly focused on learning skills from human demonstrations, such as driving and obstacle avoidance [20, 21], and robot table tennis and rope manipulation [17, 18].",
      "startOffset" : 188,
      "endOffset" : 196
    }, {
      "referenceID" : 18,
      "context" : "[19] recently applied IL to learning obstacle avoidance from a motion planner.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[27] demonstrated the difficulty of generalizing goal directed behavior from demonstration.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4] showed learning of various block stacking tasks where the goal was specified by an additional execution trace.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "The ‘one-shot’ techniques in [4], however, are complimentary to this work.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : "The impressive Alpha-Go [23] program learned a DNN strategy for Go, using a combination of IL, and reinforcement learning through self-play.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 25,
      "context" : "In [27], a value iteration planning computation was embedded within the network structure, and demonstrated successful learning on 2D gridworld navigation.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "The predictron architecture [24] uses ideas from temporal difference learning to design a DNN for reward prediction.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 27,
      "context" : "[29] proposed a DNN architecture that combines model based planning with model free components for reinforcement learning, and demonstrated results on the Sokoban domain.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "Planning: We focus on fully observable, deterministic task planning problems described in the formal language PDDL [7].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 19,
      "context" : "A straightforward IL approach is behavioral cloning [20], in which standard supervised learning is used to learn μ from the data.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 25,
      "context" : "In this work, similar to [27], we assume that the problem instance and state are given as an image observation.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "In our experiments we used the Fast-Forward (FF) planner [9], though any other PDDL planner can be used instead.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 25,
      "context" : "It is also possible to extend our work to graph representations using convolutions on graphs [27, 3].",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "It is also possible to extend our work to graph representations using convolutions on graphs [27, 3].",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : ", [20, 21]), the policy is typically structured as a mapping from observation to action a = μ(o).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 20,
      "context" : ", [20, 21]), the policy is typically structured as a mapping from observation to action a = μ(o).",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "Proposition 1 underlies classical planning methods such as triangle tables [6].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "We note that in value iteration networks [27], similar skip connections were used in an explicit neural network implementation of value iteration.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : "To remove this dependency, we employ a trick used in fully convolutional networks [14], and keep only a k × k window of the last convolution layer, centered around the current agent position.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "For imitation learning, we represent the policy with the DNNs described in Section 4 and optimize using Adam [12] (step size 0.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 26,
      "context" : "[28].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "We note a related observation in the context of a DNN representation of the value iteration planning algorithm in [27].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "We also note that a related idea was recently suggested in the context of reinforcement learning [1].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : "We also add a comparison to two state-of-the-art planners: Fast Forward (FF, [9]) and Fast Downward (FD, [8]).",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "We also add a comparison to two state-of-the-art planners: Fast Forward (FF, [9]) and Fast Downward (FD, [8]).",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "Value iteration networks (VINs; [27]) are DNNs designs that have the capacity to perform a value iteration planning computation.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "While value iteration can easily be applied to small state space problems such as 2D navigation, as shown in [27], the state space in Sokoban is much larger, as it involves the interaction of the agent with the movable objects, and not only the obstacles.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 25,
      "context" : "This shows that VINs (at least as implemented in [27]) are not suitable for learning the complex planning behavior in Sokoban.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "While representations for images based on deep conv-nets have become standard, representations for other modalities such as graphs and logical expressions are not yet mature (although see [3] for recent developments).",
      "startOffset" : 188,
      "endOffset" : 191
    } ],
    "year" : 2017,
    "abstractText" : "We consider the problem of learning for planning, where knowledge acquired while planning is reused to plan faster in new problem instances. For robotic tasks, among others, plan execution can be captured as a sequence of visual images. For such domains, we propose to use deep neural networks in learning for planning, based on learning a reactive policy that imitates execution traces produced by a planner. We investigate architectural properties of deep networks that are suitable for learning long-horizon planning behavior, and explore how to learn, in addition to the policy, a heuristic function that can be used with classical planners or search algorithms such as A∗. Our results on the challenging Sokoban domain show that, with a suitable network design, complex decision making policies and powerful heuristic functions can be learned through imitation. Videos available at https://sites.google.com/site/learn2plannips/.",
    "creator" : "LaTeX with hyperref package"
  }
}