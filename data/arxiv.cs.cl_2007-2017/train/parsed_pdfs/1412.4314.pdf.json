{
  "name" : "1412.4314.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "CODE-SWITCHING CORPUS", "Joseph Chee Chang" ],
    "emails" : [ "chuchenglin@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Mixed language data is one of the difficult yet less explored domains of natural language processing. Most research in fields like machine translation or sentiment analysis assume monolingual input. However, people who are capable of using more than one language often communicate using multiple languages at the same time. Sociolinguists believe this ”code-switching” phenomenon to be socially motivated. For example, to express solidarity or to establish authority. Most past work depend on external tools or resources, such as part-of-speech tagging, dictionary look-up, or named-entity recognizers to extract rich features for training machine learning models. In this paper, we train recurrent neural networks with only raw features, and use word embedding to automatically learn meaningful representations. Using the same mixed-language Twitter corpus, our system is able to outperform the best SVM-based systems reported in the EMNLP’14 Code-Switching Workshop by 1% in accuracy, or by 17% in error rate reduction."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Code–switching refers to the phenomenon that a speaker changes between different languages in a single utterance or conversation. Sociolinguists believe people code–switch because of sociolinguistic motivations, e.g. to express solidarity and familiarity, and to establish authority or distance.(Gumperz, 1982) It is also shown that code–switching has its own grammatical regularities, that switching points almost never occur at certain points.(Berk-Seligson, 1986) This implies using sophisticated models to identify structures in a mixed-language sentence can potentially help with the language detection task. Also, computational models of code–switching can thus be used to verify linguistic theories. Nonetheless it can also be used to guide downstream NLP applications to use correct language models, which is a practical problem for current social media processing.\nThere’s recently a workshop and shared task on modeling code–switching in social media. The participants were provided code–switched tweets from four language pairs. They then asked to label the tweets for the language at token level. This strongly resembles other established sequential labeling problems such as POS tagging and named-entity recognizer (NER); and almost all participants adapted either the conditional random fields model (CRF) or the support vector machine model (SVM). The results are not bad in general. For Nepali–English and Spanish–English most participants were able to get an F1 score around 0.90 to 0.95. However, many participants put a lot of efforts into extensive and careful feature engineering. Features used in the shared task include dictionaries, character and word language models, and even output from other NER systems and language identification systems. Despite all the efforts, the outcomes were not always positive. For example, previous work has found that the use of existing NER systems does not translate into improvement of named entities in one submission (King et al., 2014). In comparison, our proposed RNN–based classifier takes only character n–grams, pre–trained embeddings and the texts. Yet our system are competitive against the best results reported in the shared task.\nar X\niv :1\n41 2.\n43 14\nv1 [\ncs .N\nE ]\n1 4\nD ec\n2 01\nThe paper is structured as follows. In Sec. 2 we review related work in the literature. In Sec. 3 we describe the code–switched tweets and the annotation scheme in detail. And in Sec. 4 we describe our RNN–based model. The experiment results are in Sec. 5."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "In this section, we will review previous methods for language identification, both in monolingual and multilingual texts, and computational models of code switching. We will also review previous work in neural networks for processing natural language and re-representing words in a semantic vector space.\nMonolingual language identification. Monolingual language identification is generally treated as a text categorization problem, often defined as given a text t, assign its label lt ∈ L where L = {l1 . . . lN} is a predefined set of languages in this task. Baldwin & Lui (2010) is an excellent review. For the sake of completeness, we include some of the most relevant work here. By featurizing each text f(t) ∈ F , this problem can be treated as a standard supervised learning problem. Indeed, many well known classifiers such as Naive Bayes(Lui & Baldwin, 2012), SVM,(Jalam & Teytaud, 2001) and kernel methods(Kruengkrai et al., 2005) have been used in the literature. Featurizing each text is an important subproblem. Existing featurization include per–language character frequency, n–grams(Cavnar & Trenkle, 1994), and linguistically motivated features, such as stop word lists(Johnson, 1993). People have also achieved excellent accuracy with lower–level features, such as byte sequence(Chew et al., 2011).\nMultilingual langID. The monolinguality of a document is however not always realistic. For example, researchers has found that people have been posting tweets using multiple languages for some time(Ling et al., 2013). Lui et al. (2014) used topic modeling to model the languages that occur in a document. People have also used sequential labeling algorithms to identify language segments in a document, such as CRF by King & Abney (2013).\nComputational models of code–switching. Both monolingual and multilingual language identification seek to assign label(s) to a single document or sentence. Computational models of code– switching on the other hand, takes a step further, and tries to pinpoint where the switching between languages happens. Equivalently, one can say code–switching assigns language label at the token level, stead of the sentence level or document level. Linguists have proposed syntactic theories that predicts the locations of code–switching(Sankoff, 1998; Belazi et al., 1994; Cantone & Müller, 2008), which at the same time prohibits their happening at certain places. Such theories have been incorporated into computational models, such as (Li & Fung, 2014).\nRecurrent-Neural-Network for Natural Language Processing Due to the sequential natural of natural language data, many problems of natural language processing in the form of tagging are traditionally solved using the linear Hidden Markov Model (Kupiec, 1992). Other frequently used, state-of-the-art machine learning models include Chained-Conditional Random Fields (Lafferty et al., 2001), Maximum-Entropy Markov Model (McCallum et al., 2000), and sometimes Support Vector Machines (Kudo & Matsumoto, 2001). More recently, researchers has also begun to explore using RNN architectures to rival, or even outperform such machine learning approaches. Mesnil et al. (2013) project 1-hot word vectors using a task-specific, supervised embedding layer, and experimented using both Elman-type (Elman, 1990) (Mikolov et al., 2010) and Jordan-type (Jordan, 1997) RNNs. Their results show that by using the exact same features, RNNs can outperform the state-of-the-art CRF-based model by 1% F1 score. In our work, we extend their structure to include additional character ngram features, and a pre-trained, generalized embedding layer. Our results show that the proposed architecture trained on only simple lexical features can outperform state-of-the-art SVM-based systems trained on rich features generated using external tools such as named entity recognizers and dictionaries."
    }, {
      "heading" : "3 DATA",
      "text" : "All our experiments are conducted on the Twitter data provided by the EMNLP Code–switching Workshop (Solorio et al., 2014). Due to Twitter’s privacy policy, the organizers were not allowed to provide the tweets themselves. Instead, the participants were provided the unique tweet ids and character offsets. Then, the participants had to crawl the data themselves. With situations like deletion and privacy setting changes made by the users after the initial crawl, we are able to crawl 27, 255 Tweets in total. The language–pair breakdown is listed in Table 1.\nAlthough the main goal of the shared task is to predict the code–switching points, the data is annotated with a finer–grained scheme listed in Table 2. It should be noted that there is a large variance in each label’s frequency. mixed and ambiguous, despite their significance in the linguistic theory of bilinguallism, do not appear to happen often in social media. On the other hand, named entities (labeled with ne) occurs very frequently, to the extent that it was the main deciding factor of the participants’ overall performance since lang1 and lang2 are relatively easy to classify.\nAs stated in Sec. 1, we decide to make the task more realistic (and also harder) by training a classifier that considers all possible languages. Therefore we have a single corpus with (some number) of tweets. The number of tokens by language id is in Table 3."
    }, {
      "heading" : "4 METHODOLOGY",
      "text" : "The proposed network architecture is based the Elman-type and Jordan-type Recurrent Neural Network. We follow the implementation of Mesnil et. al 2013, and use a 3-word window to capture the immediate context, forming a 7 dimention context vector (1-hot). A supervised word embedding layer is used to project each word onto a 100 dimension real value vector. The second layer is a 100 node hidden layer using the sigmoid function. Finally, we use the softmax function at the output layer. Long-term dependencies are captured using either Elman-type or Jordan-type recurrent structure, where the current output depends on the output of the previous 9 hidden layer or final layer, respectively.\nExtending this architecture to incorporate character ngram features, we use the before mentioned technique to extract a 12-dimension 1-hot vectors from the current word and append it to the 7- dimension context vector, and use the same embedding layer to project them onto the real value vector space. To incorporate the pre-trained Word2Vec model, we use the current word to look up the model, and feed the projected real value vector directly to the hidden layer. Another alternative approach is to replace the supervised embedding layer completely with a pre-trained Word2Vec model, but we think having both a general embedding model and a task specific embedding model can further generalize our method.\nIn the following subsection, we will talk about the pre-trained Word2Vec model, and also how we extract simple character ngram features. In the evaluation section, we will show the performance of model trained with different network structures and using different feature combinations."
    }, {
      "heading" : "4.1 PRE-TRAINED WORD2VECTOR",
      "text" : "We employ skip–gram embeddings as our features. Skip–gram word embeddings Mikolov et al. (2013) is a log bilinear model that encourages words with similar contexts to have similar embeddings. We use the implementation from Gensim, and trained on a large Twitter corpus of random samples from the live feed. We randomly sample 10,000 tweets each day, spanning over roughly 2,000 days. We did not filter for specific languages. The idea is that words of different languages tend to share different contexts. So the embeddings should provide good separation between languages. And they proved to provide improvement in the code–switching task (Lin et al., 2014)."
    }, {
      "heading" : "4.2 CHARACTER NGRAMS",
      "text" : "Character n–grams prove to be valuable features because languages often have distinct character combinations. For example, a word that starts with “lle” is more likely to be Spanish than English. Conversely a word that ends with “tion” is more likely to be English. These features are widely used by existing programs, such as cld21 and ldig2.\nIn our approach, we extract a fix number of character ngram for each word. We use a window of 3 characters, and extract character bigrams and trigrams from both the begining and end of each word, resulting a 12 dimention character ngram vector. For example, the character ngram vector for the word architecture is [arc, rch, chi, ar, rc, ch, ure, tur, ctu, re, ur, tu].\n1https://code.google.com/p/cld2/ 2https://github.com/shuyo/ldig"
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "5.1 PRELIMINARY STUDY",
      "text" : "To test the effectiveness of different neural network structures, we first use a smaller data set for a preliminary study. Besides testing both Elman-type and Jordan-type RNN structures, we also tested the effectiveness of our proposed extensions. Our preliminary study dataset contains only a total of 2,734 tweets, where we use 1,000 for training, 1,000 for validation, and the rest for evaluation."
    }, {
      "heading" : "5.1.1 EXPERIMENTAL RESULTS",
      "text" : "In the preliminary study, we tested the five following configurations:\n• Jordan: The basic Jordan-type RNN with input as 7 1-hot word-context vectors and a supervised embedding layer for projecting words onto a real-valued Euclidean space.\n• Elman: The basic Elman-type RNN with input as 7 1-hot word-context vectors and a supervised embedding layer for projecting words onto a real-valued Euclidean space.\n• Jordan+ngram: The Jordan model with an additional 12 1-hot character ngram vector. • Elman+ngram: The Elman model with an additional 12 1-hot character ngram vector. • Elman+ngram+w2v: The Elman+ngram configuration with an addtional pre-trained\nword2vec model feeding directly to the hidden layers.\nAs shown in the figure, with the basic architecture, Jordan-type RNN and Elman-type RNN achieved very similary performance (88.4% and 88.9%). Adding character ngrams improves the F1 performance of both architectures by roughly 4% (92.8% and 92.6%). The best performing system that uses Elman architecture and both character ngrams and pre-trained Word2Vec performed a F1 score of 93.7%. Adding the pre-trained Word2Vec features directly to the hidden layer provided an additional 1.2% improvement. This suggests that comparing to the basic RNN structures, the proposed two extensions can improve the performance by roughly 5% on the language identification task."
    }, {
      "heading" : "5.2 COMPARING TO PREVIOUS WORK",
      "text" : "To evaluate our approach, we use the training data of 27,255 tweets obtained from the EMNLP 2014 Code-Switching Workshop. We use 2,734 tweets as our evaluation set, and the rest for training\nand validation. We acknowledge that we are using a different test set to evaluate our systems, and comparing the results with the systems form the workshop. But we also separate the tweets into training, validation, and evaluation sets using disjoint sets of authors. Nevertheless, both the test data we use and the test data from the workshop were collected using the exact same fashion, and both are from none overlapping sets of authors than the training set."
    }, {
      "heading" : "5.2.1 BASELINE SYSTEMS",
      "text" : "The EMNLP Code–Switching Workshop provided a simple deterministic baseline for all categories. Given a word w, it looks it up in the training corpus and pick the more frequent language label `(w) ∈ {lang1,lang2} as its label. If `(w) = 0 it returns other. And in the case of a tie it returns the language that is more frequent."
    }, {
      "heading" : "5.2.2 BEST PERFORMING SYSTEMS IN THE SHARED TASK",
      "text" : "According to the overview paper of the workshop (Solorio et al., 2014), most teams in the shared task adapted the CRF model for language identification. The best Nepali–English submission Barman et al. (2014) used SVM as the classification framework. They used character n–grams, context, capitalization, word length, and dictionaries as their features. The best Spanish–English submission (Bar & Dershowitz, 2014) also used SVM as the main classifier. They used character n–grams, context, and dictionaries. Additionally they also take language model probabilities as features."
    }, {
      "heading" : "5.2.3 EXPERIMENTAL RESULTS",
      "text" : "In Table. 3, we show the performance of our full systems. To compare with the results reported in the workshop, we evaluate our system on the accuracy on two different categories against the best performing systems reported in the EMNLP’14 Code-Switching Workshop. We used only a set of 18,521 tweets to train the networks, because we need the rest of the training set for validation and testing. This is using a significantly smaller training set comparing to previous systems, which have access to the full training data of 27,255 tweets provided by the workshop.\nSimilar to the preliminary experiment results, Elman-type and Jordan-type RNNs performed similarly. Comparing to state-of-the-art systems, our networks are able to perform 1% higher accuracy for English-Spanish category, a 17% reduction in error rate. For the English-Nepali category, our Jordan-type network performed 0.2% higher in accuracy, a 8% reduction in error rate. This shows that the recurrent neural networks and the embedding layers are able to learn meaningful representations for language detection using only the raw lexical features, and are able to rival against SVM-based systems that depend on sophisticated feature extraction to re-represent the input data."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, we tackled the important natural language processing task of language detection in a code-switching twitter corpus using recurrent neural networks. This is a novel application for RNN as most previous research focused on using machine learning methods, such as chained conditional random fields, to solve this type of problems. In fact, in the 2014 EMNLP Code-Switching Workshop most of the participating team used CRF to build their models, and the best performing two teams used SVM-based model. Previous work has already compared RNNs to CRF-based model\nfor natural language processing, and results show that RNNs can produce better accuracy by 1% on a named entity recognition task. We also tested RNN and found that by with the two proposed extensions, RNNs can also outperform state-of-the-art SVM-based system by 1% in accuracy, or a 17% in error rate reduction, while using simpler features and smaller training data."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work is initiated by the Deep Learning course at the Language Technologies Institute at Carnegie Mellon University offered by Dr. Bhiksha Raj. The authors would also like to thank the teaching assistant Zhenzhong Lan for the insightful discussions."
    } ],
    "references" : [ {
      "title" : "Language identification: The long and the short of the matter",
      "author" : [ "Baldwin", "Timothy", "Lui", "Marco" ],
      "venue" : "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Baldwin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Baldwin et al\\.",
      "year" : 2010
    }, {
      "title" : "Proceedings of the First Workshop on Computational Approaches to Code Switching, chapter The Tel Aviv University System for the Code-Switching Workshop Shared Task, pp. 139–143",
      "author" : [ "Bar", "Kfir", "Dershowitz", "Nachum" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Bar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bar et al\\.",
      "year" : 2014
    }, {
      "title" : "Proceedings of the First Workshop on Computational Approaches to Code Switching, chapter DCU-UVT: Word-Level Language Classification with Code-Mixed Data, pp. 127–132",
      "author" : [ "Barman", "Utsab", "Wagner", "Joachim", "Chrupała", "Grzegorz", "Foster", "Jennifer" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Barman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Barman et al\\.",
      "year" : 2014
    }, {
      "title" : "Code switching and x-bar theory: The functional head constraint",
      "author" : [ "Belazi", "Hedi M", "Rubin", "Edward J", "Toribio", "Almeida Jacqueline" ],
      "venue" : "Linguistic Inquiry,",
      "citeRegEx" : "Belazi et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Belazi et al\\.",
      "year" : 1994
    }, {
      "title" : "Linguistic constraints on intrasentential code-switching: A study of spanish/hebrew bilingualism",
      "author" : [ "Berk-Seligson", "Susan" ],
      "venue" : "Language in Society,",
      "citeRegEx" : "Berk.Seligson and Susan.,? \\Q1986\\E",
      "shortCiteRegEx" : "Berk.Seligson and Susan.",
      "year" : 1986
    }, {
      "title" : "i¿ un nase or una nase¡/i¿? what gender marking within switched dps reveals about the architecture of the bilingual language faculty",
      "author" : [ "Cantone", "Katja Francesca", "Müller", "Natascha" ],
      "venue" : "Lingua,",
      "citeRegEx" : "Cantone et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Cantone et al\\.",
      "year" : 2008
    }, {
      "title" : "N-gram-based text categorization",
      "author" : [ "Cavnar", "William B", "Trenkle", "John M" ],
      "venue" : "Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,",
      "citeRegEx" : "Cavnar et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Cavnar et al\\.",
      "year" : 1994
    }, {
      "title" : "Language identification of web pages based on improved n-gram algorithm",
      "author" : [ "Chew", "Yew Choong", "Mikami", "Yoshiki", "Nagano", "Robin Lee" ],
      "venue" : "International Journal of Computer Science Issues (IJCSI),",
      "citeRegEx" : "Chew et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chew et al\\.",
      "year" : 2011
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Elman", "Jeffrey L" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "Elman and L.,? \\Q1990\\E",
      "shortCiteRegEx" : "Elman and L.",
      "year" : 1990
    }, {
      "title" : "Discourse Strategies. Studies in Interactional Sociolinguistics",
      "author" : [ "Gumperz", "John J" ],
      "venue" : "URL http://books.google.com/books? id=aUJNgHWl_koC",
      "citeRegEx" : "Gumperz and J.,? \\Q1982\\E",
      "shortCiteRegEx" : "Gumperz and J.",
      "year" : 1982
    }, {
      "title" : "Kernel-based text categorisation",
      "author" : [ "R. Jalam", "O. Teytaud" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "Jalam and Teytaud,? \\Q2001\\E",
      "shortCiteRegEx" : "Jalam and Teytaud",
      "year" : 2001
    }, {
      "title" : "Solving the problem of language identification",
      "author" : [ "Johnson", "Stephen" ],
      "venue" : "Technical report, University of Leeds,",
      "citeRegEx" : "Johnson and Stephen.,? \\Q1993\\E",
      "shortCiteRegEx" : "Johnson and Stephen.",
      "year" : 1993
    }, {
      "title" : "Serial order: A parallel distributed processing approach",
      "author" : [ "Jordan", "Michael I" ],
      "venue" : "Advances in psychology,",
      "citeRegEx" : "Jordan and I.,? \\Q1997\\E",
      "shortCiteRegEx" : "Jordan and I.",
      "year" : 1997
    }, {
      "title" : "Labeling the languages of words in mixed-language documents using weakly supervised methods",
      "author" : [ "King", "Ben", "Abney", "Steven" ],
      "venue" : "In Proceedings of NAACL-HLT,",
      "citeRegEx" : "King et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "King et al\\.",
      "year" : 2013
    }, {
      "title" : "The iucl+ system: Word-level language identification via extended markov models",
      "author" : [ "King", "Levi", "Baucom", "Eric", "Gilmanov", "Timur", "Kübler", "Sandra", "Whyatt", "Dan", "Maier", "Wolfgang", "Rodrigues", "Paul" ],
      "venue" : "In Proceedings of the First Workshop on Computational Approaches to Code Switching,",
      "citeRegEx" : "King et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "King et al\\.",
      "year" : 2014
    }, {
      "title" : "Language identification based on string kernels",
      "author" : [ "C. Kruengkrai", "P. Srichaivattana", "V. Sornlertlamvanich", "H. Isahara" ],
      "venue" : "In Communications and Information Technology,",
      "citeRegEx" : "Kruengkrai et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kruengkrai et al\\.",
      "year" : 2005
    }, {
      "title" : "Chunking with support vector machines",
      "author" : [ "Kudo", "Taku", "Matsumoto", "Yuji" ],
      "venue" : "In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,",
      "citeRegEx" : "Kudo et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kudo et al\\.",
      "year" : 2001
    }, {
      "title" : "Robust part-of-speech tagging using a hidden markov model",
      "author" : [ "Kupiec", "Julian" ],
      "venue" : "Computer Speech & Language,",
      "citeRegEx" : "Kupiec and Julian.,? \\Q1992\\E",
      "shortCiteRegEx" : "Kupiec and Julian.",
      "year" : 1992
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "Lafferty", "John", "McCallum", "Andrew", "Pereira", "Fernando CN" ],
      "venue" : null,
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Language modeling with functional head constraint for code switching speech recognition",
      "author" : [ "Li", "Ying", "Fung", "Pascale" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Li et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "The cmu submission for the shared task on language identification in code-switched data",
      "author" : [ "Lin", "Chu-Cheng", "Ammar", "Waleed", "Levin", "Lori", "Dyer", "Chris" ],
      "venue" : "In Proceedings of the First Workshop on Computational Approaches to Code Switching,",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Microblogs as parallel corpora. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 176–186",
      "author" : [ "Ling", "Wang", "Xiang", "Guang", "Dyer", "Chris", "Black", "Alan", "Trancoso", "Isabel" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "Ling et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2013
    }, {
      "title" : "Langid.py: An off-the-shelf language identification tool",
      "author" : [ "Lui", "Marco", "Baldwin", "Timothy" ],
      "venue" : "In Proceedings of the ACL 2012 System Demonstrations,",
      "citeRegEx" : "Lui et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lui et al\\.",
      "year" : 2012
    }, {
      "title" : "Automatic detection and language identification of multilingual documents",
      "author" : [ "Lui", "Marco", "Lau", "Jey Han", "Baldwin", "Timothy" ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Lui et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lui et al\\.",
      "year" : 2014
    }, {
      "title" : "Maximum entropy markov models for information extraction and segmentation",
      "author" : [ "McCallum", "Andrew", "Freitag", "Dayne", "Pereira", "Fernando CN" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "McCallum et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "McCallum et al\\.",
      "year" : 2000
    }, {
      "title" : "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding",
      "author" : [ "Mesnil", "Grégoire", "He", "Xiaodong", "Deng", "Li", "Bengio", "Yoshua" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Mesnil et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mesnil et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Mikolov", "Tomas", "Karafiát", "Martin", "Burget", "Lukas", "Cernockỳ", "Jan", "Khudanpur", "Sanjeev" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "A formal production-based explanation of the facts of code-switching",
      "author" : [ "Sankoff", "David" ],
      "venue" : "Bilingualism: language and cognition,",
      "citeRegEx" : "Sankoff and David.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sankoff and David.",
      "year" : 1998
    }, {
      "title" : "Overview for the first shared task on language identification in code-switched data",
      "author" : [ "Solorio", "Thamar", "Blair", "Elizabeth", "Maharjan", "Suraj", "Bethard", "Steven", "Diab", "Mona", "Gohneim", "Mahmoud", "Hawwari", "Abdelati", "AlGhamdi", "Fahad", "Hirschberg", "Julia", "Chang", "Alison" ],
      "venue" : "EMNLP",
      "citeRegEx" : "Solorio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Solorio et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "For example, previous work has found that the use of existing NER systems does not translate into improvement of named entities in one submission (King et al., 2014).",
      "startOffset" : 146,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "Indeed, many well known classifiers such as Naive Bayes(Lui & Baldwin, 2012), SVM,(Jalam & Teytaud, 2001) and kernel methods(Kruengkrai et al., 2005) have been used in the literature.",
      "startOffset" : 124,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "People have also achieved excellent accuracy with lower–level features, such as byte sequence(Chew et al., 2011).",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "For example, researchers has found that people have been posting tweets using multiple languages for some time(Ling et al., 2013).",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : "For example, researchers has found that people have been posting tweets using multiple languages for some time(Ling et al., 2013). Lui et al. (2014) used topic modeling to model the languages that occur in a document.",
      "startOffset" : 111,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "For example, researchers has found that people have been posting tweets using multiple languages for some time(Ling et al., 2013). Lui et al. (2014) used topic modeling to model the languages that occur in a document. People have also used sequential labeling algorithms to identify language segments in a document, such as CRF by King & Abney (2013).",
      "startOffset" : 111,
      "endOffset" : 351
    }, {
      "referenceID" : 3,
      "context" : "Linguists have proposed syntactic theories that predicts the locations of code–switching(Sankoff, 1998; Belazi et al., 1994; Cantone & Müller, 2008), which at the same time prohibits their happening at certain places.",
      "startOffset" : 88,
      "endOffset" : 148
    }, {
      "referenceID" : 18,
      "context" : "Other frequently used, state-of-the-art machine learning models include Chained-Conditional Random Fields (Lafferty et al., 2001), Maximum-Entropy Markov Model (McCallum et al.",
      "startOffset" : 106,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : ", 2001), Maximum-Entropy Markov Model (McCallum et al., 2000), and sometimes Support Vector Machines (Kudo & Matsumoto, 2001).",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : "(2013) project 1-hot word vectors using a task-specific, supervised embedding layer, and experimented using both Elman-type (Elman, 1990) (Mikolov et al., 2010) and Jordan-type (Jordan, 1997) RNNs.",
      "startOffset" : 138,
      "endOffset" : 160
    }, {
      "referenceID" : 18,
      "context" : "Other frequently used, state-of-the-art machine learning models include Chained-Conditional Random Fields (Lafferty et al., 2001), Maximum-Entropy Markov Model (McCallum et al., 2000), and sometimes Support Vector Machines (Kudo & Matsumoto, 2001). More recently, researchers has also begun to explore using RNN architectures to rival, or even outperform such machine learning approaches. Mesnil et al. (2013) project 1-hot word vectors using a task-specific, supervised embedding layer, and experimented using both Elman-type (Elman, 1990) (Mikolov et al.",
      "startOffset" : 107,
      "endOffset" : 410
    }, {
      "referenceID" : 29,
      "context" : "All our experiments are conducted on the Twitter data provided by the EMNLP Code–switching Workshop (Solorio et al., 2014).",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : "And they proved to provide improvement in the code–switching task (Lin et al., 2014).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 25,
      "context" : "Skip–gram word embeddings Mikolov et al. (2013) is a log bilinear model that encourages words with similar contexts to have similar embeddings.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 29,
      "context" : "According to the overview paper of the workshop (Solorio et al., 2014), most teams in the shared task adapted the CRF model for language identification.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "The best Nepali–English submission Barman et al. (2014) used SVM as the classification framework.",
      "startOffset" : 35,
      "endOffset" : 56
    } ],
    "year" : 2017,
    "abstractText" : "Mixed language data is one of the difficult yet less explored domains of natural language processing. Most research in fields like machine translation or sentiment analysis assume monolingual input. However, people who are capable of using more than one language often communicate using multiple languages at the same time. Sociolinguists believe this ”code-switching” phenomenon to be socially motivated. For example, to express solidarity or to establish authority. Most past work depend on external tools or resources, such as part-of-speech tagging, dictionary look-up, or named-entity recognizers to extract rich features for training machine learning models. In this paper, we train recurrent neural networks with only raw features, and use word embedding to automatically learn meaningful representations. Using the same mixed-language Twitter corpus, our system is able to outperform the best SVM-based systems reported in the EMNLP’14 Code-Switching Workshop by 1% in accuracy, or by 17% in error rate reduction.",
    "creator" : "LaTeX with hyperref package"
  }
}