{"conference": "ICLR 2017 conference submission", "title": "Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks", "abstract": "We present an algorithm for policy search in stochastic dynamical systems using model-based reinforcement learning. The system dynamics are described with Bayesian neural networks (BNNs) that include stochastic input variables.  These input variables allow us to capture complex statistical patterns in the transition dynamics (e.g. multi-modality and heteroskedasticity), which are usually missed by alternative modeling approaches. After learning the dynamics, our BNNs are then fed into an algorithm that performs random roll-outs and uses stochastic optimization for policy learning. We train our BNNs by minimizing $\\alpha$-divergences with $\\alpha = 0.5$, which usually produces better results than other techniques such as variational Bayes. We illustrate the performance of our method by solving a challenging problem where model-based approaches usually fail and by obtaining promising results in real-world scenarios including the control of a gas turbine and an industrial benchmark.", "reviews": [{"is_meta_review": true, "comments": "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment\nand advocate for the $\\alpha$-divergence minimization rather than the more usual \nvariational Bayes. \n\nThe ability of alpha-divergence to capture bi-modality however \ncomes at a price and most of the paper is devoted to finding tractable approximations. \nThe authors therefore use the approach of Hernandez-Lobato\net al. (2016) as proxy to the alpha-divergence . \n\nThe environment/system dynamics is clearly defined as a well as the policy parametrization \n(section 3) and would constitute a useful reference point for other researchers. \nSimulated roll-outs, using the learned model, then provide samples of the expected \nreturn. Since a model of the environment is available, stochastic gradient descent \ncan be performed in the usual way, without policy gradient estimators, via automatic \ndifferentiation tools. \n\nThe experiments demonstrate that alpha-divergence is capable of capturing multi-model \nstructure which competing methods (variational Bayes and GP) would otherwise\nstruggle with. The proposed approach also compares favorably in a real-world\nbatch setting.\n\nThe paper is well-written, technically rich and combines many recent tools \ninto a coherent algorithm. However, the repeated use of approximations to original \nquantities seems to somehow defeat the benefits of the original problem formulation. \nThe scalability and computational effectiveness of this approach is also questionable \nand I am uncertain if many problem would warrant such complexity in their solution. \nAs with other Bayesian methods, the proposed approach would probably shine in low-samples \nregime and in this case might be preferable to other methods in the same class (VB, GP).", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "Despite it's initial emphasis on policy search, this paper is really about a learning method for Bayesian neural networks, which it then uses in a policy search setting. Specifically, the authors advocate modeling a stochastic system using a BNN trained to minimize alpha-divergence with alpha=0.5 (this involves a great deal of approximation to make computationally tractable). They then use this in the policy search setting.\n \n The paper is quite clear, and proposes a nice approach to learning BNNs. The algorithmic impact honestly seems fairly minor (the idea of using different alpha-divergences instead of KL divergence has been considered many times in the content of general variational approximations), but combining this with the policy search setting, and reasonable examples of industrial control, together these all make this a fairly strong paper.\n \n Pros:\n + Nice derivation of alternative variational formulation (I'll still call it variational even though it uses alpha-divergence with alpha=0.5)\n + Good integration into policy search setting\n + Nice application to industrial control systems\n \n Cons:\n - Advance from the algorithmic standpoint seems fairly straightforward, if complicated to make tractable (variational inference using alpha-divergence is not a new idea)\n - The RL components here aren't particularly novel, really the novelty is in the learning", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Useful contribution to model-based policy search with Bayesian neural networks", "is_meta_review": false, "comments": "This paper introduces an approach for model-based control of stochastic dynamical systems with policy search, based on (1) learning the stochastic dynamics of the underlying system with a Bayesian deep neural network (BNN) that allows some of its inputs to be stochastic, and (2) a policy optimization method based on simulated rollouts from the learned dynamics. BNN training is carried out using \\alpha-divergence minimization, the specific form of which was introduced in previous work by the authors. Validation and comparison of the approach is undertaken on a simulated domain, as well as real-world scenarios. \n\nThe paper is tightly written, and easy to follow. Its approach to fitting Bayesian neural networks with \\alpha divergence is interesting and appears novel in this context. The resulting application to model-based control appears to have significant practical impact, particularly in light of the explainability that a system model can bring to specific decisions made by the policy. As such, I think that the paper brings a valuable contribution to the literature.\n\nThat said, I have a few questions and suggestions:\n\n1) In section 2.2, it should be explained how the random z_n input is used by the neural network: is it just concatenated to the other inputs and used as-is, or is there a special treatment?\n\n2) Moreover, much case is made for the need to have stochastic inputs, but only a scalar input seems to be provided throughout. Is this enough? How computationally difficult would providing stochastic inputs of higher dimensionality be?\n\n3) How important is the normality assumption in z_n? How is the variance \\gamma established?\n\n4) It is mentioned that the hidden layers of the neural network are made of rectifiers, but no further utilization of this fact is made in the paper. Is this assumption somehow important in the optimization of the alpha-divergence (beyond what we know about rectifiers to mitigate the vanishing gradient problem) ?\n\n5) Equation (3), denominator \\mathbf{y} should be \\mathbf{Y} ?\n\n6) Section 2.3: it would be helpful to have an overview or discussion of the computational complexity of training BNNs, to understand whether and when they can practicably be used.\n\n7) Between eq (12) and (13), a citation to the statement of the time embedding theorem would be helpful, as well as an indication of how the embedding dimension should be chosen.\n\n8) Figure 1: the subplots should have the letters by which they are referenced in the text on p. 7.\n\n9) In section 4.2.1, it is not clear if the gas turbine data is publicly available, and if so where. In addition more details should be provided, such as the dimensionality of the variables E_t, N_t and A_t.\n\n10) Perhaps the comparisons with Gaussian processes should include variants that support stochastic inputs, such as Girard et al. (2003), to provide some of the same modelling capabilities as what\u2019s made use of here. At least, this strand of work should be mentioned in Section 5.\n\n\nReferences:\n\nGirard, A., Rasmussen, C. E., Qui\u00f1onero Candela, J., & Murray Smith, R. (2003). Gaussian process priors with uncertain inputs-application to multiple-step ahead time series forecasting. Advances in Neural Information Processing Systems, 545-552.\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "28 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "no pre-review question", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "17 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Policy search using Bayesian NNs", "is_meta_review": false, "comments": "The authors propose a novel way of using Bayesian NNs for policy search in stochastic dynamical systems. Specifically, the authors minimize alpha-divergence with alpha=0.5 as opposed to standard VB. The authors claim that their method is the first model-based system to solve a 20 year old benchmark problem; I'm not very familiar with this literature, so it's difficult for me to assess this claim.\n\nThe paper seems technically sound. I feel the writing could be improved. The notation in sections 2-3 feels a bit dense and there are a lot of terminology / approximations introduced, which makes it hard to follow. The writing could be better structured to distinguish between novel contributions vs review of prior work. If I understand section 2.3 correctly, it's mostly a review of black box alpha divergence minimization. If so, it would probably make sense to move this to the appendix. \n\nThere was a paper at NIPS 2016 showing promising results using SGHMC for Bayesian optimization: \"Bayesian optimization with robust Bayesian neural networks\" by Springenberg et al. Could you comment on applicability of stochastic gradient MCMC (SGLD / SGHMC) for your setup?\n\nCan you comment on the computational complexity of the different approaches?\n\nSection 4.2.1: why can't you use the original data? in what sense is it fair to simulate data using another neural network? can you evaluate PSO-P on this problem?", "IS_META_REVIEW": false, "RECOMMENDATION": 6, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Clear problem formulation, scalability questionable ", "is_meta_review": false, "comments": "This paper considers the problem of model-based policy search. The authors \nconsider the use of Bayesian Neural Networks to learn a model of the environment\nand advocate for the $\\alpha$-divergence minimization rather than the more usual \nvariational Bayes. \n\nThe ability of alpha-divergence to capture bi-modality however \ncomes at a price and most of the paper is devoted to finding tractable approximations. \nThe authors therefore use the approach of Hernandez-Lobato\net al. (2016) as proxy to the alpha-divergence . \n\nThe environment/system dynamics is clearly defined as a well as the policy parametrization \n(section 3) and would constitute a useful reference point for other researchers. \nSimulated roll-outs, using the learned model, then provide samples of the expected \nreturn. Since a model of the environment is available, stochastic gradient descent \ncan be performed in the usual way, without policy gradient estimators, via automatic \ndifferentiation tools. \n\nThe experiments demonstrate that alpha-divergence is capable of capturing multi-model \nstructure which competing methods (variational Bayes and GP) would otherwise\nstruggle with. The proposed approach also compares favorably in a real-world\nbatch setting.\n\nThe paper is well-written, technically rich and combines many recent tools \ninto a coherent algorithm. However, the repeated use of approximations to original \nquantities seems to somehow defeat the benefits of the original problem formulation. \nThe scalability and computational effectiveness of this approach is also questionable \nand I am uncertain if many problem would warrant such complexity in their solution. \nAs with other Bayesian methods, the proposed approach would probably shine in low-samples \nregime and in this case might be preferable to other methods in the same class (VB, GP).\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Related work, model, assumptions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Revision of paper", "OTHER_KEYS": "Stefan Depeweg", "comments": "Dear Reviewers,\n\nwe uploaded a revision of the paper yesterday (27th of November):\n\nIn the original version, at training time, we sampled the latent\nvariables z from the prior. In the revision, we learn a posterior\napproximation q(z) for each z. That is, for each data point (x_i,y_i)\n\\in D we infer a posterior approximation z_i ~ N(\\mu_i,\\sigma_i).This\nis very similar to how the EM algorithm works. After training, the\nmodel is used in the same way as before by sampling z from the prior\nN(0,\\gamma).\n\nBecause of this changes we repeated the experiments (also in a more\nsparse data setting) and improved a bit the presentation, simplified\nwhen it was too complicated and made the paper more compact. We also\nreadjusted the focus of this work a bit, emphasising more its strength\nover standard neural networks and gaussian processes.", "IS_META_REVIEW": false, "DATE": "28 Nov 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Stefan Depeweg, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Finale Doshi-Velez, Steffen Udluft", "accepted": true, "id": ""}
