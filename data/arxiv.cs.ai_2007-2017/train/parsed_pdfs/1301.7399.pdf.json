{
  "name" : "1301.7399.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Constructing Situation Specific Belief Networks",
    "authors" : [ "Suzanne M. Mahoney" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper describes a process for constructing situation-specific belief networks from a knowledge base of network fragments. A situation-specific network is a minimal query complete network constructed from a knowledge base in response to a query for the probability distribution on a set of target variables given evidence and context variables. We present definitions of query completeness and situation-specific networks. We describe conditions on the knowledge base that guarantee query completeness. The relationship of our work to earlier work on KBMC is also discussed.\n1 INTRODUCTION\nThe first applications of belief networks were in domains that could be characterized by a relatively small and fixed number of variables, for which it was feasible to construct a fixed model in advance of problem solving. In more complex domains it is necessary to reason about a variable number of entities that may be related to each other in varied ways. It is also necessary to reason about and distinguish between multiple instances of a given complex pattern of entities and relationships. In such domains it is infeasible to construct a complete belief network encompassing all the situations one might encounter in problem solving. Knowledge-Based Model Construction (KBMC) is the process of constructing a model for a problem instance from a knowledge base representing generic domain entities and their interrelationships. A KBMC system includes a knowledge base, search operators for retrieving problem-relevant knowledge base elements, network construction operators, network evaluation operators, and model construction control mechanisms. Objectives for a KBMC system are to minimize costs of representation, retrieval, construction and evaluation, while providing accurate responses to queries.\nMost research in KBMC has focused on bottom-up model construction, in which a model is built up\nincrementally until a response to the query can be computed (Breese, 1990; Goldman and Charniak, 1993; Provan, 1993; Haddawy, 1994; Glesner and Koller, 1995). Another line of research concerns top-down model reduction, in which one prunes parts of a large model to obtain a smaller model from which the query response is computed (Lin and Druzdzel, 1997; Baker and Boult, 1991; Geiger et al., 1990; Shachter, 1986). This paper takes the bottom-up construction approach, but draws from results in the literature on top-down reduction.\nIn this paper we propose a model construction control strategy for producing situation-specific networks from a knowledge base of network fragments (Laskey and Mahoney, 1997). We give conditions on the knowledge base that guarantee completeness and consistency of the implicitly encoded probability model. We then relax the completeness condition and give conditions for the knowledge base to be query complete for a given query. A situation-specific network is a minimal (in a sense formally defined below) network sufficient to respond to a query for which the knowledge base is query complete. Our work extends and modifies earlier work on knowledge-based model construction. Our use of the object-oriented paradigm for both knowledge representation and construction provides the ability to represent abstract types with associated structure, methods, and inheritance. Network fragments may represent semantically meaningful chunks of knowledge, or commonly co-occurring parts of a belief neiwork that should be retrieved as a unit. Influence combination provides a parsimonious representation for local structure and a mechanism for hypothesis generation. We make use of nuisance nodes (Lin and Druzdzel, 1997) to simplify constructed networks. When the set of likely queries is known in advance, the knowledge base can be preprocessed in the background to marginalize over nuisance nodes and increase run time efficiency. Finally, we make use of context nodes to index the distributions retrieved from the knowledge base.\n2 BACKGROUND\n2.1 NETWORK FRAGMENTS\nNetwork fragments (Laskey and Mahoney, 1997) provide an object-oriented representation for probabilistic knowledge. A probability model is represented implicitly as a knowledge base of belief network fragments. Each fragment consists of a set of random variables connected by a fragment graph, together with information used to construct local distributions for variables. Variables in a fragment are classified as either resident or input. The information needed to define the distributions for random variables is carried in fragments in which they are resident.\nBoth fragments and random variables are objects organized in a type hierarchy, with associated structure and methods. Each random variable and fragment has a set of identifying attributes which, when specified, create a unique identifier for an instance of the random variable or fragment. There is a mapping from identifying attributes of the fragment to identifying attributes of its random variables. Identifying attributes play the role of variables in a logic programming language (to avoid confusion, we reserve the term variable for random variables).\nWhen a variable's probability distribution has local structure, it is often convenient to specify its distribution in several different fragments to be combined at run time into a full distribution. For example, the \"Disease\" node in a medical diagnosis system may be specified as a noisy-OR distribution with perhaps hundreds of input diseases. These input distributions could be organized into groups of related diseases, each represented as a separate fragment. At run-time there may be information available that rules out entire categories of diseases, requiring only a small percentage of the groups to be included in the final constructed model.\nWe use influence combination to represent local structure (Laskey and Mahoney, 1997). Random variables have an influence type with associated influence combination method. Enabling conditions for a given influence type specify restrictions on the influencing variables. For example, a noisy-OR variable must take only binary input variables. Each individual fragment uses an influence function to represent parameters of the influence combination method. For example, the trigger probability for a candidate cause in a noisy-OR distribution is returned by the influence function in the fragment where the cause-effect link is defined.\n2.2 TOP-DOWN NETWORK REDUCTION\nLin and Druzdzel (1997) considered the problem of pruning a large belief network down to a network sufficient to respond to a query specified as\nConstructing Situation Specific Belief Networks 371\nq = P(I I !.;;), where T is a set of target variables and E is a set of evidence variables. Lin and Druzdzel first reduce the network to a set of computationally relevant nodes using a combination of d-separation (Geiger, et a!, 1990) and barren node removal (Baker and Boult, 1990). Consider the graph of Figure 1, and consider the query P(IliD, where I and fi represent vectors or lists of nodes labeled with the respective letter. 1 In this network, the nodes D 1 through D6 can be removed because they are d-separated from the target nodes given the evidence nodes. The nodes B 1 through B5 are barren (that is, they have no descendants that are either target or evidence nodes). These nodes also can be removed without affecting the result of the query. When these nodes and their associated arcs are removed, the result is a computationally relevant network, or the minimal subnetwork from which the response to the query can be computed. An interesting property of the computationally relevant network is that all nodes must be evidence nodes, target nodes, or ancestors of one or more target nodes.\nThe next step in Lin and Druzdzel's graph reduction is the marginalization and removal of nuisance nodes. Lin and Druzdzel define a nuisance node as a node that is computational!£ relevant given the query, but is on no evidential trail between an evidence and a target node. In Figure 1, the nodes labeled N1 through N5 are nuisance nodes, as are D5 and D6 (which are also d separated from the target nodes given the evidence nodes). Nuisance nodes have the interesting property that the way in which they enter into computation of the query response does not depend on the evidence. Thus, they can be marginalized out and removed prior to declaring and propagating evidence without changing the result of the query.\nNuisance nodes in any belief network can be decomposed into nonoverlapping subgraphs, in which each subgraph contains only ancestors of a single node on an evidential trail. There are five such subgraphs in Figure 1: the nuisance tree containing nodes Nl and N2, the nuisance graph containing nodes N4 and N5, and three single-node nuisance trees containing N3, D5, and D6, respectively. Note that D5 and D6 qualify as both d-separated and nuisance nodes. The single node at the base of a nuisance tree or graph is called a nuisance anchor.\nBefore calculating the response to a query, Lin and Druzdzel marginalize the nuisance nodes into their nuisance anchors. Their experimental results show that the cost to reduce the network is more than offset by the\n1 By convention, we use an underscore to indicate a vector or list. We use uppercase letters to represent random variables. Lowercase letters represent states, or possible values of random variables. Boldface lowercase letters represent identifying attributes, which are used to quantify random variables. Greek letters represent ground symbols that denote specific instances of random variables. 2 An evidential trail between two sets of nodes is a minimal active undirected path from a node in one set to a node in the other.\n372 Mahoney and Laskey\nsavings in inference cost. If common queries are known in advance, a preprocessor can be run offline to compute and cache distributions for nuisance anchors in which nuisance nodes have been marginalized out. Such preprocessing can result in enormous savings in computation.\n3 SITUATION SPECIFIC NETWORKS\n3.1 DEFINITION\nBottom-up network construction incrementally builds a belief network to respond to a query. It is assumed that the knowledge base implicitly encodes a complete probability model that is never explicitly constructed.3 The knowledge base contains variables, objects that represent a random variables. The variable V(,!:) refers to a random variable with name V and identifying attributes :!:· Dependence between variables is represented by directed arcs in fragment graphs, which we call/inks.\nNl f2\nrequesting the probability distribution for a set of target random variable instances given the states of a set of context random variable instances and a set of evidence random variable instances. The symbols g, /}, and yfill the identifying attribute slots of the random variables I(:!:), .(:(!), and .!J.(J.), respectively. In response to the query, a belief network is constructed in a model workspace. The nodes in the constructed network refer to instances of random variables in the knowledge base and the arcs correspond to links in fragment graphs in the knowledge base.\n3 The full network may be infinite in domains with an indefinite and unbounded number of objects (Goldman and Charniak, 1993; Laskey and Mahoney, 1997).\nIn our framework, context plays a similar role to evidence in that queries are conditional on given values of context variables. Colloquially, the word context is usually used to refer to knowledge that remains fixed over a large class of queries for which evidence and targets may vary. Generally, the use of context serves to simplify inference by restricting attention to relevant portions of the model. We use context to index distributions to be retrieved. We are developing a general framework for reasoning with context. For this paper, we simply assume:\n1) The cross-product of the state spaces of the context variables is partitioned into a mutually exclusive and exhaustive set of contexts;\n2) Each network fragment in the knowledge base points to one or more of these contexts;\n3) The conditioning event for any query is consistent with exactly one of these contexts.\nConceptually, context variables can be treated as root variables in the belief network implicitly encoded in the knowledge base. If each query refers to exactly one context, then distributions for context variables need not be defined.\nDuring network construction, creation of random variable instances triggers the retrieval of network fragments in which the corresponding random variables are resident. The identifying attributes of the random variable in a retrieved fragment is unified with the values of the identifying attributes of the instance that triggered its retrieval. These identifying attribute values are also unified with the same identifying attributes appearing in other random variables in the fragment. This process creates new random variable instances, which triggers the retrieval of fragments in which they are resident.\nMany KBMC systems are limited to this process of instance creation and retrieval of model components unifying with existing instances (Haddawy, 1994; Breese, 1987). In such systems, the only ground symbols appearing in the constructed network are those in ·the evidence nodes for the query. Charniak and Goldman (1991; Goldman and Charniak, 1993) permit domain entities not mentioned in the triggering query to be hypothesized. For example, the mention of an action in a plan may trigger the system to hypothesize an agent to carry out the plan and a location where the plan is carried out, neither of which may be explicitly mentioned in the sentence being processed. We require this ability to hypothesize unmentioned entities. For example, a report of a surface-to-air missile (SAM) site may trigger the system to hypothesize other SAMs in the same battalion, a battalion command post, and a military asset that the site is defending. We include a brief discussion of hypothesis management, but defer a full treatment to a later paper.\nThe goal of network construction is to construct a minimal network, called a situation-specific network, sufficient to respond to a query.\nDefinition: Let Q = P(I(g)if;.(/lJ=f.(/lJ, E.(JD= g(jj) be a query. Let N be a Bayesian network whose nodes include the random variable instances I(g) and E.(i!.4 N is a situation-specific network if all nodes in N are either target nodes or belong to evidential trails between target and evidence nodes.\n3.2 COMPLETENESS\nA situation-specific network is constructed by retrieving fragments from the knowledge base, inserting symbols in place of quantified variables, and merging identical nodes. To ensure that constructing a situation-specific network is possible requires placing constraints on the knowledge base. Haddawy (1994) describes conditions on the knowledge base that guarantee an isomorphic mapping between queries and constructed Bayesian networks. The following conditions are slightly more general than Haddawy's. They are sufficient to guarantee that the knowledge base implicitly encodes a unique probability distribution conditional on each context.\n1) Every non-context random variable is resident in at least one fragment given each context.\n2) If a random variable is resident in more than one fragment in a given context, the enabling conditions for influence combination are met for the collection of fragments in which it is resident.\n3) Identifying attributes for a random variable in a fragment are also identifying attributes for its children. More precisely, the fragment identifying attribute that maps to an identifying attribute for a variable also maps to an identifying attribute for each of its children.\n4) The link graph for the knowledge base contains no directed cycles.\nConditions 1 and 2 guarantee that each non-context random variable has a local distribution. In model construction, input variables to instantiated fragments in the model workspace trigger retrieval of fragments in which they are resident. Condition 3 guarantees that all identifying attributes in the final situation-specific network will be assigned values. Condition 4 ensures that no cycles exist in the constructed network.\nA knowledge base satisfying conditions 1 through 4 is called strongly complete. Strong completeness implies that there is a unique probability distribution implicitly encoded in the knowledge base and that for any query the construction algorithm given below terminates in a response consistent with this distribution.\n4 Context nodes index distributions to retrieve and need not be included in the constructed network.\nConstructing Situation Specific Belief Networks 373\nIf Condition 3 is relaxed but 1, 2 and 4 are satisfied, the knowledge base is complete. When Condition 3 is violated, fragment retrieval and instantiation may leave identifying attributes unspecified in ancestors of the variable triggering retrieval. These identifying attributes must be specified to create the random variable instances for the situation-specific network. The values of these identifying attributes may be associated with existing instances of their corresponding variables, or new instances with different identifying attributes may be postulated. This process is called hypothesis management, and requires modification of the algorithm below (cf., Goldman and Charniak, 1993; Laskey and Mahoney, 1998).\nIf Condition 1 is relaxed there may be variables in the knowledge base for which no distribution is specified. If the resulting situation-specific network provides a unique response to a given query, then the knowledge base is query complete for that query.\n4 NETWORK CONSTRUCTION\n4.1 FRAGMENT RETRIEVAL\nThe network constructor responds to a query by generating a network in the model workspace using three basic operations: fragment retrieval, variable instantiation, and fragment combination.\nA fragment retrieval request specifies a variable, R(/2), and a context Qfj) = f.(fj). Fragments F are retrieved containing the resident variable R(£) and matching the context. The identifying attribute slots :! in R are set to values £2. In addition, the mapping between fragment identifying attributes and variable identifying attributes is used to fill all identifying attribute slots corresponding to ,!.5 After retrieval and variable instantiation, the fragment is merged with existing fragments in the model workspace by the graph union operation and the application of influence combination when R(£) is resident in multiple fragments.\nThe context of the fragment must be more general than the context of the request. Therefore, Qfj) must include the context variables for all retrieved fragments. Moreover, if a context variable D(/lJ=d(/lJ appears in the query, then the context pointed to by the fragment must contain the valued(,!).\n4.2 BOTTOM-UP CONSTRUCTION\nTo simplify the presentation of the network construction algorithm we assume initially that the model workspace is empty. Only minor modifications are required to extend the algorithm to cover incremental extension of existing models to respond to new queries.\n5 It is possible that the retrieved fragment will contain nodes with unspecified identifying attributes. Strong completeness ensures that nodes with unfilled identifying attributes will be barren after bottom up construction terminates. Our construction algorithm removes them automatically.\n374 Mahoney and Laskey\nSimple bottom-up construction builds a Bayesian network B upward from the evidence and target variables of a query, adding nodes recursively until all ancestors of the evidence variables have been added. The knowledge base KB of network fragments provides the distributions and links to the parents for all the variables. Construction makes use of the methods defined below. The Bayesian network is constructed in a model workspace MW. Methods used in the construction are denoted by italicized function names, with parameters enclosed in parentheses. The method Method() associated with object 0 is denoted by O.Method().\nInstantiate is a model workspace method for constructing nodes for a Bayesian network. It takes a variable, U(�) , and its identifying attributes, J and returns the corresponding node, U(J). The method has two optional parameters: a value u to be declared as evidence, and a local distribution ld. Another model workspace method is NewBayesNet(), which creates an empty Bayesian network object with no nodes or arcs.\nFindFragment is a knowledge base method for retrieving network fragments. It takes a variable instance, U(Q) and a context vector, f:..(f;J)=r;_(/}) as parameters and returns a network fragment instance, F U(g) in which U(Q) is a resident variable.\nBayesNet methods include: FindNode which takes either a variable or a node as a parameter, AddNode which takes a node as a parameter and JoinParent which takes parent node and child node as parameters and unifies a parent of the local distribution of the child node with the parent node. All three methods return a Boolean value indicating whether the operation was successful.\nNetworkFragment methods include GetLocal Distribution, GetParent, and GetldentifyingAttributes, all with obvious meanings.\nSimpleBottomUpConstruction is a model workspace method for Bayesian networks that takes a query, Q, and a knowledge base, KB, as parameters and returns a computationally relevant Bayesian network, B. The method operates as follows:\nGiven query Q: P(I(g)if;.(f;J)=r;_(/}),fi(iJ=g_(iJ):\n1) Initialize:\n0 B = MW.NewBayesNet()\n0 S=0, where S is a list of parent/child sets waiting to be added to the network\n2) For each C(/}) = c(/}), B.AddNode(Instantiate(C, fj, c(f;J), 0))\n3) For each target and evidence node U(Q) mentioned in the query:\n0 Add< { U(Q), Q/})=r;.(/}) }, 0> to S\n4) Until S is empty, remove <{ U(O), Q{J)=r;.({J)}, V(£)>, from S\n0 If not B.FindNode(U (Q)):\n• Fu(,5J = KB.FindFragment(U(Q), Q/})=r;.(/})).\n• ld = F U(g)·GetLocalDistribution( U(Q))\n• U(Q) = Instantiate( U, Q, u, ld).\n• B.AddNode(U(6)).\n• For each P = F u(8).GetParent(U(O))\n• Create { P(Jp), {;_(/})=r;.(/}) } , where Jp = F U(g).!dentifyingAttributes(P)\n• Add < { P(Jp), Q/})=r;.(/})} ,U(Q)> to S.\n0 B.JoinParent(U(J), V(.§))\n5) Return B.\nConsider the example modeled in Figure 1. The variables with the gray ellipses behind them are those that are not constructed by simple bottom-up construction. They include nodes that are d-separated from the target variables by the evidence variables; D1, D2, D3 and D4: and nodes normally eliminated by barren node removal; B1, B2, B3, B4 and B5. Note that some d-separated nodes remain in the constructed model; D5 and D6.\nSimple bottom up construction produces a network containing all computationally relevant nodes using only local knowledge about a variable's parents to guide search. Many network construction systems are based on variants of bottom up construction (Haddawy, 1994; Ngo, et a!, 1996; Breese, 1991). Given unlimited resources and a complete knowledge base, the constructed network obtains the same query response as the complete network.\n4.3 SITUATION-SPECIFIC NETWORKS\nAlthough a situation-specific network can be obtained from the computationally relevant network by removing nuisances nodes (Lin and Druzdzel, 1997), search and computation can be reduced by direct generation of a situation-specific network. This requires a means of identifying nuisance nodes for a query, as well as marginal distributions for nuisance anchors.\nNuisance node detection (Lin and Druzdzel, 1997) needs the graph of all of the ancestors of a group of target and evidence variables to determine which ancestors are nuisance nodes. This information, although nonlocal, requires only the graph of the computationally relevant network. We therefore build the query graph first and use it to guide the construction of the situation-specific network.\nExperience has shown that structural relationships are relatively static once they have been elicited. Therefore, storing graphs containing nonlocal\nConstructing Situation Specific Belief Networks 375\ninformation is likely to be relatively maintenance free. We use three strategies in combination for obtaining query graphs:\n1) Store query graphs for common queries;\n2) Store ancestor graphs for likely evidence and target variables. Take the graph union of the ancestor graphs for the target and evidence variables to form a predecessor graph. Then eliminate the nuisance nodes.\n3) Use simple bottom-up construction to construct the predecessor graph. Then eliminate the nuisance nodes.\nAn ancestor graph is the graph of a variable's ancestors. For each vertex in the graph, it specifies the variable name and identifying attribute type. An ancestor graph object has a method for consistently assigning a set of identifying attribute values to the vertices in the graph. The graph only includes ancestors whose identifying attribute values are a subset of those specified for the variable. Restricting the identifying attributes values to those of the variable eliminates the infinite regression backward in time or through space via temporal and spatial dependencies.\nA predecessor graph is the graph union of a set of ancestor graphs whose identifying attribute values have been set. Each vertex corresponding to an evidence or context variable is marked as evidence. Each vertex corresponding to a target variable is marked as target.\nA practical strategy is to implement all three approaches. Query graphs for common queries can be easily stored with the network fragments for target and/or evidence variables. A more general approach is needed for less common queries. The ancestor graph alternative allows one to flexibly construct a query graph for any arbitrary query given that each target and evidence variable's fragment caches an ancestor graph for the variable. Finally, the third\nparents are explicitly represented. For conditional probability tables, each possible combination of explicitly represented parents may have its own table. In general, the alternative distributions for a nuisance anchor may be obtained by constructing the nuisance nodes, conditioning on the remaining parents and performing inference. This approach may be performed in the background or during network construction. Once computed, results may be cached and reused.\nMaintaining query graphs, ancestor graphs and alternative distributions requires that a network fragment be notified when an ancestor fragment is modified. While ancestor graphs and alternative distributions may be used to efficiently construct situation-specific networks, the cost of maintenance is relatively high in knowledge bases that are regularly updated. For static portions of the knowledge base, the cost is mostly the storage cost.\nIn summary, constructing a situation-specific network from a set of fragments has the following steps: (1) construct a predecessor graph from stored ancestor graphs; (2) reduce the predecessor graph to the query graph by eliminating vertices of nuisance nodes; (3) using the query graph as a guide, instantiate the network fragments.\n4.4 EXAMPLE\nWe consider an example from the domain of military situation assessment. The fragments in our knowledge base contains represent the behavior of military units given environmental, mission, equipment and organizational constraints.\nExample query:\nUnit A is an artillery unit and its echelon is battalion. We receive a report that unit A is at location X at time t1. The report also states that A is moving. We believe\napproach always works if the knowledge base is query complete, although it is the most expensive approach. Variable Variable Name Identifying Attributes State\nAs Lin and Druzdzel (1997) have shown, caching alternative conditional probability distributions for a nuisance anchor is beneficial. The obvious place to cache alternative distributions is with the network fragment in which the nuisance anchor is resident. In conjunction with the influence functions for each parent variable, the influence combination method for a nuisance anchor takes into account the parents that are to be explicitly represented in the network. For a noisy-OR distribution, the leak probability may depend upon which\nType\nTarget\nTarget\nTarget\nContext\nContext\nContext\nContext\nContext\nEvidence\nEvidence\nEvidence\nEvidence\nEvidence Evidence\nActivity Unit A, Time t2\nLocation Suitability_ Location Y, Unit B, Time t1 Location Suitability Location Z, Unit B, Time t1\nUnit Type Unit A Artillery\nEchelon Unit A Battalion\nUnit Type UnitB Artillery\nEchelon UnitB Battery Weather Hard Rain\nLocation Report Location X, Unit A, Time t1 Seen Here Movement Report Unit A, Time t1 Moving\nCommunications Report Unit B, Time t1 Not Moving Unit-Unit Reporting 1-Unit A, 2-Unit B 2 is part of 1 Terrain Location Y Flat w. cover Terrain Location Z Flat and open\nTable 1 Query Specification\n376 Mahoney and Laskey\nthat there is a second military unit B operating in the same area. A communications report intercepted at time t1 indicates that unit B is probably stationary. If B is a battery in A's artillery battalion, how suitable are locations Y and Z for B? Given that it is raining hard, will A still be moving at time t2?\nTable 1 summarizes the query to the network constructor. It includes sets of target variables whose states are unknown and sets of context and evidence variables whose states are known. It specifies the identifying attributes including their values for all of the variables.\nWe know that Unit A is an artillery unit and its echelon is battalion. The constructor retrieves fragments for \"Unit Type\" and \"Echelon,\" in response to FindFragment requests. The \"Unit\" identifying attribute in each fragment is unified with the value A, and the nodes are marked as evidence.\nReport\"(Unit A, Location X, Time t1), and the context \"Unit Type\"(Unit A)=Artillery and \"Echelon\"(Unit A) = Battalion. The fragment is retrieved because: evidence variable \"Location Report\" is resident; the fragment identifying attributes, Location, Unit and Time, are a subset of those specified in the request; and the fragment context is more general than that of the request. The constructor obtains a copy of the ancestor graph of \"Location Report\" from LR and unifies the identifying attributes for \"Location Report\" with the corresponding identifying attributes for the ancestor graph. See Graph A in Figure 2. The ancestor graph includes an \"Activity\" variable only for t1 because only a single time is associated with a given ''Location Report\". The node \"Location Report\" is marked as evidence.\nThe report also states that A is moving. An intercepted communications report at time t1 indicates that Unit B is probably stationary. If B is a battery in A's artillery\nbattalion, how suitable are locations Y and Zfor B? Following the pattern of fragment retrieval succeeded\nby graph manipulation illustrated for \"Location Report,\" the constructor retrieves Fragments M, C, R, and LS. It retrieves the ancestor graphs and unifies\nthem with the values of the identifying attributes. In the case of LS, two copies of the ancestor graph are made, one for Location Y and another for Location z. Some of the resulting graphs are shown as Graphs B and C in Figure 2.\nGiven that it is raining hard, will A still be moving at time t2? This time the retrieval request is FindFragment for \"Activity\"(Unit A, Time tJ, Time t2), with context\nGraph B: Movement Unit A, Time t1\nWe receive a report that Unit A is at location X at time t1• The constructor retrieves fragment LR from the knowledge base in response to FindFragment. Its parameters are the variable \"Location\nGraph C: Communications Unit B, Time t1\nFigure 2 Ancestor Graphs\nConstructing Situation Specific Belief Networks 377\nand evidence nodes. The constructor proceeds in reverse order through an ordering for the query graph. This is because fragments for the leaf nodes are always required and a fragment may have several of the variables that need to be constructed. For example, see fragment M in Figure 4.\ndistribution Al with six\n\"Unit Type\"(Unit A)=Artillery and \"Echelon\"(Unit A)=Battery. Graph D of Figure 2 shows the ancestor graph for \"Activity\" with the appropriate identifying attributes. It has two \"Activity\" vertices and their parents because two times were specified. In this case the method for retrieving ancestor graphs produces a graph in which all the specified identifying attributes are present.\nThe final predecessor graph is shown in Figure 3. Removing the nuisance nodes using the algorithm of Lin and Druzdzel eliminates all of the \"Mission\" and \"Supported Unit Type\" nodes. These are the vertices above the dark line running through the graph.\nIn the final phase of network construction the constructor retrieves network fragments from the knowledge base, instantiates them, and adds them to the situation-specific network. The query graph guides the retrieval of the network fragments.\nparents for \"Activity\" represents its full distribution, while Local Distribution A2 with just three parents represents the distribution marginalized over three of the parents. The query graph tells the constructor which parents condition each instance of \"Activity.\" The parameters for the GetLocalDistribution method now include a list of the parent variables. Given the parent set, the method returns the proper distribution for the node.\nFigure 5 shows the constructed situation-specific\nFor most variables, the network constructor can read the fragment retrieval query from the query graph. The query graph provides the variable names, identifying attributes, marked evidence and context vertices, and contextual dependencies. The query itself provides the values for context\nLaskey, K.B. and S. M. Mahoney (1997)\nnetwork.\nAcknowledgements\nThe research reported in this paper was sponsored by DARPA and the U.S. Army Topographic Engineering Center under contract DACA76-93-0025 to Information Extraction and Transport, Inc. The authors thank Steve Langs of lET for many helpful discussions. In addition, we extend grateful acknowledgment to three anonymous reviewers for helpful comments and suggestions on an earlier version of this paper.\nReferences\nBaker, M. and T. Boult, (1990) Pruning Bayesian Networks for Efficient Computation. In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, General Electric, Cambridge, MA, pp. 257-264.\nBreese, John S. (1987) Knowledge Representation and Inference in Intelligent Decision Systems. Ph.D. dissertation, Department of Engineering-Economic Systems, Stanford University.\nChamiak, E. and Goldman, R (1991) A Probabilistic Model of Plan Recognition, Proceedings of the 9th National Conference on Artificial Intelligence (AAA191) pp. 160-165.\nGeiger, D., T. Verma, and J. Pearl.(1990) d-separation: From Theorems to Algorithms. In Uncertainty in Artificial Intelligence 5, North Holland, New York, pp. 139-148.\nGlesner, S. and D. Koller (1995) Constructing Flexible Dynamic Belief Networks from First-Order\nNetwork Fragments: Representing Knowledge for Constructing Probabilistic Models. In Geiger, D. and Shenoy, P. (eds) Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference, San Francisco, CA: Morgan Kaufmann.\nLaskey, K.B. and S.M. Mahoney (1998) Network Fragments for Knowledge-based Construction of Belief Networks, to appear in Proceedings of the AAA1 Symposium on Mixed-Initiative Reasoning.\nLin, Y. and M. J. Druzdzel (1997) Computational Advantages of Relevance Reasoning in Bayesian Belief Networks. In Geiger, D. and Shenoy, P. (eds) Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference, San Francisco, CA: Morgan Kaufmann. pp. 342-350.\nNgo, L., P. Haddawy, J. Helwig (1996) A Theoretical Framework for Context-Sensitive Temporal Probability Model Construction with Application to Plan Porjection, In Besnard, P. and Hanks, S.(eds) Uncertainty in Artificial Intelligence: Proceedings of the Eleventh Conference, San Francisco, CA. Morgan Kaufmann. pp. 419-426.\nProvan, G. M. (1993) Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams. In Uncertainty in Artificial Intelligence: Proceedings of the Ninth Conference. Morgan Kaufmann Publishers, San Mateo, CA, pp. 40-47.\nShachter, R. D. (1986) Evaluating Influence Diagrams Operations Research 34 pp. 871-882."
    } ],
    "references" : [ {
      "title" : "Pruning Bayesian Networks for Efficient Computation",
      "author" : [ "M. Baker", "T. Boult" ],
      "venue" : "In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, General Electric,",
      "citeRegEx" : "Baker and Boult,? \\Q1990\\E",
      "shortCiteRegEx" : "Baker and Boult",
      "year" : 1990
    }, {
      "title" : "Knowledge Representation and Inference in Intelligent Decision Systems",
      "author" : [ "Breese", "John S" ],
      "venue" : "Ph.D. dissertation,",
      "citeRegEx" : "Breese and S.,? \\Q1987\\E",
      "shortCiteRegEx" : "Breese and S.",
      "year" : 1987
    }, {
      "title" : "A Probabilistic Model of Plan Recognition",
      "author" : [ "E. Chamiak", "R Goldman" ],
      "venue" : "Proceedings of the 9th National Conference on Artificial Intelligence",
      "citeRegEx" : "Chamiak and Goldman,? \\Q1991\\E",
      "shortCiteRegEx" : "Chamiak and Goldman",
      "year" : 1991
    }, {
      "title" : "d-separation: From Theorems to Algorithms",
      "author" : [ "D. Geiger", "T. Verma" ],
      "venue" : "In Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Geiger et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 1990
    }, {
      "title" : "Constructing Flexible Dynamic Belief Networks from First-Order",
      "author" : [ "S. Glesner", "D. Koller" ],
      "venue" : null,
      "citeRegEx" : "Glesner and Koller,? \\Q1995\\E",
      "shortCiteRegEx" : "Glesner and Koller",
      "year" : 1995
    }, {
      "title" : "Network Fragments for Knowledge-based Construction of Belief Networks, to appear in Proceedings of the AAA1 Symposium on Mixed-Initiative Reasoning",
      "author" : [ "K.B. Laskey", "S.M" ],
      "venue" : "Mahoney",
      "citeRegEx" : "Laskey and S.M.,? \\Q1998\\E",
      "shortCiteRegEx" : "Laskey and S.M.",
      "year" : 1998
    }, {
      "title" : "Computational Advantages of Relevance Reasoning in Bayesian Belief Networks",
      "author" : [ "Y. Lin", "M.J. Druzdzel" ],
      "venue" : "Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference,",
      "citeRegEx" : "Lin and Druzdzel,? \\Q1997\\E",
      "shortCiteRegEx" : "Lin and Druzdzel",
      "year" : 1997
    }, {
      "title" : "A Theoretical Framework for Context-Sensitive Temporal Probability Model Construction with Application to Plan Porjection",
      "author" : [ "L. Ngo", "P. Haddawy", "J. Helwig" ],
      "venue" : null,
      "citeRegEx" : "Ngo et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Ngo et al\\.",
      "year" : 1996
    }, {
      "title" : "Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams",
      "author" : [ "G.M. Provan" ],
      "venue" : "In Uncertainty in Artificial Intelligence: Proceedings of the Ninth Conference",
      "citeRegEx" : "Provan,? \\Q1993\\E",
      "shortCiteRegEx" : "Provan",
      "year" : 1993
    }, {
      "title" : "Evaluating Influence Diagrams",
      "author" : [ "R.D. Shachter" ],
      "venue" : "Operations Research",
      "citeRegEx" : "Shachter,? \\Q1986\\E",
      "shortCiteRegEx" : "Shachter",
      "year" : 1986
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "incrementally until a response to the query can be computed (Breese, 1990; Goldman and Charniak, 1993; Provan, 1993; Haddawy, 1994; Glesner and Koller, 1995).",
      "startOffset" : 60,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "incrementally until a response to the query can be computed (Breese, 1990; Goldman and Charniak, 1993; Provan, 1993; Haddawy, 1994; Glesner and Koller, 1995).",
      "startOffset" : 60,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "Another line of research concerns top-down model reduction, in which one prunes parts of a large model to obtain a smaller model from which the query response is computed (Lin and Druzdzel, 1997; Baker and Boult, 1991; Geiger et al., 1990; Shachter, 1986).",
      "startOffset" : 171,
      "endOffset" : 255
    }, {
      "referenceID" : 3,
      "context" : "Another line of research concerns top-down model reduction, in which one prunes parts of a large model to obtain a smaller model from which the query response is computed (Lin and Druzdzel, 1997; Baker and Boult, 1991; Geiger et al., 1990; Shachter, 1986).",
      "startOffset" : 171,
      "endOffset" : 255
    }, {
      "referenceID" : 9,
      "context" : "Another line of research concerns top-down model reduction, in which one prunes parts of a large model to obtain a smaller model from which the query response is computed (Lin and Druzdzel, 1997; Baker and Boult, 1991; Geiger et al., 1990; Shachter, 1986).",
      "startOffset" : 171,
      "endOffset" : 255
    }, {
      "referenceID" : 6,
      "context" : "We make use of nuisance nodes (Lin and Druzdzel, 1997) to simplify constructed networks.",
      "startOffset" : 30,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Lin and Druzdzel first reduce the network to a set of computationally relevant nodes using a combination of d-separation (Geiger, et a!, 1990) and barren node removal (Baker and Boult, 1990).",
      "startOffset" : 167,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "Although a situation-specific network can be obtained from the computationally relevant network by removing nuisances nodes (Lin and Druzdzel, 1997), search and computation can be reduced by direct generation of a situation-specific network.",
      "startOffset" : 124,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Nuisance node detection (Lin and Druzdzel, 1997) needs the graph of all of the ancestors of a group of target and evidence variables to determine which ancestors are nuisance nodes.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "As Lin and Druzdzel (1997) have shown, caching alternative conditional probability distributions for a nuisance anchor is beneficial.",
      "startOffset" : 3,
      "endOffset" : 27
    } ],
    "year" : 2011,
    "abstractText" : "This paper describes a process for constructing situation-specific belief networks from a knowledge base of network fragments. A situation-specific network is a minimal query­ complete network constructed from a knowledge base in response to a query for the probability distribution on a set of target variables given evidence and context variables. We present definitions of query completeness and situation-specific networks. We describe conditions on the knowledge base that guarantee query completeness. The relationship of our work to earlier work on KBMC is also discussed.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}