{
  "name" : "1703.10669.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "QoS-Aware Multi-Armed Bandits",
    "authors" : [ "Lenz Belzner", "Thomas Gabor" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "I. INTRODUCTION We consider the problem of exploration and exploitation under uncertainty and QoS requirements. Imagine a smart factory control system that is able to provide potential reconfigurations in response to events of change, e.g. on failure detection of a particular machine. In quality critical settings, such a situation may result in downtime until QoS requirements have been reestablished. For example, a factory is required to create products with a guaranteed maximum error rate. At the same time, the confidence about this error rate should be built as fast as possible.\nOne way to enable verification of QoS requirements at runtime is by performing statistical model checking of the system by using a simulation of the system and its application domain [1]. Here, i.i.d. Monte Carlo simulations of system execution are performed until satisfaction or violation of a particular requirement has been proven up to a given confidence bound.\nGiven a set of potential reconfigurations in a new situation, QoS-aware automated runtime verification pursues two goals.\n1) Identify a configuration satisfying QoS requirements. 2) Maximize the confidence about this configuration. This problem yields an exploration vs. exploitation tradeoff: Once a promising configuration has been identified, confidence about its quality should be maximized. However, the system is also interested in configurations with higher quality than the current promising candidate (because QoS confidence can be established faster for these configurations).\nMulti-armed bandits (MAB) provide a well-studied formal framework for studying exploration vs. exploitation in decision making [2]. In this paper, we will outline an approach to QoSaware decision making in the MAB framework. In Section II, we will formally describe the MAB framework and Thompson\n2016 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1109/FAS-W.2016.36\nsampling, a baseline for MAB decision making based on Bayesian statistics. In Section III, we will describe how to perform QoS-aware Thompson sampling."
    }, {
      "heading" : "II. MULTI ARMED BANDITS",
      "text" : "A multi-armed bandit is a set of distributions (e.g. of quality, payoff, utility, etc.). For simplicity, we restrict ourselves to Bernoulli distributions, which return a value of one with a probability of p, and zero otherwise.\nA typical task is to identify the optimal arm while maximizing its payoff at the same time. In the case of Bernoulli bandits, the optimal arm i is the one with maximal pi. A state-of-the-art baseline approach to the bandit problem is Thompson sampling [3], [4]. It builds a distribution about possible values pi for each arm i, representing the decision makers uncertainty (or beliefs) about the distribution parameter based on its observations.\nFor Bernoulli bandits, a convenient choice for modeling parameter uncertainty is the Beta distribution with parameters α and β [5]. It is the conjugate prior of the Bernoulli distribution, allowing for efficient posterior computation and analysis [5]. Given an arm i with si successes (i.e. si times reward one was observed) and fi failures, and assuming a uniform distribution as prior, the posterior distribution about pi is given by Beta(si + 1, fi + 1).\nThompson sampling is outlined in Algorithm 1. First, potential values for pi of each arm are sampled from the current belief distributions. Then, the arm with the best sample is played, and its observed outcomes are updated, effectively changing the belief about its parameter.\nAlgorithm 1 Thompson sampling for Bernoulli bandits. 1: procedure THOMPSON SAMPLING 2: Sample p̂i from Beta(si + 1, fi + 1) for each arm i 3: Play arm i with max p̂i 4: Update si or fi according to result\nDespite its simplicity, Thompson sampling has recently attracted research interest due to its theoretical properties and empirical success, showing comparable performance to other state-of-the-art bandit approaches such as UCB [3], [4].\nIn the context of QoS assessment, each configuration would be represented by an arm of the bandit. The arm’s probability of success is the probability that a simulation run of the given\nar X\niv :1\n70 3.\n10 66\n9v 1\n[ cs\n.L G\n] 2\n8 Fe\nb 20\n17\nconfiguration satisfies the QoS requirements. Thompson sampling provides a strategy to identify the optimal configuration wrt. QoS.\nHowever, in situations where it is not necessary to identify the optimal configuration, but rather a configuration that satisfies some QoS requirement with high confidence, standard Thompson sampling tends to put too much effort into optimization, and misses to build confidence in already promising candidate arms. We will outline a solution approach to this problem in the following."
    }, {
      "heading" : "III. QOS-AWARE THOMPSON SAMPLING",
      "text" : "A basic form of QoS-aware Thompson sampling (QATS) can be realized by determining the probabilities of QoS violation and satisfaction from the arms’ belief distributions. In fact, we are interested in the probability pv = P (X ≤ q) of the true parameter violating the QoS requirement q ∈ [0, 1]. This property can easily be determined from the cumulative density function of the belief distribution.\nThe probability pu = P (X > p̂i) = 1− P (X ≤ p̂i) that a sampled probability p̂i from a belief distribution is underestimating the true parameter of an arm is also computable from the belief distribution’s cumulative density function.\nTo solve the exploration vs. exploitation dilemma in a QoSaware manner, QATS maximizes the odds of underestimation vs. QoS violation. In fact, we prefer large probabilities of underestimation (meaning our belief sample is defensive) while at the same time preferring arms that expose a low probability of QoS requirement violation.\no = pu pv\n(1)\nAlgorithm 2 QoS-aware Thompson sampling. 1: procedure QOS-AWARE THOMPSON SAMPLING 2: Sample p̂i from Beta(si + 1, fi + 1) for each arm i 3: Play arm with max oi wrt. p̂i and QoS requirement q 4: Update si or fi according to result\nQATS is shown in Algorithm 2. We tentatively compared QATS to classic Thompson sampling (TS) in synthetic experiments with promising preliminary results. As an example, consider a four-armed bandit with pi ∈ [0, 0.2], instantiated randomly uniform each run. The QoS requirement was set to q = 0.1. We evaluated the performance of QATS and TS for 1000 decisions. Figure 1 (top) shows the system’s average confidence about QoS satisfaction (i.e. 1−pv) of chosen arms. QATS (blue line) is more confident about QoS satisfaction of chosen arms than TS (orange line). We also measured the cumulative probability of choosing an arm violating the QoS requirement. QATS shows less risk to choose QoS-violating arms. Corresponding results are shown in Figure 1 (bottom)."
    }, {
      "heading" : "IV. CONCLUSION",
      "text" : "Motivated by runtime verification of QoS requirements in self-adaptive and self-organizing systems that are able to\nreconfigure their structure and behavior in response to runtime data, we proposed QoS-aware Thompson sampling (QATS) for multi-armed bandits. QATS is applicable in settings where QoS satisfaction of an arm has to be ensured with high confidence efficiently, rather than finding the optimal arm while minimizing regret.\nPreliminary experimental results are promising and encourage further research in the field of QoS-aware decision making. It would be interesting to investigate theoretical properties of QoS-aware decision making algorithms. Another direction would be to integrate risk measures (as in financial decision making) into Thompson sampling. See [6] for a similar approach based on frequentist confidence bounds. Also, QoSaware decision making could prove useful in sequential decision making, where one decision changes optimality/quality of subsequent decisions. See [7] for an application of Thompson sampling in Monte Carlo Tree Search. Integration of QoSawareness into the optimization procedure itself (e.g. the procedure that produces potential system reconfigurations) could allow for even more efficient QoS-aware decision making."
    }, {
      "heading" : "ACKNOWLEDGMENT",
      "text" : "The authors would like to thank Matthias Hölzl and Martin\nWirsing for insightful discussions."
    } ],
    "references" : [ {
      "title" : "Statistical runtime checking of probabilistic properties",
      "author" : [ "I. Lee", "O. Sokolsky", "J. Regehr" ],
      "venue" : "International Workshop on Runtime Verification. Springer, 2007, pp. 164–175.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "arXiv preprint arXiv:1204.5721, 2012.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An empirical evaluation of thompson sampling",
      "author" : [ "O. Chapelle", "L. Li" ],
      "venue" : "Advances in neural information processing systems, 2011, pp. 2249– 2257.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Analysis of thompson sampling for the multiarmed bandit problem.",
      "author" : [ "S. Agrawal", "N. Goyal" ],
      "venue" : "COLT,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Bayesian statistics: principles, models, and applications",
      "author" : [ "S.J. Press", "J.S. Press" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1989
    }, {
      "title" : "Exploration vs exploitation vs safety: Risk-aware multi-armed bandits.",
      "author" : [ "N. Galichet", "M. Sebag", "O. Teytaud" ],
      "venue" : "ACML,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Bayesian mixture modelling and inference based thompson sampling in monte-carlo tree search",
      "author" : [ "A. Bai", "F. Wu", "X. Chen" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 1646–1654.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "One way to enable verification of QoS requirements at runtime is by performing statistical model checking of the system by using a simulation of the system and its application domain [1].",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 1,
      "context" : "exploitation in decision making [2].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "A state-of-the-art baseline approach to the bandit problem is Thompson sampling [3], [4].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "A state-of-the-art baseline approach to the bandit problem is Thompson sampling [3], [4].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "For Bernoulli bandits, a convenient choice for modeling parameter uncertainty is the Beta distribution with parameters α and β [5].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "It is the conjugate prior of the Bernoulli distribution, allowing for efficient posterior computation and analysis [5].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "state-of-the-art bandit approaches such as UCB [3], [4].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "state-of-the-art bandit approaches such as UCB [3], [4].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "In fact, we are interested in the probability pv = P (X ≤ q) of the true parameter violating the QoS requirement q ∈ [0, 1].",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "See [6] for a similar approach based on frequentist confidence bounds.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "See [7] for an application of Thompson sampling in Monte Carlo Tree Search.",
      "startOffset" : 4,
      "endOffset" : 7
    } ],
    "year" : 2017,
    "abstractText" : "Motivated by runtime verification of QoS requirements in self-adaptive and self-organizing systems that are able to reconfigure their structure and behavior in response to runtime data, we propose a QoS-aware variant of Thompson sampling for multi-armed bandits. It is applicable in settings where QoS satisfaction of an arm has to be ensured with high confidence efficiently, rather than finding the optimal arm while minimizing regret. Preliminary experimental results encourage further research in the field of QoS-aware decision making.",
    "creator" : "LaTeX with hyperref package"
  }
}