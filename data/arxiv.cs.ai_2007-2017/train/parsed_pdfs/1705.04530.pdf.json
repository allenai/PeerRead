{
  "name" : "1705.04530.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Survey of Question Answering for Math and Science Problem",
    "authors" : [ "Arindam Bhattacharya" ],
    "emails" : [ "arindam@cse.iitd.ac.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we explore the progress we have made towards the goal of making a machine smart enough to pass the standardized test. We see the challenges and opportunities posed by the domain, and note that we are quite some ways from actually making a system as smart as a even a middle school scholar."
    }, {
      "heading" : "1 Introduction",
      "text" : "Humans intelligence is often measured by asking them questions and evaluating their answers. Starting from infancy, through school, college and career, humans are judged by evaluating answers that they give to questions posed to them. Such an approach is so widespread that it is possible for someone to believe that it is the only way to measure intelligence. Yet, for long we have tried to quantify artificial intelligence in a very different way.\nThe Turing test, proposed by Alan Turing in 1950 states that: if a system can exhibit conversational behaviour that is indistinguishable from that of a human during a conversation, that system could be considered intelligence (Turing, 1950). Since then it has been often critisized of being a poor measure for the purpose (Hayes and Ford, 1995). It has also since been proposed that standardized tests in mathemetics and science could\nbe a suitable measure for judging machine intelligence (Clark and Etzioni, 2016). To this end, there has been attempts to develop Question Answering systems aimed at the task of solving standardized tests of math and science problems.\nIn this paper, we will briefly explore the various dimesions along which attempts have been made to tackle both math and science problems. For mathematics, we will explore systems that tries to solve algebraic word problems and geometry problems. For science, we will explore systems that solves questions ranging from basic questions (e.g. definitional knowledge) to questions that require complex world knowledge, and even diagrams."
    }, {
      "heading" : "2 Question Answering and the Math/Science domain",
      "text" : "Question Answering (QA) is the task of generating or extracting an answer to a question posed in natural language. Modern QA systems can be divided into two broad paradigms (Jurafsky and Martin, 2016). The first paradigm, called text-based (or IR based) QA relies on large volumes of text. Given a question, it uses information retrieval methods on the available corpus and retrieves documents which contains the answer. The candidate answers are then extracted from the text and ranked. The second paradigm is knowledge based QA, where we create a semantic representation of the question and use it to query databases of facts.\nFocusing on science and mathematics as domain for QA presents certain unique challanges. Solving these problems require not only understanding question for which we levarage language processing, but also to maintain an internal representation of the problem and often carry out symbolic computation (Clark and Etzioni, 2016). They cannot be easily answered by information retrieval\nar X\niv :1\n70 5.\n04 53\n0v 1\n[ cs\n.A I]\n1 0\nM ay\n2 01\n7\nor knowledge based methods. Along with the challanges, the domain also provides us with certain unique oppotunities. Using standardized test provides us with questions that are graduated by difficulty and multifaceted in nature: different questions explore different types of knowledge (Schoenick et al., 2016)."
    }, {
      "heading" : "3 Question Answering for Science",
      "text" : "This section will focus on Science questions of standardized tests. The types of quesions include basic fact retrieval, inference and world knowlege, and diagrams. We also explore the state of the art in these types of quesions."
    }, {
      "heading" : "3.1 Dataset",
      "text" : "The standardized test for Science used for the QA tasks is the New York Regents Science Exams (NYSED). Following are examples of types of questions in the test:"
    }, {
      "heading" : "Basic Questions",
      "text" : "These questions are factoid type questions requiring definitional knowledge. Example:\n1. Which object is the best conductor of electricity? (A) a wax crayon (B) a plastic spoon (C) a rubber eraser (D) an iron nail\n2. The movement of soil by wind or water is called (A) condensation (B) evaporation (C) erosion (D) friction\nAn IR based QA system can address these questions.\nSimple Inference\nThese are questions which require simple inference over known facts to arrive at the answer. Example:\n1. Which example describes an organism taking in nutrients? (A) dog burying a bone (B) A girl eating an apple (C) An insect crawling on a leaf (D) A boy planting tomatoes in the garden\nAnswering this question requires knowledge that eating involves taking in nutrients, and that an apple contains nutrients."
    }, {
      "heading" : "More Complex World Knowledge",
      "text" : "These questions require deeper world knowledge and more advanced liguistic capabilities for the system to be able to understand questions and produce an answer. Example:\n1. A student riding a bicycle observes that it moves faster on a smooth road than on a rough road. This happens because the smooth road has (A) less gravity (B) more gravity (C) less friction (D) more friction\nTo answer this question the system must be aware of the fact that riding a bicycle implies moving it, and infer logically the path [smooth -¿ less friction -¿ faster movement]."
    }, {
      "heading" : "Diagram",
      "text" : "Questions with diagrams are quite common in these test. The diagrams includes sketchs, maps, graphs, tables etc. These often proves to be quite difficult for the QA systems. Example:\n1. Which letter in the diagram 3.1 points to the plant structure that takes in water and nutrients?"
    }, {
      "heading" : "3.2 Models",
      "text" : "Various approaches, ranging from a combination of Information Retrieval, Statistics and Inference to Integer Programming are employed to tackle the aforementioned challanges.\n(Daniel Khashabi et al., 2016) proposes a method for solving QA using Integer Linear Programming. Given semi-structured knowledge as\ntables, the QA problem is formulated as that of finding a desirable Support Graph (Fig. 2), which in turn is formulated as ILP.\nFigure 2: TableILP searches for the best support graph (chains of reasoning) connecting the question to an answer, in this case June.\nThe state of the art system, ARISTO (Clark et al., 2016) employs an ensemble of solvers that tackle the problem at various layers. The layers are: Information Retrieval solver, Pointwise Mutual Information solver, Support Vector Machine solver, RULE solver that contains hand coded rules, and an Integer Linear Programming solver. The system is shown in Fig. 3 On non-diagram, multiple choice science questions (NDMC), Aristo system currently scores on average 75% (4th grade), 63% (8th grade), and 41% (12th grade) on (previously unseen) NY Regents Science Exams (NDMC questions only, typically 4-way multiple choice). As can be seen, questions become considerably more challenging at higher grade levels. On a broader multi-state collection of 4th grade NDMC questions, Aristo scores 65% (unseen questions). Note that these are the ”easier” questions (no diagrams, multiple choice); other question types pose additional challenges as we have described. There are no good systems that can tackle the questions out of NDMC and no system to date comes even close to passing a full 4th grade science exam."
    }, {
      "heading" : "4 Question Answering for Mathematics",
      "text" : "Math questions cannot be solved by IR systems. The basic strategy in mathematics, especially arithmatic quesions, is to understand the problem and formulate an equation that can be calculated. Geometry questions poses difficulties because of their reliance on diagrams.\nFigure 3: Aristo uses five solvers, each using different types of knowledge, to answer multiple choice questions."
    }, {
      "heading" : "4.1 Dataset",
      "text" : "For algebra questions, the standardized tests of (NYSED) is used. For geometry, questions from SATs are used."
    }, {
      "heading" : "Algebraic Problems",
      "text" : "Algebraic problems are posed as stories that require language processions. Example:\n1. Molly owns the Wafting Pie Company. This morning, her employees used 816 eggs to bake pumpkin pies. If her employees used a total of 1339 eggs today, how many eggs did they use in the afternoon?\nSome of them requires world modelling as well, for example:\n1. Saras high school won 5 basketball games this year. They lost 3 games. How many games did they play in all?\n2. John has 8 orange balloons, but lost 2 of them. How many orange balloons does John have now?\nHere, for the first, the knowledge that game is either won or lost is needed. The second example also has “lost” but results in a subtraction problem rather the addition like the first."
    }, {
      "heading" : "Geometry Problems",
      "text" : "Geometry problems combine arithmatic and diagrammatic reasoning. Example: Fig.4"
    }, {
      "heading" : "4.2 Models",
      "text" : "One of the earliest attempt at solving algebraic word problems employed simple verb categorization (Hosseini et al., 2014). The model extracted\nthe verbs from the question and tried to formulate equations based on the verb category. The model is presented in Fig. 6.\nA more sophisticated (and current state of the art) system, ALGES (Koncel-Kedziorski et al., 2015) uses Integer Linear Programming to map the word problems into equation trees. In contrast to (Hosseini et al., 2014), which covered only +,−, ALGES covers +,−, ∗, /. They are also able to solve multi-sentence problem, unlike previous models. ALGES proceeds by extracting sets of ground truths from the text, and using ILP to form trees using them. Overview of the system is presented in Fig. 5. ALGES produces an accu-\nracy of 72%, which is a 53% error reduction over the previous best methods (verb categorization).\nGeomertry quesions poses much greater diffi-\nculty, as expected, and the state of art systems can only achieve a score of 49% in standard SAT. The initial work in this area aimed at aligning text with geometric diagrams (Seo et al., 2014). It achieved the goal in three steps: identifying the primitives in the figure by picking elements that maximized the pixel coverage, agreement between the primitives and textual elements, and maximizing the coherence of the elements. Fig. 7 shows the alignment achieved by the system.\nWhile (Seo et al., 2014) understands the diagram, it could not solve geometric problems. GeoS (Seo et al., 2015) builds on the previous model and can answer geometric questions. It works in two steps: first it uses GAligner and language processing to convert the diagram and question into logical expressions, and then, it uses satisfiability solver to deduce the answer. Fig. 8 summarizes the model."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have seen that the current state of the art is not good at solving standardized tests (the smartest AI could not pass high school). There is a long way to go for AI. The field of QA systems on standardized math and science questions is fairly nascent with room for improvment with e.g. neural models, especially for diagram based questions. But the lack of results may also indicate that the current trend of data driven AI may not be the one stop solution to all problems.\nWe started the discussion with how Turing test is no longer the best measure for intelligence and agreed with (Clark and Etzioni, 2016) that standardized tests would be a better judge of intelligence for machines, that is more in line with how we measure human intelligence. But if the system could pass the standardized test, would it be intelligent? (Weston et al., 2015) argues that standardized math and science tests can not test common sense, and hence not a true test for intelligence. Nonetheless, passing the test would be a landmark in the history of AI, as was the moment of passing Turing test."
    } ],
    "references" : [ {
      "title" : "My Computer is an Honor Student - but how Intelligent is it? Standardized Tests as a Measure of AI",
      "author" : [ "Peter Clark", "Oren Etzioni." ],
      "venue" : "AI Magazine .",
      "citeRegEx" : "Clark and Etzioni.,? 2016",
      "shortCiteRegEx" : "Clark and Etzioni.",
      "year" : 2016
    }, {
      "title" : "Question Answering via Integer Programming over SemiStructured Knowledge",
      "author" : [ "Tushar Khot Daniel Khashabi", "Ashish Sabharwal", "Peter Clark", "Oren Etzioni", "Dan Roth." ],
      "venue" : "IJCAI .",
      "citeRegEx" : "Khashabi et al\\.,? 2016",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2016
    }, {
      "title" : "Turing Test Considered Harmful",
      "author" : [ "P. Hayes", "K. Ford." ],
      "venue" : "IJCAI .",
      "citeRegEx" : "Hayes and Ford.,? 1995",
      "shortCiteRegEx" : "Hayes and Ford.",
      "year" : 1995
    }, {
      "title" : "Learning to Solve Arithmetic Word Problems with Verb Categorization",
      "author" : [ "Mohammad Javad Hosseini", "Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman." ],
      "venue" : "ACL .",
      "citeRegEx" : "Hosseini et al\\.,? 2014",
      "shortCiteRegEx" : "Hosseini et al\\.",
      "year" : 2014
    }, {
      "title" : "Speech and Language Processing",
      "author" : [ "Daniel Jurafsky", "James H. Martin" ],
      "venue" : null,
      "citeRegEx" : "Jurafsky and Martin.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jurafsky and Martin.",
      "year" : 2016
    }, {
      "title" : "Parsing Algebraic Word Problems into Equations",
      "author" : [ "Rik Koncel-Kedziorski", "Hannaneh Hajishirzi", "Ashish Sabharwal", "Oren Etzioni", "Siena Dumas Ang." ],
      "venue" : "ACL .",
      "citeRegEx" : "Koncel.Kedziorski et al\\.,? 2015",
      "shortCiteRegEx" : "Koncel.Kedziorski et al\\.",
      "year" : 2015
    }, {
      "title" : "New York Regents Science Exams",
      "author" : [ "NYSED." ],
      "venue" : "http://www. nysedregents.org/.",
      "citeRegEx" : "NYSED.,? 2014",
      "shortCiteRegEx" : "NYSED.",
      "year" : 2014
    }, {
      "title" : "Moving Beyond the Turing Test with the Allen AI Science Challenge",
      "author" : [ "Carissa Schoenick", "Peter Clark", "Oyvind Tafjord", "Peter Turney", "Oren Etzioni" ],
      "venue" : null,
      "citeRegEx" : "Schoenick et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Schoenick et al\\.",
      "year" : 2016
    }, {
      "title" : "Diagram Understanding in Geometry Questions",
      "author" : [ "Min Joon Seo", "Hannaneh Hajishirzi", "Ali Farhadi", "Oren Etzioni." ],
      "venue" : "AAAI .",
      "citeRegEx" : "Seo et al\\.,? 2014",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2014
    }, {
      "title" : "Solving Geometry Problems: Combining Text and Diagram Interpretation",
      "author" : [ "Minjoon Seo", "Hannaneh Hajishirzi", "Ali Farhadi", "Oren Etzioni", "Clint Malcolm." ],
      "venue" : "EMNLP .",
      "citeRegEx" : "Seo et al\\.,? 2015",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards AI-complete Question Answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merriënboer", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "arXiv preprint arXiv:1502.05698 .",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Since then it has been often critisized of being a poor measure for the purpose (Hayes and Ford, 1995).",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "It has also since been proposed that standardized tests in mathemetics and science could be a suitable measure for judging machine intelligence (Clark and Etzioni, 2016).",
      "startOffset" : 144,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "Modern QA systems can be divided into two broad paradigms (Jurafsky and Martin, 2016).",
      "startOffset" : 58,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "Solving these problems require not only understanding question for which we levarage language processing, but also to maintain an internal representation of the problem and often carry out symbolic computation (Clark and Etzioni, 2016).",
      "startOffset" : 210,
      "endOffset" : 235
    }, {
      "referenceID" : 7,
      "context" : "Using standardized test provides us with questions that are graduated by difficulty and multifaceted in nature: different questions explore different types of knowledge (Schoenick et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 193
    }, {
      "referenceID" : 3,
      "context" : "One of the earliest attempt at solving algebraic word problems employed simple verb categorization (Hosseini et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "A more sophisticated (and current state of the art) system, ALGES (Koncel-Kedziorski et al., 2015) uses Integer Linear Programming to map the word problems into equation trees.",
      "startOffset" : 66,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "In contrast to (Hosseini et al., 2014), which covered only +,−, ALGES covers +,−, ∗, /.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "The initial work in this area aimed at aligning text with geometric diagrams (Seo et al., 2014).",
      "startOffset" : 77,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "While (Seo et al., 2014) understands the diagram, it could not solve geometric problems.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "GeoS (Seo et al., 2015) builds on the previous model and can answer geometric questions.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "We started the discussion with how Turing test is no longer the best measure for intelligence and agreed with (Clark and Etzioni, 2016) that standardized tests would be a better judge of intelligence for machines, that is more in line with how we measure human intelligence.",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "But if the system could pass the standardized test, would it be intelligent? (Weston et al., 2015) argues that standardized math and science tests can not test common sense, and hence not a true test for intelligence.",
      "startOffset" : 77,
      "endOffset" : 98
    } ],
    "year" : 2017,
    "abstractText" : "Turing test was long considered the measure for artificial intelligence. But with the advances in AI, it has proved to be insufficient measure. We can now aim to measure machine intelligence like we measure human intelligence. One of the widely accepted measure of intelligence is standardized math and science test. In this paper, we explore the progress we have made towards the goal of making a machine smart enough to pass the standardized test. We see the challenges and opportunities posed by the domain, and note that we are quite some ways from actually making a system as smart as a even a middle school scholar.",
    "creator" : "LaTeX with hyperref package"
  }
}