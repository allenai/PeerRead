{
  "name" : "1705.04862.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Alfredo V. Clemente", "Humberto N. Castejón" ],
    "emails" : [ "alfredvc@stud.ntnu.no", "humberto.castejon@telenor.com", "arjun.chandra@telenor.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION AND RELATED WORK",
      "text" : "Incorporating deep learning models within reinforcement learning (RL) presents some challenges. Standard stochastic gradient descent algorithms for training deep learning models assume all training examples to be independent and identically distributed (i.i.d.). This constraint is often violated by RL agents, given the high correlation between encountered states. Additionally, when learning on-policy, the policy affects the distribution of encountered states, which in turn affects the policy, creating a feedback loop that may lead to divergence (Mnih et al., 2013). Two main attempts have been made to solve these issues. One is to store experiences in a large replay memory and employ off-policy RL methods (Mnih et al., 2013). Sampling the replay memory can break the state correlations, thus reducing the effect of the feedback loop. Another is to execute multiple asynchronous agents in parallel, each interacting with an instance of the environment independently of each other (Mnih et al., 2016). Both of these approaches suffer from different drawbacks; experience replay can only be employed with off-policy methods, while asynchronous agents can perform inconsistent parameter updates due to stale gradients1 and simultaneous parameter updates from different threads.\nParallel and distributed compute architectures have motivated innovative modifications to existing RL algorithms to efficiently make use of parallel execution. In the General Reinforcement Learning Architecture (Gorila) (Nair et al., 2015), the DQN (Mnih et al., 2015) algorithm is distributed across multiple machines. Multiple learners learn off-policy using experiences collected into a common replay memory by multiple actors. Gorila is shown to outperform standard DQN in a variety of\n1Gradients may be computed w.r.t. stale parameters while updates applied to a new parameter set.\nar X\niv :1\n70 5.\n04 86\n2v 2\n[ cs\n.L G\n] 1\n6 M\nAtari games, while only training for 4 days. The distribution of the learning process is further explored in (Mnih et al., 2016), where multiple actor-learners are executed asynchronously on a single machine. Each actor-learner holds its own copy of the policy/value function and interacts with its own instance of the environment. This allows for both off-policy, as well as on-policy learning. The actor-learners compute gradients in parallel and update shared parameters asynchronously in a HOGWILD! (Recht et al., 2011) fashion. The authors suggest that multiple agents collecting independent experiences from their own environment instances reduces correlation between samples, thereby improving learning. The asynchronous advantage actor-critic (A3C) algorithm (Mnih et al., 2016) was able to surpass the state of the art on the Atari domain at the time of publication, while training for 4 days on a single machine with 16 CPU cores. GA3C (Babaeizadeh et al., 2016) is a GPU implementation of A3C. It batches action selection and learning using queues. Actors sample from a shared policy by submitting a task to a predictor, which executes the policy and returns an action once it has accumulated enough tasks. Similarly, learning is performed by submitting experiences to a trainer, which computes gradients and applies updates once enough experiences have been gathered. If the training queue is not empty when the model is updated, the learning will no longer be on-policy, since the remaining experiences were generated by an old policy. This leads to instabilities during training, which the authors address with a slight modification to the weight updates.\nWe propose a novel framework for efficient parallelization of deep reinforcement learning algorithms, which keeps the strengths of the aforementioned approaches, while alleviating their weaknesses. Algorithms based on this framework can learn from hundreds of actors in parallel, similar to Gorila, while running on a single machine like A3C and GA3C. Having multiple actors help decorrelate encountered states and attenuate feedback loops, while allowing us to leverage the parallel architecture of modern CPUs and GPUs. Unlike A3C and Gorila, there is only one copy of the parameters, hence parameter updates are performed synchronously, thus avoiding the possible drawbacks related to asynchronous updates. Our framework has many similarities to GA3C. However, the absence of queues allows for a much more simpler and computationally efficient solution, while allowing for true on-policy learning and faster convergence to optimal policies. We demonstrate our framework with a Parallel Advantage Actor-Critic algorithm, that achieves state of the art performance in the Atari 2600 domain after only a few hours of training. This opens the door for much faster experimentation."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "Reinforcement learning algorithms attempt to learn a policy π that maps states to actions, in order to maximize the expected sum of cumulative rewards Rt = Eπ [∑∞ k=0 γ krt+k ]\nfor some discount factor 0 < γ < 1, where rt is the reward observed at timestep t. Current reinforcement learning algorithms represent the learned policy π as a neural network, either implicitly with a value function or explicitly as a policy function."
    }, {
      "heading" : "2.1 BATCHING WITH STOCHASTIC GRADIENT DESCENT",
      "text" : "Current reinforcement learning algorithms make heavy use of deep neural networks, both to extract high level features from the observations it receives, and as function approximators to represent its policy or value functions. Consider the set of input-target pairs S = {(x0, y0), (x1, y1), ...(xn, yn)} generated by some function f∗(x). The goal of supervised learning with neural networks is to learn a parametrized function f(x; θ) that best approximates function f∗. The performance of f is evaluated with the empirical loss\nL(θ) = 1 |S| ∑ s∈S l(f(xs; θ), ys) (1)\nwhere l(f(xs; θ), ys) is referred to as a loss function, and gives a quantitative measure of how good f is at modelling f∗. The model parameters θ are learned with stochastic gradient descent (SGD) by iteratively applying the update\nθi+1 ← θi − α∇θiL(θi)\nfor a learning rate α. In SGD, L(θ) is usually approximated with\nL̄(θ) = 1 |S′| ∑ s′∈S′ l(f(xs′ ; θ), ys′),\nwhere S′ ⊂ S is a mini-batch sampled from S. The choice of α and nS′ = |S′| presents a trade-off between computational efficiency and sample efficiency. Increasing nS′ by a factor of k increases the time needed to calculate∇θL̄ by a factor of k′, for k′ ≤ k, and reduces its variance proportionally to 1 k (Bottou et al., 2016). In order to mitigate the increased time per parameter update we can increase the learning rate to α′ for some α′ ≥ α. However there are some limits on the size of the learning rate, so that in general α′ ≤ kα (Bottou et al., 2016). The hyper-parameters α′ and k are chosen to simultaneously maximize kk′ and minimize L."
    }, {
      "heading" : "2.2 MODEL-FREE VALUE BASED METHODS",
      "text" : "Model-free value based methods attempt to model the Q-funciton q(st, at) = Eπ [∑∞ k=0 γ krt+k+1 ∣∣s = st, a = at] that gives the expected return achieved by being in state st taking action at and then following the policy π. A policy can be extracted from the Q-function with π(st) = arg maxa′ q(st, a′). DQN (Mnih et al., 2013) learns a function Q(s, a; θ) ≈ q(s, a) represented as a convolutional neural network with parameters θ. Model parameters are updated based on model targets provided by the Bellman equation\nq(s, a) = Es′ [ r + γmax\na′ q(s′, a′)|s, a\n] (2)\nto create the loss function\nL̄(θi) = (rt + γmax a′\nQ(st+1, a ′; θi)−Q(st, at; θi))2 (3)\nThe parameters θ are improved by SGD with the gradient\n∇θiL(θi) ≈ ∇θiL̄(θi) = − 1\n2\n( r + γmax\na′ Q(s′, a′; θi)−Q(s, a; θi)\n) ∇θiQ(s, a; θi) (4)"
    }, {
      "heading" : "2.3 POLICY GRADIENT METHODS",
      "text" : "Policy gradient methods (Williams, 1992) directly learn a parametrized policy π(a|s; θ). This is possible due to the policy gradient theorem (Sutton et al., 1999)\n∇θL(θ) = Es,a [ q(s, a)∇θ log π(a|s; θ) ] , (5)\nwhich provides an unbiased estimate of the gradient of the return with respect to the policy parameters. Sutton et al. (1999) propose an improvement upon the basic policy gradient update by replacing the Q function with the advantage function Aπ(s, a) = (q(s, a)− v(s)) where v(s) is the value function given by Eπ [∑∞ k=0 γ krt+k+1 ∣∣s = st]. When π is continuously differentiable, θ can be optimized via gradient ascent following\n∇θL(θ) = Es,a [( q(s, a)− v(s) ) ∇θ log π(a|s; θ) ] (6)\nMnih et al. (2016) learn an estimate V (s; θv) ≈ v(s) of the value function, with both V (s; θv) and π(a|s; θ) being represented as convolutional neural networks. Additionally, they estimate the Q-function with the n-step return estimate given by\nQ(n)(st, at; θ, θv) = rt+1 + ...+ γ n−1rt+n−1 + γ nV (st+n; θv) (7)\nwith 0 < n ≤ tmax for some fixed tmax. The final gradient for the policy network is given by\n∇θL(θ) ≈ ( Q(n)(st, at; θ, θv)− V (st; θv) ) ∇θ log π(at|st; θ) (8)\nThe gradient for the value network V is given by ∇θvL(θv) ≈ ∇θv [( Q(n)(st, at; θ, θv)− V (st; θv) )2] (9)\nSamples are generated by having an actor interact with an environment by following the policy π(st) and observing the next state st+1 and reward rt+1."
    }, {
      "heading" : "3 PARALLEL FRAMEWORK FOR DEEP REINFORCEMENT LEARNING",
      "text" : "We propose a general framework for deep reinforcement learning, where multiple actors can be trained synchronously on a single machine. A set of ne environment instances are maintained, where actions for all environment instances are generated from the policy. The architecture can be seen in Figure 1. The policy function can be represented implicitly, as in value based methods, or\nexplicitly as in policy gradient methods. As suggested in Mnih et al. (2016), by having multiple environments instances in parallel it is likely that they will be exploring different locations of the state space at any given time, which reduces the correlation of encountered states and helps stabilize training. This approach can be motivated as an on-line experience memory, where experiences are sampled from the distribution currently being observed from the environment, instead of sampling uniformly from previous experience.\nAt each timestep the master generates actions for all environment instances by sampling the current policy. Note, that the policy may be sampled differently for each environment. A set of nw workers then apply all the actions to the their respective environments in parallel, and store the observed experiences. The master then updates the policy with the observed experiences. This allows the evaluation and training of the policy to be batched, which can be efficiently parallelized, leading to significant speed improvements on modern compute architectures."
    }, {
      "heading" : "4 PARALLEL ADVANTAGE ACTOR CRITIC",
      "text" : "We used the proposed framework to implement a version of the n-step advantage actor-critic algorithm proposed by Mnih et al. (2016). This algorithm maintains a policy π(at|st; θ) and an estimate V (st; θv) of the value function, both approximated by deep neural networks. The parameters θ of the policy network (the actor) are optimized via gradient ascent following\n∇θ log π(at|st; θ)A(st, at; θ, θv) + β∇θH(π(se,t; θ)) (Sutton et al., 1999), where A(st, at; θ, θv) = Q(n)(st, at; θ, θv) − V (st; θv) is an estimate of the\nadvantage function, Q(n)(st, at; θ, θv) = ∑n−1 k=0 γ krt+k + γ nV (st+n; θv), with 0 < n ≤ tmax, is\nthe n-step return estimation and H(π(se,t; θ)) is the entropy of the policy π, which as suggested by Mnih et al. (2016) is added to improve exploration by discouraging premature convergence to suboptimal deterministic policies. The parameters θv of value network (the critic) are in turn updated via gradient descent in the direction of\n∇θv [( Q(n)(st, at; θ, θv)− V (st; θv) )2]\nIn the context of our framework, the above gradients are calculated using mini batches of experiences. For each of the ne environments, tmax experiences are generated, resulting in batches of size ne· tmax. The gradients∇πθ for the policy network and the gradients∇Vθ for the value function thus take the following form:\n∇πθ ≈ 1\nne · tmax ne∑ e=1 tmax∑ t=1 ( Q(tmax−t+1)(se,t, ae,t; θ, θv)−V (se,t; θv) ) ∇θ log π(ae,t|se,t; θ)+β∇θH(π(se,t; θ))\n(10)\n∇Vθv ≈ ∇θv 1\nne · tmax ne∑ e=1 tmax∑ t=1 ( Q(tmax−t+1)(se,t, ae,t; θ, θv)− V (se,t; θv) )2 (11)\nPseudocode for our parallel advantage actor-critic algorithm (PAAC) is given in Algorithm 1. As shown in the next section, PAAC achieves state of the art performance on the Atari 2600 domain in half of the time required by GA3C and in only one eigth of the time required by A3C. Note that, although we implement an actor-critic algorithm, this framework can be used to implement any other reinforcement learning algorithm."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "We evaluated the performance of PAAC in 12 games from Atari 2600 using the Atari Learning Environment (Bellemare et al., 2013). The agent was developed in Python using TensorFlow (Abadi et al., 2015) and all performance experiments were run on a computer with a 4 core Intel i7-4790K processor and an Nvidia GTX 980 Ti GPU."
    }, {
      "heading" : "5.1 EXPERIMENTAL SETUP",
      "text" : "To compare results with other algorithms for the Atari domain we follow the same pre-processing and training procedures as Mnih et al. (2016). Each action is repeated 4 times, and the per-pixel maximum value from the two latest frames is kept. The frame is then scaled down from 210× 160 pixels and 3 color channels to 84×84 pixels and a single color channel for pixel intensity. Whenever an environment is restarted, the state is reset to the starting state and between 1 and 30 no-op actions are performed before giving control to the agent. The environment is restarted whenever the final state of the environment is reached.\nAlgorithm 1 Parallel advantage actor-critic 1: Initialize timestep counter N = 0 and network weights θ, θv 2: Instantiate set e of ne environments 3: repeat 4: for t = 1 to tmax do 5: Sample at from π(at|st; θ) 6: Calculate vt from V (st; θv) 7: parallel for i = 1 to ne do 8: Perform action at,i in environment ei 9: Observe new state st+1,i and reward rt+1,i 10: end parallel for 11: end for 12: Rtmax+1 = { 0 for terminal st V (stmax+1; θ) for non-terminal st 13: for t = tmax down to 1 do 14: Rt = rt + γRt+1 15: end for 16: dθ = 1\nne·tmax\n∑ne i=1 ∑tmax t=1 (Rt,i − vt,i)∇θ log π(at,i|st,i; θ) + β∇θH(π(se,t; θ))\n17: dθv = 1ne·tmax ∑ne i=1 ∑tmax t=1 ∇θv (Rt,i − V (st,i; θv)) 2 18: Update θ using dθ and θv using dθv . 19: N ← N + ne · tmax 20: until N ≥ Nmax\nAs in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to jointly model the policy and the value functions. For the policy function, the output is a softmax with one node per action, while for the value function a single linear output node is used. Moreover, to compare the efficiency of PAAC for different model sizes, we implemented two variants of the policy and value convolutional network. The first variant, referred to as archnips, is the same architecture used by A3C FF (Mnih et al., 2016), which is a modified version of the architecture used in Mnih et al. (2013), adapted to an actor-critic algorithm. The second variant, archnature, is an adaptation of the architecture presented in Mnih et al. (2015). The networks were trained with RMSProp. The hyperparameters used to generate the results in Table 1 were nw = 8, ne = 32, tmax = 5, Nmax = 1.15 × 108, γ = 0.99, α = 0.0224, = 0.1, β = 0.01, and a discount factor of 0.99 for RMSProp. Additionally gradient clipping (Pascanu et al., 2012) with a threshold of 40 was used."
    }, {
      "heading" : "5.2 RESULTS",
      "text" : "The performance of PAAC with archnips and archnature was evaluated on twelve different Atari 2600 games, where agents were trained for 115 million skipped frames (460 million actual frames). The results and their comparison to Gorila (Nair et al., 2015), A3C (Mnih et al., 2016) and GA3C (Babaeizadeh et al., 2016) are presented in Table 1. After a few hours of training on a single computer, PAAC is able to outperform Gorila in 8 games, and A3C FF in 8 games. Of the 9 games used to test GA3C, PAAC matches its performance in 2 of them and surpasses it in the remaining 7.\nTo better understand the effect of the number of actors (and batch size) on the score, tests were run with ne ∈ {16, 32, 64, 128, 256}. The learning rate was not tuned for each batch size, and was\nchosen to be 0.0007 · ne for all runs across all games. Increasing ne decreases the frequency of parameter updates, given that parameter updates are performed every ne ·tmax timesteps. As the theory suggests, the decreased frequency in parameter updates can be offset by increasing the learning rate. As can be seen in Figure 3 most choices of ne result in similar scores at a given timestep, however Figure 4 shows that higher values of ne reach those timesteps significantly faster. The choice of ne = 256 results in divergence in three out of the four games, which shows that the learning rate can be increased proportional to the batch size, until a certain limit is reached. A limiting factor\nin the speed of training is the time spent in agent-environment interaction. When using archnips for ne = 32 approximately 50% of the time is spent interacting with the environment, while only 37% is used for learning and action selection, as is shown in Figure 2. This has strong implications for the models and environments that can be used. Using a model-environment combination that doubles the time needed for learning and action calculation would lead to a mere 37% increase in training time. This can be seen in Figure 2 where using archnature on the GPU leads to a drop in timesteps per second 22% for ne = 32 when compared to archnips. When running on the CPU however this leads to a 41% drop in timesteps per second."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this work, we have introduced a parallel framework for deep reinforcement learning that can be efficiently parallelized on a GPU. The framework is flexible, and can be used for on-policy and offpolicy, as well as value based and policy gradient based algorithms. The presented implementation of the framework is able to reduce training time for the Atari 2600 domain to a few hours, while maintaining state-of-the-art performance. Improvements in training time, will allow the application of these algorithms to more demanding environments, and the use of more powerful models."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "Abadi", "Martın", "Agarwal", "Ashish", "Barham", "Paul", "Brevdo", "Eugene", "Chen", "Zhifeng", "Citro", "Craig", "Corrado", "Greg S", "Davis", "Andy", "Dean", "Jeffrey", "Devin", "Matthieu" ],
      "venue" : "Software available from tensorflow. org,",
      "citeRegEx" : "Abadi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Abadi et al\\.",
      "year" : 2015
    }, {
      "title" : "GA3C: GPU-based A3C for Deep Reinforcement Learning",
      "author" : [ "M. Babaeizadeh", "I. Frosio", "S. Tyree", "J. Clemons", "J. Kautz" ],
      "venue" : null,
      "citeRegEx" : "Babaeizadeh et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Babaeizadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "Journal of Artificial Intelligence Research, 47:253–279,",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Optimization Methods for Large-Scale Machine Learning",
      "author" : [ "L. Bottou", "F.E. Curtis", "J. Nocedal" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Bottou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bottou et al\\.",
      "year" : 2016
    }, {
      "title" : "Playing Atari with Deep Reinforcement Learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Asynchronous Methods for Deep Reinforcement Learning",
      "author" : [ "V. Mnih", "A. Puigdomènech Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Humanlevel control through deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Massively Parallel Methods for Deep Reinforcement Learning",
      "author" : [ "A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A. De Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver" ],
      "venue" : null,
      "citeRegEx" : "Nair et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nair et al\\.",
      "year" : 2015
    }, {
      "title" : "On the difficulty of training Recurrent Neural Networks",
      "author" : [ "R. Pascanu", "T. Mikolov", "Y. Bengio" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2012
    }, {
      "title" : "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "Recht", "Benjamin", "Re", "Christopher", "Wright", "Stephen", "Niu", "Feng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Recht et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2011
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Williams", "Ronald J" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Williams and J.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams and J.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Additionally, when learning on-policy, the policy affects the distribution of encountered states, which in turn affects the policy, creating a feedback loop that may lead to divergence (Mnih et al., 2013).",
      "startOffset" : 185,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "One is to store experiences in a large replay memory and employ off-policy RL methods (Mnih et al., 2013).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "Another is to execute multiple asynchronous agents in parallel, each interacting with an instance of the environment independently of each other (Mnih et al., 2016).",
      "startOffset" : 145,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "In the General Reinforcement Learning Architecture (Gorila) (Nair et al., 2015), the DQN (Mnih et al.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : ", 2015), the DQN (Mnih et al., 2015) algorithm is distributed across multiple machines.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "The distribution of the learning process is further explored in (Mnih et al., 2016), where multiple actor-learners are executed asynchronously on a single machine.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "The actor-learners compute gradients in parallel and update shared parameters asynchronously in a HOGWILD! (Recht et al., 2011) fashion.",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "The asynchronous advantage actor-critic (A3C) algorithm (Mnih et al., 2016) was able to surpass the state of the art on the Atari domain at the time of publication, while training for 4 days on a single machine with 16 CPU cores.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "GA3C (Babaeizadeh et al., 2016) is a GPU implementation of A3C.",
      "startOffset" : 5,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "Increasing nS′ by a factor of k increases the time needed to calculate∇θL̄ by a factor of k′, for k′ ≤ k, and reduces its variance proportionally to 1 k (Bottou et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "However there are some limits on the size of the learning rate, so that in general α′ ≤ kα (Bottou et al., 2016).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "DQN (Mnih et al., 2013) learns a function Q(s, a; θ) ≈ q(s, a) represented as a convolutional neural network with parameters θ.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "This is possible due to the policy gradient theorem (Sutton et al., 1999) ∇θL(θ) = Es,a [ q(s, a)∇θ log π(a|s; θ) ] , (5) which provides an unbiased estimate of the gradient of the return with respect to the policy parameters.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "This is possible due to the policy gradient theorem (Sutton et al., 1999) ∇θL(θ) = Es,a [ q(s, a)∇θ log π(a|s; θ) ] , (5) which provides an unbiased estimate of the gradient of the return with respect to the policy parameters. Sutton et al. (1999) propose an improvement upon the basic policy gradient update by replacing the Q function with the advantage function Aπ(s, a) = (q(s, a)− v(s)) where v(s) is the value function given by Eπ [∑∞ k=0 γ rt+k+1 ∣∣s = st].",
      "startOffset" : 53,
      "endOffset" : 248
    }, {
      "referenceID" : 4,
      "context" : "∇θL(θ) = Es,a [( q(s, a)− v(s) ) ∇θ log π(a|s; θ) ] (6) Mnih et al. (2016) learn an estimate V (s; θv) ≈ v(s) of the value function, with both V (s; θv) and π(a|s; θ) being represented as convolutional neural networks.",
      "startOffset" : 56,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "As suggested in Mnih et al. (2016), by having multiple environments instances in parallel it is likely that they will be exploring different locations of the state space at any given time, which reduces the correlation of encountered states and helps stabilize training.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "The parameters θ of the policy network (the actor) are optimized via gradient ascent following ∇θ log π(at|st; θ)A(st, at; θ, θv) + β∇θH(π(se,t; θ)) (Sutton et al., 1999), where A(st, at; θ, θv) = Q(st, at; θ, θv) − V (st; θv) is an estimate of the advantage function, Q(st, at; θ, θv) = ∑n−1 k=0 γ rt+k + γ V (st+n; θv), with 0 < n ≤ tmax, is",
      "startOffset" : 149,
      "endOffset" : 170
    }, {
      "referenceID" : 4,
      "context" : "4 PARALLEL ADVANTAGE ACTOR CRITIC We used the proposed framework to implement a version of the n-step advantage actor-critic algorithm proposed by Mnih et al. (2016). This algorithm maintains a policy π(at|st; θ) and an estimate V (st; θv) of the value function, both approximated by deep neural networks.",
      "startOffset" : 147,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "the n-step return estimation and H(π(se,t; θ)) is the entropy of the policy π, which as suggested by Mnih et al. (2016) is added to improve exploration by discouraging premature convergence to suboptimal deterministic policies.",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "We evaluated the performance of PAAC in 12 games from Atari 2600 using the Atari Learning Environment (Bellemare et al., 2013).",
      "startOffset" : 102,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "The agent was developed in Python using TensorFlow (Abadi et al., 2015) and all performance experiments were run on a computer with a 4 core Intel i7-4790K processor and an Nvidia GTX 980 Ti GPU.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "To compare results with other algorithms for the Atari domain we follow the same pre-processing and training procedures as Mnih et al. (2016). Each action is repeated 4 times, and the per-pixel maximum value from the two latest frames is kept.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to jointly model the policy and the value functions.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "The first variant, referred to as archnips, is the same architecture used by A3C FF (Mnih et al., 2016), which is a modified version of the architecture used in Mnih et al.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "Additionally gradient clipping (Pascanu et al., 2012) with a threshold of 40 was used.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to jointly model the policy and the value functions. For the policy function, the output is a softmax with one node per action, while for the value function a single linear output node is used. Moreover, to compare the efficiency of PAAC for different model sizes, we implemented two variants of the policy and value convolutional network. The first variant, referred to as archnips, is the same architecture used by A3C FF (Mnih et al., 2016), which is a modified version of the architecture used in Mnih et al. (2013), adapted to an actor-critic algorithm.",
      "startOffset" : 7,
      "endOffset" : 619
    }, {
      "referenceID" : 4,
      "context" : "As in (Mnih et al., 2016), a single convolutional network with two separate output layers was used to jointly model the policy and the value functions. For the policy function, the output is a softmax with one node per action, while for the value function a single linear output node is used. Moreover, to compare the efficiency of PAAC for different model sizes, we implemented two variants of the policy and value convolutional network. The first variant, referred to as archnips, is the same architecture used by A3C FF (Mnih et al., 2016), which is a modified version of the architecture used in Mnih et al. (2013), adapted to an actor-critic algorithm. The second variant, archnature, is an adaptation of the architecture presented in Mnih et al. (2015). The networks were trained with RMSProp.",
      "startOffset" : 7,
      "endOffset" : 759
    }, {
      "referenceID" : 7,
      "context" : "The results and their comparison to Gorila (Nair et al., 2015), A3C (Mnih et al.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : ", 2015), A3C (Mnih et al., 2016) and GA3C (Babaeizadeh et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : ", 2016) and GA3C (Babaeizadeh et al., 2016) are presented in Table 1.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "Gorila scores taken from Nair et al. (2015), A3C FF scores taken from Mnih et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "(2015), A3C FF scores taken from Mnih et al. (2016) and GA3C scores take from Babaeizadeh et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "(2016) and GA3C scores take from Babaeizadeh et al. (2016). Unavailable results are shown as N/A.",
      "startOffset" : 33,
      "endOffset" : 59
    } ],
    "year" : 2017,
    "abstractText" : "We propose a novel framework for efficient parallelization of deep reinforcement learning algorithms, enabling these algorithms to learn from multiple actors on a single machine. The framework is algorithm agnostic and can be applied to on-policy, off-policy, value based and policy gradient based algorithms. Given its inherent parallelism, the framework can be efficiently implemented on a GPU, allowing the usage of powerful models while significantly reducing training time. We demonstrate the effectiveness of our framework by implementing an advantage actor-critic algorithm on a GPU, using on-policy experiences and employing synchronous updates. Our algorithm achieves stateof-the-art performance on the Atari domain after only a few hours of training. Our framework thus opens the door for much faster experimentation on demanding problem domains. Our implementation is open-source and is made public at https://github.com/alfredvc/paac.",
    "creator" : "LaTeX with hyperref package"
  }
}