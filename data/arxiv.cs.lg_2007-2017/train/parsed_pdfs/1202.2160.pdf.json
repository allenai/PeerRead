{
  "name" : "1202.2160.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers",
    "authors" : [ "Clément Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image.\nThe scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average “purity” of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free.\nThe system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 × 240 image labeling in less than 1 second."
    }, {
      "heading" : "1. Overview",
      "text" : "Full scene labeling (FSL) is the task of labeling each pixel in a scene with the category of the object to which it belongs. FSL requires to solve the detection, segmentation, recognition and contextual integration problems simultaneously, so as to produce a globally consistent labeling. One of the obstacles to FSL is that the information necessary for the labeling of a given pixel may come from very distant pixels as well as their labels. The category of a pixel may depend on relatively short-range information (e.g. the presence of a human face generally indicates the presence of a human body nearby), as well as on very long-range dependencies (is this grey pixel part of a road, a building, or a cloud?).\nThis paper proposes a new method for FSL, depicted on Figure 1 that relies on five main ingredients: 1) Trainable, dense, multi-scale feature extraction: a multi-scale, dense feature extractor produces a series of feature vectors for regions of multiple sizes centered around every pixel in the image, covering a large context. The feature extractor is a two-stage convolutional network applied to a multi-scale contrast-normalized laplacian pyramid computed from the image. The convolutional network is fed with raw pixels and trained end to end, thereby alleviating the need for hand-engineered features; 2) Segmentation Tree: A graph over pixels is computed in which each pixel is connected to its 4 nearest neighbors through an edge whose weight is a measure of dissimilarity between the colors of the two pixels. A segmentation tree is then constructed using a classical region merging method, based on the minimum spanning tree of the graph. Each node in the tree corresponds to a potential image segment. The final image segmentation will be a judiciously chosen subset of nodes of the tree whose corresponding regions cover the entire image. 3) Region-\nar X\niv :1\n20 2.\n21 60\nv1 [\ncs .C\nV ]\n1 0\nwise feature aggregation: for each node in the tree, the corresponding image segment is encoded by a 5× 5 spatial grid of aggregated feature vectors. The aggregated feature vector of each grid cell is computed by a component-wise max pooling of the feature vectors centered on all the pixels that fall into the grid cell; This produces a scale-invariant representation of the segment and its surrounding; 4) Class histogram estimation: a classifier is then applied to the aggregated feature grid of each node. The classifier is trained to estimate the histogram of all object categories present in its input segments; 5) Optimal purity cover: a subset of tree nodes is selected whose corresponding segments cover the entire image. The nodes are selected so as to minimize the average “impurity” of the class distribution. The class “impurity” is defined as the entropy of the class distribution. The choice of the cover thus attempts to find a consistent overall segmentation in which each segment contains pixels belonging to only one of the learned categories.\nAll the steps in the process have a complexity linear (or almost linear) in the number of pixels. The bulk of the computation resides in the convolutional network feature extractor. The resulting system is very fast, producing a full parse of a 320 × 240 image in less than 1 second on a conventional CPU. Once trained, the system is parameter free, and requires no adjustment of thresholds or other knobs.\nThere are three key contributions in this paper 1) using a multi-scale convolutional net to learn good features for region classification; 2) using a class purity criterion to decide if a segment contains a single objet, as opposed to several objects, or part of an object; 3) an efficient procedure to obtain a cover that optimizes the overall class purity of a segmentation."
    }, {
      "heading" : "2. Related work",
      "text" : "The problem of scene parsing has been approached with a wide variety of methods in recent years. Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24]. Most methods rely on a pre-segmentation into super-pixels or other segment candidates [6, 13, 17, 24], and extract features and categories from individual segments and from various combinations of neighboring segments. The graphical model inference pulls out the most consistent set of segments that cover the image.\nSocher et al. [23] propose a method to aggregate segments in a greedy fashion using a trained scoring function. The originality of the approach is that the feature vector of the combination of two segments is computed from the feature vectors of the individual segments through a trainable function. Like us, they use “deep learning” methods to train their feature extractor. But unlike us, their feature extractor operates on hand-engineered features.\nOne of the main question in scene parsing is how to\ntake a wide context into account to make a local decision. Munoz et al. [17] proposed to use the histogram of labels extracted from a coarse scale as input to the labeler that look at finer scales. Our approach is somewhat simpler: our feature extractor is applied densely to an image pyramid. The coarse feature maps thereby generated are upsampled to match that of the finest scale. Hence with three scales, each feature vector has multiple fields which encode multiple regions of increasing sizes and decreasing resolutions, centered on the same pixel location.\nLike us, a number of authors have used trees to generate candidate segments by aggregating elementary segments, as in [22]. Using trees allows to rely on fast inference algorithms based on graph cuts or other methods. In this paper, we use an innovative method based on finding a set of tree nodes that cover the images while minimizing some criterion.\nOur system extracts features densely from a multiscale pyramid of images using a convolutional network (ConvNet) [14]. ConvNets can be fed with raw pixels and can automatically learn low-level and mid-level features, alleviating the need for hand-engineered features. One big advantage of ConvNets is the ability to compute dense features efficiently over large images. ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].\nThe only published work on using ConvNets for scene parsing is that of Grangier et al. [7]. While somewhat preliminary, their work showed that convolutional networks fed with raw pixels could be trained to perform scene parsing with decent accuracy. Unlike [7] however, our system uses a boundary-based over-segmentation to align the labels produced by the ConvNet to the boundaries in the image. Our system also takes advantage of the boundary-based oversegmentation to produce representations that are independent of the size of the segment through feature pooling."
    }, {
      "heading" : "3. An end-to-end trainable model for scene",
      "text" : "parsing\nThe model proposed in this paper, depicted on Figure 1, relies on two complementary image representations. In the first representation, the image is seen as a point in a high-dimensional space, and we seek to find a transform f : RP → RQ that maps these images into a space in which each pixel can be assigned a label using a simple linear classifier. This first representation typically suffers from two main problems: (1) the window considered rarely contains an object that is properly centered and scaled, and therefore offers a poor observation basis to predict the class of the underlying object, (2) integrating a large context involves increasing the grid size, and therefore the dimensionality P of the input; given a finite amount of training\ndata, it is then necessary to enforce some invariance in the function f itself. This is usually achieved by using pooling/subsampling layers, which in turn degrades the ability of the model to precisely locate and delineate objects. In this paper, f is implemented by a multiscale convolutional network, which allows integrating large contexts (as large as the complete scene) into local decisions, yet still remaining manageable in terms of parameters/dimensionality. This multiscale model, in which weights are shared across scales, allows the model to capture long-range interactions, without the penalty of extra parameters to train. This model is described in Section 3.1.\nIn the second representation, the image is seen as an edge-weighted graph, on which a hierarchy of segmentations/clusterings can be constructed. This representation yields a natural abstraction of the original pixel grid, and provides a hierarchy of observation levels for all the objects in the image. It can be used as a solution to the first problem exposed above: assuming the capability of assessing the quality of all the components of this hierarchy, a system can automatically choose its components so as to produce the best set of predictions. Moreover, these components are spatially accurate, and naturally delineate the underlying objects, as this representation conserves pixel-level precision. Section 3.2 describes our methodology."
    }, {
      "heading" : "3.1. Scale-invariant, scene-level feature extraction",
      "text" : "Our feature extractor is based on a convolutional network. Convolutional networks are natural extensions of neural networks, in which weights are replicated over space, or in other terms the linear transforms are done using 2D convolutions. A convolution can be seen as a linear transform with shared (replicated) weights. The use of weight sharing is justified by the fact that image statistics are stationary, and features and combinations of features that are relevant in one region of an image are also relevant in other regions. In fact, by enforcing this constraint, each layer of a convolutional network is explicitly forced to model features that are shift-equivariant. Because of the imposed weightsharing, convolutional networks have been used successfully for a number of image labeling problems.\nMore holistic tasks, such as full-scene understanding (pixel-wise labeling, or any dense feature estimation) require the system to model complex interactions at the scale of complete images, not simply within a patch. In this problem the dimensionality becomes unmanageable: for a typical image of 256×256 pixels, a naive neural network would require millions of parameters, and a naive convolutional network would require filters that are unreasonably large to view enough context.\nOur multiscale convolutional network overcomes these limitations by extending the concept of weight replication to the scale space. Given an input image I, a multiscale\npyramid of images Xs, ∀s ∈ {1, . . . , N} is constructed, with X1 being the size of I. The multiscale pyramid can be a Laplacian pyramid, and is typically pre-processed, so that local neighborhoods have zero mean and unit standard deviation. Given a classical convolutional network fs with parameters θs, the multiscale network is obtained by instantiating one network per scale s, and sharing all parameters across scales: θs = θ0, ∀s ∈ {1, . . . , N}.\nMore precisely, the output features are computed using the scaling/normalizing function gs as Xs = gs(I) for all s ∈ {1, . . . , N}. The convolutional network fs can then be described as a sequence of linear transforms, interspersed with non-linear symmetric squashing units (typically the tanh function): Fs = fs(Xs; θs) = WLHL−1, with Hl = tanh(WlHl−1+bl) for all l ∈ {1, . . . , L−1}, where Hl is the vector of hidden units at layer l, for a network with L layers, H0 = Xs and bl is a vector of bias parameters. The matrices Wl are Toeplitz matrices, and therefore each hidden unit vector Hl can be expressed as a regular convolution between the kernel wlpq and the previous hidden unit vector Hl−1\nHlp = tanh(blp + ∑\nq∈parents(p)\nwlpq ∗Hl−1,q). (1)\nThe filters wlpq and the biases bl constitute the trainable parameters of our model, and are collectively denoted θs.\nFinally, the output of the N networks are upsampled and concatenated so as to produce F, a map of feature vectors the size of F1, which can be seen as local patch descriptors and scene-level descriptors\nF = [F1, u(F2), . . . , u(FN )], (2)\nwhere u is an upsampling function. As mentioned above, weights are shared between networks fs. Intuitively, imposing complete weight sharing across scales is a natural way of forcing the network to learn scale invariant features, and at the same time reduce the chances of over-fitting. The more scales used to jointly train the models fs(θs) the better the representation becomes for all scales. Because image content is, in principle, scale invariant, using the same function to extract features at each scale is justified. In fact, we observed a performance decrease when removing the weight-sharing."
    }, {
      "heading" : "3.2. Parameter-free hierarchical parsing",
      "text" : "Predicting the class of a given pixel from its own feature vector is difficult, and not sufficient in practice. The task is easier if we consider a spatial grouping of feature vectors around the pixel, i.e. a neighborhood. Among all possible neighborhoods, one is the most suited to predict the pixel’s class. In Section 3.2.1 we propose to formulate the search\nfor the most adapted neighborhood as an optimization problem. The construction of the cost function that is minimized is then described in Section 3.2.2."
    }, {
      "heading" : "3.2.1 Optimal purity cover",
      "text" : "We define the neighborhood of a pixel as a connected component that contains this pixel. Let Ck, ∀k ∈ {1, . . . ,K} be the set of all possible connected components of the lattice defined on image I , and let Sk be a cost associated to each of these components. For each pixel i, we wish to find the index k∗(i) of the component that best explains this pixel, that is, the component with the minimal cost Sk∗(i):\nk∗(i) = argmin k | i∈Ck Sk (3)\nNote that components Ck∗(i) are non-disjoint sets that form a cover of the lattice. Note also that the overall cost S∗ = ∑ i Sk∗(i) is minimal.\nIn practice, the set of components Ck is too large, and only a subset of it can be considered. A classical technique to reduce the set of components is to consider a hierarchy of segmentations [18, 1, 8], that can be represented as a tree T . Solving Eq 3 on T can be done simply by exploring the tree in a depth-first search manner, and finding the component with minimal weight along each branch. Figure 2 illustrates the procedure."
    }, {
      "heading" : "3.2.2 Producing the confidence costs",
      "text" : "Given a set of components Ck, we explain how to produce all the confidence costs Sk. These costs represent the class purity of the associated components. Given the groundtruth segmentation, we can compute the cost as being the entropy of the distribution of classes present in the component. At test time, when no groundtruth is available, we need to define a function that can predict this cost by simply looking\nat the component. We now describe a way of achieving this, as illustrated in Figure 3.\nGiven the scale-invariant features F, we define a compact representation to describe objects as an elastic spatial arrangement of such features. In other terms, an object, or category in general, can be best described as a spatial arrangement of features, or parts. A simple attention function a is used to mask the feature vector map with each component Ck, producing a set of K masked feature vector patterns {F ⋂ Ck}, ∀k ∈ {1, . . . ,K}. The function a is called an attention function because it suppresses the background around the component being analyzed. The patterns {F ⋂ Ck} are resampled to produce fixed-size representations. In our model the sampling is done using an elastic max-pooling function, which remaps input patterns of arbitrary size into a fixed G×G grid. This grid can be seen as a highly invariant representation that encodes spatial relations between an object’s attributes/parts. This representation is denoted Ok. Some nice properties of this encoding are: (1) elongated, or in general ill-shaped objects, are nicely handled, (2) the dominant features are used to represent the object, combined with background subtraction, the features pooled represent solid basis functions to recognize the underlying object.\nOnce we have the set of object descriptors Ok, we define a function c : Ok → [0, 1]Nc (where Nc is the number of classes) as predicting the distribution of classes present in component Ck. We associate a cost Sk to this distribution. In this paper c is implemented as a simple 2-layer neural network, and Sk is the entropy of the predicted distribution. More formally, let xk be the feature vector associated with component Ok, d̂k the predicted class distribution, and Sk\nthe cost associated to this distribution. We have\nyk = W2 tanh(W1xk + b1), (4)\nd̂k,a = eyk,a∑\nb∈classes e yk,b\n, (5)\nSk = − ∑\na∈classes\nd̂k,a log(d̂k,a). (6)\nMatrices W1 and W2 are noted θc, and represent the trainable parameters of c. These parameters need to be learned over the complete set of hierarchies, computed on the entire training set available. The exact training procedure is described in Section 4."
    }, {
      "heading" : "4. Training procedure",
      "text" : "Let F be the set of all feature maps in the training set, and T the set of all hierarchies. Training the model described in Section 3 can be done in two steps. First, we train the low-level feature extractor fs in complete independence of the rest of the model. The goal of that first step is to produce features (F)F∈F that are maximally discriminative for pixelwise classification. Next, we construct the hierarchies (T )T∈T on the entire training set, and, for all T ∈ T train the classifier c to predict the distribution of classes in component Ck ∈ T , as well as the costs Sk. Once this second part is done, all the functions in Figure 1 are defined, and inference can be performed on arbitrary images. In the next two sections we describe these two steps."
    }, {
      "heading" : "4.1. Learning discriminative scale-invariant features",
      "text" : "As described in Section 3.1, feature vectors in F are obtained by concatenating the outputs of multiple networks fs, each taking as input a different image in a multiscale pyramid. Ideally a linear classifier should produce the correct categorization for all pixel locations i, from the feature vectors Fi. We train the parameters θs to achieve this goal. Let ci be the true target vector for pixel i and ĉi be the normalized prediction from the linear classifier, we set:\nLcat = ∑\ni∈pixels\nlcat(ĉi, ci), (7)\nlcat(ĉi, ci) = − ∑\na∈classes ci,a ln(ĉi,a), (8)\nĉi,a = ew T a Fi∑\nb∈classes e wTb Fi\n. (9)\nThe elementary loss function lcat(ĉi, ci) in Eq 7 is chosen to penalize the deviation of the multiclass prediction ĉi from the target vector ci. In this paper, we use the multiclass cross entropy loss function. In order to use this loss function, we compute a normalized predicted probability distribution over classes ĉi,a using the softmax function in Eq 9.\nThe cross entropy between the predicted class distribution and the target class distribution at a pixel location i is then measured by Eq 8. The true target probability ci,a of class a to be present at location i can either be a distribution of classes at location i, in a given neighborhood or a hard target vector: ci,a = 1 if pixel i is labeled a, and 0 otherwise. For training maximally discriminative features, we use hard target vectors in this first stage. Once the parameters θs are trained, we discard the classifier in Eq 9."
    }, {
      "heading" : "4.2. Teaching a classifier to find its best observation",
      "text" : "level\nGiven the trained parameters θs, we build F and T , i.e. we compute all the vector maps F and the hierarchies T on all the training data available, so as to produce a new training set of descriptors Ok. This time, the parameters θc of the classifier c are trained to minimize the KL-divergence between the true (known) distributions of labels dk in each component, and the prediction from the classifier d̂k (Eq 5):\nldiv = ∑\na∈classes dk,aln(\ndk,a d̂k,a ). (10)\nIn this setting, the groundtruth distributions dk are not hard target vectors, but normalized histograms of the labels present in component Ck. Once the parameters θc are trained, d̂k accurately predicts the distribution of labels, and Eq 6 can be used to assign a purity cost to the component."
    }, {
      "heading" : "5. Experiments",
      "text" : "We report results on three standard datasets. (1) The Stanford Background dataset, introduced in [6] for evaluating methods for semantic scene understanding. The dataset contains 715 images chosen from other existing public datasets so that all the images are outdoor scenes, have approximately 320 × 240 pixels, and contain at least one foreground object. We use the evaluation procedure introduced in [6], 5-fold cross validation: 572 images used for training, and 142 for testing. (2) The SIFT Flow dataset, as described in Liu et al. [15]. This dataset is composed of 2, 688 images that have been thoroughly labeled by LabelMe users. Liu et al. [15] have split this dataset into 2, 488 training images and 200 test images and used synonym correction to obtain 33 semantic labels. We use this same training/test split. (3) The Barcelona dataset, as described in Tighe et al. [24], is derived from the LabelMe subset used in [21]. It has 14, 871 training and 279 test images. The test set consists of street scenes from Barcelona, while the training set ranges in scene types but has no street scenes from Barcelona. Synonyms were manually consolidated by [24] to produce 170 unique labels.\nFor all experiments, we use a 2-stage convolutional network. The input I, a 3-channel image, is transformed into a\n16-dimension feature map, using a bank of 16 7 × 7 filters followed by tanh units; this feature map is then pooled using a 2× 2 max-pooling layer; the second layer transforms the 16-dimension feature map into a 64-dimension feature map, each component being produced by a combination of 8 7× 7 filters (512 filters), followed by tanh units; the map is pooled using a 2 × 2 max-pooling layer; finally the 64- dimension feature map is transformed into a 256-dimension feature map, each component being produced by a combination of 16 7× 7 filters (2048 filters).\nThe network is applied to a locally normalized Laplacian pyramid constructed on the input image. For these experiments, the pyramid consists of 3 rescaled versions of the input (N = 3), in octaves: 320× 240, 160× 120, 80× 60. All inputs are properly padded, and outputs of each of the 3 networks upsampled and concatenated, so as to produce a 256 × 3 = 768-dimension feature vector map F. The network is trained on all 3 scales in parallel.\nSimple grid-search was performed to find the best learning rate and regularization parameters (weight decay), using a holdout of 10% of the training dataset for validation. More regularization was necessary to train the classifier c. For both datasets, jitter was used to artificially expand the size of the training data, and ensure that the features do not overfit some irrelevant biases present in the data. Jitter includes: horizontal flipping of all images, and rotations between −8 and 8 degrees.\nIn this paper, the hierarchy used to find the optimal cover is a simple hierarchy constructed on the raw image gradient, based on a standard volume criterion [16, 4], completed by a removal of non-informative small components (less than 100 pixels). Classically segmentation methods find a partition of the segments rather than a cover. Partitioning the segments consists in finding an optimal cut in the tree (so that each terminal node in the pruned tree corresponds to a segment). We experimented with a number of graph cut methods to do so, including graph-cuts [5, 2], Kruskal [12] and Power Watersheds [3], but the results were systematically worse than with our optimal cover method.\nOn the Stanford dataset, we report two experiments: a baseline system, based on the multiscale convolutional network alone; and the full model as described in Section 3. Results are reported in Table 1. On the two other datasets, we report results for our complete model only, in Tables 2 and 3. Example parses on the SIFT Flow dataset are shown on Figure 4.\nBaseline, multiscale network: for our baseline, the multiscale network is trained as a simple class predictor for each location i, using the single classification loss Lcat defined in Eq 7. With this simple system, the pixelwise accuracy is surprisingly good, but the visual aspect of the predictions clearly suffer from poor spatial consistency, and poor object delineation.\nComplete system, network and hierarchy: in this second experiment, we use the complete model, as described in Section 3. The 2−layer neural network (Eq 4) has 3 × 3 × 3 × 256 = 6912 input units (using a 3 × 3 grid of feature vectors from F), 512 hidden units; and 8 output units are needed for the Stanford Background dataset, 33 for the SIFT Flow dataset, and 170 for the Barcelona dataset. Results are significantly better than the baseline method, in particular, much better delineation is achieved.\nFor the SIFT Flow dataset, we experimented with two sampling methods when learning the multiscale features: respecting natural frequencies of classes, and balancing them so that an equal amount of each class is shown to the network. Both results are reported in Table 2. Training with balanced frequencies allows better discrimination of small objects, and although it decreases the overall pixelwise accuracy, it is more correct from a recognition point of view. Frequency balancing was used on the Stanford Background dataset, as it consistently gave better results. For the Barcelona dataset, both sampling methods were used as well, but frequency balancing worked rather poorly in that case. This could be explained by the fact that this dataset has a large amount of classes with very few training examples. These classes are therefore extremely hard to model, and overfitting occurs much faster than for the SIFT Flow dataset. Results are shown on Table 3.\nResults in Table 1 also demonstrate the impressive computational advantage of convolutional networks over competing algorithms. Training time is also remarkably fast: results on the Stanford Background dataset were typically obtained in 24h on a regular server."
    }, {
      "heading" : "6. Discussion",
      "text" : "We introduced a discriminative framework for learning to identify and delineate objects in a scene. Our model does not rely on engineered features, and uses a multi-scale convolutional network operating on raw pixels to learn appro-\npriate low-level and mid-level features. The convolutional network is trained in supervised mode to directly produce labels. Unlike many other scene parsing systems that rely on expensive graphical models to ensure consistent labelings, our system relies on a segmentation tree in which the nodes (corresponding to image segments) are labeled with the entropy of the distribution of classes contained in the corresponding segment. Instead of graph cuts or other inference methods, we use the new concept of optimal cover to extract the most consistent segmentation from the tree.\nThe complexity of each operation is linear in the number of pixels, except for the production of the tree, which is quasi-linear (meaning cheap in practice). The system produces state-of-the-art accuracy on the Stanford Background, SIFT Flow, and Barcelona datasets (both measured per pixel, or averaged per class), while dramatically outperforming competing models in inference time.\nOur current system relies on a single segmentation tree constructed from image gradients, and implicitly assumes that the correct segmentation is contained in the tree. Future work will involve searches over multiple segmentation trees, or will use other graphs than simple trees to encode the possible segmentations (since our optimal cover algorithm can work from other graphs than trees). Other directions for improvements include the use of structured learning criteria such as Turaga et al.’s Maximin Learning [25] to learn low-level feature vectors from which better segmentation trees can be produced."
    } ],
    "references" : [ {
      "title" : "Contour Detection and Hierarchical Image Segmentation",
      "author" : [ "P. Arbeláez", "M. Maire", "C. Fowlkes", "J. Malik" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Interactive graph cuts for optimal boundary & region segmentation of objects in n-d images",
      "author" : [ "Y. Boykov", "M.P. Jolly" ],
      "venue" : "In Proceedings of International Conference of Computer Vision (ICCV),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Power Watersheds: A Unifying Graph Based Optimization Framework",
      "author" : [ "C. Couprie", "L. Grady", "L. Najman", "H. Talbot" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Incremental algorithm for hierarchical minimum spanning forests and saliency of watershed cuts",
      "author" : [ "J. Cousty", "L. Najman" ],
      "venue" : "In 10th International Symposium on Mathematical Morphology (ISMM’11),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "A simple algorithm for finding maximal network flows and an application to the hitchcock problem",
      "author" : [ "L.R. Ford", "D.R. Fulkerson" ],
      "venue" : "Technical report, RAND Corporation, Santa Monica,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1955
    }, {
      "title" : "Decomposing a scene into geometric and semantically consistent regions",
      "author" : [ "S. Gould", "R. Fulton", "D. Koller" ],
      "venue" : "IEEE 12th International Conference on Computer Vision,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Deep Convolutional Networks for Scene Parsing",
      "author" : [ "D. Grangier", "L. Bottou", "R. Collobert" ],
      "venue" : "In ICML 2009 Deep Learning Workshop,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Scale-sets image analysis",
      "author" : [ "L. Guigues", "J.P. Cocquerez", "H.L. Men" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "Learning hybrid models for image annotation with partially labeled data",
      "author" : [ "X. He", "R. Zemel" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Supervised learning of image restoration with convolutional networks",
      "author" : [ "V. Jain", "J.F. Murray", "F. Roth", "S. Turaga", "V. Zhigulin", "K. Briggman", "M. Helmstaedter", "W. Denk", "H.S. Seung" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "What is the best multi-stage architecture for object recognition",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun" ],
      "venue" : "In Proc. International Conference on Computer Vision (ICCV’09)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "On the shortest spanning subtree of a graph and the traveling salesman problem",
      "author" : [ "J.B. Kruskal" ],
      "venue" : "Proceedings of the American Mathematical Society,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1956
    }, {
      "title" : "Efficiently selecting regions for scene understanding",
      "author" : [ "M. Kumar", "D. Koller" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Gradientbased learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "Nonparametric scene parsing: Label transfer via dense scene alignment",
      "author" : [ "C. Liu", "J. Yuen", "A. Torralba" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Segmentation, minimum spanning tree and hierarchies",
      "author" : [ "F. Meyer", "L. Najman" ],
      "venue" : "Mathematical Morphology: from theory to application,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Stacked hierarchical labeling",
      "author" : [ "D. Munoz", "J. Bagnell", "M. Hebert" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Geodesic saliency of watershed contours and hierarchical segmentation",
      "author" : [ "L. Najman", "M. Schmitt" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1996
    }, {
      "title" : "Toward automatic phenotyping of developing embryos from videos",
      "author" : [ "F. Ning", "D. Delhomme", "Y. LeCun", "F. Piano", "L. Bottou", "P. Barbano" ],
      "venue" : "IEEE Trans. on Image Processing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Synergistic face detection and pose estimation with energy-based models",
      "author" : [ "M. Osadchy", "Y. LeCun", "M. Miller" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Object recognition by scene alignment",
      "author" : [ "B. Russell", "A. Torralba", "C. Liu", "R. Fergus", "W. Freeman" ],
      "venue" : "In Neural Advances in Neural Information,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Associative hierarchical CRFs for object class image segmentation",
      "author" : [ "C. Russell", "P.H.S. Torr", "P. Kohli" ],
      "venue" : "In Proc. ICCV,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "Parsing Natural Scenes and Natural Language with Recursive Neural Networks",
      "author" : [ "R. Socher", "C.C. Lin", "A.Y. Ng", "C.D. Manning" ],
      "venue" : "In Proceedings of the 26th International Confer-  ence on Machine Learning (ICML),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Superparsing: scalable nonparametric image parsing with superpixels",
      "author" : [ "J. Tighe", "S. Lazebnik" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Maximin affinity learning of image segmentation",
      "author" : [ "S. Turaga", "K. Briggman", "M. Helmstaedter", "W. Denk", "H. Seung" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 21,
      "context" : "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 12,
      "context" : "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 16,
      "context" : "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 23,
      "context" : "Many methods rely on MRFs, CRFs, or other types of graphical models to ensure the consistency of the labeling and to account for context [9, 22, 6, 13, 17, 24].",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "Most methods rely on a pre-segmentation into super-pixels or other segment candidates [6, 13, 17, 24], and extract features and categories from individual segments and from various combinations of neighboring segments.",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "Most methods rely on a pre-segmentation into super-pixels or other segment candidates [6, 13, 17, 24], and extract features and categories from individual segments and from various combinations of neighboring segments.",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "Most methods rely on a pre-segmentation into super-pixels or other segment candidates [6, 13, 17, 24], and extract features and categories from individual segments and from various combinations of neighboring segments.",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "Most methods rely on a pre-segmentation into super-pixels or other segment candidates [6, 13, 17, 24], and extract features and categories from individual segments and from various combinations of neighboring segments.",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 22,
      "context" : "[23] propose a method to aggregate segments in a greedy fashion using a trained scoring function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] proposed to use the histogram of labels extracted from a coarse scale as input to the labeler that look at finer scales.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "Like us, a number of authors have used trees to generate candidate segments by aggregating elementary segments, as in [22].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "Our system extracts features densely from a multiscale pyramid of images using a convolutional network (ConvNet) [14].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].",
      "startOffset" : 186,
      "endOffset" : 198
    }, {
      "referenceID" : 9,
      "context" : "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].",
      "startOffset" : 186,
      "endOffset" : 198
    }, {
      "referenceID" : 24,
      "context" : "ConvNets are best known for their applications to detection and recognition [20, 11], but they have also been used for image segmentation, particularly for biological image segmentation [19, 10, 25].",
      "startOffset" : 186,
      "endOffset" : 198
    }, {
      "referenceID" : 6,
      "context" : "[7].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "Unlike [7] however, our system uses a boundary-based over-segmentation to align the labels produced by the ConvNet to the boundaries in the image.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "A classical technique to reduce the set of components is to consider a hierarchy of segmentations [18, 1, 8], that can be represented as a tree T .",
      "startOffset" : 98,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "A classical technique to reduce the set of components is to consider a hierarchy of segmentations [18, 1, 8], that can be represented as a tree T .",
      "startOffset" : 98,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "A classical technique to reduce the set of components is to consider a hierarchy of segmentations [18, 1, 8], that can be represented as a tree T .",
      "startOffset" : 98,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "Once we have the set of object descriptors Ok, we define a function c : Ok → [0, 1]c (where Nc is the number of classes) as predicting the distribution of classes present in component Ck.",
      "startOffset" : 77,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "(1) The Stanford Background dataset, introduced in [6] for evaluating methods for semantic scene understanding.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "We use the evaluation procedure introduced in [6], 5-fold cross validation: 572 images used for training, and 142 for testing.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "[15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] have split this dataset into 2, 488 training images and 200 test images and used synonym correction to obtain 33 semantic labels.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24], is derived from the LabelMe subset used in [21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[24], is derived from the LabelMe subset used in [21].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "Synonyms were manually consolidated by [24] to produce 170 unique labels.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "In this paper, the hierarchy used to find the optimal cover is a simple hierarchy constructed on the raw image gradient, based on a standard volume criterion [16, 4], completed by a removal of non-informative small components (less than 100 pixels).",
      "startOffset" : 158,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "In this paper, the hierarchy used to find the optimal cover is a simple hierarchy constructed on the raw image gradient, based on a standard volume criterion [16, 4], completed by a removal of non-informative small components (less than 100 pixels).",
      "startOffset" : 158,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "We experimented with a number of graph cut methods to do so, including graph-cuts [5, 2], Kruskal [12] and Power Watersheds [3], but the results were systematically worse than with our optimal cover method.",
      "startOffset" : 82,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "We experimented with a number of graph cut methods to do so, including graph-cuts [5, 2], Kruskal [12] and Power Watersheds [3], but the results were systematically worse than with our optimal cover method.",
      "startOffset" : 82,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "We experimented with a number of graph cut methods to do so, including graph-cuts [5, 2], Kruskal [12] and Power Watersheds [3], but the results were systematically worse than with our optimal cover method.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "We experimented with a number of graph cut methods to do so, including graph-cuts [5, 2], Kruskal [12] and Power Watersheds [3], but the results were systematically worse than with our optimal cover method.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "[6] 76.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "[17] 76.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] 77.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] 78.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] 79.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "Performance of our system on the Stanford Background dataset [6]: per-pixel accuracy / average per-class accuracy.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "[15] 74.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] 76.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "Performance of our system on the SIFT Flow dataset [15]: per-pixel accuracy / average per-class accuracy.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 23,
      "context" : "[24] 66.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "Performance of our system on the Barcelona dataset [24]: per-pixel accuracy / average per-class accuracy.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "’s Maximin Learning [25] to learn low-level feature vectors from which better segmentation trees can be produced.",
      "startOffset" : 20,
      "endOffset" : 24
    } ],
    "year" : 2012,
    "abstractText" : "Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image. The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average “purity” of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free. The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 × 240 image labeling in less than 1 second. 1. Overview Full scene labeling (FSL) is the task of labeling each pixel in a scene with the category of the object to which it belongs. FSL requires to solve the detection, segmentation, recognition and contextual integration problems simultaneously, so as to produce a globally consistent labeling. One of the obstacles to FSL is that the information necessary for the labeling of a given pixel may come from very distant pixels as well as their labels. The category of a pixel may depend on relatively short-range information (e.g. the presence of a human face generally indicates the presence of a human body nearby), as well as on very long-range dependencies (is this grey pixel part of a road, a building, or a cloud?). This paper proposes a new method for FSL, depicted on Figure 1 that relies on five main ingredients: 1) Trainable, dense, multi-scale feature extraction: a multi-scale, dense feature extractor produces a series of feature vectors for regions of multiple sizes centered around every pixel in the image, covering a large context. The feature extractor is a two-stage convolutional network applied to a multi-scale contrast-normalized laplacian pyramid computed from the image. The convolutional network is fed with raw pixels and trained end to end, thereby alleviating the need for hand-engineered features; 2) Segmentation Tree: A graph over pixels is computed in which each pixel is connected to its 4 nearest neighbors through an edge whose weight is a measure of dissimilarity between the colors of the two pixels. A segmentation tree is then constructed using a classical region merging method, based on the minimum spanning tree of the graph. Each node in the tree corresponds to a potential image segment. The final image segmentation will be a judiciously chosen subset of nodes of the tree whose corresponding regions cover the entire image. 3) Region1 ar X iv :1 20 2. 21 60 v1 [ cs .C V ] 1 0 Fe b 20 12 wise feature aggregation: for each node in the tree, the corresponding image segment is encoded by a 5× 5 spatial grid of aggregated feature vectors. The aggregated feature vector of each grid cell is computed by a component-wise max pooling of the feature vectors centered on all the pixels that fall into the grid cell; This produces a scale-invariant representation of the segment and its surrounding; 4) Class histogram estimation: a classifier is then applied to the aggregated feature grid of each node. The classifier is trained to estimate the histogram of all object categories present in its input segments; 5) Optimal purity cover: a subset of tree nodes is selected whose corresponding segments cover the entire image. The nodes are selected so as to minimize the average “impurity” of the class distribution. The class “impurity” is defined as the entropy of the class distribution. The choice of the cover thus attempts to find a consistent overall segmentation in which each segment contains pixels belonging to only one of the learned categories. All the steps in the process have a complexity linear (or almost linear) in the number of pixels. The bulk of the computation resides in the convolutional network feature extractor. The resulting system is very fast, producing a full parse of a 320 × 240 image in less than 1 second on a conventional CPU. Once trained, the system is parameter free, and requires no adjustment of thresholds or other knobs. There are three key contributions in this paper 1) using a multi-scale convolutional net to learn good features for region classification; 2) using a class purity criterion to decide if a segment contains a single objet, as opposed to several objects, or part of an object; 3) an efficient procedure to obtain a cover that optimizes the overall class purity of a segmentation.",
    "creator" : "LaTeX with hyperref package"
  }
}