{
  "name" : "1610.03807.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Domain-specific Question Generation from a Knowledge Base",
    "authors" : [ "Linfeng Song", "Lin Zhao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Questions are useful for student assessment or coaching purpose in educational or professional contexts, such as student’s self-assessment or new employee training about products or industrial procedures. A large-scale question corpora is also critical to many NLP tasks including question answering, dialogue interaction and intelligent tutoring system, where large amount of annotated questions are usually needed as training data in supervised learning. Until now most of the applications rely on manual data collection which typically takes a lot of time and efforts. Although crowd-sourcing is becoming more and more popular for data collection, it is sometimes hard to\nensure the qualities of the data, especially if the worker is not an expert in the related fields for a domain-specific task. Not to say the collection has to be conducted every time when there comes a new domain or the existing domain gets updated.\nAutomatic question generation has long been of interest to the research community. Most work is focusing on generating questions from a text (Curto et al., 2012; Olney et al., 2012; Mazidi and Nielsen, 2014; Labutov et al., 2015). With the rising of structured knowledge base (KB), some work starts to utilize KB to generate questions (Seyler et al., 2015; Serban et al., 2016). KB can contain knowledge in either open domain such as Freebase (Bollacker et al., 2008) and DBpedia (Auer et al., 2007) or closed domain such as those available in many industrial domains (e.g., product KB, medical KB).\nOur work is in this line of research where we tackle the problem of question generation from domain specific KB. The limitations with existing work are that the generated questions are generally simple questions with limited scope, or a lot of manual work has to be involved to hand-craft all the question templates, making the adaptation to a new domain costly.\nPeople ask questions on the web every day, which generally represent the users’ information needs and naturally form a huge resource of question corpora. In our approach, we try to minimize the manual effort in question generation by leveraging the web resource.\nGiven a KB specific to a certain domain, We start with a small set of hand-crafted templates based on the relationship defined in the KB. These templates include placeholders to replace the string of the subject or object. A seed question set is then generated from the templates with concepts filled in. This seed set is further expanded through the web, by sending each seed question\nar X\niv :1\n61 0.\n03 80\n7v 1\n[ cs\n.C L\n] 1\n2 O\nct 2\n01 6\nto a search engine as a search query to retrieve more related question candidates. Finally a filter is applied to estimate the fluency of each candidate based on language modeling, and also its relevance to the domain based on distributional semantic similarity calculation, and only those with fluency or relevance score above a threshold are returned. We hypothesize that the resulting questions are both natural and relevant to the working domain.\nIn our framework, the manual effort is involved in only seed template construction, where we do not require an extensive number of templates to be created to cover as many as possible the different ways of forming a question. Only one or a few most common question templates is needed as seeds for each of the essential relations in the KB, so as to reduce the human efforts, with the rest of the framework fully automatic. By leveraging the huge amount of available web data, we are able to obtain more natural and open-ended questions more close to the users’ real information needs. This framework can be easily applied to other KB to effectively generate questions for a new domain or application.\nTo summarize, the main contributions of our work are:\n• We propose a new framework of KB-based question generation for domain-specific applications, which is applicable to any KB or domain.\n• We propose a mechanism to build a largescale question set with significantly reduced human efforts by exploiting the substantial web content which yields more diverse and semantically richer questions. By taking into consideration of whether the question is grammatical and whether it is relevant to the working domain, the resulting questions will be both natural and relevant.\n• We evaluate the system with human judgments on the constructed question set, and show that it is effective."
    }, {
      "heading" : "2 Related Work",
      "text" : "Automatic question generation has received increasing interest from the Natural Language Generation (NLG) community, been further advanced by the Question Generation Shared Task and Evaluation Challenge (QGSTEC) (Rus et al., 2010),\nwhich created a common corpus for empirical evaluation of question generation.\nBase on the type of input, question generation task can be divided into two categories: text-based and KB-based. Question generation from text has been relatively well studied where the input can be either single or multiple sentences.\nThe majority of existing systems generate questions from a single sentence. The source sentence is analyzed and its portions are selected as the content of the question (i.e., what to ask), then transformation rules are constructed to transform the source sentence into a valid question (i.e., how to ask). Among the work, some utilizes syntactic parsing to identify the targets of questions and construct syntactic transformation rules to generate questions (Wolfe, 1976; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012). Some utilizes semantic information (Chen et al., 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014) where semantic role labeling is applied to identify patterns in the source sentences for question generation. However, these work mainly focuses on the surface form of the sentence and question, and the scope of the generated questions is limited.\nSome work attempts to generate deeper questions from documents. Agarwal et al. (2011) uses discourse connectives to generate questions from selected text segments for different question types. Olney et al. (2012) first converts the text into concept maps from which questions are generated. Labutov et al. (2015) generate deep open-ended questions from Wikipedia text, where they use crowd-sourcing to construct question templates based on the category of the Wikipedia page, and then apply question ranking to select the final question. Although these approaches can generate questions at a deeper conceptual level, they involve big amount of human effort to formulate questions by handcrafting either question templates or transformation rules.\nOn the other hand, question generation from KB receives less attention. Seyler et al. (2015) generates quiz questions from knowledge graphs, where for each target entity, a SPARQL query is generated as an intermediate representation and turned into a natural language question by a simple predefined template. Because of the fixed template used, the type of question is limited to quiz question. Serban et al. (2016) has constructed a corpus of 30M factoid question and an-\nswer pairs by training a recurrent neural network to map KB facts into corresponding natural language questions. However, their approach needs large amount of fact-question pairs as training data which is not necessarily available for each domain. Also the trained model only works for a single KB fact, which restricts the scope of the generated questions. Our work follows this direction as we believe KB is a good resource for data generation, especially for closed domains. The main difference with the existing work is that first our approach is unsupervised without requiring any labeled data, and second, the types of the generated questions are more diverse without any restriction."
    }, {
      "heading" : "3 Domain-specific Knowledge Base",
      "text" : "A knowledge base (KB) is used to store knowledge which is typically represented using RDF, RDFS and OWL, the W3C standard for semantic web. A RDF KB can be seen as a graph, in which the nodes are entities and the labeled edges represent the relationships between the entities. The graph can also be represented as a list of triples in the form of (subject, predicate, object).\nLarge-scale KBs such as Freebase (Bollacker et al., 2008) and DBpedia (Auer et al., 2007) are very popular and widely used in many NLP applications. They contain millions of facts generally about person, location, organization and so on in the open domain. In our work, since we aim at generating corpus for a closed domain, we work on an in-house domain-specific KB in Power Tool domain. This domain describes knowledge about professional tools used in home or garden, etc. The representations of our KB include en-\ntity concepts such as tools (e.g., jigsaw, screwdriver, cordless drill), accessories (e.g., drill bit, saw blade), materials (e.g., wood, timber, multiplex), and activity concepts (e.g., sawing, drilling, screwing), together with their relationships. The main high level taxonomy is shown in Figure [add figure here].\nFigure 1 shows a fragment of the KB, which can also be represented in the form of triples as follows:\n(rdf:object, rdfs:subClassOf, Tool) (rdf:object, rdfs:subClassOf, Activity) (Tool, performActivity, Activity) (Tool, rdfs:subClassOf, GardenTool) (Tool, rdfs:subClassOf, Saw) (GardenTool, rdfs:subClassOf, Saw) (Activity, rdfs:subClassOf, Cut) (Saw, performActivity, Cut)\nwhere “rdf:object” and “rdfs:subClassOf” are predefined RDF object and label from the RDF community1.\nHere we give some definitions for further description. A parent entity is the one that the current entity connect to with “rdfs:subClassOf” edge. An entity may have multiple parent entities, such as “Saw” (in Figure 1). A sibling entity shares common all parent entities of the current entity. For example, “circular saw” is a sibling entity of “jigsaw” and vice versa."
    }, {
      "heading" : "4 Framework",
      "text" : "Shown in Figure 2, the framework contains 4 sub-processes: KB progressing, question template construction, seed question generation and question expansion. Considering a KB as a list of triples, KB processing is to remove useless ones for question generation. Generally we remove two kinds of triples: one kind of triples contain abstract subjects or objects such as “(Tool, performsActivity, Activity)” where both “Tool” and “Activity” are abstract. The other kind of triples are for relation definition. For example, “(bearingDiameter, type, FunctionalProperty)” defines “bearingDiameter” as a type of “FunctionalProperty”. We remove these kinds of triples because no natural questions can be generated from them. We manually construct question templates for each predicate. For example, “Can I do #Y# with #X#?” is constructed for the predicate of “performsAc-\n1https://www.w3.org/TR/rdf-schema/\ntivity”. Combining that template with “(Jigsaw, performsActivity, CurveCut)”, we can generate a seed question “Can I do curve cut with jigsaw?”. Finally for question expansion, each seed question is thrown into Google search2 for obtaining relevant questions."
    }, {
      "heading" : "4.1 KB Processing",
      "text" : "Given a KB, we need to pre-process it before generating questions, because not every triple in it is suitable for question generation. Generally, we remove two kinds of triples: one kind contains abstract entities (subjects or objects). For example, “Tool” and “Activity” are abstract entities, but “Saw” and “cut” are not. It is easy for human to define, yet hard for computers to quantify. To quantify whether a given entity is abstract, we consider the graph with “rdfs:subClassOf” edges. We define the depth of entities that do not have an outgoing “rdfs:subClassOf” edge to be 0. Otherwise, the depth equals to the depth of the “shallowest parent” plus 1. Shown in Figure 1, the depth of “Saw” is 2, since the depth of its shallowest parent “Tool” is 1. We consider a entity is abstract if its depth is less than n. The other kind contains predicates such as “rdfs:subClassOf” and “rdf:type” that define the framework of the KB. We obtain the predicate list according to the RDF Schema3. As a result, this module outputs a list of triples which will be used to generate questions."
    }, {
      "heading" : "4.2 Question Template Construction",
      "text" : "It is always a difficult problem to generate fluent and relevant questions from a KB, because the only input is a triple while the output is a fluent and\n2https://www.google.com/ 3https://www.w3.org/TR/rdf-schema/\nrelevant question. Recently Serban et al. (2016) introduces a method to generate 30M factoid questions from a KB. However, they use 100K humancrafted (triple, question) pairs to train their system, and the BLEU score (Papineni et al., 2002) of generated questions is only around 35.\nHere we use a template-based method (in Section 4.3). For each predicate identified in the previous step, a few question templates are manually created. Shown in Table 1, for predicate “performsActivity”, a template created is “What activities can #X# perform”, where #X# is the subject. Another template is “Can I perform #Y# with #Y#” where #X# is the subject and #Y# is the object. In this step, only a few (e.g., 2-3) representative templates are needed for a predicate, and the number of predicates are limited (tens of) as we deal with a domain-specific KB. The human effort is significantly lower than Serban et al. (2016)."
    }, {
      "heading" : "4.3 Seed Question Generation",
      "text" : "To generate seed questions, We use a two-step method which takes the constructed question templates and the triples as input. In the first step, for each template, a seed question set is generated by filling in the templates with values from associated triples. For example, for template “Can I perform #Y# with #X#?” and triple “(jigsaw, performsActivity, curve cut)”, we get the question “Can I perform curve cut with jigsaw?”. This process is repeated for each template so that all the variables are replaced with values.\nIn the second step, we replace a subject or object with a sibling class of the original entity to obtain a new question. A sibling class has the same parent class with the original class in the domainspecific KB. For example, for the question “Can I perform curve cut with jigsaw?”, we replace “jigsaw” with “circular saw” to result in a new question “Can I perform curve cut with circular saw?”. “circular saw” is a sibling class of “jigsaw” having the same parent class “saw”. We do not replace with an arbitrary class because people will not ask unreasonable questions such as “Can I per-\nform curve cut with hammer?”. An advantage of our template-base method is that information about the subject, object and predicate are naturally present, which saves the human effort for annotation."
    }, {
      "heading" : "4.4 Question expansion",
      "text" : "The seed questions are limited in terms of their scope and naturalness. To get more diversified and natural questions, we leverage web to expand our seed set. Each seed question in the seed question set is sent to a search engine to retrieve the related or suggested search queries provided by the search engine. These related queries can be again sent to the search engine to retrieve more queries. This process is iteratively performed until we get enough queries. The expansion method has the advantage that the expanded questions represent real users information needs.\nThe expanded questions may not be fluent or domain relevant, especially as the iteration go on, the domain relevance of newly expanded questions drops significantly. Previous methods either ignore this problem (Serban et al., 2016), or let human annotate training data to learn a classifier (Labutov et al., 2015). Our method does not rely on human effort by leveraging word embedding and language modeling. Taking the seed question set (section 4.3) as the in-domain data Din, we filter out questions that are either ungrammatical or domain irrelevant. For domain relevance, we first calculate the embeddings of Din and the candidate question, then discard the question if the cosine similarity between the two embeddings is lower than a threshold t. Shown in Equation 1, document embedding is calculated by averaging the word embeddings within it:\nvecd = ∑ w∈d vecw len(d)\n(1)\nwhere vecw is the embedding for word w, len(d) is the number of words in document d. Simple as it is, we show that it beats the state-of-the-art system on a domain relevance dataset in section 5.3. A question may only contains a few words resulting in data sparsity, we further enrich it with the snippets returned by searching it into a search engine.\nFor fluency, we use averaged language model score as Equation 2:\nAvgLM(sent) = LM(sent)\nN (2)\nwhere LM(sent) is the language model score (log probability), and N is the word count of sent. This method has been shown effective on natural language generation (Liu and Zhang, 2015)."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Setup",
      "text" : "We perform our main experiment with an in-house KB of power tool domain. It contains 67 different predicates, 293 different subjects and 279 different objects respectively. For the 67 predicates, we hand craft 163 templates, some examples are shown in Table 1. After KB processing we obtains 1498 triples, all of which are used for generating seed question set. After question expansion, we build dev and test sets to tune and test the fluency and domain relevance evaluation models. We first randomly select 1000 questions from the expanded question set for the development and test sets respectively, then ask 3 linguistic specialists to grade the fluency and domain relevance basing on a 3-point scheme, finally average the scores. A case is positive only if its score is greater than 2.\nIn addition to the main experiment, we also evaluate our framework on the 30M Factoid Question-Answer Corpus4, released by Serban et al. (2016). Each line in the dataset contains a triple (subject, predicate, object) and a question generated by their method. We randomly sample 10 lines, and compare our results with theirs.\nTo evaluate the fluency of the expanded questions, we train a 4-gram language model (LM) with KneserNey smoothing on gigaword (LDC2011T07), and learn word embeddings on Wikipedia5 for evaluating the relevance. We use the Skip-gram model (Mikolov and Dean, 2013) implementation from word2vec6 with default parameter setting to learn word embeddings. To en-\n4http://agarciaduran.org/ 5https://dumps.wikimedia.org/ 6https://code.google.com/archive/p/word2vec/\nrich questions with snippets, we try two ways: enrich with the related queries or with the full snippet, and compare their performances. Even though full snippet gives more context information, it also introduces more noise. On the devset we tune the thresholds of tf (fluency) and tr (relevance), which are further applied on the test set. We tune them both separately and jointly. For separate tuning, we show individual performances (Fluency and Relevance). For joint tuning, we perform grid search and show the overall performance (All).\nWe test our relevance evaluating method on the web snippet dataset, which is frequently used for domain classification of short documents. It contains 10,060 training and 2,280 test snippets (short documents) in 8 classes (domains), and there are 18 words in each snippet on average. There has been plenty of results (Phan et al., 2008; Chen et al., 2011; Ma et al., 2015) on the dataset. The fluency evaluation method has been shown competitive, especially when the test set is in a different domain (Liu and Zhang, 2015)."
    }, {
      "heading" : "5.2 Results and Analysis",
      "text" : "In the main experiment, we generated 12,228 seed questions from which 20,000 more questions are\nexpanded with Google search7. Shown in Table 2 are some expanded questions from which we can see that most of them are grammatical and relevant to the power tool domain. In addition, most questions are informative that correspond to a specific answer, except the one “do I need a hammer drill” that lacks some context. Finally, in addition to the simple factoid questions, our system generates many complex questions such as “how to cut a groove in wood without a router”.\nWe tried different values of tf and tr on the devsets, and show the curves between precision and recall in Figure 3. The dev and test results with tuned parameters are shown in Table 3. For relevance, enriching with full snippet is significantly better than the others, and enriching with related queries is significantly better than using only questions. This explains that the data sparseness is important for calculating document embedding. One example is the question “does saw 2 end” where “saw 2” actually refers to the movie “Saw II”. Its domain relatedness can not be determined only from its words, but it is much easier with the snippet. For fluency, the pick F-score is 93.3, and it only drops to around 88 if we keep all questions. This explains that the expanded questions are generally grammatical. In overall, the combined result does not drop too much which shows that there is positive correlation between the two indexes."
    }, {
      "heading" : "5.3 Domain Relevance",
      "text" : "Shown in Table 4, We compare our question relevance evaluation method with previous state of the art methods: Phan et al. (2008) first derives latent topics with LDA (Blei et al., 2003) from a set of texts from Wikipedia, then uses the topics as appended features to expand the short text. Chen et al. (2011) further extend Phan et al. (2008) by using multi-granularity topics. Ma et al. (2015) adopts a Bayesian model that the probability a document D belongs to a topic t equals to the prior of t times the probability each word w in D comes\n7https://www.google.com/\nfrom t. Our method (question) first calculate the document embedding for each test document and each domain in the training set, then assign test documents to the nearest (cosine similarity) domains. We do not enrich the short documents in this experiment.\nSimple as it is, our method outperforms all previous methods without question enrichment, which proves the effectiveness of our method. One important reason is that word embeddings directly capture the similarity between distinct words, while it is hard for traditional methods. Besides, word embeddings are composable that sentence and document embeddings can be got by summing up the word embeddings within them. On the order hand, LDA only learns the similarity between words and topics, and it is trivial to determine a proper number of topics."
    }, {
      "heading" : "5.4 Comparison on 30M Factoid Question Answer Corpus",
      "text" : "Serban et al. (2016) releases a corpus of 30M (triple, question) pairs from which we randomly select 500 triples to generate our questions. We first hand craft 53 templates, then generate 991 seed questions from the triples, finally get a expanded set of 1529 questions from Google search. We skip the KB-processing step as our input here are already triples. From the expanded question set, we select 500 by the averaged language model score as this is a domain-general corpus.\nShown in Table 5, we compare our questions with Serban et al. (2016) that questions in the same line describe the same entity. We can see that our questions are grammatical, natural as these questions are what people usually ask on the web. On the other hand, questions from Serban et al. (2016) are either ungrammatical (such as “who was someone who was involved in the leukemia ?” and “whats the title of a book of the subject of the bible ?”), unnatural (“what ’s one of the mountain where can you found in argentina in netflix ?”) or confusing (“who was someone who was involved in the leukemia ?”)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented an approach to generate natural language questions from knowledge graphs. By leveraging rich web information, the system is able to generate relevant questions with wide scope and the human effort can be significantly reduced. We evaluated our approach in terms of the grammaticality and relevance of the generated questions and the results showed its effectiveness. Note that although the work presented in this paper is for a specific domain, the working flow constitutes a general framework that could potentially be applied to other domains or KBs as well."
    } ],
    "references" : [ {
      "title" : "Automatic question generation using discourse cues",
      "author" : [ "Rakshit Shah", "Prashanth Mannem" ],
      "venue" : "In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2011
    }, {
      "title" : "Automation of question generation from sentences",
      "author" : [ "Ali et al.2010] Husam Ali", "Yllias Chali", "Sadid A Hasan" ],
      "venue" : "In Proceedings of QG2010: The Third Workshop on Question Generation,",
      "citeRegEx" : "Ali et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ali et al\\.",
      "year" : 2010
    }, {
      "title" : "Dbpedia: A nucleus for a web of open data",
      "author" : [ "Auer et al.2007] Sören Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives" ],
      "venue" : "In The semantic web,",
      "citeRegEx" : "Auer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2007
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "Blei et al.2003] David M Blei", "Andrew Y Ng", "Michael I Jordan" ],
      "venue" : "Journal of machine Learning research,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor" ],
      "venue" : "In Proceedings of the 2008 ACM SIGMOD,",
      "citeRegEx" : "Bollacker et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Generating questions automatically from informational text",
      "author" : [ "Chen et al.2009] W Chen", "G Aist", "J Mostow" ],
      "venue" : "In Proceedings of the 2nd Workshop on Question Generation,",
      "citeRegEx" : "Chen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2009
    }, {
      "title" : "Short text classification improved by learning multi-granularity topics",
      "author" : [ "Chen et al.2011] Mengen Chen", "Xiaoming Jin", "Dou Shen" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Chen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2011
    }, {
      "title" : "Question generation based on lexicosyntactic patterns learned from the web",
      "author" : [ "Curto et al.2012] Sérgio Curto", "A Mendes", "Luisa Coheur" ],
      "venue" : "Dialogue & Discourse,",
      "citeRegEx" : "Curto et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Curto et al\\.",
      "year" : 2012
    }, {
      "title" : "Good question! statistical ranking for question generation",
      "author" : [ "Heilman", "Smith2010] Michael Heilman", "Noah A Smith" ],
      "venue" : "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association",
      "citeRegEx" : "Heilman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Heilman et al\\.",
      "year" : 2010
    }, {
      "title" : "Deep questions without deep understanding",
      "author" : [ "Labutov et al.2015] Igor Labutov", "Sumit Basu", "Lucy Vanderwende" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Labutov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Labutov et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating natural language questions to support learning on-line",
      "author" : [ "Fred Popowich", "John Nesbit", "Phil Winne" ],
      "venue" : null,
      "citeRegEx" : "Lindberg et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lindberg et al\\.",
      "year" : 2013
    }, {
      "title" : "An empirical comparison between n-gram and syntactic language models for word ordering",
      "author" : [ "Liu", "Zhang2015] Jiangming Liu", "Yue Zhang" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributional representations of words for short text classification",
      "author" : [ "Ma et al.2015] Chenglong Ma", "Weiqun Xu", "Peijia Li", "Yonghong Yan" ],
      "venue" : "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,",
      "citeRegEx" : "Ma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Linguistic considerations in automatic question generation",
      "author" : [ "Mazidi", "Nielsen2014] Karen Mazidi", "Rodney D Nielsen" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "Mazidi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mazidi et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems",
      "author" : [ "Mikolov", "Dean2013] T Mikolov", "J Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Question generation from concept maps",
      "author" : [ "Olney et al.2012] Andrew M Olney", "Arthur C Graesser", "Natalie K Person" ],
      "venue" : "Dialogue and Discourse,",
      "citeRegEx" : "Olney et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Olney et al\\.",
      "year" : 2012
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Salim Roukos", "Todd Ward", "Wei-Jing Zhu" ],
      "venue" : "In Proceedings of the 40th Annual Conference of the Association for Computational Linguis-",
      "citeRegEx" : "Papineni et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning to classify short and sparse text & web with hidden topics from large-scale data collections",
      "author" : [ "Phan et al.2008] Xuan-Hieu Phan", "Le-Minh Nguyen", "Susumu Horiguchi" ],
      "venue" : "In Proceedings of the 17th international conference on World Wide Web,",
      "citeRegEx" : "Phan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Phan et al\\.",
      "year" : 2008
    }, {
      "title" : "Overview of the first question generation shared task evaluation challenge",
      "author" : [ "Rus et al.2010] Vasile Rus", "Brendan Wyse", "Paul Piwek", "Mihai Lintean", "Svetlana Stoyanchev", "Cristian Moldovan" ],
      "venue" : "In Proceedings of the Third Workshop on Question Gener-",
      "citeRegEx" : "Rus et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rus et al\\.",
      "year" : 2010
    }, {
      "title" : "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus",
      "author" : [ "Alberto Garcı́aDurán", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Serban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating quiz questions from knowledge graphs",
      "author" : [ "Mohamed Yahya", "Klaus Berberich" ],
      "venue" : "In Proceedings of the 24th International Conference on World Wide Web,",
      "citeRegEx" : "Seyler et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Seyler et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic question generation from text-an aid to independent study",
      "author" : [ "John H Wolfe" ],
      "venue" : "ACM SIGCSE Bulletin,",
      "citeRegEx" : "Wolfe.,? \\Q1976\\E",
      "shortCiteRegEx" : "Wolfe.",
      "year" : 1976
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Most work is focusing on generating questions from a text (Curto et al., 2012; Olney et al., 2012; Mazidi and Nielsen, 2014; Labutov et al., 2015).",
      "startOffset" : 58,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "Most work is focusing on generating questions from a text (Curto et al., 2012; Olney et al., 2012; Mazidi and Nielsen, 2014; Labutov et al., 2015).",
      "startOffset" : 58,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "Most work is focusing on generating questions from a text (Curto et al., 2012; Olney et al., 2012; Mazidi and Nielsen, 2014; Labutov et al., 2015).",
      "startOffset" : 58,
      "endOffset" : 146
    }, {
      "referenceID" : 20,
      "context" : "With the rising of structured knowledge base (KB), some work starts to utilize KB to generate questions (Seyler et al., 2015; Serban et al., 2016).",
      "startOffset" : 104,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "With the rising of structured knowledge base (KB), some work starts to utilize KB to generate questions (Seyler et al., 2015; Serban et al., 2016).",
      "startOffset" : 104,
      "endOffset" : 146
    }, {
      "referenceID" : 4,
      "context" : "KB can contain knowledge in either open domain such as Freebase (Bollacker et al., 2008) and DBpedia (Auer et al.",
      "startOffset" : 64,
      "endOffset" : 88
    }, {
      "referenceID" : 2,
      "context" : ", 2008) and DBpedia (Auer et al., 2007) or closed domain such as those available in many industrial domains (e.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "Automatic question generation has received increasing interest from the Natural Language Generation (NLG) community, been further advanced by the Question Generation Shared Task and Evaluation Challenge (QGSTEC) (Rus et al., 2010), which created a common corpus for empirical evaluation of question generation.",
      "startOffset" : 212,
      "endOffset" : 230
    }, {
      "referenceID" : 21,
      "context" : "Among the work, some utilizes syntactic parsing to identify the targets of questions and construct syntactic transformation rules to generate questions (Wolfe, 1976; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012).",
      "startOffset" : 152,
      "endOffset" : 228
    }, {
      "referenceID" : 1,
      "context" : "Among the work, some utilizes syntactic parsing to identify the targets of questions and construct syntactic transformation rules to generate questions (Wolfe, 1976; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012).",
      "startOffset" : 152,
      "endOffset" : 228
    }, {
      "referenceID" : 7,
      "context" : "Among the work, some utilizes syntactic parsing to identify the targets of questions and construct syntactic transformation rules to generate questions (Wolfe, 1976; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012).",
      "startOffset" : 152,
      "endOffset" : 228
    }, {
      "referenceID" : 5,
      "context" : "Some utilizes semantic information (Chen et al., 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014) where semantic role labeling is applied to identify patterns in the source sentences for question generation.",
      "startOffset" : 35,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Some utilizes semantic information (Chen et al., 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014) where semantic role labeling is applied to identify patterns in the source sentences for question generation.",
      "startOffset" : 35,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. (2011) uses discourse connectives to generate questions from selected text segments for different question types.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. (2011) uses discourse connectives to generate questions from selected text segments for different question types. Olney et al. (2012) first converts the text into concept maps from which questions are generated.",
      "startOffset" : 0,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al. (2011) uses discourse connectives to generate questions from selected text segments for different question types. Olney et al. (2012) first converts the text into concept maps from which questions are generated. Labutov et al. (2015) generate deep open-ended questions from Wikipedia text, where they use crowd-sourcing to construct question templates based on the category of the Wikipedia page, and then apply question ranking to select the final question.",
      "startOffset" : 0,
      "endOffset" : 249
    }, {
      "referenceID" : 19,
      "context" : "Seyler et al. (2015) generates quiz questions from knowledge graphs, where for each target entity, a SPARQL query is generated as an intermediate representation and turned into a natural language question by a simple predefined template.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 19,
      "context" : "Serban et al. (2016) has constructed a corpus of 30M factoid question and an-",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "Large-scale KBs such as Freebase (Bollacker et al., 2008) and DBpedia (Auer et al.",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : ", 2008) and DBpedia (Auer et al., 2007) are very popular and widely used in many NLP applications.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "However, they use 100K humancrafted (triple, question) pairs to train their system, and the BLEU score (Papineni et al., 2002) of generated questions is only around 35.",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "Recently Serban et al. (2016) introduces a method to generate 30M factoid questions from a KB.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 19,
      "context" : "The human effort is significantly lower than Serban et al. (2016).",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "Previous methods either ignore this problem (Serban et al., 2016), or let human annotate training data to learn a classifier (Labutov et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : ", 2016), or let human annotate training data to learn a classifier (Labutov et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : "In addition to the main experiment, we also evaluate our framework on the 30M Factoid Question-Answer Corpus4, released by Serban et al. (2016). Each line in the dataset contains a triple (subject, predicate, object) and a question generated by their method.",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : "There has been plenty of results (Phan et al., 2008; Chen et al., 2011; Ma et al., 2015) on the dataset.",
      "startOffset" : 33,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "There has been plenty of results (Phan et al., 2008; Chen et al., 2011; Ma et al., 2015) on the dataset.",
      "startOffset" : 33,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "There has been plenty of results (Phan et al., 2008; Chen et al., 2011; Ma et al., 2015) on the dataset.",
      "startOffset" : 33,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "18 Chen et al. (2011) 85.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "18 Chen et al. (2011) 85.31 Ma et al. (2015) 85.",
      "startOffset" : 3,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "(2008) first derives latent topics with LDA (Blei et al., 2003) from a set of texts from Wikipedia, then uses the topics as appended features to expand the short text.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "Shown in Table 4, We compare our question relevance evaluation method with previous state of the art methods: Phan et al. (2008) first derives latent topics with LDA (Blei et al.",
      "startOffset" : 110,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "(2008) first derives latent topics with LDA (Blei et al., 2003) from a set of texts from Wikipedia, then uses the topics as appended features to expand the short text. Chen et al. (2011) further extend Phan et al.",
      "startOffset" : 45,
      "endOffset" : 187
    }, {
      "referenceID" : 3,
      "context" : "(2008) first derives latent topics with LDA (Blei et al., 2003) from a set of texts from Wikipedia, then uses the topics as appended features to expand the short text. Chen et al. (2011) further extend Phan et al. (2008) by using multi-granularity topics.",
      "startOffset" : 45,
      "endOffset" : 221
    }, {
      "referenceID" : 3,
      "context" : "(2008) first derives latent topics with LDA (Blei et al., 2003) from a set of texts from Wikipedia, then uses the topics as appended features to expand the short text. Chen et al. (2011) further extend Phan et al. (2008) by using multi-granularity topics. Ma et al. (2015) adopts a Bayesian model that the probability a document D belongs to a topic t equals to the prior of t times the probability each word w in D comes",
      "startOffset" : 45,
      "endOffset" : 273
    }, {
      "referenceID" : 19,
      "context" : "Ours Serban et al. (2016)",
      "startOffset" : 5,
      "endOffset" : 26
    } ],
    "year" : 2016,
    "abstractText" : "Question generation has been a research topic for a long time, where a big challenge is how to generate deep and natural questions. To tackle this challenge, we propose a system to generate natural language questions from a domain-specific knowledge base (KB) by utilizing rich web information. A small number of question templates are first created based on the KB and instantiated into questions, which are used as seed set and further expanded through the web to get more question candidates. A filtering model is then applied to select candidates with high grammaticality and domain relevance. The system is able to generate large amount of in-domain natural language questions with considerable semantic diversity and is easily applicable to other domains. We evaluate the quality of the generated questions by human judgments and the results show the effectiveness of our proposed system.",
    "creator" : "LaTeX with hyperref package"
  }
}