{
  "name" : "1611.05083.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Probabilistic Failure Analysis in Model Validation & Verification",
    "authors" : [ "Ning Ge", "Marc Pantel", "Xavier Crégut" ],
    "emails" : [ "Xavier.Cregut}@enseeiht.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Probabilistic Failure Analysis in Model Validation & Verification\nNing Ge, Marc Pantel, and Xavier Crégut\nUniversity of Toulouse, IRIT/INPT {Ning.Ge | Marc.Pantel | Xavier.Cregut}@enseeiht.fr\nKeywords: Fault localization, Model checking, Verification, Validation, Probabilistic analysis, Hidden markov model"
    }, {
      "heading" : "1 Introduction",
      "text" : "As the size and complexity of safety critical real-time system are rapidly increasing due to the evolution of functional and non-functional requirements, Model-Driven Engineering (MDE) has become a promising means to improve the reliability and efficiency of the traditional software engineering by introducing the models and the formal methods. We use the multi V-model proposed in the ITEA TIMMO project in Fig. 1 to illustrate the use of process of MDE for developing real-time system. In order to generate reliable execution code, the verification\nR equirem ents R eq ui re m en ts V & V\nA rchitecture D esign A rc hi te ct ur e\nD es ig n V & V\nD etailed\nD esign D\net ai le d D es ig n V & V\nC ode G eneration C od e G en er at io n V & V\nR eq ui re m en ts V & V\nD et ai le d D es ig n V & V\nA rc hi te ct ur e D es ig n V & V\nA rc hi te ct ur e D es ig n V & V\nR eq ui re m en ts V & V\nR eq ui re m en ts V & V\nTime Line\nFigure 1. V-Model in Model-Driven Engineering\nand validation (V&V) are performed on each phase of system development lifecycle. The architecture design is the phase to design hardware and software architecture which can also be referred to as high-level design. It should involve\nar X\niv :1\n61 1.\n05 08\n3v 2\n[ cs\n.S E\n] 1\n8 N\nov 2\n01 6\na brief and abstract functionality of each module, their interface relationships, dependability, architecture diagrams, etc. The detailed design model can also be called modular or function design model, where the low-level design including detailed functional logic of the module can be specified. From the current practice, the architecture is usually modeled using Domain Specific Language (DSL) such as AADL and EAST-ADL or specific diagrams in a General Purpose Language (GPL) such as UML (Composite Structure Diagram), while the detailed design is usually modeled using DSL such as Simulink and SCADE or specific diagrams such as UML Activity, State Machine Diagrams, or ALF (Action Language for Foundational UML).\nIn practice, V&V in MDE is usually implemented in two manners: simulation and formal verification (such as static analysis, theorem proving and model checking). When a requirement is not satisfied, the verification results will be used to diagnose this failed design. The end users will analyze the verification results to locate the origin of fault ( an activity called fault localization). The efficient and effective failure analysis method in V&V is an important issue.\nFault localization is not trivial in MDE. In the architecture design V&V, due to the use of abstraction for scalable model checking, some unnecessary information are removed for some specific V&V purpose. This reduces the state space explosion problem, but usually leads to model with concurrent and indeterministic behaviors that are much more complicated to debug. In the detailed design V&V, fault propagation is one of main issues. When a functional constraint is violated, we detect wrong output values from some components. Usually the others’ outputs will be affected due to failure propagation.\nIn order to locate the potential origin of faults from failure scenario in the model V&V, we propose to analyze fault locations using probabilistic approaches inspired by data mining technologies. Considering the different features of architecture design and detailed design, we use Kullback-Leibler Divergence to analyze the failure from exhaustive model checking results in the architecture design V&V, and use hidden markov model to analyze the failure from simulation in the detailed design V&V. This paper describes early experiments on these two aspects."
    }, {
      "heading" : "2 Probabilistic Failure Analysis in Model Checking",
      "text" : ""
    }, {
      "heading" : "2.1 Problem Statements",
      "text" : "Generating a counterexample in case a formula is violated is a key service provided by model checkers. Counterexamples produced by model checkers often stand for error traces, which represent sequences of system states and transitions and are therefore usually lengthy and difficult to understand. The origin of error might be anywhere along these traces, thus requiring a lengthy analysis by designers. Our ultimate goal is to detect and to provide the end users with the suspicious ranked faulty elements.\nAbstraction Issue Fault localization in model checking is challenging as the models have usually a concurrent and indeterministic behavior with many possible execution traces. This behavior is due to the use of abstraction in their design. Without precise information, fault localization may not be precise enough. Given a sequential, or synchronized concurrent, program which exhibits less execution traces, various debugging methods are available to detect and locate the faulty statements. In model-based diagnosis, the use of abstraction is mandatory to reduce the state space explosion problem. At the time of writing, the conflict between model precision and verification cost is a key issue in model checking and model-driven engineering (MDE), therefore a compromise is made to remove the unnecessary information for some verification purpose while keeping all the property-related information. This usually leads to model with concurrent and indeterministic behaviors that exhibits a much larger number of execution traces and are consequently much more complicated to debug.\nFault Localization Issue Sometimes it is difficult, even for seasoned experts, to analyze the fault origin. We take a simple example (see Ex. 1) to illustrate this issue.\nExample 1 (Fault Localization Example). Assume a system consists of two concurrent processes A and B. Both execute only once. The execution time is [5,10] for A, and [3,7] for B. The expected temporal property P is Always A After B.\nIt is obvious that P is unsatisfied. The design fault occurs either on A or on B. To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4]. However, without extra information, A and B exhibit the same suspicion. If an extra information is available, e.g. the best case execution time (BCET) of B is 5, then the time constraint of B cannot anymore be replaced by [3,4], thus the suspicion of B is largely decreased.\nThis example is simple enough to be analyzed manually, while it is impossible for more complex system with thousands of transitions. Any modification on a transition may impact the verification result through time constraint propagation.\nProposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20]. Our approach will improve the effectiveness of fault localization by providing a suspiciousness factor which is used to rank the suspicious transitions in the verification model [14]. The suspiciousness factor is computed using the fault contribution of each transition on the error traces derived from the reachability graph. This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15]."
    }, {
      "heading" : "2.2 Preliminaries",
      "text" : "Reachability Graph & Violation States Reachability graphs are used to solve reachability problem in model checking. They contain all the states in the execution of a system and all the transitions between these states. When a safety property is not satisfied, there exists violation states in the reachability graph. Finding all violation states in the reachability graph is the first step of error localization.\nFault Contribution of Transition\nDefinition 1 (Fault Contribution). Fault Contribution (CF ) is a suspiciousness factor to evaluate a transition’s suspicion level. It is used to rank the suspiciousness of transitions.\nError Traces\nDefinition 2 (Error Trace). For all the states {si} on the path from an initial state s0 to a violation state sv in the reachability graph, all the outgoing transitions of si are considered as error trace π.\nWe consider not only the transitions on the path that leads S0 to Sv in the definition of error trace but also the direct outgoing transitions of all the states in execution traces that lead to correct states. Indeed, in TPN, the transitions outgoing from the same place can mutually influence each other. A faulty transition can change the way a correct transition is fired if they are both outgoings from the same place. The correct transition will diminish the CF of the faulty transition.\nExample 2 (Error Trace Example). In Fig. 2, s0 is initial state, sv is a violation state. On the execution trace from s0 to sv, there exist four states {s0, s1, s2, s3} (apart from sv). The state s3 is in a correct trace. When the system is in state s2, it is possible to transit to s7 leading to a correct trace, or to s3 leading to a violation state. If s7 is removed from the graph, s3 will have higher fault contribution for the violation state. The outgoing transitions of these four states are considered as error traces π, i.e., π = {t0, t1, t2, t1, t5, t4, t2, t3, t4}."
    }, {
      "heading" : "2.3 Kullback-Lerbler Divergence",
      "text" : "Kullback-Leibler Divergence (also called information divergence, information gain, relative entropy) [21] is a fundamental equation of information theory that qualifies the proximity of two probability distributions.\nDefinition 3 (Kullback-Leibler Divergence (KL)). KL Divergence is a measure in statistics that quantifies in bits how close a probability distribution P = {pi} is to a model (or candidate) distribution Q = {qi}. The KL-divergence of Q from P over a discrete random variable is defined as\nDKL(P ‖ Q) = ∑ i P (i) ln P (i) Q(i) (1)\nNote: In the definition above, 0 ln 00 = 0, 0 ln 0 q = 0, and p ln p 0 =∞.\nKullback-Leibler Divergence has many applications. We give an example of its application to text classification [2]. A textual document d is a discrete distribution of |d| random variables, where |d| is the number of terms in the document. Let d1 and d2 be two documents whose similarity we want to compute. This is done using DKL(d1 ‖ d2) and DKL(d2 ‖ d1).\nAnother major application is the TF-IDF (Term Frequency - Inverse Document Frequency) algorithm [19]. TF-IDF is a numerical statistic which reflects how important a term is for a given document in a corpus (collection) of documents. It is often used as a weighting factor in information retrieval and text mining. Variations of the TF-IDF weighting scheme are often used by search engines as a central tool in scoring and ranking a document’s relevance to a given user query.\nSuppose we have a collection of English textual documents and aim to determine which documents are most relevant to the query ”the model checking”. We might start by eliminating documents that do not contain the three words ”the”, ”model”, and ”checking”, but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document and sum them all together; the number of times a term occurs in a document is called its term frequency (TF).\nHowever, because the term ”the” is so common, this might incorrectly emphasize documents which happen to use the word ”the” more frequently, without giving enough weight to the more meaningful terms ”model” and ”checking”. The term ”the” is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less common words ”model” and ”checking”. Hence an inverse document frequency (IDF) factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely."
    }, {
      "heading" : "2.4 Ranking Suspicious Faulty Transitions",
      "text" : "Inspired by the TF-IDF algorithm, we propose a probabilistic fault localization approach based on the Kullback-Leibler Divergence. A relevance weight CF (t)\nis computed to assess the contribution of a transition t in error traces leading to violation states and thus its contribution to the fault.\nIn the TF-IDF algorithm, each term in the documents will contribute to the semantics of keywords. Some terms are considered as significant if they are more relevant to the semantics of keywords. This is similar to the fault contribution caused by a given transition in an error trace in model checking. Fig. 3 compares the similarity between semantic contribution of terms in documents and fault contribution of transitions in error traces. Some terms in documents have closer semantic relation to the keywords, the occurrence of these terms provide more semantic contributions to the occurrence of keywords. Similarly, the fault propagation depends on the topology of error traces, the occurrence of some transitions will provide more fault contributions to the occurrence of violation states.\nThe semantic contribution of a term in documents is measured by TF-IDF, where TF is the contribution of a term in single document, and IDF is the contribution of a term in a collection of documents. The fault contribution to the violation states {svi} caused by a transition t on error traces {πi} can also be evaluated by a similar measure CF (t), defined as TC-ITC (Transition Contribution - Inverse Trace Contribution). CF (t) = TC(t) · ITC(t)."
    }, {
      "heading" : "2.5 Experimental Results",
      "text" : "We assess our approach using two significant criteria: effectiveness and efficiency. According to the survey [25], the effectiveness can be assessed by a score EXAM in terms of the percentage of statements that have to be examined until the first statement containing the fault is reached [6,26]. The fault localization techniques in model checking, like other techniques, should terminate in a timely manner, limited by some resource constraints. The efficiency can be assessed by the scalability and the performance.\nAutomated Test Bed The test bed will generate randomly systems which might have deadlock, then apply the proposed analysis algorithm and check that it detects the introduced deadlocks. We use Time Petri Net to model system’s behavior.\nFor a given TPN system S(P,R,M), P are the processes which run infinitely and need a resource before the next task (a task is represented by a transition); R are resource which are shared by all the processes, but only accessible in an exclusive way; M is a matrix to decide whether process Pi will need to access resource Rj . Coffman identified four conditions that must hold simultaneously in order to have a deadlock [5]. To improve the success of creating a deadlock in the system, we introduced another mechanism to enforce deadlocks: randomly let some processes during some tasks forget to release a used resource. These tasks are then considered as the error source of system’s deadlock.\nEvaluation of Efficiency We have generated thousands of test cases by assigning P and R values from 5 to 20, creating 1 to 9 faulty transitions, with all the other parameters totally random. The tests are performed on a 2,4 GHz Intel Core 2 Duo processor running Mac OS X 10.6.8. The system parameters and efficiency evaluation results are shown in Table 1. The average time of evaluation shows that the approach is efficient for large scale system."
    }, {
      "heading" : "1 400 4949 / 15440 2.9092",
      "text" : ""
    }, {
      "heading" : "2 517 2428 / 7130 1.1244",
      "text" : ""
    }, {
      "heading" : "3 500 9884 / 31237 3.3533",
      "text" : ""
    }, {
      "heading" : "4 402 8811 / 26663 2.5998",
      "text" : ""
    }, {
      "heading" : "5 303 6756 / 18247 1.2196",
      "text" : "Evaluation of Effectiveness The effectiveness evaluation is shown in Table 2. We give out EXAM score, EXAM score variance, rank, and rank variance for the best cases and worst cases, and then show the average EXAM score and average rank. The EXAM score varies from 2% to 13% for best cases, and varies from 4% to 18% for worst cases. In average, EXAM varies from 3% to 16% which corresponds to ranking results from 1 to 8. The stability is represented by the variance result. These experimental results shows our approach is effective."
    }, {
      "heading" : "3 Probabilistic Failure Analysis in Simulation",
      "text" : ""
    }, {
      "heading" : "3.1 Background",
      "text" : "Fault localization algorithms usually follow two paradigms: cause-effect and effect-cause analyses. Cause-effect analysis [24,27,18] starts from possible causes\n(fault models). A simulator is used to predict system’s behavior in the presence of various faults. Then predictions are matched against observed behavior. Effect-cause analysis [1,23] reasons faulty localization based on observed behavior and expected good functions. It back-traces faulty causes from the identified suspect components.\nIn this work, we make a trade-off of cause-effect and effect-cause analyses and propose an Hidden Markov Model (HMM) [22] based approach for the automated localization of faulty components in the simulation [8,7,9]. The component can be hardware device, software modules or functional blocks in the system. This method combines forward localization analysis and backward confidence degree evaluation. HMM, as a component’s abstraction, provides statistically identical information to component’s real behavior. The core of this method is a fault localization algorithm that gives out the set of suspicious ranked faulty components and a backward algorithm that computes the matching degree between the HMM and the simulation model to evaluate the confidence degree of the localization conclusion."
    }, {
      "heading" : "3.2 HMM Modeling and Analysis",
      "text" : "An HMM is defined as a statistical model used to represent stochastic processes, where the states are not directly observed. A basic HMM can be described as follows:\n– N: number of states\n– M: number of observations\n– MI: initial probability distribution; N∑ i=1 MI(i) = 1 – MT: probability distribution of transitions from states to states; N∑ j=1 MT(i, j) = 1, i = 1...N\n– ME: emission distribution for the observations associated with states; M∑ j=1 ME(i, j) = 1, i = 1...N\nExample 3 (HMM Example). A two states HMM example abstracting a system’s health condition is given by Fig. 4, where the system owns two states Healthy (H) and Faulty (F), and two observations which represent the ouputs respect the functional constraints (R) or violate the functional constraints (V). The three\ndistributions MIMTME are: ( 0.6 0.4 )(0.7 0.3\n0.4 0.6\n)( 0.9 0.1\n0.2 0.8\n) .\nHMM, as abstract model of real system, is statistically identical to system’s real behavior. When modeling a system, HMM separates the concept into two conceptually independent paradigms: behavior and observation. Behavior refers to what the system really is; while observation to what the system exhibits that is used for its recognition. MI gives indication about the probability that a behavior becomes the first behavior when system runs. MT decides how probably will the system behave from one state to the other states. This is statistically equivalent to the real system’s behavior. ME provides a distribution that connects the behavior and the observation: if at a given time the behavior is known, how probably an observed sequence will occur.\nMI,MT and ME can be obtained by modeling or through a learning process. Once all these matrix parameters are estimated, HMM is capable to deduce, given an observed output sequence or a set of such sequences, the maximum likelihood estimation of inner-state transition sequences."
    }, {
      "heading" : "3.3 Automated Fault Localization Based on Hidden Markov Model",
      "text" : "This approach is based on component analysis. Each component is mapped to an HMM. If all the input/output pair of a component can be exhaustively listed,\nwe can get an exact distribution of how this component respects the functional constraints. This approach can be explained by using Fig. 5. An HMM, as a component’s abstraction, provides statistically similar information to simulation by MI. That is equally saying, a component can be simulated by HMM if we can be sure they behave statistically in the same way. To measure whether they behave the same, we introduce an evaluation approach by using test results. The returned evaluation metrics are used to revise or refine the parameters in HMM, until it approximates the component’s real behavior.\nSystem States A component C is mapped to an HMM. Hmm’s states are the combination of component’s faulty status and its inputs faulty status. System’s behavior is modeled by 4 states:\n– PIp: C is not faulty, C’s inputs are passed – PIf : C is not faulty, C’s inputs are failed – FIp: C is faulty, C’s inputs are passed – FIf : C is faulty, C’s inputs are failed\nObservation & Observed Sequence The observation is defined by the test results of component’s outputs. There are only 2 observations:\n– Op: outputs are passed – Of : outputs are failed\nThe dependency between components is built By defining HMM states and observations in this way. For component C, its output observation is the input (contained in the defined states) of the successor.\nInitial Probability Matrix MI Assume components’ faulty probability is ω, MI is defined as following table 3. If ω is not available, we assume each state has identical initial faulty probability, i.e. ω = 12 .\nTable 3. Initial Probability Matrix MI\nPIp PIf FIp FIf\nInit 1−ω 2 1−ω 2 ω 2 ω 2\nTable 4. Transition Probability Matrix MT\nPIp PIf FIp FIf\nPIp α(1−ω) α+β β(1−ω) α+β αω α+β βω α+β PIf γ(1−ω) γ+δ δ(1−ω) γ+δ γω γ+δ δω γ+δ FIp α(1−ω) α+β β(1−ω) α+β αω α+β βω α+β FIf γ(1−ω) γ+δ δ(1−ω) γ+δ γω γ+δ δω γ+δ\nTransition Probability Matrix MT MT contains statistical values derived from test results. If n test cases are observed, a component with m inputs corresponds to m·n input sequences. Using the m·n input sequences, we can compute the fault probabilities α, β, γ, δ respectively for the transitions the states, where α+β+γ+δ = 1. MT is then calculated as following table 4. If ω is not available, we assume ω = 1.\nEmission Probability Matrix ME The key of this approach is evaluating ME . According to previous analysis, we need to measure how well an HMM statistically simulates a given component, which is represented by Matching Level.\nDefinition 4 (Matching Level (µ)). Matching level evaluates how well HMM simulates a component’s real behavior.\nThe objective is to find an HMM with the highest matching level. This turns the problem to be an optimization problem, and many techniques can be applied, e.g. exhaustive search by minimum internal, heuristic algorithm, evolution algorithm, experimental design, etc. In this work, the matching level is derived by calculating the probability that a component’s outputs pass the tests through HMM observations. However, this matching level is a local value relative to one component, making it have no meaning to compare with others. Therefore, the forward search only is not enough to guarantee the matching level. We introduce the concept of Confidence Level to evaluate the global confidence of matching level.\nDefinition 5 (Confidence Level (ρ)). Confidence level evaluates the confidence of the matching level.\nWe propose the following algorithm to evaluate the ME in HMM (h) corresponding to component C. Tm,n = {τ1, τ2, ..., τn}m are test cases results for m outputs of C. The threshold of matching level µ and confidence level ρ are pre-defined."
    }, {
      "heading" : "3.4 Estimating System’s Behavior & Locating Fault",
      "text" : "When the HMM with high matching level and high confidence level is confirmed, we can estimate the component’s status. Using the states calculated from the\nfunction Get Behaviour States(h, Tm,n), e.g. a states behavior sequence is derived, an example of which is given (FIp, FIp, PIf, FIp, ...FIf, FIp, ...).\nIn the states behavior sequence, we focus on the status of component, shown as follow: (F, F, P, F..., F, F, ...). The status with higher occurrence probability is confirmed as this component’s faulty status.\nThis result’s confidence is guaranteed by the confidence level (ρ), and the HMM’s similarity to the component is guaranteed by the matching level (µ). ρ and µ are computed within system’s topological structure, therefore we can compare all the faulty component’s ρ and µ, and give a set of suspect faulty components ordered by ρ and µ."
    }, {
      "heading" : "3.5 Experimental Results",
      "text" : "We design a specific test bed to assess the method’s accuracy and efficiency by generating a large number of use cases. Each use case includes: the system architecture which defines the components and the ports, and their interconnections; the failure probability of each component; the functional specification corresponding to the inputs/outputs.\nThe method assumes that each component in the system has a chance to fail if it has design problems. This probability will be 0 if no design fault is presumed for this component. All functional constraints are based on input/output’s value itself, and for simplification, they are all range constraint, which means they delimit only the min/max value of the input/output. If a faulty component exists, the test bed will by chance give out an out-of-range value for this component’s output. This emulates how a system fails, whatever the model is. Each component’s input and output will be allocated to a variable by the test bed. It guarantees that the interconnected ones share the same variable. The variable will be associated with a random range, which is the functional specification. If a\ndevice is more probable to fail, the test-bed-generated value for its entire output variables will be more probable to go out of the defined range. The approach will use the generated data and the functional specification to automatically locate the faulty component. The test bed will then compare this computed conclusion with the initial context to deduce whether the method is efficient.\nThe test bed generated 1000 use cases to assess the performance in terms of accuracy. The criteria that impacts the accuracy is the complexity of system’s architecture. This can be measured by component number (Fig. 7). and component’s average input & output number (Fig. 8). We find out that this method is more sensible to the average input & output number, while it is more scalable to component number. This method deals with the fault localization for middle-range systems with a accuracy superior to 90%.\nComponent\nThe computation of ME parameters by iteratively searching algorithm consumes time, which is bounded by the iteration limit. This Monte-Carlo algorithm runs from several seconds to several minutes. However, as the computation of each component is independent, the whole method is linearly scalable. For a large system, a parallel cluster will locate the probable design faults within minutes."
    }, {
      "heading" : "4 Conclusion",
      "text" : "Automated fault localization is an important issue in model V&V. It helps the end users in analyzing the origin of failure. In this work, we have shown the early experiments with probabilistic analysis approaches in fault localization. Inspired by the Kullback-Leibler Divergence from Bayesian probabilistic theory, we propose a suspiciousness factor to compute the fault contribution for the transitions in the reachability graph of model checking. The potential faulty transitions are then ranked according to this suspiciousness factor. To automatically locate design faults in the simulation model of detailed design, we propose to use the statistical model Hidden Markov Model (HMM). HMM, as a component’s abstraction, provides statistically identical information to component’s real behavior. The core of this method is a fault localization algorithm that gives out the set of suspicious ranked faulty components and a backward algorithm that computes the matching degree between the HMM and the simulation model to evaluate the confidence degree of the localization conclusion."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work was funded by the FUI Projet P and EuroStars HiMoCo projects."
    } ],
    "references" : [ {
      "title" : "Multiple fault diagnosis in combinational circuits based on an effect-cause analysis",
      "author" : [ "M. Abramovici", "M.A. Breuer" ],
      "venue" : "Computers, IEEE Transactions on 100(6), 451– 460",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Distributional clustering of words for text classification",
      "author" : [ "L.D. Baker", "A.K. McCallum" ],
      "venue" : "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. pp. 96–103. ACM",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "From symptom to cause: localizing errors in counterexample traces",
      "author" : [ "T. Ball", "M. Naik", "S.K. Rajamani" ],
      "venue" : "ACM SIGPLAN Notices 38(1), 97–105",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Explaining abstract counterexamples",
      "author" : [ "S. Chaki", "A. Groce", "O. Strichman" ],
      "venue" : "ACM SIGSOFT Software Engineering Notes. pp. 73–82. ACM",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "System deadlocks",
      "author" : [ "E.G. Coffman", "M. Elphick", "A. Shoshani" ],
      "venue" : "ACM Computing Surveys (CSUR) 3(2), 67–78",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "A family of code coverage-based heuristics for effective fault localization",
      "author" : [ "W. Eric Wong", "V. Debroy", "B. Choi" ],
      "venue" : "Journal of Systems and Software 83(2), 188–208",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Efficient online analysis of accidental fault localization for dynamic systems using hidden markov model",
      "author" : [ "N. Ge", "S. Nakajima", "M. Pantel" ],
      "venue" : "Proceedings of the Symposium on Theory of Modeling & Simulation-DEVS Integrative M&S Symposium. p. 16. Society for Computer Simulation International",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Hidden markov model based automated fault localization for integration testing",
      "author" : [ "N. Ge", "S. Nakajima", "M. Pantel" ],
      "venue" : "Software Engineering and Service Science (ICSESS), 2013 4th IEEE International Conference on. pp. 184–187. IEEE",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Online diagnosis of accidental faults for realtime embedded systems using a hidden markov model",
      "author" : [ "N. Ge", "S. Nakajima", "M. Pantel" ],
      "venue" : "Simulation 91(10), 851–868",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Time properties verification framework for uml-marte safety critical real-time systems",
      "author" : [ "N. Ge", "M. Pantel" ],
      "venue" : "European Conference on Modelling Foundations and Applications. pp. 352–367. Springer",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Real-time property specific reduction for time petri net",
      "author" : [ "N. Ge", "M. Pantel" ],
      "venue" : "International Workshop on Petri Nets and Software Engineering (PNSE@PetriNets). pp. 165–179",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Formal specification and verification of task time constraints for real-time systems",
      "author" : [ "N. Ge", "M. Pantel", "X. Crégut" ],
      "venue" : "International Symposium On Leveraging Applications of Formal Methods, Verification and Validation. pp. 143–157. Springer",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Time properties dedicated transformation from uml-marte activity to time transition system",
      "author" : [ "N. Ge", "M. Pantel", "X. Crégut" ],
      "venue" : "ACM SIGSOFT Software Engineering Notes 37(4), 1–8",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Automated failure analysis in model checking based on data mining",
      "author" : [ "N. Ge", "M. Pantel", "X. Crégut" ],
      "venue" : "International Conference on Model and Data Engineering. pp. 13–28. Springer",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A uml-marte temporal property verification tool based on model checking",
      "author" : [ "N. Ge", "M. Pantel", "X. Crégut" ],
      "venue" : "International Conference on Embedded Real Time Software and Systems (ERTS)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Error explanation with distance metrics",
      "author" : [ "A. Groce" ],
      "venue" : "Tools and Algorithms for the Construction and Analysis of Systems pp. 108–122",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "What went wrong: Explaining counterexamples",
      "author" : [ "A. Groce", "W. Visser" ],
      "venue" : "Model Checking Software, pp. 121–136. Springer",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Finding and fixing faults",
      "author" : [ "B. Jobstmann", "S. Staber", "A. Griesmayer", "R. Bloem" ],
      "venue" : "Journal of Computer and System Sciences 78(2), 441–460",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A statistical interpretation of term specificity and its application in retrieval",
      "author" : [ "K.S. Jones" ],
      "venue" : "Journal of documentation 28(1), 11–21",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Cause clue clauses: error localization using maximum satisfiability",
      "author" : [ "M. Jose", "R. Majumdar" ],
      "venue" : "ACM SIGPLAN Notices. vol. 46, pp. 437–446. ACM",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On information and sufficiency",
      "author" : [ "S. Kullback", "R.A. Leibler" ],
      "venue" : "The Annals of Mathematical Statistics 22(1), 79–86",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1951
    }, {
      "title" : "A tutorial on hidden markov models and selected applications in speech recognition",
      "author" : [ "L.R. Rabiner" ],
      "venue" : "Proceedings of the IEEE 77(2), 257–286",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Fault diagnosis and logic debugging using boolean satisfiability",
      "author" : [ "A. Smith", "A. Veneris", "M.F. Ali", "A. Viglas" ],
      "venue" : "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on 24(10), 1606–1621",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "On diagnosing multiple stuck-at faults using multiple and single fault simulation in combinational circuits",
      "author" : [ "H. Takahashi", "K.O. Boateng", "K.K. Saluja", "Y. Takamatsu" ],
      "venue" : "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on 21(3), 362–368",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A survey of software fault localization",
      "author" : [ "W.E. Wong", "V. Debroy" ],
      "venue" : "University of Texas at Dallas, Tech. Rep. UTDCS-45-09",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Bp neural network-based effective fault localization",
      "author" : [ "W.E. Wong", "Y. Qi" ],
      "venue" : "International Journal of Software Engineering and Knowledge Engineering 19(04), 573–597",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Isolating cause-effect chains from computer programs",
      "author" : [ "A. Zeller" ],
      "venue" : "Proceedings of the 10th ACM SIGSOFT symposium on Foundations of software engineering. pp. 1–10. ACM",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "The execution time is [5,10] for A, and [3,7] for B.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "The execution time is [5,10] for A, and [3,7] for B.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "The execution time is [5,10] for A, and [3,7] for B.",
      "startOffset" : 40,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "The execution time is [5,10] for A, and [3,7] for B.",
      "startOffset" : 40,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4].",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : "To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4].",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4].",
      "startOffset" : 123,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4].",
      "startOffset" : 123,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "the best case execution time (BCET) of B is 5, then the time constraint of B cannot anymore be replaced by [3,4], thus the suspicion of B is largely decreased.",
      "startOffset" : 107,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "the best case execution time (BCET) of B is 5, then the time constraint of B cannot anymore be replaced by [3,4], thus the suspicion of B is largely decreased.",
      "startOffset" : 107,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].",
      "startOffset" : 163,
      "endOffset" : 177
    }, {
      "referenceID" : 16,
      "context" : "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].",
      "startOffset" : 163,
      "endOffset" : 177
    }, {
      "referenceID" : 15,
      "context" : "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].",
      "startOffset" : 163,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].",
      "startOffset" : 163,
      "endOffset" : 177
    }, {
      "referenceID" : 19,
      "context" : "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].",
      "startOffset" : 163,
      "endOffset" : 177
    }, {
      "referenceID" : 13,
      "context" : "Our approach will improve the effectiveness of fault localization by providing a suspiciousness factor which is used to rank the suspicious transitions in the verification model [14].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 9,
      "context" : "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 12,
      "context" : "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 20,
      "context" : "Kullback-Leibler Divergence (also called information divergence, information gain, relative entropy) [21] is a fundamental equation of information theory that qualifies the proximity of two probability distributions.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "We give an example of its application to text classification [2].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "Another major application is the TF-IDF (Term Frequency - Inverse Document Frequency) algorithm [19].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 24,
      "context" : "According to the survey [25], the effectiveness can be assessed by a score EXAM in terms of the percentage of statements that have to be examined until the first statement containing the fault is reached [6,26].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "According to the survey [25], the effectiveness can be assessed by a score EXAM in terms of the percentage of statements that have to be examined until the first statement containing the fault is reached [6,26].",
      "startOffset" : 204,
      "endOffset" : 210
    }, {
      "referenceID" : 25,
      "context" : "According to the survey [25], the effectiveness can be assessed by a score EXAM in terms of the percentage of statements that have to be examined until the first statement containing the fault is reached [6,26].",
      "startOffset" : 204,
      "endOffset" : 210
    }, {
      "referenceID" : 4,
      "context" : "Coffman identified four conditions that must hold simultaneously in order to have a deadlock [5].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "Cause-effect analysis [24,27,18] starts from possible causes",
      "startOffset" : 22,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "Cause-effect analysis [24,27,18] starts from possible causes",
      "startOffset" : 22,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "Cause-effect analysis [24,27,18] starts from possible causes",
      "startOffset" : 22,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Effect-cause analysis [1,23] reasons faulty localization based on observed behavior and expected good functions.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 22,
      "context" : "Effect-cause analysis [1,23] reasons faulty localization based on observed behavior and expected good functions.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 21,
      "context" : "In this work, we make a trade-off of cause-effect and effect-cause analyses and propose an Hidden Markov Model (HMM) [22] based approach for the automated localization of faulty components in the simulation [8,7,9].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "In this work, we make a trade-off of cause-effect and effect-cause analyses and propose an Hidden Markov Model (HMM) [22] based approach for the automated localization of faulty components in the simulation [8,7,9].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 6,
      "context" : "In this work, we make a trade-off of cause-effect and effect-cause analyses and propose an Hidden Markov Model (HMM) [22] based approach for the automated localization of faulty components in the simulation [8,7,9].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 8,
      "context" : "In this work, we make a trade-off of cause-effect and effect-cause analyses and propose an Hidden Markov Model (HMM) [22] based approach for the automated localization of faulty components in the simulation [8,7,9].",
      "startOffset" : 207,
      "endOffset" : 214
    } ],
    "year" : 2016,
    "abstractText" : "ion Issue Fault localization in model checking is challenging as the models have usually a concurrent and indeterministic behavior with many possible execution traces. This behavior is due to the use of abstraction in their design. Without precise information, fault localization may not be precise enough. Given a sequential, or synchronized concurrent, program which exhibits less execution traces, various debugging methods are available to detect and locate the faulty statements. In model-based diagnosis, the use of abstraction is mandatory to reduce the state space explosion problem. At the time of writing, the conflict between model precision and verification cost is a key issue in model checking and model-driven engineering (MDE), therefore a compromise is made to remove the unnecessary information for some verification purpose while keeping all the property-related information. This usually leads to model with concurrent and indeterministic behaviors that exhibits a much larger number of execution traces and are consequently much more complicated to debug. Fault Localization Issue Sometimes it is difficult, even for seasoned experts, to analyze the fault origin. We take a simple example (see Ex. 1) to illustrate this issue. Example 1 (Fault Localization Example). Assume a system consists of two concurrent processes A and B. Both execute only once. The execution time is [5,10] for A, and [3,7] for B. The expected temporal property P is Always A After B. It is obvious that P is unsatisfied. The design fault occurs either on A or on B. To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4]. However, without extra information, A and B exhibit the same suspicion. If an extra information is available, e.g. the best case execution time (BCET) of B is 5, then the time constraint of B cannot anymore be replaced by [3,4], thus the suspicion of B is largely decreased. This example is simple enough to be analyzed manually, while it is impossible for more complex system with thousands of transitions. Any modification on a transition may impact the verification result through time constraint propagation. Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20]. Our approach will improve the effectiveness of fault localization by providing a suspiciousness factor which is used to rank the suspicious transitions in the verification model [14]. The suspiciousness factor is computed using the fault contribution of each transition on the error traces derived from the reachability graph. This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].",
    "creator" : "LaTeX with hyperref package"
  }
}