{
  "name" : "1409.5686.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Transfer Prototype-based Fuzzy Clustering",
    "authors" : [ "Zhaohong Deng", "Yizhang Jiang", "Fu-Lai Chung", "Kup-Sze Choi", "Shitong Wang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "c-mean (FCM) algorithm, usually need sufficient data to find a good clustering partition. If\nthe available data is limited or scarce, most of the existing prototype based clustering\nalgorithms will no longer be effective. While the data for the current clustering task may be\nscarce, there is usually some useful knowledge available in the related scenes/domains. In\nthis study, the concept of transfer learning is applied to prototype based fuzzy clustering\n(PFC). Specifically, the idea of leveraging knowledge from the source domain is exploited to\ndevelop a set of transfer prototype based fuzzy clustering (TPFC) algorithms. Three\nprototype based fuzzy clustering algorithms, namely, FCM, fuzzy k-plane clustering (FKPC)\nand fuzzy subspace clustering (FSC), have been chosen to incorporate with knowledge\nleveraging mechanism to develop the corresponding transfer clustering algorithms. Novel\nobjective functions are proposed to integrate the knowledge of source domain with the data of\ntarget domain for clustering in the target domain. The proposed algorithms have been\nvalidated on different synthetic and real-world datasets and the results demonstrate their\neffectiveness when compared with both the original prototype based fuzzy clustering\nalgorithms and the related clustering algorithms like multi-task clustering and co-clustering.\nKeywords: Prototype based clustering, Transfer learning, Knowledge leverage, Fuzzy\nc-means, Fuzzy k-plane clustering, Fuzzy subspace clustering\nI. Introduction\nTransfer learning [1] is receiving more and more attentions in the fields of machine\nlearning and data mining. They work on the assumption that the data of the current scene is scarce for the current learning tasks, but some useful information of a reference scene is available. The current scene and the reference scene are commonly named as the target domain and the source domain respectively. Currently, transfer learning has been demonstrated as a promising approach to obtain an effective model for the target domain by effectively leveraging useful information from the source domain in the learning process. Its main characteristics can be summarized as follows. First, the data of target domain is not sufficient to train up a good model for the target domain. Second, the source domain is assumed to be related to the target domain, in a sense that they are different but similar to some extent, making the trained model on the source domain not directly adoptable by the target domain. Third, the modeling effect for the target domain is the main concern while that of the source domain is usually not a focus.\nIn the past decade or so, transfer learning has been studied extensively for different\napplications, such as text classification [2] and indoor WiFi location estimation [3]. The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17]. While the first three tasks have been studied quite extensively, works on transfer clustering are still limited despite of the wide range of real-world clustering applications. Clustering has long been a major research topic and many clustering algorithms have been proposed. With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35]. Among these four types of algorithms, prototype based clustering such as k-means is perhaps the most popular one and in fact has been extensively studied. In this paper, we focus on transfer learning for this type of clustering algorithms and propose several representative transfer prototype based clustering algorithms in the context of fuzzy clustering.\nLet us take a look at an example in Fig.1 to illustrate a situation where transfer learning\ncould be exploited. As shown and described in Fig.1 (left part), the data in the target domain is difficult to obtain ideal clustering result with the traditional prototype-based clustering algorithms. However, if the information of the reference scene, i.e., the source domain, in Fig.1 (right part) is considered, more promising clustering result can be expected. For example, if we know that the two domains are related with a prior, we can expect that the number of clusters in these two domains is the same and the induced knowledge in the source domain, such as the cluster centers, can be employed to guide the clustering process in the target domain. It can be seen from this example that how to realize effective knowledge/information transfer from the source domain to the target domain is the key to develop transfer clustering algorithms. It seems that the simplest way to obtain information from the source domain is to use the original data of the source domain directly in the target domain. However, borrowing data from the source domain may suffer from the following problems. First, important information such as the distribution difference between the source and the target domains will be lost if we simply merge the data from the two domains. Second, the data of the source domain may not be appropriate to be directly used in the target domain as there exist similarities as well as differences between the data distributions of the two domains and direct use of the source domain’s data may dominate the data distribution in the target domain. This is particularly true when the size of the dataset in the source domain is much larger. Third, due to the necessity of privacy protection in some applications, such as customers’ personal information, the raw data of source domain may not be directly available.\nAll these problems should be properly addressed in order to develop an effective transfer learning strategy for the prototype based clustering methods.\nIn this paper, transfer learning for prototype based clustering methods is exploited. In\nview of their representativeness, three PFC algorithms, i.e., fuzzy c-mean (FCM) with cluster center prototypes, fuzzy k-plane clustering (FKPC) with cluster direction prototypes and fuzzy subspace clustering (FSC) with cluster subspace prototypes, have been chosen to incorporate with knowledge leveraging mechanism to develop the corresponding TPFC algorithms. The knowledge induced in the source domain, such as the cluster centers and/or the subspace of the clustering results, has been appropriately used to boost the clustering performance in the target domain. A novel objective criterion is proposed to integrate the knowledge of the source domain with the data of the target domain for learning the cluster prototypes in the target domain. Thus, transferring knowledge of the source domain can effectively make up the deficiency in learning from the limited data in the target domain.\nAs mentioned, works in transfer clustering are scarce. In [16], the self-taught clustering\n(STC) has been proposed as a very first transfer clustering algorithm based on mutual information. In [17], a transfer learning version of spectral clustering is proposed. These two transfer clustering algorithms are not developed for prototype based clustering. There exist some obvious differences between these related works and the proposed TPFC algorithms. For the self-taught clustering, it is based on mutual information (MI) and therefore enough data in both of source and target domains are assumed available for estimating the probability densities properly and compute the MI accordingly [16]. Moreover, it assumes that the raw data of source domain are available and this may not always hold in some applications. For the proposed algorithms, raw data of the source domain is not required and data in the target domain can be assumed scarce. For the transfer spectral clustering algorithm [17], it is specifically designed for the spectral clustering while our work focuses on the prototype based fuzzy clustering methods.\nTransfer clustering is much related to co-clustering [36], collaborative clustering [37, 38]\nand multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved. By comparing co-clustering, collaborative and\nmulti-task clustering with the proposed transfer clustering algorithms, we can see that these three types of related clustering techniques assume that the multiple clustering tasks involved are equally important and each can benefit from the others in the clustering process. However, the proposed transfer clustering algorithms only focus on the clustering task in target domain and the source domain is only adopted to enhance the clustering performance in the target domain.\nAnother type of related works is semi-supervised clustering which can be taken as a type\nof knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45]. For example, the most commonly used information is “must_link” or “should_not_link” of the data pairs [41]. For semi-supervised clustering, some prior information, such as labels of partial data, is assumed available and data usually comes from the same domain. Compared with semi-supervised clustering, the proposed transfer clustering algorithms are different in a sense that the data comes from more than one domain.\nThe rest of the paper is organized as follows. In section II, several representative PFC\nalgorithms are briefly reviewed. In section III, the concepts of leveraging knowledge in clustering are introduced, and three TPFC algorithms are proposed. The experimental results are reported and discussed in section IV. Finally, conclusions are given in section V."
    }, {
      "heading" : "II. Prototype Based Clustering",
      "text" : "Prototype based clustering is perhaps the most extensively studied clustering technique.\nIt typically has the following form for objective function:\nJ fU Θ U Θ（ , ) （ , ), (1)\nwhere U is the partition matrix and Θ is the set of prototype parameters. Its aim is to optimize the objective function in Eq. (1) so as to obtain the optimal partition matrix U and the optimal prototype parameter set Θ . Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].\nA. Cluster Center Prototype Based Clustering\nAmong the cluster center prototype based clustering methods, two well-known fuzzy\nalgorithms are the fuzzy c-means (FCM) clustering algorithm [19] and the possibilistic c-means (PCM) clustering algorithm [20] with their objective functions defined respectively as: FCM: 2\n1 1, min C N m j iFCM iji j J u      U V vx (2-1) .st [0,1]iju  , 1 1 C iji u   , 10 N ijj u N    ,\nPCM: 2\n1 1 1 1, min (1 ) C N C Nm m j iPCM ij i iji j i j J u u            U V vx (2-2) .st [0,1]iju  , 10 N ijj u N    ,\nwhere C is the number of clusters; dj Rx is the jth data sample;  , , T i CV v v is the matrix of cluster centers with di Rv ; [ ]ij CxNuU is the fuzzy/possibilistic partition matrix whose element denotes the membership of the jth data sample belonging to the ith class; m is the fuzzy index and i are positive parameters used by PCM. Another representative cluster center prototype based clustering method is the maximal entropy clustering (MEC) algorithm [21] whose objective function is defined as MEC: 2\n1 1 1 1, min ln( )\nC N C N\nj iMEC ij ij iji j i j J u u u           U V vx (2-3)\n.st [0,1]iju  , 1\n1 C\niji u   , 10\nN ijj u N    ,,\nwhere [ ]ij CxNuU is the probabilistic partition matrix whose element denotes the probability of the jth data sample belonging to the ith class, and 1 1 ln( ) C N ij iji j u u\n   denotes the negative Shannon entropy."
    }, {
      "heading" : "B. Cluster Direction Prototype Based Clustering",
      "text" : "Cluster direction prototype based clustering is also a representative branch of prototype\nbased clustering. In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50]. The most representative cluster direction prototype based clustering methods are the k-plane clustering (KPC) algorithm [23] and the fuzzy KPC (FKPC) algorithm [47, 48]. The objective functions of KPC and FKPC can be written as: KPC/ FKPC:\n/ 1 1, , min ( ) K N m T KPC FKPC ij j i ii j J u b      U V b x v (3-1)\n.st [0,1] for FKPC\n{0,1} for KPC\nij ij\nu u  \n ,\n1 1\nK iji u   , 10 N ijj u N    , 1i i T v v ,\nwhere K is the number of clusters;  , , T i CV v v is the matrix of directional vectors of\nthe hyperplanes associated with clusters; [ , , ]Ti Kb bb  is the bias vector of the hyperplanes; [ ]ij CxNuU is the fuzzy/crisp partition matrix; for the fuzzy index m , 1m  and 1m  are set for FKPC and KPC, respectively. Another representative cluster direction prototype based clustering algorithm is the sphere k-means clustering (SKM) algorithm [49, 50] with the following objective function:\nSKM: 1 1,\nmin K N T\nSKM ij j ii j J u      U V x v (3-2)\n.st {0,1}iju  , 1\n1 K\niji u   , 10\nN ijj u N    , 1\ni i\nT v v ,\nwhere the variables are defined as in Eq.(3-1)."
    }, {
      "heading" : "C. Subspace Cluster Prototype Based Clustering",
      "text" : "Subspace cluster prototype based clustering has attracted more and more attentions of\nresearchers in recent years [22, 51-53]. It is also commonly termed as soft subspace clustering. In this type of clustering methods, the prototype of each cluster is characterized by a cluster center and a weighting vector representing the soft subspace for this cluster. One representative algorithm is the fuzzy subspace clustering (FSC) algorithm [51], whose objective function is defined as FSC:   2\n1 1 1 1 1, , min ik ik\nC N d C d\njk ikFSC iji j k i k J u w wx v            \nU V W\n  (4-1)\n.st {0,1}iju  , 1 1 C iji u   , 10 N ijj u N    , [0,1]ikw  , 1 1 d ikk w   , 10 C idi w C    ,\nwhere 1[ , , ] T CW w w is the matrix of weighting vectors and  is the fuzzy index of the fuzzy weighting; [ ]ij CxNuU is the crisp partition matrix, with other parameters defined as in Eq.(2-1). Another representative subspace cluster prototype based clustering algorithm is the entropy weighting k-means (EWKM) algorithm [52], whose objective function is defined as EWKM:   2\n1 1 1 1 1, , min ln( )\nC N d C d\njk ikEWKM ij ik ik iki j k i k J u w w wx v             U V W  (4-2) .st {0,1}iju  , 1 1 C iji u   , 10 N ijj u N    , [0,1]ikw  , 1 1 d ikk w   , 10 C idi w C    ,\nwhere 1 1\nln( ) C d\nik iki k w w    is the negative Shannon entropy and other parameters are\ndefined as in (4-1).\nThe prototype based clustering algorithms have been studied in depth in the past decades.\nHowever, in the context of transfer learning, they are scarcely studied. In the next section, transfer learning is applied to prototype based fuzzy clustering and several TPFC algorithms are developed."
    }, {
      "heading" : "III. Transfer Prototype Based Fuzzy Clustering",
      "text" : "In this section, a knowledge leverage based transfer learning mechanism is firstly\nintroduced, based upon which three TPFC algorithms are proposed."
    }, {
      "heading" : "A. Knowledge Leverage Based TPFC",
      "text" : ""
    }, {
      "heading" : "1) Framework",
      "text" : "As mentioned in the introductory section, when the data is scarce or very limited, the\nexisting prototype based clustering algorithms will no longer be effective and their performance will become inferior. A promising strategy to cope with this challenge is to learn by referring to the information in the source domain which might have useful information for the target domain. From the source domain, there are usually two kinds of information available, namely, the original data and the induced knowledge, as shown in Fig. 2. When the data in the source domain is used directly, it may not always be appropriate. In fact, it is highly difficult to control and balance the similarity and difference of data distributions between the two domains as there possibly exists a drifting of data distribution between domains. On the other hand, the influence of the induced knowledge from source domain can be conveniently controlled as the induced knowledge is much clearer and more concise than the data in source domain. Thus, we consider that it is more appropriate to exploit the use of knowledge rather than data from the source domain to help the clustering task in target domain. In this study, three TPFC algorithms with such a strategy will be presented. Fig.3 shows an illustration of the framework of knowledge leverage based TPFC.\nFig.2 Useful information from source domain for the clustering task in target domain\nSource domain\nData sampled from source\nKnowledge induced from source domain\nTarget\ndomain\nData\nKnowledge"
    }, {
      "heading" : "2) Objective Function",
      "text" : "The following general objective function is proposed for TPFC based on the\naforementioned knowledge leverage based transfer learning strategy:\nmin ;TPFC t sJ f f U Θ U Θ U Θ Θ , （ , )+（ , ), (5)\nwhich consists of two different functional terms. The first functional term tf U Θ（ , ) is inherited from the classical PFC algorithms and is used to learn from the data in target domain. The second functional term ;sf U Θ Θ（ , ) is introduced to carry out knowledge leveraging from the source domain. This term can be in various forms, of which the design depends on what prototype based clustering algorithm is formulated.\nWith this general objective function, three TPFC algorithms, namely, transfer fuzzy\nc-mean (TFCM), transfer fuzzy k-plane clustering (TFKPC) and transfer fuzzy subspace clustering (TFSC), are proposed in this paper. In fact, transfer clustering algorithm for other prototype based clustering algorithms such as possibilistic c-mean and entropy weighting k-mean subspace clustering can be similarly formulated. Before proceeding to present the new transfer clustering algorithms, let us give the assumptions of our formulations. First, the knowledge about the clusters in source domain has been induced in advance by some learning procedures and is available for the clustering task of target domain. Second, the number of\nclusters in target and source domains is the same. Our TPFC algorithms to be proposed are based on these two assumptions. The second one is quite restrictive but there exist difficulties in the practical design for the corresponding algorithms if it is not in place. We consider that developing TPFC algorithms without such an assumption is a valuable and significant research topic in the future."
    }, {
      "heading" : "B. TPFC Algorithms",
      "text" : ""
    }, {
      "heading" : "1) Transfer FCM",
      "text" : "Among the cluster center prototype based clustering methods, fuzzy c-means [19] is of\nhigh popularity and is adopted here to develop our first TPFC algorithm, i.e. transfer FCM (TFCM). To do so, the following objective function is defined:\n2 2\n11 1 1 1,\n2\n2 1 1\nmin\n)\nC N C Nm m j ji iTFCM ij iji j i j\nC N m iij ii j\nJ u u\nu\n   \n \n    \n  \n   \n \nU V v vx x\nvv\n\n\n\n （ (6)\n.st [0,1]iju  , 1\n1 C\niji u   , 10\nN ijj u N    ,\nwhere iv denotes the ith cluster center obtained in the source domain. The concepts behind such a knowledge leverage based transfer learning mechanism can be given as follows.\n(i) The first term in Eq. (6) is directly inherited from the classical FCM, which is mainly\nused to learn from the data available in target domain.\n(ii) The second and third terms are used to learn with the knowledge from source domain,\nwhere the knowledge is the cluster centers obtained in source domain by a certain clustering task.\n(iii) By having the second term, the clustering procedure is apt to obtaining the partition\nmatrix by using the cluster centers obtained in source domain, which implies the use of a supervised learning procedure, such as 1-nearest neighbor (1-NN) classification, in the target domain.\n(iv) By having the third term, the desired cluster centers will be apt to approximate the ones\nobtained in source domain, and thus guiding the clustering procedure to learn from the knowledge of source domain.\n(v) The parameters 1 and 2 , are used to balance the influence of data in target domain\nand knowledge in source domain. A parameter analysis will be given in our experimental\nsection.\nBy using similar optimization strategy in FCM, one can obtain the following learning rules based on Eq. (6).\n21 1\n21 1\nN Nm m j iij ijj j\ni N Nm m ij ijj j\nu u\nu u\n \n \n \n     vx v \n ,\n(7)\n1 ( 1)\n2 2 2\n1 2\n1 ( 1)\n2 2 2 1\n1 2\n1\n1\nm\nj ji i i i\nij m\nC\nk j jk k k k\nu\n\n\n\n          \n          \nv vv vx x\nv vv vx x\n\n\n \n  .\n(8)\nBased on Eqs.(7) and (8), the TFCM algorithm is described below.\nTFCM Algorithm 1) Initialize the number of iterations 0t  and the partition matrix (0)U randomly; Set the maximum number of iterations maxt and\nthreshold  ; Set the balance parameters 1 2,   . 2) Update the matrix of cluster centers ( )tV using (7); 3) Set 1t t  ; 4) Update the partition matrix ( )tU using (8); 5) If ( ) ( -1)t t  U U or maxt t , then terminate; else go to 2).\nFor other popular center-prototype based clustering algorithms, such as PCM [20], one\ncan develop the corresponding transfer learning versions in a similar manner."
    }, {
      "heading" : "2) Transfer FKPC",
      "text" : "Among the cluster direction prototype based clustering methods, FKPC [47] is adopted\nhere to develop the corresponding transfer clustering algorithm, i.e., TFKPC. First, the following objective function is considered:\n2 2\n1 1 1 1, min ( ) ( ) K N K Nm T m T TFKPC ij j i i ij j i ii j i j J u b u b             V b x v x v  (9) .st [0,1]iju  , 1 1 K iji u   , 10 N ijj u N    , 1Ti i v v\nwhere the first term is directly inherited from the original FKPC algorithm and the second term is designed to learn from the knowledge of source domain. Here the second term implies that the proposed algorithm will learn the partition matrix from cluster prototypes, i.e., plane parameters, , i ibv  , obtained in source domain. Thus, by minimizing Eq.(9), the clustering\nprocedure can learn from the data of target domain and the knowledge of source domain\nsimultaneously. The parameter  is used to balance the influence of data in target domain and knowledge in source domain.\nHere, we want to give an additional explanation about the difference between Eq.(6) and\nEq.(9). When comparing Eq.(6) and Eq.(9), we can see that Eq.(6) has introduced two terms, i.e., the assignment of data to the source domain clusters and the matching of the clusters obtained in source and target domains for TFCM, however, only the former term, i.e., the assignment of data to the source domain clusters is used in Eq.(9) for TFKPC. This can be justified as follows. When we design the second part of Eq. (5), i.e., the term for knowledge leveraging, certain factors should be considered. First, the introduced terms should be reasonable, which can really give the useful information for knowledge leveraging. Second, the introduced terms should not bring great difficulty for the solution of the developed objective functions. When the matching term of the clusters obtained in the source and target domains was used to construct the objective function for TFKPC, we found that it is very difficult to solve the corresponding objective function. Thus, only the assignment of data to the source domain clusters is used to develop the knowledge leveraging term of TFKPC.\nBased on similar optimization strategy in FKPC, we can obtain the following learning\nrules (Eqs.[10]-[12]) for TFKPC:\n( ) 2Ti i i i X D S X v v (10)\nwith  1, , NX x x , 1 0\n0\nm i\ni\nm iN\nu\nu\n           D      ， T i i i T i S  u u eu , 1 2( , , ) Tm m m i i i iNu u uu  , and the\neigenvector associated with the minimal positive eigenvalue obtained by Eq.(10) is just the\ndesired hyperplane direction vector iv ;\n1\n1\nN m T jij ij\ni N m ijj\nu b\nu\n\n\n   v x (11)\n1 ( 1)\n2 2 1\n1 ( 1)\n2 2 1\n1\n1\n( ) ( )\n1\n( ) ( )\nm\nT T j i i j i i\nij m C\nT T k\nj k k j k k\nb b u\nb b\n\n\n\n   \n        \n    \nx v x v\nx v x v\n\n\n\n\n(12)\nThus, the TFKPC algorithm can be described as follows.\nTFKPC Algorithm 1) Initialize the number of iterations 0t  and the partition matrix (0)U\nrandomly; Set the maximum number of iterations maxt and threshold  ; Set\nthe balance parameters  . 2) Update the matrix of directional vectors of planes ( )tV using (10); 3) Update the bias of the planes ( )ib t using (11); 4) Let 1t t  ; 5) Update the partition matrix ( )tU using (12); 6) If ( ) ( -1)t t  U U or maxt t , then terminate; else go to 2)."
    }, {
      "heading" : "3) Transfer FSC",
      "text" : "Among the existing subspace cluster prototype based clustering methods, the FSC [51] is\nadopted to study the corresponding transfer learning version, i.e., TFSC. Thus, the following objective function is proposed:\n \n   \n2\n1 1 1 1 1, ,\n2 2\n1 21 1 1 1 1 1\nmin\n( )\nC N d C dm ij jk ikTFSC ik iki j k i k\nC N d C N dm m ij jk ik ij ikik ik iki j k i j k\nJ w wu x v\nw w vu x v u v\n    \n     \n   \n     \n    \n     \nU V W\n  \n \n \n\n \n.st [0,1]iju  , 1\n1 C\niji u   , 10\nN ijj u N    , [0,1]ikw  , 1 1 d ik w   . (13)\nIn Eq.(13), the first two terms are directly inherited from the classical FSC algorithm except\nthat we have extended the hard partition {0,1}iju  in FSC to the fuzzy partition  0,1iju \nfor easy optimization and knowledge leveraging. The third and fourth terms are used to learn from the knowledge of source domain. The third term implies that we can learn the partition matrix by referring to subspace cluster prototypes obtained in the source domain, characterized by the cluster centers and weighting vectors, i.e., ikv and ikw  . The fourth\nterm intends to make the desired cluster centers apt to approximate the ones obtained in the source domain under the subspace constraints of source domain. Thus, by minimizing Eq. (13), the clustering procedure can learn from the data of target domain and the knowledge of source domain simultaneously. Again, by following the optimization strategy in FSC, we can obtain the following learning rules:\n2 31 1 1\n2 31 1 1\nN N Nm m m ij ij ij ikik jk ik jk ikj j j\nik N N Nm m m ij ij ijik ik ikj j j\nw x w x wu u u v v\nw w wu u u\n  \n  \n    \n              \n  \n  \n \n  ,\n(14)\n   \n1 ( 1) 1 ( 1)\n2 21\n1 1\n1 1d ik N Nsm m\njk ik js isij ijj j\nw u ux v x v\n \n\n                       \n \n  ,\n(15)\n  1 ( 1)1 ( 1)\n1 1 1\nmm C\nij ikij k d du\n\n        ,\n(16)\n      2 2 2\n1 21 1 1\nd d d\njk ik jk ik ikik ik ik ik ikk k k d w w w vx v x v v                  . (17)\nBased on Eqs.(14)-(17), the TFSC algorithm can be described below.\nTFSC Algorithm 1) Initialize the number of iterations 0t  and the partition matrix (0)U\nrandomly; Set the maximum number of iterations tmax and threshold  ; Set the balance parameters 1 2,   .\n2) Update the matrix of cluster centers ( )tV using (14); 3) Update the matrix of weighting vectors ( )tW using (15); 4) Set 1t t  ; 5) Update the partition matrix ( )tU using (16); 6) If ( ) ( -1)t t  U U or t=tmax, then terminate; else go to 2)."
    }, {
      "heading" : "C. Computational Complexity and Convergence Analysis",
      "text" : "The computational complexities of the proposed algorithms can be described as follows.\nFor the TFCM and TFKPC algorithms, their computational complexities are ( )O TNC TC ,\nwhere T is the total number of iterations, N is the size of dataset, and C is the number of clusters. In other words, they are of the same order as those of the classical FCM and FKPC algorithms. For the TFSC algorithm, its computational complexity is ( )O TNC TC TCd  ,\nwhere d is the number of features, which is of the same order as that of the classical FSC.\nFor the convergence of the three proposed TPFC algorithms, Zangwill’s convergence\ntheorem can be adopted as in the convergence studies of many existing clustering algorithms, e.g. [51]. Let’s recall Zangwill’s convergence theorem 1 [54, 55].\nTheorem 1 (Zangwill’s convergence theorem): Let : ( )A V P V be a point-to-set map. Given a point z(0)V, let A determine an algorithm that generates the sequence {z(k)}. Let S V be the solution set. Assume that (i) All points z(k) are in a compact subset of V.\n(ii) There is a continuous function J:V→R, satisfying (a) if zS, then for any yA(z), J(y) <\nJ(z); (b) if zΩ, then either the algorithm terminates or for any yA(z), J(y) ≤ J(z).\n(ii) The map A is closed at z if zS.\nThen, either the algorithm stops at a solution or the limit of any converged subsequence is a solution.\nBased on the above theorem, our aim is to prove that the three conditions in theorem 1\nare satisfied by the proposed three algorithms. Here, we first prove the convergence of the TFCM algorithm. With similar strategy in [51], we can prove the corresponding theorems 2a-2c with the auxiliary definitions 1-4. In order to save space of this paper, we omit the details of proofs about theorems 2a-2c. Definition 1. Let  1, , ND  x x be a finite data set in the Euclidean space dR , the set of all fuzzy C-partition of D is denoted by CM , i.e.,   satisfies the constraint AC CNM B U U , where CNB denotes the vector space of all real C N matrices and constraint A is defined as\n[0, 1]iju  , 1 i C  , 1 j N  ,\n1 1, 1\nC iji u j N     , 1 0 , 1 N ijj u N i C      .\nDefinition 2. Let 1 : Cd CG M R be a function defined as 1 1 2( ) ( , , ) T CG  U Z z z z , where the vectors 1 2( , , ) T d i i i idz z z R z  , 1 i C  , are computed via Eq. (10) using U .\nDefinition 3. Let 2 : Cd CG R M be a point-to-set map defined as  2 ( ) satisfies constraint AG Z U U . Definition 4.  1 2( , ) ( , ) ( ), ( )TFCMT G G  V U V U V U U V      , i.e., TFCM iteration can be expressed using a point-to-set map TFCMT : ( ) Cd Cd C CR M P R M   defined by the composition 2 1TFCMT A A  , where 1 1( , ) ( ( ), )A GV U U U and  2 2( , ) ( , ) ( )A G V U V U U V . Thus,      2 1 2 1 2 2( , ) ( , ) ( , ) ( ), , ( , ) ( )TFCMT A A A G A G     V U V U V U U U V U V U U V      .\nTheorem 2a. Let  1, , ND  x x contain at least C ( N ) distinct points and let (0) (0)( , )V U be the starting point of iteration of TFCMT with (0) CMU and  (0) (0)1GV U . Then, the iteration sequence  ( ) ( ), ( )r rV U , 1,2,r  , is contained in a compact subset of\nCd CR M .\nTheorem 2b. Let 1m  , 1 0  and 2 0  are fixed and  1, , ND  x x contain at least\nC ( N ) distinct points. Let the solution set S of the optimization problem     , min , Cd C TFCM R M J  U V V U be defined as\n          ( , ) , , and , , Cd C TFCM TFCM C\nTFCM TFCM\nS R M J J M\nJ J\n     \n  \nV U V U V U U\nV U V U V V .\nLet ( , )V U Cd CR M  . Then,    , ,TFCM TFCMJ JV U V U   for every ( , ) ( , )TFCMTV U V U   and\nthe inequality is strict if ( , ) SV U .\nTheorem 2c. Let 1m  , 1 0  and 2 0  are fixed and  1, , ND  x x contain at least C ( N ) distinct points. Then the point-to-set map TFCMT : ( ) Cd Cd C CR M P R M   is closed at every point in Cd CR M .\nSince the function defined in Eq.(6) is continuous, the following theorem, which\nestablishes the convergence for the TFCM algorithm, follows immediately from Theorems 2a-2c and Zangwill’s convergence Theorem1.\nTheorem 3. (Convergence of TFCM). Let  1, , ND  x x contain at least C ( N ) distinct points and let  ,TFCMJ V U define as in Eq.(6). Let (0) (0)( , )V U be the starting point of iterations with TFCMT with (0) CMU and  (0) (0)1GV U . Then, the iteration sequence  ( ) ( ),r rV U , 1,2,r  , either terminates as a point  * *,V U in the solution set S or there\nis a subsequence converging to a point in S .\nSimilarly, the convergence of TFKPC and TFSC algorithms can be proved in a similar\nmanner and is omitted here."
    }, {
      "heading" : "IV. Experimental Results",
      "text" : "The proposed three TPFC algorithms, i.e., TFCM, TFKPC and TFSC, have been\nextensively evaluated on synthetic and real-world datasets. In this section, the indices used for performance evaluation and the experimental setup are first described. Then, the performance of the proposed algorithms on synthetic and real-world text datasets is reported and discussed. A further comparison with other related algorithms is conducted on several synthetic and real-world datasets. All the experiments were implemented with Matlab codes on a computer with 1.66GHz CPU and 2GB RAM."
    }, {
      "heading" : "A. Performance Indices and Experimental Setup",
      "text" : "Two metrics, i.e., the rand index (RI) and the normalized mutual information (NMI) [56],\nare used to evaluate the performance of the clustering algorithms. RI is commonly defined as\n2/)1(\n1100\n\n \nNN\nff RI\nwhere 00f is the number of pairs of data points having different class labels and belonging to different clusters; 11f is the number of pairs of data points having the same class labels and belonging to the same clusters; N is the size of the whole dataset. NMI is defined and computed according to the formula\n \n\n \n \n\n\n\n C\ni\nC\nj\nj\nj i i\nC\ni\nC\nj ji\nji\nji\nN\nN N\nN\nN N\nNN\nNN N\nNMI\n1 1\n1 1\n,\n,\nloglog\nlog\nwhere jiN , is the number of agreements between cluster i and class j ; iN is the number of data points in cluster i ; jN is the number of data points in class j and N is the size of the whole dataset. Both RI and NMI take a value within the interval [0, 1]. The higher the values, the better the clustering performance is.\nWhen the RI and NMI are computed, the prior labels of the data must be available. In\naddition to them, an index that does not require the labels of data is also used for performance evaluation. Since most of the adopted methods for comparison are fuzzy measure based methods, the fuzzy validity measure is naturally very appropriate. Here, the classical fuzzy validity measure, i.e., Xie-Beni (XB) index, and its two modified versions were adopted to further evaluate and compare the performance of different fuzzy measure based algorithms.\nThe classical XB index and its extended version mXB can be respectively defined for FCM\nas:\n \n22\n1 1\n1\n( , ; ) sep( )min || ||\nC N j iiji j\ni ji j\nu NNXB\n  \n\n    \n   \n  vx U V X\nVv v ,\n \n2\n1 1\n1\n( , ; ) sep( )min || ||\nC N m j iiji j\nm\ni ji j\nu NNXB\n  \n\n    \n   \n  vx U V X\nVv v .\nUsually, the smaller the index values, the better the clustering performance is.\nAs the XB index was originally designed for FCM only, two modified XB indices are\npresented to evaluate the direction prototype and subspace cluster prototype based algorithms here. For FKPC and TFKPC, we propose to use the following modified XB index:\n \n2\n1 1\n1 ( )\n( , ; ) sep( )min || || | |\nC N m T ij j i ii j\nFKPC\ni j i j i j\nu b NNXB\nb b\n  \n\n    \n     \n  x v U V X\nVv v .\nFor FSC and TFSC, another modified XB index which includes the subspace information is proposed:\n \n  \n2\n1 1 1\n2\n1\n1\n( , , ; ) sep( , )min\nC N dm jk ikij iki j k\nFSC d\nik jkikki j\nu w x v NNXB w v v\n\n\n   \n\n    \n   \n  \n U V W X\nV W .\nThe proposed transfer clustering algorithms are compared with the corresponding\nnon-transfer learning counterparts in sub-sections IV-B & IV-C. In section IV-D, a comparison with several related learning algorithms is presented. In section IV-E some discussions are presented. In our experiments, the clustering process of all algorithms is repeated 20 times for each parameter setting on a dataset. The maximum number of iterations, i.e., maxt , is set to 100 for all iteration based clustering algorithms."
    }, {
      "heading" : "B. Synthetic Datasets",
      "text" : ""
    }, {
      "heading" : "1) Evaluation of TFCM Algorithm",
      "text" : "In this subsection, a synthetic dataset, denoted as D1, with predetermined cluster\nstructures is used to evaluate the performance of the proposed TFCM algorithm. Data was generated for the source domain and the target domain, respectively. Each domain contains three clusters with different Gaussian distributions as shown in Fig 4 and the parameters used to generate the data are listed in Table I. From Table I, we can see that the three clusters in the two domains have similar but different means and covariance. Fig. 4 shows that the data in the target domain is scarce while the data in the source domain should be enough to guide the clustering task in the target domain.\nalgorithm in order to evaluate its transfer learning abilities. The clustering results, in terms of RI, NMI and XB values for 20 runs, are tabulated in Tables II-V.\nby the two algorithms are reported. It shows that the optimized clustering results of TFCM are obviously superior to those of the classical FCM algorithm. For example, in terms of the NMI index, TFCM obtains the mean and standard deviation as 0.8084 and 2.34e-16 respectively for the data in target domain, while FCM only obtains the corresponding values as 0.7352 and 0.0141.\nIn addition, Table II shows that FCM has obtained better mean values of different\nperformance indices in the target domain rather than in the source domain. It can be explained by the fact that the clusters in the source domain of Fig.4 are unbalanced, i.e., two smaller clusters and one bigger cluster, which make the FCM more sensitive to initialization. For the data in target domain, the three clusters are more balanced. Thus, we can observe that the mean clustering result (RI: 0.8091) in the source domain for 20 runs is inferior to that (RI: 0.8625) obtained in the target domain due to initialization sensitivity. However, we can also see that the best clustering result (RI: 0.9622) obtained in a certain run for the source domain is obviously better than that (RI: 0.8788) obtained in the target domain. This indicates that when FCM has good initialization in both domains, the source domain can contribute useful information to the target domain in obtaining better clustering results.\nTables III-V record the means and standard deviations of the RI, NMI and XB values\nobtained by the proposed TFCM algorithm using different parameter settings. Based on these results, the following observations can be obtained. (1) When the two parameters in Eq.(6) are set as 1 0  and 2 0  , the TFCM algorithm\nobtains the worst clustering effect as indicated by the mean NMI and RI values in Part A of Tables III and IV, while clustering performances have been improved when 1 and 2 are set to different positive values. Here, when 1 0  and 2 0  , TFCM degenerates to the classical FCM. Thus this confirms the advantage of knowledge leveraging in our transfer clustering algorithm. (2) From Part B of Tables III, IV and V, we can see that when the knowledge of source\ndomain is used, the standard deviations of the three indices (RI, NMI and XB) have been reduced. This implies that the introduction of the knowledge of source domain has enhanced the stability/robustness of the algorithm. (3) Tables III-V show that the transfer of the knowledge of source domain to the clustering in\ntarget domain has to be properly controlled. Although the knowledge of source domain can improve the clustering performance, the improvement will be degraded if very large 1 and 2 values are used. From our experimental results, setting 1ϵ[0.5,1] and 2ϵ[0.5,1] can obtain more promising clustering performance on the adopted synthetic dataset D1.\n(4) Tables III-V show that the trends of the clustering results with the three indices are similar\nunder different parameter settings, i.e., when the value of one index is better, usually but not absolutely, the other two are also better. (5) Tables III-V also show that XB index is more sensitive to the parameter setting than RI\nand NMI indices. XB index only obtains the best mean value in a small range of parameter values, while RI and NMI indices have obtained the best mean values in a much larger range of parameter values. However, XB index also has its own advantage, i.e., the label information is not required in the calculation of the index values."
    }, {
      "heading" : "2) Evaluation of TFKPC Algorithm",
      "text" : "structures is used to evaluate the performance of the proposed TFKPC algorithm. Data was generated for the source domain and the target domain, respectively, where each domain\ncontains three clusters located at different hyperplanes with different cluster direction prototypes as shown in Fig 5. The parameters used to generate the data are listed in Table VI. From Table VI, we can see that the three clusters in the two domains have similar but different hyperplane parameters. In particular, Gaussian noise has been added in the synthetic dataset with the given noise parameter, i.e., the standard deviation  as shown in Table VI.\nFig. 5 shows that the data in the target domain is scarce while the data in the source domain is sufficient and helpful to improve the clustering effect in the target domain.\nthe results like those in previous subsection are recorded in Tables VII-X. From the results, one can obtain the following observations. (1) Table VII shows that the parameter optimized clustering results of TFKPC are obviously\nsuperior to those of the classical FKPC algorithm. For example, TFKPC obtained the mean and standard deviation as 0.8898 and 0 on the data in the target domain with RI index, but FKPC only obtained the corresponding values as 0.7506 and 0.1472. (2) In particular, when =0, TFKPC is degenerated into FKPC. Tables VIII-X show that\nTFKPC with =0 usually obtain the worse clustering performance while the clustering performance of TFKPC improves when  is set to non-zero positive values. This once again confirms the knowledge leveraging ability of the proposed transfer clustering algorithm. (3) From Tables VIII-X, we can see that the FKPC algorithm is more sensitive to\ninitialization, which can be observed from the higher standard deviation values of the performance indices obtained in 20 runs. The proposed TFKPC has introduced the knowledge leveraging term in the objective function, which can effectively weaken the sensitivity to initialization to some extent, and thus the clustering results become more stable. For example, in Table VIII, when λ is smaller, meaning that the influence of knowledge leveraging is smaller, the clustering results of 20 runs are more fluctuated. On the other hand, when λ becomes larger, the trend of standard deviation of the performance index shows that the clustering results are becoming steadier. The observation does not always hold as the initialization of TFKPC algorithm is random for 20 runs. For example, in Table VIII, the standard deviations of RI are 0.0524 and 0.0941 for λ=0.5 and 0.7, respectively. But in general, when the knowledge of source domain is used effectively, the standard deviations of the three performance indices can be reduced. This demonstrates that the introduction of knowledge from the source domain really enhances the stability of the proposed TFKPC algorithm. (4) Tables VIII-X also show that although the knowledge of source domain may effectively\nimprove the clustering performance, it should not be overused. The very large  values cannot result in getting the optimal clustering results. As in TFCM, an appropriate  value is needed for TFKPC to balance the influence of knowledge of the source domain and data of the target domain. From our experimental results, ϵ[0.7, 1] is appropriate for TFKPC to obtain much better clustering effect on the synthetic dataset D2."
    }, {
      "heading" : "3) Evaluation of TFSC Algorithm",
      "text" : "structures is used to evaluate the performance of the proposed TFSC algorithm. Data was\ngenerated for the source domain and the target domain, respectively, where each domain\ncontains three clusters located at different subspace as shown in Fig 6 and the corresponding\nparameters used to generate the data are listed in Table XI. Fig. 6(a) and Fig. 6(b) contain\nthree sub-figures, each of which corresponds to a cluster where the high dimensional data in a\ncluster are taken as the sequences and plotted in the corresponding sub-figure with the\nsequence number of features and the value of features as the x-coordinate and y-coordinate,\nrespectively. From these sub-figures, we can see the corresponding subspace of each cluster,\nin which the features are more important than other features to the associated cluster. For\nexample, from three sub-figures in Fig. 6(a), we can see that most features of three clusters\nhave the feature values uniformly distributed in the interval [0, 100], but for each cluster\nthere is a subset of features whose values are distinctive from other clusters. As shown in Fig.\n6(a), three clusters in the source domain have the distinctive feature subsets with the\nsequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.\nThe performance of the TFSC algorithm is compared with that of the classical FSC\nalgorithm with the results reported in Table XII. Here, the parameter analysis results like those in Tables III-V have been omitted to save space. Basically, the observations for TFSC are very similar to those for TFCM and TFKPC. Let’s summarize them as follows. (1) Table XII shows that the parameter optimized clustering result of TFSC are obviously\nsuperior to that of the classical FSC algorithm. In particular, TFSC has obtained the mean values of both NMI and RI as 1 for the data in the target domain, which implies that all the data have been correctly clustered, while the clustering effect of the traditional FSC is inferior to that of TFSC obviously.\n(2) Although omitted here, the parameter analysis results show that the introduction of\nknowledge of source domain has enhanced the stability/robustness of the proposed TFSC algorithm. Again, appropriate parameter values should be set in order to optimize the advantages of the proposed TFSC algorithm. For our experiments with synthetic dataset D3, parameter 2 should not be set too large, typically, 2≤0.1. On the other hand, parameter 1 can be set within a wider range, e.g., 1ϵ[0.5, 5].\n(3) As in TFCM, the parameter analysis results of TFSC show that the trends of the\nclustering results with the three indices are similar under different parameter settings, i.e., when the value of one index is better, the other two are typically better.\nof TFSC is also compared with that of FSC and the results are reported in Table XIII with the identification rates computed. Obviously, the identification rate of FSC (35/69) has been improved to that of TFSC (52/69) by leveraging the knowledge of source domain. Specifically, Table XIII shows that TFSC has identified more truly important features than FSC in the corresponding embedded subspace for each cluster. For example, TFSC has detected 24 important features while FSC only identifies 9 important features for the 1th cluster.\nC. 20 Newsgroups Text Clustering\nIn this subsection, the proposed three TPFC algorithms are further experimented to\nevaluate their text clustering performance. The 20 newsgroups (20NG) text datasets [57] were adopted. In order to simulate the application scenarios studied in this paper, four categories of text data were adopted and each of them contains different sub-clusters in the source domain and target domain with the details shown in Table XIV. From Table XIV, we can see that the constructed datasets have the following distinctive characteristics. (1) The data in the source and target domains are related. The data in both domains belong to\nfour different clusters, i.e., comp, rec, sci and talk.\n(2) Although the data in both domains are related, there exist obvious differences between\nthem. The same cluster in different domains contains very different sub-clusters. For example, the source data of the rec cluster belong to three sub-clusters, i.e., rec.autos, rec.motorcycles and rec.sport.baseball, while the target data only belong to the sub-cluster rec.sport.hockey. Thus, the data distributions in both domains are related but different to some extent.\n(3) The size of dataset in the target domain is much smaller than that in the source domain.\nThere is a practical requirement to improve the clustering effect on the data in the target domain by effectively using the knowledge of source domain. For the adopted data, dimensionality reduction has been applied by using the BOW\ntoolkit [58] to effectively preprocess the high dimensional data into the final data containing 800 effective features used for clustering. For the proposed algorithms, the parameter settings are similar to that used in subsection IV-B. The proposed three TPFC algorithms, i.e., TFCM, TFKPC and TFSC, and their non-transfer counterparts have been evaluated on the constructed text datasets. In our experiments, we found that the cluster direction prototype based algorithms, i.e., FKPC and TFKPC, failed to provide meaningful clustering results on this dataset due to the high dimensionality involved and thus their clustering results are omitted here.\nIn Table XV, the clustering results with the optimized parameters are reported, including\nthe means and standard deviations of NMI, RI and XB/XBFSC values after 20 runs of every algorithm with the optimal parameter setting. Meanwhile, the clustering results of the proposed TFSC algorithm under different parameter settings are reported in Tables XVI-XVIII for the performance index NMI, RI and XBFSC, respectively. To save space, the clustering results of the proposed TFCM algorithm under different parameter settings are omitted here since the trends of its clustering results are similar to that of TFSC. From the results, we can obtain the following observations. (1) As shown in Table XV, in this high dimensional dataset, the two subspace clustering\nalgorithms FSC and TFSC have shown obvious advantage to the two non-subspace clustering algorithms FCM and TFCM. Again, the experimental results follow those in\nthe previous experiments that the proposed TPFC algorithms TFCM and TFSC are superior to their non-transfer learning counterparts.\n(3) Tables XVI-XVIII show that the introduction of knowledge of source domain has really\nimproved the clustering effect of the proposed transfer clustering algorithms. Meanwhile, Tables XVI-XVIII show that appropriate parameter values should be set in order to optimize the advantages of the proposed TFSC algorithm.\n(4) By comparing Tables XVI-XVIII for different performance indices, we can see that in\nmost situations when XBFSC is better, the corresponding NMI and RI indices are also better. Since XBFSC can be directly calculated without requiring the prior label information of the data, it is a feasible strategy to take the XBFSC as a criterion to determine the approximate parameter values."
    }, {
      "heading" : "D. Comparison with Related Clustering Algorithms",
      "text" : "related works, namely, two transfer clustering algorithms, i.e., self-taught clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and co-clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] on both synthetic and real-world datasets used in subsections IV-B & C. For the proposed clustering algorithms, the parameter setting is the same as that described in subsection IV-B, and for the LSSMTC algorithm, the regularization parameter [0,1]  has been tried with\nthe values in the set of {0.01,0.1,0.3,0.5,0.7,0.9,1} and for the co-clustering algorithm, the related regularization parameters, λ and µ, have been set as λ=µ and tried in the set of {0.01, 0.1,0.3,0.5,0.7,0.9,1}. The parameter setting about TSC and STC is referred to [17]. Especially, for TSC algorithm it is required that the dimensional number of data is not smaller than the number of clusters. Thus it is not applicable to two low dimensional datasets, i.e., D1 and D2.\nFor all the algorithms, the results obtained with the optimal parameter setting are\nreported in term of the means of 20 runs. The experimental results are reported in Table XIX based upon which the following observations can be made. (1) The proposed TFCM and TFSC algorithms have shown highly competitive performance\nwith respect to the five related algorithms which have used the knowledge or data in the other tasks. In particular, the TFSC algorithm has shown better performance than the other algorithms in most of the adopted datasets.\n(2) Although the proposed TFKPC algorithm is inferior to the other non-cluster direction\nprototype based algorithms in most of the datasets, it shows an obvious advantage on the D2 dataset, which is a direction oriented dataset. Thus, TFKPC is appropriate for the datasets of this type.\n(3) Among the three proposed transfer fuzzy clustering algorithms, TFSC is obviously\nsuperior to the other two, particularly in the high dimensional datasets, such as D3 and 20NG datasets.\nIn addition, the performance index XB and modified XB obtained by different fuzzy measure based algorithms are compared in Table XX. From the results, we can see that the proposed transfer fuzzy clustering algorithms have obtained improved clustering performance compared with the corresponding no-transfer counterparts in term of the XB and modified XB indices.\nTable XXI. From these results, we can see that the speed of the proposed transfer clustering algorithms is similar to that of the corresponding non-transfer counterparts, which also validates the theoretical analyses in section III-C."
    }, {
      "heading" : "E. Discussion",
      "text" : "relations/limitations between the two dataset sizes (source/target) and the performance, which can be analyzed from three aspects. First, in this study, an assumption is given that the data in target domain is scarce such that transfer learning is required to improve the clustering performance in the target domain. Therefore, when the size of the data in the target domain is\nsmall, the proposed transfer clustering algorithms may obtain improved clustering results when compared with the non-transfer counterparts. Second, when the size of the dataset in target domain is much larger, the information is usually sufficient for obtaining effective clustering by the traditional non-transfer clustering algorithms. In this case, even if transfer clustering algorithms are adopted, its advantage will not be obvious. Third, in the proposed transfer clustering algorithms, the original raw data in the source domain is not used and only some knowledge, such as cluster centers, are adopted in the clustering procedure in the target domain. Thus, the reliability of the knowledge obtained in the source domain has an important influence on the clustering performance of the proposed transfer clustering algorithms. If the size of data in the source domain is small, the induced knowledge may not be reliable. When the size of data in the source domain is much larger, more reliable knowledge can be expected from the source domain. With such an analysis, we can infer that the larger the size of dataset in the source domain is, the more reliable the knowledge obtained in the source domain and the more effective the proposed transfer clustering algorithms will be in the target domain.\nThe second issue is about the influence of the balance parameters in the proposed\nobjective functions, such as parameter  for TFKPC. As shown by the experimental results, the balance parameter values are set at about 0.5 or larger to get good results for most synthetic datasets, but for the real-world datasets they are set at about 0.1 or less for good performance. With respect to this observation, we give the following explanations. First, the balance parameters control the influence of knowledge in the source domain to the target domain. It is expected that if the knowledge induced in the source is more useful to the target domain, the balance parameter should be set with a much larger value. For the synthetic datasets, they have been designed specifically in accordance with the scene where transfer learning is really required. Thus, the knowledge obtained in the source is more useful to the target domain and the balance parameters can be set with much larger value, such as 0.5, to get a good performance. However, for the applications to real-world datasets, the balance parameter has been set with much smaller values. It may be due to the fact that real-world datasets are very complicated and the amount of knowledge to be transferred is not known. It is better to take a more conservative approach to set the balance parameter’s value. The\nexperimental results demonstrate that even if only a smaller value is used, knowledge leveraging is still important and effective and the performance of the proposed algorithms has been obviously improved when compared with the non-transfer counterparts.\nThe third issue is how to determine the appropriate values for the related parameters. In\nthe proposed algorithms, it is necessary to set the balance parameters, i.e., 1 and 2 or , appropriately for good performance. Unlike supervised learning, no supervised information can be used by parameter selection strategies, such as the commonly used cross validation strategy. For most unsupervised learning algorithms, it is still an open problem to determine the optimal values for the parameters involved. In our experiments, we can see that with a pretty wide range of parameter values, the proposed transfer clustering algorithms can obtain improved performance compared with the non-transfer counterparts. In fact, the optimal parameters are usually dependent on applications. Thus, for a specific application, users can determine an appropriate range for the involved parameters of the adopted algorithm by the prior knowledge or the available validation dataset. In particular, a promising strategy is to determine the appropriate values for the involved parameters in the proposed fuzzy measure based clustering algorithms by using some evaluation indices, such as the XB and modified XB indices. The conclusion here is obtained based on the facts that (i) the XB index and modified XB indices, unlike the RI and NMI indices, can be computed without the supervised label information; (ii) in most situations when the XB index or the modified XB index is better in a certain parameter setting, the obtained RI and NMI indices are usually much better accordingly. In addition, in order to weaken the influence of the parameters, some other investigations can also be done as suggested in [59, 60]. For example, the ensemble learning technique seems to be a promising one. By integrating the clustering results obtained under different parameter settings, ensemble clustering will provide comparable result with the case under the optimal parameter setting [59, 60]."
    }, {
      "heading" : "V. Conclusions",
      "text" : "In this study, the concept of knowledge leveraging is used to develop a set of transfer\nprototype based fuzzy clustering methods for the application scenarios where the data is limited or insufficiently available for effective clustering. Based on the introduced\nknowledge-leverage transfer learning mechanism, several transfer prototype based clustering algorithms are exploited to address the deficiency caused by the scarceness of data in the target domain. The proposed prototype based clustering algorithms learn from not only the data of the target domain but also the knowledge of the source domain. The experimental results have demonstrated the attractiveness and effectiveness of the proposed transfer prototype based fuzzy clustering algorithms when compared with the existing classical prototype based clustering algorithms without the transfer learning abilities, and other related algorithms such as the multi-task clustering algorithm.\nIn future, more works can be addressed about the proposed transfer clustering technique.\nAs mentioned in subsection III-A-2, our work assumes that the number of clusters in the target domain is the same as that in the source domain. This assumption facilitates simplicity in the development of transfer prototype based clustering algorithms. However, it also prohibits the proposed algorithms from some applications, i.e., the cases where the number of clusters may be different between the source domain and the target domain. For such a challenge, the following problems should be well considered to develop the extended algorithms. First, an effective method should be investigated to check whether the cluster numbers between the target domain and the source domain are the same. Second, when we know that the numbers of clusters in the target and source domains are different, how to effectively transfer useful information from the source to the target domain has to be addressed. This is a challenging problem for the development of transfer prototype based clustering algorithms. Our future work on transfer clustering will also include works for non-prototype based clustering methods."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part by the Hong Kong Polytechnic University under Grant\nG-UA68, National Natural Science Foundation of China under Grants 61170122, 2012NSFC,\nNatural Science Foundation of Jiangsu Province under Grant 2012JSNSFC and JiangSu 333\nExpert Engineering Grant (BRA2011142)."
    } ],
    "references" : [ {
      "title" : "A survey on transfer learning",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "IEEE Trans. Knowledge Data Engineering, vol. 22, no. 10, pp. 1345–1359, 2010.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On minimum distribution discrepancy support vector machine for domain adaptation",
      "author" : [ "J.W. Tao", "F.L. Chung", "S.T. Wang" ],
      "venue" : "Pattern Recognition, vol. 45, no. 11, pp. 3962-3984, 2012.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive localization through transfer learning in indoor Wi-Fi environment",
      "author" : [ "Z. Sun", "Y.Q. Chen", "J. Qi", "J.F. Liu" ],
      "venue" : "Proc. 7th International Conference on Machine Learning and Applications, pp. 331–336, 2008.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Discriminative learning for differing training and test distributions",
      "author" : [ "S. Bickel", "M. Brückner", "T. Scheffer" ],
      "venue" : "Proc. 24th Int. Conf. Machine Learning, pp. 81-88, 2007.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning to learn with the informative vector machine",
      "author" : [ "N.D. Lawrence", "J.C. Platt" ],
      "venue" : "Proc. 21st Int. Conf. Machine Learning, 2004.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Knowledge transfer via multiple model local structure mapping",
      "author" : [ "J. Gao", "W. Fan", "J. Jiang", "J. Han" ],
      "venue" : "Proc. 14th ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining, pp. 283-291, 2008.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Transfer learning by mapping with minimal target data",
      "author" : [ "L. Mihalkova", "R.J. Mooney" ],
      "venue" : "Proc. Assoc. for the Advancement of Artificial Intelligence (AAAI ’08) Workshop Transfer Learning for Complex Tasks, 2008.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Deep transfer via second-order Markov logic",
      "author" : [ "J. Davis", "P. Domingos" ],
      "venue" : "Proc. Assoc. for the Advancement of Artificial Intelligence (AAAI ’08) Workshop Transfer Learning for Complex Tasks, 2008.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Domain adaptation via transfer component analysis",
      "author" : [ "S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang" ],
      "venue" : "IEEE Trans. Neural Networks, vol. 22, no.2, pp.199-210, 2011.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Transferred dimensionality reduction",
      "author" : [ "Z. Wang", "Y. Song", "C. Zhang" ],
      "venue" : "Proc. European Conf. Machine Learning and Knowledge Discovery in Databases (ECML/PKDD ’08), pp. 550-565, 2008.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Bayesian task-level transfer learning for non-linear regression",
      "author" : [ "P. Yang", "Q. Tan", "Y. Ding" ],
      "venue" : "Proc. Int. Conf. on Computer Science and Software Engineering, pp. 62-65, 2008.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Application of transfer regression to TCP throughput prediction",
      "author" : [ "L. Borzemski", "G. Starczewski" ],
      "venue" : "Proc. First Asian Conference on Intelligent Information and Database Systems, pp.  28-33, 2009.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Transfer regression model for indoor 3D location estimation",
      "author" : [ "J. Liu", "Y. Chen", "Y. Zhang" ],
      "venue" : "Lecture Note on Computer Science 5916, pp. 603–613, 2010.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Knowledge-Leverage based TSK fuzzy system modeling",
      "author" : [ "Z.H. Deng", "Y.Z. Jiang", "K.S. Choi", "F.L. Chung", "S.T. Wang" ],
      "venue" : "IEEE Trans. Neural Networks and Learning Systems, vol. 24, no. 8, pp. 1200-1212, 2013.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Knowledge-Leverage based fuzzy system and its modeling",
      "author" : [ "Z.H. Deng", "Y.Z. Jiang", "F.L. Chung", "H. Ishibuchi", "S.T. Wang" ],
      "venue" : "IEEE Trans. Fuzzy systems, vol.21, no. 4, pp. 597-609, 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Self-taught clustering",
      "author" : [ "W. Dai", "Q. Yang", "G. Xue", "Y. Yu" ],
      "venue" : "Proc. 25th Int. Conf. Machine Learning, pp. 200-207, 2008.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Transfer spectral clustering",
      "author" : [ "W.H. Jiang", "F.L. Chung" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, pp. 789-803, 2012.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A fuzzy relative of the ISODATA process and its use in detecting compact well separated clusters",
      "author" : [ "J. Dunn" ],
      "venue" : "Journal of Cybernetics, vol. 3, no. 3, pp.32–57, 1973.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Pattern Recognition with Fuzzy Objective Function Algorithms",
      "author" : [ "J.C. Bezdek" ],
      "venue" : "New York Plenum,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1981
    }, {
      "title" : "A possibilistic approach to clustering.",
      "author" : [ "R. Krishnapuram", "J.M. Keller" ],
      "venue" : "IEEE Trans. on Fuzzy Systems, vol.1,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1993
    }, {
      "title" : "A maximum entropy approach to fuzzy clustering",
      "author" : [ "R.P. Li", "M.A. Mukaidono" ],
      "venue" : "Proc on IEEE Int Conf Fuzzy Syst. Yokohama, Japan, pp. 2227-2232, 1995.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Automated variable weighting in k-means type clustering",
      "author" : [ "J.Z. Huang", "M.K. Ng", "H. Rong", "Z. Li" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell. 27 (5) (2005) 657–668.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "K-plane clustering",
      "author" : [ "P.S. Bradley", "Q.L. Mangasarian" ],
      "venue" : "Journal of Global Optimization, vol. 16, no. 1, pp. 23-32, 2000.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A density-based algorithm for discovering clusters in large spatial databases with noise",
      "author" : [ "M. Ester", "H.P. Kriegel", "J. Sander", "X. Xu" ],
      "venue" : "Proc. 2nd International Conference on KDD, pp. 226-231, 1996.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Density-based clustering in spatial databases: The algorithm GDBSCAN and its applications",
      "author" : [ "J. Sander", "M. Ester", "H.P. Kriegel", "X. Xu" ],
      "venue" : "Data Mining and Knowledge Discovery, vol. 2, no. 2, pp. 169-194, 1998.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "An efficient approach to clustering in large multimedia databases with noise",
      "author" : [ "A. Hinneburg", "D.A. Keim" ],
      "venue" : "Proc. 2nd International Conference on KDD, pp. 58–65, 1998.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "OPTICS: Ordering points to identify the clustering structure",
      "author" : [ "M. Ankerst", "M.M. Breunig", "H.P. Kriegel", "J. Sander" ],
      "venue" : "Proc. International Conference on Management of Data, pp. 49–60, 1999.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "DECODE: a new method for discovering clusters of different densities in spatial data",
      "author" : [ "T. Pei", "A. Jasra", "D.J. Hand", "A.X. Zhu", "C. Zhou" ],
      "venue" : "Data Mining and Knowledge Discovery, vol. 18, no. 3, pp. 337–369, 2009.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine. Intelligence, vol. 22, no. 8, pp. 888–905, 2000.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A clustering algorithm based on graph connectivity",
      "author" : [ "E. Hartuv", "R. Shamir" ],
      "venue" : "Information Processing Letter, vol. 76, no. 4, pp. 175–181, 2000.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "Advances in Neural Information Processing Systems, vol. 2, pp. 849-856, 2002.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Fast graph-based relaxed clustering for large data sets using minimal enclosing ball",
      "author" : [ "P.J. Qian", "F.L. Chung", "S.T. Wang", "Z.H. Deng" ],
      "venue" : "IEEE Trans. Systems, Man, and Cybernetics, Part B- Cybernetics, vol. 42, no. 3, pp. 672-687, 2012.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Support vector clustering",
      "author" : [ "A. Ben-Hur", "D. Horn", "H. Siegelmann", "V. Vapnik" ],
      "venue" : "Journal of Machine Learning Research, vol. 2, pp. 125–137, 2001.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A genetic approach to the automatic clustering problem",
      "author" : [ "L. Tseng", "S. Yang" ],
      "venue" : "Pattern Recognition, vol. 34, no. 2, pp. 415–424, 2001.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The self-organizing map",
      "author" : [ "T. Kohonen" ],
      "venue" : "Proc. IEEE, vol. 78, no. 9, pp.1464–1480, 1990.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Co-clustering on manifolds",
      "author" : [ "Q.Q. Gu", "J. Zhou" ],
      "venue" : "Proc. 15th International Conference on KDD, pp. 359-368, 2009.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Collaborative fuzzy clustering",
      "author" : [ "W. Pedrycz" ],
      "venue" : "Pattern Recognition Letters, vol. 23, no. 14, pp. 1675-1686, 2002.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Collaborative clustering with the use of Fuzzy C-Means and its quantification",
      "author" : [ "W. Pedrycz", "P. Rai" ],
      "venue" : "Fuzzy Sets and Systems, vol. 159, no. 18, pp. 2399-2427, 2008.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning the shared subspace for multi-task clustering and transductive transfer classification",
      "author" : [ "Q.Q. Gu", "J. Zhou" ],
      "venue" : "Proc. 9th IEEE International Conference on Data Mining, pp. 159-168,  2009.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multi-task clustering via domain adaptation",
      "author" : [ "Z.H. Zhang", "J. Zhou" ],
      "venue" : "Pattern Recognition, vol. 45, no. 1, pp. 465-473, 2012.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Integrating constraints and metric learning in semi-supervised clustering",
      "author" : [ "M. Bilenko", "S. Basu", "R.J. Mooney" ],
      "venue" : "Proc. 21th International Conference on Machine learning, pp.81 -88, 2004.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Semantic web content analysis: A study in proximity-based collaborative clustering",
      "author" : [ "V. Loia", "W. Pedrycz", "S. Senatore" ],
      "venue" : "IEEE Trans. Fuzzy Systems, vol. 15, no. 6, pp. 1294–1312, 2007.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Fuzzy clustering with a knowledge-based guidance",
      "author" : [ "W. Pedrycz" ],
      "venue" : "Pattern Recognition Letters, vol. 25, no. 4, pp. 469–480, 2004.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "P-FCM: A proximity based fuzzy clustering",
      "author" : [ "W. Pedrycz", "V. Loia", "S. Senatore" ],
      "venue" : "Fuzzy Sets and Systems, vol. 148, no. 1, pp. 21–41, 2004.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Clustering analysis of gene expression data based on semi-supervised visual clustering algorithm",
      "author" : [ "F.L. Chung", "S.T. Wang", "Z.H. Deng", "S.Chen", "D.W. Hu" ],
      "venue" : "Soft Comput. vol. 10, no. 11, pp. 981-993, 2006.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A modified k-plane clustering algorithm for identification of hybrid systems",
      "author" : [ "M. Tabatabaei-Pour", "K. Salahshoor", "B. Moshiri" ],
      "venue" : "Proc. 6 World Congress on Intelligent Control and Automation, pp. 1333-1337, 2006.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fuzzy k-Plane clustering algorithm",
      "author" : [ "Y. Wang", "S.C Chen", "D.Q. Zhang", "X.B. Yang" ],
      "venue" : "Pattern Recognition and Artificial Intelligence, vol. 20, no. 5, pp. 704-710, 2007.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A spatially constrained fuzzy hyper-prototype clustering algorithm",
      "author" : [ "J. Liu", "T.D. Pham" ],
      "venue" : "Pattern Recognition, vol. 45, no. 4, pp. 1759-1771, 2012.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Generative model based clustering of directional data",
      "author" : [ "A Banerjee", "I.S. Dhillon", "J. Ghosh" ],
      "venue" : "Conf erence on Knowledge Discovery in Data, Washington, DC, 2003.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Concept decompositions for large sparse text data using clustering",
      "author" : [ "I.S. Dhillon", "D.S.Modha1" ],
      "venue" : "Machine Learning, vol. 42 no.1, pp.143- 175, 2001.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A convergence theorem for the fuzzy subspace clustering (FSC) algorithm",
      "author" : [ "G.J. Gan", "J.H. Wu" ],
      "venue" : "Pattern Recognition, vol. 41, no. 6, pp. 1939–1947, 2008.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 1939
    }, {
      "title" : "An entropy weighting k-means algorithm for subspace  clustering of high-dimensional sparse data",
      "author" : [ "L.P. Jing", "M.K. Ng", "Z.X. Huang" ],
      "venue" : "IEEE Trans. Knowledge and Data Engineering, vol. 19, no. 8, pp. 1026–1041, 2007.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Enhanced soft subspace clustering integrating within-cluster and between-cluster information",
      "author" : [ "Z.H. Deng", "K.S. Choi", "F.L. Chung", "S.T. Wang" ],
      "venue" : "Pattern Recognition, vol. 43, no. 3, pp. 767-781, 2010.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An improved convergence theorem for the fuzzy c-means clustering algorithms",
      "author" : [ "R. Hathaway", "J. Bezdek", "W. Tucker" ],
      "venue" : "in: J. Bezdek (Ed.), Analysis of Fuzzy Information, vol. III, CRC Press, Boca Raton, pp. 123–131, 1987.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Non-linear Programming: A Unified Approach",
      "author" : [ "W. Zangwill" ],
      "venue" : null,
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 1969
    }, {
      "title" : "Distance-based clustering of CGH data",
      "author" : [ "J. Liu", "J. Mohammed", "J. Carter" ],
      "venue" : "Bioinformatics, vol. 22, no. 16, pp. 1971–1978, 2006.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Co-clustering based classification for out-of-domain documents",
      "author" : [ "W.Y. Dai", "G.R. Xue", "Q. Yang", "Y. Yu" ],
      "venue" : "Proc. 13th International Conference on KDD, pp. 210-219, 2007.",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering",
      "author" : [ "A.K. McCallum" ],
      "venue" : "http://www-2.cs.cmu.edu/~mccallum/bow/.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Weighted cluster ensembles: Methods and analysis",
      "author" : [ "C. Domeniconi", "M. Al-Razgan" ],
      "venue" : "ACM Transactions on Knowledge Discovery from Data (TKDD), vo. 2, no. 4, pp.1-40, 2009.",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Projective clustering ensembles",
      "author" : [ "G. Francesco", "C. Domeniconi", "A. Tagarelli" ],
      "venue" : "Data Mining and Knowledge Discovery, vol. 26, no. 3, pp. 452-511, 2013.",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Transfer learning [1] is receiving more and more attentions in the fields of machine learning and data mining.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "In the past decade or so, transfer learning has been studied extensively for different applications, such as text classification [2] and indoor WiFi location estimation [3].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "In the past decade or so, transfer learning has been studied extensively for different applications, such as text classification [2] and indoor WiFi location estimation [3].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 106,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 10,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 202,
      "endOffset" : 209
    }, {
      "referenceID" : 11,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 202,
      "endOffset" : 209
    }, {
      "referenceID" : 12,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 202,
      "endOffset" : 209
    }, {
      "referenceID" : 13,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 202,
      "endOffset" : 209
    }, {
      "referenceID" : 14,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 202,
      "endOffset" : 209
    }, {
      "referenceID" : 15,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 16,
      "context" : "The existing works can be generally divided into four categories: 1) transfer learning for classification [2-8]; 2) transfer learning for feature extraction [9, 10]; 3) transfer learning for regression [11-15] and 4) transfer learning for clustering [16, 17].",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 17,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 21,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 23,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 150,
      "endOffset" : 157
    }, {
      "referenceID" : 24,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 150,
      "endOffset" : 157
    }, {
      "referenceID" : 25,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 150,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 150,
      "endOffset" : 157
    }, {
      "referenceID" : 27,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 150,
      "endOffset" : 157
    }, {
      "referenceID" : 28,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 196,
      "endOffset" : 203
    }, {
      "referenceID" : 29,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 196,
      "endOffset" : 203
    }, {
      "referenceID" : 30,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 196,
      "endOffset" : 203
    }, {
      "referenceID" : 31,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 196,
      "endOffset" : 203
    }, {
      "referenceID" : 32,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 274,
      "endOffset" : 281
    }, {
      "referenceID" : 33,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 274,
      "endOffset" : 281
    }, {
      "referenceID" : 34,
      "context" : "With respect to the concern here, they can be classified as: 1) prototype based clustering algorithms [18-23]; 2) density based clustering algorithms [24-28]; 3) graph based clustering algorithms [29-32]; and 4) clustering algorithms based on other data modeling mechanisms [33-35].",
      "startOffset" : 274,
      "endOffset" : 281
    }, {
      "referenceID" : 15,
      "context" : "In [16], the self-taught clustering (STC) has been proposed as a very first transfer clustering algorithm based on mutual information.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "In [17], a transfer learning version of spectral clustering is proposed.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "For the self-taught clustering, it is based on mutual information (MI) and therefore enough data in both of source and target domains are assumed available for estimating the probability densities properly and compute the MI accordingly [16].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 16,
      "context" : "For the transfer spectral clustering algorithm [17], it is specifically designed for the spectral clustering while our work focuses on the prototype based fuzzy clustering methods.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 35,
      "context" : "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 36,
      "context" : "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.",
      "startOffset" : 84,
      "endOffset" : 92
    }, {
      "referenceID" : 37,
      "context" : "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.",
      "startOffset" : 84,
      "endOffset" : 92
    }, {
      "referenceID" : 38,
      "context" : "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 39,
      "context" : "Transfer clustering is much related to co-clustering [36], collaborative clustering [37, 38] and multi-task clustering [39, 40], where multiple clustering tasks are usually handled simultaneously and they may cooperate with each other in order to improve the clustering performance of all clustering tasks involved.",
      "startOffset" : 119,
      "endOffset" : 127
    }, {
      "referenceID" : 40,
      "context" : "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].",
      "startOffset" : 187,
      "endOffset" : 194
    }, {
      "referenceID" : 41,
      "context" : "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].",
      "startOffset" : 187,
      "endOffset" : 194
    }, {
      "referenceID" : 42,
      "context" : "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].",
      "startOffset" : 187,
      "endOffset" : 194
    }, {
      "referenceID" : 43,
      "context" : "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].",
      "startOffset" : 187,
      "endOffset" : 194
    }, {
      "referenceID" : 44,
      "context" : "Another type of related works is semi-supervised clustering which can be taken as a type of knowledge based clustering that makes use of some useful knowledge in the clustering procedure [41-45].",
      "startOffset" : 187,
      "endOffset" : 194
    }, {
      "referenceID" : 40,
      "context" : "For example, the most commonly used information is “must_link” or “should_not_link” of the data pairs [41].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 22,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 45,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 46,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 47,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 48,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 49,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 183,
      "endOffset" : 194
    }, {
      "referenceID" : 21,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 244,
      "endOffset" : 255
    }, {
      "referenceID" : 50,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 244,
      "endOffset" : 255
    }, {
      "referenceID" : 51,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 244,
      "endOffset" : 255
    }, {
      "referenceID" : 52,
      "context" : "Let us firstly review the following three types of prototype based clustering, namely, cluster center prototype based clustering [18-21], cluster direction prototype based clustering [23, 46-50], and subspace cluster prototype based clustering [22, 51-53].",
      "startOffset" : 244,
      "endOffset" : 255
    }, {
      "referenceID" : 18,
      "context" : "Among the cluster center prototype based clustering methods, two well-known fuzzy algorithms are the fuzzy c-means (FCM) clustering algorithm [19] and the possibilistic c-means (PCM) clustering algorithm [20] with their objective functions defined respectively as: FCM: 2 1 1 , min C N m j i FCM ij i j J u       U V v x (2-1)",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "Among the cluster center prototype based clustering methods, two well-known fuzzy algorithms are the fuzzy c-means (FCM) clustering algorithm [19] and the possibilistic c-means (PCM) clustering algorithm [20] with their objective functions defined respectively as: FCM: 2 1 1 , min C N m j i FCM ij i j J u       U V v x (2-1)",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 0,
      "context" : "st [0,1] ij u  , 1 1 C ij i u    , 1 0 N ij j u N     , PCM: 2 1 1 1 1 , min (1 ) C N C N m m j i PCM ij i ij i j i j J u u              U V v x (2-2)",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "st [0,1] ij u  , 1 0 N ij j u N     , where C is the number of clusters; d j R  x is the jth data sample;   , , T i C  V v v  is the",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 20,
      "context" : "Another representative cluster center prototype based clustering method is the maximal entropy clustering (MEC) algorithm [21] whose objective function is defined as",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "st [0,1] ij u  , 1 1 C ij i u    , 1 0 N ij j u N     ,, where [ ] ij CxN u  U is the probabilistic partition matrix whose element denotes the probability of the jth data sample belonging to the ith class, and 1 1 ln( ) C N ij ij i j u u     denotes the negative Shannon entropy.",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 45,
      "context" : "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 46,
      "context" : "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 47,
      "context" : "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 48,
      "context" : "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 49,
      "context" : "In this type of clustering methods, the directional vector is used to characterize the prototypes [23, 46-50].",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "The most representative cluster direction prototype based clustering methods are the k-plane clustering (KPC) algorithm [23] and the fuzzy KPC (FKPC) algorithm [47, 48].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 46,
      "context" : "The most representative cluster direction prototype based clustering methods are the k-plane clustering (KPC) algorithm [23] and the fuzzy KPC (FKPC) algorithm [47, 48].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 47,
      "context" : "The most representative cluster direction prototype based clustering methods are the k-plane clustering (KPC) algorithm [23] and the fuzzy KPC (FKPC) algorithm [47, 48].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "st [0,1] for FKPC {0,1} for KPC ij",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 48,
      "context" : "Another representative cluster direction prototype based clustering algorithm is the sphere k-means clustering (SKM) algorithm [49, 50] with the following objective function: SKM: 1 1 , min K N T SKM ij j i i j J u      U V x v (3-2)",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 49,
      "context" : "Another representative cluster direction prototype based clustering algorithm is the sphere k-means clustering (SKM) algorithm [49, 50] with the following objective function: SKM: 1 1 , min K N T SKM ij j i i j J u      U V x v (3-2)",
      "startOffset" : 127,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "Subspace Cluster Prototype Based Clustering Subspace cluster prototype based clustering has attracted more and more attentions of researchers in recent years [22, 51-53].",
      "startOffset" : 158,
      "endOffset" : 169
    }, {
      "referenceID" : 50,
      "context" : "Subspace Cluster Prototype Based Clustering Subspace cluster prototype based clustering has attracted more and more attentions of researchers in recent years [22, 51-53].",
      "startOffset" : 158,
      "endOffset" : 169
    }, {
      "referenceID" : 51,
      "context" : "Subspace Cluster Prototype Based Clustering Subspace cluster prototype based clustering has attracted more and more attentions of researchers in recent years [22, 51-53].",
      "startOffset" : 158,
      "endOffset" : 169
    }, {
      "referenceID" : 52,
      "context" : "Subspace Cluster Prototype Based Clustering Subspace cluster prototype based clustering has attracted more and more attentions of researchers in recent years [22, 51-53].",
      "startOffset" : 158,
      "endOffset" : 169
    }, {
      "referenceID" : 50,
      "context" : "One representative algorithm is the fuzzy subspace clustering (FSC) algorithm [51], whose objective function is defined as FSC:   2 1 1 1 1 1 , , min ik ik C N d C d jk ik FSC ij i j k i k J u w w x v              U V W    (4-1) .",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "st {0,1} ij u  , 1 1 C ij i u    , 1 0 N ij j u N     , [0,1] ik w  , 1 1 d ik k w    , 1 0 C id i w C     , where 1 [ , , ] T C  W w w  is the matrix of weighting vectors and  is the fuzzy index of the fuzzy weighting; [ ] ij CxN u  U is the crisp partition matrix, with other parameters defined as in Eq.",
      "startOffset" : 64,
      "endOffset" : 69
    }, {
      "referenceID" : 51,
      "context" : "Another representative subspace cluster prototype based clustering algorithm is the entropy weighting k-means (EWKM) algorithm [52], whose objective function is defined as",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "st {0,1} ij u  , 1 1 C ij i u    , 1 0 N ij j u N     , [0,1] ik w  , 1 1 d ik k w    , 1 0 C id i w C     ,",
      "startOffset" : 64,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "TPFC Algorithms 1) Transfer FCM Among the cluster center prototype based clustering methods, fuzzy c-means [19] is of high popularity and is adopted here to develop our first TPFC algorithm, i.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "st [0,1] ij u  , 1 1 C ij i u    , 1 0 N ij j u N     , where i v denotes the ith cluster center obtained in the source domain.",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 19,
      "context" : "For other popular center-prototype based clustering algorithms, such as PCM [20], one can develop the corresponding transfer learning versions in a similar manner.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 46,
      "context" : "2) Transfer FKPC Among the cluster direction prototype based clustering methods, FKPC [47] is adopted here to develop the corresponding transfer clustering algorithm, i.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "st [0,1] ij u  , 1 1 K ij i u    , 1 0 N ij j u N     , 1 Ti i  v v where the first term is directly inherited from the original FKPC algorithm and the second term is designed to learn from the knowledge of source domain.",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "[10]-[12]) for TFKPC: ( ) 2 T i i i i    X D S X v v (10)",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[10]-[12]) for TFKPC: ( ) 2 T i i i i    X D S X v v (10)",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 50,
      "context" : "3) Transfer FSC Among the existing subspace cluster prototype based clustering methods, the FSC [51] is adopted to study the corresponding transfer learning version, i.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "st [0,1] ij u  , 1 1 C ij i u    , 1 0 N ij j u N     , [0,1] ik w  , 1 1 d i k w    .",
      "startOffset" : 3,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "st [0,1] ij u  , 1 1 C ij i u    , 1 0 N ij j u N     , [0,1] ik w  , 1 1 d i k w    .",
      "startOffset" : 64,
      "endOffset" : 69
    }, {
      "referenceID" : 50,
      "context" : "[51].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 53,
      "context" : "Let’s recall Zangwill’s convergence theorem 1 [54, 55].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 54,
      "context" : "Let’s recall Zangwill’s convergence theorem 1 [54, 55].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 50,
      "context" : "With similar strategy in [51], we can prove the corresponding theorems 2a-2c with the auxiliary definitions 1-4.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : ",   satisfies the constraint A C CN M B   U U , where CN B denotes the vector space of all real C N  matrices and constraint A is defined as [0, 1] ij u  , 1 i C   , 1 j N   ,",
      "startOffset" : 146,
      "endOffset" : 152
    }, {
      "referenceID" : 55,
      "context" : ", the rand index (RI) and the normalized mutual information (NMI) [56], are used to evaluate the performance of the clustering algorithms.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "Both RI and NMI take a value within the interval [0, 1].",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 198,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 198,
      "endOffset" : 206
    }, {
      "referenceID" : 14,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 209,
      "endOffset" : 217
    }, {
      "referenceID" : 7,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 209,
      "endOffset" : 217
    }, {
      "referenceID" : 8,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 218,
      "endOffset" : 226
    }, {
      "referenceID" : 26,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 218,
      "endOffset" : 226
    }, {
      "referenceID" : 0,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 227,
      "endOffset" : 236
    }, {
      "referenceID" : 11,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 227,
      "endOffset" : 236
    }, {
      "referenceID" : 14,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 239,
      "endOffset" : 248
    }, {
      "referenceID" : 12,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 239,
      "endOffset" : 248
    }, {
      "referenceID" : 8,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 249,
      "endOffset" : 257
    }, {
      "referenceID" : 24,
      "context" : "Table I Parameters used to generate the synthetic dataset (D1) for evaluating TFCM algorithm Domain Source domain Target domain Cluster Cluster-1 Cluster-2 Cluster-3 Cluster-1 Cluster-2 Cluster-3 u [ 1 , 8]  [15 , 8] [9 , 27] [ 1 , 12]  [15 , 13] [9 , 25]",
      "startOffset" : 249,
      "endOffset" : 257
    }, {
      "referenceID" : 0,
      "context" : "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.",
      "startOffset" : 136,
      "endOffset" : 143
    }, {
      "referenceID" : 30,
      "context" : "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.",
      "startOffset" : 136,
      "endOffset" : 143
    }, {
      "referenceID" : 9,
      "context" : "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 39,
      "context" : "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 19,
      "context" : "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 54,
      "context" : "6(a), three clusters in the source domain have the distinctive feature subsets with the sequence number of features in the intervals of [1, 31], [10, 40] and [20, 55], respectively.",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 56,
      "context" : "The 20 newsgroups (20NG) text datasets [57] were adopted.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 57,
      "context" : "For the adopted data, dimensionality reduction has been applied by using the BOW toolkit [58] to effectively preprocess the high dimensional data into the final data containing 800 effective features used for clustering.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : ", self-taught clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and co-clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] on both synthetic and real-world datasets used in subsections IV-B & C.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : ", self-taught clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and co-clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] on both synthetic and real-world datasets used in subsections IV-B & C.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 38,
      "context" : ", self-taught clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and co-clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] on both synthetic and real-world datasets used in subsections IV-B & C.",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 39,
      "context" : ", self-taught clustering (STC) [16] and transfer spectral clustering (TSC) [17], two collaborative clustering algorithms CombKM and co-clustering (DRCC) [39] and a multi-task clustering algorithm LSSMTC [40] on both synthetic and real-world datasets used in subsections IV-B & C.",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 0,
      "context" : "For the proposed clustering algorithms, the parameter setting is the same as that described in subsection IV-B, and for the LSSMTC algorithm, the regularization parameter [0,1]   has been tried with",
      "startOffset" : 171,
      "endOffset" : 176
    }, {
      "referenceID" : 16,
      "context" : "The parameter setting about TSC and STC is referred to [17].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 58,
      "context" : "In addition, in order to weaken the influence of the parameters, some other investigations can also be done as suggested in [59, 60].",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 59,
      "context" : "In addition, in order to weaken the influence of the parameters, some other investigations can also be done as suggested in [59, 60].",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 58,
      "context" : "By integrating the clustering results obtained under different parameter settings, ensemble clustering will provide comparable result with the case under the optimal parameter setting [59, 60].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 59,
      "context" : "By integrating the clustering results obtained under different parameter settings, ensemble clustering will provide comparable result with the case under the optimal parameter setting [59, 60].",
      "startOffset" : 184,
      "endOffset" : 192
    } ],
    "year" : 2014,
    "abstractText" : "The traditional prototype based clustering methods, such as the well-known fuzzy c-mean (FCM) algorithm, usually need sufficient data to find a good clustering partition. If the available data is limited or scarce, most of the existing prototype based clustering algorithms will no longer be effective. While the data for the current clustering task may be scarce, there is usually some useful knowledge available in the related scenes/domains. In this study, the concept of transfer learning is applied to prototype based fuzzy clustering (PFC). Specifically, the idea of leveraging knowledge from the source domain is exploited to develop a set of transfer prototype based fuzzy clustering (TPFC) algorithms. Three prototype based fuzzy clustering algorithms, namely, FCM, fuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have been chosen to incorporate with knowledge leveraging mechanism to develop the corresponding transfer clustering algorithms. Novel objective functions are proposed to integrate the knowledge of source domain with the data of target domain for clustering in the target domain. The proposed algorithms have been validated on different synthetic and real-world datasets and the results demonstrate their effectiveness when compared with both the original prototype based fuzzy clustering algorithms and the related clustering algorithms like multi-task clustering and co-clustering.",
    "creator" : "þÿ"
  }
}