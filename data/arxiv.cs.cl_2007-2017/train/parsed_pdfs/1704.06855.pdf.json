{
  "name" : "1704.06855.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Multitask Learning for Semantic Dependency Parsing",
    "authors" : [ "Hao Peng", "Sam Thomson", "Noah A. Smith" ],
    "emails" : [ "hapeng@cs.washington.edu,", "nasmith@cs.washington.edu,", "sthomson@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Labeled directed graphs are a natural and flexible representation for semantics (Copestake et al., 2005; Baker et al., 2007; Surdeanu et al., 2008; Banarescu et al., 2013, inter alia). Their generality over trees, for instance, allows them to represent relational semantics while handling phenomena like coreference and coordination. Even syntactic formalisms are moving toward graphs (de Marneffe et al., 2014). However, full semantic graphs can be expensive to annotate, and efforts are fragmented across competing semantic theories, leading to a limited number of annotations in any one formalism. This makes learning to parse more difficult, especially for powerful but data-hungry machine learning techniques like neural networks.\nIn this work, we hypothesize that the overlap among theories and their corresponding represen-\nLast week , shareholders took their money and ran .\narg1\nloc\ntop\narg1 poss\narg2\n_and_c\narg1\n(a) DM\narg1\ntop\narg1 arg1\narg2 coord\narg1arg1\ncoord\nLast week , shareholders took their money ran .and (b) PAS\nLast week , shareholders took their money and ran .\nrstr twhen\ntop\nact app\npat conj\nact\ntop\ntwhen conj\n(c) PSD\nFigure 1: An example sentence annotated with the three semantic formalisms of the broad-coverage semantic dependency parsing shared tasks.\ntations can be exploited using multitask learning (Caruana, 1997), allowing us to learn from more data. We use the 2015 SemEval shared task on Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2015) as our testbed. The shared task provides an English-language corpus with parallel annotations for three semantic graph representations, described in §2. Though the shared task was designed in part to encourage comparison between the formalisms, we are the first to treat SDP as a multitask learning problem.\nAs a strong baseline, we introduce a new system that parses each formalism separately (§3). It uses a bidirectional-LSTM composed with a multi-layer perceptron to score arcs and predicates, and has efficient, nearly arc-factored inference. Experiments show it significantly improves on state-of-the-art methods (§3.4).\nWe then present two multitask extensions (§4.2\nar X\niv :1\n70 4.\n06 85\n5v 2\n[ cs\n.C L\n] 2\n5 A\npr 2\n01 7\nand §4.3), with a parameterization and factorization that implicitly models the relationship between multiple formalisms. Experiments show that both techniques improve over our basic model, with an additional (but smaller) improvement when they are combined (§4.5). Our analysis shows that the improvement in unlabeled F1 is greater for the two formalisms that are more structurally similar, and suggests directions for future work. Finally, we survey related work (§5), and summarize our contributions and findings (§6)."
    }, {
      "heading" : "2 Broad-Coverage Semantic Dependency Parsing (SDP)",
      "text" : "First defined in a SemEval 2014 shared task (Oepen et al., 2014), and then extended by Oepen et al. (2015), the broad-coverage semantic depency parsing (SDP) task is centered around three semantic formalisms whose annotations have been converted into bilexical dependencies. See Figure 1 for an example. The formalisms come from varied linguistic traditions, but all three aim to capture predicate-argument relations between content-bearing words in a sentence.\nWhile at first glance similar to syntactic dependencies, semantic dependencies have distinct goals and characteristics, more akin to semantic role labeling (SRL; Gildea and Jurafsky, 2002) or the abstract meaning representation (AMR; Banarescu et al., 2013). They abstract over different syntactic realizations of the same or similar meaning (e.g., “She gave me the ball.” vs. “She gave the ball to me.”). Conversely, they attempt to distinguish between different senses even when realized in similar syntactic forms (e.g., “I baked in the kitchen.” vs. “I baked in the sun.”).\nStructurally, they are labeled directed graphs whose vertices are tokens in the sentence. This is in contrast to AMR whose vertices are abstract concepts, with no explicit alignment to tokens, which makes parsing more difficult (Flanigan et al., 2014). Their arc labels encode broadly-\napplicable semantic relations rather than being tailored to any specific downstream application or ontology.1 They are not necessarily trees, because a token may be an argument of more than one predicate (e.g., in “John wants to eat,” John is both the wanter and the would-be eater). Their analyses may optionally leave out non–contentbearing tokens, such as punctuation or the infinitival “to,” or prepositions that simply mark the type of relation holding between other words. But when restricted to content-bearing tokens (including adjectives, adverbs, etc.), the subgraph is connected. In this sense, SDP provides a whole-sentence analysis. This is in contrast to PropBank-style SRL, which gives an analysis of only verbal and nominal predicates (Palmer et al., 2005). Semantic dependency graphs also tend to have higher levels of nonprojectivity than syntactic trees (Oepen et al., 2014). Sentences with graphs containing cycles have been removed from the dataset by the organizers, so all remaining graphs are directed acyclic graphs. Table 1 summarizes some of the dataset’s high-level statistics.\nFormalisms. Following the SemEval shared tasks, we consider three formalisms. The DM (DELPH-IN MRS) representation comes from DeepBank (Flickinger et al., 2012), which are manually-corrected parses from the LinGO English Resource Grammar (Copestake and Flickinger, 2000). LinGO is a head-driven phrase structure grammar (HPSG; Pollard and Sag, 1994) with minimal recursion semantics (Copestake et al., 2005). The PAS (Predicate-Argument Structures) representation is extracted from the Enju Treebank, which consists of automatic parses from the Enju HPSG parser (Miyao, 2006). PAS annotations are also available for the Penn Chinese Treebank (Xue et al., 2005). The PSD (Prague Semantic Dependencies) representation is extracted from the tectogrammatical layer of the Prague Czech-English Dependency Treebank (Hajič et al., 2012). PSD annotations are also available for a Czech translation of the WSJ Corpus. In this work, we train and evaluate only on English annotations.\nOf the three, PAS follows syntax most closely, and prior work has found it the easiest to predict. PSD has the largest set of labels, and parsers\n1This may make another disambiguation step necessary to use these representations in a downstream task, but there is evidence that modeling semantic composition separately from grounding in any ontology is an effective way to achieve broad coverage (Kwiatkowski et al., 2013).\nhave significantly lower performance on it (Oepen et al., 2015)."
    }, {
      "heading" : "3 Single-Task SDP",
      "text" : "Here we introduce our basic model, in which training and prediction for each formalism is kept completely separate. We also lay out basic notation, which will be reused for our multitask extensions."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "The output of semantic dependency parsing is a labeled directed graph (see Figure 1). Each arc has a label from a predefined set L, indicating the semantic relation of the child to the head. Given input sentence x, let Y(x) be the set of possible semantic graphs over x. The graph we seek maximizes a score function S:\nŷ = arg max y∈Y(x) S(x, y), (1)\nWe decompose S into a sum of local scores s for local structures (or “parts”) p in the graph:\nS(x, y) = ∑\np∈y s(p). (2)\nFor notational simplicity, we omit the dependence of s on x. See Figure 2a for examples of local structures. s is a parameterized function, whose parameters (denoted Θ and suppressed here for clarity) will be learned from the training data (§3.3). Since we search over every possible labeled graph (i.e., considering each labeled arc for each pair of words), our approach can be considered a graph-based (or all-pairs) method. The models presented in this work all share this common graph-based approach, differing only in the set of structures they score and in the parameterization of the scoring function s. This approach also underlies state-of-the-art approaches to SDP (Martins and Almeida, 2014)."
    }, {
      "heading" : "3.2 Basic Model",
      "text" : "Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016). It borrows heavily from the neural arc-scoring architectures in those works, but decodes with a different algorithm under slightly different constraints."
    }, {
      "heading" : "3.2.1 Basic Structures",
      "text" : "Our basic model factors over three types of structures (p in Equation 2): • predicate, indicating a predicate word, de-\nnoted i→·; • unlabeled arc, representing the existence of\nan arc from a predicate to an argument, denoted i→j; • labeled arc, an arc labeled with a semantic\nrole, denoted i `→ j. Here i and j are word indices in a given sentence, and ` indicates the arc label. This list corresponds to the most basic structures used by Martins and Almeida (2014). Selecting an output y corresponds precisely to selecting which instantiations of these structures are included.\nTo ensure the internal consistency of predictions, the following constraints are enforced during decoding: • i→· if and only if there exists at least one j\nsuch that i→j; • If i→j, then there must be exactly one label `\nsuch that i `→ j. Conversely, if not i→j, then there must not exist any i `→ j;\nWe also enforce a determinism constraint (Flanigan et al., 2014): certain labels must not appear on more than one arc emanating from the same token. The set of deterministic labels is decided based on their appearance in the training set. Notably, we do not enforce that the predicted graph is connected or spanning. If not for the predicate and determinism constraints, our model would be arc-factored, and decoding could be done for each i, j pair independently. Our structures do overlap though, and we employ AD3 (Martins et al., 2011) to find the highest-scoring internally consistent semantic graph. AD3 is an approximate discrete optimization algorithm based on dual decomposition. It can be used to decode factor graphs over discrete variables when scored structures overlap, as is the case here."
    }, {
      "heading" : "3.2.2 Basic Scoring",
      "text" : "Similarly to Kiperwasser and Goldberg (2016), our model learns representations of tokens in a sentence using a bi-directional LSTM (BiLSTM). Each different type of structure (predicate, unlabeled arc, labeled arc) then shares these same BiLSTM representations, feeding them into a multilayer perceptron (MLP) which is specific to the structure type. We present the architecture slightly differently from prior work, to make the transition to the multitask scenario (§4) smoother. In our presentation, we separate the model into a function φ that represents the input (corresponding to the BiLSTM and the initial layers of the MLPs), and a function ψ that represents the output (corresponding to the final layers of the MLPs), with the scores given by their inner product.2\nDistributed input representations. Long shortterm memory networks (LSTMs) are a variant of recurrent neural networks (RNNs) designed to alleviate the vanishing gradient problem in RNNs (Hochreiter and Schmidhuber, 1997). A bi-directional LSTM (BiLSTM) runs over the sequence in both directions (Schuster and Paliwal, 1997; Graves, 2012).\nGiven an input sentence x and its corresponding part-of-speech tag sequence, each token is mapped to a concatenation of its word embedding vector and POS tag vector. Two LSTMs are then run in opposite directions over the input vector sequence, outputting the concatenation of the two hidden vectors at each position i: hi = [−→ h i; ←− h i ] (we omit hi’s dependence on x and its own parameters). hi can be thought of as an encoder that contextualizes each token conditioning on all of its context, without any Markov assumption. h’s parameters are learned jointly with the rest of the model (§3.3); we refer the readers to Cho (2015) for technical details.\nThe input representation φ of a predicate structure depends on the representation of one word:\nφ(i→·) = tanh ( Cpredhi + bpred ) . (3a)\n2For clarity, we present single-layer BiLSTMs and MLPs, while in practice we use two layers for both.\nFor unlabeled arc and labeled arc structures, it depends on both the head and the modifier (but not the label, which is captured in the distributed output representation):\nφ(i→j) = tanh ( CUA [ hi;hj ] + bUA ) , (3b)\nφ(i `→ j) = tanh ( CLA [ hi;hj ] + bLA ) . (3c)\nDistributed output representations. NLP researchers have found that embedding discrete output labels into a low dimensional real space is an effective way to capture commonalities among them (Srikumar and Manning, 2014; Hermann et al., 2014; FitzGerald et al., 2015, inter alia). In neural language models (Bengio et al., 2003; Mnih and Hinton, 2007, inter alia) the weights of the output layer could also be regarded as an output embedding.\nWe associate each first-order structure p with a d-dimensional real vectorψ(p) which does not depend on particular words in p. Predicates and unlabeled arcs are each mapped to a single vector:\nψ(i→·) = ψpred, (4a) ψ(i→j) = ψUA, (4b)\nand each label gets a vector:\nψ(i `→ j) = ψLA(`). (4c)\nScoring. Finally, we use an inner product to score first-order structures:\ns(p) = φ(p) ·ψ(p). (5) Figure 3 illustrates our basic model’s architecture."
    }, {
      "heading" : "3.3 Learning",
      "text" : "The parameters of the model are learned using a max-margin objective. Informally, the goal is to learn parameters for the score function so that the gold parse is scored over every incorrect parse with a margin proportional to the cost of the incorrect parse. More formally, let D = { (xi, yi) }N i=1 be the training set consisting of N pairs of sentence xi and its gold parse yi. Training is then the following `2-regularized empirical risk minimization problem:\nmin Θ\nλ 2 ‖Θ‖2 + 1 N\nN∑\ni=1\nL ( xi, yi; Θ ) , (6)\nwhere Θ is all parameters in the model, and L is the structured hinge loss:\nL ( xi, yi; Θ ) = max y∈Y(xi) { S ( xi, y ) + c ( y, yi )}\n− S ( xi, yi ) .\n(7)\nc is a weighted Hamming distance that trades off between precision and recall (Taskar et al., 2004). Following Martins and Almeida (2014), we encourage recall over precision by using the costs 0.6 for false negative arc predictions and 0.4 for false positives."
    }, {
      "heading" : "3.4 Experiments",
      "text" : "We evaluate our basic model on the English dataset from SemEval 2015 Task 18 closed track.3 We split as in previous work (Almeida and Martins, 2015; Du et al., 2015), resulting in 33,964 training sentences from §00–19 of the WSJ corpus, 1,692 development sentences from §20, 1,410 sentences from §21 as in-domain test data, and 1,849 sentences sampled from the Brown Corpus as out-of-domain test data.\nThe closed track differs from the open and gold tracks in that it does not allow access to any syntactic analyses. In the open track, additional machine generated syntactic parses are provided, while the gold-track gives access to various goldstandard syntactic analyses. Our model is evaluated with closed track data; it does not have access to any syntactic analyses during training or test.\nWe refer the readers to §4.4 for implementation details, including training procedures, hyperparameters, pruning techniques, etc..\n3http://sdp.delph-in.net 4Paired bootstrap, p < 0.05 after Bonferroni correction.\nEmpirical results. As our model uses no explicit syntactic information, the most comparable models to ours are two state-of-the-art closed track systems due to Du et al. (2015) and Almeida and Martins (2015). Du et al. (2015) rely on graphtree transformation techniques proposed by Du et al. (2014), and apply a voting ensemble to wellstudied tree-oriented parsers. Closely related to ours is Almeida and Martins (2015), who used rich, hand-engineered second-order features and AD3 for inference.\nTable 2 compares our basic model to both baseline systems (labeled F1 score) on SemEval 2015 Task 18 test data. Scores of those systems are repeated from the official evaluation results. Our basic model significantly outperforms the best published results with a 1.1% absolute improvement on the in-domain test set and 1.6% on the out-ofdomain test set."
    }, {
      "heading" : "4 Multitask SDP",
      "text" : "We introduce two extensions to our single-task model, both of which use training data for all three formalisms to improve performance on each formalism’s parsing task. We describe a firstorder model, where representation functions are enhanced by parameter sharing while inference is kept separate for each task (§4.2). We then introduce a model with cross-task higher-order structures that uses joint inference across different tasks (§4.3). Both multitask models use AD3 for decoding, and are trained with the same marginbased objective, as in our single-task model."
    }, {
      "heading" : "4.1 Problem Formulation",
      "text" : "We will use an additional superscript t ∈ T to distinguish the three tasks (e.g., y(t), φ(t)), where T = {DM,PAS,PSD}. Our task is now to predict three graphs {y(t)}t∈T for a given input sentence x. Multitask SDP can also be understood as parsing x into a single unified multigraph y =⋃ t∈T y\n(t). Similarly to Equations 1–2, we decompose y’s score S(x, y) into a sum of local scores for local structures in y, and we seek a multigraph ŷ that maximizes S(x, y)."
    }, {
      "heading" : "4.2 Multitask SDP with Parameter Sharing",
      "text" : "A common approach when using BiLSTMs for multitask learning is to share the BiLSTM part of the model across tasks, while training specialized classifiers for each task (Søgaard and Goldberg, 2016). In this spirit, we let each task keep its own specialized MLPs, and explore two variants of our model that share parameters at the BiLSTM level.\nThe first variant consists of a set of task-specific BiLSTM encoders as well as a common one that is shared across all tasks. We denote it FREDA. FREDA uses a neural generalization of “frustratingly easy” domain adaptation (Daumé III, 2007; Kim et al., 2016), where one augments domainspecific features with a shared set of features to capture global patterns. Formally, let {h(t)}t∈T denote the three task-specific encoders. We introduce another encoder h̃ that is shared across all tasks. Then a new set of input functions {φ(t)}t∈T can be defined as in Equations 3a–3c, for example:\nφ(t)(i `→ j) = tanh ( C\n(t) LA\n[ h\n(t) i ;h (t) j ;\nh̃i; h̃j ] + b (t) LA ) . (8)\nThe predicate and unlabeled arc versions are analogous. The output representations {ψ(t)} remain task-specific, and the score is still the inner product between the input representation and the output representation.\nThe second variant, which we call SHARED, uses only the shared encoder h̃, and doesn’t use task-specific encoders {h(t)}. It can be understood as a special case of FREDA where the dimensions of the task-specific encoders are 0."
    }, {
      "heading" : "4.3 Multitask SDP with Cross-Task Structures",
      "text" : "In syntactic parsing, higher-order structures have commonly been used to model interactions be-\ntween multiple adjacent arcs in the same dependency tree (Carreras, 2007; Smith and Eisner, 2008; Martins et al., 2009; Zhang et al., 2014, inter alia). Lluı́s et al. (2013), in contrast, used second-order structures to jointly model syntactic dependencies and semantic roles. Similarly, we use higher-order structures across tasks instead of within tasks. In this work, we look at interactions between arcs that share the same head and modifier.5 See Figures 2b and 2c for examples of higher-order cross-task structures.\nHigher-order structure scoring. Borrowing from Lei et al. (2014), we introduce a low-rank tensor scoring strategy that, given a higher-order structure p, models interactions between the firstorder structures (i.e., arcs) p is made up of. This approach builds on and extends the parameter sharing techniques in §4.2. It can either follow FREDA or SHARED to get the input representations for first-order structures.\nWe first introduce basic tensor notation. The order of a tensor is the number of its dimensions. The outer product of two vectors forms a secondorder tensor (matrix) where [u⊗ v]i,j = uivj . We denote the inner product of two tensors of the same dimensions by 〈·, ·〉, which first takes their element-wise product, then sums all the elements in the resulting tensor.\nFor example, let p be a labeled third-order structure, including one labeled arc from each of the three different tasks: p = {p(t)}t∈T . Intuitively, s(p) should capture every pairwise interaction between the three input and three output representations of p. Formally, we want the score function to include a parameter for each term in the outer product of the representation vectors: s(p) = 〈 W, ⊗\nt∈T\n( φ(t) ( p(t) ) ⊗ψ(t) ( p(t) ))〉 , (9)\nwhere W is a sixth-order tensor of parameters.6\nWith typical dimensions of representation vectors, this leads to an unreasonably large number of\n5In the future we hope to model structures over larger motifs, both across and within tasks, to potentially capture when an arc in one formalism corresponds to a path in another formalism, for example.\n6This is, of course, not the only way to model interactions between several representations. For instance, one could concatenate them and feed them into another MLP. Our preliminary experiments in this direction suggested that it may be less effective given a similar number of parameters, but we did not run full experiments.\nparameters. Following Lei et al. (2014), we upperbound the rank of W by r to limit the number of parameters (r is a hyperparameter, decided empirically). Using the fact that a tensor of rank at most r can be decomposed into a sum of r rank-1 tensors (Hitchcock, 1927), we reparameterize W to enforce the low-rank constraint by construction:\nW =\nr∑\nj=1\n⊗\nt∈T\n([ U\n(t) LA ] j,: ⊗ [ V (t) LA ] j,: ) , (10)\nwhere U(t)LA,V (t) LA ∈ Rr×d are now our parameters. [·]j,: denotes the jth row of a matrix. Substituting this back into Equation 9 and rearranging, the score function s(p) can then be rewritten as: r∑\nj=1\n∏\nt∈T\n[ U\n(t) LAφ\n(t) ( p(t) )]\nj\n[ V\n(t) LAψ\n(t) ( p(t) )]\nj .\n(11) We refer readers to Kolda and Bader (2009) for mathematical details.\nFor labeled higher-order structures our parameters consist of the set of six matrices, {U(t)LA} ∪ {V(t)LA}. These parameters are shared between second-order and third-order labeled structures. Labeled second-order structures are scored as Equation 11, but with the product extending over only the two relevant tasks. Concretely, only four of the representation functions are used rather than all six, along with the four corresponding matrices from {U(t)LA} ∪ {V (t) LA}. Unlabeled crosstask structures are scored analogously, reusing the same representations, but with a separate set of parameter matrices {U(t)UA} ∪ {V (t) UA}.\nNote that we are not doing tensor factorization; we are learning U(t)LA,V (t) LA,U (t) UA, and V (t) UA directly, and W is never explicitly instantiated.\nInference and learning. Given a sentence, we use AD3 to jointly decode all three formalisms.7 The training objective used for learning is the sum of the losses for individual tasks."
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "Each input token is mapped to a concatenation of three real vectors: a pre-trained word vector; a randomly-initialized word vector; and a randomlyinitialized POS tag vector.8 All three are updated\n7Joint inference comes at a cost; our third-order model is able to decode roughly 5.2 sentences (i.e., 15.5 task-specific graphs) per second on a single Xeon E5-2690 2.60GHz CPU.\n8There are minor differences in the part-of-speech data provided with the three formalisms. For the basic models, we\nduring training. We use 100-dimensional GloVe (Pennington et al., 2014) vectors trained over Wikipedia and Gigaword as pre-trained word embeddings. To deal with out-of-vocabulary words, we apply word dropout (Iyyer et al., 2015) and randomly replace a word w with a special unksymbol with probability α1+#(w) , where #(w) is the count of w in the training set.\nModels are trained for up to 30 epochs with Adam (Kingma and Ba, 2015), with β1 = β2 = 0.9, and initial learning rate η0 = 10−3. The learning rate η is annealed at a rate of 0.5 every 10 epochs (Dozat and Manning, 2017). We apply early-stopping based on the labeled F1 score on the development set.9 We set the maximum number of iterations of AD3 to 500 and round decisions when it doesn’t converge. We clip the `2 norm of gradients to 1 (Graves, 2013; Sutskever et al., 2014), and we do not use mini-batches. Randomly initialized parameters are sampled from a uniform distribution over[ − √ 6/(dr + dc), √ 6/(dr + dc) ] , where dr and dc are the number of the rows and columns in the matrix, respectively. An `2 penalty of λ = 10−6 is applied to all weights. Other hyperparameters are summarized in Table 3.\nWe use the same pruner as Martins and Almeida (2014), where a first-order feature-rich unlabeled pruning model is trained for each task, and arcs with posterior probability below 10−4 are discarded. We further prune labeled structures that appear less than 30 times in the training set. In the development set, about 10% of the arcs remain after pruning, with a recall of around 99%.\nuse the POS tags provided with the respective dataset; for the multitask models, we use the (automatic) POS tags provided with DM.\n9Micro-averaged labeled F1 for the multitask models."
    }, {
      "heading" : "4.5 Experiments",
      "text" : "Experimental settings. We compare four multitask variants to the basic model, as well as the two baseline systems introduced in §3.4. • SHARED1 is a first-order model. It uses a sin-\ngle shared BiLSTM encoder, and keeps the inference separate for each task. • FREDA1 is a first-order model based on “frus-\ntratingly easy” parameter sharing. It uses a shared encoder as well as task-specific ones. The inference is kept separate for each task. • SHARED3 is a third-order model. It follows\nSHARED1 and uses a single shared BiLSTM encoder, but additionally employs cross-task structures and inference. • FREDA3 is also a third-order model. It com-\nbines FREDA1 and SHARED3 by using both “frustratingly easy” parameter sharing and cross-task structures and inference.\nIn addition, we also examine the effects of syntax by comparing our models to the state-of-the-art open track system (Almeida and Martins, 2015).10\nMain results overview. Table 4a compares our models to the best published results (labeled F1 score) on SemEval 2015 Task 18 in-domain test set. Our basic model improves over all closed track entries in all formalisms. It is even with the best open track system for DM and PSD, but improves on PAS and on average, without making use of any syntax. Three of our four multitask variants further improve over our basic model; SHARED1’s differences are statistically insignificant. Our best models (SHARED3, FREDA3) outperform the previous state-of-the-art closed track system by 1.7% absolute F1, and the best open track system by 0.9%, without the use of syntax.\nWe observe similar trends on the out-of-domain test set (Table 4b), with the exception that, on PSD, our best-performing model’s improvement over the open-track system of Almeida and Martins (2015) is not statistically significant.\nThe extent to which we might benefit from syntactic information remains unclear. With automatically generated syntactic parses, Almeida and Martins (2015) manage to obtain more than 1% absolute improvements over their closed track en-\n10Kanerva et al. (2015) was the winner of the gold track, which overall saw higher performance than the closed and open tracks. Since gold-standard syntactic analyses are not available in most realistic scenarios, we do not include it in this comparison.\ntry, which is consistent with the extensive evaluation by Zhang et al. (2016), but we leave the incorporation of syntactic trees to future work. Syntactic parsing could be treated as yet another output task, as explored in Lluı́s et al. (2013) and in the transition-based frameworks of Henderson et al. (2013) and Swayamdipta et al. (2016).\nEffects of structural overlap. We hypothesized that the overlap between formalisms would enable multitask learning to be effective; in this section we investigate in more detail how structural overlap affected performance. By looking at undirected overlap between unlabeled arcs, we discover that modeling only arcs in the same direction may have been a design mistake.\nDM and PAS are more structurally similar to each other than either is to PSD. Table 5 compares the structural similarities between the three for-\nmalisms in unlabeled F1 score (each formalism’s gold-standard unlabeled graph is used as a prediction of each other formalism’s gold-standard unlabeled graph). All three formalisms have more than 50% overlap when ignoring arcs’ directions, but considering direction, PSD is clearly different; PSD reverses the direction about half of the time it shares an edge with another formalism. A concrete example can be found in Figure 1, where DM and PAS both have an arc from “Last” to “week,” while PSD has an arc from “week” to “Last.”\nWe can compare FREDA3 to FREDA1 to isolate the effect of modeling higher-order structures. Table 6 shows performance on the development data in both unlabeled and labeled F1. We can see that FREDA3’s unlabeled performance improves on DM and PAS, but degrades on PSD. This supports our hypothesis, and suggests that in future work, a more careful selection of structures to model might lead to further improvements."
    }, {
      "heading" : "5 Related Work",
      "text" : "We note two important strands of related work.\nGraph-based parsing. Graph-based parsing was originally invented to handle non-projective syntax (McDonald et al., 2005; Koo et al., 2010; Martins et al., 2013, inter alia), but has been adapted to semantic parsing (Flanigan et al., 2014; Martins and Almeida, 2014; Thomson et al., 2014; Kuhlmann, 2014, inter alia). Local structure scoring was traditionally done with linear models over hand-engineered features, but lately, various forms of representation learning\nhave been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).\nMultitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning. Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daumé III, 2007)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We showed two orthogonal ways to apply deep multitask learning to graph-based parsing. The first shares parameters when encoding tokens in the input with recurrent neural networks, and the second introduces interactions between output structures across formalisms. Without using syntactic parsing, these approaches outperform even state-of-the-art semantic dependency parsing systems that use syntax. Because our techniques apply to labeled directed graphs in general, they can easily be extended to incorporate more formalisms, semantic or otherwise. In future work we hope to explore cross-task scoring and inference for tasks where parallel annotations are not available. Our code is opensource and available at https://github. com/Noahs-ARK/NeurboParser."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the Ark, Maxwell Forbes, Luheng He, Kenton Lee, Julian Michael, and Jin-ge Yao for their helpful comments on an earlier version of this draft, and the anonymous reviewers for their valuable feedback. This work was supported by NSF grant IIS-1562364 and DARPA grant FA8750-122-0342 funded under the DEFT program."
    } ],
    "references" : [ {
      "title" : "Lisbon: Evaluating TurboSemanticParser on multiple languages and out-of-domain data",
      "author" : [ "Mariana S.C. Almeida", "André F.T. Martins." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Almeida and Martins.,? 2015",
      "shortCiteRegEx" : "Almeida and Martins.",
      "year" : 2015
    }, {
      "title" : "Many languages, one parser",
      "author" : [ "Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith." ],
      "venue" : "TACL 4:431–444.",
      "citeRegEx" : "Ammar et al\\.,? 2016",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2016
    }, {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "Rie Kubota Ando", "Tong Zhang." ],
      "venue" : "JMLR 6:1817–1853.",
      "citeRegEx" : "Ando and Zhang.,? 2005",
      "shortCiteRegEx" : "Ando and Zhang.",
      "year" : 2005
    }, {
      "title" : "SemEval’07 task 19: Frame semantic structure extraction",
      "author" : [ "Collin Baker", "Michael Ellsworth", "Katrin Erk." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Baker et al\\.,? 2007",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 2007
    }, {
      "title" : "Abstract meaning representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proc. of LAW VII & ID.",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Rèjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "JMLR 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Domain adaptation with structural correspondence learning",
      "author" : [ "John Blitzer", "Ryan McDonald", "Fernando Pereira." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Blitzer et al\\.,? 2006",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2006
    }, {
      "title" : "Teoria statistica delle classi e calcolo delle probabilità",
      "author" : [ "Carlo E. Bonferroni." ],
      "venue" : "Pubblicazioni del R. Istituto Superiore di Scienze Economiche e Commerciali di Firenze 8:3–62.",
      "citeRegEx" : "Bonferroni.,? 1936",
      "shortCiteRegEx" : "Bonferroni.",
      "year" : 1936
    }, {
      "title" : "Experiments with a higherorder projective dependency parser",
      "author" : [ "Xavier Carreras." ],
      "venue" : "Proc. of CoNLL.",
      "citeRegEx" : "Carreras.,? 2007",
      "shortCiteRegEx" : "Carreras.",
      "year" : 2007
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine Learning 28(1):41–75.",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "Natural language understanding with distributed representation",
      "author" : [ "Kyunghyun Cho." ],
      "venue" : "ArXiv:1511.07916.",
      "citeRegEx" : "Cho.,? 2015",
      "shortCiteRegEx" : "Cho.",
      "year" : 2015
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proc. of ICML.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "An open source grammar development environment and broad-coverage English grammar using HPSG",
      "author" : [ "Ann Copestake", "Dan Flickinger." ],
      "venue" : "Proc. of LREC.",
      "citeRegEx" : "Copestake and Flickinger.,? 2000",
      "shortCiteRegEx" : "Copestake and Flickinger.",
      "year" : 2000
    }, {
      "title" : "Minimal recursion semantics: An introduction",
      "author" : [ "Ann Copestake", "Dan Flickinger", "Ivan A. Sag", "Carl Pollard." ],
      "venue" : "Research on Language & Computation 3(4):281–332.",
      "citeRegEx" : "Copestake et al\\.,? 2005",
      "shortCiteRegEx" : "Copestake et al\\.",
      "year" : 2005
    }, {
      "title" : "Frustratingly easy domain adaptation",
      "author" : [ "Hal Daumé III." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "III.,? 2007",
      "shortCiteRegEx" : "III.",
      "year" : 2007
    }, {
      "title" : "Universal Stanford dependencies: A cross-linguistic typology",
      "author" : [ "Marie-Catherine de Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D. Manning." ],
      "venue" : "Proc. of LREC.",
      "citeRegEx" : "Marneffe et al\\.,? 2014",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Peking: Profiling syntactic tree parsing techniques for semantic graph parsing",
      "author" : [ "Yantao Du", "Fan Zhang", "Weiwei Sun", "Xiaojun Wan." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Du et al\\.,? 2014",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2014
    }, {
      "title" : "Peking: Building semantic dependency graphs with a hybrid parser",
      "author" : [ "Yantao Du", "Fan Zhang", "Xun Zhang", "Weiwei Sun", "Xiaojun Wan." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Du et al\\.,? 2015",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2015
    }, {
      "title" : "Semantic role labeling with neural network factors",
      "author" : [ "Nicholas FitzGerald", "Oscar Täckström", "Kuzman Ganchev", "Dipanjan Das." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "FitzGerald et al\\.,? 2015",
      "shortCiteRegEx" : "FitzGerald et al\\.",
      "year" : 2015
    }, {
      "title" : "A discriminative graph-based parser for the abstract meaning representation",
      "author" : [ "Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Flanigan et al\\.,? 2014",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2014
    }, {
      "title" : "DeepBank: A dynamically annotated treebank of the Wall Street Journal",
      "author" : [ "Daniel Flickinger", "Yi Zhang", "Valia Kordoni." ],
      "venue" : "Proc. of TLT .",
      "citeRegEx" : "Flickinger et al\\.,? 2012",
      "shortCiteRegEx" : "Flickinger et al\\.",
      "year" : 2012
    }, {
      "title" : "Automatic labeling of semantic roles",
      "author" : [ "Daniel Gildea", "Daniel Jurafsky." ],
      "venue" : "Computational Linguistics 28(3):245–288.",
      "citeRegEx" : "Gildea and Jurafsky.,? 2002",
      "shortCiteRegEx" : "Gildea and Jurafsky.",
      "year" : 2002
    }, {
      "title" : "Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence",
      "author" : [ "Alex Graves." ],
      "venue" : "Springer.",
      "citeRegEx" : "Graves.,? 2012",
      "shortCiteRegEx" : "Graves.",
      "year" : 2012
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "ArXiv 1308.0850.",
      "citeRegEx" : "Graves.,? 2013",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "A universal framework for inductive transfer parsing across multi-typed treebanks",
      "author" : [ "Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu." ],
      "venue" : "Proc. of COLING.",
      "citeRegEx" : "Guo et al\\.,? 2016",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-lingual joint parsing of syntactic and semantic dependencies with a latent variable model",
      "author" : [ "James Henderson", "Paola Merlo", "Ivan Titov", "Gabriele Musillo." ],
      "venue" : "Computational Linguistics 39(4):949–998.",
      "citeRegEx" : "Henderson et al\\.,? 2013",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2013
    }, {
      "title" : "Semantic frame identification with distributed word representations",
      "author" : [ "Karl Moritz Hermann", "Dipanjan Das", "Jason Weston", "Kuzman Ganchev." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Hermann et al\\.,? 2014",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2014
    }, {
      "title" : "The expression of a tensor or a polyadic as a sum of products",
      "author" : [ "Frank L. Hitchcock." ],
      "venue" : "Journal of Mathematical Physics 6(1):164–189.",
      "citeRegEx" : "Hitchcock.,? 1927",
      "shortCiteRegEx" : "Hitchcock.",
      "year" : 1927
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Iyyer et al\\.,? 2015",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Training parsers on incompatible treebanks",
      "author" : [ "Richard Johansson." ],
      "venue" : "Proc. of NAACL.",
      "citeRegEx" : "Johansson.,? 2013",
      "shortCiteRegEx" : "Johansson.",
      "year" : 2013
    }, {
      "title" : "Turku: Semantic dependency parsing as a sequence classification",
      "author" : [ "Jenna Kanerva", "Juhani Luotolahti", "Filip Ginter." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Kanerva et al\\.,? 2015",
      "shortCiteRegEx" : "Kanerva et al\\.",
      "year" : 2015
    }, {
      "title" : "Frustratingly easy neural domain adaptation",
      "author" : [ "Young-Bum Kim", "Karl Stratos", "Ruhi Sarikaya." ],
      "venue" : "Proc. of COLING.",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Simple and accurate dependency parsing using bidirectional LSTM feature representations",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "TACL 4:313– 327.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "Tamara G. Kolda", "Brett W. Bader." ],
      "venue" : "SIAM Review 51(3):455–500.",
      "citeRegEx" : "Kolda and Bader.,? 2009",
      "shortCiteRegEx" : "Kolda and Bader.",
      "year" : 2009
    }, {
      "title" : "Dual decomposition for parsing with non-projective head automata",
      "author" : [ "Terry Koo", "Alexander M. Rush", "Michael Collins", "Tommi Jaakkola", "David Sontag." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Koo et al\\.,? 2010",
      "shortCiteRegEx" : "Koo et al\\.",
      "year" : 2010
    }, {
      "title" : "Frame-semantic role labeling with heterogeneous annotations",
      "author" : [ "Meghana Kshirsagar", "Sam Thomson", "Nathan Schneider", "Jaime Carbonell", "Noah A. Smith", "Chris Dyer." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Kshirsagar et al\\.,? 2015",
      "shortCiteRegEx" : "Kshirsagar et al\\.",
      "year" : 2015
    }, {
      "title" : "Linköping: Cubic-time graph parsing with a simple scoring scheme",
      "author" : [ "Marco Kuhlmann." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Kuhlmann.,? 2014",
      "shortCiteRegEx" : "Kuhlmann.",
      "year" : 2014
    }, {
      "title" : "Distilling an ensemble of greedy dependency parsers into one MST parser",
      "author" : [ "Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Kuncoro et al\\.,? 2016",
      "shortCiteRegEx" : "Kuncoro et al\\.",
      "year" : 2016
    }, {
      "title" : "Scaling semantic parsers with on-the-fly ontology matching",
      "author" : [ "Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Kwiatkowski et al\\.,? 2013",
      "shortCiteRegEx" : "Kwiatkowski et al\\.",
      "year" : 2013
    }, {
      "title" : "Low-rank tensors for scoring dependency structures",
      "author" : [ "Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Lei et al\\.,? 2014",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint arc-factored parsing of syntactic and semantic dependencies",
      "author" : [ "Xavier Lluı́s", "Xavier Carreras", "Lluı́s Màrquez" ],
      "venue" : "TACL",
      "citeRegEx" : "Lluı́s et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lluı́s et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural probabilistic model for non-projective MST parsing",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "ArXiv 1701.00874.",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Priberam: A turbo semantic parser with second order features",
      "author" : [ "André F.T. Martins", "Mariana S.C. Almeida." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Martins and Almeida.,? 2014",
      "shortCiteRegEx" : "Martins and Almeida.",
      "year" : 2014
    }, {
      "title" : "Turning on the turbo: Fast third-order non-projective turbo parsers",
      "author" : [ "André F.T. Martins", "Miguel B. Almeida", "Noah A. Smith." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Martins et al\\.,? 2013",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2013
    }, {
      "title" : "Concise integer linear programming formulations for dependency parsing",
      "author" : [ "André F.T. Martins", "Noah Smith", "Eric Xing." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Martins et al\\.,? 2009",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2009
    }, {
      "title" : "Dual decomposition with many overlapping components",
      "author" : [ "André F.T. Martins", "Noah A. Smith", "Pedro M.Q. Aguiar", "Mário A.T. Figueiredo." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Martins et al\\.,? 2011",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2011
    }, {
      "title" : "Online large-margin training of dependency parsers",
      "author" : [ "Ryan McDonald", "Koby Crammer", "Fernando Pereira." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "McDonald et al\\.,? 2005",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2005
    }, {
      "title" : "From linguistic theory to syntactic analysis: Corpus-oriented grammar development and feature forest model",
      "author" : [ "Yusuke Miyao" ],
      "venue" : null,
      "citeRegEx" : "Miyao.,? \\Q2006\\E",
      "shortCiteRegEx" : "Miyao.",
      "year" : 2006
    }, {
      "title" : "Three new graphical models for statistical language modelling",
      "author" : [ "Andriy Mnih", "Geoffrey Hinton." ],
      "venue" : "Proc. of ICML.",
      "citeRegEx" : "Mnih and Hinton.,? 2007",
      "shortCiteRegEx" : "Mnih and Hinton.",
      "year" : 2007
    }, {
      "title" : "SemEval 2015 task 18: Broad-coverage semantic dependency parsing",
      "author" : [ "Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Silvie Cinková", "Dan Flickinger", "Jan Hajič", "Zdeňka Urešová." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Oepen et al\\.,? 2015",
      "shortCiteRegEx" : "Oepen et al\\.",
      "year" : 2015
    }, {
      "title" : "SemEval 2014 task 8: Broad-coverage semantic dependency parsing",
      "author" : [ "Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Dan Flickinger", "Jan Hajič", "Angelina Ivanova", "Yi Zhang." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Oepen et al\\.,? 2014",
      "shortCiteRegEx" : "Oepen et al\\.",
      "year" : 2014
    }, {
      "title" : "The proposition bank: An annotated corpus of semantic roles",
      "author" : [ "Martha Palmer", "Daniel Gildea", "Paul Kingsbury." ],
      "venue" : "Computational Linguistics 31(1):71–106.",
      "citeRegEx" : "Palmer et al\\.,? 2005",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2005
    }, {
      "title" : "An effective neural network model for graph-based dependency parsing",
      "author" : [ "Wenzhe Pei", "Tao Ge", "Baobao Chang." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Pei et al\\.,? 2015",
      "shortCiteRegEx" : "Pei et al\\.",
      "year" : 2015
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Head-Driven Phrase Structure Grammar",
      "author" : [ "Carl Pollard", "Ivan A. Sag." ],
      "venue" : "The University of Chicago Press.",
      "citeRegEx" : "Pollard and Sag.,? 1994",
      "shortCiteRegEx" : "Pollard and Sag.",
      "year" : 1994
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K. Paliwal." ],
      "venue" : "IEEE Transactions on Signal Processing 45(11):2673–2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Dependency parsing by belief propagation",
      "author" : [ "David Smith", "Jason Eisner." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Smith and Eisner.,? 2008",
      "shortCiteRegEx" : "Smith and Eisner.",
      "year" : 2008
    }, {
      "title" : "Deep multi-task learning with low level tasks supervised at lower layers",
      "author" : [ "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Søgaard and Goldberg.,? 2016",
      "shortCiteRegEx" : "Søgaard and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Learning distributed representations for structured output prediction",
      "author" : [ "Vivek Srikumar", "Christopher D Manning." ],
      "venue" : "Proc. of NIPS.",
      "citeRegEx" : "Srikumar and Manning.,? 2014",
      "shortCiteRegEx" : "Srikumar and Manning.",
      "year" : 2014
    }, {
      "title" : "The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies",
      "author" : [ "Mihai Surdeanu", "Richard Johansson", "Adam Meyers", "Lluı́s Màrquez", "Joakim Nivre" ],
      "venue" : "In Proc. of CoNLL",
      "citeRegEx" : "Surdeanu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Surdeanu et al\\.",
      "year" : 2008
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Proc. of NIPS.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Greedy, joint syntacticsemantic parsing with stack LSTMs",
      "author" : [ "Swabha Swayamdipta", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proc. of CoNLL.",
      "citeRegEx" : "Swayamdipta et al\\.,? 2016",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2016
    }, {
      "title" : "Max-margin Markov networks",
      "author" : [ "Ben Taskar", "Carlos Guestrin", "Daphne Koller." ],
      "venue" : "Advances in Neural Information Processing Systems 16.",
      "citeRegEx" : "Taskar et al\\.,? 2004",
      "shortCiteRegEx" : "Taskar et al\\.",
      "year" : 2004
    }, {
      "title" : "Template kernels for dependency parsing",
      "author" : [ "Hillel Taub-Tabib", "Yoav Goldberg", "Amir Globerson." ],
      "venue" : "Proc. of NAACL.",
      "citeRegEx" : "Taub.Tabib et al\\.,? 2015",
      "shortCiteRegEx" : "Taub.Tabib et al\\.",
      "year" : 2015
    }, {
      "title" : "CMU: Arc-factored, discriminative semantic dependency parsing",
      "author" : [ "Sam Thomson", "Brendan O’Connor", "Jeffrey Flanigan", "David Bamman", "Jesse Dodge", "Swabha Swayamdipta", "Nathan Schneider", "Chris Dyer", "Noah A. Smith" ],
      "venue" : "In Proc. of Se-",
      "citeRegEx" : "Thomson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Thomson et al\\.",
      "year" : 2014
    }, {
      "title" : "Graph-based dependency parsing with bidirectional LSTM",
      "author" : [ "Wenhui Wang", "Baobao Chang." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Wang and Chang.,? 2016",
      "shortCiteRegEx" : "Wang and Chang.",
      "year" : 2016
    }, {
      "title" : "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus",
      "author" : [ "Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Martha Palmer." ],
      "venue" : "Natural Language Engineering 11(2):207–238.",
      "citeRegEx" : "Xue et al\\.,? 2005",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2005
    }, {
      "title" : "Transition-based parsing for deep dependency structures",
      "author" : [ "Xun Zhang", "Yantao Du", "Weiwei Sun", "Xiaojun Wan." ],
      "venue" : "Computational Linguistics 42(3):353–389.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Greed is good if randomized: New inference for dependency parsing",
      "author" : [ "Yuan Zhang", "Tao Lei", "Regina Barzilay", "Tommi S. Jaakkola." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Zhang et al\\.,? 2014",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    }, {
      "title" : "Stackpropagation: Improved representation learning for syntax",
      "author" : [ "Yuan Zhang", "David Weiss." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Zhang and Weiss.,? 2016",
      "shortCiteRegEx" : "Zhang and Weiss.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "tations can be exploited using multitask learning (Caruana, 1997), allowing us to learn from more data.",
      "startOffset" : 50,
      "endOffset" : 65
    }, {
      "referenceID" : 52,
      "context" : "We use the 2015 SemEval shared task on Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2015) as our testbed.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 52,
      "context" : "Numbers taken from Oepen et al. (2015).",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 53,
      "context" : "First defined in a SemEval 2014 shared task (Oepen et al., 2014), and then extended by Oepen et al.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 52,
      "context" : "First defined in a SemEval 2014 shared task (Oepen et al., 2014), and then extended by Oepen et al. (2015), the broad-coverage semantic depency parsing (SDP) task is centered around three semantic formalisms whose annotations have been converted into bilexical dependencies.",
      "startOffset" : 45,
      "endOffset" : 107
    }, {
      "referenceID" : 22,
      "context" : "While at first glance similar to syntactic dependencies, semantic dependencies have distinct goals and characteristics, more akin to semantic role labeling (SRL; Gildea and Jurafsky, 2002) or the abstract meaning representation (AMR; Banarescu et al.",
      "startOffset" : 156,
      "endOffset" : 188
    }, {
      "referenceID" : 4,
      "context" : "While at first glance similar to syntactic dependencies, semantic dependencies have distinct goals and characteristics, more akin to semantic role labeling (SRL; Gildea and Jurafsky, 2002) or the abstract meaning representation (AMR; Banarescu et al., 2013).",
      "startOffset" : 228,
      "endOffset" : 257
    }, {
      "referenceID" : 20,
      "context" : "This is in contrast to AMR whose vertices are abstract concepts, with no explicit alignment to tokens, which makes parsing more difficult (Flanigan et al., 2014).",
      "startOffset" : 138,
      "endOffset" : 161
    }, {
      "referenceID" : 54,
      "context" : "This is in contrast to PropBank-style SRL, which gives an analysis of only verbal and nominal predicates (Palmer et al., 2005).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 53,
      "context" : "Semantic dependency graphs also tend to have higher levels of nonprojectivity than syntactic trees (Oepen et al., 2014).",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : "The DM (DELPH-IN MRS) representation comes from DeepBank (Flickinger et al., 2012), which are manually-corrected parses from the LinGO English Resource Grammar (Copestake and Flickinger, 2000).",
      "startOffset" : 57,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : ", 2012), which are manually-corrected parses from the LinGO English Resource Grammar (Copestake and Flickinger, 2000).",
      "startOffset" : 85,
      "endOffset" : 117
    }, {
      "referenceID" : 57,
      "context" : "LinGO is a head-driven phrase structure grammar (HPSG; Pollard and Sag, 1994) with minimal recursion semantics (Copestake et al.",
      "startOffset" : 48,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "LinGO is a head-driven phrase structure grammar (HPSG; Pollard and Sag, 1994) with minimal recursion semantics (Copestake et al., 2005).",
      "startOffset" : 111,
      "endOffset" : 135
    }, {
      "referenceID" : 50,
      "context" : "The PAS (Predicate-Argument Structures) representation is extracted from the Enju Treebank, which consists of automatic parses from the Enju HPSG parser (Miyao, 2006).",
      "startOffset" : 153,
      "endOffset" : 166
    }, {
      "referenceID" : 69,
      "context" : "PAS annotations are also available for the Penn Chinese Treebank (Xue et al., 2005).",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 41,
      "context" : "This may make another disambiguation step necessary to use these representations in a downstream task, but there is evidence that modeling semantic composition separately from grounding in any ontology is an effective way to achieve broad coverage (Kwiatkowski et al., 2013).",
      "startOffset" : 248,
      "endOffset" : 274
    }, {
      "referenceID" : 52,
      "context" : "have significantly lower performance on it (Oepen et al., 2015).",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 45,
      "context" : "This approach also underlies state-of-the-art approaches to SDP (Martins and Almeida, 2014).",
      "startOffset" : 64,
      "endOffset" : 91
    }, {
      "referenceID" : 35,
      "context" : "Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 181
    }, {
      "referenceID" : 16,
      "context" : "Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 181
    }, {
      "referenceID" : 40,
      "context" : "Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 181
    }, {
      "referenceID" : 45,
      "context" : "This list corresponds to the most basic structures used by Martins and Almeida (2014). Selecting an output y corresponds precisely to selecting which instantiations of these structures are included.",
      "startOffset" : 59,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "We also enforce a determinism constraint (Flanigan et al., 2014): certain labels must not appear on more than one arc emanating from the same token.",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 48,
      "context" : "Our structures do overlap though, and we employ AD (Martins et al., 2011) to find the highest-scoring internally consistent semantic graph.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 35,
      "context" : "Similarly to Kiperwasser and Goldberg (2016), our model learns representations of tokens in a sentence using a bi-directional LSTM (BiLSTM).",
      "startOffset" : 13,
      "endOffset" : 45
    }, {
      "referenceID" : 29,
      "context" : "Long shortterm memory networks (LSTMs) are a variant of recurrent neural networks (RNNs) designed to alleviate the vanishing gradient problem in RNNs (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 150,
      "endOffset" : 184
    }, {
      "referenceID" : 58,
      "context" : "A bi-directional LSTM (BiLSTM) runs over the sequence in both directions (Schuster and Paliwal, 1997; Graves, 2012).",
      "startOffset" : 73,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "A bi-directional LSTM (BiLSTM) runs over the sequence in both directions (Schuster and Paliwal, 1997; Graves, 2012).",
      "startOffset" : 73,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "3); we refer the readers to Cho (2015) for technical details.",
      "startOffset" : 28,
      "endOffset" : 39
    }, {
      "referenceID" : 65,
      "context" : "c is a weighted Hamming distance that trades off between precision and recall (Taskar et al., 2004).",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 45,
      "context" : "Following Martins and Almeida (2014), we encourage recall over precision by using the costs 0.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "3 We split as in previous work (Almeida and Martins, 2015; Du et al., 2015), resulting in 33,964 training sentences from §00–19 of the WSJ corpus, 1,692 development sentences from §20, 1,410 sentences from §21 as in-domain test data, and 1,849 sentences sampled from the Brown Corpus as out-of-domain test data.",
      "startOffset" : 31,
      "endOffset" : 75
    }, {
      "referenceID" : 18,
      "context" : "3 We split as in previous work (Almeida and Martins, 2015; Du et al., 2015), resulting in 33,964 training sentences from §00–19 of the WSJ corpus, 1,692 development sentences from §20, 1,410 sentences from §21 as in-domain test data, and 1,849 sentences sampled from the Brown Corpus as out-of-domain test data.",
      "startOffset" : 31,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "Underlines indicate statistical significance with Bonferroni (1936) correction compared to the best baseline system.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : "As our model uses no explicit syntactic information, the most comparable models to ours are two state-of-the-art closed track systems due to Du et al. (2015) and Almeida and Martins (2015).",
      "startOffset" : 141,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : "(2015) and Almeida and Martins (2015). Du et al.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "(2015) and Almeida and Martins (2015). Du et al. (2015) rely on graphtree transformation techniques proposed by Du et al.",
      "startOffset" : 11,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "(2015) and Almeida and Martins (2015). Du et al. (2015) rely on graphtree transformation techniques proposed by Du et al. (2014), and apply a voting ensemble to wellstudied tree-oriented parsers.",
      "startOffset" : 11,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "(2015) and Almeida and Martins (2015). Du et al. (2015) rely on graphtree transformation techniques proposed by Du et al. (2014), and apply a voting ensemble to wellstudied tree-oriented parsers. Closely related to ours is Almeida and Martins (2015), who used rich, hand-engineered second-order features and AD for inference.",
      "startOffset" : 11,
      "endOffset" : 250
    }, {
      "referenceID" : 60,
      "context" : "A common approach when using BiLSTMs for multitask learning is to share the BiLSTM part of the model across tasks, while training specialized classifiers for each task (Søgaard and Goldberg, 2016).",
      "startOffset" : 168,
      "endOffset" : 196
    }, {
      "referenceID" : 33,
      "context" : "FREDA uses a neural generalization of “frustratingly easy” domain adaptation (Daumé III, 2007; Kim et al., 2016), where one augments domainspecific features with a shared set of features to capture global patterns.",
      "startOffset" : 77,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "In syntactic parsing, higher-order structures have commonly been used to model interactions between multiple adjacent arcs in the same dependency tree (Carreras, 2007; Smith and Eisner, 2008; Martins et al., 2009; Zhang et al., 2014, inter alia). Lluı́s et al. (2013), in contrast, used second-order structures to jointly model syntactic dependencies and semantic roles.",
      "startOffset" : 152,
      "endOffset" : 268
    }, {
      "referenceID" : 42,
      "context" : "Borrowing from Lei et al. (2014), we introduce a low-rank tensor scoring strategy that, given a higher-order structure p, models interactions between the firstorder structures (i.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 28,
      "context" : "Using the fact that a tensor of rank at most r can be decomposed into a sum of r rank-1 tensors (Hitchcock, 1927), we reparameterize W to enforce the low-rank constraint by construction:",
      "startOffset" : 96,
      "endOffset" : 113
    }, {
      "referenceID" : 41,
      "context" : "Following Lei et al. (2014), we upperbound the rank of W by r to limit the number of parameters (r is a hyperparameter, decided empirically).",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 36,
      "context" : "(11) We refer readers to Kolda and Bader (2009) for mathematical details.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 56,
      "context" : "We use 100-dimensional GloVe (Pennington et al., 2014) vectors trained over Wikipedia and Gigaword as pre-trained word embeddings.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 30,
      "context" : "To deal with out-of-vocabulary words, we apply word dropout (Iyyer et al., 2015) and randomly replace a word w with a special unksymbol with probability α 1+#(w) , where #(w) is the count of w in the training set.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 34,
      "context" : "Models are trained for up to 30 epochs with Adam (Kingma and Ba, 2015), with β1 = β2 = 0.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "5 every 10 epochs (Dozat and Manning, 2017).",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 24,
      "context" : "We clip the `2 norm of gradients to 1 (Graves, 2013; Sutskever et al., 2014), and we do not use mini-batches.",
      "startOffset" : 38,
      "endOffset" : 76
    }, {
      "referenceID" : 63,
      "context" : "We clip the `2 norm of gradients to 1 (Graves, 2013; Sutskever et al., 2014), and we do not use mini-batches.",
      "startOffset" : 38,
      "endOffset" : 76
    }, {
      "referenceID" : 45,
      "context" : "We use the same pruner as Martins and Almeida (2014), where a first-order feature-rich unlabeled pruning model is trained for each task, and arcs with posterior probability below 10−4 are discarded.",
      "startOffset" : 26,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "In addition, we also examine the effects of syntax by comparing our models to the state-of-the-art open track system (Almeida and Martins, 2015).",
      "startOffset" : 117,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "We observe similar trends on the out-of-domain test set (Table 4b), with the exception that, on PSD, our best-performing model’s improvement over the open-track system of Almeida and Martins (2015) is not statistically significant.",
      "startOffset" : 171,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : "With automatically generated syntactic parses, Almeida and Martins (2015) manage to obtain more than 1% absolute improvements over their closed track en-",
      "startOffset" : 47,
      "endOffset" : 74
    }, {
      "referenceID" : 67,
      "context" : "try, which is consistent with the extensive evaluation by Zhang et al. (2016), but we leave the incorporation of syntactic trees to future work.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 42,
      "context" : "Syntactic parsing could be treated as yet another output task, as explored in Lluı́s et al. (2013) and in the transition-based frameworks of Henderson et al.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "(2013) and in the transition-based frameworks of Henderson et al. (2013) and Swayamdipta et al.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : "(2013) and in the transition-based frameworks of Henderson et al. (2013) and Swayamdipta et al. (2016).",
      "startOffset" : 49,
      "endOffset" : 103
    }, {
      "referenceID" : 52,
      "context" : "Scores from Oepen et al. (2015).",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 35,
      "context" : "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).",
      "startOffset" : 71,
      "endOffset" : 191
    }, {
      "referenceID" : 40,
      "context" : "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).",
      "startOffset" : 71,
      "endOffset" : 191
    }, {
      "referenceID" : 68,
      "context" : "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).",
      "startOffset" : 71,
      "endOffset" : 191
    }, {
      "referenceID" : 16,
      "context" : "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).",
      "startOffset" : 71,
      "endOffset" : 191
    }, {
      "referenceID" : 44,
      "context" : "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).",
      "startOffset" : 71,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daumé III, 2007).",
      "startOffset" : 142,
      "endOffset" : 203
    }, {
      "referenceID" : 6,
      "context" : "Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daumé III, 2007).",
      "startOffset" : 142,
      "endOffset" : 203
    }, {
      "referenceID" : 8,
      "context" : "Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing.",
      "startOffset" : 0,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "Ammar et al. (2016) and Guo et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing.",
      "startOffset" : 0,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al.",
      "startOffset" : 0,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning.",
      "startOffset" : 0,
      "endOffset" : 141
    } ],
    "year" : 2017,
    "abstractText" : "We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms. By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax. We then explore two multitask learning approaches—one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly. We find that both approaches improve performance across formalisms on average, achieving a new state of the art. Our code is open-source and available at https://github.com/ Noahs-ARK/NeurboParser.",
    "creator" : "LaTeX with hyperref package"
  }
}