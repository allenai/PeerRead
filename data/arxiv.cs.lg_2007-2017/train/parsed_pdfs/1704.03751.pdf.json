{
  "name" : "1704.03751.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Enabling Embedded Inference Engine with ARM Compute Library",
    "authors" : [ "Dawei Sun", "Shaoshan Liu", "Jean-Luc Gaudiot" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Enabling Embedded Inference Engine",
      "text" : "with ARM Compute Library"
    }, {
      "heading" : "Dawei Sun, Shaoshan Liu*, and Jean-Luc Gaudiot",
      "text" : "When you need to enable deep learning on low-cost embedded SoCs, is it better to port an existing deep learning framework or should you build one from scratch? In this paper, we share our practical experiences of building an embedded inference engine using ARM Compute Library (ACL). The results show that, contradictory to conventional wisdoms, for simple models, it takes much less development time to build an inference engine from scratch compared to porting existing frameworks. In addition, by utilizing ACL, we managed to build an inference engine that outperforms TensorFlow by 25%. Our conclusion is that, on embedded devices, we most likely will use very simple deep learning models for inference, and with well-developed building blocks such as ACL, it may be better in both performance and development time to build the engine from scratch. Enabling Inference on Embedded Devices We were building an internet-of-things product with inference capabilities on our bare-metal ARM SoC, codenamed Zuluko. The Zuluko SoC contains four ARM v7 cores running at 1 GHz, as well as 512 MB of RAM. At its peak it consumes about 3 W of power. Besides low-power, it costs only about four dollars. Everything was great, except that we had to decide how to enable high-performance inference capabilities on it. An easy option was to migrate an existing deep learning platform to it, we chose to migrate TensorFlow [1] since it delivered the best performance on ARM-Linux platforms based on our study.\nWe thought we had an easy task, but it took us days to port all the dependencies of TensorFlow before we could even run TensorFlow platform. Eventually, after a week of intensive efforts, we managed to run TensorFlow on Zuluko. This experience made us wonder, whether it could be worthwhile to build a platform from scratch or better to port an existing platform. This question had two implications: first, without capable building blocks, it would be very hard to build an inference engine from scratch. Second, a built-from-scratch inference engine may not outperform a well-tested deep learning framework. Let us examine these problems in the coming sections.\nBuilding Inference Engine with ARM Compute Library Recently, ARM announced their Compute Library [2], a comprehensive collection of software functions implemented for the ARM Cortex-A family of CPU processors and the ARM Mali family of GPUs. Specifically, for Convolutional Neural Networks, it provides the basic building blocks including Activation, Convolution, Fully Connected, Locally Connected, Normalization, Pooling, and Soft-Max. These are exactly what we needed to build an inference engine. We decided to have a try, to build a SqueezeNet [3] using the building blocks.\nTo construct SqueezeNet, we started by building the fire module proposed in [3]. As shown in Figure 1, SqueezeNet utilizes a 1X1 convolution kernel to reduce the input size of the 3X3 convolution layer while maintaining similar inference accuracy. Then SqueezeNet utilizes an expand strategy to guarantee the dimension of the network does not change. This is the fire module and it is the core of SqueezeNet. We utilized the ACL core operators to implement the fire module and our implementation eliminates the extra memory copy needed for concatenation operation.\nAfter going through several fire modules, we reach the output layer of SqueezeNet. As shown in Figure 2, it includes a dropout operation and a global pooling operation. Since ACL currently does not support dropout and global pooling, we implemented our own operators from scratch. For dropout operator, the main purpose of it is to prevent overfitting in training by randomly set an output to 0, since we use the network for inference we can basically eliminate this layer. But to compensate for the change in output, we added an attenuation coefficient after pool10 layer to match the attenuation introduced in the original dropout layer."
    }, {
      "heading" : "Performance",
      "text" : "Now we have built a SqueezeNet engine using the basic building blocks from ACL. In this section, we delve into the performance of TensorFlow versus that of ACL. To ensure fair comparisons, we enabled ARM NEON vector computation optimization on TensorFlow, we also chose to use NEON-enabled building blocks when building our SqueezeNet engine. By making sure both engines utilize NEON vector computation, we ensured that any performance different would have been caused by the platform itself.\nAs shown in Figure 3, we ran SqueezeNet with TensorFlow on our Zuluko platform, using four cores, on average, it took 420 ms to process a 227X227 RGB image. Using the SqueezeNet engine built from ACL, it took only 320 ms to process the same image, delivering a 25% speedup. To better understand the performance gain, we moved one step further to divide the processing time into two groups: the first group includes convolution, RELU, and concatenate, whereas the second group includes pooling and soft-max. The breakdown results show that our SqueezeNet engine outperforms TensorFlow by 23% in group one and 110% in group two. Regarding resource utilization, when running on TensorFlow, the average CPU usage is 75% and average memory usage is about 9 MB, when running on ACL, the average CPU usage is 90% and average memory usage is about 10 MB.\nThere could be two reasons to explain the performance improvement: first, ACL provides better NEON optimization since all operators in ACL were developed using NEON intrinsics directly whereas TensorFlow relied\non the ARM compiler to provide NEON optimization. Second, TensorFlow platform itself likely introduces some performance overheads.\nNext we tried to squeeze more performance from TensorFlow and checked if it could outperform the inference engine built on top of ACL. We attempted to achieve this by using the vector quantization optimization technique [4]. The gist of this optimization is to use 8-bit weights such as to trade sacrifice accuracy for performance. In addition, with 8-bit weights, we can use vector computations to process multiple data units with one instruction. However, this optimization comes with a cost, it introduces re-quantize and de-quantize operations. We present the performance comparisons between the case with quantization vs without quantization in Figure 4. By using vector quantization, we managed to improve the performance of convolution by 25%, however, it introduces significant overheads in de-quantization and re-quantization operations. Overall, it actually slows down the whole inference process by more than 100 ms."
    }, {
      "heading" : "Discussions",
      "text" : "In this paper we shared our experiences of building a deep learning inference engine from scratch by using ACL. What led to the decision of building such an engine was the difficulty in migrating an existing engine and the unsatisfaction of its performance. Existing deep learning engines are built for generality, suitable for both training and inference tasks. Often, these engines are not optimized for embedded inference tasks. Also, these engines depend on many other third party libraries not readily available on bare-metal embedded systems, making them very hard to migrate. In contrast, speaking of performance, with building blocks provided by ACL, we could build embedded inference engines to deliver high performance since we could fully utilize the heterogeneous computing resources\nprovided by the SoC. Thus, the question becomes whether it is easier to migrate existing engines, or is it easier to build from scratch. Our experiences presented in this paper show that if the model is simple, it is much easier to build from scratch. As the model gets more and more complicated, we may hit a point where it is more efficient to port an existing engine. However, it is highly unlikely that we use highly complicated models for embedded inference tasks. Therefore, we conclude that built-from-scratch embedded inference engine may be a viable solution to bring deep learning capabilities to embedded devices."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467",
      "author" : [ "M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and<",
      "author" : [ "F.N. Iandola", "S. Han", "M.W. Moskewicz", "K. Ashraf", "W.J. Dally", "K. Keutzer" ],
      "venue" : "MB model size. arXiv preprint arXiv:1602.07360",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "An easy option was to migrate an existing deep learning platform to it, we chose to migrate TensorFlow [1] since it delivered the best performance on ARM-Linux platforms based on our study.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "We decided to have a try, to build a SqueezeNet [3] using the building blocks.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "To construct SqueezeNet, we started by building the fire module proposed in [3].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "REFERENCES: [1] Abadi, M.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 1,
      "context" : "com/compute-library/ [3] Iandola, F.",
      "startOffset" : 21,
      "endOffset" : 24
    } ],
    "year" : 2017,
    "abstractText" : "When you need to enable deep learning on low-cost embedded SoCs, is it better to port an existing deep learning framework or should you build one from scratch? In this paper, we share our practical experiences of building an embedded inference engine using ARM Compute Library (ACL). The results show that, contradictory to conventional wisdoms, for simple models, it takes much less development time to build an inference engine from scratch compared to porting existing frameworks. In addition, by utilizing ACL, we managed to build an inference engine that outperforms TensorFlow by 25%. Our conclusion is that, on embedded devices, we most likely will use very simple deep learning models for inference, and with well-developed building blocks such as ACL, it may be better in both performance and development time to build the engine from scratch.",
    "creator" : "Word"
  }
}