{
  "name" : "1610.08602.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Review of 40 Years of Cognitive Architecture Research: Focus on Perception, Attention, Learning and Applications",
    "authors" : [ "Iuliia Kotseruba", "Oscar J. Avella Gonzalez", "John K. Tsotsos" ],
    "emails" : [ "yulia_k@cse.yorku.ca", "ojavellag@cse.yorku.ca", "tsotsos@cse.yorku.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "existing architectures is nearing several hundred, most of the existing surveys do not reflect this growth and focus on a handful of well-established architectures. While their contributions are undeniable, they represent only a part of the research in the field. Thus, in this survey we wanted to shift the focus towards a more inclusive and high-level overview of the research in cognitive architectures. Our final set of 86 architectures includes 55 that are still actively developed, and borrow from a diverse set of disciplines, spanning areas from psychoanalysis to neuroscience. To keep the length of this paper within reasonable limits we discuss only the core cognitive abilities, such as perception, attention mechanisms, learning and memory structure. To assess the breadth of practical applications of cognitive architectures we gathered information on over 700 practical projects implemented using the cognitive architectures in our list.\nWe use various visualization techniques to highlight overall trends in the development of the field. For instance, our data confirms that the hybrid approach to cognitive modeling already dominates the field and will likely continue to do so in the future. Our analysis of practical applications shows that most architectures are very narrowly focused on a particular application domain. Furthermore, there is an apparent gap between general research in robotics and computer vision and research in these areas within the cognitive architectures field. It is very clear that biologically inspired models do not have the same range and efficiency compared to the systems based on engineering principles and heuristics. Another observation is related to a general lack of collaboration, which is surprising to see in the inherently interdisciplinary area of cognitive architectures. Several factors hinder communication, among which are the closed nature of the individual projects (only one-third of the reviewed here architectures are open-source) and terminological differences.\nKeywords: survey; cognitive architectures; perception; attention; learning; practical applications\n1 Introduction The goal of this paper is to provide a broad overview of the last 40 years of research in cognitive architectures with an emphasis on perception, attention and practical applications. Although the field of cognitive architectures has been steadily growing, most of the surveys published in the past 10 years do not reflect this growth and feature essentially the same set of a dozen most established architectures. The latest large-scale study was conducted in 2010 by Samsonovich et al. [1] in an attempt to catalog implemented cognitive architectures. Their survey contains descriptions of 54 cognitive architectures submitted by their respective authors. The same information is also presented online in a Comparative Table of Cognitive Architectures1. There are also other listings of cognitive architectures, but they rarely go beyond a short\ndescription and a link to the project site or a software repository.\nSince there is no exhaustive list of cognitive architectures, their exact number is unknown, but it is estimated to be around three hundred, out of which at least one-third of the projects are currently active. To form the initial list for our study we combined architectures mentioned in surveys (published within the last 10 years) and several large online catalogs. We also included more recent projects not yet mentioned in the survey literature. Figure 1 shows the visualization of 195 cognitive architectures featured in 17 sources (surveys, online catalogs and Google Scholar). It is apparent that a small group of architectures including ACT-R, Soar, CLARION, ICARUS, EPIC, RCS and LIDA is present in many sources, while all other projects are only briefly mentioned in online catalogs. While the theoretical and practical contributions of\n1 http://bicasociety.org/cogarch/architectures.htm\nthe major architectures are undeniable, they represent only part of the research in the field. Therefore, in this review we want to shift the focus from detailed descriptions of heavyweights, which has been done elsewhere, towards a high-level overview of the field.\nTo make this survey manageable we reduced the original list of architectures to 86 items. Since, our main focus is on implemented architectures with at least one practical application and several peer-reviewed publications, we do not consider some of the philosophical architectures (e.g. CogAff, Society of Mind, Global Workspace Theory, Pandemonium theory, etc.). We also exclude large scale brain modeling projects, which are low-level and do not easily map onto the breadth of cognitive capabilities modeled by other types of cognitive architectures. Further, many of the brain models do not have any practical applications yet, and thus do not fit the parameters of the present survey. Figure 2 shows all architectures featured in this survey with their approximate timelines recovered from the publications. Of these projects 55 are currently active.\nAs we mentioned earlier, the first step towards creating an inclusive and organized catalog of implemented cognitive architectures was made in [1]. This review contained extended descriptions of 26 projects with the following information: short overview, schematic diagram of major elements, common components and features (memory types, attention, consciousness, etc.), learning and cognitive development, cognitive modeling and applications, scalability and limitations. A survey of this kind brings together researchers from several disjoint communities and helps to establish a mapping between the different approaches and terminology they use. However, the descriptive or tabular format does not allow easy comparisons between architectures. Since our sample of architectures is large, we experimented with alternative visualization strategies, such as alluvial and circular diagrams which are frequently used for organizing complex tabular data. Interactive versions of these diagrams allow to explore the data and view relevant references.\nIn the following sections we will provide an overview of the definitions of cognition and approaches to grouping cognitive architectures. As one of our contributions, we map cognitive architectures according to their perception modality, implemented mechanisms of attention, memory organization, types of learning, and practical applications. Other features of cognitive architectures, such as metacognition, consciousness and emotion, are beyond the scope of this survey.\nIn the process of preparing this paper, we examined the literature widely and this activity led to a 2000 item bibliography of relevant publications. We provide this bibliography, with short summary descriptions for each paper as supplementary material2.\n2 What are Cognitive Architectures? Cognitive architectures are a part of research in general AI, which began in the 1950s with the goal of creating programs that could reason about problems across different domains, develop insights, adapt to new situations and reflect on themselves. Similarly, the ultimate goal of research in cognitive architectures is to achieve human-level artificial intelligence. According to Russell and Norvig [2] such artificial intelligence may be realized in four different ways: systems that think like humans, systems that think rationally, systems that act like humans, and systems that act rationally. The existing cognitive architectures have explored all four possibilities. For instance, human-like thought is pursued by the architectures stemming from cognitive modeling. In this case, the errors made by an intelligent system are tolerated as long as they match errors typically made by people in similar situations. This is in contrast to rationally thinking systems which are required to produce consistent and correct conclusions for arbitrary tasks. A similar distinction is made for machines that act like humans or act rationally. Machines in either of these groups are not expected to think like humans, only their actions or behavior is taken into account.\n2 Links to supplementary materials, bibliography and interactive diagrams are available at http://www.data.nvision2.eecs.yorku.ca/cognitive-architecture-survey.\nHowever, with no clear definition and general theory of cognition, each architecture was based on a different set of premises and assumptions, making comparison and evaluation difficult. Several papers were published to resolve the uncertainties, the most prominent being Sun’s desiderata for cognitive architectures [3] and Newell’s functional criteria (originally published in [4] and [5], and later restated by Anderson and Lebiere [6]). Newell’s criteria include flexible behavior, real-time operation, rationality, large knowledge base, learning, development, linguistic abilities, self-awareness and brain realization. Sun’s desiderata are broader and include ecological, cognitive and bio-evolutionary realism, adaptation, modularity, routineness and synergistic interaction. Besides defining these criteria and applying them to a range of cognitive architectures, Sun also pointed out the lack of clearly defined cognitive assumptions and methodological approaches, which hinder progress in studying intelligence. He also noted an uncertainty regarding essential dichotomies (implicit/explicit, procedural/declarative, etc.), modularity of cognition and structure of memory. However, a quick look at the existing cognitive architectures reveals persisting disagreements in terms of their research goals, structure, operation and application."
    }, {
      "heading" : "EPIC",
      "text" : "Instead of looking for a particular definition of intelligence, it may be more practical to define it as a set of competencies and behaviors demonstrated by the system. While no comprehensive list of capabilities required for intelligence exists, several broad areas have been identified that may serve as guidance for ongoing work in the cognitive architecture domain. For example, Adams et al. [7] suggest 14 areas, namely, perception, memory, attention, social interaction, planning, motivation, actuation, reasoning, communication, learning, emotion, modeling self/other, building/creation and arithmetic abilities. These are further split into subareas. Arguably, some of these categories may seem more important than the others and historically attracted more attention. For example, the list of cognitive functions frequently mentioned in the recent publications on cognitive architectures, according to Metzler and Shea [8], includes only perception, learning, reasoning, decision making, planning and acting. However, even implementing a reduced set of abilities in a single architecture is a substantial undertaking. Unsurprisingly, Artificial General Intelligence (AGI) is currently pursued by only a handful of architectures, among which are Soar, ACT-R, NARS [9], LIDA [10], and several recent projects, such as SiMA [11] and OpenCogPrime [12]. Others focus on a particular aspect of cognition, e.g. attention (ARCADIA [13], STAR [14]), emotion (CELTS [15]), perception of symmetry (the Cognitive Symmetry Engine [16]) or problem solving (FORR [17], PRODIGY [18]). There are also specialized architectures designed for particular applications, such as ARDIS [19] for visual inspection of surfaces or MusiCog [20] for music comprehension and generation.\nThe criteria on which a software system can be called a cognitive architecture are also rarely addressed. Most of the surveys broadly define cognitive architectures as a blueprint for intelligence, or more specifically, a proposal about the mental representations and computational procedures that operate on these representations enabling a range of intelligent behaviors ([21], [22], [23], [24], [25]). There is generally no need to justify inclusion of the established cognitive architectures such as Soar, ACT-R, EPIC, LIDA, ICARUS and a few others. However, when it comes to less common or new projects, reasons for considering them are less clear. As an example, AKIRA, a framework that explicitly does not self-identify as a cognitive architecture [26], is featured in some surveys anyway [27]. Similarly, a knowledge base Cyc [28], which does not make any claims about general intelligence is presented as an AGI architecture in [29].\nLaird in [30] discussed how cognitive architectures differ from other software systems. While all of them have memory storage, control components, data representation, and input/output devices, the former provide only a fixed model for general computation. Cognitive architectures, on the other hand, must change through development and efficiently use knowledge to perform new tasks. Furthermore, he suggests that agent architectures built using toolkits and frameworks also cannot be considered cognitive architectures because of their lack of theoretical commitments. This is a rather restricting set of conditions, which few architectures other than Soar and ACT-R would satisfy. This view is also not common in the\nsurvey literature, where agent architectures and toolkits used to build them are also included. For example, agent architectures 3T, PRS and ERE are reviewed in [31] and Pogamut, a framework for building intelligent agents, is included in [1].\nRecently, claims have been made that deep learning is capable of “solving AI” by Google (DeepMind3). Likewise, Facebook AI Research (FAIR4) and other companies are actively working in this direction.\nWhere does this work stand with respect to cognitive architectures? At present, some of the most publicized achievements of deep learning include vision processing for self-driving cars (Mobileye5) and Google’s\nneural networks capable of playing Go [32] and multiple video games [33].\nOn the other hand, the publications by DeepMind, not advertised in media, cover a wider range of topics. Many papers, for instance, are dedicated to the theoretical problems related to recurrent and deep neural networks. In addition, neural networks are also used for building sophisticated models of visual attention and memory, for example, an attention-based model for recognizing multiple objects in the image (e.g. house number sequences) [34]. Memory is of special importance in the deep learning domain, since in order to find and exploit complex patterns in data, a network should be able to perform chains of sequential computations. However, in deep networks the information from past computations can be affected by new information. Grid Long Short-Term Memory (Grid LSTM) solves this problem by providing an ability to dynamically select or ignore inputs to retain important memory contents during learning [35].\nOverall, DeepMind research addresses a number of important issues in AI, such as natural language understanding, perceptual processing, general learning, and strategies for evaluating artificial intelligence. Although particular models already demonstrate cognitive abilities in limited domains, at this point they do not represent a unified model of intelligence.\nUnlike DeepMind, the Facebook research team explicitly discusses their work in the broad context of developing intelligent machines in [36]. Their main argument is that AI is too complex to be built all at once and instead its general characteristics should be defined first. Two such characteristics of intelligence are defined, namely, communication and learning, and a concrete roadmap is proposed for developing them incrementally. As a first step in this direction, an artificial ecosystem (or a “kindergarten”) is proposed for teaching an intelligent agent, thus emphasizing the developmental nature of the process. The plan is to start with a simpler simulated environment and gradually increase its complexity, until eventually it can connect the artificial agent to the real world. Given the emphasis on communication and learning, a primary application of such an intelligent machine would be an electronic assistant. Authors acknowledge that similar ideas have been tried in the past (e.g. Blocks World simulation, frequently used by symbolic architectures for learning), but relied too much on the data provided by their creators.\nCurrently there are no publications about developing such a system, but overall research topics pursued by FAIR align with their proposal for AI and also the business interests of the company. Common topics include visual processing, especially segmentation and object detection, data mining, natural language processing, human-computer interaction and network security. Since current deep learning techniques are mainly applied to solving practical problems and do not represent a unified framework we do not include them in this review. However, given their prevalence in other areas of AI, deep learning methods will likely play some role in the cognitive architectures of the future.\nFor the rest of the architectures we defined the following selection criteria, striving both for inclusiveness and consistency: self-identification as cognitive, robotic or agent architecture, existing implementation (not necessarily open-source), and mechanisms for perception, memory, attention and learning. To further limit the scope of the survey we required at least several peer-reviewed papers and existing applications beyond simple illustrative examples. In order to include some of the newer architectures still under development some of these conditions were relaxed.\n3 https://deepmind.com/ 4 https://research.facebook.com/ai/ 5 http://www.mobileye.com/\n3 Taxonomies of Cognitive Architectures Many papers published within the last decade address the problem of evaluation rather than the categorization of cognitive architectures. As mentioned earlier, Newell’s criteria ([4], [5], [6]) and Sun's Desiderata [3] belong in this category. Similarly, Langley et al. [31] define a comprehensive list of capabilities, properties and evaluation criteria for cognitive architectures. The suggested set of cognitive capabilities includes recognition, decision making, perception, prediction, planning, acting, communication and learning. In order to evaluate the architectures, criteria such as generality, versatility and autonomy are proposed, among others. Along the same lines Vernon et al. [37] lists 12 characteristics to compare cognitivist and emergent approaches, which include embodiment, perception, action, adaptation, motivation, autonomy and others. Similarly, Asselman et al. [38] evaluate architectures based on 7 criteria (perception, memory, learning, modularity, goal setting, underlying model and problem-solving) and Thorisson and Helgasson [27] determine their level of autonomy based on 4 criteria (real-time operation, learning, attention and meta-learning).\nWhile many of these criteria could be used to classify the architectures, they are usually too fine-grained to be applied to a generic architecture. Thus it is more common to group cognitive architectures based on the type of information processing they represent. Three categories are defined: symbolic (also referred to as cognitivist), emergent (connectionist) and hybrid. This taxonomy based on information processing was extended by Duch et al. [21] to include typical memory and learning properties for each group (Figure 4). It has been used in other surveys as well ([38], [39]).\nSymbolic systems are often implemented as a set of if-then rules (also called production rules) that can be applied to a set of symbols representing the facts known about the world. Because it is a natural and intuitive representation of knowledge, symbolic manipulation remains very common. Although by design, symbolic systems excel at planning and reasoning, they lack the flexibility and robustness that are required for dealing with a changing environment and for perceptual processing.\nThe emergent approach resolves these issues by building massively parallel models, analogous to neural networks, where information flow is represented by a propagation of signals from the input nodes. However, the resulting system also loses its transparency, since knowledge is no longer a set of symbolic entities and instead is distributed throughout the network. For similar reasons, logical inference in a traditional sense becomes problematic in emergent architectures.\nNaturally, each paradigm has its strengths and weaknesses. For example, any symbolic architecture requires a lot of work to create an initial knowledge base, but once it is done the architecture is fully functional. On the other hand, emergent architectures are easier to design, but they must be trained in order to produce useful behavior. Furthermore, their existing knowledge may deteriorate with the subsequent learning of new behaviors.\nAs neither paradigm is capable of addressing all major AI issues, hybrid architectures attempt to\ncombine elements of both symbolic and emergent approaches. In general, there are no restrictions on how this hybridization is done, but some approaches may be more intuitive and easier to implement. For example, CLARION uses different representations depending on the type of knowledge: explicit factual knowledge is symbolic, while procedural implicit knowledge is subsymbolic [40]. 4CAPS combines a traditional symbolic production system interpreter with connectionist computational mechanisms such as thresholds, activations, weights, and parallel processing [41]. This type of hybridization is termed symbolicconnectionist by Duch et al. [21]. They also define an alternative - a localist-distributed approach. A good example of the latter is Leabra, which uses a localist representation for labels and distributed representation for features when learning invariant object detection [42]. In [43] the two types of hybridization are called a horizontal and vertical integration respectively. A more fine-grained classification of hybrid connectionist-symbolic models is presented in [44].\nInterestingly, emergent architectures have gained more weight only relatively recently, although\nresearch in this direction has been active for at least as long as traditional AI (Figure 2). For example,\nLangley et al. [22] do not include connectionist models in their survey since they do not demonstrate the same functionality as symbolic and hybrid models.\nGiven the advantages of the hybrid approach, it is not surprising that such architectures already are the most numerous with the tendency to grow even more (Figure 3). Thus, our data confirms a prediction made by Duch et al. [21] almost a decade ago.\nThere are not many alternative taxonomies in the literature. A scheme by Sun [47] (Figure 5) emphasizes modularity and communication between the modules. However, following this taxonomy requires implementation details for each architecture, which are often not available. Other classification schemes are very specific, e.g. the one by Gray [46], which focuses on the purpose and use of architectures (Figure 6).\nTherefore, in the present survey we follow the traditional distinction between symbolic, emergent and hybrid architectures. However, since the backgrounds of the architectures presented here span research areas from philosophy to neurobiology, we do not attempt to create a single organizational scheme to fit them all. Instead for each broad cognitive function, namely perception, attention, memory and learning, we extract the relevant data from the publications and group it by frequency.\n4 Perception Regardless of its design and purpose, an intelligent system cannot exist in isolation and requires an input to produce behavior. Perception is a process that transforms raw input into the system's internal representation for carrying out cognitive tasks. The amount and type of incoming perceptual data depends on the type and number of sensors available and cognitive capability of the intelligent system, i.e. what it can derive from this data.\nFigure 7 shows physical and virtual sensors used by cognitive architectures and their corresponding human senses: vision, hearing, touch, smell and proprioception. The taste modality is only featured in BBD robots and is simulated by the conductivity of the object [48]. Other sensory modalities that do not have a correlate among human senses are symbolic input (using keyboard or graphical user interface (GUI)) and various sensors such as LADAR, laser, IR, etc.\nproprioception. The “sensor” category includes various sensors that do not correspond to any human senses. The “symbolic input” category means that input to the cognitive architecture is provided as text or through GUI. This does not include text input simulating audio, visual or any other information. Subcategories of the sensory modalities are shown as bands within the sectors (e.g. in vision subcategories include Kinect, monocular cameras, simulations, etc.). Ribbons are drawn between the sensory modalities and architectures that implement them. If an architecture implements several sensory modalities, the ribbons are drawn as more opaque. Visualization is created with www.circos.ca [49]. A list of perceptual modalities for each cognitive architecture with relevant references is included in supplementary materials and in the interactive version of the diagram.\nVision\nVision is the most represented sensory modality. This is evident from the diagram in Figure 7, where the corresponding circle segment is the largest. Even though the fact that in robotics various non-visual sensors and proprioception (e.g. odometry and bumpers) are used for solving visual tasks such as navigation, obstacle avoidance and search, visual input accounts for more than half of all possible input modalities.\nReal environments for embodied cognitive architectures vary both in size and visual complexity. For example, a planetary rover controlled by ATLANTIS performs cross-country navigation in a rough rocky terrain [50], salesman robot Gualzru (CORTEX [51]) moves around a large room full of people and iCub (MACsi [52]) recognizes and picks up various toys from the table. On the other hand, simple environments with no clutter or obstacles are also used in cognitive architectures research (BECCA [53], MDB [54]). In addition, color-coding objects is a common way of simplifying visual processing. For instance, ADAPT tracks a red ball rolling on the table [55] and DAC orients itself towards targets marked with different colors [56].\nCognitive architectures that operate in realistic unstructured environments usually approach vision as an engineering problem and construct chains of standard computer vision techniques for a particular situation. For example, obstacle avoidance in navigation tasks is solved in the same manner as in robot control architectures: 4D-RCS reconstructs the scene from stereo images and applies machine learning to LADAR data and color histograms to find traversable ground [57]. Similarly, the ATLANTIS robot architecture constructs a height map from stereo images and uses it to identify obstacles [58]. The CARACaS architecture also computes a map for hazard avoidance from stereo images and range sensors. In addition, it uses optical flow and model-based segmentation to identify the moving objects in the scene [59].\nA typical task in social robotics is detecting people and their gestures. Here too a combination of input from camera and sensors, and common vision techniques are frequently used. For instance, DIARC uses a neural network for face detection, laser readings for leg detection [60] and least-squares regression over SIFT features for object recognition [61]. The cognitive architecture for the child-like robot iCub utilizes SURFfeatures, AdaBoost learning and a mixture of Gaussians for hand detection and tracking [62]. RoboCog and CORTEX use camera and a proprietary software from Kinect to find people in a robotic salesman scenario [63]. Then, local binary patterns and a SVM classifier are trained to recognize particular persons and infer their gender and age [51]. Kismet, a social robot, uses simple motion detection, skin tone feature map and neural networks to find the faces and eyes of its caregivers [64].\nFewer systems incorporate biologically plausible vision. One of the most elaborate examples is the Leabra vision system called LVis [65], which is based on the anatomy of the ventral pathway of the brain: the primary visual cortex (V1), extrastriate areas (V2, V4) and the inferotemporal (IT) cortex. As in the human visual system, the receptive fields of neurons become less location specific and more featurally complex in the higher levels of the hierarchy. All layers are reciprocally connected, allowing the higherlevel information to influence the bottom-up processing during both the initial learning and subsequent recognition of objects, and contain local, recurrent inhibitory dynamics that limit activity levels across layers. Among its many applications, Leabra is able to distinguish dozens of object categories from synthetic images with noise and occlusion [66].\nThe visual system of Darwin VIII (BBD) is also modeled on the primate ventral visual pathway. Similarly to LVis, neurons in successive areas have progressively larger receptive fields. With this system on board Darwin VIII is able to segment a scene and categorize visual objects (simple shapes and colors) [67].\nThe SASE architecture implements a Staggered Hierarchical Mapping (SHM) for low-level visual feature processing. Although it does not replicate the structure of the human visual pathways, SHM is a hierarchical neural network with localized connections, where each neuron gets input from a restricted region in the previous layer. The sizes of receptive fields within one layer are the same and increase in higher levels [68]. SHM was tested on a SAIL robot in an indoor navigation scenario [69].\nOther biologically inspired neural networks are ART [70] and HTM [71] which are used for various\nvisual recognition and classification tasks.\nGiven that deep learning methods are becoming increasingly popular in computer vision it is surprising to see that not many cognitive architectures employ them. One example is a deep learning architecture DeSTIN, which processes visual input in the OpenCogPrime architecture [72].\nOverall, the architectures that work with realistic visual input, typically in robotics, tend to include specialized pipelines for each scenario, which is becoming easier with the widely available software toolkits such as OpenCV or Kinect API. On the other hand, the systems that try to model more general purpose and biologically plausible visual systems, such as, Leabra or BBD, are not as efficient and their applications are limited to controlled environments.\nSimulations are also commonly used as an alternative to visual processing. Despite varying visual complexity and realism, simulations usually provide the same data: objects, their properties (color, shape, label, etc.), locations and properties of the agent itself and sometimes environmental factors (e.g. weather). Some simulations provide pixel-level data, for example, the one used for the experiments with Leabra [42]. This simulation is represented by 100 categories of objects rendered in 3D with various illumination levels and partial occlusions. Otherwise, the visual realism of the simulation usually serves purely aesthetic purposes (CoJACK [73], Novamente [74], Pogamut [75]).\nAudition\nAudition is less commonly implemented in cognitive architectures. Sound or voice commands are typically used to guide an intelligent system or to communicate with it. Since the auditory modality is purely functional, most architectures resort to using available speech-to-text software rather than develop models of audition. Among the few architectures modeling auditory perception are ART, ACT-R and EPIC. For example, ARTWORD and ARTSTREAM were used to study integration [76] and a model of music interpretation was developed with ACT-R [77].\nMore commonly, dedicated software is used for speech processing, which helps to achieve a high degree of complexity and realism, as is demonstrated by the following example. A salesman robot controlled by the CORTEX architecture can understand and answer questions about itself using a Microsoft Kinect Speech SDK [51]. The Playmate system based on CoSy uses a Nuance Recognizer v8.5 software for speech processing [78] and can have a meaningful conversation in a subset of English. As an illustration, consider a typical dialog from Playmate [79]:\nHuman picks up the red square and puts down a red triangle to the right of the blue\nsquare. Robot: \"What is the thing to the right of the blue square?\" Human: \"It is a red triangle.\" Robot: \"Ok.\"\nThe FORR architecture uses speech recognition for the task of ordering books from the public library by phone. The architecture in this case increases the robustness of the automated speech recognition system based on an Olympus/RavenClaw pipeline (CMU) [80]. In this sample interaction, FX2 is the automated system, the user is a person calling and ASR shows the results of the speech processing:\nFX2: What title would you like? User: Family and Friends ASR: FAMILY.FRIENDS. FX2: I have two guesses. The first is Family and Friends. The second is Family\nHappiness. Is it either of these? User: The first one ASR: ..NEXT..FIRST. FX2: Let’s try something else. Is the full title Family and Friends? User: Yes\nASR: YES\nOther examples of systems using off-the-shelf speech processing software include the Polyscheme using\nViaVoice [81] and ISAC with a Microsoft Speech engine [82].\nIt is easy to notice that most human-robot interactions have to follow a certain script in order to be successful. Some recent architectures, such as DIARC, aim at supporting more naturally sounding requests like “Can you bring me something to cut a tomato?”, however, they are still in the early stages of development [83].\nIn our sample of architectures, we find that most effort is directed at the linguistic and semantic information carried by speech and less attention is paid to the emotional content, e.g. loudness, speech rate, and intonation. Some attempts in this direction are made in social robotics. For example, the robot Kismet does not understand what is being said, but can determine approval, prohibition or soothing based on the prosodic contours of the speech [84]. A virtual agent Gendalf controlled by Ymir architecture also has a prosody analyzer together with a grammar-based speech recognizer that can understand a limited vocabulary of 100 words [85]. Even the sound itself can also be used as a cue, for example, the BBD robots can orient themselves toward the source of a loud sound [67].\nSymbolic input\nThe symbolic input category in Figure 7 combines several input methods which do not fall under physical sensors or simulations. These include input in the form of text commands and data, and through (GUI). Text input is typical for the architectures performing planning and logical inference tasks (e.g. NARS [9], OSCAR [86], MAX [87], Homer [88]). Text commands are usually written in terms of primitive predicates used in the architecture, so no additional parsing is required.\nAlthough many architectures have tools for visualization of results and the intermediate stages of computation, interactive GUIs are less common. They are mainly used in human performance research to simplify input of the expert knowledge and to allow multiple runs of the software with different parameters (IMPRINT [89], MAMID [90], OMAR [91], R-CAST [92]).\nThe data input can be in text or any other format, and is primarily used in the categorization and\nclassification applications (e.g. HTM [93], CSE [16], ART [94]).\n5 Attention Following Chun et al. [95] we consider two major categories of the attention mechanisms - external and internal. External or perceptual attention selects and modulates information incoming from various senses and internal attention modulates internally generated information, such as the contents of working memory or a set of possible behaviors in a given context6. Figure 8 shows common types of external and internal\nattention mechanisms. Here external (perceptual) attention is subdivided into region of interest (ROI) selection, gaze control, top-down (task-driven) and bottom-up (data-driven). Task selection is represented by three broad categories: predefined scenarios, planning, reactive actions and dynamic action selection. Predefined scenarios are usually represented by a predefined set of actions to perform (e.g., replicate an experiment) or in the form of finite state machine. Planning refers to a traditional planning approach common in many symbolic architectures. Here actions are selected automatically based on the computed plan. Reactive actions are typical for robotic systems, where they are connected to particular sensors and have priority over any other action. This feature is useful for safety in robotics (e.g., stop the robot if its bumper sensors touch an obstacle). Finally, dynamic action selection means that the next action depends\n6 Note that external and internal attention are not the same as exogenous (bottom-up, transient) and endogenous (top-down) attention. External attention here includes both top-down and bottom-up attention and any other mechanisms that involve perception. Internal attention involves only the internal action section mechanisms (such as emotions and drives), which may also be indirectly affected by external attention.\non multiple dynamic factors, such as the available sensory information, urgency, previous experience, biases due to emotions and drives, etc. Note that all these mechanisms are not exclusive and many architectures implement several of them at once. For example, FORR architecture integrates reactivity, situation-based behavior, and heuristic reasoning in a three-tiered hierarchical model [96]. Similar hierarchical decision making process is implemented in RCS [97].\nExternal attention A particular implementation of perceptual attention would differ depending on the sensory modality, so auditory data would be treated differently from the visual input. Several architectures can also modulate auditory data (OMAR [98], iCub [99] and MIDAS [100]), but otherwise visual attention is much more common.\nThe selection of visual data to attend can be data-driven (bottom-up) or task-driven (top-down). The bottom-up attentional mechanisms identify salient regions whose visual features are distinct from the surrounding image features, usually along a combination of dimensions, such as color channels, edges, motion, etc. Some architectures resort to the classical visual saliency algorithms, such as Guided Search [101] used by ACT-R [102] and Kismet [103], or the Itti-Koch-Niebur model [104] used by ARCADIA\n[13], iCub [99] and DAC [105]. Other approaches include filtering (DSO), finding unusual motion patterns (MACsi [106]) or discrepancies between observed and expected data (4D-RCS [107]).\nTop-down attention can be applied to further limit the sensory data provided by the bottom-up processing. For example, in visual search, knowing desired features of the object (e.g., the color red) can narrow down the options provided by the data-driven figure-ground segmentation. Many architectures resort to this mechanism to improve search efficiency (ACT-R [108], APEX [109], ARCADIA [110], CERA-CRANIUM [111], CHARISMA [112], DAC [105]). Another option is to use a hard-coded or learned heuristics. For example, CHREST looks at typical positions on a chess board [113] and MIDAS replicates common eye scan patterns of pilots [100]. The limitation of the current top-down approaches is that they can direct vision for only a limited set of predefined visual tasks, however ongoing research in STAR attempts to address this problem [114], [115].\nInternal attention Unlike external attention, which filters perceptual information, internal attention selects an appropriate action from a set of possibilities. The simplest mechanism of selection is reactive. Although few systems besides Subsumption [116] are purely reactive, hardcoded actions for particular scenarios, such as collision avoidance, are common in robotic architectures (e.g. PRS [117], ERE [118], ASMO [119]).\nThe cognitive architectures that run in a single thread or sequentially usually do not require a specific attention mechanism to select the next action or task. It happens automatically by arranging goals in a stack or queue. Since only one goal can be attended at a time, the one on top of the stack is always satisfied first. If the current goal cannot be reached, it is decomposed into subgoals, which are pushed onto the stack and attended sequentially. When all subgoals are successfully resolved, they are popped off the stack and the parent goal becomes the new focus of attention. The goals may be suspended or canceled if a more urgent goal is pushed onto the stack (e.g., bumpers on a robot signaling a collision). This approach is used in ICARUS [120], ACT-R [121], AIS [122], IMPRINT [123] and other primarily symbolic planners.\nA more flexible solution is to dynamically rank available actions based on their relevance to the current goal, urgency or previous success (e.g. CLARION [124], Pogamut [125], NARS [126], Ikon Flux [127], R-CAST [128], COGNET [129]). Since at every cognitive cycle the new sensory information may change the ranking of existing options or add new ones to the mix, these systems should theoretically be more adaptive. In addition to the perceptual evidence and past experience, other internal factors such as emotions or drives may also affect where the focus of attention will be moved next (ARS/SiMA [11]).\nDynamic attention control is also efficient in architectures that simulate cognition as multiple concurrent processes, where only a select few processes reach consciousness (associated with attention). Such architectures are based on the Global Workspace Theory of Baars [130] (ARCADIA [13], CERACRANIUM [131], CELTS [132], LIDA [133], MLECOG [134]) or Society of Mind by Minsky [135] (Cerebus [136]). Attention could be a separate process, evaluating each running process and selecting relevant ones, as in CELTS [137], or values could be updated as a result of interaction with other processes. The processes above threshold are then brought to attention automatically (CERA-CRANIUM [138], LIDA [139]).\n6 Memory Memory is an essential part of any systems-level cognitive model, regardless of whether the model is being used for studying the human mind or for solving engineering problems. For instance, all architectures featured in this review have memory systems that store intermediate results of computations, enabling learning and adaptation to the changing environment. However, despite their functional similarity, the particular implementations of memory systems differ significantly and depend on the research goals and conceptual limitations, such as biological plausibility, as well as engineering factors (e.g. programming language, software architecture, use of frameworks, etc.). Broadly speaking, there are two approaches to modeling memory: 1) introducing distinct memory stores based on duration (short- and long-term) and type\n(procedural, declarative, semantic, etc.), and 2) a single memory structure representing several types of knowledge.\nThe first approach is influenced by the multi-store memory model of Atkinson-Shiffrin (1968) [140], later modified by Baddeley (1976) [141]. Although these theories are dominant in psychology, their validity for engineering is questioned by some because they do not provide a functional description of various memory mechanisms [142]. Nevertheless, most architectures distinguish between various memory types, although the naming conventions differ depending on the conceptual background. For instance, the architectures designed for planning and problem solving have short- and long-term memory storage systems, but do not use terminology from cognitive psychology. The long-term knowledge in planners is usually referred to as a knowledge base for facts and a set of problem-solving rules, which correspond to semantic and procedural long-term memory (e.g. Disciple [143], MACsi [106], PRS [144], ARDIS [145], ATLANTIS [146], IMPRINT). Some architectures also save previously implemented tasks and solved problems, imitating episodic memory (REM [147], PRODIGY [148]). The short-term storage in planners is usually represented by a current world model and a contents of the goal stack.\nFigure 9 shows a visualization of various types of memory implemented by the architectures. Here we primarily distinguish between the long-term and short-term storage. Long-term storage is further subdivided into episodic, semantic and procedural types, which store episodes from the personal experience of the system, factual knowledge and information on what actions should be taken under certain conditions respectively. Short-term storage is split into short-term memory (STM) and working memory (WM) following [149]. STM is a very short-term buffer that stores several recent percepts. It is also referred to as perceptual or sensory memory in some architectures. Working memory is a temporary storage for percepts that also contains other items related to the current task and is frequently associated with the current focus of attention.\nShort-term memory Only a few architectures implement short-term or sensory memory, which is a buffer for temporarily holding the incoming sensory data before it is transferred to other memory structures. Iconic visual memory is part of ACT-R [102], ARCADIA [110], EPIC [150], LIDA [151] and Pogamut [125]. MusiCog implements echoic memory, a sensory memory for auditory signals [152]. In all these architectures the sensory buffer preprocesses and stores recently seen (or heard) information from tens to hundreds of milliseconds.\nWorking memory Working memory is defined as a mechanism for the temporary storage of information related to the current task. In principle, any computation inherently requires a temporary storage for partial and intermediate data, therefore every cognitive architecture on our list implements working memory to some extent. Based on the publications we reviewed, many architectures implement working memory as a dynamic storage with no explicitly defined capacity. For example, in APEX, semantic working memory is represented by an assertional database with timestamped propositions [153], BECCA keeps a weighted combination of several recently attended features and any recent actions [154], in MAMID working memory contains a currently activated set of instructions [155], and Companions utilize working memory as a cache for intermediate results [156]. Similar definitions are given for DSO [157], DIARC [61], CoSy [158], etc.\nSince, by definition, working memory is a relatively small temporary storage, for biological realism, its capacity should be limited. However, there is no agreed upon way of how this should be done. For instance, in GLAIR the contents of working memory are discarded when the agent switches to a new problem [159]. A more common approach is to gradually remove items from the memory based on their recency or relevance in the changing context. The CELTS architecture implements this principle by assigning an activation level to percepts that is proportional to the emotional valence of a perceived situation. This activation level changes over time and as soon as it falls below a set threshold, the percept is discarded [160]. The Novamente Cognitive Engine has a similar mechanism, where atoms stay in memory as long as they build links to other memory elements and increase their utility [161]. It is still unclear if under these conditions the size of working memory can grow substantially without any additional restrictions.\nA simpler solution is to set a hard limit on the number of items in memory, for example 3-6 objects in ARCADIA [110], 4 chunks in CHREST [162] or up to 20 items in MDB, and delete the oldest or the most irrelevant item to avoid overflow.\nItems can also be discarded if they have not been used for some time. The exact amount of time varies from 4-9 seconds (EPIC [150]) to 5 sec (MIDAS [163], CERA-CRANIUM [131]) to tens of seconds (LIDA [164]).\nIn the Recommendation Architecture the limit of 3-4 items in working memory emerges naturally from\nthe structure of the memory system [165].\nLong-term memory Long-term memory (LTM) preserves a large amount of information for a very long time. Typically, it is divided into procedural memory of implicit knowledge (e.g. motor skills and routine behaviors) and\ndeclarative memory, which contains (explicit) knowledge. The latter is further subdivided into semantic (factual) and episodic (autobiographical) memory.\nThe dichotomies between the explicit/implicit and the declarative/procedural long-term memories are usually merged. One of the few exceptions is CLARION, where procedural and declarative memories are separate and both subdivided into an implicit and explicit component. This distinction is preserved on the level of knowledge representation: implicit knowledge is captured by distributed subsymbolic structures like neural networks, while explicit knowledge has a transparent symbolic representation [166].\nLong-term memory is a storage for innate knowledge that enables operation of the system, therefore almost all architectures implement procedural and/or semantic memory. Procedural memory contains knowledge about how to get things done in the task domain. In symbolic production systems, procedural knowledge is represented by a set of if-then rules preprogrammed or learned for a particular domain (3T [167], 4CAPS [41], ACT-R [168], ARDIS [145], EPIC [169], SAL [170], Soar [171], APEX [172]). Other variations include sensory-motor schemas (ADAPT [173]), task schemas (ATLANTIS [146]) and behavioral scripts (FORR [174]). In emergent systems, procedural memory may contain sequences of stateaction pairs (BECCA [53]) or ANNs representing perceptual-motor associations (MDB [175]).\nSemantic memory stores facts about the objects and relationships between them. In the architectures that support symbolic reasoning, semantic knowledge is typically implemented as a network-line ontology, where nodes correspond to concepts and links represent relationships between them (Casimir [176], Cerebus [177], Disciple [178], MIDAS [179], Soar [171], CHREST [180]). In emergent architectures factual knowledge is represented as patterns of activity within the network (BBD [181], SHRUTI [182], HTM [183], ART [184]).\nEpisodic memory stores specific instances of past experience. These can later be reused if a similar situation arises (MAX [185], OMAR [186], iCub [187], Ymir [85]). However, these experiences can also be exploited for learning new semantic or procedural knowledge. For example, CLARION saves actionoriented experiences as “input, output, result” and uses them to bias future behavior [188]. Similarly, BECCA stores sequences of state-action pairs to make predictions and guide the selection of system actions [53], and MLECOG gradually builds 3D scene representation from perceived situations [189]. MAMID [190] saves the past experience together with the specific affective connotations (positive or negative), which affect the likelihood of selecting similar actions in the future. Other examples include R-CAST [191], Soar [192], Novamente [193] and Theo [194].\nDespite the evidence for the different memory systems, some architectures do not have separate representations for different kinds of knowledge or short- vs long-term memory, and instead use a unified structure to store all information in the system. CORTEX and RoboCog use an integrated, dynamic multigraph object which can represent both sensory data and high-level symbols describing the state of the robot and the environment ([51], [195]). DiPRA uses Fuzzy Cognitive Maps to represent goals and plans [196]. NARS represents all empirical knowledge, regardless of whether its declarative, episodic or procedural, as formal sentences in Narcese [197].\n7 Learning Learning is the capability of a system to improve its performance over time. Ultimately, any kind of learning is based on experience. For example, a system may be able to infer facts and behaviors from observed events or from results of its own actions. The type of learning and its realization depend on many factors, such as design paradigm (e.g. biological, psychological), application scenario, data structures and the algorithms used for implementing the architecture, etc. However, we will not attempt to analyze all these aspects given the diversity and number of the cognitive architectures surveyed. Besides, not all of these pieces of information can be easily found in the publications.\nThus, a more general summary is preferable, where four types of learning are defined: perceptual,\ndeclarative, procedural and attentional. Perceptual learning involves improving skills for processing\nperceptual data, such as recognition, discrimination, categorization, building spatial maps or finding any sort of patterns that could be further exploited. Declarative learning expands semantic knowledge about the world, specifically, the explicit knowledge of facts and relations between them. This knowledge may be learned from experience in the process of solving a problem (e.g. in production systems chunks are automatically added to memory as each goal is completed). Procedural learning enables modifying existing behaviors or generating new ones by committing to memory successful actions to be reused later (specialized behaviors) or by extending known procedures to new circumstances (generalized behaviors). Finally, attentional learning allows improvement in the way the system modifies the action selection rules through accumulating statistics of previous successes and failures of completed actions, reward mechanisms (various kinds of reinforcement learning) or via emotions and drives.\nMore involved cases combine several types of learning at once. A typical example is sensorimotor learning, where a system finds associations between percepts and behaviors. Knowledge is usually obtained in two stages: initially random actions are performed to accrue experience in the form of <action, percept> associations, followed by fitting a model to the accumulated data. This type of learning has been used for navigation and for learning spatial affordances from sensory data (e.g. FORR [198], iCub [199]).\nThere are also 22 architectures that do not implement any learning. In some areas of research learning is not necessary, for example, in human performance modeling, where accurate replication of human performance data is required instead (e.g. APEX, EPIC, IMPRINT, MAMID, MIDAS, etc.). Some of the newer architectures are still in the early development stage and may implement learning in the future (e.g. ARCADIA, SiMA).\nFigure 10 shows a visualization of the learning types for all cognitive architectures. Here the 4 types of\nlearning are further subdivided into categories and linked to the architectures implementing them."
    }, {
      "heading" : "Perceptual learning",
      "text" : "Although many systems use pre-learned components for processing perceptual data, such as object and face detectors or classifiers, we do not consider these here. Perceptual learning applies to architectures that actively change the way sensory information is handled or how patterns are learned online. This kind of learning is frequently performed to obtain implicit knowledge about the environment, such as spatial maps (4D-RCS [200], AIS [201], MicroPsi [202]), clustering visual features (HTM [203], BECCA [204], Leabra [42]) or finding associations between percepts. The latter can be used within the same sensory modality as in the case of the agent controlled by Novamente engine, which selects a picture of the object it wants to get from a teacher [205]. Learning can also occur between different modalities, for instance, the robot based on SASE architecture learns association between spoken command and action [69] and Darwin VII (BBD) learns to associate taste value of the blocks with their visual properties [206]."
    }, {
      "heading" : "Declarative learning",
      "text" : "Declarative knowledge is a collection of facts about the world and various relationships defined between them. In production systems such as ACT-R, Soar and others, which implement chunking mechanisms (SAL [207], ADAPT [208], CHREST [209], CLARION [40]), learning new declarative knowledge is automatic. That is to say, each time a goal is completed, a new chunk is added to declarative memory. Automatic acquisition of knowledge has also been demonstrated in systems with distributed representations. For example, in DSO knowledge can be directly input by human experts or learned as contextual information extracted from labeled training data [210]. New symbolic knowledge can also be acquired by applying logical inference rules to already known facts (GMU-BICA [211], Disciple [212], Casimir [176], NARS [213]). In the biologically inspired systems learning new concepts usually takes the form of learning the correspondence between the visual features of the object and its name (iCub [214], Leabra [42], MACsi [106], Novamente [205], CoSy [79], DIARC [215])."
    }, {
      "heading" : "Procedural learning",
      "text" : "Procedural learning allows an intelligent system to acquire new behaviors or modify existing ones. The simplest way of doing so is by accumulating examples of successfully solved problems to be reused later. For instance, in a navigation task, a traversed path could be saved and used again to go between the same locations later (AIS [201]). The same applies to reasoning tasks, however, in this case previously generated plans or solutions would be added to memory (Companions [216], RoboCog [217]). Obviously, this type of learning is very limited and further processing of accumulated experience is needed for better efficiency\nand flexibility. Many examples of procedural learning using episodic learning have been described in the\nprevious chapter."
    }, {
      "heading" : "Attentional learning",
      "text" : "Attentional learning affects the action selection process by adjusting the relative priority of the actions or concepts according to their experienced usefulness and relevance. The utility of a rule or action could be inferred from statistics of the past applications (Theo [194], NARS [218]), although reward-driven (reinforcement) learning is far more common and is not restricted to biologically inspired systems (e.g. 4DRCS [219], RALPH [220], BBD [221], CARACaS [222], FORR [223], ICARUS [224], etc.). Emotions\nand drives are additional factors regulating attention of the intelligent agent, although the associations between emotions and actions are usually predefined.\n8 Applications of Cognitive Architectures Most of the cognitive architectures reviewed in this paper are research tools and few are developed outside of academia. However, it is still appropriate to talk about their practical applications, since useful behavior in various situations is declared as a goal of many cognitive architectures.\nAfter a thorough search through the publications we identified more than 700 projects implemented using 86 cognitive architectures, which are shown in Figure 11. All applications were split into major groups, namely human performance modeling (HPM), games and puzzles, robotics, psychological experiments, natural language processing (NLP) and miscellaneous, which included projects not related to any major group but too rare to be separated into a group of their own. Such grouping of projects emphasizes the application aspect of each project, although the goal of the researchers may have been different.\nFor example, navigation using a mobile robot, regardless of whether it was achieved by reasoning or was done as a demonstration of a learning algorithm, is placed in the robotics group. The only exception from the rule are psychological experiments, which also include psychophysiological, fMRI and EEG experiments. The projects in this group are the particular psychological experiments (e.g. n-back task, conditioning or attentional blindness) performed by the cognitive architecture and compared against human data. The category of games and puzzles includes applications to playing board games, video games, solving puzzles and logical reasoning in various domains. HPM is concerned with building models of aircraft crews [225], operators of the nuclear power plant [226], people performing other complex tasks, e.g. telephone operators and air traffic operators. Most NLP applications are concerned with understanding and answering questions given as spoken or typed commands and are related to social robotics, however there are also projects in this group dedicated to sense disambiguation and sentence comprehension in general. Some applications belong to more than one group. For example, Soar has been used to play board games with a robotic arm [227], which is relevant to robotics and games and puzzles. Similarly, ACT-R model of Tower of Hanoi compared to human fMRI data [228] belongs to games and puzzles and psychological experiments. To avoid overcomplicating the diagram, in these cases we placed the project in the dominant group, in the case of Soar that would be game playing, since the contribution to robotics was not as significant, and psychological experiments group for the fMRI experiments using ACT-R.\nHuman Performance Modeling (HPM) Human performance modeling is an area of research concerned with building quantitative models of human performance in a specific task environment. The need for such models comes from engineering domains where the space of design possibilities is too large, so that empirical assessment is infeasible or too costly.\nThis type of modeling has been used extensively for military applications, for example, workload analysis of Apache helicopter crew [229], modeling the impact of communication tasks on the battlefield awareness [230], decision making in the AAW domain [231], etc. Common civil applications include models of air traffic control task (e.g. COGNET [232]), aircraft taxi errors [233], 911 dispatch operator HPM is dominated by a handful of specialized architectures, including OMAR, APEX, COGNET, MIDAS and IMPRINT. Soar was used to implement a pilot model for large-scale distributed military simulations (TacAir-Soar) [234], [235].\nHuman-Robot Interaction (HRI) HRI is a multidisciplinary field studying various aspects of communication between people and robots. Most of these interactions occur in the context of social, assistive or developmental robotics. Depending on\nthe level of autonomy demonstrated by the robot, interactions extend from direct control (teleoperation) to full autonomy of the robot enabling peer-to-peer collaboration. Although none of the systems presented in this survey are yet capable of full autonomy, they allow for some level of supervisory control ranging from single vowels signifying direction of movement for a robot (SASE [236]) to natural language instruction\nincluded in supplementary materials and can also be viewed in the interactive version of this diagram.\n(e.g. Soar [237], HOMER [88], iCub [238]). It is usually assumed that a command is of particular form and uses a limited vocabulary.\nSome architectures also target non-verbal aspects of HRI, for example natural turn-taking in a dialogue (Ymir [239], Kismet [240]), changing facial expression (Kismet [241]) or turning towards the caregiver (MACsi [242]).\nNatural Language Processing (NLP) The applications in this group are concerned with understanding written or spoken language. Although it is common for cognitive architectures to use off-the-shelf software for speech recognition and text parsing, some architectures contributed to the NLP research. Specific examples are anaphora resolution (Polyscheme [243], NARS [244], DIARC [245]), speech recognition benchmarks (Sigma [246], [247], SASE [248]) and learning English passive voice (NARS [244]).\nCategorization and Clustering Categorization, classification, pattern recognition and clustering are common ways of extracting general information from large datasets. In the context of cognitive architectures, these methods are useful for processing noisy sensory data. Applications in this group are almost entirely implemented by emergent architectures, such as ART and HTM, which are used as sophisticated neural networks. The ART networks, in particular, have been applied to classification problems in a wide range of domains: movie recommendations (Netflix dataset [249]), medical diagnosis (Pima-Indian diabetes dataset [250]), fault diagnostics (pneumatic system analysis [251]), vowel recognition (Peterson and Barney dataset [252]), odor identification [253], etc. The HTM architecture is geared towards the analysis of time series data, such as predicting IT failures (grokstream.com), monitoring stocks (numenta.com/htm-for-stocks), predicting taxi passenger demand [254] and recognition of cell phone usage type (email, call, etc.) based on the pressed key pattern [255].\nA few other examples from non-emergent architectures include gesture recognition from tracking suits (Ymir [256]), diagnosis of the failures in a telecommunications network (PRS [257]) and document categorization based on the information about authors and citations (OpenCogPrime [258]).\nComputer Vision Emergent cognitive architectures are also widely applied to solving typical computer vision problems. However, these are mainly standalone examples, such as hand-written character recognition (HTM [259], [260]), image classification benchmarks (HTM [261], [262]), view-invariant letter recognition (ART [263]), texture classification benchmarks (ART [264]), invariant object recognition (Leabra [265]), etc.\nThe computer vision applications that are part of more involved tasks, for example for navigation in\nrobotics, are discussed in the relevant sections.\nGames and Puzzles The category of games and puzzles includes applications like playing board games, video games and problem solving in limited domains. The simple board games with conceptual overlap like tic-tac-toe, the eight puzzle and the five puzzle are frequently used to demonstrate knowledge transfer (e.g. Soar [227], FORR [266]).\nVideo games are also used as virtual domains for cognitive architectures. By far the most popular is Unreal Tournament 2004 (UT2004), for which there is an open-source software toolkit Pogamut [267] making it easier to create intelligent virtual characters. Besides, there are several game playing competitions, which focus not only on the scores and efficiency, but also take into account the believability\nof the agent (2K BotPrize Contest7). Although Pogamut in itself implements many cognitive functions, it\nis also a recommended middleware for the BotPrize contest and is used with modifications by other groups to implement the UT2004 bots ([268], [269], [270], [271], [271], [272]). Other video games are Freeciv (REM [273]), Atari Frogger II (Soar [274]), Infinite Mario (Soar [275]), browser games (STAR [276]) and custom made games.\nPsychological Experiments The psychological experiments group includes replications of a number of psychophysiological, fMRI and EEG experiments using cognitive architectures. Here the goal is either to demonstrate that a cognitive architecture can numerically model human data or give reasonable explanations for existing psychological phenomena. If data produced by a simulation matches the human data in some or most aspects, this is taken as an indication that a given cognitive architecture can imitate human reasoning. The cognitive model can then be either used for making predictions about behaviors in different situations or analyzed further and used as an explanation for the psychological mechanisms behind known phenomena.\nMost of the experiments are conducted in simulated environments, although there are some examples\nimplemented on a physical robot (e.g. a model of perceptual categorization on DarwinVII robot [181]).\nRobotics There are numerous applications of cognitive architectures in robotics. Navigation and obstacle avoidance are basic behaviors, which can be useful on their own or used as part of more complex behaviors, for example in assistive robotics.\nThe fetch and carry tasks were very popular in the early days of robotics research as an effective demonstration of robot abilities. Some well-known examples include a trash collecting mobile robot (3T [277]) and a soda can collecting robot (Subsumption [278]). Through a combination of simple vision techniques such as edge detection and template matching, and sensors for navigation, these robots were able to find the objects of interest in unknown environments.\nMore recent cognitive architectures tend to solve search and object manipulation tasks separately. Typically, experiments involving visual search are done in very controlled environments and preference is given to objects with bright colors or recognizable shapes to minimize visual processing. Sometimes markers are used, such as printed barcodes attached to the object, to simplify recognition (Soar [279]). It is important to note that visual search in these cases is usually a part of a more involved task, such as learning by instruction. When visual search and localization is the end goal, the environments are more realistic (e.g. the robot controlled by CoSy finds a book on a cluttered shelf using a combination of sensors and SIFT features [280]).\nObject manipulation involves arm control to reach and grasp an object. While reaching is a relatively easy problem and many architectures implement some form of arm control, gripping is more challenging even in a simulated environment. Complexity of grasping depends on many factors including the type of gripper and properties of the object. One workaround is to experiment with grasping on soft objects, such as plush toys (ISAC [281]). More recent work involves objects with different grasping types (objects with handles located on top or on a side) demonstrated on a robot controlled by DIARC [282], [282]. Another example is iCub adapting its grasp to cans of different sizes, boxes and a ruler [283].\nOther applications include robotic salesman, tutor, medical robots, etc. Industrial applications are represented by a single architecture - 4D-RCS, which has been used for teleoperated robotic crane operation [284], bridge construction [285], autonomous cleaning and deburring workstation [286], and the automated stamp distribution center for the US Postal Service [287].\n7 http://botprize.org\nVirtual Agents Simulations and virtual reality are frequently used as an alternative to physical embodiment. For instance, in the military domain, simulations model behavior of soldiers in dangerous situations without risking their lives. Some examples include modeling agents in a suicide bomber scenario (CoJACK [73]), peacekeeping mission training (MAMID [190]), command and control in complex and urban terrain (RCAST [288]) and tank battle simulation (CoJACK [289]).\nSimulations are also common for modeling behaviors of intelligent agents in civil applications. One of the advantages of virtual environments is that they can provide useful information about the state of the agent at any point in time. This is useful for studying the effect of emotions on actions, for example, in the social interaction context (ARS/SiMA [290]), or in learning scenarios, such as playing fetch with a virtual dog (Novamente [291]).\n9 Discussion The main contribution of this survey is in gathering and summarizing information on a large number of cognitive architectures from various backgrounds (computer science, psychology, philosophy and neuroscience). In particular, we described common approaches to implementing important aspects of human cognition, such as perception, attention and memory. We also try to answer the question of what are the practical capabilities of the existing architectures by categorizing their demonstrated applications. Finally, we have identified some general trends in the development of the field.\nHistorically psychology and computer science were an inspiration for the first cognitive and agent architectures. Despite the differences in theory and terminology they tackled the same issues of action selection, efficient data processing and storage. For example, action selection in robotics is done using methods ranging from priority queue to reinforcement learning [292], similarly to the traditional cognitive architectures. More biologically realistic models were developed in parallel but became widely recognized as a viable alternative only relatively recently. Some of these models represent the emergent paradigm, however their support for inference and general reasoning is inadequate for solving common AI problems. Thus, hybrid models combining both symbolic and subsymbolic approaches are currently the most promising and will likely continue to be popular in the future. Another rising paradigm is represented by machine learning methods, which have found enormous practical success. However, they are mainly concerned with perception although lately there have been attempts at implementing more general inference and memory mechanisms with deep learning techniques. With respect to cognitive functions, research in cognitive architectures is focused more on higher level abilities such as learning, planning and general reasoning. While the importance of vision and perception in general is universally acknowledged, their treatment by architectures remains rather superficial. For example, simulations are frequently used to simplify or replace vision. Realistic unstructured environments are also rather rare. On the other hand, there is a trend toward implementing multimodal perception, which may lead to interesting applications in the future.\nOur data on practical applications of cognitive architectures demonstrates that with few exceptions architectures are narrowly focused on a particular area. The visualization in Figure 11 highlights the specialization areas of different types of architectures. For instance, emergent architectures are mainly applied to clustering and vision tasks, with several applications in the robotics domain. As expected, symbolic architectures are widely used for planning and reasoning, human performance modeling and psychological experiments. On the other hand, hybrid architectures are more uniformly represented across all application categories.\nOverall, there is a significant gap between general research in robotics and computer vision and research in these areas within the cognitive architectures domain. It is apparent that biologically inspired models cannot demonstrate the same range and efficiency in practical applications compared to the less theoretically restricted systems based on heuristics and engineering. Biological systems are mostly limited to controlled domains and many of their demonstrated results are proof-of-concept. Some exceptions exist,\nfor example Grok8 - a commercial application for IT analytics based on the biologically inspired HTM\narchitecture. On the other hand, as we have mentioned already, there is a clear trend towards developing hybrid cognitive architectures that take advantage of both biologically inspired and engineering techniques, so the performance gap may be reduced in the future.\nWith respect to the field as a whole, there is far less collaboration than would be expected in an interdisciplinary area such as cognitive architectures research. Some of this is due to the fact that many architectures are developed as closed-source projects in small groups, although there are numerous advantages of open-source development. For instance, cognitive architectures such as ART, ACT-R, Soar, HTM and Pogamut, attract a large community of researchers and are frequently referenced in the publications outside the main developing group. Another factor impeding communication is due to the sometimes impenetrable language used by the biologically- or neuro-inspired architectures. This is reminiscent of the terminological differences that existed between the architectures from cognitive and engineering backgrounds, although, at present, terms such as saliency, attention, working memory, etc. are far more commonplace.\nAcknowledgements\nThis research was supported through grants to the senior author, for which all the authors are grateful: Air Force Office of Scientific Research (FA9550-14-1-0393), the Canada Research Chairs Program (950-219525), and the Natural Sciences and Engineering Research Council of Canada (RGPIN-2016-05352).\nReferences [1] A. V. Samsonovich, “Toward a unified catalog of implemented cognitive architectures,” in In Proceeding of\nthe 2010 conference on biologically inspired cognitive architectures, 2010, pp. 195–244.\n[2] S. Russell and P. Norvig, Artificial Intelligence: A Modern Approach. Prentice Hall, 1995. [3] R. Sun, “Desiderata for cognitive architectures,” Philos. Psychol., vol. 17, no. 3, pp. 341–373, 2004. [4] A. Newell, “Physical symbol systems,” Cogn. Sci., vol. 4, no. 2, 1980. [5] A. Newell, “Précis of Unified theories of cognition,” Behav. Brain Sci., vol. 15, pp. 425–492, 1992. [6] J. R. Anderson and C. Lebiere, “The Newell Test for a theory of cognition.,” Behav. Brain Sci., vol. 26, no.\n5, pp. 587–601, 2003.\n[7] S. Adams, I. Arel, J. Bach, R. Coop, R. Furlan, B. Goertzel, J. S. Hall, A. Samsonovich, M. Scheutz, M.\nSchlesinger, S. C. Shapiro, and J. Sowa, “Mapping the Landscape of Human-Level Artificial General Intelligence,” AI Mag., vol. 33, no. 1, pp. 25–42, 2012.\n[8] T. Metzler and K. Shea, “Taxonomy of Cognitive Functions,” in Proceedings of the 18th International\nConference on Engineering Design, 2011, pp. 330–341.\n[9] P. Wang, “Natural language processing by reasoning and learning,” in Proceedings of the International\nConference on Artificial General Intelligence, 2013, pp. 160–169.\n[10] U. Faghihi and S. Franklin, “The LIDA Model as a Foundational Architecture for AGI,” Theor. Found.\nArtif. Gen. Intell., vol. 4, pp. 103–121, 2012.\n[11] S. Schaat, A. Wendt, S. Kollmann, F. Gelbard, and M. Jakubec, “Interdisciplinary Development and\nEvaluation of Cognitive Architectures Exemplified with the SiMA Approach,” in EuroAsianPacific Joint Conference on Cognitive Science, 2015.\n[12] B. Goertzel and G. Yu, “A cognitive API and its application to AGI intelligence assessment,” in\nInternational Conference on Artificial General Intelligence, 2014, pp. 242–245.\n[13] W. Bridewell and P. F. Bello, “Incremental Object Perception in an Attention-Driven Cognitive\nArchitecture,” Proc. 37th Annu. Meet. Cogn. Sci. Soc., pp. 279–284, 2015.\n[14] J. K. Tsotsos, “Attention and Cognition: Principles to Guide Modeling,” in Computational and Cognitive\nNeuroscience of Vision, Q. Zhao, Ed. Elsevier.\n8 http://numenta.com/grok/\n[15] U. Faghihi, P. Poirier, and O. Larue, “Emotional Cognitive Architectures,” in International Conference on\nAffective Computing and Intelligent Interaction, 2011.\n[16] T. C. Henderson and A. Joshi, “The Cognitive Symmetry Engine,” Tech. Rep. UUCS-13-004, 2013. [17] S. L. Epstein, “Metaknowledge for Autonomous Systems,” in Proceedings of AAAI Spring Symposium on\nKnowledge Representation and Ontology for Autonomous Systems, 2004.\n[18] J. G. Carbonell, J. Blythe, O. Etzioni, Y. Gil, R. Joseph, D. Kahn, C. Knoblock, S. Minton, P. Alicia, S.\nReilly, M. Veloso, and X. Wang, “PRODIGY4.0: TheManual and Tutorial,” Tech. Rep. C., 1992.\n[19] D. Martin, M. Rincon, M. C. Garcia-Alegre, and D. Guinea, “ARDIS: Knowledge-based dynamic\narchitecture for real-time surface visual inspection,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 5601, pp. 395–404, 2009.\n[20] J. B. Maxwell, “Generative Music, Cognitive Modelling, and Computer-Assisted Composition in MusiCog\nand ManuScore,” PhD Thesis, 2014.\n[21] W. Duch, R. J. Oentaryo, and M. Pasquier, “Cognitive Architectures: Where do we go from here?,” in\nFrontiers in Artificial Intelligence and Applications, vol. 171, P. Wang, B. Goertzel, and S. Franklin, Eds. IOS Press, 2008, pp. 122–136.\n[22] P. Langley, J. E. Laird, and S. Rogers, “Cognitive architectures: Research issues and challenges,” Cogn.\nSyst. Res., vol. 10, no. 2, pp. 141–160, 2009.\n[23] P. Thagard, “Cognitive Architectures,” in The Cambridge handbook of cognitive science, W. Frankish and\nW. Ramsay, Eds. Cambridge: Cambridge University Press, 2012, pp. 50–70.\n[24] S. Profanter, “Cognitive architectures,” in HaupteSeminar Human Robot Interaction, 2012. [25] A. J. Butt, N. A. Butt, A. Mazhar, Z. Khattak, and J. A. Sheikh, “The Soar of cognitive architectures,” in\nProceedings of the 2013 International Conference on Current Trends in Information Technology, CTIT 2013, 2013.\n[26] G. Pezzulo and G. Calvi, “Dynamic Computation and Context Effects in the Hybrid Architecture AKIRA,”\nInt. Interdiscip. Conf. Model. Using Context., 2005.\n[27] K. Thórisson and H. Helgasson, “Cognitive Architectures and Autonomy: A Comparative Review,” J. Artif.\nGen. Intell., vol. 3, no. 2, pp. 1–30, 2012.\n[28] D. Foxvog, “Cyc,” 2010, pp. 259–278. [29] B. Goertzel, C. Pennachin, and N. Geisweiller, “Brief Survey of Cognitive Architectures,” in Engineering\nGeneral Intelligence, Part 1, Atlantis Press, 2014, pp. 101–142.\n[30] J. E. Laird, “The Soar Cognitive Architecture,” in AISB Quarterly, vol. 171, no. 134, 2012, pp. 224–235. [31] P. Langley, J. E. Laird, and S. Rogers, “Cognitive architectures: Research issues and challenges,” Cogn.\nSyst. Res., vol. 10, no. 2, pp. 141–160, 2009.\n[32] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I.\nAntonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, and K. Kavukcuoglu, “Mastering the game of Go with deep neural networks and tree search,” Nature, vol. 529, no. 7585, pp. 484–489, 2016.\n[33] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A.\nK. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533, 2015.\n[34] J. Ba, V. Mnih, and K. Kavukcuoglu, “Multiple Object Recognition with Visual Attention,” Iclr, pp. 1–10,\n2014.\n[35] N. Kalchbrenner, I. Danihelka, and A. Graves, “Grid Long Short-Term Memory,” arXiv Prepr.\narXiv1507.01526, p. 14, 2015.\n[36] T. Mikolov, A. Joulin, and M. Baroni, “A Roadmap towards Machine Intelligence,” arXiv : 1511.08130v1,\n2015.\n[37] D. Vernon, G. Metta, and G. Sandini, “A Survye of Artificial Cognitive Systems: Implictions for the\nAutonomous Development of Mental Capbilities in Computational Agents,” IEEE Trans. Evol. Comput., pp. 1–30, 2007.\n[38] A. Asselman, S. Aammou, and A.-E. Nasseh, “Comparative Study of Cognitive Architectures,” Int. Res. J.\nComput. Sci., vol. 2, no. 9, pp. 8–13, 2015.\n[39] B. Goertzel, J. Pitt, M. Ikle, C. Pennachin, and L. Rui, “Glocal memory: A critical design principle for\nartificial brains and minds,” Neurocomputing, vol. 74, no. 1–3, pp. 84–94, 2010.\n[40] R. Sun, T. Peterson, and E. Merrill, “A Hybrid Architecture for Situated Learning of Reactive Sequential\nDecision Making,” Appl. Intell., vol. 11, pp. 109–127, 1999.\n[41] S. Varma, “A computational model of Tower of Hanoi problem solving.,” PhD Thesis, 2006. [42] R. C. O’Reilly, D. Wyatte, S. Herd, B. Mingus, and D. J. Jilk, “Recurrent processing during object\nrecognition,” Front. Psychol., vol. 4, 2013.\n[43] B. Kokinov, “Micro-Level Hybridization in the Cognitive Architecture DUAL,” Connect. Integr. From\nunified to hybrid approaches, 1997.\n[44] R. Sun, “Hybrid Connectionist-Symbolic Modules,” AI Mag., vol. 17, no. 2, pp. 99–103, 1996. [45] M. Thalgott, “Brain-Like Artificial Intelligence: Analysis of a Promising Field,” 2013. [46] W. D. Gray, “Cognitive architectures: choreographing the dance of mental operations with the task\nenvironment,” Hum. Factors, vol. 50, no. 3, pp. 497–505, 2008.\n[47] R. Sun, “Artificial intelligence: Connectionist and symbolic approaches,” International Encyclopedia of the\nSocial and Behavioral Sciences. Pergamon/Elsevier, pp. 783–789, 2001.\n[48] J. L. Krichmar and J. A. Snook, “A neural approach to adaptive behavior and multi-sensor action selection\nin a mobile device,” in Proceedings of the 2002 IEEE International Conference on Robotics and Automation, 2002.\n[49] M. Krzywinski, J. Schein, I. Birol, J. Connors, R. Gascoyne, S. J. Jones, and M. A. Marra, “Circos: an\nInformation Aesthetic for Comparative Genomics,” Genome Res., vol. 19, no. 9, pp. 1639–1645, 2009.\n[50] L. Matthies, “Stereo vision for planetary rovers: Stochastic modeling to near real-time implementation,” Int.\nJ. Comput. Vis., vol. 8, no. 1, pp. 71–91, 1992.\n[51] A. Romero-Garcés, L. V. Calderita, J. Martinez-Gomez, J. P. Bandera, R. Marfil, L. J. Manso, P. Bustos,\nand A. Bandera, “The cognitive architecture of a robotic salesman,” Conf. Spanish Assoc. Artif. Intell., vol. 15, no. 6, 2015.\n[52] S. Ivaldi, N. Lyubova, D. Gerardeaux-Viret, A. Droniou, S. M. Anzalone, M. Chetouani, D. Filliat, and O.\nSigaud, “Perception and human interaction for developmental learning of objects and affordances,” in IEEERAS International Conference on Humanoid Robots, 2012, pp. 248–254.\n[53] B. Rohrer, M. Bernard, D. J. Morrow, F. Rothganger, and P. Xavier, “Model-free learning and control in a\nmobile robot,” in Proceedings of the 5th International Conference on Natural Computation, ICNC 2009, 2009, pp. 566–572.\n[54] F. Bellas, A. Faiña, A. Prieto, and R. J. Duro, “Adaptive Learning Application of the MDB Evolutionary\nCognitive Architecture in Physical Agents,” Int. Conf. Simul. Adapt. Behav., 2006.\n[55] D. P. Benjamin, C. Funk, and D. Lyons, “A cognitive approach to vision for a mobile robot,” SPIE Defense,\nSecur. Sens., 2013.\n[56] G. Maffei, D. Santos-Pata, E. Marcos, M. Sánchez-Fibla, and P. F. M. J. Verschure, “An embodied\nbiologically constrained model of foraging: From classical and operant conditioning to adaptive real-world behavior in DAC-X,” Neural Networks, vol. 72, pp. 88–108, 2015.\n[57] J. Albus and A. Barbera, “Intelligent control and tactical behavior development: A long term NIST\npartnership with the army,” in 1st Joint Emergency Preparedness and Response/Robotic and Remote Systems Topical Meeting, 2006.\n[58] D. P. Miller and M. G. Slack, “Global symbolic maps from local navigation,” in Proceedings of the ninth\nNational conference on Artificial intelligence AAAI, 1991, pp. 750–755.\n[59] T. Huntsberger, H. Aghazarian, A. Howard, and D. C. Trotz, “Stereo vision-based navigation for\nautonomous surface vessels,” J. F. Robot., vol. 28, no. 1, pp. 3–18, 2011.\n[60] M. Scheutz, J. McRaven, and G. Cserey, “Fast, reliable, adaptive, bimodal people tracking for indoor\nenvironments,” in Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2004, pp. 1347–1352.\n[61] P. Schermerhorn, J. Kramer, T. Brick, D. Anderson, A. Dingler, and M. Scheutz, “DIARC: A Testbed for\nNatural Human-Robot Interactions,” in Proceedings of AAAI 2006 Robot Workshop, 2006, pp. 1972–1973.\n[62] C. Ciliberto, F. Smeraldi, L. Natale, and G. Metta, “Online multiple instance learning applied to hand\ndetection in a humanoid robot,” in IProceedings EEE International Conference on Intelligent Robots and Systems, 2011, pp. 1526–1532.\n[63] J. Martinez-Gomez, R. Marfil, L. V. Calderita, J. P. Bandera, L. J. Manso, A. Bandera, A. Romero-Garces,\nand P. Bustos, “Toward Social Cognition in Robotics: Extracting and Internalizing Meaning from Perception,” in Workshop of Physical Agents, 2014.\n[64] C. Breazeal and B. Scassellati, “Challenges in Building Robots That Imitate People,” in Imitation in Animals\nand Artifacts, no. 1998, K. Dautenhahn and C. Nehaniv, Eds. Cambridge, MA: MIT Press, 2002, pp. 363– 390.\n[65] R. C. O. O’Reilly, D. Wyatte, S. Herd, B. Mingus, and D. J. Jilk, “Recurrent processing during object\nrecognition,” Front. Psychol., vol. 4, 2013.\n[66] D. Wyatte, S. Herd, B. Mingus, and R. O’Reilly, “The role of competitive inhibition and top-down feedback\nin binding during object recognition,” Front. Psychol., vol. 3, 2012.\n[67] A. K. Seth, J. L. McKinstry, G. M. Edelman, and J. L. Krichmar, “Visual binding through reentrant\nconnectivity and dynamic synchronization in a brain-based device,” Cereb. Cortex, vol. 14, no. 11, pp. 1185–1199, 2004.\n[68] N. Zhang, J. Weng, and Z. Zhang, “A developing sensory mapping for robots,” in Proceedings 2nd\nInternational Conference on Development and Learning. ICDL 2002, 2002, pp. 13–20.\n[69] J. Weng and Y. Zhang, “Developmental Robots - A New Paradigm,” in Proceedings of the Second\nInternational Workshop on Epigenetic Robotics Modeling Cognitive Development in Robotic Systems, 2002, vol. 94, pp. 163–174.\n[70] G. A. Carpenter and S. Grossberg, “Adaptive Resonance Theory,” Encyclopedia of Machine Learning and\nData Mining. Springer-Verlag, 2014.\n[71] J. Hawkins and S. Ahmad, “Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in\nNeocortex,” Front. Neural Circuits, vol. 10, 2016.\n[72] B. Goertzel, T. Sanders, and J. O’Neill, “Integrating deep learning based perception with probabilistic logic\nvia frequent pattern mining,” in International Conference on Artificial General Intelligence, 2013.\n[73] R. Evertsz, M. Pedrotti, P. Busetta, H. Acar, and F. E. Ritter, “Populating VBS2 with Realistic Virtual\nActors,” in Proceedings of the 18th Conference on Behavior Representation in Modeling and Simulation, 2009, pp. 1–8.\n[74] B. Goertzel, C. Pennachin, N. Geissweiller, M. Looks, A. Senna, W. Silva, A. Heljakka, and C. Lopes, “An\nIntegrative Methodology for Teaching Embodied Non-Linguistic Agents, Applied to Virtual Animals in Second Life,” Front. Artif. Intell. Appl., vol. 171, pp. 161–175, 2008.\n[75] M. Bida, M. Cerny, J. Gemrot, and C. Brom, “Evolution of GameBots project,” in International Conference\non Entertainment Computing, 2012, pp. 397–400.\n[76] D. E. Kieras, G. H. Wakefield, E. Thompson, N. Iyer, and B. D. Simpson, “A cognitive architectural account\nof two-channel speech processing,” in Proceedings of the Human Factors and Ergonomics Society, 2014, pp. 812–816.\n[77] B. Chikhaoui, H. Pigot, M. Beaudoin, G. Pratte, P. Bellefeuille, and F. Laudares, “Learning a Song: an\nACT-R Model,” Proc. Int. Conf. Comput. Intell., pp. 405–410, 2009.\n[78] P. Lison and G.-J. Kruijff, “Salience-driven Contextual Priming of Speech Recognition for Human-Robot\nInteraction,” in Language, 2008, pp. 636–640.\n[79] N. Hawes, J. L. Wyatt, M. Sridharan, M. Kopicki, S. Hongeng, I. Calvert, A. Sloman, G.-J. Kruijff, H.\nJacobsson, M. Brenner, D. Skocaj, A. Vrecko, N. Majer, and M. Zillich, “The Playmate System,” in Cognitive Systems, 2010.\n[80] S. L. Epstein, R. Passonneau, J. Gordon, and T. Ligorio, “The Role of Knowledge and Certainty in\nUnderstanding for Dialogue,” in AAAI Fall Symposium: Advances in Cognitive Systems, 2012.\n[81] J. G. Trafton, N. L. Cassimatis, M. D. Bugajska, D. P. Brock, F. E. Mintz, and A. C. Schultz, “Enabling\nEffective Human – Robot Interaction Using Perspective-Taking in Robots,” IEEE Trans. Syst. Man, Cybern. - Part A Syst. Humans, vol. 35, no. 4, pp. 460–470, 2005.\n[82] R. A. Peters, K. Kawamura, D. M. Wilkes, K. A. Hambuchen, T. E. Rogers, and W. A. Alford, “ISAC\nHumanoid: An Architecture for Learning and Emotion,” in Proceedings of the IEEE-RAS International Conference on Humanoid Robots, 2001, no. 1, p. 459.\n[83] V. Sarathy, J. R. Wilson, T. Arnold, and M. Scheutz, “Enabling Basic Normative HRI in a Cognitive\nRobotic Architecture,” arXiv Prepr. arXiv1602.03814, 2016.\n[84] C. Breazeal and L. Aryananda, “Recognition of affective communicative intent in robot-directed speech,”\nAuton. Robots, vol. 12, no. 1, pp. 83–104, 2002.\n[85] K. R. Thorisson, Mind model for multimodal communicative creatures and humanoids, vol. 13, no. 4–5.\n1999.\n[86] J. L. Pollock, “Planning in OSCAR,” Minds Mach., vol. 2, pp. 113–144, 1993. [87] D. R. Kuokka, “Integrating Planning, Execution, and Learning,” in Proceedings of the NASA Conference on\nSpace Telerobotics, 1989, pp. 377–386.\n[88] S. Vere and T. Bickmore, “A basic agent,” Comput. Intell., vol. 6, no. 1, pp. 41–60, 1990. [89] D. K. Mitchell, “Workload Analysis of the Crew of the Abrams V2 SEP: Phase I Baseline IMPRINT\nModel,” Tehcnical Rep. ARL-TR-5028, no. September, 2009.\n[90] E. Hudlicka, G. Zacharias, and J. Psotka, “Increasing realism of human agents by modeling individual\ndifferences: Methodology, architecture, and testbed,” Simulating Hum. Agents, Am. Assoc. Artif. Intell. Fall 2000 Symp. Ser., pp. 53–59, 2000.\n[91] S. Deutsch and N. Cramer, “Omar Human Performance Modeling in a Decision Support Experiment,” in\nProceedings of the Human Factors and Ergonomics Society 42nd Annual Meeting, 1998, pp. 1232–1236.\n[92] J. From, P. Perrin, D. O’Neill, and J. Yen, “Supporting the Commander’s information requirements:\nAutomated support for battle drill processes using R-CAST,” in Proceedings of the IEEE Military Communications Conference MILCOM, 2011.\n[93] A. Lavin and S. Ahmad, “Evaluating Real-time Anomaly Detection Algorithms -- the Numenta Anomaly\nBenchmark,” in Proceedings of the14th International Conference on Machine Learning and Applications (ICMLA), 2015.\n[94] G. A. Carpenter, S. Grossberg, and J. H. Reynolds, “ARTMAP: Supervised real-time learning and\nclassification of nonstationary data by a self-organizing neural network,” Neural Networks, vol. 4, no. 5, pp. 565–588, 1991.\n[95] M. M. Chun, J. D. Golomb, and N. B. Turk-Browne, “A Taxonomy of External and Internal Attention,”\nAnnu. Rev. Psychol., vol. 62, pp. 73–101, 2010.\n[96] S. L. Epstein and S. Petrovic, Learning a Mixture of Search Heuristics. Springer Berlin Heidelberg, 2012. [97] J. S. Albus and A. J. Barbera, “RCS: A cognitive architecture for intelligent multi-agent systems,” Annu.\nRev. Control, vol. 29, no. 1, pp. 87–99, 2005.\n[98] S. E. Deutsch, J. Macmillan, N. L. Camer, and S. Chopra, “Operability Model Architecture: Demonstration\nFinal Report,” Tech. Rep. AL/HR-TR-1996-0161, 1997.\n[99] J. Ruesch, M. Lopes, A. Bernardino, J. Hornstein, J. Santos-Victor, and R. Pfeifer, “Multimodal saliency-\nbased bottom-up attention a framework for the humanoid robot iCub,” in Proceedings of the IEEE International Conference on Robotics and Automation, 2008, pp. 962–967.\n[100] B. F. Gore, B. L. Hooey, C. D. Wickens, and S. Scott-Nash, “A computational implementation of a human\nattention guiding mechanism in MIDAS v5,” in International Conference on Digital Human Modeling, 2009.\n[101] J. M. Wolfe, “Guided Search 2 . 0 A revised model of visual search,” vol. 1, no. 2, pp. 202–238, 1994. [102] E. Nyamsuren and N. A. Taatgen, “Pre-attentive and attentive vision module,” Cogn. Syst. Res., pp. 211–\n216, 2013.\n[103] C. Breazeal and B. Scassellati, “A context-dependent attention system for a social robot,” in Proceedings of\nthe International Joint Conference on Artificial Intelligence, 1999, pp. 1146–1151.\n[104] L. Itti, C. Koch, and E. Niebur, “A Model of Saliency-Based Visual Attention for Rapid Scene Analysis,”\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 20, no. 11, pp. 1254–1259, 1998.\n[105] Z. Mathews, S. B. I Badia, and P. F. M. J. Verschure, “PASAR: An integrated model of prediction,\nanticipation, sensation, attention and response for artificial sensorimotor systems,” Inf. Sci. (Ny)., vol. 186, no. 1, pp. 1–19, 2012.\n[106] S. Ivaldi, S. M. Nguyen, N. Lyubova, A. Droniou, V. Padois, D. Filliat, P. Y. Oudeyer, and O. Sigaud,\n“Object learning through active exploration,” EEE Trans. Auton. Ment. Dev., vol. 6, no. 1, pp. 56–72, 2014.\n[107] J. S. Albus, Hui-Min Huang, E. R. Messina, K. Murphy, M. Juberts, A. Lacaze, S. B. Balakirsky, M. O.\nShneier, T. H. Hong, H. a. Scott, F. M. Proctor, W. P. Shackleford, J. L. Michaloski, A. J. Wavering, T. R. Kramer, N. G. Dagalakis, W. G. Rippey, K. a. Stouffer, and S. Legowik, “4D/RCS: A Reference Model Architecture For Unmanned Vehicle Systems Version 2.0,” Proc. SPIE 16th Annu. Int. Symp. AerospaceDefense Sens. Simul. Control., no. August 2002, 2002.\n[108] D. D. Salvucci, “A Model of Eye Movements and Visual Attention,” Proc. Third Int. Conf. Cogn. Model.,\npp. 252–259, 2000.\n[109] M. A. Freed, “Simulating human performance in complex, dynamic environments,” PhD Thesis, no. June,\n1998.\n[110] P. Bello, W. Bridewell, and C. Wasylyshyn, “Attentive and Pre - Attentive Processes in Multiple Object\nTracking: A Computational Investigation Modeling Object Construction and Tracking,” in Proceedings of the 38th Annual Meeting of the Cognitive Science Society, 2016.\n[111] R. Arrabales, A. Ledezma, and A. Sanchis, “A cognitive approach to multimodal attention,” J. Phys. Agents,\nvol. 3, no. 1, pp. 53–63, 2009.\n[112] M. Conforth and Y. Meng, “CHARISMA: A Context Hierarchy-based cognitive architecture for self-\nmotivated social agents,” Proc. Int. Jt. Conf. Neural Networks, pp. 1894–1901, 2011.\n[113] P. C. R. Lane, F. Gobet, and R. L. Smith, “Attention mechanisms in the CHREST cognitive architecture,”\nLect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 5395\nLNAI, pp. 183–196, 2009.\n[114] J. K. Tsotsos and W. Kruijne, “Cognitive programs: Software for attention’s executive,” Front. Psychol.,\nvol. 5, no. NOV, pp. 1–16, 2014.\n[115] J. K. Tsotsos, A computational perspective on visual attention. MIT Press, 2011. [116] R. A. Brooks, “Elephants don’t play chess,” Rob. Auton. Syst., vol. 6, no. 1–2, pp. 3–15, 1990. [117] M. P. Georgeff and F. F. Ingrand, “Decision-Making in an Embedded Reasoning System,” in Proceedings of\nthe Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), 1989.\n[118] J. L. Bresina and M. Drummond, “Integrating planning and reaction: A preliminary report,” 1990. [119] R. Novianto, B. Johnston, and M. A. Williams, “Attention in the ASMO cognitive architecture,” Front.\nArtif. Intell. Appl., vol. 221, pp. 98–105, 2010.\n[120] P. Langley and D. Choi, “Learning recursive control programs from problem solving,” J. Mach. Learn. Res.,\nvol. 7, pp. 493–518, 2006.\n[121] J. R. Anderson and S. Douglass, “Tower of Hanoi: Evidence for the cost of goal retrieval.,” J. Exp. Psychol.\nLearn. Mem. Cogn., vol. 27, no. 6, pp. 1331–1346, 2001.\n[122] B. Hayes-Roth, K. Pfleger, P. Lalanda, P. Morignot, and M. Balabanovic, “A Domain-Specific Software\nArchitecture for Adaptive Intelligent Systems,” IEEE Trans. Softw. Eng., vol. 21, no. 4, pp. 288–301, 1995.\n[123] C. (Carnegie M. U. Lebiere, E. (Carnegie M. U. Biefeld, R. (Micro A. & D. Archer, S. (Micro A. & D.\nArcher, L. (Army R. L. Allender, and T. D. (Army R. L. Kelley, “IMPRINT/ACT-R: Integration of a task network modeling architecture with a cognitive architecture and its application to human error modeling,” Proc. 2002 Adv. Simul. Technol. Conf. San Diego, CA, Simul. Ser., vol. 34, pp. 13–19, 2002.\n[124] R. Sun and S. Hélie, Psychologically realistic cognitive agents: taking human cognition seriously, vol. 25,\nno. 1. 2012.\n[125] C. Brom, K. Pešková, and J. Lukavsky, “What Does Your Actor Remember? Towards Characters with a\nFull Episodic Memory,” in International Conference on Virtual Storytelling, 2007, pp. 89–101.\n[126] P. Wang, “The Logic of Intelligence,” in Artificial General Intelligence, Springer, 2006. [127] E. Nivel and K. R. Thórisson, “Self-Programming: Operationalizing Autonomy,” in Proceedings of the 2nd\nConference on Artificial General Intelligence, 2009.\n[128] J. Yen, X. Fan, S. Sun, T. Hanratty, and J. Dumer, “Agents with Shared Mental Models for Enhancing Team\nDecision-Makings,” Decis. Support Syst., vol. 41, no. 3, pp. 634–653, 2006.\n[129] W. Zachary, T. Santarelli, J. Ryder, and J. Stokes, “Developing a multi-tasking cognitive agent using the\nCOGNET/iGEN integrative architecture,” 2000.\n[130] B. J. Baars, “Global workspace theory of consciousness: Toward a cognitive neuroscience of human\nexperience,” Prog. Brain Res., vol. 150, pp. 45–53, 2005.\n[131] R. Arrabales, A. Ledezma, and A. Sanchis, “Simulating Visual Qualia in the CERA-CRANIUM Cognitive\nArchitecture,” in From Brains to Systems, Springer New York, 2011.\n[132] U. Faghihi, P. Fournier-Viger, and R. Nkambou, “Implementing an efficient causal learning mechanism in a\ncognitive tutoring agent,” Int. Conf. Ind. Eng. Other Appl. Appl. Intell. Syst., 2011.\n[133] B. J. Baars and S. Franklin, “Consciousness Is Computational: the LIDA Model of Global Workspace\nTheory,” Int. J. Mach. Conscious., vol. 1, no. 1, pp. 23–32, 2009.\n[134] J. A. Starzyk and D. K. Prasad, “A Computational Model of Machine Consciousness,” Int. J. Mach.\nConscious., vol. 3, no. 2, pp. 255–281, 2011.\n[135] M. Minsky, The Society of Mind. New York, NY: Simon & Shuster, Inc., 1986. [136] I. Horswill, R. Zubek, A. Khoo, C. D. Le, and S. Nicholson, “The Cerebus Project,” in Proceedings of the\nAAAI Fall Symposium on Parallel Cognition, 2000.\n[137] U. Faghihi, P. Poirier, P. Fournier-Viger, and R. Nkambou, “Human-like learning in a conscious agent,” J.\nExp. Theor. Artif. Intell., vol. 23, no. 4, pp. 497–528, 2011.\n[138] R. Arrabales, A. Ledezma, and A. Sanchis, “CERA-CRANIUM : A Test Bed for Machine Consciousness\nResearch,” in International Workshop on Machine Consciousness, 2009.\n[139] S. Franklin, T. Madl, S. D’Mello, and J. Snaider, “LIDA: A systems-level architecture for cognition,\nemotion, and learning,” IEEE Trans. Auton. Ment. Dev., vol. 6, no. 1, pp. 19–41, 2014.\n[140] R. C. Atkinson and R. M. Shiffrin, “Human Memory: A Proposed System and its Control Processes,”\nPsychol. Learn. Motiv. - Adv. Res. Theory, vol. 2, no. C, pp. 89–195, 1968.\n[141] A. D. Baddeley and G. Hitch, “Working Memory,” Psychol. Learn. Motiv. - Adv. Res. Theory, vol. 8, no. C,\npp. 47–89, 1974.\n[142] A. Perner and H. Zeilinger, “Action primitives for bionics inspired action planning system: Abstraction\nlayers for action planning based on psychoanalytical concepts,” in IEEE International Conference on\nIndustrial Informatics (INDIN), 2011, pp. 63–68.\n[143] G. Tecuci, M. Boicu, M. Bowman, D. Marcu, P. Shyr, and C. Cascaval, “An Experiment in Agent Teaching\nby Subject Matter Experts,” Int. J. Hum. Comput. Stud., vol. 53, no. 4, pp. 583–610, 2000.\n[144] M. P. Georgeff and A. L. Lansky, “Procedural Knowledge,” in Proceedings of the IEEE, 1986, vol. 74, no.\n10, pp. 1383–1398.\n[145] D. Martin, M. Rincon, M. C. Garcia-Alegre, and D. Guinea, “ARDIS: Knowledge-based architecture for\nvisual system configuration in dynamic surface inspection,” Expert Syst., vol. 28, no. 4, pp. 353–374, 2011.\n[146] E. Gat, “Integrating Planning and Reacting in a Heterogeneous Asynchronous Architecture for Controlling\nReal-World Mobile Robots,” AAAI, pp. 809–815, 1992.\n[147] W. Murdock and A. Goel, “Meta-case-Based Reasoning : Using Functional Models to Adapt Case-Based\nAgents,” in Proceedings of the 4th. International Conference on Case-Based Reasoning, 2001.\n[148] M. Veloso, “PRODILOGY/ANALOGY: Analogical reasoning in general problem solving,” in Topics in\nCase-Based Reasoning, 1993.\n[149] N. Cowan, Chapter 20 What are the differences between long-term, short-term, and working memory?, vol.\n169. Elsevier, 2008.\n[150] D. Kieras, “Modeling Visual Search of Displays of Many Objects: The Role of Differential Acuity and\nFixation Memory,” Proc. 10th Int. Conf. Cogn. Model., 2010.\n[151] S. Franklin, T. Madl, S. Strain, U. Faghihi, D. Dong, S. Kugele, J. Snaider, P. Agrawal, and S. Chen, “A\nLIDA cognitive model tutorial,” Biol. Inspired Cogn. Archit., vol. 16, 2016.\n[152] J. B. Maxwell, A. Eigenfeldt, P. Pasquier, and N. G. Thomas, “Musicog: a Cognitive Architecture for Music\nLearning and Generation,” in Proceedings of the 9th Sound and Music Computing Conference, 2012, pp. 521–528.\n[153] M. Freed and R. Remington, “A conceptual framework for predicting error in complex human-machine\nenvironments,” in Proceedings of the 20th Annual Conference of the Cognitive Science Society, 1998.\n[154] R. Brandon, “A developmental agent for learning features, environment models, and general robotics tasks,”\nFront. Comput. Neurosci., vol. 5, pp. 3–8, 2011.\n[155] E. Hudlicka, “Modeling Cultural and Personality Biases in Decision Making,” in Proceedings of the 3rd\nInternational Conference on Applied Human Factors and Ergonomics (AHFE), 2010.\n[156] K. Forbus, J. Usher, A. Lovett, K. Lockwood, and J. Wetzel, “CogSketch: Sketch understanding for\ncognitive science research and for education,” Top. Cogn. Sci., vol. 3, no. 4, pp. 648–666, 2011.\n[157] X. Pan, L. N. Teow, K. H. Tan, J. H. B. Ang, and G. W. Ng, “A cognitive system for adaptive decision\nmaking,” in 15th International Conference on Information Fusion, FUSION 2012, 2012, pp. 1323–1329.\n[158] K. Sjoo, H. Zender, P. Jensfelt, G.-J. M. Kruijff, A. Pronobis, N. Hawes, and M. Brenner, “The Explorer\nSystem,” in Cognitive Systems, 2010.\n[159] S. C. Shapiro and J. P. Bona, “The Glair Cognitive Architecture,” Int. J. Mach. Conscious., vol. 2, no. 2, pp.\n307–332, 2010.\n[160] U. Faghihi, P. Fournier-Viger, and R. Nkambou, “CELTS: A cognitive tutoring agent with human-like\nlearning capabilities and emotions,” in Smart Innovation, Systems and Technologies, Springer Berlin Heidelberg, 2013.\n[161] B. Goertzel and C. Pennachin, “The Novamente Artificial Intelligence Engine,” in Artificial General\nIntelligence, Springer Berlin Heidelberg, 2007, pp. 63–129.\n[162] M. Lloyd-Kelly, P. C. R. Lane, and F. Gobet, “The Effects of Bounding Rationality on the Performance and\nLearning of CHREST Agents in Tileworld,” Res. Dev. Intell. Syst. XXXI, 2014.\n[163] B. L. Hooey, B. F. Gore, C. D. Wickens, S. Scott-Nash, C. M. Socash, E. Salud, and D. C. Foyle, “Human\nModelling in Assisted Transportation,” in Proceeding of the Human Modeling in Assisted Transportation Conference, 2010, pp. 327–333.\n[164] S. Franklin, “A Foundational Architecture for Artificial General Intelligence,” Adv. Artif. Gen. Intell.\nConcepts, Archit. Algorithms, pp. 36–54, 2007.\n[165] L. A. Coward, “ModellingMemory and Learning Consistently from Psychology to Physiology,” in\nPerception-Action Cycle, V. Cutsuridis, A. Hussain, and J. G. Taylor, Eds. 2011, pp. 63–133.\n[166] R. Sun, “Memory systems within a cognitive architecture,” New Ideas Psychol., vol. 30, no. 2, pp. 227–240,\n2012.\n[167] R. J. Firby, “Adaptive Execution in Complex Dynamic Worlds,” 1989. [168] C. Lebiere, P. Pirolli, R. Thomson, J. Paik, M. Rutledge-Taylor, J. Staszewski, and J. R. Anderson, “A\nfunctional model of sensemaking in a neurocognitive architecture,” Comput. Intell. Neurosci., vol. 2013, 2013.\n[169] D. E. Kieras, “EPIC Architecture Principles of Operation.” 2004. [170] S. Herd, A. Szabados, Y. Vinokurov, C. Lebiere, A. Cline, and R. C. O’Reilly, “Integrating theories of\nmotor sequencing in the SAL hybrid architecture,” Biol. Inspired Cogn. Archit., vol. 8, pp. 98–106, 2014.\n[171] P. Lindes and J. E. Laird, “Toward Integrating Cognitive Linguistics and Cognitive Language Processing,”\n2016.\n[172] M. Freed and R. Remington, “Making human-machine system simulation a practical engineering tool: An\nAPEX overview,” in Proceedings of the 3rd International Conference on Cognitive Modelling, 2000.\n[173] D. P. Benjamin, D. Lyons, and D. Lonsdale, “ADAPT : A Cognitive Architecture for Robotics An\nImplementation of ADAPT,” Proc. Sixth Int. Conf. Cogn. Model., no. October, pp. 337–338, 2004.\n[174] S. L. Epstein, “For the right reasons: The FORR architecture for learning in a skill domain,” Cogn. Sci., vol.\n18, no. 3, pp. 479–511, 1994.\n[175] R. Salgado, F. Bellas, P. Caamano, B. Santos-Diez, and R. J. Duro, “A procedural Long Term Memory for\ncognitive robotics,” in Proceedings of the IEEE Conference on Evolving and Adaptive Intelligent Systems, 2012, pp. 57–62.\n[176] H. Schultheis and T. Barkowsky, “Casimir: An architecture for mental spatial knowledge processing,” Top.\nCogn. Sci., vol. 3, no. 4, pp. 778–795, 2011.\n[177] I. Horswill, “Cerebus : A Higher-Order Behavior-Based System,” AI Magazine, vol. 23, no. 1, 2001. [178] M. Boicu, D. Marcu, C. Boicu, and B. Stanescu, “Mixed-initiative Control for Teaching and Learning in\nDisciple,” in Proceedings of the IJCAI-03 Workshop on Mixed-Initiative Intelligent Systems, 2003.\n[179] K. Corker, G. Pisanich, and M. Bunzo, “A cognitive system model for human/automation dynamics in\nairspace management,” in Proceedings of the First European/US Symposium on Air Traffic Management, 1997.\n[180] M. Lloyd-Kelly, F. R. Gobet, and P. C. R. Lane, “Piece of Mind : Long-Term Memory Structure in ACT-R\nand CHREST,” in Proceedings of the 37th Annual Meeting of the Cognitive Science Society, 2015.\n[181] J. L. Krichmar and G. M. Edelman, “Brain-based devices for the study of nervous systems and the\ndevelopment of intelligent machines,” Artif. Life, vol. 11, no. 1–2, pp. 63–77, 2005.\n[182] L. Shastri, “SHRUTI: A neurally motivated architecture for rapid, scalable inference,” in Studies in\nComputational Intelligence, Springer Berlin Heidelberg, 2007, pp. 183–203.\n[183] A. Lavin, S. Ahmad, and J. Hawkins, “Sparse Distributed Representations,” 2016. [184] G. A. Carpenter, “Neural-network models of learning and memory: Leading questions and an emerging\nframework,” TRENDS Cogn. Sci., vol. 5, no. 3, pp. 114–118, 2001.\n[185] D. R. Kuokka, “MAX: A Meta-Reasoning Architecture for ‘X,’” SIGART Bull., vol. 2, no. 4, pp. 93–97,\n1991.\n[186] S. E. Deutsch, “UAV Operator Human Performance Models,” Tech. Rep. AFRL-RI-RS-TR-2006-0158,\n2006.\n[187] D. Vernon, C. von Hofsten, and L. Fadiga, “The iCub Cognitive Architecture,” in A Roadmap for Cognitive\nDevelopment in Humanoid Robots, 2010, pp. 121–153.\n[188] R. Sun, E. Merrill, and T. Peterson, “A bottom-up model of skill learning,” Proc. 20th Cogn. Sci. Soc. Conf.,\npp. 1037–1042, 1998.\n[189] M. Jaszuk and J. A. Starzyk, “Building internal scene representation in cognitive agents,” Knowledge, Inf.\nCreat. Support Syst. Recent Trends, Adv. Solut., pp. 479–491, 2016.\n[190] E. Hudlicka, “This time with feeling: Integrated model of trait and state effects on cognition and behavior,”\nAppl. Artif. Intell., vol. 16, pp. 611–641, 2002.\n[191] A. Barnes and R. J. Hammell, “Determining Information Technology Project Status using Recognition-\nprimed Decision- making enabled Collaborative Agents for Simulating Teamwork ( R-CAST ),” in Proceedings of CONISAR, 2008.\n[192] A. M. Nuxoll and J. E. Laird, “Extending Cognitive Architecture with Episodic Memory,” in Proceedings of\nthe National Conference on Artificial Intelligence., 2007.\n[193] R. Lian, B. Goertzel, R. Liu, M. Ross, M. Queiroz, and L. Vepstas, “Sentence generation for artificial\nbrains: A glocal similarity-matching approach,” Neurocomputing, vol. 74, no. 1–3, pp. 95–103, 2010.\n[194] T. Mitchell, J. Allen, P. Chalasani, J. Cheng, O. Etzioni, M. Ringuette, and J. C. Schlimmer, “Theo: A\nframework for self-improving systems,” in Architectures for intelligence, K. VanLehn, Ed. Erbaum, 1989, pp. 323–356.\n[195] P. Bustos, J. Martinez-Gomez, I. Garcia-Varea, L. Rodriguez-Ruiz, P. Bachiller, L. Calderita, L. J. Manso,\nA. Sanchez, A. Bandera, and J. P. Bandera, “Multimodal Interaction with Loki,” in Workshop of Physical Agents, 2013, pp. 1–8.\n[196] G. Pezzulo, G. Calvi, and C. Castelfranchi, “DiPRA: Distributed Practical Reasoning Architecture,” in\nProceedings of International Joint Conference on Artificial Intelligence (IJCAI), 2007, pp. 1458–1463.\n[197] P. Wang and P. Hammer, “Issues in Temporal and Causal Inference,” Int. Conf. Artif. Gen. Intell., pp. 1–10,\n2015.\n[198] S. L. Epstein, A. Aroor, M. Evanusa, E. I. Sklar, and S. Parsons, “Learning Spatial Models for Navigation,”\nin International Workshop on Spatial Information Theory, 2015.\n[199] G. Metta, L. Natale, F. Nori, G. Sandini, D. Vernon, L. Fadiga, C. von Hofsten, K. Rosander, M. Lopes, J.\nSantos-Victor, A. Bernardino, and L. Montesano, “The iCub humanoid robot: An open-systems platform for research in cognitive development,” Neural Networks, vol. 23, no. 8–9, pp. 1125–1134, 2010.\n[200] C. Schlenoff, R. Madhavan, J. Albus, E. Messina, T. Barbera, and S. Balakirsky, “Fusing Disparate\nInformation Within the 4D / RCS Architecture,” in 2005 7th International Conference on Information Fusion (FUSION) Fusing, 2005.\n[201] B. Hayes-Roth, P. Lalanda, P. Morignot, K. Pfleger, and M. Balabanovic, “Plans and Behavior in Intelligent\nAgents,” KSL Rep. No. 93-43, 1993.\n[202] J. Bach, C. Bauer, and R. Vuine, “MicroPsi: Contributions to a broad architecture of cognition,” in Annual\nConference on Artificial Intelligence, 2007, pp. 7–18.\n[203] I. Kostavelis, L. Nalpantidis, and A. Gasteratos, “Object recognition using saliency maps and HTM\nlearning,” in Proceedings of the IEEE International Conference on Imaging Systems and Techniques, 2012, pp. 528–532.\n[204] B. Rohrer, “An implemented architecture for feature creation and general reinforcement learning,”\nWorkshop on Self-Programming in AGI Systems, Fourth International Conference on Artificial General Intelligence. 2011.\n[205] B. Goertzel, “A pragmatic path toward endowing virtually-embodied AIs with human-level linguistic\ncapability,” in Proceedings of the International Joint Conference on Neural Networks, 2008, pp. 2956– 2965.\n[206] G. M. Edelman, “Learning in and from brain-based devices,” Science (80-. )., vol. 318, no. 5853, pp. 1103–\n1105, 2007.\n[207] D. J. Jilk, C. Lebiere, R. C. O’Reily, and J. R. Anderson, “SAL: An explicitly pluralistic cognitive\narchitecture,” J. Exp. Theor. Artif. Intell., vol. 20, no. 3, pp. 197–218, 2008.\n[208] D. P. Benjamin, D. Lonsdale, and D. Lyons, “Designing a Robot Cognitive Architecture with Concurrency\nand Active Perception,” in Proceedings of the AAAI Fall Symposium on the Intersection of Cognitive Science and Robotics, 2004.\n[209] M. R. G. Schiller and F. R. Gobet, “A comparison between cognitive and AI models of blackjack strategy\nlearning,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), pp. 143–155, 2012.\n[210] G. W. Ng, X. Xiao, R. Z. Chan, and Y. S. Tan, “Scene Understanding using DSO Cognitive Architecture,”\nin Proceedings of the 15th International Conference on Information Fusion (FUSION), 2012, pp. 2277– 2284.\n[211] A. V. Samsonovich, K. A. De Jong, A. Kitsantas, E. E. Peters, N. Dabbagh, and M. Layne Kalbfleisch,\n“Cognitive constructor: An intelligent tutoring system based on a biologically inspired cognitive architecture (BICA),” Front. Artif. Intell. Appl., vol. 171, pp. 311–325, 2008.\n[212] C. Boicu, G. Tecuci, and M. Boicu, “Improving Agent Learning through Rule Analysis,” in Proceedings of\nthe International Conference on Artificial Intelligence, 2005.\n[213] P. Wang, “Non-Axiomatic Logic (NAL) Specification,” pp. 1–92, 2010. [214] A. Di Nuovo, V. M. De La Cruz, and A. Cangelosi, “Grounding fingers, words and numbers in a cognitive\ndevelopmental robot,” in Proceedings of the IEEE Symposium on Computational Intelligence, Cognitive Algorithms, Mind, and Brain, 2014.\n[215] C. Yu, M. Scheutz, and P. Schermerhorn, “Investigating multimodal real-time patterns of joint attention in\nan HRI word learning task,” in Human-Robot Interaction (HRI), 2010 5th ACM/IEEE International Conference on, 2010, pp. 309–316.\n[216] M. Klenk and K. Forbus, “Measuring the level of transfer learning by an AP physics problem-solver,” Proc.\nNatl. Conf. Artif. Intell., vol. 22, no. 1, pp. 446–451, 2007.\n[217] L. J. Manso, L. V. Calderita, P. Bustos, J. Garcia, M. Martinez, F. Fernandez, A. Romero-Garces, and A.\nBandera, “A General-Purpose Architecture to Control Mobile Robots,” in XV Workshop of physical agents: Book of proceedings, 2014.\n[218] N. Slam, W. Wang, G. Xue, and P. Wang, “A framework with reasoning capabilities for crisis response\ndecision-support systems,” Eng. Appl. Artif. Intell., vol. 46, pp. 346–353, 2015.\n[219] M. A. Olivares-Mendez, P. Campoy, I. Mondragon, and C. Martinez, “Fuzzy-4D/RCS for Unmanned Aerial\nVehicles,” in Proc. International Congress of Brain Inspired Cognitive Systems BICS, 2010, vol. 51.\n[220] G. H. Ogasawara and S. J. Russell, “Planning Using Multiple Execution Architectures,” in Proceedings of\nthe International Joint Conference on Artificial Intelligence, 1993.\n[221] A. K. Seth and G. M. Edelman, “Distinguishing causal interactions in neural populations,” Neural Comput.,\nvol. 19, no. 4, pp. 910–33, 2007.\n[222] T. Huntsberger, “Cognitive architecture for mixed human-machine team interactions for space exploration,”\nin IEEE Aerospace Conference Proceedings, 2011.\n[223] J. Gordon and S. L. Epstein, “Learning to balance grounding rationales for dialogue systems,” in\nProceedings of SIGDIAL Conference, 2011, pp. 266–271.\n[224] D. Shapiro, P. Langley, and R. Shachter, “Using Background Knowledge to Speed Reinforcement Learning\nin Physical Agents,” in Proceedings of the 5th International Conference on Autonomous Agents, 2001, pp. 254–261.\n[225] S. Hart, D. Dahn, A. Atencio, and M. K. Dalal, “Evaluation and Application of MIDAS v2.0,” SAE Tech.\nPap. 2001-01-2648, 2001.\n[226] D. G. Hoecker, E. M. Roth, K. M. Corker, and M. H. Lipner, “Man-Machine Design and Analysis System\n(MIDAS) Applied to a Computer-Based Procedure-Aiding System,” in Proceedings of the Human Factors and Ergonomics Society 39th Annual Meeting, 1994, pp. 195–199.\n[227] J. R. Kirk and J. E. Laird, “Learning General and Efficient Representations of Novel Games Through\nInteractive Instruction,” Adv. Cogn. Syst., vol. 4, 2016.\n[228] J. R. Anderson, M. V. Albert, and J. M. Fincham, “Tracing problem solving in real time: fMRI analysis of\nthe subject-paced Tower of Hanoi.,” J. Cogn. Neurosci., vol. 17, no. 8, pp. 1261–1274, 2005.\n[229] L. Allender, “Modeling Human Performance: Impacting System Design, Performance, and Cost,” in\nProceedings of the Military, Government and Aerospace Simulation Symposium, 2000, pp. 139–144.\n[230] D. K. Mitchell, B. Abounader, and S. Henry, “A Procedure for Collecting Mental Workload Data During an\nExperiment That Is Comparable to IMPRINT Workload Data,” Tehcnical Rep. ARL-TR-5020, 2009.\n[231] W. W. Zachary, J. M. Ryder, J. H. Hicinbothom, J. A. Cannon-Bowers, and E. Salas, “Cognitive task\nanalysis and modeling of decision making in complex environments,” in Making decisions under stress: Implications for individual and team training., J. Cannon-Bowers and E. Salas, Eds. Washington, DC, 1998, pp. 315–344.\n[232] T. L. Seamster, R. E. Redding, J. R. Cannon, J. M. Ryder, and J. A. Purcell, “Cognitive task analysis of\nexpertise in air traffic control,” Int. J. Aviat. Psychol., vol. 3, no. 4, 1993.\n[233] C. D. Wickens, J. S. Mccarley, A. L. Alexander, L. C. Thomas, M. Ambinder, and S. Zheng, “Attention-\nSituation Awareness (A-SA) Model of Pilot Error,” Hum. Perform. Model. Aviat., pp. 213–239, 2008.\n[234] J. E. Laird, K. J. Coulter, R. M. Jones, P. G. Kenny, F. Koss, and P. E. Nielsen, “Integrating intelligent\ncomputer generated forces in distributed simulations: TacAir-Soar in STOW-97,” in Proceedings of the 1998 Spring Simulation Interoperability Workshop, 1998.\n[235] R. M. Jones, J. E. Laird, P. E. Nielsen, K. J. Coulter, P. Kenny, and F. V. Koss, “Automated Intelligent\nPilots for Combat Flight Simulation,” AI Mag., vol. 20, no. 1, pp. 27–42, 1999.\n[236] J. Weng, Y.-B. Lee, and C. H. Evans, “The Developmental Approach to Multimedia Speech Learning,” in\nProceedings., of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 1999.\n[237] J. E. Laird, K. R. Kinkade, S. Mohan, and J. Z. Xu, “Cognitive Robotics Using the Soar Cognitive\nArchitecture,” in Proceedings of the 6th International Conference on Cognitive Modelling, 2004, pp. 226– 330.\n[238] V. Tikhanoff, A. Cangelosi, and G. Metta, “Integration of speech and action in humanoid robots: iCub\nsimulation experiments,” IEEE Trans. Auton. Ment. Dev., vol. 3, no. 1, pp. 17–29, 2011.\n[239] K. R. Thorisson, O. Gislason, G. R. Jonsdottir, and H. T. Thorisson, “A Multiparty Multimodal Architecture\nfor Realtime Turntaking,” in International Conference on Intelligent Virtual Agents, 2010.\n[240] C. Breazeal, “Toward sociable robots,” Rob. Auton. Syst., vol. 42, no. 3–4, pp. 167–175, 2003. [241] C. Breazeal and R. Brooks, “Robot Emotion: A Functional Perspective,” in Who Needs Emotions?: The\nBrain Meets the Robot, 2004.\n[242] S. M. Anzalone, S. Ivaldi, O. Sigaud, and M. Chetouani, “Multimodal people engagement with iCub,” Biol.\ninspired Cogn. Archit., pp. 59–64, 2012.\n[243] U. Kurup, P. G. Bignoli, J. R. Scally, and N. L. Cassimatis, “An architectural framework for complex\ncognition,” Cogn. Syst. Res., vol. 12, no. 3–4, pp. 281–292, 2011.\n[244] O. Kilic, “Intelligent Reasoning on Natural Language Data: A Non-Axiomatic Reasoning System\nApproach,” PhD Thesis, 2015.\n[245] T. Williams and M. Scheutz, “A Framework for Resolving Open-World Referential Expressions in\nDistributed Heterogeneous Knowledge Bases,” in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n[246] H. Joshi, P. S. Rosenbloom, and V. Ustun, “Continuous Phone Recognition in the Sigma Cognitive\nArchitecture,” Biol. Inspired Cogn. Archit., 2016.\n[247] H. Joshi, P. S. Rosenbloom, and V. Ustun, “Isolated word recognition in the Sigma cognitive architecture,”\nBiol. Inspired Cogn. Archit., vol. 10, no. C, pp. 1–9, 2014.\n[248] Y. Zhang and J. Weng, “Grounded auditory development by a developmental robot,” in Proceedings of the\nInternational Joint Conference on Neural Networks, 2001.\n[249] G. A. Carpenter and S. C. Gaddam, “Biased ART: A neural architecture that shifts attention toward\npreviously disregarded features following an incorrect prediction,” Neural Networks, vol. 23, no. 3, pp. 435– 451, 2010.\n[250] A. Kaylani, M. Georgiopoulos, M. Mollaghasemi, and G. C. Anagnostopoulos, “AG-ART: An adaptive\napproach to evolvong ART architectures,” Neurocomputing, vol. 72, pp. 2079–2092, 2009.\n[251] M. Demetgul, I. N. Tansel, and S. Taskin, “Fault diagnosis of pneumatic systems with artificial neural\nnetwork algorithms,” Expert Syst. Appl., vol. 36, no. 7, pp. 10512–10519, 2009.\n[252] H. Ames and S. Grossberg, “Speaker normalization using cortical strip maps: a neural model for steady-state\nvowel categorization.,” J. Acoust. Soc. Am., vol. 124, pp. 3918–3936, 2008.\n[253] C. Distante, P. Siciliano, and L. Vasanelli, “Odor discrimination using adaptive resonance theory,” Sensors\nand Actuators, vol. 69, no. 3, pp. 248–252, 2000.\n[254] Y. Cui, S. Ahmad, and J. Hawkins, “Continuous online sequence learning with an unsupervised neural\nnetwork model,” arXiv Prepr. arXiv1512.05463, 2015.\n[255] W. J. C. Melis, S. Chizuwa, and M. Kameyama, “Evaluation of hierarchical temporal memory for a real\nworld application,” Proc. 4th Int. Conf. Innov. Comput. Inf. Control, pp. 144–147, 2009.\n[256] J. Cassell and K. R. Thorisson, “The power of a nod and a glance: Envelope vs. emotional feedback in\nanimated conversational agents,” Appl. Artif. Intell., vol. 13, no. 4–5, pp. 519–538, 1999.\n[257] A. S. Rao and M. P. George, “Intelligent Real-Time Network Management,” in Proceedings of the Tenth\nInternational Conference on AI, Expert Systems and Natural Language, 1991.\n[258] C. Harrigan, B. Goertzel, M. Ikle, A. Belayneh, and G. Yu, “Guiding probabilistic logical inference with\nnonlinear dynamical attention allocation,” in International Conference on Artificial General Intelligence, 2014, pp. 238–241.\n[259] J. Thornton, J. Faichney, M. Blumenstein, and T. Hine, “Character Recognition Using Hierarchical Vector\nQuantization and Temporal Pooling,” in Proceedings of the 21st Australasian Joint Conference on Artificial Intelligence: Advances in Artificial Intelligence, 2008, vol. 5360, pp. 562–572.\n[260] S. Stolc and I. Bajla, “Application of the computational intelligence network based on hierarchical temporal\nmemory to face recognition,” in Proceedings of the 10th IASTED International Conference on Artificial Intelligence and Applications (AIA), 2010, pp. 185–192.\n[261] W. Zhuo, Z. Cao, Y. Qin, Z. Yu, and Y. Xiao, “Image classification using HTM cortical learning\nalgorithms,” in Proceedings of the 21st International Conference on Pattern Recognition (ICPR), 2012, pp. 2452–2455.\n[262] X. Mai, X. Zhang, Y. Jin, Y. Yang, and J. Zhang, “Simple Perception-Action Strategy Based on\nHierarchical Temporal Memory,” in Proceeding of the IEEE International Conference on Robotics and Biomimetics (ROBIO), 2013, pp. 1759–1764.\n[263] A. Fazl, S. Grossberg, and E. Mingolla, View-invariant object category learning, recognition, and search:\nHow spatial and object attention are coordinated using surface-based attentional shrouds, vol. 58, no. 1. 2009.\n[264] J. Wang, G. Naghdy, and P. Ogunbona, “Wavelet-based feature-adaptive adaptive resonance theory neural\nnetwork for texture identification,” J. Electron. Imaging, vol. 6, no. 3, pp. 329–336, 1997.\n[265] R. C. O’Reilly, T. E. Hazy, J. Mollick, P. Mackie, and S. Herd, “Goal-Driven Cognition in the Brain: A\nComputational Framework,” arXiv Prepr. arXiv1404.7591, 2014.\n[266] S. L. Epstein, “Learning to Play Expertly: A Tutorial on Hoyle,” Mach. that Learn to Play games, pp. 153–\n178, 2001.\n[267] R. Kadlec, J. Gemrot, M. Bida, O. Burkert, J. Havlicek, L. Zemcak, R. Pibil, R. Vansa, and C. Brom,\n“Extensions and applications of Pogamut 3 platform,” Int. Work. Intell. Virtual Agents, vol. I, 2009.\n[268] A. M. Mora, F. Aisa, P. García-Sánchez, P. Á. Castillo, and J. J. Merelo, “Modelling a Human-Like Bot in a\nFirst Person Shooter Game,” Int. J. Creat. Interfaces Comput. Graph., vol. 6, no. 1, pp. 21–37, 2015.\n[269] R. Small and C. B. Congdon, “Agent Smith: Towards an evolutionary rule-based agent for interactive\ndynamic games,” in 2009 IEEE Congress on Evolutionary Computation, CEC 2009, 2009, pp. 660–666.\n[270] D. Cuadrado and Y. Saez, “Chuck Norris rocks!,” in Proceedings of the IEEE Symposium on Computational\nIntelligence and Games, 2009, pp. 69–74.\n[271] D. Wang, B. Subagdja, A. Tan, and G. Ng, “Creating human-like autonomous players in real-time first\nperson shooter computer games,” in Proceedings of the 21st Annual Conference on Innovative Applications of Artificial Intelligence, 2009, pp. 173–178.\n[272] N. Van Hoorn, J. Togelius, and J. Schmidhuber, “Hierarchical controller learning in a first-person shooter,”\nin 2009 IEEE Symposium on Computational Intelligence and Games, 2009, pp. 294–301.\n[273] P. Ulam, A. Goel, and J. Jones, “Reflection in Action: Model-Based Self-Adaptation in Game Playing\nAgents,” in Challenges in Game Artificial Intelligence: Papers from the AAAI Workshop., 2004.\n[274] S. Wintermute, “Imagery in cognitive architecture: Representation and control at multiple levels of\nabstraction,” Cogn. Syst. Res., vol. 19–20, pp. 1–29, 2012.\n[275] M. Shiwali and J. E. Laird, “Learning to play Mario,” Tech. Rep. CCA-TR-2009-03, 2009. [276] I. Kotseruba, Visual Attention in Dynamic Environments and Its Application To Playing Online Games. MSc\nThesis. York University, 2016.\n[277] J. R. Firby, R. E. Kahn, P. N. Prokopowicz, and M. J. Swain, “An Architecture for Vision and Action,” in\nProceedings of the 14th international joint conference on Artificial intelligence (IJCAI’95), 1995.\n[278] R. A. Brooks, “A robot that walks; emergent behaviors from a carefully evolved\\nnetwork,” Neural\nComput., vol. 1, no. 2, pp. 253–262, 1989.\n[279] A. Mininger and J. Laird, “Interactively Learning Strategies for Handling References to Unseen or\nUnknown Objects,” Adv. Cogn. Syst., vol. 5, 2016.\n[280] D. G. López, K. Sjö, C. Paul, and P. Jensfelt, “Hybrid laser and vision based object search and localization,”\nProc. IEEE Int. Conf. Robot. Autom., 2008.\n[281] K. Kawamura, R. A. I. Peters, R. E. Bodenheimer, N. Sarkar, J. Park, C. A. Clifton, and A. W. Spratley, “A\nParallel Distributed Cognitive Control System for a Humanoid Robot,” Int. J. Humanoid Robot., vol. 1, no. 1, pp. 65–93, 2004.\n[282] J. R. Wilson, E. Krause, M. Rivers, and M. Scheutz, “Analogical Generalization of Actions from Single\nExemplars in a Robotic Architecture,” in Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, 2016.\n[283] E. L. Sauser, B. D. Argall, G. Metta, and A. G. Billard, “Iterative learning of grasp adaptation through\nhuman corrections,” Rob. Auton. Syst., vol. 60, pp. 55–71, 2012.\n[284] A. M. Lytle and K. S. Saidi, “NIST research in autonomous construction,” Auton. Robots, vol. 22, no. 3, pp.\n211–221, 2007.\n[285] R. V. Bostelman, A. Jacoff, and R. Bunch, “Delivery of an Advanced Double-Hull Ship Welding,” in Third\nInternational ICSC (International Computer Science Conventions) Symposia on Intelligent Industrial Automation and Soft Computing, 1999.\n[286] K. N. Murphy, R. J. Norcross, and F. M. Proctor, “CAD directed robotic deburring,” in Proceedings of the\nsecond international symposium on robotics and manufacturing research, education, and applications, 1988.\n[287] J. S. Albus, “The NIST Real-time Control System (RCS): an approach to intelligent systems research,” J.\nExp. Theor. Artif. Intell., vol. 9, no. 2–3, pp. 157–174, 1997.\n[288] X. Fan, B. Sun, S. Sun, M. McNeese, and J. Yen, “RPD-enabled agents teaming with humans for multi-\ncontext decision making,” in Proceedings of the International Conference on Autonomous Agents, 2006.\n[289] F. E. Ritter, J. L. Bittner, S. E. Kase, R. Evertsz, M. Pedrotti, and P. Busetta, “CoJACK: A high-level\ncognitive architecture with demonstrations of moderators, variability, and implications for situation awareness,” Biol. Inspired Cogn. Archit., vol. 1, pp. 2–13, 2012.\n[290] S. Schaat, K. Doblhammer, A. Wendt, F. Gelbard, L. Herret, and D. Bruckner, “A psychoanalytically-\ninspired motivational and emotional system for autonomous agents,” Ind. Electron. Soc. IECON 2013-39th Annu. Conf., pp. 6648–6653, 2013.\n[291] A. Heljakka, B. Goertzel, W. Silva, C. Pennachin, A. Senna, and I. Goertzel, “Probabilistic Logic Based\nReinforcement Learning of Simple Embodied Behaviors in a 3D Simulation World,” Front. Artifical Intell. Appl., vol. 157, pp. 253–275, 2007.\n[292] P. Pirjanian, “Behavior Coordination Mechanisms,” Tech. Rep. IRIS-99-375, 1999."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In this paper we present a broad overview of the last 40 years of research on cognitive architectures. Although the number of<lb>existing architectures is nearing several hundred, most of the existing surveys do not reflect this growth and focus on a handful of<lb>well-established architectures. While their contributions are undeniable, they represent only a part of the research in the field. Thus, in this survey we wanted to shift the focus towards a more inclusive and high-level overview of the research in cognitive<lb>architectures. Our final set of 86 architectures includes 55 that are still actively developed, and borrow from a diverse set of<lb>disciplines, spanning areas from psychoanalysis to neuroscience. To keep the length of this paper within reasonable limits we<lb>discuss only the core cognitive abilities, such as perception, attention mechanisms, learning and memory structure. To assess the breadth of practical applications of cognitive architectures we gathered information on over 700 practical projects implemented<lb>using the cognitive architectures in our list.<lb>We use various visualization techniques to highlight overall trends in the development of the field. For instance, our data<lb>confirms that the hybrid approach to cognitive modeling already dominates the field and will likely continue to do so in the future. Our analysis of practical applications shows that most architectures are very narrowly focused on a particular application domain.<lb>Furthermore, there is an apparent gap between general research in robotics and computer vision and research in these areas within<lb>the cognitive architectures field. It is very clear that biologically inspired models do not have the same range and efficiency<lb>compared to the systems based on engineering principles and heuristics. Another observation is related to a general lack of collaboration, which is surprising to see in the inherently interdisciplinary area of cognitive architectures. Several factors hinder<lb>communication, among which are the closed nature of the individual projects (only one-third of the reviewed here architectures are<lb>open-source) and terminological differences.",
    "creator" : "Microsoft® Word 2013"
  }
}