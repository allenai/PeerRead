{
  "name" : "1704.08818.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Tribe Competition-Based Genetic Algorithm for Feature Selection in Pattern Classification",
    "authors" : [ "Benteng Ma", "Yong Xia" ],
    "emails" : [ "yxia@nwpu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "evolutionary algorithms, such as the genetic algorithm (GA), are most commonly used. However, the individual encoding scheme used in various GAs would either pose a bias on the solution or require a pre-specified number of features, and hence may lead to less accurate results. In this paper, a tribe competition-based genetic algorithm (TCbGA) is proposed for feature selection in pattern classification. The population of individuals is divided into multiple tribes, and the initialization and evolutionary operations are modified to ensure that the number of selected features in each tribe follows a Gaussian distribution. Thus each tribe focuses on exploring a specific part of the solution space. Meanwhile, tribe competition is introduced to the evolution process, which allows the winning tribes, which produce better individuals, to enlarge their sizes, i.e. having more individuals to search their parts of the solution space. This algorithm, therefore, avoids the bias on solutions and requirement of a pre-specified number of features. We have evaluated our algorithm against several state-of-the-art feature selection approaches on 20 benchmark datasets. Our results suggest that the proposed TCbGA algorithm can identify the optimal feature subset more effectively and produce more accurate pattern classification.\nKeywords: Feature selection, pattern classification, evolutionary algorithms, genetic algorithm, tribe competition"
    }, {
      "heading" : "1. Introduction",
      "text" : "Numerical features, a bridge between input and models, play a pivotal role in data mining\nand pattern recognition. Although more features are expected to have more discriminatory power, a large number of features may contain a lot of redundancy, and hence significantly degrade the accuracy and generalization of learned models as well as the learning speed, which is widely known as the curse of dimensionality [1]. Therefore, an indispensable step in data mining and pattern recognition procedures is dimensionality reduction, which can be roughly categorized into feature transformation and feature selection.\nFeature transformation targets at projecting features from a high-dimensional space into a low-dimensional space [2]. Well-known feature transformation algorithms include the principal component analysis (PCA) [3], independent component analysis (ICA) [4], linear discriminant analysis (LDA) [5] and their variants. Although being able to reduce the dimensionality of features, these algorithms may destroy the physical meaning of each feature component during the transformation, which complicates further analysis of the model and makes it less interpretable [6]. Alternatively, feature selection aims to choose a subset of available features that are associated with the response variable by excluding relevant and redundant features [7]. Since it can reduce the dimensionality of features while keeping the physical meaning of each feature component, feature selection has distinct advantages over feature transformation in terms of model readability and interpretability.\nDuring the past decades, many feature selection algorithms have been proposed in the literature, which can be divided into three categories [1]: filter methods[8], embedded methods [9] and wrapper methods [10]. In filter methods, selecting or removing a feature component is decided by a criteria function, such as the mutual information, interclass distance or statistical measures. In spite of their computational efficiency, filter methods usually have limited accuracy, due to the absence of optimizing the performance of any specific classifiers directly. On contrast, wrapper methods use the classification performance of a specific classifier to assess the discriminatory power of candidate feature subsets, and thus identify the optimal feature subset with respect to the classifier. However, since the classifier has to be trained by using each\nselected subset of features, respectively, wrapper methods have intrinsically higher complexity. As the special cases of wrapper methods, embedded methods are characterized by a deeper interaction between feature selection and classifier construction. In these methods, the optimal feature subset is generated while the classifier is constructed.\nSince the performances of features and classifiers depend mutually on each other, wrapper methods have been widely used, in which the strategy for searching the optimal feature subset can be either greedy or stochastic. Two of the most classical wrapper methods with the greedy search strategy are the sequential forward selection (SFS) [11] and sequential backward selection (SBS) [12], which, however, suffer from the nesting effect, i.e. the feature that is selected or removed cannot be removed or selected in subsequence steps. This issue can be alleviated by jointly using SFS and SBS. A typical example is the “plus \uD835\uDC59 take away \uD835\uDC5F” method [13], which enlarges the feature subset by adding \uD835\uDC59 features using SBS and then deletes \uD835\uDC5F features using SBS. To avoid the difficulty of determining appropriate values for \uD835\uDC59 and \uD835\uDC5F, Pudil et al. [14] proposed the sequential backward floating selection (SBFS) and sequential forward floating selection (SFFS) algorithm, in which the values of \uD835\uDC59 and \uD835\uDC5F are updated adaptively.\nSince greedy search makes local decisions, stochastic search should be employed to identify the globally optimal feature subset. Shehata and White [15] proposed a randomization method to assess the statistical significance for best subset regression. Despite the method corrected a non-trivial problem with Type I errors, it would still take into account the number of potential features and the inter-dependence between features. However, Evolutionary algorithms, such as the genetic algorithm (GA), genetic programming (GP), ant colony optimization (ACO) and particle swarm optimization (PSO), have proven performance in finding optimal solutions for complex and nonlinear problems [6] with neither prior domain knowledge nor a differentiable objective function, and hence are very suitable for solving the feature selection problem. Muni et al. [16] assumed that each classifier has \uD835\uDC50 trees, where \uD835\uDC50 is the number of classes, and developed a multi-tree GP algorithm for simultaneous feature selection and training a classifier. Sheikhpour et al. [10] proposed the PSO-KDE model, which combines PSO with a non-\nparametric kernel density estimation (KDE) based classifier to distinguish benign breast tumors from malignant ones. In this model, PSO simultaneously optimizes the selected feature subset and the kernel bandwidth in the KDE-based classifier. Jensen and Shen [17] applied ACO to searching a feature subset in a rough set and achieved good results on a relatively small set of features. The comparative study conducted by Santana et al. [18] shows that, if the number of features is small, ACO performs better than other evolutionary approaches; otherwise, GA performs better."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "GA is most likely the first evolutionary computation technique that has been applied to feature selection [19]. Kabir et al. [20] incorporated local search operations into GA and utilized the correlation information in conjunction with the bounded scheme to select a subset of salient features. Li et al. [21] proposed a bi-encoding scheme-based GA to select features for image annotation, in which each individual consists of a pair of strings, a binary string encoding the selection of features and a real valued string indicating the weights of selected features. Hamdani et al. [22] proposed a bi-coded representation in GA for feature selection. They encoded each individual with two chromosomes: a binary chromosome representing the presence of features in the candidate solution and a real-valued chromosome representing the confidence rates of features, which are used to assign different weights to features during the classification procedure. Chen et al. [23] developed a GA-based approach to feature selection and classification of microarray data, in which each individual has two parts: an integer that represents the number of selected features and an integer string that gives the selected feature components. Yahya et al. [24] explored variable length representation of individuals, in which each individual gives the selected features only and different individuals may have different lengths, and thus developed an evolutionary filter approach to feature selection.\nIt is commonly acknowledged that the encoding scheme for individuals plays an important role in GA-based feature selection. Above-mentioned approaches mainly employ two types of encoding schemes: binary encoding and integer encoding [23]. Supposing selecting an optimal feature subset from \uD835\uDC41 features, binary encoding defines each individual as a \uD835\uDC41-bit binary\nstring, where “1” shows the corresponding feature is selected and “0” means discarded, and integer encoding defines each individual as an integer string with the length of selected features, where each integer gives a selected feature component. Binary encoding does not pose any constraints to the number of selected features. When selecting \uD835\uDC5A feature components, we have \uD835\uDC36\uD835\uDC41 \uD835\uDC5A possible solutions. It is obvious that \uD835\uDC36\uD835\uDC41 \uD835\uDC5A1 ≫ \uD835\uDC36\uD835\uDC41 \uD835\uDC5A2 , when \uD835\uDC5A1 ≅ \uD835\uDC41\n2 and \uD835\uDC5A2 is a very small\nor very large number. That means the vast majority of possible solutions in the search space contains about \uD835\uDC41\n2 selected feature components. Thus the binary encoding is prone to resulting\nin a larger feature subset than the optimal one. The integer encoding can not only avoid this bias, but also has a fixed individual size that does not enlarge with the increase of candidate features. However, this scheme requires a pre-determined number of selected feature components, which is usually hard to estimate.\nBesides the encoding scheme, multi-population techniques have been widely investigated in recent GA-based feature selection studies. Although conventional GA has one single population, it has recently been shown that better results can be achieved by introducing multiple parallel populations [25]. Chang et al. [26] proposed a two-phase sub-population GA. In the first phase, the population is decomposed into many sub-populations, which are independent and unrelated to each other, and each sub-population is fixed for a pre-determined criterion. In the second phase, non-dominant solutions are combined and all sub-populations are unified as one big population. As Affenzeller [27] suggested that sub-populations should communicate to each other to bring better convergence and diversity and eventually to further improve the solution, they extended their two-phase method to the SPGA II algorithm [28], which introduces the mechanism to exchange information among sub-populations. Once a sub-population reaches a better non-dominated solution, other sub-populations are able to apply it directly within their searching areas. The idea of this mechanism is to share the Pareto set generated by different subpopulations and to save these Pareto sets as a global archive, which will guide all individuals in the same population searching toward the true Pareto front. However, it is difficult to know whether the decomposition is reasonable and an inappropriate decomposition may result in bad\nperformance. To further improve the interaction and cooperation among sub-populations, Li et al. [29] proposed a multi-population agent GA with a double chain-like agent structure for feature selection, in which every sub-population is connected to each other with one cycle chain and shares two common agents. Due to the shared agents, sub-populations can exchange genetic information with each other to explore the optimal solution. Different from the sharing strategy, Pourvaziri et al. [25] proposed a hybrid multi-population GA, in which multiple sub-populations first evolve independently and then are combined to form a main population that continues to evolve until the stopping criterion is met. In this way, the various parts of the solution space are most likely explored. In multi-population GAs, sub-populations can optimize different objectives [30]. Derrac et al. [31] proposed a cooperative co-evolutionary algorithm for feature selection, in which the GA has three sub-populations, one focusing on feature selection, one on instance selection and the other one on both feature and instance selection."
    }, {
      "heading" : "1.2 Outline of Our Work",
      "text" : "In this paper, we propose a tribe competition-based genetic algorithm (TCbGA) for feature selection in pattern classification, which takes the advantages of binary and integer coding schemes as well as the multi-population technique. The population of individuals is divided into multiple tribes. Initialization and evolutionary operations are designed to ensure that the number of individuals, which select a specific number of features, follows a Gaussian distribution \uD835\uDCA9(\uD835\uDF07\uD835\uDC58 , \uD835\uDF0E) in each tribe \uD835\uDC47\uD835\uDC58. Thus the tribe \uD835\uDC47\uD835\uDC58 is mainly responsible for exploring a part of the solution space, where the number of selected features ranges from \uD835\uDF07\uD835\uDC58 − 3\uD835\uDF0E to \uD835\uDF07\uD835\uDC58 + 3\uD835\uDF0E. Since the features that make up the global optimum must exist in the subspaces searched by one or two elite tribes, inter-tribe competition is introduced to the evolution to predict the elite tribe. The size of the predicted elite tribe is enlarged to give it more search power, and the size of worstperformed tribe is reduced to keep the population size unchanged. This penalty and award strategy enables the algorithm not only to search the solution space locally, but also to quickly look for the global optimal. We have evaluated the proposed algorithm against the state-of-theart feature selection methods on 20 benchmark datasets."
    }, {
      "heading" : "2. Algorithm",
      "text" : "As a heuristic-guided parallel and stochastic search approach, TCbGA searches a global\noptimal subset of features from \uD835\uDC41 candidates through evolving a population of \uD835\uDC41\uD835\uDC43 individuals. Each individual encodes a feature selection scheme using binary coding and has a fitness that is defined as the classification accuracy obtained by applying those selected features to the linear SVM [32] in 10-fold cross validation.\nSince it is hard to estimate how many features are selected in the global optimal solution and, without this value, traditional binary-coded GA is prone to select about \uD835\uDC41 2⁄ features, we divide the solution space into \uD835\uDC41\uD835\uDC47 partly overlapped subspaces and, accordingly, divide the population evenly into \uD835\uDC41\uD835\uDC47 tribes, each exploring one subspace. Those tribes evolve in a two-step iterative way. In the intra-tribe evolution step, each tribe evolves independently and the highest fitness in it improves gradually as new generations of the tribe are iteratively produced by using the selection, crossover, and mutation operations, which mimic the genetic processes of biological organisms, such as reproduction, natural selection and natural mutation. In the inter-tribe competition step, different tribes compete against each other by comparing the best individual in each of them. As a result, the tribes that produce better individuals win the right to enlarge their size, and thus obtain the privilege to have more individuals to search their part of the solution space; whereas other tribes have to reduce their size to keep the total number of individuals in the entire population unchanged. This reproduction and competition process goes through one generation to another, until it converges when the highest fitness is constant for many generations or the required number of generations \uD835\uDC41\uD835\uDC3A is reached. The diagram of the proposed algorithm is summarized in Fig. 1."
    }, {
      "heading" : "2.1 Initialization",
      "text" : "Let Ω\uD835\uDC5A be the assembly of all admissible individuals, which select \uD835\uDC5A feature components. TCbGA aims to identify the individual with highest fitness from the solution space Ω = ⋃ Ω\uD835\uDC5A \uD835\uDC41 \uD835\uDC5A=1 by simultaneously evolving \uD835\uDC41\uD835\uDC47 tribes.\nIn the \uD835\uDC58-th tribe \uD835\uDC47\uD835\uDC58, the number of individuals belonging to Ω\uD835\uDC5A, can be calculated as\nn\uD835\uDC58\uD835\uDC5A = |\uD835\uDC47\uD835\uDC58 ⋂ Ω\uD835\uDC5A| (1)\nwhere |∙| gives the cardinality of a set. We assume that n\uD835\uDC58\uD835\uDC5A follows a Gaussian distribution \uD835\uDCA9(\uD835\uDF07\uD835\uDC58 , \uD835\uDF0E 2) and {\uD835\uDF071, \uD835\uDF072, ⋯ , \uD835\uDF07\uD835\uDC41\uD835\uDC47} equally distribute in the range [1, \uD835\uDC41]. The mean value \uD835\uDF07\uD835\uDC58 largely determines which part of the solution space is explored by the tribe \uD835\uDC47\uD835\uDC58, and the standard deviation \uD835\uDF0E governs the searching scope of each tribe.\nDuring initialization, we generate random individuals and select some of them form the tribe\n\uD835\uDC47\uD835\uDC58 by ensuring\nn\uD835\uDC58\uD835\uDC5A = \uD835\uDC5F\uD835\uDC5C\uD835\uDC62\uD835\uDC5B\uD835\uDC51 (\uD835\uDC41\uD835\uDC47\uD835\uDC58 \uD835\uDCA9(\uD835\uDC5A; \uD835\uDF07\uD835\uDC58,\uD835\uDF0E\n2)\n∑ \uD835\uDCA9(\uD835\uDC56; \uD835\uDF07\uD835\uDC58,\uD835\uDF0E 2)\uD835\uDC56∈ℵ\n) (2)\nwhere \uD835\uDC5F\uD835\uDC5C\uD835\uDC62\uD835\uDC5B\uD835\uDC51(∙) is the nearest integer function, \uD835\uDCA9(\uD835\uDC5A; \uD835\uDF07\uD835\uDC58 , \uD835\uDF0E 2) =\n1\n\uD835\uDF0E√2\uD835\uDF0B \uD835\uDC52\uD835\uDC65\uD835\uDC5D [−\n(\uD835\uDC5A−\uD835\uDF07\uD835\uDF05) 2\n2\uD835\uDF0E2 ], and\n\uD835\uDC41\uD835\uDC47\uD835\uDC58 is the number of individuals in \uD835\uDC47\uD835\uDC58 and is initially set to \uD835\uDC41\uD835\uDC43\n\uD835\uDC41\uD835\uDC47 .\nObviously, n\uD835\uDC58\uD835\uDC5A takes a large value if \uD835\uDC5A approaches to \uD835\uDF07\uD835\uDC58, and a small value otherwise. Such bias enables the tribe \uD835\uDC47\uD835\uDC58 to be mainly responsible for exploring the subspace Ω\uD835\uDF07\uD835\uDC58 and its adjacent subspaces. However, there might not be enough individuals to search the subspace Ωm, which is far away from Ω\uD835\uDF07\uD835\uDC58. To compensate this bias, we let any two adjacent tribes be halfoverlapped. Thus each tribe \uD835\uDC47\uD835\uDC58 explores 2\n(\uD835\uDC41\uD835\uDC47+1) of the solution space. A typical example is\nshown in Fig. 2. Readers are referred to the discussion section for details on the settings of the standard deviation \uD835\uDF0E, population size \uD835\uDC41\uD835\uDC43 and number of tribes \uD835\uDC41\uD835\uDC47."
    }, {
      "heading" : "2.2 Intra-Tribe Evolution",
      "text" : "The intra-tribe evolution, including selection, crossover and mutation, is similar to that of traditional GA, except for keeping n\uD835\uDC58\uD835\uDC5A as fixed as possible to ensure its Gaussian assumption is satisfied.\nFirst, we use an elitism roulette wheel selection scheme based on the rank of fitness to perform selection. The individuals with high fitness have more chance to be selected even\ntautologically as parent chromosomes for the next generation. \uD835\uDC41\uD835\uDC47\uD835\uDC58 individuals are selected after this step in \uD835\uDC47\uD835\uDC58.\nThen, segments of the two parent individuals are exchanged during crossover for generating better individuals. Each individual in the original Gaussian tribe would take crossover with the one generated in the selected step. To ensure that n\uD835\uDC58\uD835\uDC5A is unchanged, the crossover between two parent individuals can be expressed by swapping the same number of non-zero elements in their strings.\nFig. 3(a) shows a typical example, in which two parent individuals χ\uD835\uDC56 = \"1011001100\" and χ\uD835\uDC57 = \"0100110000\" represent the selected feature set \uD835\uDC39\uD835\uDC56 = {\uD835\uDC531, \uD835\uDC533, \uD835\uDC534, \uD835\uDC537, \uD835\uDC538} and \uD835\uDC39\uD835\uDC57 = {\uD835\uDC532, \uD835\uDC535, \uD835\uDC536}, respectively. The crossover position in χ\uD835\uDC56 is located between the third and fourth bits. Consequently, the selected feature set \uD835\uDC39\uD835\uDC56 is divided into two subsets \uD835\uDC39\uD835\uDC56\uD835\uDC39 = {\uD835\uDC531, \uD835\uDC533} and \uD835\uDC39\uD835\uDC56\uD835\uDC45 = {\uD835\uDC534, \uD835\uDC537, \uD835\uDC538}. To ensure the same number of selected features to be swapped, the crossover position in χ\uD835\uDC57 must locate between the fifth and sixth bits, resulting in \uD835\uDC39\uD835\uDC57\uD835\uDC39 = {\uD835\uDC532, \uD835\uDC535} and \uD835\uDC39\uD835\uDC57\uD835\uDC45 = {\uD835\uDC536}. The crossover operator swaps the segments of both individuals. As a result, two newly generated children individuals are χ′\uD835\uDC56 = \"0101101100\" and χ′\uD835\uDC57 = \"1010010000\" , representing the selected feature sets \uD835\uDC39′\uD835\uDC56 = \uD835\uDC39\uD835\uDC57\uD835\uDC39 ∪ \uD835\uDC39\uD835\uDC56\uD835\uDC45 = {\uD835\uDC532, \uD835\uDC535, \uD835\uDC534, \uD835\uDC537, \uD835\uDC538} and \uD835\uDC39′\uD835\uDC57 = \uD835\uDC39\uD835\uDC56\uD835\uDC39 ∪ \uD835\uDC39\uD835\uDC57\uD835\uDC45 = {\uD835\uDC531, \uD835\uDC533, \uD835\uDC536} . Let the number of selected feature components in the individual χ\uD835\uDC56 be denoted by \uD835\uDC5B(χ\uD835\uDC56). We have \uD835\uDC5B(χ\uD835\uDC56) = \uD835\uDC5B(χ′\uD835\uDC56) = 5 and \uD835\uDC5B(χ\uD835\uDC57) = \uD835\uDC5B(χ′\uD835\uDC57) = 3. Therefore, n\uD835\uDC585 and n\uD835\uDC583 in tribe \uD835\uDC47\uD835\uDC58 keep unchanged after this crossover operation.\nIn the above example, \uD835\uDC39\uD835\uDC56\uD835\uDC39 ∩ \uD835\uDC39\uD835\uDC57\uD835\uDC45 = ∅ and \uD835\uDC39\uD835\uDC57\uD835\uDC39 ∩ \uD835\uDC39\uD835\uDC56\uD835\uDC45 = ∅. However, if any of those two intersections is not empty, a children individual will select less feature components than its corresponding parent, due to the duplication caused by the crossover. Fig. 3(b) shows a typical example of this case, where \uD835\uDC39\uD835\uDC57\uD835\uDC39 ∩ \uD835\uDC39\uD835\uDC56\uD835\uDC45 = {7} ≠ ∅. Then, after the crossover |\uD835\uDC39′\uD835\uDC56| = |\uD835\uDC39\uD835\uDC56| − 1. To keep the number of selected features unchanged, we force one bit in the child individual \uD835\uDC39′\uD835\uDC56 to reverse from 0 to 1. Alternatively, when \uD835\uDC39\uD835\uDC56\uD835\uDC39 ∩ \uD835\uDC39\uD835\uDC57\uD835\uDC45 ≠ ∅, we have |\uD835\uDC39′\uD835\uDC57| < |\uD835\uDC39\uD835\uDC57| and have to force one or more bits in the child individual \uD835\uDC39′\uD835\uDC57 to reverse from 0 to 1.\nNext, the mutation operation is applied to individuals to produce sporadic and random alteration of the bits of strings, which can bring the diversity of the species. When a bit of the individual χ\uD835\uDC56 in the tribe \uD835\uDC47\uD835\uDC58 mutates from 0 to 1, a new feature component is added to the subset specified by χ\uD835\uDC56, and hence \uD835\uDC5B(χ′\uD835\uDC56) = \uD835\uDC5B(χ\uD835\uDC56) + 1, which leads to\n{ n′\uD835\uDC58\uD835\uDC5A = n\uD835\uDC58\uD835\uDC5A − 1\nn′\uD835\uDC58(\uD835\uDC5A+1) = n\uD835\uDC58(\uD835\uDC5A+1) + 1 (3)\nwhere \uD835\uDC5A = \uD835\uDC5B(χ\uD835\uDC56). To keep the Gaussian distribution of n\uD835\uDC58\uD835\uDC5A unchanged, we randomly choose an individual χ\uD835\uDC57 ∈ \uD835\uDC47\uD835\uDC58 ⋂ Ω\uD835\uDC5A+1 and mutate one of its bits from 1 to 0, shown as in Fig. 4. Similarly, when a bit of the individual χ\uD835\uDC56 mutates from 1 to 0, we randomly choose an individual χ\uD835\uDC57 ∈ \uD835\uDC47\uD835\uDC58 ⋂ Ω\uD835\uDC5A−1 and mutate one of its bits from 0 to 1.\nAfter selection, crossover and mutation, a new generation of tribe is produced with the same number of individuals and the same Gaussian distribution of n\uD835\uDC58\uD835\uDC5A. It should be noted that, to ensure the highest fitness in each tribe increases monotonously during the evolution, we directly inherit the elitist, which has the highest fitness, to the next generation to replace the individual with the lowest fitness in the corresponding sub-solution space."
    }, {
      "heading" : "2.3 Inter-Tribe Competition",
      "text" : "Tribe competition occurs after all tribes independently evolve two generations, and is revealed by the change of the sizes of tribes. In this step, we gather the elitists of all tribes and sort them by ranking their fitness. The tribes with high-ranking elitists are more likely to be exploring the sub-solution spaces, where the global optimum lies, and hence should be awarded with more searching resources. On the contrary, the tribes with low-ranking elitists are not likely to work in the right places, and hence should release some of their resources to those capable ones. There are different strategies to adjust the size of tribes according to the competition result. In this study, we choose to enlarge the size of the tribe with the best elitist by one and accordingly to shrink the size of the tribe with the worst elitist by one.\nSpecifically, one individual is added to the enlarged tribe and one individual is discarded in\nthe shrunk tribe, which leads to\n{ \uD835\uDC41′ \uD835\uDC47\uD835\uDC58 = \uD835\uDC41\uD835\uDC47\uD835\uDC58 ± 1\nn′′\uD835\uDC58\uD835\uDC5A = ⌊ \uD835\uDCA9(\uD835\uDC5A; \uD835\uDF07\uD835\uDC58,\uD835\uDF0E\n2)\n∑ \uD835\uDCA9(\uD835\uDC5A; \uD835\uDF07\uD835\uDC58,\uD835\uDF0E 2)\uD835\uDC41\uD835\uDC5A=1\n∗ \uD835\uDC41′\uD835\uDC47\uD835\uDC58⌋ . (4)\nThe update of n′′\uD835\uDC58\uD835\uDC5A from the n\uD835\uDC58\uD835\uDC5A is smooth and the individuals would be fine-tuned according to \uD835\uDC50\uD835\uDC58\uD835\uDC5A. \uD835\uDC50\uD835\uDC58\uD835\uDC5A can be calculated as\n\uD835\uDC50\uD835\uDC58\uD835\uDC5A = n′′\uD835\uDC58\uD835\uDC5A − n\uD835\uDC58\uD835\uDC5A. (5)\nWhen \uD835\uDC50\uD835\uDC58\uD835\uDC5A is positive, \uD835\uDC47\uD835\uDC58 would add \uD835\uDC50\uD835\uDC58\uD835\uDC5A individuals {χ1,χ2,…,χ\uD835\uDC50\uD835\uDC58\uD835\uDC5A} ∈ Ω\uD835\uDC5A randomly. When \uD835\uDC50\uD835\uDC58\uD835\uDC5A is negative, \uD835\uDC47\uD835\uDC58 would cut down −\uD835\uDC50\uD835\uDC58\uD835\uDC5A individuals {χ1,χ2,…,χ−\uD835\uDC50\uD835\uDC58\uD835\uDC5A} ∈ \uD835\uDC47\uD835\uDC58 ⋂ Ω\uD835\uDC5A.\nAfter the tribe competition, tribes continue to evolve independently until the next tribe\ncompetition occurs or the convergence is reached."
    }, {
      "heading" : "3. Experiments and Results",
      "text" : "The proposed algorithm was evaluated against several state-of-the-art feature selection methods on 20 benchmark datasets acquired from the UCI Machine Learning Repository [33]. These datasets have diversified physical background, number of features, number of classes and number of instances, representing a variety of pattern classification problems. The detailed descriptions of the datasets are shown in Table 1.\nThese datasets were divided into three groups, including bi-class datasets, multiple-class datasets and high-dimensional datasets. In each bi-class dataset, the number of classes is two; in each multiple-class dataset, the number of classes ranges from 3 to 16; and in each highdimensional dataset, the number of candidate features varies from 100 to 2000.\nAs a wrapper approach, the proposed algorithm adopted the SVM with a linear kernel [32] as a classifier. The pattern classification performance of a selected feature set was assessed by the percentage of correctly classified patterns obtained in the 10-fold cross-validation. It should be noted that other classifiers can also be used in the proposed approach and may lead to different performance. However, the discussion on optimal classifier selection is beyond the scope of this work.\nThe first group of experiments was performed on the nine bi-class datasets, in which the number of features ranges from 13 to 60 and the number of instances ranges from 208 to 4601. The parameters used in the proposed algorithm were listed in Table 2. Readers are referred to the Discussion Section for more details on parameter settings.\nWe compared the proposed TCbGA algorithm to several state-of-the-art feature selection algorithms, including DEMOFS [34] , MOEA/D [35], MDisABC [36], W-QEISS [37], SB-ELM [38], HPSO-LS [39], MoDE [40], GASNCM [41], GCACO [42], GCNC [43], UFSACO [44], FFW-DGC [45], QIFS [46], FSFWISIW [47], BALO [48], MI-SC [49], VMBACO [50], HDBPSO [51] and bGWO [52]. Table 3 shows the mean and standard deviation of the classification accuracy obtained by applying our algorithm to each dataset 25 times and the performance of other algorithms reported in the literature. It reveals that TCbGA achieved the highest average classification accuracy on six datasets and the second highest average classification accuracy on the other two datasets. The results demonstrate that the proposed feature selection algorithm has substantially improved performance in bi-class pattern classification problems.\nWDBC\nHPSO-LS, 2016 GASNCM, 2016\nGCACO, 2015\nGCNC, 2015\nUFSACO, 2014\nProposed TCbGA\n98.27 ± 0.4 93.42 ± 2.0\n94.14 ± 1.36 95.34 ± 1.09 92.06 ± 0.77\n98.78 ± 0.004\nIonosphere\nMoDE, 2015\nUFSACO, 2014\nGCACO, 2015\nQIFS, 2017\nMDisABC, 2015\nFSFOA, 2016\nProposed TCbGA\n93.68 ± 0.26 88.61 ± 0.76 90.41 ± 1.90 86.69 ± 5.87 93.62 ± 1.64\n93.16\n98.32 ± 0.04\nTo demonstrate the performance of TCbGA in multi-class classification problems, the second group of experiments was performed on the seven datasets, in which the number of features ranges from 13 to 279, the number of classes ranges from 3 to 16, and the number of instances ranges from 32 to 5000. The parameters used in TCbGA were listed in Table 4.\nTable 5 shows the mean and standard deviation of the classification accuracy obtained by applying our algorithm to each dataset 25 times and the performance of other algorithms reported in the literature. It reveals that the feature subset selected by TCbGA produces the highest average classification accuracy on six datasets and the second highest average classification accuracy on the other dataset. This experiment demonstrates that the proposed feature selection algorithm has substantially improved performance in multi-class pattern classification problems.\nTo assess the performance of TCbGA in selecting optimal features from a relative large set of features, the third group of experiments was carried out on four datasets, including Musk1, Musk2, Hill-Valley and Colon Cancer data, in which the number of features ranges from 100 to 2000 and the number of instances ranges from 62 to 6598. In this experiment, we divided the population into much more tribes than those in the first two groups of experiments. For instance, when applying our algorithm to the Colon Cancer dataset, which has 2000 features, we partitioned the population into 13 tribes, set the mean of the Gaussian distribution in these tribes to 136, 280, 424, 568, 712, 856, 1000, 1144, 1288, 1432, 1576, 1720 and 1864, respectively, and set the standard deviation to 47.62. Table 6 shows the parameters used on the four datasets in TCbGA.\nLung\nGASNCM, 2016 MDisABC, 2015\nHSA, 2016\nProposed TCbGA\n95.38 ± 3.97 76.65 ± 4.36\n88.65\n96.23 ± 0.003\nWe compared the mean and standard deviation of the classification accuracy obtained by applying our algorithm to each dataset 25 times to the performance of several state-of-the-art solutions reported in the literature in Table 7. It shows that our algorithm achieves substantially higher accuracy than other approaches on these datasets. Moreover, the comparative results also suggest that TCbGA algorithm has a more obvious advantage over other feature selection methods when the number of candidate feature components is relatively large.\nFinally, the Friedman non-parametric test with a significant level of 0.05 [54] was employed to compare our proposed TCbGA algorithm against all the other methods that perform best on each dataset. In this statistic test, the null hypothesis \uD835\uDC3B0 affirms the equal behavior of the comparable methods. Hence, under \uD835\uDC3B0, each method possesses equal rank, which conforms that each method is equally efficient with others. The alternative hypothesis \uD835\uDC3B1 endorses the difference in performances among the comparable methods. The Friedman test we performed shows that the chi-square (\uD835\uDCB32) value is 5.47 and the p-value is 0.0193. The p-value is smaller than the significance level 0.05. Meanwhile, the chi-square value is larger than critical value, which is 3.84 at 0.05 significance level and (2-1) = 1 degree of freedom. Hence, \uD835\uDC3B0 is rejected and \uD835\uDC3B1 is accepted. This result demonstrates that the performance improvement of the proposed algorithm is significant.\nTable 8 gives the results of T-test. It shows that, when setting the significance level in the Ttests to 0.05, TCbGA performs significantly better than eight out of 15 start-of-the-art feature selection methods, including GCACO, GCNC, MOEA/N, GASNCM, UFSACO, MDisABC, HAS and bGWO on the datasets used in this study. Therefore, three groups of comparative experiments suggest that the proposed TCbGA algorithm is able to select better feature subset to improve significantly the accuracy of pattern classification on those 20 datasets.\n4. Discussion"
    }, {
      "heading" : "4.1 Parameter Settings",
      "text" : "As an evolutionary algorithm, the proposed feature selection method involves a number of parameters. Most of them, such as the size of population, maximum number of generations, crossover rate and mutation rate, are the commonly used parameters in traditional GA and can be set under the general guidelines for GA [55]. The number of tribes \uD835\uDC41\uD835\uDC47 and the statistical parameters in each Gaussian distribution \uD835\uDCA9(\uD835\uDF07\uD835\uDC58 , \uD835\uDF0E 2) are introduced to ensure that different groups of individuals can explore different parts of the solution space, and hence play an important role in this algorithm.\nTo ensure that the features that make up the global optimum do exist in a single tribe, every\nsubspace Ω\uD835\uDC5A (1 ≤ \uD835\uDC5A ≤ \uD835\uDC41) should be explored by one or two tribes. As {\uD835\uDF071, \uD835\uDF072, ⋯ , \uD835\uDF07\uD835\uDC41\uD835\uDC47} equally distributes in the range [1, \uD835\uDC41], we let any two adjacent tribes be half-overlap and each tribe \uD835\uDC47\uD835\uDC58 equally take care of 2\n\uD835\uDC41\uD835\uDC47+1 of the solution space, shown as an example in Fig. 2. Hence,\nthe part of solution space to be explored by the tribe \uD835\uDC47\uD835\uDC58 can be denoted by ⋃ Ω\uD835\uDC5A \uD835\uDC5A+ \uD835\uDC5A=\uD835\uDC5A− , where \uD835\uDC5A− = ⌈\uD835\uDF07\uD835\uDC58 − \uD835\uDC41\n\uD835\uDC41\uD835\uDC47+1 ⌉ and \uD835\uDC5A+ = ⌊\uD835\uDF07\uD835\uDC58 +\n\uD835\uDC41\n\uD835\uDC41\uD835\uDC47+1 ⌋ . Since \uD835\uDC47\uD835\uDC58⋂Ω\uD835\uDC5A− ≠ ∅ and \uD835\uDC47\uD835\uDC58⋂Ω\uD835\uDC5A+ ≠ ∅ , the\nstandard deviation \uD835\uDF0E, population size \uD835\uDC41\uD835\uDC43 and number of tribes \uD835\uDC41\uD835\uDC47 must satisfy the following constraint\n∀\uD835\uDC58, n\uD835\uDC58\uD835\uDC5A− = n\uD835\uDC58\uD835\uDC5A+ ≥ 1 . (6)\nApplying Eq. (2) to this constraint, we have\n1\n\uD835\uDF0E√2\uD835\uDF0B \uD835\uDC52\uD835\uDC65\uD835\uDC5D[−\n(\uD835\uDC5A+−\uD835\uDF07\uD835\uDF05) 2\n2\uD835\uDF0E2 ]\n∑ 1\n\uD835\uDF0E√2\uD835\uDF0B \uD835\uDC52\uD835\uDC65\uD835\uDC5D[−\ni2\n2\uD835\uDF0E2 ]\uD835\uDC56∈ℵ\n∙ \uD835\uDC41\uD835\uDC47\uD835\uDC58 ≥ 1\n2 . (7)\nAnother constraint on the standard deviation \uD835\uDF0E is that the search scope of each tribe\nfalls into the range [\uD835\uDF07\uD835\uDC58 − 3\uD835\uDF0E, \uD835\uDF07\uD835\uDC58 + 3\uD835\uDF0E]. Hence, we have\n\uD835\uDF0E ≥ \uD835\uDC41\n3(\uD835\uDC41\uD835\uDC47+1) . (8)\nMeanwhile, we notice that, when \uD835\uDF0E > 0.7,\n∑ 1\n\uD835\uDF0E√2\uD835\uDF0B \uD835\uDC52\uD835\uDC65\uD835\uDC5D [−\ni2\n2\uD835\uDF0E2 ]\uD835\uDC56∈ℵ ≈ 1. (9)\nThus, Eq. (7) can be rewritten as\n1\n\uD835\uDF0E√2\uD835\uDF0B \uD835\uDC52\uD835\uDC65\uD835\uDC5D [−\n(\uD835\uDC5A+−\uD835\uDF07\uD835\uDF05) 2\n2\uD835\uDF0E2 ] ∙ \uD835\uDC41\uD835\uDC47\uD835\uDC58 ≥\n1 2 . (10)\nHere we define a function\nφ(\uD835\uDC5A) = 1\n\uD835\uDF0E√2\uD835\uDF0B \uD835\uDC52\uD835\uDC65\uD835\uDC5D [−\n(\uD835\uDC5A−\uD835\uDF07\uD835\uDF05) 2\n2\uD835\uDF0E2 ] ∙ \uD835\uDC41\uD835\uDC47\uD835\uDC58 . (11)\nThen, Eq. (10) can be rewritten as\nφ(\uD835\uDC5A+) ≥ 1\n2 . (12)\nSince \uD835\uDF11(\uD835\uDC5A) decreases monotonously in the range (\uD835\uDF07\uD835\uDC58, +∞) and \uD835\uDF07\uD835\uDC58 < \uD835\uDC5A + ≤ \uD835\uDF07\uD835\uDC58 + 3\uD835\uDF0E, we get\nφ(\uD835\uDC5A+) ≥ φ(\uD835\uDF07\uD835\uDC58 + 3 ∗ \uD835\uDF0E). (13)\nTo ensure that φ(\uD835\uDC5A+) ≥ 1\n2 , we just need set\nφ(\uD835\uDF07\uD835\uDC58 + 3 ∗ \uD835\uDF0E) ≥ 1\n2 . (14)\nApplying Eq. (11) to Eq. (14), we have\n1\n\uD835\uDF0E√2\uD835\uDF0B \uD835\uDC52\uD835\uDC65\uD835\uDC5D [−\n((\uD835\uDF07\uD835\uDC58+3∗\uD835\uDF0E)−\uD835\uDF07\uD835\uDF05) 2\n2\uD835\uDF0E2 ] ∙ \uD835\uDC41\uD835\uDC47\uD835\uDC58 ≥\n1 2 . (15)\nSimplifying Eq. (15), we get\n\uD835\uDF0E ≤ 2 √2\uD835\uDF0B \uD835\uDC52−\n9 2 ∙ \uD835\uDC41\uD835\uDC47\uD835\uDC58 ≈ 0.008864\uD835\uDC41\uD835\uDC47\uD835\uDC58 . (16)\nTherefore, the standard deviation \uD835\uDF0E is theoretically computed to take a value from the range [ \uD835\uDC41\n3(\uD835\uDC41\uD835\uDC47+1) , 0.008864\uD835\uDC41\uD835\uDC47\uD835\uDC58].\nMeanwhile, applying Eq. (8) to Eq. (16), we have\n\uD835\uDC41\n3(\uD835\uDC41\uD835\uDC47+1) ≤\n2\n√2\uD835\uDF0B \uD835\uDC52−\n9 2 ∙ \uD835\uDC41\uD835\uDC47\uD835\uDC58. (17)\nHence\n\uD835\uDC41\uD835\uDC47 ≥ √2\uD835\uDF0B\uD835\uDC41\n6\uD835\uDC52 − 9 2\uD835\uDC41\uD835\uDC47\uD835\uDC58\n− 1 ≈ 37.6066 \uD835\uDC41\n\uD835\uDC41\uD835\uDC47\uD835\uDC58 − 1 . (18)\nSince the inter-tribe competition may not work when \uD835\uDC41\uD835\uDC47 ≤ 2, we suggest to set the number of tribes \uD835\uDC41\uD835\uDC47 to max (3, ⌈37.6066 \uD835\uDC41\n\uD835\uDC41\uD835\uDC47\uD835\uDC58 − 1⌉).\nWe also chose the WDBC, Lung, Dermatology and Musk1 datasets as a case study to investigate the setting of the number of tribes \uD835\uDC41\uD835\uDC47. The number of features in those datasets is 30, 56, 33 and 166, respectively. The size of tribe is set to 600 for WDBC, Lung and Dermatology, and 1000 for Musk1 due to a relatively large solution space. According to the above theoretical analysis, the optimal number of tribes should be 3, 3, 3 and 6, respectively."
    }, {
      "heading" : "4.2 Tribe Competition",
      "text" : "A distinct feature of the proposed algorithm is to incorporate tribe competition into the evolution process. Since we divided the solution space into several half-overlapped subspaces, each being explored by a tribe, the features that make up the global optimum must exist in the subspaces searched by one or two tribes, which are named as the elite tribes. Ideally, we should keep only elite tribes and allocate them all individuals. Unfortunately, we do not know which tribe is the elite. Hence, we performed the inter-tribe competition and regarded the winners as the elite tribes. Then, we enlarged the size of the predicted elite tribes to enable them to have\nmore search power and cut down the size of those defeated tribes to save computing resource. This penalty and award strategy enables the algorithm not only to search the solution space locally, but also to quickly look for the global optimal. The characteristic of human tribes in the primitive society, i.e. tribes evaluate themselves to get more adaptive ability and plunder other weaker tribes of their resources to make themselves stronger, contributes to better understand this competition strategy.\nFig.6 illustrates the change of the size of three tribes during the evolution and competition when applying the proposed algorithm to the Dermatology dataset. It shows that, although the population size is maintained, the sizes of tribes change dynamically. Our result indicates that the highest classification accuracy is achieved when using a subset of 23 features. This optimal solution lies exactly in the subspace that is explored by the third tribe, in which the average number of selected features is 25. This result is completely consistent with our observation of the increase of third tribe’s size.\nFig .6. Change of the size of three tribes (denoted by T1, T2 and T3) during the evolution\nHowever, our prediction of the elite tribe is not always correct, since it may perform worse than others at the early stage of evolution, due to the complexity of feature selection problems.\nFig.6 also reveals that the size of each tribe is not changed monotonically and the elite tribe \uD835\uDC473 was even considered to be the worst one in early competitions. Therefore, we chose the most conservative implementation of the penalty and award strategy that is to enlarge and shrink only the best and worst tribe by one, although there are different implementations, including enlarging and/or shrinking one or more tribes by adding or removing one or more individuals each time. More aggressive implementations may speed up the convergence of the evolution, but at the risk of missing the elite tribe due to shrinking it too much at a too early stage.\nAnother important issue related to the inter-tribe competition is how frequent it should be performed. Generally, more frequent competition gives the algorithm more opportunity to adjust the size of tribes, whereas less frequent competition gives each tribe more opportunity to find a better solution before its size is adjusted.\nFig.7 Accuracy (left) and time-cost (right) of our algorithm on two datasets over different settings\nWe adopted the Sonar and Dermatology datasets as a case study to explore the impact of the frequency of inter-tribe competitions on the algorithms’ performance. The results were shown in Fig. 7, in which \uD835\uDC41CS stands for performing the inter-tribe competition after every \uD835\uDC41CS generations. It reveals that setting \uD835\uDC41CS to 1 decreases the classification accuracy on both datasets; whereas using \uD835\uDC41CS greater than 2 results in almost the same accuracy but significantly increased time-cost. The reason for the increased time-cost lies in the fact that a larger \uD835\uDC41CS\nmakes the algorithm converge more slowly and require more generations of evolution to achieve a satisfying result. Therefore, considering both the accuracy and complexity, we set \uD835\uDC41CS = 2. 4.3 Robustness\nThe proposed method employs heuristic-guided stochastic search, and hence may produce\ndifferent near optimal solutions in multiple runs. Table 9 gives average performance obtained by applying this algorithm with random initializations to those 20 datasets 25 times. In this table, ACave and ACstd are the mean and standard deviation of the classification accuracy, respectively, ASnum is the average number of selected features, and Totalnum is the total number candidate feature component to be selected. The very small standard deviation of accuracy shown in this table demonstrates that the proposed algorithm is relatively robust to initializations.\nMusk1 94.27% 7.52e-02 97.3/166\nMusk2 99.23% 4.52e-02 85.5/166\nColon cancer 96.50% 2.32e-02 19.3/2000\nWine 99.60% 7.23e-03 9.0/13\nZoo 98.03% 8.67 e-03 5.1/16\nWaveform 85.43% 3.85e-03 18.0/21\nVehicle 86.05% 5.73e-02 12.5/18\nDermatology 99.65% 4.25e-03 24.0/33\nArrhythmia 74.80% 7.56e-02 34.7/279"
    }, {
      "heading" : "4.4 Computational Complexity",
      "text" : "As a wrapper feature selection method and an evolutionary approach, the proposed algorithm has a relatively high computational complexity. Most computation is spent on the evaluation of individuals’ fitness. Given the size of population N\uD835\uDC43 and maximum number of generation N\uD835\uDC3A, such evaluation is performed N\uD835\uDC43 ∙ N\uD835\uDC3A times. Each time, the feature subset specified by an individual is used to train a classifier and test its accuracy via cross validation. Therefore, the computational complexity is also determined by the number of candidate features, number of classes and number of instances. More instances and classes usually require more computation, and more candidate features may lead to large selected feature subset and thus also require increased computation. Table 10 give the average time cost of training the model and testing one instance on seven datasets (Intel Core i7-4790 CPU 3.2GHz, NVidia GTX Titan X GPU, 32GB memory and Matlab Version 2014). Although the off-line training is extremely time-consuming, the proposed algorithm has the ability to select the optimal features for various classification problems. We believe the ever increase of computational power, particularly the prevalence of GPU-based parallel computation will make it more computationally attractive. Meanwhile,\nduring online testing, applying the selected optimal feature subset to solving pattern classification problems is very efficient."
    }, {
      "heading" : "4.5 Feasibility",
      "text" : "stochastic evolutionary strategy. However, it cannot guarantee to converge to the global optimal within limited generations. As a result, when the number of candidate features is small, it may have little advantage over other approaches. We used the WBCD dataset as a case study, where there are 699 instances from two classes and each instance consists of nine features. Since there are only 29 − 1 possible feature subsets in this problem, we can find the optimal feature subset with respect to different classifiers via exhaustive searching. Table 11 give the classification accuracy and time cost of the proposed algorithm and exhaustive searching. It shows that the proposed algorithm can achieve the optimal classification accuracy, but may spend even more\ntime than exhaustive searching. Therefore, when the number of candidate feature is too small, it may not be necessary to utilize multiple tribes to search different parts of the solution space, particularly when the solution space can be explored exhaustively.\nnumber of tribes (\uD835\uDC41\uD835\uDC47 ≥ ⌈37.6066 \uD835\uDC41\n\uD835\uDC41\uD835\uDC47\uD835\uDC58 − 1⌉) or set the standard deviation \uD835\uDF0E to a large value.\nThe former increases the computationally complexity; whereas the later makes the algorithm not to be able to focus on a small solution space and hence may produce less accuracy results, unless we set the tribe size to an even larger number that satisfies \uD835\uDF0E < 0.008864\uD835\uDC41\uD835\uDC47\uD835\uDC58. Meanwhile, a huge number of instances also makes the proposed algorithm computationally intractable. In this case, we may use a randomly sampled small training set to evaluate the fitness of each individual. The discrepancy caused by this is determined by the generalization ability of the classifier used in the wrapper method."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we propose the TCbGA algorithm for feature selection in pattern classification, which divides the population into multiple tribes, each containing a cohort of individuals. We encode each individual as a binary string to represent a possible feature selection scheme and modify the initialization and evolutionary operations to ensure that the number of selected features in each tribe follows a Gaussian distribution. Besides evolving each tribe independently, we introduce tribe competition to allow the tribe elite individuals to have increased searching power. Our results suggest that the proposed algorithm outperforms several state-of-the-art\nfeature selection approaches on 20 benchmark datasets. Our future work will focus on incorporating classifier selection into the optimization process, reducing the complexity and extending this algorithm to solve feature selection problems with a super large feature set."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was supported in part by the National Natural Science Foundation of China under Grants 61471297 and 61231016, and in part by the Returned Overseas Scholar Project of Shaanxi Province, China. We appreciate the assistance we received by using the UCI Machine Learning Repository (http://mlr.cs.umass.edu/ml/)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Feature selection has always been a critical step in pattern recognition, in which evolutionary algorithms, such as the genetic algorithm (GA), are most commonly used. However, the individual encoding scheme used in various GAs would either pose a bias on the solution or require a pre-specified number of features, and hence may lead to less accurate results. In this paper, a tribe competition-based genetic algorithm (TCbGA) is proposed for feature selection in pattern classification. The population of individuals is divided into multiple tribes, and the initialization and evolutionary operations are modified to ensure that the number of selected features in each tribe follows a Gaussian distribution. Thus each tribe focuses on exploring a specific part of the solution space. Meanwhile, tribe competition is introduced to the evolution process, which allows the winning tribes, which produce better individuals, to enlarge their sizes, i.e. having more individuals to search their parts of the solution space. This algorithm, therefore, avoids the bias on solutions and requirement of a pre-specified number of features. We have evaluated our algorithm against several state-of-the-art feature selection approaches on 20 benchmark datasets. Our results suggest that the proposed TCbGA algorithm can identify the optimal feature subset more effectively and produce more accurate pattern classification.",
    "creator" : "Microsoft® Word 2016"
  }
}