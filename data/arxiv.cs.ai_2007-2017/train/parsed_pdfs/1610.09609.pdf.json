{
  "name" : "1610.09609.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene",
    "authors" : [ "Keyu Lu", "Jian Li", "Xiangjing An", "Hangen He" ],
    "emails" : [ "keyu.lu@nudt.edu.cn;", "keyu.lu@alumni.ubc.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Generalized Haar filter; Deep networks; Object detection; Traffic scene"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent advances in self-driving vehicles and advance driver assistance system (ADAS) have attracted keen attention and interest from researchers and automobile manufacturers. Vision sensor plays an important role in these areas due to its faster response, lower price and power consumption compared with other popular sensors such as LiDAR and millimeter-wave radar. Moreover, vision sensor has the ability to capture rich information from traffic scene (such as luminance, color and texture) [1], which is beneficial for object detection.\nVision-based object detection is one of the fundamental functions for self-driving vehicle systems and advance driver assistance systems (ADAS), which need to detect the objects around and check whether they are dangerous to the host vehicle. However, object detection in traffic scene from image are still challenging job. On one hand, traffic scenes are diverse and the presence of objects (e.g. vehicles and pedestrians) in\nar X\niv :1\n61 0.\n09 60\n9v 1\n[ cs\n.C V\n2 traffic scenes are extremely flexible and unpredictable. on the one hand, most traffic scene applications have several special requirements such as real-time, portable (e.g. for ADAS device), low price and power consumption, which are quite different from object detection tasks in the ILSVRC [2] and COCO [3] competitions.\nRecently, a great number of researchers have been interested in learning based approaches. Especially in recent years, the success of deep learning boosts the development of vision-based object detection. Convolutional Neural Network (CNN) [6] is one of the most popular forms of deep networks. By using the strategies of local receptive fields, weight sharing and spatial pooling [7], CNN has made a breakthrough in computer vision and image processing. For instance, AlexNet [6], which is a type of CNN, has made a startling achievement in the competition of ILSVRC-2012 [2] and demonstrated its superiority in image classification. However, with the increasing of network depth, CNN has met the bottleneck in training [8]. Recently, He et al. proposed the deep residual network [8] which employs shortcut connections to overcome the limitations in training a deeper CNNs.\nDespite the fact that CNN has achieve a tremendous success in computer vision and image processing, there still exist two main problems that have to be solved when applied to objects detection tasks. Firstly, CNN has a multitude of convolutions that have to be calculated, it would be rather inefficient in object detection if using traditional dense sliding windows paradigm [4,5]. Secondly, CNN has the property of shift invariance, that is, it is less sensitive to the shift of objects in input image patch. Consequently, it can not achieve precise localization if it is directly applied to objects detection [9].\nTo overcome these limitations of CNN, Girshick et al. introduced the framework of region proposal based CNN (called R-CNN) [10] and successfully applied it to object detection. The main idea of region proposal based CNN is performing CNNs on candidate bounding-boxes (called “proposals”) which have potential to contain objects [10]. Later, the updated version “Fast R-CNN” [9] is proposed to improve the efficiency of object detection. This method combines R-CNN with SPPnet [11]. In this way, computation of proposals can be sharing and runtime of object detection can be reduced dramatically. However, in both R-CNN and Fast R-CNN, the proposals are generated by Selective Search [12], which is quite inefficient and thus limits the detection speed of these methods. To overcome this limitation, Faster R-CNN [13] proposed to used Region Proposal Networks (RPNs) to generate proposals instead of Selective Search [12]. By this means, the stage proposal generation and CNN-based classification/regression can be performed under an unified framework and thus the detection speed can be boosted with the help of GPU.\nHowever, region proposal based CNNs are complex and not easy to optimize [14]. For this reason, several works attempt to achieve real-time objects detection by regarding it as a regression problem. YOLO [14] is one of the pioneering works on deep regression networks based object detection. The approach is able to simultaneously outputs the location of bounding box, category and its confidence score for each object in the image at a extremely high frame rate. Nevertheless, it does not work well in small objects detection. In the same vein, Liu et al. proposed SSD (Single Shot MultiBox Detector) [15] for real-time objects detection. It equipped the deep regression networks with several new techniques such as multi-scale feature maps and default boxes [15].\n3 Image Block conv1\nrelu1 pool1\nconv2 relu2 pool2\nconv3 relu3 pool3\nconv4\nrelu4\npool4\nconv5_1\nrelu5_1\nconv6_1\nrelu6_1\nconv7_1\nrelu7_1 conv8_1 Localization\nsoftmaxconv7_2 conv6_2\nrelu6_2\nconv5_2\nrelu5_2 Classification\nFig. 1: Architecture of the Deep Networks.\nDespite the fact that it is able to be faster and more accurate compared with YOLO [14], its performance on small objects detection is still unsatisfactory.\nIn this paper, we introduce a practical and robust approach for real-time object detection in traffic scene. The approach requires less storage and computing resources, and thus it is feasible for traffic scene applications. The novelties of this work lie in four fold:\n(1) We present a local regression strategy for accurate objects detection. As we know, current regression based solutions (such as YOLO [14] and SSD [15]) employ global regression strategy to regress the bounding-box of each object from the whole image, which is actually a much more difficult task compared with local regression. Thus, global regression strategy often needs to be supported by a complex or large-scale network and yet its performance on small objects detection is still unsatisfactory. To solve these problem, we introduce the local regression strategy that performs tiny regression networks on small image patches to detect and locate objects. As the regression networks are less sensitive to the scaling and shift of the objects in image patches, a series of sparse sliding-windows can be obtained from multi-scale image pyramid. Finally, objects can be detected and located efficiently based on these sparse sliding-windows;\n(2) We introduce the generalized Haar filter based deep networks where each weight larger than 3 × 3 are constrained to the form of generalized Haar filter in training phase. Owing to the strong representation of Haar filters, the networks are able to achieve a high performance. Besides, the networks consume much less storage and computing resources compared with traditional deep networks, which make them possible to be utilized in traffic scene applications. In addition, the constraint of generalized Haar filter provides a form of regularization which is able to improve the generalization ability of the deep networks;\n(3) For object detection problems in traffic scene, a sparse windows generation method is proposed. The method first generates a series of sparse sliding-windows in multiscale image pyramid by setting a specific stride according to the scale and shift tolerance of our deep network. In this way, it can be ensure that each object is completely contained in at least one window. Besides, in most object detection systems for traffic scene, camera is mounted on a fixed position (e.g. mounted on the top of a vehicle windshield). Consequently, objects (e.g. vehicles and pedestrians)\n4 produce the predetermined location-specific patterns in images. According to this assumption, perspective geometry is also utilized to further reduce the candidate windows;\n(3) We construct a tiny deep network that simultaneously outputs the bounding box, category and confidence score of detected object through two output channels: localization channel and classification channel. The network is efficient and consumes less resources. As our proposed method decomposes the global regression task into several easier local regression tasks, which can be handled effectively without the support of the complex or large-scale networks. Thus, our proposed approach can work efficiently and effectively by combining this tiny a deep network with local regression tasks.\nThe rest of this paper is organized as follows: In section 2, we introduce the architecture of our generalized Haar filter based deep networks and describe how to design their weights. In section 3, we propose the algorithm of sparse windows generation. Experimental results and corresponding discussion are presented in section 4, while conclusions and future works are given in section 5."
    }, {
      "heading" : "2 Generalized Haar Filter based Deep Networks",
      "text" : ""
    }, {
      "heading" : "2.1 Architecture of the Deep Networks",
      "text" : "To solve the problem of object detection, we construct a deep network that simultaneously outputs the bounding box, category and confidence score of detected object via two output channels: localization channel and classification channel (see Fig. 1). The location channel focuses on bounding box regression and outputs a 4-dimensional vector (dx1, dx2, dy1, dy2) which is used to describe a bounding box (see Fig. 2). The classification channel outputs the category and confidence score which are denoted by a 2-dimensional vector (l, s).\nAs shown in Fig. 1, our deep network consists of 11 convolution layers, 4 maxpooling layers and 1 softmax layer. To reduce the memory consumption, several lowlevel features are shared by localization and classification channels via conv1∼pool4. As these two channels aim at different tasks, each of them has 4 independent layers to focus on different problems.\nOur deep network is less sensitive to the scaling and shift of the input object owing to regression based localization channel. Consequently, instead of generating region proposals, objects can be detected and located efficiently based on sparse slidingwindow paradigm and perspective geometry. More details on this issue are introduced in section 3.1 and related experiments are presented in section 4."
    }, {
      "heading" : "2.2 Generalized Haar Filter based Weights Design",
      "text" : "Despite the fact that deep neural networks have achieved state-of-the-art performance in object detection, they consume considerable storage, computing resources and power [16]. Consequently, they are often unsuitable for power and memory constrained devices such\n5 1dx 2dx 1dy\n2dy\nInput Window\nObject\nFig. 2: Description of a bounding box using a 4-dimensional vector.\nas vehicle-mounted embedded systems and mobile devices. To overcome this limitation, we introduce a novel convolution weights design method which is based on generalized Haar filter.\nHaar-like filters have been successfully applied in object detection owing to their strong representation and high efficiency [4,17]. Instead of using a fixed number of rectangles and configuration types, generalized Haar filters are based on arbitrary configurations and number of rectangles [18]. In this work, the weights of our deep network are constrained based on generalized Haar filters.\nUnlike original generalized Haar filters with arbitrary configuration and number of rectangles [18], in our work, the configuration and number of rectangles are obtained in a data-driven way. For a weight wi of size m ×m (m ≥ 3), we constrain it to the following form:\nwi = ŵp · ki, (1)\nwhere ki ∈ R is a multiplication factor and ŵp is the p-th generalized Haar filter in the Haar filters space. As we know, there are 2m 2\ntypes of configurations for the generalized Haar filters of size m×m. In our case, ŵp and its negative form −ŵp can be regarded as a same filter. Thus, the Haar filter space of size m ×m contains 2m2−1 filters. That is, p ∈ [1, 2m2−1].\nFor a trained deep network for vehicle detection, Fig. (4) illustrates the filter usage in the 3×3 Haar filter space. As shown in Fig. (4), filter usage is quite “sparse”. In other words, a few generalized Haar filters tend to be used much more frequent than the rest of filters in 3× 3 Haar filter space. Thus, these filters are more important and representative than the other filters. According to this fact, we try to reserve these “important” generalized Haar filters and remove the rest of filters from Haar filter space. In this way, the configuration and number of rectangles of generalized Haar filters are obtained in a data-driven way. Let Nr denote the number of filter we try to reserve. We sort the filters according to their usage count and select the top Nr filters (Nr = 32 in our work). These selected filters are shown in Fig. (3). Then, the deep network is retrained by constraining the corresponding weights to these Nr filters with multiplication factors.\nGeneralized Haar filters are able to compress the deep networks by locking the relationship between each element in the weights larger than 3×3. In this way, considerable storage resources can be saved. For each weight of our deep network, we only need to store a multiplication factor and a filter index. In this work, each multiplication factor (single-precision float-point) takes 4 bytes and each filter index (ranging from 1 to 32) takes less than 1 byte. Thus, only 5 bytes are needed for each weight which is larger\n6\nthan 3× 3. By contrast, in traditional deep networks, 4m2 bytes are consumed for each weight of sizem×m. Besides, despite the fact that weights are constrained according to Nr generalized Haar filters, each element of the weights is still able to keep a relatively high precision owing to the multiplication factors.\nUnlike approaches in [17,4,18] where Haar filters are operated based on integral images. We calculate Haar filter based convolution by basic addition and multiplication operation. One reason is that most weights in deep networks are relatively small (3× 3 and 5× 5), accordingly, there is no obvious benefit in using integral images. Moreover, integral image of each channel is needed to be re-computed for each layer, which is inefficient. Besides, the configuration types and number of rectangles of the generalized Haar filter are diverse due to our data-driven filter selection strategy, consequently, lots of different computation rules are needed to be designed for these selected filters if calculated based on integral images. In this way, it difficult to achieve efficient batch calculation.\nIn our work, generalized Haar filters only contains two types of elements: -1 and +1. Thus, each generalized Haar filter can be regarded as a sign pattern matrix. Therefore, the weight wi can be written as:\nwi = ŵp · ki = sign(ŵp) · k. (2)\nAs we know, each convolution step can be regarded as a dot product operation. Let Pi denote the dot product between the Haar filter ŵp and the input patch xi, that is:\nPi = ŵp · xi = sign(ŵp) · xi. (3)\nAs a Haar filter can be regarded as a sign pattern matrix, equation (3) can be calculated by lookup table without multiplication. Then each convolution step can be transformed to the following form:\nwi · xi = (ŵp · xi) · ki = Pi · ki = ki · ∑ Pi, (4)\nwhere “ ∑\n” denotes the sum of the matrix elements. In this way, only one multiplication is needed for each convolution step. By contrast, in traditional deep networks,m2 multiplication are needed. In a work, our approach consume much less computing resources,\n7 1 32 64 96 128 160 192 224 256\nFilter Index\n0\n2000\n4000\n6000\n8000\n10000 12000 U sa g e C o u n t\nFig. 4: Filter usage in 3× 3 Haar filter space.\nin this way, power consumption can be also reduced. Accordingly, our deep network is suitable for embedded devices and FPGA where power and available multipliers are limited.\nIn addition, the constraint of generalized Haar filter provides a form of regularization which is able to improve the generalization ability of the deep network. The loss function and the corresponding regularization term are introduced in the next subsection."
    }, {
      "heading" : "2.3 Multi-Task Training",
      "text" : "Our deep network has two output channels: localization channel and classification channel. Each localization channel outputs a 4-dimensional vector d = (dx1, dx2, dy1, dy2). Given the ground truth location vector d̂ = (dx̂1, dx̂2, dŷ1, dŷ2), the localization loss can be defined as a squared loss form:\nLloc(d, d̂) = ∥∥∥d− d̂∥∥∥2. (5)\nAs classification channel focuses on a typical classification problem, we define the classification loss Lcla as a traditional softmax-loss form.\nFor a weight wi (m×m,m ≥ 3) in our deep network, the training goals is to obtain a Haar filter index p and a multiplication factor ki. Each wi is constrained according to least squares principle, that is:\nmin ∥∥∥(wi − ŵr · λr)2∥∥∥\n1 , r = 1, 2, . . . , 2m\n2−1. (6)\nFor each Haar filter in the Haar filter space, the corresponding multiplication factor λr can be obtained by:\nd\ndλr\n∥∥∥(wi − ŵr · λr)2∥∥∥ 1 = 0. (7)\nFrom equation (7), λr can be calculated:\nλr = ∑ wi · ŵr∑ ŵ2r . (8)\n8 Having obtained λr, we constrain each weight by adding a regularization term to its original loss function Coi(wi, w∗i ):\nCi(wi, w ∗ i ) = Coi(wi, w ∗ i ) + ϕ ·min\nr ∥∥∥(wi − ŵr · λr)2∥∥∥ 1 . (9)\nThis regularization term can not only reduce the storage consumption of our deep network, but also improve the generalization ability of the deep network. Further experiments are presented in section 4.\nAs the deep network are trained via stochastic gradient descent (SGD), the loss function Ci(wi, w∗i ) is needed to be transformed to a differentiable form. According to [19], a maximum function for Z = {z1, . . . , zn} can be smoothly approximated as:\nFmax(Z) =\nn∑ r=1 zq+1r\nn∑ r=1 zqr , (10)\nwhere p ≥ 1, ∑ zqr 6= 0 and zi ≥ 0. The continuity and differentiability have been proofed by [19]. Inspired by this work, we first define the intermediate variable zr as:\nzr = exp(− ∥∥∥(wi − ŵr · λ)2∥∥∥\n1 )\n= exp(− ∥∥∥∥∥(wi − ŵr · ∑ wi · ŵr∑ ŵ2r ) 2 ∥∥∥∥∥ 1 ). (11)\nThen the loss function of each weight can be transformed to a differentiable form:\nCi(wi, w ∗ i ) = Coi(wi, w ∗ i )− ϕ · ln  2m 2−1∑ r=1 zq+1r\n2m2−1∑ r=1 zqr  . (12) Having obtained the differentiable form of the loss function of each weight, the updated weight wt+1i can be obtained using stochastic gradient descent (SGD). Then, the Haar filter index p and a multiplication factor ki can be obtained by: p = argmin r ∥∥∥∥∥(wt+1i − ŵr · ∑ wt+1i · ŵr∑ ŵ2r ) 2 ∥∥∥∥∥ 1 ki = ∑ wt+1i · ŵp∑\nŵ2p\n. (13)\nThe procedure for training our deep network is demonstrated in algorithm 1. For the i-th weight of our deep network, we first construct the current weight using the filter index and multiplication factor which are updated in previous iteration. Then, the targetw∗i are obtained via a standard forward propagation approach. After that, based on\n9 the loss function Ci(wti , w ∗ i ) (see equation (12)), the gradients can be computed using a standard backward propagation approach. Having obtained the gradients, weight is updated using stochastic gradient descent (SGD). Lastly, using the updated weight, filter index and multiplication factor can be updated via equation (13).\nAlgorithm 1 Parameter update of the i-th weight. Input: A minibatch of inputs xi;\nLoss function Ci(wti , w ∗ i ); Generalized Haar filter space ŵr , (r = 1, 2, . . . , 2m\n2−1); Current filter index p̃; Current multiplication factor k̃i; Current learning rate lt.\nOutput: Updated filter index p; Updated multiplication factor ki; Updated learning rate lt+1. Procedure: 1: Constructing current weight:\nwti = wp̃ · k̃i; 2: Obtaining target using standard forward propagation:\nw∗i = Forward(w t i , xi);\n3: Gradients are computed via standard backward propagation: ∂Ci ∂wti = Backward(wti , w ∗ i ); 4: Updating weight using stochastic gradient descent: wt+1i = UpdateWeight(w t i , ∂Ci ∂wi\n, lt); 5: Updating filter index:\np = argmin r ∥∥∥∥(wt+1i − ŵr · ∑wt+1i ·ŵr∑ ŵ2r )2 ∥∥∥∥ 1 ;\n6: Updating multiplication factor: ki = ∑\nwt+1i ·ŵp∑ ŵ2p\n; 7: Updating learning rate:\nlt+1= UpdateLearningRate(lt, t)."
    }, {
      "heading" : "3 Sparse Windows Generation",
      "text" : "Due to the regression based localization channel, our deep network is less sensitive to the scaling and shift of the input object. Consequently, instead of traditional dense sliding-window paradigm, we employ sparse sliding-window strategy to achieve realtime object detection in traffic scene. Besides, in most object detection systems for traffic scene, camera is mounted on a fixed position (e.g. mounted on the top of a vehicle windshield). Accordingly, objects (e.g. vehicles and pedestrians) produce the predetermined location-specific patterns in images. Thus, the potential appearance of objects in the images can be obtained using the perspective geometry of the given scene. Based on\n10\nthe perspective geometry, a series of sparse windows can be generated according to the scale and shift tolerance of our deep network. Finally, the locations and categories of objects in the given image can be obtained efficiently by performing our deep network on these sparse windows."
    }, {
      "heading" : "3.1 Sparse Sliding-Window Strategy",
      "text" : "We first generate a set of input windows Us using sparse sliding-window strategy. In order to obtain Us which is able to fit the objects in different size, we define a size ratio for each input window. As shown in Fig. 5, letWs denote the size of each input window and Ls denote the size of each object bounding square. The size ratio between object bounding square and the corresponding input window is represented by Rs:\nRs = Ls\nWs . (14)\nWe then generate image pyramid by resizing the given image to different scales. For each resized image in the image pyramid, we assume that each deep network is responsible for objects with Rs ranging from 0.5∼0.7 (objects with Rs beyond this range would be detected in other image scale in the image pyramid). By setting the stride of sliding-windows to 0.3, it can be ensure that each object is completely contained in at least one window. Thus, our approach is able to combine the benefits of regression based method and sliding-window based method. Instead of applying global regression on whole image, our approach performs local regression for bounding box localization. Consequently, the approach has the potential to detected smaller objects compared with global regression based method [14,15]. Besides, the approach maintains a relatively high efficiency thanks to the sparse sliding-window paradigm. Further experiments are presented in section 4."
    }, {
      "heading" : "3.2 Perspective Geometry",
      "text" : "Having obtained the set of input windows Us by sparse sliding-window strategy. We then generate a set of input windows Up according to the perspective geometry. Let (x3D, y3D, z3D) denote a 3D point position in world coordinates, (x2D, y2D) denotes\n11\nthe corresponding 2D point position in pixel coordinates. According to the perspective geometry, they satisfy:\nz x2Dy2D 1  =M  x3D y3D z3D 1  , (15) where M is a camera projection matrix which is the product of an intrinsic matrix and an extrinsic matrix:\nM = fx 0 u0 00 fy v0 0 0 0 1 0 [ R T 0T 1 ] . (16)\nIn equation (16), R is a rotation matrix which is the result of three rotations around the world coordinate axes. As in most object detection systems for traffic scene, the angles of camera rotations are relatively small. Accordingly, the camera projection matrix M can be approximated as:\nM = m11 0 m13 m140 m22 m23 m24 0 0 1 m34  . (17) Then, the equation (15) can be transformed to: x2D = m11 · x3D +m13 · z3D +m14 z3D +m34\ny2D = m22 · y3D +m23 · z3D +m24\nz3D +m34\n. (18)\nIn this work, each input window of our deep network has the same height and width. We use d2D to represent the height or width of the input window located at (x2D, y2D) in pixel coordinates (center aligned), and let d3D denote the corresponding height or width in world coordinates. Then d2D can be formulated as:\nd2D = m11 · d3D z3D +m34 . (19)\nAs we know, the locations of most objects in traffic scene are limited (e.g. vehicles and pedestrians will not appear on sky region). In this work, the location and size of each input window of our deep network is described by a triplet (x2D, y2D, d2D) in pixel coordinates. Let [xmin3D , x max 3D ] and [y min 3D , y max 3D ] respectively represent the location ranges of the corresponding windows in x3D and y3D axes of the world coordinates. By solving the equation set that consists of equation (18) and (19), we can find 4 boundary planes in x2Dy2Dd2D space which jointly limit the locations and sizes of input windows:  d2D = x2D · gx(xmin3D )− gx(xmin3D ) ·m13 d2D = y2D · gy(ymin3D )− gy(ymin3D ) ·m23 d2D = x2D · gx(xmax3D )− gx(xmax3D ) ·m13 d2D = y2D · gy(ymax3D )− gy(ymax3D ) ·m23 , (20)\n12\nwhere:  gx(x3D) = m11 · d3D m11 · x3D −m13 ·m34 +m14 gy(y3D) = m11 · d3D\nm22 · y3D −m23 ·m34 +m24\n. (21)\nAs shown in Fig. 6, in the x2Dy2Dd2D space, possible input windows are distributed in the region of inverted pyramid which is enclosed by 4 boundary planes. We use Up to represent the set of input windows in this region, then the final sparse windows Uf can be obtained by:\nUf = Up ∩ Us. (22)\nAs shown in Fig. 7, we illustrate the final sparse windows in each image of the image pyramid using the TME Motorway dataset [20], which is a challenging and widely-used dataset in vehicle detection. Note that, all of the sparse windows have the same size (48 × 48 in this work), which means that they can be directly utilized as the input of the deep network. In this way, the sparse windows are avoided to be resized and the efficiency of the approach is ensured."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "The experiments in this section mainly focus on vehicle detections in traffic scene, which is an important issue in a wide range of applications such as autonomous driving, autonomous navigation and advance driver assistance systems (ADAS) [21]. In this work, the performance of our approach is evaluated on a broadly used datasets: TME motorway dataset [20], which is designed for vehicle detection and localization in challenging traffic scene with various lighting conditions and complex traffic situations [20].\nIn the following experiments, the output bounding-boxes of our approach are refined by mean shift and non-maximum suppression, and we use an intersection-over-union (IoU) threshold of 0.7 to determine the correctness of detection. All the experiments in this section are performed on a GTX1080 GPU.\n13"
    }, {
      "heading" : "4.1 Generalized Haar filter based weights vs traditional weights",
      "text" : "As our deep network has two output channels: classification channel and localization channel. We respectively use the classification and localization error of training and test as the performance metrics for the following experiments. The classification error of both training and test is defined as:\nErcla = FP + FN\nN , (23)\nwhere N is the total number of training or test samples, FP and FN respectively denotes the number of positive and negative samples which are incorrectly classified.\nFor the localization channel, the error of both training and test is defined as:\nErloc =\nN∑ i=1 ∥∥∥di − d̂i∥∥∥2 4N , (24)\nwhere the vector d = (dx1, dx2, dy1, dy2) represents the output of the localization channel, and d̂ = (dx̂1, dx̂2, dŷ1, dŷ2) is the ground truth location vector.\nFig. 8 illustrates how the errors of classification and localization changed when increasing the number of training iterations. According to this figure, our deep network is able to achieve a high performance when training is convergent. Despite the fact that the deep network with generalized Haar filter based weights (hereafter called G-Haar weights) has a slightly larger training error and needs more training iterations to reach the state of convergence, it is able to produce a less test error in both classification and localization tasks when training is convergent. In other words, the deep network with G-Haar weights has a stronger generalization ability than that with traditional weights.\n14\nThis is owing to the regularization effect of G-Haar weights. Besides, the network employing G-Haar weights with less Nr (the number of selected filters) needs more training iterations and tend to have stronger generalization ability when training iteration is convergent."
    }, {
      "heading" : "4.2 Storage and computing resources consumptions",
      "text" : "We investigate the storage and computing resources consumptions in this subsection. The experiment is also performed on TME motorway dataset [20]. In the experiment, the number of selected filters Nr is 32, and all the weights are stored using singleprecision floating-point format (32 bits). In the deep network (see Fig. 1), the size of conv1∼conv5 x is 3× 3, and the rest of convolution kernels have the size of 1× 1. As the weights of size 1× 1 consume much less resources than that of size 3× 3, we only evaluate the dimensions of conv1∼conv5 x and their effects on resources consumptions in table 1.\nAs shown in table 1 (column of Mem.), the networks using G-Haar weights are able to dramatically reduce memory resources (about 0.8n2 times, n = 3 in this work). This is due to the fact that only a filter index and a multiplication factor are needed to stored for each G-Haar based convolution kernel, which totally consumes 5 bytes of\n15\nmemory space. By contrast, there are n2 weights needed to stored for each traditional convolution kernel of n× n, thus, 4n2 of memory space is required.\nBesides, as multipliers are major computing resource consumed by deep networks, we use the number of multiply operations required for each convolution step to measure the consumption of computing resource for the deep networks. As we know, each traditional convolution kernel of n× n needs n2 multiply operations for each convolution step. Nevertheless, for any convolution kernel using G-Haar weights, each convolution step can be transformed to the form of equation (4). In this way, only one multiply operation is required for each G-Haar convolution step. Accordingly, as shown in table 1 (column of Mul./St.), the computing resource consumption of the deep networks using G-Haar weights is only 1/n2 of that using traditional weights (n = 3 in this experiment).\nIn addition, power consumption is directly influenced by storage and computing resources utilizations [22]. As our deep network with G-Haar weights can markedly reduce storage and computing resources (including memory accesses), it would have a great increase in power-efficiency. This merit is especially meaningful when the deep network implements on embedded systems or mobile devices (such as FPGA and ARM), which are quite sensitive to power consumption."
    }, {
      "heading" : "4.3 Comparing with state-of-the-art methods",
      "text" : "For further analysis, our proposed approach is evaluated via comparing with some state-of-the-art methods. In the following experiments, “Ours” represents our complete method that employs G-Haar weights and the stage of sparse windows generation. “Ours (Traditional Weights)” and “Ours (Without Sparse Windows)” denote the variants of our approach that utilize traditional weights instead of G-Haar weights and without the stage of sparse windows generation, respectively. The experiment settings of our approaches is the same as that in the row 7 of table 1.\nAs shown in Fig. 9, thanks to the strong representation of deep convolution neural networks, regression based deep networks (such as YOLO [14], SSD500 [15] and our approach) tend to perform better than traditional hand-craft methods. At the same time, as exhaustive sliding windows can be avoided when using regression based deep\n16\nnetworks and convolution can be efficiently computed in GPU, as shown in table 2, regression based deep networks are able to achieve real-time performance in object detection task in traffic scene. Besides, table 2 also demonstrates that the stage of sparse windows generation is able to reduce unnecessary computation and improve the efficiency dramatically.\nMoreover, quantitative evaluation in Fig. 9 and qualitative results in Fig. 10 indicate that our proposed method is able to achieve better performance on small object detection compared with other regression based deep networks such as YOLO [14] and SSD500 [15] which utilize global regression strategy. SSD500 [15] employs multi-scale feature maps to detect objects in different scales and it perform better than YOLO [14]. However, its performance on small objects detection is still unsatisfactory due to the fact that global regression, that is regressing the bounding-box of each object from the whole image, is a much more difficult task compared with our local regression strategy. Beside, input images (e.g. 1024 × 768 in TME motorway dataset [20]) have to be resized to 500 × 500 in SSD500 [15], which leads to the lacking of resolution of small objects such as vehicles and pedestrians that are far away from camera. Our proposed method decomposes the global regression task into several easier local regression tasks, and detect multi-scale objects from image pyramid. In this way, the resolution of small objects is ensured and thus better performance can be achieve.\n17"
    }, {
      "heading" : "5 Conclusions and Future Works",
      "text" : "In this paper, we presents a novel network system for object detection tasks in traffic scene. In this system, we introduce a local regression strategy for accurate objects detection. Compared with traditional global regression based object detection, the local regression task is easier to handled without the support of the complex or large-scale networks. According to this fact, we handle the local regression tasks by using several tiny deep networks which simultaneously output the bounding boxes, categories and confidence scores of detected objects. In order to satisfy the storage, power and computing source requirements of the platforms for traffic scene applications, the weights of the deep networks are constrained to the form of generalized Haar filter in training phase. Furthermore, to achieve the real-time performance, we introduce the strategy of sparse windows generation to reduce the runtime of our system.\nIn the experiments, we first evaluate the performance and generalization ability of our generalized Haar filter based weights by comparison with traditional weights. Then the consumptions of storage and computing resources are evaluated. Finally, the effectiveness and efficiency of our approach are validated in comparison with some recently published state-of-the-art methods. Experimental results demonstrate that the proposed approach is proved to be efficient, robust and source saving in challenging traffic scene.\n18\nAs the proposed approach is suitable for the platforms which have strict limitations on memory, power and computing sources, our future works will focus on implementing the proposed approach (path forward) to FPGA (Field Programmable Gate Array) for advance driver assistance system (ADAS). Owing to the generalized Haar Filter based weights, only one multiply operation is required for each convolution step. This make it possible to construct more parallel pipelines in FPGA where the number of multipliers is limited. Moreover, each local regression task is an independent computation. Thus, all of the local regression tasks can run in parallel in FPGA. In this way, the system can achieve real-time response without extra effort."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by the National Natural Science Foundation of China (Grant No. 61473303)."
    } ],
    "references" : [ {
      "title" : "Vision sensor-based road detection for field robot navigation, Sensors",
      "author" : [ "K. Lu", "J. Li", "X. An", "H. He" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Robust real-time face detection, International Journal of Computer Vision (IJCV",
      "author" : [ "P. Viola", "M.J. Jones" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "Object detection with discriminatively trained part-based models, IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "author" : [ "P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks, Advances in Neural Information",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "End-to-end text recognition with convolutional neural networks",
      "author" : [ "T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng" ],
      "venue" : "in: International Conference on Pattern Recognition (ICPR),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "in: IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Spatial pyramid pooling in deep convolutional networks for visual recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Selective search for object recognition",
      "author" : [ "J. Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders" ],
      "venue" : "in: International Journal of Computer Vision (IJCV),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, Fiber",
      "author" : [ "S. Han", "H. Mao", "W.J. Dally" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Real-time compressive tracking",
      "author" : [ "K. Zhang", "L. Zhang", "M.-H. Yang" ],
      "venue" : "in: European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Visual tracking with online multiple instance learning",
      "author" : [ "P. Dollár", "Z. Tu", "H. Tao", "S. Belongie" ],
      "venue" : "in: IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "An efficient method for solving nonlinear unconstrained min-max problem, Master’s thesis, Xi’An University of science and technology, Xi’An",
      "author" : [ "Z. Yu" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "A system for real-time detection and tracking of vehicles from a single car-mounted camera",
      "author" : [ "C. Caraffi", "T. Vojı́ř", "J. Trefný", "J. Šochman", "J. Matas" ],
      "venue" : "in: IEEE Intelligent Transportation Systems Conference,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Monocular precrash vehicle detection: features and classifiers",
      "author" : [ "Z. Sun", "G. Bebis", "R. Miller" ],
      "venue" : "IEEE Transactions on Image Processing (TIP)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Computing’s energy problem (and what we can do about it)",
      "author" : [ "M. Horowitz" ],
      "venue" : "in: IEEE International Solid State Circuits Conference,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Signifredi, A coarse-to-fine vehicle detector running in real-time",
      "author" : [ "L. Castangia", "P. Grisleri", "P. Medici", "A.A. Prioletti" ],
      "venue" : "in: IEEE Intelligent Transportation Systems Conference,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Moreover, vision sensor has the ability to capture rich information from traffic scene (such as luminance, color and texture) [1], which is beneficial for object detection.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Convolutional Neural Network (CNN) [6] is one of the most popular forms of deep networks.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "By using the strategies of local receptive fields, weight sharing and spatial pooling [7], CNN has made a breakthrough in computer vision and image processing.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "For instance, AlexNet [6], which is a type of CNN, has made a startling achievement in the competition of ILSVRC-2012 [2] and demonstrated its superiority in image classification.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "Firstly, CNN has a multitude of convolutions that have to be calculated, it would be rather inefficient in object detection if using traditional dense sliding windows paradigm [4,5].",
      "startOffset" : 176,
      "endOffset" : 181
    }, {
      "referenceID" : 2,
      "context" : "Firstly, CNN has a multitude of convolutions that have to be calculated, it would be rather inefficient in object detection if using traditional dense sliding windows paradigm [4,5].",
      "startOffset" : 176,
      "endOffset" : 181
    }, {
      "referenceID" : 5,
      "context" : "introduced the framework of region proposal based CNN (called R-CNN) [10] and successfully applied it to object detection.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "The main idea of region proposal based CNN is performing CNNs on candidate bounding-boxes (called “proposals”) which have potential to contain objects [10].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "This method combines R-CNN with SPPnet [11].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "However, in both R-CNN and Fast R-CNN, the proposals are generated by Selective Search [12], which is quite inefficient and thus limits the detection speed of these methods.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "To overcome this limitation, Faster R-CNN [13] proposed to used Region Proposal Networks (RPNs) to generate proposals instead of Selective Search [12].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "Despite the fact that deep neural networks have achieved state-of-the-art performance in object detection, they consume considerable storage, computing resources and power [16].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "Haar-like filters have been successfully applied in object detection owing to their strong representation and high efficiency [4,17].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "Haar-like filters have been successfully applied in object detection owing to their strong representation and high efficiency [4,17].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 10,
      "context" : "Instead of using a fixed number of rectangles and configuration types, generalized Haar filters are based on arbitrary configurations and number of rectangles [18].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "Unlike original generalized Haar filters with arbitrary configuration and number of rectangles [18], in our work, the configuration and number of rectangles are obtained in a data-driven way.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "Unlike approaches in [17,4,18] where Haar filters are operated based on integral images.",
      "startOffset" : 21,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Unlike approaches in [17,4,18] where Haar filters are operated based on integral images.",
      "startOffset" : 21,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "Unlike approaches in [17,4,18] where Haar filters are operated based on integral images.",
      "startOffset" : 21,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "According to [19], a maximum function for Z = {z1, .",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "The continuity and differentiability have been proofed by [19].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "7, we illustrate the final sparse windows in each image of the image pyramid using the TME Motorway dataset [20], which is a challenging and widely-used dataset in vehicle detection.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "The experiments in this section mainly focus on vehicle detections in traffic scene, which is an important issue in a wide range of applications such as autonomous driving, autonomous navigation and advance driver assistance systems (ADAS) [21].",
      "startOffset" : 240,
      "endOffset" : 244
    }, {
      "referenceID" : 12,
      "context" : "In this work, the performance of our approach is evaluated on a broadly used datasets: TME motorway dataset [20], which is designed for vehicle detection and localization in challenging traffic scene with various lighting conditions and complex traffic situations [20].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "In this work, the performance of our approach is evaluated on a broadly used datasets: TME motorway dataset [20], which is designed for vehicle detection and localization in challenging traffic scene with various lighting conditions and complex traffic situations [20].",
      "startOffset" : 264,
      "endOffset" : 268
    }, {
      "referenceID" : 12,
      "context" : "The experiment is also performed on TME motorway dataset [20].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "In addition, power consumption is directly influenced by storage and computing resources utilizations [22].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "Caraffi [20] Castangia [23] YOLO [14] SSD500 [15] Ours (Traditional Weights) Ours",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 15,
      "context" : "Caraffi [20] Castangia [23] YOLO [14] SSD500 [15] Ours (Traditional Weights) Ours",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 12,
      "context" : "1024 × 768 in TME motorway dataset [20]) have to be resized to 500 × 500 in SSD500 [15], which leads to the lacking of resolution of small objects such as vehicles and pedestrians that are far away from camera.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "Caraffi [20] 0.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 15,
      "context" : "1s Castangia [23] 0.",
      "startOffset" : 13,
      "endOffset" : 17
    } ],
    "year" : 2016,
    "abstractText" : "Vision-based object detection is one of the fundamental functions in numerous traffic scene applications such as self-driving vehicle systems and advance driver assistance systems (ADAS). However, it is also a challenging task due to the diversity of traffic scene and the storage, power and computing source limitations of the platforms for traffic scene applications. This paper presents a generalized Haar filter based deep network which is suitable for the object detection tasks in traffic scene. In this approach, we first decompose a object detection task into several easier local regression tasks. Then, we handle the local regression tasks by using several tiny deep networks which simultaneously output the bounding boxes, categories and confidence scores of detected objects. To reduce the consumption of storage and computing resources, the weights of the deep networks are constrained to the form of generalized Haar filter in training phase. Additionally, we introduce the strategy of sparse windows generation to improve the efficiency of the algorithm. Finally, we perform several experiments to validate the performance of our proposed approach. Experimental results demonstrate that the proposed approach is both efficient and effective in traffic scene compared with the state-of-the-art.",
    "creator" : "LaTeX with hyperref package"
  }
}