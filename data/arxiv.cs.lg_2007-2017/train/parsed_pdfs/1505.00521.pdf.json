{
  "name" : "1505.00521.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reinforcement Learning Neural Turing Machines",
    "authors" : [ "Wojciech Zaremba" ],
    "emails" : [ "woj.zaremba@gmail.com", "ilyasu@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 5.\n00 52\n1v 1\n[ cs\n.L G\n] 4\nM ay\nThe expressive power of a machine learning model is closely related to the number of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can perform a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation.\nWhile differentiable memory is relatively easy to implement and train, it necessitates accessing the entire memory content at each computational step. This makes it difficult to implement a fast NTM. In this work, we use the Reinforce algorithm to learn where to access the memory, while using backpropagation to learn what to write to the memory. We call this model the RL-NTM. Reinforce allows our model to access a constant number of memory cells at each computational step, so its implementation can be faster. The RL-NTM is the first model that can, in principle, learn programs of unbounded running time. We successfully trained the RL-NTM to solve a number of algorithmic tasks that are simpler than the ones solvable by the fully differentiable NTM.\nAs the RL-NTM is a fairly intricate model, we needed a method for verifying the correctness of our implementation. To do so, we developed a simple technique for numerically checking arbitrary implementations of models that use Reinforce, which may be of independent interest."
    }, {
      "heading" : "1 Introduction",
      "text" : "Different machine learning models can perform different numbers of sequential computational steps. For instance, a linear model can perform only one sequential computational step, while Deep Neural Networks (DNNs) can perform a larger number of sequential computational steps (e.g., 20). To be useful, the computational steps must be learned from input-output examples. Human beings can solve highly complex perception problems in a fraction of a second using very slow neurons, so it is conceivable that the sequential computational steps (each highly parallel) performed by a DNN are sufficient for excellent performance on perception tasks. This argument has appeared at least as early as 1982 [6] and the success of DNNs on perception tasks suggests that it may be correct.\nA model that can perform a very large number of sequential computational steps and that has an effective learning algorithm would be immensely powerful [13]. There has been some empirical work in this direction (notably in program induction and in genetic programming [3]) but the resulting systems do not scale to large problems. The most exciting recent work in that direction is Graves et\n1Work done while the author was at Google. 2Both authors contributed equally to this work.\nal. [8]’s Neural Turing Machine (NTM), a computationally universal model that can learn to solve simple algorithmic problems from input-output examples alone.\nGraves et al. [8] used interpolation to make the NTM fully differentiable and therefore trainable with backpropagation. In particular, its memory addressing is differentiable, so the NTM must access its entire memory content at each computational step, which is slow if the memory is large. This is a significant drawback since slow models cannot scale to large difficult problems.\nThe goal of this work is to use the Reinforce algorithm [16] to train NTMs. Using Reinforcement Learning for training NTMs is attractive since it requires the model to only access a constant number the memory’s cells at each computational step, potentially allowing for very fast implementations. Our concrete proposal is to use Reinforce to learn where to access the memory (and the input and the output), while using backpropagation to determine what to write to the memory (and the output). This model was inspired by the visual attention model of Mnih et al. [11]. We call it the RL-NTM.\nWe evaluate the RL-NTM on a number of simple algorithmic tasks. The RL-NTM succeeded on problems such as copying an input several times (the “repeat copy” task from Graves et al. [8]), reversing a sequence, and a few more tasks of comparable complexity. We encountered some difficulties training our initial formulation of the RL-NTM, so we developed a simple architectural modification that made the problems easier to solve and the memory easier to use. We discuss this point in more detail in section 4.3.\nFinally, we found it non-trivial to correctly implement the RL-NTM due the large number of interacting components. To address this problem, we developed a very simple procedure for numerically checking the gradients of any reasonable implementation of the Reinforce algorithm. The procedure may be of independent interest."
    }, {
      "heading" : "2 The Neural Turing Machines and Related Models",
      "text" : "The Neural Turing Machine [8] is an ambitious, computationally universal model that can be trained (or “automatically programmed”) with the backpropagation algorithm using only input-output examples. The key idea of Graves et al. [8] is to use interpolation to make the model differentiable. Simple Turing Machine-like models usually consist of discrete controllers that read and write to discrete addresses in a large memory. The NTM replaces each of the discrete controller’s actions with a distribution over actions, and replaces its output with the superposition of the possible outputs, weighted by their probabilities. So while the original discrete action was not differentiable, the new action is a linear (and hence differentiable) function of the input probabilities. This makes it possible to train NTMs with backpropagation.\nIn more detail, the NTM is an LSTM [9] controller that has an external memory module. The controller decides on where to access the memory and on what to write to it. Memory access is implemented in such a way that a memory address is represented with a distribution over the all possible memory addresses.\nThere have been several other models that are related to the NTM. A predecessor of the NTM which used a similar form of differentiable attention achieved compelling results on Machine Translation [2] and speech recognition [5]. Earlier, Graves [7] used a more restricted form of differentiable attention for handwritten text synthesis which, to the best of our knowledge, is the first differentiable attention model.\nSubsequent work used the idea of interpolation in order to train a stack augmented RNN, which is essentially an NTM but with a much simpler memory addressing mechanism [10]. The Memory Network [15] is another model with an external explicit memory, but its learning algorithm does not infer the memory access pattern since it is provided with it. Sukhbaatar et al. [14] addressed this problem using differentiable attention within the Memory Network framework.\nThe use of Reinforce for visual attention models was pioneered by Mnih et al. [11], and our model uses a very similar formulation in order to learn to control the memory address. There have since been a number of papers on visual attention that have used both Reinforce and differentiable attention [1, 17]."
    }, {
      "heading" : "3 The Reinforce Algorithm",
      "text" : "The Reinforce algorithm [16] is the simplest Reinforcement learning algorithm. It takes actions according to its action distribution and observes their reward. If the reward is greater than average, then Reinforce increases their reward. While the Reinforce algorithm is not particularly efficient, it has a simple mathematical formulation that can be obtained by differentiating a cost function.\nSuppose that we have an action space, a ∈ A, a parameterized distribution pθ(a) over actions, and a reward function r(a). Then the Reinforce objective is given by\nJ(θ) = ∑\na∈A\npθ(a)r(a) (1)\nand its derivative is ∇J(θ) = ∑\na∈A\npθ(a)∇ log pθ(a)(r(a) − b) (2)\nThe arbitrary parameter b is called the reward baseline, and it is justified by the identity ∑\na pθ(a)∇ log pθ(a) = 0. The coefficient b is important because its choice affects the variance of the gradient estimator ∇ log pθ(a)(r(a) − b), which is lowest when b is equal to E[‖∇ log pθ(a)‖2r(a)]\nE[‖∇ log pθ(a)‖2] , although it is common to use E[r] since it is easier to estimate.\nLet xba denote (xa, . . . , xb). In this paper we are especially interested in the episodic case where we repeatedly take actions in sequence until the episode is complete — in the case of the RL-NTM, we take an action at each computational step, of which there is a limited number. Thus, we have a trainable distribution over actions πθ(at|st1, a t−1 1\n) and a fixed but possibly unknown distribution over the world’s state dynamics p(st|a t−1 1 , st−1 1\n). In this setting, it is possible to reduce the variance of gradient estimates of action distributions near the end of an episode [12]. Letting Pθ(a, s) denote the implied joint distribution over sequences of actions and states, we get the following, where the expectations are taken over (aT1 , s T 1 ):\n∇J(θ) = E\n[\nT ∑\nt=1\nrt∇ logPθ(a T 1 , s T 1 )\n]\n= E\n[\nT ∑\nt=1\nrt\nT ∑\nτ=1\n∇ log πθ(aτ |s τ 1 , a τ−1 1 )\n]\n= E\n[\nT ∑\nτ=1\nT ∑\nt=1\nrt∇ log πθ(aτ |s τ 1 , a τ−1 1 )\n]\n≡?\nE\n[\nT ∑\nτ=1\nT ∑\nt=τ\nrt∇ log πθ(aτ |s τ 1 , a τ−1 1 )\n]\n= E\n[\nT ∑\nτ=1\nRτ∇ log πθ(aτ |s τ 1 , a τ−1 1 )\n]\nwhere Rτ ≡ ∑T\nt=τ rt is the cumulative future reward from timestep τ onward. The nontrivial equation marked by ≡? is obtained by eliminating the terms EaT\n1 ,sT 1\n[\nrt∇ log πθ(aτ |sτ1 , a τ−1 1 ) ]\nwhenever τ > t, which are zero because a future action aτ cannot possibly influence a past reward rt. It is equivalent to a separate application of the Reinforce algorithm at each timestep where the total reward is the future reward, which explains is why gradient estimates of action distributions near the end have lower variance. In this setting, we are also permitted a per-timestep baseline:\n∇J(θ) = EaT 1 ,sT 1\n[\nT ∑\nτ=1\n(Rτ − bτ )∇ log πθ(aτ |s τ 1 , a τ−1 1 )\n]\n(3)\nHere bτ is a function of sτ1 and a τ−1 1 . The per-timestep bias bτ is best computed with a separate neural network that will have sτ1 and a τ−1 1 as inputs."
    }, {
      "heading" : "4 The RL-NTM",
      "text" : "In this section we describe the RL-NTM and explain the precise manner in which it uses Reinforce.\nThe RL-NTM has a one-dimensional input tape, a one-dimensional memory, and a one-dimensional output tape. It has pointers to a location on the input tape, a location on the memory tape, and a location on the output tape. The RL-NTM can move its input and memory tape pointers in any direction, but it can move its position on the output tape only in the forward direction; furthermore, each time it does so, it must predict the desired output by emitting a distribution over the possible outcomes, and suffer the log loss of predicting it.\nAt the core of the RL-NTM is an LSTM which receives the following inputs at each timestep:\n1. The value of each input in a window surrounding the current position on the input tape.\n2. The value of each address in a window surrounding the current address in the memory.\n3. A feature vector listing all actions taken in the previous time step — the actions are stochastic, and it can sometimes be impossible to determine the identity of the chosen action from the observations alone.\nThe window surrounding a position is a subsequence of length 5 centered at the current position.\nAt each step, the LSTM produces the following outputs:\n1. A distribution over the possible moves for the input tape pointer (which are -1, 0, +1).\n2. A distribution over the possible moves for the memory pointer (which are -1, 0, +1).\n3. A distribution over the binary decision of whether or not to make a prediction.\n4. A new value for the memory cell at the current address. The model is required to output an additive contribution to the memory cell, as well as a forget gate-style vector. Thus each memory address is an LSTM cell.\n5. A prediction of the next target output, which is a distribution over the set of possible symbols (softmax). This prediction is needed only if the binary decision at item 3 has decided make a prediction — in which case the log probability of the desired output under this prediction is added to the RL-NTM’s total reward on this sequence. Otherwise the prediction is discarded.\nThe first three output distributions in the above list are trained with Reinforce, while the last two outputs are trained with standard backpropagation. The RL-NTM is setup to maximize the total log probability of the desired outputs, and Reinforce treats this log probability as its reward.\nThe RL-NTM is allowed to choose when to predict the desired output. Since the RL-NTM has a fixed number of computational steps, it is important to avoid the situation where the RL-NTM chooses to make no prediction and therefore experiences no negative reward (the reward is log\nprobability, which is always negative). To prevent it from happening, we forced the RL-NTM to predict the next desired output (and experience the associated negative reward) whenever the number of remaining desired outputs is equal to the number of remaining computational steps."
    }, {
      "heading" : "4.1 Baseline Networks",
      "text" : "The Reinforce algorithm works much better whenever it has accurate baselines (see section 3 for a definition). The reward baseline is computed using a separate LSTM as follows:\n1. Run the baseline LSTM over the entire input tape to produce a hidden state that summarizes the input.\n2. Continue running the baseline LSTM in tandem with the controller LSTM, so that the baseline LSTM receives precisely the same inputs as the controller LSTM, and output a baseline bτ at each timestep τ .\nSee figure 2. The baseline LSTM is trained to minimize ∑\nτ (Rτ − bτ ) 2.\nWe found it important to first have the baseline LSTM go over the entire input before computing the baselines bτ . It is especially beneficial whenever there is considerable variation in the difficulty of the examples. For example, if the baseline LSTM can recognize that the current instance is unusually difficult, it can output a large negative value for bτ=1 in anticipation of a large and a negative R1. In general, it is cheap and therefore worthwhile to provide the baseline network with all of the available information, even if this information would not be available at test time, because the baseline network is not needed at test time."
    }, {
      "heading" : "4.2 Curriculum Learning",
      "text" : "DNNs are successful because they are easy to optimize while NTMs are difficult to optimize. Thus, it is plausible that curriculum learning [4], which has not been helpful for DNNs because their training objectives are too easy, will be useful for NTMs since their objectives are harder. In our experiments, we used curriculum learning whose details were borrowed from Zaremba and Sutskever [18]. For each task, we manually define a sequence of subtasks of increasing difficulty, where the difficulty of a problem instance is often measured by its size or length. During training, we maintain a distribution over task difficulties; as the performance of the RL-NTM exceeds a threshold, we shift our distribution so that it focuses on more difficult problem instances. But, and this is critical, the distribution over difficulties always maintains some non-negligible mass over the hardest difficulty levels [18].\nIn more detail, the distribution over task difficulties is indexed with an integer D, and is defined by the following procedure:\n• With probability 10%: pick d uniformly at random from the possible task difficulties.\n• With probability 25%: select d uniformly from [1, D + e], where e is a sample from a geometric distribution with a success probability of 1/2.\n• With probability 65%: select d to be D + e.\nWe then generate a training sample from a task of difficulty d.\nWhenever the average zero-one-loss (normalized by the length of the target sequence) of our RLNTM decreases below 0.2, we increment D by 1. We kept doing so until D reaches its maximal allowable value. Finally, we enforced a refractory period to ensure that successive increments of D are separated by at least 100 parameter updates, since we encountered situations where D increased in rapid succession but then learning failed.\nWhile we did not tune the coefficients in the curriculum learning setup, we experimentally verified that the first item is important and that removing it makes the curriculum much less effective. We also verified that our tasks were completely unsolvable (in an all-or-nothing sense) for all but the shortest sequences when we did not use a curriculum."
    }, {
      "heading" : "4.3 The Modified RL-NTM",
      "text" : "All the tasks that we consider involve rearranging the input symbols in some way. For example, a typical task is to reverse a sequence (section 5.1 lists the tasks). For such tasks, the controller would benefit from a built-in mechanism for directly copying an appropriate input to memory and to the output. Such a mechanism would free the LSTM controller\nfrom remembering the input symbol in its control variables (“registers”), and would shorten the backpropagation paths and therefore make learning easier. We implemented this mechanism by adding the input to the memory and the output, and also adding the memory to the output and to the adjacent memories (figure 3), while modulating these additive contribution by a dynamic scalar (sigmoid) which is computed from the controller’s state. This way, the controller can decide to effectively not add the current input to the output at a given timestep. Unfortunately the necessity of this architectural modification is a drawback of our implementation, since it is not domain independent and would therefore not im-\nprove the performance of the RL-NTM on many tasks of interest. Nonetheless, we report all of our experiments with the Modified RL-NTM since it made our tasks solvable."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Tasks",
      "text" : "We now describe the tasks used in the experiments. Let each xt be a uniformly random symbol chosen from {1, . . . , 40}.\n1. DuplicatedInput. A generic input has the form x1x1x1x2x2x2x3 . . . xL−1xLxLxL∅ while the desired output is x1x2x3 . . . xL∅. Thus each input symbol is replicated three times, so the RL-NTM must emit every third input symbol. The length of the input sequence is variable and is allowed to change. The input sequence and the desired output both terminate with a special end-of-sequence symbol ∅.\n2. Reverse. A generic input is x1x2 . . . xL−1xL∅ and the desired output is xLxL−1 . . . x2x1∅.\n3. RepeatCopy. A generic input is mx1x2x3 . . . xL∅ and the desired output is x1x2 . . . xLx1 . . . xLx1 . . . xL∅, where the number of copies is given by m. Thus the goal is to copy the input m times, where m can be only 2 or 3.\n4. ForwardReverse. The task is identical to Reserve, but the RL-NTM is only allowed to move its input tape pointer forward. It means that a perfect solution must use the NTM’s external memory.\nA task instance is specified by a maximal value of L (call it Lmax ) and a maximal number of computational steps that are provided to the RL-NTM. The difficulty levels of the curriculum are usually determined by gradually increasing L, although the difficulty of a RepeatCopy task is determined by (m − 1) · Lmax + L: in other words, we consider all instances where m = 2 to be easier than m = 3, although other choices would have likely been equally effective."
    }, {
      "heading" : "5.2 Training Details",
      "text" : "The details of our training procedure are as follows:\n1. We trained our model using SGD with a fixed learning rate of 0.05 and a fixed momentum of 0.9. We used a batch of size 200, which we found to work better than smaller batch sizes (such as 50 or 20). We normalized the gradient by batch size but not by sequence length.\n2. We independently clip the norm of the gradients w.r.t. the RL-NTM parameters to 5, and the gradient w.r.t. the baseline network to 2. The gradients of the RL-NTM and the gradients of the baseline LSTM are rescaled separately.\n3. We initialize the RL-NTM controller and the baseline model using a spherical Gaussian whose standard deviation is 0.1.\n4. All our tasks would either solve the problem in under than 20,000 parameter updates or would fail to solve the problem. The easier problems required many fewer parameter updates (e.g., 1000).\n5. We used an inverse temperature of 0.01 for the different action distributions. Doing so reduced the effective learning rate of the Reinforce derivatives.\n6. The LSTM controller has 35 memory cells and the number of memory addresses in the memory module is 30. While this memory is small it has been large enough for our experiments. In addition, given that our implementation runs in time independent of the size of our memory and the RL-NTM changes its memory addresses with increments of 1 and -1, we would have obtained precisely the same results with a vastly greater memory.\n7. Both the initial memory state and the controller’s initial hidden states were set to the zero vector.\n5.3 Experimental Results\nE0fC5703Bg+ | +gB3075Cf0E1 | E | _ | * 0 | _ | * f | _ | * C | _ | * 5 | _ | * 7 | _ | * 0 | _ | * 3 | _ | * B | _ | * g | _ | * + | + | * + | g | * + | B | *\n| 3 | * | 0 | * | 7 | * | 5 | * | _ | * | C | * | _ | * | f | * | 0 | * | E | *\nFigure 4: The RL-NTM solving ForwardReverse using its external memory. The vertical depicts execution time. The first row shows the input (first column) and the desired output (second column). The followings row show the input pointer, output pointer, and memory pointer (with the ∗ symbol) at each step of the RL-NTM’s execution. Note that we represent the set {1, . . . , 30} with 30 distinct symbols.\nThe number of computational steps and the length of the maximal input for each task is given in the table below:\nTask # compt. steps Lmax DuplicatedInput 60 20\nRepeatCopy 50 10 Reverse 30 15 ForwardReverse 30 15\nWith these settings, the RL-NTM reached perfect performance on the tasks in section 5.1 with the learning algorithm from the previous section. We present a number of sample execution traces of the RL-NTM in the appendix in figures 5-9.\nThe ForwardReverse task is particularly interesting. Figure 5 (appendix) shows an execution trace of an RL-NTM which reverses the input sequence. In order to solve the problem, the RL-NTM moves towards the end of the sequence without making any predictions, and then moves over the input sequence in reverse, while emitting the correct outputs one at a\ntime. However, doing so is clearly impossible when the input tape is allowed to only move forward. Under these circumstances, the RL-NTM learned to use its external memory: it wrote the input sequence into its memory, and then made use of its memory when reversing the sequence. See Figure 4. Sadly, we have not been able to solve the RepeatCopy task when we forced the input tape pointer to only move forward.\nWe have also experimented with a number of additional tasks but with less empirical success. Tasks we found to be too difficult include sorting, long integer addition (in base 3 for simplicity), and RepeatCopy when the input tape is forced to only move forward. While we were able to achieve reasonable performance on the sorting task, the RL-NTM learned an ad-hoc algorithm and made excessive use of its controller memory in order to sort the sequence. Thus, our results and the results of Graves et al. [8] suggest that a differentiable memory may result in models that are easier to train. Nonetheless, it is still possible that a more careful implementation of the RL-NTM would succeed in solving the harder problems.\nEmpirically, we found all the components of the RL-NTM essential to successfully solving these problems. We were completely unable to solve RepeatCopy, Reverse, and Forward reverse with the unmodified RL-NTM, and we were also unable to solve any of these problems at all without a curriculum (except for small values Lmax , such as 5). By a failure, we mean that the model converges to an incorrect memory access pattern (see fig. 9 in the appendix for an example) which makes it impossible to perfectly solve the problem.\nFinally, the ForwardReverse task had the high success rate of at least 80%, judging by five successful runs with the reported hyperparameters. However, this problem was found to be highly sensitive to the choice of the hyperparameters, and finding the good hyperparameter setting required a substantial effort."
    }, {
      "heading" : "6 Gradient Checking",
      "text" : "An independent contribution of this work is a simple strategy for implementing a gradient checker for Reinforce. The RL-NTM is complex, so we needed a way of verifying the correctness of our implementation. We discovered a technique that makes it possible to easily implement a gradient checker for nearly any model that uses Reinforce.\nThe strategy is to create a sufficiently small problem instance where the set of all possible action sequences is of a manageable size (such as 1000; call this size N ). Then, we would run the RLNTM on every action sequence, multiplying the loss and the gradient of each such sequence by its probability. To accomplish this, we precompute a sequence of actions and overload the sampling function to (a) produce said sequence of precomputed actions, and (b) maintain a product of the probabilities of the forced actions. As a result, a forward pass provides us with the probability of the complete action sequence and the model’s loss on this action sequence, while the backward pass gives us its gradient. Importantly, the above change requires that we only overload the sampling function, which makes it easy to do from an engineering point of view.\nThis information can be used to compute the exact expected reward and its exact gradient by iterating over the set of all possible action sequences and taking an appropriate weighted sum. While this approach will be successful, it will require many forward passes through the model with a batch of size 1, which is not efficient due to the model’s overhead. We found it much more practical to assemble the set of all possible action sequences into a single minibatch, and to obtain the exact expected loss and the exact expected gradient using only a single forward and a backward pass.\nOur approach also applies to models with continuous action spaces. To do so, we commit to very small set of discrete actions, and condition the model’s continuous action distributions on this discreet subset. Once we do so, our original gradient checking technique becomes applicable.\nFinally, it may seem that computing the expectation over the set of all possible action sequences is unnecessary, since it should suffice to ensure that the symbolic gradient of an individual action sequence matches the corresponding numerical gradient. Yet to our surprise, we found that the expected symbolic gradient was equal to the numerical gradient of the expected reward, even though they were different on the individual action sequences. This happens because we removed the terms\nrτ∇ log π(at|st1, a t−1 1 ) for τ > t from the gradient estimate since they are zero in expectation. But they are not zero for individual action sequences. If we add these terms back to the gradient, the symbolic gradient matches the numerical gradient for the individual action sequences."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have shown that the Reinforce algorithm is capable of training an NTM-style model to solve very simple algorithmic problems. While the Reinforce algorithm is very general and is easily applicable to a wide range of problems, it seems that learning memory access patterns with Reinforce is difficult. We currently believe that a differentiable approach to memory addressing will likely yield better results in the near term. And while the Reinforce algorithm could still be useful for training NTM-style models, it would have to be used in a manner different from the one in this paper. Our gradient checking procedure for Reinforce can be applied to a wide variety of implementations. We also found it extremely useful: without it, we had no way of being sure that our gradient was correct, which made debugging and tuning much more difficult."
    }, {
      "heading" : "A. RL-NTM Execution Traces",
      "text" : "We present several execution traces of the RL-NTM.\nExplanation of the Figures: Each figure shows execution traces of the trained RL-NTM on each of the tasks. The first column corresponds to the input tape and the second column corresponds to the output tape. The first row shows the input tape and the desired output, while each subsequent row shows the RL-NTM’s position on the input tape and its prediction for the output tape. In these examples, the RL-NTM solved each task perfectly, so the predictions made in the output tape perfectly match the desired outputs listed in the first row."
    } ],
    "references" : [ {
      "title" : "Multiple object recognition with visual attention",
      "author" : [ "Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1412.7755,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Genetic programming: an introduction, volume 1",
      "author" : [ "Wolfgang Banzhaf", "Peter Nordin", "Robert E Keller", "Frank D Francone" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of the 26th annual international conference on machine learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "End-to-end continuous speech recognition using attention-based recurrent nn: First results",
      "author" : [ "Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.1602,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Connectionist models and their properties",
      "author" : [ "Jerome A Feldman", "Dana H Ballard" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1982
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka" ],
      "venue" : "arXiv preprint arXiv:1410.5401,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "Inferring algorithmic patterns with stack-augmented recurrent nets",
      "author" : [ "Armand Joulin", "Tomas Mikolov" ],
      "venue" : "arXiv preprint arXiv:1503.01007,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Volodymyr Mnih", "Nicolas Heess", "Alex Graves" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Policy gradient methods for robotics",
      "author" : [ "Jan Peters", "Stefan Schaal" ],
      "venue" : "In Intelligent Robots and Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "A formal theory of inductive inference",
      "author" : [ "Ray J Solomonoff" ],
      "venue" : "i. Information and control,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1964
    }, {
      "title" : "Weakly supervised memory networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1503.08895,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1992
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "This argument has appeared at least as early as 1982 [6] and the success of DNNs on perception tasks suggests that it may be correct.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "A model that can perform a very large number of sequential computational steps and that has an effective learning algorithm would be immensely powerful [13].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "There has been some empirical work in this direction (notably in program induction and in genetic programming [3]) but the resulting systems do not scale to large problems.",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "[8]’s Neural Turing Machine (NTM), a computationally universal model that can learn to solve simple algorithmic problems from input-output examples alone.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] used interpolation to make the NTM fully differentiable and therefore trainable with backpropagation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "The goal of this work is to use the Reinforce algorithm [16] to train NTMs.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : "[11].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[8]), reversing a sequence, and a few more tasks of comparable complexity.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "The Neural Turing Machine [8] is an ambitious, computationally universal model that can be trained (or “automatically programmed”) with the backpropagation algorithm using only input-output examples.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "[8] is to use interpolation to make the model differentiable.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "In more detail, the NTM is an LSTM [9] controller that has an external memory module.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "A predecessor of the NTM which used a similar form of differentiable attention achieved compelling results on Machine Translation [2] and speech recognition [5].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "A predecessor of the NTM which used a similar form of differentiable attention achieved compelling results on Machine Translation [2] and speech recognition [5].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 6,
      "context" : "Earlier, Graves [7] used a more restricted form of differentiable attention for handwritten text synthesis which, to the best of our knowledge, is the first differentiable attention model.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "Subsequent work used the idea of interpolation in order to train a stack augmented RNN, which is essentially an NTM but with a much simpler memory addressing mechanism [10].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "[14] addressed this problem using differentiable attention within the Memory Network framework.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11], and our model uses a very similar formulation in order to learn to control the memory address.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "There have since been a number of papers on visual attention that have used both Reinforce and differentiable attention [1, 17].",
      "startOffset" : 120,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "There have since been a number of papers on visual attention that have used both Reinforce and differentiable attention [1, 17].",
      "startOffset" : 120,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "The Reinforce algorithm [16] is the simplest Reinforcement learning algorithm.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "In this setting, it is possible to reduce the variance of gradient estimates of action distributions near the end of an episode [12].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "Thus, it is plausible that curriculum learning [4], which has not been helpful for DNNs because their training objectives are too easy, will be useful for NTMs since their objectives are harder.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "[8] suggest that a differentiable memory may result in models that are easier to train.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2015,
    "abstractText" : "The expressive power of a machine learning model is closely related to the number of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can perform a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation. While differentiable memory is relatively easy to implement and train, it necessitates accessing the entire memory content at each computational step. This makes it difficult to implement a fast NTM. In this work, we use the Reinforce algorithm to learn where to access the memory, while using backpropagation to learn what to write to the memory. We call this model the RL-NTM. Reinforce allows our model to access a constant number of memory cells at each computational step, so its implementation can be faster. The RL-NTM is the first model that can, in principle, learn programs of unbounded running time. We successfully trained the RL-NTM to solve a number of algorithmic tasks that are simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is a fairly intricate model, we needed a method for verifying the correctness of our implementation. To do so, we developed a simple technique for numerically checking arbitrary implementations of models that use Reinforce, which may be of independent interest.",
    "creator" : "LaTeX with hyperref package"
  }
}