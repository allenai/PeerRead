{
  "name" : "1606.01700.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Gated Word-Character Recurrent Language Model",
    "authors" : [ "Yasumasa Miyamoto", "Kyunghyun Cho" ],
    "emails" : [ "yasumasa.miyamoto@nyu.edu", "kyunghyun.cho@nyu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recurrent neural networks (RNNs) achieve state-ofthe-art performance on fundamental tasks of natural language processing (NLP) such as language modeling (RNN-LM) (Józefowicz et al., 2016; Zoph et al., 2016). RNN-LMs are usually based on the wordlevel information or subword-level information such as characters (Mikolov et al., 2012), and predictions are made at either word level or subword level respectively.\nIn word-level LMs, the probability distribution over the vocabulary conditioned on preceding words\nis computed at the output layer using a softmax function. 1 Word-level LMs require a predefined vocabulary size since the computational complexity of a softmax function grows with respect to the vocabulary size. This closed vocabulary approach tends to ignore rare words and typos, as the words do not appear in the vocabulary are replaced with an outof-vocabulary (OOV) token. The words appearing in vocabulary are indexed and associated with highdimensional vectors. This process is done through a word lookup table.\nAlthough this approach brings a high degree of freedom in learning expressions of words, information about morphemes such as prefix, root, and suffix is lost when the word is converted into an index. Also, word-level language models do not differentiate between the OOV words, and it assigns the exactly same vector to all the OOV words. These are the major limitations of word-level LMs.\nIn order to alleviate these issues, we introduce an RNN-LM that utilizes both character-level and word-level inputs. In particular, our model has a gate that adaptively choose between two distinct ways to represent each word: a word vector derived from the character-level information and a word vector stored in the word lookup table. This gate is trained to make this decision based on the input word.\nAccording to the experiments, our model with the gate outperforms other models on the Penn Treebank (PTB), BBC, and IMDB Movie Review datasets. Also, the trained gating values show that the gating mechanism effectively utilizes the character-level information when it encounters rare words.\n1softmax function is defined as f(xi) = exp xi∑ k exp xk .\nar X\niv :1\n60 6.\n01 70\n0v 1\n[ cs\n.C L\n] 6\nJ un\n2 01\nRelated Work Character-level language models that make word-level prediction have recently been proposed. Ling et al. (2015) introduce the compositional character-to-word (C2W) model that takes as input character-level representation of a word and generates vector representation of the word using a bidirectional LSTM (Graves and Schmidhuber, 2005). Kim et al. (2015) propose a convolutional neural network (CNN) based character-level language model and achieve the state-of-the-art perplexity on the PTB dataset with a significantly fewer parameters.\nMoreover, word–character hybrid models have been studied on different NLP tasks. Kang et al. (2011) apply a word–character hybrid language model on Chinese using a neural network language model (Bengio et al., 2003). Santos and Zadrozny (2014) produce high performance part-of-speech taggers using a deep neural network that learns character-level representation of words and associates them with usual word representations. Bojanowski et al. (2015) investigate RNN models that predict characters based on the character and word level inputs. Luong and Manning (2016) present word–character hybrid neural machine translation systems that consult the character-level information for rare words."
    }, {
      "heading" : "2 Model Description",
      "text" : "The model architecture of the proposed word– character hybrid language model is shown in Fig. 1.\nWord Embedding At each time step t, both the word lookup table and a bidirectional LSTM take the same word wt as an input. The word-level input is projected into a high-dimensional space by a word lookup table E ∈ R|V |×d, where |V | is the vocabulary size and d is the dimension of a word vector:\nxwordwt = E >wwt , (1)\nwhere wwt ∈ R|V | is a one-hot vector whose i-th element is 1, and other elements are 0. The character– level input is converted into a word vector by using a bidirectional LSTM. The last hidden states of forward and reverse recurrent networks are linearly combined:\nxcharwt = W fhfwt +W rhrwt + b, (2)\nwhere hfwt ,hrwt ∈ R d are the last states of the forward and the reverse LSTM respectively. Wf ,Wr ∈ Rd×d and b ∈ Rd are trainable parameters, and xcharwt ∈ R\nd is the vector representation of the word wt using a character input. The generated vectors xwordwt and x char wt are mixed by a gate gwt as\ngwt = σ ( v>g x word wt + bg ) xwt = (1− gwt)xwordwt + gwtx char wt ,\n(3)\nwhere vg ∈ Rd is a weight vector, bg ∈ R is a bias scalar, σ(·) is a sigmoid function. This gate value is independent of a time step. Even if a word appears in different contexts, the same gate value is applied.\nLanguage Modeling The output vector xwt is used as an input to a LSTM language model. Since the word embedding part is independent from the language modeling part, our model retains the flexibility to change the architecture of the language modeling part. We use the architecture similar to the nonregularized LSTM model by Zaremba et al. (2014). One step of LSTM computation corresponds to\nft = σ (Wfxwt +Ufht−1 + bf )\nit = σ (Wixwt +Uiht−1 + bi)\nc̃t = tanh (Wc̃xwt +Uc̃ht−1 + bc̃)\not = σ (Woxwt +Uoht−1 + bo)\nct = ft ct−1 + it c̃t ht = ot tanh (ct) ,\n(4)\nwhere Ws,Us ∈ Rd×d and bs ∈ Rd for s ∈ {f, i, c̃, o} are parameters of LSTM cells. σ(·) is an element-wise sigmoid function, tanh(·) is an element-wise hyperbolic tangent function, and is an element-wise multiplication.\nThe hidden state ht is affine-transformed followed by a softmax function:\nPr (wt+1 = k|w<t+1) = exp\n( v>k ht + bk )∑ k′ exp ( v>k′ht + bk′\n) , (5)\nwhere vk is the k-th column of a parameter matrix V ∈ Rd×|V | and bk is the k-th element of a bias vector b ∈ Rd. In the training phase, we minimize the negative log-likelihood with stochastic gradient descent."
    }, {
      "heading" : "3 Experimental Settings",
      "text" : "We test four different model architectures on the three English corpora. Each model has an unique word embedding method, but all models share the same LSTM language modeling architecture, that has 2 LSTM layers with 200 hidden units, d = 200. Except for the character only model, all weights are initialized with uniform random variables between -0.1 and 0.1. All biases are initialized to zero.\nStochastic gradient decent (SGD) with mini-batch size of 32 is used to train the models. In the first four epochs, the learning rate is 1. After the fourth epoch, the learning rate is halved each epoch. The hyperparameters of SGD are tuned for each model based on the validation dataset.\nAs the standard metric for language modeling, perplexity (PPL) is used to evaluate the model performance. Perplexity over the test set is computed as PPL = exp ( − 1N ∑N i=1 log p(wi|w<i) ) , where N is the number of words in the test set, and p(wi|w<i)\nis the conditional probability of a word wi given all the preceding words in a sentence. We use Theano (2016) to implement all the models."
    }, {
      "heading" : "3.1 Model Variations",
      "text" : "Word Only (baseline) This is a traditional wordlevel language model and is a baseline model for our experiments.\nCharacter Only This is a language model where each input word is represented as a character sequence similar to the C2W model in (Ling et al., 2015). The bidirectional LSTM have 200 hidden units, and their weights are initialized with Xavier initialization (Glorot and Bengio, 2010). In addition, the weights of the forget, input, and output gates are scaled by a factor of 4. The weights in the LSTM language model are initialized with uniform random variables between -0.1 and 0.1. All biases are initialized to zero. A learning rate is fixed at 0.2.\nWord & Character This model simply concatenates the vector representations of a word constructed from the character input xcharwt and the word input xwordwt to get the final representation of a word xwt , i.e.,\nxwt = [ xcharwt ;x word wt ] . (6)\nBefore being concatenated, the dimensions of xcharwt and xwordwt are reduced by half to keep the size of xwt comparably to other models.\nGated Word & Character This model uses a gate to combine vector representations of a word constructed from the character input xcharwt and the word input xwordwt as the Eq. (3)."
    }, {
      "heading" : "3.2 Datasets",
      "text" : "Penn Treebank We use the Penn Treebank Corpus (Marcus et al., 1993) preprocessed by Mikolov et\nal. (2010). We use 10k most frequent words and 51 characters. In the training phase, we use only sentences with less than 50 words.\nBBC We use the BBC corpus prepared by Greene & Cunningham (2006). We use 10k most frequent words and 62 characters. In the training phase, we use sentences with less than 50 words.\nIMDB Movie Reviews We use the IMDB Move Review Corpus prepared by Maas et al. (2011). We use 30k most frequent words and 74 characters. In the training phase, we use sentences with less than 50 words. In the validation and test phases, we use sentences with less than 500 characters."
    }, {
      "heading" : "3.3 Pre-training",
      "text" : "For the word–character hybrid models, we applied a pre-training procedure to encourage the model to use both representations. The entire model is trained only using the word-level input for the first m epochs and only using the character-level input in the next m epochs. In the first m epochs, a learning rate is fixed at 1, and a smaller learning rate 0.1 is used in the next m epoch. After the 2m-th epoch, both the character-level and the word-level inputs are used. We use m = 2 for PTB and BBC, m = 1 for IMDB."
    }, {
      "heading" : "4 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "4.1 Perplexity",
      "text" : "Table 1 compares the models on each dataset. On the PTB and BBC, the gated word & character model achieves the lowest perplexity. On the IMDB Movie Review, the gated word & character model without pre-training achieves the lowest perplexity closely followed by the gated word & character model initialized with pre-training."
    }, {
      "heading" : "4.2 Values of Word–Character Gate",
      "text" : "The BBC and IMDB datasets retain out-ofvocabulary (OOV) words while the OOV words have been replaced by <unk> in the Penn Treebank dataset. On the BBC and IMDB datasets, our model assigns a significantly high gating value on the unknown word token UNK compared to the other words.\nAs can be seen in Fig. 2, we observe that the gating value is in general higher for less frequent words, implying that the recurrent language model has learned to exploit the spelling of a word when its word vector could not have been estimated properly. We conjecture that this flexibility of modulating between word-level and character-level representations resulted in a better language model.\nOverall, the gating values are small. However, this does not mean the model does not utilize the character-level inputs. We observed that the word vectors constructed from the character-level inputs usually have a larger L2 norm than the word vectors constructed from the word-level inputs do. For instance, the mean values of L2 norm of the 1000 most frequent words in the IMDB training set are 21.73 and 6.21 respectively. The small gate values compensate for this difference."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We introduced a recurrent neural network language model with LSTM units and a word–character gate. Our model was empirically found to utilize the character-level input especially when the model encounters rare words. The experimental results suggest the gate can be efficiently trained so that the model can find a good balance between the wordlevel and character-level inputs."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is done as a part of the course DS-GA 1010-001 Independent Study in Data Science at the Center for Data Science, New York University. KC thanks Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015- 2016). YM thanks Kentaro Hanaki, Israel Malkin, and Tian Wang for their helpful feedback."
    } ],
    "references" : [ {
      "title" : "A neural probabilistic language model",
      "author" : [ "Bengio et al.2003] Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Alternative structures for character-level rnns",
      "author" : [ "Armand Joulin", "Tomas Mikolov" ],
      "venue" : null,
      "citeRegEx" : "Bojanowski et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning character-level representations for part-of-speech tagging",
      "author" : [ "dos Santos", "Bianca Zadrozny" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Santos et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2014
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
      "author" : [ "Graves", "Schmidhuber2005] Alex Graves", "Jürgen Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Graves et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "Practical solutions to the problem of diagonal dominance in kernel document clustering",
      "author" : [ "Greene", "Cunningham2006] Derek Greene", "Padraig Cunningham" ],
      "venue" : "In Machine Learning, Proceedings of the Twenty-Third International Conference (ICML",
      "citeRegEx" : "Greene et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Greene et al\\.",
      "year" : 2006
    }, {
      "title" : "Exploring the limits of language modeling. CoRR, abs/1602.02410",
      "author" : [ "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu" ],
      "venue" : null,
      "citeRegEx" : "Józefowicz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Józefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Mandarin word-character hybridinput neural network language model",
      "author" : [ "Kang et al.2011] Moonyoung Kang", "Tim Ng", "Long Nguyen" ],
      "venue" : "INTERSPEECH",
      "citeRegEx" : "Kang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2011
    }, {
      "title" : "Character-aware neural language models. CoRR, abs/1508.06615",
      "author" : [ "Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush" ],
      "venue" : null,
      "citeRegEx" : "Kim et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2015
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Ling et al.2015] Wang Ling", "Tiago Luís", "Luís Marujo", "Rámon Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso" ],
      "venue" : null,
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Achieving open vocabulary neural machine translation with hybrid word-character models. CoRR, abs/1604.00788",
      "author" : [ "Luong", "Manning2016] Minh-Thang Luong", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Luong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Maas et al.2011] Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts" ],
      "venue" : "In The 49th Annual Meeting of the Association for Computational Linguistics:",
      "citeRegEx" : "Maas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Beatrice Santorini", "Mary Ann Marcinkiewicz" ],
      "venue" : null,
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Martin Karafiát", "Lukás Burget", "Jan Cernocký", "Sanjeev Khudanpur" ],
      "venue" : "INTERSPEECH",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Subword language modeling with neural networks",
      "author" : [ "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2012
    }, {
      "title" : "Recurrent neural network regularization. CoRR, abs/1409.2329",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals" ],
      "venue" : null,
      "citeRegEx" : "Zaremba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "Simple, fast noise-contrastive estimation for large rnn vocabularies",
      "author" : [ "Zoph et al.2016] Barret Zoph", "Ashish Vaswani", "Jonathan May", "Kevin Knight" ],
      "venue" : null,
      "citeRegEx" : "Zoph et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Recurrent neural networks (RNNs) achieve state-ofthe-art performance on fundamental tasks of natural language processing (NLP) such as language modeling (RNN-LM) (Józefowicz et al., 2016; Zoph et al., 2016).",
      "startOffset" : 162,
      "endOffset" : 206
    }, {
      "referenceID" : 16,
      "context" : "Recurrent neural networks (RNNs) achieve state-ofthe-art performance on fundamental tasks of natural language processing (NLP) such as language modeling (RNN-LM) (Józefowicz et al., 2016; Zoph et al., 2016).",
      "startOffset" : 162,
      "endOffset" : 206
    }, {
      "referenceID" : 14,
      "context" : "RNN-LMs are usually based on the wordlevel information or subword-level information such as characters (Mikolov et al., 2012), and predictions are made at either word level or subword level respectively.",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "(2011) apply a word–character hybrid language model on Chinese using a neural network language model (Bengio et al., 2003).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "Ling et al. (2015) introduce the compositional character-to-word (C2W) model that takes as input character-level representation of a word and generates vector representation of the word using a bidirectional LSTM (Graves and Schmidhuber, 2005).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "Kim et al. (2015) propose a convolutional neural network (CNN) based character-level language model and achieve the state-of-the-art perplexity on the PTB dataset with a significantly fewer parameters.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "Kang et al. (2011) apply a word–character hybrid language model on Chinese using a neural network language model (Bengio et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "(2011) apply a word–character hybrid language model on Chinese using a neural network language model (Bengio et al., 2003). Santos and Zadrozny (2014) produce high performance part-of-speech taggers using a deep neural network that learns character-level representation of words and associates them with usual word representations.",
      "startOffset" : 102,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "(2011) apply a word–character hybrid language model on Chinese using a neural network language model (Bengio et al., 2003). Santos and Zadrozny (2014) produce high performance part-of-speech taggers using a deep neural network that learns character-level representation of words and associates them with usual word representations. Bojanowski et al. (2015) investigate RNN models that predict characters based on the character and word level inputs.",
      "startOffset" : 102,
      "endOffset" : 357
    }, {
      "referenceID" : 0,
      "context" : "(2011) apply a word–character hybrid language model on Chinese using a neural network language model (Bengio et al., 2003). Santos and Zadrozny (2014) produce high performance part-of-speech taggers using a deep neural network that learns character-level representation of words and associates them with usual word representations. Bojanowski et al. (2015) investigate RNN models that predict characters based on the character and word level inputs. Luong and Manning (2016) present word–character hybrid neural machine translation systems that consult the character-level information for rare words.",
      "startOffset" : 102,
      "endOffset" : 475
    }, {
      "referenceID" : 15,
      "context" : "We use the architecture similar to the nonregularized LSTM model by Zaremba et al. (2014). One step of LSTM computation corresponds to",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "Character Only This is a language model where each input word is represented as a character sequence similar to the C2W model in (Ling et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 148
    }, {
      "referenceID" : 12,
      "context" : "Penn Treebank We use the Penn Treebank Corpus (Marcus et al., 1993) preprocessed by Mikolov et",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "IMDB Movie Reviews We use the IMDB Move Review Corpus prepared by Maas et al. (2011). We use 30k most frequent words and 74 characters.",
      "startOffset" : 66,
      "endOffset" : 85
    } ],
    "year" : 2016,
    "abstractText" : "We introduce a recurrent neural network language model (RNN-LM) with long shortterm memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and wordlevel inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-ofvocabulary words and outperforms word-level language models on several English corpora.",
    "creator" : "LaTeX with hyperref package"
  }
}