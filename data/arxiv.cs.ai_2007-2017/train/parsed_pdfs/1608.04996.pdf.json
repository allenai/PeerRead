{
  "name" : "1608.04996.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Open Problem: Approximate Planning of POMDPs in the class of Memoryless Policies",
    "authors" : [ "Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar" ],
    "emails" : [ "KAZIZZAD@UCI.EDU", "ALESSANDRO.LAZARIC@INRIA.FR", "A.ANANDKUMAR@UCI.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 8.\n04 99\n6v 1\n[ cs\n.A I]\n1 7\nMore complex models like Partially Observable Markov Decision Process (POMDP) can compensate for this drawback. Fitting this model to the environment, where the partial observation is given to the agent, generally gives dramatic performance improvement, sometimes unbounded improvement, compared to MDP. In general, finding the optimal policy for the POMDP model is computationally intractable and fully non convex, even for the class of memoryless policies. The open problem is to come up with a method to find an exact or an approximate optimal stochastic memoryless policy for POMDP models."
    }, {
      "heading" : "1. Introduction",
      "text" : "The concept of planning, as a part of decision theory, in the AI literature has a long history. It is the bases for a variety of popular agent-environment interaction problems like Reinforcement Learning (RL). RL is an effective approach to solve the problem of sequential decision making under uncertainty. RL agents learn how to maximize longterm reward using a experience obtained by direct interaction with a stochastic environment (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998). Since the environment is initially unknown, the agent has to balance between exploring the environment to estimate its structure, and exploiting the estimates to compute a policy that maximizes the long-term reward. As a result, designing a RL algorithm requires three different elements: 1) an\nc© 2016 K. Azizzadenesheli, A. Lazaric & A. Anandkumar.\nestimator for the environment’s structure, 2) a planning algorithm to compute the optimal policy of the estimated environment (LaValle, 2006), and 3) a strategy to make a trade off between exploration and exploitation to minimize the regret, i.e., the difference between the performance of the exact optimal policy and the rewards accumulated by the agent over time.\nMost of RL literature assumes that the environment can be modeled as a Markov decision process (MDP), with a Markovian state evolution that is fully observed. A number of exploration–exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.g., noisy sensors in robotics). In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model.\nMany challenges arise in designing RL algorithms for POMDPs. Unlike in MDPs, the estimation problem (element 1) involves identifying the parameters of a latent variable model (LVM). The planning problem (element 2), i.e., computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.e., a distribution over the hidden state of the POMDP). Finally, integrating estimation and planning in an exploration–exploitation strategy (element 3) with guarantees is non-trivial and no no-regret strategies are currently known. Previous works Ross et al. (2007) and Poupart and Vlassis (2008) present new active learning algorithms to estimate the belief state in a model-based Bayesian RL approach, where a distribution over possible MDPs is updated over time. The proposed algorithms are such that the Bayesian inference can be done at each step, a POMDP is sampled from the posterior and the corresponding optimal policy is executed. The regret bound and sample complexity are not provided.\nRecently, the learning POMDP model parameter and imposing trade off between exploration and estimation (elements 1 and 3), are done at Azizzadenesheli et al. (2016). They propose the theoretical guaranty on regret bound given the oracle memoryless policy. Therefore to close the learning, planing, and exploration-exploitation loop, the missing part, planning (element 2), is the remaining part. Therefore, planing is a problem of finding the optimal memoryless policy, under uncertainty, in the class of stochastic memoryless polices. The overview complexity of planing in POMDP domain is discussed in Kaelbling et al. (1998)."
    }, {
      "heading" : "2. Formal Definition",
      "text" : "A POMDP M is a tuple 〈X ,A,Y ,R, fT , fR, fO〉, where X is a finite state space with cardinality |X | = X , A is a finite action space with cardinality |A| = A, Y is a finite observation space with cardinality |Y| = Y , and R is a finite reward space with cardinality |R| = R and largest reward rmax. In addition fT denotes the transition density, so that fT (x′|x, a) is the probability of transition to x′ given the state-action pair (x, a), fR is the reward density, so that fR(r|x, a) is the probability of receiving the reward in R corresponding to the value\nof the indicator vector r given the state-action pair (x, a), and fO is the observation density, so that fO(y|x) is the probability of receiving the observation in Y corresponding to the indicator vector y given the state x. Whenever convenient, we use tensor forms for the density functions such that Ti,j,l = P[xt+1 = j|xt = i, at = l] = fT (j|i, l), s.t. T ∈ RX×X×A, On,i = P[y = en|x = i] = fO(en|i), s.t. O ∈ R\nY×X , and Γi,l,m = P[r = em|x = i, a = l] = fR(em|i, l), s.t. Γ ∈ R\nX×A×R. We also denote by T:,j,l the fiber (vector) in R\nX obtained by fixing the arrival state j and action l and by T:,:,l ∈ RX×X the transition matrix between states when using action l. The graphical model associated to the POMDP is illustrated in Fig. 1.\nWe focus on stochastic memoryless policies which map observations to actions and for any policy π we denote by fπ(a|y) its density function. Acting according to a policy π in a POMDP M defines a Markov chain characterized by a transition density fT,π(x′|x) =∑\na ∑ y fπ(a|y)fO(y|x)fT (x\n′|x, a), and a stationary distribution ωπ over states such that ωπ(x) = ∑ x′ fT,π(x ′|x)ωπ(x ′). The expected average reward performance of a policy π is\nη(π;M) = ∑\nx ωπ(x)rπ(x), where rπ(x) is the expected reward of executing policy π in state x defined as rπ(x) = ∑ a ∑ y fO(y|x)fπ(a|y)r(x, a), and r(x, a) = ∑ r rfR(r|x, a) is the expected reward for the state-action pair (x, a). The best stochastic memoryless policy is π∗ = argmax\nπ η(π;M) and we denote by\nη∗ = η(π∗;M) its average reward. Finding the optimal policy π∗ requires solving nonconvex optimization and it is the desired open problem."
    }, {
      "heading" : "3. Related Work",
      "text" : "Planning on uncertainty in a dynamic internal process is studied for infinite horizon Sondik (1978). It is shown that, finding the exact optimal policy for POMDP is followed by the curse of dimensionality and the curse of history. People uses point-based value iteration Pineau et al. (2006) to reduce the complexity of the planning. It is also common to use heuristic search value iteration Smith and Simmons (2004) and also policy tree with limited depth Kaelbling et al. (1998) to reduce the planning complexity. For a finite horizonBut the computation complexity of finding optimal policy grows exponentially by horizon. For an infinite horizon, each vector of state distribution can be any point in the continues space of the simplex subspace. This means the planning is over the continuous space which is PSPACE-complete. Sondik (1978) presented a method to partition the continuous space\nof the state distribution and then the policy is just a mapping from these partitions to the action. In general, planning in the space of memoryless policy has a lower level of complexity. Although, it seems to be easier than belief based planning, it is still an NP − hard problem Vlassis et al. (2012). To breaking down this complexity, Li et al. (2011) presented a novel method for finding the optimal policy in the class of deterministic memoryless policies. Meanwhile, deterministic policies act poorly in the general case of POMDPs. The geometric representation of POMDP planning problem is shown in Montufar et al. (2015) and its geometric structure is well studied. Therefore, proposing a novel method to find the exact or approximated optimal memoryless policy (policy with performance ǫ − close to the performance of optimal policy) or limited history dependent policy under some mild conditions is the next step in the world of POMDP planning."
    } ],
    "references" : [ {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Peter Auer", "Thomas Jaksch", "Ronald Ortner" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Auer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2009
    }, {
      "title" : "Reinforcement learning of pomdps using spectral methods",
      "author" : [ "Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar" ],
      "venue" : "arXiv preprint arXiv:1602.07764,",
      "citeRegEx" : "Azizzadenesheli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Azizzadenesheli et al\\.",
      "year" : 2016
    }, {
      "title" : "Planning and acting in partially observable stochastic domains",
      "author" : [ "Leslie Pack Kaelbling", "Michael L Littman", "Anthony R Cassandra" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Kaelbling et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kaelbling et al\\.",
      "year" : 1998
    }, {
      "title" : "Finding optimal memoryless policies of pomdps under the expected average reward criterion",
      "author" : [ "Yanjie Li", "Baoqun Yin", "Hongsheng Xi" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "Li et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "Geometry and determinism of optimal stationary control in partially observable markov decision processes",
      "author" : [ "Guido Montufar", "Keyan Ghazi-Zahedi", "Nihat Ay" ],
      "venue" : "arXiv preprint arXiv:1503.07206,",
      "citeRegEx" : "Montufar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Montufar et al\\.",
      "year" : 2015
    }, {
      "title" : "The complexity of markov decision processes",
      "author" : [ "Christos Papadimitriou", "John N. Tsitsiklis" ],
      "venue" : "Math. Oper. Res.,",
      "citeRegEx" : "Papadimitriou and Tsitsiklis.,? \\Q1987\\E",
      "shortCiteRegEx" : "Papadimitriou and Tsitsiklis.",
      "year" : 1987
    }, {
      "title" : "Anytime point-based approximations for large pomdps",
      "author" : [ "Joelle Pineau", "Geoffrey Gordon", "Sebastian Thrun" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Pineau et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Pineau et al\\.",
      "year" : 2006
    }, {
      "title" : "Model-based bayesian reinforcement learning in partially observable domains",
      "author" : [ "P. Poupart", "N. Vlassis" ],
      "venue" : "In International Symposium on Artificial Intelligence and Mathematics (ISAIM),",
      "citeRegEx" : "Poupart and Vlassis.,? \\Q2008\\E",
      "shortCiteRegEx" : "Poupart and Vlassis.",
      "year" : 2008
    }, {
      "title" : "Bayes-adaptive pomdps",
      "author" : [ "Stephane Ross", "Brahim Chaib-draa", "Joelle Pineau" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Ross et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2007
    }, {
      "title" : "Heuristic search value iteration for pomdps",
      "author" : [ "Trey Smith", "Reid Simmons" ],
      "venue" : "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "Smith and Simmons.,? \\Q2004\\E",
      "shortCiteRegEx" : "Smith and Simmons.",
      "year" : 2004
    }, {
      "title" : "The optimal control of partially observable Markov processes",
      "author" : [ "E.J. Sondik" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "Sondik.,? \\Q1971\\E",
      "shortCiteRegEx" : "Sondik.",
      "year" : 1971
    }, {
      "title" : "The optimal control of partially observable markov processes over the infinite horizon: Discounted costs",
      "author" : [ "Edward J Sondik" ],
      "venue" : "Operations research,",
      "citeRegEx" : "Sondik.,? \\Q1978\\E",
      "shortCiteRegEx" : "Sondik.",
      "year" : 1978
    }, {
      "title" : "Introduction to reinforcement learning",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "On the computational complexity of stochastic controller optimization in pomdps",
      "author" : [ "Nikos Vlassis", "Michael L Littman", "David Barber" ],
      "venue" : "ACM Transactions on Computation Theory (TOCT),",
      "citeRegEx" : "Vlassis et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vlassis et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "RL agents learn how to maximize longterm reward using a experience obtained by direct interaction with a stochastic environment (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998).",
      "startOffset" : 128,
      "endOffset" : 184
    }, {
      "referenceID" : 10,
      "context" : "In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model.",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : ", computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.",
      "startOffset" : 85,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "A number of exploration–exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.",
      "startOffset" : 159,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "A number of exploration–exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.g., noisy sensors in robotics). In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model. Many challenges arise in designing RL algorithms for POMDPs. Unlike in MDPs, the estimation problem (element 1) involves identifying the parameters of a latent variable model (LVM). The planning problem (element 2), i.e., computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.e., a distribution over the hidden state of the POMDP). Finally, integrating estimation and planning in an exploration–exploitation strategy (element 3) with guarantees is non-trivial and no no-regret strategies are currently known. Previous works Ross et al. (2007) and Poupart and Vlassis (2008) present new active learning algorithms to estimate the belief state in a model-based Bayesian RL approach, where a distribution over possible MDPs is updated over time.",
      "startOffset" : 159,
      "endOffset" : 1190
    }, {
      "referenceID" : 0,
      "context" : "A number of exploration–exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.g., noisy sensors in robotics). In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model. Many challenges arise in designing RL algorithms for POMDPs. Unlike in MDPs, the estimation problem (element 1) involves identifying the parameters of a latent variable model (LVM). The planning problem (element 2), i.e., computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.e., a distribution over the hidden state of the POMDP). Finally, integrating estimation and planning in an exploration–exploitation strategy (element 3) with guarantees is non-trivial and no no-regret strategies are currently known. Previous works Ross et al. (2007) and Poupart and Vlassis (2008) present new active learning algorithms to estimate the belief state in a model-based Bayesian RL approach, where a distribution over possible MDPs is updated over time.",
      "startOffset" : 159,
      "endOffset" : 1221
    }, {
      "referenceID" : 0,
      "context" : "A number of exploration–exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.g., noisy sensors in robotics). In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model. Many challenges arise in designing RL algorithms for POMDPs. Unlike in MDPs, the estimation problem (element 1) involves identifying the parameters of a latent variable model (LVM). The planning problem (element 2), i.e., computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.e., a distribution over the hidden state of the POMDP). Finally, integrating estimation and planning in an exploration–exploitation strategy (element 3) with guarantees is non-trivial and no no-regret strategies are currently known. Previous works Ross et al. (2007) and Poupart and Vlassis (2008) present new active learning algorithms to estimate the belief state in a model-based Bayesian RL approach, where a distribution over possible MDPs is updated over time. The proposed algorithms are such that the Bayesian inference can be done at each step, a POMDP is sampled from the posterior and the corresponding optimal policy is executed. The regret bound and sample complexity are not provided. Recently, the learning POMDP model parameter and imposing trade off between exploration and estimation (elements 1 and 3), are done at Azizzadenesheli et al. (2016). They propose the theoretical guaranty on regret bound given the oracle memoryless policy.",
      "startOffset" : 159,
      "endOffset" : 1787
    }, {
      "referenceID" : 0,
      "context" : "A number of exploration–exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.g., noisy sensors in robotics). In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model. Many challenges arise in designing RL algorithms for POMDPs. Unlike in MDPs, the estimation problem (element 1) involves identifying the parameters of a latent variable model (LVM). The planning problem (element 2), i.e., computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.e., a distribution over the hidden state of the POMDP). Finally, integrating estimation and planning in an exploration–exploitation strategy (element 3) with guarantees is non-trivial and no no-regret strategies are currently known. Previous works Ross et al. (2007) and Poupart and Vlassis (2008) present new active learning algorithms to estimate the belief state in a model-based Bayesian RL approach, where a distribution over possible MDPs is updated over time. The proposed algorithms are such that the Bayesian inference can be done at each step, a POMDP is sampled from the posterior and the corresponding optimal policy is executed. The regret bound and sample complexity are not provided. Recently, the learning POMDP model parameter and imposing trade off between exploration and estimation (elements 1 and 3), are done at Azizzadenesheli et al. (2016). They propose the theoretical guaranty on regret bound given the oracle memoryless policy. Therefore to close the learning, planing, and exploration-exploitation loop, the missing part, planning (element 2), is the remaining part. Therefore, planing is a problem of finding the optimal memoryless policy, under uncertainty, in the class of stochastic memoryless polices. The overview complexity of planing in POMDP domain is discussed in Kaelbling et al. (1998).",
      "startOffset" : 159,
      "endOffset" : 2249
    }, {
      "referenceID" : 7,
      "context" : "Related Work Planning on uncertainty in a dynamic internal process is studied for infinite horizon Sondik (1978). It is shown that, finding the exact optimal policy for POMDP is followed by the curse of dimensionality and the curse of history.",
      "startOffset" : 99,
      "endOffset" : 113
    }, {
      "referenceID" : 5,
      "context" : "People uses point-based value iteration Pineau et al. (2006) to reduce the complexity of the planning.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "People uses point-based value iteration Pineau et al. (2006) to reduce the complexity of the planning. It is also common to use heuristic search value iteration Smith and Simmons (2004) and also policy tree with limited depth Kaelbling et al.",
      "startOffset" : 40,
      "endOffset" : 186
    }, {
      "referenceID" : 2,
      "context" : "It is also common to use heuristic search value iteration Smith and Simmons (2004) and also policy tree with limited depth Kaelbling et al. (1998) to reduce the planning complexity.",
      "startOffset" : 123,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "It is also common to use heuristic search value iteration Smith and Simmons (2004) and also policy tree with limited depth Kaelbling et al. (1998) to reduce the planning complexity. For a finite horizonBut the computation complexity of finding optimal policy grows exponentially by horizon. For an infinite horizon, each vector of state distribution can be any point in the continues space of the simplex subspace. This means the planning is over the continuous space which is PSPACE-complete. Sondik (1978) presented a method to partition the continuous space",
      "startOffset" : 123,
      "endOffset" : 508
    }, {
      "referenceID" : 11,
      "context" : "Although, it seems to be easier than belief based planning, it is still an NP − hard problem Vlassis et al. (2012). To breaking down this complexity, Li et al.",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "To breaking down this complexity, Li et al. (2011) presented a novel method for finding the optimal policy in the class of deterministic memoryless policies.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "To breaking down this complexity, Li et al. (2011) presented a novel method for finding the optimal policy in the class of deterministic memoryless policies. Meanwhile, deterministic policies act poorly in the general case of POMDPs. The geometric representation of POMDP planning problem is shown in Montufar et al. (2015) and its geometric structure is well studied.",
      "startOffset" : 34,
      "endOffset" : 324
    } ],
    "year" : 2016,
    "abstractText" : "Planning plays an important role in the broad class of decision theory. Planning has drawn much attention in recent work in the robotics and sequential decision making areas. Recently, Reinforcement Learning (RL), as an agent-environment interaction problem, has brought further attention to planning methods. Generally in RL, one can assume a generative model, e.g. graphical models, for the environment, and then the task for the RL agent is to learn the model parameters and find the optimal strategy based on these learnt parameters. Based on environment behavior, the agent can assume various types of generative models, e.g. Multi Armed Bandit for a static environment, or Markov Decision Process (MDP) for a dynamic environment. The advantage of these popular models is their simplicity, which results in tractable methods of learning the parameters and finding the optimal policy. The drawback of these models is again their simplicity: these models usually underfit and underestimate the actual environment behavior. For example, in robotics, the agent usually has noisy observations of the environment inner state and MDP is not a suitable model. More complex models like Partially Observable Markov Decision Process (POMDP) can compensate for this drawback. Fitting this model to the environment, where the partial observation is given to the agent, generally gives dramatic performance improvement, sometimes unbounded improvement, compared to MDP. In general, finding the optimal policy for the POMDP model is computationally intractable and fully non convex, even for the class of memoryless policies. The open problem is to come up with a method to find an exact or an approximate optimal stochastic memoryless policy for POMDP models.",
    "creator" : "LaTeX with hyperref package"
  }
}