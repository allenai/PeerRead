{"conference": "ICLR 2017 conference submission", "title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "reviews": [{"is_meta_review": true, "comments": "In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications.\n\nI wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well.\n\nFurther, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard.\n\nIf one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7.\n\nSummary: \n\n+ An interesting approach is presented that might be useful for real-world limited data scenarios.\n+ Limited data results look promising.\n- Adversarial examples are not investigated in the experimental section.\n- No realistic small-data problem is addressed.\n\nMinor:\n- The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days.\n- Some typos: tacke, developping, learni.\n\n[1]", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The program committee appreciates the authors' response to concerns raised in the reviews. Reviewers are generally excited about the combination of predefined representations with CNN architectures, allowing the model to generalize better in the low data regime. This was an extremely borderline paper, and the PCs have determined that the paper would have needed to be further revised and should be rejected.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Comments on the revision of the paper", "OTHER_KEYS": "Edouard Oyallon", "comments": "Dear reviewers,\n\nHere are two elements that I have added thanks to your constructive and helpful suggestions:\n\n- I have added a note in the Appendix B that quantifies precisely the additive perturbations of a Deep network. It proves the unstability of the hybrid deepnetwork is always smaller than the unstability of the cascaded deepnetwork and discusses the equality case. Besides, no straightforward softwares were available in Lua. Since I am convinced that without an appropriate constraints during the optimization the deep network will not perform a contraction, I decided not to investigate experimentally this question.\nHowever a scattering transform can build invariance to deformations for instance, which is not raised (to my knowledge) by the works that try to fool deepnetworks.\n\n- I have added as well state-of-the-art results on the STL10 dataset that leads to  77.4% accuracy. I would like to highlight that no specific fine tuning of the architecture was performed, and that using the scattering layers is quite straightforward.\n\nI will put in few weeks the code online on github, since it needs to be cleaned. For the potential readers of this review that might be interested in this work, we will release in few weeks a software that we scaled for Imagenet, which will be available on my website.\n\nThank you again very much for your remarks and comments that were helpful to improve this paper.\n\nBest regards,\nEO\n", "IS_META_REVIEW": false, "DATE": "03 Jan 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Interesting work with promising first results.", "is_meta_review": false, "comments": "In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications.\n\nI wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well.\n\nFurther, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard.\n\nIf one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7.\n\nSummary: \n\n+ An interesting approach is presented that might be useful for real-world limited data scenarios.\n+ Limited data results look promising.\n- Adversarial examples are not investigated in the experimental section.\n- No realistic small-data problem is addressed.\n\nMinor:\n- The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days.\n- Some typos: tacke, developping, learni.\n\n[1] ", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "18 Dec 2016 (modified: 20 Jan 2017)", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Review", "is_meta_review": false, "comments": "The paper investigates a hybrid network consisting of a scattering network followed by a convolutional network. By using scattering layers, the number of parameters is reduced, and the first layers are guaranteed to be stable to deformations. Experiments show that the hybrid network achieves reasonable performance, and outperforms the network-in-network architecture in the small-data regime.\n\nI have often heard researchers ask why it is necessary to re-learn low level features of convolutional networks every time they are trained. In theory, using fixed features could save parameters and training time. As far as I am aware, this paper is the first to investigate this question. In my view, the results show that using scattering features in the bottom layers does not work as well as learned CNN features. This is not completely obvious a priori, and so the results are interesting, but I disagree with the framing that the hybrid network is superior in terms of generalization.\n\nFor the low-data regime, the hybrid network sometimes gives better accuracy than NiN, but this is quite an old architecture and its capacity has not been tuned to the dataset size. For the full dataset, the hybrid network is clearly outperformed by fully learned models. If I understood correctly, the authors have not simply compared identical architectures with and without scattering as the first layers, which further complicates drawing a conclusion.\n\nThe authors claim the hybrid network has the theoretical advantage of stability. However, only the first layers of a hybrid network will be stable, while the learned ones can still create instability. Furthermore, if potentially unstable deep networks outperform stable scattering nets and partially-stable hybrid nets, we have to question the importance of stability as defined in the theory of scattering networks. \n\nIn conclusion, I think the paper investigates a relevant question, but I am not convinced that the hybrid network really generalizes better than standard deep nets. Faster computation (at test time) could be useful e.g. in low power and mobile devices, but this aspect is not really fleshed out in the paper.\n\nMinor comments:\n-section 3.1.2: \u201clearni\u201d\n", "IS_META_REVIEW": false, "RECOMMENDATION": 5, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "review", "is_meta_review": false, "comments": "Thanks a lot for your detailed response and clarifications.\n\nThe paper proposes to use a scattering transform as the lower layers of a deep network. This fixed representation enjoys good geometric properties (local invariance to deformations) and can be thought as a form of regularization or prior. The top layers of the network are trained to perform a given supervised task. This is the final model can be thought as plugging a standard deep convolutional network on top of the scattering transform. Evaluation on CIFAR 10 and 100 shows that the proposed approach achieves performance competitive with high performing baselines.\n\nI find the paper very interesting. The idea of cascading these representations seems very natural thing to try. To the best of my knowledge this is the first work that combines predefined and generic representations with modern CNN architectures achieving competitive performance to high performing approaches. While the state of the art (Resnets and variants) achieves significantly higher performances, I believe that this work strongly delivers it's point.\n\nThe paper convincingly shows that lower level invariances can be obtained from analytic representations (scattering transform), simplifying the training process (using less parameters) and allowing for faster evaluation. The of the hybrid approach become crucial in the low data regime. \n\nThe author argues that with the scattering initialisation instabilities cannot occur in the first layers contrary as the operator is non-expansive. This naturally suggests that the model is more robust to adversarial examples. It would be extremely interesting to present an empirical evaluation of this task. What's the practical impact? Can this hybrid network be fooled with adversarial? If this is the case, it would render the use of scattering initialization very attractive.\n\n", "IS_META_REVIEW": false, "RECOMMENDATION": 7, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"TITLE": "Baseline", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "3.2.2 Replacement", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "02 Dec 2016", "is_meta_review": false}, {"TITLE": "Scattering as a CNN, and downsampling", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "29 Nov 2016", "is_meta_review": false}], "SCORE": 7, "authors": "Edouard Oyallon", "KEYWORDS": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "accepted": false, "id": ""}
