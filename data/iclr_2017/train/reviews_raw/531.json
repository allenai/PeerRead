{"conference": "ICLR 2017 conference submission", "title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "reviews": [{"is_meta_review": true, "comments": "This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.\n\nSome results are presented on simple synthetic data (such as a moving rectangle on a black background or the \u201cMoving MNIST\u201d data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.\n\nThe model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.\n\nFinally, the exposition in this paper is short on many details and I don\u2019t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is\u2026  Low-level details (which are very important) are also not presented, such as initialization strategy.", "IS_META_REVIEW": true}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "The strengths and weaknesses pointed out by the reviews were:\n \n Strengths\n Interesting methodology to achieve differentiable translation of an image (R1, R2)\n Graphics-motivation is unique, inspiring (R1)\n \n Weaknesses\n Experiments not clear (R2)\n Novelty is not enough to justify synthetic-only experiments (R1,R2)\n Missing important citations (R3)\n Lacking details, concern with reproducibility (R3)\n \n The authors acknowledged the reviews with a short sentence but did not provide any feedback or revisions.\n \n The AC and PC agree with the reviewers that the paper presents interesting preliminary work which is more suitable for a workshop in its current form.", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": ".", "is_meta_review": false, "comments": "This paper proposes a generative model of videos composed of a background and a set of 2D objects (sprites). Optimization is performed under a VAE framework.\n\nThe authors' proposal of an outer product of softmaxed vectors (resulting in a 2D map that is delta-like), composed with a convolution, is a very interesting way to achieve translation of an image with differentiable parameters. It seems to be an attractive alternative to more complicated differentiable resamplers (such as those used by STNs) when only translation is needed.\n\nBelow I have made some comments regarding parts of the text, especially the experiments, that are not clear. The experimental section in particular seems rushed, with some results only alluded to but not given, not even in the appendix.\n\nFor an extremely novel and exotic proposal, showing only synthetic experiments could be excused. However, though there is some novelty in the method, it is disappointing that there isn't even an attempt at trying to tackle a problem with real data.\n\nI suggest as an example aerial videos (such as those taken from drone platforms), since the planar assumption that the authors make would most probably hold in that case.\n\nI also suggest that the authors do another pass at proof-reading the paper. There are missing references (\"Fig. ??\"), unfinished sentences (caption of Fig. 5), and the aforementioned issues with the experimental exposition.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 3}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "TITLE": "Experimental results are too preliminary", "is_meta_review": false, "comments": "This paper presents an approach to modeling videos based on a decomposition into a background + 2d sprites with a latent hidden state. The exposition is OK, and I think the approach is sensible, but the main issue with this paper is that it is lacking experiments on non-synthetic datasets. As such, while I find the graphics inspired questions the paper is investigating interesting, I don't think it is clear that this work introduces useful machinery for modeling more general videos.\n\nI think this paper is more appropriate as a workshop contribution in its current form.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "Mostly incremental generative model of video data with preliminary experimental results", "is_meta_review": false, "comments": "This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.\n\nSome results are presented on simple synthetic data (such as a moving rectangle on a black background or the \u201cMoving MNIST\u201d data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.\n\nThe model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.\n\nFinally, the exposition in this paper is short on many details and I don\u2019t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is\u2026  Low-level details (which are very important) are also not presented, such as initialization strategy.", "IS_META_REVIEW": false, "RECOMMENDATION": 4, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"TITLE": "Questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "16 Dec 2016", "is_meta_review": false}, {"TITLE": "divergence in figure 4", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer1", "comments": "", "IS_META_REVIEW": false, "DATE": "05 Dec 2016", "is_meta_review": false}], "SCORE": 4, "authors": "Eder Santana, Jose C Principe", "KEYWORDS": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network", "accepted": false, "id": ""}
