{
  "name" : "1202.3711.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Logical Characterization of Constraint-Based Causal Discovery",
    "authors" : [ "Tom Claassen" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present a novel approach to constraintbased causal discovery, that takes the form of straightforward logical inference, applied to a list of simple, logical statements about causal relations that are derived directly from observed (in)dependencies. It is both sound and complete, in the sense that all invariant features of the corresponding partial ancestral graph (PAG) are identified, even in the presence of latent variables and selection bias. The approach shows that every identifiable causal relation corresponds to one of just two fundamental forms. More importantly, as the basic building blocks of the method do not rely on the detailed (graphical) structure of the corresponding PAG, it opens up a range of new opportunities, including more robust inference, detailed accountability, and application to large models."
    }, {
      "heading" : "1 Introduction",
      "text" : "Causal discovery remains at the heart of most scientic research to date. Understanding which variables in a causal system influence which other is crucial for predicting the effects of actions and policies. Learning such relations from observational data is challenging, especially when latent confounders (hidden common causes) and selection bias (affecting the chance of inclusion in the data set) can be present.\nWith the introduction of the FCI algorithm in the seminal work of (Spirtes et al., 2000), it was shown that, under reasonable assumptions, it is indeed possible to infer valid causal information from observed probabilistic independencies in the large sample limit. Subsequent results and contributions from various researchers (Spirtes et al., 1999; Ali et al., 2005; Zhang,\n2008a) have developed this into a method that produces a provably sound and complete output model that captures all identifiable causal information.\nPerhaps surprisingly, this does not mean that the problem of causal discovery from data is now considered ‘solved’ by the wider research community: the method has trouble handling large models, and worse, in practice the output is often seen as unreliable.\nMost current constraint-based approaches to causal discovery rely on a two step process: a structure identification phase from observed independencies, followed by a (graphical) orientation phase. In real-world data, the large sample limit does not apply, and if one or more incorrect independence decisions are made, then this can lead to a series of erroneous orientations. But this ambiguity is not apparent in the output, severely limiting the interpretability of the entire model, also because there is little or no accountability prospect for any of the causal relations found. Bayesian scorebased methods such as GES (Chickering, 2002) are better suited to deal with this kind of problem, as they can produce multiple output models (Heckerman et al., 1999). However, they have trouble accounting for hidden variables and selection bias, and also suffer from the complexity problem. Would it be possible to give a (complete) characterization of identifiable causal relations, without first building a global structure?\nRelated work\nOne of the first to identify causal relations without recourse to a global structure was Cooper (1997), who presented an algorithm that could infer a causal relation from certain independence relations between three variables, in combination with information that one of these was known to be an uncaused variable. Later Mani et al. (2006) showed that certain independence relations between four variables, corresponding to a so-called embedded Y-structure, indicate the presence of a causal relation without the need for such background knowledge. A different approach to tackle the\nlarge scale complexity was taken by (Spirtes, 2001), who introduced a variant of FCI that could be interrupted at various stages in the inference process, with an output that is correct, but perhaps less informative than if the algorithm had been allowed to complete.\nThe perceived lack of robustness in the graphical orientation phase (due to incorrect independence decisions from limited available data), was addressed by Ramsey et al. (2006) who introduced checks to identify certain inconsistencies in the observed independencies that violate so-called ‘orientation faithfulness’, and avoid propagating these to the rest of the graph. In a related paper, Zhang and Spirtes (2008) also consider testable instances of adjacency unfaithfulness.\nIn this paper we introduce three rules to convert observed minimal (in)dependencies into logical statements about causal relations. We show that straightforward inference on these logical statements, using standard properties of causality, is sufficient to obtain all identifiable causal information. The result is the first provably sound and complete alternative to the augmented FCI algorithm. As such it generalizes other methods that do not need to build a global independence structure first, and is easily formulated as an anytime algorithm. The fact that the method only requires three simple rules that can be combined in any desired order of occurrence offers hope that this approach can be adapted to improve robustness on real-world data sets as well.\nThe paper is organized as follows. Section 2 describes some standard methods and terminology. Section 3 introduces the logical inference rules. Sections 4 and 5 show these rules are complete. Proofs are provided in the Appendix and in (Claassen and Heskes, 2011)."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Mixed graphical models",
      "text" : "A mixed graph G is a graphical model that can contain three types of edges between pairs of nodes: directed (−→), bi-directed (←→), and undirected (−−). In a mixed graph, standard graph-theoretical notions, e.g. child/parent, ancestor/descendant, directed path, cycle, still apply. So, a vertex Z is a collider on a path u = 〈. . . , X, Z, Y, . . .〉 if there are arrowheads at Z on both edges from X and Y , otherwise it is a noncollider ; if X and Y are not adjacent in G, then the subpath 〈X, Z, Y 〉 is unshielded.\nA mixed graph G is ancestral, iff an arrowhead at X on an edge to Y implies there is no directed path from X to Y in G, and there are no arrowheads at nodes with undirected edges. As a result, arrowhead marks\ncan be read as ‘is not an ancestor of’. In a mixed graph G, a vertex X is m-connected to Y by a path u, relative to a set of vertices Z, iff every noncollider on u is not in Z, and every collider on u is an ancestor of Z. If there is no such path, then X and Y are m-separated by Z. An ancestral graph is maximal (MAG) if for any two non-adjacent vertices there is a set that separates them. A directed acyclic graph (DAG) is a special kind of MAG, containing only → edges, for which m-separation reduces to the familiar d -separation criterion. The Markov property links the structure of an ancestral graph G to observed probabilistic independencies: X ⊥⊥ Y |Z, if X and Y are m-separated by Z. Faithfulness implies that the only observed independencies in a system are those entailed by the Markov property. For more details, see (Koller and Friedman, 2009; Spirtes et al., 2000).\nAn important concept is that of a minimal conditional (in)dependence, capturing the notion that really all variables in the minimal set, indicated by brackets, play a role in making two variables (in)dependent:\n- X⊥⊥Y |W ∪ [Z] ≡ ∀Z′( Z : X⊥⊥ Y |W ∪ Z′, - X⊥⊥ Y |W ∪ [Z] ≡ ∀Z′( Z : X⊥⊥Y |W ∪ Z′.\nFinally, three path definitions that appear in the orientation rules in the next section: in a MAG, a path u = 〈X, . . . , W, Z, Y 〉 is a discriminating path for Z if X is not adjacent to Y , and every node between X and Y is a collider along u, and is a parent of Y . In a PAG P (see below), a path u = 〈V0, . . . , Vn+1〉 is said to be an uncovered potentially directed (p.d.) path, if each successive triple along u is unshielded, and no edge Vi ∗−∗Vi+1 has an arrowhead at Vi or a tail at Vi+1. If all edges on u are of the form ◦−◦ , then the path is called an uncovered circle path."
    }, {
      "heading" : "2.2 Causal models and ancestral graphs",
      "text" : "A popular and intuitive way of representing a causal system is in the form of a causal DAG GC , where the arrows represent direct causal interactions between variables in a system (Pearl, 2000; Zhang, 2008b). We say there is a causal relation X ⇒ Y , iff there is a directed path from X to Y in GC . Absence of such a path is denoted X ⇒ Y . The following properties follow readily from this definition\nProposition 1. Causal relations in a DAG GC are:\nirreflexive : X ⇒ X ` false acyclic : X ⇒ Y ` Y ⇒ X transitive : (X ⇒ Y ) ∧ (Y ⇒ Z) ` X ⇒ Z\nOther definitions are possible, for example we may want to allow for non-recursive relations (feedback) or include threshold effects. Such extensions imply\nthat the causal system is not faithful to a causal DAG, which may impact the conclusions in this article.\nWhen some variables in the causal DAG are hidden, or when there is possible selection bias (Spirtes et al., 1999), the independence relations between the observed variables can be represented in the form of a maximal ancestral graph (Richardson and Spirtes, 2002). Roughly speaking, hidden common causes become bi-directed edges, and selection bias on common effects gives undirected edges. The (complete) partial ancestral graph (PAG) represents all invariant features that characterize the equivalence class [G] of such a MAG, with a tail ‘−’ or arrowhead ‘>’ end mark on an edge, iff it is invariant in [G], otherwise it has a circle mark ‘◦’, see (Zhang, 2008a). Tails in a PAG are associated with identifiable (define?) direct causal relations, and arrowheads with the absence thereof, (Zhang, 2008b). Figure 1 illustrates the relation between these three types of graphs. Note that when selection bias may be present, an invariant arc in a PAG P by itself not necessarily implies a causal relation, e.g. link B −→ F in Figure 1.3). However, if the tail node also has an incoming invariant arrowhead from another node, as for arc E−→F , then it does represent a definite, identifiable causal relation.\nThe challenge of causal discovery from observed independencies is how to identify all these invariant features from a given data set, in order to determine which variables do or do not have a directed path to which others in the underlying causal DAG."
    }, {
      "heading" : "2.3 Augmented FCI algorithm",
      "text" : "The famous Fast Causal Inference (FCI) algorithm (Spirtes et al., 2000) was one of the first algorithms that was able to validly infer causal relations from conditional independence statements in the large sample limit, even in the presence of latent and selection variables. It consists of an efficient search for a conditional independence between each pair of variables to identify the skeleton of the underlying causal MAG, followed by an orientation stage to identify invariant tail and arrowhead marks. It was shown to be sound in the large sample limit (Spirtes et al., 1999), although not\nyet complete. Ali et al. (2005) proved that the seven graphical orientation rules employed by FCI were sufficient to identify all invariant arrowheads in the equivalence class [G], given a single MAG G. Later, Zhang (2008a) introduced another set of seven rules to orient all remaining invariant tails. Augmented with this set of rules the FCI algorithm is also provably complete.\nLoosely speaking, the augmented FCI algorithm consists of an ingenious adjacency search based on conditional independencies (details of which will not concern us here), to find the skeleton of the PAG P, followed by an orientation phase based on a set of graphical rules, detailed in Table 11. Inspection reveals a certain hierarchy in which rules can trigger which others, reflected in the structure of Algorithm 1.\nStarting from the fully ◦−◦ connected graph in line 1, R0a eliminates all edges between conditionally independent nodes to obtain the skeleton of P with only ◦−◦ edges (line 4). Then rules R0b-R4b obtain all invariant arrowheads (as well as some tails). Rules R5 −R10 then suffice to identify all and only the remaining invariant tails. For example, in Figure 1, the arrowheads at C from A and B are identified by R0b, and the tailmark at B−→F follows from R9.\n1We follow the numbering from (Zhang, 2008a). The metasymbol ∗ stands for an arbitrary edge mark, and X− −Y explicitly indicates the absence of an edge between X and Y in the PAG P; see also Figures 2 and 3.\nInput : independence oracle for V Output : complete PAG P over V\n1: P ← fully ◦−◦ connected graph over V 2: for all {X, Y } ∈ V do 3: search in some clever way for a X⊥⊥Y |Z 4: P ← R0a (eliminate X− −Y ) 5: record Sep(X, Y )← Z 6: end for 7: P ← R0b (unshielded colliders) 8: repeat P ← R1−R4b until finished 9: P ← R5 (uncovered circle paths)\n10: repeat P ← R6−R7 until finished 11: repeat P ← R8a−R10 until finished\nAlgorithm 1: Augmented FCI algorithm"
    }, {
      "heading" : "3 Inference from causal logic",
      "text" : "Note: we use X, Y , Z, etc. to denote disjoint (sets of) observed variables, and S to denote the (possibly empty) set of selection nodes in a causal DAG GC ."
    }, {
      "heading" : "3.1 Logical rules from minimal independence",
      "text" : "There is a well-known, fundamental connection between minimal (in)dependencies and causal relations:\nLemma 2. If a node Z changes an (in)dependence relation between X and Y in a causal DAG, then:\n1. X⊥⊥Y |W ∪ [Z] ` Z ⇒ (X ∪ Y ∪W ∪ S),\n2. X⊥⊥ Y |W ∪ [Z] ` Z ⇒ (X ∪ Y ∪W ∪ S). with special case X⊥⊥Y | [W ∪ Z] ` Z ⇒ (X∪Y ∪S).\nThis means that, using X ⇒ Y def= ¬(X ⇒ Y ), we can translate observed minimal (in)dependencies directly into logical statements about causal relations:\nLemma 3. For observed minimal (in)dependencies between nodes in a causal DAG GC :\n1. X⊥⊥Y | [W ∪ Z] ` Z ⇒ X ∨ Z ⇒ Y ∨ Z ⇒ S 2. X⊥⊥ Y |W ∪ [Z] ` Z ⇒ X ∧ Z ⇒ Y ∧\nZ ⇒ W ∧ Z ⇒ S\nBy establishing which minimal (in)dependencies hold in a distribution, a list L can be compiled of logical statements of the form:\n1: Z ⇒ X ∨ Z ⇒ Y ∨ Z ⇒ S 2: X ⇒ Y 3: Y ⇒ X ∨ Y ⇒W ∨ Y ⇒ S, etc.\nEach line states a truth, for one specific node, about the causal relations it has with one or more others. New statements can be inferred by substituting the subject of one line in another, and then reduce by using the three causal properties from Proposition 1.\nTo illustrate the inference process in deriving (new) causal information, consider these two examples:\nExample 1. Suppose in a causal system GC both X⊥⊥Y | [Z] and X⊥⊥ U |W ∪ [Z] have been observed, for some Z ∈ Z. Then this corresponds to\n1: Z ⇒ X ∨ Z ⇒ Y ∨ Z ⇒ S 2: Z ⇒ X ∧ Z ⇒ U ∧ Z ⇒ S ∧ Z ⇒ W\nUsing (2:) to eliminate Z ⇒ X and Z ⇒ S from (1:) then gives (3:)\n` (false) ∨ Z ⇒ Y ∨ (false) 3: Z ⇒ Y\nThis case corresponds to the embedded Y-structure from Mani et al. (2006), and matches the conditions for orientation rule R1.\nExample 2. Suppose in a causal system GC both Z⊥⊥W | [UZW ∪X] and X⊥⊥Y | [UXY ∪ Z ∪W ] have been observed, with UXY and UZW two possibly empty/overlapping sets of nodes. Then for the inference list this gives statement (1:) from the first independence, and (2:) and (3:) from the second:\n1: X ⇒ Z ∨ X ⇒W ∨ ∨ X ⇒ S 2: Z ⇒ X ∨ ∨ Z ⇒ Y ∨ Z ⇒ S 3: W ⇒ X ∨ W ⇒ Y ∨ W ⇒ S\nUsing transitivity and irreflexivity, when substituting (2:) and (3:) in (1:), this reduces to (4:)\n` X ⇒ X ∨ X ⇒ X ∨ X ⇒ Y ∨ X ⇒ S 4: X ⇒ Y ∨ X ⇒ S\nThis case matches instances of R9, where all alternatives for X from the first minimal independence necessarily lead to a causal relation to node Y (or S). However, contrary to Example 1, selection bias cannot be eliminated from these two statements alone."
    }, {
      "heading" : "3.2 Inferred statements",
      "text" : "Remarkably enough, Lemma 3 and Proposition 1 are already sufficient to identify almost all causal information that can be discovered from probabilistic independencies, by repeatedly executing the substitute and reduce steps on the list of logical statements L. There is just one more piece of information needed to complete the puzzle.\nLemma 4 (Inferred blocking node). In a causal system GC , if X ⊥⊥ Y | [Z], and there is a subset {Z1, . . . , Zk, Z} ⊆ Z, such that in the sequence [U] ≡ [U0, . . . , Uk+2] = [X, Z1, . . . , Zk, Z, Y ] it holds that:\n- Ui ⇒ {Ui−1, Ui+1}, - Uj⊥⊥ Uj+1 |Z′,\nwith i = 1..k, and with j = 0..(k + 1) and ∀Z′ ⊆ Z \\ {Uj , Uj+1}, then Z ⇒ (Zk ∪ Y ∪ S).\nIn other words, if we find an ‘inferred blocking node’, then we can add the following statement to the list: 1: Z ⇒ Zk ∨ Z ⇒ Y ∨ Z ⇒ S\nLemma 4 is clearly a generalization of rule R4a: if the nodes in the sequence [U] are adjacent in P, then it corresponds to a discriminating path for Z, and the non-independence tests in the second item can be omitted. Note that resulting statement (1:) reveals that the discriminating path for Z in R4a behaves identical to a node Z observed in a minimal independence between Zk and Y . As a result, whether or not we observe Zk⊥⊥Y | [.. ∪ Z], the fact that in the given conditions Z does not create a dependency between X and Y , allows us to infer that Z blocks some path between Zk and Y ; hence ‘inferred blocking node’.\nOne remark: the set of possible independence relations involved in lemma 4 may seem quite daunting. However, in section 5 we will see that ultimately only a handful need to be checked."
    }, {
      "heading" : "3.3 Direct and indirect causal relations",
      "text" : "Reasoning with presence or absence of causal relations implies that we are not limited to direct causal influences only, but can draw on other, indirect sources of causal information as well: both can be used to derive new information in exactly the same way. But sometimes it can be very useful to distinguish between direct and indirect causes. In the PAG, a missing edge represents absence of a direct cause. Lemma 5. In a causal system GC , a (minimal) conditional independence X⊥⊥Y |Z implies that all causal paths X ⇒ Y or X ⇐ Y , or common causes of X and Y in GC are mediated by nodes in Z.\nFor independent nodes X⊥⊥Y |∅ it implies neither is a cause of the other: (X ⇒ Y ) ∧ (Y ⇒ X).\nLemma 5 gives the global structure (skeleton) of the PAG. If we want to distinguish between direct and indirect causal relations, we can simply use the MAG definition for tail/arrowhead marks (see Richardson and Spirtes, 2002, §4.2) to project the causal information in the list L onto this skeleton: Lemma 6. The causal information from statements in the list L can be transferred to invariant edge marks between adjacent nodes in the corresponding PAG P:\n- if X ⇒ Y ∈ L, then X←∗Y , - if X ⇒ Y (∨ X ⇒ S) ∈ L, then X−−∗Y ,\nIn the next section we will see that this is also complete. It means that after the logical causal inference (LoCI) process has completed, we can optionally choose to reproduce the PAG, provided the global\nstructure could be established. If not, for example because only an arbitrary subset of (in)dependence relations was available, then all causal information in the list L remains valid, even though the orientation rules in Table 1 can no longer be applied."
    }, {
      "heading" : "4 A logical characterization of causal information",
      "text" : "In this section we show that the combination of the logical statements, derived directly from observed/inferred conditional (in)dependencies via lemmas 3 and 4, together with the three inference rules in proposition 1, are sufficient to obtain all invariant orientations (tails and arrowheads) in the PAG. We do this by matching each orientation rule to specific instances of the lemmas, and already inferred information. In doing so, we make good use of the known completeness of the augmented FCI algorithm."
    }, {
      "heading" : "4.1 Invariant arrowheads",
      "text" : "First we show that all graphical orientation rules that can identify invariant arrowheads, see Figure 2, are, in fact, different graphical instances of just two cases, that can be found from minimal independencies and subsequent dependencies.\nWe would like to emphasize that there is no need to search for the specific cases discussed in this section: they automatically pop up when running the causal logic rules in proposition 1 on the list of statements L. We use them here to characterize all causal information that can be identified in this way, and thus, since the augmented FCI algorithm is complete, by any algorithm for causal discovery.\nLemma 7. In a PAG P, all invariant arrowheads Z ∗→Y are instances of\n(1): U⊥⊥ V |W ∪ [Y ], created from U⊥⊥V | [W], with Z ∈ (U ∪ V ∪W),\n(2): X⊥⊥Y | [W ∪ Z], with Z ⇒ (X ∪S) from either case (1) or case (2).\nIn words: all invariant arrowheads originate from either an observed conditional dependence (1), or as the reverse of a definite causal relation (2).\nAs a result, all seven arrowhead orientation rules R0b−R4b are covered by lemma 3. Note that, when starting from the full set of (in)dependence statements in lemma 3, it is not necessary to consider discriminating paths (nor ‘inferred blocking nodes’), in order to guarantee arrowhead completeness, contrary to when starting from a MAG, as in (Ali et al., 2005).2"
    }, {
      "heading" : "4.2 Invariant tails",
      "text" : "The previous section will not only find all arrowheads, but also a number of invariant tails, as case (2) in lemma 7 already covers all instances of rule R1, including the tail Z−→Y . In this section we show that all remaining invariant tails from rules R5−R10, see Figure 3, correspond to three cases, that can be found from minimal independencies in combination with the three inference rules in proposition 1.\nTo do that, we first introduce the following concept:\nDefinition. A transitive relation from X to Y is a sequence of nodes [X, Z1, . . . , Zk, V1, . . . , Vm, Y ] (not necessarily distinct), such that:\n- ∀Zi,∃Ui : Zi−1⊥⊥Zi+1 | [Ui ∪ Zi], - ∀Vj : Vj ⇒ (Vj+1 ∪ S),\nwith Z0 = X,Zk+1 = V1, Vm+1 = Y , for k,m ≥ 0.\nIn words: a series of overlapping minimal conditional independencies, followed by a causal relation. A transitive relation can be as short as a single independence X ⊥⊥ Y | [Z1], or a relation X ⇒ Y . As such, it is a generalization of the uncov. p.d. path in section 2.3.\nThe reason for this introduction is the property:\nCorollary 8. In a causal system GC , if there is a transitive relation [X, Z1, . . . , Y ], then:\n- X ⇒ (Z1 ∪ S) ` X ⇒ (Y ∪ S).\nWe can now state:\n2This may seem contradictory, as a MAG is just an encoding of an independence model, but it is not possible to read which set separates X and Y inR4a/b from the MAG, without actually checking for the discriminating path.\nLemma 9. In a PAG P, all invariant tails Z −−∗Y from graphical orientation rulesR4a, R5, R7, R9, and R10 are instances of:\n(2b): X⊥⊥Y | [W ∪ Z], with X ⇒ (Z∪S) from either case (3) or another instance of (2b),\n(3): U⊥⊥V | [W ∪W ], with two transitive relations [W, U, .., Y ] + [W, V, .., Y ], and Z ∈ {U, V, W},\n(4): X⊥⊥Y | [Z], with inferred blocking node Z ∈ Z, together with Zk ⇒ (Y ∪ S) from either case (2) or case (4).\nCase (2b) covers rule R7, and is so named because of its similarity/overlap with case (2) for R1. Case (3) covers all instances of rules R5, R9, and R10, and case (4) accounts for tails from orientation rule R4a. In most instances of case (3) the transitive relation requires only a single minimal conditional independence, even for long paths. Often, both transitive relations can be captured together in a single independence, as in Example 2.\nA nice property is that all identifiable selection nodes X ⇒ S also pop out ‘automatically’ by applying the inference rules in lemma 1 on instances of case (3):\nCorollary 10. In a PAG P, all identifiable selection nodes X ⇒ S are covered by case (3), in the form of a minimal independence with two transitive relations back to itself.\nThat leaves just tails from three more orientation rules to handle. However, these too follow implicitly from the existing cases:\nCorollary 11. In a PAG P, all invariant tails from orientation rules R6, R8a, and R8b, are covered by the causal logic rules applied to cases (1)-(4)."
    }, {
      "heading" : "5 Reconstructing the PAG",
      "text" : "In this section we look at the logical inference process itself, and provide an efficient anytime algorithm for deriving the PAG."
    }, {
      "heading" : "5.1 Inference procedure",
      "text" : "A nice property is that the logical substitute/reduce steps take on a particularly simple form: it only involves statements that are a logical disjunction of at most two causal relations and possible selection bias, or a single term for the absence of a causal relation. In other words, the list L always keeps the form in section 3.1. Each step consists of a substitution of one statement in another followed by a reduction to this standard form. Furthermore, as more information becomes available, statements in the list can simplify from three to two or even one term. Cf. example 1, where inferred statement (3:) replaces (1:), as there is no point in keeping the original.\nThe next result limits the independence search:\nLemma 12. In the logical causal inference (LoCI) approach, finding a single, arbitrary X⊥⊥Y | [Z], for each pair of nodes (X, Y ) in the graph (if it exists) is sufficient to find all invariant features of the PAG.\nFortunately, the current implementation of the FCI algorithm already finds only minimal conditional independencies for each pair of nodes (if it exists), as it looks for sets of increasing size until it finds one that separates the two. (This is also the dominant factor in the time-complexity of the algorithm.) For each pair found, we still need to check for other nodes that can destroy this independence (lemma 3, item 2), however, this is negligible compared to the search itself.\nFurthermore, the inferred blocking node from lemma 4, can be tackled efficiently, after all invariant arrowheads have been found from cases (1) and (2): it makes it possible to establish the ‘non-ancestor’ conditions in the sequence in one go. Together with a restriction to a sequence of non-separated nodes (avoiding the additional dependence tests), this greatly reduces the number of candidates to check.\nThe final step is to use lemmas 5 and 6 to transfer the logical information in the list L to invariant edge marks in the skeleton of P."
    }, {
      "heading" : "5.2 The LoCI algorithm",
      "text" : "We can now give the outline of an algorithm that is able to infer the complete PAG, using the logical causal inference approach described in section 3.\nAlgorithm 2 borrows the initial search for (minimal)\nInput : independence oracle for V Output : complete PAG P over V\n1: for all {X, Y } ∈ V do 2: search in some clever way for a X⊥⊥Y | [Z] 3: ∀Z ∈ Z : L← Z ⇒ (X ∪ Y ∪ S) 4: ∀W, X⊥⊥ Y |Z ∪W : 5: L←W ⇒ (X ∪ Y ∪ Z ∪ S) 6: repeat L← substitute/reduce until finished 7: end for 8: L← Z ⇒ (Zk ∪ Y ∪ S),∀Z : inferred block. node 9: repeat L← substitute/reduce until finished\n10: P ← fully ◦−◦ connected graph over V 11: eliminate X− −Y , iff X⊥⊥Y | [∗] 12: orient X−−∗Y , iff X ⇒ (Y ∪ S) ∈ L 13: orient X←∗Y , iff X ⇒ Y ∈ L Algorithm 2: Logical Causal Inference (LoCI) algorithm\nconditional independencies from the standard FCI algorithm. If it finds one it is recorded in the list L, line 3, and checked for nodes that destroy this independence (also recorded in L). Each time a minimal independence has been found, line 6 runs the inference rules to update the identifiable causal information. This step could be run just once, after the independence search has completed, but in practice the impact on performance is negligble and far outweighed by the fact that most causal information is already identifiable (available) in the early stages of the process. At line 8, all non-ancestor relations (X ⇒ Y ) have been found (see lemma 7), which makes it relatively easy to find the remaining ‘inferred blocking nodes’ from lemma 4 in line 8. If any are found that contain new information, then line 9 infers the remaining relations. Finally, lines 10− 13 construct the equivalent PAG representation from the list L.\nThese results can now be summarized as:\nTheorem 1. The Logical Causal Inference (LoCI) algorithm is sound and complete."
    }, {
      "heading" : "6 Discussion and conclusion",
      "text" : "In this paper we developed a new approach to constraint-based causal discovery: observed minimal (in)dependencies are converted into logical statements about causal relations, and these statements are subsequently combined using basic properties of causality.\nIt leads to a remarkably simple characterization, in which all identifiable causal relations take the form of an (inferred) minimal conditional independence with either elimination of one alternative, or both alternatives leading to the same conclusion.\nThe resulting logical causal inference (LoCI) method was put to work in an efficient anytime algorithm, the first alternative to the augmented FCI-algorithm shown to be both sound and complete. The LoCI algorithm is strikingly simpler than its counterpart in section 2.3. Even though it is not necessarily faster, as for both the overall complexity is dominated by the independence search, the fact that the implementation takes on this very simple and elegant form suggests it is somehow more ‘natural’ to causal discovery.\nThe way in which the LoCI algorithm builds up this causal information is markedly different from many other constraint-based methods: instead of focussing on combinations of node-pairs that may or may not be separable (the essence of graphical orientation rules), the LoCI algorithm focusses on the nodes that separate them. In particular, as it does not need to search for pairs of nodes that cannot be separated by any set (the edges forming the skeleton of the PAG), the approach taken by the algorithm could be dubbed ‘structure independent’. As a result, it can be adapted to search for target causal relations in large models, updating each time as new independence information becomes available; of course, if we want to ensure completeness, we still have to find all of them.\nThe simplicity of the LoCI algorithm raises the question if a similar approach is viable in other applications as well. For example, incorporation of causal information from background knowledge or derived from other properties of the distribution (Shimizu et al., 2006; Mooij et al., 2010), is straightforward. The same holds for additional assumptions, such as ‘no selection bias’. Including interventional information also fits nicely in this framework, and requires only minor modifications of the minimal independence lemma 2. An extension to multiple models, similar to (Triantafillou et al., 2010; Claassen and Heskes, 2010), seems feasible as well. To prove completeness, we had to rely on the known completeness of the augmented FCI algorithm, but we suspect that a more direct proof should be possible.\nPerhaps the most promising aspect of the LoCI approach lies in the flexibility it offers in deriving causal information. For example, we are free to ignore any suspect, ‘borderline’ (in)dependence decisions, by not including them in the list L in lines 3 and 5: all inferred causal relations remain valid. This should definitely increase the reliability of the output, even though it is no longer guaranteed to be complete. Finally, the ‘structure independent’ aspect implies there are many different ways to arrive at the same conclusion. This makes it possible to choose the most reliable combination(s) of independencies for a more robust conclusion and to detect inconsistencies. Tracking which logical\nstatements in L are combined to identify new relations could also improve accountability for the output, indicating exactly why a causal relation was found."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This research was supported by VICI grant 639.023.604 from the Netherlands Organization for Scientific Research (NWO)."
    }, {
      "heading" : "Appendix A. Proofs",
      "text" : "This section contains the key steps of all proofs; for details, see supplement (Claassen and Heskes, 2011).\nLemma 2. Proof sketch. A variant of two well-known results, see (Spirtes et al., 1999; Claassen and Heskes, 2010). (1.) If Z blocks the (final) unblocked path between X and Y given W, then it must be a noncollider on a trek between two of the other nodes involved; hence a directed path in GC , and so a causal relation. (2.) A node can only unblock a path, if there are unblocked paths into that node given the others. Therefore, if Z has a directed path to any (W ∪ S), then conditioning on Z is not needed to unblock the path, and if it has a directed path to X or Y then without Z there is already an unblocked path (via Z). The special case follows from (1.) and acyclicity.\nLemma 4 Proof. In words: if no node Zi has a causal relation (directed path in GC) to either of its neighbors in the sequence [X,Z1, . . . , Zk, Z, Y ], and all neighboring nodes in the sequence are dependent given any subset of Z, then Z has a causal relation to Zk, Y , and/or S. By construction, in GC there is an unblocked path from X to Z, given Z. If both Zk and Y have paths that are into Z, then the sequence would represent an unblocked path between X and Y given Z ∪ Z in GC , which would make X and Y dependent, contrary the given. By the second item (dependent neigbors), all neighbors in the sequence, so also (Zk, Z) and (Z, Y ), are connected by treks between them (or treks to S), that are not blocked by any nodes from Z. Not both these paths from Zk and Y are into Z, therefore Z must either have a directed path to S in GC and/or be an ancestor of Zk or Y .\nLemma 7 Proof sketch. Both cases are sound: (1.) By lemma 3, item 2, the first gives (Y ⇒ Z) ∧ (Y ⇒ S), which, by definition, implies that if Y has an edge to Z in P, then the mark at Y is an (invariant) arrowhead. (2.) The second is an application of lemma 3, item\n1, giving (Z ⇒ X) ∨ (Z ⇒ Y ) ∨ (Z ⇒ S), where the first and third are eliminated by the arrowhead at X ∗→Z (def). Therefore Z ⇒ Y , and so (acyclicity) also Y ⇒ Z, but also Y ⇒ S, otherwise (transitivity) Z ⇒ S. Therefore, if Y has an edge to Z in P, then it has an arrowhead mark at Y . The proof that they are also complete follows by induction on the graphical orientation rules R0b−R4b, showing that none of them introduces a violation of lemma 7. As these rules are sufficient for arrowhead completeness, it follows that the lemma holds for all invariant arrowheads.\nCorollary 8 Proof. The transitive relation implies:\n1: Z1 ⇒ X ∨ Z1 ⇒ Z2 ∨ Z1 ⇒ S k: Zk ⇒ Zk−1∨ Zk ⇒ V1 ∨ Zk ⇒ S k+m: Vm ⇒ Y ∨ Vm ⇒ S\nBack substituting in reverse order gives finally, ` Z1 ⇒ X ∨ Z1 ⇒ Y ∨ Z1 ⇒ S to substitute in X ⇒ Z1 ∨X ⇒ S.\nLemma 9 Proof sketch. All three cases are sound: (2b) By lemma 3, X ⊥⊥ Y | [W ∪ Z] gives (Z ⇒ X) ∨ (Z ⇒ Y )∨ (Z ⇒ S). Combined with X ⇒ Z ∨X ⇒ S this reduces to (Z ⇒ Y )∨ (Z ⇒ S), and so a tail at Z if it has an edge to Y in P. (3) Idem, U⊥⊥V | [W ∪W ] gives (W ⇒ U) ∨ (W ⇒ V ) ∨ (W ⇒ S). From corollary 8, the transitive relations give (W ⇒ {U ∪ S}) ` (W ⇒ {Y ∪ S}), and (W ⇒ {V ∪S}) ` (W ⇒ {Y ∪S}). Substituting these two in the first then gives (W ⇒ Y )∨ (W ⇒ S). This holds for all nodes on the two transitive chains, hence if Z ∈ {U, V, W}, then (Z ⇒ Y )∨ (Z ⇒ S), and therefore a tail Z−−∗Y , if they are connected in P. (4) By lemma 4, as Z is an inferred blocking node between X and Y given Z, there is a Zk ∈ Z such that Z ⇒ Zk ∨ Z ⇒ Y ∨ Z ⇒ S. Together with the given Zk ⇒ Y ∨ Zk ⇒ S, this reduces to Z ⇒ Y ∨ Z ⇒ S, and hence an invariant tail Z−→Y . For completeness it is fairly straightforward to see that in a PAG P, all instances of rule R7 match (2b), instances of rules R5, R9, and R10 always match case (3), and R4a matches case (4).\nCorollary 10 Proof sketch. By corollary 8 a transitive relation [W, U, .., W ] implies that W ⇒ (U ∪ S) ` W ⇒ S. Idem for [W, V, .., W ]. Two such statements connected by U ⊥⊥ V | [W ∪W ] then reduce (W ⇒ U) ∨ (W ⇒ V ) ∨ (W ⇒ S), from lemma 3, to (W ⇒ S), i.e. identifiable selection bias. That all nodes with identifiable selection bias have this form follows from the fact that only rules R5 − R7 can produce undirected edges on\nnodes (corresponding to identifiable selection). The uncovered circle path from R5 already has this form; R6 can be ignored when identifying new nodes, andR7 can only produce undirected edges on transitive chains connecting two distinct circle-path components.\nCorollary 11 Proof. R8a and R8b do not take (in)dependence information as input, but only need lemma 1 to combine two relations already found as a result of cases (1)-(4). The tail from rule R6 only signifies that Z ⇒ S, which will be found as case (3), by corollary 10.\nLemma 12 Proof sketch. We know from lemmas 7 and 9 that all orientation rules are covered by some combination of minimal independencies and subsequent dependencies. From the graphical description, we see that the rules orient tails/arrowheads between adjacent nodes that either involve a noncollider between two nonadjacent nodes, (and are therefore part of all minimal conditional independencies between the two), or as one of the separated nodes in the conditional independence (so any will do), possibly with a direct link to a node that destroys this independence (which will therefore also be found). The only rule that is not entirely straightforward is R2a, but this boils down to a similar case as for the invariant arrowheads in lemma 7, to which the same argument can be applied, to show that the node Z is a necessary part of at least some minimal independence that is destroyed by Y . Therefore all rules are covered, if we have at least one minimal independence for each pair of nonadjacent nodes, in combination with the subsequent dependencie. As we know that the set of graphical orientation rules is sufficient to find all invariant features in the PAG (Zhang, 2008a), this proves the lemma.\nTheorem 1 Proof sketch. Soundness follows from the validity of the lemmas 3 and 4, that produce the logical statements in the list L, in combination with the causal logic rules in lemma 1. Completeness follows from the fact that all rules are instancess of cases (1)-(4) for a single, arbitrary minimal independence between nodes, in combination with subsequent dependencies (lemma 11), the fact that all logical inference in each of the cases (1)-(4) is covered by lemma 3, the fact that case (1) and (2) will find all required non-ancestor relations (= invariant arrowheads, see Zhang, 2008a, lemma 6), needed to obtain the only remaining piece of information (inferred blocking node for case (4) from lemma 4). After running the logical rules on this set of statements to completion, all invariant edge marks have been found and can be transferred to the PAG."
    } ],
    "references" : [ {
      "title" : "Towards characterizing markov equivalence classes for directed acyclic graphs with latent variables",
      "author" : [ "R.A. Ali", "T. Richardson", "P. Spirtes", "J. Zhang" ],
      "venue" : "In Proc. of the 21st Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Ali et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ali et al\\.",
      "year" : 2005
    }, {
      "title" : "Optimal structure identification with greedy search",
      "author" : [ "D. Chickering" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chickering.,? \\Q2002\\E",
      "shortCiteRegEx" : "Chickering.",
      "year" : 2002
    }, {
      "title" : "Causal discovery in multiple models from different experiments",
      "author" : [ "T. Claassen", "T. Heskes" ],
      "venue" : "In NIPS-23,",
      "citeRegEx" : "Claassen and Heskes.,? \\Q2010\\E",
      "shortCiteRegEx" : "Claassen and Heskes.",
      "year" : 2010
    }, {
      "title" : "Proof supplement to ‘A logical characterization of constraint-based causal discovery",
      "author" : [ "T. Claassen", "T. Heskes" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Claassen and Heskes.,? \\Q2011\\E",
      "shortCiteRegEx" : "Claassen and Heskes.",
      "year" : 2011
    }, {
      "title" : "A simple constraint-based algorithm for efficiently mining observational databases for causal relationships",
      "author" : [ "G. Cooper" ],
      "venue" : "Data Min. Knowl. Discov,",
      "citeRegEx" : "Cooper.,? \\Q1997\\E",
      "shortCiteRegEx" : "Cooper.",
      "year" : 1997
    }, {
      "title" : "A Bayesian approach to causal discovery. In Computation, Causation, and Discovery, pages 141–166",
      "author" : [ "D. Heckerman", "C. Meek", "G. Cooper" ],
      "venue" : null,
      "citeRegEx" : "Heckerman et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 1999
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Koller and Friedman.,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedman.",
      "year" : 2009
    }, {
      "title" : "A theoretical study of Y structures for causal discovery",
      "author" : [ "S. Mani", "G. Cooper", "P. Spirtes" ],
      "venue" : "In Proc. of the 22nd Conference in Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Mani et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Mani et al\\.",
      "year" : 2006
    }, {
      "title" : "Probabilistic latent variable models for distinguishing between cause and effect",
      "author" : [ "J.M. Mooij", "O. Stegle", "D. Janzing", "K. Zhang", "B. Schölkopf" ],
      "venue" : "In NIPS-23,",
      "citeRegEx" : "Mooij et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mooij et al\\.",
      "year" : 2010
    }, {
      "title" : "Causality: models, reasoning and inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl.,? \\Q2000\\E",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2000
    }, {
      "title" : "Adjacencyfaithfulness and conservative causal inference",
      "author" : [ "J. Ramsey", "J. Zhang", "P. Spirtes" ],
      "venue" : "In Proc. of the 22nd Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Ramsey et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ramsey et al\\.",
      "year" : 2006
    }, {
      "title" : "Ancestral graph Markov models",
      "author" : [ "T. Richardson", "P. Spirtes" ],
      "venue" : "Ann. Stat.,",
      "citeRegEx" : "Richardson and Spirtes.,? \\Q2002\\E",
      "shortCiteRegEx" : "Richardson and Spirtes.",
      "year" : 2002
    }, {
      "title" : "A linear non-Gaussian acyclic model for causal discovery",
      "author" : [ "S. Shimizu", "P. Hoyer", "A. Hyvärinen", "A. Kerminen" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shimizu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Shimizu et al\\.",
      "year" : 2003
    }, {
      "title" : "An anytime algorithm for causal inference",
      "author" : [ "P. Spirtes" ],
      "venue" : "In Proc. of the Eighth International Workshop on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Spirtes.,? \\Q2001\\E",
      "shortCiteRegEx" : "Spirtes.",
      "year" : 2001
    }, {
      "title" : "An algorithm for causal inference in the presence of latent variables and selection bias. In Computation, Causation, and Discovery, pages 211–252",
      "author" : [ "P. Spirtes", "C. Meek", "T. Richardson" ],
      "venue" : null,
      "citeRegEx" : "Spirtes et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 1999
    }, {
      "title" : "Causation, Prediction, and Search",
      "author" : [ "P. Spirtes", "C. Glymour", "R. Scheines" ],
      "venue" : null,
      "citeRegEx" : "Spirtes et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 2000
    }, {
      "title" : "Learning causal structure from overlapping variable sets",
      "author" : [ "S. Triantafillou", "I. Tsamardinos", "I. Tollis" ],
      "venue" : "In Proc. of the 13th Int. Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Triantafillou et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Triantafillou et al\\.",
      "year" : 2010
    }, {
      "title" : "On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias",
      "author" : [ "J. Zhang" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Zhang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2008
    }, {
      "title" : "Causal reasoning with ancestral graphs",
      "author" : [ "J. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Zhang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2008
    }, {
      "title" : "Detection of unfaithfulness and robust causal inference",
      "author" : [ "J. Zhang", "P. Spirtes" ],
      "venue" : "Minds and Machines,",
      "citeRegEx" : "Zhang and Spirtes.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zhang and Spirtes.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "With the introduction of the FCI algorithm in the seminal work of (Spirtes et al., 2000), it was shown that, under reasonable assumptions, it is indeed possible to infer valid causal information from observed probabilistic independencies in the large sample limit.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "Subsequent results and contributions from various researchers (Spirtes et al., 1999; Ali et al., 2005; Zhang, 2008a) have developed this into a method that produces a provably sound and complete output model that captures all identifiable causal information.",
      "startOffset" : 62,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "Subsequent results and contributions from various researchers (Spirtes et al., 1999; Ali et al., 2005; Zhang, 2008a) have developed this into a method that produces a provably sound and complete output model that captures all identifiable causal information.",
      "startOffset" : 62,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "Bayesian scorebased methods such as GES (Chickering, 2002) are better suited to deal with this kind of problem, as they can produce multiple output models (Heckerman et al.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "Bayesian scorebased methods such as GES (Chickering, 2002) are better suited to deal with this kind of problem, as they can produce multiple output models (Heckerman et al., 1999).",
      "startOffset" : 155,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : "One of the first to identify causal relations without recourse to a global structure was Cooper (1997), who presented an algorithm that could infer a causal relation from certain independence relations between three variables, in combination with information that one of these was known to be an uncaused variable.",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "One of the first to identify causal relations without recourse to a global structure was Cooper (1997), who presented an algorithm that could infer a causal relation from certain independence relations between three variables, in combination with information that one of these was known to be an uncaused variable. Later Mani et al. (2006) showed that certain independence relations between four variables, corresponding to a so-called embedded Y-structure, indicate the presence of a causal relation without the need for such background knowledge.",
      "startOffset" : 89,
      "endOffset" : 340
    }, {
      "referenceID" : 13,
      "context" : "large scale complexity was taken by (Spirtes, 2001), who introduced a variant of FCI that could be interrupted at various stages in the inference process, with an output that is correct, but perhaps less informative than if the algorithm had been allowed to complete.",
      "startOffset" : 36,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "The perceived lack of robustness in the graphical orientation phase (due to incorrect independence decisions from limited available data), was addressed by Ramsey et al. (2006) who introduced checks to identify certain inconsistencies in the observed independencies that violate so-called ‘orientation faithfulness’, and avoid propagating these to the rest of the graph.",
      "startOffset" : 156,
      "endOffset" : 177
    }, {
      "referenceID" : 10,
      "context" : "The perceived lack of robustness in the graphical orientation phase (due to incorrect independence decisions from limited available data), was addressed by Ramsey et al. (2006) who introduced checks to identify certain inconsistencies in the observed independencies that violate so-called ‘orientation faithfulness’, and avoid propagating these to the rest of the graph. In a related paper, Zhang and Spirtes (2008) also consider testable instances of adjacency unfaithfulness.",
      "startOffset" : 156,
      "endOffset" : 416
    }, {
      "referenceID" : 3,
      "context" : "Proofs are provided in the Appendix and in (Claassen and Heskes, 2011).",
      "startOffset" : 43,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "For more details, see (Koller and Friedman, 2009; Spirtes et al., 2000).",
      "startOffset" : 22,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "For more details, see (Koller and Friedman, 2009; Spirtes et al., 2000).",
      "startOffset" : 22,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "A popular and intuitive way of representing a causal system is in the form of a causal DAG GC , where the arrows represent direct causal interactions between variables in a system (Pearl, 2000; Zhang, 2008b).",
      "startOffset" : 180,
      "endOffset" : 207
    }, {
      "referenceID" : 14,
      "context" : "When some variables in the causal DAG are hidden, or when there is possible selection bias (Spirtes et al., 1999), the independence relations between the observed variables can be represented in the form of a maximal ancestral graph (Richardson and Spirtes, 2002).",
      "startOffset" : 91,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : ", 1999), the independence relations between the observed variables can be represented in the form of a maximal ancestral graph (Richardson and Spirtes, 2002).",
      "startOffset" : 127,
      "endOffset" : 157
    }, {
      "referenceID" : 15,
      "context" : "The famous Fast Causal Inference (FCI) algorithm (Spirtes et al., 2000) was one of the first algorithms that was able to validly infer causal relations from conditional independence statements in the large sample limit, even in the presence of latent and selection variables.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "It was shown to be sound in the large sample limit (Spirtes et al., 1999), although not yet complete.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "Ali et al. (2005) proved that the seven graphical orientation rules employed by FCI were sufficient to identify all invariant arrowheads in the equivalence class [G], given a single MAG G.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Ali et al. (2005) proved that the seven graphical orientation rules employed by FCI were sufficient to identify all invariant arrowheads in the equivalence class [G], given a single MAG G. Later, Zhang (2008a) introduced another set of seven rules to orient all remaining invariant tails.",
      "startOffset" : 0,
      "endOffset" : 210
    }, {
      "referenceID" : 7,
      "context" : "This case corresponds to the embedded Y-structure from Mani et al. (2006), and matches the conditions for orientation rule R1.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "Note that, when starting from the full set of (in)dependence statements in lemma 3, it is not necessary to consider discriminating paths (nor ‘inferred blocking nodes’), in order to guarantee arrowhead completeness, contrary to when starting from a MAG, as in (Ali et al., 2005).",
      "startOffset" : 260,
      "endOffset" : 278
    }, {
      "referenceID" : 8,
      "context" : "For example, incorporation of causal information from background knowledge or derived from other properties of the distribution (Shimizu et al., 2006; Mooij et al., 2010), is straightforward.",
      "startOffset" : 128,
      "endOffset" : 170
    }, {
      "referenceID" : 16,
      "context" : "An extension to multiple models, similar to (Triantafillou et al., 2010; Claassen and Heskes, 2010), seems feasible as well.",
      "startOffset" : 44,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "An extension to multiple models, similar to (Triantafillou et al., 2010; Claassen and Heskes, 2010), seems feasible as well.",
      "startOffset" : 44,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "This section contains the key steps of all proofs; for details, see supplement (Claassen and Heskes, 2011).",
      "startOffset" : 79,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "A variant of two well-known results, see (Spirtes et al., 1999; Claassen and Heskes, 2010).",
      "startOffset" : 41,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "A variant of two well-known results, see (Spirtes et al., 1999; Claassen and Heskes, 2010).",
      "startOffset" : 41,
      "endOffset" : 90
    } ],
    "year" : 2011,
    "abstractText" : "We present a novel approach to constraintbased causal discovery, that takes the form of straightforward logical inference, applied to a list of simple, logical statements about causal relations that are derived directly from observed (in)dependencies. It is both sound and complete, in the sense that all invariant features of the corresponding partial ancestral graph (PAG) are identified, even in the presence of latent variables and selection bias. The approach shows that every identifiable causal relation corresponds to one of just two fundamental forms. More importantly, as the basic building blocks of the method do not rely on the detailed (graphical) structure of the corresponding PAG, it opens up a range of new opportunities, including more robust inference, detailed accountability, and application to large models.",
    "creator" : "TeX"
  }
}