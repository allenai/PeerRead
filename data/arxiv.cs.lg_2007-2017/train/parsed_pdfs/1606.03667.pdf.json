{
  "name" : "1606.03667.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Reinforcement Learning with a Combinatorial Action Space for Predicting and Tracking Popular Discussion Threads",
    "authors" : [ "Ji He", "Mari Ostendorf", "Xiaodong He", "Jianshu Chen", "Jianfeng Gao", "Lihong Li", "Li Deng" ],
    "emails" : [ "jvking@uw.edu", "ostendor@uw.edu", "xiaohe@microsoft.com", "jianshuc@microsoft.com", "jfgao@microsoft.com", "lihongli@microsoft.com", "deng@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "This paper is concerned with learning policies for sequential decision-making tasks, where a system takes actions given options characterized by natural language with the goal of maximizing a longterm reward. More specifically, we consider tasks with a combinatorial action space, where each action is a set of multiple interdependent sub-actions. The problem of a combinatorial natural language action space arises in many applications. For example, in real-time news feed recommendation, a user may want to read diverse topics of interest, and an action (i.e. recommendation) from the computer agent would consist of a set of news articles that are not all similar in topics (Yue and Guestrin, 2011). In advertisement placement, an action is a selection of sev-\neral ads to display, and bundling with complementary products might receive higher click-throughrate than displaying all similar popular products.\nIn this work, we consider Reddit popularity prediction and tracking, which is similar to news feed recommendation but different in two respects. First, our goal is not to make recommendations based on an individual’s preferences, but instead based on the anticipated interest level of a broad group of readers from a target community. Second, we try to predict rather than detect popularity. Unlike individual interests, community interest level is not often immediately clear; there is a time lag before the level of interest starts to take off. Here, the goal is for the recommendation system to identify and track written documents (e.g. news articles, comments in discussion forum threads, or scientific articles) in real time – attempting to identify hot updates before they become hot to keep the reader at the leading edge. The premise is that the user’s bandwidth is limited, and only a limited number of things can be recommended out of several possibilities. In our experimental work, we use discussion forum text, where the recommendations correspond to recent posts or comments, assessing interest based on community response as observed in “likes” or other positive reactions to those comments. For training purposes, we can use community response measured at a time much later than the original post or publication. This problem is quite naturally suited to reinforcement learning paradigm, since the reward (the level of community uptake or positive response) is not immediately known, so the system needs to learn a mechanism for estimating future reactions. Differar X iv :1 60 6. 03 66 7v 1 [ cs .C L ] 1\n2 Ju\nn 20\nent than typical reinforcement learning, the action space is combinatorial since an action corresponds to a set of comments (sub-actions) chosen from a larger set of candidates. The sub-action itself can be any natural language comment (or document, for another variant of this task). An advantage of working with discussion forum comments is that the comments represent relatively small documents which reduces the scale of the problem for initial development.\nTwo challenges associated with this problem include the potentially high computational complexity of the combinatorial action space and the development of a framework for estimating the long-term reward (the Q-value in reinforcement learning) from a combination of sub-actions characterized by natural language. Here, we focus on the second problem, exploring different deep neural network architectures in an effort to efficiently account for the potential redundancy and/or temporal dependency of different sub-actions in relation to the state space. We sidestep the computational complexity issue (for now) by working with a task where the number of combinations is not too large and by further reducing costs by random sampling.\nThere are two main contributions in this paper. First, we propose a novel reinforcement learning task with both states and combinatorial actions defined by natural language,1 which is introduced in section 2. This task, which is based on comment popularity prediction using data from the Reddit discussion forum, can serve as a benchmark in social media recommendation and trend spotting. The second contribution is the development of a novel deep reinforcement learning architecture for handling a combinatorial action space associated with natural language. The approach builds on prior work in deep reinforcement learning summarized in section 3. Details for the new models as well as alternative architectures used as baselines are described in section 4. Experimental results in section 5 show the proposed methods capture the combinatorial structure and outperform baseline models, with DRRNBiLSTM performing consistently the best. A brief summary of findings and open questions are in section 6.\n1Simulators and codes will be released."
    }, {
      "heading" : "2 Popularity Prediction and Tracking",
      "text" : "Experiments are based on Reddit2, one of the world’s largest public forums. On Reddit, registered users initiate a post and people respond with comments, either to the original post or one of its associated comments. Together, the comments and the original post form a discussion tree. As time goes on, more comments will appear and the tree continues to grow. Reddit discussions are grouped into different domains, called subreddits, according to different topics or themes. Depending on the popularity of the subreddit, a post can receive hundreds of comments.\nComments (and posts) are associated with positive and negative votes (i.e., likes and dislikes) from registered users that are combined to get a karma score, which can be used as a measure for popularity. An example of the top of a Reddit discussion tree is given in Figure 1. The scores in red boxes mark the current karma (popularity) of each comment, and it is quite common that a lower karma comment (e.g. “Yeah, politics aside, this one looks much cooler”, compared to “looks more like zombama”) will lead to more popular discussions in the future (with more children and higher accumulated scores). Note that the karma scores are dynamic, changing as readers react to the evolving discussion and eventually settling down as the discussion trails off. In a real-time comment recommendation system, the eventual karma of a comment is not immediately available, so prediction of popularity is based on the text in the comment in the context of prior comments in the subtree and other comments in the current time window.\nPopularity prediction and tracking in the Reddit setting presented in this paper is a proxy task for studying reinforcement learning to model long-term rewards in a combinatorial action space. At each time step, the state corresponds to the collection of comments previously recommended. The system aims at automatically picking a few lines of the discussion to follow from the new set of comments in a given window, which is a combinatorial action. In this work, we only consider new comments associated with the threads of the discussion that we are currently following to limit the number of pos-\n2http://www.reddit.com\nsible sub-actions at each time step and with the assumption that prior context is needed to interpret the comments. In other words, the new recommendation should focus on comments that are in the subtrees of previously recommended comments. (A variant relaxing this restriction is suggested in the conclusion section.) Typically, one would expect some interdependencies between comments made in the same window if they fall under the same subtree, because they correspond to a reply to the same parent. In addition, there may be some temporal dependency, since one sub-action may be a comment on the other. These dependencies will affect the combined utility of the sub-actions.\nAccording to our experiments, the performance is significantly worse when we tried learning a myopic policy compared to reinforcement learning using the same feature set. This shows that long-term dependency indeed matters and an example is given in Figure 1. This serves as a justification that reinforcement learning is an appropriate approach for modeling popularity of a discussion thread."
    }, {
      "heading" : "3 Related Work",
      "text" : "There is a large body of work on reinforcement learning. Among those of most interest here are deep reinforcement learning methods that leverage neural networks because of their success in handling large discrete state/action spaces. Early work such as TD-gammon used a neural network to approximate the state value function (Tesauro, 1995). Recent advances in deep learning (LeCun et al., 2015; Deng and Yu, 2014; Hinton et al., 2012; Krizhevsky\net al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011).\nRecently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2015) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for language based sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy.\nPredicting and tracking hot discussion topics on Reddit differs from these previous tasks (Allan, 2012; Mathioudakis and Koudas, 2010; Weninger et al., 2013) in that a user generated score is representative of popularity, and we aim to maximum the long term accumulated scores in sequential discussion threads. To the best of our knowledge, this is the first work that frames tracking hot topics in social media with deep reinforcement learning.\nIn a closely related paper, Dulac-Arnold and Evans et al. (2016) investigated the problem of large discrete action spaces. A Wolpertinger architecture is proposed to reduce computational complexity of evaluating all actions. While combinatorial action space is also large and discrete, their method does not directly apply in our case. This is because for Reddit popularity tracking, actions defined by natural language are changing over different states. In this paper we also focus more on modeling the combined action value function rather than reducing the computational complexity. Other work that targets at studying a structured action space includes: Hausknecht and Stone (2016) considered an actor-critic algorithm where actions can have real-\nvalued parameters; factored Markov Decision Process (MDP) (Guestrin et al., 2001; Sallans and Hinton, 2004) assume certain independence between a next-state component and a sub-action. As for bandits setting, Yue and Guestrin (2011) considered diversification of multi-item recommendation, but their methodology is limited to using linear approximation with hand-crafted features."
    }, {
      "heading" : "4 Characterizing a combinatorial action space",
      "text" : ""
    }, {
      "heading" : "4.1 Notation",
      "text" : "In this sequential decision making problem, at each time step t, the agent receives a text string that describes the state st ∈ S (i.e., “state-text”) and picks a text string that describes the action at ∈ A (i.e., “action-text”), where S and A denote the state and action spaces, respectively. Here, we assume at is chosen from a set of given candidates. In our case both S and A are described by natural language. Given the state-text and action-texts, the agent aims to select the best action in order to maximize its long-term reward. Then the environment state is updated st+1 = s′ according to a probability p(s′|s, a), and the agent receives a reward rt+1 for that particular transition. We may define action value function (i.e. Q-function)Q(s, a) as the expected return starting from s and taking the action a:\nQ(s, a) = E { +∞∑ l=0 γlrt+1+l|st = s, at = a }\nwhere γ ∈ (0, 1) denotes a discount factor. The Q-function associated with an optimal policy can be found by the Q-learning algorithm (Watkins and Dayan, 1992):\nQ(st, at)← Q(st, at)+ ηt · ( rt+1 + γ ·max\na Q(st+1, a)−Q(st, at) ) where ηt is a learning rate parameter.\nThe set of comments that are being tracked at time step t is denoted as Mt. All previously tracked comments, as well as the post (root node of the tree), is considered as state st (st = {M0,M1, · · · ,Mt}), and we initialize s0 = M0 to be the post. An action is taken when a total of N new comments\n{ct,1, ct,2, · · · , ct,N} appear as nodes in the subtree of Mt, and the agent picks a set of K comments to be tracked in the next time step t+1. Thus we have:\nat = {c1t , c2t , · · · , cKt }, cit ∈ {ct,1, ct,2, · · · , ct,N} and cit 6= c j t if i 6= j (1)\nand Mt+1 = at. At the same time, by taking action at at state st, the reward rt+1 is the accumulated karma scores, i.e. sum over all comments in Mt+1. Note that the reward signal is used in online training, while at model deployment (testing stage), the scores are only used as an evaluation metric.\nFollowing the reinforcement learning tradition, we call tracking of a single discussion tree from start (root node post) to end (no more new comments appear) an episode. We also randomly partition all discussion trees into separate training and testing sets, so that texts seen by the agent in training and testing are from the same domain but different discussions. For each episode, depending on whether training/testing, the simulator randomly picks a discussion tree, and presents the agent with the current state and N new comments."
    }, {
      "heading" : "4.2 Q-function alternatives",
      "text" : "With the real-time setting, it is clear that action at will affect next state st+1 and furthermore the future expected reward. The action at consists of K comments (sub-actions), making modeling Q-values Q(st, at) difficult. To handle a large state space, Mnih et al. (2015) proposed a Deep Q-Network (DQN). In case of a large action space, we may use both state and action representations as input to a deep neural network. It is suggested that the Deep Reinforcement Relevance Network (DRRN, Figure 2(b)), i.e. two separate deep neural networks for modeling state embedding and action embedding, performs better than per-action DQN (PA-DQN) in Figure 2(a), as well as other DQN variants for dealing with large action spaces(He et al., 2015).\nOur baseline models include Linear, PADQN and DRRN. We concatenate the K subactions/comments to form the action representation. The Linear and PA-DQN (Figure 2(a)) take input as a concatenation of state and action representations, and model a single Q-value Q(st, at) using linear or DNN function approximations. The DRRN consists\nof a pair of DNNs, one for the state text embedding and the other for action text embeddings, which are then used to compute Q(st, at) via a pairwise interaction function (Figure 2(b)).\nOne simple alternative approach by utilizing this combinatorial structure is to compute an embedding for each sub-action cit. We can then model the value in picking a particular sub-action, Q(st, cit), through a pairwise interaction between state and this subaction. Q(st, cit) represents the expected accumulated future rewards by including this sub-action. The agent then greedily picks the top-K sub-actions with highest values to achieve the highest Q(st, at). In this approach, we are assuming the long-term rewards associated with sub-actions are independent of each other. More specifically, greedily picking the top-K sub-actions is equivalent to maximizing the following action value function:\nQ(st, at) = K∑ i=1 Q(st, c i t) (2)\nwhile satisfying (1). We call this proposed method DRRN-Sum, and its architecture is shown in Figure 2(c). Similarly as in DRRN, we use two networks to model state side and action side. However, for different sub-actions, we keep the network parameters tied. We also use the same top layer dimension and the same pairwise interaction function for all subactions.\nIn the case of a linear additive interaction, such as an inner product or bilinear operation, Equation (2) is equivalent to computing interaction between the state embedding and an action embedding, where the action embedding is obtained linearly by summing over K sub-action embeddings. When subactions have strong correlation, this independence assumption is invalid and can result in a poor estimation of Q(st, at). For example, most people are interested in the total information stored in the combined action at. Due to content redundancy in the sub-actions c1t , c 2 t , · · · , cKt , we expect Q(st, at) to\nbe smaller than ∑\niQ(st, c i t).\nTo come up with a general model for handling a combinatorial action value function, we further propose DRRN-BiLSTM (Figure 2(d)). In this architecture, we use a DNN to model embedding of each comment. Then a Bidirectional Long Short-Term Memory (Graves and Schmidhuber, 2005) is used to combine a sequence of K comment embeddings. As the Bidirectional LSTM has a larger capacity due to its nonlinear structure, we expect it will capture more details on how the embeddings in each subactions combined into an action embedding. Note that both of our proposed methods (DRRN-Sum and DRRN-BiLSTM) can handle a varying value of K, while for baselines DQN and DRRN, we need to use a fixed K in training and testing."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets and Experimental Configurations",
      "text" : "Our data consists of 5 subreddits (askscience, askmen, todayilearned, worldnews, nfl) with diverse topics and genres. In our experiments, in order to have long enough discussion threads, we filter out discussion trees with fewer than 100 comments. For each subreddit, we randomly partition 90% of the data for online training, and 10% of the data for testing (deployment). The basic subreddit statistics are shown in Table 1. We report the random policy performances and heuristic upper bound performances (averaged over 10000 episodes) in Table 2 and Table 3.3 The upper bound performances are obtained using stabilized karma scores and offline constructed tree structure. The mean and standard deviation are obtained by 5 independent runs.\nIn all our experiments we set N = 10. Explicitly representing all N -choose-K actions requires a lot of memory and does not scale up. We therefore use\n3Upper bounds are estimated by greedily searching through each discussion tree to find K max karma discussion threads (overlapped comments are counted only once). This upper bound may not be attainable in real-time setting."
    }, {
      "heading" : "K Random Upper bound",
      "text" : "a variant of Q-learning: when taking the max over possible next-actions, we instead randomly subsample m′ actions and take the max over them. We set m′ = 10 throughout our experiments. This heuristic technique works well in our experiments.\nFor text preprocessing we remove punctuations and lowercasing capital letters. For each state st and comment cit, we use a bag-of-words representation and they share the same vocabulary. The vocabulary contains the most frequent 5000 words, and out-ofvocabulary rate is 7.1%.\nIn terms of the Q-learning agent, fully-connected neural networks are used for text embeddings. The network has L = 2 hidden layers, each with 20 nodes, and model parameters are initialized with small random numbers. -greedy is used for exploration-exploitation, and we keep = 0.1 throughout online training and testing. We pick the discount factor γ = 0.9. During online training, we use experience replay (Lin, 1993) and the memory size is set to 10000 tuples of (st, at, rt+1, st+1). For each experience replay, 500 episodes are generated and stored in a first-in-first-out fashion, and multiple epochs are trained for each model. Minibatch stochastic gradient descent is implemented with a batch size of 100. The learning rate is constant: ηt = 0.000001.\nThe proposed methods are compared with three baseline models: Linear, per-action DQN (PA-\nDQN), and DRRN. For both Linear and PA-DQN, the state and comments are concatenated as an input. For DRRN, the state and comments are sent through two separate deep neural networks. However, in our baselines, we do not explicitly model how values associated with each comment are combined to form the action value. For the DRRN baseline and proposed methods (DRRN-Sum and DRRN-BiLSTM), we use an inner product as the pairwise interaction function."
    }, {
      "heading" : "5.2 Experimental Results",
      "text" : "In Figure 3 we provide learning curves of different models on the askscience subreddit during online learning. In this experiment, we set N = 10,K = 3. Each curve is obtained by averaging over 3 independent runs, and the error bars are also shown. All models start with random performance, and converge after approximately 15 experience replays. The DRRN-Sum converges as fast as baseline models, with better converged performance. DRRNBiLSTM converges slower than other methods, but with the best converged performance.\nAfter we train all the models on the training set, we fix the model parameters and apply (deploy) on the test set, where the models predict which action to take but no reward is shown until evaluation. The test performance is averaged over 1000 episodes, and we report mean and standard deviation over 5 independent runs."
    }, {
      "heading" : "K DRRN-Sum DRRN-BiLSTM",
      "text" : "On askscience, we try multiple settings with N = 10, K = 2, 3, 4, 5 and the results are shown in Table 4. Both DRRN-Sum and DRRN-BiLSTM consistently outperform baseline methods. The DRRNBiLSTM performs better with larger K, probably due to the greater chance of redundancy in combining more sub-actions.\nWe also perform online training and test across different subreddits. With N = 10,K = 3, the test performance gains over the linear baseline are shown in Figure 4. Again, the test performance is averaged over 1000 episodes, and we report mean and standard deviation over 5 independent runs. The findings are consistent with those for askscience. Since different subreddits may have very different karma scores distributions and language style, this suggests the algorithms apply to different text genres.\nIn actual model deployment, a possible scenario is that users may have different requests. For example, a user may ask the agent to provide K = 2 discussion threads on one day, due to limited reading time, and ask the agent to provide K = 5 discussion threads on the other day. For the baseline models (Linear, PA-DQN, DRRN), we will need to train separate models for different K’s. The proposed methods (DRRN-Sum and DRRN-BiLSTM), on the other hand, can easily handle a varying K. To test whether the performance indeed generalizes well, we train proposed models on askscience with N = 10,K = 3 and test them with N = 10,K ∈ 2, 4, 5, as shown in Table 5. Compared to the proposed models that are specifically trained for these K’s (Table 4), the generalized test performance indeed degrades, as expected. However, in many cases, our proposed methods still outperform all three baselines (Linear, PA-DQN and DRRN) that are trained specifically for these K’s. This shows that the proposed methods can generalize to varying K’s even if it is trained on a particular value of K.\nIn Table 6, we show an anecdotal example with state and sub-actions. The two sub-actions are strongly correlated and have redundant information. By combining the second sub-action compared to choosing just the first sub-action alone, DRRN-Sum and DRRN-BiLSTM predict 86% and 26% relative increase in action-value, respectively. Since these two sub-actions are highly redundant, we hypothesize DRRN-BiLSTM is better than DRRN-Sum at capturing interdependency between sub-actions."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we introduce a new reinforcement learning task associated with predicting and tracking popular threads on Reddit. The states and actions are all described by natural language so the\ntask is useful for language studies. We then develop novel deep Q-learning architectures to better model the state-action value function with a combinatorial action space. The proposed DRRN BiLSTM method not only performs better across different experimental configurations and domains, but also generalizes well for scenarios where the user can request changes in the number tracked.\nThis work represents a first step towards addressing the popularity prediction and tracking problem. While performance of the system beats several baselines, it still falls far short of the oracle result. Prior work has shown that timing is an important factor in predicting popularity (Lampe and Resnick, 2004; Jaech et al., 2015), and all the proposed models would benefit from incorporating this information. Another variant might consider short-term reactions to a comment, if any, in the update window. It would also be of interest to explore implementations of backtracking in the sub-action space (incurring a cost), in order to recommend comments that were not selected earlier but have become highly popular. Lastly, it will be important to study principled solutions for handling the computational complexity of the combinatorial action space."
    } ],
    "references" : [ {
      "title" : "Topic detection and tracking: event-based information organization, volume 12",
      "author" : [ "J. Allan" ],
      "venue" : null,
      "citeRegEx" : "Allan.,? \\Q2012\\E",
      "shortCiteRegEx" : "Allan.",
      "year" : 2012
    }, {
      "title" : "Reinforcement learning for mapping instructions to actions",
      "author" : [ "H. Chen", "L. Zettlemoyer", "R. Barzilay" ],
      "venue" : "In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th IJCNLP,",
      "citeRegEx" : "Branavan et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Branavan et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning to win by reading manuals in a monte-carlo framework",
      "author" : [ "D. Silver", "R. Barzilay" ],
      "venue" : "In Proc. of the Annual Meeting of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Branavan et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Branavan et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep learning: Methods and applications",
      "author" : [ "Deng", "Yu2014] L. Deng", "D. Yu" ],
      "venue" : "Foundations and Trends in Signal Processing,",
      "citeRegEx" : "Deng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2014
    }, {
      "title" : "Benchmarking deep reinforcement learning for continuous control",
      "author" : [ "Duan et al.2016] Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning (ICML)",
      "citeRegEx" : "Duan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679",
      "author" : [ "R. Evans", "H. van Hasselt", "P. Sunehag", "T. Lillicrap", "J. Hunt" ],
      "venue" : null,
      "citeRegEx" : "Dulac.Arnold et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dulac.Arnold et al\\.",
      "year" : 2016
    }, {
      "title" : "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "author" : [ "Graves", "Schmidhuber2005] A. Graves", "J. Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Graves et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "Multiagent planning with factored mdps",
      "author" : [ "Guestrin et al.2001] C. Guestrin", "D. Koller", "R. Parr" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Guestrin et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Guestrin et al\\.",
      "year" : 2001
    }, {
      "title" : "Deep reinforcement learning in parameterized action space",
      "author" : [ "Hausknecht", "Stone2016] M. Hausknecht", "P. Stone" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Hausknecht et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hausknecht et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep reinforcement learning with an action space defined by natural language. arXiv preprint arXiv:1511.04636",
      "author" : [ "He et al.2015] J. He", "J. Chen", "X. He", "J. Gao", "L. Li", "L. Deng", "M. Ostendorf" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research",
      "author" : [ "Hinton et al.2012] G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury" ],
      "venue" : null,
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Talking to the crowd: What do people react to in online discussions",
      "author" : [ "Jaech et al.2015] A. Jaech", "V. Zayats", "H. Fang", "M. Ostendorf", "H. Hajishirzi" ],
      "venue" : null,
      "citeRegEx" : "Jaech et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jaech et al\\.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "I. Sutskever", "G. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Slash(dot) and burn: distributed moderation in a large online conversation space",
      "author" : [ "Lampe", "Resnick2004] C. Lampe", "P. Resnick" ],
      "venue" : "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,",
      "citeRegEx" : "Lampe et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lampe et al\\.",
      "year" : 2004
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "J. J Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning for robots using neural networks",
      "author" : [ "L-J. Lin" ],
      "venue" : "Technical report, DTIC Document",
      "citeRegEx" : "Lin.,? \\Q1993\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 1993
    }, {
      "title" : "Twittermonitor: trend detection over the twitter stream",
      "author" : [ "Mathioudakis", "Koudas2010] M. Mathioudakis", "N. Koudas" ],
      "venue" : "In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data,",
      "citeRegEx" : "Mathioudakis et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mathioudakis et al\\.",
      "year" : 2010
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Mnih et al.2015] V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. A Rusu", "J. Veness", "M. G Bellemare", "A. Graves", "M. Riedmiller", "A. K Fidjeland", "G. Ostrovski" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Language understanding for textbased games using deep reinforcement learning",
      "author" : [ "T. Kulkarni", "R. Barzilay" ],
      "venue" : "In Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving information extraction by acquiring external evidence with reinforcement learning",
      "author" : [ "A. Yala", "R. Barzilay" ],
      "venue" : "arXiv preprint arXiv:1603.07954",
      "citeRegEx" : "Narasimhan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2016
    }, {
      "title" : "Webnav: A new large-scale task for natural language based sequential decision making",
      "author" : [ "Nogueira", "Cho2016] R. Nogueira", "K. Cho" ],
      "venue" : "arXiv preprint arXiv:1602.02261",
      "citeRegEx" : "Nogueira et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nogueira et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning with factored states and actions",
      "author" : [ "Sallans", "Hinton2004] B. Sallans", "G. E Hinton" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Sallans et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Sallans et al\\.",
      "year" : 2004
    }, {
      "title" : "Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning",
      "author" : [ "Scheffler", "Young2002] K. Scheffler", "S. Young" ],
      "venue" : null,
      "citeRegEx" : "Scheffler et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Scheffler et al\\.",
      "year" : 2002
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search",
      "author" : [ "Silver et al.2016] D. Silver", "A. Huang", "C. J Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot" ],
      "venue" : null,
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning for spoken dialogue systems",
      "author" : [ "Singh et al.1999] S. P Singh", "M. J Kearns", "D. J Litman", "M. A Walker" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Singh et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 1999
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Sordoni et al.2015] A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan" ],
      "venue" : "NAACL-HLT",
      "citeRegEx" : "Sordoni et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal difference learning and td-gammon",
      "author" : [ "G. Tesauro" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Tesauro.,? \\Q1995\\E",
      "shortCiteRegEx" : "Tesauro.",
      "year" : 1995
    }, {
      "title" : "A network-based end-to-end trainable task-oriented dialogue system",
      "author" : [ "Wen et al.2016] T.-H. Wen", "M. Gasic", "N. Mrksic", "L. M Rojas-Barahona", "P.-H. Su", "S. Ultes", "D. Vandyke", "S. Young" ],
      "venue" : "arXiv preprint arXiv:1604.04562",
      "citeRegEx" : "Wen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2016
    }, {
      "title" : "An exploration of discussion threads in social news sites: A case study of the reddit community",
      "author" : [ "Weninger et al.2013] T. Weninger", "X.A. Zhu", "J. Han" ],
      "venue" : "In Advances in Social Networks Analysis and Mining (ASONAM),",
      "citeRegEx" : "Weninger et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Weninger et al\\.",
      "year" : 2013
    }, {
      "title" : "Linear submodular bandits and their application to diversified retrieval",
      "author" : [ "Yue", "Guestrin2011] Y. Yue", "C. Guestrin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Yue et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Early work such as TD-gammon used a neural network to approximate the state value function (Tesauro, 1995).",
      "startOffset" : 91,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "Recent advances in deep learning (LeCun et al., 2015; Deng and Yu, 2014; Hinton et al., 2012; Krizhevsky et al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al.",
      "startOffset" : 33,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "Recent advances in deep learning (LeCun et al., 2015; Deng and Yu, 2014; Hinton et al., 2012; Krizhevsky et al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al.",
      "startOffset" : 33,
      "endOffset" : 140
    }, {
      "referenceID" : 25,
      "context" : "Recent advances in deep learning (LeCun et al., 2015; Deng and Yu, 2014; Hinton et al., 2012; Krizhevsky et al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al.",
      "startOffset" : 33,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : ", 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016).",
      "startOffset" : 93,
      "endOffset" : 176
    }, {
      "referenceID" : 23,
      "context" : ", 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016).",
      "startOffset" : 93,
      "endOffset" : 176
    }, {
      "referenceID" : 14,
      "context" : ", 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016).",
      "startOffset" : 93,
      "endOffset" : 176
    }, {
      "referenceID" : 4,
      "context" : ", 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016).",
      "startOffset" : 93,
      "endOffset" : 176
    }, {
      "referenceID" : 24,
      "context" : "In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016).",
      "startOffset" : 167,
      "endOffset" : 232
    }, {
      "referenceID" : 27,
      "context" : "In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016).",
      "startOffset" : 167,
      "endOffset" : 232
    }, {
      "referenceID" : 1,
      "context" : "There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011).",
      "startOffset" : 162,
      "endOffset" : 208
    }, {
      "referenceID" : 2,
      "context" : "There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011).",
      "startOffset" : 162,
      "endOffset" : 208
    }, {
      "referenceID" : 17,
      "context" : "Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "He et al. (2015) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interac-",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 18,
      "context" : "Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve in-",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "valued parameters; factored Markov Decision Process (MDP) (Guestrin et al., 2001; Sallans and Hinton, 2004) assume certain independence between a next-state component and a sub-action.",
      "startOffset" : 58,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "valued parameters; factored Markov Decision Process (MDP) (Guestrin et al., 2001; Sallans and Hinton, 2004) assume certain independence between a next-state component and a sub-action. As for bandits setting, Yue and Guestrin (2011) considered diversification of multi-item recommendation, but their methodology is limited to using linear approximation with hand-crafted features.",
      "startOffset" : 59,
      "endOffset" : 233
    }, {
      "referenceID" : 9,
      "context" : "two separate deep neural networks for modeling state embedding and action embedding, performs better than per-action DQN (PA-DQN) in Figure 2(a), as well as other DQN variants for dealing with large action spaces(He et al., 2015).",
      "startOffset" : 212,
      "endOffset" : 229
    }, {
      "referenceID" : 16,
      "context" : "To handle a large state space, Mnih et al. (2015) proposed a Deep Q-Network (DQN).",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 15,
      "context" : "During online training, we use experience replay (Lin, 1993) and the memory size is set to 10000 tuples of (st, at, rt+1, st+1).",
      "startOffset" : 49,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "Prior work has shown that timing is an important factor in predicting popularity (Lampe and Resnick, 2004; Jaech et al., 2015), and all the proposed models would benefit from incorporating this information.",
      "startOffset" : 81,
      "endOffset" : 126
    } ],
    "year" : 2017,
    "abstractText" : "We introduce an online popularity prediction and tracking task as a benchmark task for reinforcement learning with a combinatorial, natural language action space. A specified number of discussion threads predicted to be popular are recommended, chosen from a fixed window of recent comments to track. Novel deep reinforcement learning architectures are studied for effective modeling of the value function associated with actions comprised of interdependent sub-actions. The proposed model, which represents dependence between sub-actions through a bi-directional LSTM, gives the best performance across different experimental configurations and domains, and it also generalizes well with varying numbers of recommendation requests.",
    "creator" : "LaTeX with hyperref package"
  }
}