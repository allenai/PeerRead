{
  "name" : "1411.5268.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sparse distributed localised gradient fused features of objects",
    "authors" : [ "Swathikiran Sudhakaran", "Alex Pappachen James" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The sparse, hierarchical and modular processing of natural signals are characteristics that relate to the ability of humans to recognise objects with high accuracy. In this paper, we report a sparse feature processing and encoding method targeted at improving the recognition performance of automated object recognition system. Randomly distributed selection of localised gradient enhanced features followed by the application of aggregate functions represents a modular and hierarchical approach to detect the object features. These object features, in combination with minimum distance classifier, results in object recognition system accuracies of 93% using ALOI, 92% using COIL-100 databases and 69% using PASCAL visual object challenge 2007 database, respectively. Robustness of object recognition performance is tested for variations in noise, object scaling and object shifts. Finally, a comparison with 8 existing object recognition methods indicated an improvement in recognition accuracy of 10% in ALOI, 8% in case of COIL-100 databases and 10% in PASCAL visual object challenge 2007 database.\nKeywords: Object recognition, Object features, Sparse features, Feature fusion, Hierarchy, Modularity, Brain inspired systems\n1. Introduction\nHumans can identify objects from naturally recorded images even under a diverse range of natural variability such as noise, pose changes and missing features, with high accuracy and speed. This robustness and accuracy of recognition is attributed to the structural and functional organisation of brain processing. It is understood that hierarchical structure of the brain processing, and the ability to integrate the information in a modular form attributes to this innate robustness. In the past, this idea from neuroscience has been explored by the computer scientist to successfully develop techniques such as hierarchical temporal networks [22], memory networks [25] and cognitive algorithms [23] for useful pattern recognition applications. In this paper, we inspire from the hierarchical processing, modularity and additional constraint of sparsity in developing a set of features useful for automated object recognition. We demonstrate that incorporating these ideas in the automated object recognition task can result in improved recognition accuracies.\nThe ability of the brain to process the information in a sparse manner allows the freedom of recognising objects even when the feature descriptions are partial and incomplete. This property is tightly linked not just to this functional aspect, rather also to the structural organisation of the brain. While the brain is constituted of billions of interconnected neurons, the information about a specific object is represented as an activation of a small number of neurons. Depending on the strength of the signal i.e. how frequently the object is observed by the human, the neuron responds in a distributed manner across the neuron network to encode and store the object features. These features are sparse in representation and offer several advantages [34]. The primary advantage is the ability to identify and store discriminative information with limited number of features. Since neuronal firing is a charge governed operation, the overall\n∗ Tel: +7 (7172) 709133\nEmail address: apj@ieee.org (Alex Pappachen James ) URL: www.biomircosystems.info/apj (Alex Pappachen James )\nPreprint submitted to Pattern Recognition September 28, 2014\nenergy consumption is reduced by minimising the number of neurons used for the representation of object. In computing terms, a reduced complexity of the data resulting from localised feature processing with sparsity constraints and hierarchy often results in lower complexity in implementing real-time systems that often appreciate a data driven scalable parallel computing solution.\nIn terms of signal processing literature, the sparse representation or sparse coding consists of generating a basis set for a signal that can be represented as linear combination of the elements from the set [40]. The main feature of this sparse representation is that only a small number of features are required for the representation of signals. This is analogous to the small number of active neurons within the large scale neural network that corresponds to discriminant object features. The disadvantage of this sparse coding technique is that these are signal dependant, i.e., the basis set generated for one set of signals cannot be used for others. Another major disadvantage associated with the sparse coding techniques is the computational complexity possessed by these techniques. The basis set is computed as an optimization problem requiring complex computations [26, 4, 28, 38] and is seen to have application in sparse representation of images [33]. Currently, the sparse coding of images is applied in many fields such as image denoising [30, 18], super-resolution [51, 50], face recognition [49], background modelling [12, 16], and image classification [29, 52].\nSparse representation of objects inspires from a number of biological vision theories [8, 27, 36, 47] and has shown to result in improved computational efficiency [3, 32]. The sparse representations reduces the dimensionality of the input data thereby simplifying the processing of the data. Although, reduction of the redundant information can lead to reduced feature dimensionality, the philosophy of sparse processing does not warrant reduction in data itself, instead it is more reflective of the preservation of useful information. The term “useful” here is strictly dependent on the application itself. For example, for object compression, it is the useful information that should reveal and reconstruct the details of the object, while for object recognition the identifying and preserving the distinct and discriminative features of the object become most useful.\nIn this paper, we inspire from the sparse processing ability of human brain, in developing a method for hierarchical and modular processing of features that involve sparse pixel selection, feature grouping, feature encoding and feature fusion. The resulting features are used for object recognition application, in a view to increase the intra-class similarities and to increase the inter-class differences between the object representations, which is reflected through the reported recognition accuracies. We show that these steps results in improved recognition accuracy of the object recognition task, and supports our view to inspire from the vision processing of biological systems in object recognition problems.\n2. Background\nThe image features represented as pixels can be represented as a linear regression problem with the unknown feature values projected onto a lower dimensional space y = Φx + n, where x ∈ <N is the feature vector, y ∈ <M is the measurement vector with M < N, Φ ∈ <M×N is the measurement matrix, and n ∈ <M is the additive noise. The measurement matrix is formed from the random selection of feature values from the images under consideration. The extraction of x from y in general is ill-posed as M < N and Φ would have non-trivial null space. This would mean that there are infinitely many solutions for identifying the true x [1, 2, 5, 6, 7]. This problem is of importance when the question at hand is signal recovery, and it not very much a serious issue in object recognition problem. However, the discriminative information if preserved by the signal recovery mechanisms by ignoring the redundant information can be useful for object recognition. A direct sparse representation such that x = Ψα in basis Ψ ∈ <N×N with K << N coefficients of α is sufficient to approximate the signal x. This can be seen as a basis pursuit problem. The determination of any Φ on x is the process of encoding and the process of recovering the x from y is referred to as decoding. Both these operations have a wide range of applications, nonetheless, in this paper, the attempt is on developing an encoding scheme. The development of any decoding scheme is ignored in this paper as its practically not required for an object recognition problem.\nAmong the leading examples of encoding and decoding are compressive sensing techniques [6, 7], that is used in a variety of signal processing applications. In addition, there exists several algorithms [10] for solving such basis pursuit problems by treating this as a subset of convex optimisation problem [11]. Some of the applications include signal component separation, missing data estimation, deconvolution, denoising and signal representation. Although, sparsity is widely used in signal processing with a view to find a generic model for representing object features, much\nl=1 b l\nof these thoughts correlate with the sparsity constraints critical for learning biologically plausible models. A common approach to impose such constraint is by inhibiting the less dominant responses, and using only the most dominant responses for feature encoding [34].\nWe incorporate the idea of biological systems to ignore the components of the signals that are least dominant to enforce the sparsity criteria. The localised nature of features is the another peculiarity with object features, as they are geometrically and spatially connected. However, these connectedness does not always ensure a distinctiveness in recorded images, as even minor variability between the test and training images, results in a statistically significant differences between them. As opposed to the approaches that impose geometric constraints [21, 9], we ignore the geometric constraints and form a randomised selection of features to create group of features. The idea of feature groups can be observed in some of the prominent object recognition methods [14].\nHierarchical processing is another major component of the human brain and visual system[15], which we try to accommodate in the feature encoding process. As opposed to the idea of hierarchical processing as observed in [42], we propose to implement the multi-level processing through the selection of the dominant features along the feature bit planes followed by digital to analog conversion. The digital to analog conversion can be seen as a subset of aggregate feature fusion method implemented using weighted aggregate operator function [17].\n3. Proposed method\n3.1. Sparse Distributed Fused Features of Objects\nThe proposed feature extraction process inspires from hierarchy, modular processing and sparsity constraints that reflect typical characteristics of brain and cognitive processing. Pixels in images form the primary unit of representation of object. These units when arranged spatially across two dimensions form a region of object and several such regions when geometrically placed in a visually perceptive manner leads to image representation of an object. Object information is encoded in a sparse manner within the physically and functionally localised neuron modules. This process of encoding features incorporates sparseness constraints that enable modular and hierarchical processing of features. The proposed approach creates the pixel groups by randomly selecting the pixels from the original image across its bit planes. After the feature groups are processed in parallel using a localised sparse selection, aggregation and binarization, the bit planes are fused to obtain encoded features.\nAlgorithm 1 Feature extraction using sparse distributed representation\nInput: Image(I) of dimension m×n Output: Feature vector subset (F∗) of length C = k×m×n/W\n1: Convert the image I to P bit planes 2: for p=0 to P − 1 do 3: Select pth bit plane 4: for c=1 to C do 5: Choose W number of binary pixels b∗ from I randomly 6: B(c) = PW ∗ 7: end for 8: Divide the feature vector Bp into groups of X number of B ∗ feature cells\n9: Bp = [{Bp (1), Bp (2)..Bp (C/X)}, ..., {Bp (C − C/X), Bp (C − 1)..Bp (C)}] 10: for every element in a group in Bp do 1, B∗(c) = max(B∗) 11: F∗ (c) = p\n12: end for 13: end for 14: for c=1 to C do 15: F∗(c) = PP\n0, otherwise\n2p−1 F∗ (c)\n16: end for p=1 p\nl=1 l\np\np\nThe list of steps involved in the proposed system is shown in Fig. 1 and formally described in Algorithm 1. We consider a quantised model of pixels for our algorithm, such that the integers when converted to the binary representation can be used to form image bit planes. As an example, consider an image with 8 bit pixel representation that can be used to create 8 bit planes or 8 binary images. The image is divided into several regions, and feature cells are formed from each of the distinct regions by randomly selecting the bits from it. Each element of the feature cell is formed by repeated and sparse selection of bits from random locations in the region followed by the summation of such selected bits.\nThe process of obtaining the feature cell F∗(c) requires the stages outlined in Fig. 1. The image I is binarized along its P bit planes to from binary images Ib . Given Rb ∈ Ib is a region in the image, the binary pixel b ∗ ∈ Rb is randomly selected W number of times, and aggregated to form the binary feature B(c) = PW b∗. The binary feature vector B is partitioned into several subsets B∗. The binary feature element F∗ (c) in plane p ∈ [0, P] is determined as:\nF∗ 1, B∗(c) = max(B∗)\np (c) = (1) 0, otherwise\nThe element F∗(c) in feature cell can be determined as the summation of binary weighted product typically used\nfor binary to continuous data conversion: P\nF∗(c) = X 2p−1 F∗ (c) (2)\np=1\nSuppose there are W pixels in the selected for generating the F∗ feature subset, several such cells form the feature vector F of the image. The length of the feature vector is given by (k×m×n)/W , m and n are the height and width of the image. The value of k determines whether there is any overlap of pixels among cells, and is referred to as the degree of overlap. If the value of k is equal to one, then the pixels are equally shared among the cells. If the value of k is less than one, then some of the pixels in the image will not be selected during the pixel selection step. In contrast, if the value of k is greater than one, some pixels will get selected by multiple cells. Fig 1(b) shows that the pixel at the location (5,1) was selected by both cell 1 and cell 2. The selected inputs are combined using aggregation operator to result in feature values of a cell. After computing the feature values of all the cell elements, we arrange the entire cells into several groups, such that each group contains X number of cells. These cells are computed in parallel and may be considered as the first level of modular processing. Inside each group, the cell with the highest value will be changed to 1 and the remaining cell’s value will be changed to 0. This binary operation signify the identification of most dominant value, and is inspired from the idea that neurons in the brain respond to the most dominant input responses that is often referred to as winner-take-all principle [35]. We repeat this operation for all the groups, and for all the bit planes. The resulting set of binary feature groups across the 8 bit planes represent the most dominant feature locations, and we consider this as a second stage of modular processing in analogy with limited number of fired neurons across the neuron network. In the final stage, the final feature vector is obtained by converting the 8 bit planes to integer form. This fusion process is essentially a weighted aggregate operation, and results in a global encoded feature vector of the object. The multiple stages of localised operations on pixel groups indicate hierarchical processing and the encoded features reflect sparse representation of the object.\n3.2. Gradient Enhanced Features\nHuman eye has the ability to detect the spatial change between the neighbourhood of pixels. The sensory neurons are responsible for detecting the changes as it responds to inter-pixel intensity changes across image space. This idea has been widely used in image processing community for developing first and second order gradient filters that calculate gradient features from the summation of weighted differences between the neighbourhood pixels across the entire image. To incorporate this aspect into our feature encoding scheme, we extend the idea of sparse selection of pixel bits and aggregative operators on gradients of pixel intensities.\nIn the proposed method, three different variants of features F are extracted from the images to capture different types of object features. The first feature vector is extracted directly from the pixel intensity features as outlined in Section 3.1, while the remaining two feature vectors are extracted from the gradient of the image. The gradient images are formed using the Sobel [44] and Prewitt [39] operator, and the vector of the gradient magnitude and gradient angles\np\nare used for generating the feature vector. Let I be an image of dimension m × n, then the gradient image G is given by:\nG(i, j) = I ∗ w = X X\nI(i − k, j − l)(w(k, l)) (3) k l\nwhere, ∗ represents the convolution operation, and w is the kernel matrix that defines the Sobel [44] and Prewitt [39] operators. Let Gx and Gy be the gradient images in the horizontal and vertical directions respectively. The gradient magnitude image |G| is formed as:\n|G(i, j)| = q\nGx (i, j) 2 + Gy (i, j) 2 (4)\nThe feature vector is generated from this gradient magnitude image using the sparse distributed representation technique illustrated in Fig 1. The second feature vector is generated from the gradient direction. The gradient direction Gθ is formulated as:\nGy (i, j) Gθ (i, j) = tan −1\nGx (i, j)\n(5)\nThe resulting gradient direction image will be having values in the range [-900 ,900 ]. To avoid 2’s complement computations resulting from negative numbers, the range of values is normalised to [00 ,1800 ] by adding 900 to remove the negative values. The feature vector will be then generated from this gradient direction image.\nThe vector consisting of pixel values I, gradient magnitude |G| and gradient direction Gθ form the primary input to the Algorithm 1 instead of just the pixel values I. This results in a feature vector F that is three times bigger in dimensions, however, incorporates the spatial change features improving the robustness of feature representation.\n3.3. Algorithm Parameters\nThe number of times pixels are selected (window size) W , number of elements in feature subset (cell size) X and the degree of overlap factor k are the major parameters of the proposed object feature extraction method. The length of the feature vector is determined by these parameters. The window size W determines the number of pixels used to determine the value of element within each feature subset. The value of W will be greater than one and is limited by\nthe number of pixels present in the image. Larger the value of W , more will be the number of pixel values that are encoded into a single feature vector cell. A larger value of W ensures a wider coverage of variations in the pixels in the object and ensure robustness the feature value calculation at the expense of higher computational cost. Feature subset or cell size (X) determines the number of inputs to be given to the winner take all network. X number of feature vector cells are selected and value of the cell with the highest value will be changed to one, while values of remaining cells will be changed to zero. This process is continued for the next set of X cells until all the feature vector values are changed to zero or one. As the cell size(X) increases, the number of pixels applied to the winner take all method also increases and as a result, the effectiveness of the algorithm in combating the local and global variations will get reduced. The degree of overlap (k) determines whether a single pixel is used to more than one number of cells. If the value of k is greater than one, then it means a single pixel is used by more than one feature vector cell. For k less than one, all the pixel values will not be selected for feature representation and can result in loose of discriminative information for forming the object features. The optimal parameter values selected for the experiments explained in Section 4 are W =16, X=4, k=2. The explanations for the same are given in Section 4.3.\n4. Experiments and Results\n4.1. Experimental Setup\nThe developed sparse distributed localised gradient fused features along with a minimum distance nearest neighbour classifier [13] form the modules of the object recognition experiment. In minimum distance classification, the distance between the test feature vector and each of the training feature vectors is computed and the pair with the least distance is selected as the most similar. In this paper, nearest neigbour classification is performed with three different metrics namely, Cityblock distance, Euclidean distance and Shepard’s similarity measure [43]. Equations (6), (7) and (8) show the cityblock distance, Euclidean distance and Shepard’s similarity function between two feature vectors Ftrain and Ftest . The pair with the least distance value is selected as the similar pair in cityblock distance and Euclidean distance, while for Shepard’s similarity measure, the pair with the highest value is selected as the most similar pair. This is because Shepard’s similarity measure represents the perceptive degree of similarity between two sequences rather than the distance as is given by cityblock and Euclidean distances.\nd = X\n|Ftrain (i) − Ftest (i)| (6) i\nd = X (Ftrain (i) − Ftest (i)) 2 (7)\ni\nd = X e−|Ftrain (i)−Ftest (i)| (8)\ni\nIn order to check the performance of the proposed method, object recognition experiments was performed on three different standard databases namely, Amsterdam library of object images (ALOI) [24], Columbia object image library (COIL-100) [31] and PASCAL visual object challenge 2007 database [20]. ALOI database contain images of 1000 different objects while the COIL-100 database contain images of 100 objects. The ALOI database contains images with dimension 192 × 144 pixels where as the dimension of the images in the COIL-100 database is 128 × 128 pixels. In both the databases, for each object, 72 different images are present, each taken at a different rotation angle at steps of 50 . The training set was formed by choosing images taken at 450 intervals. i.e., for each object class, eight images are used as training set. The remaining images are used as the test set. The PASCAL visual object challenge (PVOC) 2007 dataset contains images of 20 different objects. The training set consists of 5011 images and the test set consist of 4952 images. From each image, the required object is cropped using the given annotations and the dimension of the cropped object is changed to a standard size of 192 × 192 pixels. The recognition accuracy is calculated as the percentage of images correctly classified. Figure 2 shows sample images from the ALOI and COIL-100 database. In all the experiments mentioned in this section, the feature vector consists of pixel values, gradient magnitude and gradient direction with parameter values W =16, X=4 and k=2.\n4.2. Recognition Performance\nThe performance of the system is evaluated in terms of recognition accuracy. The recognition accuracy is computed as the ratio of number of images correctly classified to the total number of images used for classification. Seven different feature vectors were used for object recognition, each formed using different combinations of the three feature vectors explained in Section 3.2. The recognition accuracies for each of these feature vectors are listed in Table 1. In Table 1, the feature pix represents the feature vector generated from the image, feature mag represents the feature vector generated from the gradient magnitude image, feature dir represents the feature vector generated from the gradient direction image. Features pixmag, pixdir, magdir and pixmagdir represents the different combinations of the pix, mag and dir features.\n4.3. Effect of Window Size, Number of Cells and Feature Size\nAs mentioned in Section 3.3, these three parameters determine the robustness of the object feature and thereby the recognition performance. Figure 3(a) shows the recognition accuracy for the proposed method for different window sizes and different values of X with K = 1. The graph shows the effect of window size and number of cells in recognition performance. The degree of overlap (k) determines whether or not all the pixel values are chosen for feature extraction. In order to analyse their effects, the value of k is fixed as 1 and the values of W and X are varied from 2 to 128. The value of k is fixed at 1 so that all the pixel values are used atleast once for the computation of the feature vector. From the graph, it can be seen that the recognition accuracy is increased as the window size (W ) is increased. As mentioned in Section 3.3, the feature vector length will be longer for smaller W . On the other hand, the recognition accuracy gets increased as X increases and later it gets decreased. The recognition accuracy was higher for a value of X=4. So this value is selected as the final value of X. Figure 3(b) shows the variation in recognition accuracy for different values of k. The values of W and X is fixed as 16 and 4 respectively. It is observed that there is only a slight increase in the recognition accuracy as the value of k is increased. So the value of k is selected as 2 so that each pixel value is selected twice for the computation of the feature vector. In all the above experiments, the feature vector pixmagdir with Prewitt gradient was used.\n4.4. Noise Robustness\nTo test the effect of noise on the recognition accuracy, the test images were degraded with noise and the recognition accuracy is computed. The robustness of the proposed method against Gaussian noise, salt and pepper noise and speckle noise are studied. Figures 4(a), and 4(b) shows the recognition accuracy when Gaussian noise and speckle noise of different variances were added to the test images. Figure 4(c) shows the recognition accuracies when salt and pepper noise with different noise density was added to the test images. It is observed that increase in noise levels results in reduced recognition accuracy as a result of increased intra-class variability between the test and training images. The feature vector consists of pixel values, gradient magnitude and gradient direction with parameter values W =16, X=4 and k=2.\n4.5. Image Scaling and Translation Effects\nFigure 5(a) shows the impact of translation on recognition accuracy. Here, the test images were first subjected to pixel translations and the recognition accuracy computed. Shifting is done in the horizontal direction as well as in the vertical directions. From the graph it is observed that the recognition accuracy is least affected by horizontal shifting of the test images. To test the robustness of the proposed technique against scaling effect of images, the test images were scaled and the recognition accuracy computed. It is evident from Figure 5(b) that the proposed method is resilient to minor changes in scaling, while recognition performance drops with increasing variability in scaling.\n4.6. Variation in recognition accuracy with number of classes and training images\nFigure 6(a) shows the variation in the recognition accuracy when the number of classes is changed from 200 to\n1000. As can be seen from the graph, the recognition accuracy gets increased as the number of classes is reduced. Figure 6(b) shows the plot of recognition accuracy when the number of training images per class is varied. The performance of the system is very much improved with increase in number of training images. The recognition accuracy reaches 99.97% when the number of training images used is 32 which. i.e., when 44% of the images are used for training, a recognition accuracy of 99.97% is achieved. Also the accuracy reached 100% when 88% of images (64 images) are used for training.\n4.7. Comparison with Other Methods\nTable 2 shows the comparison between the proposed method and some of the commonly used recognition algorithms for object recognition on the ALOI and COIL-100 databases [48][19][41]. It is evident from the table that the proposed object recognition algorithm improved the recognition accuracy by 10% in ALOI and 8% in case of COIL100 databases compared to previous object recognition algorithms. Table 3 compares the performance of the proposed method on the Pascal visual object challenge 2007 database [20] with several other algorithms [45][53][37][53][46]. The parameters used are W =16, X=4, k=2 with Prewitt mask as gradient filter. Thus the proposed method outperformed the previous object recognition methods in terms of object recognition accuracy. There are several reasons for this improved recognition accuracy. The distributed pixel selection technique in the feature extraction provides robustness against local variations in an image. Local variations in an image may be defined as changes that occur to small regions of the image. In the pixel selection step, pixels from random positions are selected, added together and stored in cells. Since the pixels are selected in a distributed manner, the local variations also gets distributed among the cells and their effect on the feature vector gets nullified. The other type of variation that can occur in images is global variations that causes a change in pixel values throughout the image such as those resulting from scaling or shifting of images, and illumination changes. The inhibition step in Fig. 1(c) helps in minimising the global effect on the images. In the inhibition step, the cells are grouped together and in each group, the cell with the highest value is changed to 1 and all others are changed to 0. If a global variation occur, then the all the values in the cells will get increased. But the inhibition step will eliminate this effect, since all the cells get increased with almost the same value. Selection of distances measures such as Shepard’s similarity measure also has an important role in the increased recognition accuracy. For example, the Shepard’s similarity measure is known to eliminate the effect of outliers present in the feature vectors thereby reducing the intra-class mismatches and improving the recognition accuracy.\n5. Conclusion\nWe presented a set of sparsely distributed selection of pixels for a fused encoding scheme using both object image\npixel values and their pixel gradients. The properties of hierarchy, modularity, and sparsity constraints inspired from\nhuman visual processing system were applied to develop the proposed object recognition system. Hierarchy was implemented using multi-levels of processing that involved the feature grouping and bit plane processing followed by feature encoding. Modularity was incorporated by arranging and randomly selecting features in groups and repeating the same process over several times to form multiple feature groups. The sparsity constraints are imposed by the random selection of features to the groups and binarization within the group that can result in the exclusion of least important information from the original image. We demonstrated improved recognition accuracies for the proposed method when tested on the Amsterdam library of object images (ALOI) database, Columbia object image library (COIL-100) database and PASCAL visual object challenge 2007 database, validating the claims of hierarchy, modular processing and sparsity constraints being a useful concepts for implementing automated object recognition. The robustness of the proposed method against variability in noise, scaling and horizontal shifts was tested. The comparison with 8 different methods indicated a statistically significant improvement in recognition accuracies. In addition, the proposed method due to its modular processing ability supports implementation in scalable parallel processing architectures. The results indicate that like most of object matching methods the proposed method is not scale and shift invariant for larger variations between the test and train images. In addition, the noise also would have an impact in the recognition accuracies, as the linear difference operations in the gradient filtering improves the feature diversity, however, cannot inhibit noise. This remains a challenge and is a necessary future work to improve the robustness of the method.\nReferences\n[1] M. V. Afonso, J. M. Bioucas-Dias, M. A. T. Figueiredo, Fast image recovery using variable splitting and constrained optimization, IEEE\nTrans. on Image Processing 19 (9) (2010) 2345–2356.\n[2] M. V. Afonso, J. M. Bioucas-Dias, M. A. T. Figueiredo, An augmented lagrangian approach to the constrained optimization formulation of\nimaging inverse problems, IEEE Trans. Image Process. 20 (3) (2011) 681–695.\n[3] S. Agarwal, A. Awan, D. Roth, Learning to detect objects in images via a sparse, part-based representation, Pattern Analysis and Machine\nIntelligence, IEEE Transactions on 26 (11) (2004) 1475–1490.\n[4] M. Aharon, M. Elad, A. M. Bruckstein, K-svd and its non-negative variant for dictionary design, in: Optics & Photonics 2005, International\nSociety for Optics and Photonics, 2005, pp. 591411–591411.\n[5] J.-F. Aujol, G. Aubert, L. Blanc-Feraud, A. Chambolle, Image decomposition into a bounded variation component and an oscillating compo-\nnent, J. Math. Imag. Vis. 22 (2005) 71–88.\n[6] R. G. Baraniuk, Compressive sensing [lecture notes], IEEE Signal Processing Magazine 24 (4) (2007) 118–121. [7] R. G. Baraniuk, E. Candes, M. Elad, Y. Ma, Special issue on applications of sparse representation and compressive sensing, Proc. IEEE\n98 (6).\n[8] I. Biederman, Recognition-by-components: a theory of human image understanding., Psychological review 94 (2) (1987) 115. [9] G. Bouchard, B. Triggs, Hierarchical part-based visual object categorization, in: CVPR, 2005.\n[10] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, Distributed optimization and statistical learning via the alternating direction method of\nmultipliers, Foundations and Trends in Machine Learning 3 (1) (2011) 1–122.\n[11] S. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004. [12] V. Cevher, A. Sankaranarayanan, M. F. Duarte, D. Reddy, R. G. Baraniuk, R. Chellappa, Compressive sensing for background subtraction,\nin: Computer Vision–ECCV 2008, Springer, 2008, pp. 155–168.\n[13] T. Cover, P. Hart, Nearest neighbor pattern classification, Information Theory, IEEE Transactions on 13 (1) (1967) 21–27. [14] G. Csurka, C. Dance, J. Willamowski, L. Fan, C. Bray, Visual categorization with bags of keypoints, in: ECCV International Workshop on\nStatistical Learning in Computer Vision, 2004.\n[15] J. J. DiCarlo, D. Zoccolan, N. C. Rust, How does the brain solve visual object recognition?, Neuron 73 (3) (2012) 415–434. [16] M. Dikmen, T. S. Huang, Robust estimation of foreground in surveillance videos by sparse error estimation, in: Pattern Recognition, 2008.\nICPR 2008. 19th International Conference on, IEEE, 2008, pp. 1–4.\n[17] D. Dubois, H. Prade, On the use of aggregation operations in information fusion processes, Fuzzy Sets and Systems 142 (1) (2004) 143–161. [18] M. Elad, M. Aharon, Image denoising via sparse and redundant representations over learned dictionaries, Image Processing, IEEE Transac-\ntions on 15 (12) (2006) 3736–3745.\n[19] L. Elazary, L. Itti, A bayesian model for efficient visual search and recognition, Vision Research 50 (14) (2010) 1338–1352. [20] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, A. Zisserman, The PASCAL Visual Object Classes Challenge 2007 (VOC2007)\nResults, http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.\n[21] R. Fergus, P. Perona, A. Zisserman, Object class recognition by unsupervised scale-invariant learning, in: CVPR, 2003. [22] D. George, J. Hawkins, Towards a mathematical theory of cortical micro-circuits, PLoS Computational Biology 5 (10). [23] D. George, B. Jaros, The htm learning algorithms, Tech. rep., Numenta (March 2007). [24] J.-M. Geusebroek, G. J. Burghouts, A. W. Smeulders, The amsterdam library of object images, International Journal of Computer Vision\n61 (1) (2005) 103–112.\n[25] A. James, S. Dimitrijev, Cognitive memory network, IET Electronics Letters 46 (10) (2010) 677 – 678. [26] H. Lee, A. Battle, R. Raina, A. Ng, Efficient sparse coding algorithms, in: Advances in neural information processing systems, 2006, pp.\n801–808.\n[27] N. K. Logothetis, D. L. Sheinberg, Visual object recognition, Annual review of neuroscience 19 (1) (1996) 577–621. [28] J. Mairal, F. Bach, J. Ponce, G. Sapiro, A. Zisserman, Discriminative learned dictionaries for local image analysis, in: Computer Vision and\nPattern Recognition, 2008. CVPR 2008. IEEE Conference on, IEEE, 2008, pp. 1–8.\n[29] J. Mairal, F. Bach, J. Ponce, G. Sapiro, A. Zisserman, Supervised dictionary learning, arXiv preprint arXiv:0809.3083. [30] J. Mairal, M. Elad, G. Sapiro, Sparse representation for color image restoration, Image Processing, IEEE Transactions on 17 (1) (2008)\n53–69.\n[31] S. A. Nene, S. K. Nayar, H. Murase, Columbia object image library (coil-20), Dept. Comput. Sci., Columbia Univ., New York.[Online]\nhttp://www. cs. columbia. edu/CAVE/coil-20. html 62.\n[32] G. L. Oliveira, E. R. Nascimento, A. W. Vieira, M. F. M. Campos, Sparse spatial coding: A novel approach for efficient and accurate object\nrecognition, in: Robotics and Automation (ICRA), 2012 IEEE International Conference on, IEEE, 2012, pp. 2592–2598.\n[33] B. A. Olshausen, D. J. Field, Sparse coding with an overcomplete basis set: A strategy employed by v1?, Vision research 37 (23) (1997)\n3311–3325.\n[34] B. A. Olshausen, D. J. Field, Sparse coding of sensory inputs, Current opinion in neurobiology 14 (4) (2004) 481–487. [35] M. Oster, R. Douglas, S.-C. Liu, Computation with spikes in a winner-take-all network, Neural Computation 21 (9). [36] S. E. Palmer, Hierarchical structure in perceptual representation, Cognitive psychology 9 (4) (1977) 441–474. [37] F. Perronnin, C. Dance, Fisher kernels on visual vocabularies for image categorization, in: Computer Vision and Pattern Recognition, 2007.\nCVPR’07. IEEE Conference on, IEEE, 2007, pp. 1–8.\n[38] C. Poultney, S. Chopra, Y. L. Cun, et al., Efficient learning of sparse representations with an energy-based model, in: Advances in neural\ninformation processing systems, 2006, pp. 1137–1144.\n[39] J. M. Prewitt, Object enhancement and extraction, vol. 75, Academic Press, New York, 1970. [40] B. D. Rao, Signal processing with the sparseness constraint, in: Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998\nIEEE International Conference on, vol. 3, IEEE, 1998, pp. 1861–1864.\n[41] D. Roth, M.-H. Yang, N. Ahuja, Learning to recognize three-dimensional objects, Neural Computation 14 (5) (2002) 1071–1103. [42] T. Serre, L. Wolf, T. Poggio, Object recognition with features inspired by visual cortex, in: CVPR, 2005. [43] R. N. Shepard, et al., Toward a universal law of generalization for psychological science, Science 237 (4820) (1987) 1317–1323. [44] I. Sobel, G. Feldman, A 3x3 isotropic gradient operator for image processing, a talk at the Stanford Artificial Project in (1968) 271–272. [45] J. Van De Weijer, C. Schmid, Coloring local feature extraction, in: Computer Vision–ECCV 2006, Springer, 2006, pp. 334–348. [46] V. Viitaniemi, J. Laaksonen, Techniques for image classification, object detection and object segmentation, in: Visual Information Systems.\nWeb-Based Visual Information Search and Management, Springer, 2008, pp. 231–234.\n[47] E. Wachsmuth, M. Oram, D. Perrett, Recognition of objects and their component parts: responses of single units in the temporal cortex of the\nmacaque, Cerebral Cortex 4 (5) (1994) 509–522.\n[48] K. Welke, E. Oztop, A. Ude, R. Dillmann, G. Cheng, Learning feature representations for an object recognition system, in: Humanoid Robots,\n2006 6th IEEE-RAS International Conference on, IEEE, 2006, pp. 290–295.\n[49] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, Y. Ma, Robust face recognition via sparse representation, Pattern Analysis and Machine\nIntelligence, IEEE Transactions on 31 (2) (2009) 210–227.\n[50] J. Yang, J. Wright, T. Huang, Y. Ma, Image super-resolution as sparse representation of raw image patches, in: Computer Vision and Pattern\nRecognition, 2008. CVPR 2008. IEEE Conference on, IEEE, 2008, pp. 1–8.\n[51] J. Yang, J. Wright, T. S. Huang, Y. Ma, Image super-resolution via sparse representation, Image Processing, IEEE Transactions on 19 (11)\n(2010) 2861–2873.\n[52] J. Yang, K. Yu, Y. Gong, T. Huang, Linear spatial pyramid matching using sparse coding for image classification, in: Computer Vision and\nPattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE, 2009, pp. 1794–1801.\n[53] J. Zhang, M. Marszałek, S. Lazebnik, C. Schmid, Local features and kernels for classification of texture and object categories: A comprehen-\nsive study, International journal of computer vision 73 (2) (2007) 213–238."
    } ],
    "references" : [ {
      "title" : "Fast image recovery using variable splitting and constrained optimization",
      "author" : [ "M.V. Afonso", "J.M. Bioucas-Dias", "M.A.T. Figueiredo" ],
      "venue" : "IEEE Trans. on Image Processing 19 (9) ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An augmented lagrangian approach to the constrained optimization formulation of imaging inverse problems",
      "author" : [ "M.V. Afonso", "J.M. Bioucas-Dias", "M.A.T. Figueiredo" ],
      "venue" : "IEEE Trans. Image Process. 20 (3) ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning to detect objects in images via a sparse",
      "author" : [ "S. Agarwal", "A. Awan", "D. Roth" ],
      "venue" : "part-based representation, Pattern Analysis and Machine Intelligence, IEEE Transactions on 26 (11) ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "K-svd and its non-negative variant for dictionary design",
      "author" : [ "M. Aharon", "M. Elad", "A.M. Bruckstein" ],
      "venue" : "in: Optics & Photonics 2005, International Society for Optics and Photonics",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Image decomposition into a bounded variation component and an oscillating component",
      "author" : [ "J.-F. Aujol", "G. Aubert", "L. Blanc-Feraud", "A. Chambolle" ],
      "venue" : "J. Math. Imag. Vis. 22 ",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Compressive sensing [lecture notes",
      "author" : [ "R.G. Baraniuk" ],
      "venue" : "IEEE Signal Processing Magazine 24 (4) ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Recognition-by-components: a theory of human image understanding., Psychological review",
      "author" : [ "I. Biederman" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1987
    }, {
      "title" : "Hierarchical part-based visual object categorization",
      "author" : [ "G. Bouchard", "B. Triggs" ],
      "venue" : "in: CVPR",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning 3 (1) ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Compressive sensing for background subtraction",
      "author" : [ "V. Cevher", "A. Sankaranarayanan", "M.F. Duarte", "D. Reddy", "R.G. Baraniuk", "R. Chellappa" ],
      "venue" : "in: Computer Vision–ECCV 2008, Springer",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Nearest neighbor pattern classification",
      "author" : [ "T. Cover", "P. Hart" ],
      "venue" : "Information Theory, IEEE Transactions on 13 (1) ",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Visual categorization with bags of keypoints",
      "author" : [ "G. Csurka", "C. Dance", "J. Willamowski", "L. Fan", "C. Bray" ],
      "venue" : "in: ECCV International Workshop on Statistical Learning in Computer Vision",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "How does the brain solve visual object recognition",
      "author" : [ "J.J. DiCarlo", "D. Zoccolan", "N.C. Rust" ],
      "venue" : "Neuron 73 (3) ",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Robust estimation of foreground in surveillance videos by sparse error estimation",
      "author" : [ "M. Dikmen", "T.S. Huang" ],
      "venue" : "in: Pattern Recognition, 2008. ICPR 2008. 19th International Conference on, IEEE",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On the use of aggregation operations in information fusion processes",
      "author" : [ "D. Dubois", "H. Prade" ],
      "venue" : "Fuzzy Sets and Systems 142 (1) ",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Image denoising via sparse and redundant representations over learned dictionaries",
      "author" : [ "M. Elad", "M. Aharon" ],
      "venue" : "Image Processing, IEEE Transactions on 15 (12) ",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A bayesian model for efficient visual search and recognition",
      "author" : [ "L. Elazary", "L. Itti" ],
      "venue" : "Vision Research 50 (14) ",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A",
      "author" : [ "M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn" ],
      "venue" : "Zisserman, The PASCAL Visual Object Classes Challenge 2007 ",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Object class recognition by unsupervised scale-invariant learning",
      "author" : [ "R. Fergus", "P. Perona", "A. Zisserman" ],
      "venue" : "in: CVPR",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The htm learning algorithms",
      "author" : [ "D. George", "B. Jaros" ],
      "venue" : "Tech. rep., Numenta ",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The amsterdam library of object images",
      "author" : [ "J.-M. Geusebroek", "G.J. Burghouts", "A.W. Smeulders" ],
      "venue" : "International Journal of Computer Vision 61 (1) ",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Cognitive memory network",
      "author" : [ "A. James", "S. Dimitrijev" ],
      "venue" : "IET Electronics Letters 46 (10) ",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Efficient sparse coding algorithms",
      "author" : [ "H. Lee", "A. Battle", "R. Raina", "A. Ng" ],
      "venue" : "in: Advances in neural information processing systems",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Visual object recognition",
      "author" : [ "N.K. Logothetis", "D.L. Sheinberg" ],
      "venue" : "Annual review of neuroscience 19 (1) ",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Discriminative learned dictionaries for local image analysis",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman" ],
      "venue" : "in: Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, IEEE",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Sparse representation for color image restoration",
      "author" : [ "J. Mairal", "M. Elad", "G. Sapiro" ],
      "venue" : "Image Processing, IEEE Transactions on 17 (1) ",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Sparse spatial coding: A novel approach for efficient and accurate object recognition",
      "author" : [ "G.L. Oliveira", "E.R. Nascimento", "A.W. Vieira", "M.F.M. Campos" ],
      "venue" : "in: Robotics and Automation (ICRA), 2012 IEEE International Conference on, IEEE",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sparse coding with an overcomplete basis set: A strategy employed by v1",
      "author" : [ "B.A. Olshausen", "D.J. Field" ],
      "venue" : "Vision research 37 (23) ",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Sparse coding of sensory inputs",
      "author" : [ "B.A. Olshausen", "D.J. Field" ],
      "venue" : "Current opinion in neurobiology 14 (4) ",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Hierarchical structure in perceptual representation",
      "author" : [ "S.E. Palmer" ],
      "venue" : "Cognitive psychology 9 (4) ",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Fisher kernels on visual vocabularies for image categorization",
      "author" : [ "F. Perronnin", "C. Dance" ],
      "venue" : "in: Computer Vision and Pattern Recognition, 2007. CVPR’07. IEEE Conference on, IEEE",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Y",
      "author" : [ "C. Poultney", "S. Chopra" ],
      "venue" : "L. Cun, et al., Efficient learning of sparse representations with an energy-based model, in: Advances in neural information processing systems",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Object enhancement and extraction",
      "author" : [ "J.M. Prewitt" ],
      "venue" : "vol. 75, Academic Press, New York",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Signal processing with the sparseness constraint",
      "author" : [ "B.D. Rao" ],
      "venue" : "in: Acoustics, Speech and Signal Processing, 1998. Proceedings of the 1998 IEEE International Conference on, vol. 3, IEEE",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Learning to recognize three-dimensional objects",
      "author" : [ "D. Roth", "M.-H. Yang", "N. Ahuja" ],
      "venue" : "Neural Computation 14 (5) ",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Object recognition with features inspired by visual cortex",
      "author" : [ "T. Serre", "L. Wolf", "T. Poggio" ],
      "venue" : "in: CVPR",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Toward a universal law of generalization for psychological science, Science 237",
      "author" : [ "R.N. Shepard" ],
      "venue" : null,
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1987
    }, {
      "title" : "A 3x3 isotropic gradient operator for image processing",
      "author" : [ "I. Sobel", "G. Feldman" ],
      "venue" : "a talk at the Stanford Artificial Project in ",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Coloring local feature extraction, in: Computer Vision–ECCV",
      "author" : [ "J. Van De Weijer", "C. Schmid" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2006
    }, {
      "title" : "Techniques for image classification",
      "author" : [ "V. Viitaniemi", "J. Laaksonen" ],
      "venue" : "object detection and object segmentation, in: Visual Information Systems. Web-Based Visual Information Search and Management, Springer",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Recognition of objects and their component parts: responses of single units in the temporal cortex of the macaque",
      "author" : [ "E. Wachsmuth", "M. Oram", "D. Perrett" ],
      "venue" : "Cerebral Cortex 4 (5) ",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Learning feature representations for an object recognition system",
      "author" : [ "K. Welke", "E. Oztop", "A. Ude", "R. Dillmann", "G. Cheng" ],
      "venue" : "in: Humanoid Robots, 2006 6th IEEE-RAS International Conference on, IEEE",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Robust face recognition via sparse representation",
      "author" : [ "J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on 31 (2) ",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Image super-resolution as sparse representation of raw image patches",
      "author" : [ "J. Yang", "J. Wright", "T. Huang", "Y. Ma" ],
      "venue" : "in: Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, IEEE",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Image super-resolution via sparse representation",
      "author" : [ "J. Yang", "J. Wright", "T.S. Huang", "Y. Ma" ],
      "venue" : "Image Processing, IEEE Transactions on 19 (11) ",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Linear spatial pyramid matching using sparse coding for image classification",
      "author" : [ "J. Yang", "K. Yu", "Y. Gong", "T. Huang" ],
      "venue" : "in: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Local features and kernels for classification of texture and object categories: A comprehensive study",
      "author" : [ "J. Zhang", "M. Marszałek", "S. Lazebnik", "C. Schmid" ],
      "venue" : "International journal of computer vision 73 (2) ",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "In the past, this idea from neuroscience has been explored by the computer scientist to successfully develop techniques such as hierarchical temporal networks [22], memory networks [25] and cognitive algorithms [23] for useful pattern recognition applications.",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 20,
      "context" : "In the past, this idea from neuroscience has been explored by the computer scientist to successfully develop techniques such as hierarchical temporal networks [22], memory networks [25] and cognitive algorithms [23] for useful pattern recognition applications.",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 29,
      "context" : "These features are sparse in representation and offer several advantages [34].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 34,
      "context" : "In terms of signal processing literature, the sparse representation or sparse coding consists of generating a basis set for a signal that can be represented as linear combination of the elements from the set [40].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 23,
      "context" : "The basis set is computed as an optimization problem requiring complex computations [26, 4, 28, 38] and is seen to have application in sparse representation of images [33].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "The basis set is computed as an optimization problem requiring complex computations [26, 4, 28, 38] and is seen to have application in sparse representation of images [33].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "The basis set is computed as an optimization problem requiring complex computations [26, 4, 28, 38] and is seen to have application in sparse representation of images [33].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 32,
      "context" : "The basis set is computed as an optimization problem requiring complex computations [26, 4, 28, 38] and is seen to have application in sparse representation of images [33].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : "The basis set is computed as an optimization problem requiring complex computations [26, 4, 28, 38] and is seen to have application in sparse representation of images [33].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 26,
      "context" : "Currently, the sparse coding of images is applied in many fields such as image denoising [30, 18], super-resolution [51, 50], face recognition [49], background modelling [12, 16], and image classification [29, 52].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "Currently, the sparse coding of images is applied in many fields such as image denoising [30, 18], super-resolution [51, 50], face recognition [49], background modelling [12, 16], and image classification [29, 52].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 45,
      "context" : "Currently, the sparse coding of images is applied in many fields such as image denoising [30, 18], super-resolution [51, 50], face recognition [49], background modelling [12, 16], and image classification [29, 52].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 44,
      "context" : "Currently, the sparse coding of images is applied in many fields such as image denoising [30, 18], super-resolution [51, 50], face recognition [49], background modelling [12, 16], and image classification [29, 52].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 43,
      "context" : "Currently, the sparse coding of images is applied in many fields such as image denoising [30, 18], super-resolution [51, 50], face recognition [49], background modelling [12, 16], and image classification [29, 52].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "Currently, the sparse coding of images is applied in many fields such as image denoising [30, 18], super-resolution [51, 50], face recognition [49], background modelling [12, 16], and image classification [29, 52].",
      "startOffset" : 170,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "Currently, the sparse coding of images is applied in many fields such as image denoising [30, 18], super-resolution [51, 50], face recognition [49], background modelling [12, 16], and image classification [29, 52].",
      "startOffset" : 170,
      "endOffset" : 178
    }, {
      "referenceID" : 46,
      "context" : "Currently, the sparse coding of images is applied in many fields such as image denoising [30, 18], super-resolution [51, 50], face recognition [49], background modelling [12, 16], and image classification [29, 52].",
      "startOffset" : 205,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "Sparse representation of objects inspires from a number of biological vision theories [8, 27, 36, 47] and has shown to result in improved computational efficiency [3, 32].",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "Sparse representation of objects inspires from a number of biological vision theories [8, 27, 36, 47] and has shown to result in improved computational efficiency [3, 32].",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : "Sparse representation of objects inspires from a number of biological vision theories [8, 27, 36, 47] and has shown to result in improved computational efficiency [3, 32].",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 41,
      "context" : "Sparse representation of objects inspires from a number of biological vision theories [8, 27, 36, 47] and has shown to result in improved computational efficiency [3, 32].",
      "startOffset" : 86,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "Sparse representation of objects inspires from a number of biological vision theories [8, 27, 36, 47] and has shown to result in improved computational efficiency [3, 32].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 27,
      "context" : "Sparse representation of objects inspires from a number of biological vision theories [8, 27, 36, 47] and has shown to result in improved computational efficiency [3, 32].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "This would mean that there are infinitely many solutions for identifying the true x [1, 2, 5, 6, 7].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : "This would mean that there are infinitely many solutions for identifying the true x [1, 2, 5, 6, 7].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "This would mean that there are infinitely many solutions for identifying the true x [1, 2, 5, 6, 7].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "This would mean that there are infinitely many solutions for identifying the true x [1, 2, 5, 6, 7].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "Among the leading examples of encoding and decoding are compressive sensing techniques [6, 7], that is used in a variety of signal processing applications.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "In addition, there exists several algorithms [10] for solving such basis pursuit problems by treating this as a subset of convex optimisation problem [11].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "In addition, there exists several algorithms [10] for solving such basis pursuit problems by treating this as a subset of convex optimisation problem [11].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 29,
      "context" : "A common approach to impose such constraint is by inhibiting the less dominant responses, and using only the most dominant responses for feature encoding [34].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 19,
      "context" : "As opposed to the approaches that impose geometric constraints [21, 9], we ignore the geometric constraints and form a randomised selection of features to create group of features.",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "As opposed to the approaches that impose geometric constraints [21, 9], we ignore the geometric constraints and form a randomised selection of features to create group of features.",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "The idea of feature groups can be observed in some of the prominent object recognition methods [14].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "Hierarchical processing is another major component of the human brain and visual system[15], which we try to accommodate in the feature encoding process.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 36,
      "context" : "As opposed to the idea of hierarchical processing as observed in [42], we propose to implement the multi-level processing through the selection of the dominant features along the feature bit planes followed by digital to analog conversion.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "The digital to analog conversion can be seen as a subset of aggregate feature fusion method implemented using weighted aggregate operator function [17].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 38,
      "context" : "The gradient images are formed using the Sobel [44] and Prewitt [39] operator, and the vector of the gradient magnitude and gradient angles",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 33,
      "context" : "The gradient images are formed using the Sobel [44] and Prewitt [39] operator, and the vector of the gradient magnitude and gradient angles",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 38,
      "context" : "where, ∗ represents the convolution operation, and w is the kernel matrix that defines the Sobel [44] and Prewitt [39] operators.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 33,
      "context" : "where, ∗ represents the convolution operation, and w is the kernel matrix that defines the Sobel [44] and Prewitt [39] operators.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : "The developed sparse distributed localised gradient fused features along with a minimum distance nearest neighbour classifier [13] form the modules of the object recognition experiment.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 37,
      "context" : "In this paper, nearest neigbour classification is performed with three different metrics namely, Cityblock distance, Euclidean distance and Shepard’s similarity measure [43].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : "different standard databases namely, Amsterdam library of object images (ALOI) [24], Columbia object image library (COIL-100) [31] and PASCAL visual object challenge 2007 database [20].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 18,
      "context" : "different standard databases namely, Amsterdam library of object images (ALOI) [24], Columbia object image library (COIL-100) [31] and PASCAL visual object challenge 2007 database [20].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 42,
      "context" : "Gabor Jet based object representations[48] 83 -",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 17,
      "context" : "SalBayes[19] 83.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 17,
      "context" : "SIFT[19] 72.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "HMAX[19] 83.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 35,
      "context" : "SNoW/intensity[41] 85.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 35,
      "context" : "SNoW/edges[41] 89.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 35,
      "context" : "Linear SVM[41] 84.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 35,
      "context" : "Nearest Neighbour[41] 79.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 39,
      "context" : "Proposed method INRIA Genetic[45] INRIA Flat[53] XRCE[37] QMUL LSPCH[53] TKK[46]",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 47,
      "context" : "Proposed method INRIA Genetic[45] INRIA Flat[53] XRCE[37] QMUL LSPCH[53] TKK[46]",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 31,
      "context" : "Proposed method INRIA Genetic[45] INRIA Flat[53] XRCE[37] QMUL LSPCH[53] TKK[46]",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 47,
      "context" : "Proposed method INRIA Genetic[45] INRIA Flat[53] XRCE[37] QMUL LSPCH[53] TKK[46]",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 40,
      "context" : "Proposed method INRIA Genetic[45] INRIA Flat[53] XRCE[37] QMUL LSPCH[53] TKK[46]",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 42,
      "context" : "Table 2 shows the comparison between the proposed method and some of the commonly used recognition algorithms for object recognition on the ALOI and COIL-100 databases [48][19][41].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 17,
      "context" : "Table 2 shows the comparison between the proposed method and some of the commonly used recognition algorithms for object recognition on the ALOI and COIL-100 databases [48][19][41].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 35,
      "context" : "Table 2 shows the comparison between the proposed method and some of the commonly used recognition algorithms for object recognition on the ALOI and COIL-100 databases [48][19][41].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 18,
      "context" : "Table 3 compares the performance of the proposed method on the Pascal visual object challenge 2007 database [20] with several other algorithms [45][53][37][53][46].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 39,
      "context" : "Table 3 compares the performance of the proposed method on the Pascal visual object challenge 2007 database [20] with several other algorithms [45][53][37][53][46].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 47,
      "context" : "Table 3 compares the performance of the proposed method on the Pascal visual object challenge 2007 database [20] with several other algorithms [45][53][37][53][46].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 31,
      "context" : "Table 3 compares the performance of the proposed method on the Pascal visual object challenge 2007 database [20] with several other algorithms [45][53][37][53][46].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 47,
      "context" : "Table 3 compares the performance of the proposed method on the Pascal visual object challenge 2007 database [20] with several other algorithms [45][53][37][53][46].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 40,
      "context" : "Table 3 compares the performance of the proposed method on the Pascal visual object challenge 2007 database [20] with several other algorithms [45][53][37][53][46].",
      "startOffset" : 159,
      "endOffset" : 163
    } ],
    "year" : 2014,
    "abstractText" : "The sparse, hierarchical and modular processing of natural signals are characteristics that relate to the ability of humans to recognise objects with high accuracy. In this paper, we report a sparse feature processing and encoding method targeted at improving the recognition performance of automated object recognition system. Randomly distributed selection of localised gradient enhanced features followed by the application of aggregate functions represents a modular and hierarchical approach to detect the object features. These object features, in combination with minimum distance classifier, results in object recognition system accuracies of 93% using ALOI, 92% using COIL-100 databases and 69% using PASCAL visual object challenge 2007 database, respectively. Robustness of object recognition performance is tested for variations in noise, object scaling and object shifts. Finally, a comparison with 8 existing object recognition methods indicated an improvement in recognition accuracy of 10% in ALOI, 8% in case of COIL-100 databases and 10% in PASCAL visual object challenge 2007 database.",
    "creator" : "Microsoft® Word 2010"
  }
}