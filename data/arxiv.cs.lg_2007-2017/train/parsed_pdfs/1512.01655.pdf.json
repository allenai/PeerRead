{
  "name" : "1512.01655.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Approximated and User Steerable tSNE for Progressive Visual Analytics",
    "authors" : [ "Nicola Pezzotti", "Boudewijn P.F. Lelieveldt", "Laurens van der Maaten", "Thomas Höllt", "Elmar Eisemann", "Anna Vilanova" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n01 65\n5v 3\n[ cs\n.C V\n] 1\n6 Ju\nn 20\n16 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. -, NO. -, MONTH - 1\nIndex Terms—High Dimensional Data, Dimensionality Reduction, Progressive Visual Analytics, Approximate Computation\n✦"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "IMPORTANT: this work has been extended and the final version is published on the Transaction on Visualization and Computer Graphics journal. Please refer to it using the following DOI: http://dx.doi.org/10.1109/TVCG.2016.2570755\nV ISUAL analysis of high dimensional data is a chal-lenging process. Direct visualizations such as parallel coordinates [1] or scatterplot matrices [2] work well for a few dimensions, but do not scale to hundreds or thousands of dimensions. Typically indirect visualization is used for these cases. First the dimensionality of the data is reduced, usually to two or three dimensions, then the remaining dimensions are used to lay out the data for visual inspection, for example in a two dimensional scatterplot. Dimensionality reduction techniques have been an active field of research in the last years, resulting in a number of viable techniques [3]. A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12]. tSNE produces 2D and 3D embeddings that are meant to preserve local structure in the high-dimensional data. The analyst looks at the embeddings with the goal to identify clusters or patterns that are used to generate new hypothesis on the data, however, the computational complexity of this technique does\n• N. Pezzotti, B. P.F. Lelieveldt, L. van der Maaten, T. Höllt, E. Eisemann, and A. Vilanova are with the Department of Intelligent Systems, Delft University of Technology, Delft, the Netherlands.\n• B. P.F. Lelieveldt is with the Division of Image Processing, Department of Radiology, Leiden University Medical Center, Leiden, the Netherlands.\nManuscript received August 4, 2015; revised -, -.\nnot allow direct employment in interactive systems. This limitation makes the analytic process a time consuming task that can take hours, or even days, to adjust the parameters and generate the right embedding to be analyzed.\nRecently Stolper et al. [13], as well as Mühlbacher et al. [14] introduced Progressive Visual Analytics. The idea of Progressive Visual Analytics is to provide the user with meaningful intermediate results, in case computation of the final result is too costly. Based on these intermediate results the user can start with the analysis process. Mühlbacher et al. also provide a set of requirements, which an algorithm needs to fulfill in order to be suitable for Progressive Visual Analytics. Based on these requirements they analyze a series of different algorithms, commonly deployed in visual analytics systems, and conclude that, for example, tSNE fulfills all requirements. The reason being that the minimization in tSNE is built on the iterative gradient descent technique [5] and can therefore be used directly for a per-iteration visualization, as well as interaction with the intermediate results. However, Mühlbacher et al. ignore the fact that the distances in the high-dimensional space need to be precomputed to start the minimization process. In fact this initialization process is dominating the overall performance of tSNE. Even with a per-iteration visualization of the intermediate results [13], [14], [15] the initialization time will force the user to wait minutes, or even hours, before the first intermediate result can be generated on a state-of-theart desktop computer. Every modification of the data, for example, the addition of data-points or a change in the highdimensional space, will force the user to wait for the full reinitialization of the algorithm.\nIn this work we present A-tSNE, a novel approach to adapt the complete tSNE pipeline, including a distance computation for the Progressive Visual Analytics paradigm. Instead of precomputing precise distances we propose to\napproximate the distances using Approximated K-Nearest Neighborhood queries. This allows to start the computation of the iterative minimization nearly instantly after loading the data. Based on the intermediate results of the tSNE, the user can now start the interpretation process of the data immediately. Further, we modified the gradient descent of tSNE such that it allows the incorporation of updated data during the iterative process. This allows for a continuous refining of approximated neighborhoods in the background and an update of the embedding without restarting the optimization and, eventually, arriving at the precise solution. Furthermore, we allow the user to steer the level of approximation by selecting points of interest, such as clusters which appear in the very early stages of the optimization and enable an interactive exploration of the high-dimensional data.\nOur contributions are as follows:\n1) We present A-tSNE, a twofold evolution of the tSNE algorithm, which\na) minimizes initialization time and as such enables immediate inspection of preliminary computation results. b) allows for interactive modification, removal or addition of high-dimensional data, without disrupting the visual analysis process.\n2) Using a set of standard benchmark data sets, we show large performance increases of A-tSNE compared to the state of the art while maintaining high precision. 3) We developed an interactive system for a visual analysis of high dimensional data, allowing the user to inspect and steer the level of approximation and we illustrate the benefits of exploratory possibilities in a real-world research scenario and for the realtime analysis of high-dimensional streams."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "The tSNE [5] algorithm builds the foundation of this work. As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18]. tSNE is a non-linear dimensionality-reduction algorithm that aims at preserving local structures in the embedding, whilst showing global information, such as the presence of clusters at several scales.\ntSNE’s computational and memory complexity is O(N2), where N is the number of data-points, which constrains the application of the technique. An evolution of the algorithm, called Barnes-Hut-SNE (BH-SNE) [4], [6], reduces the computational complexity to O(N log(N)) and the memory complexity to O(N). This approach was also developed in parallel by Yang et al. [19]. However, despite the increased performance, it still cannot be used to interactively explore the data in a desktop environment.\nInteractive performance is at the center of the latest developments in Visual Analytics. New analytical tools and algorithms, which are able to trade accuracy for speed and offer the possibility to interactively refine results [20], [21],\nare needed to deal with the scalability issues of existing analytics algorithms like tSNE. Mühlbacher et al. [14] defined different strategies to increase the user involvement in existing algorithms. They provide an in-depth analysis on how the interconnection between the visualization and the analytic modules can be achieved. Stolper et al. [13] defined the term Progressive Visual Analytics that describes techniques that allow the analyst to directly interact with the analytics process. Visualization of intermediate results is used to help the user, for example, to find optimal parameter settings or filter the data. The authors also provide a guideline for the design of Progressive Visual Analytics. Many algorithms, however, are not suited right away for Progressive Visual Analytics since the production of intermediate results is computationally too intensive or they do not generate useful intermediate results at all. tSNE is an example of such an algorithm because of its initialization process.\nTo overcome this problem, we propose to compute an approximation of tSNE’s initialization stage, followed by a user steerable [22] refinement of the level of approximation. To compute the conditional probabilities needed by BHSNE, a K-Nearest Neighborhood (KNN) search must be evaluated for each point in the high-dimensional space. Under these conditions, a traditional algorithm and data structure, such as a KD-Tree [23], will not perform well. In the BH-SNE [6] algorithm, a Vantage-Point Tree [24] is used for the KNN search, but it is slow to query. In this work, we propose to use an approximated computation of the KNN in the initialization stage to start the analysis as soon as possible. The level of approximation is then refined on the fly during the analytics process.\nOther dimensionality-reduction algorithms implement approximation and steerability to increase performance as well. For example MDSteer [25] works on a subset of the data and allows the user to control the insertion of points by selecting areas in the reduced space. Yang et al. [26] present a dimensionality-reduction technique using a dissimilarity matrix as input. By means of a divide-and-conquer approach, the computational complexity of the algorithm can be reduced. Multiple other techniques provide steerability by means of guiding the dimensionality reduction via user input. For example Joja et al. [27] and Paulovich et al. [28] let the user place a small number of control points. In other work, Paulovich et al. [29], propose the use of a non-linear dimensionality-reduction algorithm on a small number of automatically-selected control points. For these techniques the position of the data points is then finally obtained by linear-interpolation schemes that make use of the control points. All these techniques, however, limit the non-linear dimensionality reduction to a subset of the dataset. In this work, we provide a way to directly use the complete data allowing the analyst to immediately start the analysis on all the data points.\nIngram and Munzner’s Q-SNE [30] is based on a similar idea, also using Approximated KNN queries for the computation of the high-dimensional similarities. However, their work is designed to exploit the sparse structure of highdimensional spaces obtained from document collections and, therefore, cannot be used for the analysis of dense highdimensional spaces as presented in this work. Furthermore,\nwe provide a visualization of the degree of approximation, the ability to steer the computation by refining the approximation level based on user input and the ability to interactively manipulate the high-dimensional data.\nDensity-based visualization of the tSNE embedding has been used in several works [4], [7], [12], however, they employ slow to compute offline techniques. In our work, we integrate real time Kernel Density Estimation (KDE) as described by Lampe and Hauser [31]. The interaction with the embedding is important to allow the analyst to explore the high-dimensional data. Selection operations in the embedding and the visualization of the data in a coordinated multiple-view system are necessary to enable this exploration. The iVisClassifier system [32] is an example of such a solution. In our work, we take a similar approach, providing a coordinated multiple-view framework for the visualization of a selection in the embedding."
    }, {
      "heading" : "3 TSNE",
      "text" : "In this section, we provide a short introduction to tSNE [5], which is necessary to explain our contribution. tSNE interprets the overall distances between data-points in the highdimensional space as a symmetric joint-probability distribution P . Likewise a joint-probability distribution Q is computed, that describes the similarity in the low-dimensional space. The goal is to achieve a representation, referred to as embedding, in the low dimensional space where Q faithfully represents P . This is achieved by optimizing the positions in the low-dimensional space to minimize the cost function C given by the Kullback-Leibler (KL) divergence between the joint-probability distributions P and Q:\nC(P,Q) = KL(P ||Q) = N ∑\ni=1\nN ∑\nj=1\npij ln\n(\npij qij\n)\n(1)\nGiven two data points xi and xj the probability pij models the similarity of these points in the high-dimensional space. To this extent, for each point a Gaussian kernel, Pi, is chosen whose variance σi is defined according to the local density in the high-dimensional space and then pij is described as follows:\npij = pi|j + pj|i\n2N , (2)\nwhere pj|i = exp(−(||xi − xj ||2)/(2σ2i ))\n∑N k 6=i exp(−(||xi − xk|| 2)/(2σ2i )) (3)\npj|i can be seen as a relative measure of similarity based on the local neighborhood of a data-point xi. The perplexity value µ is a user-defined parameter that describes the effective number of neighbors considered for each data-point. The value of σi is chosen such that for fixed µ and each i:\nµ = 2− ∑N j pj|i log2 pj|i (4)\nA Student’s t-Distribution with one degree of freedom is used to compute the joint-probability distribution in the low-dimensional space Q, where the positions of the datapoints should be optimized. Given two low-dimensional\npoints yi and yj , the probability qij that describes their similarity is given by:\nqij = ( (1 + ||yi − yj || 2)Z )−1 (5)\nwith Z = N ∑\nk=1\nN ∑\nl 6=k\n(1 + ||yk − yl|| 2)−1 (6)\nThe gradient of the Kullback-Leibler divergence between P and Q is used to minimize C (see Eq. 1). It indicates the change in position of the low-dimensional points for each step of the gradient descent and is given by:\nδC δyi = 4\nN ∑\ni=1\n(F attri − F rep i ) (7)\n= 4 N ∑\ni=1\n( N ∑\nj 6=i\npijqijZ(yi − yj)− N ∑\nj 6=i\nq2ijZ(yi − yj)) (8)\nThe gradient descent can be seen as a N-body simulation [33], where each data-point exerts an attractive and a repulsive force on all the other points (F attri and F rep i )."
    }, {
      "heading" : "3.1 Barnes-Hut-SNE",
      "text" : "In the original tSNE, the force is computed using a bruteforce approach, resulting in computational and memory complexity of O(N2). Barnes-Hut-SNE (BH-SNE) [4], [6] is an evolution of the tSNE algorithm that introduces two different approximations to reduce the computational complexity to O(N log(N)) and the memory complexity to O(N).\nThe first approximation is based on the observation that the probability pij is infinitesimal if xi and xj are dissimilar. Therefore, the similarities of a data-point xi can be computed taking into account only the points that belong to the set of nearest neighbors Ni. The cardinality of Ni can be set to K = ⌊3µ⌋, where µ is the user-selected perplexity and ⌊·⌋ describes a rounding to the next-lower integer. Without compromising the quality of the embedding [4], [6], we can adopt a sparse approximation of the high-dimensional similarities. Eq. 3 can now be written as follows:\npj|i =\n\n\n\nexp(−(||xi−xj || 2)/(2σ2i ))∑\nN k 6=i exp(−(||xi−xk|| 2)/(2σ2i )) if j ∈ Ni\n0 otherwise (9)\nThe computation of the K-Nearest Neighbors is performed using a Vantage-Point Tree (VP-Tree) [24]. A VPTree is data structure that computes KNN queries in a high-dimensional metric space, in O(log(N)) time. It is a binary tree that stores for each non leaf-node a hyper-sphere centered on a data-point. The left children of each node will contain the points that reside inside the hyper-sphere, whereas the right one will contain the points outside it.\nThe second approximation makes use of the formulation of the gradient presented in Eq. 7. As described above tSNE can be seen as an N-body simulation and thus the BarnesHut algorithm [34] can be used to reduce the computational complexity to O(N log(N)). For further details, we refer to van der Maaten [4], [6]."
    }, {
      "heading" : "4 A-TSNE FOR PROGRESSIVE VISUAL ANALYTICS",
      "text" : "In this work, we introduce Approximated-tSNE (A-tSNE), an evolution of the BH-SNE algorithm, using approximated computations of high-dimensional similarities to generate meaningful intermediate results. The level of approximation can be defined by the user to allow control on the trade off between speed and quality. The level of approximation can be refined by the analyst in interesting regions of the embedding, making A-tSNE a computational steerable algorithm [22].\ntSNE is well suited for the application in Progressive Visual Analytics: after the initialization of the algorithm, the intermediate results generated during the iterative optimization process can be interpreted by the analyst while they change over time, as shown in previous work [14], [15]. Fig. 1a shows a typical Progressive Visual Analytics workflow for tSNE.\nAlgorithms that can be used in a Progressive Visual Analytics system often have a computational module, e.g. the initialization of the technique, that cannot be implemented in an iterative way, creating a speed bump [13] in the user analysis. tSNE is a good example of this limitation, it consists of two computational modules that are serialized. In the first part of the algorithm, similarities between highdimensional points are calculated. In the second module, a minimization of the cost function (Eq. 1) is computed by means of a gradient descent. The first module, depicted in\nlight grey in Fig. 1a, is slow to compute and does not create any meaningful intermediate results.\nWe extend the Progressive Visual Analytics paradigm by introducing approximated computation rather than aiming at exact computations, in the modules that are not suited for a per-iteration visualization. Fig. 1b shows the analytical workflow for A-tSNE. While the generation and the inspection of the intermediate results is not changed, we introduce a refinement module, depicted in red in Fig. 1b, which can be used to refine the level of the approximation in the embedding in a concurrent way. Furthermore, the increased performance of the initialization module and the ability to update the high-dimensional similarities during the gradient descent minimization, allows the analyst to manipulate the high-dimensional data in an interactive way. We impose the following requirements to our approximation:\n1) The performance gain due to the approximation must be high enough to enable interaction. 2) The approximation should lead to a gradual and controllable degradation of the accurate results. 3) The approximation quality can be measured and visualized to avoid misleading the user. 4) The approximation can be refined during the evolution of the algorithm and, possibly, steered by the user.\nIn the following section, we describe the A-tSNE algorithm in detail using the MNIST [35] dataset for illustration. The dataset consists of 60k labeled gray scale images of handwritten digits (compare Fig. 2a). Each image is represented as a 784 dimensional vector, corresponding to the gray values of the pixels in the image."
    }, {
      "heading" : "4.1 A-tSNE",
      "text" : "A-tSNE improves the BH-SNE algorithm using fast and Approximated KNN computations to build the approximated high-dimensional joint-probability distribution PA, instead of the exact distribution P . The cost function C(PA, QA) is then minimized in order to obtain the approximated embedding described by QA.\nThe similarity between points can be computed using the set of approximated neighbors NAi , instead of the exact neighborhood Ni (see Eq. 9). We define the precision of the algorithm as ρ. ρ describes the average percentage of points in the approximated neighborhood NAi that belongs to the exact neighborhood Ni:\nρ = N ∑\ni=1\nρi N ρk = |NAk ∩Nk| |Nk| , (10)\nwhere | · | indicates the cardinality of the neighborhood. The cardinality of Nk is indirectly specified by the user as explained in Sec. 3.1, as three times the value of the perplexity parameter µ. ρ is an input parameter that can be defined by the user, setting ρ to a large value means the joint-probability distribution PA is similar to P , leading to a similar embedding.\nTo better understand the effect of the approximated queries, it is useful to interpret the BH-SNE algorithm as a force-directed layout algorithm [36], which acts on an\nundirected graph created by the KNN relationships. A data point xi is repelled by all other data-points but to a subset of the data-points given by its neighborhood relationships, where attraction forces are created by a set of springs which connect xi with all the points in Ni.\nWhen specifying a lower precision ρ, resulting in a coarser approximation, some springs connecting points, which are close in the high-dimensional space will be missing and instead distant points are connected. This will result in a false repulsion between the points missing the connecting spring. Using PA reduces the quality of the embedding but improves its computation time by several orders of magnitude. However, reasonable results can usually be achieved even with low values for ρ, because each data point is usually connected to a large number of springs and, therefore, the overall structure can be maintained. This observation holds for local as well as global structures. Intuitively, even if two points are no longer connected, they might share a common neighbor, which indirectly connects both.\nFig. 2 shows the embeddings generated using different precision values ρ for the computation of the highdimension similarities. We use the whole MNIST dataset as the input and we color each data-point accordingly to the digit it represents for validation purposes. Fig. 2b shows the embedding generated with the exact neighborhood, whereas Fig. 2c shows the embedding generated with a precision of ρ = 0.34. It can be seen that similar structures are preserved using approximated neighborhoods. Fig. 2e shows the embedding generated with ρ = 0.07. Even though the embedding visually differs from the exact embedding, depicted in Fig. 2b, the overall clustering of the data is preserved rather well, whilst the time needed for the computation of the similarities is greatly reduced. Where the original algorithm needs 3191 seconds for the initialization using a precision of ρ = 0.34 we can achieve a speedup of two orders of magnitude, resulting in a computation time of 30 seconds. By using a precision of ρ = 0.07, it is further reduced to 13 seconds."
    }, {
      "heading" : "4.2 Approximated KNN",
      "text" : "We achieve different levels of precision by means of different parameterizations of an approximated KNN algorithm\ncalled Forest of Randomized Kd-Trees. In this section, we describe this technique and how its parameters can be mapped to the precision ρ.\nWhen the dimensionality of the data is high, there are no exact KNN algorithms performing better than linear search [37]. Therefore, the development of approximated KNN algorithms is needed to deal with high-dimensional spaces. A survey on existing algorithms, including an extensive set of experiments can be found in the work of Muja et al. [38].\nIn this work, we use a space partitioning technique called Forest of Randomized KD-Trees [39] to compute the approximated neighborhoods. This technique has proven to be fast and effective in querying of high-dimensional spaces [37].\nA KD-Tree [23] is a binary tree used to partition a kdimensional space. Each node in the tree is a k − 1 dimensional hyper-plane, orthogonal to one of the initial kdimensions, that splits the space into two half spaces. The recursive splitting creates a hierarchical partition of the kdimensional space.\nIn a Forest of Randomized KD-Trees, a number T of KDTrees are generated. The splitting hyper-planes are selected by splitting along a randomly selected dimension among the V dimensions characterized by the highest variance. A KNN search is computed on all T KD-Trees, while a maximum number of leaves L are visited. A priority-queue, ordered by increasing distances to the decision boundary, is used to decide which nodes must be visited first across the forest. The process is stoppedwhen the necessary number of leaves have been evaluated.\nThe parameterization of the Forest of Randomized KDTrees can overburden the typical end user. To hide this complexity, we integrate the work by Muja et al. [37] and expose only the single precision parameter ρ to the user. The parameters involved in the creation and querying of the Forest of Randomized KD-Trees, (T ,V ,L), are estimated by a minimization process that uses the precision ρ as input, an estimation of the structure of the dataset, e.g., correlation between dimensions, and a desired balance between the building time of the trees and the cost of a single KNN search. For further details we refer to Muja et al. [37]."
    }, {
      "heading" : "4.3 Steerability",
      "text" : "A-tSNE is computationally steerable [22], in the sense that the user can define the level of approximation to specific, interesting areas. In this section, we present the changes we made to the BH-SNE algorithm to allow for the refining of the approximation.\nThe refinement that we propose is done by computing the exact neighborhood for one point at a time. This process leads to a mix of exact and approximated neighborhoods. For each updated neighborhood, a Gaussian distribution Pi is computed and the sparse joint-probability distribution PA must be updated accordingly. This update, however, is not straightforward. First, the symmetrization of PA in Eq. 2 requires to combine Gaussian distributions enforced by different data-points and, second, the sparse nature of the distribution PA renders fast updates challenging.\nWe solve these issues by observing that a direct computation of PA can be avoided and the distribution can be indirectly obtained using the Gaussian distributions enforced by the K-Nearest Neighbors. Eq. 2 can be split into two components which correspond only to the Gaussian distributions Pi and Pj :\npij = pj|i 2N + pi|j 2N\n(11)\nUsing this formulation, we need to store only one Gaussian distribution for every point, therefore points can be handled individually without any performance loss. This key point of our technique allows us to refine the highdimensional similarities in parallel to the gradient descent, and it is the base for the manipulation of the highdimensional data. Furthermore, we are not constrained to update the neighborhood of a data-point just once. The analyst can request different levels of approximation for a given area before starting the computation of the exact highdimensional similarities. For each data-point we store ρi as the requested precision for the neighborhood Ni.\nA change in a neighborhood, however, yields a change in the cost function C, see Eq. 1, which we are minimizing. To avoid the risk of getting stuck in a local minimum during the gradient descent, we introduce an optimization strategy called Selective Exaggeration with Exponential Decay.\nOur strategy is inspired by the optimization strategy called Early Exaggeration presented by van der Maaten et al. [5]. The idea of Early Exaggeration is that, by exaggerating the attractive forces, see Eq. 7, by a factor τ during the first Iτ iterations of the gradient descent, local minima can be avoided.\nUsing the Selective Exaggeration with Exponential Decay, we apply an exaggeration τ to the attractive forces acting on a data-point xi when it is refined. The exaggeration is then smoothly removed on a per-point basis using an exponential decay of the exaggeration factor. This can be interpreted as a localized reinitialization of the gradient descent triggered by user interaction with the embedding."
    }, {
      "heading" : "4.4 Performance and Accuracy Benchmarking",
      "text" : "In this section, we present a detailed performance analysis of A-tSNE compared to the BH-SNE using several standard benchmark datsets. All performance measurements were\nobtained using a DELL Precision T3600 workstation with a 8-core Intel Xeon E5 1650 CPU @ 3.2GHz, 32GB RAM and a NVIDIA GTX 680.\nWe apply the same preprocessing steps as presented by van der Maaten [4], without applying a preliminary dimensionality-reduction by means of a Principal Component Analysis. We use the MNIST dataset [35] (60k data-points with 784 dimensions), the NORB dataset [40] (24300 data-points with 9216 dimensions), the CIFAR-10 dataset [41] (50k points with 1024 dimensions) and the TIMIT dataset [42], consisting of 1M data-points with 39 dimensions. Throughout the experiments we used a parameter setup similar to the one used to benchmark the BHSNE [4], [6]. We use a fixed perplexity value of µ = 30. First, we evaluate the performances of A-tSNE in relation to the parameters (T ,V ,L) used in the Forest of Randomized KDTreeswhere, T is the number of trees generate splitting along the V dimensions characterized by the highest variance and L is the number of leaves visited in a single query. We set the approximation parameters to three different configurations: T = 4 L = 1024, T = 2 L = 512 and T = 1 L = 1, while we fix V = 5 as suggested by Muja et al. [37].\nThe left chart in Fig. 3 shows the comparison of computation times, in logarithmic scale, of the high-dimensional similarities on the MNIST dataset obtained by our technique and by the BH-SNE algorithm. The right chart in Fig. 3 depicts the precision ρ of the neighborhoods. The precision is given by Eq. 10 and it is computed using the exact and the approximated neighborhoods. Generally, our approach generates a good embedding very efficiently for any given dataset we tested. Fig. 2(b-e) show the embeddings generated using the described parameter settings for the MNIST dataset after 1000 iterations. It can clearly be seen that we achieve comparable results more than two orders of magnitude faster than the BH-SNE implementation.\nFig. 3 shows how the precision decreases when increasing the data size for a fixed parameter setting. The number of leaves (corresponding to data points) to visit, included in the parameter setting, is fixed indepentently of the data size. When the data size increases the same number of leaves, corresponding to a smaller fraction of the overall data, is visited, causing the lower precision. Since these parameters are not exposed to the user, but rather only the precision\nNORB (24k)\nMNIST (60k)\nTIMIT (1M)\nT = 4 L = 1024 T = 2 L = 512\nT = 1 L = 1\nvalue, this does not have an effect in the acutal use case. In general, we can see that with a small reduction in precision, the computation time can be greatly reduced.\nFinally, we analyze the error introduced by the approximation of the similarities in the high-dimensional space using the NORB, MNIST and TIMIT datasets. For the results of the CIFAR-10 dataset we refer to the supplemental material, as they are very similar to the results obtained on the MNIST dataset. The cost function C(P,Q) is the most direct indication of the quality of the embedding and we compare minimizing of the cost function C(P,QA) to C(P,Q). QA is the joint-probability distribution that describes similarities in the approximated embedding obtained by the minimization of C(PA, QA). Fig. 4 shows the C(P,QA)/C(P,Q) ratio. Smaller values indicate less error, with a value of 1 meaning that no approximation error is present. The Early Exaggeration of the attractive forces (see Sec. 4.3) is responsible for the peak in the ratio that is visible during the first 250 iterations. By exaggerating the attractive forces the approximation error is increased. Notice that the absolute value of the cost is not depicted in Fig. 4 and decreases with every iteration.\nThe usage of a Forest of Randomized KD-Trees with T = 1 L = 1 generates an embedding with a large error. This configuration is an upper bound of the error and a lower bound in computation time: by visiting only one leaf during the traversal of the forest composed by just one tree, the approximated KNN algorithm becomes a greedy algorithm [43]. We can also note that, when the size of the data increases, the approximation error decreases. In the TIMIT dataset we observe that the approximation errors generated using T = 2 L = 512 and T = 4 L = 1024, are similar or even better, than the exact one. By increasing the number of points in the embedding, the effect of the false repulsive forces (Sec. 4.1) is compensated by the increasing number of attractive forces among data-points.\nThe presented results clearly show that we can rapidly provide very accurate embeddings allowing immediate interaction, without misleading the user. With a large number of data points we effectively generate tSNE embeddings as demonstrated by the reduced approximation error."
    }, {
      "heading" : "5 INTERACTIVE ANALYSIS SYSTEM",
      "text" : "Using A-tSNE, the user is able to start the analysis of the data without waiting for the exact computation of the similarities in the high-dimensional space.\nThe embedding, however, will be created based on approximated information. We present different strategies that the user can apply to refine the high-dimensional similarities, leading to the generation of different, and more precise, embeddings.\nDuring the refinement the user must be aware of the level approximation in the embedding. Therefore, we present a visualization that supports the inspection of the level of approximation and, at the same time, is able to deal with the increased sized of the embeddings that A-tSNE is able to generate interactively.\nWe also take advantage of the steerability of A-tSNE to allow for a direct manipulation of the high-dimensional data and its representation, for example, by adding and removing data-points or by changing the high-dimensional representation of the data.\nFinally, we implemented these techniques in a coordinated multiple-views framework that allows for the direct inspection of the data in the embedding."
    }, {
      "heading" : "5.1 User Driven Refinement",
      "text" : "The refinement process used to steer the computation of an A-tSNE embedding works on a per-point basis, see Sec. 4.3. We propose three different strategies that can be used to select the points to be refined.\nThe very basic strategy is to refine the neighborhoods of all the points in X in a random order. When computational resources are sparse, however, it makes sense to steer the refinement process to increase precision in areas of the embedding that the analyst finds interesting, e.g., based on initial visual clusters appearing in the embedding."
    }, {
      "heading" : "5.1.1 User Selection",
      "text" : "We allow the user to select a subset of points for immediate refinement. By brushing in the embedding the user can steer the refinement to interesting areas. This strategy is less effective when just a few points are selected for refinement, as the forces exerted on its neighbors are still approximated, which can lead to an unfaithful description of the highdimensional data."
    }, {
      "heading" : "5.1.2 Breadth-First Search",
      "text" : "If only a few points are selected for refinement, we extend the process to include their neighborhoods. Therefore, a breadth-first visit [43] of the graph created by the KNN relationships can be used to extend the refinement. If a priority queue [43] is used to keep track of the points that must be refined, different strategies can be implemented. For example, the priority in the queue can be given by the distances in the high-dimensional space or it can be determined based on a domain-specific criteria."
    }, {
      "heading" : "5.1.3 Density-Based Refinement",
      "text" : "If the user is more interested in gaining a global overview of the potential embedding, a density-based refinement strategy can be used instead of a local refinement. This strategy\nis based on the observation that points in the less dense areas of the high-dimensional space are responsible for the creation of the global relationship in a tSNE embedding [5]. The data-points are refined with an order given by the density in the high-dimensional space. An indication of this density is the variance σi of the Gaussian distribution, as explained in Sec. 3. This strategy can work within a userdefined selection or on the whole dataset."
    }, {
      "heading" : "5.2 Visualization and Interaction",
      "text" : "The visualization of the tSNE embedding is not insightful if not combined with the ability to inspect the highdimensional data. Particularly in a context of exploratory data analysis, where a classification as the one presented in Fig. 2 is not available, such a solution is very helpful. In our system the user can explore the embedding using a density-based visualization. Selections in the embedding are used to visualize the high-dimensional data in a coordinated multiple-view framework. To indicate the approximation level in the embedding, we use two specifically-designed visualizations."
    }, {
      "heading" : "5.2.1 Density-Based Visualization",
      "text" : "The visualization of the embedding, using simple points, is affected by visual clutter when the number of points increases. Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales. We apply a real-time kernel density estimation (KDE) [31] for the creation of an interactive density-based visualization of the embedding. We use changes in the color hue to visualize selections, for example to highlight data points that are selected to be analyzed in other views of the coordinated multiple-view framework. The KDE is computed by assigning a value for each pixel p using the kernel density estimator f(p, h) as follows:\nf(p, h) = 1\nN\nN ∑\ni=1\nG(||p − yi||, h). (12)\nG(d, h) is a zero mean Gaussian distribution with standard deviation h, which can be interactively chosen by the user in order to reveal clusters at different scales. Additionally, we introduce a transfer function, mapping f(p, h) to a color, in order to highlight user-defined selections. Areas with a large percentage of selected points are visualized with a different transfer function, and selection outliers are shown as points. To achieve this goal, we introduce a new kernel density estimator s(p, h), which illustrates the density of the user selection in a pixel p. Given a set of selected data-points S we use:\ns(p, h) = 1\nf(p, h)\n1\n|S|\n∑\nyi∈S\nG(||p− yi||, h) (13)\nIf s(p, h) is higher than a threshold Sthresh, a transfer function based on a different hue and with a higher luminance is used. We found empirically that a value Sthresh = 0.5 performs satisfactorily without compromising the quality of the visualization.We also use a point-based visualization of isolated selected data-points and, unselected\ndata-points in selected regions. Finally, the user has control over the opacity of points or opacity of the density-based visualization to adjust the visualization to the needs of the analysis. An example of different visualizations of the embedding is presented in Fig. 5 where theMNIST dataset is used. The analyst can change the bandwidth h, the transfer function, and the opacity interactively in order to show clusters at different scales and outliers in the selection.\nFor example, Fig. 5b shows the selection of a high-level cluster. If a different bandwith is chosen, as in Fig. 5c, clusters at a different level appear. Finally, if the labels are used to make a selection in the embedding, as in Fig. 5d, it is possible to see the distribution of the outliers in the density-based visualization."
    }, {
      "heading" : "5.2.2 Visualization of the Approximation",
      "text" : "During the refinement, the precision of the highdimensional similarities is gradually refined until exact. In order to give to the user the ability to inspect the level of approximation, we enhance our density-based visualization to show the requested precision ρi. Note that ρi is different for every data-point and changes during the refinement process, as described in Sec. 4.3\nFor each pixel p we assign a value given by the function a(p, h) that represents the approximation value given the bandwidth h:\na(p, h) = 1\nf(p, h)\n1 ∑N\ni=1 ρi\nN ∑\ni=1\nρiG(||p− yi||, h)\na(p, h) is the precision ρi weighted kernel-density divided by the kernel-density estimator f(p, h). The value a(p, h) is between zero and one and can be used directly for encoding of the approximation in the visualization.\nThe value of the function a(p, h) can then be included in the visualization in two different ways. First, we introduce a Magic Lens [45] that shows the approximation with a minimal conceal of the data. We use a circular lens that can be overlayed on the density-based visualization and a(p, h) is used to define the transparency α of every pixel in the lens. To better highlight the refined areas, we use α = 1 − a(p, h)k, where k is a user selected parameter, to compute α. We provide a default value of k = 2.\nFig. 6a shows the lens over a cluster that is already refined and, therefore, is visible through the lens. The green tone indicates the area where similarities are still approximated. Contours in approximated areas are preserved to indicate the structure of the embedding. We color the areas without points in green to put more emphasis on refined areas.\nIn addition to the Magic Lens, we provide the possibility to map approximation to the complete view. This is especially useful to inspect the embedding when one of the global refinement strategies was selected. Fig. 6b shows the approximation in the embedding using this approach. It is possible to see that two clusters are already refined, relying on exact neighborhood relationships. The user selected a Breadth-first search refinement strategy, therefore, the refinement is spreading through the embedding, leading to some areas in the top-right corner having the original color.\n(a) Magic Lens (b) Full View Mode\nFig. 6. Visualization of the approximation in the embedding by means of a magic lens (a) and the full view mode (b).\nHowever the perception of clusters is reduced by removing the color information inside the contours."
    }, {
      "heading" : "5.3 Data Manipulation",
      "text" : "In Sec. 4.3, we show that we are able to update highdimensional similarities between data-points during the gradient-descent minimization. In this section, we take advantage of this possibility, introducing different operations that the analyst can use to manipulate the original datapoints in their high-dimensional feature space.\nThe embedding does not need to be recomputed but evolves dynamically as the data changes. At the center of an interactive exploration of data is the ability to add or remove data on demand, use different representations of the same dataset or adapt to any changes in the data."
    }, {
      "heading" : "5.3.1 Inserting Points",
      "text" : "For a point xa, which we want to add to the embedding, its neighborhood Na needs to be computed. An approximated algorithm can be used to compute the neighborhood and a refinement can be scheduled. To complete the insertion we need to check whether xa belongs to the KNN of each point in X . We define dMaxi as the maximum distance between a point xi and the points in its neighborhood Ni. The update of the neighborhoods is written as follows:\n∀xi ∈ X if ||xa − xi|| < d Max i\nthen xa ∈ Ni and xj 6∈ Ni : ||xi − xj || = d Max i\n(14)\nThis operation is computed in O(N) if dMaxi is cached. A priority queue is used to efficiently update dMaxi after the insertion of xa in a given neighborhood Ni. It is important to observe that the insertion of xa in Ni will not reduce the estimated precision ρi. The initial position in the embedding ya is given by the average position of its neighbors Na weighted by their similarity pj|i : xj ∈ Ni. The new point xa must be added in the Forest of Randomized KD-Trees. This operation can be performed in O(log(N)) ."
    }, {
      "heading" : "5.3.2 Deleting Points",
      "text" : "Removing a point xr ∈ X is performed by deleting xr from the KNN of every point xi ∈ X . This operation has a computational complexity of O(N). By removing xr from a neighborhood Ni we reduce the number of xi neighbors to K − 1 and a new neighbor must be found to maintain the precision level. However, the new point in the neighborhood is the most dissimilar of the points in Ni thus its attractive force is rather small and we propose to ignore the contribution of the missing point, decreasing the estimated precision ρi by 1/K . As for the insertion of a new data-point, the Forest of Randomized KD-Trees is updated in O(log(N))."
    }, {
      "heading" : "5.3.3 Dimensionality Modification",
      "text" : "We handle changes in the description of a single highdimensional data-point, for example, when the data is time varying, by a combination of removal and addition operations. If the user wants to completely change the highdimensional representation of the data, e.g., by adding or removing dimensions, a new approximated joint-probability distribution PA is computed and the embedding is free to evolve accordingly."
    }, {
      "heading" : "5.4 Visual Analysis Tool",
      "text" : "We implemented A-tSNE as a module in an integrated, interactive, multi-view system for the analysis of highdimensional data. Fig. 7 shows a screenshot of the system and its different views.\nFig. 7. Screenshot of our integrated system using multiple linked views for interaction. The system comprises an embedding viewer (a), a data viewer (b) and a refinement viewer (c). Controls on the gradient descent (d), the density-based visualization (e), the data-manipulation (f) and the refinements (g) are at the bottom of the interface.\nThe interface is divided into two main areas. At the top, three different views are used to show the intermediate embeddings (7a), the data (7b) and the state of refinement processes (7c), respectively. Controls on the generation of intermediate embeddings (7d), visualization of the embedding (7e), data manipulation (7f) and refinement (7g) are at the bottom of the interface.\nThe data subject to the analysis are visualized in theData View (7b). Selections in the embeddings can be reflected in the Data View with strategies that depend on the data type. For example, in Fig. 7 the Data View is used to visualize data-points that represent voxels in a 3D volume viewer and using slices of the volume along the three main axes (7b). Selections are highlighted in the anatomical planes by a change of hue and by adjusting the transfer function in the volume viewer. The Data View can support selections based on a data-driven criteria, e.g. voxels with the same axial coordinate.\nWe implemented multiple widgets that can be switched out in the Data View, to support the analysis process of different data types. These widgets include a heatmap view and an image view. If necessarymultiple and different views can be combined for the analysis.\nThe Refinement-Status View (7c) is used to give an overview of the progress of the refinements triggered by the user. The user can steer the evolution of the embedding by refining areas with strategies as described in Sec. 5.1. A refinement process is identified by the snapshot of the embedding when the user triggered the refinement, a userdefined description, and a progress bar that shows the percentage of the refined data-points over the selected ones."
    }, {
      "heading" : "6 IMPLEMENTATION",
      "text" : "We implemented the system using a combination of C++ and Qt, as well as OpenGL with custom shaders in GLSL for the visualization of the embedding. Where possible, we exploited parallelizability of our approach. The approximated neighborhoods are computed using the FLANN li-\nbrary [37], which implements KNN algorithms. The densitybased visualization is computed on the GPU using OpenGL and GLSL shaders. A precomputed floating-point texture is generated using a Gaussian kernel. A geometry shader is used to generate a quad for each point that is colored using the precomputed texture, the KDE is obtained by drawing into a Frame Buffer Object using an additive blending [31]."
    }, {
      "heading" : "7 CASE STUDY I: EXPLORATORY ANALYSIS OF GENE EXPRESSION IN THE MOUSE BRAIN",
      "text" : "In this section, we demonstrate the advantages of using AtSNE in our visual analysis tool for the visual analysis of high-dimensional data. Therefore, we present a use case, based on the work by Mahfouz et al. [11], who use tSNE to explore the Allen Mouse Brain dataset [46]. The dataset is composed by 61164 voxels obtained by slicing the mouse brain in 68 slices. Each voxel is a 4345-dimensional vector, containing the genetic expression at the corresponding spatial position. tSNE is computed using the voxels as datapoints and the expression of the genes as high-dimensional space. Note that no spatial information is used to build the high-dimensional space.\nMahfouz et al. discuss the hypothesis that genetic information can be used to differentiate anatomical structures in the brain. Some regions in the brain, e.g. the Cerebellum, are known to have a highly different genetic footprint compared to the rest of the brain. In their work, Mahfouz et al. demonstrated that tSNE is effective in separating different anatomical structures, e.g. white and grey matter, based on their genetic footprint.\nFig. 8 depicts the typical analytic workflow using our visual analysis tool. The first goal during the analysis is to validate the input data. The acquisition process may not be perfect, data can be incomplete or noisy, therefore, it must be re-acquired or preprocessed before interesting results can be generated. Driven by the need to validate the data as soon as possible, the user selects a reasonably low value for the desired precision, e.g. ρ = 0.2, that will be used to estimate the parameters of the KNN algorithm. With such a parameterization, A-tSNE computes the high-dimensional similarities in ≈ 51 seconds while 3 hours and 50 minutes are required by BH-SNE.\nThe user then start analyzes the intermediate embeddings, produced by A-tSNE, in order to validate the input data. After ≈ 170 seconds several clusters become visible in the embedding as depicted in Fig. 8a. The clusters are stable for several iterations indicating that they are not an artifact of the minimization process but represent clusters in the high-dimensional space. The user can validate this by selecting the clusters in the embedding and inspect them in more detail, for example, by highlighting their spatial positions in the feature view, see Fig 8a. Points or clusters are selected by brushing in the embedding. During a brushing operation the generation of intermediate embeddings is stopped to make sure the user does not accidentally brush areas as they change. Selected points are then highlighted by a change of hue, in this case from blue to orange. Further inspection using the Data View in our interactive system, shows that each cluster corresponds to a slice in the dataset.\nFig. 8. Analysis of the gene expression in the mouse brain using A-tSNE. The first embedding (a) are generated in ≈ 51 seconds while 3 hours and 50 minutes are required by BH-SNE. The analyst inspects a cluster and finds that it corresponds to a slice in the data. The cluster do not disappear after the neighborhoods are refined, as shown by the lens in (b). A change in the high-dimensional data reveal that genetic information can be used to differentiate anatomical regions. (c) shows the final embedding based on a small number of Principal Components where three clusters are highlighted and (d) shows the corresponding regions in the brain.\nFig. 8a shows a cluster, highlighted in orange, and the corresponding slice in the volume.\nTo make sure the clusters are not an artifact introduced by the approximated similarities, the user can refine the selected data-points while the embedding evolves. Fig. 8b shows the embedding after the refinement is complete. The user can inspect the degree of approximation in the embedding using the interactive lens. The lens is less transparent over approximated areas of the embedding and transparent on the areas that contain no approximation, i.e., the selected points. After the refinement of the high-dimensional similarities of the selected data points, the clusters do not disappear and it becomes clear that the clusters are caused by properties of the data, rather than by the approximation.\nTherefore, the user can stop the generation of the embedding. Further analysis of the input data reveals that missing values in the input data cause the formation of small clusters in the embedding. Mahfouz et al. removed this effect by using the first 10 components, extracted by a Principal Component Analysis of the raw data, as the highdimensional space.\nIn our system, the user can now directly change the high-dimensional space, without restarting the computation of the embedding. Approximately 200 seconds after the change in the high-dimensional data, a stable embedding is obtained. Fig. 8c shows the final embedding, where three different clusters are highlighted. Fig. 8d depicts the selected voxels in the brain, note how the anatomical structures are now revealed. It is possible to see how the clusters that were present in the first intermediate results disappear, showing that the cluster fragmentation is removed.\nVoxels that belongs to the same anatomical structure are close together in the embedding. A-tSNE is able to separate anatomical structures based on the gene expression of the 4345 genes.\nIn their work, Mahfouz et al., presented embeddings created using 2, 3, 5, 10 and 20 principal components as the high-dimensional space. Identifying the right number of components is a time consuming task and the adoption of our analytic workflow helps the user in finding a good compromise interactively analyzing the resulting embed-\nding generated changing the number of components."
    }, {
      "heading" : "8 CASE STUDY II: REAL-TIME MONITORING OF HIGH-DIMENSIONAL STREAMS",
      "text" : "The improved computation time and the ability to manipulate data are the key for applying tSNE in new application scenarios, such as the real-time monitoring of highdimensional data streams. The original tSNE algorithm fails in providing a solution for such an application. The computation of a tSNE map imposes a time constraint that cannot be ignored, when the rate in which new data is generated is higher than the time required for the computation of a tSNE map.\nAs proof of concept, we selected a dataset for physical activity monitoring [47] that comprises readings of three Inertial Measurement Units (IMU) and a heart rate monitor applied to 9 different subjects. Every IMU generates 17 readings every 10 ms, while the heart rate monitor generates one reading every 100 ms. Taking into account all the sensors, we have a stream of data consisting of 52 readings, where a new data is generated every 100 ms for each subject. Every subject has also a device to label the physical activity among 24 different activities, e.g. lying down, standing, running. We use the labeling of every reading to validate the insights obtained by the analysis of the embeddings.\nWe analyze the stream of a subject by keeping the readings of the previous M minutes in the embedding. When a new reading is generated, we add it to the embedding using the technique described in Sec. 5.3. Similarly, when a reading is older then M minutes, we remove it from the embedding. In the test presented in this section, M = 10 is set leading to an embedding composed, in average, by 6000 data-points that is updated every 100 ms.\nWe add a point-based visualization to our density-based visualization, which shows the last points inserted in the embedding. The new points are colored according to the classification of the activity made by the subject and they will fade out in F seconds. By showing the new datapoints the analyst can identify where new points are added,\n(a) (b) (c) (d)\nFig. 9. A-tSNE is used for the real-time analysis of high-dimensional streams. The embeddings are generated using the readings of the last 10 minutes. As new readings arrive they are inserted in the embedding and they are highlighted using a point-based visualization. (a) shows the initial embedding, the color of the data-points indicates that the subject is lying down. The embedding evolves as in (b), the new cluster indicates the creation of a set of different readings. This insight is confirmed by a change in the color of the data-points that indicates a new type of label activity. (c) shows an evolution of the embedding presented in (a) where new readings are generated from a miscalibrated sensor and, therefore, are clustered together. By removing the features corresponding to the miscalibrated sensor the embedding evolves as in (d). The cluster that identifies miscalibrated readings is removed.\nproviding at the same time an overview of the embedding in the last M minutes and the trend of the last F seconds.\nFig. 9a shows an embedding obtained from subject 105, where the color of the data-points, green in this specific case, indicates that the subject is lying down. The embedding is composed of a single big cluster that represent the lying down activity. The cluster is divided in four different subclusters that identify different readings of the sensors. The readings of the last 30 seconds belong to a single subcluster and can be seen as points on the right side of the embedding. The embedding evolves based on new readings from the sensors, after few seconds the new data-points start to be placed further away from the original cluster, leading to the creation of a new cluster, as depicted in Fig. 9b. After a few seconds the subject changes the classification of his activity from lying down to an unclassified activity, whose corresponding data-points are colored in purple. It is interesting to note that, simply by looking at the embedding, it is possible to predict a change in the labeled activity before the subject is able to record the change on his labeling device. It can be seen by the fact that few data-points labeled as a lying down activity, hence colored in green, are in the same cluster as the ones identified as unclassified activity. In this particular case, we can guess that the subject seated before changing the labeled activity.\nFinally, we simulated a miscalibration in an inertial measurement unit. Differently from a faulty sensor (not generating any readings), a miscalibrated one generates readings affected by a constant offset that is different for every dimension. We simulate the miscalibration by enforcing a random offset to the readings generated by one of the IMUs. A miscalibrated sensor generates readings that are different from the normal one and, therefore, they should be clustered together as faulty readings. Fig. 9c shows the evolution of the embedding presented in Fig. 9a where the miscalibrated readings are grouped by A-tSNE. After the inspection of the readings generated from the IMUs, the analyst can identify that something is wrong with one of the sensors. At this point the sensor may be replaced or, in case this is not possible, the readings from the miscalibrated sensor can be excluded by removing the corresponding dimensions from the high-dimensional space. Fig. 9d shows how the previous embedding evolves when the readings\ngenerated by the miscalibrated sensor are removed from the high-dimensional space. It is possible to see that the readings affected by the miscalibration are now close to the cluster that represent the lying down activity.\nTo conclude, we demonstrated how A-tSNE can be used for the real-time analysis of high-dimensional streams. It is not possible to compute a standard tSNE embedding every 100 ms without the fast computation of the highdimensional similarities and the ability to directly manipulate the high-dimensional data."
    }, {
      "heading" : "9 CONCLUSIONS",
      "text" : "Motivated by the need of interactivity in Visual Analytics, we developed Approximated-tSNE (A-tSNE). A-tSNE enables the rapid generation of approximate tSNE embeddings. We used a fast approximated KNN algorithm for the computation of the high-dimensional similarities. Our algorithm is designed to be used within the Progressive Visual Analytics context, allowing the user to have a quick preview of the data. The insight obtained using the approximated embeddings can be validated by removing the approximation in interesting areas with different strategies. The user is made aware of the level of approximation in the embeddings by the usage of two specifically designed visualizations.\nWe demonstrate that A-tSNE generates meaningful tSNE embeddings two orders of magnitude faster than the stateof-the-art, BH-SNE. Furthermore, our method enables a new way of handling large datasets, by integrating a real-time density-based visualization to support the user in interacting with the data. The selections can be visualized in a coordinated multiple-view framework making it easy to steer the analysis into relevant regions. A-tSNE can effectively replace BH-SNE for all use cases, as the full precision of BH-SNE can always be reached by setting the precision parameter accordingly, or refining the data. We present three different operations for the direct manipulation of the highdimensional data and their application for the real-time monitoring of high-dimensional streams.\nIn our work we focus on the fast computation of highdimensional similarities enabling interactive analysis. However, when dealing with Big Data, e.g., more than a million data-points, the iterative minimization becomes slower\nand limits the interactivity of A-tSNE. In the future we want to address this limitation and explore the application of A-tSNE in other research scenarios. In particular, we are interested in investigating the application of A-tSNE in the analysis of heterogeneous data and different highdimensional streams, such as climate readings."
    } ],
    "references" : [ {
      "title" : "Parallel coordinates",
      "author" : [ "A. Inselberg", "B. Dimsdale" ],
      "venue" : "Human- Machine Interactive Systems. Springer, 1991, pp. 199–233.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Printer graphics for clustering.",
      "author" : [ "J.A. Hartigan" ],
      "venue" : "Journal of Statistical Computing and Simulation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1975
    }, {
      "title" : "Dimensionality reduction: A comparative review",
      "author" : [ "L. van der Maaten", "E.O. Postma", "H.J. van den Herik" ],
      "venue" : "pp. 66–71, 2008.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Accelerating t-sne using tree-based algorithms",
      "author" : [ "L. van der Maaten" ],
      "venue" : "Journal of Machine Learning Research, vol. 15, pp. 3221– 3245, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Visualizing data using t-SNE",
      "author" : [ "L. van der Maaten", "G. Hinton" ],
      "venue" : "Journal of Machine Learning Research, vol. 9, no. 2579-2605, p. 85, 2008.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Barnes-Hut-SNE",
      "author" : [ "L. van der Maaten" ],
      "venue" : "International Conference on Learning Representations, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "viSNE enables visualization of high dimensional single-cell data and reveals phenotypic heterogeneity of leukemia",
      "author" : [ "E.-a. D. Amir", "K.L. Davis", "M.D. Tadmor", "E.F. Simonds", "J.H. Levine", "S.C. Bendall", "D.K. Shenfeld", "S. Krishnaswamy", "G.P. Nolan", "D. Pe’er" ],
      "venue" : "Nature biotechnology, vol. 31, no. 6, pp. 545–552, 2013.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deorphanizing the human transmembrane genome: A landscape of uncharacterized membrane proteins",
      "author" : [ "J.J. Babcock", "M. Li" ],
      "venue" : "Acta Pharmacologica Sinica, vol. 35, no. 1, pp. 11–23, 2013.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "High-dimensional analysis of the murine myeloid cell system",
      "author" : [ "B. Becher", "A. Schlitzer", "J. Chen", "F. Mair", "H.R. Sumatoh", "K.W.W. Teng", "D. Low", "C. Ruedl", "P. Riccardi-Castagnoli", "M. Poidinger" ],
      "venue" : "Nature immunology, vol. 15, no. 12, pp. 1181–1189, 2014.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Alignmentfree visualization of metagenomic data by nonlinear dimension reduction",
      "author" : [ "C.C. Laczny", "N. Pinel", "N. Vlassis", "P. Wilmes" ],
      "venue" : "Scientific reports, vol. 4, 2014.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Visualizing the spatial gene expression organization in the brain through nonlinear similarity embeddings",
      "author" : [ "A. Mahfouz", "M. van de Giessen", "L. van der Maaten", "S. Huisman", "M. Reinders", "M.J. Hawrylycz", "B.P. Lelieveldt" ],
      "venue" : "Methods, vol. 73, pp. 79 – 89, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Automatic classification of cellular expression by nonlinear stochastic embedding (ACCENSE)",
      "author" : [ "K. Shekhar", "P. Brodin", "M.M. Davis", "A.K. Chakraborty" ],
      "venue" : "Proceedings of the National Academy of Sciences, vol. 111, no. 1, pp. 202–207, 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Progressive visual analytics: User-driven visual exploration of in-progress analytics",
      "author" : [ "C. Stolper", "A. Perer", "D. Gotz" ],
      "venue" : "Visualization and Computer Graphics, IEEE Transactions on, vol. 20, no. 12, pp. 1653–1662, Dec 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Opening the black box: Strategies for increased user involvement in existing algorithm implementations",
      "author" : [ "T. Mühlbacher", "H. Piringer", "S. Gratzl", "M. Sedlmair", "M. Streit" ],
      "venue" : "Visualization and Computer Graphics, IEEE Transactions on, vol. 20, no. 12, pp. 1643–1652, Dec 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "PIVE: A per-iteration visualization environment for supporting real-time interactions with computational methods",
      "author" : [ "J. Choo", "H. Kim", "C. Lee", "H. Park" ],
      "venue" : "Visual Analytics Science and Technology (VAST), 2014 IEEE Symposium on, 2014.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "DeViSE: A deep visual-semantic embedding model",
      "author" : [ "A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 2121– 2129.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 580–587.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, vol. 518, no. 7540, pp. 529–533, Feb. 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Scalable optimization of neighbor embedding for visualization",
      "author" : [ "Z. Yang", "J. Peltonen", "S. Kaski" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013, pp. 127–135.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Visual analytics infrastructures: From data management to exploration",
      "author" : [ "J.-D. Fekete" ],
      "venue" : "Computer, vol. 46, no. 7, pp. 22–29, July 2013.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Trust me, i’m partially right: Incremental visualization lets analysts explore large datasets faster",
      "author" : [ "D. Fisher", "I. Popov", "S. Drucker", "m. Schraefel" ],
      "venue" : "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ser. CHI ’12, 2012, pp. 1673–1682.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A survey of computational steering environments",
      "author" : [ "J.D. Mulder", "J.J. van Wijk", "R. van Liere" ],
      "venue" : "Future generation computer systems, vol. 15, no. 1, pp. 119–129, 1999.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "An algorithm for finding best matches in logarithmic expected time",
      "author" : [ "J.H. Friedman", "J.L. Bentley", "R.A. Finkel" ],
      "venue" : "ACM Transactions on Mathematical Software (TOMS), vol. 3, no. 3, pp. 209– 226, 1977.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Data structures and algorithms for nearest neighbor search in general metric spaces",
      "author" : [ "P.N. Yianilos" ],
      "venue" : "Proceedings of the fourth annual ACM-SIAM Symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 1993, pp. 311–321.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Steerable, progressive multidimensional scaling",
      "author" : [ "M. Williams", "T. Munzner" ],
      "venue" : "Information Visualization, 2004. INFOVIS 2004. IEEE Symposium on. IEEE, 2004, pp. 57–64.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A fast approximation to multidimensional scaling",
      "author" : [ "T. Yang", "J. Liu", "L. McMillan", "W. Wang" ],
      "venue" : "Proceedings of the ECCV Workshop on Computation Intensive Methods for Computer Vision (CIMCV), 2006, pp. 354–359.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Local affine multidimensional projection",
      "author" : [ "P. Joia", "F. Paulovich", "D. Coimbra", "J. Cuminato", "L. Nonato" ],
      "venue" : "Visualization and Computer Graphics, IEEE Transactions on, vol. 17, no. 12, pp. 2563–2571, 2011.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Two-phase mapping for projecting massive data sets",
      "author" : [ "F.V. Paulovich", "C.T. Silva", "L.G. Nonato" ],
      "venue" : "Visualization and Computer Graphics, IEEE Transactions on, vol. 16, no. 6, pp. 1281–1290, 2010.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Least square projection: A fast high-precision multidimensional projection technique and its application to document mapping",
      "author" : [ "F.V. Paulovich", "L.G. Nonato", "R. Minghim", "H. Levkowitz" ],
      "venue" : "IEEE Transactions on Visualization and Computer Graphics, vol. 14, no. 3, pp. 564–575, 2008.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Dimensionality reduction for documents with nearest neighbor queries",
      "author" : [ "S. Ingram", "T. Munzner" ],
      "venue" : "Neurocomputing, vol. 150, pp. 557–569, 2015.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Interactive visualization of streaming data with kernel density estimation",
      "author" : [ "O. Lampe", "H. Hauser" ],
      "venue" : "Pacific Visualization Symposium (PacificVis), 2011 IEEE, 2011, pp. 171–178.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction.",
      "author" : [ "J. Choo", "H. Lee", "J. Kihm", "H. Park" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2010
    }, {
      "title" : "Gravitational N-Body Simulations",
      "author" : [ "S.J. Aarseth" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2003
    }, {
      "title" : "A hierarchical O(N log N) force-calculation algorithm",
      "author" : [ "J. Barnes", "P. Hut" ],
      "venue" : "Nature, vol. 324, no. 4, pp. 446–449, 1986.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "The mnist database of handwritten digits",
      "author" : [ "Y. LeCun", "C. Cortes", "C.J. Burges" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1999
    }, {
      "title" : "Graph drawing by force-directed placement",
      "author" : [ "T.M. Fruchterman", "E.M. Reingold" ],
      "venue" : "Software: Practice and experience, vol. 21, no. 11, pp. 1129–1164, 1991.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Fast approximate nearest neighbors with automatic algorithm configuration",
      "author" : [ "M. Muja", "D.G. Lowe" ],
      "venue" : "International Conference on Computer Vision Theory and Application VISSAPP’09), 2009, pp. 331–340.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Scalable nearest neighbor algorithms for high dimensional data",
      "author" : [ "M. Muja", "D. Lowe" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, no. 11, pp. 2227–2240, Nov 2014.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Optimised kd-trees for fast image descriptor matching",
      "author" : [ "C. Silpa-Anan", "R. Hartley" ],
      "venue" : "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 2008, pp. 1–8.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning methods for generic object recognition with invariance to pose and lighting",
      "author" : [ "Y. LeCun", "F.J. Huang", "L. Bottou" ],
      "venue" : "Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, vol. 2. IEEE, 2004, pp. II–97.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : "Computer Science Department, University of Toronto, Tech. Rep, vol. 1, no. 4, p. 7, 2009.  IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. -, NO. -, MONTH -  14",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Large margin gaussian mixture modeling for phonetic classification and recognition",
      "author" : [ "F. Sha", "L.K. Saul" ],
      "venue" : "Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, vol. 1. IEEE, 2006, pp. I–I.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Density estimation for statistics and data analysis",
      "author" : [ "B.W. Silverman" ],
      "venue" : "CRC press,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1986
    }, {
      "title" : "A survey on interactive lenses in visualization",
      "author" : [ "C. Tominski", "S. Gladisch", "U. Kister", "R. Dachselt", "H. Schumann" ],
      "venue" : "EuroVis State-ofthe-Art Reports, pp. 43–62, 2014.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Genome-wide atlas of gene expression in the adult mouse brain",
      "author" : [ "E.S. Lein", "M.J. Hawrylycz", "N. Ao", "M. Ayres", "A. Bensinger", "A. Bernard", "A.F. Boe", "M.S. Boguski", "K.S. Brockway", "E.J. Byrnes" ],
      "venue" : "Nature, vol. 445, no. 7124, pp. 168–176, 2007.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Creating and benchmarking a new dataset for physical activity monitoring",
      "author" : [ "A. Reiss", "D. Stricker" ],
      "venue" : "Proceedings of the 5th International Conference on PErvasive Technologies Related to Assistive Environments. ACM, 2012, p. 40.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Direct visualizations such as parallel coordinates [1] or scatterplot matrices [2] work well for a few dimensions, but do not scale to hundreds or thousands of dimensions.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "Direct visualizations such as parallel coordinates [1] or scatterplot matrices [2] work well for a few dimensions, but do not scale to hundreds or thousands of dimensions.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "viable techniques [3].",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].",
      "startOffset" : 237,
      "endOffset" : 240
    }, {
      "referenceID" : 7,
      "context" : "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].",
      "startOffset" : 242,
      "endOffset" : 245
    }, {
      "referenceID" : 8,
      "context" : "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].",
      "startOffset" : 247,
      "endOffset" : 250
    }, {
      "referenceID" : 9,
      "context" : "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 10,
      "context" : "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 11,
      "context" : "A variant of tSNE [4], [5], the Barnes Hut SNE [6] has been accepted as the state of the art for nonlinear dimensionality reduction applied to visual analysis of high-dimensional space in several application areas, such as life sciences [7], [8], [9], [10], [11], [12].",
      "startOffset" : 264,
      "endOffset" : 268
    }, {
      "referenceID" : 12,
      "context" : "[13], as well as Mühlbacher et",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] introduced Progressive Visual Analytics.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[5] and can therefore be used directly for a per-iteration visualization, as well as interaction with the intermediate results.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "Even with a per-iteration visualization of the intermediate results [13], [14], [15] the initialization time will force the user to wait minutes, or even hours, before the first intermediate result can be generated on a state-of-theart desktop computer.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "Even with a per-iteration visualization of the intermediate results [13], [14], [15] the initialization time will force the user to wait minutes, or even hours, before the first intermediate result can be generated on a state-of-theart desktop computer.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "Even with a per-iteration visualization of the intermediate results [13], [14], [15] the initialization time will force the user to wait minutes, or even hours, before the first intermediate result can be generated on a state-of-theart desktop computer.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "The tSNE [5] algorithm builds the foundation of this work.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 6,
      "context" : "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 7,
      "context" : "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 8,
      "context" : "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 9,
      "context" : "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 10,
      "context" : "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 11,
      "context" : "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 15,
      "context" : "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 16,
      "context" : "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 17,
      "context" : "As described above, tSNE is used for visualization of highdimensional data in a wide field of applications, from life sciences to the analysis of deep-learning algorithms [7], [8], [9], [10], [11], [12], [16], [17], [18].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 3,
      "context" : "An evolution of the algorithm, called Barnes-Hut-SNE (BH-SNE) [4], [6], reduces the computational complexity to O(N log(N)) and the memory complexity to O(N).",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "An evolution of the algorithm, called Barnes-Hut-SNE (BH-SNE) [4], [6], reduces the computational complexity to O(N log(N)) and the memory complexity to O(N).",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 18,
      "context" : "[19].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "New analytical tools and algorithms, which are able to trade accuracy for speed and offer the possibility to interactively refine results [20], [21], are needed to deal with the scalability issues of existing",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 20,
      "context" : "New analytical tools and algorithms, which are able to trade accuracy for speed and offer the possibility to interactively refine results [20], [21], are needed to deal with the scalability issues of existing",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "[14] defined different strategies to increase the user involvement in existing algorithms.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] defined the term Progressive Visual Analytics that describes techniques that allow the analyst to directly interact with the analytics process.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "approximation of tSNE’s initialization stage, followed by a user steerable [22] refinement of the level of approximation.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "Under these conditions, a traditional algorithm and data structure, such as a KD-Tree [23], will not perform well.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "In the BH-SNE [6] algorithm, a Vantage-Point Tree [24] is used for the KNN search, but it is slow to query.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 23,
      "context" : "In the BH-SNE [6] algorithm, a Vantage-Point Tree [24] is used for the KNN search, but it is slow to query.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : "For example MDSteer [25] works on a subset of the data and allows the user to control the insertion of points by selecting areas in the reduced space.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 25,
      "context" : "[26] present a dimensionality-reduction technique using a dissimilarity matrix as input.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] and Paulovich et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[28]",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[29], propose the use of a non-linear dimensionality-reduction algorithm on a small number of automatically-selected control points.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "Ingram and Munzner’s Q-SNE [30] is based on a similar idea, also using Approximated KNN queries for the computation of the high-dimensional similarities.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "Density-based visualization of the tSNE embedding has been used in several works [4], [7], [12], however, they employ slow to compute offline techniques.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "Density-based visualization of the tSNE embedding has been used in several works [4], [7], [12], however, they employ slow to compute offline techniques.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "Density-based visualization of the tSNE embedding has been used in several works [4], [7], [12], however, they employ slow to compute offline techniques.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 30,
      "context" : "In our work, we integrate real time Kernel Density Estimation (KDE) as described by Lampe and Hauser [31].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 31,
      "context" : "The iVisClassifier system [32] is an example of such a solution.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "In this section, we provide a short introduction to tSNE [5], which is necessary to explain our contribution.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 32,
      "context" : "The gradient descent can be seen as a N-body simulation [33], where each data-point exerts an attractive and a repulsive force on all the other points (F attr i and F rep i ).",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "Barnes-Hut-SNE (BH-SNE) [4], [6] is an evolution of the tSNE algorithm that introduces two different approximations to reduce the computational complexity to O(N log(N)) and the memory complexity to O(N).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "Barnes-Hut-SNE (BH-SNE) [4], [6] is an evolution of the tSNE algorithm that introduces two different approximations to reduce the computational complexity to O(N log(N)) and the memory complexity to O(N).",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "Without compromising the quality of the embedding [4], [6], we can adopt a sparse approximation of the high-dimensional similarities.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "Without compromising the quality of the embedding [4], [6], we can adopt a sparse approximation of the high-dimensional similarities.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : "The computation of the K-Nearest Neighbors is performed using a Vantage-Point Tree (VP-Tree) [24].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 33,
      "context" : "As described above tSNE can be seen as an N-body simulation and thus the BarnesHut algorithm [34] can be used to reduce the computational complexity to O(N log(N)).",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "For further details, we refer to van der Maaten [4], [6].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "For further details, we refer to van der Maaten [4], [6].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 21,
      "context" : "The level of approximation can be refined by the analyst in interesting regions of the embedding, making A-tSNE a computational steerable algorithm [22].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "timization process can be interpreted by the analyst while they change over time, as shown in previous work [14], [15].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "timization process can be interpreted by the analyst while they change over time, as shown in previous work [14], [15].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "the initialization of the technique, that cannot be implemented in an iterative way, creating a speed bump [13] in",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 34,
      "context" : "In the following section, we describe the A-tSNE algorithm in detail using the MNIST [35] dataset for illustration.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 35,
      "context" : "To better understand the effect of the approximated queries, it is useful to interpret the BH-SNE algorithm as a force-directed layout algorithm [36], which acts on an",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 36,
      "context" : "search [37].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 37,
      "context" : "[38].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "In this work, we use a space partitioning technique called Forest of Randomized KD-Trees [39] to compute the approximated neighborhoods.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 36,
      "context" : "to be fast and effective in querying of high-dimensional spaces [37].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "A KD-Tree [23] is a binary tree used to partition a kdimensional space.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 36,
      "context" : "[37] and expose only the single precision parameter ρ to the user.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "[37].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "A-tSNE is computationally steerable [22], in the sense that the user can define the level of approximation to specific, interesting areas.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "[5].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "We apply the same preprocessing steps as presented by van der Maaten [4], without applying a preliminary dimensionality-reduction by means of a Principal Component Analysis.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 34,
      "context" : "We use the MNIST dataset [35] (60k data-points with 784 dimensions), the NORB dataset [40] (24300 data-points with 9216 dimensions), the CIFAR-10 dataset [41] (50k points with 1024 dimensions) and the",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 39,
      "context" : "We use the MNIST dataset [35] (60k data-points with 784 dimensions), the NORB dataset [40] (24300 data-points with 9216 dimensions), the CIFAR-10 dataset [41] (50k points with 1024 dimensions) and the",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 40,
      "context" : "We use the MNIST dataset [35] (60k data-points with 784 dimensions), the NORB dataset [40] (24300 data-points with 9216 dimensions), the CIFAR-10 dataset [41] (50k points with 1024 dimensions) and the",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 41,
      "context" : "TIMIT dataset [42], consisting of 1M data-points with 39 dimensions.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "Throughout the experiments we used a parameter setup similar to the one used to benchmark the BHSNE [4], [6].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Throughout the experiments we used a parameter setup similar to the one used to benchmark the BHSNE [4], [6].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 36,
      "context" : "[37].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "Computation time of the high-dimensional similarities in the MNIST dataset by using BH-SNE [6] and by using A-tSNE with different parameters.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "areas of the high-dimensional space are responsible for the creation of the global relationship in a tSNE embedding [5].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 42,
      "context" : "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "Density-based [44] visualizations are commonly used to show a tSNE embedding [4], [7], [9], [12] because of their ability to visualize features at different scales.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 30,
      "context" : "We apply a real-time kernel density estimation (KDE) [31] for the creation of an interactive density-based visualization of the embedding.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 43,
      "context" : "First, we introduce a Magic Lens [45] that shows the approximation with a minimal conceal of the data.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 36,
      "context" : "The approximated neighborhoods are computed using the FLANN library [37], which implements KNN algorithms.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 30,
      "context" : "A geometry shader is used to generate a quad for each point that is colored using the precomputed texture, the KDE is obtained by drawing into a Frame Buffer Object using an additive blending [31].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 10,
      "context" : "[11], who use tSNE to explore the Allen Mouse Brain dataset [46].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 44,
      "context" : "[11], who use tSNE to explore the Allen Mouse Brain dataset [46].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 45,
      "context" : "As proof of concept, we selected a dataset for physical activity monitoring [47] that comprises readings of three Inertial Measurement Units (IMU) and a heart rate monitor applied to 9 different subjects.",
      "startOffset" : 76,
      "endOffset" : 80
    } ],
    "year" : 2016,
    "abstractText" : "Progressive Visual Analytics aims at improving the interactivity in existing analytics techniques by means of visualization as well as interaction with intermediate results. One key method for data analysis is dimensionality reduction, for example, to produce 2D embeddings that can be visualized and analyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is a well-suited technique for the visualization of several high-dimensional data. tSNE can create meaningful intermediate results but suffers from a slow initialization that constrains its application in Progressive Visual Analytics. We introduce a controllable tSNE approximation (A-tSNE), which trades off speed and accuracy, to enable interactive data exploration. We offer real-time visualization techniques, including a density-based solution and a Magic Lens to inspect the degree of approximation. With this feedback, the user can decide on local refinements and steer the approximation level during the analysis. We demonstrate our technique with several datasets, in a real-world research scenario and for the real-time analysis of high-dimensional streams to illustrate its effectiveness for interactive data analysis.",
    "creator" : null
  }
}