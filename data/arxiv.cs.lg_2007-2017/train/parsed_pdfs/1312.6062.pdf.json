{
  "name" : "1312.6062.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error",
    "authors" : [ "David Buchaca Prats", "Enrique Romero Merino" ],
    "emails" : [ "DAVIDBUCHACA@GMAIL.COM", "EROMERO@LSI.UPC.EDU", "FERRAN.MAZZANTI@UPC.EDU", "JDELGADO@LSI.UPC.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.\nProceedings of the 2nd International Conference on Learning Representations, Banff, Canada, 2014. Copyright 2014 by the author(s)."
    }, {
      "heading" : "1. Introduction",
      "text" : "Learning algorithms for deep multi-layer neural networks have been known for a long time (Rumelhart et al., 1986), though none of them have been widely used to solve large scale real-world problems. In 2006, Deep Belief Networks (DBNs) (Hinton et al., 2006) came out as a real breakthrough in this field, since the learning algorithms proposed ended up being a feasible and practical method to train these networks, with spectacular results (Hinton & Salakhutdinov, 2006; Larochelle et al., 2009; Lee et al., 2009; Le et al., 2012). DBNs have Restricted Boltzmann Machines (RBMs) (Smolensky, 1986) as their building blocks.\nRBMs are topologically constrained Boltzmann Machines (BMs) with two layers, one of hidden and another of visible neurons, and no intra-layer connections. This property makes working with RBMs simpler than with regular BMs, and in particular the stochastic computation of the log-likelihood gradient may be performed more efficiently by means of Gibbs sampling (Bengio, 2009).\nIn 2002, the Contrastive Divergence learning algorithm (CD) was proposed as an efficient training method for product-of-expert models, from which RBMs are a special case (Hinton, 2002). It was observed that using CD to train\nar X\niv :1\n31 2.\n60 62\nv2 [\nRBMs worked quite well in practice. This fact was important for deep learning since some authors suggested that a multi-layer deep neural network is better trained when each layer is pre-trained separately as if it were a single RBM (Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Larochelle et al., 2009). Thus, training RBMs with CD and stacking up RBMs seems to be a good way to go when designing deep learning architectures.\nHowever, the picture is not as nice as it looks. CD is not a flawless training algorithm. Despite CD being an approximation of the true log-likelihood gradient (Bengio & Delalleau, 2009), it is biased and it may not converge in some cases (Carreira-Perpiñán & Hinton, 2005; Yuille, 2005; MacKay, 2001). Moreover, it has been observed that CD, and variants such as Persistent CD (Tieleman, 2008) or Fast Persistent CD (Tieleman & Hinton, 2009) can lead to a steady decrease of the log-likelihood during learning (Fischer & Igel, 2010; Desjardins et al., 2010). Therefore, the risk of learning divergence imposes the requirement of a stopping criterion. The two main methods used to decide when to stop the learning process are reconstruction error and Annealed Importance Sampling (AIS) (Neal, 1998). Reconstruction error is easy to compute and it has been often used in practice, though its adequacy remains unclear (Fischer & Igel, 2010). AIS seems to work better than reconstruction error in some cases, though it my also fail (Schulz et al., 2010).\nIn this paper we propose an alternative stopping criteria for CD and show its preliminary results. These criteria are based on the computation of two probabilities that do not require from the knowledge of the partition function of the system. The early detection of the decrease of the likelihood allows to overcome the reconstruction error faulty observed behavior."
    }, {
      "heading" : "2. Learning in Restricted Boltzmann Machines",
      "text" : ""
    }, {
      "heading" : "2.1. Energy-based Probabilistic Models",
      "text" : "Energy-based probabilistic models define a probability distribution from an energy function, as follows:\nP (x,h) = e−Energy(x,h)\nZ , (1)\nwhere x stand for visible variables and h are hidden variables (typically binary) introduced to increase the expressive power of the model. The normalization factor Z is called partition function and reads\nZ = ∑ x,h e−Energy(x,h) . (2)\nSince only x is observed, one is only interested in the\nmarginal distribution\nP (x) =\n∑ h e −Energy(x,h)\nZ , (3)\nbut the evaluation of the partition function Z is computationally prohibitive since it involves an exponentially large number of terms.\nThe energy function depends on several parameters θ, that are adjusted at the learning stage. This is done by maximizing the likelihood of the data. In energy-based models, the derivative of the log-likelihood can be expressed as\n−∂ logP (x; θ) ∂θ = E P (h|x)\n[ ∂Energy(x,h)\n∂θ ] − E\nP ( ∼ x) [ E P (h| ∼ x) [ ∂Energy( ∼ x,h) ∂θ ]] , (4)\nwhere the first term is called the positive phase and the second term is called the negative phase. Similar to (3), the exact computation of the derivative of the log-likelihood is usually unfeasible because of the second term in (4), which comes from the derivative of the partition function."
    }, {
      "heading" : "2.2. Restricted Boltzmann Machines",
      "text" : "Restricted Boltzmann Machines are energy-based probabilistic models whose energy function is:\nEnergy(x,h) = −btx− cth− htWx . (5)\nRBMs are at the core of DBNs (Hinton et al., 2006) and other deep architectures that use RBMs to unsupervised pre-training previous to the supervised step (Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Larochelle et al., 2009).\nThe consequence of the particular form of the energy function is that in RBMs both P (h|x) and P (x|h) factorize. In this way it is possible to compute P (h|x) and P (x|h) in one step, making possible to perform Gibbs sampling efficiently (Geman & Geman, 1984) that can be the basis of the computation of an approximation of the derivative of the log-likelihood (4)."
    }, {
      "heading" : "2.3. Contrastive Divergence",
      "text" : "The most common learning algorithm for RBMs uses an algorithm to estimate the derivative of the log-likelihood of a Product of Experts model called CD (Hinton, 2002).\nThe algoritmh for CDn estimates the derivative of the loglikelihood as\n−∂ logP (x1; θ) ∂θ ' E P (h|x1)\n[ ∂Energy(x1,h)\n∂θ ] − E\nP (h|xn+1)\n[ ∂Energy(xn+1,h)\n∂θ\n] . (6)\nwhere xn+1 is the last sample from the Gibbs chain starting from x1 obtained after n steps:\nh1 ∼ P (h|x1)\nx2 ∼ P (x|h1)\n...\nhn ∼ P (h|xn)\nxn+1 ∼ P (x|hn) .\nFor binary RBMs, E P (h|x)\n[ ∂Energy(x,h)\n∂θ\n] can be easily\ncomputed.\nSeveral alternatives to CDn are Persistent CD (PCD) (Tieleman, 2008), Fast PCD (FPCD) (Tieleman & Hinton, 2009) or Parallel Tempering (PT) (Desjardins et al., 2010)."
    }, {
      "heading" : "2.4. Monitoring the Learning Process in RBMs",
      "text" : "Learning in RBMs is a delicate procedure involving a lot of data processing that one seeks to perform at a reasonable fast speed in order to be able to handle large spaces with a huge amount of states. In doing so, drastic approximations that can only be understood in a statistically averaged sense are performed (section 2.3).\nOne of the most relevant points to consider at the learning stage is to find a good way to determine whether a good solution has been found or not, and so to determine when should the learning process stop. One of the most widely used criteria for stopping is the reconstruction error, which is a measure of the capability of the network to produce an output that is consistent with the data at input. Since RBMs are probabilistic models, the reconstruction error of a data point x(i) is computed as the probability of x(i) given the expected value of h for x(i):\nR(x(i)) = P ( x(i)|E [ h|x(i) ]) , (7)\nwhich is the equivalent of the sum-of-squares reconstruction error for deterministic networks.\nSome authors have shown that it may happen that learning induces an undesirable decrease in likelihood that goes undetected by the reconstruction error (Schulz et al., 2010; Fischer & Igel, 2010). It has been studied (Fischer & Igel, 2010) that the reconstruction error defined in (7) usually decreases monotonically. Since no increase in the reconstruction error takes place during training there is no apparent way to detect the change of behavior of the log-likelihood for CDn."
    }, {
      "heading" : "3. Proposed Stopping Criteria",
      "text" : "The proposed stopping criteria are based on the monitorization of the ratio of two probabilities: the probability of the data (that should be high) and the probability of points in the input space whose probability should be low. More formally, it can be defined as:\nξ = P (X)\nP (Y ) = N∏ i=1 P (x(i)) P (y(i)) , (8)\nwhere X stands for the complete training set of N samples and Y is a suitable artificially generated data set. The data set Y can be generated in different ways (see below).\nThe idea behind ξ comes from the fact that the standard gradient descent update rule used during learning requires from the evaluation of two terms: the positive and negative phases. The positive phase tends to decrease the energy (hence increase the probability) of the states related to the training data, while the negative phase tends to increase the energy of the whole set of states with the corresponding decrease in probability. In this way, if Y is selected so as to have low probability, the numerator in ξ is expected to increase while the denominator is expected to decrease during the learning process, making ξ maximal when learning is achieved.\nMost relevant to the discussion is the fact that, being a ratio of probabilities computed at every step of the Markov chain built on-the-fly, the partition functions Z involved in P (X) and P (Y ) cancel out in ξ. In other words, the computation of ξ can be equivalently defined as\nξ = P (X)\nP (Y ) = N∏ i=1 ∑ h e −Energy(x(i),h)∑ h e −Energy(y(i),h) . (9)\nThe particular topology of RBMs allows to compute∑ h e −Energy(x,h) efficiently. This fact dramatically decreases the computational cost involved in the calculation, which would otherwise become unfeasible in most realworld problems where RBMs could been successfully applied.\nWhile P (x(i)) in ξ is directly evaluated from the data in the training set, the problem of finding suitable values for Y still remains. In order to select a point y(i) with low probability, one may seek for zones of the space distant from x(i), thus representing the complementary of the features to be learnt. This point should not be difficult to find. On the one hand, in small spaces one can enumerate the states. On the other hand, in large spaces with a small training set X the probability that a state picked up at random does not belong to X should be large. A second possibility is, for fixed x(i), to suitably change the values of the hidden units during learning in such a way that they differ\nsignificantly from the values they should take during data reconstruction. We expect that, once learning is done, the reconstruction vectors should be independent of the value of the hidden units. However, this may not be the case while the system is still learning, as the basins of attraction of the energy functional depend explicitly on the values of the weights and bias terms, which can change significantly. This is in fact the main idea behind the stopping criteria proposed in this work, that we shall exploit in the following.\nWith all that in mind, two different alternatives have been explored:\ni) y(i) = E[x|hs], where hs is a random vector whose components are drawn from the uniform distribution in [0,1].\nii) y(i) = E[x|hs], where hs = 1 − h(i)1 , i.e., the complementary of the first hidden vector obtained in the Gibbs chain for x(i).\nRegarding the first alternative, random hidden vectors are expected to lead to regions of low reconstruction probability, at least while the system is still learning. In the second alternative, we expect that if a good reconstruction of x(i) is achieved for a certain value of h(i)1 (see Eq. (7)), the opposite should happen when 1− h(i)1 is used instead.\nOther related possibilities like monitoring the average value E[h|x(i)1 ] and using its complementary instead of 1 − h (i) 1 have also been explored and yield similar results to the ones shown in the following.\nNotice that the reconstruction error only gathers information from the training set X , while the proposed estimator ξ in equation (8) samples also states from the rest of the input space."
    }, {
      "heading" : "4. Experiments",
      "text" : "We performed several experiments to explore the aforementioned alternatives defined in section 3 and compare the behavior of the estimator ξ to that of the actual loglikelihood and the reconstruction error in a couple of problems.\nThe first problem, denoted Bars and Stripes (BS), tries to identify vertical and horizontal lines in 4×4 pixel images. The training set consists in the whole set of images containing all possible horizontal or vertical lines (but not both), ranging from no lines (blank image) to completely filled images (black image), thus producing 2 × 24 − 2 = 30 different images (avoiding the repetition of fully back and fully white images) out of the space of 216 possible images with black or white pixels. The second problem, named\nLabeled Shifter Ensemble (LSE), consists in learning 19-bit states formed as follows: given an initial 8-bit pattern, generate three new states concatenating to it the bit sequences 001, 010 or 100. The final 8-bit pattern of the state is the original one shifting one bit to the left if the intermediate code is 001, copying it unchanged if the code is 010, or shifting it one bit to the right if the code is 100. One thus generates the training set using all possible 28 × 3 = 768 states that can be created in this form, while the system space consists of all possible 219 different states one can build with 19 bits. These two problems have already been explored in (Fischer & Igel, 2010) and are adequate in the current context since, while still large, the dimensionality of space allows for a direct monitorization of the partition function and the log-likelihood during learning.\nIn the following we discuss the learning processes of both problems with single RBMs, employing the Contrastive Divergence algorithm CDn with n = 1 and n = 10 as described in section 2.3. In all cases, binary visible and hidden units were used. In the BS case the RBM had 16 visible and 8 hidden units, while in the LSE problem these numbers were 19 and 10, respectively. Every simulation was carried out for a total of 50000 epochs, with measures being taken every 50 epochs. Moreover, every point in the subsequent plots was the average of ten different simulations starting from different random values of the weights and bias. Other parameters affecting the results that were changed along the analysis are the learning rate (LR) involved in the weight and bias update rules and a weight decay parameter (WD) that prevents weights from achieving large values that would saturate the sigmoid functions present in the analytical expressions associated to binary RBMs.\nWe present the results for the two problems at hand, showing for each instance analyzed three different plots corresponding to the actual log-likelihood of the problem, log(ξ) (ξ as defined in (9)) and the logarithm of the reconstructed error (7), all three quantities monitored during the learning process.\nFigure 1 shows results for the BS problem using the alternatives i) and ii) defined in section 3 using CD1 with LR=0.01 and WD=0. The left panel corresponds to alternative i) and the right panel corresponds to alternative ii). As can be seen, the log-likelihood increases very rapidly, reaches a maximum and then starts decreasing, thus indicating that further learning only worsens the model. In both cases, though, the log probability of the reconstruction converges towards a constant value (very near 0, indicating high probabilities for the reconstructed data), giving the false impression that going on with the learning process will neither improve nor worsen the predictions produced by the network. Interestingly enough, though, the middle\nFigure 1. Log-likelihood, log(ξ) and log-probability of the reconstruction (upper, middle and lower panels) for the BS problem. Left and right columns correspond to options i) and ii) when choosing values for the hidden units using CD1 with LR=0.01 and WD=0.\nplot on the right panel indicate that ii) is able to capture the increasing and decreasing behavior of the log-likelihood, a feature that i) seems to miss. At this point it looks like ii) is a better estimator of optimal log-likelihood than the reconstruction error. This same behaviour is seen in figure 2 where a weight decay value WD=0.001 is employed.\nThe LSE problem yields somewhat similar results under the same learning and monitoring conditions. The loglikelihood, log(ξ) and log-reconstruction error are shown as before in the upper, middle and lower panels of figure 3, with options i) and ii) on the left and right, respectively. In this case the learning rate has been set to LR=0.001 (otherwise the log-likelihood of the problem decreases monotonically). In this case, however, both estimators i) and ii) are able to find the region where the log-likelihood is max-\nimal, decreasing similarly to the later when this point is surpassed.\nThese results seem to indicate that estimator ii) is more robust than estimator i). Still, these two are better than the reconstruction error which always present a similar pattern, both for the BS and LSE problems, with a transient regime that always stabilizes to a plateau that apparently has little to do with the actual behavior of the log-likelihood.\nAll these results have been obtained in the CD1 approximation. Since it is known that CDn with increasing n can lead to better learning results because of the increased statistical independence of the input and output values generated, estimators i) and ii) can also be used in this case. We have checked their performance using CD10 on the same two problems at hand. Results for the LSE problem using CD10, LR=0.01 and WD=0 are shown in the left and right panels of figure 4 for estimators i) and ii), respectively. In this case, none of the estimators is able to detect the region\nFigure 3. Results for the LSE problem as reported in figure 1 for CD1, LR=0.001 and WD=0.001.\nof maximal likelihood, stressing that none of these shall be used as a test to stop the learning algorithm. However, the reconstruction error has a similar behavior, thus indicating that it is not a good testing quantity either. Similar results for the BS problem are obtained when using CD10. A possible explanation can be related to the fact that the Markov chain involved in the process tends to lose memory with increasing number of steps. Therefore, ξ is computed with more independent data in CD10 than in CD1. Anyway, the behavior of the proposed criteria with CD10 should be further studied.\nAs a final remark, we note that for the BS problem the trained RBM stopped using the proposed criteria is able to qualitatively generate samples similar to those in the training set. We show in figure 5 the complete training set (two upper rows) and the same number of generated samples obtained from the RBM stopped after 3000 epochs in the training process using CD1 as discussed above, cor-\nresponding to the maximum value of the proposed criterion ii), which coincides with the optimal value of the loglikelihood (two lower rows in the same figure)."
    }, {
      "heading" : "5. Conclusion",
      "text" : "Based on the fact that learning tries to increase the contribution of the relevant states while decreasing the rest, two new estimators based on the ratio of two probabilities have been proposed and discussed as an alternative to the reconstruction error. It has been shown that the better one, obtained by replacing the value of the (binary) hidden units h by 1 − h, can at some point be able to monitor the actual behavior of the log-likelihood of the model without additional computational cost. This estimator works well for CD1 but for CD10, which is considered to yield better learning results at the expense however of a linear increase in computational cost. We believe that the use of the estimator presented here in CD1 learning problems provides a\nfaster stopping criteria for the learning algorithm that can yield results compatible in quality to those obtained in standard CDn learning for moderate n. Future work along this line will be carried out in an attempt to formalize that statement."
    }, {
      "heading" : "Acknowledgments",
      "text" : "JD: This work was partially supported by MICINN project TIN2011-27479-C04-03 (BASMATI) and by SGR20091428 (LARCA).\nFM: This work has been supported by grant No. FIS201125275 from DGI (Spain) and Grant No. 2009-SGR1003 from the Generalitat de Catalunya (Spain).\nER: This research is partially funded by Spanish research project TIN2012-31377."
    } ],
    "references" : [ {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Y. Bengio" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bengio,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio",
      "year" : 2009
    }, {
      "title" : "Justifying and Generalizing Contrastive Divergence",
      "author" : [ "Y. Bengio", "O. Delalleau" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Bengio and Delalleau,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio and Delalleau",
      "year" : 2009
    }, {
      "title" : "Greedy Layer-wise Training of Deep Networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle" ],
      "venue" : "In Advances in Neural Information Processing (NIPS’06),",
      "citeRegEx" : "Bengio et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2007
    }, {
      "title" : "On Contrastive Divergence Learning",
      "author" : [ "M.A. Carreira-Perpiñán", "G.E. Hinton" ],
      "venue" : "In International Workshop on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Carreira.Perpiñán and Hinton,? \\Q2005\\E",
      "shortCiteRegEx" : "Carreira.Perpiñán and Hinton",
      "year" : 2005
    }, {
      "title" : "Empirical Analysis of the Divergence of Gibbs Sampling Based Learning Algorithms for Restricted Boltzmann Machines",
      "author" : [ "A. Fischer", "C. Igel" ],
      "venue" : "In International Conference on Artificial Neural Networks (ICANN),",
      "citeRegEx" : "Fischer and Igel,? \\Q2010\\E",
      "shortCiteRegEx" : "Fischer and Igel",
      "year" : 2010
    }, {
      "title" : "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images",
      "author" : [ "S. Geman", "D. Geman" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Geman and Geman,? \\Q1984\\E",
      "shortCiteRegEx" : "Geman and Geman",
      "year" : 1984
    }, {
      "title" : "Training Products of Experts by Minimizing Contrastive Divergence",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton,? \\Q2002\\E",
      "shortCiteRegEx" : "Hinton",
      "year" : 2002
    }, {
      "title" : "Reducing the Dimensionality of Data with",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "Neural Networks. Science,",
      "citeRegEx" : "Hinton and Salakhutdinov,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton and Salakhutdinov",
      "year" : 2006
    }, {
      "title" : "A Fast Learning Algorithm for Deep Belief Nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y. Teh" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Exploring Strategies for Training Deep Neural Networks",
      "author" : [ "H. Larochelle", "Y. Bengio", "J. Lourador", "P. Lamblin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Larochelle et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Larochelle et al\\.",
      "year" : 2009
    }, {
      "title" : "Building High-level Features Using Large Scale Unsupervised Learning",
      "author" : [ "Q.V. Le", "M.A. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "A.Y. Ng" ],
      "venue" : "In 29th International Conference on Machine Learning,",
      "citeRegEx" : "Le et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2012
    }, {
      "title" : "Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations",
      "author" : [ "H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng" ],
      "venue" : "In International Conference on Machine Learning, pp",
      "citeRegEx" : "Lee et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2009
    }, {
      "title" : "Investigating Convergence of Restricted Boltzmann Machine Learning",
      "author" : [ "H. Schulz", "A. Müller", "S. Behnke" ],
      "venue" : "In NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning,",
      "citeRegEx" : "Schulz et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Schulz et al\\.",
      "year" : 2010
    }, {
      "title" : "Information Processing in Dynamical Systems: Foundations of Harmony Theory",
      "author" : [ "P. Smolensky" ],
      "venue" : "Parallel Distributed Processing: Explorations in the Microstructure of Cognition (vol",
      "citeRegEx" : "Smolensky,? \\Q1986\\E",
      "shortCiteRegEx" : "Smolensky",
      "year" : 1986
    }, {
      "title" : "Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient",
      "author" : [ "T. Tieleman" ],
      "venue" : "In 25th International Conference on Machine Learning,",
      "citeRegEx" : "Tieleman,? \\Q2008\\E",
      "shortCiteRegEx" : "Tieleman",
      "year" : 2008
    }, {
      "title" : "Using Fast Weights to Improve Persistent Contrastive Divergence",
      "author" : [ "T. Tieleman", "G.E. Hinton" ],
      "venue" : "In 26th International Conference on Machine Learning,",
      "citeRegEx" : "Tieleman and Hinton,? \\Q2009\\E",
      "shortCiteRegEx" : "Tieleman and Hinton",
      "year" : 2009
    }, {
      "title" : "The Convergence of Contrastive Divergence",
      "author" : [ "A. Yuille" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS’04),",
      "citeRegEx" : "Yuille,? \\Q2005\\E",
      "shortCiteRegEx" : "Yuille",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure.",
      "startOffset" : 148,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "In 2006, Deep Belief Networks (DBNs) (Hinton et al., 2006) came out as a real breakthrough in this field, since the learning algorithms proposed ended up being a feasible and practical method to train these networks, with spectacular results (Hinton & Salakhutdinov, 2006; Larochelle et al.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : ", 2006) came out as a real breakthrough in this field, since the learning algorithms proposed ended up being a feasible and practical method to train these networks, with spectacular results (Hinton & Salakhutdinov, 2006; Larochelle et al., 2009; Lee et al., 2009; Le et al., 2012).",
      "startOffset" : 191,
      "endOffset" : 281
    }, {
      "referenceID" : 11,
      "context" : ", 2006) came out as a real breakthrough in this field, since the learning algorithms proposed ended up being a feasible and practical method to train these networks, with spectacular results (Hinton & Salakhutdinov, 2006; Larochelle et al., 2009; Lee et al., 2009; Le et al., 2012).",
      "startOffset" : 191,
      "endOffset" : 281
    }, {
      "referenceID" : 10,
      "context" : ", 2006) came out as a real breakthrough in this field, since the learning algorithms proposed ended up being a feasible and practical method to train these networks, with spectacular results (Hinton & Salakhutdinov, 2006; Larochelle et al., 2009; Lee et al., 2009; Le et al., 2012).",
      "startOffset" : 191,
      "endOffset" : 281
    }, {
      "referenceID" : 13,
      "context" : "DBNs have Restricted Boltzmann Machines (RBMs) (Smolensky, 1986) as their building blocks.",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "This property makes working with RBMs simpler than with regular BMs, and in particular the stochastic computation of the log-likelihood gradient may be performed more efficiently by means of Gibbs sampling (Bengio, 2009).",
      "startOffset" : 206,
      "endOffset" : 220
    }, {
      "referenceID" : 6,
      "context" : "In 2002, the Contrastive Divergence learning algorithm (CD) was proposed as an efficient training method for product-of-expert models, from which RBMs are a special case (Hinton, 2002).",
      "startOffset" : 170,
      "endOffset" : 184
    }, {
      "referenceID" : 2,
      "context" : "This fact was important for deep learning since some authors suggested that a multi-layer deep neural network is better trained when each layer is pre-trained separately as if it were a single RBM (Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Larochelle et al., 2009).",
      "startOffset" : 197,
      "endOffset" : 273
    }, {
      "referenceID" : 9,
      "context" : "This fact was important for deep learning since some authors suggested that a multi-layer deep neural network is better trained when each layer is pre-trained separately as if it were a single RBM (Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Larochelle et al., 2009).",
      "startOffset" : 197,
      "endOffset" : 273
    }, {
      "referenceID" : 16,
      "context" : "Despite CD being an approximation of the true log-likelihood gradient (Bengio & Delalleau, 2009), it is biased and it may not converge in some cases (Carreira-Perpiñán & Hinton, 2005; Yuille, 2005; MacKay, 2001).",
      "startOffset" : 149,
      "endOffset" : 211
    }, {
      "referenceID" : 14,
      "context" : "Moreover, it has been observed that CD, and variants such as Persistent CD (Tieleman, 2008) or Fast Persistent CD (Tieleman & Hinton, 2009) can lead to a steady decrease of the log-likelihood during learning (Fischer & Igel, 2010; Desjardins et al.",
      "startOffset" : 75,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "AIS seems to work better than reconstruction error in some cases, though it my also fail (Schulz et al., 2010).",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 8,
      "context" : "RBMs are at the core of DBNs (Hinton et al., 2006) and other deep architectures that use RBMs to unsupervised pre-training previous to the supervised step (Hinton & Salakhutdinov, 2006; Bengio et al.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : ", 2006) and other deep architectures that use RBMs to unsupervised pre-training previous to the supervised step (Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Larochelle et al., 2009).",
      "startOffset" : 112,
      "endOffset" : 188
    }, {
      "referenceID" : 9,
      "context" : ", 2006) and other deep architectures that use RBMs to unsupervised pre-training previous to the supervised step (Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Larochelle et al., 2009).",
      "startOffset" : 112,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "The most common learning algorithm for RBMs uses an algorithm to estimate the derivative of the log-likelihood of a Product of Experts model called CD (Hinton, 2002).",
      "startOffset" : 151,
      "endOffset" : 165
    }, {
      "referenceID" : 14,
      "context" : "Several alternatives to CDn are Persistent CD (PCD) (Tieleman, 2008), Fast PCD (FPCD) (Tieleman & Hinton, 2009) or Parallel Tempering (PT) (Desjardins et al.",
      "startOffset" : 52,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "Some authors have shown that it may happen that learning induces an undesirable decrease in likelihood that goes undetected by the reconstruction error (Schulz et al., 2010; Fischer & Igel, 2010).",
      "startOffset" : 152,
      "endOffset" : 195
    } ],
    "year" : 2014,
    "abstractText" : "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning. Proceedings of the 2 International Conference on Learning Representations, Banff, Canada, 2014. Copyright 2014 by the author(s).",
    "creator" : "TeX"
  }
}