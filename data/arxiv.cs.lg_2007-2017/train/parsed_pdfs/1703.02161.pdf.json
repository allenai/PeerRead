{
  "name" : "1703.02161.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distance Metric Learning using Graph Convolutional Networks: Application to Functional Brain Networks",
    "authors" : [ "Sofia Ira Ktena", "Sarah Parisot", "Enzo Ferrante", "Martin Rajchl", "Matthew Lee", "Ben Glocker", "Daniel Rueckert" ],
    "emails" : [ "ira.ktena@imperial.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The highly challenging problem of inexact graph matching entails the evaluation of how much two graphs share or, conversely, how much they differ [9]. Obtaining a measure of global similarity between two graphs can facilitate classification and clustering problems. This concept is particularly valuable in brain connectivity studies, which involve the representation of the structural and/or functional connections within the brain as labelled graphs. Resting-state fMRI (rs-fMRI) can be used to map the connections between spatially remote regions in order to obtain functional networks incorporating the strength of these connections in their edge labels. At the same time, disruptions to this functional network organisation have been associated with neurodevelopmental disorders, such as autism spectrum disorder (ASD) [1]. As a result, studying the brain’s organisation has the potential to identify predictive biomarkers for neurodevelopmental\n? The support of the EPSRC CDT (HiPEDS, Grant Reference EP/L016796/1) is greatfully acknowledged. Email correspondence to: ira.ktena@imperial.ac.uk\nar X\niv :1\n70 3.\n02 16\n1v 2\n[ cs\n.C V\n] 1\n4 Ju\nn 20\n2 disorders, a task of great importance for understanding the disorder’s underlying mechanisms. Such tasks require an accurate metric of similarity/distance between brain networks to apply statistical and machine learning analyses.\nRelated work: The estimation of (dis)similarity between two graphs has, most commonly, been dealt with using four mainstream approaches [9]: graph kernels, graph embedding, motif counting and graph edit distance. Graph kernels have been employed to compare functional brain graphs [15], but often fail to capture global properties as they compare features of smaller subgraphs. Graph embedding involves obtaining a feature vector representation that summarizes the graph topology in terms of well-known network features. This method has been widely used to estimate brain graph similarity [1], since it facilitates the application of traditional classification or regression analyses. However, it often discards valuable information about the graph structure. Counting motifs, i.e. occurrences of significant subgraph patterns, has also been used [13], but is a computationally expensive process. Finally, methods based on graph edit distance neatly model both structural and semantic variation within the graphs and are particularly useful in cases of unknown node correspondences [12], but are limited by the fact that they require the definition of the edit costs in advance.\nRecently, different neural network models have been explored to learn a similarity function that compares images patches [16,8]. The network architectures investigated employ 2D convolutions to yield hierarchies of features and deal with the different factors that affect the final appearance of an image. However, the application of convolutions on irregular graphs, such as brain connectivity graphs, is not straightforward. One of the main challenges is the definition of a local neighbourhood structure, which is required for convolution operations. Recent work has attempted to address this challenge by employing a graph labelling procedure for the construction of a receptive field [11], but requires node features to meet certain criteria dictated by the labelling function (e.g. categorical values). Shuman et al. [14] introduced the concept of signal processing on graphs, through the use of computational harmonic analysis to perform data processing tasks, like filtering. This allows convolutions to be dealt as multiplications in the graph spectral domain, rendering the extension of CNNs to irregular graphs feasible. Recent work by [3,7] relies on this property to define polynomial filters that are strictly localised and employ a recursive formulation in terms of Chebyshev polynomials that allows fast filtering operations.\nContributions: In this work, we propose a novel method for learning a similarity metric between irregular graphs with known node correspondences. We use a siamese graph convolutional neural network applied to irregular graphs using the polynomial filters formulated in [3]. We employ a global loss function that, according to [8], is robust to outliers and provides better regularisation. Along with that the network learns latent representations of the graphs that are more discriminative for the application at hand. As a proof of concept, we demonstrate the model performance on the functional connectivity graphs of 871 subjects from the challenging Autism Brain Imaging Data Exchange (ABIDE) database [5], which contains heterogeneous rs-fMRI data acquired at multiple\ninternational sites with different protocols. To the best of our knowledge, this is the first application of graph convolutional networks for distance metric learning."
    }, {
      "heading" : "2 Methodology",
      "text" : "Fig. 1 gives an overview of the proposed model for learning to compare brain graphs. In this section, we first introduce the concept of graph convolutions and filtering in the graph spectral domain in 2.1, as well as the proposed network model and the loss function that we intend to minimise in 2.2. Finally, we present the dataset used and the process through which functional brain graphs are derived from fMRI data in 2.3."
    }, {
      "heading" : "2.1 Spectral Graph Filtering and Convolutions",
      "text" : "The classical definition of a convolution operation cannot be easily generalised to the graph setting, since traditional convolutional operators are only defined for regular grids, e.g. 2D or 3D images. Spectral graph theory makes this generalisation feasible by defining filters in the graph spectral domain. An essential operator in spectral graph analysis is the normalised graph Laplacian [14], defined as L = IR − D−1/2AD−1/2, where A ∈ RR×R is the adjacency matrix associated with the graph G, D is the diagonal degree matrix and IR is the identity matrix. L can be decomposed as L = UΛUT , where U is the matrix of eigenvectors and Λ the diagonal matrix of eigenvalues. The eigenvalues represent the frequencies of their associated eigenvectors, i.e. eigenvectors associated with larger eigenvalues oscillate more rapidly between connected nodes. The graph Fourier transform of a signal c can, then, be expressed as c = U ĉ. This allows to define a convolution on a graph as a multiplication in the spectral domain of the signal c with a filter gθ = diag(θ) as:\n4 gθ ∗ c = UgθUT c, (1) where θ ∈ RR is a vector of Fourier coefficients and gθ can be regarded as a function of the eigenvalues of L, i.e. gθ(Λ) [7].\nTo render the filters K-localised in space and reduce their computational complexity they can be approximated by a truncated expansion in terms of Chebyshev polynomials of order K [6]. The Chebyshev polynomials are recursively defined as Tk(c) = 2cTk−1(c) − Tk−2(c), with T0(c) = 1 and T1(c) = c. The filtering operation of a signal c with a K-localised filter is, then, given by:\ny = gθ(L)c = K∑ k=0 θkTk(L̃)c, (2)\nwith L̃ = 2λmaxL− IR, where λmax denotes the largest eigenvalue of L. The j th output feature map of sample s in a Graph Convolutional Network (GCN) is then given by:\nys,j = Fin∑ i=1 gθi,j (L)cs,i ∈ RR, (3)\nyielding Fin × Fout vectors of trainable Chebyshev coefficients θi,j ∈ RK , where cs,i denotes the input feature maps."
    }, {
      "heading" : "2.2 Loss Function and Network Architecture",
      "text" : "Our siamese network, presented in Fig. 1, consists of two identical sets of convolutional layers sharing the same weights, each taking a graph as input. An inner product layer combines the outputs from the two branches of the network and is followed by a single fully connected (FC) output layer with a sigmoid activation function and one output, that corresponds to the similarity estimate. The FC layer accounts for integrating global information about graph similarity from the preceding localised filters. Each convolutional layer is succeeded by a non-linear activation, i.e. Rectified Linear Unit (ReLU).\nWe train the network using the pairwise similarity global loss function proposed in [8] that yields superior results in the problem of learning local image descriptors compared to traditional losses. This loss maximises the mean similarity µ+ between embeddings belonging to the same class, minimises the mean similarity between embeddings belonging to different classes µ− and, at the same time, minimises the variance of pairwise similarities for both matching σ2+ and non-matching σ2− pairs of graphs. The formula of this loss function is given by:\nJg = ( σ2+ + σ2−) + λ max (0,m− (µ+ − µ−)), (4) where λ balances the importance of the mean and variance terms, and m is the margin between the means of matching and non-matching similarity distributions. An additional l2 regularisation term on the weights of the fully connected layer is introduced to the loss function.\n5"
    }, {
      "heading" : "2.3 From fMRI Data to Graph Signals",
      "text" : "The dataset is provided by the Autism Brain Imaging Data Exchange (ABIDE) initiative [5] and has been preprocessed by the Configurable Pipeline for the Analysis of Connectomes (C-PAC) [2], which involves skull striping, slice timing correction, motion correction, global mean intensity normalisation, nuisance signal regression, band-pass filtering (0.01-0.1Hz) and registration of fMRI images to standard anatomical space (MNI152). It includes N = 871 subjects from different imaging sites that met the imaging quality and phenotypic information criteria, consisting of 403 individuals suffering from ASD and 468 healthy controls. We, subsequently, extract the mean time series for a set of regions from the Harvard Oxford (HO) atlas comprising R = 110 cortical and subcortical ROIs [4] and normalise them to zero mean and unit variance.\nSpectral graph convolutional networks filter signals defined on a common graph structure for all samples, since these operations are parametrised on the graph Laplacian. As a result, we model the graph structure solely from anatomy, as the k-NN graph G = {V, E}, where each ROI is represented by a node vi ∈ V (located at the centre of the ROI) and the edges E = {eij} of the graph represent the spatial distances between connected nodes using eij = d(vi, vj) =√ ||vi − vj ||2. For each subject, node vi is associated with a signal csi : vi → RR,\ns = 1, ..., N which contains the node’s connectivity profile in terms of Pearson’s correlation between the representative rs-fMRI time series of each ROI."
    }, {
      "heading" : "3 Results",
      "text" : "We evaluate the performance of the proposed model for similarity metric learning on the ABIDE database. Similarly to the experimental setup used in [16], we train the network on matching and non-matching pairs. In this context, matching pairs correspond to brain graphs representing individuals of the same class (ASD or controls), while non-matching pairs correspond to brain graphs representing subjects from different classes. Although the ground truth labels are binary, the network output is a continuous value, hence training is performed in a weakly supervised setting. To deal with this task, we train a siamese network with 2 convolutional layers consisting of 64 features each. A binary feature is introduced at the FC layer indicating whether the subjects within the pair were scanned at the same site or not. The different network parameters are optimised using cross-validation. We use dropout ratio of 0.2 at the FC layer, regularisation 0.005, learning rate 0.001 with an Adam optimisation and K = 3, where the filters at each convolution are taking into account neighbours that are at most K steps away from a node. For the global loss function, the margin m is set to 0.6, while the weight λ is 0.35. We train the model for 100 epochs on 43000 pairs in mini-batches of 200 pairs. These pairs result from 720 different subjects (after random splitting), comprising 21802 matching and 21398 non-matching graph pairs, and we make sure that all graphs are fed to the network the same number of times to avoid biases. The test set consists of all combinations between the\nremaining 151 subjects, i.e. 11325 pairs, 5631 of which belong to the same class (either ASD or controls) and 5694 belong to different classes. We also ensure that subjects from all 20 sites are included in both training and test sets.\nTo illustrate how challenging the problem under consideration is, we show the pairwise Euclidean distances between functional connectivity matrices for 3 of the largest acquisition sites and the full test set after applying dimensionality reduction (PCA) in Fig. 2. It can be observed that networks are hardly comparable using standard distance functions, even within the same acquisition site. “All sites” refers to all pairs from the test set, even if the subjects were scanned at different sites. It can be seen that the learned metric, which corresponds to the network output and is shown at the bottom of Fig. 2, is significantly improving the separation between matching and non-matching pairs for the total test set, as well as for most individual sites. In order to demonstrate the learned metric’s ability to facilitate a subject classification task (ASD vs control), we use a simple k-nn classifier with k = 3 based the estimated distances, and summarise results in Table 1. Improvement in classification scores reaches 11.9% on the total test set and up to 40% for individual sites. Results for smaller sites are omitted, since they have very few subjects in the test set to draw conclusions from.\nFig. 3 illustrates the results on the test set through receiver operating characteristic (ROC) curves for the task of classification between matching and nonmatching graphs for the biggest 5 sites, as well as across all sites, along with the estimated area under curve (AUC). Fig. 3a shows promising results, with an overall improved performance of the proposed learned metric compared to a traditional distance measure on the whole database. The performance of the network is more striking between pairs from the same site. We obtain higher AUC values for all of the 5 biggest sites, with increases of up to 0.44 (for site 18). The limited performance for “all sites” could be attributed to the heterogeneity of the data across sites, as illustrated in Fig. 2."
    }, {
      "heading" : "4 Discussion",
      "text" : "In this work, we propose a novel metric learning method to estimate similarity between irregular graphs. We leverage the recent concept of graph convolutions through a siamese architecture and employ a loss function tailored for our task. We apply the proposed model to functional brain connectivity graphs from the ABIDE database, aiming to separate subjects from the same class and subjects from different classes. We obtain promising results across all sites, with significant increases in performance between same site pairs. While applied to brain networks, our proposed method is flexible and general enough to be applied to any problem involving comparisons between graphs, e.g. shape analysis.\nThe proposed model could benefit from several extensions. The architecture of our network is relatively simple, and further improvement in performance\n8 could be obtained by exploring more sophisticated networks. A particularly exciting prospect would be to use autoencoders and adversarial training to learn lower dimensional representation of connectivity networks that are site independent. Additionally, exploring the use of generalisable GCNs defined in the graph spatial domain [10] would allow to train similarity metrics between graphs of different structures."
    } ],
    "references" : [ {
      "title" : "Deriving reproducible biomarkers from multi-site resting-state data: An autism-based example",
      "author" : [ "A. Abraham", "M. Milham", "A. Di Martino", "R.C. Craddock", "D. Samaras", "B. Thirion", "G. Varoquaux" ],
      "venue" : "NeuroImage",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Towards automated analysis of connectomes: The configurable pipeline for the analysis of connectomes (C-PAC)",
      "author" : [ "C. Craddock", "S. Sikka", "B. Cheung", "R. Khanuja", "S Ghosh" ],
      "venue" : "Front Neuroinform 42",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convolutional neural networks on graphs with fast localized spectral filtering",
      "author" : [ "M. Defferrard", "X. Bresson", "P. Vandergheynst" ],
      "venue" : "NIPS. pp. 3837–3845",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest",
      "author" : [ "R.S. Desikan", "F. Ségonne", "B. Fischl", "B.T. Quinn", "Dickerson", "B.C" ],
      "venue" : "NeuroImage 31(3), 968–980",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism",
      "author" : [ "A. Di Martino", "C.G. Yan", "Q. Li", "E. Denio", "Castellanos", "F.X" ],
      "venue" : "Molecular Psychiatry 19(6), 659–667",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Wavelets on graphs via spectral graph theory",
      "author" : [ "D.K. Hammond", "P. Vandergheynst", "R. Gribonval" ],
      "venue" : "Applied and Computational Harmonic Analysis 30(2)",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Semi-supervised classification with graph convolutional networks",
      "author" : [ "T.N. Kipf", "M. Welling" ],
      "venue" : "arXiv preprint arXiv:1609.02907",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions",
      "author" : [ "B. Kumar", "G. Carneiro", "I Reid" ],
      "venue" : "IEEE CVPR. pp. 5385–5394",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The graph matching problem",
      "author" : [ "L. Livi", "A. Rizzi" ],
      "venue" : "Pattern Analysis and Applications 16(3), 253–283",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Geometric deep learning on graphs and manifolds using mixture model CNNs",
      "author" : [ "F. Monti", "D. Boscaini", "J. Masci", "E. Rodolà", "J. Svoboda", "M.M. Bronstein" ],
      "venue" : "arXiv preprint arXiv:1611.08402",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Learning convolutional neural networks for graphs",
      "author" : [ "M. Niepert", "M. Ahmed", "K. Kutzkov" ],
      "venue" : "ICML",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Network-level analysis of cortical thickness of the epileptic brain",
      "author" : [ "A. Raj", "S.G. Mueller", "K. Young", "K.D. Laxer", "M. Weiner" ],
      "venue" : "NeuroImage 52(4), 1302–1313",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Efficient graphlet kernels for large graph comparison",
      "author" : [ "N. Shervashidze", "S. Vishwanathan", "T. Petri", "K. Mehlhorn", "K.M. Borgwardt" ],
      "venue" : "AISTATS. vol. 5, pp. 488–495",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
      "author" : [ "D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst" ],
      "venue" : "IEEE Signal Processing Magazine 30(3), 83–98",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Graph-based inter-subject pattern analysis of fMRI data",
      "author" : [ "S. Takerkart", "G. Auzias", "B. Thirion", "L. Ralaivola" ],
      "venue" : "PloS one 9(8), e104586",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning to compare image patches via convolutional neural networks",
      "author" : [ "S. Zagoruyko", "N. Komodakis" ],
      "venue" : "IEEE CVPR. pp. 4353–4361",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "The highly challenging problem of inexact graph matching entails the evaluation of how much two graphs share or, conversely, how much they differ [9].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "At the same time, disruptions to this functional network organisation have been associated with neurodevelopmental disorders, such as autism spectrum disorder (ASD) [1].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 8,
      "context" : "Related work: The estimation of (dis)similarity between two graphs has, most commonly, been dealt with using four mainstream approaches [9]: graph kernels, graph embedding, motif counting and graph edit distance.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "Graph kernels have been employed to compare functional brain graphs [15], but often fail to capture global properties as they compare features of smaller subgraphs.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "This method has been widely used to estimate brain graph similarity [1], since it facilitates the application of traditional classification or regression analyses.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "occurrences of significant subgraph patterns, has also been used [13], but is a computationally expensive process.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "Finally, methods based on graph edit distance neatly model both structural and semantic variation within the graphs and are particularly useful in cases of unknown node correspondences [12], but are limited by the fact that they require the definition of the edit costs in advance.",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 15,
      "context" : "Recently, different neural network models have been explored to learn a similarity function that compares images patches [16,8].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "Recently, different neural network models have been explored to learn a similarity function that compares images patches [16,8].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "Recent work has attempted to address this challenge by employing a graph labelling procedure for the construction of a receptive field [11], but requires node features to meet certain criteria dictated by the labelling function (e.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "[14] introduced the concept of signal processing on graphs, through the use of computational harmonic analysis to perform data processing tasks, like filtering.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "Recent work by [3,7] relies on this property to define polynomial filters that are strictly localised and employ a recursive formulation in terms of Chebyshev polynomials that allows fast filtering operations.",
      "startOffset" : 15,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "Recent work by [3,7] relies on this property to define polynomial filters that are strictly localised and employ a recursive formulation in terms of Chebyshev polynomials that allows fast filtering operations.",
      "startOffset" : 15,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "We use a siamese graph convolutional neural network applied to irregular graphs using the polynomial filters formulated in [3].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "We employ a global loss function that, according to [8], is robust to outliers and provides better regularisation.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "As a proof of concept, we demonstrate the model performance on the functional connectivity graphs of 871 subjects from the challenging Autism Brain Imaging Data Exchange (ABIDE) database [5], which contains heterogeneous rs-fMRI data acquired at multiple",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 13,
      "context" : "An essential operator in spectral graph analysis is the normalised graph Laplacian [14], defined as L = IR − D−1/2AD−1/2, where A ∈ RR×R is the adjacency matrix associated with the graph G, D is the diagonal degree matrix and IR is the identity matrix.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "gθ(Λ) [7].",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "To render the filters K-localised in space and reduce their computational complexity they can be approximated by a truncated expansion in terms of Chebyshev polynomials of order K [6].",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "We train the network using the pairwise similarity global loss function proposed in [8] that yields superior results in the problem of learning local image descriptors compared to traditional losses.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "The dataset is provided by the Autism Brain Imaging Data Exchange (ABIDE) initiative [5] and has been preprocessed by the Configurable Pipeline for the Analysis of Connectomes (C-PAC) [2], which involves skull striping, slice timing correction, motion correction, global mean intensity normalisation, nuisance signal regression, band-pass filtering (0.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "The dataset is provided by the Autism Brain Imaging Data Exchange (ABIDE) initiative [5] and has been preprocessed by the Configurable Pipeline for the Analysis of Connectomes (C-PAC) [2], which involves skull striping, slice timing correction, motion correction, global mean intensity normalisation, nuisance signal regression, band-pass filtering (0.",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 3,
      "context" : "We, subsequently, extract the mean time series for a set of regions from the Harvard Oxford (HO) atlas comprising R = 110 cortical and subcortical ROIs [4] and normalise them to zero mean and unit variance.",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 15,
      "context" : "Similarly to the experimental setup used in [16], we train the network on matching and non-matching pairs.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : "Additionally, exploring the use of generalisable GCNs defined in the graph spatial domain [10] would allow to train similarity metrics between graphs of different structures.",
      "startOffset" : 90,
      "endOffset" : 94
    } ],
    "year" : 2017,
    "abstractText" : "Evaluating similarity between graphs is of major importance in several computer vision and pattern recognition problems, where graph representations are often used to model objects or interactions between elements. The choice of a distance or similarity metric is, however, not trivial and can be highly dependent on the application at hand. In this work, we propose a novel metric learning method to evaluate distance between graphs that leverages the power of convolutional neural networks, while exploiting concepts from spectral graph theory to allow these operations on irregular graphs. We demonstrate the potential of our method in the field of connectomics, where neuronal pathways or functional connections between brain regions are commonly modelled as graphs. In this problem, the definition of an appropriate graph similarity function is critical to unveil patterns of disruptions associated with certain brain disorders. Experimental results on the ABIDE dataset show that our method can learn a graph similarity metric tailored for a clinical application, improving the performance of a simple k-nn classifier by 11.9% compared to a traditional distance metric.",
    "creator" : "LaTeX with hyperref package"
  }
}