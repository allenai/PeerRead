{
  "name" : "1512.08553.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Conditional probability generation methods for high reliability effects-based decision making",
    "authors" : [ "Wolfgang Garna", "Panos Louvierisb" ],
    "emails" : [ "w.garn@surrey.ac.uk", "panos.louvieris@brunel.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n08 55\n3v 1\n[ cs\n.A I]\n2 8\nD ec\n2 01\n5\nDecision making is often based on Bayesian networks. The building blocks for Bayesian networks are its conditional probability tables (CPTs). These tables are obtained by parameter estimation methods, or they are elicited from subject matter experts (SME). Some of these knowledge representations are insufficient approximations. Using knowledge fusion of cause and effect observations lead to better predictive decisions. We propose three new methods to generate CPTs, which even work when only soft evidence is provided. The first two are novel ways of mapping conditional expectations to the probability space. The third is a column extraction method, which obtains CPTs from nonlinear functions such as the multinomial logistic regression. Case studies on military effects and burnt forest desertification have demonstrated that so derived CPTs have highly reliable predictive power, including superiority over the CPTs obtained from SMEs. In this context, new quality measures for determining the goodness of a CPT and for comparing CPTs with each other have been introduced. The predictive power and enhanced reliability of decision making based on the novel CPT generation methods presented in this paper have been confirmed and validated within the context of the case studies.\nKeywords: Decision Support Systems, Uncertainty Modelling, Bayesian Networks, Conditional Probability Table, Multinomial Logistic Regression, Reliability"
    }, {
      "heading" : "1. Introduction",
      "text" : "Decision Making and Modeling in high pressure, fast moving, complex environments is often confounded by the inability of the decision model to capture the requisite variety of the situation in a parsimonious manner. Novel techniques for CPT generation which address this capability gap are presented in this paper together with their concomitant case studies. These approaches are considered for application and evaluation herein.\n∗Tel.:+44(0)1483682005;fax:+44(0)1483689511 Email addresses: w.garn@surrey.ac.uk\n(Wolfgang Garn), panos.louvieris@brunel.ac.uk (Panos Louvieris)\nBayesian Networks have long been established in the literature as a useful tool for reasoning under uncertainty. Normally there are two stages involved in creating a Bayesian Network. The first stage is the derivation of a representative graph (topology). Pearl (1988), Mengshoel et al. (2006) and many more have provided fundamental work in this field. The second stage - which this paper focuses on deals with the challenge of obtaining probability distributions enabling the graph to be used for reasoning under uncertainty.\nBefore introducing our novel CPT generation techniques and confirming their predictive power a review of current and past approaches used to derive CPTs are considered.\nPreprint submitted to arXiv December 31, 2015\nMethods for generating conditional probability tables (CPTs) are: noisy-max, noisyor and noisy-and. Pearl (1986) introduced and discussed the noisy-or which can be traced back to Good (1961a) and Good (1961b). These approaches only work for binary variables. Dı́ez (1993) showed a generalized noisy-or gate for multivalued variables. Zagorecki and Druzdzel (2013) examined three test instances and found that noisy-max gates were a good fit for half of the given CPTs. Another generalization of the noisy-or is the recursive noisy-or by Lemmer and Gossink (2004). Vomlel and Tichavský (2014) introduced noisy-thresholds in combination with a novel tensor representation of CPTs. These and other noisy-methods belong to class of noisy functional dependency methods. These operate on a parsimonious set of input data (in general directly proportional to the number of states) and require additional constraints (e.g. order, independence). Techniques employed for CPT generation in order to support decision makers in our problem space must be flexible enough to accommodate a greater degree of variety than that imposed by the Noisy-Max method which imposes a sequence order constraint on the CPT output Li et al. (2011). Such a restriction is prone to produce erroneous CPTs. Furthermore, these noisy-functional methods require the observation of hard evidence and corresponding “single” effect probabilities. However, often only soft evidence is available combined with uncertainty in the outcome. Noisy methods have not be designed for abundant or contradicting information. Therefore, there is a requirement to develop methods that overcome the above mentioned limitations. In this paper we present techniques that solve the above mentioned issues.\nCPTs can be determined using parameter estimation methods. They can be based on large data sets. Jensen (2001) introduce several batch learning methods. On the other hand CPTs may adapt, whilst in\noperation by incrementally improving with each new case. Some of these methods are even capable of creating the Bayesian network structure. Cooper and Dietterich (1992) present the construction of Bayesian Networks from databases. They showed ways to derive the conditional expectancy by means of frequency observations. In the subsequent analysis we will extend this conditional expectancy approach in a natural manner by the usage of regression based CPTs. Popular parameter estimation techniques include the maximum likelihood estimation (MLE), Bayesian estimation and Expectation-Maximization (EM). We will discuss MLE and EM in section 3.2 and 3.3 respectively. These methods can be seen as having an objective subject to constraints. Zhou et al. (2014) describe a constrained optimization approach, which reveals similar conceptual ideas to the ones used in this paper but realizing it with different methods. In general it is tempting to refer to to statistical learning techniques in order to derive CPTs, but ignoring the fact that most of them cannot easily be mapped to the probability space. This paper helps in closing this gap in the body of knowledge. Our new heuristics create CPTs based on conditional expectation. A comparison of such a CPT with the ones elicited by subject matter experts demonstrate their predictive power. The process of eliciting the information with minimum effort has been addressed by Bhattacharjya et al. (2014) by determining the order in which the CPT should be queried from a single expert. Xiang and Jia (2007) investigated ways of finding CPTs by proposing a causal tree model. Finding consensus between experts is a challenge as we observed in our military case study. This issue has also been investigated in López-Cruz et al. (2014), who developed Bayesian network to facilitate this task. Our research will offer another way to offer a combined view that may be used for consensus. Furthermore, our experiments indicate that a small number of cause and effect obser-\nvations can be sufficient to determine a CPT. New methods to compare CPTs with each other are introduced. Moreover, a methodology on the variance and goodness of CPTs is fully developed and presented. A case study on military effects and one on burnt forest desertification demonstrate the stability and accuracy of the derived CPT. In general the technique is especially useful when causes and effects have been measured, leading to a wide range of applications such as healthcare Velikova et al. (2014).\nIn this section existing methods have been reviewed that derive conditional probability tables. Section 2 introduces an operator that joins multiple probability vectors into a single one. This gives the opportunity to use matrix analysis to describe fundamentals of Bayesian networks. In section 3 we will show novel approaches to generate CPTs. New techniques for the goodness of conditional probability tables (CPTs) are proposed in section 4. In section 5 and 6 we consolidate and validate the theory by using case studies. These case studies demonstrate the robustness of CPTs in their predictive power."
    }, {
      "heading" : "2. Bayesian Network",
      "text" : "In this section we will introduce Bayesian network topics which are fundamental for the generation of conditional probability tables. Furthermore we prove the equivalence between two particular Bayesian network structures."
    }, {
      "heading" : "2.1. The Basics",
      "text" : "The fundamental difference of a Bayesian Network to other network structures is the usage of random variables (conditional probabilities) as “weights” for the nodes. The conditional probability P(Z|X) of event Z occurring given that event X has happened first is defined by P(Z = z|X = x) := P(Z = z, X = x)/P(X = x). We will relate this to a simple Bayesian network with two nodes:\neffect (child) Z and cause (parent) X as illustrated in figure 1 (a).\nWe introduce a vector-matrix notation for the probabilities which is necessary since the probability/statistic literature tends to mix single value and matrix notation. The cause probabilities are P(X) (which is a n×1 vector) and the effect probabilities are P(Z) (which is a m × 1 vector). Occasionally we will abbreviate the probability vectors as x and z (rather than reserving these letters to symbolize the state of the random variable). For now we assume that the conditional probability table (CPT) for the Bayesian node Z is known. The CPT P(Z|X) is abbreviated - with the new notation, z|x, which is the m× n matrix. In Pearl (1988) the notation Mz|x is used for the CPT and called conditional probability matrix or link matrix. To make rows and columns explicit:\nP(Z|X) =  P(Z = z1|X = x1) . . . P(Z = z1|X = xn) ... . . . ...\nP(Z = zm|X = x1) . . . P(Z = zm|X = xn)\n .\n(1)\nTypical possible notations for the conditional probability table are: P(Z|X) = Mz|x = C.\nThe joint probability matrix is defined in a similar way ẑ, x := P(Z, X), which is also a m × n matrix. It can be derived from the conditional probability matrix by:\nP(Z, X) = P(Z|X) · (P(X) en), (2)\nwhere the point · represents element by element multiplication and en is the a row vector n ones. A more efficient notation would be ẑ, x := z|x · (xen).\nThe effect probabilities in matrix notation are computed by:\nP(Z) = P(Z|X)P(X), (3)\nwhich follows from the definition of the conditional probability and the marginalisation of the joint probability table. Again this could be more efficient expressed as z = z|x x.\nThe new notation provides a simplified and more efficient representation of the probability matrix formulation and proves particularly useful when it comes to representing complex Bayesian Network computations. Here, we would like to draw the readers attention to Vomlel and Tichavský (2014) representation of CPTs as tensors."
    }, {
      "heading" : "2.2. Cause Computation",
      "text" : "Given the effects and conditional probabilities it is possible to determine the causes using Bayes Rules:\nP(Z = z|X = x)P(X = x) = P(X = x|Z = z)P(Z = z). (4)\nFrom Bayes rule we obtain P(X = x|Z = z) = P(X = x, Z = z)/P(Z = z), which reverses the effect computation and leads to a conditional probability matrix. Let us put this into matrix formulation:\nx = x̂, z ÷ (emz ′) ⇔ P(X) = P(X, Z) ÷ (emP(Z) ′), (5)\nwhere em is the vector consisting out of m ones and ÷ is the element-wise division operation. Thus we have a method to determine the cause probabilities given the effects. This allows us to deduce the causes given the effects. Note that z = Cx ⇔ (CC′)−1C′z = x cannot be used because it leaves the probability space."
    }, {
      "heading" : "2.3. Converging Network",
      "text" : "Definition 2.1. A Bayesian network B is a simple directed acyclic graph, which consists out of a sequence of nodes N with an associated sequence of weights W and a sequence of arcs A.\nThe weights in a discrete Bayesian network are conditional probability tables or probability vectors. Note that a probability vector can be interpreted as a special case of a conditional probability table. The literature Russell and Norvig (2010) distinguish between three types of Bayesian Networks: discrete, continuous and hybrid Bayesian Networks. In this document we will deal only with discrete BN and will simply call them BN.\nDefinition 2.2. Let B1 be a Bayesian Network (BN) illustrated in figure 1(b) and defined by :\nN := 〈Z, X1, X2, . . . , Xk〉, W := 〈P(Z|X1, . . . , Xk), P(X1), P(X2), . . . , P(Xk)〉, A := 〈(X1, Z), (X2, Z), . . . , (Xk, Z)〉 and k ≥ 1.\n(6)\nAny Bayesian networks of the above structure will be called a converging Bayesian network.\nA representation of multiple nodes as a single node is known as a cluster node Pearl (1988).\nDefinition 2.3. A BN B2 (shown in figure\n1(a)) is defined by :\nN := 〈Z, X〉,W := 〈P(Z|X), P(X)〉, A := 〈(X, Z)〉 (7)\nand will be referred to as simple Bayesian network.\nAny Bayesian Networks of the structure shown in figure 1(b) can be transformed to a Bayesian network having the structure displayed in 1(a). That means the parent X in figure 1(a) has n = n1 + n2 + · · · + nk states.\nTheorem 2.1. There is a bijective mapping between a converging BN B1 and a simple BN B2, such that the effect probabilities are equal.\nProof. If n = 1 the mapping is trivial. If n = 2 then the transpose of the outer product Y = (x1x′2)\n′ is a n2 × n1 matrix. Let Y: designate the transformation of the Y matrix into the y column vector by concatenating all column vectors of Y . y represents P(X) for B2. Conversely P(X) can be split into x1 and x2, if the number of states are known by considering the matrix Y . The vectors are obtained by realising that the outer product constitutes a joint probability table. Applying Bayes rule (eq. 4) leads to the reverse CPT.\nIf n ∈ N, then P(X) can be derived by algorithm 1. The other direction requires the knowledge of the number of states and that we can reverse the CPT multiplication. We will elaborate on this, let C = z|x1x2 . . . xn = P(Z|X1, . . . , Xn). The effect probabilities can be computed by z = Cx. Note that x cannot be determined by x = (C′C)−1C′z even when C′C is nonsingular (det (C′C) , 0), because this would violate probability axioms. Hence, we have to determine the reverse of C namely x|z by using Bayes’ rule (see equation 4). Knowledge of the number of states allows us to create a joint matrix. Marginalisation of the joint matrix leads to the probability vectors x1, x2, . . . , xn. Thus we have shown the computation of the probability product.\nHence, the computed effect probabilities of B1 and B2 are equal."
    }, {
      "heading" : "2.4. Combine Operator",
      "text" : "Important in the above proof was the usage of algorithm 1, which combines multiple independent probability vectors into a single one; we define this as the combine operator : [0, 1]n1×· · ·×[0, 1]nk → [0, 1]n1×···×nk . Note\nAlgorithm 1 Multiple parents to single parent. (Operator ) Require: x1, . . . , xn . . . parallel causes (column vectors) Ensure: x . . . product cause node from paral-\nlel causes\n1: x := x1 assign first cause 2: for k = 2 to n (for all remaining parallel\ncauses) do 3: Y := (xx′k)\n′ compute outer product and transpose\n4: x := Y: transform matrix into one single column vector by concatenating all columns 5: end for\nthat algorithm 1 has a repeated application of the outer product and a matrix to vector transformation. Just like with any other operator based on multiplication it is only possible in special cases to reverse the combine operation in a unique way."
    }, {
      "heading" : "2.5. Evidence",
      "text" : "Evidence may be given as a sequence of probability ratios. For instance assume that the odds are 7 to 5, which is transformed into the probability vector (.58 .42). This type of evidence will be called soft evidence. It is also common in the BN literature to express evidence by stating that a node has assumed a certain state, which we will call hard evidence. This is equivalent to having a 100% probability in one state. We will call such a node an evidence node. A converging network with all parents having hard evidence\nleads to the selection of exactly one column from the CPT.\nProposition 2.2. The effect probabilities are identical to one unique column of the CPT if all parents are evidence nodes (i.e. each parent has exactly one 100% state) in a converging subnet.\nThis proposition is significant for the relationship between multinomial logistic regression and CPTs, which will be discussed later.\nOf course a combination of multiple evidence nodes is also possible which leads to selection of certain columns and their summation. This linear combination may be used to compute effect probabilities in a more efficient way."
    }, {
      "heading" : "2.6. Joint Probability using the Combine Operator",
      "text" : "In general the joint probability of a converging network as shown in figure 1 (b) is computed by:\nP(X1, X2, . . . , Xn, Z) = P(Z|X1, . . .Xn) n∏\nk=1\nP(Xk).\n(8)\nThis has to be done for each state configuration. Of course we could apply the combine operator (”vectorising” outer product), which gives us the matrix representation:\n̂z, x1, x2, . . . , xn = z|x1x2 . . . xn.(e n k=1 xk, )\n(9)\nwhere e is the ones vector needed for the point-wise multiplication with the combinational product of all probability vectors."
    }, {
      "heading" : "2.7. Summary",
      "text" : "In this section we have proven the computational identity of converging and simple BN. We have proposed a novel probability combination operator , which transforms\nprobabilities of multiple parents into a single probability vector. This new operator integrates well with matrix notation. Furthermore we established an important new evidence proposition, which will be used to extract CPTs out of linear and nonlinear functions."
    }, {
      "heading" : "3. CPT Generation",
      "text" : ""
    }, {
      "heading" : "3.1. Introduction",
      "text" : "In the beginning of the paper techniques to generate CPTs were outlined and their deficiencies were discussed. Here, the MLE and EM method will be revisited and explanations for their failings will be given. These techniques are more commonly placed in the area of parameter learning methods (see Koller and Friedman (2009) for details). Then this section introduces new methods that can generate CPTs. They are based on the assumption that causes and effects can be observed. Even if they cannot be observed it is still better and easier for a subject matter expert to provide the causes and effects and use the techniques presented herein. Measures for goodness of these models (section 4) will show that little information is required to generate CPTs with the new methods."
    }, {
      "heading" : "3.2. Maximum Likelihood Estimation",
      "text" : "A classic approach to obtain CPTs is to use the Maximum Likelihood Estimation (MLE):\nθ̂ = arg max θ L(Mθ|D), (10)\nwhere θ are the parameters, Mθ the models, D the given data and L(Mθ |D) = ∏ d∈D P(d|M). The data for a CPT is generally assumed to state the cause and effect state (as hard evidence). Hence relative frequencies can be derived, for instance:\nP(zi|x j) = N(zi, x j)\nN(x j) . (11)\nN(x j) represents the number of occurrences of case x j in the data, and N(zi, x j) the simultaneous occurrence of state zi and x j. A few problems can be observed. (1) No occurrences of case x j are recorded; (2) N(zi, x j) is zero (i.e. impossibility of P(zi|x j)), or (3) the case information was provided in a fuzzy way (soft evidence, e.g. certainty of a case x j is d(x j) = .7). The fuzzy specification can be transformed into number measures by rounding ⌊d(x j)⌉, reducing accuracy further. The main issue observed in the case studies was the incompleteness of data, which meant that the MLE could not be used to obtain meaningful CPTs."
    }, {
      "heading" : "3.3. Expectation Maximization",
      "text" : "The MLE showed issues arising with incomplete data. These can be overcome using the expectation maximization algorithm. The aim is to find parameters θ such that the likelihood of the model is maximized. Initially the model and its parameters θt, t = 0 have to be assumed. This is followed by the “Expectation” step. Here the responsibilities are computed, which are the expected counts for the CPT:\nE(N(zi, x j)|D) = ∑\nd∈D\nP(zi, x j|d, θt). (12)\nThe maximization step determines a new estimate for θt, t ← t + 1:\nθt = E(N(zi, x j)|D)∑m\nj=1 E(N(z j, x j|D)) (13)\nThe expectation and maximization step are continued until the algorithm has converged, i.e.\n| ln P(D|θt+1) − ln P(D|θt)| ≤ ǫ. (14)\nOne of the main issues with this procedure is the choice of the initial parameters, which will influence the local (or global) maximum achieved. One should be aware that this is a local search heuristic. That means embedding\nit into multi-start procedures or meta heuristics can improve the solution quality. However, this adds to the run-time of this computationally expensive method. Another disadvantage - when using this method in its “classic” form - is that it uses occurrences (hard evidence) rather than accommodating soft evidence. The advantage of the EM method is the iterative usage of conditional expectation.\nWe will now begin to introduce new methods that use the benefits of MLE and EM. These are derived from the familiar conditional expectation (regression), but have to stay within the probability space."
    }, {
      "heading" : "3.4. Conditional Mean Basis",
      "text" : "Assume that causes x and effects z were k times observed. We will show how a CPT basis can be estimated from these observations. This is the basis for probability boundary limitation method and the probability potential surge method introduced in the subsequent sections to derive the real CPT, i.e. a CPT basis is a matrix which may still violate the probability constraints explained below. The conditional mean function E(z|x) is also called regression of z on x. An estimator of E(z|x) := ∑ z z f (z|x) will be used to obtain the CPT. where f (.) is the probability density function (continuous case) or probability distribution (discrete case). f (.) has to follow Kolmogorov’s axioms. General discussions of multiple regressions can be found in Hastie et al. (2001) and Greene (2000). The application of the conditional mean to approximate the conditional probability table was used in Cooper and Dietterich (1992) in relation to Bayesian Networks. Roughly speaking they used frequencies to obtain the probabilities.\nThe new approach will need the following preparation. Given are k observations of the effects Z = (z1, . . . , zk)′ and causes X = (x1, . . . , xk)′. That means Z can be represented as a k × m matrix and X as a k × n matrix. The objective is to determine the CPT basis B := (bi j) := z|x′, such that the squared\nsum S (B) := (Z − XB)′(Z − XB) obtains a minimum:\nB∗ = min B∈B {tr((Z − XB)′(Z − XB))} , (15)\nwhere B transposed is the m × n matrix with elements in the interval [0, 1]:\nbi j ∈ [0, 1] ∀i, j. (16)\nIn order to satisfy Kolmogorov’s axioms another constraint on B must be fulfilled. The row sum of B must add up to one:\nm∑\nj=1\nbi j ! = 1 ∀i ∈ {1, 2, . . . ,m} . (17)\nB is the set of all matrices fulfilling the above mentioned two constraints. Note that the symbol ! = denotes “must equal”. Other objective functions to determine B∗ may be appropriate depending on the nature of the underlying problem. However, we have used the least square method because of its characteristics and popularity. We will limit ourselves to deriving a CPT basis using matrix calculus. We obtain the first derivative of S (B) by:\n∂S (B) ∂B = −2X′(Z − XB). (18)\nSetting the first derivative to zero will give us the optimum:\n−2X′(Z − XB) = 2X′Z − 2X′XB = 0, (19)\nunder the assumption that X has full column rank, which guarantees that X′X is positive definite. Thus, an optimal CPT basis B∗ is:\nk×m︷︸︸︷ B∗ = n×n︷ ︸︸ ︷ (X′X)−1 n×k︷︸︸︷ X′ k×m︷︸︸︷ Z . (20)\nAn alternative way of obtaining the above result is shown in the footnote 1. Note that it\n1Equation (20) could have been derived in a different way by looking for a B∗ which minimises the error matrix E in Z = XB∗ + E. Thus E must be orthogo-\ncannot be guaranteed that the elements in B∗ fulfill the two constraints (16) and (17), because of the matrix inversion and multiplication. The case study given in section 6 supports this statement.\nIn order to enforce a CPT basis to become a proper CPT we propose the probability boundary limitation and probability potential surge methods."
    }, {
      "heading" : "3.5. Probability boundary limitation method",
      "text" : "If the value that is supposed to represent the probability is out of range it will be set to its closest limit. That means if xi < 0 then xi is set to zero and if xi > 1 it is assigned the value one, i.e.\nxi 1\n0 := min {max {xi, 0} , 1} ∀i. (21)\nAfterwards the column vector x̃ = ( xi 1\n0\n) is\nnormalised by x = x̃∑ i x̃i . An exception which could happen is that all approximated “probabilities” are smaller than zero, in this case one could make the approximation by shifting. A crude solution, if all values are zero or one, is obtained by assign a uniform probability to x."
    }, {
      "heading" : "3.6. Probability potential surge method",
      "text" : "The x̃i values of a column undergo a translation into the positive range by adding the negative of the lowest x̃i value. For instance x̃ = (−.2 − .3 .4)′ then the lowest value is m = −.3 and the surge in potential leads to x̄ = (.1 .0 .7)′, which must be normalised by α−1 := x̄′e3 = .1 + 0 + .7 = .8. Below is the formal expression of the probability potential surge method:\n∃m ∈ x : m < 0 α(x̃ − en min x̃︸ ︷︷ ︸ x̄ ), (22)\nwhere α is the normalisation coefficient (α−1 = ∑ i x̄i = x̄ ′en).\nnal to XB. That means: XB∗⊥E ⇔ (XB∗)′E = 0 ⇔ (XB∗)′(Z − XB∗) = 0 ⇔ B∗′(X′Z − X′XB∗) = 0 ⇒ B∗ = (X′X)−1X′Z.\nIn section 6 we will apply these heuristics to case studies.\nBerry (1993) noted that dichotomous outcomes violate the assumption of linear relationship of the variables in general. However, a logarithmic transformation preserves the non-linearity as shown in Berry and Feldman (1985). This motivates that we will introduce the logistic regression and examine its applicability."
    }, {
      "heading" : "3.7. Probability base vector extraction method",
      "text" : "In this section we show a new method to extract a CPT from a logistic regression. However, the method can be applied to any function operating on [0, 1].\nThe multinominal distribution is given by:\nP(Z = zk) = eyk (1 + ∑n k=1 e yk )−1, ∀k < n;\nP(Z = zn) = (1 + ∑n k=1 e yn)−1,\n(23)\nwhere yk = bk0 + ∑n j=1 bk jx j = bk[1; x ′]. The logistic model is a special case of the generalised linear model, explained in McCullagh and Nelder (1989). The effect described by its probabilities P(Z) (dependent variable) is computed given cause probabilities (independent variables). The likelihood is l(b) := ∏n k=1 P(Z = zk|X = x), where b describes the regression parameters. We will abbreviate P(Z = zk|X = x) by pk(x, b) to emphasise the dependence on b. This is equivalent to the logarithmic likelihood:\nL(b) := log l(b) = n∑\nk=1\nlog pk(x, b). (24)\nLet us assume that pk(x, b) = p(x, b)ck (1 − p(x, b))1−ck ∀k, where ck describes the class of the kth observation and its case weight. Now we can write (24) as\nL(b) = n∑\nk=1\nck log pk(x, b) + (1 − ck) log(1 − pk(x, b)).\n(25)\nTo find the maximum probability of (25) we take the first derivative and require that it must be zero:\n∂L(b) ∂b =\nn∑\nk=1\nxk(yk − p(xk, b)) ! = 0. (26)\nUsually equation (26) is solved using the Newton-Raphson algorithm in order to obtain b. This gives us a convenient way of computing the effect probabilities via equation 23. So far this has been “classic” multivariate data analysis which is explained in Hair et al. (1998), Hastie et al. (2001) and Field (2009).\nNext we will introduce the new probability base vector extraction method and show that logistic regression can be used to determine the conditional probability table. Recall that in section 2 we have shown that any column of a CPT matrix can be obtained by giving hard evidence (proposition 2.2). That means a canonical cause matrix consisting entirely out of hard evidence determines the CPT. Thus the elements of the CPT are determined by:\nP(Z = zk|X = ėi) = e bk0+bki\n1+ebk0 ∑n j=1 e bk j , ∀k < m, i ≤ n;\nP(Z = zm|X = ėi) = 11+ebn0 ∑nj=1 ebn j , ∀i ≤ n,\n(27)\nwhere ėi is the hard evidence vector, i.e. element i is one and the other elements in the vector are zero. This means that yk obtains a very simple form yk = bk0 + bki, which is reflected in equation (27). Rijmen (2008) has discussed this method in more detail. This method will be applied to the “burnt forest” case study.\nWhat are the benefits of using a logistic regression CPT rather than the logistic regression result directly? In a standard Bayesian Network we can use CPTs which are derived from logistic regression but not the logit function itself. Furthermore the computation of the effect probabilities using the CPT is more efficient than using the logit function.\nThe here proposed extension - the probability base vector extraction method - of Logistic\nRegression provides a “bridge” to Bayesian Networks."
    }, {
      "heading" : "3.8. Summary and Concluding Remarks",
      "text" : "We have introduced several novel methods to generate CPTs. These methods have extended existing popular techniques. The ease of applying the introduced extensions to the existing techniques should help to make the new methods valuable assets. Two of the CPT newly introduced generation methods were based on modifying the conditional mean through potential surge and boundary limitation heuristics. The third - column extraction method - demonstrated how to create a CPT from a non-linear function (logistic regression). Here our proposition 2.2 played an important role to extract an approximate CPT.\nThe case studies in later sections will be used to evaluate the goodness of these models. A new set of measures is introduced in the following section, which we argue are simple, more intuitive and give the possibility of comparing the quality of CPT approximation models."
    }, {
      "heading" : "4. Goodness of Models",
      "text" : "There are many classical assessment methods of the goodness for regression (e.g. R2) and logistic regression (e.g. R-statistic). Often the Kullback-Leibler (KL) divergence measure is found in the literature (e.g. Zhou et al. (2014); Zagorecki and Druzdzel (2013)) for CPT comparisons, which was originally developed to measure the relative entropy in information. It measures the divergence of C̃ from C by using ∑ i xi ∑ j ci j ln ci j c̃i j\n, where C = P(Z|X) is the correct CPT and C̃ the approximated CPT (here x is P(X)). However, the logarithm is difficult to explain in the context of conditional probabilities. Some authors have used the Euclidean distance instead, i.e. the distance measure between CPTs is: ∑ i xi ∑ j(ci j − c̃i j)\n2. This section introduce measures that determine the goodness of effect estimates and approximated CPTs given training and test data."
    }, {
      "heading" : "4.1. Effect comparison",
      "text" : "There are four measures we are interested in: diagnostic error (d), total average shift error (s̄), mean state error (s j), and absolute effect observation error (δi).2 A good illustration of these errors can be found in figure 3. In the previous sections we demonstrated methods to determine an approximate CPT C̃ given causes X (k × n matrix) and effects Z (k × m matrix) training data. The quality of the CPT is evaluated by using test data Xt and Zt. Of course this assumes that we trust the test data to be meaningful. The above four measures are the result of comparing Zt and Z̃t := XtC with each other.\nDefinition 4.1. The absolute effect observation error compares each effect observation with the corresponding approximation. Let zi ∈ Zt be the ith effect observation. That means zi := (zi1, . . . , zim) is a probability vector consisting out of m states. The same applies to the approximated effect z̃i ∈ Z̃t. Thus we can describe the absolute effect error δi for the ith observation by:\nδi := 1 2\nm∑\nj=1\n|zi j − z̃i j|. (28)\nThis measure adds up all deviations and thus gives us the total shift error of an approximated effect. We observe that 0 ! ≤ δi ! ≤ 1. If the probability vector has a highly likely state then the error can be more significant. An effect error can be interpreted as the likeliness of determining the wrong state. 1 − δi is the goodness of the effect approximation.\nOf course the average absolute error is also of interest:\nδ̄ := 1 k\nk∑\ni=1\nδi. (29)\nDefinition 4.2. Comparing each state j across all k samples defines the mean state error.\n2If both components are estimates then “deviation” is a better term than “error”.\nMore precisely:\ns j := 1 k\nk∑\ni=1\n|zi j − z̃i j|. (30)\nThis is the likelihood of a certain state being wrong. 1− s j is mean goodness of a state.\nNow we are in the position to summarise the goodness of all states. We determine the average of the states being wrong by:\ns̄ := 1 m\nm∑\nj=1\ns j. (31)\nThe average goodness of all states is thus 1−s̄. This is a “dangerous” measure, because it depends on the number of states. The more states an effect has the more a wrong impression of goodness is perceived.\nThe second possible interpretation of s̄ focuses on the shifting of probabilities between the states.\nDefinition 4.3. Thus, s̄ will be also called total average shift error.\nDefinition 4.4. The diagnostic goodness 3 g counts the number of agreeing effect maximum states and sets them in proportion to the total number of observations:\ng := 1 k\nk∑\ni=1\n[ argmax j∈{1,...m} zi j ? = argmax j∈{1,...m} z̃i j ] . (32)\nNote that the symbol ? = denotes the boolean operator (query equality) that gives the value one if the expression on the left is equal to the expression on the right side and otherwise zero.\nIn the above equation we made use of the Iverson brackets. 4\n3The diagnostic goodness will be also called diagnostic power.\n4Iverson (1962) introduced these brackets for true-\nor-false statements [S ] :=  1 if S is true; 0 otherwise. . For instance the Kronecker delta is defined by δi j := [i ? = j].\nExample 4.1. Assume that z1 = (.3 .7), z2 = (.4 .6), z3 = (.9 .1) and z̃1 = (.2 .8), z̃2 = (.5 .5), z̃3 = (.7 .3). Thus the diagnostic goodness is determined by: g := 13([2 ? = 2] + [2 ? = {1, 2}] + [2 ? = 1]) = 13 (1 + 0 + 0) = 1 3 . Hence, the diagnostic error is 23 .\nDefinition 4.5. The diagnostic error d is:\nd := 1 − g (33)"
    }, {
      "heading" : "4.2. CPT comparison",
      "text" : "The KL and Euclidean method can be used to compare CPTs. Here, we will introduce a new method. We assume that one CPT C = (ci j) = P(Z|X) is “correct” and the other one is an approximation C̃ = (c̃i j). It is tempting to introduce a relative error measure. However, it can be easily shown that these are inappropriate for probability measures. In the previous section we introduced the absolute effect observation error. This measure is based on the observation that probabilities are “shifted” between states. We apply this principle to the CPT to obtain the\nDefinition 4.6. CPT shift error\nδ̄ := 1 n\nn∑\nj=1\n( 1 2\nm∑\ni=1\n∥∥∥ci j − c̃i j ∥∥∥) = 1\n2n\nn∑\nj=1\nm∑\ni=1\n∥∥∥ci j − c̃i j ∥∥∥ .\n(34)\nThis was previously known as the average of the absolute effect observation error. As before completely distinct CPTs are identified.\nExample 4.2 (Distinct CPTs). Given are two distinct CPTS C and C̃.\nC =  1 0 0 0 1 0 0 0 1 0 0 0  , C̃ =  0 0 1 1 0 0 0 1 0 0 0 0  (35)\nThe CPT shift error is δ̄ = 100%."
    }, {
      "heading" : "5. Case Study (1) - Burnt forest desertification risk",
      "text" : "A Bayesian network to determine the risk of burnt forest desertification was introduced in Stassopoulou et al. (1998). The main purpose of this paper was to obtain the correspondence between Bayesian and Neural Networks. As well as discussing this link a case study about forest desertification was given.\nOur objective in this section is to make use of the given Bayesian Network and the corresponding test data. We generate from this data several CPTs applying the probability boundary limitation method, probability potential surge method and the probability base vector extraction method. The goodness will be tested in respect of the CPTs predictive power and its total average shift error."
    }, {
      "heading" : "5.1. Bayesian Network and Data",
      "text" : "Regeneration of Mediterranean forests is usually achieved between two to five years. The potential of this regeneration depends on the soil depth, ground aspects and animals in the area. On the other hand there is risk of erosion, which is influenced by the slope, the rock type and again the soil depth. In figure 2 we show the complete GIS Bayesian Network. We focus on the lower part of this network described by the following nodes: Risk of Erosion E, Regeneration Potential R and\nRisk of Desertification D. E and R have three states each, whilst D has five states. Thus the resulting CPT of D will be 5 × 9 matrix. The paper Stassopoulou et al. (1998) details two tables of data, which we will use as input to compute the conditional probability table. For the convenience of the reader we reproduced the tables and added them to the electronic companion. The first table describes the training data (39 rows) and the second table gives data (14 rows) for comparative purposes. Each table contains four main columns: the first one describes the site and the other three represent probability vectors for each Bayesian node. We have examined the data for any obvious errors. We observed that each probability vector adds up to one as it should. We have also checked whether observations repeat themselves. Inspection of the data shows that some of the combined E and R observations repeat themselves up to five times, which results in 19 distinct combined E and R observations only. The test data Et,Rt and Zt for comparative purposes consists out of 14 tuples of which are 11 distinct."
    }, {
      "heading" : "5.2. Boundary limitation and potential surge regressions",
      "text" : "We will now determine the CPT and use it to check its goodness. The background of how to derive the CPT has been explained in previous sections. First we combine parent observations (X := E R, see algorithm 1). Next we compute the CPT basis by B∗ := b∗i, j := ((X\n′X)−1X′D)′. Finally we transform the CPT basis into a CPT B that guarantees that each element is a probability and that the column sum adds up to one. We use boundary limitations and potential surge methods to achieve the fulfilment of these constraints. The heuristics operate on column 1,2, 7 and 8 and lead to a maximum change of 13.4% for b∗2,2. The boundary limitation method leads to a CPT, that has a lower average shift error 1.17% than the potential surge method on the test data. This CPT is shown\nin table 1. The average shift error between n parent observations (Et,Rt) multiplied with the CPT B and the test data Dt is computed by:\nǫ = 1\nmn\n∑\ni, j\n|B(Et Rt) − Dt|. (36)\nThe CPTs have the same predictive power on the test data, i.e. they select the maximum probability state in 13 out of 14 cases (1314 = 92.9%) correctly. The highest average shift error of 1.31 % occurs when using the distinct data set and the shift-normalisation heuristic. As we can see the difference between the lowest and highest total average shift error of 0.14% is insignificant. Thus we have demonstrated with this case study that boundary limitation and potential surge methods generate CPTs which posses good (92.9%) predictive power that contain very low (less than 1.31%) total average shift errors."
    }, {
      "heading" : "5.3. Logistic Regression",
      "text" : "The multinomial logistic regression gives an average shift error between approximated and observed effect test data of 2.1%. This was derived from the distinct data set. As a first step we combined the nodes E and R and determined the logistic regression parameters. Using these it would be possible to compute any effect probabilities. However, in this paper the focus is on the generation of a CPT. Thus it is necessary to determine the CPT based on the logistic regression parameters. This is achieved by applying a 9 × 9 canonical matrix E as input to the equation system 27. This results in the CPT C̃ shown\nin table 2. We use this CPT in order to determine the effect probabilities for the test data by Z̃ = C̃(Et Rt). This must be compared with the effects from the test data Zt. In figure 3 we show several comparisons. We have placed each test data effect next to an approximated effect. Thus we have compared 14 effects and their approximation with each other. The absolute errors of each pair are given. We notice that effect observation two and three lead to approximations with deviations of 22.5% and 11.6% respectively. Such deviations indicate irregularity and suggest a verification of the test data. Hence, this indicates that our methods can be used to identify data irregularities. On average absolute effect observation errors are 5.3%, which appears to be quite high. That means if we require that all states are correctly identified we will be wrong in one out of 20 cases roughly spoken. On the other hand if we are only interested in identifying one state correctly the average state error is 2.1%. Notice that the first state (no/slight) is approximated best with 0.6%. The worst state to approximate is the “high” state, which has a mean state error of 2.9%. 13 out of 14 sample states were chosen correctly, i.e. 92.9% diagnostic goodness. If we compare the potential surge method and the logistic regression total average shift errors with each other we see that 1.3% − 2.1% = −0.8% the logistic regression is performing worse.\nThe comparison of the effect training data and the approximations, show that the total average shift error for the test data is 1.4%. In case we use the distinct training data we get\nthe following error measures. The diagnostic error is 7.1%. The absolute effect observation error is on average 5.3%. The total average shift error is 2.1%.\nUsing the potential surge method gave a diagnostic value of 92.9% and a total average shift error of 1.3%. The logistic regression had the same diagnostic value but a slightly higher total average shift error of 1.4%. Thus we can assume that both CPT approximations are fairly reliable."
    }, {
      "heading" : "6. Case Study (2) - Military Effects",
      "text" : "The case study presented here contains military effects, which constitute critical success factors (CSF) for a vignette. Typical military effects are find, destroy, secure and many more (see Louvieris et al. (2007) for details). These effects are assessed by means of a Bayesian Network."
    }, {
      "heading" : "6.1. Bayesian Network and Data",
      "text" : "The introduced effect models depend on the status of the situation and the relative morale. In this paper we will not discuss the challenges of obtaining evidence for these measures nor the derivation of this Bayesian Network - respective information can be found in Louvieris et al. (2005). We will focus our discussion on converging Bayesian Networks. The effect CSF node E has two parents: situation S and relative morale M.\nEach node has three states. The objective is to derive the CPT for E. Two lots of four subject matter experts (SME) were asked to provide the CPT directly, i.e. each group had to do a consensus labelling. Additionally they had to provide probability vectors for S , M and E for the effects observed in the vignettes, whenever they noticed changes. The consensus labelling produced a table of 83 rows and 9 columns (see table in electronic companion). Deleting identical entries gives 54 distinct sample points (rows).\nSubject matter experts provided the CPT show in table 3. We noted that certain prob-\nability entries varied from one SME to another by up to 25% before agreement. A direct comparison of the SME CPT and the Regression CPT with probability potential surge method shows significant differences. This indicates that SMEs have substantially different interpretations of factors."
    }, {
      "heading" : "6.2. Probability boundary limitation and potential surge method",
      "text" : "Using the probability boundary limitation method leads to an absolute column sum error of 88.4% and an average shift error of 29.5%. When the regression CPT was applied to the observations the correct effect state was chosen in 38 out of 54 cases (70.4%). Here “correct” is based on the assumption that the CPT provided by the SME is without any fault. If we assume the effect observations are the basis for comparison, we obtain that the correct effect state with B is chosen 50 times (i.e. diagnostic goodness of 92.6%), whilst the usage of the SME CPT lead to 40 correct state selections (i.e. diagnostic power of 74.0%).\nFor the command and control agents the correct choice of the state is important so that agents can take the appropriate actions. The above results are encouraging because given a similar scenario agents will make right decisions in the majority of cases (this case study suggest 93%). On the other hand without prior training data (i.e. SME CPT only) the right choice would have been done in 74% of the observation points. Thus the generated CPT has a 19% better diagnostic value.\nWhen we apply the probability potential surge method we can improve the correct state choice up to 98.2% (only one state was chosen incorrectly) under the assumption that effect states were correct. Table 4 shows the regression CPT with the potential surge method.\nThus we have shown in this case study that the approximated CPT has a high diagnostic value (98.2%) of correct decisions, whilst the SME CPT managed 74% correct decision - assuming correct effect probabilities.\nThis demonstrated that a generated CPT from the cause and effect observations will have a better diagnostic power (by 24.2%) than the SME CPT."
    }, {
      "heading" : "7. Conclusion",
      "text" : "A review on the existing CPT generation methods revealed the need to apply multivariate data analysis techniques to Bayesian Networks rather than noisy-functional methods. Thus, the main contributions of this paper are new CPT generation methods in particular when the input from parent nodes is observed as soft evidence. The first two methods - the boundary limitation and potential surge method - map conditional expectations to the probability space. The third - probability base vector extraction - method is manifested through its application to the multinomial logistic regression. Introducing Bayesian Networks using matrix notation enabled us to propose tools such as a combine operator and\nthe necessary “column extraction” proposition. The operator combines parent probability vectors of a converging Bayesian Network into a single probability vector. This allows the efficient computation of effect probabilities and assists in the generation of CPTs. The “column extraction” proposition proves particular beneficial when using non-linear methods to obtain a representative CPT.\nThe CPT generation methods proposed in this paper were applied to two case studies and demonstrated high predictive power. The proposed measures of goodness give clear and intuitive statements about the quality of the approximations and are found to be the best measurement methods in this context. The military effects case study showed that generic subject matter expert CPTs do not achieve the quality of the generated CPTs in respect to the effect probability computations. The burnt forest case study showed the goodness of the developed multivariate data CPT generation techniques. A comparison of the methods in respect to the case studies suggest similar goodness of the techniques. Moreover, the potential surge and boundary limitation techniques demonstrated to be better than probability base vector extraction method applied to multinomial logistic regression.\nThere are several avenues for future research. The first is to gather further evidence of the effectiveness of the proposed methods by applying them to the classic Bayesian Network test instances5. However, this would require the generation of hard and soft evidence for the nodes. Furthermore, this should also be accompanied by a theoretical study. The\n5http://www.bnlearn.com/bnrepository/\nsecond avenue of research should be an analysis of the here introduced CPT comparison measure versus the K-L and euclidean measure. As a third avenue it should be investigated of whether the converging Bayesian Networks are superior to the Extended Belief Rule-Based systems Calzada et al. (2015).\nIn summary the here proposed CPT generation methods will lead to more reliable decision making within Bayesian Networks when it comes to the prediction of effects."
    }, {
      "heading" : "Acknowledgement",
      "text" : "The work reported in this paper was sponsored by the United Kingdom MOD Data and Information Fusion Defence Technology Centres (DIF DTC) Research Programme.\nThe “burnt forest paper” Stassopoulou et al. (1998) proofed to be a useful resource. We would like to thank Maria Petrou for providing feedback."
    } ],
    "references" : [ {
      "title" : "Understanding Regression Assumptions",
      "author" : [ "W.D. Berry" ],
      "venue" : "Sage Publications,",
      "citeRegEx" : "Berry,? \\Q1993\\E",
      "shortCiteRegEx" : "Berry",
      "year" : 1993
    }, {
      "title" : "Multiple Regression in Practice",
      "author" : [ "W.D. Berry", "S. Feldman" ],
      "venue" : null,
      "citeRegEx" : "Berry and Feldman,? \\Q1985\\E",
      "shortCiteRegEx" : "Berry and Feldman",
      "year" : 1985
    }, {
      "title" : "A myopic approach to ordering nodes for parameter elicitation in bayesian belief networks. Knowledge and Data Engineering",
      "author" : [ "D. Bhattacharjya", "L. Deleris", "B. Ray" ],
      "venue" : "IEEE Transactions on 26,",
      "citeRegEx" : "Bhattacharjya et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bhattacharjya et al\\.",
      "year" : 2014
    }, {
      "title" : "A new dynamic rule activation method for extended belief rule-based systems. Knowledge and Data Engineering",
      "author" : [ "A. Calzada", "J. Liu", "H. Wang", "A. Kashyap" ],
      "venue" : "IEEE Transactions on",
      "citeRegEx" : "Calzada et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Calzada et al\\.",
      "year" : 2015
    }, {
      "title" : "A Bayesian Method for the Induction of Probabilistic Networks from Data",
      "author" : [ "G.F. Cooper", "T. Dietterich" ],
      "venue" : "in: Machine Learning,",
      "citeRegEx" : "Cooper and Dietterich,? \\Q1992\\E",
      "shortCiteRegEx" : "Cooper and Dietterich",
      "year" : 1992
    }, {
      "title" : "Parameter adjustement in Bayes networks. The generalized noisy OR–gate",
      "author" : [ "F.J. Dı́ez" ],
      "venue" : "in: Proceedings of the 9th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Dı́ez,? \\Q1993\\E",
      "shortCiteRegEx" : "Dı́ez",
      "year" : 1993
    }, {
      "title" : "Discovering Statistics Using SPSS (Introducing Statistical Methods series)",
      "author" : [ "A. Field" ],
      "venue" : "Third edition ed.,",
      "citeRegEx" : "Field,? \\Q2009\\E",
      "shortCiteRegEx" : "Field",
      "year" : 2009
    }, {
      "title" : "1961a. A CAUSAL CALCULUS (I)",
      "author" : [ "I.J. Good" ],
      "venue" : "Br J Philos Sci XI,",
      "citeRegEx" : "Good,? \\Q1961\\E",
      "shortCiteRegEx" : "Good",
      "year" : 1961
    }, {
      "title" : "1961b. A CAUSAL CALCULUS (II)",
      "author" : [ "I.J. Good" ],
      "venue" : "Br J Philos Sci XII,",
      "citeRegEx" : "Good,? \\Q1961\\E",
      "shortCiteRegEx" : "Good",
      "year" : 1961
    }, {
      "title" : "Multivariate Data Analysis (5th Edition)",
      "author" : [ "J.F. Hair", "R.L. Tatham", "R.E. Anderson", "W. Black" ],
      "venue" : null,
      "citeRegEx" : "Hair et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Hair et al\\.",
      "year" : 1998
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2001
    }, {
      "title" : "A Programming Language",
      "author" : [ "K.E. Iverson" ],
      "venue" : null,
      "citeRegEx" : "Iverson,? \\Q1962\\E",
      "shortCiteRegEx" : "Iverson",
      "year" : 1962
    }, {
      "title" : "Bayesian Networks and Decision Graphs (Information Science and Statistics)",
      "author" : [ "F.V. Jensen" ],
      "venue" : null,
      "citeRegEx" : "Jensen,? \\Q2001\\E",
      "shortCiteRegEx" : "Jensen",
      "year" : 2001
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Koller and Friedman,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "Recursive noisy OR - a rule for estimating complex probabilistic interactions",
      "author" : [ "J.F. Lemmer", "D.E. Gossink" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B",
      "citeRegEx" : "Lemmer and Gossink,? \\Q2004\\E",
      "shortCiteRegEx" : "Lemmer and Gossink",
      "year" : 2004
    }, {
      "title" : "Exploiting Structure in Weighted Model Counting Approaches to Probabilistic Inference",
      "author" : [ "W. Li", "P. Poupart", "P. van Beek" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "Li et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "Bayesian network modeling of the consensus between experts: An application to neuron classification",
      "author" : [ "P.L. López-Cruz", "P. Larrañaga", "J. DeFelipe", "C. Bielza" ],
      "venue" : "International Journal of Approximate Reasoning",
      "citeRegEx" : "López.Cruz et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "López.Cruz et al\\.",
      "year" : 2014
    }, {
      "title" : "Decision Support System using Dynamic Parsimonious Information Fusion Architectures",
      "author" : [ "P. Louvieris", "W. Garn", "G. White", "N. Mashanovich", "C. Papathanassiou" ],
      "venue" : "DIF DTC,",
      "citeRegEx" : "Louvieris et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Louvieris et al\\.",
      "year" : 2007
    }, {
      "title" : "Agent-Based Parsimonious Decision Support Paradigm Employing Bayesian Belief",
      "author" : [ "P. Louvieris", "A. Gregoriades", "N. Mashanovich", "G. White", "R. O’Keefe", "J. Levine", "S. Henderson" ],
      "venue" : null,
      "citeRegEx" : "Louvieris et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Louvieris et al\\.",
      "year" : 2005
    }, {
      "title" : "Generalized linear models (Second edition)",
      "author" : [ "P. McCullagh", "J.A. Nelder" ],
      "venue" : null,
      "citeRegEx" : "McCullagh and Nelder,? \\Q1989\\E",
      "shortCiteRegEx" : "McCullagh and Nelder",
      "year" : 1989
    }, {
      "title" : "Controlled generation of hard and easy Bayesian networks: Impact on maximal clique size in tree clustering",
      "author" : [ "O.J. Mengshoel", "D.C. Wilkins", "D. Roth" ],
      "venue" : "Artificial Intelligence 170,",
      "citeRegEx" : "Mengshoel et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Mengshoel et al\\.",
      "year" : 2006
    }, {
      "title" : "Fusion, propagation, and structuring in belief networks",
      "author" : [ "J. Pearl" ],
      "venue" : "Artificial Intelligence 29,",
      "citeRegEx" : "Pearl,? \\Q1986\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1986
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Bayesian networks with a logistic regression model for the conditional probabilities",
      "author" : [ "F. Rijmen" ],
      "venue" : "International Journal of Approximate Reasoning",
      "citeRegEx" : "Rijmen,? \\Q2008\\E",
      "shortCiteRegEx" : "Rijmen",
      "year" : 2008
    }, {
      "title" : "Artificial Intelligence: A Modern Approach (3rd Edition). 3 ed",
      "author" : [ "S. Russell", "P. Norvig" ],
      "venue" : null,
      "citeRegEx" : "Russell and Norvig,? \\Q2010\\E",
      "shortCiteRegEx" : "Russell and Norvig",
      "year" : 2010
    }, {
      "title" : "Application of a Bayesian Network in a GIS Based Decision Making System",
      "author" : [ "A. Stassopoulou", "M. Petrou", "J. Kittler" ],
      "venue" : "International Journal of Geographical Information Science",
      "citeRegEx" : "Stassopoulou et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Stassopoulou et al\\.",
      "year" : 1998
    }, {
      "title" : "Exploiting causal functional relationships in Bayesian network modelling for personalised healthcare",
      "author" : [ "M. Velikova", "J.T. van Scheltinga", "P.J. Lucas", "M. Spaanderman" ],
      "venue" : "International Journal of Approximate Reasoning",
      "citeRegEx" : "Velikova et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Velikova et al\\.",
      "year" : 2014
    }, {
      "title" : "Probabilistic inference with noisy-threshold models based on a CP tensor decomposition",
      "author" : [ "J. Vomlel", "P. Tichavský" ],
      "venue" : "International Journal of Approximate Reasoning",
      "citeRegEx" : "Vomlel and Tichavský,? \\Q2014\\E",
      "shortCiteRegEx" : "Vomlel and Tichavský",
      "year" : 2014
    }, {
      "title" : "Modeling causal reinforcement and undermining for efficient cpt elicitation",
      "author" : [ "Y. Xiang", "N. Jia" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions",
      "citeRegEx" : "Xiang and Jia,? \\Q2007\\E",
      "shortCiteRegEx" : "Xiang and Jia",
      "year" : 2007
    }, {
      "title" : "Knowledge engineering for bayesian networks: How common are noisy-max distributions in practice? Systems, Man, and Cybernetics: Systems",
      "author" : [ "A. Zagorecki", "M. Druzdzel" ],
      "venue" : "IEEE Transactions on",
      "citeRegEx" : "Zagorecki and Druzdzel,? \\Q2013\\E",
      "shortCiteRegEx" : "Zagorecki and Druzdzel",
      "year" : 2013
    }, {
      "title" : "Bayesian network approach to multinomial parameter learning using data and expert judgments",
      "author" : [ "Y. Zhou", "N. Fenton", "M. Neil" ],
      "venue" : "International Journal of Approximate Reasoning 55,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Pearl (1988), Mengshoel et al.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 20,
      "context" : "Pearl (1988), Mengshoel et al. (2006) and many more have provided fundamental work in this field.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "Pearl (1986) introduced and discussed the noisy-or which can be traced back to Good (1961a) and Good (1961b).",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 6,
      "context" : "Pearl (1986) introduced and discussed the noisy-or which can be traced back to Good (1961a) and Good (1961b).",
      "startOffset" : 79,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "Pearl (1986) introduced and discussed the noisy-or which can be traced back to Good (1961a) and Good (1961b). These approaches only work for binary variables.",
      "startOffset" : 79,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "Dı́ez (1993) showed a generalized noisy-or gate for multivalued variables.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "Dı́ez (1993) showed a generalized noisy-or gate for multivalued variables. Zagorecki and Druzdzel (2013) examined three test instances and found that noisy-max gates were a good fit for half of the given CPTs.",
      "startOffset" : 0,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "Dı́ez (1993) showed a generalized noisy-or gate for multivalued variables. Zagorecki and Druzdzel (2013) examined three test instances and found that noisy-max gates were a good fit for half of the given CPTs. Another generalization of the noisy-or is the recursive noisy-or by Lemmer and Gossink (2004). Vomlel and Tichavský (2014) introduced noisy-thresholds in combination with a novel tensor representation of CPTs.",
      "startOffset" : 0,
      "endOffset" : 304
    }, {
      "referenceID" : 5,
      "context" : "Dı́ez (1993) showed a generalized noisy-or gate for multivalued variables. Zagorecki and Druzdzel (2013) examined three test instances and found that noisy-max gates were a good fit for half of the given CPTs. Another generalization of the noisy-or is the recursive noisy-or by Lemmer and Gossink (2004). Vomlel and Tichavský (2014) introduced noisy-thresholds in combination with a novel tensor representation of CPTs.",
      "startOffset" : 0,
      "endOffset" : 333
    }, {
      "referenceID" : 5,
      "context" : "Dı́ez (1993) showed a generalized noisy-or gate for multivalued variables. Zagorecki and Druzdzel (2013) examined three test instances and found that noisy-max gates were a good fit for half of the given CPTs. Another generalization of the noisy-or is the recursive noisy-or by Lemmer and Gossink (2004). Vomlel and Tichavský (2014) introduced noisy-thresholds in combination with a novel tensor representation of CPTs. These and other noisy-methods belong to class of noisy functional dependency methods. These operate on a parsimonious set of input data (in general directly proportional to the number of states) and require additional constraints (e.g. order, independence). Techniques employed for CPT generation in order to support decision makers in our problem space must be flexible enough to accommodate a greater degree of variety than that imposed by the Noisy-Max method which imposes a sequence order constraint on the CPT output Li et al. (2011). Such a restriction is prone to produce erroneous CPTs.",
      "startOffset" : 0,
      "endOffset" : 960
    }, {
      "referenceID" : 10,
      "context" : "Jensen (2001) introduce several batch learning methods.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "Cooper and Dietterich (1992) present the construction of Bayesian Networks from databases.",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "Cooper and Dietterich (1992) present the construction of Bayesian Networks from databases. They showed ways to derive the conditional expectancy by means of frequency observations. In the subsequent analysis we will extend this conditional expectancy approach in a natural manner by the usage of regression based CPTs. Popular parameter estimation techniques include the maximum likelihood estimation (MLE), Bayesian estimation and Expectation-Maximization (EM). We will discuss MLE and EM in section 3.2 and 3.3 respectively. These methods can be seen as having an objective subject to constraints. Zhou et al. (2014) describe a constrained optimization approach, which reveals similar conceptual ideas to the ones used in this paper but realizing it with different methods.",
      "startOffset" : 0,
      "endOffset" : 619
    }, {
      "referenceID" : 2,
      "context" : "The process of eliciting the information with minimum effort has been addressed by Bhattacharjya et al. (2014) by determining the order in which the CPT should be queried from a single expert.",
      "startOffset" : 83,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "The process of eliciting the information with minimum effort has been addressed by Bhattacharjya et al. (2014) by determining the order in which the CPT should be queried from a single expert. Xiang and Jia (2007) investigated ways of finding CPTs by proposing a causal tree model.",
      "startOffset" : 83,
      "endOffset" : 214
    }, {
      "referenceID" : 2,
      "context" : "The process of eliciting the information with minimum effort has been addressed by Bhattacharjya et al. (2014) by determining the order in which the CPT should be queried from a single expert. Xiang and Jia (2007) investigated ways of finding CPTs by proposing a causal tree model. Finding consensus between experts is a challenge as we observed in our military case study. This issue has also been investigated in López-Cruz et al. (2014), who developed Bayesian network to facilitate this task.",
      "startOffset" : 83,
      "endOffset" : 440
    }, {
      "referenceID" : 26,
      "context" : "In general the technique is especially useful when causes and effects have been measured, leading to a wide range of applications such as healthcare Velikova et al. (2014).",
      "startOffset" : 149,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "In Pearl (1988) the notation Mz|x is used for the CPT and called conditional probability matrix or link matrix.",
      "startOffset" : 3,
      "endOffset" : 16
    }, {
      "referenceID" : 27,
      "context" : "Here, we would like to draw the readers attention to Vomlel and Tichavský (2014) representation of CPTs as tensors.",
      "startOffset" : 53,
      "endOffset" : 81
    }, {
      "referenceID" : 24,
      "context" : "The literature Russell and Norvig (2010) distinguish between three types of Bayesian Networks: discrete, continuous and hybrid Bayesian Networks.",
      "startOffset" : 15,
      "endOffset" : 41
    }, {
      "referenceID" : 21,
      "context" : "A representation of multiple nodes as a single node is known as a cluster node Pearl (1988).",
      "startOffset" : 79,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "These techniques are more commonly placed in the area of parameter learning methods (see Koller and Friedman (2009) for details).",
      "startOffset" : 89,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "General discussions of multiple regressions can be found in Hastie et al. (2001) and Greene (2000).",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "General discussions of multiple regressions can be found in Hastie et al. (2001) and Greene (2000). The application of the conditional mean to approximate the conditional probability table was used in Cooper and Dietterich (1992) in relation to Bayesian Networks.",
      "startOffset" : 60,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "The application of the conditional mean to approximate the conditional probability table was used in Cooper and Dietterich (1992) in relation to Bayesian Networks.",
      "startOffset" : 101,
      "endOffset" : 130
    }, {
      "referenceID" : 19,
      "context" : "The logistic model is a special case of the generalised linear model, explained in McCullagh and Nelder (1989). The effect described by its probabilities P(Z) (dependent variable) is computed given cause probabilities (independent variables).",
      "startOffset" : 83,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "So far this has been “classic” multivariate data analysis which is explained in Hair et al. (1998), Hastie et al.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "So far this has been “classic” multivariate data analysis which is explained in Hair et al. (1998), Hastie et al. (2001) and Field (2009).",
      "startOffset" : 80,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "(2001) and Field (2009).",
      "startOffset" : 11,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "Rijmen (2008) has discussed this method in more detail.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 29,
      "context" : "Zhou et al. (2014); Zagorecki and Druzdzel (2013)) for CPT comparisons, which was originally developed to measure the relative entropy in information.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 29,
      "context" : "(2014); Zagorecki and Druzdzel (2013)) for CPT comparisons, which was originally developed to measure the relative entropy in information.",
      "startOffset" : 8,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "4Iverson (1962) introduced these brackets for true-",
      "startOffset" : 1,
      "endOffset" : 16
    }, {
      "referenceID" : 25,
      "context" : "A Bayesian network to determine the risk of burnt forest desertification was introduced in Stassopoulou et al. (1998). The main purpose of this paper was to obtain the correspondence between Bayesian and Neural Networks.",
      "startOffset" : 91,
      "endOffset" : 118
    }, {
      "referenceID" : 25,
      "context" : "The paper Stassopoulou et al. (1998) details two tables of data, which we will use as input to compute the conditional probability table.",
      "startOffset" : 10,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "Typical military effects are find, destroy, secure and many more (see Louvieris et al. (2007) for details).",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "In this paper we will not discuss the challenges of obtaining evidence for these measures nor the derivation of this Bayesian Network - respective information can be found in Louvieris et al. (2005). We will focus our discussion on converging Bayesian Networks.",
      "startOffset" : 175,
      "endOffset" : 199
    }, {
      "referenceID" : 3,
      "context" : "As a third avenue it should be investigated of whether the converging Bayesian Networks are superior to the Extended Belief Rule-Based systems Calzada et al. (2015).",
      "startOffset" : 143,
      "endOffset" : 165
    }, {
      "referenceID" : 25,
      "context" : "The “burnt forest paper” Stassopoulou et al. (1998) proofed to be a useful resource.",
      "startOffset" : 25,
      "endOffset" : 52
    } ],
    "year" : 2015,
    "abstractText" : "Decision making is often based on Bayesian networks. The building blocks for Bayesian networks are its conditional probability tables (CPTs). These tables are obtained by parameter estimation methods, or they are elicited from subject matter experts (SME). Some of these knowledge representations are insufficient approximations. Using knowledge fusion of cause and effect observations lead to better predictive decisions. We propose three new methods to generate CPTs, which even work when only soft evidence is provided. The first two are novel ways of mapping conditional expectations to the probability space. The third is a column extraction method, which obtains CPTs from nonlinear functions such as the multinomial logistic regression. Case studies on military effects and burnt forest desertification have demonstrated that so derived CPTs have highly reliable predictive power, including superiority over the CPTs obtained from SMEs. In this context, new quality measures for determining the goodness of a CPT and for comparing CPTs with each other have been introduced. The predictive power and enhanced reliability of decision making based on the novel CPT generation methods presented in this paper have been confirmed and validated within the context of the case studies.",
    "creator" : "LaTeX with hyperref package"
  }
}