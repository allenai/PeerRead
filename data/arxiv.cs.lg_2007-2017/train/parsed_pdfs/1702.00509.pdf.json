{
  "name" : "1702.00509.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Segmentation of optic disc, fovea and retinal vasculature using a single convolutional neural network",
    "authors" : [ "Jen Hong Tan", "U. Rajendra Acharya", "Sulatha V. Bhandary", "Kuang Chua Chua", "Sobha Sivaprasad" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We have developed and trained a convolutional neural network to automatically and simultaneously segment optic disc, fovea and blood vessels. Fundus images were normalized before segmentation was performed to enforce consistency in background lighting and contrast. For every effective point in the fundus image, our algorithm extracted three channels of input from the point’s neighbourhood and forwarded the response across the 7-layer network. The output layer consists of four neurons, representing background, optic disc, fovea and blood vessels. In average, our segmentation correctly classified 92.68% of the ground truths (on the testing set from Drive database). The highest accuracy achieved on a single image was 94.54%, the lowest 88.85%. A single convolutional neural network can be used not just to segment blood vessels, but also optic disc and fovea with good accuracy.\nKeywords – optic disc segmentation, blood vessels segmentation, fovea segmentation, convolutional neural network, fundus image"
    }, {
      "heading" : "1. Introduction",
      "text" : "Automated segmentation of retinal vasculature and optic disc are well-studied in literature, but less so for fovea. Blood vessels and optic disc on fundus images generally have distinct boundaries (see Figure 1), and therefore exhibit a clearly delineated region. Fovea, however, shows no border.\nTo date there is no single study that works to automatically and simultaneously segment optic disc, fovea and vasculature. Most of the proposed automated segmentation focused only on either one of them, with majority of the works dedicated to vasculature.\nFraz et al. [1] has made an extensive review on the works that solely segment vasculature, in which they have divided the proposed solutions in literature into seven categories: pattern recognition [2-4], matched filtering [5,6], vessel tracking/tracing [7], mathematical morphology [8,9], multiscale approach [10,11], model based approach [12,13] and parallel/hardware approach [14]. Most of the methodologies reported their performances in terms of accuracy, sensitivity, specificity and area under the curve, and majority of them were evaluated on DRIVE database [1].\nOn the other hand, for optic disc most of the proposed solutions [15] used methods either based on intensity [16,17], template matching [18,19] or vasculature structure [20,21]. These published works can also be categorized into two types [21]: methods that locate the optic disc but do not segment optic disc [16,22], and methods that segment optic disc [17,18]. When segmentation was actually performed, overlap measure was reported for performance.\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n3\nHowever, on fovea segmentation, overlap measure was never reported, for nearly all of the published works on fovea segmentation were designed to locate the position of the fovea, not to segment the region of interest. When the fovea is located, the positions acquired can be used for the assessment of diabetic retinopathy, as detailed in the work by Medhi and Dandapat [23].\nThe solutions proposed in literature [24] often require the information of optic disc and/or blood vessels, and in some methodologies, the fovea was located with the knowledge of the specific branches of vasculature and the position of the optic disc. For example, Li and Chutatape [25] have used a modified active shape model to get the contours converged on the outer rim of optic disc and its neighbouring vascular arch. From there the algorithm determined the position of the fovea.\nAnd it is not only Li and Chutatape [25] that used shape model to look for the optic disc and fovea. In the work by Niemeijer et al. [20], they also used a point distribution model to locate similar objects. The algorithm proposed did not delineate any boundary, and it was only three years later, in another paper [26], a kNN-regressor and a circular template were proposed by the authors to enclose the boundary of optic disc and fovea. Even then they only evaluated their algorithm against the position of the optic disc and fovea, not the amount of area overlapped with a ground truth.\nOn the other hand, Sinthanayothin et al. [16] has in a study developed methods to segment blood vessels and locate the position of optic disc and fovea automatically. The blood vessels were identified using a three-layer perceptron. The optic disc was localized based on the intensity variation among adjacent pixels, and then the location of fovea was determined at a distance 2.5 times the diameter of the optic disc from the position of the optic disc.\nBesides, Narasimha-Iyer et al. [27] have proposed a solution to segment vasculature, optic disc and fovea using three separate algorithms. However, as the focus of the paper was to detect longitudinal changes in fundus image, not segmentation, they only visually evaluated the segmentation of optic disk and fovea, which they found acceptable [27].\nNone of the above studies performed a simultaneous segmentation of vasculature, optic disc and fovea. When vasculature, optic disc and fovea were actually segmented, separate algorithms were used. Furthermore, these algorithms work by assuming vasculature, optic disc and fovea are present in the image of interest. They cannot indicate the absence of a feature when the feature is not available in the image.\nIn this paper we are going to propose a solution not just to locate, but to simultaneously segment vasculature, optic disc and fovea. We use a 7-layer convolutional neural network (CNN) to classify every pixel into one of the four classes: background, blood vessels, optic disc and fovea. When any of the features is not\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n4\navailable in the image, our algorithm is unlikely to produce any segmentation of the missing feature.\nCNN has been adopted recently for segmentation on fundus images [3,28,29]. Among the works, Wang et al. [3] has produced the best accuracy on DRIVE database [30]. The team has used a 7-layer CNN together with random forest algorithm to extract retinal vasculature. However, in their work CNN was used only as a trainable feature extractor; the classification was in fact performed by an ensemble of random forest.\nThe input to their neural network is a 25 x 25 image patch, extracted from green channel. But in our case, to segment fovea and optic disc, a larger input, or an input of other form is definitely needed to discriminate a pixel among the four classes.\nThe reasons can be seen in Figure 2. On the first row of the figure, there are two frames (each of them sizes 25 x 25) that should be classified as fovea and another two as background. But there are almost no discernible differences among the four (except the colour, which is not a useful clue). On the other hand, although optic disc is generally considered to have a distinct appearance that separates itself from the rest of the image, this is not always the case when we frame part of the region under a box of 25 x 25, as illustrated in the second row of Figure 2.\nHowever, an input of size say 101 x 101 is not really a good choice, for the training will take too much time and too many computer memories. To make our segmentation possible, we propose an input of size 33 x 33, but with three channels. For a pixel at location (\uD835\uDC65, \uD835\uDC66), the first channel of the input is a 7 x 7 neighbourhood around the pixel but scaled to a size of 33 x 33. The second channel is just a 33 x 33 neighbourhood surrounding the pixel. The final channel is a region spanning 165 x 165, with the pixel as its center but scaled down to a size of 33 x 33. Further details on the architecture will be elaborated in section 3.\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n5\nd from background. Similar confusion can also arise for pixels from optic disc and regions other than optic disc. In the second row, frame f, g are taken from optic disc region; e, h from non-optic-disc region."
    }, {
      "heading" : "2. Materials",
      "text" : "We developed and tested our algorithm on the Drive database [30], which is available for public. The database provides a total of 40 fundus images, in which 33 of them exhibits no symptom of diabetic retinopathy, with the rest shows traits of mild early diabetic retinopathy. The database also provides every image a mask, so that nonfundus regions can be excluded during analysis.\nCanon CR 5 non-mydriatic 3CCD camera was used to take the images. The field of view is 45 degree. Each image sizes 565 x 584, and the entire database is divided into a training and a testing set, each consists of 20 images.\nWe trained our CNN on the training set and tested it on the testing set. The ground truths on the vasculature for the training and the testing set were provided by the database. However, we used only the vasculature ground truths provided in the folder ‘1st_manual’ for testing. For optic disc and fovea, all the ground truths were delineated by an ophthalmologist who has a clinical experience of more than 10 years.\n3. Methods 3.1 Normalization\nWe normalized the colour image before we performed any classification (see Figure 3). We used the procedures laid out in the Section 3.A (official manuscript)/Section 3.1 (uploaded manuscript) from [31] to do the normalization, with only a change. In the original article, the processes were done on the green channel of a fundus image to produce a normalized grey-scale image for vasculature extraction. In this study, we converted a fundus image from RGB colour space to LUV colour space, ran the procedures on the L channel (luminance channel), and with the adjusted L channel, we converted the image back to RGB colour space. In this case the product is a normalized colour image, which has a wider usage than the output by the previous study.\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n8\n3.2 The architecture\nFigure 4. The architecture of the proposed CNN.\nThe structure of the proposed neural network is illustrated in Figure 4 and detailed in Table 1. The input (layer 0) consists of 3 channel, each sizes 33 x 33. For each channel, its 33 x 33 matrix is convolved with 10 5 x 5 filters to produce 10 feature maps (layer 1). On each feature map a max-pooling sizing 2 x 2 is applied (layer 2).\nAfter that, the 10 feature maps are convolved with a 5 x 5 x 10 filters to a produce a 10 x 10 feature map (layer 3). And there are 15 such feature maps produced in this layer. Max-pooling of size 2 x 2 is again applied on every feature map (layer 4). Lastly, the neurons of every feature maps in layer4 (which includes output from other channels) are connected fully to a 100 neurons in layer 5, which are also fully connected to the four outputs (neurons) in layer 6.\nWe use only green colour channel from the normalized image. It is standardized using equation 11 in [31] before we extract any patches from it. For a given pixel at (\uD835\uDC65\uD835\uDC56 , \uD835\uDC66\uD835\uDC56), the first channel to the input is made up by a 7 x 7 neighbourhood scaled up to a size of 33 x 33. For the second channel, it is formed by a neighbourhood sizing 33 x 33 around the pixel. For the final channel, a block spanning 165 x 165 centered on the\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n9\npixel are extracted from the standardized green channel of the original fundus image, but scaled down to size of 33 x 33. All of the scalings are performed using bi-cubic interpolation. Figure 5 shows the typical inputs to our CNN.\nLeaky rectifier linear unit (LReLU) [32] is used as the activation function for layer 1, 3, and 5. For layer 6, we use softmax function instead. Xavier initialization [33] is used for the weights of layer 1, 3 and 5. For biases, they are set to a value of 1 on layer 1, 3 and 5. On the other hand, the value of the biases at the layer 4 is randomly generated on a Gaussian distribution.\nBefore we settled the input size to our network on 33 x 33, we have tested several other sizes, from 9 x 9 to 45 x 45. We found that when input size of 9 x 9 was used, the computation time was much shorter, but the accuracy rarely exceeded 90%. When we increased the input size to 45 x 45, the accuracy at best hovered around 91%. And after numerous observations, we concluded that 33 x 33 gave the best accuracy.\nWe have also tried an input size of 71 x 71, single channel, with a network consisting of 9 layers. The computation time was much slower, which took nearly two days to complete a single epoch. The segmentation accuracy was lower than the proposed setup, and therefore the input size was not adopted.\n10\n3.3 Training\nStandard backpropagation [34] was used to learn and stochastic gradient descent [35], with a batch size of 10, was set up to train our CNN. The weights are updated by\n\uD835\uDC30\uD835\uDC59 = (1 − \uD835\uDF02\uD835\uDF06\n\uD835\uDF11 )\uD835\uDC30\uD835\uDC59−1 −\n\uD835\uDF02 \uD835\uDF05 \uD835\uDF15\uD835\uDC50 \uD835\uDF15\uD835\uDC30\n( 1 )\nwhere \uD835\uDC59 is the layer number, \uD835\uDF02 is the learning rate, \uD835\uDF11 is the total number of training samples, \uD835\uDF06 is the regularization parameter, \uD835\uDF05 is the batch size. \uD835\uDC50 denotes cost function, which in our case is log-likelihood function. Biases are updated through\n\uD835\uDC1B\uD835\uDC59 = \uD835\uDC1B\uD835\uDC59−1 − \uD835\uDF02\n\uD835\uDF05\n\uD835\uDF15\uD835\uDC50 \uD835\uDF15\uD835\uDC1B\n( 2 )\nThe learning rate and the regularization parameter are set to 0.01 and 0.1 respectively.\nA point is said effective if it is part of the fundus in fundus image. In total there are 4,541,006 effective points in the training set (20 fundus images), of which 3,817,049 of them are categorized as background, and an amount of 569,415, 79,321 and 75,221 points are determined as blood vessels, optic disc and fovea respectively. We did not use all of the points for training, as it would take too much time and too many computer memories to run. Instead, we took only a portion of all the available points to do the training, as listed in Table 2.\n11\n3.4 Testing\nAt the end of every training epoch, our algorithm performs a testing on our neural network model. The testing set provided by DRIVE database consists of 4,538,439 effective points (Table 3 shows the break-down on each class). In each class, for every four effective points, our algorithm takes one to test the model’s accuracy. 40 epochs of training and testing were run. The model that gives the best performance among the 40 epochs is then picked to run a complete classification on all the available effective points from the testing set.\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n12"
    }, {
      "heading" : "4. Results",
      "text" : "We developed our algorithm/code in MATLAB without using any other library or toolbox (except MATLAB’s image processing toolbox for reading image and visualization). Some of the heavy-duty processes were written in C MEX file to speed up calculation. We train our net on a workstation which has two Intel Xeon 2.20 GHz (E5-2650 v4) processor and a 512GB RAM. It typically took about 38,749 seconds to complete an epoch of training.\nWe performed the complete testing on a Mac Pro, which has two Intel 2.66GHz Xeon ‘Westmere’ processors and a 24GB RAM. Our net delivered an accuracy of 92.68% (average) on the complete testing (which classifieds 4,538,439 effective points). The highest accuracy achieved on a single image was 94.54%, the lowest 88.85% (see Table 4 for more details). It took in average 3750.55 seconds to completely segment an image. Table 5 tabulates the confusion matrix on our four-class classification; Table 6 presents the matrix in terms of percentage. Figure 6 shows some of the segmentation output.\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n13\nTable 7 tabulated the sensitivity and specificity in each classes. For a particular class, the sensitivity is calculated by the number of pixels correctly classified as the class divided by the total number of pixels that belong to the class. For specificity, it is calculated by the number of pixels correctly classified as not the class divided by the total number of pixels that do not belong to the class.\nTable 8 detailed the comparison of the performance of our algorithm against previous studies. We only included studies that reported sensitivity and specificity or overlap,\n14\nevaluated on DRIVE database. For vasculature segmentation, we did not include all the literature in the table; we selected only some of the best results in each type of methods (according to the categories proposed by [1]).\nIt can be seen that in terms of sensitivity, our method performed reasonably well as compared to previous works, despite the fact that our algorithm was arranged to simultaneously segmented vasculature, optic disc and fovea, instead of only one or two of them. Our method has a lower specificity, which we think part of the reason, is because at the region of optic disc, some of the pixels that belong to optic disc were misidentified as blood vessels and vice versa.\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx"
    }, {
      "heading" : "5. Discussion",
      "text" : "Convolutional neural network has worked wonderfully on a number of image identification problems. It has been trained to discriminate Arabic numerals with almost perfect accuracy [41]. In terms of categorizing images (of 1000 classes), it even outperformed human observer [32].\nBut using a convolutional neural network to segment blood vessels, optic disc and fovea out of a fundus image is not the same as numeral recognition, or image categorization in at least three aspects. First, in our problem we perform segmentation by classifying the membership of every single pixel, whereas most of the other problems only identify the membership of an image, or a segment of image.\nSecond, the membership of a pixel can be ambiguous, especially along the boundary. Take for example, in Figure 7a, should the pixel at the middle (marked by a green dot) be classified as background or blood vessels? A boundary that is sharp and clear when viewed at its original scale does not necessarily imply a clear separation of classes at pixel level. When this happens, it is never easy to determine a pixel’s membership with good confidence.\nThird, the membership of some pixels can be inferred only with reference to a larger neighbourhood around the pixel of interest. This is especially the case for optic disc and fovea, as illustrated in Figure 2. However, taking a large neighbourhood as input to neural network cannot be considered as good solution, for two reasons.\nFirst, the memory requirement to train and test the net is excessive. Assume a frame of 101 x 101 is sufficient to discriminate all classes of pixels. A single such input extracted from a grey image will take 101 x 101 x 8 = 81,608 bytes (assume every pixel in an image is expressed by an unsigned integer, not double). If a computer is to hold the entire training set—which has 750,000 samples—in random access memory, it will\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n17\nneed around 60 Gigabytes memory space, and this is not inclusive of another million samples required to test the net at the end of every training epoch!\nSecond, although a large neighbourhood is useful to determine the pixels of optic disc and fovea, majority of the pixels in the neighbourhood are unnecessary for the determination of the pixels of blood vessels and background. Wang et al. [3] has proposed a net that could accurately discriminate a pixel between classes of blood vessels and background using only a 25 x 25 input. Assume that size of input is what is only needed for the task, this will render nearly 94% of the 101 x 101 input useless to the separation of blood vessels and background. And since pixels of blood vessels and background make up almost 95% of the total effective pixels, it is rather inefficient to train and test a net where most of the information provided at input is redundant.\nInstead, we have designed a net with an input of three channels—each distinctly scaled—to perform the segmentation. Channel 1 is put in to overcome the problem of ambiguity along boundary. It is inserted to push the net to learn extensively and exclusively on the pixels that are close to the point of interest, so that it can resolve the confusion arisen at the boundary.\nChannel 3, on the other hand, is included to capture the macro pattern around the point of interest. The pixels of optic disc and fovea are relatively straightforward to determine if the macro pattern around the point of interest is available to the net. And the solution works, as illustrated in Table 6. For optic disc and fovea, 87.90% and 88.53% of their respective pixels were correctly identified, as compared to 75.37% for blood vessels (it is important to note that, however, blood vessels have much larger presence in fundus image).\nSegmentation of blood vessels is important in the cardiovascular field and ophthalmic diseases. For example, morphological changes in retinal blood vessels are associated with cardiovascular disease [42] and hypertension [43]. Current tools to quantify these changes are cumbersome and time-consuming. The ability to automate the segmentation of blood vessels may help us to progress in the research on whether blood vessel changes may be a risk factor for cardiovascular disease. Venous dilatation and beading in diabetic retinopathy is again a useful tool for predicting progression [44] from severe non-proliferative diabetic retinopathy to proliferative retinopathy defined by the development of new blood vessels. This change in grade indicates the need to treat retinopathy. Therefore, accurate segmentation of blood vessel change may be used as a marker of progression if automated grading is utilized in diabetic retinopathy screening in future. Treatment response to panretinal photocoagulation [45] can also be monitored by blood vessel caliber changes if we can accurately define width of the blood vessels. In babies, the presence of plus disease is again defined by vessel width and tortuosity [46]. Like the iris, the retinal vascular tree is unique for each individual and may be used for biometric identification [47].\nSegmentation of the optic disc is the first step towards automated screening and diagnosis of glaucoma [48]. The optic disc blood vessels and the peripapillary atrophy\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n18\nare challenges for accurate assessment of cup disc ratio even for a human observer. Defining the boundary of the optic disc is therefore crucial in automated segmentation.\nThe average diameter of the foveal avascular zone shows considerable inter-individual variation [49]. However, serial measurement of the foveal avascular zone is a predictive marker of macular ischemia [50]. Thus segmentation of the fovea will be very useful in annual screening of diabetic retinopathy.\nOur algorithm so far took about 3751 seconds to complete segmentation on a single image. However that was achieved using only CPU. It is agreed that convolutional neural network runs faster on GPU, with probably 10 or 20 times quicker in processing speed [51]. In this case, our algorithm can complete the segmentation just under 5 minutes if it is run on a GPU, and much quicker if it is run on multiple GPUs.\nFurthermore, unlike many other methods, our algorithm would not produce segmentation of optic disc or fovea when any of them is absent in a fundus image. It does not perform searching across the image by assuming optic disc or fovea is present in the image. We believe this is a desirable property that makes our algorithm more robust and versatile compared to many other techniques proposed in literature."
    }, {
      "heading" : "6. Conclusion",
      "text" : "Most of the segmentation on fundus image can only segment/locate either vasculature, optic disc and fovea. We have developed an algorithm that can automatically and simultaneously segment all three of them. The algorithm consists of two parts: background normalization and 7-layer convolutional neural network. We have trained and tested our method on DRIVE database. It has achieved an average of 92.68% in terms of accuracy."
    }, {
      "heading" : "Acknowledgement",
      "text" : "The research was supported by the National Institute for Health Research (NIHR) Biomedical Research Centre based at Moorfields Eye Hospital NHS Foundation Trust and UCL Institute of Ophthalmology. The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health.\nReferences\n[1] M.M. Fraz, P. Remagnino, A. Hoppe, B. Uyyanonvara, A.R. Rudnicka, C.G.\nOwen, et al., Blood vessel segmentation methodologies in retinal images – A\nsurvey, Computer Methods and Programs in Biomedicine. 108 (2012) 407–\n433. doi:10.1016/j.cmpb.2012.03.009.\n[2] C. Zhu, B. Zou, R. Zhao, J. Cui, X. Duan, Z. Chen, et al., Retinal vessel\nsegmentation in colour fundus images using Extreme Learning Machine,\nComputerized Medical Imaging and Graphics. (2016) 1–10.\ndoi:10.1016/j.compmedimag.2016.05.004.\n[3] S. Wang, Y. Yin, G. Cao, B. Wei, Y. Zheng, G. Yang, Hierarchical retinal blood\nvessel segmentation based on feature and ensemble learning,\nNeurocomputing. 149 (2014) 1–11. doi:10.1016/j.neucom.2014.07.059.\n[4] M.M. Fraz, P. Remagnino, A. Hoppe, B. Uyyanonvara, A.R. Rudnicka, C.G.\nOwen, et al., An Ensemble Classification-Based Approach Applied to Retinal\nBlood Vessel Segmentation, IEEE Trans. Biomed. Eng. 59 (2012) 2538–2548.\ndoi:10.1109/TBME.2012.2205687.\n[5] B. Zhang, L. Zhang, L. Zhang, F. Karray, Retinal vessel extraction by matched\nfilter with first-order derivative of Gaussian, Computers in Biology and\nMedicine. 40 (2010) 438–445. doi:10.1016/j.compbiomed.2010.02.008.\n[6] M.A. Amin, H. Yan, High speed detection of retinal blood vessels in fundus\nimage using phase congruency, Soft Comput. 15 (2010) 1217–1230.\ndoi:10.1007/s00500-010-0574-2.\n[7] K.K. Delibasis, A.I. Kechriniotis, C. Tsonos, N. Assimakis, Automatic model-\nbased tracing algorithm for vessel segmentation and diameter estimation,\nComputer Methods and Programs in Biomedicine. 100 (2010) 108–122.\ndoi:10.1016/j.cmpb.2010.03.004.\n[8] M.S. Miri, A. Mahloojifar, Retinal Image Analysis Using Curvelet Transform\nand Multistructure Elements Morphology by Reconstruction, IEEE Trans.\nBiomed. Eng. 58 (2011) 1183–1192. doi:10.1109/TBME.2010.2097599.\n[9] A.M. Mendonca, A. Campilho, Segmentation of retinal blood vessels by\ncombining the detection of centerlines and morphological reconstruction,\nIEEE Trans. Med. Imaging. 25 (2006) 1200–1213.\ndoi:10.1109/TMI.2006.879955.\n[10] M. Vlachos, E. Dermatas, Multi-scale retinal vessel segmentation using line\ntracking, Computerized Medical Imaging and Graphics. 34 (2010) 213–227.\ndoi:10.1016/j.compmedimag.2009.09.006.\n[11] M.E. Martinez-Perez, A.D. Hughes, S.A. Thom, A.A. Bharath, K.H. Parker,\nSegmentation of blood vessels from red-free and fluorescein retinal images,\nMedical Image Analysis. 11 (2007) 47–61. doi:10.1016/j.media.2006.11.004.\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n21\n[12] L. Wang, A. Bhalerao, R. Wilson, Analysis of Retinal Vasculature Using a\nMultiresolution Hermite Model, IEEE Trans. Med. Imaging. 26 (2007) 137–\n152.\n[13] L. Espona, M.J. Carreira, M.G. Penedo, M. Ortega, Retinal vessel tree\nsegmentation using a deformable contour model, in: IEEE, 2008: pp. 1–4.\ndoi:10.1109/ICPR.2008.4761762.\n[14] M.A. Palomera-Pérez, M.E. Martinez-Perez, Parallel multiscale feature\nextraction and region growing: application in retinal blood vessel detection,\nIEEE Transactions on …. (2010).\ndoi:10.1109/TITB.2009.2036604\",\"rightsLink\":\"http://s100.copyright.com/Ap\npDispatchServlet?publisherName=ieee&publication=1089-\n7771&title=Parallel+Multiscale+Feature+Extraction+and+Region+Growing%3\nA+Application+in+Retinal+Blood+Vessel+Detection&isbn=&publicationDate=\nMarch+2010&author=Miguel+A.+Palomera-\nP%C3%A9rez&ContentID=10.1109/TITB.2009.2036604&orderBeanReset=tru\ne&startPage=500&endPage=506&volumeNum=14&issueNum=2\",\"displayPu\nblicationTitle“:”IEEE.\n[15] L. Xiong, H. Li, An approach to locate optic disc in retinal images with\npathological changes, Computerized Medical Imaging and Graphics. 47\n(2016) 40–50. doi:10.1016/j.compmedimag.2015.10.003.\n[16] C. Sinthanayothin, J.F. Boyce, H.L. Cook, T.H. Williamson, Automated\nlocalisation of the optic disc, fovea, and retinal blood vessels from digital\ncolour fundus images, British Journal of Ophthalmology. 83 (1999) 902–910.\n[17] H.-K. Hsiao, C.-C. Liu, C.-Y. Yu, S.-W. Kuo, S.-S. Yu, A novel optic disc detection\nscheme on retinal images, Expert Systems with Applications. 39 (2012)\n10600–10606. doi:10.1016/j.eswa.2012.02.157.\n[18] J. Lowell, A. Hunter, D. Steel, A. Basu, R. Ryder, E. Fletcher, et al., Optic nerve\nhead segmentation, IEEE Trans. Med. Imaging. 23 (2004) 256–264.\ndoi:10.1109/TMI.2003.823261.\n[19] H. Li, O. Chutatape, Automatic location of optic disk in retinal images, Image\nProcessing. (2001).\ndoi:10.1109/ICIP.2001.958624\",\"rightsLink“:”http://s100.copyright.com/App\nDispatchServlet?publisherName=ieee&publication=proceedings&title=Auto\nmatic+location+of+optic+disk+in+retinal+images&isbn=0-7803-6725-\n1&publicationDate=2001&author=+Huiqi+Li&ContentID=10.1109/ICIP.2001.\n958624&orderBeanReset=true&startPage=837&endPage=840.\n[20] M. Niemeijer, M.D. Abràmoff, B. van Ginneken, Segmentation of the Optic\nDisc, Macula and Vascular Arch in Fundus Photographs, IEEE Trans. Med.\nImaging. 26 (2006) 116–127. doi:10.1109/TMI.2006.885336.\n[21] D. Welfer, J. Scharcanski, C.M. Kitamura, M.M.D. Pizzol, L.W.B. Ludwig, D.R.\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n22\nMarinho, Segmentation of the optic disk in color eye fundus images using an\nadaptive morphological approach, Computers in Biology and Medicine. 40\n(2010) 124–137. doi:10.1016/j.compbiomed.2009.11.009.\n[22] M. Foracchia, E. Grisan, A. Ruggeri, Detection of Optic Disc in Retinal Images\nby Means of a Geometrical Model of Vessel Structure, IEEE Trans. Med.\nImaging. 23 (2004) 1189–1195. doi:10.1109/TMI.2004.829331.\n[23] J.P. Medhi, S. Dandapat, An effective fovea detection and automatic\nassessment of diabetic maculopathy in color fundus images, Computers in\nBiology and Medicine. 74 (2016) 30–44.\ndoi:10.1016/j.compbiomed.2016.04.007.\n[24] D. Welfer, J. Scharcanski, D.R. Marinho, Fovea center detection based on the\nretina anatomy and mathematical morphology, Computer Methods and\nPrograms in Biomedicine. 104 (2011) 397–409.\ndoi:10.1016/j.cmpb.2010.07.006.\n[25] H. Li, O. Chutatape, Automated Feature Extraction in Color Retinal Images by\na Model Based Approach, IEEE Trans. Biomed. Eng. 51 (2004) 246–254.\ndoi:10.1109/TBME.2003.820400.\n[26] M. Niemeijer, M.D. Abràmoff, B. van Ginneken, Fast detection of the optic\ndisc and fovea in color fundus photographs, Medical Image Analysis. 13\n(2009) 859–870. doi:10.1016/j.media.2009.08.003.\n[27] H. Narasimha-Iyer, A. Can, B. Roysam, C.V. Stewart, H.L. Tanenbaum, A.\nMajerovics, et al., Robust Detection and Classification of Longitudinal\nChanges in Color Retinal Fundus Images for Monitoring Diabetic Retinopathy,\nIEEE Trans. Biomed. Eng. 53 (2006) 1084–1098.\ndoi:10.1109/TBME.2005.863971.\n[28] Q. Li, B. Feng, L. Xie, P. Liang, H. Zhang, T. Wang, A Cross-Modality Learning\nApproach for Vessel Segmentation in Retinal Images, IEEE Trans. Med.\nImaging. 35 (2015) 109–118. doi:10.1109/TMI.2015.2457891.\n[29] P. Liskowski, K. Krawiec, Segmenting Retinal Blood Vessels With Pub\n_newline Deep Neural Networks, IEEE Trans. Med. Imaging. 35 (2016) 2369–\n2380. doi:10.1109/TMI.2016.2546227.\n[30] J. Staal, M.D. Abramoff, M. Niemeijer, M.A. Viergever, B. van Ginneken,\nRidge-Based Vessel Segmentation in Color Images of the Retina, IEEE Trans.\nMed. Imaging. 23 (2004) 501–509. doi:10.1109/TMI.2004.825627.\n[31] J.H. Tan, R. Acharya U, K.C. Chua, C. Cheng, A. Laude, Automated extraction\nof retinal vasculature, Med. Phys. 43 (2016) 2311–2322.\ndoi:10.1118/1.4945413.\n[32] K. He, X. Zhang, S. Ren, J. Sun, Delving Deep into Rectifiers: Surpassing\nHuman-Level Performance on ImageNet Classification, (2015) 1026–1034.\n[33] X. Glorot, Y. Bengio, Understanding the difficulty of training deep\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n23\nfeedforward neural networks, Aistats. (2010).\n[34] J. Bouvrie, Notes on Convolutional Neural Networks, 2007.\n[35] Y.A. LeCun, L. Bottou, G.B. Orr, K.-R. Müller, Efficient BackProp, in: Neural\nNetworks: Tricks of the Trade, Springer Berlin Heidelberg, Berlin, Heidelberg,\n2012: pp. 9–48. doi:10.1007/978-3-642-35289-8_3.\n[36] D. Welfer, J. Scharcanski, D.R. Marinho, A morphologic two-stage approach\nfor automated optic disk detection in color eye fundus images, Pattern\nRecognition Letters. 34 (2013) 476–485. doi:10.1016/j.patrec.2012.12.011.\n[37] K. Sta̧por, A. Świtonski, R. Chrastek, G. Michelson, Segmentation of Fundus\nEye Images Using Methods of Mathematical Morphology for Glaucoma\nDiagnosis, in: D.B. Lomet (Ed.), Foundations of Data Organization and\nAlgorithms, Springer Berlin Heidelberg, Berlin, Heidelberg, 2004: pp. 41–48.\ndoi:10.1007/978-3-540-25944-2_6.\n[38] G.B. Kande, P.V. Subbaiah, T.S. Savithri, Segmentation of Exudates and Optic\nDisk in Retinal Images, in: 2008 Sixth Indian Conference on Computer Vision,\nGraphics & Image Processing, n.d.: pp. 535–542.\n[39] J.M. Seo, K.K. Kim, J.H. Kim, K.S. Park, H. Chung, Measurement of ocular\ntorsion using digital fundus image, in: The 26th Annual International\nConference of the IEEE Engineering in Medicine and Biology Society, n.d.: pp.\n1711–1713.\n[40] T. Walter, J. Klein, P. Massin, A. Erginay, A contribution of image processing\nto the diagnosis of diabetic retinopathy-detection of exudates in color\nfundus images of the human retina, IEEE Trans. Med. Imaging. 21 (2002)\n1236–1243. doi:10.1109/TMI.2002.806290.\n[41] L. Wan, M. Zeiler, S. Zhang, Y.L. Cun, R. Fergus, Regularization of Neural\nNetworks using DropConnect, (2013) 1058–1066.\n[42] M.L. Rasmussen, R. Broe, U. Frydkjaer-Olsen, B.S. Olsen, H.B. Mortensen, T.\nPeto, et al., Retinal vascular geometry and its association to microvascular\ncomplications in patients with type 1 diabetes: the Danish Cohort of\nPediatric Diabetes 1987 (DCPD1987), Graefes Arch Clin Exp Ophthalmol.\n(2016) 1–7. doi:10.1007/s00417-016-3454-3.\n[43] A. Triantafyllou, B. Al-Diri, P. Anyfanti, A. Hunter, S. Douma, 7D.08:\nDETECTING HYPERTENSIVE RETINOPATHY USING RETINAL VASCULAR\nGEOMETRY, J. Hypertens. 33 Suppl 1 (2015) e102.\ndoi:10.1097/01.hjh.0000467625.92095.c6.\n[44] M.S. Habib, B. al-Diri, A. Hunter, D.H. Steel, The association between retinal\nvascular geometry changes and diabetic retinopathy and their role in\nprediction of progression – an exploratory study, BMC Ophthalmol. 14 (2014)\n2057–11. doi:10.1186/1471-2415-14-89.\n[45] S.C. Kaufman, C.A. Wilson, F.L. Ferris, E. Stefánsson, L. Klombers, L.D.\ndx.doi.org/xx.xxxx/xxxxxxxxxxxxxxx\n24\nHubbard, Optic Disk Neovascularization and Retinal Vessel Diameter in\nDiabetic Retinopathy, American Journal of Ophthalmology. 106 (1988) 131–\n134.\n[46] C.E. Solarte, A. Awad, C. Wilson, A. Ells, Plus disease: Why is it important in\nretinopathy of prematurity? Middle East Afr J Ophthalmol. 17 (2010) 148–15.\ndoi:10.4103/0974-9233.63080.\n[47] C. Köse, C. İkiḃaş, A personal identification system using retinal vasculature\nin retinal fundus images, Expert Systems with Applications. 38 (2011) 13670–\n13681. doi:10.1016/j.eswa.2011.04.141.\n[48] E.M. Hoffmann, L.M. Zangwill, J.G. Crowston, R.N. Weinreb, Optic disk size\nand glaucoma, Survey of Ophthalmology. 52 (2007) 32–49.\ndoi:10.1016/j.survophthal.2006.10.002.\n[49] T.Y.P. Chui, Z. Zhong, H. Song, S.A. Burns, Foveal Avascular Zone and Its\nRelationship to Foveal Pit Shape, Optometry and Vision Science. 89 (2012)\n602–610. doi:10.1097/OPX.0b013e3182504227.\n[50] A.M. MANSOUR, A. SCHACHAT, G. BODIFORD, R. HAYMOND, FOVEAL\nAVASCULAR ZONE IN DIABETES MELLITUS, Retina. 13 (1993).\n[51] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature. 521 (2015) 436–444.\ndoi:10.1038/nature14539."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We have developed and trained a convolutional neural network to automatically and simultaneously segment optic disc, fovea and blood vessels. Fundus images were normalized before segmentation was performed to enforce consistency in background lighting and contrast. For every effective point in the fundus image, our algorithm extracted three channels of input from the point’s neighbourhood and forwarded the response across the 7-layer network. The output layer consists of four neurons, representing background, optic disc, fovea and blood vessels. In average, our segmentation correctly classified 92.68% of the ground truths (on the testing set from Drive database). The highest accuracy achieved on a single image was 94.54%, the lowest 88.85%. A single convolutional neural network can be used not just to segment blood vessels, but also optic disc and fovea with good accuracy.",
    "creator" : "Microsoft Word"
  }
}