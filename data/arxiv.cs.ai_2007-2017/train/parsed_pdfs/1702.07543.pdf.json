{
  "name" : "1702.07543.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n07 54\n3v 1\n[ cs\n.A I]\n2 4\nFe b\n20 17"
    }, {
      "heading" : "1 Introduction",
      "text" : "Knowledge graphs (KGs) store rich information of the real world in the form of graphs, which consist of nodes (entities) and labelled edges (relation types between entities) (e.g., (Trump,PresidentOf,USA)). This sort of structured data can be interpreted by computers and applied in various fields such as information retrieval [Hoffmann et al., 2011] and word sense disambiguation [Pritsker et al., 2015]. Although powerful in representing structured data, the symbolic nature of relations makes KGs, especially large-scale KGs, difficult to manipulate. Predicting missing entries (known as link prediction) is of great importance in knowledge graph. To do this task, vector space embeddings of knowledge graphs have\nbeen widely adopted. The key idea is to embed entities and relation types of a KG into a continuous vector space.\nMany approaches have been proposed to learn embeddings of entities and relations, such as TransE [Bordes et al., 2013], NTN [Socher et al., 2013], HOLE [Nickel et al., 2015], ComplEx [Trouillon et al., 2016], and so on. They, however, only focus on knowledge triples, ignoring rich knowledge from logic rules. Logic rules are taken as complex formulae constructed by combining atoms with logical connectives. To leverage logic rules in knowledge graph embeddings, [Wang et al., 2015; Wei et al., 2015; Rocktschel et al., 2015; Shu et al., 2016] propose to utilize both knowledge triples and logic rules for KB completion. In their works, however, logic rules need to be grounded. Each rule needs to be instantiated with concrete entities. A rule can be grounded into plenty of ground rules, since there are many entities connected by the same relation type in logic rules. As a result, the works cannot scale well to larger KGs since rules will be grounded with more entities. And also, They all neglect the properties of transitivity and antisymmetry of rules.\nFor example, suppose we have two rules: “CapitalOf ⇒ LocatedIn” and “LocatedIn ⇒ ContainedBy”, which indicates the relation that x is a capital of y implies another relation that x is located in y, and the relation that x is located in y implies x is contained by y. Provided that we know Paris is a capital of France from the knowledge base, we can infer that Paris is located in France and contained by France as well, according to the transitivity of rules. In addition, even though we know Paris is located in France, we cannot infer that Paris is a capital of France according to the antisymmetry of rules: “CapitalOf ⇒ LocatedIn” 6= “LocatedIn ⇒ CapitalOf”.\nTo leverage the properties of transitivity and antisymmetry of rules, we propose a novel knowledge representation learning model to capture the ordering of relations, and infer potential new relations based on the ordering of existing relations and properties of transitivity and antisymmetry of rules. We integrate knowledge graphs, existing relations and logic rules together to learn the knowledge graph embeddings. Logic rules are incorporated into relation type representations directly, rather than instantiated with concrete entities. The embeddings learned are therefore compatible not only with triples but also with rules, and the embeddings of relation types are approximately ordered. We call our learn-\ning approach TARE, short for Embedding knowledge graphs based on Transitivity and Antisymmetry of Rules.\nIn the remainder of the paper, we first review previous work related to our work. Then we formulate our problem and present our learning algorithm in detail. After that we evaluate our approach by comparing our approach with exiting state-of-the-art algorithms. Finally we conclude the paper with future work."
    }, {
      "heading" : "2 Related Work",
      "text" : "Many works have made great efforts on modelling knowledge graphs. Some works explain triples via latent representation of entities and relations such as tensor factorization [Nickel et al., 2011; Riedel et al., 2013; Chang et al., 2014] and multiway neural networks [Socher et al., 2013]. The key of relational latent feature models is that the relationship between entities can be derived from interactions of their latent features. Many ways are discovered to model these interactions. RESCAL [Nickel et al., 2011] is a relational latent feature model which explains triples via pairwise interactions of latent features. However, it requires a large number of parameters. TransE [Bordes et al., 2013] translates the latent feature representations via a relation-specific offset. Both entities and relations are projected into the same continuous low-dimensional vector space and relations are interpreted as translating operations between head and tail entities. TransE is efficient when modelling simple relations. To improve the performance of TransE on complicated relations, TransH [Wang et al., 2014], TransR [Lin et al., 2015] and TransD [Zhao et al., 2015] are proposed. Unfortunately, these models miss simplicity and efficiency of TransE. To combine the power of tensor product with the efficiency and simplicity of TransE, HOLE [Nickel et al., 2015] uses the circular correlation of vectors to represent pairs of entities. Circular correlation has the advantage, comparing to tensor product, that it does not increase the dimensionality of the composite representation. However, due to the asymmetry of circular correlation, HOLE is unable to deal with symmetric relation. Complex [Trouillon et al., 2016] makes use of embeddings with complex value and is able to handle a large number of binary relations, in particular symmetric and antisymmetric relations.\nThese models perform the embedding task based solely on triples contained in a KG. Recent work put growing interest in logic rules. [Wang et al., 2015; Wei et al., 2015] tries to utilize rules via integer linear programming or Markov logical networks. However, rules are modeled separately from embedding models and will not help obtaining better embeddings. [Rocktschel et al., 2015] proposes a joint model which injects first-order logic into embeddings. This work focus on the relation extraction task and created vector embeddings for entity pairs rather than individual entities. As a result, relations between unpaired entities cannot be effectively discovered. KALE-Joint [Shu et al., 2016] proposes a new approach which learns entity and relation embeddings by jointly modelling knowledge triples and logic rules. However, all logic rules need to be grounded in these works, and thus they do not scale well to larger KGs. Also, the embed-\ndings of the relation types in rules are not ordered, and thus the transitivity and antisymmetry of logic rules are missed.\nTo address above issues, we propose a novel approach which learns embeddings by combining logic rules with knowledge triples. Logic rules are incorporated into relation type representations directly and the embeddings of relation types in logic rules are approximately ordered to leverage the transitivity and antisymmetry of rules."
    }, {
      "heading" : "3 Problem Definition",
      "text" : "In this section, we give a formal definition of the problem. A knowledge graph G is defined as a set of triples of the form (s, r, o). s, o ∈ E denote the subject and object entity, respectively. r ∈ R denotes the relation type. E denotes the set of all entities andR denotes the set of all relation types in G.\nCreate from G a set of logic rules: ra ⇒ rb (in form of (ra, rb)) denotes that ra logically implies rb, which means that any two entities linked by relation ra should also be linked by relation rb; ra ∧ rb ⇒ rc (in form of (ra, rb, rc)) denotes that the conjunction of ra and rb logically implies rc: if e0 and e1 are linked by ra, e1 and e2 are linked by rb, then e0 and e2 are linked by rc. ra, rb, rc ∈ LR, where LR ⊆ R is the subset of relation types observed in logic rules.\nOur objective is to learn embeddings of entities, relations more precisely by approximately ordering the embeddings of relation types in logic rules, to predict relation types between entities. The embeddings are set in Rd and denoted with the same letters in boldface."
    }, {
      "heading" : "4 Our Model",
      "text" : ""
    }, {
      "heading" : "4.1 Restricted Triple Model(RTM)",
      "text" : "In RTM, we aim to embed entities and relation types to capture the correlations between them. The embeddings of relation types are restricted to be non-negative. Given two entities s, o ∈ E , the log-odd of the probability of the truth of fact (s, r, o) is:\nP (Ysro = 1|Θ) = σ(φ(s, r, o)) (1)\nwhere σ(x) = 1/(1+exp(−x)) denotes the logistic function; φ() is the energy function which is based on a factorization of the observed knowledges and indicates the correlation of relation r and the entity pair (s, o). Θ = {ei} ne i=1 ∪ {rk} nr k=1 denotes the the set of all embeddings ve, vr ∈ R d of the correspondingmodel, ne and nr is the number of entities and relation types in the given KG respectively. {Ysro}(s,r,o)∈Ω ∈ {−1, 1}|Ω| is a set of labels (true or false) of the triples, where Ω ∈ E ⊗ R ⊗ E . Ysro = 1 if (s, r, o) is positive. Otherwise, Ysro = −1.\nThe energy function φ(s, r, o; Θ) in our model is based on existing model Complex [Trouillon et al., 2016], in which complex vectors ve, vr ∈ R\nd are learned for each entity e ∈ E and each relation type r ∈ R. It models the score of a triple as:\n(2)\nφ(s, r, o) = Re(〈wreseo〉)\n= Re(\nd∑\ni=0\nwriesieoi)\n= d∑\ni=0\nRe(wri)Re(esi)Re(eoi)\n+\nd∑\ni=0\nIm(wri)Im(esi)Im(eoi)\n+\nd∑\ni=0\nIm(wri)Re(esi)Im(eoi)\n− d∑\ni=0\nIm(wri)Im(esi)Re(eoi)\nwhere wr, es, eo are complex vector embeddings for entities and relation types, Re(x) and Im(x) represent the real part and imaginary part of the complex vector embedding x, respectively. This function is antisymmetric when wr is purely imaginary (i.e.its real part is zero), and symmetric when wr is real. To approximately order the relation types in logic rules, we constrain the real part and imaginary part of the vector embeddings of relation types to be nonnegative and reduce the problem to Non-negative Matrix Factorization (NMF). There are many ways to solve Non-negative Matrix Factorization such as Multiplicative Update [Lee and Seung, 2000], Gradient based Update [Lin, 2007] and Alternating Non-negative Least Squares [Kim and Park, 2007; Cichocki and Zdunek, 2007]. In our model, we adopt the approach which updates the embeddings based on gradient. Translate the general constrained optimization problem into an unconstrained optimization problem. The embeddings of relation types are translated into unconstrained complex vectors as follows:\nRe(wr) = Re(q) (2), Im(wr) = Im(q) (2) (3)\nwhere x(2) denotes the element-wise square of the vector x. In other words, Re(wri) = Re(qi) 2 and Im(wri) = Im(qi) 2. Plug eq.(3) into eq.(2) , we get a new energy function:\n(4)\nφ(s, r, o) =\nd∑\ni=0\nRe(qi) 2Re(esi)Re(eoi)\n+\nd∑\ni=0\nIm(qi) 2Im(esi)Im(eoi)\n+ d∑\ni=0\nIm(qi) 2Re(esi)Im(eoi)\n−\nd∑\ni=0\nIm(qi) 2Im(esi)Re(eoi)\nThe model is trained by minimizing the negative loglikelihood of the logistic model with L2 regularization on the\nparametersΘ:\nLK = min Θ\n∑ log(1+exp(−Ysroφ(s, r, o)))+λ‖Θ‖ 2 (5)\nwhere λ is the regularization parameter. The real part and imaginary part of complex vector q are both unconstrained. Therefore, the novel objective function can be solved by applying Stochastic Gradient Descent (SGD) directly. The negative set of knowledges is generated by local closed world assumption(LCWA) proposed in [Krompa et al., 2015]."
    }, {
      "heading" : "4.2 Approximate Order Logic Model(AOLM)",
      "text" : "In AOLM, We aim to embed relation types to capture the ordering between relations. We work on two kinds of logic rules: ra ⇒ rb and ra ∧ rb ⇒ rc. Since there is no natural linear ordering on the set of complex numbers, we approximately order the complex vector embeddings by ordering the real part and imaginary part of the embeddings respectively. For their vector representations we require that the component-wise inequality holds:\nra ⇒ rb if and only if\nd∧\ni=0\nRe(rai) 6 Re(rbi)\nand\nd∧\ni=0\nIm(rai) 6 Im(rbi) (6)\nand\nra ∧ rb ⇒ rc if and only if\nd∧\ni=0\nRe(rai)Re(rbi) 6 Re(rci)\nand d∧\ni=0\nIm(rai)Im(rbi) 6 Im(rci)\n(7)\nfor all vectors with non-negative coordinates, where ∧\ndenotes the conjunction. Smaller coordinates imply higher position: ra ⇒ rb if and only if all entries of the real part and imaginary part of the vector embedding of ra are less than or equal to that of rb. The penalty for an ordered pair (ra, rb) of a given logic rule ra ⇒ rb is defined as follows:\n(8) F (ra, rb) = ‖max(0, Re(ra)−Re(rb))\n+max(0, Im(ra)− Im(rb))‖ 2\nwheremax(0, x) returns the greater one by element between 0 and x. Crucially, if ra ⇒ rb, F (ra, rb) = 0. F (ra, rb) is positive if ra ⇒ rb is not satisfied. F (ra, rb) = 0 if and only ifmax(0, Re(ra)−Re(rb)) = 0 andmax(0, Im(ra)− Im(rb)) = 0. That is, the real part of the embeddings of ra is less than or equal to that of rb, and the imaginary part of the embeddings of ra is less than or equal to that of rb. This satisfies the condition in eq.(6), and encourages the learned embeddings of relation types to satisfy the order properties of transitivity and antisymmetry. For example, if ra ⇒ rb and rb ⇒ rc, then ra ⇒ rc, since ∧d i=0 Re(rai) 6 Re(rbi) 6\nRe(rci) and ∧d\ni=0 Im(rai) 6 Im(rbi) 6 Im(rci), then F (ra, rc) = 0. For logic rule ra ∧ rb ⇒ rc the penalty for (ra, rb, rc) is:\nF (ra, rb, rc) = ‖max(0, Re(ra) ∗Re(rb)−Re(rc))\n+max(0, Im(ra) ∗Re(rb)− Im(rc))‖ 2\n(9)\nwhere ∗ denotes the element-wise multiplication of two vectors. For a ∗ b ∈ Rd:\n[a ∗ b]i = aibi (10)\nSimilarly, if ra ∧ rb ⇒ rc, F (ra, rb, rc) = 0. F (ra, rb, rc) is positive otherwise. F (ra, rb, rc) = 0 if and only if max(0, Re(ra)∗Re(rb)−Re(rc)) = 0 andmax(0, Im(ra)∗ Im(rb)− Im(rc)) = 0. That is, the multiplication of the real part of the embeddings of ra and rb is less than or equal to that of rc, and the multiplication of the imaginary part of the embeddings of ra and rb is less than or equal to that of rc. This satisfies the condition in eq.(7). The set of all relation types in logic rules is the subset of all relation types in the given KG. Therefore, the relation types in logic rules are translated similarly to eq.(3):\nRe(ra|b|c) = Re(qa|b|c) (2), Im(ra|b|c) = Im(qa|b|c) (2) (11) To learn the approximate order-embedding of relation types in logic rules, we could use a max-margin loss. For rule ra ⇒ rb:\n(12) LR = min\n∑\n(ra,rb)∈P\nF (ra, rb)\n+ ∑\n(r′ a ,r′ b )∈N\nmax(0, α− F (r′a, r ′ b))\nIf the rule is ra ∧ rb ⇒ rc:\n(13)\nLR = min ∑\n(ra,rb,rc)∈P\nF (ra, rb, rc)\n+ ∑\n(r′ a ,r′ b ,r′ c )∈N\nmax(0, α− F (r′a, r ′ b, r ′ c))\nwhere P andN denote the positive and negative sets of logic rules. If ra ⇒ rb, we construct negatives by replacing rb in the consequent with a random relation r ∈ R. If ra∧rb ⇒ rc, we construct negatives by replacing rc in the consequent with a random relation r ∈ R. α is a hyper-parameter of margin. F (ra, rb), F (ra, rb, rc) is the penalty function score of positive logic rule, and F (r′a, r ′ b), F (r ′ a, r ′ b, r ′ c) is that of negative logic rule calculated by eq.(8) or eq.(9). This loss encourages positive examples to have zero penalty, and negative examples to have penalty greater than a margin."
    }, {
      "heading" : "4.3 Global Objective",
      "text" : "With both knowledge triples and logic rules modelled, embeddings are learned by minimizing a global loss over this general representation:\nL = LK + LR (14)\nwhere LK is calculated by eq.(10) and LR is calculated by eq.(12) or eq.(13) . The embeddings of relation types are constrained to be non-negative, and are translated into unconstrained complex vector embeddings in loss function. Therefore, stochastic gradient descent (SGD) in mini-batch mode and AdaGrad [Duchi et al., 2010] for tuning the learning rate can be used to carry out the minimization directly. Embeddings learned are able to be compatible with both triples and logic rules. And the embeddings of relation types in logic rules are approximately ordered to capture the transitivity and antisymmetry of rules."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets and Experiment Settings",
      "text" : "Datasets We evaluate our model on knowledge graph completion using two commonly used large-scale knowledge graph datasets and a relational learning dataset: WN36 WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms called synsets. It provides short definitions and usage examples, and records a number of relations among these synonym sets or their members. WordNet can thus be seen as a combination of dictionary and thesaurus. The WN18 dataset is a subset of WordNet which contains 40,943 entities, 18 relation types and 151,442 binary triples. Since there are no logic rules among all relation types in WN18, we first add the reversed relations into training set. For example, * hypernym is the reversed relation type of hypernym. We add the triple (e1, ∗ hypernym, e0) into training set according to the positive triple observed (e0, hypernym, e1). Then we can find some rules in newly generated training set. We create 14 implication rules. FB15k Freebase is a large-scale and growing collaborative KG which provides general facts of the real world. For example, the triple (Barack Obama, Spouse, Michelle Obama) describes there is a relation Spouse between Barack Obama and Michelle Obama. The FB15k dataset is a subset of Freebase which contains 14,951 entities, 1,345 relation types, and 592,213 triples. We use original training, validation and test set splits as provided by [Bordes et al., 2013]. We create 200 implication rules. Countries The countries dataset provided by [Bouchard et al., 2015] consists of 244 countries, 22 subregions and 5 regions. Each country is located in exactly one region and subregion, each subregion is located in exactly one region, and each country can have a number of neighbour countries. We construct a set of triple relations from the raw data of two relations LocatedIn and NeighborOf . We create 2 conjunction rules. The statistics of WN36 and FB15k are listed in Table 1. Examples of rules created are shown in Table 2.\nExperiment Settings We use a grid search among the following parameters: d ∈ {20, 50, 100, 150, 200}, n ∈ {1, 2, 5, 10}, a ∈ {1.0, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01},λ ∈ {0.1, 0.03, 0.01, 0.003, 0.001, 0.0, 0.0003},m ∈ {2.0, 1.0, 0.5, 0.2, 0.05, 0.01} to find the optimal parameters, where d denotes the embedding size of the\nvectors of the entity and relation type representations; n denotes the number of negatives sampled for per positive triple observed in training set or logic rule; a denotes the initial learning rate which will be tuned during AdaGrad; λ denotes the L2 regularization parameter, andm is the margin between the positive logic rules and the negative logic rules."
    }, {
      "heading" : "5.2 Knowledge Base Completion",
      "text" : "Knowledge base completion aims to complete a triple (s, r, o) when one of s, r, o is missing. In the task of knowledge base completion, we compare our model with several state-of-art models including TransE [Bordes et al., 2013], TransR [Lin et al., 2015], HOLE [Nickel et al., 2015], ComplEx [Trouillon et al., 2016] and KALEJoint [Shu et al., 2016]. The former four models only focus on knowledge triples, and KALE-Joint learns embeddings by jointly modelling knowledge triples and logic rules. Rules need to be grounded and are not ordered in KALE-Joint.\nWe evaluate the performance of our model with Mean Reciprocal Rank (MRR) and top n (Hits@n) which have been widely used for evaluation in previous works. Replace the subject or object entity of each triple (s, r, o) in the testing set with each entity in the whole dataset: (s′, r, o) and (s, r, o′), where ∀s′, ∀o′ ∈ E . Afterwards, rank all candidate entities in the dataset according to their scores calculated by eq.(4) in ascending order. Mean Reciprocal Rank (MRR) and the ratio of correct entities ranked in top n (Hits@n) are the standard evaluation measures, which measure the quality of the ranking. They fall into two categories: raw and filtered. The filtered rankings are computed after filtering all other positive triples observed in the whole dataset, whereas the raw rankings do not filter these. We report both filtered and raw MRR, and filtered Hits@10, 3, 1 in Table 3 and Table 4 for the models.\nIt can be seen that TARE is able to outperform TransE, TransR, HOLE, ComplEx on MRR and Hits@ on WN36 and FB15k. This demonstrates the effectiveness of joint logic rules into knowledges. TARE largely outperforms KALEJoint, with a filtered MRR of 0.955 and 93.6% of Hits@1,\ncompared to 0.662 and 85.5% for KALE-Joint. This demonstrates the effectiveness of considering the transitivity and antisymmetry of logic rules."
    }, {
      "heading" : "5.3 Relational Learning",
      "text" : "We test the relational learning capabilities of our model on the countries dataset. Most of the test triples in the countries dataset can be inferred by directly applying logic rules on the training set. However, to evaluate our model, we do not use the pure logical inference. we split all countries randomly in train (80%), validation (10%), and test (10%) countries, then training, validation, and test set is composed of the relations which start from all countries in the training validation, and test countries respectively.\nRemove all triples of the form (c, LocatedIn, r) for each country c in the validation and test set. In the new set S1, (c, LocatedIn, r) can be predicted by LocatedIn ∧ LocatedIn ⇒ LocatedIn.\nBased on S1, remove (c, LocatedIn, s) for all countries in the validation and test set. In the new set S2, (c, LocatedIn, r) can be predicted by NeighborOf ∧ LocatedIn ⇒ LocatedIn.\nBased on S2, remove (cn, LocatedIn, r) for all neighbour countries cn of all countries in the validation and test set. In the new set S3 , (c, LocatedIn, r) can be predicted by NeighborOf ∧ LocatedIn ⇒ LocatedIn and LocatedIn ∧ LocatedIn ⇒ LocatedIn.\nThe prediction quality is measured by the area under the precision-recall curve (AUC-PR), we compute the mean AUC-PR after 10 fold cross-validation. The results are shown in Table 5. It can be seen that our model performs well in this task. It achieves 13.4% improvement on S2 and 19.3% improvement on S3."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this paper, we propose TARE model for representation learning of knowledge graphs by integrating existing relations and logic rules together. Logic rules are incorporated into relation type representations directly, rather than instantiated with concrete entities. We model logic rules by approximately ordering the relation types in logic rules to leverage the transitivity and antisymmetry of rules, and thus obtain better embeddings for entities and relation types. To be ordered, the vector embeddings of relation types are constrained to be non-negative, the general constrained optimization problem is translated into an unconstrained optimization problem in our model. In experiments, we evaluate our models on knowledge base completion and relational learning tasks. Experimental results show that TARE brings significant and consistent improvements over exiting state-of-the-art methods.\nFor future work, we would like to explore the following research directions: (1)more complex types of logic rules such as ¬ and ∨ would be modelled to obtain better performance. (2) logic rules can be extracted from text. There is richer information in text than triples, more logic rules can be obtained if we joint the information in text. (3) TARE only consider the order of relation types, the order over entities would also be helpful to obtain better embeddings."
    } ],
    "references" : [ {
      "title" : "Advances in Neural Information Processing Systems",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko. Translating embeddings for modeling multi-relational data" ],
      "venue" : "pages 2787–2795,",
      "citeRegEx" : "Bordes et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and Tho Trouillon",
      "author" : [ "Guillaume Bouchard", "Sameer Singh" ],
      "venue" : "On approximate reasoning capabilities of low-rank vector spaces.",
      "citeRegEx" : "Bouchard et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Typed tensor decomposition of knowledge bases for relation extraction",
      "author" : [ "Kai Wei Chang", "Wen Tau Yih", "Bishan Yang", "Chris Meek" ],
      "venue" : "EMNLP,",
      "citeRegEx" : "Chang et al.. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Regularized alternating least squares algorithms for non-negative matrix/tensor factorization",
      "author" : [ "Cichocki", "Zdunek", "2007] Andrzej Cichocki", "Rafal Zdunek" ],
      "venue" : "In Advances in Neural Networks",
      "citeRegEx" : "Cichocki et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cichocki et al\\.",
      "year" : 2007
    }, {
      "title" : "Journal of Machine Learning Research",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization" ],
      "venue" : "12(7):257–269,",
      "citeRegEx" : "Duchi et al.. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Knowledge-based weak supervision for information extraction of overlapping relations. In Meeting of the Association for Computational Linguistics: Human Language",
      "author" : [ "Hoffmann et al", "2011] Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2011
    }, {
      "title" : "Bioinformatics",
      "author" : [ "Hyunsoo Kim", "Haesun Park. Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis" ],
      "venue" : "23(12):1495,",
      "citeRegEx" : "Kim and Park. 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "and Volker Tresp",
      "author" : [ "Denis Krompa", "Stephan Baier" ],
      "venue" : "Type-constrained representation learning in knowledge graphs.",
      "citeRegEx" : "Krompa et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "In NIPS",
      "author" : [ "Daniel D. Lee", "H. Sebastian Seung. Algorithms for non-negative matrix factorization" ],
      "venue" : "pages 556–562,",
      "citeRegEx" : "Lee and Seung. 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "and X",
      "author" : [ "Y. Lin", "Z. Liu", "M. Sun", "Y. Liu" ],
      "venue" : "Zhu. Learning entity and relation embeddings for knowledge graph completion.",
      "citeRegEx" : "Lin et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Projected gradient methods for nonnegative matrix factorization",
      "author" : [ "C.J. Lin" ],
      "venue" : "Neural Computation, 19(10):2756",
      "citeRegEx" : "Lin. 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "ICML 2011",
      "author" : [ "Maximilian Nickel", "Volker Tresp", "Hans Peter Kriegel. A three-way model for collective learning on multi-relational data. In International Conference on Machine Learning" ],
      "venue" : "Bellevue, Washington, Usa, June 28 - July, pages 809–816,",
      "citeRegEx" : "Nickel et al.. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Holographic embeddings of knowledge graphs",
      "author" : [ "Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio" ],
      "venue" : "Computer Science,",
      "citeRegEx" : "Nickel et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "In NAACL-HLT",
      "author" : [ "Sebastian Riedel", "Limin Yao", "Andrew Mccallum", "Benjamin M Marlin. Relation extraction with matrix factorization", "universal schemas" ],
      "venue" : "page xxixxii,",
      "citeRegEx" : "Riedel et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Injecting logical background knowledge into embeddings for relation extraction",
      "author" : [ "Tim Rocktschel", "Sameer Singh", "Sebastian Riedel" ],
      "venue" : "North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Rocktschel et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Guo Li",
      "author" : [ "Guo Shu", "Wang Quan", "Lihong Wang", "Bin Wang" ],
      "venue" : "Jointly embedding knowledge graphs and logical rules.",
      "citeRegEx" : "Shu et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "R. Socher", "D. Chen", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "pages 464–469",
      "citeRegEx" : "Socher et al.. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "ric Gaussier",
      "author" : [ "Tho Trouillon", "Johannes Welbl", "Sebastian Riedel" ],
      "venue" : "and Guillaume Bouchard. Complex embeddings for simple link prediction.",
      "citeRegEx" : "Trouillon et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Knowledge graph embedding by translating on hyperplanes",
      "author" : [ "Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen" ],
      "venue" : "AAAI - Association for the Advancement of Artificial Intelligence,",
      "citeRegEx" : "Wang et al.. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "In International Conference on Artificial Intelligence",
      "author" : [ "Quan Wang", "Bin Wang", "Li Guo. Knowledge base completion using embeddings", "rules" ],
      "venue" : "pages 1859–1865,",
      "citeRegEx" : "Wang et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Large-scale knowledge base completion: Inferring via grounding network sampling over selected instances",
      "author" : [ "ZhuoyuWei", "Jun Zhao", "Kang Liu", "Zhenyu Qi", "Zhengya Sun", "Guanhua Tian" ],
      "venue" : "ŁŁ, pages 1331–1340,",
      "citeRegEx" : "Wei et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Shizhu He",
      "author" : [ "Jun Zhao", "Liheng Xu", "Kang Liu", "Guoliang Ji" ],
      "venue" : "Knowledge graph embedding via dynamic mapping matrix.",
      "citeRegEx" : "Zhao et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Many approaches have been proposed to learn embeddings of entities and relations, such as TransE [Bordes et al., 2013], NTN [Socher et al.",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : ", 2013], NTN [Socher et al., 2013], HOLE [Nickel et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : ", 2013], HOLE [Nickel et al., 2015], ComplEx [Trouillon et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : ", 2015], ComplEx [Trouillon et al., 2016], and so on.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : "To leverage logic rules in knowledge graph embeddings, [Wang et al., 2015; Wei et al., 2015; Rocktschel et al., 2015; Shu et al., 2016] propose to utilize both knowledge triples and logic rules for KB completion.",
      "startOffset" : 55,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "To leverage logic rules in knowledge graph embeddings, [Wang et al., 2015; Wei et al., 2015; Rocktschel et al., 2015; Shu et al., 2016] propose to utilize both knowledge triples and logic rules for KB completion.",
      "startOffset" : 55,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "To leverage logic rules in knowledge graph embeddings, [Wang et al., 2015; Wei et al., 2015; Rocktschel et al., 2015; Shu et al., 2016] propose to utilize both knowledge triples and logic rules for KB completion.",
      "startOffset" : 55,
      "endOffset" : 135
    }, {
      "referenceID" : 15,
      "context" : "To leverage logic rules in knowledge graph embeddings, [Wang et al., 2015; Wei et al., 2015; Rocktschel et al., 2015; Shu et al., 2016] propose to utilize both knowledge triples and logic rules for KB completion.",
      "startOffset" : 55,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "Some works explain triples via latent representation of entities and relations such as tensor factorization [Nickel et al., 2011; Riedel et al., 2013; Chang et al., 2014] and multiway neural networks [Socher et al.",
      "startOffset" : 108,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "Some works explain triples via latent representation of entities and relations such as tensor factorization [Nickel et al., 2011; Riedel et al., 2013; Chang et al., 2014] and multiway neural networks [Socher et al.",
      "startOffset" : 108,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "Some works explain triples via latent representation of entities and relations such as tensor factorization [Nickel et al., 2011; Riedel et al., 2013; Chang et al., 2014] and multiway neural networks [Socher et al.",
      "startOffset" : 108,
      "endOffset" : 170
    }, {
      "referenceID" : 16,
      "context" : ", 2014] and multiway neural networks [Socher et al., 2013].",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "RESCAL [Nickel et al., 2011] is a relational latent feature model which explains triples via pairwise interactions of latent features.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "TransE [Bordes et al., 2013] translates the latent feature representations via a relation-specific offset.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "To improve the performance of TransE on complicated relations, TransH [Wang et al., 2014], TransR [Lin et al.",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : ", 2014], TransR [Lin et al., 2015] and TransD [Zhao et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : ", 2015] and TransD [Zhao et al., 2015] are proposed.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "To combine the power of tensor product with the efficiency and simplicity of TransE, HOLE [Nickel et al., 2015] uses the circular correlation of vectors to represent pairs of entities.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "Complex [Trouillon et al., 2016] makes use of embeddings with complex value and is able to handle a large number of binary relations, in particular symmetric and antisymmetric relations.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "[Wang et al., 2015; Wei et al., 2015] tries to utilize rules via integer linear programming or Markov logical networks.",
      "startOffset" : 0,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "[Wang et al., 2015; Wei et al., 2015] tries to utilize rules via integer linear programming or Markov logical networks.",
      "startOffset" : 0,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "[Rocktschel et al., 2015] proposes a joint model which injects first-order logic into embeddings.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "KALE-Joint [Shu et al., 2016] proposes a new approach which learns entity and relation embeddings by jointly modelling knowledge triples and logic rules.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "The energy function φ(s, r, o; Θ) in our model is based on existing model Complex [Trouillon et al., 2016], in which complex vectors ve, vr ∈ R d are learned for each entity e ∈ E and each relation type r ∈ R.",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "There are many ways to solve Non-negative Matrix Factorization such as Multiplicative Update [Lee and Seung, 2000], Gradient based Update [Lin, 2007] and Alternating Non-negative Least Squares [Kim and Park, 2007; Cichocki and Zdunek, 2007].",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "There are many ways to solve Non-negative Matrix Factorization such as Multiplicative Update [Lee and Seung, 2000], Gradient based Update [Lin, 2007] and Alternating Non-negative Least Squares [Kim and Park, 2007; Cichocki and Zdunek, 2007].",
      "startOffset" : 138,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "There are many ways to solve Non-negative Matrix Factorization such as Multiplicative Update [Lee and Seung, 2000], Gradient based Update [Lin, 2007] and Alternating Non-negative Least Squares [Kim and Park, 2007; Cichocki and Zdunek, 2007].",
      "startOffset" : 193,
      "endOffset" : 240
    }, {
      "referenceID" : 7,
      "context" : "The negative set of knowledges is generated by local closed world assumption(LCWA) proposed in [Krompa et al., 2015].",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "Therefore, stochastic gradient descent (SGD) in mini-batch mode and AdaGrad [Duchi et al., 2010] for tuning the learning rate can be used to carry out the minimization directly.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "We use original training, validation and test set splits as provided by [Bordes et al., 2013].",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "Countries The countries dataset provided by [Bouchard et al., 2015] consists of 244 countries, 22 subregions and 5 regions.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "In the task of knowledge base completion, we compare our model with several state-of-art models including TransE [Bordes et al., 2013], TransR [Lin et al.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : ", 2013], TransR [Lin et al., 2015], HOLE [Nickel et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : ", 2015], HOLE [Nickel et al., 2015], ComplEx [Trouillon et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : ", 2015], ComplEx [Trouillon et al., 2016] and KALEJoint [Shu et al.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : ", 2016] and KALEJoint [Shu et al., 2016].",
      "startOffset" : 22,
      "endOffset" : 40
    } ],
    "year" : 2017,
    "abstractText" : "Representation learning of knowledge graphs encodes entities and relation types into a continuous low-dimensional vector space, learns embeddings of entities and relation types. Most existing methods only concentrate on knowledge triples, ignoring logic rules which contain rich background knowledge. Although there has been some work aiming at leveraging both knowledge triples and logic rules, they ignore the transitivity and antisymmetry of logic rules. In this paper, we propose a novel approach to learn knowledge representations with entities and ordered relations in knowledges and logic rules. The key idea is to integrate knowledge triples and logic rules, and approximately order the relation types in logic rules to utilize the transitivity and antisymmetry of logic rules. All entries of the embeddings of relation types are constrained to be non-negative. We translate the general constrained optimization problem into an unconstrained optimization problem to solve the non-negative matrix factorization. Experimental results show that our model significantly outperforms other baselines on knowledge graph completion task. It indicates that our model is capable of capturing the transitivity and antisymmetry information, which is significant when learning embeddings of knowledge graphs.",
    "creator" : "LaTeX with hyperref package"
  }
}