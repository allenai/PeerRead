{
  "name" : "1305.4204.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Machine learning on images using a string-distance",
    "authors" : [ "Uzi Chester", "Joel Ratsaby" ],
    "emails" : [ "uzichester@gmail.com,", "ratsaby@ariel.ac.il," ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Image classification research aims at finding representations of images that can be automatically used to categorize images into a finite set of classes. Typically, algorithms that classify images require some form of pre-processing of an image prior to classification. This process may involve extracting relevant features and segmenting images into sub-components based on some prior knowledge about their context [2,3].\nIn [1] we introduced a new distance function, called Universal Image Distance (UID), for measuring the distance between two images. The UID first transforms each of the two images into a string of characters from a finite alphabet and then uses the string distance of [4] to give the distance value between the images. According to [4] the distance between two strings x and y is a normalized difference between the complexity of the concatenation xy of the strings ?? Corresponding author.\nar X\niv :1\n30 5.\n42 04\nv1 [\ncs .L\nG ]\n1 7\nM ay\nand the minimal complexity of each of x and y. By complexity of a string x we mean the Lempel-Ziv complexity [5].\nIn the current paper we use the UID to create a finite-dimensional representation of an image. The ith component of this vector is like a feature that measures how different the image is from the ith image prototype. One of the advantages of the UID is that it can compare the distance between two images of different sizes and thus the prototypes which are representative of the different feature categories may be relatively small. For instance, the prototypes of an urban category can be small images of size 45 × 17 pixels of various parts of cities.\nIn this paper we introduce a process to convert the image into a labeled case (feature vector). Doing this systematically for a set of images each labeled by its class yields a data set which can be used for training any supervised and unsupervised learning algorithms. After describing our method in details we report on the accuracy results of several classification-learning algorithms on such data. As an example, we apply out method to satellite image classification and clustering.\nWe note that our process for converting an image into a finite dimensional feature vector is very straightforward and does not involve any domain knowledge about the images. In contrast to other image classification algorithms that extract features based on sophisticated mathematical analysis, such as, analyzing the texture, the special properties of an image, doing edge-detection, or any of the many other methods employed in the immense research-literature on image processing, our approach is very basic and universal. It is based on the complexity of the ’raw’ string-representation of an image. Our method extracts features automatically just by computing distances from a set of prototypes. It is therefore scalable and can be implemented using parallel processing techniques, such as on system-on-chip and FPGA hardware implementation [6,7,8].\nOur method extracts image features that are unbiased in the sense that they do not employ any heuristics in contrast to other common image-processing techniques[2]. The features that we extract are based on information implicit in the image and obtained via a complexity-based UID distance which is an information-theoretic measure. In our method, the feature vector representation of an image is based on the distance of the image from some fixed set of representative class-prototypes that are initially and only once picked by a human user running the learning algorithm.\nLet us now summarize the organization of the paper: in section 2 we review the definitions of LZ-complexity and a few string distances. In section 3 we define the UID distance. In section 4 we describe the algorithm for selecting class prototypes. In section 5 we describe the algorithm that generates a feature-vector representation of an image. In section 6 we discuss the classification learning method and in section we conclude by reporting on the classification accuracy results."
    }, {
      "heading" : "2 LZ-complexity and string distances",
      "text" : "The UID distance function [1] is based on the LZ- complexity of a string. The definition of this complexity follows [5]: let S,Q and R be strings of characters that are defined over the alphabet A. Denote by l(S) the length of S, and S(i) denotes the ith element of S. We denote by S(i, j) the substring of S which consists of characters of S between position i and j. An extension R = SQ of S is reproducible from S (denoted as S → R) if there exists an integer p ≤ l(S) such that Q(k) = R(p+ k − 1) for k = 1, . . . , l(S). For example, aacgt→ aacgtcgtcg with p = 3 and aacgt→ aacgtac with p = 2. R is obtained from S (the seed) by copying elements from the pth location in S to the end of S.\nA string S is producible from its prefix S(1, j) (denoted S(1, j) ⇒ R), if S(1, j) → S(1, l(S) − 1). For example, aacgt → aacgtac and aacgt → aacgtacc both with pointers p = 2. The production adds an extra ’different’ character at the end of the copying process which is not permitted in a reproduction.\nAny string S can be built using a production process where at its ith step we have the production S(1, hi−1)→ S(1, hi) where hi is the location of a character at the ithstep. (Note that S(1, 0)⇒ S(1, 1)).\nAn m-step production process of S results in parsing of S in which H(S) = S(1, h1) ·S(h1+1, h2) · · ·S(hm−1+1, hm) is called the history of S and Hi(S) = S(hi−1+1, hi) is called the ith component ofH(S). For example for S = aacgtacc we have H(S) = a · ac · g · t · acc as the history of S.\nIf S(1, hi) is not reproducible from S(1, hi−1) then Hi(S) is called exhaustive meaning that the copying process cannot be continued and the component should be halted with a single character innovation. Moreover, every string S has a unique exhaustive history [5].\nLet us denote by cH(S) the number of components in a history of S. then the LZ complexity of S is c(S) = min {cH(S)} where the minimum is over all histories of S. It can be shown that c(S) = cE(S) where cE(S) is the number of components in the exhaustive history of S.\nA distance for strings based on the LZ-complexity was introduced in [4] and is defined as follows: given two strings X and Y , denote by XY their concatenation then define\nd(X,Y ) := max {c(XY )− c(X), c(Y X)− c(Y )} .\nAs in [1] we use the following normalized distance function\nd∗∗(X,Y ) := c(XY )−min {c(X), c(Y )}\nmax {c(X), c(Y )} . (1)\nWe note in passing that (1) resembles the normalized compression distance of [9] except that here we do not use a compressor but rather the LZ-complexity c of a string. Note that d∗∗ is not a metric since it does not satisfy the triangle inequality and a distance of 0 implies that the two strings are close but not necessarily identical.\nThis d∗∗ is universal in the sense that it is not based on some specific representation of a string (such as the alphabet of symbols), nor on some heuristics that are common to other string distances, e.g., edit-distances [10]. Instead it only relies on the string’s LZ-complexity which is purely an information quantity independent of the string’s context or representation."
    }, {
      "heading" : "3 Universal Image Distance",
      "text" : "Based on d∗∗ we now define a distance between images. The idea is to convert each of two images I and J into strings X(I) and X(J) of characters from a finite alphabet of symbols. Once in string format, we use d∗∗(X(I), X(J)) as the distance between I and J . The details of this process are described in Algorithm 1 below.\nAlgorithm 1 UID distance measure 1. Input: two color images I, J in jpeg format (RGB representation) 2. Transform the RGB matrices into gray-scale by forming a weighted sum of the\nR, G, and B components according to the following formula: grayscaleV alue := 0.2989R + 0.5870G + 0.1140B, (used in Matlab©). Each pixel is now a single numeric value in the range of 0 to 255 . We refer to this set of values as the alphabet and denote it by A. 3. Scan each of the grayscale images from top left to bottom right and form a string of symbols from A. Denote the two strings by X(I) and X(J).\n4. Compute the LZ-complexities: c ( X(I) ) , c ( X(J) ) and the complexity of their con-\ncatenation c ( X(I)X(J) ) 5. Output: UID(I, J) := d∗∗ ( X(I), X(J) ) .\nIn the next section we describe the process of selecting the image prototypes."
    }, {
      "heading" : "4 Prototype selection",
      "text" : "In this section we describe the algorithm for selecting image prototypes from each of the feature categories . This process runs only once before the stage of converting the images into finite dimensional vectors, that is, it does not run once per image but once for all images. For an image I we denote by P ⊂ I a sub-image P of I where P can be any rectangular-image obtained by placing a window over the image I where the window is totally enclosed by I.\nAlgorithm 2 Prototypes selection 1. Input:M image feature categories, and a corpus CN of N unlabeled colored images {Ij}Nj=1 .\n2. for (i := 1 to M) do (a) Based on any of the images Ij in CN , let the user select Li prototype images{\nP (i) k }Li k=1 and set them as feature category i. Each prototype is contained by some image, P (i)k ⊂ Ij , and the size of P (i) k can vary, in particular it can be\nmuch smaller than the size of the images Ij , 1 ≤ j ≤ N . (b) end for;\n3. Enumerate all the prototypes into a single unlabeled set {Pk}Lk=1, where L =∑M i=1 Li and calculate the distance matrix H = [ UID ( X(Pk), X(Pl) )]L k=1,l=1\nwhere the (k, l) component of H is the UID distance between the unlabeled prototypes Pk and Pl.\n4. Run hierarchical clustering on H and obtain the associated dendrogram. 5. If there are M clusters with the ith cluster consisting of the prototypes { P\n(i) k }Li k=1\nthen terminate and go to step 7. 6. Else go to step 2. 7. Output: the set of labaled prototypes PL := {{ P (i) k }Li k=1 }M i=1 where L is the\nnumber of prototypes.\nFrom the theory of learning pattern recognition, it is known that the dimensionality M of a feature-vector is usually taken to be small compared to the data size N . A large L will obtain better feature representation accuracy of the image, but it will increase the time for running Algorithm 3 (described below).\nAlgorithm 2 convergence is based on the user’s ability to select good prototype images. We note that from our experiments this is easily achieved primarily because the UID permits to select prototypes P (i)k which are considerably smaller in size and hence simpler than the full images I(i)j . For instance, in our experiments we used 45× 17 pixels prototype size for all feature categories. This fact makes it easy for a user to quickly choose typical representative prototypes from every feature-cateory. This way it is easy to find informative prototypes, that is, prototypes that are distant when they are from different feature-categories and close when they are from the same feature category. Thus Algorithm 2 typically converges rapidly.\nAs an example, Figure 1a displays 12 prototypes selected by a user from a corpus of satellite images. The user labeled prototypes 1, . . . , 3 as representative of the feature category urban, prototypes 4, . . . , 6 as representatives of class sea, prototypes 7, . . . , 9 as representative of feature roads and prototypes 10, . . . , 12 as representative of feature arid. The user easily found these representative prototypes as it is easy to fit in a single picture of size 45 × 17 pixels a typical image. The dendrogram produced in step 4 of Algorithm 2 for these\nset of 12 prototypes is displayed in Figure 1b. It is seen that the following four clusters were found {10, 12, 11} , {1, 2, 3} , {7, 8, 9} , {4, 6, 5} which indicates that the prototypes selected in Algorithm 2 are good."
    }, {
      "heading" : "5 Image feature-representation",
      "text" : "In the previous section we described Algorithm 2 by which the prototypes are manually selected. This algorithm is now used to create a feature-vector representation of an image. It is described as Algorithm 3 below (in [1] we used a similar algorithm UIC to soft-classify an image whilst here we use it to only produce a feature vector representation of an image which later serves as a single labeled case for training any supervised learning algorithm or a single unlabeled case for training an unsupervised algorithm).\nAlgorithm 3 Feature-vector generation 1. Input: an image I to be represented on the following feature categories 1 ≤ i ≤M ,\nand given a set PL := {{ P (i) k }Li k=1 }M i=1 of labeled prototype images (obtained\nfrom Algorithm 2). 2. Initialize the count variables ci := 0, 1 ≤ i ≤M 3. Let W be a rectangle of size equal to the maximum prototype size. 4. Scan a window W across I from top-left to bottom-right in a non-overlapping way,\nand let the sequence of obtained sub-images of I be denoted as {Ij}mj=1. 5. for (j := 1 to m) do\n(a) for (i := 1 to M) do i. temp := 0 ii. for (k := 1 to Li) do\nA. temp := temp+ ( UID(Ij , P (i) k ) )2\nB. end for; iii. ri := √ temp\niv. end for; (b) Let i∗(j) := argmin1≤i≤Mri, this is the decided feature category for sub-image\nIj . (c) Increment the count, ci∗(j) := ci∗(j) + 1 (d) end for;\n6. Normalize the counts, vi := ci∑M l=1 cl , 1 ≤ i ≤M 7. Output: the normalized vector v(I) = [v1, . . . vM ] as the feature-vector representation for image I"
    }, {
      "heading" : "6 Supervised and unsupervised learning on images",
      "text" : "Given a corpus C of images and a set PL of labeled prototypes we use Algorithm 3 to generate the feature-vectors v(I) corresponding to each image I in C. At this point we have a database D of size equal to |C| which consists of feature vectors of all the images in C. This database can be used for unsupervised learning, for instance, discover interesting clusters of images. It can also be used for supervised learning provided that each of the cases can be labeled according to a value of some target class variable which in general may be different from the feature categories. Let us denote by T the class target variable and the database DT which consists of the feature vectors of D with the corresponding target class values. The following\nAlgorithm 4 Image classification learning 1. Input: (1) a target class variable T taking values in a finite set T of class categories,\n(2) a database DT which is based on the M -dimensional feature-vectors database D labeled with values in T (3) any supervised learning algorithm A 2. Partition DT using n-fold cross validation into Training and Testing sets of cases 3. Train and test algorithm A and produce a classifier C which maps the feature\nspace [0, 1]M into T 4. Define Image classifier as follows: given any image I the classification is F (I) :=\nC(v(I)), where v(I) is the M -dimensional feature vector of I 5. Output: classifier F"
    }, {
      "heading" : "7 Experimental setup and results",
      "text" : "We created a corpus C of 60 images of size 670×1364 pixels from GoogleEarth©of various types of areas (Figure 2 displays a few scaled-down examples of such images). From these images we let a user define four feature-categories: sea, urban, arid, roads and choose three relatively-small image-prototype of size 45×17 pixels from each feature-category, that is, we ran Algorithm 2 with M = 4 and Li = 3 for all 1 ≤ i ≤ M . We then ran Algorithm 3 to generate the feature-vectors for each image in the corpus and obtained a database D.\nWe then let the user label the images by a target variable Humidity with possible values 0 or 1. An image is labeled 0 if the area is of low humidity and labeled 1 if it is of higher humidity. We note that an image of a low humidity region may be in an arid (dry) area or also in the higher-elevation areas which are not necessarily arid. Since elevation information is not available in the featurecategories that the user has chosen then the classification problem is hard since the learning algorithm needs to discover the dependency between humid regions and areas characterized only by the above four feature categories.\nWith this labeling information at hand we produced the labeled database DHumidity. We used Algorithm 4 to learn an image classifier with target Humidity. As the learning algorithm A we used the following standard supervised algorithms: J48, CART , which learn decision trees, NaiveBayes and Multi-Layer Perceptrons (backpropagation) all of which are available in the WEKA©toolkit.\nWe performed 10-fold cross validation and compared their accuracies to a baseline classifier (denoted as ZeroR) which has a single decision that corresponds to the class value with the highest prior empirical probability. As seen in Table 1 (generated by WEKA©) J48, CART, NaiveBayes and Backpropagation performed with an accuracy of 86.5%, 81.5%, 89.25%, and 87.25%, respectively, compared to 50% achieved by the baseline ZeroR classifier. The comparison concludes that all three learning algorithms are significantly better than the baseline classifier, based on a T-test with a significance level of 0.05.\nNext, we performed clustering on the unlabeled database D. Using the kmeans algorithm, we obtained 3 significant clusters, shown in Table 2. The first\ncluster captures images of highly urban areas that are next to concentration of roads, highways and interchanges while the second cluster contains less populated (urban) areas in arid locations (absolutely no sea feature seen) with very low concentration of roads. The third cluster captures the coastal areas and here we can see that there can be a mixture of urban (but less populated than images of the first cluster) with roads and extremely low percentage of arid land.\nThe fact that such interesting knowledge can be extracted from raw images using our feature-extraction method is very significant since as mentioned above our method is fully automatic and requires no image analysis or any sophisticated preprocessing stages that are common in image pattern analysis."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We introduced a method for automatically defining and measuring features of colored images.The method is based on a universal image distance that is measured by computing the complexity of the string-representation of the two images and their concatenation. An image is represented by a feature-vector which consists of the distances from the image to a fixed set of small image prototypes, defined once by a user. There is no need for any sophisticated mathematical-based image analysis or pre-processing since the universal image distance regards the image as a string of symbols which contains all the relevant information of the image. The simplicity of our method makes it very attractive for fast and scalable implementation, for instance on a specific-purpose hardware acceleration chip. We applied our method to supervised and unsupervised machine learning on satellite images. The results show that standard machine learning algorithms perform well based on our feature-vector representation of the images."
    } ],
    "references" : [ {
      "title" : "Universal distance measure for images",
      "author" : [ "U.A. Chester", "J. Ratsaby" ],
      "venue" : "In Electrical Electronics Engineers in Israel (IEEEI),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Image Analysis, Classification and Change Detection in Remote Sensing: With Algorithms for Envi/Idl",
      "author" : [ "M.J. Canty" ],
      "venue" : "CRC/Taylor & Francis,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "Remote sensing and image interpretation",
      "author" : [ "T.M. Lillesand", "R.W. Kiefer", "J.W. Chipman" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "A new sequence distance measure for phylogenetic tree",
      "author" : [ "K. Sayood", "H.H. Otu" ],
      "venue" : "construction. Bioinformatics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "On the complexity of finite sequences",
      "author" : [ "J. Ziv", "A. Lempel" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1976
    }, {
      "title" : "Fpga-based data compressor based on prediction by partial matching",
      "author" : [ "J. Ratsaby", "V. Sirota" ],
      "venue" : "In Electrical Electronics Engineers in Israel (IEEEI),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "An fpga-based pattern classifier using data compression",
      "author" : [ "J. Ratsaby", "D. Zavielov" ],
      "venue" : "In Proc. of 26 IEEE Convention of Electrical and Electronics Engineers in Israel, Eilat,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Parallel processing algorithm for bayesian network inference",
      "author" : [ "G. Kaspi", "J. Ratsaby" ],
      "venue" : "In Electrical Electronics Engineers in Israel (IEEEI),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Clustering by compression",
      "author" : [ "R. Cilibrasi", "P. Vitanyi" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "Encyclopedia of Distances",
      "author" : [ "M. Deza", "E. Deza" ],
      "venue" : "Computer Science. Springer-Verlag,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We use the recently introduced Universal Image Distance (UID) [1] to compare the similarity between an image and a prototype image.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "This process may involve extracting relevant features and segmenting images into sub-components based on some prior knowledge about their context [2,3].",
      "startOffset" : 146,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "This process may involve extracting relevant features and segmenting images into sub-components based on some prior knowledge about their context [2,3].",
      "startOffset" : 146,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "In [1] we introduced a new distance function, called Universal Image Distance (UID), for measuring the distance between two images.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "The UID first transforms each of the two images into a string of characters from a finite alphabet and then uses the string distance of [4] to give the distance value between the images.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "According to [4] the distance between two strings x and y is a normalized difference between the complexity of the concatenation xy of the strings",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 4,
      "context" : "By complexity of a string x we mean the Lempel-Ziv complexity [5].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "It is therefore scalable and can be implemented using parallel processing techniques, such as on system-on-chip and FPGA hardware implementation [6,7,8].",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "It is therefore scalable and can be implemented using parallel processing techniques, such as on system-on-chip and FPGA hardware implementation [6,7,8].",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "It is therefore scalable and can be implemented using parallel processing techniques, such as on system-on-chip and FPGA hardware implementation [6,7,8].",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "Our method extracts image features that are unbiased in the sense that they do not employ any heuristics in contrast to other common image-processing techniques[2].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "The UID distance function [1] is based on the LZ- complexity of a string.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "The definition of this complexity follows [5]: let S,Q and R be strings of characters that are defined over the alphabet A.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "Moreover, every string S has a unique exhaustive history [5].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "A distance for strings based on the LZ-complexity was introduced in [4] and is defined as follows: given two strings X and Y , denote by XY their concatenation then define",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "As in [1] we use the following normalized distance function",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "We note in passing that (1) resembles the normalized compression distance of [9] except that here we do not use a compressor but rather the LZ-complexity c of a string.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : ", edit-distances [10].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "It is described as Algorithm 3 below (in [1] we used a similar algorithm UIC to soft-classify an image whilst here we use it to only produce a feature vector representation of an image which later serves as a single labeled case for training any supervised learning algorithm or a single unlabeled case for training an unsupervised algorithm).",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "Train and test algorithm A and produce a classifier C which maps the feature space [0, 1] into T 4.",
      "startOffset" : 83,
      "endOffset" : 89
    } ],
    "year" : 2013,
    "abstractText" : "We present a new method for image feature-extraction which is based on representing an image by a finite-dimensional vector of distances that measure how different the image is from a set of image prototypes. We use the recently introduced Universal Image Distance (UID) [1] to compare the similarity between an image and a prototype image. The advantage in using the UID is the fact that no domain knowledge nor any image analysis need to be done. Each image is represented by a finite dimensional feature vector whose components are the UID values between the image and a finite set of image prototypes from each of the feature categories. The method is automatic since once the user selects the prototype images, the feature vectors are automatically calculated without the need to do any image analysis. The prototype images can be of different size, in particular, different than the image size. Based on a collection of such cases any supervised or unsupervised learning algorithm can be used to train and produce an image classifier or image cluster analysis. In this paper we present the image feature-extraction method and use it on several supervised and unsupervised learning experiments for satellite image data.",
    "creator" : "LaTeX with hyperref package"
  }
}