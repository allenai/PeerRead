{
  "name" : "1706.01723.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A General-Purpose Tagger with Convolutional Neural Networks",
    "authors" : [ "Xiang Yu", "Ngoc Thang Vu" ],
    "emails" : [ "xiangyu@ims.uni-stuttgart.de", "falensaa@ims.uni-stuttgart.de", "thangvu@ims.uni-stuttgart.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recently, character composition models have shown great success in many NLP tasks, mainly because of their robustness in dealing with outof-vocabulary (OOV) words by capturing subword informations. Among the character composition models, bidirectional long short-term memory (LSTM) models and convolutional neural networks (CNN) are widely applied in many tasks, e.g. part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimarães, 2015), language modeling (Ling et al., 2015; Kim et al., 2016), machine translation (Costa-jussà and Fonollosa, 2016) and dependency parsing (Ballesteros et al., 2015; Yu and Vu, 2017).\nIn this paper, we present a state-of-the-art general-purpose tagger that uses CNNs both to compose word representations from characters and to encode context information for tagging.1 We show that the CNN model is more capable than the LSTM model for both functions, and more stable for unseen or unnormalized words, which is the main benefit of character composition models.\n1We will release the code in the camera-ready version.\nYu and Vu (2017) compared the performance of CNN and LSTM as character composition model for dependency parsing, and concluded that CNN performs better than LSTM. In this paper, we show that this is also the case for POS tagging. Furthermore, we extend the scope to morphological tagging and supertagging, in which the tag set is much larger and long-distance dependencies between words are more important.\nIn these three tagging tasks, we compare our tagger with the bilstm-aux tagger (Plank et al., 2016) and the CRF-based morphological tagger MarMot (Müller et al., 2013). The CNN tagger shows robust performance accross the three tasks, and achieves the highest average accuracy in all tasks. It (significantly) outperforms LSTM in morphological tagging, and outperforms both baselines in supertagging by a large margin.\nTo test the robustness of the taggers against the OOV problem, we also conduct experiments using artificially constructed unnormalized text by corrupting words in the normal dev set. Again, the CNN tagger outperforms the two baselines by a very large margin.\nTherefore we conclude that our CNN tagger is a robust state-of-the-art general-purpose tagger that can effectively compose word representation from characters and encode context information."
    }, {
      "heading" : "2 Model",
      "text" : "Our proposed CNN tagger has two main components: the character composition model and the context encoding model. Both components are essentially CNN models, capturing different levels of information: the first CNN captures morphological information from character n-grams, the second one captures contextual information from word n-grams. Figure 1 shows a diagram of both models of the tagger.\nar X\niv :1\n70 6.\n01 72\n3v 1\n[ cs\n.C L\n] 6\nJ un\n2 01\n7"
    }, {
      "heading" : "2.1 Character Composition Model",
      "text" : "The character composition model is similar to Yu and Vu (2017), where several convolution filters are used to capture character n-grams of different sizes. The outputs of each convolution filter are fed through a max pooling layer, and the pooling outputs are concatenated to represent the word."
    }, {
      "heading" : "2.2 Context Encoding Model",
      "text" : "The context encoding model captures the context information of the target word by scanning through the word representations of its context window. The word representation could be only word embeddings (~w), only composed vectors (~c) or the concatenation of both (~w + ~c)\nA context window consists of N words to both sides of the target word and the target word itself. To indicate the target word, we concatenate a binary feature to each of the word representations with 1 indicating the target and 0 otherwise, similar to Vu et al. (2016). Additional to the binary feature, we also concatenate a position embedding to encode the relative position of each context word, similar to Gehring et al. (2017)."
    }, {
      "heading" : "2.3 Hyper-parameters",
      "text" : "For the character composition model, we take a fixed input size of 32 characters for each word, with padding on both sides or cutting from the middle if needed. We apply four convolution filters with sizes of 3, 5, 7, and 9. Each filter has an output channel of 25 dimensions, thus the composed vector is 100-dimensional. We apply Gaussian noise with standard deviation of 0.1 is applied on the composed vector during training.\nFor the context encoding model, we take a context window of 15 (7 words to both sides of the target word) as input and predict the tag of the target word. We also apply four convolution filters with sizes of 2, 3, 4 and 5, each filter is stacked by another filter with the same size, and the output has\n128 dimensions, thus the context representation is 512-dimensional. We apply one 512-dimensional hidden layer with ReLU non-linearity before the prediction layer. We apply dropout with probability of 0.1 after the hidden layer during training.\nThe model is trained with averaged stochastic gradient descent with a learning rate of 0.1, momentum of 0.9 and mini-batch size of 100. We apply L2 regularization with a rate of 10−5 on all the parameters of the network except the embeddings."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Data",
      "text" : "We use treebanks from version 1.2 of Universal Dependencies2 (UD), and in the case of several treebanks for one language, we only use the canonical treebank. There are in total 22 treebanks, as in Plank et al. (2016).3 Each treebank splits into train, dev, and test sets, we use the dev sets for early stop, and test on the test sets."
    }, {
      "heading" : "3.2 Tasks",
      "text" : "We evaluate our method on three tagging tasks: POS tagging (POS), morphological tagging (MORPH) and supertagging (STAG).\nFor POS tagging we use Universal POS tags, which is an extension of Petrov et al. (2012). The universal tag set tries to capture the “universal” properties of words and facilitate cross-lingual learning. Therefore the tag set is very coarse and leaves out most of the language-specific properties to morphological features.\nMorphological tags encode the languagespecific morphological features of the words, e.g., number, gender, case. They are represented in the UD treebanks as one string which contains several key-value pairs of morphological features.4\nSupertags (Joshi and Bangalore, 1994) are tags that encode more syntactic information than standard POS tags, e.g. the head direction or the subcategorization frame. We use dependency-based supertags (Foth et al., 2006) which are extracted from the dependency treebanks. Adding such tags into feature models of statistical dependency parsers significantly improves their performance (Ouchi et al., 2014; Faleńska et al., 2015). Supertags can be designed with different levels of\n2http://universaldependencies.org 3We use all training data for Czech, while Plank et al. (2016) only use a subset. 4German, French and Indonesian do not have MORPH tags in UD-1.2, thus not evaluated in this task.\ngranularity. We use the standard Model 1 from Ouchi et al. (2014), where each tag consists of head direction, dependency label and dependent direction. Even with the basic supertag model, the STAG task is more difficult than POS and MORPH because it generally requires taking long-distance dependencies between words into consideration.\nWe select these tasks as examples for tagging applications because they differ strongly in tag set sizes. Generally, the POS set sizes for all the languages are no more than 17 and STAG set sizes are around 200. When treating morphological features as a string (i.e. not splitting into key-value pairs), the sizes of the MORPH tag sets range from about 100 up to 2000."
    }, {
      "heading" : "3.3 Setups",
      "text" : "As baselines to our models, we take the two stateof-the-art taggers MarMot5 (denoted as CRF) and bilstm-aux6 (denoted as LSTM). We train the taggers with the recommended hyper-parameters from the documentation.\nTo ensure a fair comparison (especially between LSTM and CNN), we generally treat the three tasks equally, and do not apply task-specific tuning on them, i.e., using the same features and same model hyper-parameters in each single task. Also, we do not use any pre-trained word embeddings.\nFor the LSTM tagger, we use the recommended hyper-parameters in the documentation7 including 64-dimensional word embeddings (~w) and 100- dimensional composed vectors (~c). We train the ~w, ~c and ~w + ~c models as in Plank et al. (2016). We train the CNN taggers with the same dimensionalities for word representations.\nFor the CRF tagger, we predict POS and MORPH jointly as in the standard setting for MarMot, which performs much better than with separate predictions, as shown in Müller et al. (2013) and in our preliminary experiments. Also, it splits the morphological tags into key-value pairs, whereas the neural taggers treat the whole string as a tag.8 We predict STAG as a separate task.\n5http://cistern.cis.lmu.de/marmot/ 6https://github.com/bplank/bilstm-aux 7We use the most recent version of the tagger and stacking 3 layers of LSTM as recommended. The average accuracy for POS in our evaluation is slightly lower than reported in the paper, presumably because of different versions of the tagger, but it does not influence the conclusion.\n8Since we use the CRF tagger as a non-neural baseline model, we prefer to use the settings which maximize its performances than the rigorously equal but suboptimal settings."
    }, {
      "heading" : "3.4 Results",
      "text" : "The test results for the three tasks are shown in Table 1 in three groups. The first group of seven columns are the results for POS, where both LSTM and CNN have three variations of input features: word only (~w), character only (~c) and both (~w + ~c). For MORPH and STAG, we only use the ~w + ~c setting for both LSTM and CNN.\nOn macro-average, three taggers perform close in the POS task, with the CNN tagger being slightly better. In the MORPH task, CNN is again slightly ahead of CRF, while LSTM is about 2 points behind. In the STAG task, CNN outperforms both taggers by a large margin: 2 points higher than LSTM and 8 points higher than CRF.\nWhile considering the input features of the LSTM and CNN taggers, both taggers perform close with only ~w as input, which suggests that the two taggers are comparable in encoding context for tagging POS. However, with only ~c, CNN performs much better than LSTM (95.54 vs. 92.61), and close to ~w + ~c (96.18). Also, ~c consistently outperforms ~w for all languages. This suggests that the CNN model alone is capable of learning most of the information that the word-level model can learn, while the LSTM model is not.\nThe more interesting cases are MORPH and STAG, where CNN performs much higher than LSTM. We hypothesize three possible reasons to explain the considerably large difference. First, the LSTM tagger may be more sensitive to hyperparameters and requires task specific tuning. We use the same setting which is tuned for the POS task, thus it underperforms in the other tasks. Second, the LSTM tagger may not deal well with large tag sets. The tag set size for MORPH are larger than POS in orders of magnitudes, especially for Czech, Basque, Finnish and Slovene, all of which have more than 1000 distinct MORPH tags in the training data, and the LSTM performs poorly on these languages. Third, the LSTM has theoretically unlimited access to all the tokens in the sentence, but in practice it might not learn the context as good as the CNN. In the LSTM model, the information of long-distance contexts will gradually fade away during the recurrence, whereas in the CNN model, all words are treated equally as long as they are in the context window. Therefore the LSTM underperforms in the STAG task, where the information from long-distance context is more important."
    }, {
      "heading" : "3.5 Unnormalized Text",
      "text" : "It is a common scenario to use a model trained with news data to process text from social media, which could include intentional or unintentional misspellings. Unfortunately, we do not have social media data to test the taggers. However, we design an experiment to simulate unnormalized text, by systematically editing the words in the dev sets with three operations: insertion, deletion and substitution. For example, if we modify a word abcdef at position 2 (0-based), the modified words would be abxcdef, abdef, and abxdef, where x is a random character from the alphabet of the language.\nFor each operation, we create a group of modified dev sets, where all words longer than two characters are edited by the operation with a probability of 0.25, 0.5, 0.75, or 1. For each language, we use the models trained on the normal training sets and predict POS for the three groups of modified dev set. The average accuracies are shown in Figure 2.\nGenerally, all models suffer from the increasing degrees of unnormalized texts, but CNN always suffers the least. In the extreme case where almost all words are unnormalized, CNN performs 4 to 8 points higher than LSTM and 4 to 11 points higher than CRF. This suggests that the CNN is more ro-\nbust to misspelt words. While looking into the specific cases of misspelling, CNN is more sensitive to insertion and deletion, while CRF and LSTM are more sensitive to substitution."
    }, {
      "heading" : "100 insertion",
      "text" : ""
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we propose a general-purpose tagger that uses two CNNs for both character composition and context encoding. On the universal dependency treebanks (v1.2), the tagger achieves state-of-the-art results for POS tagging and morphological tagging, and to the best of our knowledge, it also performs best for supertagging. The tagger works well across different tagging tasks without tuning the hyper-parameters, and it is also robust against unnormalized text."
    } ],
    "references" : [ {
      "title" : "Improved transition-based parsing by modeling characters instead of words with lstms",
      "author" : [ "Miguel Ballesteros", "Chris Dyer", "A. Noah Smith." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associa-",
      "citeRegEx" : "Ballesteros et al\\.,? 2015",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "Character-based neural machine translation",
      "author" : [ "R. Marta Costa-jussà", "R. José A. Fonollosa." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics,",
      "citeRegEx" : "Costa.jussà and Fonollosa.,? 2016",
      "shortCiteRegEx" : "Costa.jussà and Fonollosa.",
      "year" : 2016
    }, {
      "title" : "Boosting named entity recognition with neural character embeddings",
      "author" : [ "Cicero dos Santos", "Victor Guimarães." ],
      "venue" : "Proceedings of the Fifth Named Entity Workshop. Association for Computational Linguistics, pages 25–33.",
      "citeRegEx" : "Santos and Guimarães.,? 2015",
      "shortCiteRegEx" : "Santos and Guimarães.",
      "year" : 2015
    }, {
      "title" : "Learning character-level representations for part-ofspeech tagging",
      "author" : [ "Cicero dos Santos", "Bianca Zadrozny." ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning (ICML-14). pages 1818–1826.",
      "citeRegEx" : "Santos and Zadrozny.,? 2014",
      "shortCiteRegEx" : "Santos and Zadrozny.",
      "year" : 2014
    }, {
      "title" : "Stacking or supertagging for dependency parsing – what’s the difference? In Proceedings of the 14th International Conference on Parsing Technologies",
      "author" : [ "Agnieszka Faleńska", "Anders Björkelund", "Özlem Çetinoğlu", "Wolfgang Seeker." ],
      "venue" : "Association",
      "citeRegEx" : "Faleńska et al\\.,? 2015",
      "shortCiteRegEx" : "Faleńska et al\\.",
      "year" : 2015
    }, {
      "title" : "Guiding a constraint dependency parser with supertags",
      "author" : [ "Kilian A. Foth", "Tomas By", "Wolfgang Menzel." ],
      "venue" : "ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association",
      "citeRegEx" : "Foth et al\\.,? 2006",
      "shortCiteRegEx" : "Foth et al\\.",
      "year" : 2006
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin." ],
      "venue" : "arXiv preprint arXiv:1705.03122 .",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing",
      "author" : [ "Aravind K. Joshi", "Srinivas Bangalore." ],
      "venue" : "Proceedings of the 15th Conference on Computational Linguistics Volume 1. Association for Computational Linguis-",
      "citeRegEx" : "Joshi and Bangalore.,? 1994",
      "shortCiteRegEx" : "Joshi and Bangalore.",
      "year" : 1994
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush." ],
      "venue" : "Proceedings of the Thirtieth AAAI",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Chris Dyer", "W. Alan Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient Higher-Order CRFs for Morphological Tagging",
      "author" : [ "Thomas Müller", "Helmut Schmid", "Hinrich Schütze." ],
      "venue" : "In Proceedings of EMNLP.",
      "citeRegEx" : "Müller et al\\.,? 2013",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving Dependency Parsers with Supertags",
      "author" : [ "Hiroki Ouchi", "Kevin Duh", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2:",
      "citeRegEx" : "Ouchi et al\\.,? 2014",
      "shortCiteRegEx" : "Ouchi et al\\.",
      "year" : 2014
    }, {
      "title" : "A universal part-of-speech tagset",
      "author" : [ "Slav Petrov", "Dipanjan Das", "Ryan McDonald." ],
      "venue" : "Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12). European Language Resources Association (ELRA), Is-",
      "citeRegEx" : "Petrov et al\\.,? 2012",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2012
    }, {
      "title" : "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "author" : [ "Barbara Plank", "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Plank et al\\.,? 2016",
      "shortCiteRegEx" : "Plank et al\\.",
      "year" : 2016
    }, {
      "title" : "Combining recurrent and convolutional neural networks for relation classification",
      "author" : [ "Thang Ngoc Vu", "Heike Adel", "Pankaj Gupta", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Vu et al\\.,? 2016",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2016
    }, {
      "title" : "Character composition model with convolutional neural networks for dependency parsing on morphologically rich languages",
      "author" : [ "Xiang Yu", "Ngoc Thang Vu." ],
      "venue" : "arXiv preprint arXiv:1705.10814 .",
      "citeRegEx" : "Yu and Vu.,? 2017",
      "shortCiteRegEx" : "Yu and Vu.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimarães, 2015), language modeling (Ling et al.",
      "startOffset" : 29,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : ", 2016), named entity recognition (dos Santos and Guimarães, 2015), language modeling (Ling et al., 2015; Kim et al., 2016), machine translation (Costa-jussà and Fonollosa, 2016) and dependency parsing (Ballesteros et al.",
      "startOffset" : 86,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : ", 2016), named entity recognition (dos Santos and Guimarães, 2015), language modeling (Ling et al., 2015; Kim et al., 2016), machine translation (Costa-jussà and Fonollosa, 2016) and dependency parsing (Ballesteros et al.",
      "startOffset" : 86,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : ", 2016), machine translation (Costa-jussà and Fonollosa, 2016) and dependency parsing (Ballesteros et al.",
      "startOffset" : 29,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : ", 2016), machine translation (Costa-jussà and Fonollosa, 2016) and dependency parsing (Ballesteros et al., 2015; Yu and Vu, 2017).",
      "startOffset" : 86,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : ", 2016), machine translation (Costa-jussà and Fonollosa, 2016) and dependency parsing (Ballesteros et al., 2015; Yu and Vu, 2017).",
      "startOffset" : 86,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Yu and Vu (2017) compared the performance of CNN and LSTM as character composition model for dependency parsing, and concluded that CNN performs better than LSTM.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 13,
      "context" : "In these three tagging tasks, we compare our tagger with the bilstm-aux tagger (Plank et al., 2016) and the CRF-based morphological tagger MarMot (Müller et al.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : ", 2016) and the CRF-based morphological tagger MarMot (Müller et al., 2013).",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "The character composition model is similar to Yu and Vu (2017), where several convolution filters are used to capture character n-grams of different sizes.",
      "startOffset" : 46,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "nary feature to each of the word representations with 1 indicating the target and 0 otherwise, similar to Vu et al. (2016). Additional to the binary feature, we also concatenate a position embedding to encode the relative position of each context word, similar to Gehring et al.",
      "startOffset" : 106,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "Additional to the binary feature, we also concatenate a position embedding to encode the relative position of each context word, similar to Gehring et al. (2017).",
      "startOffset" : 140,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "There are in total 22 treebanks, as in Plank et al. (2016).3 Each treebank splits into train, dev, and test sets, we use the dev sets for early stop, and test on the test sets.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "For POS tagging we use Universal POS tags, which is an extension of Petrov et al. (2012). The",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "4 Supertags (Joshi and Bangalore, 1994) are tags that encode more syntactic information than standard POS tags, e.",
      "startOffset" : 12,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "We use dependency-based supertags (Foth et al., 2006) which are extracted from the dependency treebanks.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "Adding such tags into feature models of statistical dependency parsers significantly improves their performance (Ouchi et al., 2014; Faleńska et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : "Adding such tags into feature models of statistical dependency parsers significantly improves their performance (Ouchi et al., 2014; Faleńska et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 155
    }, {
      "referenceID" : 13,
      "context" : "org We use all training data for Czech, while Plank et al. (2016) only use a subset.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "We use the standard Model 1 from Ouchi et al. (2014), where each tag consists of head direction, dependency label and dependent direction.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "~ w, ~c and ~ w + ~c models as in Plank et al. (2016). We train the CNN taggers with the same dimensionalities for word representations.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "For the CRF tagger, we predict POS and MORPH jointly as in the standard setting for MarMot, which performs much better than with separate predictions, as shown in Müller et al. (2013) and in our preliminary experiments.",
      "startOffset" : 163,
      "endOffset" : 184
    } ],
    "year" : 2017,
    "abstractText" : "We present a general-purpose tagger based on convolutional neural networks (CNN), used for both composing word vectors and encoding context information. The CNN tagger is robust across different tagging tasks: without task-specific tuning of hyper-parameters, it achieves state-of-theart results in part-of-speech tagging, morphological tagging and supertagging. The CNN tagger is also robust against the outof-vocabulary problem, it performs well on artificially unnormalized texts.",
    "creator" : "LaTeX with hyperref package"
  }
}