{
  "name" : "1610.03321.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Keystroke dynamics as signal for shallow syntactic parsing",
    "authors" : [ "Barbara Plank" ],
    "emails" : [ "b.plank@rug.nl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We postulate that keystroke dynamics contain information about syntactic structure that can inform shallow syntactic parsing. To test this hypothesis, we explore labels derived from keystroke logs as auxiliary task in a multi-task bidirectional Long Short-Term Memory (bi-LSTM). Our results show promising results on two shallow syntactic parsing tasks, chunking and CCG supertagging. Our model is simple, has the advantage that data can come from distinct sources, and produces models that are significantly better than models trained on the text annotations alone."
    }, {
      "heading" : "1 Introduction",
      "text" : "As people produce text, they unconsciously produce loads of cognitive side benefit such as keystroke logs, brain activations or gaze patterns. However, natural language processing (NLP) hitherto almost exclusively relied on the written text itself. We argue that cognitive processing data contains potentially useful information beyond the linguistic signal and propose a novel source of information for shallow syntactic parsing, keystroke logs.\nKeystroke dynamics concerns a user’s typing pattern. When a person types, the latencies between successive keystrokes and their duration reflect the unique typing behavior of a person. Keystroke logs, the recordings of a user’s typing dynamics, are studied mostly in cognitive writing and translation process research to gain insights into the cognitive load involved in the writing process. However, until now this source has not yet been explored to inform NLP models.\nVery recent work has shown that cognitive processing data carries valuable signal for NLP. For instance, eye tracking data can inform sentence compression (Klerke et al., 2016) and gaze is predictive for part-of-speech (Barrett and Søgaard, 2015; Barrett et al., 2016).\nKeystroke logs have the distinct advantage over other cognitive modalities like eye tracking or brain scanning, that they are readily available and can be harvested easily, because they do not rely on any special equipment beyond a keyboard. Moreover, they are non-intrusive, inexpensive, and have the potential to offer continuous adaptation to specific users. Imagine integrating keystroke logging into (online) text processing tools.\nWe hypothesize that keystroke logs carry syntactic signal. Writing time between words can be seen as proxy of the planning process involved in writing, and thus represent structural information between words. To test our hypothesis, we evaluate a multi-task bidirectional Long-Short Term Memory (biLSTM) model that is—to the best of our knowledge—the first to exploit keystroke logs to improve NLP models. We test our model on two shallow syntactic tasks, chunking and CCG supertagging. The choice of tasks is motivated by the fact that writing research analyzes so-called bursts of writing (i.e., consecutive spans of text, cf. Section 2.2), which are related to shallow syntactic annotation.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/\nar X\niv :1\n61 0.\n03 32\n1v 1\n[ cs\n.C L\n] 1\n1 O\nct 2\n01 6\nTo exploit the keystroke log information we model it as auxiliary task in a multi-task setup (cf. Section 3). This setup has the advantage that the syntactic data and keystroke information can come from distinct sources, thus we are not restricted to the requirement of jointly labeled data (a corpus with both annotations). Our exploratory evaluation shows that little keystroke data suffices to improve a syntactic chunker on out-of-domain data, and that keystrokes also aid CCG tagging.\nContributions We are the first to use keystroke logs as signal to improve NLP models. In particular, the contributions of this paper are the following: i) we present a novel bi-LSTM model that exploits keystroke logs as auxiliary task for syntactic sequence prediction tasks; ii) we show that our model works well for two tasks, syntactic chunking and CCG supertagging, and iii) we make the code available at: https://github.com/bplank/coling2016ks."
    }, {
      "heading" : "2 Keystroke dynamics",
      "text" : "We see keystroke dynamics as providing a complementary view on the data beyond the linguistic signal, which can be harvested easily and is particularly attractive to build robust models for out-of-domain setups. Keystroke logging data can be seen as an instance of fortuitous data (Plank, 2016); it is side benefit of behavior that we want to exploit here. However, keystroke log is raw data, thus first needs to be refined before it can be used. Our idea is to treat the duration of pauses before words as a simple sequence labeling problem.\nWe first describe the process of obtaining auto-labeled data from raw keystroke logs, and then provide background and motivation for this choice. Section 3 then describes our model, i.e., by solving the keystroke sequence labeling problem jointly with shallow syntactic parsing tasks (chunking and CCG supertagging) we want to aid shallow parsing."
    }, {
      "heading" : "2.1 From keystroke logs to auxiliary labels",
      "text" : "While keystroke dynamics considers a number of timing metrics, such as holding time and time press and time release between every keystroke (p in Figure 1), in this study we are only concerned with the pause preceding a word (i.e., the third p in Figure 1).1 We here use a simple tokenization scheme. Whitespace delimits tokens, punctuation delimits sentence boundaries.\nAn example of pre-word pauses (in the remainder simply called pauses) calculated from our actual keylog data is shown in Table 1. If we take an arbitrary threshold of 500ms, the chunks indicated by the brackets are derived. This affirms that pre-word pauses carry constituency-like information.\nHowever, typing behavior of users differs, as illustrated in Figure 2. Hence, rather than finding a global metric we rely on per-user calculated aggregate statistics and discretize them to obtain autoderived labels, as explained next.\nWe calculate p, the pause duration before a token, and bin it into the following categories, using BIO encoding, where median is the per-user median and mad the median absolute deviation. In this way, we automatically gather labels from keystrokes representing pause durations.\nIn particular, we use the following discretization, i.e., a label for a token is calculated by:\n1Figure inspired by the figure in (Goodkind and Rosenberg, 2015).\nlabel = <m if p < median; <m+.5 if p < median+ 0.5 ∗mad; <m+1 if p < median+mad; >m1 else; O for punctuation symbols.\nThe label is further enriched with a prefix in BIO encoding style, motivated by the fact that we want to model spans of information. Punctuation symbols are treated as O, because due to their location at boundary positions the pause information varies highly. We leave treating punctuation separately as future work. Klerke et al. (2016) use a related encoding scheme to discretize fixation durations obtained from eye tracking data, however, in contrast to them we here use median-based measures which are better suited for such highly skewed data (Leys et al., 2013). An actual example of automatically labeled keystroke data is given in Table 2."
    }, {
      "heading" : "2.2 Background",
      "text" : "The major scientific interest in keystroke dynamics is that it provides a non-intrusive method for studying cognitive processes involved in writing. Keystroke logging has developed to a promising tool in writing research (Sullivan et al., 2006; Nottbusch et al., 2007; Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012), where time measurements—pauses, bursts and revisions (described below)—are studied as traces of the recursive nature of the writing process.\nIn its raw form, keystroke logs contain information on which key was pressed for how long (key, time press, time release). This data is then used to calculate between keystroke pause durations, such as pre-word pauses. It has been shown that pauses reflect the planning of the unit of text itself (Baaijen\net al., 2012) and that they correlate with clause and sentence boundaries (Spelman Miller and Sullivan, 2006). Writing research is interested in bursts of writing, defined as consecutive chunks of text produced and defined by a 2000ms time of inactivity (Wengelin, 2006), or revisions. Such a cutoff is rather arbitrary (Baaijen et al., 2012), and from our own experience results in long chunks. Taking writing research as a starting point, we postulate that keystrokes contain further fine-grained information that help identify syntactic chunks. We aim at a finer-grained representation, and transform user-based average statistics into automatically derived labels (cf. above).\nWe notice that the literature defines different ways to define a pause. Goodkind and Rosenberg (2015), coming from a stylometry background, use the difference between release time of the previous key and the timepress of the current key to calculate pre-word pause duration.2 In contrast, writing research (Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012) defines pauses as the start time of a keystroke until the start time of the next keystroke. We experimented with both types of pause definitions, and found the former slightly more robust, hence we use that to calculate pauses throughout this paper.\nIn order to get a better feel of word pause durations, we examine various properties of them. First, do we need to normalize pauses for word length? Goodkind and Rosenberg (2015) found a linear relationship between pre-word pauses and word length in their dataset. We calculated the correlation between word length and pauses in our dataset, but could not observe such a relation in our data (cf.\n2Goodkind sets negative pause durations (which can arise in this setup) to 0 (personal communication).\nFigure 3; plots for the other participants looks similar). Even if we break the data down by POS and calculate per-POS wise correlations we found no relation between pause duration and word length.3 Hence we do not normalize word pause durations. In addition, Figure 4 plots pauses for various part-ofspeech, showing that function POS (determiner, particles) are preceded by shorter pauses than content POS (we obtain similar plots for other participants).\nSecond, keystroke logs are presumably idiosyncratic, can we still use it? In fact, user keystroke biometrics are successfully used for author stylometry and verification in computer security research (Stewart et al., 2011; Monaco et al., 2013; Locklear et al., 2014). However, also eye tracking data like scanpaths (the resulting series of fixations and saccades in eye tracking) are known to be idiosyncratic (Kanan et al., 2015). Nevertheless it has been shown that gaze patterns help to inform NLP (Barrett and Søgaard, 2015; Klerke et al., 2016). We believe this is also the case for biometric keystroke logging data."
    }, {
      "heading" : "3 Tagging with bi-LSTMs",
      "text" : "We draw on the recent success of bi-directional recurrent neural network (bi-RNNs) (Graves and Schmidhuber, 2005), in particular Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997). They read the input sequences twice, in both directions. Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015)."
    }, {
      "heading" : "3.1 Bidirectional Long-Short Term Memory Models",
      "text" : "Our model is a a hierarchical bi-LSTM as illustrated in Figure 5. It takes as input word embeddings ~w concatenated with character embeddings obtained from the last two states (forward, backward) of running a lower-level bi-LSTM on the characters. Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016).\nIn more detail, our model is a context bi-LSTM taking as input word embeddings ~w. Character embeddings~c are incorporated via a hierarchical bi-LSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings ~w to form the input to the context bi-LSTM at the upper layers.\nFor the hidden layers, we use stacked LSTMs with h=3 layers. The 3-layer bi-LSTM and lower-level character bi-LSTM represents the shared structure between tasks. From the topmost (h=3) layer labels for the different tasks (e.g., chunking, pauses) are predicted using a softmax. In Figure 5, the main\n3POS annotations were obtained by looking up the possible tag of a token in English wiktionary (Li et al., 2012).\ntask (chunking or CCG tagging) is represented by the solid arrow, the auxiliary task (keystroke logs) is indicated by the dashed arrow.\nDuring training, we randomly sample a task and instance, and backpropagate the loss of the current instance through the shared deep network. In this way, we learn a joint model from distinct sources. Note that we also experimented with predicting the pause durations at lower levels (h=1), motivated by having lower-level tasks at lower layers in the network (Søgaard and Goldberg, 2016), however, we found the setup with both tasks at the outer layer more robust. Predicting all tasks at the outermost layer is the most commonly used form of multi-task learning in neural networks (Caruana, 1998; Collobert et al., 2011)."
    }, {
      "heading" : "4 Experiments",
      "text" : "We implement our model in CNN/pycnn.4 For all experiments, we use the same hyperparameters, set on a held-out portion of the CoNLL 2000 data, i.e., SGD with cross-entropy loss, no mini-batches, 30 epochs, default learning rate (0.1), 64 dimensions for word embeddings, 100 for character embeddings, random initialization for all embeddings, 100 hidden states, h = 3 stacked layers, Gaussian noise with σ=0.2. As training is stochastic, we use a fixed seed throughout (chosen and fixed upfront). No further unlabeled data is considered.\nDatasets An overview of the syntactic datasets considered in this paper is given in Table 3. For chunking, we use the original CoNLL data (Tjong Kim Sang and Buchholz, 2000) from WSJ (WSJ sections 15-18 as training data and section 20 as test data, containing 8936 and 2012 sentences, respectively).5 For testing we take out-of-domain data whenever available, to test the adaptability of the method to noisy out-of-domain data. For chunking we use Twitter data from Ritter (2011) (all, 2364 tweets) and Foster et al. (2011) (250 sentences), converted to chunks (Plank et al., 2014).\nThe CCG supertagging data also comes from WSJ (39604 training and 2407 test sentences). We unfortunately do not have access to out-of-domain test data, hence use the CCG tagging test set.\nThe keystroke logging data stems from students taking an actual test on spreadsheet modeling in a university course (Stewart et al., 2011; Monaco et al., 2013). The advantage of this dataset is that it contains free-text input.6 We used data from 38 users,7 which produced on average 250 sentences. The data totals to 7699 sentences.\nTo evaluate our models we use standard evaluation measures computed with conlleval.pl with default parameters, i.e., we report F1 on chunks and accuracy on CCG tags. Statistical significance is computed using the approximate randomization test (Noreen, 1989) using i = 1000 iterations and p-values are reported (Søgaard et al., 2014)."
    }, {
      "heading" : "4.1 Results",
      "text" : "Baseline model Both or baseline models are comparable to prior work, while being simpler. The results are summarized in Table 4. Our chunking baseline achieves an F1 of 93.21 on CoNLL, compared to the F1 of 93.88 of Suzuki and Isozaki (2008), who use a CRF and gold POS tags. We do not use any POS information. A similar bi-LSTM achieves 93.64 (Huang et al., 2015), however, additionally uses\n4https://github.com/clab/cnn 5http://www.cnts.ua.ac.be/conll2000/chunking/ 6In contrast to http://www.casmacat.eu/ data that logs revisions from MT post-editing. 7Disregarding users due to issues with logging (Stewart et al., 2011).\nPOS embeddings. Our baseline CCG supertagging model achieves 92.41, compared to the more complex model by Xu et al. (2015) achieving an accuracy of 93.00. Very recently even higher accuracies were reported, e.g. (Vaswani et al., 2016), however, in this exploratory paper we are interested in examining whether we find signal in keystroke data, and are not interested in beating the latest state-of-the-art.\nKeystroke pauses The aim of our experiments is to gauge whether through joint learning of shallow syntax and pause duration the system learns to generalize over the pause information and thus aids the syntactic signal.\nThe results in Table 5 support our hypothesis that keystroke dynamics contains useful information for chunking. We here report the average over models trained on a per-user basis, i.e., 38 participants. The results show that overall F1 chunking score improves over all datasets. For instance on the Ritter data, for 25/38 participants using their keystroke information as auxiliary task helps to improve overall chunking performance. However, if we combine all data and train a single model, performance degrades on chunking. We attribute this effect to the fact that the chunking data is relatively small, and higher amounts of keystroke data show signs of overfitting. In fact, similar effects have been shown in a multitask machine translation and parsing setup (Luong et al., 2016), where mixing coefficients were used to downplay the importance of the auxiliary parsing data that otherwise swamped the main task data. We leave examining task-specific weights for the loss for future work.\nIn contrast in CCG tagging, where we have more training data, we see a positive effect of using keystroke data when training a model that uses all keystroke data at once (concatenation of all keystroke data from all users), see last column in Table 5. Note that all results in Table 5 are significant."
    }, {
      "heading" : "5 Discussion",
      "text" : "To gain better insights into what the model has learned, Table 6 provides the per-label breakdown for chunking, Table 7 for CCG tagging. Most of the improvements come from noun phrases (NP) chunks. From manual inspection we determine that the model improves particularly on non-conventional spelling and fragmented noun phrases typical for Twitter, see examples given in Table 8.\nAs Table 6 shows, keystroke data also helps for verb phrases on one dataset. The current encoding is not so beneficial for PPs. Pauses before prepositions are short, as illustrated in Figure 4, and pauses often fall within segments in the auxiliary annotation, while prepositions constitute separate tokens in chunking. Hence, it is unsurprising that the model fares worse on PPs.\nWe believe that our pause encoding mainly captures structural information between words, less morphosyntactic information itself, i.e., that pauses are more informative of syntactic structure than of partof-speech. This intuition is in fact confirmed by initial experiments on POS tagging (UD English), which are less promising. We observe small improvements for low amounts of auxiliary data, however, they are not significant. Thus keystrokes seem to capture mostly structural shallow syntactic information, as confirmed in our experimental evaluation. However, this is only a first exploration, with one way of using keystroke logging data, but given our promising results, further experiments are warranted."
    }, {
      "heading" : "6 Related Work",
      "text" : "Keystroke logging has developed into a promising tool for research into writing (Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012), as time measurements can give insights into cognitive processes involved in writing (Nottbusch et al., 2007) or translation studies. In fact, most prior work that uses keystroke logs focuses on experimental research. For example, Hanoulle et al. (2015) study whether a bilingual glossary reduces the working time of professional translators. They consider pause\ndurations before terms extracted from keystroke logs and find that a bilingual glossary in the translation process of documentaries reduces the translators’ workload. Other translation research has combined eye-tracking data with keystroke logs to study the translation process (Carl et al., 2016). An analysis of users’ typing behavior was studied by Baba and Suzuki (2012). They collect keystroke logs of online users describing images to measure spelling difficulty. They analyzed corrected and uncorrected spelling mistakes in Japanese and English and found that spelling errors related to phonetic problems remain mostly unnoticed.\nGoodkind and Rosenberg (2015) is the only study prior to us that use keystroke loggings in NLP. In particular, they investigate the relationship between pre-word pauses and multi-word expressions and found within MWE pauses vary depending on cognitive task. We take a novel approach and learn keystroke patterns and use them to inform shallow syntactic parsing.\nA recent related line of work explores eye tracking data to inform sentence compression (Klerke et al., 2016) and induce part-of-speech (Barrett and Søgaard, 2015). Similarly, there are recent studies that predict fMRI activation from reading (Wehbe et al., 2014) or use fMRI data for POS induction (Bingel et al., 2016). The distinct advantage of keystroke dynamics is that it is easy to get, non-expensive and non-intrusive."
    }, {
      "heading" : "7 Conclusions",
      "text" : "Keystroke dynamics contain useful information for shallow syntactic parsing. Our model, a bi-LSTM, integrates keystroke data as auxiliary task, and outperforms models trained on the linguistic signal alone. We obtain promising results for two syntactic tasks, chunking and CCG supertagging. This warrants many directions for future research, e.g., using information from the non-linear writing process, which we here disregarded (e.g., revisions), evaluating on other languages and going to the full parsing task."
    }, {
      "heading" : "Acknowledgements",
      "text" : "I would like to thank Veerle Baaijen for insightful discussions, and the three anonymous reviewers as well as Héctor Martı́nez Alonso, Maria Barrett and Zeljko Agić for comments on earlier drafts of this paper. I acknowledge the Center for Information Technology of the University of Groningen for their support and for providing access to the Peregrine high performance computing (HPC) cluster, as well as NVIDIA corporation for supporting my research."
    } ],
    "references" : [ {
      "title" : "Keystroke analysis: Reflections on procedures and measures",
      "author" : [ "David Galbraith", "Kees de Glopper" ],
      "venue" : "Written Communication,",
      "citeRegEx" : "Baaijen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Baaijen et al\\.",
      "year" : 2012
    }, {
      "title" : "How are spelling errors generated and corrected?: a study of corrected and uncorrected spelling errors using keystroke logs",
      "author" : [ "Baba", "Suzuki2012] Yukino Baba", "Hisami Suzuki" ],
      "venue" : "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume",
      "citeRegEx" : "Baba et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Baba et al\\.",
      "year" : 2012
    }, {
      "title" : "Improved transition-based parsing by modeling characters instead of words with lstms",
      "author" : [ "Chris Dyer", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Ballesteros et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "Using reading behavior to predict grammatical functions",
      "author" : [ "Barrett", "Søgaard2015] Maria Barrett", "Anders Søgaard" ],
      "venue" : "In Workshop on Cognitive Aspects of Computational Language Learning",
      "citeRegEx" : "Barrett et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Barrett et al\\.",
      "year" : 2015
    }, {
      "title" : "Part-of-speech induction from eye-tracking data",
      "author" : [ "Joachim Bingel", "Anders Søgaard" ],
      "venue" : null,
      "citeRegEx" : "Barrett et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Barrett et al\\.",
      "year" : 2016
    }, {
      "title" : "Part-of-speech induction from fmri",
      "author" : [ "Maria Barrett", "Anders Søgaard" ],
      "venue" : null,
      "citeRegEx" : "Bingel et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bingel et al\\.",
      "year" : 2016
    }, {
      "title" : "Measuring the translation process",
      "author" : [ "Carl et al.2016] Michael Carl", "Isabel Lacruz", "Masaru Yamada", "Akiko Aizawa" ],
      "venue" : "In The 22nd Annual Meeting of the Association for Natural Language Processing",
      "citeRegEx" : "Carl et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Carl et al\\.",
      "year" : 2016
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana" ],
      "venue" : "In Learning to learn,",
      "citeRegEx" : "Caruana.,? \\Q1998\\E",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1998
    }, {
      "title" : "Natural language understanding with distributed representation. ArXiv, abs/1511.07916",
      "author" : [ "Kyunghyun Cho" ],
      "venue" : null,
      "citeRegEx" : "Cho.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cho.",
      "year" : 2015
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Transition-based dependency parsing with stack long short-term memory",
      "author" : [ "Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Dyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "From news to comments: Resources and benchmarks for parsing the language of Web 2.0",
      "author" : [ "Ozlem Cetinoglu", "Joachim Wagner", "Josef Le Roux", "Joakim Nivre", "Deirde Hogan", "Josef van Genabith" ],
      "venue" : "In IJCNLP",
      "citeRegEx" : "Foster et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Foster et al\\.",
      "year" : 2011
    }, {
      "title" : "Multilingual language processing from bytes",
      "author" : [ "Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya" ],
      "venue" : null,
      "citeRegEx" : "Gillick et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2015
    }, {
      "title" : "A primer on neural network models for natural language processing. ArXiv, abs/1510.00726",
      "author" : [ "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Goldberg.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2015
    }, {
      "title" : "Muddying the multiword expression waters: How cognitive demand affects multiword expression production",
      "author" : [ "Goodkind", "Rosenberg2015] Adam Goodkind", "Andrew Rosenberg" ],
      "venue" : "In Proceedings of MWE",
      "citeRegEx" : "Goodkind et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goodkind et al\\.",
      "year" : 2015
    }, {
      "title" : "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "author" : [ "Graves", "Schmidhuber2005] Alex Graves", "Jürgen Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Graves et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "The translation of documentaries: Can domain-specific, bilingual glossaries reduce the translators’ workload? an experiment involving professional translators",
      "author" : [ "Véronique Hoste", "Aline Remael" ],
      "venue" : "New Voices in Translation Studies,",
      "citeRegEx" : "Hanoulle et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hanoulle et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Bidirectional lstm-crf models for sequence tagging",
      "author" : [ "Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu" ],
      "venue" : "arXiv preprint arXiv:1508.01991",
      "citeRegEx" : "Huang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Humans have idiosyncratic and task-specific scanpaths for judging faces",
      "author" : [ "Dina NF Bseiso", "Nicholas A Ray", "Janet H Hsiao", "Garrison W Cottrell" ],
      "venue" : "Vision research,",
      "citeRegEx" : "Kanan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kanan et al\\.",
      "year" : 2015
    }, {
      "title" : "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations. ArXiv e-prints",
      "author" : [ "Kiperwasser", "Goldberg2016] E. Kiperwasser", "Y. Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Kiperwasser et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kiperwasser et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving sentence compression by learning to predict gaze",
      "author" : [ "Klerke et al.2016] Sigrid Klerke", "Yoav Goldberg", "Anders Søgaard" ],
      "venue" : null,
      "citeRegEx" : "Klerke et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Klerke et al\\.",
      "year" : 2016
    }, {
      "title" : "Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median",
      "author" : [ "Leys et al.2013] Christophe Leys", "Christophe Ley", "Olivier Klein", "Philippe Bernard", "Laurent Licata" ],
      "venue" : "Journal of Experimental Social Psychology,",
      "citeRegEx" : "Leys et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Leys et al\\.",
      "year" : 2013
    }, {
      "title" : "Wiki-ly supervised part-of-speech tagging",
      "author" : [ "Li et al.2012] Shen Li", "João Graça", "Ben Taskar" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2012
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis" ],
      "venue" : "In EMNLP",
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Fine-grained opinion mining with recurrent neural networks and word embeddings",
      "author" : [ "Liu et al.2015] Pengfei Liu", "Shafiq Joty", "Helen Meng" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Continuous authentication with cognition-centric text production and revision features",
      "author" : [ "Sathya Govindarajan", "Zdenka Sitova", "Adam Goodkind", "David Guy Brizan", "Andrew Rosenberg", "Vir V Phoha", "Paolo Gasti", "Kiran S Balagani" ],
      "venue" : "In Biometrics (IJCB),",
      "citeRegEx" : "Locklear et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Locklear et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-task sequence to sequence learning",
      "author" : [ "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser" ],
      "venue" : "In ICLR",
      "citeRegEx" : "Luong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2016
    }, {
      "title" : "Behavioral biometric verification of student identity in online course assessment and authentication of authors in literary works",
      "author" : [ "Monaco et al.2013] John V Monaco", "John C Stewart", "Sung-Hyuk Cha", "Charles C Tappert" ],
      "venue" : "In Biometrics: Theory, Applications and Systems (BTAS),",
      "citeRegEx" : "Monaco et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Monaco et al\\.",
      "year" : 2013
    }, {
      "title" : "Computer-intensive methods for testing hypotheses: an introduction",
      "author" : [ "Eric Noreen" ],
      "venue" : null,
      "citeRegEx" : "Noreen.,? \\Q1989\\E",
      "shortCiteRegEx" : "Noreen.",
      "year" : 1989
    }, {
      "title" : "From written word to written sentence production",
      "author" : [ "Rdiger Weingarten", "Said Sahel" ],
      "venue" : "Writing and cognition: Research and applications.,",
      "citeRegEx" : "Nottbusch et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Nottbusch et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning part-of-speech taggers with inter-annotator agreement loss",
      "author" : [ "Plank et al.2014] Barbara Plank", "Dirk Hovy", "Anders Søgaard" ],
      "venue" : "In EACL",
      "citeRegEx" : "Plank et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Plank et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "author" : [ "Plank et al.2016] Barbara Plank", "Anders Søgaard", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Plank et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Plank et al\\.",
      "year" : 2016
    }, {
      "title" : "What to do about non-standard (or non-canonical) language in NLP",
      "author" : [ "Barbara Plank" ],
      "venue" : "In KONVENS",
      "citeRegEx" : "Plank.,? \\Q2016\\E",
      "shortCiteRegEx" : "Plank.",
      "year" : 2016
    }, {
      "title" : "Extracting Knowledge from Twitter and The Web",
      "author" : [ "Alan Ritter" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Ritter.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ritter.",
      "year" : 2011
    }, {
      "title" : "Deep multi-task learning with low level tasks supervised at lower layers",
      "author" : [ "Søgaard", "Goldberg2016] Anders Søgaard", "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Søgaard et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Søgaard et al\\.",
      "year" : 2016
    }, {
      "title" : "What’s in a p-value in nlp? In CoNLL",
      "author" : [ "Anders Johannsen", "Barbara Plank", "Dirk Hovy", "Héctor Martı́nez Alonso" ],
      "venue" : null,
      "citeRegEx" : "Søgaard et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Søgaard et al\\.",
      "year" : 2014
    }, {
      "title" : "Keystroke logging: an introduction",
      "author" : [ "Spelman Miller", "Kirk PH Sullivan" ],
      "venue" : null,
      "citeRegEx" : "Miller et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2006
    }, {
      "title" : "An investigation of keystroke and stylometry traits for authenticating online test takers",
      "author" : [ "John V Monaco", "Sung-Hyuk Cha", "Charles C Tappert" ],
      "venue" : "In Biometrics (IJCB),",
      "citeRegEx" : "Stewart et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Stewart et al\\.",
      "year" : 2011
    }, {
      "title" : "Computer keystroke logging and writing: Methods and applications",
      "author" : [ "Eva Lindgren" ],
      "venue" : null,
      "citeRegEx" : "Sullivan and Lindgren,? \\Q2006\\E",
      "shortCiteRegEx" : "Sullivan and Lindgren",
      "year" : 2006
    }, {
      "title" : "Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data",
      "author" : [ "Suzuki", "Isozaki2008] Jun Suzuki", "Hideki Isozaki" ],
      "venue" : null,
      "citeRegEx" : "Suzuki et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Suzuki et al\\.",
      "year" : 2008
    }, {
      "title" : "Introduction to the conll2000 shared task: Chunking",
      "author" : [ "Tjong Kim Sang", "Sabine Buchholz" ],
      "venue" : "In CoNLL",
      "citeRegEx" : "Sang et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Sang et al\\.",
      "year" : 2000
    }, {
      "title" : "Keystroke logging in writing research: Observing writing processes with inputlog. GFL-German as a foreign language, 2(3):41–64",
      "author" : [ "Mariëlle Leijten", "Daphne Van Weijen" ],
      "venue" : null,
      "citeRegEx" : "Waes et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Waes et al\\.",
      "year" : 2009
    }, {
      "title" : "Supertagging with lstms",
      "author" : [ "Yonatan Bisk", "Kenji Sagae", "Ryan Musa" ],
      "venue" : null,
      "citeRegEx" : "Vaswani et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2016
    }, {
      "title" : "Part-of-speech tagging with bidirectional long short-term memory recurrent neural network. pre-print, abs/1510.06168",
      "author" : [ "Wang et al.2015] Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Aligning context-based statistical models of language with brain activity",
      "author" : [ "Wehbe et al.2014] Leila Wehbe", "Ashish Vaswani", "Kevin Knight", "Tom Mitchell" ],
      "venue" : null,
      "citeRegEx" : "Wehbe et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wehbe et al\\.",
      "year" : 2014
    }, {
      "title" : "Ccg supertagging with a recurrent neural network",
      "author" : [ "Xu et al.2015] Wenduan Xu", "Michael Auli", "Stephen Clark" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "For instance, eye tracking data can inform sentence compression (Klerke et al., 2016) and gaze is predictive for part-of-speech (Barrett and Søgaard, 2015; Barrett et al.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : ", 2016) and gaze is predictive for part-of-speech (Barrett and Søgaard, 2015; Barrett et al., 2016).",
      "startOffset" : 50,
      "endOffset" : 99
    }, {
      "referenceID" : 33,
      "context" : "Keystroke logging data can be seen as an instance of fortuitous data (Plank, 2016); it is side benefit of behavior that we want to exploit here.",
      "startOffset" : 69,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "(2016) use a related encoding scheme to discretize fixation durations obtained from eye tracking data, however, in contrast to them we here use median-based measures which are better suited for such highly skewed data (Leys et al., 2013).",
      "startOffset" : 218,
      "endOffset" : 237
    }, {
      "referenceID" : 21,
      "context" : "Klerke et al. (2016) use a related encoding scheme to discretize fixation durations obtained from eye tracking data, however, in contrast to them we here use median-based measures which are better suited for such highly skewed data (Leys et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 30,
      "context" : "Keystroke logging has developed to a promising tool in writing research (Sullivan et al., 2006; Nottbusch et al., 2007; Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012), where time measurements—pauses, bursts and revisions (described below)—are studied as traces of the recursive nature of the writing process.",
      "startOffset" : 72,
      "endOffset" : 180
    }, {
      "referenceID" : 0,
      "context" : "Keystroke logging has developed to a promising tool in writing research (Sullivan et al., 2006; Nottbusch et al., 2007; Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012), where time measurements—pauses, bursts and revisions (described below)—are studied as traces of the recursive nature of the writing process.",
      "startOffset" : 72,
      "endOffset" : 180
    }, {
      "referenceID" : 0,
      "context" : "Such a cutoff is rather arbitrary (Baaijen et al., 2012), and from our own experience results in long chunks.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "2 In contrast, writing research (Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012) defines pauses as the start time of a keystroke until the start time of the next keystroke.",
      "startOffset" : 32,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "Such a cutoff is rather arbitrary (Baaijen et al., 2012), and from our own experience results in long chunks. Taking writing research as a starting point, we postulate that keystrokes contain further fine-grained information that help identify syntactic chunks. We aim at a finer-grained representation, and transform user-based average statistics into automatically derived labels (cf. above). We notice that the literature defines different ways to define a pause. Goodkind and Rosenberg (2015), coming from a stylometry background, use the difference between release time of the previous key and the timepress of the current key to calculate pre-word pause duration.",
      "startOffset" : 35,
      "endOffset" : 497
    }, {
      "referenceID" : 0,
      "context" : "Such a cutoff is rather arbitrary (Baaijen et al., 2012), and from our own experience results in long chunks. Taking writing research as a starting point, we postulate that keystrokes contain further fine-grained information that help identify syntactic chunks. We aim at a finer-grained representation, and transform user-based average statistics into automatically derived labels (cf. above). We notice that the literature defines different ways to define a pause. Goodkind and Rosenberg (2015), coming from a stylometry background, use the difference between release time of the previous key and the timepress of the current key to calculate pre-word pause duration.2 In contrast, writing research (Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012) defines pauses as the start time of a keystroke until the start time of the next keystroke. We experimented with both types of pause definitions, and found the former slightly more robust, hence we use that to calculate pauses throughout this paper. In order to get a better feel of word pause durations, we examine various properties of them. First, do we need to normalize pauses for word length? Goodkind and Rosenberg (2015) found a linear relationship between pre-word pauses and word length in their dataset.",
      "startOffset" : 35,
      "endOffset" : 1192
    }, {
      "referenceID" : 38,
      "context" : "Second, keystroke logs are presumably idiosyncratic, can we still use it? In fact, user keystroke biometrics are successfully used for author stylometry and verification in computer security research (Stewart et al., 2011; Monaco et al., 2013; Locklear et al., 2014).",
      "startOffset" : 200,
      "endOffset" : 266
    }, {
      "referenceID" : 28,
      "context" : "Second, keystroke logs are presumably idiosyncratic, can we still use it? In fact, user keystroke biometrics are successfully used for author stylometry and verification in computer security research (Stewart et al., 2011; Monaco et al., 2013; Locklear et al., 2014).",
      "startOffset" : 200,
      "endOffset" : 266
    }, {
      "referenceID" : 26,
      "context" : "Second, keystroke logs are presumably idiosyncratic, can we still use it? In fact, user keystroke biometrics are successfully used for author stylometry and verification in computer security research (Stewart et al., 2011; Monaco et al., 2013; Locklear et al., 2014).",
      "startOffset" : 200,
      "endOffset" : 266
    }, {
      "referenceID" : 19,
      "context" : "However, also eye tracking data like scanpaths (the resulting series of fixations and saccades in eye tracking) are known to be idiosyncratic (Kanan et al., 2015).",
      "startOffset" : 142,
      "endOffset" : 162
    }, {
      "referenceID" : 21,
      "context" : "Nevertheless it has been shown that gaze patterns help to inform NLP (Barrett and Søgaard, 2015; Klerke et al., 2016).",
      "startOffset" : 69,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 245
    }, {
      "referenceID" : 24,
      "context" : "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 245
    }, {
      "referenceID" : 44,
      "context" : "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 245
    }, {
      "referenceID" : 18,
      "context" : "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 245
    }, {
      "referenceID" : 10,
      "context" : "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 245
    }, {
      "referenceID" : 2,
      "context" : "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 245
    }, {
      "referenceID" : 25,
      "context" : "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).",
      "startOffset" : 68,
      "endOffset" : 245
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015).",
      "startOffset" : 8,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015).",
      "startOffset" : 8,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 211
    }, {
      "referenceID" : 12,
      "context" : "Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 211
    }, {
      "referenceID" : 32,
      "context" : "Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 211
    }, {
      "referenceID" : 2,
      "context" : "Character embeddings~c are incorporated via a hierarchical bi-LSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016).",
      "startOffset" : 111,
      "endOffset" : 157
    }, {
      "referenceID" : 32,
      "context" : "Character embeddings~c are incorporated via a hierarchical bi-LSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016).",
      "startOffset" : 111,
      "endOffset" : 157
    }, {
      "referenceID" : 23,
      "context" : "POS annotations were obtained by looking up the possible tag of a token in English wiktionary (Li et al., 2012).",
      "startOffset" : 94,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "Predicting all tasks at the outermost layer is the most commonly used form of multi-task learning in neural networks (Caruana, 1998; Collobert et al., 2011).",
      "startOffset" : 117,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : "Predicting all tasks at the outermost layer is the most commonly used form of multi-task learning in neural networks (Caruana, 1998; Collobert et al., 2011).",
      "startOffset" : 117,
      "endOffset" : 156
    }, {
      "referenceID" : 31,
      "context" : "(2011) (250 sentences), converted to chunks (Plank et al., 2014).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 30,
      "context" : "For chunking we use Twitter data from Ritter (2011) (all, 2364 tweets) and Foster et al.",
      "startOffset" : 38,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "For chunking we use Twitter data from Ritter (2011) (all, 2364 tweets) and Foster et al. (2011) (250 sentences), converted to chunks (Plank et al.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 38,
      "context" : "The keystroke logging data stems from students taking an actual test on spreadsheet modeling in a university course (Stewart et al., 2011; Monaco et al., 2013).",
      "startOffset" : 116,
      "endOffset" : 159
    }, {
      "referenceID" : 28,
      "context" : "The keystroke logging data stems from students taking an actual test on spreadsheet modeling in a university course (Stewart et al., 2011; Monaco et al., 2013).",
      "startOffset" : 116,
      "endOffset" : 159
    }, {
      "referenceID" : 29,
      "context" : "Statistical significance is computed using the approximate randomization test (Noreen, 1989) using i = 1000 iterations and p-values are reported (Søgaard et al.",
      "startOffset" : 78,
      "endOffset" : 92
    }, {
      "referenceID" : 36,
      "context" : "Statistical significance is computed using the approximate randomization test (Noreen, 1989) using i = 1000 iterations and p-values are reported (Søgaard et al., 2014).",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 18,
      "context" : "64 (Huang et al., 2015), however, additionally uses https://github.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 38,
      "context" : "Disregarding users due to issues with logging (Stewart et al., 2011).",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 46,
      "context" : "88 Xu et al. (2015) 93.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 43,
      "context" : "(Vaswani et al., 2016), however, in this exploratory paper we are interested in examining whether we find signal in keystroke data, and are not interested in beating the latest state-of-the-art.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 45,
      "context" : "41, compared to the more complex model by Xu et al. (2015) achieving an accuracy of 93.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "In fact, similar effects have been shown in a multitask machine translation and parsing setup (Luong et al., 2016), where mixing coefficients were used to downplay the importance of the auxiliary parsing data that otherwise swamped the main task data.",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "6 Related Work Keystroke logging has developed into a promising tool for research into writing (Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012), as time measurements can give insights into cognitive processes involved in writing (Nottbusch et al.",
      "startOffset" : 95,
      "endOffset" : 156
    }, {
      "referenceID" : 30,
      "context" : ", 2012), as time measurements can give insights into cognitive processes involved in writing (Nottbusch et al., 2007) or translation studies.",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : ", 2009; Baaijen et al., 2012), as time measurements can give insights into cognitive processes involved in writing (Nottbusch et al., 2007) or translation studies. In fact, most prior work that uses keystroke logs focuses on experimental research. For example, Hanoulle et al. (2015) study whether a bilingual glossary reduces the working time of professional translators.",
      "startOffset" : 8,
      "endOffset" : 284
    }, {
      "referenceID" : 6,
      "context" : "Other translation research has combined eye-tracking data with keystroke logs to study the translation process (Carl et al., 2016).",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 21,
      "context" : "A recent related line of work explores eye tracking data to inform sentence compression (Klerke et al., 2016) and induce part-of-speech (Barrett and Søgaard, 2015).",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 45,
      "context" : "Similarly, there are recent studies that predict fMRI activation from reading (Wehbe et al., 2014) or use fMRI data for POS induction (Bingel et al.",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : ", 2014) or use fMRI data for POS induction (Bingel et al., 2016).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Other translation research has combined eye-tracking data with keystroke logs to study the translation process (Carl et al., 2016). An analysis of users’ typing behavior was studied by Baba and Suzuki (2012). They collect keystroke logs of online users describing images to measure spelling difficulty.",
      "startOffset" : 112,
      "endOffset" : 208
    }, {
      "referenceID" : 5,
      "context" : "Other translation research has combined eye-tracking data with keystroke logs to study the translation process (Carl et al., 2016). An analysis of users’ typing behavior was studied by Baba and Suzuki (2012). They collect keystroke logs of online users describing images to measure spelling difficulty. They analyzed corrected and uncorrected spelling mistakes in Japanese and English and found that spelling errors related to phonetic problems remain mostly unnoticed. Goodkind and Rosenberg (2015) is the only study prior to us that use keystroke loggings in NLP.",
      "startOffset" : 112,
      "endOffset" : 500
    } ],
    "year" : 2016,
    "abstractText" : "Keystroke dynamics have been extensively used in psycholinguistic and writing research to gain insights into cognitive processing. But do keystroke logs contain actual signal that can be used to learn better natural language processing models? We postulate that keystroke dynamics contain information about syntactic structure that can inform shallow syntactic parsing. To test this hypothesis, we explore labels derived from keystroke logs as auxiliary task in a multi-task bidirectional Long Short-Term Memory (bi-LSTM). Our results show promising results on two shallow syntactic parsing tasks, chunking and CCG supertagging. Our model is simple, has the advantage that data can come from distinct sources, and produces models that are significantly better than models trained on the text annotations alone.",
    "creator" : "LaTeX with hyperref package"
  }
}