{
  "name" : "1511.07211.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Noisy Submodular Maximization via Adaptive Sampling with Applications to Crowdsourced Image Collection Summarization",
    "authors" : [ "Adish Singla", "Sebastian Tschiatschek", "Andreas Krause" ],
    "emails" : [ "adish.singla@inf.ethz.ch", "sebastian.tschiatschek@inf.ethz.ch", "krausea@ethz.ch" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Many applications involve the selection of a subset of items, e.g., summarization of content on the web. Typically, the task is to select a subset of items of limited cardinality with the goal of maximizing their utility. This utility is often measured via properties like diversity, information, relevance or coverage. Submodular set functions naturally capture the fore-mentioned notions of utility. Intuitively, submodular functions are set functions that satisfy a natural diminishing returns property. They have been widely used in diverse applications, including content summarization and recommendations, sensor placement, viral marketing, and numerous machine learning and computer vision tasks (Krause and Guestrin 2008; Bilmes 2015).\nSummarization of image collections. One of the motivating applications, which is the subject of our experimental evaluation, is summarization of image collections. Given a collection of images, say, from Venice on Flickr, we would\nCopyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nlike to select a small subset of these images that summarizes the theme Cathedrals in Venice, cf., Figure 1. We cast this summarization task as the problem of maximizing a submodular set function f under cardinality constraints.\nSubmodular maximization. Usually, it is assumed that f is known, i.e., f can be evaluated exactly by querying an oracle. In this case, a greedy algorithm is typically used for maximization. The greedy algorithm builds up a set of items by picking those of highest marginal utility in every iteration, given the items selected that far. Despite its greedy nature, this algorithm provides the best constant factor approximation to the optimal solution computable in polynomial time (Nemhauser, Wolsey, and Fisher 1978).\nMaximization under noise. However, in many realistic applications, the function f is not known and can only be evaluated up to (additive) noise. For instance, for the image summarization task, (repeatedly) querying users’ feedback in form of clicks or ratings on the individual images or image-sets can provide such noisy evaluations. There are other settings for which marginal gains are hard to compute exactly, e.g., computing marginal gains of nodes in viral marketing applications (Kempe, Kleinberg, and Tardos 2003) or conditional information gains in feature selection tasks (Krause and Guestrin 2005). In such cases, one can apply a naive uniform sampling approach to estimate all marginal gains up to some error and apply the standard greedy algorithm. While simple, this uniform sampling approach could have high sample complexity, rendering it impractical for real-world applications. In this work, we propose to use adaptive sampling strategies to reduce sample complexity while maintaining high quality of the solutions.\nOur approach for adaptive sampling. The first key insight for efficient adaptive sampling is that we need to estimate the marginal gains only up to the necessary confidence to decide for the best item to select next. However, if the difference in the marginal gains of the best item and the second best item is small, this approach also suffers from high sample complexity. To overcome this, we exploit our second key insight that instead of focusing on selecting the single best item, it is sufficient to select an item from a small subset of any size l ∈ {1, . . . , k} of high quality items, where k is the cardinality constraint. Based on these insights, we propose a novel exploration module – TOPX – that proposes good elements by adaptively sampling noisy function evaluations.\nar X\niv :1\n51 1.\n07 21\n1v 2\n[ cs\n.A I]\n1 D\nec 2\n01 5\nOur contributions. Our main contributions are: • We provide a greedy algorithm EXPGREEDY for sub-\nmodular maximization of a function via noisy evaluations. The core part of this algorithm, the exploration module TOPX, is invoked at every iteration and implements a novel adaptive sampling scheme for efficiently selecting a small set of items with high marginal utilities.\n• Our theoretical analysis and experimental evaluation provide insights of how to trade-off the quality of subsets selected by EXPGREEDY against the number of evaluation queries performed by TOPX.\n• We demonstrate the applicability of our algorithms in a real-world application of crowdsourcing the summarization of image collections via eliciting crowd preferences based on (noisy) pairwise comparisons, cf., Figure 1."
    }, {
      "heading" : "Related Work",
      "text" : "Submodular function maximization (offline). Submodular set functions f(S) arise in many applications and, therefore, their optimization has been studied extensively. For example, a celebrated result of Nemhauser, Wolsey, and Fisher (1978) shows that non-negative monotone submodular functions under cardinality constraints can be maximized up to a constant factor of (1 − 1/e) by a simple greedy algorithm. Submodular maximization has furthermore been studied for a variety of different constraints on S, e.g., matroid constraints or graph constraints (Krause et al. 2006; Singh, Krause, and Kaiser 2009), and in different settings, e.g., distributed optimization (Mirzasoleiman et al. 2013). When the function f can only be evaluated up to (additive) noise, a naive uniform sampling approach has been employed to estimate all the marginal gains (Kempe, Kleinberg, and Tardos 2003; Krause and Guestrin 2005), an approach that could have high sampling complexity.\nLearning submodular functions. One could approach the maximization of an unknown submodular function by first learning the function of interest and subsequently optimizing it. Tschiatschek et al. (2014) present an approach for learning linear mixtures of known submodular component functions for image collection summarization. Our work is complimentary to that approach wherein we directly target the subset selection problem without learning the underlying function. In general, learning submodular functions from\ndata is a difficult task — Balcan and Harvey (2011) provide several negative results in a PAC-style setting.\nSubmodular function maximization (online). Our work is also related to online submodular maximization with (opaque) bandit feedback. Streeter and Golovin (2008) present approaches for maximizing a sequence of submodular functions in an online setting. Their adversarial setting forces them to use conservative algorithms with slow convergence. Yue and Guestrin (2011) study a more restricted setting, where the objective is an (unknown) linear combination of known submodular functions, under stochastic noise. While related in spirit, these approaches aim to minimize cumulative regret. In contrast, we aim to identify a single good solution performing as few queries as possible — the above mentioned results do not apply to our setting.\nBest identification (pure exploration bandits). In exploratory bandits, the learner first explores a set of actions under time / budget constraints and then exploits the gathered information by choosing the estimated best action (top1 identification problem) (Even-Dar, Mannor, and Mansour 2006; Bubeck, Munos, and Stoltz 2009). Beyond best individual actions, Zhou, Chen, and Li (2014) design an ( , δ)PAC algorithm for the topm identification problem where the goal is to return a subset of size m whose aggregate utility is within compared to the aggregate utility of them best actions. Chen et al. (2014) generalize the problem by considering combinatorial constraints on the subsets that can be selected, e.g., subsets must be size m, represent matchings, etc. They present general learning algorithms for all decision classes that admit offline maximization oracles. Dueling bandits are variants of the bandit problem where feedback is limited to relative preferences between pairs of actions. The best-identification problem is studied in this weaker information model for various notions of ranking models (e.g., Borda winner), cf., Busa-Fekete and Hüllermeier (2014). However, in contrast to our work, the reward functions considered by Chen et al. (2014) and other existing algorithms are modular, thus limiting the applicability of these algorithms. Our work is also related to contemporary work by Hassidim and Singer (2015), who treat submodular maximization under noise. While their algorithms apply to persistent noise, their technique is computationally demanding, and does not enable one to use noisy preference queries."
    }, {
      "heading" : "Problem Statement",
      "text" : "Utility model. Let V = {1, 2, . . . , N} be a set of N items. We assume a utility function f : 2V → R over subsets of V . Given a set of items S ⊆ V , the utility of this set is f(S). Furthermore, we assume that f is non-negative, monotone and submodular. Monotone set functions satisfy f(S) ≤ f(S′) for all S ⊆ S′ ⊆ V; and submodular functions satisfy the following diminishing returns condition: for all S ⊆ S′ ⊆ V\\{a}, it holds that f(S∪{a})−f(S) ≥ f(S′∪ {a}) − f(S′). These conditions are satisfied by many realistic, complex utility functions (Krause and Guestrin 2011; Krause and Golovin 2012). Concretely, in our image collection summarization example, V is a collection of images, and f is a function that assigns every summary S ⊆ V a score, preferring relevant and diverse summaries.\nObservation model. In classical submodular optimization, f is assumed to be known, i.e., f can be evaluated exactly by an oracle. In contrast, we only assume that noisy evaluations of f can be obtained. For instance, in our summarization example, one way to evaluate f is to query users to rate a summary, or to elicit user preferences via pairwise comparisons of different summaries. In the following, we formally describe these two types of queries in more detail:\n(1) Value queries. In this variant, we query the value for f(a|S) = f({a} ∪ S) − f(S) for some S ⊆ V and a ∈ V . We model the noisy evaluation or response to this query by a random variable Xa|S with unknown sub-Gaussian distribution. We assume that Xa|S has mean f(a|S) and that repeated queries for f(a|S) return samples drawn i.i.d. from the unknown distribution.\n(2) Preference queries. Let a, b ∈ V be two items and S ⊆ V . The preference query aims at determining whether an item a is preferred over item b in the context of S (i.e., item a has larger marginal utility than another item b). We model the noisy response of this pairwise comparison by the random variable Xa>b|S that takes values in {0, 1}. We assume that Xa>b|S follows some unknown distribution and satisfies the following two properties: (i) Xa>b|S has mean larger than 0.5 iff f(a|S) > f(b|S); (ii) the mapping from utilities to probabilities is monotone in the sense, that if given some set S, and given that the gaps in utilities satisfy f(a|S)− f(b|S) ≥ f(a′|S)− f(b′|S) for items a, b, a′, b′ ∈ V , then the mean of Xa>b|S is greater or equal to the mean of Xa′>b′|S . For instance, the distribution induced by the commonly used Bradley-Terry-Luce preference model (Bradley and Terry 1952; Luce 1959) satisfies these conditions. We again assume that repeated queries return samples drawn i.i.d. from the unknown distribution.\nValue queries are natural, if f is approximated via stochastic simulations (e.g., as in Kempe, Kleinberg, and Tardos (2003)). On the other hand, preference queries may be a more natural way to learn what is relevant / interesting to users compared to asking them to assign numerical scores, which are difficult to calibrate.\nObjective. Our goal is to select a set of the items S ⊆ V with |S| ≤ k that maximizes the utility f(S). The optimal\nsolution to this problem is given by Sopt = arg max\nS⊆V,|S|≤k f(S). (1)\nNote that obtaining optimal solutions to problem (1) is intractable (Feige 1998). However, a greedy optimization scheme based on the marginal utilities of the items can provide a solution Sgreedy such that f(Sgreedy) ≥ (1 − 1e ) · f(Sopt), i.e., a solution that is within a constant factor of the optimal solution can be efficiently determined.\nIn our setting, we can only evaluate the unknown utility function f via noisy queries and thus cannot hope to achieve the same guarantees. The key idea is that in a stochastic setting, our algorithms can make repeated queries and aggregate noisy evaluations to obtain sufficiently accurate estimates of the marginal gains of items. We study our proposed algorithms in a PAC setting, i.e., we aim to design algorithms that, given positive constants ( , δ), determine a set Sthat is -competitive relative to a reference solution with probability of at least 1− δ. One natural baseline is a constant factor approximation to the optimal solution, i.e., we aim to determine a set S such that with probability at least 1− δ,\nf(S) ≥ (1− 1 e ) · f(Sopt)− . (2)\nOur objective is to achieve the desired ( , δ)-PAC guarantee while minimizing sample complexity (i.e., the number of evaluation queries performed)."
    }, {
      "heading" : "Submodular Maximization Under Noise",
      "text" : "We now present our algorithm EXPGREEDY for maximizing submodular functions under noise. Intuitively, it aims to mimic the greedy algorithms in noise-free settings, ensuring that it selects a good element in each iteration. Since we cannot evaluate the marginal gains exactly, we must experiment with different items, and use statistical inference to select items of high value. This experimentation, the core part of our algorithm, is implemented via a novel exploration module called TOPX.\nThe algorithm EXPGREEDY, cf., Algorithm 1, iteratively builds up a set S ⊆ V by invoking TOPX( ′, δ′, k′, S) at every iteration to select the next item. TOPX returns candidate items that could potentially be included in S to maximize its utility. The simplest adaptive sampling strategy that TOPX could implement is to estimate the marginal gains up to the necessary confidence to decide for the next best item to select (top1 identification problem). However, if the difference in the marginal gains of the best and the second best item is small, this approach could suffer from high sample complexity. Extending ideas from the randomized greedy algorithm (Buchbinder et al. 2014), we show in Theorem 1 that in every iteration, instead of focusing on top1, it is sufficient for EXPGREEDY to select an item from a small subset of items with high utility. This corresponds to the following two conditions on TOPX:\n1. TOPX returns a subset A 6= ∅ of size at most k′. 2. With probability at least 1− δ′, the items in A satisfy\n1 |A| ∑ a∈A f(a|S) ≥ max B⊆V |B|=|A|\n[ 1\n|B| ∑ b∈B f(b|S)\n] − ′. (3)\nAlgorithm 1: EXPGREEDY 1 Input: Ground set V; No. of items to pick k; , δ > 0; 2 Output: Set of items S ⊆ V : |S| ≤ k, such that S is -competitive with probability at least (1− δ); 3 Initialize: S = ∅ foreach j = 1, . . . , k do 4 A = TOPX( ′, δ′, k′, S) 5 Sample s uniformly at random from A 6 S = S ∪ {s} 7 return S\nAt any iteration, satisfying these two conditions is equivalent to solving a topl identification problem for l = |A| with PAC-parameters ( ′, δ′). TOPX essentially returns a set of size l containing items of largest marginal utilities given S. Let us consider two special cases, (i) l = 1 and (ii) l = k for all j ∈ {1, . . . , k} iterations. Then, in the noise free setting, EXPGREEDY for case (i) mimics the classical greedy algorithm (Nemhauser, Wolsey, and Fisher 1978) and for case (ii) the randomized greedy algorithm (Buchbinder et al. 2014), respectively. As discussed in the next section, the sample complexity of these two cases can be very high. The key insight we use in EXPGREEDY is that if we could efficiently solve the topl problem for any size l ∈ {1, . . . , k} (i.e., |A| is neither necessarily 1 or k), then the solution to the submodular maximization problem is guaranteed to be of high quality. This is summarized in the following theorem: Theorem 1. Let > 0, δ ∈ (0, 1). Using ′ = k , δ\n′ = δk and k′ = k for invoking TOPX, Algorithm 1 returns a set S that satisfies E[f(S)] ≥ (1 − 1e ) · f(S\nopt) − with probability at least 1 − δ. For the case that k′ = 1, the guarantee is f(S) ≥ (1− 1e ) · f(S\nopt)− with probability at least 1− δ. The proof is provided in Appendix A of the extended version of paper (Singla, Tschiatschek, and Krause 2016). As it turns out, solutions of the greedy algorithm in the noiseless setting Sgreedy often have utility larger than (1 − 1 e ) · f(S\nopt). Therefore, we also seek algorithms that with probability at least 1− δ identify solutions satisfying\nf(S) ≥ f(Sgreedy)− . (4) This can be achieved according to the following theorem: Theorem 2. Let δ ∈ (0, 1). Using ′ = 0, δ′ = δk and k\n′ = 1 for invoking TOPX, Algorithm 1 returns a set S that satisfies f(S) = f(Sgreedy) with probability at least 1− δ.\nIf we set k′ = 1 and = 0, then condition (3) is actually equivalent to requiring that TOPX, with high probability, identifies the element with largest marginal utility. The proof follows by application of the union bound. This theorem ensures that if we can construct a corresponding exploration module, we can successfully compete with the greedy algorithm that has access to f . However, this can be prohibitively expensive in terms of the required number of queries performed by TOPX, cf., Appendix B of the extended version of this paper (Singla, Tschiatschek, and Krause 2016)."
    }, {
      "heading" : "Exploration Module TOPX",
      "text" : "In this section, we describe the design of our exploration module TOPX used by EXPGREEDY."
    }, {
      "heading" : "TOPX with Value Queries",
      "text" : "We begin with the observation that for fixed l ∈ {1, . . . , k′} (for instance l = 1 or l = k′), an exploration module TOPX satisfying condition (3) can be implemented by solving a topl identification problem. This can be seen as follows.\nGiven input parameter S, for each a ∈ V \\ S define its value va = f(a|S), and define va = 0 for each a ∈ S. Then, the value of any set A (in the context of S) is v(A) :=∑ a∈A va, which is a modular (additive) set function. Thus, in this setting (fixed l), meeting condition (3) requires identifying a setAmaximizing a modular set function under cardinality constraints from noisy queries. This corresponds to the topl best-arm identification problem. In particular, Chen et al. (2014) proposed an algorithm – CLUCB – for identifying a best subset for modular functions under combinatorial constraints. At a high level, CLUCB maintains upper and lower confidence bounds on the item values, and adaptively samples noisy function evaluations until the pessimistic estimate (lower confidence bound) for the current best topl subset exceeds the optimistic estimate (upper confidence bound) of any other subset of size l. The worst case sample complexity of this problem is characterized by the gap between the values of l-th item and (l+1)-th item (with items indexed in descending order according to the values va).\nAs mentioned, the sample complexity of the topl problem can vary by orders of magnitude for different values of l, cf., Figure 2. Unfortunately, we do not know the value of l with the lowest sample complexity in advance. However, we can modify CLUCB to jointly estimate the marginal gains and solve the topl problem with lowest sample complexity.\nThe proposed algorithm implementing this idea is presented in Algorithm 2. It maintains confidence intervals for marginal item values, with the confidence radius of an item i\ncomputed as radt(i) = R √ 2 log ( 4Nt3\nδ′\n) /Tt(i), where the\ncorresponding random variables Xi|S are assumed to have an R-sub-Gaussian tail and Tt(i) is the number of observations made for item i by time t. If the exact value of R is not known, it can be upper bounded by the range (as long as the variables have bounded centered range). Upon termination, the algorithm returns a set A that satisfies condition (3). Extending results from Chen et al. (2014), we can bound the sample complexity of our algorithm as follows. Consider the gaps ∆l = f(π(l)|S)− f(π(l + 1)|S), where π : V → {1, . . . , N} is a permutation of the items such that\nAlgorithm 2: TOPX 1 Input: Ground set V; Set S; integer k′; ′, δ′ > 0; 2 Output: Set of items A ⊆ V : |A| ≤ k′, such that A\nsatisfies (3) with probability at least 1− δ′; 3 Initialize: For all i = 1, . . . , |V|: observe Xi|S and set vi to that value, set T1(i) = 1; for t = 1, . . . do 4 Compute confidence radius radt(i) for all i; 5 Initialize list of items to query Q = []; for l ∈ 1, . . . , k′ do 6 Mt = arg maxB⊆V,|B|=l ∑ i∈B vi; 7 foreach i = 1, . . . , |V| do 8 If i ∈Mt set ṽi = vi − radt(i), otherwise set ṽi = vi + radt(i) 9 M̃t = arg maxB⊆V,|B|=l ∑ i∈B ṽi;\n10 if [ ∑ i∈M̃t ṽi − ∑ i∈Mt ṽi] ≤ l ·\n′ then 11 Set A = Mt and return A 12 Set q = arg max\ni∈(M\\M̃)∪(M̃\\M) radt(i); 13 Update list of items to query:\nQ = Q.append(q) foreach q ∈ Q do\n14 Query and observe output Xq|S ; 15 Update empirical means vt+1 using the output; 16 Update observation counts Tt+1(q) = Tt(q) + 1\nand Tt+1(j) = Tt(j) for all j 6= q;\nf(π(1)|S) ≥ f(π(2)|S) ≥ . . . ≥ f(π(N)|S). For every fixed l, the sample complexity of identifying a set of top l items is characterized by this gap ∆l. The reason is that if ∆l is small, many samples are needed to ensure that the confidence bounds radt are small enough to distinguish the top l elements from the runner-up. Our key insight is that we can be adaptive to the largest ∆l, for l ∈ {1, . . . , k′}. That is, as long as there is some value of l with large ∆l, we will be able to enjoy low sample complexity (cf., Figure 2): Theorem 3. Given ′ > 0, δ′ ∈ (0, 1), S ⊆ V and k′, Algorithm 2 returns a set A ⊆ V, |A| ≤ k′ that with probability at least 1− δ′ satisfies condition (3) using at most\nT ≤ O ( k′ min l=1,...,k′ [ R2H(l, ′) log ( R2 δ′ H(l, ′) )]) samples, where H(l, ′) = N min{ 4∆2l , 1 ′2 }. A proof sketch is given in Appendix C of the extended version of this paper (Singla, Tschiatschek, and Krause 2016)."
    }, {
      "heading" : "TOPX with Preference Queries",
      "text" : "We now show how noisy preference queries can be used. As introduced previously, we assume that there exists an underlying preference model (unknown to the algorithm) that induces probabilities Pi>j|S for item i to be preferred over item j given the values f(i|S) and f(j|S). In this work, we focus on identifying the Borda winner, i.e., the item i maximizing the Borda score P (i|S), formally given as\n1 (N−1) · ∑ j∈V\\{i} Pi>j|S . The Borda score measures the probability that item i is preferred to another item chosen uniformly at random. Furthermore, in our model where\nwe assume that an increasing gap in the utilities leads to monotonic increase in the induced probabilities, it holds that the top l items in terms of marginal gains are the top l items in terms of Borda scores. We now make use of a result called Borda reduction (Jamieson et al. 2015; Busa-Fekete and Hüllermeier 2014), a technique that allows us to reduce preference queries to value queries, and to consequently invoke Algorithm 2 with small modifications.\nDefining values vi via Borda score. For each item i ∈ V , Algorithm 2 (step 3, 15) tracks and updates mean estimates of the values vi. For preference queries, these values are replaced with the Borda scores. The sample complexity in Theorem 3 is then given in terms of these Borda scores. For instance, for the Bradley-TerryLuce preference model (Bradley and Terry 1952; Luce 1959), the Borda score for item i is given by 1(N−1) ·∑ j∈V\\{i} 1 1+exp (−β(f(i|S)−f(j|S))) . Here, β captures the problem difficulty: β →∞ corresponds to the case of noisefree responses, and β → 0 corresponds to uniformly random binary responses. The effect of β is further illustrated in the synthetic experiments.\nIncorporating noisy responses. Observing the value for item i in the preference query model corresponds to pairing i with an item in the set V \\{i} selected uniformly at random. The observed preference response provides an unbiased estimate of the Borda score for item i. More generally, we can pick a small set Zi of fixed size τ selected uniformly at random with replacement from V \\ {i}, and compare i against each member of Zi. Then, the observed Borda score for item i is calculated as 1τ · ∑ j∈Zi Xi>j|S . Here, τ is a parameter of the algorithm. One can observe that the cost of one preference query is τ times the cost of one value query. The effect of τ will be further illustrated in the experiments.\nThe subtle point of terminating Algorithm 2 (step 10) is discussed in Appendix D of the extended version of this paper (Singla, Tschiatschek, and Krause 2016)."
    }, {
      "heading" : "Experimental Evaluation",
      "text" : "We now report on the results of our synthetic experiments."
    }, {
      "heading" : "Experimental Setup",
      "text" : "Utility function f and set of items V . In the synthetic experiments we assume that there is an underlying submodular utility function f which we aim to maximize. The exploration module TOPX performs value or preference queries, and receives noisy responses based on model parameters and the marginal gains of the items for this function. For our experiments, we constructed a realistic probabilistic coverage utility function following the ideas from El-Arini et al. (2009) over a ground set of N = 60 items. Details of this construction are not important for the results stated below and can be found in Appendix E of the extended version of this paper (Singla, Tschiatschek, and Krause 2016).\nBenchmarks and Metrics. We compare several variants of our algorithm, referred to by EXPGREEDYθ, where θ refers to the parameters used to invoke TOPX. In particular, θ = O means k′ = 1 and ′ = /k (i.e., competing with (1 − 1e ) of f(S opt)); θ = G means k′ = 1 but ′ = 0\n(i.e., competing with f(Sgreedy)); omitting θ means k′ = k and = /k. As benchmarks, we compare the performance with the deterministic greedy algorithm GREEDY (with access to noise-free evaluations), as well as with random selection RANDOM. As a natural and competitive baseline, we compare our algorithms against UNIFORM (Kempe, Kleinberg, and Tardos 2003; Krause and Guestrin 2005) — replacing our TOPX module by a naive exploration module that uniformly samples all the items for best item identification. For all the experiments, we used PAC parameters ( = 0.1, δ = 0.05) and a cardinality constraint of k = 6."
    }, {
      "heading" : "Results",
      "text" : "Sample Complexity. In Figure 3(a), we consider value queries, and compare the number of queries performed by different algorithms under varying noise-levels until convergence to the solution with the desired guarantees. For variance σ2, we generated the query responses by sampling uniformly from the interval [µ − σ2, µ + σ2], where µ is the expected value of that query. For reference, the query cost of GREEDY with access to the unknown function f is marked (which equals N · k). The sample complexity differs by orders of magnitude, i.e., the number of queries performed by EXPGREEDY grows much slower than that of EXPGREEDYG and EXPGREEDYO. The sample complexity of UNIFORM is worse by further orders of magnitude compared to any of the variants of our algorithm.\nVarying σ2 for value queries and (β, τ ) for preference queries. Next, we investigate the quality of obtained solutions for a limited budget on the total number of queries that can be performed. Although convergence may be slow, one may still get good solutions early in many cases. In Figures 3(d)–3(f) we vary the available average budget per item\nper iteration on the x-axis — the total budget available in terms of queries that can be performed is equivalent to N · k times the average budget shown on x-axis. In particular, we compare the quality of the solutions obtained by EXPGREEDY for different σ2 in Figure 3(d) and for different (β, τ) in Figure 3(e)–3(f), averaged over 50 runs. The parameter β controls the noise-level in responses to the preference queries, whereas τ is the algorithm’s parameter indicating how many pairwise comparisons are done in one query to reduce variance. For reference, we show the extreme case of σ2 =∞ and β = 0 which is equivalent to RANDOM; and the case of σ2 = 0 and (β = ∞, τ = N) which is equivalent to GREEDY. In general, for higher σ in value queries, and for lower β or smaller τ in preference queries, more budget must be spent to achieve solutions with high utility.\nComparison with UNIFORM exploration. In Figure 3(d) and Figure 3(e)–3(f), we also report the quality of solutions obtained by UNIFORM for the case of σ2 = 10 and (β = 0.5, τ = 1) respectively. As we can see in the results, in order to achieve a desired value of total utility, UNIFORM may require up to 3 times more budget in comparison to that required by EXPGREEDY. Figure 3(b) compares how different algorithms allocate budget across items (i.e., the distribution of queries), in one of the iterations. We observe that EXPGREEDY does more exploration across different items compared to EXPGREEDYG and EXPGREEDYO. However, the exploration is heavily skewed in comparison to UNIFORM because of the adaptive sampling by TOPX. Figure 3(c) shows the marginal gain in utility of the considered algorithms at different iterations for a particular execution instance (no averaging over multiple executions of the algorithms was performed). For EXPGREEDY, the size of topl solutions returned by TOPX in every iteration is indicated\nin the Figure, demonstrating that TOPX adaptively allocates queries to efficiently identify the largest gap ∆l."
    }, {
      "heading" : "Image Collection Summarization",
      "text" : "We now present results on a crowdsourced image collection summarization application, performed on Amazon’s Mechanical Turk platform. As our image set V , we retrieved 60 images in some way related to the city of Venice from Flickr, cf., Figure 1(a). A total of over 100 distinct workers participated per summarization task.\nThe workers were queried for pairwise preferences as follows. They were told that our goal is to summarize images from Venice, motivated by the application of selecting a small set of pictures to send to friends after returning from a trip to Venice. The detailed instructions can be found in Appendix F of the extended version of this paper (Singla, Tschiatschek, and Krause 2016). We ran three instances of the algorithm for three distinct summarization tasks for the themes (i) Venice, (ii) Venice Carnival, and (iii) Venice Cathedrals. The workers were told the particular theme for summarization. Additionally, the set of images already selected and two proposal images a and b were shown. Then they were asked which of the two images would improve the summary more if added to the already selected images.\nFor running EXPGREEDY, we used an average budget of 25 queries per item per iteration and τ = 3, cf., Figure 3(f). Note that we did not make use of the function f constructed for the synthetic experiments at all, i.e., the function we maximized was not known to us. The results of this experiment are shown in Figure 1(b), demonstrating that our methodology works in real-word settings, produces high quality summaries and captures the semantics of the task."
    }, {
      "heading" : "Conclusions",
      "text" : "We considered the problem of cardinality constrained submodular function maximization under noise, i.e., the function to be optimized can only be evaluated via noisy queries. We proposed algorithms based on novel adaptive sampling strategies to achieve high quality solutions with low sample complexity. Our theoretical analysis and experimental evaluation provide insights into the trade-offs between solution quality and sample complexity. Furthermore, we demonstrated the practical applicability of our approach on a crowdsourced image collection summarization application.\nAcknowledgments. We would like to thank Besmira Nushi for helpful discussions. This research is supported in part by SNSF grant 200021 137971 and the Nano-Tera.ch program as part of the Opensense II project."
    }, {
      "heading" : "Utility function f",
      "text" : "For synthetic image collection summarization experiments, we constructed the submodular utility function f(S) which we aim to maximize. We retrieved 350 images from Flickr along with their associated meta-data (asosciated tags, the number of tags in total, and the view-count), representing images in some way related to the city of Venice 1. Following El-Arini et al. (2009), we constructed f(S) = ∑20 j=1 wj fj(S), where each fj(S) is a probabilistic coverage function with range [0, 1] quantifying how well some topic j is covered, and where wj is a weight quantifying the importance of topic j. More specifically, we did this as follows: For each of the 350 images from Flickr we are given the asosciated tags, the number of tags in total, and the view-count. As preprocessing, we kept only the 20 most frequent tags. For image s, this information can be represented by the triplet (Ts, ns, vs), where Ts is the set of present topics (equivalent to tags in our setting), ns = |Ts|, and vs is the view-count. Without loss of generality, we can assume that each of the tags is identified by an integer in {1, . . . , 20}. Using these tags, we define the utility function f(S) = ∑20 j=1 wj fj(S), where each fj(S) is a set function with range [0, 1] quantifying how well topic j (this corresponds to the jth tag) is covered, and where wj is a weight quantifying the importance of topic j. These weights are selected as wj = 20 ∑ a 1[j∈Ta]∑\na na such that\n∑20 j=1 wj = 20 (normalization to any other value is clearly possible, resulting in a different\nscaling of the observed function values).2.\n1We used the flickr.photos search function of Flickr API with tag query ‘Venice’ 2We obtained the following topics and corresponding weights: gondola 1.91; water 1.91; canal 1.47; bridge 1.32;\nart 1.18; rialto 1.18; ponte 1.03; piazzasanmarco 0.88; bw 0.88; street 0.88; night 0.74; murano 0.74; blackwhite 0.74; boat 0.74; blue 0.74; grandcanal 0.74; girl 0.74; carnival 0.74; architecture 0.74; sunset 0.74"
    }, {
      "heading" : "Set of items V",
      "text" : "In order to construct our ground set V , we downloaded another set of 60 test images (along with the associated meta-data) that formed our test collection that we seek to summarize 3. These test images are shown in Figure 1(a). Our goal is to summarize this test collection in simulations for the theme Venice via the function constructed above.\nTo this end, we define the coverage of a set of images S for topic j as fj(S) = 1− ∏ s∈S(1− f ′ j(s)), i.e., as a type of probabilistic\ncoverage (El-Arini et al. 2009), where f ′j(s) ∝ log(vs) 1[j∈Ts] ns\nnormalized to [0, 1] across all j and s. Clearly, more advanced models could be used for scoring image collection summaries, e.g., V-ROUGE which is a scoring function tailored for the task of image collection summarization (Tschiatschek et al. 2014).\n330 test images were retrieved for the tag query ‘Venice’ and remaining 30 for the query ‘Venice & Church’. We ensured that these 60 images were from different users than the 350 images used to build model"
    } ],
    "references" : [ {
      "title" : "N",
      "author" : [ "M. Balcan", "Harvey" ],
      "venue" : "J. A.",
      "citeRegEx" : "Balcan and Harvey 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "M",
      "author" : [ "R.A. Bradley", "Terry" ],
      "venue" : "E.",
      "citeRegEx" : "Bradley and Terry 1952",
      "shortCiteRegEx" : null,
      "year" : 1952
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "Munos Bubeck", "S. Stoltz 2009] Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In ALT,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "Submodular maximization with cardinality constraints",
      "author" : [ "Buchbinder" ],
      "venue" : null,
      "citeRegEx" : "Buchbinder,? \\Q2014\\E",
      "shortCiteRegEx" : "Buchbinder",
      "year" : 2014
    }, {
      "title" : "and Hüllermeier",
      "author" : [ "R. Busa-Fekete" ],
      "venue" : "E.",
      "citeRegEx" : "Busa.Fekete and Hüllermeier 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "M",
      "author" : [ "S. Chen", "T. Lin", "I. King", "Lyu" ],
      "venue" : "R.; and Chen, W.",
      "citeRegEx" : "Chen et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Turning down the noise in the blogosphere",
      "author" : [ "El-Arini" ],
      "venue" : null,
      "citeRegEx" : "El.Arini,? \\Q2009\\E",
      "shortCiteRegEx" : "El.Arini",
      "year" : 2009
    }, {
      "title" : "Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems",
      "author" : [ "Mannor Even-Dar", "E. Mansour 2006] Even-Dar", "S. Mannor", "Y. Mansour" ],
      "venue" : null,
      "citeRegEx" : "Even.Dar et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2006
    }, {
      "title" : "and Singer",
      "author" : [ "A. Hassidim" ],
      "venue" : "Y.",
      "citeRegEx" : "Hassidim and Singer 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "R",
      "author" : [ "K.G. Jamieson", "S. Katariya", "A. Deshpande", "Nowak" ],
      "venue" : "D.",
      "citeRegEx" : "Jamieson et al. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Maximizing the spread of influence through a social network",
      "author" : [ "Kleinberg Kempe", "D. Tardos 2003] Kempe", "J. Kleinberg", "É. Tardos" ],
      "venue" : null,
      "citeRegEx" : "Kempe et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kempe et al\\.",
      "year" : 2003
    }, {
      "title" : "and Golovin",
      "author" : [ "A. Krause" ],
      "venue" : "D.",
      "citeRegEx" : "Krause and Golovin 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "and Guestrin",
      "author" : [ "A. Krause" ],
      "venue" : "C.",
      "citeRegEx" : "Krause and Guestrin 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "and Guestrin",
      "author" : [ "A. Krause" ],
      "venue" : "C.",
      "citeRegEx" : "Krause and Guestrin 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "and Guestrin",
      "author" : [ "A. Krause" ],
      "venue" : "C.",
      "citeRegEx" : "Krause and Guestrin 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Near-optimal sensor placements: Maximizing information while minimizing communication cost",
      "author" : [ "Krause" ],
      "venue" : null,
      "citeRegEx" : "Krause,? \\Q2006\\E",
      "shortCiteRegEx" : "Krause",
      "year" : 2006
    }, {
      "title" : "R",
      "author" : [ "Luce" ],
      "venue" : "D.",
      "citeRegEx" : "Luce 1959",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Distributed submodular maximization: Identifying representative elements in massive data",
      "author" : [ "Mirzasoleiman" ],
      "venue" : null,
      "citeRegEx" : "Mirzasoleiman,? \\Q2013\\E",
      "shortCiteRegEx" : "Mirzasoleiman",
      "year" : 2013
    }, {
      "title" : "An analysis of the approximations for maximizing submodular set functions",
      "author" : [ "Wolsey Nemhauser", "G. Fisher 1978] Nemhauser", "L. Wolsey", "M. Fisher" ],
      "venue" : "Math. Prog",
      "citeRegEx" : "Nemhauser et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "Nemhauser et al\\.",
      "year" : 1978
    }, {
      "title" : "W",
      "author" : [ "A. Singh", "A. Krause", "Kaiser" ],
      "venue" : "J.",
      "citeRegEx" : "Singh. Krause. and Kaiser 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Noisy submodular maximization via adaptive sampling with applications to crowdsourced image collection summarization (extended version). http://arxiv.org/abs/1511.07211",
      "author" : [ "Tschiatschek Singla", "A. Krause 2016] Singla", "S. Tschiatschek", "A. Krause" ],
      "venue" : null,
      "citeRegEx" : "Singla et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Singla et al\\.",
      "year" : 2016
    }, {
      "title" : "and Golovin",
      "author" : [ "M. Streeter" ],
      "venue" : "D.",
      "citeRegEx" : "Streeter and Golovin 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning mixtures of submodular functions for image collection summarization",
      "author" : [ "Tschiatschek" ],
      "venue" : null,
      "citeRegEx" : "Tschiatschek,? \\Q2014\\E",
      "shortCiteRegEx" : "Tschiatschek",
      "year" : 2014
    }, {
      "title" : "and Guestrin",
      "author" : [ "Y. Yue" ],
      "venue" : "C.",
      "citeRegEx" : "Yue and Guestrin 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Optimal PAC multiple arm identification with applications to crowdsourcing",
      "author" : [ "Chen Zhou", "Y. Li 2014] Zhou", "X. Chen", "J. Li" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2014
    }, {
      "title" : "Proof of Theorem 1 In the following, we provide the proof of Theorem 1. Proof. This proof adopts the proof of (Buchbinder et al",
      "author" : [ "A. Appendix" ],
      "venue" : null,
      "citeRegEx" : "Appendix,? \\Q2014\\E",
      "shortCiteRegEx" : "Appendix",
      "year" : 2014
    }, {
      "title" : "Proof Sketch of Theorem 3 For proving Theorem 3, consider a simpler variant of Algorithm 2. Assume that we solve the top identification problems for l ∈",
      "author" : [ "C. Appendix" ],
      "venue" : null,
      "citeRegEx" : "Appendix,? \\Q2014\\E",
      "shortCiteRegEx" : "Appendix",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We address the problem of maximizing an unknown submodular function that can only be accessed via noisy evaluations. Our work is motivated by the task of summarizing content, e.g., image collections, by leveraging users’ feedback in form of clicks or ratings. For summarization tasks with the goal of maximizing coverage and diversity, submodular set functions are a natural choice. When the underlying submodular function is unknown, users’ feedback can provide noisy evaluations of the function that we seek to maximize. We provide a generic algorithm – EXPGREEDY – for maximizing an unknown submodular function under cardinality constraints. This algorithm makes use of a novel exploration module – TOPX – that proposes good elements based on adaptively sampling noisy function evaluations. TOPX is able to accommodate different kinds of observation models such as value queries and pairwise comparisons. We provide PAC-style guarantees on the quality and sampling cost of the solution obtained by EXPGREEDY. We demonstrate the effectiveness of our approach in an interactive, crowdsourced image collection summarization application.",
    "creator" : "LaTeX with hyperref package"
  }
}