{
  "name" : "1705.06369.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Decoding Sentiment from Distributed Representations of Sentences",
    "authors" : [ "Edoardo Maria Ponti", "Ivan Vulić", "Anna Korhonen" ],
    "emails" : [ "ep490@cam.ac.uk", "iv250@cam.ac.uk", "alk23@cam.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Distributed representations of sentences are acquired in an unsupervised fashion from raw texts. Those inferred from different algorithms are prone to grasp parts of their meaning and disregard others. Representations have been evaluated thoroughly, both intrinsically (interpretation through distance measures) and extrinsically (performance on downstream tasks). Moreover, several methods were considered, based on both the composition of word embeddings (Milajevs et al., 2014; Marelli et al.,\n2014; Sultan et al., 2015) and direct generation (Hill et al., 2016). The evaluation was focused solely on English, and it rarely concerned other languages. As a consequence, many ‘core’ methods to learn distributed sentence representations are largely under-explored in a variety of typologically diverse languages, and still lack a demonstration of their usefulness in actual downstream tasks.\nIn this work, we study how well distributed sentence representations capture the polarity of a sentence. To this end, we choose the Sentiment Analysis task as an extrinsic evaluation protocol: it directly detects the polarity of a text, where polarity is defined as the attitude of the speaker with respect to the whole content of the string or one of the entities mentioned therein. This attitude is measured quantitatively on a scale spanning from negative to positive with arbitrary granularity. As such, polarity consists in a crucial part of the meaning of a sentence, which should not be lost.\nThe polarity of a sentence depends heavily on a complex interaction between lexical items endowed with an intrinsic polarity, and morphosyntactic constructions altering polarity, most notably negation and concession. The interaction is deemed to be recursive, hence some approaches take into account word order and phrase boundaries in order to apply the correct composition (Socher et al., 2013). However, some languages lack continuous constituents and a rigid word order (Ponti, 2016). Moreover, the expression of negation varies across languages, as demonstrated by works in Linguistic Typology (Dahl, 1979, inter alia). In particular, negation can appear as a bounded morpheme or a free morpheme; it can precede or follow the verb; it can ‘agree’ or not in polarity with indefinite pronouns; it can alter the expression of verbal categories (e.g. tense, aspect, or modality).\nWe explore a series of methods endowed with different features: some hinge upon word order,\nar X\niv :1\n70 5.\n06 36\n9v 1\n[ cs\n.C L\n] 1\n7 M\nay 2\n01 7\nothers on sentence order, others on neither. We evaluate these representations using a Multi-Layer Perceptron which uses the generated sentence representations as input and predicts sentiment classes (positive vs. negative) as output. Training and evaluation are based on a collection of annotated databases. Owing to the variety of methods and languages, we expect to observe a variation in the performance correlated with the properties of both.\nMoreover, we offer a comparison between the simple method based on decoding distributed representations and deep learning architectures that achieve state-of-art scores in the Sentiment Analysis task. In particular, we also evaluate a bidirectional LSTM (Li et al., 2015) on the same task. These models have advantage over distributed representations as: i) they are specialised on a single task rather than built as general-purpose representations; ii) their recurrent nature allows to capture the sequential composition of polarity in a sentence. However, since training these models requires large amounts of annotated data, resource scarcity in other languages hampers their portability.\nThe aim of this work is to assess which algorithm for distributed sentence representations is the most appropriate for a given language. Moreover, we study how language-specific properties have an impact on performance, finding an explanation in Language Typology. We also provide an in-depth analysis of the most relevant features by visualising the activation of hidden neurons. This will hopefully contribute to advancing the Sentiment Analysis task in the multilingual scenarios. In § 2, we survey prior work on multilingual sentiment analysis. Afterwards, we present the dataset in the algorithms to generate distributed representations of sentences in § 3. In § 4, we sketch the dataset and the experimental setup. Finally, § 5 examines the results in light of the sensitivity of the algorithms and the typology of negation."
    }, {
      "heading" : "2 Multilingual Sentiment Analysis",
      "text" : "Sentiment classification achieves unsatisfactory results in resource-lean languages because of the scarcity of resources to train dedicated models (Denecke, 2008). This afflicts state-of-art deep learning architectures even more compared to traditional machine learning algorithms (Chen et al., 2016). As a consequence, previous works resorted to language transfer or joint multilingual learning. The former adapts models from a source resource-rich\nlanguage to a target resource-poor language; the latter infers a single model portable across languages. Approaches based on distributed representations induced in an unsupervised fashion do not face the difficulty resulting from resource scarcity: they are portable to other tasks and languages. In this section we survey adaptive models, deep learning techniques and distributed representations for sentiment classification in a multilingual scenario.\nLanguage transfer can bridge between topics in multiple languages through e.g. supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010). Another approach leverages on wordaligned parallel texts where the source language text is either manually or automatically annotated. The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al., 2015). Chen et al. (2016) devised a multi-task network where an adversarial branch spurs the shared layers to learn language-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language. Alternatively, sentences from another languages are translated into English and assigned a sentiment based on lexical resources (Denecke, 2008) or supervised methods (Balahur and Turchi, 2014).\nDeep learning algorithms for sentiment classification are designed to deal with compositionality. Hence, they often rely on recurrent networks tracing the sequential history of a sentence, or special compositional devices to obtain the best performance on standard benchmarks like the Stanford Sentiment Treebank. Recurrent models include bi-directional LSTMs (Li et al., 2015), possibly enriched with context (Mousa and Schuller, 2017). On the other hand, Socher et al. (2013) put forth a Recursive Neural Tensor Network, which composes representations recursively through a single tensor-based composition function. Subsequent improvements of this line of research include the Structural Attention Neural Networks (Kokkinos and Potamianos, 2017), which adds structural information around each node of a syntactic tree.\nFinally, cross-lingual sentiment classification can leverage on shared distributed representations. Zhou et al. (2016) captured shared high-level features across aligned sentences through autoencoders. In this latent space, distances were optimized to reflect differences in sentiment. On the other hand, Fernández et al. (2015) exploited bilin-\ngual word representations, where vector dimensions mirror the distributional overlap with respect to a pivot. Le and Mikolov (2014) concatenated sentence representations obtained through variants of Paragraph Vector and trained a Logistic Regression model on top of them.\nPrevious studies thus demonstrated that sentence representations retain information about polarity, and that they partly alleviate the drawbacks of deep architectures (single-purposed and datademanding). Hence, Sentiment Analysis seems convenient to compare different sentence representation architectures. Nonetheless, a systematic evaluation has never taken place for this task. Moreover, a large-scale study over typologically diverse languages has never been attempted for any of the algorithms reviewed. We intend to fill these gaps, considering the methods to generate sentence representations outlined in the next section."
    }, {
      "heading" : "3 Distributed Sentence Representations",
      "text" : "Word vectors can be combined through various compositional operations to obtain representations of phrases and sentences. Mitchell and Lapata (2010) explored two operations: addition and multiplication. Notwithstanding their simplicity, they are hardly outperformed by more sophisticated operations (Rimell et al., 2016). Some of these compositional representations based on matrix multiplication were also evaluated on sentiment classification (Yessenalina and Cardie, 2011). Alternatively, sentence representations can be induced directly with no intermediate step at the word level. In this paper, we focus on sentence representations that are generated in an unsupervised fashion. Furthermore, they are ‘fixed’, that is, they are not fine-tuned for any particular downstream task, since we are interested in their intrinsic content.1"
    }, {
      "heading" : "3.1 Algorithms",
      "text" : "We explore several methods to generate sentence representations. One exploits a compositional operation (addition) over word representations stemming from a Skip-Gram model (§ 3.1.1). Others are direct methods, including FastSent (§ 3.1.2), a Sequential Denoising AutoEncoder (SDAE, § 3.1.3) and Paragraph Vector (§ 3.1.4). Note that FastSent relies on sentence order, SDAE on word order, and\n1This excludes methods concerned with phrases, like the ECO embeddings (Poliak et al., 2017), or requiring structured knowledge, like CHARAGRAM (Wieting et al., 2016).\nParagraph Vector on neither. All these algorithms were trained on cleaned-up Wikipedia dumps.\nThe choice of the algorithms was based on following criteria: i) their performance reported in recent surveys (n.b., the surveys were limited to English and evaluated on other tasks), most notably Hill et al. (2016) and Milajevs et al. (2014); ii) the variety of their modelling assumptions and features encoded. The referenced surveys already hinted that the usefulness of a representation is largely dependent on the actual application. Shallower but more interpretable representations can be decoded with spatial distance metrics. Others, more deep and convoluted architectures, outperform the others in supervised tasks. We will inquire whether the generalization is tenable also in the task of Sentiment Analysis targeting sentence polarity."
    }, {
      "heading" : "3.1.1 Additive Skip-Gram",
      "text" : "As a bottom-up method, we train word embeddings using skip-gram with negative sampling (Mikolov et al., 2013). The algorithm finds the parameter θ such that, given a pair of a word w and a context c, the model discriminates correctly whether it belongs to a set of sentences S or a set of randomly generated incorrect sentences S′:\n∏ (w,c)∈S p(S = 1|w, c, θ) ∏ (w,c)∈S′ p(S′ = 0|w, c, θ)\nThe representation of a sentence was obtained via element-wise addition of the vectors of the words belonging to it (Mitchell and Lapata, 2010)."
    }, {
      "heading" : "3.1.2 FastSent",
      "text" : "The FastSent model was proposed by Hill et al. (2016). It hinges on a sentence-level distributional hypothesis (Polajnar et al., 2015; Kiros et al., 2015). In other terms, it assumes that the meaning of a sentence can be inferred by the neighbour sentences in a text. It is a simple additive log-linear model conceived to mitigate the computational expensiveness of algorithms based on a similar assumption. Hence, it was preferred over SkipThought (Kiros et al., 2015) because of i) these efficiency issues and ii) its competitive performances reported by Hill et al. (2016). In FastSent, sentences are represented as bags of words: a context of sentences is used to predict the adjacent sentence. Each word w corresponds to a source vector uw and a target vector vw. A sentence Si is represented as the sum of the source vectors of its words ∑ w∈Si uw.\nHence, the cost C of a representation is given by the softmax σ(x) of a sentence representation and the target vectors of the words in its context c.\nCSi = ∑\nc∈Si−1∪Si+1 σ( ∑ w∈Si uw, vc) (1)\nThis model does not rely on word order, but rather on sentence order. It encodes new sentences by summing over the source vectors of their words."
    }, {
      "heading" : "3.1.3 Sequential Denoising AutoEncoder",
      "text" : "Sequential Denoising AutoEncoders (SDAEs) combine features of Denoising AutoEncoders (DAE) and Sequence-to-Sequence models. In DAE, the input representation is corrupted by a noise function and the algorithms learns to recover the original (Vincent et al., 2008). Intuitively, this makes the model more robust to changes in input that are irrelevant for the task at hand. This architecture was later adapted to encode and decode variablelength inputs, and the corruption process was implemented in the form of dropout (Iyyer et al., 2015). In the implementation by Hill et al. (2016),2 the corruption function is defined as f(S|po, px). S is a list of words (a sentence) where each has a probability po to be deleted, and the order of the words in every distinct bigram has a probability px to be swapped. The architecture consists in a Recurrent Layer and predicts p(S|f(S|po, px))."
    }, {
      "heading" : "3.1.4 Paragraph Vector",
      "text" : "Paragraph Vector is a collection of log-linear models proposed by Le and Mikolov (2014) for paragraph/sentence representation. It consists of two different models, namely the Distributed Memory model (DM) and the Distributed Bag Of Words model (DBOW). In DM, the ID of every distinct paragraph (or sentence) is mapped to a unique vector in a matrix D and each word is mapped to a unique vector in matrix W. Given a sentence i and a window size k, the vector Di,· is used in conjunction with the concatenation of the vectors of the words in a sampled context 〈wi1 , . . . , wik〉 to predict the next word through logistic regression:\np(Wik+1 | 〈Di,Wi1 , . . . ,Wik〉) (2)\nNote that the sentence ID vector is shared by the contexts sampled from the same sentence. On the other hand, DBOW focuses on predicting the word\n2https://github.com/fh295/ SentenceRepresentation\nembedding Wij for a sampled word j belonging to sentence i given the sentence representation Di. As a result, the main difference between the two Paragraph Vector models is that the first is sensitive to word order (represented by the word vector concatenation), whereas the second is insensitive with respect to it. These models store a representation for each sentence in the training set, hence they are memory demanding. We use the gensim implementation of the two models available as Doc2Vec.3"
    }, {
      "heading" : "3.2 Hyper-parameters",
      "text" : "The choice of the models’ hyper-parameters was based on two (contrasting) criteria: i) conservativeness with those proposed in the original models and ii) comparability among the models in this work. In particular, we ensured that each model had the same sentence vector dimensionality: 300. The only exception is SDAE: we kept the recommended value of 2400. Paragraph Vector DBOW and SkipGram were trained for 10 epochs, with a window size of 10, a minimum frequency count of 5, and a sampling threshold of 10−5. FastSent was set as having a minimum count of 3, and no sampling. The probabilities in the corruption function of the SDAE were set as po = 0.1 (deletion) and px = 0.1 (swapping). The dimension of the RNN (GRU) hidden states (and hence sentence vector) was 2400, whereas single words were assigned 100 dimensions. The learning rate was set to 0.01 without decay, and the training lasted 7.2 hours on a NVIDIA Titan X GPU. The main properties of each algorithm are summarized in Table 1."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Now, we evaluate the quality of the distributed sentence representations from § 3 on Sentiment Analysis. In § 4.1 we introduce the datasets of all the considered languages, and the evaluation protocol in § 4.2. Finally, to provide a potential performance ceiling, we compare the obtained results with those of a deep, state-of-art classifier, outlined in § 4.3.\n3https://radimrehurek.com/gensim/ models/doc2vec.html"
    }, {
      "heading" : "4.1 Datasets",
      "text" : "The data for training and testing are sourced from the SemEval 2016: Task 5 (Pontiki et al., 2016). These datasets provide customer reviews in 8 languages labelled with Aspect-Based Sentiment, i.e., opinions about specific entities or attributes rather than generic stances. The languages include Arabic (hotels), Chinese (electronics), Dutch (restaurants and electronics), English (restaurants and electronics), French (restaurants), Russian (restaurants), Spanish (restaurants), and Turkish (restaurants). We mapped the labels to an overall polarity class (positive or negative) by selecting the majority class among the aspect-based sentiment classes for a given sentence. Moreover, we added data for Italian from the SENTIPOLC shared task in EVALITA 2016 (Barbieri et al., 2016). We discarded neutral stances from the corpus, and retained only positive and negative ones. In Table 2, we list the size of the dataset partitions for each language."
    }, {
      "heading" : "4.2 Evaluation Protocol",
      "text" : "After mapping each sentence in the dataset to its distributed representation, we fed them to a MultiLayer Perceptron (MLP), trained to detect the sentence polarity. In this network, a logistic regression layer is stacked onto a 60-dimensional hidden layer with a hyperbolic tangent activation. The weights are initialized drawing from the random distribution proposed by Glorot and Bengio (2010). The cross-entropy loss was normalized with the L2norm of the weights scaled by λ = 10−3. The optimization with gradient descent ran for 20 epochs with early stopping. Batch size was 10 and the learning rate 10−2."
    }, {
      "heading" : "4.3 Comparison with State-of-Art Models",
      "text" : "In addition to distributed sentence representations, we test a bi-directional Long Short-Term Memory neural network (bi-LSTM) on the same task. This is a benchmark to compare against results of deep\nstate-of-art architectures. The choice is based on the competitive results of this algorithm and on its sensitivity to word order. The accuracy of this architecture is 45.7 for 5-class and 85.4 for 2-class Sentiment Analysis on the standard dataset of the Stanford Sentiment Treebank.\nThe importance of word order is evident from the architecture of the network. In a recurrent model, the word embedding of a word wt at time t is combined with the hidden state ht−1 from the previous time step. The process is iterated throughout the whole sequence of words of a sentence. This model can be extended to multiple layers. LSTM is a refinement associating each time epoch with an input, control and memory gate, in order to filter out irrelevant information (Hochreiter and Schmidhuber, 1997). This model is bi-directional if it is split in two branches reading simultaneously the sentence in opposite directions (Schuster and Paliwal, 1997).\nContrary to the evaluation protocol sketched in § 4.2, the bi-LSTM does not utilize unsupervised sentence representations. Rather, it is trained directly on the datasets from § 4.1. The optimization ran for 20 epochs, with a batch size of 20 and a learning rate of 5 · 10−2. The 60-dimensional hidden layer had a dropout probability of 0.2. Finally, the word embeddings were initialised with the Skip-Gram model described in § 3.1.1."
    }, {
      "heading" : "5 Results",
      "text" : "The results are showed in Figure 1. Both accuracy scores and weighted F1 scores are reported, since the two classes (positive and negative) are unbalanced. The results are not straightforward: there is no algorithm outperforming the others in each language; unexpectedly not even the bi-LSTM used as a ceiling. However, the variation in performance follows certain trends, depending on the properties of languages and algorithms. We now examine: i) how performance is affected by the properties of the algorithms, such as those summarized in Table 1; ii) how typological features concerning negation could make polarity harder to detect; iii) a visualization of the contribution of each word to the predicted class probabilities."
    }, {
      "heading" : "5.1 Feature Sensitivity of the Algorithms",
      "text" : "The state-of-art bi-LSTM algorithm chosen as a ceiling is not the best choice in some languages (English, Italian, and Turkish). In these cases, it is always surpassed by the same model: additive Skip-\nGram. These languages hardly share any relevant property, hence no correlation emerges. Note that the drop in Italian is possibly linked to its dataset in specific, since all the algorithms behave similarly badly. However, the general high performance of additive Skip-Gram is noteworthy: it shows that a simple method achieves close-to-best results in almost every language among decoded distributed representations. Exceptions include Arabic, Russian, and Chinese, where SDAE is often the best alternative. They are explained by the sparsity of information due to the number of characters in Chinese, and in general by the anomaly of their morphological system compared to the rest of the sample. A crucial feature in these languages appear to be word order, as SDAE is the best alternative and bi-LSTM outdistances the others by far.\nIn general, algorithms sensitive to the same features behave similarly, e.g., SDAE and bi-LSTM. They follow the same trend in relative improvements from one language to another, with the exception of Chinese. Instead, FastSent does not respond to the word-level properties of a language but rather to the discourse structure, in terms of cohesion and coherence. A relatively small and accessible dataset (Wikipedia) is sufficient to provide a reliable model in some languages but not in others: we see a performance drop for languages with less training data."
    }, {
      "heading" : "5.2 Typology of Negation",
      "text" : "In some languages, the scores are very scattered: this divergence might be due to their peculiar morphological properties. In particular, Arabic is an introflexive language, Chinese is a radically isolating language, and Turkish an agglutinative language. On the other hand, the algorithms achieve better scores in the fusional languages, save Italian.\nA fine-grained analysis shows also that the performance is affected by the typology of the negation in each language. Semantically, negation is crucial in switching or mitigating the polarity of lexical items and phrases.\nMorpho-syntactically, negation is expressed through several constructions across the languages of the world. Constructions differ in many respects, which are classified as feature-value pairs in databases like the World Atlas of Language Structures (Dryer and Haspelmath, 2013).4\n4The features considered here for negation are 113A ‘Symmetric and Asymmetric Standard Negation’, 114A ‘Subtypes of Asymmetric Standard Negation’, 115A ‘Negative Indefi-\nNegation can affect the declarative verbal main clauses. In fact, negative clauses can be: i) symmetric, i.e. identical to the affirmative counterpart except for the negative marker; ii) asymmetric, i.e. showing structural differences between negative and affirmative clauses (in constructions or paradigms); iii) showing mixed behaviour. Alterations concern for instance finiteness, the obligatory marking of unreality status, or the expression of verbal categories. Secondly, negation interacts with indefinite pronoun (e.g. nobody, nowhere, never). Negative indefinites can i) co-occur with standard negation; ii) be forbidden in concurrence; iii) display a mixed behaviour. Finally, the relation of the negative marker with respect to verb is prone to change. Firstly, it can be either an affix or a prosodically independent word. Secondly, its position can be anchored to the verb (preceding, following, or both). Thirdly, negation can be omitted, doubled or even tripled.\nLanguages where negative pronouns in conjunction with negative verbs are possible show worse scores, whereas those where they are forbidden or restricted to few constructions gain better results. The non-redundancy of the negation probably makes the composition easier (especially for simple operators like addition). At the same time, asymmetrical negation may be responsible for a little decrease in English, Turkish, and Chinese. In fact, the additional changes in the sentence construction and/or verb paradigm might create noise. Additional reasons of difficulty may occur when negation is doubled (French) or affixed (Turkish), since this makes negation redundant or sparse.\nThese comments on the results based on linguistic properties can also suggest speculative solutions for future works. For algorithms based on sentence order, it is not clear whether the problem lies in the lack of wider collections of texts in some languages, or rather on the maximum amount of information about polarity that is learnt through a sentence-level Distributional Hypothesis. On the other hand, impairments of the other algorithms seem to be linked with redundancies and noise. Probably filtering out words that contribute to this effect might benefit the quality of the representation. Moreover, the sparsity due to cases where negation is an affix might be mitigated by introducing character-level features.\nnite Pronouns and Predicate Negation’, and 143A ‘Order of Negative Morpheme and Verb’."
    }, {
      "heading" : "5.3 Visualisation",
      "text" : "Since languages vary in the “polarity agreement” between verbs and indefinite pronouns, algorithms may weigh these as features differently. We analyzed their role through a visualization of the activation in the hidden layer of the bi-LSTM. In particular, we approximated the objective function through a linear function, and estimated the contribution of each word to the true class probability by computing the prime derivative of the output scores with respect to the embeddings. This technique is presented and detailed by Li et al. (2015). The results are shown in Figure 2.\nThe embeddings correspond to the translation of two English sentences. The first is positive: ‘I like everything in this restaurant’; the second is negative: ‘I don’t like anything in this restaurant’. These include a domain-specific but sentiment-neutral word that plays the role of a touchstone. The more a cell tends to blue, the higher its activation. In some languages (e.g. Arabic), the sentiment verb elicits a stronger reaction in the positive polarity, whereas the indefinite pronoun dominates in the negative polarity. In several other languages (e.g. Spanish), indefinite pronouns are more relevant than any other feature. Somewhere else (e.g. Russian), only sentiment verbs always provoke a reaction. These differences might be related to the “polarity agreement” of these languages, which happens always, sometimes, and never, respectively. In some other\nlanguages, however, no evidence is found of any activation pattern."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we examined how much sentiment polarity information is retained by distributed representations of sentences in multiple typologically diverse languages. We generated the representations through various algorithms, sensitive to different properties from training corpora (e.g, word or sentence order). We decoded them through a simple MLP and compared their performance with one of the state-of-art algorithms for Sentiment Analysis: bi-directional LSTM. Unexpectedly, for several languages Bi-LSTM is outperformed by simple strategies like the addition of the word embeddings obtained from a Skip-Gram model. This demonstrates i) that no algorithm is the best across the board; and ii) that some simple models are to be preferred even for downstream tasks, which partially contrasts with the conclusions of Hill et al. (2016). Moreover, representation algorithms sensitive to word order have similar trends, but they do not always achieve performance superior to algorithms based on the sentence order. Finally, some properties of languages (i.e. their type of negation) appear to have a bad impact on the scores: in particular the ‘polarity agreement’ of indefinite pronouns with verbs, the asymmetry of negative and affirmative clauses, and the doubling of negative markers."
    } ],
    "references" : [ {
      "title" : "Aligning opinions: Cross-lingual opinion mining with dependencies",
      "author" : [ "Mariana SC Almeida", "Cláudia Pinto", "Helena Figueira", "Pedro Mendes", "André FT Martins." ],
      "venue" : "Proc. of the Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Almeida et al\\.,? 2015",
      "shortCiteRegEx" : "Almeida et al\\.",
      "year" : 2015
    }, {
      "title" : "Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis",
      "author" : [ "Alexandra Balahur", "Marco Turchi." ],
      "venue" : "Computer Speech & Language 28(1):56–75.",
      "citeRegEx" : "Balahur and Turchi.,? 2014",
      "shortCiteRegEx" : "Balahur and Turchi.",
      "year" : 2014
    }, {
      "title" : "Overview of the evalita 2016 sentiment polarity classification task",
      "author" : [ "Francesco Barbieri", "Valerio Basile", "Danilo Croce", "Malvina Nissim", "Nicole Novielli", "Viviana Patti." ],
      "venue" : "Proceedings of Third Italian Conference on Computational Linguistics",
      "citeRegEx" : "Barbieri et al\\.,? 2016",
      "shortCiteRegEx" : "Barbieri et al\\.",
      "year" : 2016
    }, {
      "title" : "Holistic sentiment analysis across languages: Multilingual supervised latent dirichlet allocation",
      "author" : [ "Jordan Boyd-Graber", "Philip Resnik." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Boyd.Graber and Resnik.,? 2010",
      "shortCiteRegEx" : "Boyd.Graber and Resnik.",
      "year" : 2010
    }, {
      "title" : "Adversarial deep averaging networks for cross-lingual sentiment classification",
      "author" : [ "Xilun Chen", "Ben Athiwaratkun", "Yu Sun", "Kilian Weinberger", "Claire Cardie." ],
      "venue" : "arXiv preprint arXiv:1606.01614 .",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Typology of sentence negation",
      "author" : [ "Östen Dahl." ],
      "venue" : "Linguistics 17(1-2):79–106.",
      "citeRegEx" : "Dahl.,? 1979",
      "shortCiteRegEx" : "Dahl.",
      "year" : 1979
    }, {
      "title" : "Using sentiwordnet for multilingual sentiment analysis",
      "author" : [ "Kerstin Denecke." ],
      "venue" : "Data Engineering Workshop, 2008. ICDEW 2008. IEEE 24th International Conference on. pages 507–512.",
      "citeRegEx" : "Denecke.,? 2008",
      "shortCiteRegEx" : "Denecke.",
      "year" : 2008
    }, {
      "title" : "Distributional correspondence indexing for cross-lingual and cross-domain sentiment classification",
      "author" : [ "Alejandro Moreo Fernández", "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "Journal of Artificial Intelligence Research 55:131–163.",
      "citeRegEx" : "Fernández et al\\.,? 2015",
      "shortCiteRegEx" : "Fernández et al\\.",
      "year" : 2015
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "Aistats. volume 9, pages 249–256.",
      "citeRegEx" : "Glorot and Bengio.,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Learning distributed representations of sentences from unlabelled data",
      "author" : [ "Felix Hill", "Kyunghyun Cho", "Anna Korhonen." ],
      "venue" : "arXiv preprint arXiv:1602.03483 .",
      "citeRegEx" : "Hill et al\\.,? 2016",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Mohit Iyyer", "Varun Manjunatha", "Jordan L BoydGraber", "Hal Daumé III." ],
      "venue" : "ACL (1). pages 1681–1691.",
      "citeRegEx" : "Iyyer et al\\.,? 2015",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Advances in neural information processing systems. pages 3294–3302.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Structural attention neural networks for improved sentiment analysis",
      "author" : [ "Filippos Kokkinos", "Alexandros Potamianos." ],
      "venue" : "EACL 2017.",
      "citeRegEx" : "Kokkinos and Potamianos.,? 2017",
      "shortCiteRegEx" : "Kokkinos and Potamianos.",
      "year" : 2017
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V Le", "Tomas Mikolov." ],
      "venue" : "ICML. volume 14, pages 1188–1196.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Visualizing and understanding neural models in nlp",
      "author" : [ "Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1506.01066 .",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Joint bilingual sentiment classification with unlabeled parallel corpora",
      "author" : [ "Bin Lu", "Chenhao Tan", "Claire Cardie", "Benjamin K Tsou." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Lu et al\\.,? 2011",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2011
    }, {
      "title" : "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual",
      "author" : [ "Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli" ],
      "venue" : null,
      "citeRegEx" : "Marelli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Evaluating neural word representations in tensor-based compositional settings",
      "author" : [ "Dmitrijs Milajevs", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver." ],
      "venue" : "idea 10(47):39.",
      "citeRegEx" : "Milajevs et al\\.,? 2014",
      "shortCiteRegEx" : "Milajevs et al\\.",
      "year" : 2014
    }, {
      "title" : "Composition in distributional models of semantics",
      "author" : [ "Jeff Mitchell", "Mirella Lapata." ],
      "venue" : "Cognitive science 34(8):1388–1429.",
      "citeRegEx" : "Mitchell and Lapata.,? 2010",
      "shortCiteRegEx" : "Mitchell and Lapata.",
      "year" : 2010
    }, {
      "title" : "Contextual bidirectional long short-term memory recurrent neural network language models: A generative approach to sentiment analysis",
      "author" : [ "Amr El-Desoky Mousa", "Björn Schuller." ],
      "venue" : "EACL 2017.",
      "citeRegEx" : "Mousa and Schuller.,? 2017",
      "shortCiteRegEx" : "Mousa and Schuller.",
      "year" : 2017
    }, {
      "title" : "An exploration of discourse-based sentence spaces for compositional distributional semantics",
      "author" : [ "Tamara Polajnar", "Laura Rimell", "Stephen Clark." ],
      "venue" : "Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics (LSDSem).",
      "citeRegEx" : "Polajnar et al\\.,? 2015",
      "shortCiteRegEx" : "Polajnar et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient, compositional, order-sensitive n-gram embeddings",
      "author" : [ "Adam Poliak", "Pushpendre Rastogi", "M Patrick Martin", "Benjamin Van Durme." ],
      "venue" : "EACL 2017 page 503.",
      "citeRegEx" : "Poliak et al\\.,? 2017",
      "shortCiteRegEx" : "Poliak et al\\.",
      "year" : 2017
    }, {
      "title" : "Divergence from syntax to linear order in ancient greek lexical networks",
      "author" : [ "Edoardo Maria Ponti." ],
      "venue" : "The 29th International FLAIRS Conference.",
      "citeRegEx" : "Ponti.,? 2016",
      "shortCiteRegEx" : "Ponti.",
      "year" : 2016
    }, {
      "title" : "Semeval-2016 task 5: Aspect based sentiment analysis",
      "author" : [ "talia Loukachevitch", "Evgeny Kotelnikov", "Nuria Bel", "Salud Marıa Jiménez-Zafra", "Gülsen Eryigit" ],
      "venue" : "In Proceedings of the 10th International Workshop on Semantic Evaluation,",
      "citeRegEx" : "Loukachevitch et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Loukachevitch et al\\.",
      "year" : 2016
    }, {
      "title" : "Relpron: A relative clause evaluation dataset for compositional distributional semantics",
      "author" : [ "Laura Rimell", "Jean Maillard", "Tamara Polajnar", "Stephen Clark." ],
      "venue" : "Computational Linguistics .",
      "citeRegEx" : "Rimell et al\\.,? 2016",
      "shortCiteRegEx" : "Rimell et al\\.",
      "year" : 2016
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal." ],
      "venue" : "IEEE Transactions on Signal Processing 45(11):2673–2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the conference on",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Dls@ cu: Sentence similarity from word alignment and semantic vector composition",
      "author" : [ "Md Arafat Sultan", "Steven Bethard", "Tamara Sumner." ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation. pages 148–153.",
      "citeRegEx" : "Sultan et al\\.,? 2015",
      "shortCiteRegEx" : "Sultan et al\\.",
      "year" : 2015
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol." ],
      "venue" : "Proceedings of the 25th international conference on Machine learning. ACM, pages 1096–",
      "citeRegEx" : "Vincent et al\\.,? 2008",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "Charagram: Embedding words and sentences via character n-grams",
      "author" : [ "John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu." ],
      "venue" : "arXiv preprint arXiv:1607.02789 .",
      "citeRegEx" : "Wieting et al\\.,? 2016",
      "shortCiteRegEx" : "Wieting et al\\.",
      "year" : 2016
    }, {
      "title" : "Compositional matrix-space models for sentiment analysis",
      "author" : [ "Ainur Yessenalina", "Claire Cardie." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 172–182.",
      "citeRegEx" : "Yessenalina and Cardie.,? 2011",
      "shortCiteRegEx" : "Yessenalina and Cardie.",
      "year" : 2011
    }, {
      "title" : "A subspace learning framework for cross-lingual sentiment classification with partial parallel data",
      "author" : [ "Guangyou Zhou", "Tingting He", "Jun Zhao", "Wensheng Wu." ],
      "venue" : "Proceedings of the international",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    }, {
      "title" : "Cross-lingual sentiment classification with bilingual document representation learning",
      "author" : [ "Xinjie Zhou", "Xianjun Wan", "Jianguo Xiao" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Moreover, several methods were considered, based on both the composition of word embeddings (Milajevs et al., 2014; Marelli et al., 2014; Sultan et al., 2015) and direct generation (Hill et al.",
      "startOffset" : 92,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "Moreover, several methods were considered, based on both the composition of word embeddings (Milajevs et al., 2014; Marelli et al., 2014; Sultan et al., 2015) and direct generation (Hill et al.",
      "startOffset" : 92,
      "endOffset" : 158
    }, {
      "referenceID" : 29,
      "context" : "Moreover, several methods were considered, based on both the composition of word embeddings (Milajevs et al., 2014; Marelli et al., 2014; Sultan et al., 2015) and direct generation (Hill et al.",
      "startOffset" : 92,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : ", 2015) and direct generation (Hill et al., 2016).",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 28,
      "context" : "The interaction is deemed to be recursive, hence some approaches take into account word order and phrase boundaries in order to apply the correct composition (Socher et al., 2013).",
      "startOffset" : 158,
      "endOffset" : 179
    }, {
      "referenceID" : 24,
      "context" : "However, some languages lack continuous constituents and a rigid word order (Ponti, 2016).",
      "startOffset" : 76,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "In particular, we also evaluate a bidirectional LSTM (Li et al., 2015) on the same task.",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "Sentiment classification achieves unsatisfactory results in resource-lean languages because of the scarcity of resources to train dedicated models (Denecke, 2008).",
      "startOffset" : 147,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : "This afflicts state-of-art deep learning architectures even more compared to traditional machine learning algorithms (Chen et al., 2016).",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010).",
      "startOffset" : 39,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 33,
      "context" : ", 2015), also leveraging non-parallel data (Zhou et al., 2015).",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "Alternatively, sentences from another languages are translated into English and assigned a sentiment based on lexical resources (Denecke, 2008) or supervised methods (Balahur and Turchi, 2014).",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "Alternatively, sentences from another languages are translated into English and assigned a sentiment based on lexical resources (Denecke, 2008) or supervised methods (Balahur and Turchi, 2014).",
      "startOffset" : 166,
      "endOffset" : 192
    }, {
      "referenceID" : 0,
      "context" : "The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al., 2015). Chen et al. (2016) devised a multi-task network where an adversarial branch spurs the shared layers to learn language-independent features.",
      "startOffset" : 66,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al., 2015). Chen et al. (2016) devised a multi-task network where an adversarial branch spurs the shared layers to learn language-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language.",
      "startOffset" : 66,
      "endOffset" : 310
    }, {
      "referenceID" : 15,
      "context" : "Recurrent models include bi-directional LSTMs (Li et al., 2015), possibly enriched with context (Mousa and Schuller, 2017).",
      "startOffset" : 46,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : ", 2015), possibly enriched with context (Mousa and Schuller, 2017).",
      "startOffset" : 40,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "Subsequent improvements of this line of research include the Structural Attention Neural Networks (Kokkinos and Potamianos, 2017), which adds structural information around each node of a syntactic tree.",
      "startOffset" : 98,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "Recurrent models include bi-directional LSTMs (Li et al., 2015), possibly enriched with context (Mousa and Schuller, 2017). On the other hand, Socher et al. (2013) put forth a Recursive Neural Tensor Network, which composes representations recursively through a single tensor-based composition function.",
      "startOffset" : 47,
      "endOffset" : 164
    }, {
      "referenceID" : 32,
      "context" : "Zhou et al. (2016) captured shared high-level features across aligned sentences through autoencoders.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, Fernández et al. (2015) exploited bilin-",
      "startOffset" : 19,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "Le and Mikolov (2014) concatenated sentence representations obtained through variants of Paragraph Vector and trained a Logistic Regression model on top of them.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 26,
      "context" : "Notwithstanding their simplicity, they are hardly outperformed by more sophisticated operations (Rimell et al., 2016).",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 32,
      "context" : "Some of these compositional representations based on matrix multiplication were also evaluated on sentiment classification (Yessenalina and Cardie, 2011).",
      "startOffset" : 123,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "Mitchell and Lapata (2010) explored two operations: addition and multiplication.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 23,
      "context" : "This excludes methods concerned with phrases, like the ECO embeddings (Poliak et al., 2017), or requiring structured knowledge, like CHARAGRAM (Wieting et al.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 31,
      "context" : ", 2017), or requiring structured knowledge, like CHARAGRAM (Wieting et al., 2016).",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : ", the surveys were limited to English and evaluated on other tasks), most notably Hill et al. (2016) and Milajevs et al.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : ", the surveys were limited to English and evaluated on other tasks), most notably Hill et al. (2016) and Milajevs et al. (2014); ii) the variety of their modelling assumptions and features encoded.",
      "startOffset" : 82,
      "endOffset" : 128
    }, {
      "referenceID" : 20,
      "context" : "The representation of a sentence was obtained via element-wise addition of the vectors of the words belonging to it (Mitchell and Lapata, 2010).",
      "startOffset" : 116,
      "endOffset" : 143
    }, {
      "referenceID" : 22,
      "context" : "It hinges on a sentence-level distributional hypothesis (Polajnar et al., 2015; Kiros et al., 2015).",
      "startOffset" : 56,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "It hinges on a sentence-level distributional hypothesis (Polajnar et al., 2015; Kiros et al., 2015).",
      "startOffset" : 56,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Hence, it was preferred over SkipThought (Kiros et al., 2015) because of i) these efficiency issues and ii) its competitive performances reported by Hill et al.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "The FastSent model was proposed by Hill et al. (2016). It hinges on a sentence-level distributional hypothesis (Polajnar et al.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "The FastSent model was proposed by Hill et al. (2016). It hinges on a sentence-level distributional hypothesis (Polajnar et al., 2015; Kiros et al., 2015). In other terms, it assumes that the meaning of a sentence can be inferred by the neighbour sentences in a text. It is a simple additive log-linear model conceived to mitigate the computational expensiveness of algorithms based on a similar assumption. Hence, it was preferred over SkipThought (Kiros et al., 2015) because of i) these efficiency issues and ii) its competitive performances reported by Hill et al. (2016). In FastSent, sentences are represented as bags of words: a context of sentences is used to predict the adjacent sentence.",
      "startOffset" : 35,
      "endOffset" : 576
    }, {
      "referenceID" : 30,
      "context" : "In DAE, the input representation is corrupted by a noise function and the algorithms learns to recover the original (Vincent et al., 2008).",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "This architecture was later adapted to encode and decode variablelength inputs, and the corruption process was implemented in the form of dropout (Iyyer et al., 2015).",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 9,
      "context" : "In the implementation by Hill et al. (2016),2 the corruption function is defined as f(S|po, px).",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : "Paragraph Vector is a collection of log-linear models proposed by Le and Mikolov (2014) for paragraph/sentence representation.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 2,
      "context" : "Moreover, we added data for Italian from the SENTIPOLC shared task in EVALITA 2016 (Barbieri et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "The weights are initialized drawing from the random distribution proposed by Glorot and Bengio (2010). The cross-entropy loss was normalized with the L2norm of the weights scaled by λ = 10−3.",
      "startOffset" : 77,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "LSTM is a refinement associating each time epoch with an input, control and memory gate, in order to filter out irrelevant information (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 135,
      "endOffset" : 169
    }, {
      "referenceID" : 27,
      "context" : "This model is bi-directional if it is split in two branches reading simultaneously the sentence in opposite directions (Schuster and Paliwal, 1997).",
      "startOffset" : 119,
      "endOffset" : 147
    }, {
      "referenceID" : 15,
      "context" : "This technique is presented and detailed by Li et al. (2015). The results are shown in Figure 2.",
      "startOffset" : 44,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "This demonstrates i) that no algorithm is the best across the board; and ii) that some simple models are to be preferred even for downstream tasks, which partially contrasts with the conclusions of Hill et al. (2016). Moreover, representation algorithms sensitive to word order have similar trends, but they do not always achieve performance superior to algorithms based on the sentence order.",
      "startOffset" : 198,
      "endOffset" : 217
    } ],
    "year" : 2017,
    "abstractText" : "Distributed representations of sentences have been developed recently to represent their meaning as real-valued vectors. However, it is not clear how much information such representations retain about the polarity of sentences. To study this question, we decode sentiment from sentence representations learned with different architectures (sensitive to the order of words, the order of sentences, or none) in 9 typologically diverse languages. Sentiment results from the (recursive) composition of lexical items and grammatical strategies such as negation and concession. The results are manifold: we show that there is no ’one-sizefits-all’ representation architecture outperforming the others across the board. Rather, the top-ranking architectures depend on the language at hand. Moreover, we find that in several cases the additive composition model based on skip-gram word vectors may surpass state-of-art architectures such as bi-directional LSTMs. Finally, we provide a possible explanation of the observed variation based on the type of negative constructions in each language.",
    "creator" : "LaTeX with hyperref package"
  }
}