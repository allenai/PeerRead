{
  "name" : "1703.08013.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Content-based similar document image retrieval using fusion of CNN features",
    "authors" : [ "Mao Tan", "Siping Yuan", "Yongxin Su" ],
    "emails" : [ "mr.tanmao@gmail.com", "201610171906@smail.xtu.edu.cn", "su_yong_xin@163.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Rapid increase of digitized document give birth to high demand of document image retrieval. While conventional document image retrieval approaches depend on complex OCR-based text recognition and text similarity detection, this paper proposes a new content-based approach, in which more attention is paid to features extraction and fusion. In the proposed approach, multiple features of document images are extracted by different CNN models. After that, the extracted CNN features are reduced and fused into weighted average feature. Finally, the document images are ranked based on feature similarity to a provided query image. Experimental procedure is performed on a group of document images that transformed from academic papers, which contain both English and Chinese document, the results show that the proposed approach has good ability to retrieve document images with similar text content, and the fusion of CNN features can effectively improve the retrieval accuracy.\nCCS CONCEPTS\n• Computing methodologies →Artificial intelligence; Computer vision; Computer vision tasks; Visual content-based indexing and retrieval\nKEYWORDS\nText retrieval, document image retrieval, convolutional neural networks, feature fusion, multi models fusion"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Due to development of digital media technology, the scale of multimedia resources including the document images is getting bigger and bigger. Document image retrieval, the task of which is to find useful information or similar document images from a large dataset for a given user query, has become an important research domain in natural language processing. Many approaches based on Optical Character Recognition (OCR) have been proposed, which recognize text content from images and then use text similarity detection to implement document image retrieval system.\nConventional document image retrieval depends on complex model of the OCR-based approach, has some weaknesses such as high computational cost, language dependency, and it is sensitive\nto image resolution. Direct recommendation and retrieval on the basis of arbitrary multi-character text in unconstrained image require a recognition-free retrieval approach to learn and recognize deep visual features in images. The new document image recognition-free retrieval approach will be conducive to detect the re-contributed and re-published text content on the database of academic journals theses, or query the relevant literature in massive resources.\nDocument images may be noisy, distorted, and skewed, digitized text need to be processed using different pre-processing methods. According to the type of document image dataset, various pre-processing methods are applied to the document images. In some cases, converting colourful images to grayscale images, adjustment of images' sizes, border removal and normalization of the text line width in the initial steps can enhance document images [1-3].\nIn early studies on text recognition and retrieval, the extraction of features requires layout analysis, line segmentation, word segmentation, word recognition, etc. But over the last decade, deep learning based features extraction has become an key research direction. Among various deep learning models, the Convolutional Neural Networks (CNNs) are the most powerful networks in image processing tasks. When CNNs are trained in images database, a deep representation of the image is constructed to make object information increasingly explicit along the processing hierarchy [4]. During the CNN feature training phase, Redmon et al. [5] proposed an improved model that inspired by the GoogLeNet model[6] for image classification. They pretrained the model’s convolutional layers on ImageNet dataset for approximately a week, and used the initial convolutional layers of the network to extract features from the image while the fully connected layers to predict the result. Gatys et al. [7] obtained a style representation of an input image and generated results on the basis of the VGGNet, which is a CNN that rivals human performance on a common visual object recognition benchmark task [8]. For learning visual features of multi-character text, Ian et al. [9] proposed a unified approach that integrates the localization, segmentation, and recognition steps via the use of a deep convolutional neural network that operates directly on the image pixels. Hong et al. [10] studied the efficacy of the conceptual relationships by applying them to augment imperfect image tags, and then the relevant results are subsequently used in contentbased image retrieval for improving efficiency.\nHowever, compared with other similar methods, the parameter space of CNN network is too large to train a CNN model in a short time. Fortunately, there are some open pretrained models that we can easily use, such as MatConvNet [11]. Besides that, training the CNN features on a large dataset and fine-tuning by target dataset can significantly improve the performance [12]. Furthermore, we can use the PCA method to reduce the dimension of the CNN features according to the investigation in reference [13], which is mainly to evaluate the performance of compressed neural codes, and it declared that plain PCA or a combination of PCA with discriminative dimensionality reduction can result in very short codes and good (state-of-the-art) performance.\nTo the best of our knowledge, model fusion is a very powerful technique to increase accuracy on a variety of machine learning tasks. The most basic and convenient way to fusion is to ensemble the features or predictions from multiple different models on the test set, which is a quick way to ensemble already existing model when teaming up. When averaging the outputs from multiple different models, not all predictors are perfectly calibrated or the predictions clutter around a certain range. Fusion methods is key to the solutions, better results can be obtained, if it is given by a linear combination of the ensemble the features or predictions. In this case, the combination coefficients have to be determined by some optimization procedure [14]. A ranking average is proposed in [15] that first turn the predictions into ranks, and then averaging these ranks, which do well on improving the exact same fusion used an average. Moreira et al. [16] specifically tested two un-supervised rank aggregation approaches well known in the information retrieval literature, namely CombSUM and CombMNZ. These algorithms are used to aggregate the information gathered from different outputs or features in order to achieve more accurate ranking results than using individual scores.\nSimilarity measurement is another key technique to determine the effectiveness of the retrieval system. There are many ways to measure the similarity of image content. An efficient and widespread method is computing pair-wise image cosine similarity based on visual features of all images, and then used this parameter value to retrieve the high similarity images [17].\nIn this paper, we try to establish a content-based approach to document image retrieval with the purpose of finding out the similar document through a query document image. We choose a document image similarity retrieval method with CNN feature extraction and cosine similarity matching as a basic framework. At the same time, a multiple models fusion method is proposed, which using Rank_age of each CNN network to obtain the weighted average fusion feature, and then integrate these methods in the framework in order to improve the accuracy of retrieval system. In the experimental procedure, we slice a batch of English and Chinese academic papers into document images as the image dataset, a group of document images with changed text content is used as the query image, several case studies are provided to evaluate the adaptability and accuracy of the proposed method in different conditions."
    }, {
      "heading" : "2 METHODOLOGY",
      "text" : "In this section, we mainly discuss several key steps of the document image similarity retrieval based on the multiple CNN models fusion features of images. Firstly, we fine-tune the pretrained CNN models using MatConvNet, and set repeatedly the crop size of experimental document image. After that, we use multiple different fine-tuned CNN models to extract diverse CNN features from experimental document image dataset, which can convert the visual content into a deep representation. As the CNN feature matrix trained by the CNN model are high-dimensional, we further perform the PCA method to reduce the dimensions and make the each CNN feature matrix has identical dimension in order to subsequent model fusion. Then ensemble the multiple CNN feature matrix by corresponding combination coefficients that calculated from the Rank_age of its CNN network, obtain a weighted average fusion feature. After that, we compute and rank the cosine similarity of document images to the query images based on the weighted average fusion feature, output the final retrieval result. In the following section, we elaborate on each of steps in detail, the entire processes are shown in Figure 1.\nAs shown in Figure 1, firstly, we convert original images to processed images by using some mature pre-processing methods. We extract the CNN feature to obtain the deep visual representations by fine-tune the multiple pre-trained CNN network model on the target document image dataset. After obtaining the CNN feature matrix, we reduce the dimension of the matrix and improve the efficiency of the algorithm by PCA. Then, we ensemble the various features from multiple network models based on Rank_age value. We measure the cosine similarity between the query document image and each image in the training dataset based on multiple model fusion features, and show the most similar images."
    }, {
      "heading" : "2.1 CNN Feature Extraction",
      "text" : "It is necessary to extract the primitive features of document image as the constructive parameter of the training model. The quality of the feature extraction directly determines the retrieval effect.\nRecently, CNNs have achieved impressive results in some areas such as image recognition and object detection. It can input image into the network directly, avoiding the complex feature extraction and data reconstruction process in traditional recognition algorithm. As described above, we choose some state-of-the-art CNN models that submitted for the ImageNet challenge over the last 5 years as the training network. Among them, AlexNet, the first entry that use a deep neural network in 2012 ImageNet competition, has strong generalization ability in computer vision tasks. VGGNet is a preferred multi-layer neural network model for extracting the CNN features of image. The VGGNet use small-size convolution filters and deep network layers, which also has strong generalization ability in many computer vision applications and other image recognition datasets. GoogLeNet can improve utilization of the computing resources inside the network by a carefully crafted design, which allows for increasing the depth and width of the network while keeping the computational budget constant, has good prediction performance in image classification. In addition, ResNet uses a residual learning framework to ease the training of deeper networks but still owes low complexity of the network, and it outperforms the humanlevel accuracy in the 2015 ImageNet competition.\nIn general, most CNN models are trained by composing simple linear or non-linear filtering operations, while their implementation need to be trained on large dataset and learned from vast amounts of data. Therefore, we fine-tune the above mentioned pre-trained models on the target document image dataset. At the training phase, we input the fixed-size images that turned by a series of pre-processing to multiple networks and removes the mean. After that, we retain the CNN feature matrix of the penultimate layer of this deep CNN representation, which can be used as a powerful image descriptor applicable to many types of datasets."
    }, {
      "heading" : "2.2 Dimension Reduction by PCA",
      "text" : "After CNN feature extraction with various CNN model, we obtain some high-dimensional image deep representation. We use the PCA method to compress the CNN feature matrix to 256-D. It reduces some information redundancy, and make the CNN feature matrix has identical dimension to facilitate the subsequent model fusion.\nIn order to avoid the influence of the sample units, and simplify the calculation of covariance matrix, we use the PCA method to find the 256 largest variation feature vectors in this matrix. Therefore, covariance matrix C can be calculated according to each feature vector xi in normalized CNN feature matrix, which can be expressed as\n1\n1 = ,\nn\nn T\ni i\ni C x x   (1)\nwhere C represents the covariance matrix of the feature matrix,\nand n represents the number of feature vectors.\nAfter that, the eigenvalue equation based on C can be\nexpressed as\n,i i iC   (2)\nwhere i is the eigenvalue of the covariance matrix, and i is the corresponding eigenvector of the covariance matrix.\nThen, we use the resulting 256 normalized feature vectors to constitute the main feature matrix to form a 256-D space. Based on that, we project the high-dimensional CNN feature matrix onto the 256-D dimensional space. Finally, the CNN feature projection matrix is indexed to improve the retrieval efficiency."
    }, {
      "heading" : "2.3 Fusion of CNN Features",
      "text" : "Through the above method, we obtain various fine-turned CNN models to extract image features. It has been confirmed that creating ensembles from multiple individual files can reduces the generalization error. Therefore, we fuse the features from multiple different existing models respectively, propose the multiple models fusion method based on Rank_age.\nThe features trained by different CNN models might represent different characteristics of document image, and utilizing different features effectively through multiple models fusion method will have positive effect on document image similarity retrieval. We improve the model fusion method that based on ranking average in [15], ensemble the features from multiple model by corresponding combination coefficients that calculated from the Rank_age of its model network.\nA small scale document image dataset is created in advance to calculate the Rank_age of each model, which include 422 pair similar document images and the index of each pair of images. Then, the Rank_age of each model can be calculated according to the retrieval results that learned by corresponding model on this dataset, which is more adaptable to ensemble different models that have significant difference. The Rank_age can be calculated as\nn\ni=1 i\nscore Rank_age = ,\nrank  (3)\nwhere n=422, score is the mean accuracy in the top-5 similar images to the each query when using certain model, ranki is the ranking of the i-th image's similar image in its retrieval result.\nAfter that, normalizing the Rank_age between 0 and 1 can get the corresponding combination coefficient ε. Finally, we ensemble the three CNN feature matrix MVD, MVE and MG trained by VGGNet-D, VGGNet-E and GoogLeNet respectively according to corresponding ε and obtain the weighted average fusion feature that can be expressed as\n,VD VD VE VE G GM = * M + * M + * M   (4)\nwhere εVD, εVE and εG are the corresponding coefficients for the feature matrix to ensemble, and εVD + εVE + εG = 1."
    }, {
      "heading" : "2.4 Similarity Metric",
      "text" : "Cosine similarity has been proved to be an effective metric system because of its accuracy. The 256-D weighted average fusion feature matrix [z1, z2, …, zn] T could describe the main CNN features of the document images in the dataset, where n is the\nnumber of document images in the datasets. The cosine similarity calculated from CNN feature vector can approximately measure the similarity between document images.\nFor each pair of feature vector (Zu, Zv) where u ≠ v, the pair-\nwise image cosine similarity Ts can be expressed as\n     \n   \n1\n2 2\n1 1\n, * ,\n, ,\n, * ,\nk\nu i v i\ni\ns u v k k\nu i v i\ni i\nF Z u F Z v\nT Z Z\nF Z u F Z v\n\n \n \n  (5)\nwhere K = 256, and F(Zu ,ui ) is the value of the i-th column element of the 256-D dimensional feature vector corresponding to the document image Zu. Ts(Zu ,Zv ) is the pair-wise document image cosine similarity. Through equation (5), we can retrieve out some high similarity document images to query image."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 Data Collection and Evaluation Metric",
      "text" : "In this work, to evaluate the proposed method, we collect a group of English and Chinese academic papers as the text database, and cut them into many small pieces of heterogeneous document image to construct a training dataset, which contains 2017 images totally. Then, we select some text paragraphs from the original article and edit them by various ways. After that, we store the edited text paragraphs as images to construct an query image dataset including 422 images totally, which is used to evaluate the accuracy of the proposed approach in various situations. In addition, we select the 422 query images and their original images to construct a small scale document image dataset, and create a <query image name, original image name> index to calculate the Rank_age of each CNN network in advance.\nWe performed different experiments with different CNN model. 422 query document images is selected to retrieve out the similar document images in image dataset. The proposed method is evaluated using the accuracy value measured based on the results ranked among the Top-1, Top-3, Top-5 and Top-10 similar images to a query document image."
    }, {
      "heading" : "3.2 Experimental Results and Analyses",
      "text" : "The training and query dataset includes English and Chinese document images, and there are 10 types edited images in query dataset, including retranslating by Google, changing the font color, adding another statement in the content, omitting lots of content, adjusting the line spacing of the text and reversing the word order, and so on. Therefore, we retrieve separately different images each time to see the retrieval effect of different text language and content, or local deformation of layout. At first, we choose an English document image as query image, which is converted from the abstract of the an English article, The query document image is shown as Figure 2(a). After that, we calculate the query document image’s similarity to each document image in training dataset by using MMF (VGGNet-D + VGGNet-E + GoogLeNet),\nthe original document image that is shown in Figure 2(b) can be retrieved out in first.\nAnother case study is provided to evaluate the retrieval effect when querying through Chinese document image, which is retranslated by Google and modified in its original text content. The Top-1 retrieval result image is shown in Figure 3, and in these result we can see that similar text content with some different characters and visual presentations can be recognized by the proposed approach.\nThen we consider the Top-1, Top-3, Top-5 and Top-10 accuracy in ranked result using various individual CNN model, and compare them with the accuracy that obtained by multiple models using weighted average fusion feature. The condition that fuse the features of AlexNet and VGGNet-E is named MMF-1, the fusion of AlexNet, VGGNet-D and VGGNet-E is named MMF-2, the fusion of AlexNet and GoogLeNet is named MMF-3, and MMF-4 represents fusion of AlexNet and ResNet-152. According to experimental performance, for GoogLeNet and ResNet, the crop size of document images in the case is fixed as 288×288, and for other models it is set to 256×256. The accuracies obtained from the above situation are shown in Table 1. During the model fusion, we got the Rank_age and ε of each CNN network in advance, which is obtained by training each network with the small scale document image dataset.\nIn Table 1, it can be seen that by features fusion, as a whole the retrieval accuracy are improved, MMF-1 and MMF-2 obtains better performance on retrieval accuracy than the best individual\nmodel AlexNet in our case, which result is mainly caused by good individual models. At the same time, it should be noted that the accuracy improvement fluctuate slightly if GoogLeNet or ResNet152 is adopted in features fusion. We can see that minor performance improvement in MMF-3 but slight accuracy decreases of MMF-4 in TOP-5 retrieval, which should be caused by the low accuracy of individual model in the two fusion models. The above results show us that the chosen of individual CNN model with good performance is important for the proposed fusion approach. Further in more, Figure 4 illustrates intuitively that the similarity retrieval results through various individual models and the proposed fusion method."
    }, {
      "heading" : "4 CONCLUSIONS",
      "text" : "In this paper, a new content-based approach to document image retrieval is proposed. All of the experimental results indicate that the proposed approach is effective to realize document image recognition-free retrieval for different language characters without using OCR. By using Rank_age to fuse the features obtained from several classical CNN model, the retrieval accuracy can be significantly improved in most of conditions with different transformations of text content or layout. In our next works, for obtaining higher retrieval accuracy, more methods will be chosen and tested to fuse multiple CNN models. When this approach is further improved to adapt more complex transformations, it is expected to be applied in paper plagiarism identification or literature recommendation."
    } ],
    "references" : [ {
      "title" : "Segmentation-free word spotting in historical printed documents",
      "author" : [ "B Gatos", "I. Pratikakis" ],
      "venue" : "Document Analysis and Recognition,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "A novel word spotting method based on recurrent neural networks",
      "author" : [ "V Frinken", "A Fischer", "R Manmatha" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Unified photo enhancement by discovering aesthetic communities from flickr",
      "author" : [ "R Hong", "L Zhang", "D. Tao" ],
      "venue" : "IEEE transactions on Image Processing",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks. arXiv:1505.07376, http://arxiv.org/abs/1505.07376",
      "author" : [ "L.A. Gatys", "A.S. Ecker", "M. Bethge" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "You Only Look Once: Unified, Real- Time Object Detection",
      "author" : [ "J Redmon", "S Divvala", "R Girshick" ],
      "venue" : "Computer Science,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C Szegedy", "W Liu", "Y Jia" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks",
      "author" : [ "J Goodfellow I", "Y Bulatov", "J Ibarz" ],
      "venue" : "Computer Science",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "MatConvNet: Convolutional Neural Networks for MATLAB",
      "author" : [ "A Vedaldi", "K. Lenc" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Learning visual semantic relationships for efficient visual retrieval",
      "author" : [ "R Hong", "Y Yang", "M Wang", "XS. Hua" ],
      "venue" : "IEEE Transactions on Big Data",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Return of the Devil in the Details: Delving Deep into Convolutional Nets",
      "author" : [ "K Chatfield", "K Simonyan", "A Vedaldi" ],
      "venue" : "Computer Science",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Combining predictions for accurate recommender systems",
      "author" : [ "M Jahrer", "A Töscher", "R. Legenstein" ],
      "venue" : "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Using rank aggregation for expert search in academic digital libraries",
      "author" : [ "C Moreira", "B Martins", "P. Calado" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Image recommendation based on keyword relevance using absorbing Markov chain and image features",
      "author" : [ "D Sejal", "V Rashmi", "R. Venugopal K" ],
      "venue" : "International Journal of Multimedia Information Retrieval,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In some cases, converting colourful images to grayscale images, adjustment of images' sizes, border removal and normalization of the text line width in the initial steps can enhance document images [1-3].",
      "startOffset" : 198,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "In some cases, converting colourful images to grayscale images, adjustment of images' sizes, border removal and normalization of the text line width in the initial steps can enhance document images [1-3].",
      "startOffset" : 198,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "In some cases, converting colourful images to grayscale images, adjustment of images' sizes, border removal and normalization of the text line width in the initial steps can enhance document images [1-3].",
      "startOffset" : 198,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "When CNNs are trained in images database, a deep representation of the image is constructed to make object information increasingly explicit along the processing hierarchy [4].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "[5] proposed an improved model that inspired by the GoogLeNet model[6] for image classification.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[5] proposed an improved model that inspired by the GoogLeNet model[6] for image classification.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "[9] proposed a unified approach that integrates the localization, segmentation, and recognition steps via the use of a deep convolutional neural network that operates directly on the image pixels.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[10] studied the efficacy of the conceptual relationships by applying them to augment imperfect image tags, and then the relevant results are subsequently used in contentbased image retrieval for improving efficiency.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "Fortunately, there are some open pretrained models that we can easily use, such as MatConvNet [11].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "Besides that, training the CNN features on a large dataset and fine-tuning by target dataset can significantly improve the performance [12].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : "In this case, the combination coefficients have to be determined by some optimization procedure [14].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "[16] specifically tested two un-supervised rank aggregation",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "An efficient and widespread method is computing pair-wise image cosine similarity based on visual features of all images, and then used this parameter value to retrieve the high similarity images [17].",
      "startOffset" : 196,
      "endOffset" : 200
    } ],
    "year" : 2017,
    "abstractText" : "Rapid increase of digitized document give birth to high demand of document image retrieval. While conventional document image retrieval approaches depend on complex OCR-based text recognition and text similarity detection, this paper proposes a new content-based approach, in which more attention is paid to features extraction and fusion. In the proposed approach, multiple features of document images are extracted by different CNN models. After that, the extracted CNN features are reduced and fused into weighted average feature. Finally, the document images are ranked based on feature similarity to a provided query image. Experimental procedure is performed on a group of document images that transformed from academic papers, which contain both English and Chinese document, the results show that the proposed approach has good ability to retrieve document images with similar text content, and the fusion of CNN features can effectively improve the retrieval accuracy.",
    "creator" : "Microsoft® Word 2010"
  }
}