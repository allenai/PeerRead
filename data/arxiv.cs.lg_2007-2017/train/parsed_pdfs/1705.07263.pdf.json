{
  "name" : "1705.07263.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Adversarial Examples Are Not Easily Detected:  Bypassing Ten Detection Methods",
    "authors" : [ "Nicholas Carlini", "David Wagner" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Recent years have seen rapid growth in the area of machine learning. Neural networks, an idea that dates back decades, have been a driving force behind this rapid advancement. Their successes have been demonstrated in awide set of domains, from classifying images [35], to beating the best humans at Go [32], to translation and NLP [29], and to self driving cars [3, 5].\nIn this paper, we study neural networks applied to image classification. While neural networks are the most accurate machine learning approach known to date, they turn out to be weak against an adversary who attempts to fool the classifier. That is, given a valid image x , an adversary can easily produce a visually similar image x ′ that has a different classification. Such an instance x ′ is known as an adversarial example [36], and they have been shown to exist in nearly all domains that neural networks are used.\nThe research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37]. Unfortunately, most of these defenses are not effective at classifying adversarial examples correctly.\nDue to this difficulty, recent work has turned to attempting to detect them instead. We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense. On simple datasets, the attacks slightly increase the distortion required, but on more complex datasets, adversarial examples are indistinguishable from the original images.\nBy studying these recent schemes that detect adversarial examples, we challenge the assumption that adversarial examples have intrinsic differences from valid images. We also use this experimentation to obtain a better understanding of the space of adversarial examples.\nWe introduce three types of attacks in this paper. First, we evaluate the security of these schemes against generic attacks that don’t take any specific measures to fool any particular detector. We use methods to generate high-confidence adversarial examples: instead of generating adversarial examples to minimize the total amount of distortion required to just cross the decision boundary, we construct\nadversarial examples that are classified as an incorrect class, with high confidence scores, but still have low distortion. We show that six of the ten defenses are significantly less effective than believed (although not completely broken) under this threat model.\nSecond, we introduce novel white-box attacks, which are tailored for defeating a particular defense. These attacks require the adversary to have knowledge of the defense’s model parameters. At a technical level, our attacks work by defining a special attackerloss function that captures the requirement that the adversarial examples must fool the defense. Our white-box attacks can break all of the defenses.\nFinally, we introduce black-box attacks, which are tailored towards a particular defense but do not require knowledge of the defense’s model parameters. These attacks work by exploiting transferability [36], and our techniques for generating adversarial examples that transfer well may be of independent interest. We again show that all of the defenses can be broken in this way.\nOur results suggest that there is a need for better ways to evaluate potential defenses. We believe our attacks would be a useful baseline: to be worth considering, a proposed defense must at least defeat the attacks described here.\nThe code to reproduce our results is available online at http://nicholas.carlini.com/code/nn_breaking_detection.\nWe make the following contributions:\n• We find that many defenses are unable to detect highconfidence adversarial examples, even when the attacker is oblivious to the specific defense used. • We break all existing detection methods in the white-box setting. • We also show how to extend our attacks to the black-box setting, where the adversary does not have access to the defense model parameters. • We provide recommendations for evaluating future defenses."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : "The remainder of this section contains a brief survey of the field of neural networks and adversarial machine learning. We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26], and [6]."
    }, {
      "heading" : "2.1 Notation",
      "text" : "Let F (·) denote a neural network used for classification. The final layer in this network is a softmax activation, so that the output is a probability distribution where F (x)i represents the probability that object x is labeled with class i .\nAll neural networks we study are feed-forward networks consisting of multiple layers F i taking as input the result of previous\nar X\niv :1\n70 5.\n07 26\n3v 1\n[ cs\n.L G\n] 2\n0 M\nay 2\n01 7"
    }, {
      "heading" : "10 classes in the test set. The second row shows untargeted adversaries examples generated by Carlini and Wagner’s attack algorithm for the L2 distance metric. Adversarial examples on MNIST have low distortion; distortion on CIFAR is 10× smaller.",
      "text" : "layers. The outputs of the final layer are known as logits; we represent them by Z (·). Some layers involve the non-linear ReLU [25] activation. Thus the ith layer computes\nF i (x) = ReLU(Ai · F i−1(x) + bi )\nwhere Ai is a matrix and bi is a vector. Let Z (x) denote the output of the last layer (before the softmax), i.e., Z (x) = Fn (x). Then the final output of the network is\nF (x) = softmax(Z (x)).\nWhen we write C(x) we mean the classification of F (·) on x :\nC(x) = arg maxi (F (x)i ).\nAlong with the neural network, we are given a set of training instances with their corresponding labels (x ,y) ∈ X."
    }, {
      "heading" : "2.2 Adversarial Examples",
      "text" : "We call an input to the classifier F (·) valid if it is instance that was benignly created. All instances in the training set and testing set are valid instances.\nGiven a network F (·) and a valid input x so that C(x) = l we say that x ′ is an (untargeted) adversarial example if x ′ is close to x and C(x ′) , l . A more restrictive case is where the adversary picks a target t , l and seeks to find x ′ close to x such that C(x ′) = t ; in this case we call x ′ a targeted adversarial example.1 When we say a neural network is robust we mean that it is difficult to find adversarial examples on it.\nTo define closeness, most attacks use an Lp distance, defined as ∥d ∥p = ( ∑n i=0 |vi |p ) 1 p . Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L∞, a measure of the maximum absolute change to any pixel [10].2 If the total distortion under any of these three distance metrics is small, the images will likely appear visually similar.\nOne further property of adversarial examples we will make use of is the transferability property [10, 36]. It is often the case that, when given two models F (·) and G(·), an adversarial example on F will also be an adversarial example on G, even if they are trained in completely different manners, on completely different training sets.\nThere has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37]. To\n1Untargeted adversarial examples are only interesting if the network predicted the instance correctly initially; targeted adversarial examples are only interesting if the target class is not the correct class. 2L0 is defined as the limit of Lδ as δ → 0 (and similarly for L∞).\ndate, no defenses has been able to classify adversarial examples correctly.\nGiven this difficulty in correctly classifying adversarial examples, recent defenses have instead turned to detecting adversarial examples and reject them. We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23]."
    }, {
      "heading" : "2.3 Threat Model",
      "text" : "We consider three different threat models in this paper:\n(1) An Oblivious Adversary generates adversarial examples on the unsecured model F and is not aware that the detectorD is in place. The detector is successful if it can detect these adversarial examples. (2) A White-Box Adversary is aware the neural network is being secured with a given detection scheme D, knows the model parameters used by D, and can use these to attempt to evade both the original network F and the detector simultaneously. (3) A Black-Box Adversary is aware the neural network is being secured with a given detection scheme, knows how it was trained, but does not have access to the trained detector D (or the exact training data).\nWe evaluate each defense under these three threat models. We discuss our evaluation technique in Section 3.\nFalse Positive vs. False Negative Tradeoff. In order to be useful in practice, an adversarial detection scheme must be able to support a very low false-positive rate while still detecting adversarial examples, since false positives correspond directly to a decrease in accuracy. The trivial defense that reports every input as being an adversarial input will have a 100% true positive rate, but also will be entirely useless.\nIn this paper, all of our attacks are constructed in such a way that the defense can detect our attack with probability no better than random guessing. Specifically, we assume the defender is willing to tolerate a false-positive rate as high as 50%, and we show attacks that reduce the true positive rate to only 50%, equivalent to random guessing.\nIn practice, a defender would probably need the false positive rate to be well below 1%, and an attacker might be satisfied with an attack that succeeds with probability well under 50%. Therefore, our attacks go well beyond what would be needed to break a scheme; they show that the defenses we analyze are not effective."
    }, {
      "heading" : "2.4 Datasets",
      "text" : "We consider two datasets in this paper.\nTheMNIST dataset [22] consists of 70, 000 28× 28 greyscale images of handwritten digits from 0 to 9. Our standard convolutional network achieves 99.4% accuracy on this dataset.\nThe CIFAR-10 dataset [20] consists of 60, 000 32 × 32 color images of ten different objects (e.g., truck, airplane, etc). This dataset is substantially more difficult: the state of the art approaches achieve 95% accuracy [33]. For comparison with prior work, we use the CIFAR-10 architecture from Metzen et al. [15]. The model is a 32- layer ResNet [14] with 470k parameters that we train with SGD with momentum set to 0.9. The learning rate is initially set to 0.1 and is reduced to 0.01 at epoch 20, and further reduced to 0.001 at epoch 40. We train with dataset augmentation, randomly flipping the image horizontally, and shifting by 3 pixels in any dimension. We apply L2 regularization and use BatchNorm [18] after every convolution. This model achieves a 91.5% accuracy.\nIn Figure 1 we show images from each of these datasets, as well as the nearest untargeted adversarial example under the L2 distance metric. Notice that the total distortion required to change classification on MNIST is much larger than that of CIFAR: because CIFAR images are harder to classify, it is easier to fool the classifier into misclassifying them.\nSome of the defenses we evaluate also argue robustness against ImageNet [7], a database of a million 224×224 images. Prior work [6, 24] has clearly demonstrated that constructing adversarial examples on ImageNet is a strictly easier task than MNIST or CIFAR, and constructing defenses is strictly harder. As a simple comparison, an ImageNet classification can be changed by only flipping the lowest bit of each pixel, whereas CIFAR requires distortions 3× larger (and MNIST 10×). We therefore do not consider ImageNet in this paper, since all defenses are broken on MNIST and CIFAR."
    }, {
      "heading" : "2.5 Defenses",
      "text" : "We briefly describe the ten proposed defenses described in the seven papers we study.\n(1) Grosse et al. [12] propose two schemes. The first uses a high-dimensional statistical test (Maximum Mean Discrepancy) to detect adversarial examples. The second trains the neural network with a new “adversarial” class. (2) Gong et al. [9] detect adversarial examples by building a second neural network that detects adversarial examples from valid images.\n(3) Metzen et al [15] follow a similar approach, but train the detector on the inner layers of the classifier. (4) Li et al. [23] propose two schemes. The first performs PCA on the internal convolutional layers of the primary network and trains classifier to distinguish between valid and adversarial data. The second scheme applies a mean-blur to images before feeding them to the network. (5) Hendrycks & Gimpel [16] perform PCA on the pixels of an image and argue adversarial examples place higher emphasis on larger components. (6) Feinman et al. [8] detect adversarial examples by keeping dropout [34] on during evaluation; additionally, they construct a kernel density measure and show that adversarial examples are drawn from a different distribution than valid images. (7) Bhagoji et al. [2] show that adversarial images require use of more PCA dimensions than valid images.\nWe summarize our results in Table 1, and Figure 6 and 7 (appendix) for images of adversarial examples on the MNIST and CIFAR datasets, when the defense success was greater than 0%."
    }, {
      "heading" : "2.6 Generating Adversarial Examples",
      "text" : "We use the L2 attack algorithm of Carlini and Wagner [6] to generate targeted adversarial examples, as it is superior to other published attacks. Given a neural network F with logits Z , the attack uses gradient descent to solve\nminimize ∥ 1 2 (tanh(w) + 1) − x ∥22 + c · ℓ( 1 2 (tanh(w) + 1))\nwhere the loss function ℓ is defined as\nℓ(x ′) = max(max{Z (x ′)i : i , t} − Z (x ′)t ,−κ).\nWe now give some intuition behind this loss function. The difference max{Z (x ′)i : i , t} − Z (x ′)t is used to compare the target class t with the next-most-likely class. However, this is minimized when the target class is significantly more likely than the second most likely class, which is not a property we want. This is fixed by taking the maximum of this quantity with −κ, which controls the confidence of the adversarial examples. When κ = 0, the adversarial examples are called low-confidence adversarial examples and are only just classified as the target class. As κ increases, the model\nclassifies the adversarial example as increasingly more likely, we call these high-confidence adversarial examples.\nThe constant c is chosen via binary search. If c is too small, the distance function dominates and the optimal solution will not have a different label. If c is too large, the objective term dominates and the adversarial example will not be nearby.\nAside from C&W’s attack, there are two other weaker attacks that are used throughout the literature to evaluate defenses. As we will see throughout this paper, these attacks are easy to detect and we recommend that future work does not evaluate against them.\n• The Fast Gradient Sign attack [10] takes a single step, for all pixels, in the direction of the gradient. This attack aims to optimize the L∞ distance metric and is very efficient to implement. • JSMA [27] is an attack that greedily modifies one pixel at a time until the image is classified incorrectly. At each iteration the pixel chosen is the one that maximizes a a specially-crafted loss function. This attack seeks to optimize the L0 distance metric."
    }, {
      "heading" : "3 ATTACK APPROACH",
      "text" : "In order to evaluate the robustness of each of the above defenses, we take three approaches to target each of the three threat models introduced earlier.\nEvaluate with a strong attack: In this step we generate adversarial examples with C&W’s attack and check whether the defense can detect this strong (but oblivious) attack. This evaluation approach has the weakest threat model (the attacker is not even aware the defense is in place), so any defense should trivially be able to detect this attack. Failing this test implies that the second two tests will also fail.\nPerform an adaptive, white-box attack: The most powerful threat model, we assume here the adversary has access to the detector and can mount an adaptive attack. To perform this attack, we construct a new loss function based on the loss function used earlier, and generate adversarial examples that are targeted to both fool the classifier and also evade the detector.\nThe most difficult step in this attack is to construct a differentiable loss function that can be used to generate adversarial examples. In some cases, such a loss function might not be readily available (if, for example, the defense is not a differentiable function). In other cases, one may exist, but it may not be well-suited to performing gradient descent over. We describe how we construct such a loss function for each attack.\nConstruct a black-box attack: In the final, and most difficult, threat model, we assume the adversary knows what defense is in place but does not know the detector’s paramaters. This evaluation is only interesting if (a) the strong attack failed to generate adversarial examples, and (b) the adaptive white-box attack succeeded. If the strong attack alone succeeded, when the adversary was not aware of the defense, they could mount the same attack in this black-box case. Conversely, if the white-box attack failed, then a black-box attack will also fail (since the threat model is strictly harder).\nIn order to mount this attack, we rely on the transferability property: we as the attacker train a substitute model in the same way as the original model, but on a separate training set. We have access to the substitute model’s parameters, so we perform a whitebox attack on the substitute to generate high-confidence adversarial examples. Finally, we evaluate whether these adversarial examples transfer to the actual model.\nWhen the classifier and detector are separate models, we assume the adversary has access to the classifier but not the detector (we are analyzing the increase in security by using the detector).\nIf the detector and classifier are not separable (i.e., the classifier is trained to also act as a detector), then to perform a fair evaluation, we compare the adversarial examples generated with black-box access to the (unsecured) classifier to adversarial examples generated with only black-box access to both the classifier and detector.\nAs with any experiment, it is better to change only one variable at a time: if we were to compare white-box-no-detector distortion to black-box-with-detector distortion, we would not know if the detector has added any robustness, or if it just that adversarial examples require larger distortion to transfer (which they do [6, 21])."
    }, {
      "heading" : "4 SECONDARY CLASSIFICATION BASED DETECTION",
      "text" : "The first category of detection schemes we study build a second classifier which attempts to detect adversarial examples. Three of the approaches take this direction.\nFor the remainder of this subsection, define F (·) to be the classification network and D(·) to be the detection network. F (·) is defined as in Section 2.1 outputting a probability distribution over the 10 classes, and D : Rw ·h ·c → (−∞,∞) represent the logits of the liklehood the instance is adversarial. That is, sigmoid(D(x)) : Rw ·h ·c → [0, 1] represents the probability the instance is adversarial."
    }, {
      "heading" : "4.1 Adversarial Retraining",
      "text" : "Grosse et al. [12] propose a variant on adversarial re-training. Instead of attempting to classify the adversarial examples correctly (by adding adversarial examples to the training set, with their correct labels), they introduce a new N + 1st class — solely for adversarial examples — and train the network to detect adversarial examples. Specifically, they propose the following procedure:\n(1) Train a model Fbase on the training data X0 = X. (2) Generate adversarial examples on model Fbase for each\n(xi ,yi ) ∈ X . Call these examples x ′i . (3) Let X1 = X0 ∪ {(x ′i ,N + 1) : i ∈ |X|} where N + 1 is the\nnew label for adversarial examples. (4) Train a model Fsecured on the training data X1.\nGong et al. [9] construct a similar detector; however, instead of re-training the full network to detect the original examples along with adversarial examples, they train a second binary classifier to predict whether the instance is adversarial or valid. That is, they do the following:\n(1) Train a model Fbase on the training data X0 = X. (2) Generate adversarial examples on model Fbase for each\n(xi ,yi ) ∈ X . Call these examples x ′i .\n(3) Let X1 = {(xi , 1) : i ∈ |X|} ∪ {(x ′i , 0) : i ∈ |X|}. (4) Train a model D on the training data X1.\nFor the remainder of this section we report results on the MNIST dataset; neither paper claims robustness on CIFAR (a strictly harder task). When we perform the attacks in this section on CIFAR, our results only improve.\nSince we were unable to obtain source code for the defense of Grosse et al., we re-implement it and confirm their results papers: adversarial retraining is able to detect adversarial examples when generated with the fast gradient sign and JSMA attacks with near100% accuracy.\nOblivious Attack Evaluation. We train these two schemes on the entire MNIST training set, using C&W’s attack to generate adversarial examples. In this way we construct a model Fsecured / D.\nWe then construct adversarial examples for Fbase from each image in the test set using C&W’s attack. Both approaches detect these previously unseen test adversarial examples. Grosse et al. detects 98.5% of attacks as adversarial. Further, it classifies half of the remaining 1.5% correctly. Gong et al. achieve 98% accuracy in detecting adversarial examples.\nInvestigating further, we find that even if we train on adversarial examples generated using an untargeted attack, both schemes detect targeted adversarial examples (for any target class). Additionally, we find that both detection schemes when trained on low-confidence adversarial examples are able to detect high-confidence adversarial examples.\nWhite-box Attack Evaluation. Next, we evaluate these defenses assuming the adversary is aware of these defenses and the model parameters. That is, we directly attack the defended model. Our experiments revealed that these defenses are ineffective and add no robustness. We can produce adversarial examples with 100% success for both.\nFor Grosse’s defense, we use C&W’s attack on Fsecured to generate adversarial examples; it succeeds 100% of the time.We computed the mean L2-distance from the original sample to the adversarial example. Adversarial examples against Fbase are at average L2 distance of 2.05 from the original sample; adversarial examples against Fsecured have an average distance of 2.26. Thus the defense has not reduced the success rate at generating adversarial examples, and has only increased the mean distortion by 10%.\nGong’s defense does not fare any better. To help construct adversarial examples that will simultaneously fool Fbase and D, we define a new functionG(·) that represents the combination of the classifier (with logits ZF (·)) and detector (with logits ZD (·)). In particular, we define\nG(x)i = { ZF (x)i if i ≤ N (ZD (x) + 1) ·max\ni ZF (x)i if i = N + 1\nEffectively, G acts as a classifier on N + 1 classes. It is constructed to have two useful properties: if ZD (x) > 0 (i.e., if the detector classifies x as malicious) then we will have\narg maxi (G(x)i ) = N + 1\n(where N is the new adversarial class), and if ZD (x) < 0 (i.e., if the detector classifies x as valid) then we will have\narg maxi (G(x)i ) = arg maxi (ZF (x)i ).\nWhy did we choose this particular function G(·)? Recall from earlier that when using a gradient-descent based attack algorithm, there is a constant c in the loss function that trades off the relative importance of reducing the distance from the valid instance vs. classifying the input as the target class. If the value of the loss function changes by several orders of magnitude, no value of c will be optimal and adversarial examples will be very hard to find. Therefore, we construct G so that it is mostly linear.\nNow we treat this functionG as if it were a neural network, and feed it directly into C&W’s attack algorithm instead of the function Z . It is easy to see that if an instance x ′ is classified by G as label l ≤ N then both C(x ′) = l and the detector classifies x ′ as valid (non-adversarial). This approach successfully generates adversarial examples against Gong’s defense with 100% success. The mean L2 distortion of these adversarial examples is 2.31, an improvement of about 10% over the unprotected scheme.\nThus, Gong’s scheme performs about the same as Grosse’s defense: neither is effective. In retrospect, it should not be surprising that the robustness increase on these two defenses is nearly identical – they are very similar.\nBlack-Box Evaluation. Next, we consider black-box attacks, where the parameters of the defended network are not known to the adversary. For brevity, we only describe the attack on Grosse’s scheme (Gong’s results are identical). We generate transferable adversarial examples as described in Section 3: instead of training one model on the complete training set, we split the training set in half. We assume the first half is known to the attacker and the second half is used by the defender to train their network. Thus, we construct two models R1 and R2 each trained on half of the training data. We treat R2 as the defender’s model (the target model for which we want to produce adversarial examples) and give the attacker white-box access to all parameters of model R1. Then we apply the C&W attack to each instance in the test set to generate adversarial examples for R1, and we test whether they fool R2.\nAs a baseline, we applied this procedure to the unsecured model. The attack succeeded 98% of the time, with a mean L2 distortion of 5.1. Then, we generated two secured models R1 and R2 implementing Grosse’s defense, and applied the same attack. We observed a 98% success rate, with a mean L2 distortion of 5.3, which is only 4% higher than the baseline. Therefore, we conclude that Grosse’s defense is ineffective and can be attacked even by an attacker who does not the exact model parameters of the secured model."
    }, {
      "heading" : "4.2 Examining Convolutional Layers",
      "text" : "In contrast to the prior approach, which attempts to detect adversarial examples based on the contents of the image itself, Metzen et al. [15] detect adversarial examples by looking at the inner convolutional layers of the network. They augment the classification neural network with a detection neural network that takes its input from various intermediate layers of the classification network. This detection network is trained by generating adversarial examples\nfor the classification network. We refer interested readers to the original paper for complete details on the detector setup [15].\nWe copy their network architecture exactly and reproduce their results as described. Our ResNet-32 architecture obtains 91.5% accuracy on CIFAR-10. We confirmed that their detector is able to detect adversarial examples with 99% probability on simple attacks (e.g., fast gradient sign or JSMA). Their paper demonstrates that it is also able to detect adversarial examples generated by two other attacks, DeepFool [24] and the Basic Iterative Method [21].\nOblivious Attack Evaluation. We take each instance in the training set and find the nearest adversarial example using the C&W attack. We then train the detector by attaching it to the output of the first residual block as done in [15]. We found training the detector to be very difficult and highly sensitive; we experimented with different optimizers, hyper-parameters, and initializers to reduce the effect of this.\nFor evaluation, we generate adversarial examples for each instance in the test set, using the C&W attack. The best detector we were able to train correctly predicts 72% of the test data as valid, and 81% of the adversarial examples as adversarial. This is in stark contrast to the 99% success at detecting simpler attacks, which highlights that weak attacks give a biased view of defense robustness. This defense is unable to perform well even when the adversary is not attempting to evade it.\nWe experiment with several other approaches to increase the accuracy of the detector, including increasing the number of model parameters, adding additional regularization, placing the detector at different layers in the network, and applying an ensemble of multiple detectors placed at different layers in the network. None of these variants work substantially better.\nWe do not evaluate Metzen’s defense on MNIST, because the defense is only applicable to ResNet architectures, which dramatically over-fit on MNIST.\nWhite-box Attack Evaluation. Our white-box attack completely defeats Metzen’s defense: it is able to produce adversarial examples that simultaneously are mis-classified by the original network and evade the detector. We generate adversarial examples using C&W’s attack applied to the same functionG(·) defined in Section 4.1. The mean distance to adversarial examples increases from 0.169 L2 distortion on the unsecured model to 0.227 on the secured scheme, an improvement of 34%. However, in absolute terms, the adversarial examples generated are still indistinguishable from the original inputs.\nBlack-Box Evaluation. To investigate if this defense is robust to attacks in a black-box setting, we perform our standard transferability test. We split the training data in half, and train two detector models, one on each half of the training data. Then, we attack the second detector given only white-box access to the first detector.\nWe found that even low-confidence adversarial examples transfer 84% of the time between the two detectors when the classifier network is known by the adversary. By using high-confidence adversarial examples, the attack success rate can be increased to 98% at the cost of increasing the mean distortion by a further 28%, which is small enough that adversarial examples remain indistinguishable from the original images."
    }, {
      "heading" : "5 PRINCIPAL COMPONENT ANALYSIS DETECTION",
      "text" : "Principal Component Analysis (PCA) transforms a set of points in a n-dimensional space to a new set of points in a k-dimensional space (k ≤ n) through a linear transformation. We assume the reader is familiar with PCA for the remainder of this section."
    }, {
      "heading" : "5.1 Input Image PCA",
      "text" : "Hendrycks & Gimpel [16] use PCA to detect valid images from adversarial examples, finding that adversarial examples place a higher weight on the larger principal components than valid images (and lower weight on the earlier principal components).\nOblivious Attack Evaluation. We first reproduce their results by running PCA on MNIST. To see if adversarial examples really do use larger principal components more often, we compute how much each component is used. Let X1, . . . ,Xn be the training set instances. We define the score S(j) of the jth PCA component as\nS(j) = 1 N N∑ i=1 |PCA(Xi )j |.\nWe train a classification network on the training set and compute the component scores S(1), . . . , S(784). Then, for each image in the test set, we find the nearest adversarial example with C&W’s attack and we compute the component scores on these adversarial examples. The results are plotted in Figure 2.\nOur results agree with Hendrycks et. al [16]: there is no difference on the first principal components, but there is a substantial difference between valid and adversarial instances on the later components. On the MNIST data set, their defense does detect oblivious attacks, if the attacker does not attempt to defeat the defense.\nLooking Deeper. At first glance, this might lead us to believe that PCA is a powerful and effective method for detecting adversarial examples. However, whenever there are large abnormalities in the data, one must be careful to understand their cause.\nIn this case, the reason for the difference is that there are pixels on the MNIST dataset that are almost always set to 0. Since the MNIST dataset is constructed by taking 24x24 images and centering them (by center-of-mass) on a 28x28 grid, the majority of the pixels on the boundary of the image are always zero. Because of this, the top two rows of pixels are all zero 99.9% of the time. The same is true for the left two columns and right two columns. The bottom row is zero 99% of the time. Because these border pixels are essentially always zero for valid instances, the last principal components are heavily concentrated on these border pixels.\nDue to this effect, the last 74 principal components (9.4% of the components) explain less than 10−30 of the variance on the training set. (This is non-zero due exclusively to floating-point rounding errors.) When we look only at the first 710 principal components, we see a much less dramatic difference, and only a slight difference on components 700 to 710.\nIn short, the difference between valid and adversarial examples is because the border pixels are nearly always zero for valid MNIST instances, while existing attack algorithms often change the border pixels to non-zero values. While adversarial examples are different from valid images on MNIST, the reason is not an intrinsic property of adversarial examples and is instead due to an artifact of the MNIST dataset. When we perform the above evaluation on CIFAR, there is no detectable difference between adversarial examples and valid data. As a result, the Hendrycks defense is not effective for CIFAR — it is specific to MNIST. Also, this deeper understanding of why the defense works on MNIST suggests that smarter attacks might be able to avoid detection by simply leaving those pixels unchanged.\nWhite-Box and Black-Box Evaluation. We found that theHendrycks defense can be broken by a white-box attacker with knowledge of the defense, and even by a black-box attacker. Details are deferred to Section 5.2, where we break a strictly stronger defense. In particular, we found in our experiments that we can generate adversarial examples that are restricted to change only the first k principal components (i.e., leave all later components unchanged), and these adversarial examples that are not detected by the Hendrycks defense."
    }, {
      "heading" : "5.2 Dimensionality Reduction",
      "text" : "Bhagoji et al. [2] propose a defense based on dimensionality reduction: instead of training a classifier on the original training data, they reduce theW ·H ·C = N -dimensional input (e.g., 784 for MNIST) to a much smaller K-dimensional input (e.g., 20) and train a classifier on this smaller input. The classifier uses a fully-connected neural network: PCA loses spatial locality, so a convolutional network cannot be used.\nThis defense restricts the attacker so they can only manipulate the first K components, as the other components are ignored by the classifier. If adversarial examples rely on the last principal components (as Hendrycks et. al hypothesize), then restricting the attack\nto only the first K principal components should dramatically increase the required distortion to produce an adversarial example. We test this prediction empirically.\nWe reimplement their algorithm with the same model (a fullyconnected network with two hidden layers of 100 units). We train 26 models with different values ofK , ranging from 9 to 784 dimensions. Models with fewer than 25 dimensions have lower accuracy; all models with more than 25 dimensions have 97% or higher accuracy.\nWhite-box Attack Evaluation. We evaluate Bhagoji’s defense by constructing untargeted attacks against all 26 models we trained. We show the mean distortion for each model in Figure 3. The most difficult model to attack uses only the first 25 principal components; it is nearly 3× more robust than the model that keeps all 784 principal components.\nHowever, crucially, we find that even the model that keeps the first 25 principal components is less robust than almost any standard, unsecured convolutional neural network; an unprotected network achieves both higher accuracy (99.5% accuracy) and better robustness to adversarial examples (measured by the mean distortion). In summary, Bhagoji’s defense is not secure against white-box attacks.\nLooking Deeper. Next, we show that this result is not an artifact of the network architecture — it is not caused just because fullyconnected network are less robust than convolutional networks. We study a second algorithm that Bhagoji et al. present but did not end up using, which combines PCA with a convolutional neural network architecture. This allows us to perform an experiment where the network architecture is held fixed, and the only change is whether dimensionality reduction is used or not. In particular, instead of feeding the first K principal components directly into a fully-connected network, they use PCA to map each image into the reduced-dimensionality PCA space, and then immediately map it\nback to the image space. This effectively projects the image into a reduced-dimension manifold. They train a convolutional classifier on the projected images. This classifier achieves a significantly higher accuracy (99% when using at least 25 dimensions).\nIn our experiments we found that this approach is nomore robust than an unsecured convolutional network (applied to the original image), despite only using a limited number of the principal components. We conclude that defenses based on limiting the adversary to only the first principal components are not effective. It follows that Hendrycks et. al’s defense is broken as well, as the adversarial images generated in this way change only the first K components and leave all later components unchanged."
    }, {
      "heading" : "5.3 Hidden Layer PCA",
      "text" : "Li et al. [23] apply PCA to the values after inner convolutional layers of the neural network, and use a cascade classifier to detect adversarial examples. Specifically, they propose building a cascade classifier that accepts the input as valid only if all classifiers Ci accept the input, but rejects it if any do. Each classifier Ci is implemented as a linear SVM that acts on the PCA of the ith convolutional layer of the network.\nThey evaluate their scheme on ImageNet. In the remainder of this section we demonstrate their defense is not effective on MNIST and CIFAR; based on experience with other defenses, we expect that attacking an ImageNet classifier would be even easier.\nOblivious Attack Evaluation. Li et al. generated adversarial examples with Szegedy et. al’s L-BFGS algorithm [36] and found that the first linear SVM achieved 80% true positive rate at 0% false positive rate – an ideal use-case for a cascade classifier. We evaluated the effectiveness of their method at detecting adversarial examples generated with C&W’s attack (when trained on adversarial examples generated with the C&W attack). For MNIST, their scheme only achieves a 62% true positive rate at a 37% false positive rate (and 56% at 40% on CIFAR). When we cascade this classifier, over 92% of valid instances are rejected as adversarial on MNIST and only 8% of adversarial examples are detected; for CIFAR, 98% of valid images are rejected and only 1% of adversarial examples are detected. This is even worse than random guessing. We conclude that Li’s defense is not effective against the C&W attack."
    }, {
      "heading" : "6 DISTRIBUTIONAL DETECTION",
      "text" : "Next, we study two defenses that detect adversarial examples by comparing the distribution of valid images to the distribution of adversarial examples. They use classical statistical methods to distinguish valid images from adversarial images. We show that neither of these defenses are able to detect a white-box or black-box adversary."
    }, {
      "heading" : "6.1 MaximumMean Discrepancy",
      "text" : "Grosse et al. [12] consider a significantly more powerful (perhaps so powerful as to be impractical) threat model: assume we are given two sets of images S1 and S2, such that we know S1 contains only valid images, and we know that S2 contains either all adversarial examples, or all valid images. They ask the question: can we determine which of these two situations is the case?\nTo achieve this, they use theMaximumMeanDiscrepancy (MMD) test [4, 11], a statistical hypothesis test that answers the question “are these two sets drawn from the same underlying distribution?”\nLet F represents a (possibly unbounded) set of functions. The optimal formulation of the MMD is\nMMD(F ,X1,X2) = sup f ∈F E(f (X1)) − E(f (X2)).\nIt can be proven that if the support |X1 | and |X2 | are sufficiently large then the two distributions X1,X2 are equal if and only if the MMD is equal to zero.\nSince this formal definition of MMD may not be computable, instead an approximation is used. We omit the exact details of the approximation used. In our experiments, we use the same approximation used by Grosse et al. [11].\nTo test whetherX1 andX2 are drawn from the same distribution, Grosse et al. use a permutation test with the MMD test statistic:\n(1) Let a = MMDapprox(X1,X2). (2) Let Y = X1 ∪ X2, the set of all samples. (3) Randomly split Y into two halves, Y1 and Y2. (4) Test whether a < MMDapprox(Y1,Y2).\nThis test is repeated many times (1000 in our implementation). If samples from the two distributions X1 and X2 are drawn from the same distribution, then wewill fail to reject the null hypothesis with high probability, because the sets Y1 and Y2 will also be drawn from the same distribution. On the other hand, if X1 and X2 are drawn from different distributions, we should expect to reject the null hypothesis since Y1 and Y2 are drawn from the same distribution and so the MMD between them should be small.\nTo reproduce Grosse’s results, we verify that the MMD test is able to distinguish weak attacks. As a first step, we verified that MMD correctly detects valid instances as being valid. To do this, we let the distributionX1 be 1000 randomly sampled training instances, andX2 be 1000 randomly sampled testing instances. MMD correctly fails to reject the null hypothesis (p > 0.05) in distinguishing the two distributions.\nThen, we run this same experiment comparing random samples from the training set with on adversarial examples generated on the test set with weak attacks. Using a set size of 50, MMD can detect adversarial examples generated with FGS (ϵ = 0.275) with p < 0.001 and detect JSMA adversarial examples (changing 20 pixels) with p < 0.001. This is consistent with the results reported by Grosse et al.\nOblivious Attack Evaluation. We repeated this experiment, this time producing untargeted adversarial examples with C&W’s attack algorithm. Even when using a set of 100 images, MMD fails to reject the null hypothesis (p > 0.05). MMD also fails to detect attacks on CIFAR, even with a set size as large as 10, 000 (which uses every sample in the test set). In short, Grosse’s defense is not effective against oblivious attacks.\nThis should not be surprising: the MMD test acts directly on the pixel values. As long as the attack makes only slight changes to each image, it is unlikely a simple statistical test will be able to distinguish between adversarial examples and valid data.\nWe draw two important conclusions: first, MNIST is easier to protect than CIFAR (by a large margin), and second, we take this\nresult as the strongest indication yet that FGS and JSMA should not be used exclusively as an evaluation benchmark."
    }, {
      "heading" : "6.2 Kernel Density Estimation",
      "text" : "Feinman et al. [8] present a defense they call kernel density estimation. They use a Gaussian Mixture Model to model outputs from the final hidden layer of a neural network, and argue that adversarial examples belong to a different distribution than that of valid images.\nSpecifically, given an instance x classified as label t , kernel density estimation estimates the likelihood of x as\nKDE(x) = 1|Xt | ∑ s ∈Xt exp( |F n−1(x) − Fn−1(s)|2 σ 2 )\nwhere Xt is the set of training instances with label t and Fn−1(x) is the output of the final hidden layer on input x . The detector is therefore constructed by selecting a threshold τ and reporting x as adversarial if KDE(x) < τ , otherwise reporting x as valid.\nThe motivation behind this approach is that the later hidden layers of a neural network have been shown to capture high-level semantic information about the input. Therefore, using a simple classifier on this final layer will be more accurate than if it were applied to the original input images, as the prior defense did.\nOblivious Attack Evaluation. Like many of the other defenses, we notice a difference in the efficacy of density estimation on the MNIST and CIFAR datasets. However, the difference is much more dramatic in this case. On MNIST, Feinman’s defense can detect even adversarial examples generated using C&W’s attack.\nHowever, on CIFAR, simply using C&W’s attack is enough to defeat density estimation. For each image in the test set x and closest adversarial example x ′, we compare KDE(x ′) to KDE(x). Surprisingly, we find that 80% of the time, the adversarial example has a higher likelihood score than the original image (Figure 4, appendix). Therefore, Feinman’s defense cannot work on CIFAR. In the remainder of this section, we show how to break this defense on MNIST.\nWhite-box Attack Evaluation. To mount a white-box attack, we construct a newminimization formulation that differs from the original only in that we introduce a new loss term ℓ2(x ′) that penalizes being detected by the detector:\nminimize ∥x − x ′∥22 + c · (ℓ(x ′) + ℓ2(x ′))\nwhere we define\nℓ2(x ′) = max(− log(KDE(x ′)) − ϵ, 0) where ϵ controls the likelihood measure of the adversarial examples. In our attack, we set ϵ to themedian of− log(KDE(·)) on the training set, so that ℓ2(x ′) ≤ 0 if and only if KDE(x ′) is greater than half of the training instances KDE.\nIn practice, we mount this attack in two phases. First, we solve the original C&W minimization formulation to obtain an adversarial example x̂ . Typically x̂ will be detected by the detector, so in the second phase we modify it to no longer be detected: we use this x̂ as the initial value of x ′ in the above optimization problem and use gradient descent to improve it. Performing this two-step optimization is useful to allow for different constants c chosen for\ninitially generating an adversarial example and for making it not detected.\nThis approach is effective at generating adversarial examples for MNIST that are not detected by Feinman’s defense. The mean distortion is 3.81, up from 2.05 on an unsecured network. In order to implement this attack more efficiently, instead of computing the KDE across all training instances during the inner optimization loop, we randomly sample 100 instances (< 5% of the total) at first and use only those. A distortion of 3.81 begins to allow humans to notice the adversarial nature of these images: Figure 6 in the appendix shows adversarial examples generated against this defense.\nBlack-Box Evaluation. Note that our white-box attack algorithm doesn’t use the entire training set; it uses only a random sample of 100 instances from the training set (less than 5% of the training instances). This hints that black-box attacks might succeed. This is indeed the case: we found that when the defender uses the first 95% of the training data, and the last 5% of the training data is left for the adversary (who uses it to form their own KDE estimator), the attack still succeeds with equal distortion."
    }, {
      "heading" : "7 NORMALIZATION DETECTION",
      "text" : ""
    }, {
      "heading" : "7.1 Dropout Randomization",
      "text" : "Feinman et al. propose a second detection method called Bayesian neural network uncertainty that measures the uncertainty of the neural network on the given input. Instead of relying on the reported confidence of the network (which can easily be controlled by an adversary), they add randomization to the network. The hope is that a valid image will have the same (correct) label regardless of the random values chosen, while adversarial examples won’t always be predicted with the same label.\nThey use dropout, a common approach used during training to reduce network overfitting [34], for randomization. Dropout is applied to a layer of a network, and randomly sets a neuron’s output to 0with some probability p. Conventionally dropout is used only during training and is turned off during test-time, however Feinman et al. keep dropout turned on for both training and test. They use p = 0.5.\nTo determine the uncertainty of the network on a given instance, we apply the randomized network Fr (with dropout enabled) to the input L times. Then, the uncertainty is defined as\nU (x) = ( 1 L L∑ i=1 ∥Fr (x)∥ ) − 1L L∑ i=1 Fr (x)\nwhere we take ∥y∥ to mean yTy (the squared L2 norm). Notice that if the predictions Fr (x) are the same every time, the uncertainty is 0. In other words, the uncertainty is the sum of the variances of each component of the output, computed over the L outputs from the randomized network.\nTo classify an input x as adversarial or valid, we pick a threshold τ and a number of times to evaluate the network L and compute U (x). We have found the algorithm is not sensitive to the choice of L as long as it is greater than 20 (and we pick L = 30 for the remainder of this section). We return “adversarial” ifU (x) > τ and “valid” otherwise.\nWhen we run this detection scheme on the adversarial examples generated by the FGS and JSMA attack algorithms on MNIST, it detects 96% of adversarial examples with a false positive rate lower than 1% when τ = 0.02.\nOblivious Attack Evaluation. We generated high-confidence adversarial examples with C&W’s attack. 75% of these adversarial examples are detected, in contrast to 96% of examples generated with weaker attacks.\nWhite-box Attack Evaluation. Our white-box attacks work by defining an objective function that combines the model and detector. We begin by showing a simple approach that was not effective at producing adversarial examples, to illustrate the challenges in selecting a useful function. Following the approach taken to break the prior detection schemes, let\nG(x)i = { Z (x)i if i ≤ N (1 +U (x) − τ ) ·max\ni Z (x)i if i = N + 1\n(where N + 1 is the new adversarial class). This objective function is defined so that if U (x) > τ (i.e., x is classified as adversarial by Feinman’s defense) then we will have\narg maxi (G(x)i ) = N + 1, and ifU (x) < τ (i.e., x is classified as valid) then\narg maxi (G(x)i ) = arg maxi (F (x)i ). We attempted to generated adversarial examples by applying C&W’s attack toG(·). However, this failed: because values of the function U (·) are computed by calling F (·) (and not Z (·), as we like to do), there is no good constant c that C&W’s algorithm can select. This causes gradient descent to perform poorly and rarely returns successful adversarial examples.\nTherefore, for this defense, we abandon the approach of constructing a single function to optimize over and instead change the loss function ℓ used by the attack. Recall that the typical loss function used (for a given network Z (·)) is\nℓZ (x ′) = max(max{Z (x ′)i : i , t} − Z (x ′)t ,−κ). If every prediction Fr (x) of the randomized network produced a very similar result, the uncertainty valueU (x) would be very small.\nWe sampleK different deterministic networks {Z j (·) : j ∈ [1,K]} each with different randomness used during dropout. If we were able to have arg maxiZ j (x)i = t for every network j, for K big enough, it would be highly likely that Fr (x) would always produce label t for any randomness. Thus, we construct a new loss function\nℓ′(x ′) = K∑ j=1 ℓZ j (x ′)\nas the average of the loss functions on each fixed model Z j . Then we use C&W’s attack with this revised loss function.\nThis successfully generates adversarial examples that fool the dropout defense, with 98% success. Themean l2 distortion is 3.68, up from the baseline of 2.05, so defeating the dropout defense requires the largest distortion of any of the defenses we have evaluated. Nonetheless, in an absolute sense, the adversarial examples are often indistinguishable from the originals, especially for the CIFAR dataset — see Figure 7. We show a plot of the uncertainty of valid\nimages compared to adversarial images when constructed with our white-box attack in Figure 5 (appendix).\nBlack-Box Evaluation. It turns out that adversarial examples generated with the previous approach transfer with high accuracy across models. This is due to the fact that our white-box attack assumes we do not know the exact randomization settings, and therefore construct adversarial examples that are effective regardless of randomization. This is similar to the black-box threat model, where the adversary does not have access to the model parameters.\nWe again construct two models F (·) andG(·) on different subsets of the training data. We provide the adversary access to the parameters of F and use the white-box attack above to generate adversarial examples for F ; we then test whether they fool G. We found that adversarial examples generated with our algorithm transfer to G with 90% success. We can increase the transfer rate to 98% at the cost of increasing the mean distortion only 15%, to 4.23."
    }, {
      "heading" : "7.2 Mean Blur",
      "text" : "The second detection method proposed by Li et al. applies a 3 × 3 average filter to blur the image before applying the classifier. The authors admit this defense is “overly simplistic” but still argue it is effective at alleviating adversarial examples. We confirm this simple defense can remove adversarial examples generated with fast gradient sign, as they found in their paper.\nOblivious Attack Evaluation. Whenwe use C&W’s attack, we find that this defense effectively removes low-confidence adversarial examples: 80% of adversarial examples (at a mean L2 distortion of 2.05) are no longer classified incorrectly.\nThis attack can even partially alleviate high-confidence adversarial examples. To ensure they remain adversarial after blurring, we must increase the distortion by a factor of 3×.\nWhite-box Attack Evaluation. Observe that taking the mean over every 3 × 3 region on the image is the same as adding another convolutional layer to the beginning of the neural network with one output channel that performs this calculation. Given the network F , we define F ′(x) = F (blur(x)) and apply C&W’s attack against F ′. When we do so, we find that the mean distance to adversarial examples does not increase. Therefore, blurring is not an effective defense."
    }, {
      "heading" : "8 LESSONS",
      "text" : "After examining these ten defenses, we believe we have learned some general lessons about what worked, what didn’t work, and advice on how to evaluate future defenses."
    }, {
      "heading" : "8.1 What Worked?",
      "text" : "Applying randomness to the network (through dropout) was the most effective defense to our attacks on MNIST: it makes generating adversarial examples on the network as difficult as generating transferable adversarial examples. If it were possible to find a way to eliminate transferability, a randomization-based defense may be able to detect adversarial examples.\nKernel density estimation, the other defense that significantly increased the required distortion, was only effective on MNIST. We\nbelieve understanding why this defense works so well on MNIST but not at all on CIFAR is an interesting direction of future work."
    }, {
      "heading" : "8.2 What Didn’t Work?",
      "text" : "Across all of the defenses we evaluate, the least effective schemes used another neural network (or more neural network layers) to attempt to identify adversarial examples. Given that adversarial examples can fool a single classifier, it makes sense that adversarial examples can fool a classifier and detector. None of these approaches add more than a 30% increase in robustness on MNIST (and much less against CIFAR) when the adversary was aware of the model, and black-box attacks are also possible and almost as effective as white-box attacks.\nDefenses that operated directly on the pixel values were too simple to succeed. On MNIST, these defenses provided reasonable robustness against weak attacks; however when evaluating on stronger attacks, these defenses all failed. This should not be surprising: the reason neural networks are used is that they are able to extract deep and meaningful features from the input data. A simple linear detector is not effective at classification when operating on raw pixel values, so it should not be surprising it does not work at detecting adversarial examples. (This can be seen especially well on CIFAR, where even weak attacks often succeed against defenses that operate on the input pixel space.)\nFinally, for all defenses we evaluate, the transferability property allowed us to break them even if an adversary was not aware of the model parameters. Constructing a secure defense will require eliminating transferability."
    }, {
      "heading" : "8.3 Recommendations for Defenses",
      "text" : "We have several recommendations for how researchers proposing new defenses can better evaluate their proposals:\nEvaluate using a strong attack. Evaluate proposed defenses using the strongest attacks known. Do not use fast gradient sign or JSMA exclusively: these are weak attacks, and even if a defense can stop them, it is not possible to know if the defense is truly effective, or if it is simply the attack that is failing. Fast gradient sign was never designed to produce high-quality attacks. It was designed as a demonstration that neural networks are highly linear. As this paper has clearly demonstrated by breaking ten detection methods, JSMA is easily detected while other strong iterative attack algorithms are not. Using these algorithms as a first test is reasonable, but not sufficient. New schemes should demonstrate that they can stop C&W’s attack.\nDemonstratewhite-box attacks fail. It is not sufficient to show that a defense can detect adversarial examples: one must also show that an adversary who is aware of the defense can not generate attacks that evade detection. We show how to perform that kind of evaluation: construct a differentiable function that is minimized when the image fools the classifier and is treated as valid by the detector, and apply a strong iterative attack (e.g., C&W’s attack) to this function.\nDemonstrate black-box attacks fail. If the scheme does not stop white-box attacks, at minimum it needs to stop black-box attacks. To evaluate security in the black-boxmodel, we recommend\ngenerating high-confidence adversarial examples and testing how well they transfer.\nReport mean distortion of adversarial examples. Many defenses we evaluated reported the success probability at a single distortion d . This makes it harder to compare multiple schemes, when they use different values of d . While a single number may not fully capture the robustness of a network, reporting the mean distance to the nearest adversarial example is a better single metric than the success rate at an arbitrary distance.\nReport false positive and true positive rates. When constructing a detection-based defense, it is not enough to report the accuracy of the detector. A 60% accuracy can either be very useful (e.g., if it achieves a high true-positive rate at a 0% false-positive rate) or entirely useless (e.g., if it detects most adversarial images as adversarial at the cost of many valid images as adversarial). Instead, report both the false positive and true positive rates. To allow for comparisons with other work, we suggest reporting at least the true positive rate at 1% false positive rate; showing a ROC curve would be even better.\nEvaluate on CIFAR. We have found that defenses that only evaluated on the MNIST dataset typically either (a) were unable to produce an accurate classifier on CIFAR, (b) were entirely useless on CIFAR and were not able to detect even the fast gradient sign attack, or (c) were even weaker against attack on CIFAR than the other defenses we evaluated. Future schemes need to be evaluated on multiple data sets — evaluating their security solely on MNIST is not sufficient. While we have found CIFAR to be a reasonable task for evaluating security, in the future as defenses improve it may become necessary to evaluate on harder datasets (such as ImageNet [7]).\nRelease source code. In order to allow others to build on their work, authors should release the source code of their defenses. Not releasing source code only sets back the research community and hinders future security analysis."
    }, {
      "heading" : "9 CONCLUSION",
      "text" : "Unlike standard machine-learning tasks, where achieving a higher accuracy on a single benchmark is in itself a useful and interesting result, this is not sufficient for secure machine learning. We must consider how an attacker might react to any proposed defense, and evaluate whether the defense will remain secure against an attacker who knows how the defense works.\nIn this paper we evaluate ten proposed defenses and demonstrate that none of them are able to withstand a white-box attack. They all fail even in a black-box setting where the adversary only knows the technique the defender is planning on using but does not know the specific model parameters being used.\nBy studying these ten defenses, we have drawn two lessons: adversarial examples are much more difficult to detect than previously recognized, and existing defenses lack thorough security evaluations. We hope that our work will help raise the bar for evaluation of proposed defenses and perhaps help others to construct more\neffective defenses. We believe that constructing defenses to adversarial examples is a critical challenge that must be overcome before these networks are used in potentially security-critical domains."
    }, {
      "heading" : "10 ACKNOWLEDGEMENTS",
      "text" : "We would like to thank Kathrin Grosse, Fuxin Li, Reuben Feinman, Metzen Jan Hendrik for discussing their defenses with us."
    }, {
      "heading" : "A ADDITIONAL FIGURES",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Measuring neural net robustness with constraints",
      "author" : [ "Osbert Bastani", "Yani Ioannou", "Leonidas Lampropoulos", "Dimitrios Vytiniotis", "Aditya Nori", "Antonio Criminisi" ],
      "venue" : "In Advances In Neural Information Processing Systems",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Dimensionality Reduction as a Defense against Evasion Attacks on Machine Learning Classifiers",
      "author" : [ "Arjun Nitin Bhagoji", "Daniel Cullina", "Prateek Mittal" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2017
    }, {
      "title" : "Integrating structured biological data by kernel maximum mean discrepancy",
      "author" : [ "Karsten M Borgwardt", "Arthur Gretton", "Malte J Rasch", "Hans-Peter Kriegel", "Bernhard Schölkopf", "Alex J Smola" ],
      "venue" : "Bioinformatics 22,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Bringing Big Neural Networks to Self-Driving Cars, Smartphones, and Drones. http://spectrum.ieee.org/computing/embedded-systems/ bringing-big-neural-networks-to-selfdriving-cars-smartphones-and-drones",
      "author" : [ "Katherine Bourzac" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Towards evaluating the robustness of neural networks",
      "author" : [ "Nicholas Carlini", "David Wagner" ],
      "venue" : "IEEE Symposium on Security and Privacy",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2017
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Detecting Adversarial Samples from Artifacts",
      "author" : [ "Reuben Feinman", "Ryan R Curtin", "Saurabh Shintre", "Andrew B Gardner" ],
      "venue" : "arXiv preprint arXiv:1703.00410",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2017
    }, {
      "title" : "Adversarial and Clean Data Are Not Twins",
      "author" : [ "Zhitao Gong", "Wenlu Wang", "Wei-Shinn Ku" ],
      "venue" : "arXiv preprint arXiv:1704.04960",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2017
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Schölkopf", "Alexander Smola" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "On the (Statistical) Detection of Adversarial Examples",
      "author" : [ "Kathrin Grosse", "Praveen Manoharan", "Nicolas Papernot", "Michael Backes", "Patrick McDaniel" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Towards deep neural network architectures robust to adversarial examples",
      "author" : [ "Shixiang Gu", "Luca Rigazio" ],
      "venue" : "arXiv preprint arXiv:1412.5068",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "On Detecting Adversarial Perturbations",
      "author" : [ "Jan Hendrik Metzen", "Tim Genewein", "Volker Fischer", "Bastian Bischoff" ],
      "venue" : "arXiv preprint arXiv:1702.04267",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2017
    }, {
      "title" : "EarlyMethods for Detecting Adversarial Images",
      "author" : [ "DanHendrycks", "Kevin Gimpel" ],
      "venue" : "In International Conference on Learning Representations (Workshop Track)",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2017
    }, {
      "title" : "Learning with a strong adversary",
      "author" : [ "RuitongHuang", "Bing Xu", "Dale Schuurmans", "Csaba Szepesvári" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Robust Convolutional Neural Networks under Adversarial Noise",
      "author" : [ "Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello" ],
      "venue" : "arXiv preprint arXiv:1511.06306",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "Adversarial examples in the physical world",
      "author" : [ "Alexey Kurakin", "Ian Goodfellow", "Samy Bengio" ],
      "venue" : "In International Conference on Learning Representations (Workshop Track)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "The MNIST database of handwritten digits",
      "author" : [ "Yann LeCun", "Corinna Cortes", "Christopher JC Burges" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1998
    }, {
      "title" : "Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics",
      "author" : [ "Xin Li", "Fuxin Li" ],
      "venue" : "arXiv preprint arXiv:1612.07767",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Deepfool: a simple and accurate method to fool deep neural networks",
      "author" : [ "Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 27th international conference on machine learning",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "Nicolas Papernot", "PatrickMcDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "In Security and Privacy (EuroS&P),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Distillation as a defense to adversarial perturbations against deep neural networks",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami" ],
      "venue" : "IEEE Symposium on Security and Privacy",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Announcing syntaxnet: The world’s most accurate parser goes open source",
      "author" : [ "Slav Petrov" ],
      "venue" : "Google Research Blog, May",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Adversarial diversity and hard positive generation",
      "author" : [ "Andras Rozsa", "Ethan M Rudd", "Terrance E Boult" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization",
      "author" : [ "Uri Shaham", "Yutaro Yamada", "Sahand Negahban" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Striving for simplicity: The all convolutional net",
      "author" : [ "Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller" ],
      "venue" : "In International Conference on Learning Representations (Workshop Track)",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research 15,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2014
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Improving the robustness of deep neural networks via stability training",
      "author" : [ "Stephan Zheng", "Yang Song", "Thomas Leung", "Ian Goodfellow" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "Their successes have been demonstrated in awide set of domains, from classifying images [35], to beating the best humans at Go [32], to translation and NLP [29], and to self driving cars [3, 5].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 27,
      "context" : "Their successes have been demonstrated in awide set of domains, from classifying images [35], to beating the best humans at Go [32], to translation and NLP [29], and to self driving cars [3, 5].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "Their successes have been demonstrated in awide set of domains, from classifying images [35], to beating the best humans at Go [32], to translation and NLP [29], and to self driving cars [3, 5].",
      "startOffset" : 187,
      "endOffset" : 193
    }, {
      "referenceID" : 33,
      "context" : "Such an instance x ′ is known as an adversarial example [36], and they have been shown to exist in nearly all domains that neural networks are used.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 145,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 145,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 145,
      "endOffset" : 176
    }, {
      "referenceID" : 17,
      "context" : "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 145,
      "endOffset" : 176
    }, {
      "referenceID" : 26,
      "context" : "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 145,
      "endOffset" : 176
    }, {
      "referenceID" : 28,
      "context" : "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 145,
      "endOffset" : 176
    }, {
      "referenceID" : 29,
      "context" : "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 145,
      "endOffset" : 176
    }, {
      "referenceID" : 34,
      "context" : "The research community has reacted to this observation in force, proposing many defenses that attempt to classify adversarial examples correctly [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 145,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "We study ten detection schemes proposed in seven papers over the last year [2, 8, 9, 12, 15, 16, 23], and show that in every case the defense can be evaded by an adversary who targets that specific defense.",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 33,
      "context" : "These attacks work by exploiting transferability [36], and our techniques for generating adversarial examples that transfer well may be of independent interest.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 33,
      "context" : "We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26], and [6].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26], and [6].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26], and [6].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : "We encourage readers unfamiliar with this area to read the following papers (in this order): [36], [10], [26], and [6].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : "Some layers involve the non-linear ReLU [25] activation.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L∞, a measure of the maximum absolute change to any pixel [10].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L∞, a measure of the maximum absolute change to any pixel [10].",
      "startOffset" : 113,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L∞, a measure of the maximum absolute change to any pixel [10].",
      "startOffset" : 113,
      "endOffset" : 124
    }, {
      "referenceID" : 33,
      "context" : "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L∞, a measure of the maximum absolute change to any pixel [10].",
      "startOffset" : 113,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "Common choices of p include: L0, a measure of the number of pixels changed [27]; L2, the standard Euclidean norm [6, 24, 36]; or L∞, a measure of the maximum absolute change to any pixel [10].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "One further property of adversarial examples we will make use of is the transferability property [10, 36].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 33,
      "context" : "One further property of adversarial examples we will make use of is the transferability property [10, 36].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 94,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 94,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 94,
      "endOffset" : 109
    }, {
      "referenceID" : 25,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 94,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 167,
      "endOffset" : 198
    }, {
      "referenceID" : 11,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 167,
      "endOffset" : 198
    }, {
      "referenceID" : 15,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 167,
      "endOffset" : 198
    }, {
      "referenceID" : 17,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 167,
      "endOffset" : 198
    }, {
      "referenceID" : 26,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 167,
      "endOffset" : 198
    }, {
      "referenceID" : 28,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 167,
      "endOffset" : 198
    }, {
      "referenceID" : 29,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 167,
      "endOffset" : 198
    }, {
      "referenceID" : 34,
      "context" : "There has been a significant amount of work studyingmethods to construct adversarial examples [6, 10, 24, 27] and to make networks robust against adversarial examples [1, 13, 17, 19, 28, 30, 31, 37].",
      "startOffset" : 167,
      "endOffset" : 198
    }, {
      "referenceID" : 1,
      "context" : "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "We study these defenses in this paper [2, 8, 9, 12, 15, 16, 23].",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "[15] NA/34% (§4.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[9] 11%/24% (§4.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "[12] 10%/8% (§4.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "[8] 85%/0% (§6.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 21,
      "context" : "[23] 0%/0% (§5.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[2] 0%/0% (§5.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "[16] 0%/0% (§5.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "TheMNIST dataset [22] consists of 70, 000 28× 28 greyscale images of handwritten digits from 0 to 9.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : "The CIFAR-10 dataset [20] consists of 60, 000 32 × 32 color images of ten different objects (e.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 30,
      "context" : "This dataset is substantially more difficult: the state of the art approaches achieve 95% accuracy [33].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "[15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "The model is a 32layer ResNet [14] with 470k parameters that we train with SGD with momentum set to 0.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "We apply L2 regularization and use BatchNorm [18] after every convolution.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "Some of the defenses we evaluate also argue robustness against ImageNet [7], a database of a million 224×224 images.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "Prior work [6, 24] has clearly demonstrated that constructing adversarial examples on ImageNet is a strictly easier task than MNIST or CIFAR, and constructing defenses is strictly harder.",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 22,
      "context" : "Prior work [6, 24] has clearly demonstrated that constructing adversarial examples on ImageNet is a strictly easier task than MNIST or CIFAR, and constructing defenses is strictly harder.",
      "startOffset" : 11,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "[12] propose two schemes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[9] detect adversarial examples by building a second neural network that detects adversarial examples from valid images.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "(3) Metzen et al [15] follow a similar approach, but train the detector on the inner layers of the classifier.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 21,
      "context" : "[23] propose two schemes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "(5) Hendrycks & Gimpel [16] perform PCA on the pixels of an image and argue adversarial examples place higher emphasis on larger components.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "[8] detect adversarial examples by keeping dropout [34] on during evaluation; additionally, they construct a kernel density measure and show that adversarial examples are drawn from a different distribution than valid images.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 31,
      "context" : "[8] detect adversarial examples by keeping dropout [34] on during evaluation; additionally, they construct a kernel density measure and show that adversarial examples are drawn from a different distribution than valid images.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "[2] show that adversarial images require use of more PCA dimensions than valid images.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "We use the L2 attack algorithm of Carlini and Wagner [6] to generate targeted adversarial examples, as it is superior to other published attacks.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "• The Fast Gradient Sign attack [10] takes a single step, for all pixels, in the direction of the gradient.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "• JSMA [27] is an attack that greedily modifies one pixel at a time until the image is classified incorrectly.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 4,
      "context" : "As with any experiment, it is better to change only one variable at a time: if we were to compare white-box-no-detector distortion to black-box-with-detector distortion, we would not know if the detector has added any robustness, or if it just that adversarial examples require larger distortion to transfer (which they do [6, 21]).",
      "startOffset" : 323,
      "endOffset" : 330
    }, {
      "referenceID" : 19,
      "context" : "As with any experiment, it is better to change only one variable at a time: if we were to compare white-box-no-detector distortion to black-box-with-detector distortion, we would not know if the detector has added any robustness, or if it just that adversarial examples require larger distortion to transfer (which they do [6, 21]).",
      "startOffset" : 323,
      "endOffset" : 330
    }, {
      "referenceID" : 0,
      "context" : "That is, sigmoid(D(x)) : Rw ·h ·c → [0, 1] represents the probability the instance is adversarial.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "[12] propose a variant on adversarial re-training.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[9] construct a similar detector; however, instead of re-training the full network to detect the original examples along with adversarial examples, they train a second binary classifier to predict whether the instance is adversarial or valid.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "[15] detect adversarial examples by looking at the inner convolutional layers of the network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "We refer interested readers to the original paper for complete details on the detector setup [15].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : "Their paper demonstrates that it is also able to detect adversarial examples generated by two other attacks, DeepFool [24] and the Basic Iterative Method [21].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "Their paper demonstrates that it is also able to detect adversarial examples generated by two other attacks, DeepFool [24] and the Basic Iterative Method [21].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 13,
      "context" : "We then train the detector by attaching it to the output of the first residual block as done in [15].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "Hendrycks & Gimpel [16] use PCA to detect valid images from adversarial examples, finding that adversarial examples place a higher weight on the larger principal components than valid images (and lower weight on the earlier principal components).",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "al [16]: there is no difference on the first principal components, but there is a substantial difference between valid and adversarial instances on the later components.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "[2] propose a defense based on dimensionality reduction: instead of training a classifier on the original training data, they reduce theW ·H ·C = N -dimensional input (e.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 21,
      "context" : "[23] apply PCA to the values after inner convolutional layers of the neural network, and use a cascade classifier to detect adversarial examples.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "al’s L-BFGS algorithm [36] and found that the first linear SVM achieved 80% true positive rate at 0% false positive rate – an ideal use-case for a cascade classifier.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "[12] consider a significantly more powerful (perhaps so powerful as to be impractical) threat model: assume we are given two sets of images S1 and S2, such that we know S1 contains only valid images, and we know that S2 contains either all adversarial examples, or all valid images.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "They ask the question: can we determine which of these two situations is the case? To achieve this, they use theMaximumMeanDiscrepancy (MMD) test [4, 11], a statistical hypothesis test that answers the question “are these two sets drawn from the same underlying distribution?” Let F represents a (possibly unbounded) set of functions.",
      "startOffset" : 146,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "They ask the question: can we determine which of these two situations is the case? To achieve this, they use theMaximumMeanDiscrepancy (MMD) test [4, 11], a statistical hypothesis test that answers the question “are these two sets drawn from the same underlying distribution?” Let F represents a (possibly unbounded) set of functions.",
      "startOffset" : 146,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "[11].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "[8] present a defense they call kernel density estimation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 31,
      "context" : "They use dropout, a common approach used during training to reduce network overfitting [34], for randomization.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "While we have found CIFAR to be a reasonable task for evaluating security, in the future as defenses improve it may become necessary to evaluate on harder datasets (such as ImageNet [7]).",
      "startOffset" : 182,
      "endOffset" : 185
    } ],
    "year" : 2017,
    "abstractText" : "Neural networks are known to be vulnerable to adversarial examples: inputs that are close to valid inputs but classified incorrectly. We investigate the security of ten recent proposals that are designed to detect adversarial examples. We show that all can be defeated, even when the adversary does not know the exact parameters of the detector. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and we propose several guidelines for evaluating future proposed defenses.",
    "creator" : "LaTeX with hyperref package"
  }
}