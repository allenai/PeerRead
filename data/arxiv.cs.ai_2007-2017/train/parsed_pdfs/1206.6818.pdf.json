{
  "name" : "1206.6818.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sensitivity Analysis for Threshold Decision Making with Dynamic Networks",
    "authors" : [ "Theodore Charitos" ],
    "emails" : [ "theodore@cs.uu.nl", "linda@cs.uu.nl" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Probabilistic graphical models are often used in contexts where human decision makers have to make a decision in uncertainty. The marginal probability distributions yielded by the model then are taken as input to a decision-making model. The simplest model for choosing between alternative decisions is the threshold decision-making model, in which an output probability is compared against a number of fixed threshold probabilities which demarcate the boundaries for the various decisions [13]. In our application for ICU care, for example, a clinician has to decide whether or not to start antibiotics treatment for a patient who is suspected of having ventilator-associated pneumonia (VAP), based upon the probability of VAP being present.\nProbabilistic graphical models are typically learned from data or constructed with the help of domain experts. Due to incompleteness of data and partial knowledge of the domain under study, the numerical parameters of the model tend to be inaccurate to at least some degree. The inaccuracies may affect the output probabilities of the model as well as the decisions based upon these probabilities. The effects\nof inaccuracies in the parameters of a network on its output probabilities can be studied by subjecting the network to a sensitivity analysis [2, 7, 8, 9]. In view of a decisionmaking model, however, robustness of the output of a probabilistic graphical model pertains not just to the computed output probabilities but also to the decisions based upon these probabilities. In this paper, we study this type of robustness for dynamic Bayesian networks (DBNs) in view of the threshold decision-making model.\nPrevious work on sensitivity analysis of Bayesian networks (BNs) in general showed that any posterior probability for an output variable is a quotient of two linear functions in any of the network’s parameters [8]; the posterior probability can further be expressed as a sum of such functions in all parameters from a single conditional probability table [2]. Building upon these results, we show that in sensitivity analysis of DBNs a quotient of polynomial functions is obtained, where the order of these polynomials is linear in the time scope taken into consideration [4, 6]. We further show how the resulting functions can be used to study the robustness of a decision that is based upon an output probability of the network. By doing so, we focus not just on the effect of varying a single parameter, but also on the effect of varying all parameters from a given conditional probability table. In our medical application, for example, we thus provide for studying the extent to which the sensitivity and specificity rates of a particular diagnostic test can be varied without affecting the clinician’s decision for treatment. We illustrate the various concepts involved by means of a sensitivity analysis of our dynamic network in the field of infectious disease [5].\nEstablishing the sensitivity functions for a DBN has a time complexity similar to that of performing exact inference in such a model. Using quotients of higher-order polynomials for further processing in view of the threshold decisionmaking model can also be highly demanding from a computational point of view [6]. To handle the complexity involved, we present an approximate method for sensitivity analysis that reduces the runtime requirements involved yet incurs only a small loss in accuracy."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "The simplest type of dynamic network is a hidden Markov model (HMM) H = (X, Y,A, O, Γ) involving a single stochastic process [14]. We use Xn to denote the hidden variable of the modelled process at time step n. Xn has the possible states xni , i = 1, . . . , l, l ≥ 2. The transition matrix for Xn is denoted as A= {ai,j} with elements ai,j =p(Xn+1 =xn+1j | Xn =xni ), i, j = 1, . . . , l, for all n. We denote the observable variables by Yn, with values ynj , j =1, . . . , m,m ≥ 2. The value of Yn is generated from the state of the hidden variable according to a timeinvariant observation matrix O={oi,j} with oi,j =p(Yn = ynj | Xn = xni ), i = 1, . . . , l, j = 1, . . . , m, for all n. We further denote by Γ={γi} the initial probability vector for the hidden variable, with γi = p(X1 =x1i ), i=1, . . . , l. A DBN of more general structure is an extension of an HMM, capturing a compound process that involves a collection of hidden variables. We assume that our dynamic networks are time invariant, that is, neither the topology nor the parameters of the model change across time steps.\nIn this paper, we focus on the task of monitoring in DBNs, that is, on computing marginal distributions for the model’s hidden variables for some time step n given the observations that are available up to and including that time step. For this purpose, the interface algorithm is available [11], which basically is an extension of the junction-tree algorithm for probabilistic inference in graphical models in general. The interface algorithm efficiently exploits the concept of forward interface, which is the set of variables at time step n that affect some variables at time step n + 1 directly; in the sequel, we use FI to denote this set of variables. The computational complexity of the interface algorithm is exponential in the number of hidden variables at each time step, which is prohibitive for larger models.\nThroughout the paper we will use the dVAP network for illustration purposes. The dVAP network is a real-life DBN for diagnosing VAP in ICU patients and is destined for use in clinical practice [5]. The network includes two interacting hidden processes (colonisation and pneumonia),\nthree input processes (summarised in immunological status), three input observable variables (hospitalisation, mechanical ventilation, and previous antibiotics), one hidden input variable (aspiration), and seven output observable variables (summarised in symptoms-signs). Per time step, representing a single day, the model includes 30 variables. Each of the interacting processes consists of seven subprocesses that are a-priori independent. The transition matrices of these subprocesses are moderately stochastic. Figure 1 shows the dVAP network in a compact way."
    }, {
      "heading" : "3 Sensitivity analysis revisited",
      "text" : "Having been studied extensively in the context of BNs, sensitivity analysis has received less attention in DBNs. In this section, we briefly review previous work on sensitivity analysis in BNs and extend it to a dynamic context."
    }, {
      "heading" : "3.1 Sensitivity analysis of BNs",
      "text" : "Sensitivity analysis of a BN amounts to establishing, for each of the network’s parameter probabilities, a function that expresses an output probability of interest in terms of the parameter under study [7, 9, 16]. We take the posterior probability p(b | e) for our probability of interest, where b is a specific value of the variable B and e denotes the available evidence; we further let θ = p(hi | π) be our parameter under study, where hi is a value of the variable H and π is a specific combination of values for the parents of H . The sensitivity of the probability p(b | e) to variation of the parameter θ now is expressed by a sensitivity function p(b | e)(θ). If the parameters p(hj | π), hj 6= hi, specified for H are co-varied proportionally according to\np(hj | π)(θ) = {\nθ if j = i p(hj | π) · 1−θ1−p(hi|π) otherwise\nfor p(hi | π) < 1, then this function is a quotient of two linear functions in θ, that is,\np(b | e)(θ) = p(b, e)(θ) p(e)(θ) = c1 · θ + c0 d1 · θ + d0\nwhere c1, c0, d1 and d0 are constants with respect to θ [7]. Under the assumption of proportional co-variation, therefore, any sensitivity function is characterised by at most three constants. Note that for parameters of which the probability of interest is algebraically independent, the function simply equals the posterior probability p(b | e). Any computations can therefore be restricted to the sensitivity set for the variable of interest, which can be readily established from the network’s graphical structure. An efficient scheme for sensitivity analysis is available [8], which requires an inward propagation in the junction-tree for processing evidence and an outward propagation for establishing the constants of the sensitivity functions for all parameters per output probability."
    }, {
      "heading" : "3.2 Sensitivity analysis of DBNs",
      "text" : "The main difference with sensitivity analysis of BNs is that a parameter occurs multiple times in a DBN. In previous work [4], we derived functional forms to express the sensitivity of a probability of interest of an HMM in terms of a parameter under study. We briefly review these sensitivity functions. We begin by studying the sensitivities of an HMM in which no evidence has been entered as yet. The probability of interest is the prior probability p(xnr ) of some state xr of the hidden variable Xn. Let θa = ai,j ∈ A be a transition parameter in the model such that Xn is algebraically dependent on θa. Then,\np(xnr )(θa) = c n−1 n,r · θn−1a + . . . + c1n,r · θa + c0n,r\nwhere cn−1n,r , . . . , c 0 n,r are constants with respect to θa dependent on n; note that the function is a polynomial of order n − 1 in the parameter under study. For an initial parameter θγ = γi ∈ Γ, the function is linear\np(xnr )(θγ) = c 1 n,r · θγ + c0n,r\nwhere c1n,r and c 0 n,r are constants with respect to θγ . For an HMM in which no evidence has been entered, the observable variables do not belong to the sensitivity set of the hidden variable. Its prior probability therefore is algebraically independent of any observation parameter.\nWe now assume that some evidence has been entered into the model; we use en to denote the cumulated evidence up to and including time step n. For the sensitivity function that expresses the posterior probability p(xnr | en) in terms of a transition parameter θa = ai,j ∈ A, we find that\np(xnr , en)(θa) p(en)(θa) = cn−1n,r ·θn−1a + . . . + c1n,r · θa + c0n,r dn−1n,r ·θn−1a + . . . + d1n,r ·θa + d0n,r\nwhere cn−1n,r , . . . , c 0 n,r, d n−1 n,r , . . . , d 0 n,r again are constants with respect to θa. The function thus is a quotient of two polynomials of order n − 1. For an observation parameter θo = oi,j , the sensitivity function is\np(xnr , en)(θo) p(en)(θo) = cbn,r ·θbo + . . . + c1n,r · θo + c0n,r dnn,r ·θno + . . . + d1n,r ·θo + d0n,r\nwhere b = n if r = i and b = n − 1 otherwise; cbn,r, . . . , c 0 n,r, d n n,r, . . . , d 0 n,r are constants with respect to θo. The order of the polynomials involved again grows linearly with n. For an initial parameter θγ , to conclude, we have that the sensitivity function is a quotient of two linear functions. Similar results hold for probabilities of interest belonging to any possible time step no <n or no >n [4].\nThe results for HMMs are readily generalised to DBNs. We consider the posterior probability of interest p(bnr | en) of the state br of the hidden variable Bn given the evidence en. Then, for any variable Hn ∈Sens(Bn, en), the sensitivity function expressing p(bnr |en) in θ=p(hni |π) is a quotient\nof two polynomials of order n−1 if Hn∈FI , or of order n otherwise.\nAs an example sensitivity function, Figure 2 depicts, for the dVAP network, the effect of varying the parameter θ = p(leucocytosis = yes | pneumonia = yes) on the probability of pneumonia at day 4 given evidence e4 for a specific patient. The depicted function is a quotient of two polynomials of order 4 each. For computing the constants in the various sensitivity functions, we combined the interface algorithm with the junction-tree scheme for sensitivity analysis; further details of the resulting algorithm are out of the scope of this paper."
    }, {
      "heading" : "4 Threshold decision making",
      "text" : "BNs in general yield marginal probability distributions for their output. Often these marginal distributions are input to a decision maker who has to make a decision. The simplest model for choosing between alternative decisions is the threshold decision-making model. In this section, we briefly review the threshold model for decision making and describe sensitivity analysis of BNs in view of this model.\nAlthough generally applicable, the threshold decisionmaking model is used most notably for patient management in medicine [13]. Since our dVAP network also pertains to the field of medicine, we present the model in medical terms. With the model, a clinician decides whether or not to gather additional information from diagnostic tests and whether or not to give treatment based upon the probability of disease for a patient. The threshold model to this end builds upon various threshold probabilities of disease.\nThe treatment threshold probability p∗ is the probability at which the clinician is indifferent between giving treatment and withholding treatment. If, for a specific patient, the probability of disease exceeds the treatment threshold probability, then the clinician will decide to treat the patient as if the disease were known to be present with certainty. Alternatively, if the probability of disease is less than p∗, the clinician will basically withhold treatment. As a consequence of the uncertainty concerning the true condition\nof the patient however, additional information from a diagnostic test may affect the clinician’s basic management decision. The threshold model therefore includes another two threshold probabilities. The no treatment-test threshold probability of disease p− is the probability at which the clinician is indifferent between the decision to withhold treatment and the decision to obtain additional diagnostic information. The test-treatment threshold probability p+ is the probability at which the clinician is indifferent between obtaining additional information and starting treatment rightaway. Figure 3 illustrates the various threshold probabilities employed by the model.\nIn view of the threshold model for decision making, sensitivity of the output of a network pertains no longer to just a probability of interest computed from the network, but also to the decision based upon it. To take the various threshold probabilities employed into consideration, the method of sensitivity analysis of BNs has been enhanced with the computation of upper and lower bounds between which a network’s parameters can be varied without inducing a change in decision [15]. The computation of these bounds builds upon the sensitivity functions relating the probability of interest to the network’s parameters. By equating the function for a specific parameter to the various threshold probabilities, bounds are obtained between which the parameter can be varied. Since the sensitivity functions for a BN are either monotonically non-decreasing or monotonically non-increasing, a single lower bound and a single upper bound are guaranteed to exist."
    }, {
      "heading" : "5 Sensitivity analysis of decisions with DBNs",
      "text" : "The probabilities established from a dynamic network are also often employed for decision making. The goal of the dVAP model, for example, is to monitor the onset of ventilator-associated pneumonia in ICU patients and to start appropriate treatment as soon as possible. Sensitivity to parameter variation then pertains not just to the probability of VAP but also to the decision that the clinician makes based upon this probability. To provide for studying this type of sensitivity, we extend sensitivity analysis of DBNs in view of threshold decision making."
    }, {
      "heading" : "5.1 Analysis of single parameters",
      "text" : "Suppose that a posterior probability p(xnr | en) of interest has been computed from a DBN; based upon this probability, a particular decision has been established from the threshold decision-making model. We now are interested\nin the effect of variation of a parameter θ on this decision. To compute upper and lower bounds between which the parameter can be varied without inducing a change in decision, the sensitivity function p(xnr | en)(θ) is equated to the threshold probabilities p− and p+, respectively. The lower and upper bounds thus are solutions of the equations\np(xnr , en)(θ) p(en)(θ) = p− and p(xnr , en)(θ) p(en)(θ) = p+ (1)\nWe recall that for DBNs a sensitivity function in general is a quotient of higher-order polynomials. Contrary to threshold decision making for BNs therefore, there is no guarantee that these functions are monotonically non-decreasing or non-increasing. The equations stated above may thus have multiple solutions instead of single ones.\nWe begin by studying a parameter for which single solutions exist for the two threshold equations above. Suppose that the lower and upper bounds computed from the equations are θ− and θ+ respectively. If p(xnr | en) < p−, then the decision to withhold treatment remains unaltered for any value of θ within the interval (−∞, θ−) ∩ [0, 1]. If p(xnr | en) > p+, the decision to start treatment immediately remains unaltered for any value of θ within (θ+,+∞) ∩ [0, 1]. If p− ≤ p(xnr | en) ≤ p+, then the decision to gather further information will be the same for any value of θ within the interval (θ−, θ+) ∩ [0, 1]. As an example from the dVAP network, we illustrate the bounds on variation of the parameter θ=p(leucocytosis= yes | pneumonia = yes) in view of the management decision for a particular patient at day 4; Figure 2 shows the sensitivity function that expresses the probability of VAP for this patient in terms of θ. Suppose that the three threshold probabilities are fixed at p∗ = 0.2, p− = 0.12, and p+ = 0.64. From the dVAP network, we have that p(vap4 | e4) = 0.134 and hence that p− ≤ p(vap4 | e4) ≤ p+. The clinician thus decides to gather additional information for the patient. Solving the two threshold equations from (1), we find a lower bound on the parameter under study equal to θ− = 0.676; the upper bound is θ+ = 1.1063. For any value of the parameter within the interval (0.676, 1], therefore, the decision to gather additional information will remain unaltered. Since the original value of the parameter has been assessed at 0.7, we conclude that the decision is not very robust with regard to this parameter.\nWe now turn to parameters for which the threshold equations have multiple solutions. Suppose that from the notreatment-test threshold probability p−, we find the vector of solutions θ− = (θ−1 , θ − 2 , . . . , θ − r ), in which the parameter values θ−i are given in ascending order; from the test-treatment threshold probability p+, we find the vector θ+ = (θ+1 , θ + 2 , . . . , θ + s ), again with the θ + i in ascending order. We further use v(θi) to denote the value of the first derivative of the sensitivity function for the parameter θ at θi. The value of the first derivative helps in determining\nthe intervals in which a particular decision holds. Now, if the output probability p is smaller than p−, the decision to withhold treatment remains unaltered for any value of θ that belongs to the compound interval\nΘ−+ = {\n[0, θ−1 )∪(θ−2 , θ−3 )∪. . .∪(θ−r , 1] r is even [0, θ−1 )∪(θ−2 , θ−3 )∪. . .∪(θ−r−1, θ−r ) r is odd\nwhenever v(θ−1 ) > 0, and for any value of θ belonging to Θ−−= { (θ−1 , θ − 2 )∪(θ−3 , θ−4 )∪. . . ∪ (θ−r−1, θ−r ] r is even\n(θ−1 , θ − 2 )∪(θ−3 , θ−4 )∪. . . ∪ (θ−r , 1] r is odd\nwhenever v(θ−1 ) < 0. Similarly, if the output probability p is larger than p+, we find compound intervals Θ+− and Θ + + for the parameter θ within which the decision to start treatment immediately remains unaltered. Finally, if the output probability lies between p− and p+, the vectors θ− and θ+ are merged into the vector θm = (θm1 , θ m 2 , . . . , θ m q ), q = r + s. Now, the decision will be the same for any value of θ within the interval Θm Θm = {\n[0, θm1 )∪(θm2 , θm3 )∪. . .∪(θmq−1, θmq ) v(θm1 )<0 (θm1 , θ m 2 )∪(θm3 , θm4 )∪. . .∪(θmq−1, θmq ) v(θm1 )>0\nNote that in case v(θm1 )>0 and the value of the sensitivity function for θ=0 is greater than p−, the interval Θm is the same as when v(θm1 )<0.\nAs an example, Figure 4 depicts the probability of pneumonia at day 4 given evidence e4 in terms of the parameter θ = p(temperature = low | pneumonia = no, a.drugs = yes); the original value of θ equals 0.2, giving p(vap4 | e4) = 0.278. The figure also shows the intersection points with the threshold probabilities, which have been set at p− = 0.34 and p+ = 0.88. We compute the two lower bounds to be θ−1 = 0.0918 and θ − 2 = 0.4335; the upper bound is θ+1 = 0.8781. Using v(θ − 1 ) < 0, we find that the decision to withhold treatment remains unaltered for any value of θ in (0.0918, 0.4335). We conclude that the decision is relatively robust with regard to the parameter."
    }, {
      "heading" : "5.2 Analysis of full conditional probability tables",
      "text" : "In addition to single parameters, we may be interested in the effects of varying multiple parameters, for example\nfrom a single conditional probability table (CPT) [3]. In a medical application for instance, we may wish to study the robustness of a decision in terms of both the sensitivity and the specificity of a particular diagnostic test and not just in one of these rates. Recall that these rates express the probabilities that a test result is found to be positive (negative) in a patient who does (does not) have the disease. We now extend the previous results for single parameters to CPTs to provide for such an analysis.\nFor BNs, any posterior probability for the output variable is a quotient of sums of linear functions in the parameters of a CPT [3, 8]. For a dynamic network we obtain sums of polynomial functions. For a joint probability p(bnr , en) we find that\np(bnr , en) = |πCn |∑\ni=1\ngi(θi)\nwhere |πCn | denotes the number of parent configurations of the variable Cn and gi(θi) represents a polynomial function in the parameter θi for a specific parent configuration for Cn. The polynomial functions gi(θi) are all of the same order and can be computed individually using the considerations of the previous section. For the joint probability p(en) a similar result holds. We conclude that, for a DBN, the sensitivity function for a CPT is a quotient of sums of higher-order polynomials in the parameters under study.\nAs an example, Figure 5 illustrates the effect of varying the sensitivity and specificity rates of the CPT for radiological signs of pneumonia at day 5 given evidence e5 for a specific patient. The depicted sensitivity function is a quotient of sums of two polynomial functions of order 5 each.\nUpon studying the effects of varying all parameters from a CPT in view of threshold decision making, we have to solve threshold equations similar to (1). For a single parameter, we identified intervals for the parameter’s value within which a decision remains unaltered. For a CPT, we now identify areas in higher-space with the same meaning. In the remainder of this section, we consider a CPT with\nsensitivity and specificity rates as in the previous example; similar results hold for more complex CPTs.\nWe begin again by studying a CPT for which single lower bounds exist for the two rates, denoted as θ−se and θ − sp respectively. By re-arranging the individual polynomial functions included, we can express the relationship between θ−se and θ−sp as\ng̃(θ−se) = ĝ(θ − sp)\nwhere g̃ and ĝ are polynomials of the same order. If g̃ is invertible in [0, 1], we have that\nθ−se = g̃ −1(ĝ(θ−sp))\nwhich defines the relationship between the θ−se and θ − sp. The horizontal line test can be used for checking whether g̃ is invertible in [0, 1]. Establishing g̃−1, however, is computationally expensive if not infeasible [10].\nTo determine the relationship between θ−se and θ − sp and thereby study the robustness of a recommended decision, we use a numerical approximation procedure. We repeatedly pick a value θ′−se ∈ [0, 1] and solve for θ′−sp ∈ [0, 1]. From the pairs (θ′−se , θ′−sp ) thus obtained, we construct a line l− representing the relationship between θ−se and θ − sp. A similar approach is used to find a line l+ that represents the relationship between the upper bounds θ+se and θ + sp for the two rates. We note that our procedure requires solving just a single polynomial equation, which is feasible in general [12]. For larger CPTs, however, the procedure becomes computationally more demanding, since a larger sample of points is required to assure good results.\nWe now have that, if the probability of interest p(xnr | en) falls below p−, the decision to withhold treatment remains unaltered for any pair (θse, θsp) below l−. If p(xnr | en) > p+, the decision to start treatment remains unaltered for any pair (θse, θsp) above l+. Finally, if p− ≤ p(xnr | en) ≤ p+, the decision will be the same for any pair (θse, θsp) between l− and l+.\nFigure 6 now illustrates the sensitivity analysis. With our approximation procedure, two lines are established that\nserve to divide the unit square into three areas in which different decisions apply. For our example patient, we have that p(vap5 | e5)=0.9842 > p+. Since the original values for the sensitivity and specificity rates under study are 0.9 and 0.95 respectively, we conclude that the decision to start treatment right away is quite robust with regard to the CPT. The three bullets in the figure highlight three other interesting cases. For the bullet with θsp = 0.6, we observe that the decision to test is only moderately robust since a small change in θsp or θse can alter the decision. For the bullet with θsp = 0.8, the decision to treat is quite robust since only a major change in θsp or θse can induce another decision. Finally, for the bullet with θsp = 0.87, the decision to test is not robust at all since a small change in θsp or θse suffices to alter the decision.\nTo conclude, we note that when the function g̃−1 is not invertible, our sampling procedure will result in multiple solutions. The unit square for θse and θsp will then be divided in compound areas per decision, similar to the compound intervals in the single-parameter case."
    }, {
      "heading" : "6 An approximate scheme",
      "text" : "The number of constants in the sensitivity functions of a DBN and the number of propagations required to compute these constants grows linearly with n. Moreover, the computational burden of solving polynomials of high order can grow dramatically [12]. For a larger time scope, therefore, sensitivity analysis in view of threshold decision making can become quite hard. To reduce the order of the polynomials in the sensitivity functions and thereby the runtime requirements, we present a method for approximate sensitivity analysis that builds on the concept of contraction of a Markov process [1]. We discuss our method for DBNs with a single hidden process and review its extension to DBNs with multiple processes.\nWe consider two probability distributions µ and µ′ over a variable W . Conditioning on a set of observations is known to never increase the relative entropy of these distributions. Denoting the conditioning by o(·), we thus have that\nD[o(µ)‖o(µ′)] ≤ D[µ‖µ′] (2) where D stands for the relative entropy. Now, consider the extreme case where µ and µ′ have their entire probability mass on two different states wi and wk respectively. We denote by A(·) the distribution that results from processing through the transition matrix A. Even though µ and µ′ do not agree on any state, processing will cause them to place some mass on some state wj . They then agree for a mass of min [ A(µ(wj ; wi)), A(µ′(wj ; wk)) ] on that state wj . Based on this property, the minimal mixing rate of the matrix A is defined as\nδA = min i,k\n∑\nj\nmin [ A(µ(wj ; wi)), A(µ′(wj ; wk)) ]\nGiven the minimal mixing rate of a transition matrix A, the following theorem now holds [1]:\nD[A(µ)‖A(µ′)] ≤ (1− δA) ·D[µ‖µ′]\nWe say that the stochastic process with transition matrix A contracts with probability δA. Combining equation (2) with the previous theorem gives\nD[A(o(µ))‖A(o(µ′))] ≤ (1− δA) ·D[µ‖µ′]\nPerforming conditioning on two different distributions and subsequently transitioning them, will thus result in two new distributions whose distance in terms of relative entropy is reduced by a factor smaller than one.\nOur approximate method for sensitivity analysis now builds on the contraction property reviewed above. Suppose that we are interested in the probability of some state of the hidden variable Xn at time step n. After entering the available evidence en into the model, we can compute the exact posterior distribution p(Xn | en). Building on the contraction property, however, we can also compute an approximate distribution p̃(Xn | en) starting from time step nφ, with 1 < nφ < n, without losing too much accuracy. We define the backward acceptable window ωφn,² for time step n with a specified level of accuracy ², to be the number of time steps we need to use from the past to compute the probability distribution of the hidden variable at time step n within an accuracy of ². We now propose to perform sensitivity analysis for time step n considering only the backward acceptable window ωφn,². Note that the resulting functions then include polynomials of order O(n − nφ) rather than of order O(n) compared to the true functions.\nFor a given level of accuracy ², we have to determine the maximum value of nφ for which\nD[p(Xn |en)‖p̃(Xn |en)] ≤ (1− δA)n−nφ ·D[p(Xnφ |enφ)‖p(X1)]≤²\nwhere p̃(Xn | en) denotes the approximate distribution of Xn that is computed using ωφn,². Solving for nφ, we find that\nnφ =max { 1, n− ⌊ log ( ²/D[p(Xnφ |enφ)‖p(X1)] )\nlog(1− δA)\n⌋}\n(3) where b·c stands for the integer part. Starting from nφ = n and decreasing the value of nφ one step at a time, we can readily establish the value of nφ that first satisfies the equation (3). To this end, the interface algorithm needs to have computed and stored the exact posterior distributions p(Xno |eno) for all no≤n, given evidence eno . The procedure to compute the optimal value nφ requires at most n computations of (3) and thus is not very demanding from a computational point of view. We recall, how-\never, that for the computation of nφ, the interface algorithm needs to have established the exact posterior distributions given the available evidence. Now in a full sensitivity analysis, the effects of parameter variation are being studied for a number of evidence profiles. The above procedure may then become rather demanding since for every such profile a full propagation with the interface algorithm is required. An alternative way would then be to approximate nφ given ² from the start and perform the entire analysis with the backward acceptable window ωφn,². If we assume that D[p(Xnφ)‖p(X1)] is bounded from above by a known constant M , we find that an approximate value for nφ would satisfy\nnφ ≈ max { 1, n− ⌊\nlog(²/M) log(1− δA)\n⌋}\nNote that for a given ² and δA, the higher the value of M , the smaller the value of nφ and hence the larger the backward acceptable window. Knowledge of the domain under study can help in determining a suitable value for M . For a patient profile for example, M can be determined by inserting worst-case scenario observations for the first time step and computing for that time the posterior probability distribution for the hidden variable from which M can be readily established. The complexity that our method now entails is just the complexity of computing M which is similar to performing a single propagation for a single time step. This computational burden is considerably less than the burden of performing nφ time steps of exact inference, which we thereby forestall in the sensitivity analysis. Note that for some patients the computation of nφ based upon this value M will lead to a larger backward acceptable window than the one computed directly from equation (3).\nIn view of sensitivity analysis, we observe that the value of nφ that is established as outlined above, is based on the original values of all parameters of the model under study. We further observe that the minimal mixing rate δA used in the computation of nφ is algebraically dependent only on the model’s transition parameters. Using ωφn,² based upon nφ for sensitivity analysis, therefore, is guaranteed to result in approximate sensitivity functions within an accuracy of ² for any non-transition parameter. For transition parameters, this guarantee does not hold in general. We note, however, that for the original value of a transition parameter, the difference between the true probability of interest and the approximate one is certain to be smaller than ². Since the value nφ changes with δA in a stepwise manner only, this property holds for a range of values for the parameter. Figure 7 illustrates the relationship between nφ and δA given particular values for n, ² and M . We observe from the figure that there is a range of values of δA for which the value of nφ stays the same. We expect a similar property to hold for a range of values for the transition parameter θa. We are currently studying this issue and hope to report results in the near future.\nIn general, a DBN with multiple interacting subprocesses can be represented as a single-process stochastic model with a global transition matrix AG by enumerating all combinations of values for the subprocesses. We can show that a lower bound on the global mixing rate δAG can be computed from knowledge of the contraction rates of the individual subprocesses of the model [6]. For a DBN composed of several sparsely interacting subprocesses each of which is fairly stochastic, we expect a reasonable lower bound on the mixing rate δAG . We recall that the larger the mixing rate, the larger nφ and the smaller the backward window that we can acceptably use for the sensitivity analysis. For all patients in our application for ICU care for example, we found that instead of using the observations for 10 days upon performing a sensitivity analysis for the probability of VAP, we could use the observations from day 5 on with an average error smaller than ² = 0.003. This result is quite promising as it shows that even if the dynamic processes of a DBN are only moderately stochastic, the backward acceptable window can still be small enough to allow for good approximations of the sensitivity functions."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper we presented functional forms to express the sensitivity of a probability of interest of a DBN in terms of one or more parameters from a single CPT. We used these sensitivity functions for studying the robustness of the output of the network in view of the threshold decision making. In addition, we presented an approximate scheme for computing the constants involved in the sensitivity functions that is less demanding than an exact algorithm yet incurs only a small loss in accuracy. We illustrated our results with a real-life dynamic network for ICU care."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research was (partly) supported by the Netherlands Organization for Scientific Research (NWO)."
    } ],
    "references" : [ {
      "title" : "Inference and Learning in Complex Stochastic Processes",
      "author" : [ "X. Boyen" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "When do numbers really matter",
      "author" : [ "H. Chan", "A. Darwiche" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Sensitivity analysis in Bayesian networks: From single to multiple parameters",
      "author" : [ "H. Chan", "A. Darwiche" ],
      "venue" : "Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Sensitivity properties of Markovian models",
      "author" : [ "T. Charitos", "L.C. Van der Gaag" ],
      "venue" : "Proceedings AISTA Conference in Cooperation with IEEE Computer Society,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "A dynamic Bayesian network for diagnosing ventilator-associated pneumonia in ICU patients",
      "author" : [ "T. Charitos", "L.C. Van der Gaag", "S. Visscher", "K. Schurink", "P. Lucas" ],
      "venue" : "Working notes of the 10th Workshop on Intelligent Data Analysis in Medicine and Pharmacology,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Sensitivity analysis of Markovian models",
      "author" : [ "T. Charitos", "L.C. Van der Gaag" ],
      "venue" : "Proceedings of the 19th International Florida Artificial Intelligence Research Society Conference,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Properties of sensitivity analysis of Bayesian belief networks",
      "author" : [ "V.M.H. Coupé", "L.C. Van der Gaag" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "Making sensitivity analysis computationally efficient",
      "author" : [ "U. Kjaerulff", "L.C. Van der Gaag" ],
      "venue" : "Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Sensitivity analysis for probability assessments in Bayesian networks",
      "author" : [ "K.B. Laskey" ],
      "venue" : "IEEE Transactions on Systems, Man and Cybernetics",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1995
    }, {
      "title" : "Dynamic Bayesian Networks: Representation, Inference and Learning",
      "author" : [ "K.P. Murphy" ],
      "venue" : "Ph.D. diss,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Solving a polynomial equation: Some history and recent progress",
      "author" : [ "V.Y. Pan" ],
      "venue" : "SIAM Review",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1997
    }, {
      "title" : "The threshold approach to clinical decision making",
      "author" : [ "Pauker", "S.G", "J.P. Kassirer" ],
      "venue" : "New England Journal of Medicine",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1980
    }, {
      "title" : "A tutorial on hidden Markov models and selected applications in speech recognition",
      "author" : [ "L.R. Rabiner" ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1989
    }, {
      "title" : "Sensitivity analysis for threshold decision making with Bayesian belief networks. AI*IA 99: Advances in Artificial Intelligence, Lecture",
      "author" : [ "L.C. Van der Gaag", "Coupé", "V.M.H" ],
      "venue" : "Notes in Artificial Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2000
    }, {
      "title" : "Analysing sensitivity data from probabilistic networks",
      "author" : [ "L.C. Van der Gaag", "S. Renooij" ],
      "venue" : "Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "The simplest model for choosing between alternative decisions is the threshold decision-making model, in which an output probability is compared against a number of fixed threshold probabilities which demarcate the boundaries for the various decisions [13].",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 1,
      "context" : "to a sensitivity analysis [2, 7, 8, 9].",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "to a sensitivity analysis [2, 7, 8, 9].",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "to a sensitivity analysis [2, 7, 8, 9].",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "to a sensitivity analysis [2, 7, 8, 9].",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "Previous work on sensitivity analysis of Bayesian networks (BNs) in general showed that any posterior probability for an output variable is a quotient of two linear functions in any of the network’s parameters [8]; the posterior probability can further be expressed as a sum of such functions in all parameters from a single conditional probability table",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "[2].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "Building upon these results, we show that in sensitivity analysis of DBNs a quotient of polynomial functions is obtained, where the order of these polynomials is linear in the time scope taken into consideration [4, 6].",
      "startOffset" : 212,
      "endOffset" : 218
    }, {
      "referenceID" : 5,
      "context" : "Building upon these results, we show that in sensitivity analysis of DBNs a quotient of polynomial functions is obtained, where the order of these polynomials is linear in the time scope taken into consideration [4, 6].",
      "startOffset" : 212,
      "endOffset" : 218
    }, {
      "referenceID" : 4,
      "context" : "We illustrate the various concepts involved by means of a sensitivity analysis of our dynamic network in the field of infectious disease [5].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "Using quotients of higher-order polynomials for further processing in view of the threshold decisionmaking model can also be highly demanding from a computational point of view [6].",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : "The simplest type of dynamic network is a hidden Markov model (HMM) H = (X, Y,A, O, Γ) involving a single stochastic process [14].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "For this purpose, the interface algorithm is available [11], which basically is an extension of the junction-tree algorithm for probabilistic inference in graphical models in general.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "The dVAP network is a real-life DBN for diagnosing VAP in ICU patients and is destined for use in clinical practice [5].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "each of the network’s parameter probabilities, a function that expresses an output probability of interest in terms of the parameter under study [7, 9, 16].",
      "startOffset" : 145,
      "endOffset" : 155
    }, {
      "referenceID" : 8,
      "context" : "each of the network’s parameter probabilities, a function that expresses an output probability of interest in terms of the parameter under study [7, 9, 16].",
      "startOffset" : 145,
      "endOffset" : 155
    }, {
      "referenceID" : 14,
      "context" : "each of the network’s parameter probabilities, a function that expresses an output probability of interest in terms of the parameter under study [7, 9, 16].",
      "startOffset" : 145,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "where c1, c0, d1 and d0 are constants with respect to θ [7].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "An efficient scheme for sensitivity analysis is available [8], which requires an inward propagation in the junction-tree for processing evidence and an outward propagation for establishing the constants of the sensitivity functions for all parameters per output probability.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "In previous work [4], we derived functional forms to express the sensitivity of a probability of interest of an HMM in terms of a parameter under study.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "Similar results hold for probabilities of interest belonging to any possible time step no <n or no >n [4].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "Although generally applicable, the threshold decisionmaking model is used most notably for patient management in medicine [13].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "To take the various threshold probabilities employed into consideration, the method of sensitivity analysis of BNs has been enhanced with the computation of upper and lower bounds between which a network’s parameters can be varied without inducing a change in decision [15].",
      "startOffset" : 269,
      "endOffset" : 273
    }, {
      "referenceID" : 0,
      "context" : "If p(xr | en) < p−, then the decision to withhold treatment remains unaltered for any value of θ within the interval (−∞, θ−) ∩ [0, 1].",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "If p(xr | en) > p, the decision to start treatment immediately remains unaltered for any value of θ within (θ+,+∞) ∩ [0, 1].",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "If p− ≤ p(xr | en) ≤ p, then the decision to gather further information will be the same for any value of θ within the interval (θ−, θ) ∩ [0, 1].",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "from a single conditional probability table (CPT) [3].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "For BNs, any posterior probability for the output variable is a quotient of sums of linear functions in the parameters of a CPT [3, 8].",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "For BNs, any posterior probability for the output variable is a quotient of sums of linear functions in the parameters of a CPT [3, 8].",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "If g̃ is invertible in [0, 1], we have that",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "The horizontal line test can be used for checking whether g̃ is invertible in [0, 1].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "We repeatedly pick a value θ′− se ∈ [0, 1] and solve for θ′− sp ∈ [0, 1].",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "We repeatedly pick a value θ′− se ∈ [0, 1] and solve for θ′− sp ∈ [0, 1].",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "We note that our procedure requires solving just a single polynomial equation, which is feasible in general [12].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "Moreover, the computational burden of solving polynomials of high order can grow dramatically [12].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "To reduce the order of the polynomials in the sensitivity functions and thereby the runtime requirements, we present a method for approximate sensitivity analysis that builds on the concept of contraction of a Markov process [1].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 0,
      "context" : "Given the minimal mixing rate of a transition matrix A, the following theorem now holds [1]:",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "We can show that a lower bound on the global mixing rate δAG can be computed from knowledge of the contraction rates of the individual subprocesses of the model [6].",
      "startOffset" : 161,
      "endOffset" : 164
    } ],
    "year" : 2006,
    "abstractText" : "The effect of inaccuracies in the parameters of a dynamic Bayesian network can be investigated by subjecting the network to a sensitivity analysis. Having detailed the sensitivity functions involved in our previous work, we now study the effect of parameter inaccuracies on a recommended decision in view of a threshold decisionmaking model. We describe the effect of varying one or more parameters from a conditional probability table and present a computational procedure for establishing bounds between which assessments for these parameters can be varied without inducing a change in the recommended decision. We illustrate the various concepts by means of a real-life dynamic network in the field of infectious disease.",
    "creator" : " TeX output 2006.05.19:1913"
  }
}