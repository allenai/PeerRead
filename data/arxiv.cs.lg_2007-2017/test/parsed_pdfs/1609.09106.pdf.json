{
  "name" : "1609.09106.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "David Ha", "Andrew Dai", "Quoc V. Le" ],
    "emails" : [ "hadavid@google.com", "adai@google.com", "qvl@google.com" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In this work, we consider an approach of using a small network (called the ”hypernetwork”) to generate the weights for a larger network (called the main network). The behavior of the main network is the same with any usual neural network: it learns to map some raw inputs to their desired targets; whereas the hypernetwork takes a set of inputs that contain information about the structure of the weights and generates the weight for that layer (see Figure 1).\nFigure 1: A hypernetwork generates the weights for a feedforward network. Black connections and parameters are associated the main network whereas orange connections and parameters are associated with the hypernetwork.\nHyperNEAT (Stanley et al., 2009) is an example of a hypernetwork where the inputs are a set of virtual coordinates for each weight in the main network. In this work, we will focus on a more powerful approach where the input is an embedding vector that describes the entire weights of a given layer. Our embedding vectors can be fixed parameters that are also learned during end-to-end training, allowing approximate weight-sharing within a layer and across layers of the main network. In\n∗Work done as a member of the Google Brain Residency program (g.co/brainresidency).\nar X\niv :1\n60 9.\n09 10\n6v 1\n[ cs\n.L G\n] 2\n7 Se\np 20\n16\naddition, our embedding vectors can also be generated dynamically by our hypernetwork, allowing the weights of a recurrent network to change over timesteps and also adapt to the input sequence.\nWe perform experiments to investigate the behaviors of hypernetworks in a range of contexts. We find that hypernetworks mix well with other techniques such as batch normalization and layer normalization. Our main finding is that hypernetworks can generate non-shared weights for LSTM that work better than the standard version of LSTM (Hochreiter & Schmidhuber, 1997). On language modeling tasks with Character Penn Treebank, Hutter Prize Wikipedia datasets and a handwriting generation task with IAM handwriting dataset, hypernetworks for LSTM achieve state-of-the-art results. On image classification with CIFAR-10, hypernetworks, when being used to generate weights for a deep convnet (LeCun et al., 1990), obtain respectable results compared to state-of-the-art models while having fewer learnable parameters."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "The concept of using a neural network to generate weights for another neural network has an origin in evolutionary computing. It is often difficult for evolutionary algorithms to directly operate in large search spaces consisting of millions of weight parameters. A more efficient method is to first evolve a smaller network to generate the structure of weights for a larger network, and search within the much smaller parametrized weight space. An instance of this is the work on Hypercubebased NEAT or HyperNEAT framework (Stanley et al., 2009). In the HyperNEAT framework, Compositional Pattern-Producing Networks (CPPNs) are evolved to define the weight structure of much larger main network. Closely related to our approach is a simplified variation of HyperNEAT, where the structure is fixed and the weights are evolved through Discrete Cosine Transform is called Compressed Weight Search (Koutnik et al., 2010). Even more closely related to our approach is Differentiable Pattern Producing Networks (DPPNs), where the structure is evolved but the weights are learned (Fernando et al., 2016).\nMost reported results using these methods, however, are in small scales, perhaps because they are both slow to train and require a lot of heuristics to be efficient. The main difference between our approach and HyperNEAT (even with DPPNs) is that hypernetworks in our approach are trained end-to-end with gradient descent together with the main network, and therefore are more efficient.\nIn addition to end-to-end learning with gradient descent, our approach strikes a good balance between Compressed Weight Search and HyperNEAT in terms of model flexibility and training simplicity. Firstly, it can be argued that Discrete Cosine Transform used in Compressed Weight Search may be too simple and using the FFT prior may not be suitable for many problems. Secondly, even though HyperNEAT is more flexible, evolving both the architecture and the weights in HyperNEAT is often an overkill for most practical problems.\nThe focus of this work is to generate weights for practical architectures, such as convolutional networks and recurrent networks by taking layer embedding vectors as inputs. However, our hypernetworks can also be utilized to generate weights for a fully connected networks by taking coordinate information as inputs similar to DPPNs. Using this setting, hypernetworks can approximately recover the convolutional architecture without explicitly being told to do so, a similar result obtained by ”Convolution by Evolution” (Fernando et al., 2016). This result is described in Appendix A.1."
    }, {
      "heading" : "3 METHODS",
      "text" : "Even though hypernetworks can be used to learn weight-sharing within a layer in a fully connected network, as mentioned above, the focus of this work is to make hypernetworks useful for very deep convolutional networks and very long recurrent networks. In this context, we view convolutional networks and recurrent networks as two ends of a spectrum. On one end, recurrent networks can be seen as imposing weight-sharing across layers, which makes it inflexible and difficult to learn due to vanishing gradient. On the other end, convolutional networks enjoy the flexibility of not having weight-sharing, at the expense of having lots of parameters when the networks are deep. Hypernetworks can be seen as a form of relaxed weight-sharing, and therefore strikes a balance between the two ends."
    }, {
      "heading" : "3.1 STATIC HYPERNETWORK: A WEIGHT FACTORIZATION APPROACH FOR DEEP CONVOLUTIONAL NETWORKS",
      "text" : "In this section we will describe how we construct a hypernetwork for the purpose of generating the weights of a feedforward convolutional network (similar to Figure 2). In a typical deep convolutional network, the majority of model parameters resides in the kernels within the convolutional layers. Each kernel contain Nin ×Nout filters and each filter has dimensions fsize × fsize. Let’s suppose that these parameters are stored in a matrix Kj ∈ RNinfsize×Noutfsize for each layer j = 1, .., D, whereD is the depth of the main convolutional network. For each layer j, the hypernetwork receives a layer embedding zj ∈ RNz as input and outputs Kj , which can be generally written as follows:\nKj = g(zj), ∀j = 1, ..., D (1)\nWe first note that this matrix Kj can be broken down as Nin slices of a smaller matrix with dimensions fsize ×Noutfsize, each slice of the kernel is denoted as Ki ∈ Rfsize×Noutfsize . Therefore, in our approach, the hypernetwork is a two-layer linear network. The first layer of the hypernetwork takes the input vector zj and linearly projects it into the Nin inputs, with Nin different matrices Wi ∈ Rd×Nz and bias vectors Bi ∈ Rd, where d is the size of the hidden layer in the hypernetwork. For our purpose, we fix d to be equal to Nz although they can be different. The final layer of the hypernetwork is a linear operation which takes an input vector ai of size d and linearly projects that into Ki using a common tensor Wout ∈ Rfsize×Noutfsize×d and bias matrix Bout ∈ Rfsize×Noutfsize . The final kernel Kj will be a concatenation of every Kji . Thus g(z j) can be written as follows:\naji =Wiz j +Bi, ∀i = 1, .., Nin,∀j = 1, ..., D\nKji = 〈Wout, aji 〉 1 +Bout, ∀i = 1, .., Nin,∀j = 1, ..., D Kj = ( Kj1 K j 2 ... K j i ... K j Nin ) , ∀j = 1, ..., D\n(2)\nIn our system, the learnable parameters are Wi, Bi, Wout, Bout together with all zj’s. During inference, the model simply takes the layer embeddings zj learned during training to reproduce the kernel weights for layer j in the main convolutional network. As a side effect, the number of trainable model parameters in hypernetwork will be much lower than the main convolutional network. In fact, the total number of learnable parameters in hypernetwork is Nz ×D+ d× (Nz + 1)×Ni+fsize×Nout×fsize×(d+1) compared to theD×Nin×fsize×Nout×fsize parameters for the kernels of the main convolutional network.\nOur approach of constructing g(.) is similar to the hierarchically semiseparable matrix approach proposed by Xia et al. (2010). Note that even though it seems redundant to have a two-layered linear hypernetwork as that is equivalent to a one-layered hypernetwork, the fact that Wout and Bout are shared makes our two-layered hypernetwork more compact than a one-layered hypernetwork. More\n1Tensor dot product between W ∈ Rm×n×d and a ∈ Rd. Result 〈W,a〉 ∈ Rm×n\nconcretely, a one-layered hypernetwork would have Nz × Nin × fsize × Nout × fsize learnable parameters which is usually much bigger than a two-layered hypernetwork does.\nThe above formulation assumes that the network architecture consists of kernels with same dimensions. In practice, deep convolutional network architectures consists of kernels of varying dimensions. Typically, in many designs, the kernel dimensions are integer multiples of a basic size. This is indeed the case in the residual network family of architectures (He et al., 2016a) that we will be experimenting with later is an example of such a design. In our experiments, although the kernels of a residual network do not share the same dimensions, the Ni and Nout dimensions for each kernel are integer multiples of 16. To modify our approach to work with this architecture, we have our hypernetwork generate kernels for this basic size of 16, and if we require a larger kernel for a certain layer, we will concatenate multiple basic kernels together to form the larger kernel.\nK32×64 = ( K1 K2 K3 K4 K5 K6 K7 K8 ) (3)\nFor example, if we need to generate a kernel with Ni = 32 and Nout = 64, we will tile eight basic kernels together. Each basic kernel is generated by a unique z embedding, hence the larger kernel will be expressed with eight embeddings. Therefore, kernels that are larger in size will require a proportionally larger number of embedding vectors. For visualizations of concatenated kernels, please see Appendix A.2.1."
    }, {
      "heading" : "3.2 DYNAMIC HYPERNETWORK: ADAPTIVE WEIGHT GENERATION FOR RECURRENT NETWORKS",
      "text" : "In the previous section, we outlined a procedure for using a single hypernetwork to generate the weights for a deep convolutional network. In the extreme case, we can force zj to be to be identical across layers, enforcing hard weight-sharing across all the layers, which is often found in recurrent networks. Therefore, when being used for recurrent networks, hypernetworks can be seen as a form of relaxed weight-sharing, which is a compromise between hard weight-sharing, and no weight-sharing. While weight-sharing reduces the number of model parameters, it may also limit the expressiveness of the model. The relaxed weight-sharing methodology via hypernetworks allows us to control the trade off between the number of model parameters and model expressiveness.\nFor static hypernetwork models, a vector zj is learned during training and does not change during inference. We would also like to explore models that can dynamically generate zj during inference, thereby generating a customized set of weights tailored for each individual input sample.\nIn this section, we will describe an extension to RNNs where the hard weight-sharing assumption is relaxed. Through the application of a dynamic hypernetwork, we can generate a different set of weights for an RNN at each timestep, and also for each individual sample. Our hypernetwork will also be an RNN, albeit a much smaller one. At every timestep, we concatenate both the input sample and also the hidden states of the main RNN, and feed this concatenated vector as the input into the HyperRNN. The HyperRNN will generate a small signal vector at each time step which is then used to generate the weight matrix used by the main RNN at the same timestep. Both the HyperRNN and the main RNN is trained end-to-end together at the same time."
    }, {
      "heading" : "3.2.1 HYPERRNN",
      "text" : "The standard formulation of a Basic RNN is given by:\nht = φ(Whht−1 +Wxxt + b) (4)\nwhere ht is the hidden state, φ is a non-linear operation such as tanh or relu, and the weight matrices and bias Wh ∈ RNh×Nh ,Wx ∈ RNh×Nx , b ∈ RNh is fixed each timestep for an input sequence X = (x1, x2, . . . , xT ).\nIn HyperRNN, we allow Wh and Wx to float over time by using a smaller hypernetwork to generate these parameters of the main RNN at each step (see Figure 3). More concretely, the parameters Wh,Wx, b of the main RNN are different at different time steps, so that ht can now be computed as:\nht = φ ( Wh(zh)ht−1 +Wx(zx) + b(zb) ) , where\nWh(zh) = 〈Whz, zh〉 Wx(zx) = 〈Wxz, zx〉 b(zb) =Wbzzb + b0\n(5)\nWhere Whz ∈ RNh×Nh×Nz ,Wxz ∈ RNh×Nx×Nz ,Wbz ∈ RNh×Nz , b0 ∈ RNh and zh, zx, zz ∈ RNz . We use a recurrent hypernetwork to compute zh, zx and zb as a function of xt and ht−1:\nx̂t = ( ht−1 xt )\nĥt = φ(Wĥĥt−1 +Wx̂x̂t + b̂)\nzh =Wĥhĥt−1 + bĥh zx =Wĥxĥt−1 + bĥx zb =Wĥbĥt−1\n(6)\nWhere Wĥ ∈ RNĥ×Nĥ ,Wx̂ ∈ RNĥ×(Nh+Nz), b ∈ RNĥ , and Wĥh,Wĥx,Wĥb ∈ RNz×Nĥ and bĥh, bĥx ∈ RNz . This HyperRNN Cell has Nĥ hidden units. Typically Nĥ is much smaller than Nh. As the embeddings zh, zx and zb are of dimensions Nz , which is typically smaller than the hidden state size Nĥ of the HyperRNN cell, a linear network is used to project the output of the HyperRNN cell into the embeddings in Equation 6. After the embeddings are computed, they will be used to generate the full weight matrix of the main RNN.\nThe above is a general formulation of a linear dynamic hypernetwork applied to RNNs. However, we found that in practice, Equation 5 is often not practical because the memory usage becomes too large for real problems. The amount of memory required in the system described in Equation 5 will be Nz times the memory of a Basic RNN, which limits the number of hidden units we can use and in many applications.\nWe can modify the dynamic hypernetwork system described in Equation 5 so that it can be much more scalable and memory efficient. Our approach borrows from the static hypernetwork section and we will use an intermediate hidden vector d(z) ∈ RNh to parametrize, where d(z) will be a linear projection of z. To dynamically modify a weight matrix W , we will allow each row of this\nweight matrix to be scaled linearly by an element in vector d. We refer d as a weight scaling vector. Below is the modification to W (z):\nW (z) =W ( d(z) ) =   d0(z)W0 d1(z)W1\n... dNh(z)WNh\n  (7)\nWhile we sacrifice the ability to express construct an entire weight matrix from a linear combination of Nz matrices of the same size, we are able to linearly scale the rows of a single matrix with Nz degrees of freedom. We find this to be a good trade off, as this formulation of converting W (z) into W (d(z)) decreases the amount of memory required by the dynamic hypernetwork. Rather than requiring Nz times the memory of a Basic RNN, we will only be using memory in the order Nz times the number of hidden units, which is an acceptable amount of extra memory usage that is often available in many applications. In addition, the row-level operation in Equation 7 can be shown to be equivalent to an element-wise multiplication operator and hence computationally much more efficient in practice. Below is the more memory efficient version of the setup of Equation 5:\nht = φ ( dh(zh) Whht−1 + dx(zx) Wxxt + b(zb) ) , where\ndh(zh) =Whzzh dx(zx) =Wxzzx\nb(zb) =Wbzzb + b0\n(8)\nThis formulation of the HyperRNN has some similarities to Recurrent Batch Normalization (Cooijmans et al., 2016) and Layer Normalization (Ba et al., 2016). The central idea for the normalization techniques is to calculate the first two statistical moments of the inputs to the activation function, and to linearly scale the inputs to have zero mean and unit variance. An additional set of fixed parameters are learned to unscale the activations if required.\nSince the HyperRNN cell can indirectly modify the rows of each weight matrix and also the bias of the main RNN, it is implicitly also performing a linear scaling to the inputs of the activation function. The difference here is that the linear scaling parameters can be different for each timestep and also for for each input sample. It will be interesting to compare the scaling policy that the HyperRNN cell comes up with, to the hand engineered statistical-moments based scaling approaches. In addition, we note that the existing normalization approaches can work together with the HyperRNN approach, where the HyperRNN cell will be tasked with discovering a better dynamical scaling policy to complement normalization. We will also explore this combination in our experiments."
    }, {
      "heading" : "3.2.2 HYPERLSTM",
      "text" : "In this section we will discuss extension of HyperRNN to the Long Short-Term Memory (LSTM) architecture, which is usually better than the Basic RNN at storing and retrieving information over longer time steps. Our focus will be on the basic version of the LSTM architecture Hochreiter & Schmidhuber (1997), given by:\nit =W i hht−1 +W i xxt + b i gt =W g hht−1 +W g xxt + b g\nft =W f h ht−1 +W f x xt + b f ot =W o hht−1 +W o xxt + b o\nct = σ(ft) ct−1 + σ(it) φ(gt) ht = σ(ot) φ(ct)\n(9)\nwhere W yh ∈ RNh×Nh ,W yx ∈ RNh×Nx , by ∈ RNh , σ is the sigmoid operator, φ is the tanh operator. For brevity, y is one of {i, g, f, o}.1\n1In practice, all eight weight matrices are concatenated into one large matrix for computational efficiency.\nSimilar to the previous section, we will make the weights and biases a function of an embedding, and the embedding for each {i, g, f, o} will be generated from a smaller HyperLSTM cell. As discussed earlier, we will also experiment with adding the option to use a Layer Normalization layer in the HyperLSTM. The HyperLSTM Cell is given by:\nx̂t = ( ht−1 xt )\nît = LN(W î ĥ ĥt−1 +W î x̂x̂t + b̂ î) ĝt = LN(W ĝ\nĥ ĥt−1 +W ĝ x̂ x̂t + b̂ ĝ)\nf̂t = LN(W f̂\nĥ ĥt−1 +W f̂ x̂ x̂t + b̂ f̂ )\nôt = LN(W ô ĥ ĥt−1 +W ô x̂ x̂t + b̂ ô)\nĉt = σ(f̂t) ĉt−1 + σ(̂it) φ(ĝt) ĥt = σ(ôt) φ(LN(ĉt))\n(10)\nThe weight matrices for each of the four {i, g, f, o} gates will be a function of a set of embeddings zx, zh, and zb unique to each gates, just like the HyperRNN. These embeddings are linear projections of the hidden states of the HyperLSTM Cell. For brevity, y is one of {i, g, f, o} to avoid writing four sets of identical equations:\nzyh =W y ĥh ĥt−1 + b y ĥh zyx =W y\nĥx ĥt−1 + b\ny\nĥx\nzyb =W y ĥb ĥt−1\n(11)\nAs in the memory efficient version of the HyperRNN, we will focus on the efficient version of the HyperLSTM, where we use weight scaling vectors d to modify the rows of the weight matrices:\nyt = LN ( dyh W y hht−1 + d y x W yxxt + by(zyb ) ) , where\ndyh(zh) =W y hzzh dyx(zx) =W y xzzx by(zyb ) =W y bzz y b + b y 0\n(12)\nIn our implementation, the cell and hidden state update equations for the main LSTM will incorporate a single dropout (Hinton et al., 2012) gate, as developed in Recurrent Dropout without Memory Loss (Semeniuta et al., 2016), as we found this to help regularize the entire model during training:\nct = σ(ft) ct−1 + σ(it) DropOut(φ(gt)) ht = σ(ot) φ(LN(ct))\n(13)\nThis dropout operation is only inside the main LSTM, not in the smaller HyperLSTM cell. In our experiments, we will focus on this LSTM version of the HyperRNN. One of the questions we want to answer is whether the HyperLSTM cell can learn an policy of adjusting the weights to match the statistical moments-based normalization methods, hence Layer Normalization will be one of our baseline methods. Therefore, we will conduct experiments on two versions of the HyperLSTM cell, one with and one without the application of Layer Normalization."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In the following experiments, we will benchmark the performance of static hypernetworks on image recognition with MNIST and CIFAR-10, and the performance of dynamic hypernetworks on language modeling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting generation."
    }, {
      "heading" : "4.1 USING STATIC HYPERNETWORKS TO GENERATE FILTERS FOR CONVOLUTIONAL NETWORKS AND MNIST",
      "text" : "We start by applying a hypernetwork to generate the filters for a convolutional network on MNIST. Our main convolutional network is a small 2-layered network and the hypernetwork is used to generate the kernel for the second layer (7x7x16x16), which contains the bulk of the trainable parameters in the system. Our weight matrix will be summarized by an embedding of size Nz = 4. Table 1 shows a comparison of accuracies and parameter counts for a normal convolutional network and a hypernetwork. See Appendix A.3.1 for further experimental setup details."
    }, {
      "heading" : "Model Test Error Params of 2nd Kernel",
      "text" : "We see that we can achieve similar accuracy in this toy MNIST classification task, while being able to represent the entire weight matrix with four numbers, and training a generative network to product a set of weights. In this example, there are 4240 parameters in the generative hypernetwork. We can see the weight matrix this network produced by the hypernetwork in Figure 2. Now the question is whether we can also train a deep convolutional network, using a single hypernetwork generating a set of weights for each layer, on a dataset more challenging than MNIST."
    }, {
      "heading" : "4.2 STATIC HYPERNETWORKS FOR RESIDUAL NETWORK ARCHITECTURE AND CIFAR-10",
      "text" : "The residual network architectures (He et al., 2016a; Zagoruyko & Komodakis, 2016) are popular for image recognition tasks, as they can accommodate very deep networks while maintaining effective gradient flow across layers using skip connections. The original resnet and subsequent derivatives (Zhang et al., 2016; Huang et al., 2016a) achieved state-of-the-art image recognition performance on a variety of public datasets. While residual networks can be be very deep, and in some experiments as deep as 1001 layers ((He et al., 2016b), it is important to understand whether some these layers share common properties and can be reduced effectively by introducing weight sharing. If we enforce weight-sharing across many layers of a deep feed forward network, the network may share many properties to that of a recurrent network. In this experiment, we want to explore this idea of enforcing relaxed weight sharing across all of the layers of a deep residual network. We will take a simple version of residual network, use a single hypernetwork to generate the weights of all of its layers for image classification task on the CIFAR-10 dataset.\nOur experiment will use a version of the wide residual network (Zagoruyko & Komodakis, 2016), described in Table 2, a popular and simple variant of the family of residual network architectures, and we will focus configurations (N = 6,K = 1) and (N = 6,K = 2), referred to as WRN 40-1 and WRN 40-2 respectively. In this setup, we will use a hypernetwork to generate all of the kernels in conv2, conv3, and conv4, so we will generate 36 layers of kernels in total. The WRN architecture uses a filter size of 3 for every kernel. We use the method outlined in the Methods section to deal\nwith kernels of varying sizes, and use the an embedding size of Nz = 64 in our experiments. See Appendix A.3.2 for further experimental setup details."
    }, {
      "heading" : "Model Test Error Param Count",
      "text" : "We obtained similar classification accuracy numbers as reported in (Zagoruyko & Komodakis, 2016) with our own implementation. We also note that the weights generated by the hypernetwork are used in a batch normalization setting without modification to the original model. In principle, hypernetworks can also be applied to the newer variants of residual networks with more skip connections, such as DenseNets and ResNets of Resnets.\nFrom the results, we see that enforcing a relaxed weight sharing constraint to the deep residual network cost us ∼ 1.5% classification accuracy, while drastically reducing the number of parameters in the model as a trade off. One reason for this reduction in accuracy for the residual network case, but not for the MNIST case, is because different layers of a deep network is trained to extract different levels of features, and require different kinds of filters to perform optimally. The hypernetwork enforces some commonality between every layer, but offers each layer 64 degrees of freedom to distinguish itself from the other layers. While the network is no longer able to learn the optimal set of filters for each layer, and must learn the best set of filters constrained by a hypernetwork, the total number of free model parameters drastically gets reduced as a result.\nOur observation here is that a single hypernetwork embedded within the main residual network, can be trained at the same time, to generate the entire set of kernels that the main network requires to perform a classification task. Each kernel will be generated by common weights within the hypernetwork and as a result we end up using much fewer trainable parameters at the expense of classification accuracy. This observation led us to explore the possibility of applying the reverse procedure to networks that were designed with hard weight-sharing constraints in the first place, such as recurrent networks. If we can allow the weights of a recurrent network to change at each time step, and also be customized for each specific input sequence with the help of a hypernetwork, and go from hard weight-sharing to relaxed-weight sharing, we may be able to obtain better performance compared to normal recurrent networks, at the expense of an increase in training parameters."
    }, {
      "heading" : "4.3 HYPERLSTM FOR CHARACTER-LEVEL PENN TREEBANK LANGUAGE MODELING",
      "text" : "The HyperLSTM model is evaluated on character level prediction task on the Penn Treebank corpus (Marcus et al., 1993) using the train/validation/test split outlined in (Mikolov et al., 2012). As the dataset is quite small is prone to over fitting, we apply dropout on both input and output layers with a keep probability of 0.90. Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al., 2016) with the same dropout probability.\nWe compare our model to the basic LSTM cell (Hochreiter & Schmidhuber, 1997), stacked LSTM cells (Graves, 2013), and LSTM with layer normalization applied. In addition, we also experimented with applying layer normalization to HyperLSTM. Using the setup in (Graves, 2013), we use networks with 1000 units and train the network to predict the next character. In this task, the HyperLSTM cell has 128 units and a signal size of 4. As the HyperLSTM cell has more trainable parameters compared to the basic LSTM Cell, we also experimented with an LSTM Cell with 1250 units as well. For more details regarding experimental setup, please refer to Appendix A.3.4\nIt is interesting to note that combining Recurrent Dropout with a basic LSTM cell achieves quite formidable performance. Our implementation of Recurrent Dropout Basic LSTM cell reproduced similar results as (Semeniuta et al., 2016), where they have also experimented with different dropout settings. We also found that Layer Norm LSTM performed quite well when combined with recurrent dropout, making it both a formidable baseline and also an extension for HyperLSTM.\nIn addition to outperforming both the larger or deeper version of the LSTM network, HyperLSTM also achieved similar performance of Layer Norm LSTM. This suggests by dynamically adjusting the weight scaling vectors, the HyperLSTM cell has learned a policy of scaling inputs to the activation functions that is as efficient as the statistical moments-based strategy employed by Layer Norm, and that the required extra computation required is embedded inside the extra 128 units inside the HyperLSTM cell. When we combine HyperLSTM with Layer Norm, we see an additional performance gain, achieving to our knowledge state-of-the-art results for this task, implying that the HyperLSTM cell may have learned an adjustment policy that goes beyond moments-based regularization.\nThe Character Penn Treebank dataset is of specific interest to us compared to larger datasets. For larger datasets, one can generally build a larger model to get a gain in performance. This is not the case for a small dataset. We see that increasing both the size and the depth of the LSTM network does not achieve the gains provided by the HyperLSTM. In many applications, we may be constrained by the size of training data we have, so we value the ability for a model to generalize well."
    }, {
      "heading" : "4.4 HYPERLSTM FOR HUTTER PRIZE WIKIPEDIA LANGUAGE MODELING",
      "text" : "We train our model on the larger and more challenging Hutter Prize Wikipedia dataset, also known as enwik8 (Hutter, 2012) consisting of a sequence of 100M characters composed of 205 unique characters. Unlike Penn Treebank, enwik8 contains some foreign words (Latin, Arabic, Chinese), indented XML, metadata, and internet addresses, making it a more realistic and practical dataset to test character language models. Examples of these mixed variety of text samples that the model can generate is in Appendix A.4. For more details regarding experimental setup, please refer to Appendix A.3.5\n0Based on results of version 2 at the time of writing. http://arxiv.org/abs/1609.01704v2 1We do not compare against methods that use dynamic evaluation. 2Our implementation.\nWe see that HyperLSTM is once again competitive to Layer Norm LSTM, and if we combine both techniques, the Layer Norm HyperLSTM achieves near state-of-the-art performance for this task. In addition, HyperLSTM converges quicker per training step compared to LSTM and Layer Norm LSTM. Please refer to Figure 6 for the loss graphs.\n In 1955-37 most American and Europeans signed into the sea. An absence of [[Japan (Korea city)|Japan]], the Mayotte like Constantino\n ple (in its first week, in [[880]]) that served as the mother of emperors, as the Corinthians, Bernard on his continued sequel toget\n her ordered [[Operation Moabili]]. The Gallup churches in the army promulgated the possessions sitting at the reservation, and [[Mel\n ito de la Vegeta Provine|Felix]] had broken Diocletian desperate from the full victory of Augustus, cited by Stephen I. Alexander Se\n nate became Princess Cartara, an annual ruler of war (777-184) and founded numerous extremiti of justice practitioners.\nFigure 4: Example text generated from HyperLSTM model. We visualize how four of the main RNN’s weight matrices (W ih, W g h , W f h , W o h ) effectively change over time by plotting the norm of the changes below each generated character. High intensity represent large changes being made to weights of main RNN.\nWhen we use this prediction model as a generative model to sample a text passage, we use main RNN to model a probability distribution over possible characters conditioned over the preceding characters. In the case of the HyperRNN, we allow the model parameters of this generative model to vary over time, so in effect the HyperRNN cell is choosing the best model at any given time to generate a probability distribution to sample from. We can demonstrate this by visualizing how the weight scaling vectors of the main RNN change during the character sampling process. In Figure 4, we examine a sample text passage generated by HyperLSTM after training on enwik8 along with the weight differences below the text. We see that in regions of low intensity, which means the weights of the main RNN are relatively static, the types of phrases generated seem more deterministic. For example, the weights do not change much during the words Europeans, possessions and reservation. The regions of high intensity is when the HyperRNN cell is making relatively large changes to the weights of the main RNN. These tend to happen in the areas between words, or sometimes during brackets.\nOne might also wonder whether the HyperLSTM cell (without Layer Norm), via dynamically tuning the weight scaling vectors, has developed a policy that is similar to the statistics-based approach used by Layer Norm, given that both methods have similar performance. One way to see this effect is to look at the histogram of the hidden states in the network. In Figure 5, we examine the histograms of φ(ct), the hidden state of the LSTM before applying the output gate.\nWe see that the normalization process employed by Layer Norm reduces the saturation effects compared to the vanilla LSTM. However, for the case of the HyperLSTM, we notice that most of the time the cell is saturated. The HyperLSTM cell’s dynamic weight adjustment policy appears to be doing something very different compared to statistical normalization, although the policy it came up with ended up providing similar performance as Layer Norm. It is interesting to see that when we combine both methods, the HyperLSTM cell will need to determine an adjustment policy in spite of the normalization forced upon it by Layer Norm. An interesting question is whether there are problems where statistical normalization may actually be a setback to the policy developed by the HyperLSTM, and the best strategy is to ignore it."
    }, {
      "heading" : "4.5 HYPERLSTM FOR HANDWRITING SEQUENCE GENERATION",
      "text" : "In addition to modelling discrete sequential data, we want to see how the model performs when modelling sequences of real valued data. We will train our model on the IAM online handwriting database (Liwicki & Bunke, 2005) and have our model predict pen strokes as per Section 4.2 of (Graves, 2013). The dataset has contains 12179 handwritten lines from 221 writers, digitally recorded from a tablet. We will model the (x, y) coordinate of the pen location at each recorded time step, along with a binary indicator of pen-up/pen-down. The average sequence length is around 700 steps and the longest around 1900 steps, making the training task particularly challenging as the network needs to retain information about both the stroke history and also the handwriting style in order to predict plausible future handwriting strokes. For experimental setup details, please refer to Appendix A.3.6.\nIn this task, we note that data augmentation and applying recurrent dropout improved the performance of all models, compared to the original setup by (Graves, 2013). In addition, for the LSTM model, increasing unit count per layer may not help the performance compared to increasing the layer depth. We notice that a 3-layer 400 unit LSTM outperforms a 1-layer 900 unit one, and we\n1Our implementation, to replicate setup of (Graves, 2013). 2Our implementation, with data augmentation, dropout and recurrent dropout.\nfound that a 2-layer 650 unit LSTM outperforming most configurations. While layer norm helps with the performance, we found that in this task, layer norm does not combine well with HyperLSTM, and in this task the 900 unit HyperLSTM without layer norm achieved the best performance.\nUnlike the language modelling task, perhaps statistical normalization is far from the optimal approach for a weight adjustment policy. The policy learned by the HyperLSTM cell not only performed well against the baseline, its convergence rate is also as fast as the 2-layer LSTM model. Please refer to Figure 6 for the loss graphs.\nIn Appendix A.4, we display three sets of handwriting samples generated from LSTM, Layer Norm LSTM, and HyperLSTM, corresponding to log-loss scores of -1055, -1096, and -1162 nats respectively in Table 6. Qualitative assessments of handwriting quality is always subjective, and depends an individual’s taste in calligraphy. From looking at the examples produced by the three models, our opinion is that the samples produced by LSTM is noisier than the other two models. We also find HyperLSTM’s samples to be a bit more coherent than the samples produced by Layer Norm LSTM. We leave to the reader to judge which model produces handwriting samples of higher quality.\nSimilar to the earlier character generation experiment, we show a generated handwriting sample from the HyperLSTM model in Figure 7, along with a plot of how the weight scaling vectors of the main RNN is changing over time below the sample. We see that the regions of high intensity seem to be concentrated at many discrete instances, rather than slowly varying over time. Which means during generation, the HyperRNN cell usually leaves the main RNN model alone, and when it acts it will make large changes to the generative model. We can see that many of these weight changes occur between the written words, and sometimes between written characters. While LSTM model alone already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the parameters of this parameter-generating RNN model during the generative process is one of the key advantages of HyperRNN over a normal RNN."
    }, {
      "heading" : "4.6 CONCLUSION",
      "text" : "In this paper, we presented a method to use a hypernetwork to generate weights for another neural network. Our hypernetworks are trained end-to-end with backpropagation and therefore are efficient and scalable. We focused on two use cases of hypernetworks: static hypernetworks to generate weights for a convolutional network, dynamic hypernetworks to generate weights for recurrent networks. We found that the method works well while using fewer parameters. On image recognition, language modeling and handwriting generation, hypernetworks are competitive to or sometimes better than state-of-the-art models."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We thank Jeff Dean, Geoffrey Hinton, Mike Schuster and the Google Brain team for their help with the project."
    }, {
      "heading" : "A APPENDIX",
      "text" : ""
    }, {
      "heading" : "A.1 HYPERNETWORKS TO LEARN FILTERS FOR A FULLY CONNECTED NETWORKS",
      "text" : "We ran an experiment where the hypernetwork receives the x, y locations of both the input pixel and the weight, and predicts the value of the hidden weight matrix in a fully connected network that learns to classify MNIST digits. In this experiment, the fully connected network (784-256-10) has one hidden layer of 16 × 16 units, where the hypernetwork is a pre-defined small feedforward network. The weights of the hidden layer has 784×256 = 200704 parameters, while the hypernetwork is a 801 parameter four layer feed forward relu network that would generate the 786 × 256 weight matrix. The result of this experiment is shown in Figure 8. We want to emphasize that even though the network can learn convolutional-like filters during end-to-end training, its performance is rather poor: the best accuracy is 93.5%, compared to 98.5% for the conventional fully connected network.\nWe find that the virtual coordinates-based approach to hypernetworks that is used by HyperNEAT and DPPN has its limitations in many practical tasks, such as image recognition and language modelling, and therefore developed our embedding vector approach in this work."
    }, {
      "heading" : "A.2 KERNEL GENERATION EXAMPLES WITH STATIC HYPERNETNETWORK",
      "text" : ""
    }, {
      "heading" : "A.2.1 FILTER VISUALIZATIONS FOR RESIDUAL NETWORKS",
      "text" : "In Figures 9 and 10 are example visualizations for various kernels in a deep residual network. Note that the 32x32x3x3 kernel generated by the hypernetwork was constructed by concatenating 4 basic kernels together."
    }, {
      "heading" : "A.3 EXPERIMENT SETUP DETAILS AND HYPER PARAMETERS",
      "text" : ""
    }, {
      "heading" : "A.3.1 USING STATIC HYPERNETWORKS TO GENERATE FILTERS FOR CONVOLUTIONAL NETWORKS AND MNIST",
      "text" : "We train the network with a 55000 / 5000 / 10000 split for the training, validation and test sets and use the 5000 validation samples for early stopping, and train the network using Adam (Kingma & Ba, 2015) with a learning rate of 0.001 on mini-batches of size 1000. To decrease over fitting, we pad MNIST training images to 30x30 pixels and random crop to 28x28."
    }, {
      "heading" : "A.3.2 STATIC HYPERNETWORKS FOR RESIDUAL NETWORK ARCHITECTURE AND",
      "text" : "CIFAR-10\nWe train both the normal residual network and the hypernetwork version using a 45000 / 5000 / 10000 split for training, validation, and test set. The 5000 validation samples are randomly chosen and isolated from the original 50000 training samples. We train the entire setup with a mini-batch size of 128 using Nesterov Momentum SGD for the normal version and Adam for the hypernetwork version, both with a learning rate schedule. We apply L2 regularization on the kernel weights, and also on the hypernetwork-generated kernel weights of 0.0005%. To decrease over fitting, we apply light data augmentation pad training images to 36x36 pixels and random crop to 32x32, and perform random horizontal flips.\nA.3.3 IMPLEMENTATION DETAILS AND WEIGHT INITIALIZATION FOR HYPERLSTM\nThis section may be useful to readers who may want to implement their own version of the HyperLSTM Cell, as we will discuss initialization of the parameters for Equations 10 to 13. We recommend implementing the HyperLSTM within the same interface as a normal recurrent network cell so that using the HyperLSTM will not be any different than using a normal RNN. These initialization parameters have been found to work well with our experiments, but they may be far from optimal depending on the task at hand.\nThe HyperLSTM Cell will be located inside the HyperLSTM, as described in Equation 10. It is a normal LSTM cell with Layer Normalization. The inputs to the HyperLSTM Cell will be the concatenation of the input signal and the hidden units of the main LSTM cell. The biases in Equation 10 are initialized to zero and Orthogonal Initialization (Henaff et al., 2016) is performed for all weights.\nThe embedding vectors are produced by the HyperLSTM Cell at each timestep by linear projection described in Equation 11. The weights for the first two equations are initialized to be zero, and the biases are initialized to one. The weights for the third equation are initialized to be a small normal random variable with standard deviation of 0.01.\nThe weight scaling vectors that modify the weight matrices are generated from these embedding vectors, as per Equation 12. Orthogonal initialization is applied to the Wh and Wx, while b0 is initialized to zero. Wbz is also initialized to zero. For the weight scaling vectors, we used a method described in Recurrent Batch Normalization (Cooijmans et al., 2016) where the scaling vectors are initialized to 0.1 rather than 1.0 and this has shown to help gradient flow. Therefore, for weight matrices Whz and Wxz , we initialize to a constant value of 0.1/Nz to maintain this property.\nThe only place we use dropout is in the single location in Equation 13, developed in Recurrent Dropout without Memory Loss (Semeniuta et al., 2016). We can use this dropout gate like any other normal dropout gate in a feed-forward network."
    }, {
      "heading" : "A.3.4 CHARACTER-LEVEL PENN TREEBANK",
      "text" : "For Character-level Penn Treebank, we use mini-batches of size 128, to train on sequences of length 100. We trained the model using Adam (Kingma & Ba, 2015) with a learning rate of 0.001 and gradient clipping of 1.0. During evaluation, we generate the entire sequence, and do not use information about previous test errors for prediction, e.g., dynamic evaluation (Graves, 2013; Rocki, 2016b). For baseline models, Orthogonal Initialization (Henaff et al., 2016) is performed for all weights."
    }, {
      "heading" : "A.3.5 HUTTER PRIZE WIKIPEDIA",
      "text" : "As enwik8 is a bigger dataset compared to Penn Treebank, we will use 1800 units for our networks, unless specified otherwise in the table. In addition, we perform training on sequences of length 250. Our HyperLSTM Cell will consist of 256 units, and we will use a signal size of 32.\nOur setup is similar in the previous experiment, using the same mini-batch size, learning rate, weight initialization, gradient clipping parameters and optimizer. We do not use dropout for the input and output layers, but still apply recurrent dropout with the same dropout probability for all models unless otherwise specified in the table. For baseline models, Orthogonal Initialization (Henaff et al., 2016) is performed for all weights.\nAs in (Chung et al., 2015), we train on the first 90M characters of the dataset, use the next 5M as a validation set for early stopping, and the last 5M characters as the test set."
    }, {
      "heading" : "A.3.6 HANDWRITING SEQUENCE GENERATION",
      "text" : "We will use the same model architecture described in (Graves, 2013) and use a Mixture Density Network layer (Bishop, 1994) to generate a mixture of bi-variate Gaussian distributions to model at each time step to model the pen location. We normalize the data and use the same train/validation split as per (Graves, 2013) in this experiment. We remove samples less than length 300 as we found these samples contain a lot of recording errors and noise. After the pre-processing, as the dataset is small, we introduce data augmentation of chosen uniformly from +/- 10% and apply a this random scaling a the samples used for training.\nOne concern we want to address is the lack of a test set in the data split methodology devised in (Graves, 2013). In this task, qualitative assessment of generated handwriting samples is arguably just as important as the quantitative log likelihood score of the results. Due to the small size of the dataset, we want to use as large as possible the portion of the dataset to train our models in order to generate better quality handwriting samples so we can also judge our models qualitatively in addition to just examining the log-loss numbers, so for this task we will use the same training / validation split as (Graves, 2013), with a caveat that we may be somewhat over fitting to the validation set in the quantitative results. In future works, we will explore using larger datasets to conduct a more rigorous quantitative analysis.\nFor model training, will apply recurrent dropout and also dropout to the output layer with a keep probability of 0.95. The model is trained on mini-batches of size 32 containing sequences of variable length. We trained the model using Adam (Kingma & Ba, 2015) with a learning rate of 0.0001 and gradient clipping of 5.0. Our HyperLSTM Cell consists of 128 units and a signal size of 4. For baseline models, Orthogonal Initialization (Henaff et al., 2016) is performed for all weights.\nA.4 EXAMPLES OF GENERATED SAMPLES FROM EXPERIMENTS. (NOT CHERRY PICKED)"
    } ],
    "references" : [ {
      "title" : "Mixture density networks",
      "author" : [ "Christopher M. Bishop" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Bishop.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1994
    }, {
      "title" : "Gated feedback recurrent neural networks",
      "author" : [ "Junyoung Chung", "Caglar Gülçehre", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.02367,",
      "citeRegEx" : "Chung et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical multiscale recurrent neural networks",
      "author" : [ "Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1609.01704,",
      "citeRegEx" : "Chung et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast and accurate deep network learning by exponential linear units (ELUs)",
      "author" : [ "Djork-Arné Clevert", "Thomas Unterthiner", "Sepp Hochreiter" ],
      "venue" : "arXiv preprint arXiv:1511.07289,",
      "citeRegEx" : "Clevert et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Clevert et al\\.",
      "year" : 2015
    }, {
      "title" : "Convolution by evolution: Differentiable pattern producing networks",
      "author" : [ "Chrisantha Fernando", "Dylan Banarse", "Malcolm Reynolds", "Frederic Besse", "David Pfau", "Max Jaderberg", "Marc Lanctot", "Daan Wierstra" ],
      "venue" : "arXiv preprint arXiv:1606.02580,",
      "citeRegEx" : "Fernando et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fernando et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : null,
      "citeRegEx" : "Graves.,? \\Q2013\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1603.05027,",
      "citeRegEx" : "He et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Orthogonal RNNs and long-memory tasks",
      "author" : [ "Mikael Henaff", "Arthur Szlam", "Yann LeCun" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Henaff et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Henaff et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Juergen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Kilian Q. Weinberger" ],
      "venue" : "arXiv preprint arXiv:1608.06993,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger" ],
      "venue" : "arXiv preprint arXiv:1603.09382,",
      "citeRegEx" : "Huang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "The human knowledge compression contest",
      "author" : [ "Marcus Hutter" ],
      "venue" : "URL http://prize. hutter1.net/",
      "citeRegEx" : "Hutter.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2012
    }, {
      "title" : "Grid long short-term memory",
      "author" : [ "Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Evolving neural networks in compressed weight space",
      "author" : [ "Jan Koutnik", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "In GECCO,",
      "citeRegEx" : "Koutnik et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Koutnik et al\\.",
      "year" : 2010
    }, {
      "title" : "Zoneout: Regularizing RNNs by randomly preserving hidden activations",
      "author" : [ "David Krueger", "Tegan Maharaj", "János Kramár", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1606.01305,",
      "citeRegEx" : "Krueger et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krueger et al\\.",
      "year" : 2016
    }, {
      "title" : "Handwritten digit recognition with a back-propagation network",
      "author" : [ "Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1990
    }, {
      "title" : "IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard",
      "author" : [ "Marcus Liwicki", "Horst Bunke" ],
      "venue" : "In ICDAR,",
      "citeRegEx" : "Liwicki and Bunke.,? \\Q2005\\E",
      "shortCiteRegEx" : "Liwicki and Bunke.",
      "year" : 2005
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "Marcus et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Subword language modeling with neural networks",
      "author" : [ "Tomáš Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "Jan Cernocky" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2012
    }, {
      "title" : "Regularizing recurrent networks-on injected noise and normbased methods",
      "author" : [ "Saahil Ognawala", "Justin Bayer" ],
      "venue" : "arXiv preprint arXiv:1410.5684,",
      "citeRegEx" : "Ognawala and Bayer.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ognawala and Bayer.",
      "year" : 2014
    }, {
      "title" : "Recurrent memory array structures",
      "author" : [ "Kamil Rocki" ],
      "venue" : "arXiv preprint arXiv:1607.03085,",
      "citeRegEx" : "Rocki.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rocki.",
      "year" : 2016
    }, {
      "title" : "Surprisal-driven feedback in recurrent networks",
      "author" : [ "Kamil Rocki" ],
      "venue" : "arXiv preprint arXiv:1608.06027,",
      "citeRegEx" : "Rocki.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rocki.",
      "year" : 2016
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1412.6550,",
      "citeRegEx" : "Romero et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent dropout without memory loss",
      "author" : [ "Stanislaw Semeniuta", "Aliases Severyn", "Erhardt Barth" ],
      "venue" : null,
      "citeRegEx" : "Semeniuta et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Semeniuta et al\\.",
      "year" : 2016
    }, {
      "title" : "Training very deep networks",
      "author" : [ "Rupesh Srivastava", "Klaus Greff", "Jürgen Schmidhuber" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "A hypercube-based encoding for evolving large-scale neural networks",
      "author" : [ "Kenneth O. Stanley", "David B. D’Ambrosio", "Jason Gauci" ],
      "venue" : "Artificial Life,",
      "citeRegEx" : "Stanley et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Stanley et al\\.",
      "year" : 2009
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Ilya Sutskever", "James Martens", "Geoffrey E. Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "On multiplicative integration with recurrent neural networks",
      "author" : [ "Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast algorithms for hierarchically semiseparable matrices",
      "author" : [ "Jianlin Xia", "Shivkumar Chandrasekaran", "Ming Gu", "Xiaoye S. Li" ],
      "venue" : "Numerical Linear Algebra with Applications,",
      "citeRegEx" : "Xia et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2010
    }, {
      "title" : "Wide residual networks",
      "author" : [ "Sergey Zagoruyko", "Nikos Komodakis" ],
      "venue" : "In BMVC,",
      "citeRegEx" : "Zagoruyko and Komodakis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zagoruyko and Komodakis.",
      "year" : 2016
    }, {
      "title" : "Residual networks of residual networks: Multilevel residual networks",
      "author" : [ "Ke Zhang", "Miao Sun", "Tony X. Han", "Xingfang Yuan", "Liru Guo", "Tao Liu" ],
      "venue" : "arXiv preprint arXiv:1608.02908,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent highway networks",
      "author" : [ "Julian Zilly", "Rupesh Srivastava", "Jan Koutnı́k", "Jürgen Schmidhuber" ],
      "venue" : "arXiv preprint arXiv:1607.03474,",
      "citeRegEx" : "Zilly et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zilly et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "HyperNEAT (Stanley et al., 2009) is an example of a hypernetwork where the inputs are a set of virtual coordinates for each weight in the main network.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : "On image classification with CIFAR-10, hypernetworks, when being used to generate weights for a deep convnet (LeCun et al., 1990), obtain respectable results compared to state-of-the-art models while having fewer learnable parameters.",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 28,
      "context" : "An instance of this is the work on Hypercubebased NEAT or HyperNEAT framework (Stanley et al., 2009).",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "Closely related to our approach is a simplified variation of HyperNEAT, where the structure is fixed and the weights are evolved through Discrete Cosine Transform is called Compressed Weight Search (Koutnik et al., 2010).",
      "startOffset" : 198,
      "endOffset" : 220
    }, {
      "referenceID" : 4,
      "context" : "Even more closely related to our approach is Differentiable Pattern Producing Networks (DPPNs), where the structure is evolved but the weights are learned (Fernando et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 178
    }, {
      "referenceID" : 4,
      "context" : "Using this setting, hypernetworks can approximately recover the convolutional architecture without explicitly being told to do so, a similar result obtained by ”Convolution by Evolution” (Fernando et al., 2016).",
      "startOffset" : 187,
      "endOffset" : 210
    }, {
      "referenceID" : 31,
      "context" : ") is similar to the hierarchically semiseparable matrix approach proposed by Xia et al. (2010). Note that even though it seems redundant to have a two-layered linear hypernetwork as that is equivalent to a one-layered hypernetwork, the fact that Wout and Bout are shared makes our two-layered hypernetwork more compact than a one-layered hypernetwork.",
      "startOffset" : 77,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "In our implementation, the cell and hidden state update equations for the main LSTM will incorporate a single dropout (Hinton et al., 2012) gate, as developed in Recurrent Dropout without Memory Loss (Semeniuta et al.",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 26,
      "context" : ", 2012) gate, as developed in Recurrent Dropout without Memory Loss (Semeniuta et al., 2016), as we found this to help regularize the entire model during training:",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 33,
      "context" : "The original resnet and subsequent derivatives (Zhang et al., 2016; Huang et al., 2016a) achieved state-of-the-art image recognition performance on a variety of public datasets.",
      "startOffset" : 47,
      "endOffset" : 88
    }, {
      "referenceID" : 25,
      "context" : "81% FitNet (Romero et al., 2014) 8.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 27,
      "context" : "22% Highway Networks (Srivastava et al., 2015) 7.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "72% ELU (Clevert et al., 2015) 6.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 33,
      "context" : "5 M ResNet of ResNet 58-4 (Zhang et al., 2016) 3.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : "The HyperLSTM model is evaluated on character level prediction task on the Penn Treebank corpus (Marcus et al., 1993) using the train/validation/test split outlined in (Mikolov et al.",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : ", 1993) using the train/validation/test split outlined in (Mikolov et al., 2012).",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al.",
      "startOffset" : 27,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al., 2016) with the same dropout probability.",
      "startOffset" : 161,
      "endOffset" : 182
    }, {
      "referenceID" : 5,
      "context" : "We compare our model to the basic LSTM cell (Hochreiter & Schmidhuber, 1997), stacked LSTM cells (Graves, 2013), and LSTM with layer normalization applied.",
      "startOffset" : 97,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "Using the setup in (Graves, 2013), we use networks with 1000 units and train the network to predict the next character.",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : "Model1 Test Validation Param Count ME n-gram (Mikolov et al., 2012) 1.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "32 Recurrent Dropout LSTM (Semeniuta et al., 2016) 1.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : "338 Zoneout RNN (Krueger et al., 2016) 1.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "30 HM-LSTM0 (Chung et al., 2016) 1.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "Our implementation of Recurrent Dropout Basic LSTM cell reproduced similar results as (Semeniuta et al., 2016), where they have also experimented with different dropout settings.",
      "startOffset" : 86,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : "We train our model on the larger and more challenging Hutter Prize Wikipedia dataset, also known as enwik8 (Hutter, 2012) consisting of a sequence of 100M characters composed of 205 unique characters.",
      "startOffset" : 107,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "Model1 enwik8 Param Count Stacked LSTM (Graves, 2013) 1.",
      "startOffset" : 39,
      "endOffset" : 53
    }, {
      "referenceID" : 29,
      "context" : "0 M MRNN (Sutskever et al., 2011) 1.",
      "startOffset" : 9,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "60 GF-RNN (Chung et al., 2015) 1.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : "0 M Grid-LSTM (Kalchbrenner et al., 2016) 1.",
      "startOffset" : 14,
      "endOffset" : 41
    }, {
      "referenceID" : 30,
      "context" : "8 M MI-LSTM (Wu et al., 2016) 1.",
      "startOffset" : 12,
      "endOffset" : 29
    }, {
      "referenceID" : 34,
      "context" : "44 Recurrent Highway Networks (Zilly et al., 2016) 1.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "40 HM-LSTM0 (Chung et al., 2016) 1.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "2 of (Graves, 2013).",
      "startOffset" : 5,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "In this task, we note that data augmentation and applying recurrent dropout improved the performance of all models, compared to the original setup by (Graves, 2013).",
      "startOffset" : 150,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "Our implementation, to replicate setup of (Graves, 2013).",
      "startOffset" : 42,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "Model Log-Loss Param Count LSTM, 900 units (Graves, 2013) -1,026 3-Layer LSTM, 400 units (Graves, 2013) -1,041 3-Layer LSTM, 400 units, adaptive weight noise (Graves, 2013) -1,058 LSTM, 900 units, no dropout, no data augmentation.",
      "startOffset" : 43,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "Model Log-Loss Param Count LSTM, 900 units (Graves, 2013) -1,026 3-Layer LSTM, 400 units (Graves, 2013) -1,041 3-Layer LSTM, 400 units, adaptive weight noise (Graves, 2013) -1,058 LSTM, 900 units, no dropout, no data augmentation.",
      "startOffset" : 89,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Model Log-Loss Param Count LSTM, 900 units (Graves, 2013) -1,026 3-Layer LSTM, 400 units (Graves, 2013) -1,041 3-Layer LSTM, 400 units, adaptive weight noise (Graves, 2013) -1,058 LSTM, 900 units, no dropout, no data augmentation.",
      "startOffset" : 158,
      "endOffset" : 172
    } ],
    "year" : 2016,
    "abstractText" : "This work explores hypernetworks: an approach of using a small network, also known as a hypernetwork, to generate the weights for a larger network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype – the hypernetwork – and a phenotype – the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-art results on a variety of language modeling tasks with Character-Level Penn Treebank and Hutter Prize Wikipedia datasets, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}