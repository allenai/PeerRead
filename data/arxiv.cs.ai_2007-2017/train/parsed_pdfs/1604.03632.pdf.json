{
  "name" : "1604.03632.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Strategyproof Peer Selection",
    "authors" : [ "Haris Aziza", "Omer Levb", "Nicholas Matteia", "Jeffrey S. Rosenscheinc", "Toby Walsha" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Strategyproof Peer Selection",
      "text" : "Haris Aziza, Omer Levb, Nicholas Matteia, Jeffrey S. Rosenscheinc, and Toby Walsha\naData61/NICTA and UNSW, Sydney, NSW 1466, Australia; bUniversity of Toronto, Toronto, Ontario M5S 3G4, Canada; cThe Hebrew University of Jerusalem, Jerusalem 91904, Israel\nPeer review, evaluation, and selection is the foundation on which modern science is built. Funding bodies the world over employ experts to study and select the best proposals of those submitted for funding. The problem of peer selection, however, is much more universal: a professional society may want give a subset of its members awards based on the opinions of all the members; an instructor for a MOOC or online course may want to crowdsource grading; or a marketing company may select ideas from group brainstorming sessions based on peer evaluation. We make three fundamental contributions to the study of procedures or mechanisms for peer selection, a specific type of group decision making problem studied in computer science, economics, political science, and beyond. First, we detail a novel mechanism that is strategyproof, i.e., agents cannot benefit themselves by reporting insincere valuations, in addition to other desirable normative properties. Second, we demonstrate the effectiveness of our mechanism through a comprehensive simulation based comparison of our mechanism with a suite of mechanisms found in the computer science and economics literature. Finally, our mechanism employs a randomized rounding technique that is of independent interest, as it can be used as a randomized method to addresses the ubiquitous apportionment problem that arises in various settings where discrete resources such as parliamentary representation slots need to be divided fairly.\npeer review | peer selection | social choice | allocation | apportionment\nS ince the beginning of civilization, societies have been se-lecting small groups from within. Athenian society, for example, selected a random subset of citizens to participate in the Boule, the council of citizens which ran daily affairs in Athens. Peer review, evaluation, and selection has been the foundational process by which scientific funding bodies have selected the best subset of proposals for funding [1] and peer evaluation is becoming increasingly popular and necessary to scale grading in MOOCs [2, 3]. In these settings, however, we do not wish to select an arbitrary subset of size k, but the “best k”, and we need, therefore, a procedure in which the candidates are rated according to the opinions of the group. In peer selection problems we are not seeking an external, “independent” agent to make choices, but desire a crowdsourced approach, in which participants are those making the selection. Mechanisms for peer selection and their properties receive considerable attention within economics, political science, and computer science [1, 4–9].\nOur motivation comes from the US National Science Foundation (NSF) recent “mechanism design pilot,” which attempted to spread the review load amongst all the submitters of proposals [1, 10]. The program uses “reviewers assigned from among the set of PIs whose proposals are being reviewed.” Reviewers’ own proposals get “supplemented with “bonus points” depending upon the degree to which his or her ranking agrees with the consensus ranking ([11], Page 46).” This mechanism used by the NSF is not strategyproof; reviewers are incentivized to guess what others are thinking, not provide their\nhonest feedback (an idea supported by [12]). Removing the bonus may be worse, as reviewers would then be able to increase the chance of their own proposal being accepted by rating other proposals lower [13]. In either case, reviewers can benefit from reporting something other than their truthful values. When agents have the incentive to misrepresent their truthful reports, the effect on the results of the aggregation or selection mechanism can be problematic. Indeed, in a comprehensive evaluation of the peer review process, Wenneras and Wold [14] wrote in Nature that “...the development of peerreview systems with some built-in resistance to the weakness of human nature is therefore of high priority.”\nWe propose a novel strategyproof (or impartial) mechanism, where agents may never gain by being insincere. There are many reasons to prefer a mechanism which is strategyproof: first, the mechanism does not favor “sophisticated” agents who have the expertise to behave strategically. Second, agents with partial or no knowledge about the rankings of other agents are not disadvantaged when using a strategyproof mechanism. Third, normative properties of a mechanism typically assume a sincere behavior. If agents act strategically, we may lose some desirable normative properties. Fourth, it is easier to persuade people to use a strategyproof mechanism than one which can be (easily) manipulated. Note that while strategyproofness does not handle all biases agents may have, it eliminates an obvious “weakness in human nature.”\nTo achieve strategyproofness we could use a lottery (as in the Athenian democracy). However, this method does not select based on merit. A different option is to use a mechanism based on a voting rule. However, following Gibbard and Satterthwaite [15, 16], any “reasonable” mechanism based on voting will not be strategyproof unless it is a dictatorship. Another option is to use a mechanism like the Page Rank algorithm that use Markov chains to compute a ranking of the agents [17]. However, such mechanisms are also not strategyproof.\nContributions. First, we propose a novel peer selection mechanism (Exact Dollar Partition) that satisfies several desirable axiomatic properties including strategyproofness and two natural monotonicity properties. Second, we conduct a detailed experimental comparison with previously introduced strategyproof mechanisms with regard to their ability to recover the “ground truth”. Our experiments demonstrate that Exact Dollar Partition selects more high quality agents more often, selects more high quality agents in the worst case, and has more consistent quality than any other strategyproof mechanism in the literature. Third, our mechanism uses a novel randomized apportionment subroutine to fairly round selected fractional group sizes to integers. This subroutine is interesting in its own right as it provides a compelling solution to the\n| 1–9\nar X\niv :1\n60 4.\n03 63\n2v 1\n[ cs\n.G T\n] 1\n3 A\npr 2\n01 6\nfundamental problem of apportionment: allocating representatives or resources in proportion to the group sizes or demands. Young [18] motivates the problem as follows:“This surprisingly difficult problem has concerned statesmen, political analysts and mathematicians for over two hundred years.”\nDiscussion and Related Work. Peer review and peer selection are the cornerstone of modern science and hence, the quality, veracity, and accuracy of peer review and peer evaluation is a topic of interest across a broad set of disciplines. Most empirical evaluations of peer review and peer selection study the effectiveness and limits of the system; typically by assembling large corpora of peer reviewed proposals and cross examining them with new panels or review processes [19, 20]. Questions of bias, nepotism, sexism, cronyism, among other issues, have received extensive coverage, and have been substantiated to varying degrees, in the literature [14, 19, 21]. However, a consistent conclusion in all of these works is that, in order to decrease the role of chance and/or any systematic bias, the community needs to broaden the base of reviewers. Indeed, one way for the results of the review process to reflect the views of the entire scientific constituency and provide more value to community is to increase the number of reviewers [22, 23]. The key scientific question lies in finding a mechanism that allows for crowdsourcing the work of reviewing, without compromising the incentives and quality of the peer review and selection process.\nThe criticism that prominent peer selection mechanisms such as those under consideration by American and European funding bodies [1, 10] are not strategyproof [13] has underscored the need to devise mechanisms with better incentive properties. The literature most directly relevant to this article is a series of papers on strategyproof (impartial) selection [4, 7]. We survey these mechanisms in the next section. Most of the work on strategyproof peer selection focuses on the setting in which agents simply approve (nominate) a subset of agents [4, 6, 7, 24] with the latter three restricting attention to the setting in which exactly one agent is selected (k = 1). Kurakowa et al. [8] presented an interesting strategyproof mechanism (Credible Subset) that performs well when each agent reviews a very small number of agents relative to the total number of agents. Other recent work focused on tradeoffs between different axioms concerning peer selection [25, 26].\nThe peer selection problem is closely related to peer-based grading/marking [2, 17, 23, 27–30] especially when students are graded based on percentile scores. For peer grading, mechanisms have been proposed that make a student’s grade slightly dependent on the student’s grading accuracy (see e.g., [17] and [10]). However such mechanisms are not strategyproof as one may alter one’s reviews to obtain a better personal grade."
    }, {
      "heading" : "Setup and Survey of Existing Mechanisms",
      "text" : "We have a set N of agents {1, . . . , n}. An agent, depending on the setting, evaluates some 0 ≤ m ≤ n − 1 of the other agents. Based on these messages, around k agents are selected. Each agent reports a valuation (review) over the other agents (proposals). These messages could be cardinal valuations vi(j) for agent i’s valuations of agent j, or they could be a weak order (i.e., some agents may have the same rank) reported by agent i of agents in N \\ {i}, which may be transformed to cardinal valuations using a scoring rule. Some mechanisms,\nsuch as Credible Subset, may not always return a size of exactly k even if the target size is k.\nA particular family of mechanisms which we will deal with are based on partitioning. The general idea of partitioning based mechanisms is to divide the agents into a set of clusters C = {C1, . . . , C`}. This partition can be done using either a random or predetermined process, without adding randomness to the process. We will often abuse notation and refer to the value that one cluster has for another cluster vCi (Cj); the valuation of all agents in Ci for the agents in Cj : ∑ r∈Ci,j∈Cj\nvr(j). We will assume that cluster sizes are such that selection from them is not a problem: for all 1 ≤ i ≤ `, k ≤ |Ci|.\nMechanisms. There are three prominent mechanisms for peer selection that appear in the literature. Vanilla: select the k agents with highest total value based on their reviews by other agents (as done today, for example, in many scientific conferences). Vanilla is not strategyproof; unselected agents have an incentive to lower the reported valuations of selected agents. Partition: divide the agents into ` clusters and select a preset number of agents from each cluster, typically k/`, according to the valuations of the agents not in that cluster. This class of mechanisms is a straightforward generalization of the Partition mechanism [4] (and in an early version of [8]) and is strategyproof. Credible Subset [8]: let T be the set of agents who have the top k scores, as in Vanilla. Let P be the set of agents who do not have the top k scores but will make it to the top k if they do not contribute any score to other agents (hence |P | ≤ m). With probability (k + |P |)/(k +m), Credible Subset selects a set of k agents uniformly at random from T ∪ P , and with probability 1 − (k + |P |)/(k + m), it selects no one. The mechanism is strategyproof. There are a number of other mechanisms that are tailor-made for k = 1 and when agents only mark approval of a subset of agents: Partition [7]; Permutation [6]; and Slicing [24].\nIn designing our mechanism, we were inspired by a prominent strategyproof mechanism for dividing a continuous resource from the economics literature. Dividing a Dollar[31]: Each agent i has a value vi(j) of his estimation of how much of the resource j should receive, and these values are normalized so that ∑ j∈N\\{i} vi(j) = 1/n. Then the Dollar share of each\nagent i is xi = ∑\nj∈N\\{i} vj(i).\nProperties of Mechanisms. We consider some basic axioms of peer selection mechanisms: (i) Non-imposition: for any target set W , there is a valuation profile and a randomization seed that achieves W ; (ii) Strategyproofness (Impartiality): agents cannot affect their own selection; (iii) Monotonicity: if an agent i is selected, then if in a modified setting in which that agent is reinforced (score or ordinal ranking of i is improved with respect to other agents) by some agents in N \\ {i}, and no other agent’s value/ranking improved, then i will remain selected; (iv) Committee Monotonicity: if W is the outcome when the target set size is k, then the agents in W are still selected if the target set size is k + 1. These final two properties regard probability of selection when algorithms involve randomization."
    }, {
      "heading" : "Exact Dollar Partition",
      "text" : "The algorithm Exact Dollar Partition is formally described in the algorithm box. Broadly, it works as follows:\n2 | Aziz et al.\nExactDollarPartition(N, vi) Input: Set of agents N .\nSet of valuations for each agent i, vi. Output: Set of winning agents W . 1: Initialize W ← ∅ 2: Generate a partition {C1, . . . , C`} of N where the\ndifference between the sizes of any two clusters is at most 1. 3: Each i ∈ N reviews m agents outside C(i), where C(i) is the cluster of agent i.\n4: Ensure ∑\nj /∈C(i) vi(j) = 1 by normalizing. 5: xi, the value of a cluster Ci, is defined as:\nxi ← 1 n × ∑ j∈Ci,j′ /∈Ci vj′ (j).\n6: Using the xi values, we compute the number of agents ti to be chosen from each cluster Ci. First let each share si ← xi ∗ k for each i ∈ {1, . . . , `}. 7: (t1, . . . , t`)← AllocationFromShares(s1, . . . , s`) where (t1, . . . , t`) are the number of agents to be allocated from each cluster. 8: For each i ∈ C(i), the score of agent i is∑ i′ /∈C(i) vi′ (i). 9: Select tj agents with the highest scores from each cluster Cj and place them in set W .\n10: return W\nagents are partitioned such that the sizes of clusters are equal or as near as possible, with difference at most 1. Each agent i ∈ N assigns a value vi(j) to each agent j that is among the m agents that i reviews, none of which are in i’s cluster. Agent i may directly give a cardinal value to the agents they review or the cardinal value may be obtained by a scoring function that converts the ordinal ranking given by i to cardinal values. In either case, the values that i gives are normalized so that agent i assigns a total value of 1 to agents outside their own cluster. Based on the values from agents outside the cluster we assign the normalized weight (Dollar Share) xj to each cluster Cj . Based on each Dollar Share xi, each cluster has a possibly real valued quota si = xi · k. If all si’s are integers, then each si is the quota of cluster Ci, i.e., the top graded si agents are selected from cluster Ci. If not all si are integers, then we use the function AllocationFromShares in line 7 (detailed in the next section) to enumerate discrete cluster allocations in which each cluster gets an allocation of either bsic or dsie is used. AllocationFromShares then computes a probability distribution over these discrete allocations, requiring at most, ` such allocations, so that the expected quota for each cluster will be exactly si. We draw a discrete allocation (t1, . . . , t`) using the distribution computed in AllocationFromShares and select exactly the ti agents from each cluster Ci with highest score.\nWe defer our proofs and analysis of the properties of the function AllocationFromShares to the next section. For the analysis of the overall mechanism, it is enough to assume it chooses an allocation with exactly k shares from a probability space constructed so that the expected share of each cluster j is sj . As no agent is treated differently in the mechanism, Exact Dollar Partition is anonymous and satisfies non-imposition. In the supplemental material, we show that Exact Dollar\nPartition retains key normative properties that are important for any peer selection setting.\nTheorem 1 Exact Dollar Partition is strategyproof.\nRemark 2 Exact Dollar Partition is not just strategyproof but even group-strategyproof if manipulating coalitions involve agents from the same cluster (so potentially colluding agents, e.g., with conflict of interest, can be put in the same cluster).\nTheorem 3 Exact Dollar Partition is monotonic.\nTheorem 4 Exact Dollar Partition is committee monotonic."
    }, {
      "heading" : "A Randomized Apportionment Rule",
      "text" : "The randomized allocation technique used for Exact Dollar Partition is of independent interest since it addresses the classic apportionment problem in a randomized way. Consider the problem in which ` disjoint groups are to be allocated a given number of slots k < n in proportion to the group sizes. The problem is ubiquitous in apportionment settings such as proportional representation of seats in the US congress, European Parliament, and the German Bundestag as well as various other committee selection settings [18, 32–36]. It has been studied in political science, economics, operations research and computer science.\nIn these settings, each group i has a quota si with ∑n\ni=1 si = k, which we call its target quota. Since si may not be an integer, we have to resort to apportionment which means that in order to allocate exactly k slots, some group may be assigned an integer quota slightly more or less that its target quota.\nNumerous apportionment procedures have been introduced in the literature including the methods of Hamilton, Jefferson, Webster, Adams, and Hill [32]; each with its own drawbacks. In fact, no deterministic apportionment procedure can satisfy a group of three minimal axioms [32]: (1) Quota Rule, each group should get quota that is the result of the target quota being rounded up or down; (2) Committee Monotonicity, if k increases then then the quotas do not decrease; and (3) Monotonicity, if group i’s size increases and group j’s size decreases by the same margin, then then there is no transfer of quota from i to j. We call discrete quota allocations that satisfy the quota rule and allocate exactly k slots nice allocations.\nCurse of Determinism: The Need for Randomization. In view of the result by Balinski and Young [32], we first demonstrate that randomization is a necessary feature of an apportionment mechanism in order to maintain both exactness and stragetyproofness. Therefore, any deterministic and strategyproof method of using fractional quotas to derive integer quotas can result in outcomes that are not of the target size (e.g., [37]).\nTheorem 5 No deterministic rounding of quotas that guarantees selection of k agents is strategyproof.\nHence, we resort to randomization to achieve ex ante fairness and the target size. Using randomization, our goal is to ensure that the target number of total agents chosen is k in expectation and exactly k ex post. We will additionally require that the integer quota allocation returned by the lottery is a nice allocation. Randomization has been used in various settings such as voting and fair allocation of indivisible goods to achieve ex ante as well as procedural fairness [38–40].\nAziz et al. | 3–9\nOne possible way to achieve the appropriate randomization is to enumerate all the feasible discrete quota allocations and then solve equations to find the probability distribution over these quota allocations. If such a probability distribution exists, the method outlined involves enumerating an exponential number of such quota allocations that is computationally infeasible if the number of groups is large (for example, in US elections, ` is 50). Hence, some suggested approaches to this problem require multiple rounds of randomization and also do not enumerate the possible ex post outcomes (there may be an exponential number of them) [41]. Previously, a stochastic apportionment rule was presented that achieves the quota requirements [42] by two randomizations, one of them using a stochastic continuous variable. This means that not only does it not involve a probability distribution over discrete nice allocations, hence it cannot be used to achieve fairness via repeated representation, but due to computers not being able to reproduce truly continuous values, this may compromise strategyproofness.\nIn view of these challenges, we present a simple method AllocationFromShares that achieves the target quotas, relies on probability distribution over linear number of nice allocations, and the probability distribution can be computed in linear time, and requires minimal randomization (only one round). Our randomized procedure can be easily de-randomized in repetitive settings. In frequently repeated allocation settings, one could use the nice allocations computed by AllocationFromShares (at most `, compared to the potential exponential number) in a way such that the allocations have the same frequency as the probability distribution computed by the algorithm. In this sense, our randomized apportionment routine has an advantage over other proposed methods.\nInformally, our method proceeds gradually from quotas which need to be rounded up with low probability, while keeping an eye on our two main constraints: not rounding up a quota too much, on one hand, while not being left with not enough probability for allocations with quotas which need to be rounded up with high probability. Example 6 We give an example of the working of AllocationFromShares when a set of quotas are not integers. Suppose we have the following Dollar shares for ` = 5, (1.1, 1.1, 1.3, 1.7, 1.8) and we wish to select k = 7 agents. The α, computed on line 5, is 2, and we start with low = 1;high = 5.\nWe begin considering the allocation (2, 2, 1, 1, 1). Since 0.1 = s1 − bs1c < ds5e − s5 = 0.2, it will get a probability of 0.1. Now low = 2, p̄ = 0.1, and we “slide” our allocation by one to the right, and look at (1, 2, 2, 1, 1). Since the second cluster has been rounded up with a probability of 0.1 in the previous allocation, s2 − bs2c − p(v2) = 0. Therefore this allocation is given probability 0, p̄ does not change and now low = 3. Moving to allocation (1, 1, 2, 2, 1), we see 0.3 = s3 − bs3c − p(v3) > ds5e−s5− p̄+p(v5) = 0.1, so this allocation is given probability 0.1, p̄ = 0.2, α = 1 and high = 4. We now look at (1, 1, 2, 1, 2), since 0.2 = s3−bs3c− p(v3) = ds4e− s4− p̄+ p(v4) = 0.2, we give it the probability 0.2, p̄ = 0.4, and low = 4. Finally, we look at (1, 1, 1, 2, 2), giving it the probability s4−bs4c−p(v4) = 0.6.\nOverall, we have probability distribution [(2, 2, 1, 1, 1) : 0.1, (1, 1, 2, 2, 1) : 0.1, (1, 1, 2, 1, 2) : 0.2, (1, 1, 1, 2, 2) : 0.6]. Theorem 7 AllocationFromShares defines a distribution and the expected allocation of each cluster i is its share si.\nAllocationFromShares(s1, . . . , s`) Input: A real allocation (s1, . . . , s`) over ` objects. Output: A discrete allocation (t1, . . . , t`) over ` objects. 1: Sort and renumber (s1, . . . , s`) according to size of si − bsic, with s1 − bs1c being minimal.\n2: Let (p1, . . . , p`) ← (0, . . . , 0) where pi is the probability of rounding up cluster i. 3: Let p̄← 0, the total probability allocated so far. 4: Let D ← ∅, where D maps: allocation → probability. 5: α← ∑` i=1(si − bsic) 6: low ← 1; high← ` 7: while low ≤ high do 8: Let allocation← (bs1c, . . . , bslow−1c, dslowe, . . . , dslow+α−1e, bslow+αc, . . . , bshighc,dshigh+1e, . . . , ds`e) Where if low=1, we start with ds1e; if high = `, we end with bshighc; and if α = 0, we have only bslowc.\n9: prob← 0 10: prevLow ← low; prevHigh← high 11: if α = 0 then 12: prob← 1− p̄;high← high− 1 13: else 14: if slow−bslowc−plow < dshighe−shigh−p̄+phigh\nthen 15: prob← slow − bslowc − plow; low ← low + 1 16: else 17: prob← dshighe − shigh − p̄+ phigh 18: high← high− 1; α← α− 1 19: for all i such that prevLow ≤ i < prevLow+α or\nprevHigh < i ≤ ` do 20: pi ← pi + prob 21: p̄← p̄+ prob 22: D ← D ∪ (allocation→ prob) 23: Select an allocation (t1, . . . , t`) according to D. 24: return (t1, . . . , t`)"
    }, {
      "heading" : "Analytical Comparisons with Other Mechanisms",
      "text" : "Though Exact Dollar Partition draws inspiration from Dividing a Dollar and Partition, there are key differences.\nComparison with other Dollar Based Mechanisms. Although Exact Dollar Partition is partly based on the Dollar mechanism for dividing a bonus (division of a divisible item between agents), it is more desirable than other mechanisms one can construct based on the Dollar framework. Consider the following possible adaptations of the Dollar framework and their shortcomings. Dollar Raffle computes the relative fractions of how much of a dollar each agent should get via the Dollar mechanism [31]. Using these shares as probabilities, repeatedly select randomly an agent according to its dollar share probabilities until k different agents are selected. Dollar Partition Raffle takes the Dollar shares of the clusters in Dollar Raffle and uses these shares to define a probability distribution over the clusters. A cluster is drawn with respect to the cluster Dollar probabilities and the next best agent, based on reviews of agents outside the cluster, is selected, until k different agents are selected. Top Dollar selects agents with maximum Dollar shares.1\nBoth Dollar Raffle and Dollar Partition Raffle have a non1Vanilla is equivalent to Top Dollar when agents’ valuations are normalized.\n4 | Aziz et al.\nzero probability of selecting the k worse agents. While Top Dollar is not strategyproof for any k, Dollar Raffle and Dollar Partition Raffle are strategyproof for k = 1. None, however, are strategyproof for k > 1. We can also show the following theorem (which can be extended to the various mechanisms mentioned for k = 1 (e.g., [6])).\nTheorem 8 Dollar Raffle, Dollar Partition Raffle and Top Dollar are not strategyproof for k > 1.\nComparison with Partition Mechanisms. Exact Dollar Partition seems similar to the Partition mechanism but while Partition must preset the number of agents to be selected from each cluster, Exact Dollar Partition relies on the peer reviews to decide the number of agents to be selected from each cluster. This difference allows Exact Dollar Partition to have more consistent performance, no matter the clustering. Hence, in contrast to Exact Dollar Partition, the rigidity of Partition means that it may not choose a large proportion of the best agents even if agents have unanimous valuations.\nExample 9 Consider the setting in which N = {1, . . . , 18}, k = 6, and ` = 3. Let the clusters be C1 = {1, . . . , 6}, C2 = {7, . . . , 12}, C3 = {13, . . . , 18}. C1 puts all its weight on C2, equally dividing its points between 7, 8, . . . , 12, with a slight edge to 7 and 8, C2 and C3 put all the weight on C1, dividing their points between 1, 2, 3 and 4. Now Partition will choose 1, 2, 7, 8, 13, 14 where everyone thinks that 1, 2, 3, 4, 7, 8 are the best. Exact Dollar Partition will select exactly that set. Moreover, if we increase the number of clusters, the disparity between Exact Dollar Partition and Partition only grows.\nPartition, in contrast to Exact Dollar Partition, performs poorly ex post2 if the clusters are lopsided, with some cluster containing all good agents and other clusters containing low value agents. One natural fix is to deliberately choose a balanced partition where the weight of a cluster is based on the ratings of agents outside the cluster and we choose a clustering that minimize the difference between the cluster weights. However for this and various notions of balanced partitions, computing the most balanced partition is NP-hard. What is even more problematic is that if we choose a balanced partition, the resulting mechanism is not strategyproof.\nWe point out that there are instances where Partition may perform better than Exact Dollar Partition even if the rankings of the agents are unanimous. Consider a case where a highly preferred agent is in the same group as the lowest preferred agents, while other groups only contain medium preferred agents. In that case the weight of the cluster with the highest preferred agent might be so high that lowest ranked agents might also be selected.The normalization of scores entailed in Exact Dollar Partition causes a certain loss of information and granularity compared to the other mechanisms. However, even in the example above, Exact Dollar Partition will ensure that when agents have highly correlated or unanimous preferences, the agent(s) that are unanimously on the top will be selected, even if some low-ranked agents are also selected."
    }, {
      "heading" : "Experimental Comparison with Other Mechanisms",
      "text" : "Using Python and extending code from PrefLib [43] we have implemented the Exact Dollar Partition, Credible Subset,\n2For high stakes outcomes, we want a mechanism that performs well on average and never returns an especially bad outcome.\nPartition, Dollar Raffle, Dollar Partition Raffle, and Vanilla peer selection mechanisms. All the code developed for this project is available open sourced at . In all simulations there are many parameters to consider that can drastically affect the outcome (see e.g., [44]). By focusing on a target domain, the NSF Mechanism Design Pilot [10, 11], we can draw focused conclusions from our simulations. In 2014, this program had n = 131 proposals, with each submitter reviewing m = 7 other proposals, broken into ` = 4 clusters. The acceptance numbers are not broken out from the global ≈20% acceptance rate, so we use this as the acceptance rate.\nExperimental Setup. To create NSF-like data we generate a sparse N = {1, . . . n} scoring matrix (profile) using a Mallows Model to generate the ordinal evaluation [45, 46]. Mallows models are parameterized by a reference order (σ) and a dispersion parameter (φ). The reference order σ can be thought of as the underlying ground truth; the NSF mechanism assumes implicitly that there is some true ordering of proposals that the reviewers are attempting to guess. Intuitively, the dispersion parameter φ is the probability of committing ranking error by swapping neighboring elements according to σ, where φ = 0 means that no agent ever commits an error and φ = 1.0 means that orderings are generated uniformly at random [47]. Mallows models are used when each agent is assumed to have the same reference ranking subject to a common, independent noise model. Each agent i ∈ N ends up with a ranking rank(i, j)→ {0, . . . ,m − 1} over each agent j ∈ N where rank(i, j) = 0 means j is the highest reviewed proposal by i.\nEach agent reviewsm of the n proposals and is also reviewed by m other agents. Agents are clustered under the constraint that each agent reviews m agents outside his cluster. We call a reviewer assignments satisfying these constraints a balanced m-regular assignments. To maximize inter-cluster comparison, we want the m reviews provided by agent i to be balanced among the clusters (less Ci) so agent i in cluster Ci reviews in total |Ci|·m\n`−1 agents from each other cluster. We generate this assignment randomly and as close to balanced as possible.\nWe generate a sparse n × n score matrix by: drawing a balanced m-regular assignment; generating a complete ordinal ranking using a Mallows model for agent i; removing all candidates from i’s ordinal ranking not assigned to i; and assigning score m−rank(i, j) to each agent j that i ranks (known as the Borda score). This process mimics the underlying assumption of the NSF mechanism in that, if a reviewer were to see all proposals, they could strictly order the complete set. This process leaves us with a sparse n×n score matrix which obeys an m-regular assignment of agents partitioned into ` clusters.\nWe use two different orderings to evaluate the performance of all the mechanisms presented: the ground truth (GT) ordering and the ordering selected by vanilla (V). Since a Vanilla like mechanism is used in many settings, it is fitting to see how well the strategyproof mechanisms approximate it (though we assume it is not manipulated by the participants despite not being strategyproof). This allows us to understand the “price” we are paying for strategyproofness. Formally, letW,W ′ be the winning sets returned by two mechanisms, we measure the similarity of W to W ′ by |W ∩W ′|/k. For n = 130 and s = 1000, we looked at a “NSF Like” space with k ∈ {15, 20, 25, 30, 35}, m ∈ {5, 7, 9, 11, 13, 15}, φ ∈ {0.0, 0.10, 0.20, 0.35, 0.50}, and ` ∈ {3, 4, 5, 6}.\nAziz et al. | 5–9\nGeneral Results. It is hard to directly compare results for Credible Subset due to the large probability of returning an empty set under the given parameters. In fact, counter to intuition, Credible Subset performs worse as we increase the number of reviews because this increases the chance of returning an empty set (see, e.g., Figure 1.) This problem is not easily overcome; removing the ability to return an empty set means Credible Subset is no longer strategyproof. When Credible Subset does return a set, it performs very well, on par with Vanilla. However, the minimum (0 in some cases), average, and standard deviation for Credible Subset are all unacceptable for practical implementation.\nThroughout the testing Exact Dollar Partition strictly out performs, for all parameter settings, the other Dollar based mechanisms. Hence, we conclude that the extra steps developed for Exact Dollar Partition are necessary and result in dramatically increased performance. In this discussion we will focus on comparing the performance of Exact Dollar Partition and Partition only, as these two mechanisms are the top two performing mechanisms which are also strategyproof. Broadly, we find that Exact Dollar Partition outperforms Partition. On average, Exact Dollar Partition selects between 0.5% and 5% more top agents, measured against V or GT. It does this with between 3% and 25% lower standard deviation, and selects as many or, in the most extreme cases, up to five more top performing agents. This improvement in the worst case means Exact Dollar Partition is an improvement of up to 25% when the clustering of the agents is lopsided. Hence, Exact Dollar Partition is the best in our experiments: it selects more top agents, more often, with better worst case performance and lower variance than any other strategyproof mechanism.\nVarying Noise(φ) and Clusters (`). Varying the noise parameter φ has little to no effect on the approximation to V for all mechanisms. This is not surprising as all mechanisms receive the same (noisy) information. When approximating GT, the value of φ has negligible effect on the performance of the mech-\nanisms unless φ ≥ 0.95; for the remainder of the discussion we fix φ = 0.5. Varying the setting of ` we see that all mechanisms perform best with respect to GT when we set ` = 5 and with respect to V when ` = 3; with a decrease in performance as we continue to increase `. No matter the setting, increasing the number of clusters hurt Vanilla and Exact Dollar Partition’s performance the least, i.e., their performance decreases less quickly than the other mechanisms. For the remainder we set ` = 4 as was done for the NSF pilot.\nVarying the Number of Selections (k) and Reviews (m). Figure 1 captures our metrics as we vary the number of selections k, in the top row, and the number of reviews per item m, in the bottom row. Varying the setting to k we observe fairly consistent performance by the mechanisms with Exact Dollar Partition maintaining a 1.5% to 3% advantage. The biggest percentage wise advantages are found when k = 15, where Exact Dollar Partition selects up to two better agents more according to V, in the worst case, resulting in a ≈ 25% improvement. In the worst case, up to two more top agents according to GT are selected by Exact Dollar Partition than Partition. Because both mechanisms perform worse (in absolute terms) than they do as measured by V, this translates to a 10–20% increase in performance for Exact Dollar Partition when k is small and a 5–10% increase when k is large. Measured agains both V and GT draw the general conclusion that, as we increase k, ExactDollarPartition increases its advantage over Partition.\nFor the most NSF like setting, n = 130, k = 30, l = 4, φ = 0.5 and sweeping m ∈ {5, 7, 9, 11, 13, 15} is depicted as the bottom row of Figure 1. Looking closely at the numbers as measured against V we see that Exact Dollar Partition performs 2.6 to 3.0% better on average, i.e., one better agent. ExactDollarPartition does this with a nearly 20% smaller standard deviation, always selecting at least one more and up to three more top agents (15%) in the worst case. This pattern is similar across settings to the other parameters as vary m; ExactDollarPartition performs consistently better. as we in-\n6 | Aziz et al.\ncrease the number of reviews, ExactDollarPartition continues to outpace Partition. Compared to the performance of Vanilla, ExactDollarPartition selects about one worse agent on average, up to two worse agents in the worst case (≈ 7%). Hence, the loss in performance we see for moving to a strategyproof mechanism is similar to the worse performance of Partition compared to ExactDollarPartition.\nACKNOWLEDGMENTS. The authors wish to thank Allan Borodin, Manuel Cebrian, Serge Gaspers, Ian Kash, Julian Mestre, and Hervé Moulin for useful comments. Data61 (formerly known as NICTA) is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. This research has also been partly funded by Microsoft Research through its PhD Scholarship Program, Israel Science Foundation grant #1227/12, and NSERC grant 482671. This work has also been partly supported by COST Action IC1205 on Computational Social Choice.\n1. Hazelrigg GA (2013) Dear Colleague Letter: Information to Principal Investigators (PIs) Planning to Submit Proposals to the Sensors and Sensing Systems (SSS) Program October 1, 2013, Deadline (NSF Website, http://www.nsf.gov/pubs/2013/nsf13096/nsf13096.jsp). 2. Piech C et al. (2013) Tuned models of peer assessment in MOOCs in Proceedings of The 6th International Conference on Educational Data Mining (EDM). (Memphis, Tennessee), pp. 153–160. 3. Luo H, Robinson AC, Park JY (2014) Peer grading in a MOOC: Reliability, validity, and perceived effects. Journal of Asynchronous Learning Networks 18(2). 4. Alon N, Fischer F, Procaccia AD, Tennenholtz M (2011) Sum of us: Strategyproof selection from the selectors in Proceedings of the 13th Conference on Theoretical Aspects of Rationality and Knowledge (TARK). pp. 101–110. 5. Lakhani KR, Garvin DA, Lonstein E (2010) Topcoder (a): Developing software through crowdsourcing. Harvard Business Review. 6. Fischer F, Klimm M (2014) Optimal impartial selection in Proceedings of the 15th ACM Conference on Economics and Computation (ACM-EC). (ACM Press), pp. 803–820. 7. Holzman R, Moulin H (2013) Impartial nominations for a prize. Econometrica 81(1):173–196. 8. Kurokawa D, Lev O, Morgenstern J, Procaccia AD (2015) Impartial peer review in Proceed-\nings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI). (AAAI Press). 9. Roos M, Rothe J, Scheuermann B (2011) How to calibrate the scores of biased reviewers by\nquadratic programming in Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI). pp. 255–260. 10. Merrifield MR, Saari DG (2009) Telescope time without tears: a distributed approach to peer review. Astronomy & Geophysics 50(4):4–16. 11. Foundation TNS (2014) Report to the national science board on the national science foundation?s merit review process fiscal year 2014, (The National Science Foundation (USA)), Technical report. 12. Court D, Gillen B, McKenzie J, Plott CR (2015) Two information aggregation mechanisms for predicting the opening weekend box office revenues of films: Boxoffice Prophecy and guess of guesses, (California Institute of Technology), Technical Report SSWP-1412. 13. Naghizadeh P, Liu M (2013) Incentives, quality, and risks: A look into the NSF proposal review pilot. arXiv preprint arXiv:1307.6528 pp. 1–10. 14. Wenneras C, Wold A (1997) Nepotism and sexism in peer-review. Nature 387:341–343. 15. Gibbard A (1973) Manipulation of voting schemes. Econometrica 41(4):587–602. 16. Satterthwaite MA (1975) Strategy-proofness and Arrow’s conditions: Existence and corre-\nspondence theorems for voting procedures and social welfare functions. Journal of Economic Theory 10(2):187–217. 17. Walsh T (2014) The PeerRank method for peer assessment in Proceedings of the 21st European Conference on Artificial Intelligence (ECAI). pp. 909–914. 18. Young HP (1994) Equity: in Theory and Practice. (Princeton University Press).\n19. Cole S, Cole J, Simon G (1981) Chance and consensus in peer review. Science 214(4523):881–886. 20. Li D, Agha L (2015) Research funding. big names or big ideas: do peer-review panels select the best science proposals? Science (New York, N.Y.) 348(6233):434–438. 21. McNutt RA, Evans AT, Fletcher RH, Fletcher SW (1990) The effects of blinding on the quality of peer review: A randomized trial. The Journal of the American Medical Association 263(10):1371–1376. 22. Price E (2014) The NIPS experiment (http://blog.mrtz.org/2014/12/15/the-nipsexperiment.html). 23. Joachims T, Raman K (2015) Bayesian ordinal aggregation of peer assessments: A case study on KDD 2015, (Cornell University), Technical report. 24. Bousquet N, Norin S, Vetta A (2014) A near-optimal mechanism for impartial selection in Proceedings of the 10th International Workshop on Internet and Network Economics (WINE), Lecture Notes in Computer Science (LNCS). 25. Mackenzie A (2015) Symmetry and impartial lotteries. Games and Economic Behavior 94(1):15–28. 26. Berga D, Gjorgjiev R (2014) Impartial social rankings. Working paper. 27. Alfaro LD, Shavlovsky M (2014) CrowdGrader: Crowdsourcing the Evaluation of Homework\nAssignments in Proceedings of the ACM Technical Symposium on Computer Science Education (ACM-SIGCSE). pp. 415–420. 28. Kulkarni C, Wei K, Le H, Chia KD (2013) Peer and self assessment in massive online classes. ACM Transactions on Computer Human Interaction (TOCHI) 20(6):1–31. 29. Robinson R (2001) Calibrated peer review an application to increase student reading and writing skills. The American Biology Teacher 63(7):474–476. 30. Wright J, Thornton C, Leyton-Brown K (2015) Mechanical TA: Partially automated high-stakes peer grading in Proceedings of the ACM Technical Symposium on Computer Science Education (ACM-SIGCSE). 31. de Clippel G, Moulin H, Tideman N (2008) Impartial division of a dollar. Journal of Economic Theory 139:176–191. 32. Balinski M, Young HP (1982) Fair Representation. (Yale University Press). 33. Balinski ML, Young HP (1980) The webster method of apportionment. Proceedings of the\nNational Academy of Sciences (PNAS) 77(1):1–4. 34. Birkhoff G (1976) House monotone apportionment schemes. Proceedings of the National\nAcademy of Sciences (PNAS) 73(3):684–686. 35. Mayberry JP (1978) Quota methods for congressional apportionment are still non-unique.\nProceedings of the National Academy of Sciences (PNAS) 75(8):3537–3539. 36. Pukelshei F (2014) Proportional Representation: Apportionment Methods and Their Applica-\ntions. (Springer). 37. Aziz H, Lev O, Mattei N, Rosenchein JS, Walsh T (2016) Strategyproof peer selection: Mech-\nanisms, analyses, and experiments in Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI). 38. Gibbard A (1977) Manipulation of schemes that mix voting with chance. Econometrica 45(3):665–681. 39. Budish E, Che YK, Kojima F, Milgrom P (2013) Designing random allocation mechanisms: Theory and applications. American Economic Review 103(2):585–623. 40. Bogomolnaia A, Moulin H (2001) A new solution to the random assignment problem. Journal of Economic Theory 100(2):295–328. 41. Gandhi R, Khuller S, Parthasarathy S, Srinivasan A (2006) Dependent rounding and its applications to approximation algorithms. Journal of the ACM 53(3):324–360. 42. Grimmet G (2004) Stochastic apportionment. The American Mathematical Monthly 111(4):299–307. 43. Mattei N, Walsh T (2013) Preflib: A library for preferences. HTTP://WWW.PREFLIB.ORG in Proceedings of the 3rd International Conference on Algorithmic Decision Theory (ADT). pp. 259–270. 44. Popova A, Regenwetter M, Mattei N (2013) A behavioral perspective on social choice. Annals of Mathematics and Artificial Intelligence 68(1–3):135–160. 45. Mallows C (1957) Non-null ranking models. Biometrika 44(1):114–130. 46. Marden JI (1996) Analyzing and Modeling Rank Data, Monographs on Statistics and Applied\nProbability. (CRC Press) No. 64. 47. Lu T, Boutilier C (2011) Learning Mallows models with pairwise preferences in Proceedings\nof the 28th International Conference on Machine Learning (ICML). pp. 145–152.\nAziz et al. | 7–9"
    }, {
      "heading" : "Deferred Proofs",
      "text" : ""
    }, {
      "heading" : "A. Proof of Theorem 1",
      "text" : "Proof: Suppose agent i is in cluster Cj of the generated partition. Agent i will be selected in W if and only if its rank according to the v scores given by agents outside of Cj is at least tj . Therefore agent i can either manipulate by increasing tj or by increasing its score relative to other agents in Cj given by agents outside Cj . Since agent i cannot affect the latter, the only way it can manipulate is by increasing tj . We argue that agent i cannot change its expected tj by changing its valuation vi for agents outside the cluster. Note that i contributes a probability weight of 1/n to agents outside Cj and zero probability weight to agents in Cj . Hence it cannot affect the value xj of cluster Cj . As sj is derived from xj , agent i cannot affect sj . Note that any agent in cluster Cj changing its report, not only is the expected tj (which is the same as sj) does not change but the respective probabilities of getting bsjc and dshe allocated do not change as well. The reason is that sj is obtained as a unique convex combination of bsic and dsie."
    }, {
      "heading" : "B. Proof of Theorem 3",
      "text" : "Proof: Let us compare the valuation profile v when i is not reinforced and v′ when i is reinforced. The relative ranking of i is at least as high when i is reinforced. Since any decrease in valuation that an agent j in C(i) receives translates into the same increase in the valuation received by agent i, hence the total valuation that C(i) receives does not decrease and hence the number of agent to be selected from C(i) is at least as high as before."
    }, {
      "heading" : "C. Proof of Theorem 4",
      "text" : "Proof: The only difference between running the algorithm for different target k values is when calculating the quota vector ~s. However, if agent i in cluster Cj was selected, that means its ranking in the cluster Cj was above tj . When k increases, sj will only increase (as xj remains the same), and hence so will tj , ensuring that i will be selected again."
    }, {
      "heading" : "D. Proof of Theorem 5",
      "text" : "We first prove the following related theorem. Theorem 10 There is no anonymous deterministic allocation of quotas that guarantees selection of k agents. Proof: Let ` = 3, with clusters being of equal size. All agents in cluster 1 rank agents in cluster 2 before any in cluster 3; agents in cluster 2 rank those in cluster 3 ahead of cluster 1; and those in cluster 3 rank agents in cluster 1 ahead of those in 2. So share of each cluster is equal, and for k < ` there is no deterministic anonymous way to allocate quotas. Lemma 11 In a deterministic strategyproof allocation mechanism to select k agents from ` clusters, the number of slots allocated to a cluster i with a share xi will not change, regardless of the rest of the shares. Proof: Let x = (x1, . . . , x`) be a share allocation, under which cluster i receives y slots. Now, let x′ = (x′1, . . . , x′i−1, xi, x ′ i+1, . . . , x ′ `) be a different share allocation. We wish to show that the allocation to cluster i remains y. We know ∑ j 6=i xj = ∑ j 6=i x ′ j . If ∑ j 6=i |k(xj − x ′ j)| ≤ 2, this means a single agent can cause the change. As the share values do not contain data on actual agents votes, that single agent could be in cluster i (as xi did not change at all). Thanks to strategyproofness, this means cluster i’s share is still y.\nIf ∑\nj 6=i |k(xj − x ′ j)| > 2, we make the move from x to x ′ using intermediary steps, x0, . . . , xs such that xj = (xj1, . . . , x j ` ) a share\nallocation where xji = xi, x 0 = x, xs = x′, and for 1 ≤ t ≤ s,∑\nj 6=i |k(x t j − x t−1 j )| ≤ 2. Thanks to the argument in the previous paragraph, the allocation for cluster i stays y in x1, and using the argument again (with x1 as x and x2 as x′), it stays the same in x2. We apply this argument again and again, until we reach cluster i’s allocation in xs = x′ is y as well.\nProof of Theorem 5: Suppose there is a rounding of quotas that guarantees selection of k agents. Let us assume k clusters and k > 3 is odd. Using Lemma 11, we know that each cluster’s slot allocation is fixed according to its share, regardless of other clusters’ share. Hence, for each cluster with a share of 1.5, it receives an allocation of either 1 slot or 2. Case I: There are 2 clusters which allocate 2 slots when their share is 1.5. Suppose these 2 clusters have a share of 1.5, another cluster has 0, and any remaining ones 1. Hence we allocated k shares, but received k + 1 slot allocations. Case II: There are 2 clusters which allocate 1 slot when their share is 1.5. Suppose these 2 clusters have a share of 1.5, another cluster has 0, and any remaining ones 1. Hence we allocated k shares, but received k − 1 slot allocations."
    }, {
      "heading" : "E. Lemmata for Theorem 7",
      "text" : "To prove this theorem, we first need several lemmas. Note that any cluster i needs to be rounded up with probability of si − bsic, and rounded down with probability dsie − si. Lemma 12 Let x = (x1, . . . , x`) ∈ N` and let set A = {x′ ∈ N`|in α ∈ N coordinates x′i = xi + 1. In the rest x ′ i = xi}. For any\nx̂ = (x̂1, . . . , x̂`) that is a simplex of A (i.e., x̂ = ∑\na∈A paa such that ∑ a∈A pa = 1), ∑` i=1(x̂i − xi) = α.\nProof: We shall prove it for any α by induction over `. The base case is ` = α+1. In that case, any element in set A has one coordinate where it is equal to x. Hence, for each coordinate, 1− (x̂i − xi) is the weight assigned to the element of A where coordinate i is xi. Hence, we know\n`∑ i=1 (1− (x̂i − xi)) =1\n`− ∑̀ i=1 (x̂i − xi) =1\n=⇒ `∑ i=1 (x̂i − xi) = `− 1 =α\nWe now assume the lemma is true for ` = h (h > α), and we shall prove it for ` = h + 1. Let x̂ = ∑ a∈A waa, and let set B ⊂ A be all the elements of A in which coordinate 1 is x1 + 1. The weight assigned to these elements is x̂1 − x1, and therefore∑\na∈(A\\B) wa = 1− (x̂1 − x1). We create x̂′ = ∑ a∈(A\\B) 1 1−(x̂1−x1)\nwaa, which is a simplex of elements of A, and as we can ignore its first coordinate (which remains fixed), and look at it as an element in an h dimensional space, the induction hypothesis tells us ∑` i=2(x̂\n′ i − xi) = α. Therefore,∑` i=1(( ∑ a∈(A\\B) waa)i − ( ∑\na∈(A\\B) wa)xi) = α(1− (x̂1 − x1)). Every element in B contributes its weight to α coordinates (one\nof them is 1, according to B’s definition). So for element b ∈ B,∑` i=1((wbb+ ∑ a∈(A\\B) waa)i− (wb+ ∑ a∈(A\\B) wa)xi) = α(1− (x̂1−x1))+αwb. Adding all the elements of B (which weigh, together, (x̂1−x1)), we get ∑` i=1(x̂i−xi) = α(1−(x̂1−x1))+α(x̂1−x1) = α.\nNote that Lemma 12 is applicable to our algorithm, as we can consider x = (bs1c, . . . , bs`c) as a basis, and in each allocation the algorithm rounds up α coordinates, and this rounding up is equivalent to taking α coordinates, and instead of putting the value from x, we round up and add 1 to the coordinate. Lemma 13 At no point in the algorithm is a cluster rounded up or down in allocations that, together, have more probability than it should (i.e., for any cluster i, it is always true that pi ≤ si − bsic and p̄− pi ≤ dsie − si) Moreover, for any i < low, probability that cluster i is rounded up is si − bsic. For any i > high, probability that cluster i is rounded down is dsie − si.\n8 | Aziz et al.\nProof: Recall the probability cluster i is rounded up needs to be si−bsic. When a set of clusters is being rounded up, the probability of the allocation is bounded by slow − bslowc − plow (Line 14), i.e., the probability slow should be rounded up which still remains to be allocated. Any other cluster i rounded up in the same allocation has been rounded with slow in every previous allocation when it has been rounded up (since i > low), so pi ≤ plow. Since the clusters are ordered according to it, si−bsic > slow−bslowc. Hence, slow − bslowc − plow ≤ si − bsic − pi, so no cluster is rounded up more than it should be.\nAt any point in the algorithm, If i < low, then there was a stage where low = i, and line 15 changed low to i+ 1. However, at that point, cluster i is rounded up exactly at the additional probability it needed to be rounded up (si−bsic− pi) , and no further allocation in the algorithm will round it up.\nSimilarly, If i > high, there was a stage where high = i, and line 18 changed high to i− 1. However, at that point, cluster i is rounded down exactly at the additional probability it needs to be rounded down. It could not have been rounded down too much previously, as every allocation’s probability is bounded so that cluster high will not be rounded down more than it is supposed to (dshighe − shigh). Once again, once an index is high+ 1, no further allocation will round it down.\nFor cluster i, low ≤ i ≤ high, it has only been rounded down when shigh was rounded down as well (though not vice versa) and rounded up when slow was rounded up (again, not vice versa), and as the dsie−si ≥ dshighe−shigh and si−bsic ≥ slow−bslowc, these clusters have not had allocations with more than the probability of them being rounded up or down. Therefore, the amount of probabilities allocated is never larger than 1. Hence, also, for clusters i < low, as they have received the exact needed probability of being rounded up, and since the sum of allocations does not exceed 1, they have not been rounded down more than needed."
    }, {
      "heading" : "F. Proof of Theorem 7",
      "text" : "Proof: We first show all the allocations we consider only round up∑` i=1(si−bsic) clusters, as otherwise, allocations are not allocating exactly k agents. As long as low + α ≤ high in the algorithm this is trivially true. We now wish to show the situation low+α > high cannot happen (as that results in too few clusters rounded up).\nIf low+α > high, this means there was a stage in which low+α = high, and then we executed line 15. I.e., cluster high needs more unallocated probability to be rounded down than cluster low needs unallocated probability to be rounded up. Moreover, thanks to monotonicity of si vector, we know cluster high still has need for more allocations with positive probability in which it is rounded up. But that means that if we did advanced low and assigned all remaining unassigned probability to the allocation rounding up clusters low+1, . . . , `, we would be rounding them up too much, and for clusters low + 1, . . . , high, strictly so. But we know (Lemma 13) that for all i ≤ low, we have rounded it up exactly correctly, so looking at the vector x̂ ∈ N` in which each coordinate is the expected allocation for that cluster, we have:\n`∑ i=1 (x̂i − bsic) = low∑ i=1 (si − bsic) + `∑ i=low+1 x̂i − bsic >\n> `∑ i=1 (si − bsic) = α\nThis contradicts Lemma 12, as all allocations had exactly α clusters rounded up. So it cannot be that cluster high still needed more probability to be rounded down, and therefore, if low + α = high, line 15 would not have been executed at this point.\nSince low + α ≤ high at all times, the algorithm will end when low = high. Hence, the previous step ended with α becoming 0 (Line 18) (α = 0 only in this case: otherwise, it means the sum of expected value – that is, probability to be rounded up – over all clusters is above α: we have too many clusters that need to be rounded up). We now wish to prove that the last step, where the clusters high + 1, . . . , ` are rounded up, results in what we desired. Since clusters 1, . . . , low − 1 have been allocated the right probability to be rounded up, as well as clusters high + 1, . . . , ` (Lemma 13), we only need to verify this for cluster low. But according to Lemma 12, the probability of low being rounded up is exactly α− ∑ 1≤i≤`,i 6=low(si − bsic), which exactly slow − bslowc, which means cluster low has the correct allocation."
    }, {
      "heading" : "G. Proof of Theorem 8",
      "text" : "Proof: For Dollar Raffle and Dollar Partition Raffle, the proof follows a similar path: The mechanism iterates until it chooses k different agents, which is equivalent to eliminating each selected agent and re-normalizing the dollar partitions, as once some agent is selected we ignore its repeated selection. This re-normalization prevents the mechanism from being strategyproof, as now the probabilities of others matter for each agent. For example, an agent will prefer to contribute to a very strong agent (which, once eliminated, will make our agent’s probability increase significantly). Suppose k = 2, if Dollar Raffle (Dollar Partition Raffle), suppose all agents (clusters) except b1, b2, b3 allocate their points equally between those 3. b1 divides its point equally between b2 and b3, as does b2 between b1 and b3. Suppose b3 believes it should also divide its point equally between b1 and b3. In that case, it has a probability 13 of being selected first, and a probability of 1 3 of being selected second, ultimately, 23 . But if agent (in) b3 decides to give its point fully to cluster b1, the probability of b3 being selected first does not change. But the probability of b1 being selected and then b3 are ( 13 + 1 2n ) 1 3\n2 3− 1 2n\n, and the probability of b2 being selected\nand then b1 are ( 13 − 1 2n ) 1 3\n2 3 + 1 2n . The sum of these is more than 13 , hence doing so would improve agent (in cluster) b3 chances of being selected. This proof can easily be extended to any additional k.\nThe proof of this theorem carries on to the various mechanisms presented for k = 1 (e.g., [6]): simply running the algorithm several times destroys its strategyproofness. This is true even for mechanisms that are strategyproof for k = 1, as long as any agent has the power to influence the outcome (i.e., not purely random, a dictatorship, or a combination of both).\nFor Top Dollar, agents are a1, . . . , ak+1. Agents a1, . . . ak−1 allocate each of their points by giving 1\nk − 1 k2 to agent ak+1, 1k2 to agent ak and 1k to all other agents. Agent ak gives 1 k to all other agents. Agent ak+1 would like to allocate its point to agent ak, but that would mean it would not be selected itself. Giving its point to other agents will mean it will be, contradicting strategyproofness.\nAziz et al. | 9–9"
    } ],
    "references" : [ {
      "title" : "Dear Colleague Letter: Information to Principal Investigators (PIs) Planning to Submit Proposals to the Sensors and Sensing Systems (SSS",
      "author" : [ "GA Hazelrigg" ],
      "venue" : "Program October",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Tuned models of peer assessment in MOOCs in Proceedings of The 6th International Conference on Educational Data Mining (EDM)",
      "author" : [ "C Piech" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Peer grading in a MOOC: Reliability, validity, and perceived effects",
      "author" : [ "H Luo", "AC Robinson", "JY Park" ],
      "venue" : "Journal of Asynchronous Learning Networks",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Sum of us: Strategyproof selection from the selectors in Proceedings of the 13th Conference on Theoretical Aspects of Rationality and Knowledge (TARK)",
      "author" : [ "N Alon", "F Fischer", "AD Procaccia", "M Tennenholtz" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Topcoder (a): Developing software through crowdsourcing",
      "author" : [ "KR Lakhani", "DA Garvin", "E Lonstein" ],
      "venue" : "Harvard Business Review",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Optimal impartial selection in Proceedings of the 15th ACM Conference on Economics and Computation (ACM-EC)",
      "author" : [ "F Fischer", "M Klimm" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Impartial nominations for a prize",
      "author" : [ "R Holzman", "H Moulin" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Impartial peer review",
      "author" : [ "D Kurokawa", "O Lev", "J Morgenstern", "AD Procaccia" ],
      "venue" : "Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI). (AAAI Press)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "How to calibrate the scores of biased reviewers by quadratic programming",
      "author" : [ "M Roos", "J Rothe", "B Scheuermann" ],
      "venue" : "Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI)",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Telescope time without tears: a distributed approach to peer review",
      "author" : [ "MR Merrifield", "DG Saari" ],
      "venue" : "Astronomy & Geophysics",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Two information aggregation mechanisms for predicting the opening weekend box office revenues of films: Boxoffice Prophecy and guess of guesses, (California Institute of Technology)",
      "author" : [ "D Court", "B Gillen", "J McKenzie", "CR Plott" ],
      "venue" : "Technical Report SSWP-1412",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Incentives, quality, and risks: A look into the NSF proposal review pilot",
      "author" : [ "P Naghizadeh", "M Liu" ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Nepotism and sexism in peer-review",
      "author" : [ "C Wenneras", "A Wold" ],
      "venue" : "Nature",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1997
    }, {
      "title" : "Strategy-proofness and Arrow’s conditions: Existence and correspondence theorems for voting procedures and social welfare functions",
      "author" : [ "MA Satterthwaite" ],
      "venue" : "Journal of Economic Theory 10(2):187–217",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1975
    }, {
      "title" : "The PeerRank method for peer assessment",
      "author" : [ "T Walsh" ],
      "venue" : "Proceedings of the 21st European Conference on Artificial Intelligence (ECAI)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Equity: in Theory and Practice",
      "author" : [ "HP Young" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1994
    }, {
      "title" : "Chance and consensus in peer review",
      "author" : [ "S Cole", "J Cole", "G Simon" ],
      "venue" : "Science",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1981
    }, {
      "title" : "Research funding. big names or big ideas: do peer-review panels select the best science proposals? Science (New York, N.Y.) 348(6233):434–438",
      "author" : [ "D Li", "L Agha" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "The effects of blinding on the quality of peer review: A randomized trial",
      "author" : [ "RA McNutt", "AT Evans", "RH Fletcher", "SW Fletcher" ],
      "venue" : "The Journal of the American Medical Association",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1990
    }, {
      "title" : "The NIPS experiment (http://blog.mrtz.org/2014/12/15/the-nipsexperiment.html)",
      "author" : [ "E Price" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Bayesian ordinal aggregation of peer assessments: A case study on KDD 2015, (Cornell University), Technical report",
      "author" : [ "T Joachims", "K Raman" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "A (2014) A near-optimal mechanism for impartial selection",
      "author" : [ "N Bousquet", "S Norin", "Vetta" ],
      "venue" : "Proceedings of the 10th International Workshop on Internet and Network Economics (WINE),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Symmetry and impartial lotteries. Games and Economic Behavior 94(1):15–28",
      "author" : [ "A Mackenzie" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Impartial social rankings",
      "author" : [ "D Berga", "R Gjorgjiev" ],
      "venue" : "Working paper",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "CrowdGrader: Crowdsourcing the Evaluation of Homework Assignments",
      "author" : [ "LD Alfaro", "M Shavlovsky" ],
      "venue" : "Proceedings of the ACM Technical Symposium on Computer Science Education (ACM-SIGCSE)",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "Peer and self assessment in massive online classes",
      "author" : [ "C Kulkarni", "K Wei", "H Le", "KD Chia" ],
      "venue" : "ACM Transactions on Computer Human Interaction",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Calibrated peer review an application to increase student reading and writing skills. The American Biology Teacher 63(7):474–476",
      "author" : [ "R Robinson" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2001
    }, {
      "title" : "Mechanical TA: Partially automated high-stakes peer grading",
      "author" : [ "J Wright", "C Thornton", "K Leyton-Brown" ],
      "venue" : "Proceedings of the ACM Technical Symposium on Computer Science Education (ACM-SIGCSE)",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Impartial division of a dollar",
      "author" : [ "G de Clippel", "H Moulin", "N Tideman" ],
      "venue" : "Journal of Economic Theory 139:176–191",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2008
    }, {
      "title" : "The webster method of apportionment",
      "author" : [ "ML Balinski", "HP Young" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1980
    }, {
      "title" : "Quota methods for congressional apportionment are still non-unique",
      "author" : [ "JP Mayberry" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1978
    }, {
      "title" : "Proportional Representation: Apportionment Methods and Their Applications",
      "author" : [ "F Pukelshei" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Strategyproof peer selection: Mechanisms, analyses, and experiments",
      "author" : [ "H Aziz", "O Lev", "N Mattei", "JS Rosenchein", "T Walsh" ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI)",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Manipulation of schemes that mix voting with chance",
      "author" : [ "A Gibbard" ],
      "venue" : null,
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1977
    }, {
      "title" : "Designing random allocation mechanisms: Theory and applications",
      "author" : [ "E Budish", "YK Che", "F Kojima", "P Milgrom" ],
      "venue" : "American Economic Review",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2013
    }, {
      "title" : "A new solution to the random assignment problem",
      "author" : [ "A Bogomolnaia", "H Moulin" ],
      "venue" : "Journal of Economic Theory 100(2):295–328",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2001
    }, {
      "title" : "Dependent rounding and its applications to approximation algorithms",
      "author" : [ "R Gandhi", "S Khuller", "S Parthasarathy", "A Srinivasan" ],
      "venue" : "Journal of the ACM 53(3):324–360",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2006
    }, {
      "title" : "Preflib: A library for preferences",
      "author" : [ "N Mattei", "T Walsh" ],
      "venue" : "HTTP://WWW.PREFLIB.ORG in Proceedings of the 3rd International Conference on Algorithmic Decision Theory (ADT)",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2013
    }, {
      "title" : "A behavioral perspective on social choice",
      "author" : [ "A Popova", "M Regenwetter", "N Mattei" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2013
    }, {
      "title" : "Non-null ranking models. Biometrika 44(1):114–130",
      "author" : [ "C Mallows" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1957
    }, {
      "title" : "Analyzing and Modeling Rank Data, Monographs on Statistics and Applied Probability",
      "author" : [ "JI Marden" ],
      "venue" : null,
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Peer review, evaluation, and selection has been the foundational process by which scientific funding bodies have selected the best subset of proposals for funding [1] and peer evaluation is becoming increasingly popular and necessary to scale grading in MOOCs [2, 3].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "Peer review, evaluation, and selection has been the foundational process by which scientific funding bodies have selected the best subset of proposals for funding [1] and peer evaluation is becoming increasingly popular and necessary to scale grading in MOOCs [2, 3].",
      "startOffset" : 260,
      "endOffset" : 266
    }, {
      "referenceID" : 2,
      "context" : "Peer review, evaluation, and selection has been the foundational process by which scientific funding bodies have selected the best subset of proposals for funding [1] and peer evaluation is becoming increasingly popular and necessary to scale grading in MOOCs [2, 3].",
      "startOffset" : 260,
      "endOffset" : 266
    }, {
      "referenceID" : 0,
      "context" : "Mechanisms for peer selection and their properties receive considerable attention within economics, political science, and computer science [1, 4–9].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "Mechanisms for peer selection and their properties receive considerable attention within economics, political science, and computer science [1, 4–9].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "Mechanisms for peer selection and their properties receive considerable attention within economics, political science, and computer science [1, 4–9].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "Mechanisms for peer selection and their properties receive considerable attention within economics, political science, and computer science [1, 4–9].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Mechanisms for peer selection and their properties receive considerable attention within economics, political science, and computer science [1, 4–9].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 7,
      "context" : "Mechanisms for peer selection and their properties receive considerable attention within economics, political science, and computer science [1, 4–9].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 8,
      "context" : "Mechanisms for peer selection and their properties receive considerable attention within economics, political science, and computer science [1, 4–9].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "Our motivation comes from the US National Science Foundation (NSF) recent “mechanism design pilot,” which attempted to spread the review load amongst all the submitters of proposals [1, 10].",
      "startOffset" : 182,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "Our motivation comes from the US National Science Foundation (NSF) recent “mechanism design pilot,” which attempted to spread the review load amongst all the submitters of proposals [1, 10].",
      "startOffset" : 182,
      "endOffset" : 189
    }, {
      "referenceID" : 10,
      "context" : "” This mechanism used by the NSF is not strategyproof; reviewers are incentivized to guess what others are thinking, not provide their honest feedback (an idea supported by [12]).",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 11,
      "context" : "Removing the bonus may be worse, as reviewers would then be able to increase the chance of their own proposal being accepted by rating other proposals lower [13].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 12,
      "context" : "Indeed, in a comprehensive evaluation of the peer review process, Wenneras and Wold [14] wrote in Nature that “.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "However, following Gibbard and Satterthwaite [15, 16], any “reasonable” mechanism based on voting will not be strategyproof unless it is a dictatorship.",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "Another option is to use a mechanism like the Page Rank algorithm that use Markov chains to compute a ranking of the agents [17].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "Young [18] motivates the problem as follows:“This surprisingly difficult problem has concerned statesmen, political analysts and mathematicians for over two hundred years.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 16,
      "context" : "Most empirical evaluations of peer review and peer selection study the effectiveness and limits of the system; typically by assembling large corpora of peer reviewed proposals and cross examining them with new panels or review processes [19, 20].",
      "startOffset" : 237,
      "endOffset" : 245
    }, {
      "referenceID" : 17,
      "context" : "Most empirical evaluations of peer review and peer selection study the effectiveness and limits of the system; typically by assembling large corpora of peer reviewed proposals and cross examining them with new panels or review processes [19, 20].",
      "startOffset" : 237,
      "endOffset" : 245
    }, {
      "referenceID" : 12,
      "context" : "Questions of bias, nepotism, sexism, cronyism, among other issues, have received extensive coverage, and have been substantiated to varying degrees, in the literature [14, 19, 21].",
      "startOffset" : 167,
      "endOffset" : 179
    }, {
      "referenceID" : 16,
      "context" : "Questions of bias, nepotism, sexism, cronyism, among other issues, have received extensive coverage, and have been substantiated to varying degrees, in the literature [14, 19, 21].",
      "startOffset" : 167,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : "Questions of bias, nepotism, sexism, cronyism, among other issues, have received extensive coverage, and have been substantiated to varying degrees, in the literature [14, 19, 21].",
      "startOffset" : 167,
      "endOffset" : 179
    }, {
      "referenceID" : 19,
      "context" : "Indeed, one way for the results of the review process to reflect the views of the entire scientific constituency and provide more value to community is to increase the number of reviewers [22, 23].",
      "startOffset" : 188,
      "endOffset" : 196
    }, {
      "referenceID" : 20,
      "context" : "Indeed, one way for the results of the review process to reflect the views of the entire scientific constituency and provide more value to community is to increase the number of reviewers [22, 23].",
      "startOffset" : 188,
      "endOffset" : 196
    }, {
      "referenceID" : 0,
      "context" : "The criticism that prominent peer selection mechanisms such as those under consideration by American and European funding bodies [1, 10] are not strategyproof [13] has underscored the need to devise mechanisms with better incentive properties.",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "The criticism that prominent peer selection mechanisms such as those under consideration by American and European funding bodies [1, 10] are not strategyproof [13] has underscored the need to devise mechanisms with better incentive properties.",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "The criticism that prominent peer selection mechanisms such as those under consideration by American and European funding bodies [1, 10] are not strategyproof [13] has underscored the need to devise mechanisms with better incentive properties.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "The literature most directly relevant to this article is a series of papers on strategyproof (impartial) selection [4, 7].",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "The literature most directly relevant to this article is a series of papers on strategyproof (impartial) selection [4, 7].",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Most of the work on strategyproof peer selection focuses on the setting in which agents simply approve (nominate) a subset of agents [4, 6, 7, 24] with the latter three restricting attention to the setting in which exactly one agent is selected (k = 1).",
      "startOffset" : 133,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "Most of the work on strategyproof peer selection focuses on the setting in which agents simply approve (nominate) a subset of agents [4, 6, 7, 24] with the latter three restricting attention to the setting in which exactly one agent is selected (k = 1).",
      "startOffset" : 133,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "Most of the work on strategyproof peer selection focuses on the setting in which agents simply approve (nominate) a subset of agents [4, 6, 7, 24] with the latter three restricting attention to the setting in which exactly one agent is selected (k = 1).",
      "startOffset" : 133,
      "endOffset" : 146
    }, {
      "referenceID" : 21,
      "context" : "Most of the work on strategyproof peer selection focuses on the setting in which agents simply approve (nominate) a subset of agents [4, 6, 7, 24] with the latter three restricting attention to the setting in which exactly one agent is selected (k = 1).",
      "startOffset" : 133,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "[8] presented an interesting strategyproof mechanism (Credible Subset) that performs well when each agent reviews a very small number of agents relative to the total number of agents.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 22,
      "context" : "Other recent work focused on tradeoffs between different axioms concerning peer selection [25, 26].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 23,
      "context" : "Other recent work focused on tradeoffs between different axioms concerning peer selection [25, 26].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "The peer selection problem is closely related to peer-based grading/marking [2, 17, 23, 27–30] especially when students are graded based on percentile scores.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "The peer selection problem is closely related to peer-based grading/marking [2, 17, 23, 27–30] especially when students are graded based on percentile scores.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "The peer selection problem is closely related to peer-based grading/marking [2, 17, 23, 27–30] especially when students are graded based on percentile scores.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 24,
      "context" : "The peer selection problem is closely related to peer-based grading/marking [2, 17, 23, 27–30] especially when students are graded based on percentile scores.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "The peer selection problem is closely related to peer-based grading/marking [2, 17, 23, 27–30] especially when students are graded based on percentile scores.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 26,
      "context" : "The peer selection problem is closely related to peer-based grading/marking [2, 17, 23, 27–30] especially when students are graded based on percentile scores.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 27,
      "context" : "The peer selection problem is closely related to peer-based grading/marking [2, 17, 23, 27–30] especially when students are graded based on percentile scores.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : ", [17] and [10]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : ", [17] and [10]).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "This class of mechanisms is a straightforward generalization of the Partition mechanism [4] (and in an early version of [8]) and is strategyproof.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "This class of mechanisms is a straightforward generalization of the Partition mechanism [4] (and in an early version of [8]) and is strategyproof.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "Credible Subset [8]: let T be the set of agents who have the top k scores, as in Vanilla.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "There are a number of other mechanisms that are tailor-made for k = 1 and when agents only mark approval of a subset of agents: Partition [7]; Permutation [6]; and Slicing [24].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "There are a number of other mechanisms that are tailor-made for k = 1 and when agents only mark approval of a subset of agents: Partition [7]; Permutation [6]; and Slicing [24].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 21,
      "context" : "There are a number of other mechanisms that are tailor-made for k = 1 and when agents only mark approval of a subset of agents: Partition [7]; Permutation [6]; and Slicing [24].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 28,
      "context" : "Dividing a Dollar[31]: Each agent i has a value vi(j) of his estimation of how much of the resource j should receive, and these values are normalized so that ∑ j∈N\\{i} vi(j) = 1/n.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : "The problem is ubiquitous in apportionment settings such as proportional representation of seats in the US congress, European Parliament, and the German Bundestag as well as various other committee selection settings [18, 32–36].",
      "startOffset" : 217,
      "endOffset" : 228
    }, {
      "referenceID" : 29,
      "context" : "The problem is ubiquitous in apportionment settings such as proportional representation of seats in the US congress, European Parliament, and the German Bundestag as well as various other committee selection settings [18, 32–36].",
      "startOffset" : 217,
      "endOffset" : 228
    }, {
      "referenceID" : 30,
      "context" : "The problem is ubiquitous in apportionment settings such as proportional representation of seats in the US congress, European Parliament, and the German Bundestag as well as various other committee selection settings [18, 32–36].",
      "startOffset" : 217,
      "endOffset" : 228
    }, {
      "referenceID" : 31,
      "context" : "The problem is ubiquitous in apportionment settings such as proportional representation of seats in the US congress, European Parliament, and the German Bundestag as well as various other committee selection settings [18, 32–36].",
      "startOffset" : 217,
      "endOffset" : 228
    }, {
      "referenceID" : 32,
      "context" : ", [37]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 33,
      "context" : "Randomization has been used in various settings such as voting and fair allocation of indivisible goods to achieve ex ante as well as procedural fairness [38–40].",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 34,
      "context" : "Randomization has been used in various settings such as voting and fair allocation of indivisible goods to achieve ex ante as well as procedural fairness [38–40].",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 35,
      "context" : "Randomization has been used in various settings such as voting and fair allocation of indivisible goods to achieve ex ante as well as procedural fairness [38–40].",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 36,
      "context" : "Hence, some suggested approaches to this problem require multiple rounds of randomization and also do not enumerate the possible ex post outcomes (there may be an exponential number of them) [41].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 28,
      "context" : "Dollar Raffle computes the relative fractions of how much of a dollar each agent should get via the Dollar mechanism [31].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : ", [6])).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 37,
      "context" : "Using Python and extending code from PrefLib [43] we have implemented the Exact Dollar Partition, Credible Subset,",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 38,
      "context" : ", [44]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "By focusing on a target domain, the NSF Mechanism Design Pilot [10, 11], we can draw focused conclusions from our simulations.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 39,
      "context" : "n} scoring matrix (profile) using a Mallows Model to generate the ordinal evaluation [45, 46].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 40,
      "context" : "n} scoring matrix (profile) using a Mallows Model to generate the ordinal evaluation [45, 46].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : ", [6]): simply running the algorithm several times destroys its strategyproofness.",
      "startOffset" : 2,
      "endOffset" : 5
    } ],
    "year" : 2016,
    "abstractText" : "S ince the beginning of civilization, societies have been selecting small groups from within. Athenian society, for example, selected a random subset of citizens to participate in the Boule, the council of citizens which ran daily affairs in Athens. Peer review, evaluation, and selection has been the foundational process by which scientific funding bodies have selected the best subset of proposals for funding [1] and peer evaluation is becoming increasingly popular and necessary to scale grading in MOOCs [2, 3]. In these settings, however, we do not wish to select an arbitrary subset of size k, but the “best k”, and we need, therefore, a procedure in which the candidates are rated according to the opinions of the group. In peer selection problems we are not seeking an external, “independent” agent to make choices, but desire a crowdsourced approach, in which participants are those making the selection. Mechanisms for peer selection and their properties receive considerable attention within economics, political science, and computer science [1, 4–9]. Our motivation comes from the US National Science Foundation (NSF) recent “mechanism design pilot,” which attempted to spread the review load amongst all the submitters of proposals [1, 10]. The program uses “reviewers assigned from among the set of PIs whose proposals are being reviewed.” Reviewers’ own proposals get “supplemented with “bonus points” depending upon the degree to which his or her ranking agrees with the consensus ranking ([11], Page 46).” This mechanism used by the NSF is not strategyproof; reviewers are incentivized to guess what others are thinking, not provide their honest feedback (an idea supported by [12]). Removing the bonus may be worse, as reviewers would then be able to increase the chance of their own proposal being accepted by rating other proposals lower [13]. In either case, reviewers can benefit from reporting something other than their truthful values. When agents have the incentive to misrepresent their truthful reports, the effect on the results of the aggregation or selection mechanism can be problematic. Indeed, in a comprehensive evaluation of the peer review process, Wenneras and Wold [14] wrote in Nature that “...the development of peerreview systems with some built-in resistance to the weakness of human nature is therefore of high priority.” We propose a novel strategyproof (or impartial) mechanism, where agents may never gain by being insincere. There are many reasons to prefer a mechanism which is strategyproof: first, the mechanism does not favor “sophisticated” agents who have the expertise to behave strategically. Second, agents with partial or no knowledge about the rankings of other agents are not disadvantaged when using a strategyproof mechanism. Third, normative properties of a mechanism typically assume a sincere behavior. If agents act strategically, we may lose some desirable normative properties. Fourth, it is easier to persuade people to use a strategyproof mechanism than one which can be (easily) manipulated. Note that while strategyproofness does not handle all biases agents may have, it eliminates an obvious “weakness in human nature.” To achieve strategyproofness we could use a lottery (as in the Athenian democracy). However, this method does not select based on merit. A different option is to use a mechanism based on a voting rule. However, following Gibbard and Satterthwaite [15, 16], any “reasonable” mechanism based on voting will not be strategyproof unless it is a dictatorship. Another option is to use a mechanism like the Page Rank algorithm that use Markov chains to compute a ranking of the agents [17]. However, such mechanisms are also not strategyproof.",
    "creator" : "LaTeX with hyperref package"
  }
}