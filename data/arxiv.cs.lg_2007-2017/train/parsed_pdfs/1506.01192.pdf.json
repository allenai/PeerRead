{
  "name" : "1506.01192.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Personalizing A Universal Recurrent Neural Network Language Model with User Characteristic Features by Crowdsouring over Social Networks",
    "authors" : [ "Bo-Hsiang Tseng", "Hung-Yi Lee", "Lin-Shan Lee" ],
    "emails" : [ "r02942037@ntu.edu.tw,", "lslee@gate.sinica.edu.tw" ],
    "sections" : [ {
      "heading" : null,
      "text" : "recognizer becomes more realizable today and highly attractive. Each mobile device is primarily used by a single user, so it’s possible to have a personalized recognizer well matching to the characteristics of individual user. Although acoustic model personalization has been investigated for decades, much less work have been reported on personalizing language model, probably because of the difficulties in collecting enough personalized corpora. Previous work used the corpora collected from social networks to solve the problem, but constructing a personalized model for each user is troublesome. In this paper, we propose a universal recurrent neural network language model with user characteristic features, so all users share the same model, except each with different user characteristic features. These user characteristic features can be obtained by crowdsouring over social networks, which include huge quantity of texts posted by users with known friend relationships, who may share some subject topics and wording patterns. The preliminary experiments on Facebook corpus showed that this proposed approach not only drastically reduced the model perplexity, but offered very good improvement in recognition accuracy in n-best rescoring tests. This approach also mitigated the data sparseness problem for personalized language models. Index Terms: Recurrent Neural Network, Personalized Language Modeling, Language Modeling, Social Network, LM adaptation"
    }, {
      "heading" : "1. Introduction",
      "text" : "Personalization of various application and services towards each individual user has been a major trend nowadays. Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6]. In the area of speech recognition, with popularity of mobile devices such as smart phones and wearable clients, personalized recognizer becomes much more realizable and highly attractive. Each mobile device is primarily used by a single user, and can be connected to a personalized recognizer stored in the cloud with much better performance, because this recognizer can be well matched to the linguistic characteristics of the individual user. Acoustic model adaptation [7, 8, 9] is the part of speech recognition in which personalization has been investigated for decades and very impressive improvements have been achieved with many approaches based on either HMM/GMM or CD-DNNHMM [10]. However, there has been much less work done on language model (LM) personalization. Although LM adaptation [11, 12, 13] primarily focus on the problem of crossdomain or cross-genre linguistic mismatch, while the cross individual linguistic mismatch is often ignored. A good reason\nfor this is probably the difficulties in collecting personalized corpora for personalization of LMs. However, the situation has been changed in recent years. Nowadays, many individuals post large quantities of texts over the social networks, on which huge quantities of posted texts with known authors and given friend relationships among the authors are available. It is therefore possible to train personalized LMs because it may be reasonable to assume that users with close friend relationships may share some common subject topics, wording habits and linguistic patterns.\nPersonalization of LM was proposed and investigated based on both N-gram-based LM [14] and recurrent neural network(RNNLM) [15] in the very limited previous works. In these previous works, the text posted by many individual users and other information (such as friend relationships among users) were collected from the social networks. A background LM (either N-gram-based or RNN-based) was then adapted toward an individual user’s wording patterns by incorporating social texts that the target user and other users had posted, considering different aspects of relationships and similarities between the users. In these previous works, the personalization was realized by obtaining an LM for each individual. There are inevitable shortcomings with this framework. First, even with help of the social networks, the text corpora helpful to a particular user for adapting a background LM towards a personalized LM is always limited. As a result, the personalized LM thus obtained may easily overfit to the limited data, and therefore yield relatively poor performance on the new data of the target user. Second, to train and store a personalized LM for every user is time-consuming and memory-intensive in any case, especially when the number of users can be very large in the future.\nConsidering the defects in the previous framework as mentioned above, we propose a new paradigm for personalizing LMs in this paper based on RNNLM. In the conventional RNNLM [16, 17, 18], the 1-of-N encoding of wach word is taken as the input of the RNN, and then given the history word sequence, RNN outputs the estimated probability distribution for the next word. In the new paradigm proposed here, however, each user is represented by a feature vector encoding some characteristics of the user, and this feature vector augments the 1-of-N encoding feature of each word. A universal RNNLM is thus based on these user features augmenting word representations for the texts over social networks by a large number of users. The standard training method is used, except now the same words produced by different users in the training set augmented by different user characteristic features. For each new user, his characteristic feature has to be extracted for extending the 1-of-N word encoding, with which the universal RNNLM can be used to recognize his speech. Because the same words produced by different users are augmented with ar X iv :1\n50 6.\n01 19\n2v 1\n[ cs\n.C L\n] 3\nJ un\n2 01\n5\ndifferent features, given the same history word sequence, the universal RNNLM may predict different distribution of the next word for different users. In this way, the personalization can be achieved even though all users share the same universal RNNLM. This universal RNNLM trained from the social text produced by many users may not suffer from overfitting because a very large training set can be obtained by aggregating the social texts of many users. Moreover, since the recognizer for each user only needs to have his characteristic features rather than the while model, the new paradigm is time-saving for training and memory-saving in real implementation. The concept of modifying input features for personalization is similar to the i-vectors used in deep neural network (DNN) -based acoustic models [19, 20], in which the i-vector of each speaker is used to extend the acoustic features like MFCC. Preliminary experiments showed that the proposed method not only reduced the model perplexity, but reduced the word error rate in n-best rescoring tests. In addition, it is found that the information from the friends of the target user on social networks can be also helpful in extracting the characteristic feature for the target user."
    }, {
      "heading" : "2. Scenario of LM personalization",
      "text" : "Crowdsourcing [21, 22] has varying definitions and was widely applied in various tasks. For example, a crowdsourcing approach was proposed to collect queries for information retrieval considering temporal information [23]. The MIT movie browser [24, 25] relied on Amazon Mechanical Turk to build a crowd-supervised spoken language system. In this work, a cloud-based application helping users access their social network via voice was implemented and viewed as a crowdsourcing platform for collecting personal data. When the user logs into his Facebook account, he may choose to grant this application the authority to collect his acoustic and linguistic data for personalization of the voice access service. The user should enjoy the benefits of better recognition accuracy brought by the personalized recognizer due to the crawled data.\nThe scenario of the proposed approach is in Fig. 1. For a user A, represented as an red figure in the left part of Fig. 1, the texts of his posts on the social network is crawled to form the personal corpus of the user (the red circle in Fig. 1). Besides, the posts of all friends of user A in the social network represented as green figures, are also collected to form the friend corpus of user A (the red cloud surrounding the circle). Considering the fact that all these collected personal corpora are small, they were used for LM adaptation in the previous work [11], in which the personal corpus and friend corpus of a user are treated as the adaptation corpus to adapt a background LM trained with a large background corpus not sufficiently related to the target task. However, such LM adaptation suffers from overfitting due to limited adaptation data and heavy training/memory load as mentioned above."
    }, {
      "heading" : "3. Proposed Approach",
      "text" : "In this paper, instead of building a personalized RNNLM for each user, a single universal RNNLM is used by all users instead. As shown in the right part of Figure 1, the universal RNNLM comprises three layers: the input layer, the hidden layer, and the output layer are used [16], except that the input layer is the concatenation of the word vector w(t) representing the t-th word in a sentence using an 1-of-N encoding and an additional user characteristic feature f. The user characteristic feature f is connected to both the hidden layer s(t) and output\nlayers y(t) 1. This feature f help the model take the specific user into account. The network weights to be learned are the matrices W,F, S,G and O in the right of Figure 1. The posts from a large group of users serve as the training data for the universal RNNLM."
    }, {
      "heading" : "3.1. Extraction of User Characteristic Feature",
      "text" : "In order to have the user characteristic feature f reflect the semantics of the frequently-mentioned topics of a user, it is natured to represent the user characteristic f as the topic distributions of the social texts that the user posted on the social network.\nHowever, in our preliminary experiments we found that simply using such topic distributions as user characteristic features led to poor results because in the real world a user has a wide variety of topics and switches topics very frequently. To address this issue, we make the user characteristic feature not only user-dependent, but also sentence-dependent. We first trained a topic model from a large corpus, and then the topic model is used to infer the topic distribution of all sentences in the social network data set. During the training phase of RNNLM, given a sentence of a particular user (e.g. sentence i of user A in left of Fig. 1), the N sentences within that user’s personal corpus (personal corpus of user A, the red circle) or friend corpus (friend corpus of user A, the red cloud) with smallest Euclidean distances in terms of topic distribution to the sentence considered (sentence i of user A in Fig. 1) are found, and the averaged topic distribution of these N sentences serves as the user characteristic feature (fA). It turns out that every training sentence i has a user characteristic feature, and all these features are used to train the universal RNNLM. During testing, when an utterance of a particular user (user B in Fig. 1) is entered, its N-best list is first generated. The topic distribution can be inferred from the N-best list with the topic model, and the N most similar sentences can then be obtained from the personal or friend corpus of the user (user B) to form the characteristic feature (fB)."
    }, {
      "heading" : "3.2. Effect of user characteristic feature on RNNLM",
      "text" : "Here we use real example from the Facebook data to show the effect of the user characteristic features on RNNLM. User A mentioned a lot about issues related to ”coffee” in the Facebook data, while User B never did so. This gave very different user characteristic features for the two users. Given a sentence ”A bottle of milk can make 3 cups of latte” which was more likely to be produced by User A, the perplexity evaluated by the conventional RNNLM and the personalized RNNLM with different user characteristic features are listed in Table 1. The conventional RNNLM is in row (a). We see that the personalized RNNLM with the user characteristic feature fA of User A drastically decreased the PPL (152 vs 355) as in row (b), while with the user characteristic feature fB of User B increased the perplexity significantly (604 vs 355) in row (c).\n1This structure is parallel to the context dependent RNNLM variant [18], except that the context feature in the input layer is replaced by the user characteristic feature f."
    }, {
      "heading" : "4. Experiments",
      "text" : ""
    }, {
      "heading" : "4.1. Experimental Setups",
      "text" : ""
    }, {
      "heading" : "4.1.1. Corpus & LMs",
      "text" : "Our experiments are conducted on a crawled Facebook corpus. A total of 42 users logged in and authorized this project to collect their messages and basic information for research. These 42 users were our target users, and divided into 3 groups for cross validation, i.e., to train the universal LM using the data of two groups and test those for the rest. Furthermore, with their consent, the observable public data (personal and friends corpus) of these 42 target users were also available to the experiments. This gave the personal data of 93,000 anonymous people and a total of 2.4 million sentences. The number of sentences for each user among the 93,000 ranged from 1 to 8,566 with a mean of 25.7, comprising 10.6 words (Chinese, English, or mixed) per sentence in average. A total of 12,000 sentences for the 42 target users was taken as the testing set, and among them 948 produced by the respective target users were taken as testing utterances for ASR experiments.\nFor the background LM, 500k sentences were collected from another popular social networking site called Plurk to train the topic model. There were both Chinese and English words in the Plurk data with a mixing rate of 9:1. The topic model we used is trained by Latent Dirichlet Allocation (LDA) [26] with Mallet toolkit [27], taking each sentence as a document. The modified Kneser-Ney algorithm [28] was used for the N-gram LM smoothing. The most frequent 18,000 English words and\n46,000 Chinese words appearing in the corpus were selected to form the lexicon. The SRILM [29] toolkit was used for the Ngram LM training and adaptation, while RNNLM toolkit [30] is used for RNNLM here. The hidden later size in the RNNLM is fixed at 50."
    }, {
      "heading" : "4.1.2. N-best rescoring",
      "text" : "We used lattices produced using the HTK toolkit [31] to generate 1,000-best lists for rescoring. The LM used to generate the n-best lists was a trigram model, adapted with the personal corpora and friend corpora with Kneser-Ney smoothing (KN3). The Mandarin triphone models used for first-pass decoding were trained on the ASTMIC corpus [ref], whereas the English triphone models were trained on the Sinica Taiwan English corpus [ref], both including hundreds of speakers. Both sets of models were adapted by unsupervised MLLR."
    }, {
      "heading" : "4.2. Experimental Results",
      "text" : ""
    }, {
      "heading" : "4.2.1. Extraction of User Characteristic Feature",
      "text" : "As mentioned in Section 3.1, only the N sentences most similar to the sentence considered are used to build the user characteristic feature. The perplexity with different N (out of the user corpus plus friend corpus) and different number of topics for LDA is shown in Fig. 2 2. In Fig. 2, there was almost no difference between N=1 and N=2, but as N increased beyond 2, the perplexity went higher implying very wide topic variety even for the same user and his friends. We thus chose N=1 in the following experiments."
    }, {
      "heading" : "4.2.2. Perplexity and rescoring result",
      "text" : "The results of the perplexity and recognition accuracy are listed in table 2. Part (a) is for Kneser-Ney 3grams, row (a-1) without adaptation, while rows (a-2) and (a-3) adapted by personal corpus (labelled with P) or personal plus friend corpora (labelled with P+F) respectively. It is clear that adapting Kneser-Ney\n2Only one-tenth of personal corpus is used in this preliminary experiment.\n3gram LM with personal and friend corpora are helpful (rows (a-2), (a-3) vs (a-1)). Part (b) is the RNNLM baseline without adaptation. Here the RNNLM baseline yielded worse perplexity compared with Kneser-Ney 3grams (parts (b) vs (a) in perplexity) 3, but it improved recognition accuracy (parts (b) vs (a) in accuracy). Part (c) is for RNNLM personalization with adaptation in previous work [15]. We find that adaptation improved both perplexity and accuracy (parts (c) vs (b)), and the friend corpus is helpful (rows (c-2) vs (c-1)). Part (d) is for the proposed approach with user characteristic feature (UCF) respectively estimated (N=1) from personal corpus (row (d-1), labelled with P), friend corpus (row (d-2), labelled with F) and both personal and friend corpora (row (d-3), labelled with P+F). We see the proposed personalization approach achieved better results than the previous work of adaptation in both perplexity and accuracy (parts (d) vs (c)). Also, the friend corpus brought more improvement than personal corpus (rows (d-2) v.s. (d1)), probably because the friend corpus included much more data, so better user characteristic can be extracted. Using both personal and friend corpus improved the perplexity but slightly decreased the accuracy (rows (d-3) vs (d-2)), probably because the friend corpus is large enough to exhibit the user information, so adding personal corpus was not really beneficial. In the best result of row (d-2), the relative perplexity can be reduced to 31.4% compared to the backgroud N-grams (a-1) even when the hidden later size is only 50, and 46.7% relative reduction compared to RNNLM baseline of row (b). The absolute word error rate reduction of 0.69% is achieved (rows (d-2) vs (b))."
    }, {
      "heading" : "4.2.3. Sizes of personal corpora",
      "text" : "As mentioned above, the previous approach of adapting the background model into a personalized model may suffer from overfitting with limited adaptation data and thus may yield poor performance on the new data of the particular user. This is verified in Fig. 3. The horizontal axis of Fig. 3 is the percentage of the original personal corpora used, and 1.00 means using the original personal corpora or the cases row (c-1) and (d-1) in Table 2. We see in Fig. 3 that as less data were available, the proposed approach (RNNLM/UCF, P) had perplexity increased much slower and a much more stable accuracy, while the pre-\n3With the relatively small hidden layer size of 50 in the preliminary experiments, it is possible that RNNLM obtained perplexity higher than n-gram-based LM [15].\nviously proposed approach (RNNLM/adapt, P) had perplexity increased much faster and accuracy degraded seriously."
    }, {
      "heading" : "5. Conclusions",
      "text" : "In this paper, we proposed a new framework for personalizing a universal RNNLM using data crawled over social networks. The proposed approach is based on the user characteristic feature extracted from the user corpus and friend corpus, which is not only user-dependent but sentence-dependent. This universal RNNLM can predict different word distributions for different users given the same content. Experiments showed that very good improvements were achieved in both perplexity and accuracy, and the proposed approach is much more robust to the data sparseness than the previous work."
    }, {
      "heading" : "6. References",
      "text" : "[1] G. Zweig and C. Shuang yu, “Personalizing model m for voice-\nsearch,” in Proc. on InterSpeech, 2011.\n[2] M. Speretta and S. Gauch, “Personalized search based on user search histories,” in Proc. on Web Intelligence, 2005.\n[3] Y. H. Cho, J. K. Kim, and S. H. Kim, “A personalized recommender system based on web usage mining and decision tree induction,” Expert Systems with Applications, 2002.\n[4] Y. Koren, R. Bell, and C. Volinsky, “Matrix factorization techniques for recommender systems,” Computer, 2009.\n[5] F. Walter, S. Battiston, and F. Schweitzer, “A model of a trustbased recommendation system on a social network,” Autonomous Agents and Multi-Agent Systems, 2008.\n[6] M.-H. Park, J.-H. Hong, and S.-B. Cho, “Location-based recommendation system using bayesian users preference model in mobile devices,” in Ubiquitous Intelligence and Computing, 2007.\n[7] C. J. Leggetter and P. C. Woodland, “Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models,” Computer Speech and Language, 1995.\n[8] P. C. Woodland, “Speaker adaptation for continuous density hmms: A review,” in Proc. on ITRW on Adaptation Methods for Speech Recognition, 2001.\n[9] P. C. Woodland, “Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains,” IEEE Transactions on Speech and Audio Processing, 1994.\n[10] Hinton, G.; Li Deng; Dong Yu; Dahl, G.E.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T.N.; Kingsbury, B., “Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups,” Signal Processing Magazine, IEEE , vol.29, no.6, pp.82,97, Nov. 2012\n[11] PJ. R. Bellegarda, “Statistical language model adaptation: review and perspectives,” Speech Communication, 2004.\n[12] A. Heidel and L.-S. Lee, “Robust topic inference for latent semantic language model adaptation,” in Proc. on ASRU, 2007.\n[13] H. Bo-June and J. Glass, “Style and topic language model adaptation using hmm-lda,” in Proc. on EMNLP, 2006.\n[14] T.-H. Wen, H.-Y. Lee, T.-Y. Chen, and L.-S. Lee, “Personalized language modeling by crowd sourcing with social network data for voice access of cloud applications,” in Proc. on IEEE SLT workshop, 2012.\n[15] T.-H. Wen, A. Heidel, H.-Y. Lee, Yu Tsao, and L.-S. Lee, “Recurrent Neural Network Based Language Model Personalization by Social Network Crowdsourcing,” in Proc. on InterSpeech, 2013.\n[16] T. Mikolov, M. Karafit, L. Burget, J. Cernock, and S. Khudanpur, “Recurrent neural network based language model,” in Proc. on InterSpeech, 2010.\n[17] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur, “Extensions of recurrent neural network language model,” in Proc. on ICASSP, 2011.\n[18] T. Mikolov and G. Zweig, “Context dependent recurrent neural network language model,” in Proc. on IEEE SLT workshop, 2012.\n[19] Saon, G.; Soltau, H.; Nahamoo, D.; Picheny, M., , “Speaker adaptation of neural network acoustic models using i-vectors ,” in Proc. on ASRU, 2013.\n[20] Gupta, V.; Kenny, P.; Ouellet, P.; Stafylakis, T., “I-vector-based speaker adaptation of deep neural networks for French broadcast audio transcription,” in Proc. on ICASSP, 2014.\n[21] A. Doan, R. Ramakrishnan, and A. Y. Halevy, “Crowdsourcing systems on the world-wide web,” Communications of the ACM, 2011.\n[22] Munro and Robert, “Crowdsourcing and language studies: the new generation of linguistic data,” in Proc. on NAACL, 2010.\n[23] K. Berberich, S. Bedathur, O. Alonso, and G. Weikum, “A language modeling approach for temporal information needs,” in Advances in Information Retrieval, 2010.\n[24] J. Liu, S. Cyphers, P. Pasupat, I. McGraw, and J. Glass, “A conversational movie search system based on conditional random field,” in Proc. on InterSpeech, 2012.\n[25] I. McGraw, S. Cyphers, P. Pasupat, J. Liu, and J. Glass, “Automating crowd-supervised learning for spoken language systems,” in Proc. on InterSpeech, 2012.\n[26] David M. Blei, Andrew Y. Ng, and Michael I. Jordan, “Latent dirichlet allocation,” J. Mach. Learn. Res., 2003.\n[27] McCallum, Andrew Kachites, “MALLET: A Machine Learning for Language Toolkit,” http://mallet.cs.umass.edu. 2002.\n[28] F. James, “Modified kneser-ney smoothing of n-gram models modified kneser-ney smoothing of n-gram models,” Tech. Rep., 2000.\n[29] A. Stolcke, “Srilm - an extensible language modeling toolkit,” in Proc. on Spoken Language Processing, 2002.\n[30] T. Mikolov, S. Kombrink, A. Deoras, L. Burget, and J. Cernocky, “Rnnlm - recurrent neural network language modeling toolkit,” in Proc. on ASRU, 2011.\n[31] S. J. Young, D. Kershaw, J. Odell, D. Ollason, V. Valtchev, and P.Woodland, The HTK Book Version 3.4. Cambridge University Press, 2006."
    } ],
    "references" : [ {
      "title" : "Personalizing model m for voicesearch",
      "author" : [ "G. Zweig", "C. Shuang yu" ],
      "venue" : "Proc. on InterSpeech, 2011.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Personalized search based on user search histories",
      "author" : [ "M. Speretta", "S. Gauch" ],
      "venue" : "Proc. on Web Intelligence, 2005.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A personalized recommender system based on web usage mining and decision tree induction",
      "author" : [ "Y.H. Cho", "J.K. Kim", "S.H. Kim" ],
      "venue" : "Expert Systems with Applications, 2002.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Matrix factorization techniques for recommender systems",
      "author" : [ "Y. Koren", "R. Bell", "C. Volinsky" ],
      "venue" : "Computer, 2009.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A model of a trustbased recommendation system on a social network",
      "author" : [ "F. Walter", "S. Battiston", "F. Schweitzer" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems, 2008.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Location-based recommendation system using bayesian users preference model in mobile devices",
      "author" : [ "M.-H. Park", "J.-H. Hong", "S.-B. Cho" ],
      "venue" : "Ubiquitous Intelligence and Computing, 2007.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models",
      "author" : [ "C.J. Leggetter", "P.C. Woodland" ],
      "venue" : "Computer Speech and Language, 1995.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Speaker adaptation for continuous density hmms: A review",
      "author" : [ "P.C. Woodland" ],
      "venue" : "Proc. on ITRW on Adaptation Methods for Speech Recognition, 2001.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains",
      "author" : [ "P.C. Woodland" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing, 1994.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups",
      "author" : [ "G. Hinton", "Li Deng", "Dong Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury" ],
      "venue" : "Signal Processing Magazine, IEEE , vol.29, no.6, pp.82,97, Nov. 2012",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Statistical language model adaptation: review and perspectives",
      "author" : [ "PJ.R. Bellegarda" ],
      "venue" : "Speech Communication, 2004.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Robust topic inference for latent semantic language model adaptation",
      "author" : [ "A. Heidel", "L.-S. Lee" ],
      "venue" : "Proc. on ASRU, 2007.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Style and topic language model adaptation using hmm-lda",
      "author" : [ "H. Bo-June", "J. Glass" ],
      "venue" : "Proc. on EMNLP, 2006.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Personalized language modeling by crowd sourcing with social network data for voice access of cloud applications",
      "author" : [ "T.-H. Wen", "H.-Y. Lee", "T.-Y. Chen", "L.-S. Lee" ],
      "venue" : "Proc. on IEEE SLT workshop, 2012.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Recurrent Neural Network Based Language Model Personalization by Social Network Crowdsourcing",
      "author" : [ "T.-H. Wen", "A. Heidel", "H.-Y. Lee", "Yu Tsao", "L.-S. Lee" ],
      "venue" : "Proc. on InterSpeech, 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov", "M. Karafit", "L. Burget", "J. Cernock", "S. Khudanpur" ],
      "venue" : "Proc. on InterSpeech, 2010.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Extensions of recurrent neural network language model",
      "author" : [ "T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur" ],
      "venue" : "Proc. on ICASSP, 2011.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Context dependent recurrent neural network language model",
      "author" : [ "T. Mikolov", "G. Zweig" ],
      "venue" : "Proc. on IEEE SLT workshop, 2012.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Speaker adaptation of neural network acoustic models using i-vectors",
      "author" : [ "G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny" ],
      "venue" : "Proc. on ASRU, 2013.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "I-vector-based speaker adaptation of deep neural networks for French broadcast audio transcription",
      "author" : [ "V. Gupta", "P. Kenny", "P. Ouellet", "T. Stafylakis" ],
      "venue" : "Proc. on ICASSP, 2014.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Crowdsourcing systems on the world-wide web",
      "author" : [ "A. Doan", "R. Ramakrishnan", "A.Y. Halevy" ],
      "venue" : "Communications of the ACM, 2011.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Crowdsourcing and language studies: the new generation of linguistic data",
      "author" : [ "Munro", "Robert" ],
      "venue" : "Proc. on NAACL, 2010.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A language modeling approach for temporal information needs",
      "author" : [ "K. Berberich", "S. Bedathur", "O. Alonso", "G. Weikum" ],
      "venue" : "Advances in Information Retrieval, 2010.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A conversational movie search system based on conditional random field",
      "author" : [ "J. Liu", "S. Cyphers", "P. Pasupat", "I. McGraw", "J. Glass" ],
      "venue" : "Proc. on InterSpeech, 2012.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Automating crowd-supervised learning for spoken language systems",
      "author" : [ "I. McGraw", "S. Cyphers", "P. Pasupat", "J. Liu", "J. Glass" ],
      "venue" : "Proc. on InterSpeech, 2012.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan" ],
      "venue" : "J. Mach. Learn. Res., 2003.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "MALLET: A Machine Learning for Language Toolkit",
      "author" : [ "McCallum", "Andrew Kachites" ],
      "venue" : "http://mallet.cs.umass.edu. 2002.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Modified kneser-ney smoothing of n-gram models modified kneser-ney smoothing of n-gram models",
      "author" : [ "F. James" ],
      "venue" : "Tech. Rep., 2000.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Srilm - an extensible language modeling toolkit",
      "author" : [ "A. Stolcke" ],
      "venue" : "Proc. on Spoken Language Processing, 2002.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Rnnlm - recurrent neural network language modeling toolkit",
      "author" : [ "T. Mikolov", "S. Kombrink", "A. Deoras", "L. Burget", "J. Cernocky" ],
      "venue" : "Proc. on ASRU, 2011.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "Good examples includes personalized web search [1, 2] and personalized recommendation systems [3, 4, 5, 6].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : "Acoustic model adaptation [7, 8, 9] is the part of speech recognition in which personalization has been investigated for decades and very impressive improvements have been achieved with many approaches based on either HMM/GMM or CD-DNNHMM [10].",
      "startOffset" : 26,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "Acoustic model adaptation [7, 8, 9] is the part of speech recognition in which personalization has been investigated for decades and very impressive improvements have been achieved with many approaches based on either HMM/GMM or CD-DNNHMM [10].",
      "startOffset" : 26,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "Acoustic model adaptation [7, 8, 9] is the part of speech recognition in which personalization has been investigated for decades and very impressive improvements have been achieved with many approaches based on either HMM/GMM or CD-DNNHMM [10].",
      "startOffset" : 26,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "Acoustic model adaptation [7, 8, 9] is the part of speech recognition in which personalization has been investigated for decades and very impressive improvements have been achieved with many approaches based on either HMM/GMM or CD-DNNHMM [10].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 10,
      "context" : "Although LM adaptation [11, 12, 13] primarily focus on the problem of crossdomain or cross-genre linguistic mismatch, while the cross individual linguistic mismatch is often ignored.",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "Although LM adaptation [11, 12, 13] primarily focus on the problem of crossdomain or cross-genre linguistic mismatch, while the cross individual linguistic mismatch is often ignored.",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : "Although LM adaptation [11, 12, 13] primarily focus on the problem of crossdomain or cross-genre linguistic mismatch, while the cross individual linguistic mismatch is often ignored.",
      "startOffset" : 23,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : "Personalization of LM was proposed and investigated based on both N-gram-based LM [14] and recurrent neural network(RNNLM) [15] in the very limited previous works.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "Personalization of LM was proposed and investigated based on both N-gram-based LM [14] and recurrent neural network(RNNLM) [15] in the very limited previous works.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "In the conventional RNNLM [16, 17, 18], the 1-of-N encoding of wach word is taken as the input of the RNN, and then given the history word sequence, RNN outputs the estimated probability distribution for the next word.",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "In the conventional RNNLM [16, 17, 18], the 1-of-N encoding of wach word is taken as the input of the RNN, and then given the history word sequence, RNN outputs the estimated probability distribution for the next word.",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "In the conventional RNNLM [16, 17, 18], the 1-of-N encoding of wach word is taken as the input of the RNN, and then given the history word sequence, RNN outputs the estimated probability distribution for the next word.",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "The concept of modifying input features for personalization is similar to the i-vectors used in deep neural network (DNN) -based acoustic models [19, 20], in which the i-vector of each speaker is used to extend the acoustic features like MFCC.",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 19,
      "context" : "The concept of modifying input features for personalization is similar to the i-vectors used in deep neural network (DNN) -based acoustic models [19, 20], in which the i-vector of each speaker is used to extend the acoustic features like MFCC.",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "Crowdsourcing [21, 22] has varying definitions and was widely applied in various tasks.",
      "startOffset" : 14,
      "endOffset" : 22
    }, {
      "referenceID" : 21,
      "context" : "Crowdsourcing [21, 22] has varying definitions and was widely applied in various tasks.",
      "startOffset" : 14,
      "endOffset" : 22
    }, {
      "referenceID" : 22,
      "context" : "For example, a crowdsourcing approach was proposed to collect queries for information retrieval considering temporal information [23].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "The MIT movie browser [24, 25] relied on Amazon Mechanical Turk to build a crowd-supervised spoken language system.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : "The MIT movie browser [24, 25] relied on Amazon Mechanical Turk to build a crowd-supervised spoken language system.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "Considering the fact that all these collected personal corpora are small, they were used for LM adaptation in the previous work [11], in which the personal corpus and friend corpus of a user are treated as the adaptation corpus to adapt a background LM trained with a large background corpus not sufficiently related to the target task.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "As shown in the right part of Figure 1, the universal RNNLM comprises three layers: the input layer, the hidden layer, and the output layer are used [16], except that the input layer is the concatenation of the word vector w(t) representing the t-th word in a sentence using an 1-of-N encoding and an additional user characteristic feature f.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 17,
      "context" : "1This structure is parallel to the context dependent RNNLM variant [18], except that the context feature in the input layer is replaced by the user characteristic feature f.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "The topic model we used is trained by Latent Dirichlet Allocation (LDA) [26] with Mallet toolkit [27], taking each sentence as a document.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "The topic model we used is trained by Latent Dirichlet Allocation (LDA) [26] with Mallet toolkit [27], taking each sentence as a document.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 27,
      "context" : "The modified Kneser-Ney algorithm [28] was used for the N-gram LM smoothing.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : "The SRILM [29] toolkit was used for the Ngram LM training and adaptation, while RNNLM toolkit [30] is used for RNNLM here.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 29,
      "context" : "The SRILM [29] toolkit was used for the Ngram LM training and adaptation, while RNNLM toolkit [30] is used for RNNLM here.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "Part (c) is for RNNLM personalization with adaptation in previous work [15].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "3With the relatively small hidden layer size of 50 in the preliminary experiments, it is possible that RNNLM obtained perplexity higher than n-gram-based LM [15].",
      "startOffset" : 157,
      "endOffset" : 161
    } ],
    "year" : 2017,
    "abstractText" : "With the popularity of mobile devices, personalized speech recognizer becomes more realizable today and highly attractive. Each mobile device is primarily used by a single user, so it’s possible to have a personalized recognizer well matching to the characteristics of individual user. Although acoustic model personalization has been investigated for decades, much less work have been reported on personalizing language model, probably because of the difficulties in collecting enough personalized corpora. Previous work used the corpora collected from social networks to solve the problem, but constructing a personalized model for each user is troublesome. In this paper, we propose a universal recurrent neural network language model with user characteristic features, so all users share the same model, except each with different user characteristic features. These user characteristic features can be obtained by crowdsouring over social networks, which include huge quantity of texts posted by users with known friend relationships, who may share some subject topics and wording patterns. The preliminary experiments on Facebook corpus showed that this proposed approach not only drastically reduced the model perplexity, but offered very good improvement in recognition accuracy in n-best rescoring tests. This approach also mitigated the data sparseness problem for personalized language models.",
    "creator" : "LaTeX with hyperref package"
  }
}