{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Learning Efficient Convolutional Networks through Network Slimming", "abstract": "The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.", "histories": [["v1", "Tue, 22 Aug 2017 07:35:26 GMT  (2089kb,D)", "http://arxiv.org/abs/1708.06519v1", "Accepted by ICCV 2017"]], "COMMENTS": "Accepted by ICCV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["zhuang liu", "jianguo li", "zhiqiang shen", "gao huang", "shoumeng yan", "changshui zhang"], "accepted": false, "id": "1708.06519"}
