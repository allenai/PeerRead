{
  "name" : "1601.05893.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "GeoTextTagger: High-Precision Location Tagging of Textual Documents using a Natural Language Processing Approach",
    "authors" : [ "Shawn Brunsting", "Hans De Sterck", "Remco Dolman", "Teun van Sprundel" ],
    "emails" : [ "*hans.desterck@monash.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Location tagging, also known as geotagging or geolocation, is the process of assigning geographical coordinates to input data. In this paper we present an algorithm for location tagging of textual documents. Our approach makes use of previous work in natural language processing by using a state-of-the-art part-of-speech tagger and named entity recognizer to find blocks of text which may refer to locations. A knowledge base (OpenStreatMap) is then used to find a list of possible locations for each of these blocks of text. Finally, one location is chosen for each block of text by assigning distance-based scores to each location and repeatedly selecting the location and block of text with the best score. We tested our geolocation algorithm with Wikipedia articles about topics with a well-defined geographical location that are geotagged by the articles’ authors, where classification approaches have achieved median errors as low as 11 km. However, the maximum accuracy of these approaches is limited by the class size, so future work may not yield significant improvement. Our algorithm tags a location to each block of text that was identified as a possible location reference, meaning a text is typically assigned multiple tags. When we considered only the tag with the highest distancebased score, we achieved a 10th percentile error of 490 metres and median error of 54 kilometres on the Wikipedia dataset we used. When we considered the five location tags with the greatest scores, we found that 50% of articles were assigned at least one tag within 8.5 kilometres of the article’s author-assigned true location. We also tested our approach on a set of Twitter messages that are tagged with the location from which the message was sent. This dataset is more challenging than the geotagged Wikipedia articles, because Twitter texts are shorter, tend to contain unstructured text, and may not contain information about the location from where the message was sent in the first place. Nevertheless, we make some interesting observations about potential use of our geolocation algorithm for this type of input. We explain how we use the Spark framework for data analytics to collect and process our test data. In general, classification-based approaches for location tagging may be reaching their upper limit for accuracy, but our precision-focused approach has high accuracy for some texts and shows significant potential for improvement overall."
    }, {
      "heading" : "1 Introduction",
      "text" : "This paper explores the problem of extracting location information from text. The goal is to assign high precision geographical coordinates to places that are mentioned in texts\n1/35\nar X\niv :1\n60 1.\n05 89\n3v 1\n[ cs\n.A I]\n2 2\nJa n\n20 16\nfrom various sources. This section discusses some background information and previous work in the area of location tagging. In the literature this is also referred to as geotagging or geolocation. Section 2 describes in detail the geolocation algorithm that we developed to approach this problem. We test this algorithm using geotagged Wikipedia articles in Section 3, and geotagged Twitter messages in Section 4. Finally, we summarize the results and discuss some future work in Section 5. Appendix A lists the external software that was used in the implementation of our project, Appendix B describes how the Wikipedia test data for Section 3 was obtained, and Appendix C describes how the Twitter test data for Section 4 was composed. D provides links to where our code and data is available for download online."
    }, {
      "heading" : "1.1 Previous Work using Classification Approaches",
      "text" : "Most studies in location tagging formulate it as a classification problem and use various machine learning approaches to solve it. Classification problems begin with a defined set of classes. In location tagging, these classes can take many forms including cities, countries, or areas within a range of latitude and longitude coordinates. The goal of the classification problem is to assign a class (or a ranked list of most likely classes) to each text. Note that this problem is obviously most relevant for texts that indeed talk about a specific location, or a set of locations, but one can attempt to use the resulting algorithms to geotag any text.\nFor example, several papers in the location tagging literature [1, 2] use test data sets of Wikipedia articles about topics with a well-defined geographical location, and for which the Wikipedia authors have added a geographical location tag to each article. For these geotagged Wikipedia articles, the geographical coordinates in the (primary) location tag that was added by the authors is considered the single “true location” of the text (but note that, clearly, the text may mention multiple locations). This data set can be used to train machine learning models and test their accuracy with respect to the “true location” measure.\nWing and Baldridge created classes using simple geodesic grids of varying sizes [1]. For example, one of their grids divided the surface of the Earth into 1° by 1° regions. Each of these regions was a class for their classification problem. They compared different models on various grid sizes, and tested these models using geotagged Wikipedia articles. They measured the distance between the article’s true location and the location that was predicted by the model, and their best model achieved a median distance of 11.8 km.\nRoller et al. realized that defining classes using a uniform latitude and longitude grid can be sensitive to the distribution of documents across the Earth [2]. They looked at building an adaptive grid, which attempts to define classes such that each class has a similar number of training documents. They also tested their models using geotagged Wikipedia articles, and found a median error of 11.0 km. This is an improvement over the previous work [1].\nHan et al. focused on location tagging of Twitter messages [3]. They attempted to find the primary home location of a user (which is the city in which most of their tweets occur, and is known for their training/testing data set) by assembling all their tweets into a single document, which was then used as input to their models. Their classes were major cities along with their surrounding area. Their best model that only used the text of the tweet obtained a median error of 170 km. They obtained much greater accuracy when they incorporated additional information such as the home location that was listed on the user’s profile, but this type of data is not available for general text geolocation.\nOne of the major challenges with Twitter messages is their unstructured nature. Tweets often contain spelling errors, abbreviations, and grammar mistakes which can make them difficult to interpret. Furthermore, some early work on our project discovered\n2/35\nthat most geotagged tweets (tweets that contain metadata with the GPS coordinates of the location the tweet was sent from) contain little geographic information in their text. This means that there is a very low threshold for the maximum accuracy we can expect to achieve when attempting to apply any geolocation algorithm to the text of tweets, attempting to establish where they were sent from only based on the text they contain."
    }, {
      "heading" : "1.2 Drawbacks of Classification Approaches",
      "text" : "It was thus decided that formulating our problem as a classification problem would not be feasible, as our goal was to obtain high precision. For example, if a text mentions the CN Tower in the city of Toronto then we want to return the coordinates of that building, rather than the coordinates for Toronto. Formulating this as a classification problem with this level of precision would ultimately require defining a class for every named location in the world. Furthermore, to apply these machine learning approaches we would need to have a large training set, ideally with multiple sample texts for each class. Obtaining this training data would be difficult, and even if the data was available it would likely be computationally infeasible to train such a model. In general, we did not want to use any approach that relies heavily on training data, as we want our location tagger to be as general as possible. A tagger trained with Wikipedia articles might show worse performance when given other types of text, such as news articles or tweets. Acquiring good training data that is representative of all the types of texts we want to geolocate would be very difficult. So we abandoned the classification approaches and turned towards natural language processing."
    }, {
      "heading" : "1.3 Natural Language Processing",
      "text" : "Natural language processing (NLP) encompasses the tasks that are related to understanding human language. In this paper we wish to understand location references in text, so it is natural to apply NLP techniques to this problem.\nPart of Speech Tagging Part-of-speech (POS) tagging is the process of assigning part-of-speech tags to words in a text. The tagset may vary by language and the type of text, but typically includes tags such as nouns, verbs, and adjectives. Assigning these tags is an important step towards understanding text in many NLP tasks.\nThe Stanford Natural Language Processing Group provides a state-of-the-art POS tagger based on the research of Toutanova et al. [4, 5]. They use various models that observe features of words in the text (such as capitalization, for example) and predict the most likely part-of-speech tag for each word. They train these models using a large set of annotated articles from the Wall Street Journal.\nTheir software has two main types of models. One of these types is called the left-3-words model. The tag that is applied to each word in this model is influenced by the tags assigned to some of the previous words in the sentence. The other type of model is known as the bidirectional model [5]. Tags in these models are influenced by tags on both sides of the word, not just the previous words. This makes these models more complex. They have a slight accuracy improvement over the left-3-words models, but at the cost of an order of magnitude longer running time. In this project we use Stanford’s POS tagger with one of their pre-trained left-3-words models (see also Appendix A). How our geolocation algorithm uses this software will be discussed in Section 2.3.\nNamed Entity Recognition A Named Entity Recognizer (NER) finds proper nouns in text and determines to what type of entity the noun refers. This is a valuable tool in our approach to location tagging, as we are primarily looking for locations that are named\n3/35\nentities in the text. The Stanford Natural Language Processing Group also provides a NER based on the work of Finkel et al. [6]. Their software uses a conditional random field model to identify different classes of named entities in text. Their pre-trained model that was used in this paper attempts to identify each named entity as a location, person, or organization (see Appendix A). In Section 2.3 our algorithm will use entities that are identified as locations by this software.\nNamed Entity Disambiguation After completing an NER task there is often some ambiguity. For example, if the NER software determines that some text mentions a location called London, does it refer to a city in the UK or a city in Ontario? Choosing the correct interpretation is called disambiguation.\nHabib [7] explored both the extraction and disambiguation steps for named entity recognition, along with the link between these steps. He discovered that there is a feedback loop, where results from the disambiguation step can be used to improve the extraction step. This occurs because the disambiguation step can help to identify false positives, that is, words that were identified as a named entity by the extraction step but are not true named entities. Named entities which refer to locations are called toponyms, and a major portion of Habib’s [7] work discusses the extraction and disambiguation of toponyms in semi-formal text. He disambiguated toponymns in vacation property descriptions to determine in which country the property was located. While country-level locations do not provide the level of precision we desire for this project, his work served as inspiration for many steps of the algorithm we present in Section 2.\nMore generally, named entity disambiguation has been an active topic of research throughout the past decade, see, e.g., [8–12] and references therein. For example, the AIDA system [10,13] is a general framework for collective disambiguation exploiting the prior probability of an entity being mentioned, similarity between the context of the mention and its candidates, and the coherence among candidate entities for all mentions together. AIDA uses knowledge bases such as DBpedia or YAGO and employs a graphbased algorithm to approximate the best joint mention-entity mapping. Compared to general named entity disambiguation frameworks such as AIDA, our contribution in this paper is that we present, as part of other new elements in our broader geolocation algorithm, a new named entity disambiguation approach that is specifically suited for disambiguating potential location references. It achieves high accuracy by relying on new distance-based scoring functions to measure the coherence between candidate locations. These distance-based scoring functions are used to disambiguate between multiple possible interpretations of word sequences and multiple possible locations for a potential location reference. Our approach, which uses the OpenStreetMap knowledge base [14], is specific to geolocation, and provides a high-precision and versatile method for geolocation of texts."
    }, {
      "heading" : "2 Methods",
      "text" : "Given a segment of text, we want to find locations that are mentioned in the text. For example, in a text that states “Bob drove from Waterloo to Toronto” we want to find the names Waterloo and Toronto and determine which locations are meant by those words. Does Waterloo mean a city in Ontario, Iowa, Belgium, or somewhere else? The mention of Toronto, which can refer to another city in Ontario, suggests that the correct answer is Waterloo, Ontario. This type of reasoning was developed into a geolocation algorithm that is formally described in this section.\nSection 2.1 gives a high-level overview of the algorithm. Section 2.2 defines the terminology we will use in the rest of the paper. Section 2.3 describes how names like Waterloo and Toronto are extracted from the text using part-of-speech tagging\n4/35\nand named entity recognition. Section 2.4 describes how we discover that geographic names like Waterloo can refer to multiple locations (e.g., Ontario, Iowa, or Belgium) by searching a knowledge base. Section 2.5 describes how we disambiguate between the possible locations for each name (e.g., determine that Waterloo and Toronto refer to cities in Ontario) and between multiple interpretations for word sequences. Finally, the complete geolocation algorithm is summarized in Section 2.6."
    }, {
      "heading" : "2.1 Overview",
      "text" : "Algorithm 1 gives a general high-level overview of the steps in the geolocation algorithm we propose in this paper.\nAlgorithm 1 Simplified overview of geolocation algorithm\n1: Extract potential location references from the text. This is described in detail in Section 2.3. 2: Search for each potential location reference in a knowledge base. This will give a list of result locations that are possible matches for the reference. This is described in Section 2.4. 3: For each potential location reference, determine to which of the knowledge base results it most likely refers, and resolve conflicting interpretations of sequences of words. This is called disambiguation, and is described in detail in Section 2.5.\nSection 2.2 will define some terminology which will allow us to write Algorithm 1 more precisely. Section 2.6 will summarize Section 2 to give a more detailed version of Algorithm 1."
    }, {
      "heading" : "2.2 Terminology",
      "text" : "Before we continue with the description of our geolocation algorithm, we need to precisely define our terminology.\nConsider, as an example, the following short text: “Let’s go shopping at Conestoga Mall in Waterloo. Waterloo lies between London and Guelph.” This text talks about three cities in the Province of Ontario, Canada, and about a shopping mall in one of the three cities.\n• A term is a word or sequence of adjacent words in the text. If a sequence of words occurs multiple times in the text, then every occurrence is considered a different term. Our algorithm will initially consider any sequence of words that occurs in the text as a potential location reference, i.e., it will initially consider all terms (sequences of words) of length one, two, three, etc.\n• Using part-of-speech tagging, named entity recognition, and a set of rules, the initial full set of terms (with their associated locations in the sentence) is reduced to a smaller set of terms that are judged potential location references. This (reduced) term set is called T . For the text example above, the term set T would be {Conestoga, Mall, Conestoga Mall, Waterloo, Waterloo, London, Guelph} (with the location in the sentence also attached to each term; we omit to show this for simplicity).\n• A word sequence may have multiple possible interpretations. For example, in the word sequence “Conestoga Mall”, the two-word term “Conestoga Mall” itself may refer to a location, or, alternatively, the author of the text may have intended that either (or both) of the separate terms “Conestoga” or “Mall” refer to a location.\n5/35\nTo allow the disambiguation step to decide between these two mutually exclusive interpretations for the word sequence “Conestoga Mall”, all three of the terms “Conestoga”, “Mall”, and “Conestoga Mall” are initially retained in the term set T . We say that, for the word sequence “Conestoga Mall”, the term “Conestoga Mall” conflicts (or overlaps) with the terms “Conestoga” and “Mall”, and the disambiguation step will choose one of the mutually exclusive interpretations by eliminating conflicting terms using an approach that employs distance-based scoring functions.\n• We say that each term corresponds to a phrase, where every phrase is only recorded once for the text. The set of unique phrases for the text is called P . Word sequences that occur multiple times in the text only occur once in P . For the text example above with term set T , the corresponding phrase set P would be {Conestoga, Mall, Conestoga Mall, Waterloo, London, Guelph}. (The second occurrence of “Waterloo” is removed.) Note that |P | ≤ |T |.\n• A result is a single geographical location that is listed in the knowledge base. For each phrase p ∈ P we seek a set of results Rp from the knowledge base. For any term t ∈ T , we let tphr be the phrase in P associated with the term. So Rtphr is the set of results for the phrase corresponding to term t. As a shorthand, we also use Rt instead of Rtphr . When terms occur multiple times in a text, we will assume that they refer to the same potential location. For example, a text with two occurrences of Waterloo will assume both terms refer to the same location. However, terms with phrases Waterloo Ontario and Waterloo Belgium will not be assumed to refer to the same location as they have different phrases. Therefore, our disambiguation step will attempt to determine a single most likely location for each phrase p ∈ P , using an approach that employs distance-based scoring functions.\nWe can now give an overview of the geolocation algorithm using this terminology: Algorithm 2 is a re-writing of Algorithm 1 using the terms defined above.\nAlgorithm 2 Technical overview of geolocation algorithm\n1: Find all terms in the text that are potential location references. This is done using part-of-speech tagging and named entity recognition, and gives us the sets T and P . 2: For each phrase p ∈ P , use p as a search query in the knowledge base. The results of the query are the set Rp. 3: Reduce the set T to remove terms which conflict with each other. Update the set P accordingly to reflect changes in T . For each phrase p ∈ P , match p to a single result r ∈ Rp. This is done by assigning distance-based scores to each result for every term, and selecting results with the greatest score in a greedy iterative fashion."
    }, {
      "heading" : "2.3 Location Extraction",
      "text" : "The extraction phase of the algorithm uses the part-of-speech (POS) tagger and named entity recognizer (NER) from Stanford that were described in Section 1.3. The entire text is tagged by both of these models. Based on the output of these taggers, the algorithm creates the set of terms T which represents the potential location references in the text we consider further.\nThe development of our approach was initially guided by short unstructured texts coming from classified ads for rental housing, and we found that relying on NER tags only did not provide sufficient accuracy. Indeed, if the NER were perfect, we could\n6/35\nsimply use the NER to find all locations in the text and this step of the algorithm would be complete. Instead, our approach supplements the NER tags with the POS tagger, and considers more potential location references that the ones that are provided by the NER alone. The motivation for some of the heuristic algorithmic choices made in this section is further illustrated in an example at the end of the section.\nTagging the Text The POS tagger from Stanford assigns tags to each word in the text. The full list of possible tags is given in [15], but our algorithm only looks for a subset of tags which are relevant to our problem. Some of these tags are grouped together and considered to be equivalent by our algorithm. The POS tags used and how they are grouped are described in Table 1. The NER tags each word in the text with one of four possibilities: LOCATION, PERSON, ORGANIZATION, or O (meaning “other”). When we initially extract relevant terms (see below), words with a LOCATION tag from the NER are considered equivalent to words with a noun tag from the POS tagger. However, when reducing the size of the term set T in a subsequent step, we will give preference to words with LOCATION tags from the NER.\nExtracting Terms After each word in the text has been tagged by both the POS tagger and the NER, we build our set of terms T which holds all potential location references. A word sequence in the text is considered a potential location reference if it satisfies two properties:\n1. All words in the sequence must be tagged with a noun or adjective tag from Table 1, or a LOCATION tag from the NER.\n2. The sequence must contain at least one word with a noun tag from the POS tagger or a LOCATION tag from the NER (so sequences of only adjectives are not considered).\nThe details about why these properties were chosen are described in the rest of this section. Note that for the remainder of this section, a noun refers to any word that is tagged with LOCATION by the NER, or with one of the noun tags in Table 1 by the POS tagger (but not to adjectives).\nSome locations, such as New York, have multiple words in their name. Each word is tagged individually by the POS tagger and NER, so we need to consider these multi-word possibilities when building T . If multiple nouns occur adjacent to each other in the text, then we do not know if these nouns refer to multiple locations or to one location. So we add all possibilities to the set T , and we will resolve conflicts in Section 2.5. For example, if the algorithm discovers the phrase “New York City” in the text, with each of the three words tagged as a noun (note that the POS actually tags “New” as a noun\n7/35\ndue to the capitalization), then the algorithm would add six terms to the set T : New, York, City, New York, York City, and New York City.\nDuring the development of our geolocation algorithm it was discovered that some location references contain words tagged as adjectives. For example, given the text “georgian college” the POS tagger decides that georgian is an adjective and that college is a noun. The word georgian is part of the name, even though it was not tagged as a noun. So our algorithm was modified to consider adjectives when they are part of a multi-word phrase with other nouns. However, it does not consider phrases that only contain adjectives (otherwise we could simply add adjectives to the noun group in Table 1). In the “georgian college” example, both college and georgian college are added to T . Also, if a text contains a street address, we want the street number to be part of the term. This will allow for greater precision in finding the proper location. Numbers are treated the same way as adjectives for this reason. So the example text “200 University Avenue” would generate five terms: University, Avenue, University Avenue, 200 University, and 200 University Avenue.\nNote that our set T is not finalized yet. Some of the terms we added in this section may be removed for efficiency reasons, as described next.\nRemoving Terms The amount of time required for the knowledge base searches in Section 2.4 grows linearly with |P |, and the amount of time required for the disambiguation in Section 2.5 is cubic in |T |. Furthermore, including too many terms in T which are not true location references may make it difficult to disambiguate properly. Therefore it is advantageous to keep the size of T small.\nThe first step in reducing T is to check whether any of the terms contain words that were tagged as locations by the NER model. If there are LOCATION tags for any word in any term in T , then we keep only terms which contain at least one word tagged with LOCATION. All others are removed from the set T . The idea behind this is that if the NER finds words that are locations, then terms with these words are the best bet for successful disambiguation and geolocation.\nIf no words were tagged with LOCATION in the entire text, then the geolocation algorithm must rely solely on the results of the POS tagger. However, the set of nouns in a text can be quite large, so we still wish to filter T .\nIn the case where no LOCATION tags were found, the next step is to look for terms that occurred after prepositions. Many prepositions describe spatial relationships, so they can be strong indicators that a term does refer to a location. For example, the text “Bob travelled from Waterloo” contains the preposition from. In this case, the preposition indicates that Waterloo is a location. However, prepositions can describe not only spatial relationships, but also temporal ones. For example, “Bob lived there for five years” uses for as a temporal preposition. This type of preposition should not be included in our algorithm. The implementation that was used in this paper explicitly ignored the word for as a preposition, as this was observed to increase accuracy for some development examples (from classified ads). All other prepositions identified by the POS tagger were retained. Future work could refine this list to exclude more non-spatial prepositions.\nIf the text contains terms that occur after prepositions but no words tagged with LOCATION, then these terms are retained while all others are discarded from T . A term is considered to be after a preposition if all words between the preposition and the term are tagged with any of the tags in Table 1. It is for this reason that conjunctions are included in that table. Including conjunctions ensures that a text such as “Guests travelled from Waterloo and Toronto” will consider both Waterloo and Toronto.\nFinally, if the text contains no LOCATION tags and there are no terms that follow prepositions, then no terms are removed from T .\n8/35\nPostal Codes Some of the development example texts contained postal codes. Postal codes can give very precise location information, so regular expressions were used to find postal codes that match the formats used by Canada, the United States, and the Netherlands. (The Netherlands were included because Spotzi, a partner company for this paper, has a branch there.) All occurrences of postal codes are added to the set T if they are not already in the set due to previous steps. This occurs after the filtering to remove terms that was described above.\nExample To demonstrate how the extraction step works, we will walk through an example in this section. This short example was used in the development of our geolocation algorithm, and shows many of the cases that were described in the previous sections. We will continue using this example in Section 2.5.\nThe text is from a Kijiji classifieds listing, and consists of a single sentence: “A beautifull clean house for rent, Walking distance to RVH and Georgian college.” This text is not properly structured. In particular, it has a spelling error and some improper capitalization. This makes the text challenging.\nFirst we use the NER and POS tagger to tag all words in the text. The full list of tags for this example is given in Table 2. We can see that the NER assigns no LOCATION tags in this text, so we must rely solely on the POS tags.\nNext, we start building the set T . We take all nouns, along with adjectives that are adjacent to them. So our set P that corresponds to T is:\nP = {beautifull, beautifull clean, beautifull clean house, clean house, house, rent, distance,RVH,Georgian college, college}\nNow we start to reduce this set. We have no terms that contain words tagged with LOCATION, but we do have terms that occur after prepositions. The preposition to occurs before RVH, so the term for RVH remains in T . Similarly, Georgian college and college are considered to occur after this preposition, because they are separated from this preposition only by tags in Table 1. The terms with phrases beautifull, beautifull clean, beautifull clean house, clean house, house, and distance are all discarded because they do not occur after a preposition. The term with rent occurs after the preposition for, but, as described in Section 2.3, this is assumed to not be a spatial preposition, so rent is also discarded.\n9/35\nSo the set P that corresponds to the final set T is:\nP = {RVH,Georgian college, college}\nThis set is used in the next step of the geolocation algorithm."
    }, {
      "heading" : "2.4 Searching the Knowledge Base",
      "text" : "After we generate the set T (and the set P that corresponds to it), the next step is to search for each p ∈ P in a knowledge base. OpenStreetMap is used as the knowledge base for this paper [14]. The OpenStreetMap data is queried using a tool called Nominatim (https://nominatim.openstreetmap.org/).\nOpenStreetMap is a database that contains mapping data for the entire globe, including roadways, cities, and points of interest. This data is created and corrected by a large community of contributors, and is freely available. Nominatim is an opensource tool that allows users to search the OpenStreetMap data. Using clever indexing and a significant amount of system resources, Nominatim allows queries of the vast OpenStreetMap data to be completed in seconds.1\nNominatim allows a number of parameters to be specified with the query. For example, searches can be limited to a particular region or country. One can also specify the maximum number of search results that should be returned for each query. In this paper the maximum number was set to 10. If Nominatim does not produce any result for a phrase, we discard all terms in T that correspond to the phrase. If the number of results ≥ 1, then we use these results in the disambiguation step (step 3 of Algorithm 2), which selects for each phrase p the correct result in Rp. Each result r ∈ Rp contains latitude and longitude coordinates which are used to calculate distances between results as described in Section 2.5.\nAnother field that is included with each Nominatim search result is called “importance”. This field is not well-documented, but after reading the source code it appears that a number of different factors are used to calculate this importance. These factors include:\n• The type of the location (building, city, country, etc.)\n• The PageRank of the Wikipedia article about the location (if applicable)\n• The string similarity between the query string and the name of the location\nWe use the importance field as a tie-breaker in some steps of the disambiguation algorithm in Section 2.5. It is also used by Nominatim to sort results. If Nominatim finds more than 10 results for a query, it will return only the 10 most important results.\nExample Here we continue the example from Section 2.3. When we last saw this example we had three terms, and their corresponding phrases were\nP = {RVH,Georgian college, college}\nWe use each phrase as a search query with Nominatim. However, for this example we will limit Nominatim to return a maximum of three results for each phrase (instead of\n1 There are donated servers running Nominatim which are free for light use, but they have a usage policy to prevent users from overloading the server. This usage policy gives a limit of one query per second. Our geolocation algorithm requires one query for each p ∈ P , so anyone who wants to use it extensively should set up their own Nominatim server. The tests in Section 3 did use the freely available servers with a delay written into the code to ensure that at least one second passed between each query sent to Nominatim. This meant the geolocation algorithm completed much slower than it would otherwise, but it saved time on the engineering effort required to set up a Nominatim server.\n10/35\n10, which was used for the rest of this paper). The results for each query are given in Table 3. Nominatim found only one result for RVH, two for Georgian college, and three for college. The phrase college would have more results if we increased the result limit."
    }, {
      "heading" : "2.5 Disambiguation",
      "text" : "In this section we describe in detail how our novel distance-based disambiguation procedure works. We begin with an overview of the approach using two simple examples.\nOverview of Distance-Based Disambiguation Approach Consider again the short text “Let’s go shopping at Conestoga Mall in Waterloo. Waterloo lies between London and Guelph.” The term set T={Conestoga, Mall, Conestoga Mall, Waterloo, Waterloo, London, Guelph}, and the phrase set P={Conestoga, Mall, Conestoga Mall, Waterloo, London, Guelph}.\nThe disambiguation phase of our geolocation algorithm has two goals. First, it serves to decide between mutually exclusive interpretations of word sequences (i.e., it decides which terms to retain when terms are conflicting (or overlapping), see Section 2.2). In the example, the sequence “Conestoga Mall” may refer to a single location called “Conestoga Mall”, or, alternatively, one (or both) of the terms “Conestoga” and “Mall” may refer to a location instead. The disambiguation step will select one of these conflicting interpretations. Second, for each phrase p ∈ P , we will use the disambiguation step to select one of the results r in the result set Rp. Both of these processes will rely on the physical distances between result locations, and will be based on a table of pairwise\n11/35\ndistances that is computed between the different candidate results for the different terms in the text, as, for example, in Tables 4 and 5.\nTo calculate the distance between two results a and b from Nominatim, let ax and bx be their longitude coordinates, and let ay and by be their latitude coordinates. The distance between a and b is then calculated by\nd(a, b) = 6371 arccos (sin(ay) sin(by) + cos(ax) cos(bx) cos(ax − bx)) (1)\nEquation (1) is the great circle distance on a sphere, where 6371 is the mean radius of the Earth in kilometres.\nLet us first consider how disambiguation would work for the short text “Waterloo lies between London and Guelph”. For this sentence, there are no conflicting interpretations of word sequences, and we only have to resolve the multiple results for each phrase. Table 4 shows mutual distances between pairs of results for each of the terms in the term set T={Waterloo, London, Guelph}. The first column of the table shows that the first result for Waterloo (the Ontario result) is located within a short distance from the Ontario results for London and Guelph (and, of course, the fourth column shows that the Ontario results for London and Guelph are also closeby). This indicates that the Ontario results for Waterloo, London and Guelph are the coherent choice for disambiguation. In this small example with three terms who have two results each, this is easy to see, but for larger problems with tens or more of terms and 5 or more results per term, we need a systematic procedure to find coherent groups of results. We use the following systematic approach.\nFirst we compute, for each result r and every term t (except the term for which r is a result), the shortest distance c(r, t) between result r and any of the results for term t. For example, for the first column of Table 4, c(Waterloo(Ontario), London)=79 and c(Waterloo(Ontario), Guelph)=24. Then we compute, for every result r of every term t, the scoring function Str, where the value of the scoring function measures how good a candidate a result is as a choice for disambiguation. The simplest scoring function we consider is the Total Distance scoring function, given here by Str = − ∑ t2∈T,t2 6=t c(r, t2). This scoring function is computed in the bottom line of Table 4, for each of the six results. For example, in the first column of the table, -103 is the Total Distance score for result Waterloo (Ontario) of term Waterloo. It is the negative of the sum of the shortest distances of result Waterloo (Ontario) to the terms London and Guelph. Our approach then proceeds by selecting the result with the overall highest score (closest to 0, i.e., smallest sum of distances) as the first disambiguation result. In our example, this means that result Waterloo (Ontario) is chosen for term Waterloo. All results for Waterloo are then removed from the table (except for the result that was chosen), and the process repeats: the scoring functions of the reduced table are recomputed, and the result with the next largest score is selected to disambiguate the next term. In this manner, the\n12/35\nOntario results for Waterloo, London and Guelph are selected. In practice, the process is more complex (see below), but this is a simple illustration of the general principle.\nWe next consider disambiguating the short text “Let’s go shopping at Conestoga Mall in Waterloo.” This is more complicated, because the word sequence “Conestoga Mall” has two conflicting interpretations. Table 5 shows mutual distances between pairs of results for each of the terms in the term set T={Waterloo, Conestoga Mall, Conestoga, Mall}. The term “Conestoga Mall” conflicts (or overlaps) with “Conestoga” and “Mall” (since the author cannot have intended both as locations at the same time), so no mutual distances are computed. The shortest distances c(r, t) are computed as before. When computing the scoring functions Str for every result r of every term t, we now introduce weights W t1t2 for any pair of terms t1, t2 ∈ T . These weights are used to reduce bias in calculating scores when there are terms in T that conflict with each other and multiple mutually exclusive interpretations need to be considered.\nFor example, for the first column of Table 5, when computing the scoring function Str for result Waterloo (Ontario) of term Waterloo, we cannot just add the shortest distances of Waterloo (Ontario) to all of the terms “Conestoga Mall”, “Conestoga” and “Mall”, as this would bias the score since only one or at most two distances should be counted, given the mutually exclusive interpretations of the word sequence “Conestoga Mall”. Since the correct interpretation has not been chosen yet, the best we can do is to take all the three shortest distances into account, but we take a weighted average of these three distances to reduce the bias in the procedure (compared to, for example, results for term “Conestoga Mall”, which only have one distance in their scoring function). When computing the scoring functions for the results of term t1 (e.g., “Waterloo”), we define weights W t1t2 with respect to t1 for all the terms t2 that correspond to a word sequence with conflicting interpretations (e.g., “Conestoga Mall”), in such a way that the sum of the weights equals 1, and with equal weight for every interpretation. Each of the two interpretations of the phrase“Conestoga Mall” is given an equal weight of 1/2. Within the second interpretation (“Conestoga” and “Mall”), each of the terms have equal weight, so the final weights are W t1t2 = 1/4 for t2 =“Conestoga”, W t1 t2 = 1/4\nfor t2 =“Mall”, and W t1 t2 = 1/2 for t2 =“Conestoga Mall”. So for our example, for the first column of Table 5, we compute the scoring function Str for result Waterloo (Ontario) of term Waterloo using the Weighted Distance scoring function, given here by Str = − ∑ t2∈T,t2 6=t W t t2c(r, t2), resulting in S t r = −279.5 for result Waterloo (Ontario) of term Waterloo. After the scoring functions are computed for all terms of each result in this way, we decide on the first disambiguation result in the same greedy fashion as before: the highest score in the bottom row of Table 5 is the -4 for result 1 of term “Conestoga Mall”. This result is selected for this term, and (in our simplest algorithm) this settles at the same time the interpretation for the word sequence “Conestoga Mall”. All results for the term “Conestoga Mall”, except for the one we choose, are removed from the table, as well as the terms “Conestoga” and “Mall” that conflict with it, and this finalizes the disambiguation since the Ontario location for Waterloo is the one that is closest to the chosen result for “Conestoga Mall”. In practice, the weighting procedure can be significantly more complex and may have to be applied in a modified way when longer sequences of words may have a larger number of possible interpretations (see below), but this example is a simple illustration of the general principle.\nNote that, in our general procedure, if a sequence of words with multiple interpretations occurs several times in a text, then the interpretation is determined for each occurrence separately (based on eliminating overlapping terms). This approach was chosen because the location of terms in the text (overlap of adjacent terms) is important when disambiguating mutually exclusive interpretations. In contrast, if a phrase occurs several times in a text, a single location result is selected for all occurrences of the phrase.\n13/35\nW a te rl oo\nC o n es to ga\nM a ll\nC o n es to ga\nM a ll\n1 2\n1 2\n1 2\n1 2\nW a te rl oo\n1 (O\nn ta ri o )\nx x\n4 1 4 9 5\n5 2 3\n3 2 6 4\n5 8 7\n5 7 9 7\n2 (B\nel g iu m )\nx x\n6 1 1 7\n7 3 7 6\n6 1 0 5\n8 9 7 4\n6 2 2 5\n3 2 8\nC o n es to ga M a ll\n1 (W\na te rl o o )\n4 6 1 1 7\nx x\nx x\nx x\n2 (N\neb ra sk a )\n1 4 9 5\n7 3 7 6\nx x\nx x\nx x\nC o n es to ga\n1 (P\nen n sy lv a n ia )\n5 2 3\n6 1 0 5\nx x\nx x\n1 3 0\n5 7 7 8\n2 (C\na li fo rn ia )\n3 2 6 4\n8 9 7 4\nx x\nx x\n3 5 4 1\n8 6 8 1\nM a ll\n1 (W\nas h in g to n D C )\n5 8 7\n6 2 2 5\nx x\n1 3 0\n3 5 4 1\nx x\n2 (L\no n d o n U K )\n5 7 9 7\n3 2 8\nx x\n5 7 7 8\n8 6 8 1\nx x\nS c o re\n-2 7 9 .5\n-4 6 6 6 .7 5\n-4 -1 4 9 5\n-6 5 3\n-6 8 0 5\n-7 1 7\n-6 1 0 6\nT a b le\n5 . S ec on\nd ex am\np le\nof d is ta n ce -b as ed\nd is a m b ig u a ti o n , fo r th e se n te n ce\n“ C o n es to g a M a ll in\nW a te rl o o ” . T h er e a re\ntw o m u tu a ll y ex cl u si ve\nin te rp re ta ti on\ns fo r th e w or d se q u en ce\n“C o n es to g a M a ll ” . In\nth e fi rs t in te rp re ta ti o n , th e co m b in ed\nte rm\n“ C o n es to g a M a ll ” re fe rs\nto a p o te n ti a l\nlo ca ti on\n, an\nd in\nth e se co n d in te rp re ta ti on\nth e tw\no se p ar at e te rm\ns “C\non es to ga ” an\nd “M\nal l”\nar e co n si d er ed\np ot en ti al\nlo ca ti on\ns. W\nh en\nd et er m in in g\nth e d is ta n ce -b as ed\nsc or in g fu n ct io n fo r ea ch\no f th e p o ss ib le\nre su lt s fo r W a te rl o o , th e d is ta n ce s to\nth e te rm\ns “ C o n es to g a M a ll ” , “ C o n es to g a ” a n d\n“M al l”\nar e re q u ir ed , b u t si n ce\nth e tw\no in te rp re ta ti o n s a re\nm u tu a ll y ex cl u si ve , w ei g h ts\na re\nu se d in\nth e W ei g h te d D is ta n ce\nsc o ri n g fu n ct io n th a t is\nem p lo ye d in\nth e ta b le .\n14/35\nThis simplifies the implementation. In what follows, we provide complete details of the full distance-based disambiguation procedure. Multiple scoring functions were tested, and they are described after giving full details on the weights mechanism. Finally, two algorithms will be given that use the scores to perform the disambiguation.\nWeights Weights are used when there are terms in the set T that conflict with each other. Terms conflict when they overlap in the text. This is best explained in detail with the New York City example that was introduced in Section 2.3. For this example, the extraction step finds the nouns New, York, and City adjacent to each other in the text, and six terms are added to the set T : New, York, City, New York, York City, and New York City. The terms York and New York conflict with each other because the text could not have meant to refer to a location called York and a different location called New York. This is because the terms York and New York overlap. For this example, the full list of terms that conflict with any given term is provided in Table 6. The purpose of the weights we will define in this section is to properly account for conflicting terms in the disambiguation step. Assigning weights to conflicting terms will allow us to reduce bias in the scoring functions until we can determine which interpretation of the text segment is correct.\nWe can think of the weight W t1t2 as the probability that term t2 is a true location reference in the text given that t1 is a true location reference. Weights are defined such that 0 ≤W t1t2 ≤ 1 ∀t1, t2 ∈ T . If no terms in T conflict with each other then all weights are equal to 1. The remainder of this section describes how weights are calculated when some terms do conflict.\nWe begin our definition of weight with the case of two terms that conflict (i.e., overlap in the text):\nW t1t2 = 0 ∀t1, t2 ∈ T, t1 6= t2 | t1 and t2 conflict (2)\nThis simply expresses that conflicting terms cannot both refer to true locations at the same time, and it means that distances between conflicting terms never need to be taken into account in scoring functions. Thinking of the weights as the probabilities we described earlier, we can define:\nW tt = 1 ∀t ∈ T. (3)\nBefore we define the weight in the other cases, we need another definition. Conflicting terms appear in groups. We define G(t) to be the group of term t. G(t) is the smallest set of all terms that overlap with t or any of the other terms in G(t). I.e., a group is a minimal set of terms such that all terms that overlap with any of the terms in the group are in the group. Note that G(t) = {t} iff t does not overlap with any other term. Using this definition, we can conclude that all terms in Table 6 are in the same group. Groups are disjoint, and each group corresponds to a segment (sequence of words) in the text. These segments are disjoint.\n15/35\nGroups with more than one term have multiple interpretations. An interpretation of a group is a maximal subset of the group which contains terms that do not conflict. An interpretation does not have to cover all words in the segment of text, but it must contain enough terms such that no non-conflicting terms can be added to the group.2 Table 7 lists all four interpretations for the New York City example. When computing distance-based scores for the candidate results of a term t1, we will, for every group G 6= G(t1), define weights W t1t2 with respect to t1 for all the terms t2 in group G. For every group G these weights sum up to 1. If G has only one element t2 (which does not conflict with t1 since G 6= G(t1)), then W t1t2 = 1. If G has more than one element, the weights W t1t2 are defined as follows. Let q be the number of interpretations of group G, each with weight 1/q, and let ni be the number of terms in interpretation i, each with weight 1/ni in interpretation i. Then\nW t1t2 = ∑\ninterpretations i that contain t2\n1\nq\n1\nni . (4)\nFor our New York City example, the weights given to each term in each interpretation are included in the last column of Table 7. Table 8 continues the calculations from Table 7 to give the weights W t1t2 for the case where t2 is a term in the New York City group and t1 /∈ G(t2). For example, this table tells us that when t1 /∈ G(New) then W t1New = 5 24 .\nFinally, we must define W t1t2 for the case where t1 ∈ G(t2) and t1 and t2 do not conflict. As discussed earlier, the weight W t1t2 is the weight of t2 when we assume that t1 is in the text. Therefore, all we need to do to calculate the weight in this case is temporarily remove terms from T that conflict with t1. This will change the groups G(t1) and G(t2) such that t1 is no longer in G(t2) (because t1 no longer conflicts with\n2In our New York City example all our interpretations cover all words in that segment of text. However, if the term New were tagged as an adjective and thus missing from T , then {York,City} and {New York,City} would both be valid interpretations. However, {York} would not be valid, since the term for City can still be added.\n16/35\nany terms, G(t1) = {t1}). Our calculation of the weights then proceeds as before with Equation (4).\nFor example, suppose we want to calculate WNewt2 for t2 ∈ G(New). W New New York = WNewNew York City = 0 because those terms conflict. We remove those terms, so New becomes the only member of its group. The remaining terms have only two interpretations: {York,City} and {York City}. When we calculate those weights we find that WNewYork = WNewCity = 1 4 and W New York City = 1 2 .\nScoring Functions We now formulate eight different candidate scoring functions that can be used in the disambiguation step of our geolocation algorithm. The accuracy obtained with these different scoring functions is tested in Section 3. Note that all scoring functions are defined such that greater values of the scoring function indicate a “better” (higher) score.\nThe first scoring function we consider is called Total Distance: St1r1 = − ∑ t2∈T. W\nt1 t2 6=0\nc(r1, t2) (5)\nFor a term t1 ∈ T and a result r1 ∈ Rt1 , this function simply adds up the minimum distance between r1 and the closest result for each other term in T . Note that for any term t such that tphr = t1,phr (including the original term t = t1) we define c(r1, t) = 0. Therefore we do not need to explicitly ensure that t2 6= t1 in the summation in Equation (5). However, we do explicitly ensure that W t1t2 6= 0 so we do not consider conflicting terms. The Total Distance score is always negative, and values close to 0 indicate desirable scores (small sum of minimal distances).\nThe second scoring function we consider is called Weighted Distance: St1r1 = − ∑ t2∈T W t1t2 c(r1, t2). (6)\nThis is equivalent to Equation (5) except for the multiplication by the weight. The reasoning behind these weights and how they are calculated was described above. Note that the minus sign was added to scoring functions (5) and (6) to ensure that greater values of the scoring function indicate a “better” (higher) score (closer to zero, for these scoring functions).\nScoring functions (5) and (6) may be sensitive to terms that are extremely far away from the others: in the sums (5) and (6), one very large smallest distance may drown out the discriminative power of smaller smallest distances. This can occur, for example, when the location extraction step (step 1 of Algorithm 2) extracts a phrase that is not intended to refer to a location in the text, but does have results in the knowledge base. The location extraction step is not perfect, so this is a common occurrence. The remaining scoring functions attempt to deal with this difficulty.\nThe next two scoring functions we consider are called Inverse:\nSt1r1 = ∑ t2∈T\nt1,phr 6=t2,phr W\nt1 t2 6=0\n1\nmax (c(r1, t2), 10−3) , (7)\nand Weighted Inverse:\nSt1r1 = ∑ t2∈T\nt1,phr 6=t2,phr\nW t1t2 max (c(r1, t2), 10−3) . (8)\n17/35\nThey are similar to Equations (5) and (6) respectively, except that we use the reciprocal of the c(r, t) function. To avoid issues with division by zero, we take the maximum of c and 10−3. Due to the use of the reciprocal, small smallest distances increase the score strongly (as desired), and large smallest distances due to outliers have little effect.\nNext we consider some normalization of Equation (8) to attempt to make scores more comparable to each other. Indeed, for a given text, the number of terms in the sums of Equation (8) can be quite different for different terms t1, and the weights do not fully compensate for this. This may bias the scores. For example, for the example of\nTable 5, the St1r1 scoring function of Equation (8) has only one term for t1 =“Conestoga Mall”, but has two terms for t1 =“Conestoga” or t1 =“Mall”. To remedy this difficulty, we consider the Weighted Normalized Inverse scoring function, which is given by\nSt1r1 =\n∑ t2∈T\nt1,phr 6=t2,phr W t1t2\n(\nmin\nr′1∈R t1 max(c(r\n′ 1,t2),10 −3) max(c(r1,t2),10−3)\n)\n∑\nt2∈T t1,phr 6=t2,phr\nW t1t2 . (9)\nEquation (9) was constructed by multiplying the numerator in Equation (8) by the minimum distance between any result for t1 and any result for t2, not just between result r1 and any result for t2. Finally, we divide the whole expression by the total weight of all terms we looked at. This means that the score Str is always between 0 and 1. A score of Str = 1 means that when we consider the closest pair of results between t and another term, r is always the result from t that is part of that closest pair.\nIn scoring functions (7), (8), and (9) we skip all terms t2 ∈ T that have the same phrase as t1. However, if a phrase appears many times in the text then it is likely more important. So it might be helpful to give a bonus to the scores based on how often the phrase occurs. We add this feature to Equations (7), (8), and (9) to produce scoring functions Inverse Frequency:\nSt1r1 =  ∑ t2∈T\nt1,phr 6=t2,phr W\nt1 t2 6=0\n1\nmax (c(r1, t2), 10−3)  ∑ t2∈T\nt1,phr=t2,phr\n1, (10)\nWeighted Inverse Frequency:\nSt1r1 =  ∑ t2∈T\nt1,phr 6=t2,phr\nW t1t2 max (c(r1, t2), 10−3)  ∑ t2∈T\nt1,phr=t2,phr\nW t1t2 , (11)\nand Weighted Normalized Inverse Frequency:\nSt1r1 =\n∑ t2∈T\nt1,phr 6=t2,phr W t1t2\n( min\nr′1∈R t1 c(r\n′ 1,t2)\nmax(c(r1,t2),10−3) ) ∑\nt2∈T t1,p 6=t2,p\nW t1t2\n∑ t2∈T\nt1,phr=t2,phr\nW t1t2 . (12)\nIn Equations (11) and (12) this is done by multiplying the original expression by the total weight of all terms with the same phrase. Since scoring function (7) is the same as (8) but with all non-zero weights set to 1, we similarly set all non-zero weights to 1 to obtain the multiplicative factor in Equation (10).\n18/35\nWe have now created eight different scoring functions, which we will test in Section 3. We will find that many of these scoring functions perform similarly. The Weighted Inverse Frequency scoring function (Equation 11) will turn out to perform best in our Wikipedia tests. Combined with the two disambiguation algorithms discussed in the next section, the eight scoring functions give 16 versions of our geolocation algorithm that will be compared.\nDisambiguation Algorithms We present two versions of the disambiguation step of our geolocation algorithm. The first version, called the 1-phase disambiguation algorithm, is described in Algorithm 3.\nAlgorithm 3 1-Phase disambiguation algorithm\n1: Calculate W t1t2 ∀t1, t2 ∈ T using Equations (2), (3), and (4) 2: while (∃p ∈ P |(|Rp| > 1)) or (∃t1, t2 ∈ T |(W t1t2 6= 1)) do 3: for all t ∈ T do 4: for all r ∈ Rt do 5: Calculate Str using one of scoring functions (5)–(12) 6: end for 7: end for 8: t∗, r∗ ← (t ∈ T, r ∈ Rt) that maximize Str and satisfy (|Rt| > 1) or (∃t′ ∈\nT |W t′t 6= 1) 9: Rt ∗ p ← {r∗}\n10: for all t ∈ T do 11: if W t ∗\nt = 0 then 12: T ← (T \\ {t}) 13: end if 14: end for 15: Update P to reflect changes in T 16: Recalculate W t1t2 ∀t1, t2 ∈ T as in step 1 17: end while\nStep 2 repeats while there are still terms that need to be disambiguated: there are still phrases with multiple results, or there are still conflicting (i.e., overlapping) terms. Step 8 is the key step in our greedy algorithm. It finds the term t∗ and result r∗ that have the highest score St ∗\nr∗ . The greedy algorithm selects the highest score, which is considered the likely most coherent match with other terms and results. (In the case of a tie, the importance that Nominatim assigned to the results is used as the tie-breaker.) Step 8 only considers terms that need to be disambiguated: multiple results still exist for these terms, or the terms still overlap with at least one other term.\nAfter step 8 finds the result r∗ with the best score, step 9 makes r∗ the only result that is considered for phrase t∗p. Also, steps 10 to 14 remove any terms that conflict with t∗, settling on the interpretation that contains term t∗. This means that after step 14 the term t∗ has been completely disambiguated, and the algorithm is one step closer to disambiguating all terms in the text.\nThe other version of the disambiguation algorithm is called the 2-phase disambiguation algorithm, which is described in Algorithm 4. The difference between the 1-phase and 2-phase versions is that the 2-phase algorithm attempts to first resolve all conflicting terms (i.e., it first settles all interpretation choices), and then disambiguates between location search results in a second phase.\nInstead of one while-loop as in Algorithm 3, there are now two while-loops corresponding to the two phases. The first loop from steps 2 to 16 reduces the set T until there are no more conflicting terms. The second loop from steps 17 to 25 picks a location\n19/35\nsearch result for each remaining term. The 2-phase algorithm cleanly separates the disambiguation of interpretations from the disambiguation of possible location results. The weights are only used to disambiguate interpretations, since all weights are one in the second phase, where location results are disambiguated. In the second phase, all location results for all the terms that remain after the first phase are still considered, and the distance functions that are used to select the location results for each term only depend on actual distances and are not biased by weights or ambiguities coming from unresolved interpretations. Our tests in Section 3 will investigate whether there are advantages to this approach.\nAlgorithm 4 2-Phase disambiguation algorithm\n1: Calculate W t1t2 ∀t1, t2 ∈ T using Equations (2), (3), and (4) 2: while ∃t1, t2 ∈ T |(W t1t2 6= 1) do 3: for all t ∈ T do 4: for all r ∈ Rt do 5: Calculate Str using one of scoring functions (5)–(12) 6: end for 7: end for 8: t∗, r∗ ← (t ∈ T, r ∈ Rt) that maximize Str and satisfy ∃t′ ∈ T |W t ′\nt 6= 1 9: for all t ∈ T do\n10: if W t ∗\nt = 0 then 11: T ← (T \\ {t}) 12: end if 13: end for 14: Update P to reflect changes in T 15: Recalculate W t1t2 ∀t1, t2 ∈ T as in step 1 16: end while 17: while ∃p ∈ P |(|Rp| > 1) do 18: for all t ∈ T do 19: for all r ∈ Rt do 20: Calculate Str using one of scoring functions (5)–(12) 21: end for 22: end for 23: t∗, r∗ ← (t ∈ T, r ∈ Rt) that maximize Str and satisfy |Rt| > 1 24: Rt ∗ p ← {r∗} 25: end while"
    }, {
      "heading" : "2.6 Full geolocation algorithm",
      "text" : "Now that each step in Algorithm 2 has been explained in more detail, we can write a more comprehensive summary of the full geolocation algorithm. This is given by Algorithm 5. Note that the filtering in steps 4–18 to obtain the relevant term set T can be modified and possibly be made more efficient depending on the type of document one wants to geolocate. However, the filtering steps in Algorithm 5 are organized in such a way that the geolocation algorithm can be applied to a broad class of texts, including Wikipedia articles, Twitter messages, Kijiji classified ads, etc.\nAt the end of Algorithm 5 we have a collection of terms that are mentioned in the text along with a single location for each. Scores can be recalculated for these results using the same scoring function that was used for the disambiguation. (Note that all weights will be 1 at this point.) They can then be sorted in order of decreasing score to provide a ranking for the top places mentioned in the text. This is done in Section 3 to\n20/35\ncompare the best results to the true location for articles.\nAlgorithm 5 Full geolocation algorithm\n1: Tag the input text using the POS and NER taggers from [4–6]. 2: S ← ordered list of words in the text 3: T ← ∅ 4: for every s ⊆ S where s is an ordered sequence of adjacent words do 5: if s contains at least one word tagged as a NER LOCATION or POS noun then 6: if every word in s is tagged as noun, adjective, number, or LOCATION then 7: t← a term made from the words in s 8: T ← (T ∪ t) 9: end if 10: end if 11: end for 12: if T contains terms that have words tagged with a NER LOCATION then 13: Remove terms from T that do not contain any words tagged with LOCATION. 14: else 15: if T contains terms that occured after POS prepositions then 16: Remove terms from T that did not occur after prepositions. 17: end if 18: end if 19: for every s ∈ S where s matches a Canadian, American, or Dutch postcode format\ndo 20: if s is not represented by any term in T then 21: t← a term made from the words in s 22: T ← (T ∪ t) 23: end if 24: end for 25: P ← set of phrases represented in T 26: for every p ∈ P do 27: Rp ← search results from the knowledge base for query p 28: if |Rp| = 0 then 29: for every t ∈ T do 30: if tphr = p then 31: T ← (T \\ {t}) 32: end if 33: end for 34: end if 35: end for 36: Run Algorithm 3 for 1-phase or Algorithm 4 for 2-phase disambiguation 37: Return set of non-conflicting terms T and one result in Rp for every p ∈ P ."
    }, {
      "heading" : "3 Results for Wikipedia Test Data",
      "text" : "In this section we describe the performance of our geolocation algorithm with each of the eight scoring functions and both disambiguation algorithms that were discussed in Section 2.5, for a dataset of Wikipedia articles.\n21/35"
    }, {
      "heading" : "3.1 Test Data",
      "text" : "We tested our geolocation algorithm with a subset of English geotagged Wikipedia articles, that is, articles about topics with a well-defined geographical location where the authors have provided latitude and longitude coordinates of the primary “true location” the article describes. This data required a significant amount of preprocessing, which is described in Appendix B. Our full dataset contained 920,176 geotagged Wikipedia articles, but the tests in this section used only a sample of 5,976 articles. This subset was used due to time and resource constraints (in particular, the one query per second limit on the free Nominatim servers which was discussed in Section 2.4). The subset was sufficiently large to thoroughly test the capabilities of our geolocation algorithm, as indicated below."
    }, {
      "heading" : "3.2 Filtering by NER and POS Tags",
      "text" : "Section 2.3 described how we filter our set of initial terms to come up with the term set T that we disambiguate. See also Algorithm 5 (steps 1–24) for a summary of the filtering steps. If the text contains NER LOCATION tags, then we only use terms that have at least one word with this tag (steps 12-13). If there are no LOCATION tags then we only keep terms that occur after POS prepositions (steps 15–17). If there are also no terms that follow prepositions, then we maintain the initial term list based on POS nouns (steps 4–11).\nWe ran our geolocation algorithm on 5,976 articles. Of these articles, we rejected 500 outright because the different text parsers used by the NER and POS taggers produced inconsistent tokens.3 Table 9 shows how often each of the filtering methods were used for the 5,476 articles we considered in our tests. Over 99% of articles were able to use terms with NER LOCATION tags only, and only rarely did our geolocation algorithm need to fall back on POS prepositions or POS-noun-based terms. This is not surprising for Wikipedia articles that are geotagged by their authors, because they presumably discuss a well-defined location, and are thus likely to contain a good number of terms that the NER may recognize as a LOCATION. As such, these Wikipedia tests will mostly investigate the performance of Algorithm 5 for cases where the algorithm relies solely on terms recognized by the NER as LOCATIONs. However, the approach of Algorithm 5 is more general and was developed to also be able to handle shorter or unstructured texts with fewer clear NER LOCATION terms, by relying on POS prepositions and/or nouns. These capabilities of our approach will be tested in Section 4.\n3Note that this does not represent a limitation of our approach. It is rather a minor technical issue that arises because the data was not sufficiently cleaned by the opensource WikiExtractor program we used to pre-process Wikipedia raw data dumps, see Appendix B. This happened, for example, when html tags (like “<br>”) were not properly removed. In principle, the tokenizers of the NER and POS taggers could be made to match, or, alternatively, efforts could be made to clean the data further and any remaining inconsistencies could be reconciled in a suitable ad-hoc fashion. We chose to simply reject articles that produced these inconsistencies, because we had a very large number of articles that produced consistent NER and POS tokens (many more than we needed for our tests).\n22/35"
    }, {
      "heading" : "3.3 Comparison of Disambiguation Algorithms",
      "text" : "Our next step was to determine which versions of the geolocation algorithm are the most accurate. There are eight scoring functions and two disambiguation algorithms, giving a total of 16 versions to compare. Each geotagged Wikipedia article in our data set has one set of coordinates which is considered to be the true location for the article. Our geolocation algorithm provided as final output a list of geolocated terms with one most likely Nominatim result each. This means we needed to compare our list of multiple locations to the one true location for the article. We first measured error by calculating the distance between the true location and the result with the highest distance-based score, where scores were calculated using the same scoring function that the disambiguation algorithm used. In Section 3.5 we will use a different error measurement that takes multiple ranked outputs into account. Articles were sorted in order of increasing error for each version of the geolocation algorithm, and Figure 1 plots the cumulative distribution of the error (in km) obtained for the articles, with the x-axis indicating the fraction of articles that have an error below the error value indicated on the logarithmic y-axis. For example, this graph tells us that all versions of our geolocation algorithm had an error of less than one kilometre for at least 11% of articles. Note that the horizontal axis of Figure 1 does not reach 100%. This is because disambiguation algorithms were terminated when their execution time exceeded 100 seconds. Out of 87,616 disambiguation attempts (16 algorithm versions times 5476 articles), this cutoff was used 496 times (0.57%). We expected this graph to show one or\n23/35\n0\" 0.1\" 0.2\" 0.3\" 0.4\" 0.5\" 0.6\" 0.7\" 0.8\" 0.9\"\nWeighted\"Distance\"(1\"Phase)\"\nWeighted\"Normalized\"Inverse\"(1\"Phase)\"\nTotal\"Distance\"(2\"Phase)\"\nWeighted\"Normalized\"Inverse\"Frequency\"(2\"Phase)\"\nInverse\"(2\"Phase)\"\nWeighted\"Inverse\"(2\"Phase)\"\nInverse\"Frequency\"(2\"Phase)\"\nWeighted\"Inverse\"Frequency\"(2\"Phase)\"\nError$(km)$\n0\" 0.1\" 0.2\" 0.3\" 0.4\" 0.5\" 0.6\" 0.7\" 0.8\" 0.9\"\nWeighted\"Distance\"(1\"Phase)\"\nWeighted\"Normalized\"Inverse\"(2\"Phase)\"\nWeighted\"Normalized\"Inverse\"(1\"Phase)\"\nWeighted\"Distance\"(2\"Phase)\"\nTotal\"Distance\"(2\"Phase)\"\nTotal\"Distance\"(1\"Phase)\"\nWeighted\"Normalized\"Inverse\"Frequency\"(2\"Phase)\"\nWeighted\"Normalized\"Inverse\"Frequency\"(1\"Phase)\"\nInverse\"(2\"Phase)\"\nWeighted\"Inverse\"(1\"Phase)\"\nWeighted\"Inverse\"(2\"Phase)\"\nInverse\"(1\"Phase)\"\nInverse\"Frequency\"(2\"Phase)\"\nInverse\"Frequency\"(1\"Phase)\"\nWeighted\"Inverse\"Frequency\"(2\"Phase)\"\nWeighted\"Inverse\"Frequency\"(1\"Phase)\"\nError$(km)$\nFigure 2. Wikipedia tests. Error for disambiguation algorithms at the 10th percentile.\n0\" 1\" 2\" 3\" 4\" 5\" 6\" 7\" 8\" 9\" 10\"\nTotal\"Distance\"(2\"Phase)\"\nInverse\"(2\"Phase)\"\nWeighted\"Normalized\"Inverse\"(2\"Phase)\"\nWeighted\"Inverse\"Frequency\"(2\"Phase)\"\nTotal\"Distance\"(1\"Phase)\"\nInverse\"(1\"Phase)\"\nWeighted\"Normalized\"Inverse\"(1\"Phase)\"\nWeighted\"Inverse\"Frequency\"(1\"Phase)\"\nError$(km)$\n0\" 1\" 2\" 3\" 4\" 5\" 6\" 7\" 8\" 9\" 10\"\nTotal\"Distance\"(2\"Phase)\"\nWeighted\"Distance\"(2\"Phase)\"\nInverse\"(2\"Phase)\"\nWeighted\"Inverse\"(2\"Phase)\"\nWeighted\"Normalized\"Inverse\"(2\"Phase)\"\nInverse\"Frequency\"(2\"Phase)\"\nWeighted\"Inverse\"Frequency\"(2\"Phase)\"\nWeighted\"Normalized\"Inverse\"Frequency\"(2\"Phase)\"\nTotal\"Distance\"(1\"Phase)\"\nWeighted\"Distance\"(1\"Phase)\"\nInverse\"(1\"Phase)\"\nWeighted\"Inverse (1\"Pha )\nWeighted\"Normalized\"Inverse\"(1\"Phase)\"\nInverse\"Frequency\"(1\"Phase)\"\nWeighted\"Inverse\"Frequency\"(1\"Phase)\"\nWeighted\"Normalized\"Inverse\"Frequency\"(1\"Phase)\"\nError$(km)$"
    }, {
      "heading" : "3.4 Further Analysis with the Winning Algorithm",
      "text" : "Results by Article Type The author-provided location tags for Wikipedia articles each have a type associated with them. We investigated whether some of these types are more difficult for the algorithm to geolocate than others. Nineteen different types of tags were observed in the test set, in addition to a NULL (or missing) type. Ten of\n24/35\n0\" 10\" 20\" 30\" 40\" 50\" 60\" 70\" 80\" 90\" 100\"\nWeighted\"Normalized\"Inverse\"Frequency\"(2\"Phase)\"\nWeighted\"Normalized\"Inverse\"(1\"Phase)\"\nWeighted\"Distance\"(1\"Phase)\"\nTotal\"Distance\"(2\"Phase)\"\nInverse\"(2\"Phase)\"\nInverse\"Frequency\"(2\"Phase)\"\nInverse\"Frequency\"(1\"Phase)\"\nWeighted\"Inverse\"Frequency\"(1\"Phase)\"\nWeighted\"Normalized\"Inverse\"Frequency\"(2\"Phase)\"\nWeighted\"Normalized\"Inverse\"(2\"Phase)\"\nWeighted\"Normalized\"Inverse\"(1\"Phase)\"\nWeighted\"Normalized\"Inverse\"Frequency\"(1\"Phase)\"\nWeighted\"Distance\"(1\"Phase)\"\nWeighted\"Distance\"(2\"Phase)\"\nTotal\"Distance\"(2\"Phase)\"\nTotal\"Distance\"(1\"Phase)\"\nInverse\"(2\"Phase)\"\nWeighted\"Inverse\"(2\"Phase)\"\nInverse\"Frequency\"(2\"Phase)\"\nWeighted\"Inverse\"Frequency\"(2\"Phase)\"\nInverse\"Frequency\"(1\"Phase)\"\nInverse\"(1\"Phase)\"\nWeighted\"Inverse\"Frequency\"(1\"Phase)\"\nWeighted\"Inverse\"(1\"Phase)\"\nthese types, including the NULL type, occurred at least 50 times in the 5476 articles that were tested. Table 10 shows the median error (50th percentile) for each of these types. The table shows that airports and railway stations appear to be the easiest to geolocate, as they have the lowest median error. These types of articles are likely to name both the location and the nearby cities they serve, so our algorithm can expect to find a small cluster of results with good scores resulting in easy disambiguation.\nIn contrast, rivers and second-level administrative regions (districts, counties, etc.) appear to be the most difficult to locate. This makes sense, as these types of locations can be spread across a large area but are only represented by a single point in the Wikipedia “true location”. We use only the single points that are associated with locations in the OpenStreetMap knowledge base, and we compute distances between these single points. There is no pre-specified way for a Wikipedia author to assign a single location to a large geographic entity (like a state or province, or, even more so, an essentially one-dimensional entity like a river), and it is not to be expected that Wikipedia authors assign locations in a way that is consistent with OpenStreetMap locations, for these types of large geographic entities. It is therefore to be expected that large deviations occur between the Wikipedia “true location” and what our geolocation algorithm can obtain based on OpenStreetMap (and deviations of this type can, in fact, hardly be called “errors”). For example, if an article is about a river and our algorithm returns a location beside the river, this could show very poor accuracy if the returned point is far from the centre point that is assigned to the river by OpenStreetMap. One possible way to improve this in future work is to consider a radius for geographic objects in addition to a centre point. If Wikipedia articles and OpenStreetMap entities were tagged with a (centre point, radius) pair, we could use shortest distances between discs in our distance-based scoring functions and in our error measures. For essentially onedimensional entities like rivers, the geometric moments could be considered to augment the location description. These improvements are a daunting task since the required data is not readily available, but it could certainly be expected to dramatically reduce the perceived error of geolocation algorithms like the one proposed in this paper, for geographic entities that are large or essentially one-dimensional.\nConfidence Estimation A valuable addition to our geolocation algorithm would be an estimator for the accuracy of the algorithm when the true error is unknown. This would allow us to assign a confidence value to each location tag. Figure 5 shows the\n25/35\nerror in the location tagging as a function of the score of the top result. It could be expected that there would be a clear downward trend, where larger scores correlate to smaller errors. This would give us a simple relationship where the score of the top result would also be our confidence measure. However, our results did not indicate such a relationship and, as shown in Figure 5, there were three main clusters instead. The bottom left cluster does show some of the desired trend, where larger scores indicate lower error. However, this is not simple to interpret, as there is another cluster directly above with a horizontal shape. The third cluster is on the right, and spans the entire y-axis range of the graph. This cluster shows some vertical stratification starting around 1000 km. This is likely an artefact of the maximum we imposed on the denominator of Equation (11), where any distances less than 1 metre were increased to 1 metre as an attempt to avoid division by zero. Every time this restriction is imposed the score increases by 1000. This is likely the major cause of the stratification that is observed around the integer multiples of 1000 in Figure 5.\nWhen the top result produced by our geolocation algorithm does not correspond to the “true location” of the Wikipedia article, the error can easily be large to very large. In this case, the score of the top result for the weighted inverse frequency distance function can be very large if the top result is very close to other results (which may produce the multiple-1000 stratification in the right cluster of Figure 5), or it can be moderately large if the top result is moderately close to other results (the top cluster in Figure 5). Large geographical entities could also easily lead to errors that appear much larger than they are in reality. Even though Wikipedia articles may have a single location tag, they can be expected to often contain (clusters of) terms referring to locations that are not close to the single location tag, just because Wikipedia articles typically contain a large number of words and describe their topics in a broad context. This may also easily lead to large errors in geolocation based on NER and POS combined with our distance-based scoring functions, for some geotagged Wikipedia articles.\nThe results indicated in Figure 5 do not give us a simple answer in our search for a confidence measure. A few other confidence measures were tried with similar results, including the ratio between the top two scores, as well as the score multiplied or divided by the number of terms used to calculate the score. None of these showed a clearer relationship, so we do not yet have a confidence measure for our geolocation algorithm, and it may be difficult to obtain one that is highly reliable, given the difficulty of the geolocation problem (for a fraction of Wikipedia articles our approach works very well, but there will likely always be a fraction of difficult cases for which it does not work well). For example, we replotted the error in location tagging as a function of the score of the top result using the Weighted Distance (1-phase) version of the algorithm (plot not shown), and this did not show the stratification that was apparent in Figure 5.\n26/35\nHowever, there was still no clear relationship between score and error. Future work includes investigating all 16 versions, although given the similarities between them it is unlikely that this will result in a reliable confidence measure."
    }, {
      "heading" : "3.5 Analysis of Top 5 Results",
      "text" : "In previous sections we used the distance between the true location and the result with the greatest score as a measurement of error. This may not be the best approach, as our algorithm provides more information in a list of multiple locations mentioned in the text. So far we used only one of these locations to calculate the error. For example, if a text describes a river and mentions the cities that lie along it, then the geolocation algorithm would produce locations for the cities as well as a location for the river. The “true location” for the article would be a single point somewhere along the river. If one of the cities on the river has the highest score in our result set, and this city happens to be far from the point assigned to the river by the author of the article, then this may give a very large error value even though our output may be considered accurate. It may make more sense for a case like this to consider a group of locations that are returned by the geolocation algorithm. We therefore redefined our error measure to be the shortest distance between the “true location” and any of the five locations produced by our geolocation algorithm with the greatest scores, rather than just the top location with the greatest score.\nFigure 6 is a modified version of Figure 1 which uses this Top 5 error measurement. Here we see the curves converge closer to each other than in Figure 1, meaning the 16 algorithms are more similar in this case. We also see a significant improvement in error. For example, the Weighted Inverse Frequency (1 phase) algorithm had a 10th percentile error of 490 metres with the Top 1 error, while for the Top 5 this error is 163 metres. Similarly, the error at the 25th percentile improved from 4.70 km to 0.87 km. Finally, the median error improved from 54 km to 8.5 km. This new error measurement makes our median error similar to those observed by Roller et al. [2] using a classification\n27/35\napproach. However, this error measure has access to the true answer can and pick the top 5 result that is closest to it. This does tell us that we might significantly improve our accuracy by using a more-informed scoring function to rank the final results. This is left for future work. Recall also that the maximum accuracy of classification-type approaches is effectively limited by the class size, and the accuracies we achieve for large numbers of articles (e.g., 0.87 km error at the 25th percentile) cannot be attained by classification approaches."
    }, {
      "heading" : "4 Results for Twitter Test Data",
      "text" : "We now turn to tests with Twitter data, to further evaluate the performance of our geolocation algorithm. We consider geotagged tweets, i.e., tweets that contain the geographic location from which they were sent in their metadata. We will attempt to geolocate these tweets (or concatenations of tweets sent out by the same user) solely based on the text in the tweets, and will assess which fraction of the tweets (or concatenations) are geolocated by our algorithm to a location close to the location from which the tweets were sent (which we consider as the “true location” for assessing our geolocation algorithm for tweets). Geolocating tweets is inherently challenging: tweets are short and unstructured, and are less likely to contain NER LOCATIONs. This means that our geolocation algorithm will more often need to rely on POS prepositions and nouns (see Algorithm 5). Moreover, unlike the Wikipedia articles about well-defined geographic entities that were specifically geotagged by their authors, the contents of most geotagged\n28/35\ntweets may actually not at all be about the location from which the tweet was sent. We thus expect our algorithm to geolocate few tweets to a location close to the location from which the tweets were sent. In fact, in experiments reported below we estimate an upper bound on the fraction of geotagged tweets that can in principle be geolocated to the location they were sent from based on information in the tweet text. Regardless of these limitations, the tests reported below are an interesting way to assess the possible applicability of our approach for short and unstructured text messages, and as such complement the Wikipedia results discussed in Section 3."
    }, {
      "heading" : "4.1 Test Data",
      "text" : "We have compiled a Twitter dataset containing nearly all geotagged tweets from Canada, the US, the Netherlands, and Australia for the period of February, March, and April 2015. The dataset contains Tweets from 3,764,507 unique users. The dataset was collected using the Twitter API via the Spark framework for data analytics, as explained in Appendix C.\nIt is interesting to first consider what fraction of geotagged tweets contain words or sequences of words that can be interpreted (using a knowledge base) as locations within a reasonable distance of the location the tweet was sent from. In particular, we considered a 10% sample of the February 2015 tweets we collected, consisting of 3,466,922 geotagged tweets from our larger dataset. For each tweet we considered all word sequences with up to 5 words (no POS or NER was used; we just considered every possible sequence of 5 or less consecutive words). We then did a case-insensitive search through all locations in the GeoNames knowledge base (http://www.geonames.org). This search included “alternative names” for locations with more than one name. We measured the distance between each match and the coordinates of the tweet, and kept the shortest distance for each tweet. Figure 7 shows a cumulative plot of the distance between the tweet coordinates and the closest mention in the tweet text, as a function of the fraction of tweets. We see immediately that few tweets contain location references in the tweet text that are close to the tweet coordinates. In particular, we find that\n• 82.5% of tweets contain a word sequence that matches a location in GeoNames\n• 2.2% of tweets mention a location within 10 km\n• 5.4% of tweets mention a location within 100 km\n• 8.3% of tweets mention a location within 161 km (100 miles)\nThis shows that we cannot expect high accuracy when attempting to geolocate tweets and comparing to the location the tweet was sent from. Nevertheless, it is interesting to see whether our approach manages to correctly geolocate tweets that do contain relevant location information in the text. (Note that we used GeoNames instead of OpenStreetMap for this particular test, since the GeoNames data was in a more practical format for our current purposes. The general points we made here do not depend on whether one uses GeoNames or OpenStreetMap.)"
    }, {
      "heading" : "4.2 Results",
      "text" : "Since we cannot expect high recall accuracy for single tweets, we first consider concatenations of tweets sent by the same user, which may have more useful NER LOCATIONs or POS nouns/prepositions to base geolocation on.\nIn our first set of tests, we combined, for every user, the text of all the user’s tweets into one text. For each user, we set the user’s latitude and longitude to be the mean\n29/35\nlatitude and longitude of all the user’s tweets in the dataset. We considered this for a sample of 882 users, and find the following results:\n• 10th percentile: 603 km\n• 25th percentile: 2,222–2,247 km (note: we report the range for the 16 different methods)\n• 50th percentile: 6,646–6,665 km\nNote that, as expected, this compares poorly with the Wikipedia results from Figures 2–4, where the best algorithm produced an error of about 0.5 km at the 10th percentile, 5.5 km at the 25th percentile, and 53 km at the 50th percentile. It is, however, roughly in line with the GeoNames analysis above, which showed that only 8.3% of tweets mention a location within 161 km.\nIt is interesting to consider whether our geolocation algorithm found NER LOCATIONs, or had to rely on POS prepostions or nouns. Table 11 shows that NER LOCATIONs were only encountered in 1.47% of the (combined) texts, in stark opposition to the 99.45% for geotagged Wikipedia articles, see Table 9. Instead, our geolocation algorithm had to rely on POS nouns for 48.07% of the texts, and on POS prepositions for 33.56% of the texts. Also, for 9.30% of the (combined) Twitter texts, terms were considered, but there were no Nominatim results for any of the terms in the term list, and for 7.60% of the texts no terms were found by Algorithm 5. This shows that geotagged Twitter texts (even when combined) are much harder to geolocate than Wikipedia articles that describe geographic entities.\nFor completeness, we briefly report on test results for single tweets. We considered 18,720 tweets, and find a 10th percentile error in the range 1,042–1,043 km (range for the 16 different methods), a 25th percentile in the range 3,406–3,414 km, and a 50th percentile error in the range 7,882–7,892 km. The results for the case of tweets combined by user that were reported above are somewhat better, as expected.\n30/35"
    }, {
      "heading" : "5 Discussion",
      "text" : "This paper has presented a geolocation algorithm for finding precise locations that are named in text. It uses part-of-speech tagging and named entity recognition to find potential location references. It then uses the open-source tool Nominatim to search the OpenStreetMap database and obtain a list of possible locations for each unique phrase. Finally, conflicting terms are resolved and one location is chosen for each phrase. The algorithm returns a list of non-conflicting terms and the location that was chosen for each of them.\nSixteen versions of the algorithm were compared, with similar results. The Weighted Inverse Frequency scoring function (Equation (11)) with the 1-phase disambiguation algorithm (Algorithm 3) performed consistently near the top, and was subsequently used to report detailed accuracy results.\nWhen the location with the highest score was chosen to represent the overall location of the article, our median error was 54 km. This is worse than the 11.0 km reported by Roller et al. [2] for a classification approach, but many of our articles are geolocated with much higher precision than what can be achieved by classification approaches (for example, 25% of articles were geolocated with an error below 5.5 km). Also, when we consider the closest of the five highest-scoring results our median error reduces to 8.5 km. This implies that our algorithm, which is designed to return multiple locations, includes some very accurate results in most cases. Note that future work on the final sorting of results may improve our 54 km median error.\nThere are many ways in which our geolocation algorithm can be improved in future work. Although the algorithm was developed heuristically, it is a good starting point for a new class of algorithms for high-precision location tagging.\nFor example, Habib [7] demonstrated that there is a feedback loop between the extraction step and the disambiguation step in named entity recognition. Results from the disambiguation step can be used to improve results from the extraction step, which can then improve results for the disambiguation step. This concept was partially used in this paper. In particular, the extraction step gathered all possible interpretations, then the disambiguation step refined the list of extracted terms by resolving those that conflicted. Our geolocation algorithm could benefit from more use of this feedback loop. The disambiguation algorithm could be modified to discard terms which have extremely low scores, not just those that conflict with better terms, as these terms may not refer to real locations and should not have been included in the extraction step. This may improve accuracy when disambiguating the other terms in the text.\nIf our geolocation algorithm is to be applied to a specific type of text, then the algorithm could be improved by using part-of-speech taggers and named entity recognizers that are specifically trained for the target style of text. For example, the Stanford POS tagger we used could be replaced by the state-of-the-art Twitter POS tagger developed by Derczynski et al. [16] when geolocating tweets.\nIt may be interesting to consider applying elements of our approach, for example, our\n31/35\ndistance-based scoring, to more general named entity disambiguation frameworks such as AIDA [10,13]. This may benefit the accuracy of these frameworks when the entities to be disambiguated include locations."
    }, {
      "heading" : "A External Software",
      "text" : "The following software was used in the implementation of our project.\n• Apache Spark version 1.4.0. Based on the work of [17]. Available at http: //spark.apache.org/.\n• Nominatim. It was queried using the API described at http://wiki.openstreetmap. org/wiki/Nominatim. It can also be used through a user interface at https: //nominatim.openstreetmap.org/.\n• PHP-Stanford-NLP. October 18, 2014 version. Written by Anthony Gentile. Available at https://github.com/agentile/PHP-Stanford-NLP. Used as a PHPwrapper to access the Stanford POS tagger and NER.\n• Stanford Log-linear Part-Of-Speech Tagger version 3.5.2 (April 20, 2015). Based on the work of [4] and [5]. Available at http://nlp.stanford.edu/software/ tagger.shtml. There are multiple pre-trained models available with this software, and our project used the model titled english-left3words-distsim.\n• Stanford Named Entity Recognizer version 3.5.2 (April 20, 2015). Based on the work of [6]. Available at http://nlp.stanford.edu/software/CRF-NER.shtml. There are multiple pre-trained models available with this software, and our project used the model titled english.all.3class.distsim.crf.\n• WikiExtractor. June 14, 2015 version. Written by Giuseppe Attardi. Available at https://github.com/attardi/wikiextractor."
    }, {
      "heading" : "B Wikipedia Data",
      "text" : "At the time of writing, data dumps of the English Wikipedia could be obtained from https://dumps.wikimedia.org/enwiki/. This paper used articles from a dump of the English Wikipedia on June 2, 2015 (file name enwiki-20150602-pages-articles.xml.bz2). Location tags were also obtained from the June 2, 2015 dump, and are available as a separate download on the same site (file name enwiki-20150602-geo tags.sql.gz). Our full dataset contained 920,176 geotagged Wikipedia articles, and our tests used a sample of 5,976 articles.\nThe raw dump file was processed using WikiExtractor (https://github.com/attardi/ wikiextractor), which takes the XML file and produces the raw text for each article, removing special formatting and annotations such as images, tables, or references. However, the output of WikiExtractor does not contain location tags, so we had to join this data with the set of location tags ourselves. The geo tags file contains all location tags in the English Wikipedia. There are two types of tags: primary and non-primary. Primary tags apply to an entire article. Non-primary tags apply to a particular mention in the text of an article. Our project only used primary tags.4\n4 It is worth noting that our set of articles is different from the one created by Wing and Baldridge [1], which was also used by Roller et al. [2]. They used a dump of the English Wikipedia from September 4, 2010 while we used a dump from June 2, 2015. Their dataset contained 10,355,226 articles (including non-geotagged ones), while ours contained 4,855,711. It should be expected that the more recent data would contain more articles, but the opposite is observed. However, Wing and Baldridge had to explicitly\n32/35\nThese two data sets were joined using Apache Spark, which is a scalable cluster computing system [17]. Spark now has a highly active community of both developers and users. Spark’s APIs made it easy to read in both data sets, match location tags to their articles, and output in a clear format using less than 80 lines of Scala code. The output of this Spark program was directly usable by our geolocation algorithm implementation."
    }, {
      "heading" : "C Twitter Data",
      "text" : "Twitter provides an API to allow applications to stream a fraction of the publicly available tweets (https://dev.twitter.com/streaming). Each application can receive up to approximately 1% of public tweets, though the sample of tweets that are received can be adjusted with the API. Apache Spark’s streaming library (http://spark.apache. org/streaming/) includes a wrapper for Twitter’s API, which allows these tweets to be processed using Spark. A Spark Streaming application was written to receive a stream of tweets and save them to Parquet files for further processing. This program was run for several months to obtain the dataset that was used in Section 4.\nNote that Twitter’s API allows applications to specifically request geotagged tweets (i.e., tweets that contain the location from which the tweet was sent as part of the tweet’s metadata), but Spark’s wrapper for Twitter does not implement this feature. The Spark source code was modified for this paper to allow requests for geotagged tweets within a specified range of coordinates (we specify Canada, the US, the Netherlands, and Australia). Without this, the 1% sample of tweets would be a random sample of all public tweets on Twitter, and would not specifically be composed of geotagged tweets. By requesting geotagged tweets from these specific regions, the requested sample was less than 1% of the total amount of public tweets. This meant that the Spark Streaming application collected nearly all public geotagged tweets in those areas."
    }, {
      "heading" : "D Code and Data",
      "text" : "The code for our geolocation algorithm can be obtained at https://github.com/ spotzi/Geotagger.\nThe Spark programs we used to pre-process the Wikipedia data and to collect and pre-process the Twitter data can be obtained at https://github.com/sjbrunst/ GeoTextTagger-data. The Wikipedia and Twitter data we used for our tests can also be obtained at https://github.com/sjbrunst/GeoTextTagger-data."
    } ],
    "references" : [ {
      "title" : "Simple Supervised Document Geolocation with Geodesic Grids. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL): Human Language Technologies (HLT)",
      "author" : [ "BP Wing", "J. Baldridge" ],
      "venue" : "Stroudsburg, PA,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Text-based Twitter User Geolocation Prediction",
      "author" : [ "B Han", "P Cook", "T. Baldwin" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-speech Tagger",
      "author" : [ "Toutanova K", "Manning CD" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "Feature-rich Part-of-speech Tagging with a Cyclic Dependency Network",
      "author" : [ "K Toutanova", "D Klein", "CD Manning", "Y. Singer" ],
      "venue" : "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL). Stroudsburg, PA,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling",
      "author" : [ "JR Finkel", "T Grenager", "C. Manning" ],
      "venue" : "Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Named entity extraction and disambiguation for informal text: the missing link",
      "author" : [ "Habib Morgan MB" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Learning to link with wikipedia",
      "author" : [ "Milne D", "Witten IH" ],
      "venue" : "Proceedings of the 17th ACM conference on Information and knowledge management",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Collective annotation of Wikipedia entities in web text",
      "author" : [ "S Kulkarni", "A Singh", "G Ramakrishnan", "S. Chakrabarti" ],
      "venue" : "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Robust disambiguation of named entities in text",
      "author" : [ "J Hoffart", "MA Yosef", "I Bordino", "H Fürstenau", "M Pinkal", "M Spaniol" ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics;",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "AGDISTISgraph-based disambiguation of named entities using linked data. In: The Semantic Web–ISWC",
      "author" : [ "R Usbeck", "ACN Ngomo", "M Röder", "D Gerber", "SA Coelho", "S Auer" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Entity linking meets word sense disambiguation: a unified approach",
      "author" : [ "A Moro", "A Raganato", "R. Navigli" ],
      "venue" : "Transactions of the Association for Computational Linguistics",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "An online tool for accurate disambiguation of named entities in text and tables",
      "author" : [ "MA Yosef", "J Hoffart", "I Bordino", "M Spaniol", "Aida Weikum G" ],
      "venue" : "Proceedings of the VLDB Endowment",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Openstreetmap: User-generated street maps",
      "author" : [ "M Haklay", "P. Weber" ],
      "venue" : "Pervasive Computing,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Part-Of-Speech Tagging Guidelines for the Penn Treebank Project (3rd revision)",
      "author" : [ "B. Santorini" ],
      "venue" : "Philadelphia, PA, USA: Department of Linguistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1990
    }, {
      "title" : "Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data",
      "author" : [ "L Derczynski", "A Ritter", "S Clark", "K. Bontcheva" ],
      "venue" : "Recent Advances in Natural Language Processing;",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Resilient Distributed Datasets: A Fault-tolerant Abstraction for In-memory Cluster Computing",
      "author" : [ "M Zaharia", "M Chowdhury", "T Das", "A Dave", "J Ma", "M McCauley" ],
      "venue" : "Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation (NSDI). Berkeley, CA,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For example, several papers in the location tagging literature [1, 2] use test data sets of Wikipedia articles about topics with a well-defined geographical location, and for which the Wikipedia authors have added a geographical location tag to each article.",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "Wing and Baldridge created classes using simple geodesic grids of varying sizes [1].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "This is an improvement over the previous work [1].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "focused on location tagging of Twitter messages [3].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "[4, 5].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "[4, 5].",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "The other type of model is known as the bidirectional model [5].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "[6].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "Habib [7] explored both the extraction and disambiguation steps for named entity recognition, along with the link between these steps.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "Named entities which refer to locations are called toponyms, and a major portion of Habib’s [7] work discusses the extraction and disambiguation of toponyms in semi-formal text.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : ", [8–12] and references therein.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : ", [8–12] and references therein.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : ", [8–12] and references therein.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : ", [8–12] and references therein.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : ", [8–12] and references therein.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : "For example, the AIDA system [10,13] is a general framework for collective disambiguation exploiting the prior probability of an entity being mentioned, similarity between the context of the mention and its candidates, and the coherence among candidate entities for all mentions together.",
      "startOffset" : 29,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "For example, the AIDA system [10,13] is a general framework for collective disambiguation exploiting the prior probability of an entity being mentioned, similarity between the context of the mention and its candidates, and the coherence among candidate entities for all mentions together.",
      "startOffset" : 29,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "Our approach, which uses the OpenStreetMap knowledge base [14], is specific to geolocation, and provides a high-precision and versatile method for geolocation of texts.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "The full list of possible tags is given in [15], but our algorithm only looks for a subset of tags which are relevant to our problem.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : "OpenStreetMap is used as the knowledge base for this paper [14].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "1: Tag the input text using the POS and NER taggers from [4–6].",
      "startOffset" : 57,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "1: Tag the input text using the POS and NER taggers from [4–6].",
      "startOffset" : 57,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "1: Tag the input text using the POS and NER taggers from [4–6].",
      "startOffset" : 57,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "For example, Habib [7] demonstrated that there is a feedback loop between the extraction step and the disambiguation step in named entity recognition.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "[16] when geolocating tweets.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "distance-based scoring, to more general named entity disambiguation frameworks such as AIDA [10,13].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "distance-based scoring, to more general named entity disambiguation frameworks such as AIDA [10,13].",
      "startOffset" : 92,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "Based on the work of [17].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Based on the work of [4] and [5].",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "Based on the work of [4] and [5].",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "Based on the work of [6].",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "4 It is worth noting that our set of articles is different from the one created by Wing and Baldridge [1], which was also used by Roller et al.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "These two data sets were joined using Apache Spark, which is a scalable cluster computing system [17].",
      "startOffset" : 97,
      "endOffset" : 101
    } ],
    "year" : 2016,
    "abstractText" : "Location tagging, also known as geotagging or geolocation, is the process of assigning geographical coordinates to input data. In this paper we present an algorithm for location tagging of textual documents. Our approach makes use of previous work in natural language processing by using a state-of-the-art part-of-speech tagger and named entity recognizer to find blocks of text which may refer to locations. A knowledge base (OpenStreatMap) is then used to find a list of possible locations for each of these blocks of text. Finally, one location is chosen for each block of text by assigning distance-based scores to each location and repeatedly selecting the location and block of text with the best score. We tested our geolocation algorithm with Wikipedia articles about topics with a well-defined geographical location that are geotagged by the articles’ authors, where classification approaches have achieved median errors as low as 11 km. However, the maximum accuracy of these approaches is limited by the class size, so future work may not yield significant improvement. Our algorithm tags a location to each block of text that was identified as a possible location reference, meaning a text is typically assigned multiple tags. When we considered only the tag with the highest distancebased score, we achieved a 10th percentile error of 490 metres and median error of 54 kilometres on the Wikipedia dataset we used. When we considered the five location tags with the greatest scores, we found that 50% of articles were assigned at least one tag within 8.5 kilometres of the article’s author-assigned true location. We also tested our approach on a set of Twitter messages that are tagged with the location from which the message was sent. This dataset is more challenging than the geotagged Wikipedia articles, because Twitter texts are shorter, tend to contain unstructured text, and may not contain information about the location from where the message was sent in the first place. Nevertheless, we make some interesting observations about potential use of our geolocation algorithm for this type of input. We explain how we use the Spark framework for data analytics to collect and process our test data. In general, classification-based approaches for location tagging may be reaching their upper limit for accuracy, but our precision-focused approach has high accuracy for some texts and shows significant potential for improvement overall.",
    "creator" : "LaTeX with hyperref package"
  }
}