{"conference": "ICLR 2017 conference submission", "title": "Neural Architecture Search with Reinforcement Learning", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "reviews": [{"is_meta_review": true, "comments": "This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.\nThe pros of the paper are:\n1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.\n2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.\n\nThe cons of the paper are:\n1. The training time of the network is long, even with a lot of computing resources. \n2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.\n\nOverall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models.", "IS_META_REVIEW": true}, {"TITLE": "Hyperparameters for SotA PTB word level LM", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "The paper reports that \"[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.\"\n\nIs it possible for you to include (or provide here) the hyperparameters and type of dropout (i.e. recurrent dropout, embedding dropout, ...) used? Without them, replication would at best require a great deal of trial and error. As with \"Recurrent Neural Network Regularization\" (Zaremba et al., 2014), releasing a base set of hyper parameters greatly assists in the future work of the field.\n\nThis will likely also be desired for the other experiments, such as character LM.", "IS_META_REVIEW": false, "DATE": "19 May 2017 (modified: 02 Jun 2017)", "is_meta_review": false}, {"TITLE": "Inconsistent performance numbers between abstract and section 1 for CIFAR-10 network ? ", "OTHER_KEYS": "(anonymous)", "comments": "The abstract says: \"Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than\nthe previous state-of-the-art model\" while the last paragraph of section one has: \"Our CIFAR-10 model achieves a 3.84 test set error, while being 1.2x faster than the current best model.\". Should these be the same numbers or what is the reason for the difference (or is there a newer revision of the paper) ?", "IS_META_REVIEW": false, "DATE": "25 Apr 2017", "is_meta_review": false}, {"TITLE": "Auto-regressive model", "OTHER_KEYS": "(anonymous)", "comments": "In the paper, you wrote that the controller is auto-regressive and that \"each prediction is fed into the next time step as input\". What is the input to the controller for the prediction of the first token - a random seed or constant?\n\nAdditionally, is the controller an extension of the sequence decoder used in sequence-to-sequence learning? What are the important changes between the NAS controller and the S2S decoder?\n\nThanks for the excellent paper and congratulations on the acceptance!", "IS_META_REVIEW": false, "DATE": "09 Feb 2017", "is_meta_review": false}, {"TITLE": "ICLR committee final decision", "OTHER_KEYS": "ICLR 2017 pcs", "comments": "This was one of the most highly rated papers submitted to the conference. The reviewers all enjoyed the idea and found the experiments rigorous, interesting and compelling. Especially interesting was the empirical result that the model found architectures that outperform those that are used extensively in the literature (i.e. LSTMs).", "IS_META_REVIEW": false, "DATE": "06 Feb 2017", "is_meta_review": false}, {"TITLE": "Code available for the design space (not just the final models)?", "OTHER_KEYS": "(anonymous)", "comments": "Thanks for a very interesting paper!\n\nIt is nice to see that the authors will make the code for running the models found by their controller available.\nI am wondering whether the code for the design space will also be available. I.e., the spaces from which the policy samples, for CIFAR and Penn Treebank -- e.g. as code with free choices. That would make it much more likely that someone else can reproduce these results. Thanks!\n", "IS_META_REVIEW": false, "DATE": "02 Feb 2017", "is_meta_review": false}, {"TITLE": "Control experiments", "OTHER_KEYS": "(anonymous)", "comments": "Thank you for the really interesting paper. I have a few questions about the control experiments.\n\nIn control experiment 1, how well did the model with the max/sin functions do in terms of perplexity? That seems perhaps more important than how similar the architecture itself is.\n\nWhat was the distribution over architectures used by random search in control experiment 2? Did you also do grid finetuning of hyperparameters with random search?", "IS_META_REVIEW": false, "DATE": "06 Jan 2017", "is_meta_review": false}, {"TITLE": "On validation dataset", "OTHER_KEYS": "(anonymous)", "comments": "Dear Authors,\n\nThank you for sharing such an impressive work. I just have a question, in your experimental part, \"We then run a small grid search over learning rate, weight decay,\nbatchnorm epsilon and what epoch to decay the learning rate.\" May I know that in such a grid search, is the validation accuracy also computed on your 5,000 validation samples from which the reward (used in REINFORCE) is computed?\n\nThanks a lot!", "IS_META_REVIEW": false, "DATE": "06 Jan 2017", "is_meta_review": false}, {"TITLE": "Some relevant related work", "OTHER_KEYS": "(anonymous)", "comments": "Overall, I find it very exciting that the actor-critic framework can produce such good results. \n\nThere is some related work in Bayesian optimization for architecture search that the authors may not know about. The authors are correct in stating that standard Bayesian optimization methods are limited to a fixed-length space, but there are also some Bayesian optimization methods that support so-called conditional parameters that allow them to effectively search over variable-depth architectures. Two particularly relevant papers are: \n\n1. ", "IS_META_REVIEW": false, "DATE": "20 Dec 2016", "is_meta_review": false}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "TITLE": "Review", "is_meta_review": false, "comments": "This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.\n\nThe paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.\n\nThis is a well written paper on an interesting topic with strong results. I recommend it be accepted.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "17 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "TITLE": "Review", "is_meta_review": false, "comments": "This paper presents search for optimal neural-net architectures based on actor-critic framework. The method treats DNN as a variable length sequence, and uses RL to find the target architecture, which acts as an actor. The node selection is an action in the RL context, and evaluation error of the outcome architecture corresponds to reward. A auto-regressive two-layer LSTM is used as a controller and critic. The method is evaluated on two different problems, and each compared with number of other human-created architectures.\n\nThis is very exciting paper! Hand selecting architectures is difficult, and it is hard to know how far from optimal results the hand-designed networks are. The presented method is  novel. The authors do an excellent job of describing it in detail, with all the improvements that needed to be done. The tested data represents well the capability of the method. It is very interesting to see the differences between the generated architectures and human generated ones. The paper is written very clearly, and is very accessible. The coverage and contrast with the related literature is done well.\n\nIt would be interesting to see the data about the time needed for training, and correlation between time/resources needed to train and the quality of the model. It would also be interesting to see how human bootstrapped models perform and involve.\n\nOverall, an excellent and interesting paper.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "16 Dec 2016", "REVIEWER_CONFIDENCE": 4}, {"OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "TITLE": "A nice paper", "is_meta_review": false, "comments": "This paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.\nThe pros of the paper are:\n1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.\n2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.\n\nThe cons of the paper are:\n1. The training time of the network is long, even with a lot of computing resources. \n2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.\n\nOverall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models.", "IS_META_REVIEW": false, "RECOMMENDATION": 9, "DATE": "14 Dec 2016", "REVIEWER_CONFIDENCE": 5}, {"TITLE": "How do you position yourself to: https://arxiv.org/abs/1606.02492 \"Convolutional Neural Fabrics\"", "OTHER_KEYS": "(anonymous)", "comments": "They obtain a performance which is worse that reported in your work, however they train their model on a single GPU in contrast to 800 used in the proposed work.\n\n", "IS_META_REVIEW": false, "DATE": "13 Dec 2016", "is_meta_review": false}, {"TITLE": "several questions", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer3", "comments": "", "IS_META_REVIEW": false, "DATE": "08 Dec 2016", "is_meta_review": false}, {"TITLE": "How many networks were trained in total? 102.400 for CIFAR-10?", "OTHER_KEYS": "(anonymous)", "comments": "This is very interesting work. \nI am wondering how many networks you trained in total. For CIFAR-10, you say you trained the controller for 12,800 iterations. Since every controller used m=8 child replicas in every iteration, does this mean you trained 12.800 * 8 = 102.400 networks in total for CIFAR-10? Thanks!", "IS_META_REVIEW": false, "DATE": "07 Dec 2016", "is_meta_review": false}, {"TITLE": "Why CPUs, reward choices, generalization, BPTT length, dataset choice", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer2", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016 (modified: 02 Jun 2017)", "is_meta_review": false}, {"TITLE": "Generalization", "OTHER_KEYS": "ICLR 2017 conference AnonReviewer4", "comments": "", "IS_META_REVIEW": false, "DATE": "03 Dec 2016", "is_meta_review": false}, {"TITLE": "Request for details on running time and number of machines", "OTHER_KEYS": "(anonymous)", "comments": "This is a very interesting paper! Can you please provide some information on the amount of time that was required in order to train your model and the hardware that you used?", "IS_META_REVIEW": false, "DATE": "30 Nov 2016", "is_meta_review": false}], "SCORE": 9, "authors": "Barret Zoph, Quoc Le", "accepted": true, "id": ""}
