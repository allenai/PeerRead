{
  "name" : "1206.6442.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Minimizing The Misclassification Error Rate  Using a Surrogate Convex Loss",
    "authors" : [ "Shai Ben-David", "David Loker", "Nathan Srebro", "Karthik Sridharan" ],
    "emails" : [ "shai@cs.uwaterloo.ca", "dloker@cs.uwaterloo.ca", "nati@ttic.edu", "skarthik@wharton.upenn.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Perhaps the most fundamental question studied in the theory of machine learning is that of binary classification with half-spaces. However the problem of agnostically learning half-spaces is known to be NP-hard in general (Kearns et al., 1994). Even when one only wants to learn a half-space relative to the best possible M -margin error, Ben-david & Simon (2000) show that (subject to P 6= NP ) there exists no proper learning algorithm (i.e. returning a linear predictor) that runs in time polynomial in both 1/M and the desired accuracy. Under a cryptographic hardness assumption, (Shalev-Shwartz et al., 2010) extend this result to improper learning (i.e. when the algorithm may output any predictor, as long as it generalizes well).\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nIn practice, one typically reverts to minimizing a convex surrogate, such as the hinge-loss, squared loss, logistic loss, exp-loss etc. Minimizing such a convex loss is usually easy, and can be done in polynomial time, but the question then is how well does minimizing such a convex surrogate perform relative to minimizing the actual classification error. An important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of “classification calibrated” loss functions (Zhang, 2004; Bartlett et al., 2006). As discussed in detail in Section 4, this notion is relevant only when the Bayes predictor is in our hypothesis class—i.e. for linear prediction, when the Bayes predictor is exactly linear. Here we consider the more realistic agnostic setting, where we make no such assumption. Instead, we only rely on the existence of some linear predictor with small error rate at some margin, and ask the question of what misclassification error can be guaranteed by minimizing a convex loss. We obtain guarantees for specific loss functions, which allow us to compare between them, as well as a lower bound that holds for any convex loss. We now proceed to discuss and compare our work with other related work."
    }, {
      "heading" : "1.1. More Related Work",
      "text" : "Several other authors, beyond those discussed in the previous section, also address the question of choosing appropriate surrogate loss functions for binary classification. Masnadi-Shirazi & Vasconcelos (2009) and Nock & Nielsen (2008) study various choices of surrogate losses for the classification problem and argue that the “right” loss depends on the underlying un-\nknown distribution. In the agnostic setting, which we consider here, we do not know the underlying distribution, and so seek a distribution free guarantee. Christmann & Steinwart (2004) investigate robustness properties of learning algorithms that are based on convex risk minimization in terms of the so called influence function corresponding to the loss. They argue that most statistical models are intended to be approximations of true model generating data. Robustness of the method basically implies that when the true underlying distributions deviate from the model only slightly then the performance of the method is not affected much. Hence, they use this to argue about the e cacy of algorithms based on convex losses. While robustness property tells us that deviating from the model by a small amount doesn’t a↵ect performance much, it does not address the issue of agnostic learning where no assumptions are made about the underlying distribution (except that a hypothesis in the hypothesis class has low risk). In contrast, our work directly looks at how minimizing a surrogate loss corresponds to minimizing the misclassification error rate without any assumptions about the model generating the data or assuming even that the approximate Bayes optimal, w.r.t. the surrogate loss, belongs to the hypothesis class.\nIn terms of directly minimizing the misclassification error rate, without using a convex surrogate loss, Kalai et al. (2008) provide non-asymptotic finite sample bounds for e cient binary prediction with half spaces. However, to do so they assume the inputs are uniformly distributed on the surface of a sphere, which is an extremely strong an unrealistic assumption. Similarly, Kalai & Sastry (2009) also provide an e cient algorithm for minimizing the misclassification error, but only under the assumption that the conditional distribution of the label given the input x is some monotonic function of w · x for some w—again significantly departing from the agnostic setting.\nAs mentioned earlier, although they focus on boosting (coordinate descent optimization), Long & Servedio (2008) essentially establish that if one does not assume that margin error, ⌫, of the optimal linear classifier is small enough then any algorithm minimizing any convex loss (which they think of as a “potential”) can be forced to su↵er a large misclassification error. They do not, however, consider the bounded-margin case, that is when the margin error ⌫ of the best linear classifier is small compared to the margin M . In concurrent work, (Long & Servedio, 2011) do show that using convex surrogate losses for learning can at best only guarantee that the zero-one is bounded by O(⌫/M), which is a result very similar to our Theorem 4, though this\na much higher constant. They do not however show lower bounds for specific loss functions which gives one the tool to compare various convex surrogate losses in a precise manor. More interestingly, they provide a randomized improper learning algorithm whose zeroone loss is bounded by ⌫/(M log( 1\nM ))+ ✏ in time polynomial in 1/M and 1/✏ (where ⌫ is the misclassification error rate at margin M). This shows that, at least when improper learning is allowed (i.e. we are allowed to return a non-linear predictor, as long as it generalizes well), it is possible to do (slightly) better than minimizing a convex surrogate. The question of whether this is possible with proper learning remains open."
    }, {
      "heading" : "2. Setting",
      "text" : "Let D(x, y) be a distribution over U⇥{+1, 1}, where, for some d, U = {x 2 Rd : kxk  1} is the ddimensional unit sphere. We will often actually consider finite samples, in which case D should be understood as a uniform distribution over points in the sample.\nA linear predictor is described by a vector and a bias term: (w, w0), w 2 Rd, w0 2 R. For a loss function : R! R, the -risk is give by:\nRD (w, w0) = E(x,y)⇠D [ (y(hw,xi+ w0))]. When the distribution D is understood from the context, we will simply use R\n(w, w0). We will be particularly interested in the 0-1 loss 01(z) = 1 {z  0} and the margin-loss\nm (z) = 1 {z < 1}, and the corresponding risks:\nR01(w, w0) = R 01(w, w0) = Pr (y(hw,xi+ w0)  0) R\nm (w, w0) = R m(w, w0) = Pr (y(hw,xi+ w0) < 1) Note that we are considering a prediction of zero as an error both for the positive class and the negative class, thus always predicting zero yields an error rate of one. We are also using\nm and R m to denote the error relative to a margin of 1, and so we will actually encode the margin through the norm of w, i.e. the actual margin is 1/ kwk."
    }, {
      "heading" : "3. Misclassification Error Guarantee",
      "text" : "We begin by showing that for any convex loss function, and any arbitrarily low misclassification error rate ⌫ > 0, there exists a sample which is linearly separable with error rate ⌫, but for which the predictor minimizing the surrogate loss would have error rate arbitrarily close to 1. In other words, there are\ntraining samples over which an algorithm that minimizes the surrogate loss will output a classifier whose actual training error is close to 1, in spite of the fact that these samples can be classified with small error by another half-space (which is missed by the loss minimization algorithm, due to having high surrogate loss). Furthermore, such examples exist even if the domain space is just the real interval. A similar result was also essentially shown by Long & Servedio (2008)— they discuss “boosting”, i.e. loss minimization by coordinate descent, here we refer more directly to the loss minimizer.\nFor a loss (·), define the Misclassification Error Guarantee relative to the zero-one loss as:\nEG( , ⌫) = sup D s.t. 9w, w0, R01(w, w0)  ⌫ sup (ŵ, ŵ0) s.t. 8w,w0 R (ŵ, ŵ0)  R (w, w0) R01(ŵ, ŵ0) (1)\nThat is, we are asking: what is the largest misclassification error su↵ered by the linear predictor minimizing -risk when the underlying distribution is such that the misclassification error rate of the best half-space is bounded by ⌫.\nWhen ⌫ = 0, i.e. in the separable case, this is essentially a question about “classification calibration”, and we have that for convex , EG( , 0) = 0 if and only if is di↵erentiable at zero and 0(0) < 0 (Bartlett et al., 2006)—see Section 4. This is a mild condition that holds for most common loss functions, but here we are interested in EG( , ⌫) for ⌫ > 0.\nOur initial claim can be state as follows:\nProposition 1. For any convex function , and any ⌫ > 0, EG( , ⌫) = 1.\nThis follows from the one-dimensional source distribution below:\n• There are ⌫/2 points at x = 1 labelled +1\n• There are ⌫/2 points at x = +1 labelled 1\n• There are 1 ⌫2 points at x = M labelled +1\n• There are 1 ⌫2 points at x = M labelled 1\nHere, the optimal linear predictor is one that labels points to the right of 0 as positive and left of 0 as negative, yielding misclassification error ⌫. However as M ! 0, the minimizer of any classification calibrated convex surrogate loss will label points to the right of 0 as negative and left of 0 as positive because the points close to 0 (M close) su↵er small loss under the convex loss. This simple example shows that EG( , ⌫) = 1 for any convex loss."
    }, {
      "heading" : "3.1. Surrogate loss minimization when margins exist",
      "text" : "We have just shown that the existence of a linear predictor with low misclassification error is not enough to ensure the success of surrogate loss minimization. Our next step is to analyze the success of this paradigm under stronger assumptions - the existence of a good linear classifier with respect to some positive margin. Our next definition considers the worst possible error of a surrogate loss minimizer when the data allows a low error classifier with some margin. Our main result in this section, Theorem 4, implies that, up to a factor of 2, the hinge loss is optimal among all convex loss functions in that respect.\nEG( , ⌫, B)\n= sup D s.t. 9w, w0, kwk  B, Rm(w, w0)  ⌫ sup (ŵ, ŵ0) s.t. 8w, w0, R (ŵ, ŵ0)  R (w, w0) R01(ŵ, ŵ0) (2)\nHere, B specifies the margin, and we will sometimes refer to it directly as M = 1/B. We have that EG( , ⌫) = EG( , ⌫,1). A more careful look at the lower bound on EG( , ⌫) in the previous section reveals that for ⌫ 1/(B + 1) = M/(M + 1), we have EG( , ⌫, B) = 1 for any convex loss . However for smaller values of ⌫, it is possible to get meaningful bounds on EG( , ⌫, B). In particular, for the hinge loss the simple observation that B + 1 times the margin loss upper bounds the hinge loss in the interval [ B,B] gives the below upper bound on EG( , ⌫, B). Proposition 2. For the hinge loss hinge(z) = max(0, 1 z), we have that\nEG( hinge, ⌫, B)  (B + 1)⌫\nIn order to prove our main result, it would be useful to generalize the above result a parametric family of “scaled” hinge losses give by\nhinge(z) = max(0, 1 z\n), with a parameter > 0. Thus, if = 1, hinge\nis hinge.\nTheorem 3. For all > 0, if ⌫(B + 1) < 1, then\nEG( hinge, ⌫, B) min\n⇢ ⌫(B + 1)\n2 , 1 2⌫\nUsing the above theorem, we prove our main result :\nTheorem 4. For any convex loss function , we have\nEG( , ⌫, B) min ⇢ ⌫(B + 1)\n2 , 1 2\nThe above theorem and Proposition 2 together show that hinge loss is optimal up to constant factor 2.\nWe would also like to compare the error guarantees of specific losses or loss families. To this end, we derive the following generic “recipe” for obtaining lower bounds specific to loss functions (See Appendix, Section A for a proof):\nLemma 3.1. For any non-negative convex loss ,\nEG( , ⌫, B) (3)\nmin (\nsup 2[0,1] inf ↵2[0, 4p\n5 ]\n(1 ⌫) (↵)+⌫ ( (B 5)↵2 ) (2 ) 2( ( 2 ) (2 )) , 1\n4\n)\nBased on the above lemma, we show a lower bound for any strongly convex loss function, which shows that choosing strongly convex loss functions is in fact qualitatively worse in the worst case sense (See Appendix, Section B for a proof).\nCorollary 5. For any -strongly convex surrogate loss that is L-Lipschitz in the interval [ 1, 1], we have that\nEG( , ⌫, B) min ⇢\n64L ⌫ (B 1)2, 1 16"
    }, {
      "heading" : "3.2. Bounds for Specific Losses",
      "text" : "Before we proceed we would like to give an alternate bound to Equation (3) which is often easier to get a handle on. To this end note that by (3.1) :\nEG( , ⌫, B) min (\n1 4 ,\nsup 2[0,1] inf ↵2[0, 4p 5 ] max\n⇢\n(↵) (2 ) 4 ( 2 ) (2 ) , ⌫( ( (B 5)↵2 ) (2 )) 2( ( 2 ) (2 ))\n)\nFor the first term in the max, note that for some fixed x2 the ratio\n(x1) (x2) x1 x2 is monotonically non-\ndecreasing in x1 and so,\nEG( , ⌫, B)\nmin (\n1 4 , sup 2[0,1] inf ↵2[0, 4p 5 ] max\n⇢\n2 ↵ 8 , ⌫( ( (B 5)↵2 ) (2 )) 2( ( 2 ) (2 ))\n)\nFurther note that for > ↵, 2 ↵4 14 and so we can conclude that,\nEG( , ⌫, B) min (\nsup 2[0,1]\n⌫( ( (B 5) 2 ) (2 )) 2( ( 2 ) (2 )) , 1 8\n)\n(4)\nExample 3.1 (Hinge Loss). The hinge loss is give by (z) = max(1 z, 0). Simply using Theorem 4 we get\nEG( , ⌫, B) min ⇢ ⌫(B + 1)\n2 , 1 2\nExample 3.2 (Squared Hinge Loss). The squared hinge loss is given by (z) = max(1 z, 0)2. Using Equation 4 with = 12 , we get\nEG( , ⌫, B) min ⇢ ⌫(B 1)2\n128 , 1 8\nExample 3.3 (Exponential Loss). Exponential loss is given by (z) = e z. For Exponential loss using Equation 4 with = 1/2 we get\nEG( , ⌫, B) min ⇢ ⌫(eB 1) 2(e2 1) , 1 8\nExample 3.4 (Logistic Loss). For Logistic loss is given by (z) = log(1 + e z). For logistic loss, using Equation 4 with = 1/2 we get, EG( , ⌫, B) min ( ⌫ log(1 + e(B 5)/4) 0.32\n2 , 1 8\n)\nNotice that for large B this behaves similar to hinge loss. Also notice that the squared hinge loss (and similarly square loss) behave quadratically in B and exponential loss has exponential dependence on B. Thus we see that for large B hinge loss gives qualitatively better bound that squared loss of exponential loss."
    }, {
      "heading" : "3.3. General Hypothesis Classes",
      "text" : "The misclassification error guarantee (EG) was specific to linear predictors (with norm bounded by B). One can easily generalize this definition of misclassification error guarantee w.r.t. an arbitrary hypothesis class H as follows :\nEGH( , ⌫, B)\n= sup D s.t. 9h 2 H,\nsup x,x0 |h(x) h(x0)|  2B,\nRm(h)  ⌫\nsup bh 2 H s.t.\n8h 2 H, R (bh)  R (h)\nR01(bh) (5)\nSince the linear hypothesis with norm bounded by B is a particular case of a hypothesis class that satisfies sup\nx,x 0 |hw,xi hw,x0i|  2B, we have that for any loss :\nEG( , ⌫, B)  supH EGH( , ⌫, B) On the other hand, note that Proposition 2 was only based on the fact that hinge loss can be bounded by the margin loss times the maximum value of the loss (given maximal value of predictor is B). Hence the proposition directly extends to any hypothesis class and so we can conclude that\nsup H\nEGH( hinge, ⌫, B)  ⌫(B + 1). (6)\nAnd so, all our upper and lower bounds can also be interpreted as upper and lower bounds on supH EG\nH( hinge, ⌫, B), i.e. on the best misclassification error guarantee that is possible based only on the loss function, and is required to hold independent of the hypothesis class.\nFurthermore, by Theorem 4, the right hand side in (6) is in turn bounded by 2EG( hinge, ⌫, B), and we see that the “extreme” hypothesis class for the hinge-loss is the linear class. This can also be extended to the other commonly used losses referred to in Section 3.2. Hence, even if we want misclassification error guarantees that focus only on the loss and are required to hold regardless of the hypothesis class, studying the linear class, i.e. EG( , ⌫, B), is often su cient.\nThe main reason we focus our presentation on linear predictors is that learning linear predictors (possibly linear in a feature space, which includes kernel methods) combined with convex losses is essentially the only situation that yields a convex optimization problem, which is one of our goals when using convex surrogates."
    }, {
      "heading" : "4. Classification Calibration and the",
      "text" : "Misclassification Error Guarantee\nAn important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of “classification calibrated” loss functions (Zhang, 2004; Bartlett et al., 2006). A basic notion here is that of a loss being “classification calibrated”, i.e. ensuring that zero excess loss (beyond the Bayes optimal) translates to zero excess misclassification error (beyond the Bayes optimal). This ensures that if we consider a class rich enough to include the Bayes optimal predictor then minimizing the expected loss indeed also minimizes the misclassification error. Classification calibration can be seen as an extreme point of the misclassification error guarantee (EG) in two ways:\nFirst, for any convex loss , EG( , 0) = 0 if and only if is classification calibrated (both are equivalent to the derivative at zero being defined and negative). Discussing ⌫ = 0 corresponds to considering only the separable case, which is in a sense the point of intersection of our study and that of Zhang (2004); Bartlett et al. (2006).\nSecond, we can think of classification calibration as referring to EGM, where M is the set of all measurable functions. That is, a surrogate loss is classification calibrated if and only if EGM( , nu) = ⌫.\nAnalyzing either EG( , 0) or EGM( , ⌫) is not satisfactory as they don’t correspond to the agnostic learning case where we are interested in doing as well as the best hypothesis in the function class of interest. Typically, classification calibration based results are used in conjunction with approximation theory to argue that as the number of training samples increase one can consider richer and richer hypothesis classes, and hence eventually converge to the set of all measurable functions M, where EGM( , ⌫), and hence the notion of classification calibration, is relevant. However, such an analysis typically only establishes asymptotic behavior (unless strong assumptions are made). The analysis in this work neither needs to assume that data is linearly separable nor assume that the Bayes optimal predictor under the surrogate loss function is linear (with norm bounded by B).\nFor example, based on Zhang (2004), Rosasco et al. (2004) argue that the hinge loss (and also logistic loss) enjoy better rates than other losses like squared loss. However these results are also based on convergence to the Bayes optimal and so we either need to take very rich hypothesis classes or assume that the Bayes optimal predictor under surrogate loss is contained in the hypothesis class used."
    }, {
      "heading" : "5. Including Estimation Error Rates",
      "text" : "It is interesting to consider how misclassification error guarantee (EG) combines with estimation error rate. In practice, we get a finite training sample and when picking the hypothesis that minimizes some empirical loss, the estimation error involved in minimizing empirical objective, rather than the true expected objective, comes into the picture. While choosing the loss one should take into account both the misclassification error guarantee (EG) associated with the loss and also the associated estimation error for the problem. For example, thinking of only estimation error, one might think that squared error is better as one might expect a 1/n rate where n is the sample size. However, EG for squared loss is large as we argued in Section 3.2.\nFor high dimensional cases, one can argue that hinge loss is the loss of choice even when we take estimation error into account. For the conservative update algorithm w.r.t. the hinge loss with its corresponding analysis by (Shalev-Shwartz, 2007), or for the exact minimizer (which corresponds to the SVM) of empirical hinge loss using results in (Srebro et al., 2010) (and noticing that hinge loss upper bounds a smooth version of margin loss which in turn upper bounds the zero-one loss) one can show that if bw\nn is the linear predictor returned by one of these algorithms, then, in\nexpectation over training sample\nR01(ŵn)  2 inf w:kwkB R hinge(w) +O\n✓ B2\nn\n◆\n 2 (B + 1)⌫ +O ✓ B2\nn\n◆\n(7)\nwhere ⌫ = inf w:kwkB R01(w). In order to compare this with the squared loss, we first note that the best misclassification error guarantee one can give for an algorithm that minimizes the expected squared loss is bounded by EG( squared, ⌫, B) min{⌫(B 1)2/128, 1/8}. Further, when the dimensionality is large (compared to the sample size) then the estimation error rate for the squared loss (with linear predictors) can be lower bounded by B2/n (see for instance (Srebro et al., 2010)) and so the best guarantee that can be provided on the classification risk of estimator obtained by minimizing squared loss scales is ⌫(B 1)2 + B2/n. Comparing this with the upper bound in equation 7 shows that the hinge loss is qualitatively superior (in the worst case) even if one takes into account the estimation error rates.\nA similar analysis can be repeated w.r.t. other losses where e↵ectively hinge loss (and also logistic loss) can be shown to have qualitatively better performance than, for instance, squared loss or exponential loss, or any strongly convex loss. We would like to point out that the low-dimensional analysis requires a bit more care, as the estimation error for the squared loss might be much lower than for methods based on other loss functions."
    }, {
      "heading" : "6. Proofs of Theorems 3 and 4",
      "text" : "Proof of Theorem 3. The distribution for this theorem is as follows:\n• There are ⌫ points at x = 1 labelled +1 • There are points at x = M labelled 1 • There are 1 ⌫ points at x = M labelled +1\nAlthough the data lies on the real line, we consider the example to be in R2. Now consider the classifier found when minimizing the convex surrogate loss\nM hinge. We aim to show that the vector w? = (0, 1) with w?0 = M is the optimal classifier. It has R (w\n?,w?0) = 2 . The margin loss of (w?,w?0) is clearly . Any other classifier that misclassifies fewer than points, must cross the x-axis.\nCase 1\nAssume that we have a classifier that misclassifies fewer than points, and that this classifier intersects\nthe x-axis between [ M, 0] and mislabels only (and all) ⌫ points at x = 1. Assume that it crosses at c 2 [ M, 0]. Assume also that it crosses the x-axis with some angle ✓. Thus, w = (sin(✓), cos(✓)) and w0 = sin(✓)c.\nConsider the case where (w, w0) is further than M away from the 1 ⌫ points at x = M . Then, R (w, w0) = ⌫(1+ sin(✓)(1 c)\nM\n+ (1 sin(theta)(M c) M ).\nBy taking the derivative, we can see this is increasing as ✓ increases, thus taking ✓ to be as small as possible while maintaining the M distance from the points at x = M is best. This gives, sin(✓) = M\nM+c , which yields\nR (w, w0) = 1 M+c · (⌫(1 +M) + 2c ). Finally, taking the derivative with respect to c we find that if < ⌫ 1+M2M then R (w, w0 is minimized at c = M with a cost of R\n(w, w0) = ⌫ 1+M 2M + .\nWe do not need to consider similar classifiers that intersect the x-axis between (0,M ], as they will only increase the cost of points at x = 1 and add cost of points from x = M . This will never beat the classifier mentioned above when c = 0, which is already beaten by the classifier with c = M .\nCase 2\nNow assume that we have a classifier that misclassifies fewer than points and that this classifier intersects the x-axis between [ 1+M2 , M ], and that the classifier mislabels only (and all) 1 ⌫ points at x = M . Assume the classifier crosses the x-axis at position c, and that it crosses with some angle ✓. Thus, w = ( sin(✓), cos(✓)) and w0 = sin(✓)c. If the classifier is at least distance M from all of the points, then R\n(w, w0) (1 ⌫) 2c c M , which is\nminimized when c is largest. Thus, R (w, w0) 2(1 ⌫) 1+M1 M . Note that 2 < 2(1 ⌫) 1+M1 M if ⌫ < M 1+M , which is one of our assumptions. So, for this case, (w, w0) is not optimal.\nFinally, for this case, we have where the points at x = M are within the margin of the classifier, and so they contribute to the -loss. Let c0 = c M . This gives R (w, w0) = (1 sin(✓)c 0\nM ) + (1 ⌫)(1 + sin(✓)(c0+2M)\nM\n).\nUsing the derivative, we find that it is minimal when sin(✓) = M1 c0 M . This yields R (w, w0) =\n1 1 c0 M ( 2 (c0 +M) + (1 ⌫)(1 +M)). Taking the derivative with respect to c0, yields the minimum when c0 = 0 (which means c = M). Therefore, the cost is R (w, w0) = 1\n1 M ( 2 M + (1 ⌫)(1 +M)). Again, we find that this cost is larger than 2 when ⌫ < M1+M .\nProvided 1 ⌫ > ⌫, we do not need to consider similar classifiers that intersect the x-axis between [ 1, 1+M2 ), as they always have higher cost."
    }, {
      "heading" : "Remaining cases",
      "text" : "Any classifiers outside of those discussed in Case 1 and Case 2 misclassify at least points, as long as < 12 .\nTherefore, since R (w?,w?0) = 2 we merely need that 2 < ⌫ 1+M2M + to handle Case 1, which is true when < ⌫ 1+M2M . This was our assumption. Finally, the EG(·) found here applies to all hinge losses because M hinge(x) = hinge(\nM x), and thus they are equivalent losses with respect to their error guarantee. Thus what we have shown is that EG(\nhinge, ⌫, B) for any such that ⌫ < < ⌫(B+1)\n2 and 1 > 2⌫. Thus taking the largest of such ’s gives the final form of the bound.\nProof of Theorem 4. There exists an ↵, with 0 < ↵ such that the horizontal line above the x-axis, labelling all points as +1, has cost · ( ↵)+(1 ) · (↵) < 1, as long as < 12 , which holds for this theorem since < ⌫M+12M .\nFor any convex function , there exists a > 0 such that (x)\nhinge(x), for all x. Thus, any classifier (w,w0) that misclassifies fewer than points must satisfy R\n(w,w0) + ⌫2 · (1+B). This follows from the proof of Theorem 3, because it is the minimum value of R\nhinge(w,w0), where (w,w0) misclassifies fewer than points.\nHowever, for any classifiers (w,w0) that misclassifies fewer than points, it is some distance c away from at least ⌫ points that it misclassifies, where c > 0. This follows from our distribution having three groups of points: a group of size ⌫, a group of size , and a group of size 1 ⌫. The smallest group size is ⌫ because > ⌫ and 1 > 2⌫ and that (w,w0) must misclassify at least one of the groups of points. Therefore, R\n(w,w0) + ⌫2 · (1 + B) + ⌫( ( c) hinge( c)).\nNote that ( c) hinge( c) 0. From here, we break the analysis into two cases. First, for all x  0, (x) =\nhinge(x). Second, is where there exists x0 < 0 such that (x0) > hinge(x0).\nFor both cases, we will make use of the cost of a horizontal line above the x-axis that labels all points as +1. Recall from above that the cost of this classifier, which is (0, w00) for some w00 > 0, is given by · ( ↵) + (1 ) · (↵) < 1. Also, this inequality holds for any (·). We refer to this classifier as h⇤ and\nits risk is R (h⇤).\nCase 1:\nIn this case, ( c) hinge( c) = 0 for all c > 0. Thus, we are back to R\n(w,w0) + ⌫2 · (1 + B). However, we can simplify R (h⇤) in this case.\nR (h⇤) = · ( ↵) + (1 ) · (↵) = · hinge( ↵) + (1 ) · (↵)\nNow, since R (w,w0) + ⌫2 · (1+B) for any convex loss function (·), we can replace (x) with 0(x) = (kx) for any k > 0 and the same inequality must hold for 0(x). Further, 0(x) =\nhinge(kx), for all x  0 and k > 0. Recall that as x ! 1, 0(x) ! 0. Let 0\nhinge(x) = hinge(kx).\nR 0(h ⇤) = 0( ↵) + (1 ) 0(↵)\n= hinge( k↵) + (1 ) (k↵) = 0 hinge( ↵) + (1 )( 0 hinge(↵) + ✏ 0) = R 0 hinge(h ⇤) + (1 )✏ 0 < R 0 hinge(h ⇤) + ✏ 0\nAs k !1, ✏ 0 ! 0. Finally, we have from Theorem 3, that, for all k > 0, R\n0 hinge (h⇤) < + ⌫2 · (1 + B). This implies that there exists an ✏ > 0 such that R\n0 hinge (h⇤) + ✏ < + ⌫2 · (1 + B). Take k to be large enough such that ✏\n0  ✏. This implies that R 0(h⇤) < R\n0 hinge (h⇤) + ✏ < + ⌫2 · (1 +B). Therefore, there exists a k > 0 such that for 0(x) = (kx), EG( 0(x), ⌫, B) . Finally, because 0(x) = (kx), where k > 0, they are equivalent losses. Therefore, EG( (x), ⌫, B) . Case 2:\nConsider loss 0(x) = (kx), where k > 0 is large enough such that ( kc)\nhinge( kc) > 1 ⌫\n2 · (1 + B). This is possible since (·) is convex and we assumed that for some x < 0, (x) >\nhinge(x).\nTherefore, R 0(w,w0) > 1. But we know that there exists ↵ > 0 such that (0, w00), with w00 > 0 (i.e. a horizontal line above the x-axis that labels all points as +1) has cost · 0( ↵) + (1 ) · 0(↵) < 1. Thus, for 0(·), EG( 0(·), ⌫, B) . Finally, because 0(x) = (kx), where k > 0, they are equivalent losses. Therefore, if 0 < ⌫ < M\nM+1 = 1\nB+1 , then EG( (·), ⌫, B) , for all such that ⌫ < < ⌫\n2 · (1 +B) and 1 > 2⌫.\nTo conclude the proof note that as already mentioned, when ⌫(B+1) 1 we anyway get that EG( , ⌫, B) 1 and what we showed in the proof is that when ⌫(B+ 1) < 1, then EG( , ⌫, B) for any s.t. < ⌫(B+1)\n2 and 1 > 2⌫. Since B > 1, ⌫ < 1/2 from which we conclude the proof."
    }, {
      "heading" : "7. Discussion",
      "text" : "In this paper, we provide lower bounds on the best misclassification error achievable by algorithms minimizing convex surrogate losses in terms of the M - margin error. Specifically, we show that the misclassification error rate of the linear predictor minimizing expected hinge loss is bounded by ⌫(B+1), where ⌫ is the bound on the M -margin error and B = 1/M . Further, by showing that when using linear predictors any algorithm minimizing any convex loss has a misclassification error of at least ⌫(B+1)2 , we conclude that the hinge loss is optimal up to factor 2. We also show lower bounds for specific convex losses and that any strongly convex loss has a qualitatively worse guarantee when compared to hinge loss. We argue that the analysis can be used to qualitatively compare convex surrogate losses used for binary classification, and show that the hinge loss is the loss of choice for classification problems. The relationship of the misclassification error guarantee term, which we introduce in this paper, with the notion of classification calibration of loss function is also explored. Specifically, we show how classification calibration can be seen as arising from an extreme case of our misclassification error guarantee term. As an example of the implications of our results, we argue that even when one takes estimation error rates into consideration, hinge loss is optimal up to constant factor (in the worst case sense, at least for high dimensional problems)."
    }, {
      "heading" : "Christmann, Andreas and Steinwart, Ingo. On robustness",
      "text" : "properties of convex risk minimization methods for pattern recognition. J. Mach. Learn. Res., 5:1007–1034, December 2004. ISSN 1532-4435."
    }, {
      "heading" : "Kalai, Adam Tauman, Klivans, Adam R., Mansour,",
      "text" : "Yishay, and Servedio, Rocco A. Agnostically learning halfspaces. SIAM J. Comput., 37:1777–1805, March 2008. ISSN 0097-5397.\nKalai, A.T. and Sastry, R. The isotron algorithm: Highdimensional isotonic regression. In Proceedings of the 22th Annual Conference on Learning Theory. Citeseer, 2009."
    }, {
      "heading" : "Kearns, Michael, Schapire, Robert E., Sellie, Linda M., and",
      "text" : "Hellerstein, Lisa. Toward e cient agnostic learning. In Machine Learning, pp. 341–352. ACM Press, 1994."
    }, {
      "heading" : "Long, Phil and Servedio, Rocco A. Learning large-margin",
      "text" : "halfspaces with more malicious noise. In Advances in Neural Information Processing Systems 24, pp. 91–99. 2011.\nLong, Philip M. and Servedio, Rocco A. Random classification noise defeats all convex potential boosters. In Proceedings of the 25th international conference on Machine learning, ICML ’08, pp. 608–615, New York, NY, USA, 2008. ACM. ISBN 978-1-60558-205-4."
    }, {
      "heading" : "Masnadi-Shirazi, Hamed and Vasconcelos, Nuno. On the",
      "text" : "Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost. In Advances in Neural Information Processing Systems 21, pp. 1049– 1056. 2009.\nNock, Richard and Nielsen, Frank. On the e cient minimization of classification calibrated surrogates. In NIPS’08, pp. 1201–1208, 2008."
    }, {
      "heading" : "Rosasco, L., De, E., Caponnetto, Vito A., Piana, M., and",
      "text" : "Verri, A. Are loss functions all the same. Neural Computation, 16(5), 2004.\nShalev-Shwartz, S. Online Learning: Theory, Algorithms, and Applications. PhD thesis, The Hebrew University, 2007.\nShalev-Shwartz, S., Shamir, O., and Sridharan, K. Learning kernel-based halfspaces with the zero-one loss. Arxiv preprint arXiv:1005.3681, 2010."
    }, {
      "heading" : "Srebro, N., Sridharan, K., and Tewari, A. Smoothness, low",
      "text" : "noise and fast rates. In NIPS, 2010.\nZhang, T. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Proceedings of the Twenty-First International Conference on Machine Learning, 2004."
    } ],
    "references" : [ {
      "title" : "Convexity, classification, and risk bounds",
      "author" : [ "P.L. Bartlett", "M.I. Jordan", "J.D. McAuli↵e" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2006
    }, {
      "title" : "E cient learning of linear perceptrons",
      "author" : [ "Ben-david", "Shai", "Simon", "Hans Ulrich" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Ben.david et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Ben.david et al\\.",
      "year" : 2000
    }, {
      "title" : "On robustness properties of convex risk minimization methods for pattern recognition",
      "author" : [ "Christmann", "Andreas", "Steinwart", "Ingo" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Christmann et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Christmann et al\\.",
      "year" : 2004
    }, {
      "title" : "Agnostically learning halfspaces",
      "author" : [ "Kalai", "Adam Tauman", "Klivans", "Adam R", "Mansour", "Yishay", "Servedio", "Rocco A" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Kalai et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 2008
    }, {
      "title" : "The isotron algorithm: Highdimensional isotonic regression",
      "author" : [ "A.T. Kalai", "R. Sastry" ],
      "venue" : "In Proceedings of the 22th Annual Conference on Learning Theory. Citeseer,",
      "citeRegEx" : "Kalai and Sastry,? \\Q2009\\E",
      "shortCiteRegEx" : "Kalai and Sastry",
      "year" : 2009
    }, {
      "title" : "Toward e cient agnostic learning",
      "author" : [ "Kearns", "Michael", "Schapire", "Robert E", "Sellie", "Linda M", "Hellerstein", "Lisa" ],
      "venue" : "In Machine Learning,",
      "citeRegEx" : "Kearns et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 1994
    }, {
      "title" : "Learning large-margin halfspaces with more malicious noise",
      "author" : [ "Long", "Phil", "Servedio", "Rocco A" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Long et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2011
    }, {
      "title" : "Random classification noise defeats all convex potential boosters",
      "author" : [ "Long", "Philip M", "Servedio", "Rocco A" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Long et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2008
    }, {
      "title" : "On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost",
      "author" : [ "Masnadi-Shirazi", "Hamed", "Vasconcelos", "Nuno" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Masnadi.Shirazi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Masnadi.Shirazi et al\\.",
      "year" : 2009
    }, {
      "title" : "On the e cient minimization of classification calibrated surrogates",
      "author" : [ "Nock", "Richard", "Nielsen", "Frank" ],
      "venue" : "In NIPS’08,",
      "citeRegEx" : "Nock et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Nock et al\\.",
      "year" : 2008
    }, {
      "title" : "Are loss functions all the same",
      "author" : [ "L. Rosasco", "E. De", "Caponnetto", "Vito A", "M. Piana", "A. Verri" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Rosasco et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rosasco et al\\.",
      "year" : 2004
    }, {
      "title" : "Online Learning: Theory, Algorithms, and Applications",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "PhD thesis, The Hebrew University,",
      "citeRegEx" : "Shalev.Shwartz,? \\Q2007\\E",
      "shortCiteRegEx" : "Shalev.Shwartz",
      "year" : 2007
    }, {
      "title" : "Learning kernel-based halfspaces with the zero-one loss",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "K. Sridharan" ],
      "venue" : "Arxiv preprint arXiv:1005.3681,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2010
    }, {
      "title" : "Smoothness, low noise and fast rates",
      "author" : [ "N. Srebro", "K. Sridharan", "A. Tewari" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Srebro et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2010
    }, {
      "title" : "Solving large scale linear prediction problems using stochastic gradient descent algorithms",
      "author" : [ "T. Zhang" ],
      "venue" : "In Proceedings of the Twenty-First International Conference on Machine Learning,",
      "citeRegEx" : "Zhang,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "However the problem of agnostically learning half-spaces is known to be NP-hard in general (Kearns et al., 1994).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "Under a cryptographic hardness assumption, (Shalev-Shwartz et al., 2010) extend this result to improper learning (i.",
      "startOffset" : 43,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "However the problem of agnostically learning half-spaces is known to be NP-hard in general (Kearns et al., 1994). Even when one only wants to learn a half-space relative to the best possible M -margin error, Ben-david & Simon (2000) show that (subject to P 6= NP ) there exists no proper learning algorithm (i.",
      "startOffset" : 92,
      "endOffset" : 233
    }, {
      "referenceID" : 14,
      "context" : "An important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of “classification calibrated” loss functions (Zhang, 2004; Bartlett et al., 2006).",
      "startOffset" : 174,
      "endOffset" : 210
    }, {
      "referenceID" : 0,
      "context" : "An important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of “classification calibrated” loss functions (Zhang, 2004; Bartlett et al., 2006).",
      "startOffset" : 174,
      "endOffset" : 210
    }, {
      "referenceID" : 3,
      "context" : "In terms of directly minimizing the misclassification error rate, without using a convex surrogate loss, Kalai et al. (2008) provide non-asymptotic finite sample bounds for e cient binary prediction with half spaces.",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "In terms of directly minimizing the misclassification error rate, without using a convex surrogate loss, Kalai et al. (2008) provide non-asymptotic finite sample bounds for e cient binary prediction with half spaces. However, to do so they assume the inputs are uniformly distributed on the surface of a sphere, which is an extremely strong an unrealistic assumption. Similarly, Kalai & Sastry (2009) also provide an e cient algorithm for minimizing the misclassification error, but only under the assumption that the conditional distribution of the label given the input x is some monotonic function of w · x for some w—again significantly departing from the agnostic setting.",
      "startOffset" : 105,
      "endOffset" : 401
    }, {
      "referenceID" : 0,
      "context" : "in the separable case, this is essentially a question about “classification calibration”, and we have that for convex , EG( , 0) = 0 if and only if is di↵erentiable at zero and 0(0) < 0 (Bartlett et al., 2006)—see Section 4.",
      "startOffset" : 186,
      "endOffset" : 209
    }, {
      "referenceID" : 14,
      "context" : "An important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of “classification calibrated” loss functions (Zhang, 2004; Bartlett et al., 2006).",
      "startOffset" : 174,
      "endOffset" : 210
    }, {
      "referenceID" : 0,
      "context" : "An important line of work focused on relating the excess loss to the excess misclassification error, and introducing the notion of “classification calibrated” loss functions (Zhang, 2004; Bartlett et al., 2006).",
      "startOffset" : 174,
      "endOffset" : 210
    }, {
      "referenceID" : 13,
      "context" : "Discussing ⌫ = 0 corresponds to considering only the separable case, which is in a sense the point of intersection of our study and that of Zhang (2004); Bartlett et al.",
      "startOffset" : 140,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "Discussing ⌫ = 0 corresponds to considering only the separable case, which is in a sense the point of intersection of our study and that of Zhang (2004); Bartlett et al. (2006).",
      "startOffset" : 154,
      "endOffset" : 177
    }, {
      "referenceID" : 13,
      "context" : "For example, based on Zhang (2004), Rosasco et al.",
      "startOffset" : 22,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "For example, based on Zhang (2004), Rosasco et al. (2004) argue that the hinge loss (and also logistic loss) enjoy better rates than other losses like squared loss.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "the hinge loss with its corresponding analysis by (Shalev-Shwartz, 2007), or for the exact minimizer (which corresponds to the SVM) of empirical hinge loss using results in (Srebro et al.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "the hinge loss with its corresponding analysis by (Shalev-Shwartz, 2007), or for the exact minimizer (which corresponds to the SVM) of empirical hinge loss using results in (Srebro et al., 2010) (and noticing that hinge loss upper bounds a smooth version of margin loss which in turn upper bounds the zero-one loss) one can show that if b w",
      "startOffset" : 173,
      "endOffset" : 194
    }, {
      "referenceID" : 13,
      "context" : "Further, when the dimensionality is large (compared to the sample size) then the estimation error rate for the squared loss (with linear predictors) can be lower bounded by B2/n (see for instance (Srebro et al., 2010)) and so the best guarantee that can be provided on the classification risk of estimator obtained by minimizing squared loss scales is ⌫(B 1)2 + B2/n.",
      "startOffset" : 196,
      "endOffset" : 217
    } ],
    "year" : 2012,
    "abstractText" : "We carefully study how well minimizing convex surrogate loss functions corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors. We consider the agnostic setting, and investigate guarantees on the misclassification error of the loss-minimizer in terms of the margin error rate of the best predictor. We show that, aiming for such a guarantee, the hinge loss is essentially optimal among all convex losses.",
    "creator" : "TeXShop"
  }
}