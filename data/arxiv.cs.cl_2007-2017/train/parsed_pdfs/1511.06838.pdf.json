{
  "name" : "1511.06838.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Mapping Images to Sentiment Adjective Noun Pairs with Factorized Neural Nets",
    "authors" : [ "Takuya Narihira", "Damian Borth", "Stella X. Yu" ],
    "emails" : [ "takuya.narihira@jp.sony.com", "damian.borth@dfki.de", "stellayu@berkeley.edu", "kni@iqt.org", "trevor@berkeley.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29]. One key element towards achieving this is the use of Adjective Noun Pair (ANP) concepts as a mid-level representation of visual content. We consider the task of labeling user-generated images by ANPs that visually convey a plausible sentiment, e.g. adorable girls in Fig. 1. This task can be more subjective and holistic, e.g. beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23]. It also has a simpler focus than image captioning which aims to describe an image as completely and objectively as possible [21, 13].\nANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27]. Borth et al. [3] uses a bank\nof linear SVMs (SentiBank), and [4] uses deep CNNs. Both approaches aim to only detect known ANP from the dataset. Deep CNNs have also been used for sentiment prediction [26, 29], but they are unable to model sentiment prediction by a mid-level representation such as ANPs.\nOur goal is to map an image onto embedding derived from the visual sentiment ontology [3] that is built completely from visual data and respects visual correlations along adjective (A) and noun (N) semantics. By conditioning A on N, the combined concept of ANP becomes more visually detectable; by partitioning the visual space\n1\nar X\niv :1\n51 1.\n06 83\n8v 1\n[ cs\n.C V\n] 2\n1 N\nov 2\nof nouns along adjectives, ANP forms a unique two-factor embedding for visual learning.\nANP images in Fig. 1 exhibit structured correlations. Along each N column is the same type of objects; across the N columns are related objects and parts. Along each A row is the same type of positive sentiment manifested in different objects; across the A rows are sometimes interchangeable sentiments but most times distinctive ones in their own ways. For example, not every ANP is popular on platforms like Flickr: adorable eyes and attractive baby are not frequent enough to have associated images in the visual sentiment dataset [3], suggesting that adorable is reserved more for overall impressions, whereas attractive is more for sexual appeal. When an ANP classifier captures the rowcolumn structure, it can fill in the semantic blanks where there is no training data available and extend the concept consistent with other known ANPs.\nLearning a data-driven factorized adjective-noun embedding is necessary not only for finding semantic structures, i.e., some ANPs are more similar than others (pretty girls and attractive girls vs. ugly girls), but also for filtering out annotation noise and removing inherent ambiguity. Fig. 2 illustrates issues common to ANP images. The same noun could mean different entities: baby often refers to human baby, but it could also refer to one’s pet or favorite thing\nsuch as cupcakes, whereas an adjective could be used in a sarcastic manner to indicate an opposite sentiment, and such usage is dependent on the particular noun that it is paired with: images tagged as attractive girls are mostly positive, but images tagged as attractive face are often negative, with people making funny faces.\nWe present a nonlinear factorization model for ANP classification based on the composition of two deep neural networks (Fig. 3). Unlike the classical bilinear factorization model [8] which decomposes an image into style and content variations in a generative process, our model is discriminative and nonlinear. Compared to the bilinear SVM classifiers [22] which represents the classifier as a product of two low-rank matrices, our model learns both the feature and the classifier in a deep neural network achitecture.\nWe emphasize that our factorized ANP CNN is only seemingly similar to the recent bilinear CNN model [18]; we differ completely on the problem, the architecture, and the technical approach. 1) The bilinear CNN model [18] is a feature extractor; it takes the particular form of CNN products and the two CNNs have no particular meaning. Our bilinear model reflects structured outputs and is in fact independent of how we extract the features, i.e., we could additionally use their bilinear model for feature extraction. As a result, we can deal with unseen class labels, while\nfc6\nfc7\nanp\ninput\nVGG conv1-5\nFC\nFC\nFC\n4096\n4096\n|ANP|\nfc6\ninput\nfc7-a fc7-n\nVGG conv1-5\nFC\nFCFC\nFCFC\nA N\n4096\n4096\n4096\nadj noun\nfc6\ninput\nfc7-a fc7-n\nmat-a mat-n\nmat-anp\nVGG conv1-5\nFC FC\nFC\nMatrix Multiplication\nFCFC\nAxN\n4096\n40964096\nAxM NxM\na) ANP-Net b) Fork-Net c) Fact-Net d) N-LSTM-A e) A-LSTM-N\nFigure 4: Five deep convolutional neural network architectures used in our experiments.\ntheirs does not address any such issue. 2) The blinear CNN model generalizes spatial pooling and only uses conv layers, whereas the bilinear term of our model is a product of latent representations for A and N, two aspects of the label, the effect of which is entirely different from spatial pooling.\nWe explicitly map the output of A and N nets onto an individual representation that is to be combined bilinearly for final classification. Such a factorization provides our model not only the much needed regularization across different ANPs, but also the capability to classify and retrieve ANPs never seen during the training. Experimental results on the publicly available dataset [3] demonstrate that our model significantly outperforms independent ANP classification on unseen ANPs, and on retrieving images of new ANP vocabulary. That is, our model based on a factorized representation of ANP not only generalizes better, but can also expands the ANP vocabulary on its own."
    }, {
      "heading" : "2. Sentiment ANP CNN Classifiers",
      "text" : "We develop three CNN models that output a sentiment ANP label for an input image (Fig. 4). The first is a simple classification model that treats each ANP as an independent class, whereas the other two models have separate A and N processing streams and can thus predict ANPs never seen in the training data. The second model is based on a shared-CNN architecture with two forked output layers, and the third model further incorporates an explicit factorization layer for A and N which is subsequently multiplied together for representing the ANP class.\nANP-Net: Basic ANP CNN Classifier. Fig. 4a shows the baseline model that treats the ANP prediction as a straightforward classification problem. We use VGG 16- layer [25] as a base model and replace the final fully connected layer “fc8” from predicting 1,000 ImageNet categories to predicting 1,523 sentiment ANP classes. The model is fine tuned from the ImageNet pretrained version. We minimize the cross entropy loss with respect to the entire network through mini-batch stochastic gradient descent with momentum. Given image I and ground truth label t, the cross entropy loss between t and the softmax of Kcategory network output vector y ∈ RK is defined as\nL(t, I, θ) = − log p(y = t|I) (1)\np(y = k|I) = softmax(y)k = exp(yk)∑K\nm=1 exp(ym) (2)\nFork-Net: Forked Adjective-Noun CNN Classifier. Fig. 4b shows an alternative model which predicts A and N separately from the input image. The two streams share earlier layers of computation. That is, the network tries to learn first a common representation useful for both A and N, and then an independent classifier for A and N separately. As for ANP-Net, we use softmax cross-entropy loss for the A or N output, i.e., the network tries to learn universal A and N classifiers regardless of which N or A they are paired with, ignoring the correlation between A and N. At the test time, we calculate the ANP response score from the product of A output yA and N output yN :\np(y = (i, j)|I) = p(yA = i|I)× p(yN = j|I) (3)\nFact-Net: Bilinearly Factorized ANP CNN Classifier. Fig. 4c shows a model with early layers of Fork-Net followed by a new product layer that combines the A and N outputs bilinearly for the final ANP output. That is, with adjective i and noun j represented in the same M - dimensional latent space, ai ∈ RM and nj ∈ RM respectively, where aim and njm denote m-th hidden variable for adjective i and noun j, the Fact-Net output yij is yij = ∑ m∈M aimnjm. Let A,N denote the numbers of adjectives and nouns. We have in matrix notations:\nYA×N = AA×M ·N′N×M , (4)\nA =  a1 a2 ... aA  , N =  n1 n2 ... nN  . (5) The Fact-Net learns to map an image to a factorized A-N matrix representation Y by minimizing a cross entropy loss L, with gradients over latent A and N net outputs:\n∂L ∂A = ∂L ∂Y N, ∂L ∂N = ( ∂L ∂Y )′ A. (6)\nThe entire network can be learned end-to-end with back propagation. We find the network to learn better with the softmax function normalizing only over ANPs seen in the training set, in order to ignore the effect of ANP activations Yij which are unseen during training.\nN-LSTM-A and A-LSTM-N are two baseline recurrent algorithms, where networks predict ANPs sequentially. For example, Fig. 4d first predicts the best noun given an image (i.e. p(yN = j|I)), and then conditioned on the noun, an adjective is predicted p(yA = i|yN = j, I). Likewise, Fig. 4e predicts first the best adjective, and then the best noun conditioned on that. These two networks are inspired by image captioning models, most of which are in response to the creation of the MSCOCO Dataset [17]."
    }, {
      "heading" : "3. Experiments and Results",
      "text" : "We describe our ANP ontology and its associated publicly available dataset, present our experimental details, and show our detection and retrieval performance.\nANPs from Visual Sentiment Ontology (VSO). VSO was created by mining online platforms such as Flickr and Youtube by the 24 emotions from Plutchnik’s Wheel of Emotions [3]. Derived from an analysis of tags associated with retrieved images and videos from this mining process, an ontology of roughly 3,000 ANPs was established, e.g. beautiful flowers or sad eyes, See Table 1 for statistics.\nANP Dataset. We use the publicly available dataset of Flickr images introduced in [3] with SentiBank 1.1. Please note that we experiment on the larger “non-creative common (Non-CC)” also refered to as the “Full VSO” dataset\nand not the smaller “creative common (CC)” only dataset. In the Non-CC dataset, for each ANP, at most 1,000 images tagged with the ANP have been downloaded, resulting in about one million images for the 3,316 ANPs of the VSO.\nWe first filter out the ANPs with fewer than 200 images, as such small categories are either non-representative or with poorly generalizable evaluation. We also remove ANPs which have unintended semantics against their general usage, e.g. dark funeral refers to images of a heavymetal band. We then remove any ANP that have fewer than two supports on both the adjectives and the noun, i.e. two ANPs support each other if they share A or N. Such pruning results in 1,523 ANPs with 737, 264 images, 172 adjectives and 240 nouns. For each ANP, 20% of images are randomly selected for testing, while others are used for training. We do ensure that images of one ANP from an uploader (Flickr user) go to either training or testing but not both, i.e., there is no user sharing between training and testing images.\nOur ANP labels come from Flickr user tags for images. These labels may be incomplete and noisy, i.e., not all true labels are annotated and there could be falsely assigned labels. We do not manually refine them; we use the labels as is and thus will refer to them pseudo ground truth (PGT).\nModel Details. We fine tune the models in Fig. 4 from VGG-net pretrained on ImageNet dataset. For ANP-net, the fully connected layer for final classification is randomly initialized. For Fork-net and Fact-net, we initialize fc7-a and fc7-n and all the following layers randomly. The fc7-a and fc7-n layers are followed by a parametric ReLU (PReLU) layer for better convergence [10]. We use 0.01 for the learning rate throughout training except the learning rates of pretrained weights are reduced by a factor of 10. Our models are implemented using our modified branch of CAFFE [12]. We use the polynomial decay learning rate scheduler. We train our models though stochastic gradient descent with momentum 0.9, weight decay 0.0005 and mini-batch size 256 for five epochs, taking 2-3 days for training convergence on a single GPU. For the two recurrent models, we expand and modify Andrej Karpathy’s ”neuraltalk” Github branch [13]. After incorporating various hidden layer sizes, we settle on a hidden layer of 128, word+image encoding size of 128, and a single recurrent layer. However, we did not pretrain on any word/semantic data on other corporat\n(e.g., MSCOCO), but rather only sequentially trained adjectives and nouns from Sentibank.\nTop-k Accuracy on Seen and Unseen ANPs. ANP classes are either seen or unseen depending on whether the ANP concept was given during training. While images of an explicitly unseen ANP class, e.g. beautiful men, might be new to a model, images sharing the same A or N, e.g. beautiful girls or handsome men, have been seen by the model. Our unseen ANPs come from those valid ANPs which are excluded from training due to their fewer than 200 examples. For the unseen dataset for our evaluation, we filter out the unseen ANPs with less than 100 examples. We have 293 unseen ANPs with 43, 133 examples in total.\nWe use top-k accuracy, k = 1, 5, 10, to evaluate a model. We examine whether the PGT label of an image is among the top k ANP labels suggested by a model output. The average hit rate for test images of an ANP indicates how accurate a model is at differentiating the ANP from others. The top-k accuracy on seen ANPs shows how good the model is fitting the training data, whereas that on unseen ANPs shows how well the model can generalize to new ANPs.\nWe take the DeepSentiBank model [4] as a baseline, which already outperforms the initial SentiBank 1.1. model [3]. It uses the AlexNet architecture [14] but fine-tuned to the ANP classes. We apply the same CNN architecture and setup to our set of 1,523 ANPs from the NON-CC dataset.\nTable 2a shows top-k accuracy on seen ANPs. ANPNet produces the best accuracies, since it is trained for\ndirectly optimizing the classification accuracies on these ANPs. Fact-Net outperforms Fork-Net for a number of choices of M , suggesting that our factorized representation better captures discriminative information between ANPs. Also, our VGG-net based models all outperform the Alexnet based DeepSentiBank model, confirming that deeper CNN architectures build stronger classification models.\nTable 2b shows top-k accuracy on unseen ANPs. Consistent with the results for seen ANPs, Fact-Net always outperforms Fork-Net. More importantly, the top-k accuracies on unseen ANPs decrease with increasingM , with Fact-Net at M = 2 significantly outperforms Fork-Net. That is, the larger the internal representation for A and N, the poorer the generalization to new ANPs. Since models like DeepSentiBank or the individual ANP-net are only capable of classifying what they have seen during training, we leave the corresponding entries in the Table blank.\nThe top-k accuracies on our two baseline image captioning models, while significantly above the chance level due to the large number of ANP classes, still seem surprisingly low. These poor results demonstrate the challenge of our ANP task, and in turn corroborate the effectiveness of factorized ANP CNN model.\nOur ANP task differs from the common language+vision problems in two significant ways: 1) It aims to capture not so much the semantics of word adjective-noun pairs such as (bull shark, blue shark, tiger shark), but rather pairs of adjectives and nouns with the sentiment evoked by pure visual experience such as (cute dog, scary dog, dirty dog). In this sense, our adjectives just happen to be words, the subjective aspect of our labels for conditioning our nouns, in order to partition the visual space instead of the semantic space. Word semantics reflected in the co-occurrence of natural text has little to do with our visual sentiment analysis. Our task is thus entirely different from the slew of language model and image captioning works. 2) We are generalizing not along a conceptual hierarchy with obvious visual similarity basis, e.g. from boxer and dog to canine and animal, but across two different conceptual trees with subtle visual basis, e.g. from (beautiful + sky / landscape / person) and (dead / dry / .../ old + tree) to (beautiful tree). Our task is thus much more challenging.\nBest and worst ANPs by Fact-Net. We look into the classification accuracy on individual ANPs and compare our Fact-Net with M = 2 against the best alternative ANPNet for the seen ANPs and Fork-Net for the unseen ANPs. The former could help us understand how the training data are organized and the latter how the model could fill in the blanks of the ANP space and generalize to new classes.\nFig. 5a lists the top and bottom 10 seen ANPs when they are sorted by the difference in top-10 accuracy between Fact-Net and ANP-Net, and Fig. 5b lists the top and bottom 10 unseen ANPs when they are sorted by the difference in\ntop-10 accuracy between Fact-Net and Fork-Net. The range of accuracy gap is (−0.6, 0.4) for the unseen, much wider than (−0.3, 0.3) for the seen ANP case. We analyze the accuracies with respect to the number of images as well as the number of different ANPs seen during the training, and obtain correlation coefficients at the order of 0.05, suggesting that the gain of individual ANPs cannot be explained by the amount of exposure to training instances, but it has more to do with the correlations between ANPs. Fig. 5c-d show sample images from the top and bottom 3 ANPs for the seen and unseen ANPs. Our Fact-Net always seems to have a larger gain over ANPs with fewer varieties.\nImage Retrieval by Fact-Net and Fork-Net. We also\ncompare models on retrieving images of a particular ANP. We rank the model output for all the images corresponding to the ANP, and return the images with top responses. The ANP could be seen or unseen in our dataset, or completely novel, e.g. dangerous summer. For Fork-net, we use the product of A-net and N-net output components corresponding to the ANP parts; for Fact-net, we use the output component directly corresponding to the ANP.\nFig. 6 shows side-by-side comparisons of top retrievals for 1 seen ANP (beautiful sky) and 3 unseen/novel ANPs by Fact-Net and Fork-Net. 1) Images returned by Fact-Net are in general more accurate on the noun: e.g. ugly baby and ugly sky images contain mostly baby and sky scenes,\nwhereas Fork-Net results contain mostly fish and buildings instead. 2) Fact-Net retrievals have more varieties on the adjective: e.g. beautiful sky images have both warm and cool color tones, whereas Fork-Net results have mostly cool\ncolor tones. 3) Fact-Net results correct more annotation mistakes: e.g. man with a tie tagged hot girls is rightly retrieved for beautiful men, whereas those mistakes such as females tagged sexy fashion and fragile body are retained\nin Fork-Net results for beautiful men. 4) Our Fact-Net can also be used for consensus re-tagging: while beauty is in the eyes of the beholder, we see that the images tagged beautiful sky become top retrievals for ugly sky, which do share characteristics with other gloomy scenes.\nConclusions. From our extensive experimentation, we gain two insights into the unique and challenging sentiment ANP detection task, unlike other well-defined image classification or captioning task. 1) For seen ANPs, the ANP-net is the winner, but it cannot handle unseen ANPs, a killing caveat. We set our initial goal to exceed the ANP-net baseline, after numerous trials, we realize that there will always be a baseline version that no factorized model could beat, since the former directly optimizes the performance over each ANP. However, such CNNs neither see the connections between As and Ns nor generalize as ours. 2) Our Fact-Net on unseen ANPs is substantially better than all baselines. In addition, due to noisy labels (Fig. 2 and Fig. 6), the results are actually even better: e.g., in Fig. 6, beautiful men retrieves correct results with wrong user labels of hot girls or cold beer. Our factorized ANP CNN not only trains better from noisy labels, generalizes better to new images, but can also expands the ANP vocabulary on its own."
    } ],
    "references" : [ {
      "title" : "What makes a beautiful landscape beautiful: Adjective noun pairs attention by eyetracking and gaze analysis",
      "author" : [ "Mohammad Al-Naser", "Seyyed Saleh Mozafari Chanijani", "Syed Saqib Bukhari", "Damian Borth", "Andreas Dengel" ],
      "venue" : "In Proceedings of the 1st International Workshop on Affect & Sentiment in Multimedia,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Automatic Attribute Discovery and Characterization from Noisy Web Data",
      "author" : [ "T. Berg", "A. Berg", "J. Shih" ],
      "venue" : "ECCV, pages 663–676. Springer, September",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Large-scale Visual Sentiment Ontology and Detectors Using Adjective Noun Pairs",
      "author" : [ "D. Borth", "R. Ji", "T. Chen", "T. Breuel", "S.-F. Chang" ],
      "venue" : "ACM MM, pages 223–232, October",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks",
      "author" : [ "T. Chen", "D. Borth", "T. Darrell", "S.-F. Chang" ],
      "venue" : "arXiv:1410.8586, October",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Object- Based Visual Sentiment Concept Analysis and Application",
      "author" : [ "T. Chen", "F. Yu", "J. Chen", "Y. Cui", "Y.-Y. Chen", "S.-F. Chang" ],
      "venue" : "ACM MM, pages 367–376, Novenber",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Studying Aesthetics in Photographic Images using a Computational Approach",
      "author" : [ "R. Datta", "D. Joshi", "J. Li", "J. Wang" ],
      "venue" : "ECCV, pages 288–301. Springer, May",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Describing Objects by their Attributes",
      "author" : [ "A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth" ],
      "venue" : "CVPR, pages 1778–1785, June",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning bilinear models for two-factor problems in vision",
      "author" : [ "William T. Freeman", "J.B. Tenenbaum" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Multi-scale orderless pooling of deep convolutional activation features",
      "author" : [ "Yunchao Gong", "Liwei Wang", "Ruiqi Guo", "Svetlana Lazebnik" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "CoRR, abs/1502.01852,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "What Makes an Image Memorable",
      "author" : [ "P. Isola", "J. Xiao", "A. Torralba", "A. Oliva" ],
      "venue" : "In CVPR, July",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "ImageNet Classification with Deep Convolutional Neural Networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "NIPS, pages 1106– 1114, December",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning to Detect Unseen Object Classes by Between-class Attribute Transfer",
      "author" : [ "C. Lampert", "H. Nickisch", "S. Harmeling" ],
      "venue" : "CVPR, pages 951–958, June",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification",
      "author" : [ "L.-J. Li", "H. Su", "L. Fei-Fei", "E. Xing" ],
      "venue" : "NIPS, pages 1378–1386, December",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bilinear cnn models for fine-grained visual recognition",
      "author" : [ "Tsung-Yu Lin", "Aruni Roy Chowdhury", "Subhransu Maji" ],
      "venue" : "In Arxiv,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Affective Image Classification using Features Inspired by Psychology and Art Theory",
      "author" : [ "J. Machajdik", "A. Hanbury" ],
      "venue" : "ACM MM, pages 83–92, October",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Assessing the Aesthetic Quality of Photographs using Generic Image Descriptors",
      "author" : [ "L. Marchesotti", "F. Perronnin", "D. Larlus", "G. Csurka" ],
      "venue" : "ICCV, November",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Im2text: Describing images using 1 million captioned photographs",
      "author" : [ "Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Bilinear classifiers for visual recognition",
      "author" : [ "H. Pirsiavash", "D.Ramanan", "C.C. Fowlkes" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "Attribute Learning in Large-scale Datasets",
      "author" : [ "O. Russakovsky", "L. Fei-Fei" ],
      "venue" : "Trends and Topics in Computer Vision, pages 1–14. Springer,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "arXiv:1409.1556, September",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Visual sentiment prediction with deep convolutional neural networks",
      "author" : [ "Can Xu", "Suleyman Cetintas", "Kuang-Chih Lee", "Li-Jia Li" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings",
      "author" : [ "V. Yanulevskaya", "J. Uijlings", "E. Bruni", "A. Sartori", "E. Zamboni", "F. Bacci", "D. Melcher", "N. Sebe" ],
      "venue" : "ACM MM, pages 349–.358, October",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Emotional Valence Categorization using Holistic Image Features",
      "author" : [ "V. Yanulevskaya", "J. van Gemert", "K. Roth", "A. Herbold", "N. Sebe", "J.M. Geusebroek" ],
      "venue" : "In ICIP,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2008
    }, {
      "title" : "Robust image sentiment analysis using progressively trained and domain transferred deep networks",
      "author" : [ "Quanzeng You", "Jiebo Luo", "Hailin Jin", "Jianchao Yang" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 24,
      "context" : "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 27,
      "context" : "Automatic assessment of sentiment from visual content has gained considerable attention [3, 4, 5, 26, 29].",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].",
      "startOffset" : 127,
      "endOffset" : 141
    }, {
      "referenceID" : 14,
      "context" : "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].",
      "startOffset" : 127,
      "endOffset" : 141
    }, {
      "referenceID" : 1,
      "context" : "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].",
      "startOffset" : 127,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "beautiful landscape [1], as compared to object detection [16, 24], scene categorization [9], or pure visual attribute analysis [7, 15, 2, 23].",
      "startOffset" : 127,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "It also has a simpler focus than image captioning which aims to describe an image as completely and objectively as possible [21, 13].",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "It also has a simpler focus than image captioning which aims to describe an image as completely and objectively as possible [21, 13].",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].",
      "startOffset" : 133,
      "endOffset" : 145
    }, {
      "referenceID" : 26,
      "context" : "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].",
      "startOffset" : 133,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "ANP labeling is related to broader and more abstract image analysis for aesthetics [6, 20], interestingness [11], affect or emotions [19, 28, 27].",
      "startOffset" : 133,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "[3] uses a bank Adjective Noun girls baby face eyes",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "of linear SVMs (SentiBank), and [4] uses deep CNNs.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 24,
      "context" : "Deep CNNs have also been used for sentiment prediction [26, 29], but they are unable to model sentiment prediction by a mid-level representation such as ANPs.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 27,
      "context" : "Deep CNNs have also been used for sentiment prediction [26, 29], but they are unable to model sentiment prediction by a mid-level representation such as ANPs.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "Our goal is to map an image onto embedding derived from the visual sentiment ontology [3] that is built completely from visual data and respects visual correlations along adjective (A) and noun (N) semantics.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "For example, not every ANP is popular on platforms like Flickr: adorable eyes and attractive baby are not frequent enough to have associated images in the visual sentiment dataset [3], suggesting that adorable is reserved more for overall impressions, whereas attractive is more for sexual appeal.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "Unlike the classical bilinear factorization model [8] which decomposes an image into style and content variations in a generative process, our model is discriminative and nonlinear.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "Compared to the bilinear SVM classifiers [22] which represents the classifier as a product of two low-rank matrices, our model learns both the feature and the classifier in a deep neural network achitecture.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 16,
      "context" : "We emphasize that our factorized ANP CNN is only seemingly similar to the recent bilinear CNN model [18]; we differ completely on the problem, the architecture, and the technical approach.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "1) The bilinear CNN model [18] is a feature extractor; it takes the particular form of CNN products and the two CNNs have no particular meaning.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "Experimental results on the publicly available dataset [3] demonstrate that our model significantly outperforms independent ANP classification on unseen ANPs, and on retrieving images of new ANP vocabulary.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : "We use VGG 16layer [25] as a base model and replace the final fully connected layer “fc8” from predicting 1,000 ImageNet categories to predicting 1,523 sentiment ANP classes.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 2,
      "context" : "VSO was created by mining online platforms such as Flickr and Youtube by the 24 emotions from Plutchnik’s Wheel of Emotions [3].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "We use the publicly available dataset of Flickr images introduced in [3] with SentiBank 1.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "The fc7-a and fc7-n layers are followed by a parametric ReLU (PReLU) layer for better convergence [10].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "Our models are implemented using our modified branch of CAFFE [12].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "For the two recurrent models, we expand and modify Andrej Karpathy’s ”neuraltalk” Github branch [13].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "Table 1: Visual Sentiment Ontology ANP statistics [3].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "DeepSentiBank [4] 9.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "DeepSentiBank [4] - ANP-Net - Fork-Net 0.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "We take the DeepSentiBank model [4] as a baseline, which already outperforms the initial SentiBank 1.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "model [3].",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : "It uses the AlexNet architecture [14] but fine-tuned to the ANP classes.",
      "startOffset" : 33,
      "endOffset" : 37
    } ],
    "year" : 2015,
    "abstractText" : "We consider the visual sentiment task of mapping an image to an adjective noun pair (ANP) such as ”cute baby”. To capture the two-factor structure of our ANP semantics as well as to overcome annotation noise and ambiguity, we propose a novel factorized CNN model which learns separate representations for adjectives and nouns but optimizes the classification performance over their product. Our experiments on the publicly available SentiBank dataset show that our model significantly outperforms not only independent ANP classifiers on unseen ANPs and on retrieving images of novel ANPs, but also image captioning models which capture word semantics from co-occurrence of natural text; the latter turn out to be surprisingly poor at capturing the sentiment evoked by pure visual experience. That is, our factorized ANP CNN not only trains better from noisy labels, generalizes better to new images, but can also expands the ANP vocabulary on its own.",
    "creator" : "LaTeX with hyperref package"
  }
}