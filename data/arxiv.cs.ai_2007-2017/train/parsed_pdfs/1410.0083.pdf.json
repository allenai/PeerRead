{
  "name" : "1410.0083.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Integrating active sensing into reactive synthesis with temporal logic constraints under partial observations",
    "authors" : [ "Jie Fu", "Ufuk Topcu" ],
    "emails" : [ "utopcu@seas.upenn.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Reactive synthesis; Active sensing; Partial observation; Temporal logic.\nI. INTRODUCTION Control synthesis under partial observations has been an important topic since complete and precise information (about the system and environment states) during the execution of a controller is often not available in practice. However, synthesis methods for systems under partial observations are of high complexity and have limitations in their applications. With incomplete information, the problem of synthesizing a controller in a partially observable Markov decision process (POMDP) has been shown to be PSPACEcomplete, even for finite planning horizons [9]. When the control specification is given in temporal logic and the environment is dynamic and possibly adversarial, the interaction between a system and its environment can be captured in a two-player partially observable game with infinite stages, for which the qualititive-analysis problem under finite-memory strategies is EXPTIME-complete [3].\nFor temporal logic constraints, synthesis algorithms for stochastic systems modeled as POMDPs have been studied\nThis work is supported by AFOSR grant number FA9550-12-1-0302, ONR grant number N000141310778 and NSF CNS award number 1446479.\n1Jie Fu and Ufuk Topcu are with the Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, 19104, USA jief, utopcu@seas.upenn.edu.\nin [11], [12]. To deal with a partially observable, dynamic environment, synthesis algorithms for two-player game with partial observations have been developed under two qualitative correctness criteria [2], [4]: sure-winning and almostsure winning controllers. A sure-winning controller ensures the satisfaction of a specification whereas an almost-sure winning controller is a randomized strategy and ensures satisfaction with probability 1. These solutions rely on a subset construction and has complexity exponential in the size of the state space [3], [5].\nAn interesting question that has not been investigated much is the following: Since the high computational complexity is caused by incomplete information, is it possible to reduce the computational effort and still ensure correctness of the control design by acquiring new information at run time? In this paper, we give a method that provides a partial, affirmative answer to this question. Particularly, we study a system with actions to obtain information, referred to as sensing actions, and show how to utilize these actions in a way that a given linear temporal logic (LTL) specification is satisfied almost surely with reduced computational effort.\nThe new approach in this paper is inspired by [10], where the authors propose a method of online planning with partial observations and sensing actions as a way to overcome such complexity since the system only needs to compute a strategy for a finite number of steps, and replans with new information obtained through sensing actions. For temporal logic specifications, online planning method in [10] has no correctness guarantee. We propose a similar framework of active sensing and reactive synthesis under temporal logic constraints. The basic approach is the following: During control execution, the system maintains a belief, which is a set of states it thinks the current state must be in based on its partial observation for the game history. The belief is updated under two cases: In one of these cases, the system or the environment makes a move, the belief is updated to the set of states possibly arrived at as a result of move. Alternatively, the system can activate a sensor, detecting the value of some propositional formula and revises its belief according to the additional information obtained through sensing. In the second case, the system applies an active sensing strategy. A sequence of sensor queries are made to obtain the most useful information for reducing the system’s uncertainty in the current state. The benefit of performing the combined active sensing and reactive planning is that we can indeed avoid solving a two-player zero-sum game with partial observations. Rather, we transform the sure-winning ar X iv :1 41 0.\n00 83\nv1 [\ncs .S\nY ]\n1 O\nct 2\n01 4\nstrategy for the system in the same game with perfect observations, into a randomized, belief-based strategy. By construction, the randomized strategy may not be defined for every belief the system can encounter at run time. During control execution, the system alternates between the randomized strategy and the active sensing strategy. We prove that if the set of available sensors meets a sufficient condition, the temporal logic specification can be satisfied with probability 1, i.e., almost surely.\nThe rest of the paper is organized as follows. We begin with some preliminaries and the formulation of the problem in section II. Section III presents the main results on synthesizing provably correct, online reactive controllers with sensing actions for temporal logic constraints. In Section IV we illustrate the method using a robot motion planning example in a partially observed environment."
    }, {
      "heading" : "II. PROBLEM FORMULATION AND PRELIMINARIES",
      "text" : "A probability distribution on a finite set S is a function D : S → [0, 1] such that ∑ s∈S D(s) = 1. The set of probability distributions on a finite set S is denoted D(S). The support of D is the set Supp(D) = {s ∈ S | D(s) > 0}. Let Σ be a finite alphabet. Σ∗, Σω , and Σ+ are sets of strings over Σ with finite length, infinite length, and length greater than or equal 1, respectively. Given u and v in Σ∗, uv is the concatenation of u with v. A string u ∈ Σ∗ is a prefix of w ∈ Σ∗ (or w ∈ Σω) if there exists v ∈ Σ∗ (or v ∈ Σω) such that w = uv. For a string w, the set of symbols occurring infinitely often in w is denoted Inf(w). The last symbol in a finite string w is denoted Last(w)."
    }, {
      "heading" : "A. Game, specification and strategies",
      "text" : "Through abstraction for systems with continuous and discrete dynamics, the interaction of a system and its dynamic environment can be captured by a labeled finite-state transition system [7], [8]:\nM = 〈S,Σ, δ, s0,AP, L〉\nwhere 1) S = S1 ∪ S2 is the set of states. At each state in S1, the system takes an action. At each state in S2, the environment takes an action. 2) Σ = Σ1 ∪ Σ2 is the set of actions. Σ1 is the set of actions for the system, and Σ2 is the set of actions for the environment. 3) s0 is the initial state. 4) δ : S×Σ→ S is the transition function. 5) L : S → 2AP is the labeling function that maps a state s ∈ S to a set of atomic propositions L(s) ⊆ AP that evaluate true at s.\nWe use a fragment of LTL [1] to specify the desired system properties such as safety, reachability, liveness and stability. Given a temporal logic formula ϕ in this class, one can always represent it by a deterministic Büchi automaton (DBA) Aϕ = 〈H, 2AP , δϕ, h0, Fϕ〉 where H is the set of states, 2AP is the set of alphabet, δϕ : H × 2AP → H is the transition function. h0 is the initial state and Fϕ is the set of final states. A word w = a0a1 . . . ∈ (2AP)ω induces a state sequence h0h1 . . . ∈ Hω where hi+1 = δϕ(hi, ai), for all i ≥ 0. A word w is accepted in Aϕ if and only if the\nstate sequence ρ ∈ Hω induced from w visits some states in Fϕ infinitely often.\nA product operation is applied to incorporate the temporal logic specification into the labeled transition system, giving rise to a two-player turn-based Büchi game between the system (player 1) and its environment (player 2):\nG = 〈Q,Σ, T, q0, F 〉 = M nAϕ\nwhere the components are defined as follows. • Q = Q1 ∪Q2 is the set of states, where Q1 = S1×H\nand Q2 = S2 ×H . • T : Q × Σ → Q is the transition function. Given\n(s, h) ∈ Q, σ ∈ Σ, if δ(s, σ) = s′, then T (q, σ) = q′ where q′ = (s′, δϕ(h, L(s′))). • q0 = (s0, δϕ(h0, L(s0))) is the initial state. • F ⊆ Q × Fϕ is a subset of states that determines a\nBüchi winning condition. A play in G is either a finite sequence of interleaving states and actions ρ = q0a0q1a1 . . . qn ∈ (Q ∪ Σ)∗Q or an infinite sequence ρ = q0a0q1a1 . . . ∈ (Q∪Σ)ω such that q0 is the initial state and T (qi, ai) = qi+1 for all i ≥ 0. If ρ is finite, the last element of ρ is a state, denoted Last(ρ). An infinite play ρ is winning for player 1 in G if and only if Inf(ρ) ∩ F 6= ∅.\nIn game G, each state in Q is associated with a truth assignment to a set P of predicates. Note that P may not equal AP . This association is captured by the interpretation function π such that for any q ∈ Q, for any predicate p ∈ P , π(q)(p) ∈ {true, false}. We write π(q) = ∧p∈P`p where `p = p if π(q)(p) = true and `p = ¬p if π(q)(p) = false, ∧, ¬ are the logical connectives for conjunction and negation, respectively. In the set P , there is a predicate t indicating whose turn it is to play: If t = 1, then the system takes an action, otherwise the environment makes a move. It is assumed that the value of t is globally observable, which means, the system always knows whose turn it is to play.\nWe consider the case when the system has partial observation of values for the set P of predicates. Following [4], this partial observation can be defined by an equivalence relation over the set of states, denoted R ⊆ Q×Q. Two states q and q′ are observation-equivalent, that is, (q, q′) ∈ R, if both q and q′ provide the same state information observable by the system, i.e., the value of p ∈ P is observable at q if and only if it is observable at q′, and π(q)(p) = π(q′)(p). We denote the observations of states for the system by O ⊆ 2Q, which is defined by the observation-equivalence classes. Clearly, O is a partition of the state space. We define an observation function Obs : Q∪Σ→ O∪Σ1∪{−} such that 1) q ∈ Obs(q); 2) for every q1, q2 ∈ Obs(q), (q1, q2) ∈ R, 3) if σ ∈ Σ1 Obs(σ) = σ; and 4) if σ ∈ Σ2, Obs(σ) = − . The last two properties express that the system observes (knows) which action it performed but does not directly observe the action of the environment. The information received by the system on the environment’s action is from the effect of that action, reflected in the observed arrived state.\nThe observation sequence of a play ρ = q0a0q1 . . . is a sequence Obs(ρ) = Obs(q0)Obs(a0)Obs(q1) . . .. It is worth mentioning that two states q = (s, h) and q = (s′, h′) can be observation-equivalent even if h 6= h′. Therefore, two observation-equivalent ρ and ρ′ can differ in their state projections onto the set Q of states in the specification automaton Aϕ.\nLet Pref(G) denote the set of finite prefixes of all plays in G, each of which ends with a state in Q. For both players 1 and 2, a deterministic strategy for player i is a function fi : Pref(G)→ Σi and a randomized strategy is a function fi : Pref(G)→ D(Σi). We say that player i follows strategy fi if for any finite prefix ρ ∈ Pref(G) at which fi is defined, player i takes the action fi(ρ) if fi is deterministic, or an action σ ∈ Supp(fi(ρ)) with probability fi(ρ)(σ) if fi is randomized. Since the system has partial information of the states, it can only execute an observation-based strategy f1, in the sense that if for any two prefixes ρ and ρ′ ∈ Pref(G), if Obs(ρ) = Obs(ρ′), then f1(ρ) = f1(ρ′). A strategy is memoryless if and only if fi(ρ) = fi(Last(ρ)). For Büchi game G with complete information, there exists a deterministic, memoryless winning strategy for one of the players."
    }, {
      "heading" : "B. Partial observation, belief and sensing actions",
      "text" : "With partial observations, the system keeps track of the play in the game by maintaining and updating a set B ⊆ Q of states, referred to as the belief, which is the set of states the system thinks the game can be in, given the observation history. In which follows, we show how the belief is obtained and updated. The set of beliefs in the game is denoted B ⊆ 2Q. We define a function α : Pref(G)→ B that maps a prefix of g into a belief as follows: given a prefix ρ = q0a0 . . . qn, the belief of the system is α(ρ) = {Last(ρ′) ∈ Q | ρ′ ∈ Pref(G) and Obs(ρ′) = Obs(ρ)}.\nDuring the interaction with the environment, the system’s belief is updated in two ways: (i) The system applies a control action, obtains a new observation of the arrived state, and updates its belief to one in which the current state could be. (ii) The environment takes some action. The system obtains an observation o ∈ O of the arrived state, and subsequently updates its belief that includes its hypothesis for the current state. Formally, this process is called belief update, which can be captured by the function\nUpdate : B × (Σ1 ∪ {−})×O → B, (1)\nIt is reminded that the symbol “−” is the observation for an action of the environment. Given a belief B, the system takes an action a ∈ Σ1 and gets an observation o ∈ O. Then it updates its belief to B′ = o ∩Update(B, a, o) = {q′ | ∃q ∈ B such that T (q, a) = q′}. If it is the environment’s turn, after the environment takes some action, the system gets an observation o ∈ O and then updates its current belief B to B′ = Update(B,−, o) = o ∩ {q′ | ∃q ∈ B, ∃σ ∈ Σ2 such that T (q, σ) = q′}.\nWe distinguish a set Γ of sensing actions for the system and explain how the sensing actions affects the system’s belief as follows.\nDefinition 1: Consider the set P of atomic propositions and the set Γ of sensing actions. For each sensing action a ∈ Γ, there exists at least one propositional formula φ over P such that after applying the sensing action a, the truth value of φ is known. Depending on the value of φ, the system can partition a belief B into two subsets, expressed by\nKnows(φ, a,B) := (B′, B \\B′),\nwhere B′ is the set of states in which φ evaluates true and B \\B′ is the set of states in which φ evaluates false. Hence, if φ is true, the belief is revised to be B′, otherwise to be B \\B′. To capture both global and local sensing capabilities, for a given state q, we denote Γq ⊆ Γ to be a set of sensing actions enabled at q. The set of sensing actions enabled at a belief B ⊆ Q is ⋂ q∈B Γq .\nThe following assumption is made for sensing actions. Assumption 1: A sensing action will not change the value of variables and/or predicates in P . The assumption is not restrictive because if an action introduces both physical and epistemic changes, we simply consider it as an ordinary control action and include it into Σ1. We call an action in Γ sensing to emphasize that it provides information of the current state, and an action in Σ physical to emphasize it changes the state of the game. We assume that at each turn of the system, it can either choose a physical action, or several sensing actions followed by a physical action.\nWe solve the following problem in this paper. Problem 1: Given a two-player turn-based Büchi game G = 〈Q,Σ, T, q0, F 〉, and a set Γ of sensing actions, design an observation-based strategy f : Q∗ → D(Σ1) ∪ Γ∗ with which the specification is satisfied with probability 1, i.e., almost surely, whenever such a strategy exists."
    }, {
      "heading" : "III. MAIN RESULTS",
      "text" : "For games with partial information, algorithms in [5] can be used to synthesize observation-based controllers which ensure given temporal logic specifications are satisfied surely, or almost surely, i.e., with probability 1, whenever such controllers exist. In this paper, we only consider the cases in which observation-based controllers do not exist and thus require additional information at run time for satisfying given temporal logic specifications. We distinguish two phases in the online planning: Progress phase and sensing phase. As the names suggest, during the progress phase, the system takes physical actions in order to satisfy the temporal logic constraints, and during the sensing phase, the system takes sensing actions to reduce the uncertainty in its belief for the current game state. The transition from one phase to another will be explained after we introduce the methods for synthesizing strategies used in both phases."
    }, {
      "heading" : "A. A belief-based strategy for making progress",
      "text" : "For a game with partial observation, we aim to synthesize a belief-based, memoryless and randomized strategy fP : B → D(Σ1) that can be applied for making progress towards satisfying the given LTL fragment formula ϕ.\nIn the two-player Büchi game G, the deterministic surewinning strategy WS : Q → Σ1 can be computed (with methods in [6]) but requires complete information to execute at run time. The belief-based strategy fP is constructed from the sure-winning strategy WS in the following way: Let Win1 ⊆ Q be the set of states at which WS are defined. Given B ∈ B, let\nProgress(B) = ⋃ q∈B WS(q), and\nallow(B) = ⋂ q∈B allow(q),\nwhere allow(q) = {σ ∈ Σ1 | T (q, σ) ∈Win1}.\nFor each state q ∈ B, the sure-winning strategy will suggest action WS(q) to be taken by the system, which is then included into a set Progress(B). The set allow(B) is a set of actions with the following property: No matter in which state of B the game is, by taking an action in allow(B), the next state will still be one for which the sure-winning strategy is defined. Then, if Progress(B) ⊆ allow(B), we let fP (B)(σ) = 1|Progress(B)| for each σ ∈ Progress(B). Otherwise, fP is undefined for B. Note that since the computation fP can be essentially reduced to computing the interaction of two sets, there is no need to compute fP for all possible subset of Q. Rather, we can efficiently compute fP for each belief B encountered at run time.\nWe have transformed the sure-winning strategy with complete information in the Büchi game into a randomized, belief-based strategy. During control execution, the system maintains its current belief. At each turn of the system, after applying an action σ ∈ Σ1 at the state B, the system receives an observation o ∈ O, updates its belief to B′ = Update(B, σ, o). When it is a move made by the environment, the system obtains another observation o′ ∈ O, updates its belief to B′′ = Update(B′,−, o′). The system applies fP (B′′) as long as fP is defined for B′′. When fP is undefined for the current belief B, then we switch to the sensing phase for actively acquiring more information to reduce the uncertainty in its current belief."
    }, {
      "heading" : "B. An active sensing strategy for reducing uncertainty",
      "text" : "During the progress phase with the randomized, beliefbased strategy fP , if the system runs into a belief at which fP is undefined, it needs to update its belief through sensing until either it finds itself in a state for which fP is defined, or it cannot further refine its belief: A belief B cannot be refined if for any sensing action a enabled at B and for any formula φ such that (B1, B2) = Knows(φ, a,B), it holds that for either i = 1 or i = 2, Bi = B. We represent the process of belief revision with sensing actions as a tree structure, referred to as a belief revision tree, and then\npropose a synthesis method for an active sensing strategy using the belief revision tree.\nGiven a belief Bo ∈ B, the belief revision tree with the root Bo is a tuple BRTree(Bo) = 〈N , E〉, where N is the set of nodes in the tree, consisting a subset of beliefs, and E ⊆ N × Γ × N is the set of edges. It is constructed as follows.\n1) The root of the tree is Bo. 2) At each node B ∈ N , for each enabled sensing\naction a ∈ ΓB , if there exists a formula φ such that (B1, B2) = Knows(φ, a,B) and both B1, B2 are not empty, then we add two children B1, B2 of B, and include edges (B, a,B1), (B, a,B2) into the edges E . 3) A node B is a leaf of the tree if and only if either 1) B cannot be further revised by any sensing action, or 2) fP is defined for B.\nThe active sensing strategy fS : B → Γ is computed as follows. First, in the tree BRTree(Bo), we compute a set of target nodes Reach ⊂ N such that a node B′ is included in Reach if and only if fP (B′) is defined. The objective is to apply the least number of sensing actions in order to reach a belief in Reach for which fP is defined. For this purpose, we have the following recursion:\n1) X0 = Reach, i = 0. 2) Xi+1 = Xi ∪ {B ∈ B | ∃a ∈ Γ, such that ∀B′ ∈ B, (B, a,B′) ∈ E , B′ ∈ Xi} and let fS(B) = a. In other words, a belief B is included into Xi+1 if there exists a sensing action a such that when a is applied at B, no matter which belief the system might reach, it must be in Xi. 3) Until i is increased to some number m ∈ N such that Xm+1 = Xm, we output the sensing strategy fS obtained so far.\nWe denote Xm = attr(Reach), following the notion of an attractor of the set Reach. For any state in attr(Reach), there exists a sensing strategy fS such that for whatever outcome resulted by applying sensing actions, the system can arrive at some belief in Reach in finitely many steps by following fS . Furthermore, it can be proven that fS minimizes the number of sensing actions required for the sensing phase under the constraint that the system will not run into a dead end, which is a belief that cannot be further refined yet is undefined by fP . The number of sensing actions during the sensing phase is upper bounded by the index i for which Bo ∈ Xi and Bo /∈ Xi−1. The proof follows from the property of attractor [6] and is omitted here.\nRemark: It is worth mentioning that for a given belief B, the active sensing strategy is unique. Thus, we can store and continuously update a set of active sensing strategies synthesized at run time: When the system encounters a belief B for which fP is undefined but it has seen before, it can use the stored active sensing strategy for B without recomputing a new one. For a large-scale system with a large number of sensing actions, one can also pre-compute a library of active sensing strategies and then augment the library with\nnew active sensing strategies computed at run time."
    }, {
      "heading" : "C. A composite, almost-sure winning strategy",
      "text" : "At run time, the system alternates between strategy fP for making progress and strategy fS for refining its belief. We name the system’s strategy at run time a composite strategy, denoted f : B → D(Σ1) ∪ Γ, defined by,\nf(B) = { fP (B) if fP (B) is defined. fS(B) if fS(B) is defined.\n(2)\nNote that by construction, the domains of fP and fS is always disjoint.\nThe following assumption provides a sufficient condition for avoiding dead-ends at run time.\nAssumption 2: For each state B encountered during the progress phase, if fP (B) is undefined, then fS(B) is defined.\nSince we cannot predict which beliefs the system might have during control execution with online planning, in the extreme case, for each predicate p ∈ P , we need to have a sensing action or a combination of sensing actions to detect its truth value. However, this condition is not necessary and may include some sensing actions that will never be used at run time. As the system does not need to know the exact state by extensive sensing, it is at the system’s disposal whether to apply a sensing action and what shall be applied.\nNext we prove the correctness of the composite strategy. To this end, we recall some property in the solution for Büchi games with complete information from [6]: The winning region of the Büchi game G can be partitioned as Win1 = ⋃m i=0Wi for some m ∈ N, m ≥ 0. For any state q ∈Win1, there exists a unique ordinal i such that q ∈Wi. If q ∈ Q1∩Wi for some 0 < i ≤ m, then the winning strategy on q outputs σ ∈ Σ1, with which the system reaches a state q′ ∈ Wi−1 ∩Q2. If i = 0, then with the action WS(q), we arrive at a state q′ ∈ Win1. If q ∈ Q2, then for any action σ ∈ Σ2 enabled at q, T (q, σ) ∈Wi−1 if i 6= 0, or q′ ∈Win1 otherwise.\nLemma 1: Given a game G = 〈Q,Σ, T, q0, F 〉. Let B0 = Obs(q0) be the initial belief. If Assumption 2 is satisfied and q0 ∈Win1, the composite strategy f defined by (2) ensures that some states in F of G is infinitely often visited with probability 1.\nProof: Consider an arbitrary belief B ∈ B for which fP is defined. By definition of fP , for each σ ∈ Progress(B), the probability of choosing action σ is 1u , where u = |Progress(B)|. If the actual state is q and q ∈ Wi, for some i 6= 0, then with probability 1u , the system will reach a state in Wi−1. Thus, the probability of the next state being in Wi−1 is 1u ≥ 1 |Q| > 0. For other σ\n′ ∈ fP (s), σ′ 6= WS(q), the next state after taking σ′ is in Wj for some 0 ≤ j ≤ m. Let Pr(q,♦iW0) denote the probability of reaching W0 from state q in i turns. When system applies the strategy f , it is Pr(q,♦iW0) ≥ ( 1|Q| )\ni > 0 and the probability of not reaching W0 in i turns is less than or equal to 1 − ( 1|Q| ) i ≤ 1 − ( 1|Q| ) m+1 = r < 1 where m + 1 is the total number of partitions in Win1. If after\ni steps the state is not in W0, it must be in Wj for some 0 < j ≤ m, and again the probability of not reaching W0 in m steps is less than or equal to r. Therefore, under the policy f , the probability eventually reaching W0 from any state q ∈ Win1 is Pr(v,♦W0) = limk→∞ Pr(v,♦kW0) = limk→∞(1 − Pr(v,¬♦kW0)) = limk→∞(1 − rk/m) = 1− limk→∞ rk/m = 1.\nOnce entering W0, the system will take an action to remain in Win1, and the above reasoning applies again. In this way, in the absence of dead ends (Assumption 2), the system can revisit the set W0 of states with probability 1 by following the composite strategy f . Since W0 ⊆ F , the probability of system always eventually visiting some states in F is 1.\nTo conclude this section, Algorithm 1 describes the procedure of online planning with sensing actions."
    }, {
      "heading" : "IV. EXAMPLES",
      "text" : "We apply the algorithm to a robotic motion planning example, which is a variant of the so-called “Wumpus game” in a 7× 7 gridworld. Figure 2 consists of one mobile robot, one monster called “Wumpus”. The robot is capable of moving in eight compass directions with actions ‘N’, ‘S’ , ‘E’, ‘W’, ‘NE’, ‘NW’, ‘SE’, ‘SW’ (horizontally, vertically and diagonally), one step at a time. The robot and the Wumpus does not move concurrently. The Wumpus can move in four compass directions with actions ‘N’, ‘S’, ‘E’ and ‘W’ within a restricted area Region and emits stench to its surrounding cells. The objective of the robot is to infinitely revisit region R1, R2, and R3 in this order, while avoiding running into\nthe Wumpus. Formally, the temporal logic formula is ϕ = ♦(xr, yr) = R1 ∧ ♦ ((xr, yr) = R2 ∧ ♦(xr, yr) = R3) ∧ ¬(xr = xw ∧ yr = yw) where (xr, yr), (xw, yw) are the positions of the robot and the Wumpus, respectively. Yet, the robot only knows his own position. For this case of partial observation, without the inclusion of sensing actions, it can be shown that with the algorithms in [5], observation-based, sure-winning strategies and almost-sure winning strategies do not exist.\nHere, we introduce a set of sensing actions to the game. For the robot to know the position of the moving obstacles, it needs to apply a sensing action — smell(x, y) to detect if there exists stench at cell (x, y). Thus, when the robot applies smell(x, y), if the result is True, then the Wumpus must be some cells in the set S = {(x′, y′) | x′ ≤ x+1, y′ ≤ y+1, x′, y′ ∈ N}∩Region. Otherwise, it is not possible that the Wumpus is in any cell in S.\nWe illustrate how the robot updates his belief using sensing action smell(x, y) where (x, y) is a cell in the gridworld. Suppose that the robot does not know where the Wumpus is and hypothesizes it can be in any cell in the Region. Once it applies the sensing action (2, 2), since the cell has stench and the sensor returns True. Then, immediately the robot will know the Wumpus is in one of the cells in the set S = {(1, 1), (2, 1), (3, 1), (1, 2), (2, 2), (3, 2), (3, 1), (3, 2), (3, 3)}, because only if the Wumpus is in a cell of S, there can be stench in cell (2, 2).\nFrom the numerical experimental result, after 1000 steps (a step includes either a robot’s (sensing or physical) action or a movement of the Wumpus), the robot visited the set F in the formulated two-player game G 14 times and can continue to visit F infinite often. In Figure 3 we show the belief updates by applying alternatively the exploitation strategy and active sensing strategy for the initial 100 steps. It is observed that the maximum cardinality of the belief set is 43 over the control execution, which means that the robot thinks the Wumpus can be in any cell in its restricted region. However, if there is no danger of running into the Wumpus in a few next steps, there is no need to exercising any sensing action. The implementations are in Python on a desktop with Intel(R) Core(TM) i5 processor and 16 GB of\nmemory. The average time for the robot making a decision is 8.55 × 10−4 seconds. The computation of the product game took 40.14 seconds and the winning strategy under complete information is computed within 14 seconds."
    }, {
      "heading" : "V. CONCLUSIONS",
      "text" : "Our work shows that when additional information can be obtained through sensing actions, one can transform a sure-winning strategy with complete information to a beliefbased, randomized strategy, which is then combined, at run time, with an active sensing strategy to ensure a given temporal logic specification is satisfied with probability 1. The synthesis method avoids a subset construction for solving games with partial information. Meanwhile, the active sensing strategy leads to a cost-efficient way of sensor design: Although we require a sufficient set of sensing actions to avoid dead-ends at run time, the system minimizes the usage of sensing actions by asking the most revealing queries, depending on what specification is to be satisfied, and how much uncertainty the system has about the game state at run time. In future work, we will consider more examples for practical robotic motion planning under partial observations. It is also important to consider the uncertainty in the sensors. For example, a sensor query might return a probabilistic distribution over a set of states, rather than a\nbinary answer to proposition logical formulae considered herein. For this extension, we are currently investigating modifications that need to be made to account for delays, uncertainty in the information provided by the sensors."
    } ],
    "references" : [ {
      "title" : "Deterministic generators and games for LTL fragments",
      "author" : [ "Rajeev Alur", "Salvatore La Torre" ],
      "venue" : "ACM Transactions on Computational Logic,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Games for synthesis of controllers with partial observation",
      "author" : [ "A Arnold", "A Vincent", "I Walukiewicz" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "The complexity of partialobservation parity games. In Logic for Programming, Artificial Intelligence, and Reasoning, pages",
      "author" : [ "Krishnendu Chatterjee", "Laurent Doyen" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Partial-Observation Stochastic Games: How to Win When Belief Fails",
      "author" : [ "Krishnendu Chatterjee", "Laurent Doyen" ],
      "venue" : "Annual IEEE Symposium on Logic in Computer Science,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Algorithms for omega-regular games with imperfect information",
      "author" : [ "Krishnendu Chatterjee", "Laurent Doyen", "Thomas A Henzinger", "Jean-François Raskin" ],
      "venue" : "Logical Methods in Computer Science,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Automata Logics, and Infinite Games: A Guide to Current Research",
      "author" : [ "Erich Grädel", "Wolfgang Thomas", "Thomas Wilke", "editors" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "A fully automated framework for control of linear systems from temporal logic specifications",
      "author" : [ "M. Kloetzer", "C. Belta" ],
      "venue" : "Automatic Control, IEEE Transactions on,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Correct, reactive robot control from abstraction and temporal logic specifications",
      "author" : [ "Hadas Kress-Gazit", "Tichakorn Wongpiromsarn", "Ufuk Topcu" ],
      "venue" : "IEEE Robotics and Automation Magazine,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Algorithms for sequential decision making",
      "author" : [ "Michael Lederman Littman" ],
      "venue" : "PhD thesis, Brown University,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1996
    }, {
      "title" : "Replanning in domains with partial information and sensing actions",
      "author" : [ "Guy Shani", "Ronen I Brafman" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2026
    }, {
      "title" : "Formal methods for control synthesis in partially observed environments : application to autonomous robotic manipulation",
      "author" : [ "Rangoli Sharan" ],
      "venue" : "Dissertation (Ph.D.), California Institute of Technology. PhD thesis, California Institute of Technology,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Control of probabilistic systems under dynamic, partially known environments with temporal logic specifications",
      "author" : [ "Tichakorn Wongpiromsarn", "Emilio Frazzoli" ],
      "venue" : "In Proceedings of the 51th IEEE Conference on Decision and Control,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "complete, even for finite planning horizons [9].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "which the qualititive-analysis problem under finite-memory strategies is EXPTIME-complete [3].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "in [11], [12].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "in [11], [12].",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "To deal with a partially observable, dynamic environment, synthesis algorithms for two-player game with partial observations have been developed under two qualitative correctness criteria [2], [4]: sure-winning and almostsure winning controllers.",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 3,
      "context" : "To deal with a partially observable, dynamic environment, synthesis algorithms for two-player game with partial observations have been developed under two qualitative correctness criteria [2], [4]: sure-winning and almostsure winning controllers.",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "These solutions rely on a subset construction and has complexity exponential in the size of the state space [3], [5].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "These solutions rely on a subset construction and has complexity exponential in the size of the state space [3], [5].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "The new approach in this paper is inspired by [10], where the authors propose a method of online planning with partial",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "For temporal logic specifications, online planning method in [10] has no correctness guarantee.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "A probability distribution on a finite set S is a function D : S → [0, 1] such that ∑ s∈S D(s) = 1.",
      "startOffset" : 67,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "namic environment can be captured by a labeled finite-state transition system [7], [8]:",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "namic environment can be captured by a labeled finite-state transition system [7], [8]:",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "We use a fragment of LTL [1] to specify the desired system properties such as safety, reachability, liveness and stability.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "Following [4], this",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "For games with partial information, algorithms in [5] can be used to synthesize observation-based controllers which ensure given temporal logic specifications are satisfied surely, or almost surely, i.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "In the two-player Büchi game G, the deterministic surewinning strategy WS : Q → Σ1 can be computed (with methods in [6]) but requires complete information to execute at run time.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "The proof follows from the property of attractor [6] and is omitted here.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "To this end, we recall some property in the solution for Büchi games with complete information from [6]: The winning region of the Büchi game G can be partitioned as Win1 = ⋃m i=0Wi for some m ∈ N, m ≥ 0.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "For this case of partial observation, without the inclusion of sensing actions, it can be shown that with the algorithms in [5], observation-based, sure-winning strategies and almost-sure winning strategies do not exist.",
      "startOffset" : 124,
      "endOffset" : 127
    } ],
    "year" : 2014,
    "abstractText" : "We introduce the notion of online reactive planning with sensing actions for systems with temporal logic constraints in partially observable and dynamic environments. With incomplete information on the dynamic environment, reactive controller synthesis amounts to solving a two-player game with partial observations, which has impractically computational complexity. To alleviate the high computational burden, online replanning via sensing actions avoids solving the strategy in the reactive system under partial observations. Instead, we only solve for a strategy that ensures a given temporal logic specification can be satisfied had the system have complete observations of its environment. Such a strategy is then transformed into one which makes control decisions based on the observed sequence of states (of the interacting system and its environment). When the system encounters a belief—a set including all possible hypotheses the system has for the current state—for which the observation-based strategy is undefined, a sequence of sensing actions are triggered, chosen by an active sensing strategy, to reduce the uncertainty in the system’s belief. We show that by alternating between the observation-based strategy and the active sensing strategy, under a mild technical assumption of the set of sensors in the system, the given temporal logic specification can be satisfied with probability 1.",
    "creator" : "LaTeX with hyperref package"
  }
}