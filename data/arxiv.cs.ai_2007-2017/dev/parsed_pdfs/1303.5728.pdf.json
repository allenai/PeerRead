{
  "name" : "1303.5728.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Conflict and Surprise: Heuristics for Model Revision",
    "authors" : [ "Kathryn Blackmond Laskey" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Any probabilistic model of a problem is based on assumptions which, if violated, invalidate the model. Users of probability based decision aids need to be alerted when cases arise that are not covered by the aid's model. Diagnosis of model failure is also necessary to control dynamic model construction and revision. This paper presents a set of decision theoretically motivated heuristics for diagnosing situations in which a model is likely to provide an inadequate representation of the process being modeled.\n1 INTRODUCTION Building a model for an inference problem involves constructing and reasoning within a restricted universe of propositions relevant to the inference problem at hand. Following Savage (1954), I term this restricted universe a small world (actually, a small set of possible worlds). The small world must obviously include those propositions of direct inferential interest. It also includes certain other propositions which bear on the propositions of interest and about which information may be available, either directly or indirectly. A model for the inference problem specifies relationships (logical or probabilistic) between propositions in the small world, and inference rules for revising beliefs as information is obtained.\nThe small world includes those propositions represented explicitly in the inference system. But a model of the relationships between these propositions often depends on a background context that is not explicitly represented. The model may make inferences that are seriously in error if these background assumptions are violated. For example, a medical diagnosis system may confidently misdiagnose a patient who is actually suffering from a disease it does not know about. A threat assessment system will be deceived by electronic interference that puts \"ghost targets\" on the radar screen if the behavior of the interference device is not\nrepresented in its knowledge base. A navigation system may go awry if unforseen weather conditions impact the performance of its sensors.\nNo modeler can hope to cover all possibilities that might arise. If the model is a good one, the situations it cannot handle should be rare. But it is important to be able to recognize such situations when they arise. Even when a system cannot revise its own model of the situation, it can alert the user that its model may be inadequate in the current situation. As research in dynamic model construction matures, model failure indicators will provide an important component of a control strategy for model construction and revision. It may be necessary or desirable not to explore some search paths during network construction, or to prune parts of the network when hypotheses become improbable or nearly independent of the hypotheses of interest. But an improbable hypothesis may become more probable as more evidence is observed. In such situations, a trigger is needed to alert the system that it may be necessary to explore search paths that were initially ignored, or to bring back pruned parts of the network.\n2 BACKGROUND A small world for an inference problem can be represented as a vector X of propositional variables. Each variable Xi can take on values in the set XiE { XiJ, ... , Xiki I. Let & denote a subvector of X whose values have been observed; these are called the e vidence variables. Let Xt denote a subvector of target variables, or variables whose values have not been observed, but are of direct interest. Denote the remaining unobserved variables as Xu. Unobserved variables may become evidence variables if their values are observed at a future time, but at present their values are of interest only because of their relationship to the target variables. Assume the variables are ordered so that X = IXt. Xe. Xu I. The goal of inference is to draw conclusions about the values of the target variables X 1 given observed values & for the evidence variables.\n198 Laskey\nA probabilistic model for the small world assigns a probability distribution over the variables in the small world. The a ssessed probability for an assignment of values to the variables is denoted by Pa(K). Inference within the model consists of conditioning the assessed probability distribution on the observed values for the evidence variables:\npa�.Xu I�)= P aw -- ,or\npa�)\nPa� I�) (I)\nA model is at best an approximation of what is being modeled. The distribution PaO is assessed relative to some assumed context; if the assumptions are violated, then the model no longer applies. If Pa(-) is a good approximation, one should feel confident that the assumptions underlying the model are at least approximately correct. Intuitively, the approximation is good if Pa(Xt I Xe) is nearly correct for most&·\nThis intuitive notion of a good approximation is far from precise, and glosses over some important questions. What does it mean for P•(Kt I &) to be \"nearly correct?\" What should be the definition of \"most & ?\" I return to these questions later, when I formalize the idea of approximating a model. For now, an intuitive understanding of the quality of an approximation should suffice.\nUntil recently, research on automated probabilistic inference has focused on computationally efficient methods for computing (1). Much less consideration has been given to the issue of deciding whether (I) is an adequate representation of the data generating process. The statistical community has devoted more attention to this issue, and there is a large literature on the theory of statistical hypothesis testing. However, the statistical hypothesis testing paradigm assumes an explicit, well formulated alternative model against which the current model is tested. In artificial intelligence applications, the purpose of model diagnosis is to initiate search for an alternative model. Requiring explicit representation and computation of the alternative model prior to model diagnosis defeats the purpose of the entire enterprise.\nResearch on model diagnosis from an AI perspective is beginning to receive more attention. There is general agreement that suspicion should be aroused when a combination of evidence items occurs that was initially assessed to be highly improbable: that is, when Pa(Ke) = LOW .1 The problem with this idea is defining LOW.\n1 This recommendation appears to violate the likelihood principle, a central tenet of Bayesian theory. According to the likelihood\nWhen there are many uncertain evidence items, the probability of any one combination of values is bound to be quite small. Thus, the definition of a low probability evidence combination must be relative to the other evidence combinations that might have occurred. Habbema (1976) suggests identifying a set of \"surprising\" observations. An observation in the surprising set triggers extra diagnostic attention for the case in question. According to Habbema's definition, each observation identified as surprising must be less probable than all observations not considered surprising; and the total probability of the set of surprising observations must be less than some threshold a. Laskey ( 1990) suggests comparing Pa(&) with the expected value E[PaCX.e)l of the probability of the evidence. These suggested approaches do provide measures of relative improbability, but they appear to be computationally intractable for the inference network models common in the literature on uncertainty in AI.\nJensen et al. (1990) suggest a tractable indicator of conflict between items of evidence. Their conflict measure can be written: ( ?\"(xe l) ... pa(xek) J c1 = Iog2 . Pa(xe l •· .. ,Xek) (2) where the Xei are the observed values of the components of the evidence vector Xe = (Xe 1, ... , Xek). This measure is easy to compute using any of the currently popular evidence propagation algorithms. Both the numerator and denominator of (2) can be produced as a natural byproduct of evidence accumulation, if each node stores its original prior probability distribution Pa(Xi) along with its current conditional probability distribution p•(xi I&). The numerator of (2) is just the product of the prior probabilities of the observed values of the evidence variables. The denominator is also straightforward to compute as follows. Assume that evidence items Xe1· ... , Xek-1 have been observed, that P(Xe1 .... ,Xek-1) has been computed, and that the evidence absorption algorithm has resulted in the computation of a revised distribution for Xek· namely pa(Xek I XeJ, ... ,Xek-1). Now, when Xek is observed, the required joint probability can easily be computed: Pa(Xe1 , ... ,xek)=Pa(XekiXeJ , ... ,Xek-J)Pa(Xe1 , ... ,Xek-J).\nJensen et al. justify CJ heuristically: they simply assert that one would expect the observed evidence to have higher probability than the product of the joi�t probabilities. Intuitively, if Xe1 has been observed, It might be reasonable to expect to observe values for Xez\nprinciple. the likelihood of data that might have been observed is irrelevant; all that matters is what was observed. But the likelihood principle applies to the agent's \"true\" model. The likelihood of unobserved data may indeed be relevant to the issue of whether the current approximation remains tenable.\nConflict and Surprise: Heuristics for Model Revision 199\nFigure 1: CJ Positive With Probability .55\nP(x,y) P(x)P(y) Vl V? Y''l Yl n Y''\\\n� .1125 .H .H .3325 Xl .RH4 .1106 .1106 .3325\nX2 .1125 .H .H .3325 X2 .RU4 .1106 .1106 .3325\nX� .H .1125 .1125 .335 q .1122 .RU4 .u li4 .335 .335 .3325 .3325 .335 .3325 .3325\nCJ given (x,y) Yl Y2 Y<\nXJ -.0413 .0073 .0073\nX2 -.0413 .0073 .0073\nX� .0289 -.0413 -.0413\nthat are made more likely by the observation of XeJ, that is, values Xe2 for which\nP(x02 I x01) > P(x02) , or\n(3)\nBut the example in Table 1 demonstrates that it is not necessarily ture that (3) is satisfied most of the time. In this example, there is a .55 chance that c1 is greater than zero, i.e., there is a better than even chance that the product of the marginal probabilities exceeds the joint probability.\nAlthough it may be probable that c1 is greater than zero, I will show in the next section that very large values of c1 are highly improbable. Furthermore, c1 is never positive in expected value. Its expectation is given by:\nE[c1l = -L F\"'�)logz( a p•�� ) . (4) 3, P (x01) .. ·P (x.k)\nThis quantity is the negative of the information theoretic distance from .the probability distribution P\"O to the distribution P'O, in which all the Xei are independent, but their marginal distributions are the same as their marginal distributions under Pa(-). That is, the expected value of c1 is a l)leasure of how closely the probability distribution P'O approximates the assessed distribution PaO. A well-known result from information theory states that ( 4) is never positive &nd is equal to zero only if the distributions P\"O and P'O are identical (Kullback, 1959). In other words, the expected value of c 1 is more negative the greater the\nnonind.ependency among evidence items--that is, the less well P'O approximates Pa(-).\nWhen c1 is positive, the probability Pi(Xe) of & under the independence model is greater than the probability Pa(&) under the assessed distribution. In other words, a model in which the evidence items are independent fits the observed evidence better than does the assessed distribution. Now, the whole modeling exercise was based on the assumption that the Xei were related to Xt. and therefore to each other. That is, combinations of evidence items characteristic of a particular l>t should tend to occur together. When the independence model fits better than the assessed distribution, it is an indication that the particular combination & of observed values is characteristic of no l>t under the current model.\nThe next section generalizes c1 to a family of \"model suspicion\" measures based on comparing how well pao fits the evidence relative to an alternative model. Unlike formal statistical hypothesis testing, the alternative model is generally not taken seriously as a candidate model for the data generating process. Rather, its purpose is to alert a system or user that the current model may be inadequate given the current situation.\n3 THEORETICAL FRAMEWORK It is now time to develop a formal framework for model approximation and model failure diagnosis. Assume that the small world X is embedded within a larger world W = (X'.Y). The vector X' represents the same variables as X. but each may have additional outcomes in the larger world that are not represented in the small world. That is, XlE {XjJ, ... , Xik-o Xi(k-+1) . ... , Xir· },\nf. l I I I\nwhere only the ust ki va ues are also possible values for X. The vector Y corresponds to variables that are not explicitly represented in the small world. Assume that there is some probability distribution PO on the larger world which PaO is intended to approximate.\n200 Laskey\nInterpreting the distribution PO raises philosophical issues which lie beyond the scope of this paper, but a few words on how I interpret PO are appropriate. My main concern is with an automated reasoning system which computes belief values for the target variables Xe based on any evidence � that has been observed to date. Such systems are usually engineered by assessing probabilities from some expert or experts in the domain about which the system reasons. In this context, pliO represents a probability distribution assessed from the expert, who has restricted attention to the small world X. I assume that pa( ·) approximates some distribution PO over W, in the sense that PaO i s obtained from PO by conditioning on the small world X. This does not necessarily mean that PO exactly represents the expert's beliefs over W, or even that the expert has well defined and coherent \"true\" probabilities over W. What I do assume is that if the expert were to make the effort to assess beliefs over the expanded world W, that this would result in the distribution P( · ), and that this distribution is a more accurate representation of the expert's beliefs than the assessed distribution pll(-). In other words, if computation and assessment burden were not an issue, and if a second system were built using the distribution PO, the expert would feel more satisfied with the output of the second system than with the output of the system based on the model PaO. It is also assumed that the expert thinks it is unlikely that the assumptions underlying the assessments are violated--that is, the expert thinks the small world is probable.\nA somewhat different interpretation is appropriate for systems designed for dynamic model revision. Here it is assumed that the full distribution PO was assessed from the expert, but is represented only implicitly in the system's knowledge base. The system has explicitly constructed just a small portion of this large implicit joint probability distribution. The assumptions underlying the model the system constructs amount to conditioning on the small world.\nLet the proposition q represent the assumptions underlying the assessed distribution. The proposition q includes the restriction of the values of X' to X. as well as some assumptions about the variables y:2\nq = (/\\ Xje {xi!•···•xik})A(V Y=j:). (5) I ):j\nP\\·) was assessed under the assumption that q was the case. That is, for any x in the small world X:\n2This formulation might seem to preclude assumptions about the relationship between variables in X. (e.g., that X; and Xj are independent). However, one could [psot that X; and Xj are independent conditional on Y k• where Yk has high probability.\nPa(x) = P(x 1 q) = L P(xs. I q) . (6) Y.\nIf q is not the case, the model Pa(-) is not appropriate, and should be replaced by:\nP0(x) = P(x I -,q) = L P(xs_ I ...,q) . (7) Y.\nNeither the alternative model nor the probability P(q) is assessed explicitly. However, I assume that the context q is assumed because it is thought to be probable, i.e., P(q) = 1-e, where e is small. The model PO. restricted to the variables x·. can be written:\n(8)\n(where pa(X) is understood to be equal to zero if one of the Xi is outside the range of Xi). Because e is very small, Pa(A) provides a good approximation to the correct joint probability P(X). To summarize, I assume that the small world model pao over X is obtained by a combination of conditioning on assumptions thought to be probable (i.e., conditioning on q) and marginalizing on variables not of direct interest (i.e., summing over values of Y).\nThe goal of inference is to estimate P(& IXe). Because the conditioning operator is nonlinear, there is no guarantee that the approximation error will remain small as evidence is absorbed. The relationship between the actual and estimated posterior probabilities given &=� can be seen from the following expression:\nP(� I x_) =\n(1-e)Pa�.x_) + eP0(�,x_)\n(1-e)Pa(x_) + eP0(x_)\n= (1-e*)Pa� I x_) + e*P0� I x_), (9)\nwhere\ne* = (1-e)Pa(x.) + eP0{x.)\nComparing (8 ) and (9), it is clear that the \"true\" prior and posterior probabilities have the same form: both are weighted averages of the assessed model probability PaO and the unknown alternative model probability J><'O. The prior probability e of the assessed model is replaced in (9) by e*, the posterior probability of the assessed model given that the evidence variables Xe take on values�.\nNote that\nE* e --·--. (l-e) Pa�) (10) 1-E*\nThus, the approximation error becomes large when P0(.1\\e) is so much larger than Pa(.l\\e) that it swamps the difference in magnitude between e and 1-e. The first term in (10) is called the prior odds ratio; the second term is called the likelihood ratio of the data given the two models being compared. It would seem straightforward, then, to decide when a model is no longer a good approximation to the observed data. Simply compare the probability of the observations under the approximate model to the probability under the alternative model, and i�i�i�te model revision when the ratio of these probabilities becomes too small. But a moment's thought reveals that this will not do: the reason for adopting the approximation in the first place was to avoid the expense of explicitly constructing a detailed model of the many improbable contexts in which P\"O does not apply. In other words, you need to have already performed model revision in order to have pO(.l\\e) available for computing (10). However, it may be possible to formulate \"straw models,\" which capture some of the expert's intuitions about how the model could go wrong but are computationally much simpler than the fully s�cifi�d alternate model. The independence model descnbed m Section 2 is just such a model. The following theorem shows that a straw model is unlikely to fit much better than the assessed model in cases for which the assessed model applies. Theorem 1: Let p•o and P5(-) be probability distributions over X. Define the index of surprise at evidence Xe under p•o relative to pS(-) as:\nc5 = log{ :�� ] . (II) Let 1tK be the probability under p•o that cs is greater than K. Then 1tK < 2-K. Proof:\n� L 2K.pa�) = 2K1tK . p'�) K\n-->2 p'�\nConflict and Surprise: Heuristics for Model Revision 201\n-K Therefore, 1tK < 2 •\nA trivial consequence of Theorem 1 is that high values of conflict are a priori unlikely when the assessed model is considered probable. That is, any alternative model specified a priori is unlikely to fit the data much better than the assessed model. 3 Corollary 1: If Xe is distributed according to P(k) = (1-e)P•(Xe) + eP0(&,), where pO(·) is a distribution not necessarily the same as P\"O, then thg probability that cs is less thanK exceeds (1 - e)(1 - 2- ). The trick is to specify an alternate model that is likely to fit the data better than the assessed model when p•o does not apply. That is, we would like for cs to be large when e* is large (or at least in the most probable situations in which e* is large). To construct a model revision indicator, then, the modeler thinks carefully about situations in which the model might not apply, and about how the predictions of the model might fail when this happens. The modeler then formulates a straw model P50 which is computationally simple but captures some important features of situations in which the assessed model is likely to fail. Consider for example the conflict indicator c J. The evidence variables Xe are predictors of the target variables. They should therefore be expected to be marginally dependent--certain patterns of values of the Xe· are likely to occur together because they indicate a particular target vector �1; other combinations of values are unlikely because they are unlikely given any target vector. Without formulating an alternate model in detail, the modeler can still use situations in which the independence model fits the data better than the assessed model to diagnose possible problems with the assessed model.\n4 REBUTTALS A general theoretical framework for heuristic model revision indicators has been presented. In summary, the system is assumed to reason under a current model, which is regarded as an approximation to some more accurate model which may or may not be represented implicitly in the system's knowledge base. The approximate model is conditioned on an assumed context which the system regards as highly probable. If the assessed model is an accurate model of the phenomenon in question, it would be expected to fi� t�e data better than other candidate models. A heunstlc model revision indicator can be constructed by building\n3of course, one can always specify after the fact a model that predicted the observed data exactly. When there are many uncertain evidence items, the observed evidence will have a very low probability, and this \"20-20 hindsight\" model will fit much better than the assessed model. However, the a priori probability that exactly this model would fit the data was extremely low.\n202 Laskey\nan alternate \"straw model\" which is not necessarily taken seriously as a model of the phenomenon in question, but which is likely to fit better than the assessed model in some class of situations which violate the assumed context. I have already considered one example of a \"straw model\" which gives rise to Jensen et al.'s conflict indicator c1. In this section, I relate another proposed model revision strategy (Laskey, 1990) to the framework proposed in Section 3. A Bayesian network for the probability model P80 is a directed acyclic graph in which each node corresponds to a variable Xi and the directed arcs encode direct conditional dependencies. Suppose that N is a Bayesian network for P80 and Xi!' ... , Xik is an ordering of the variables such that all preaecessors of Xi- in N are also predecessors in the ordering. Then Xi. i� independent of all preceding Xi given its immediate predecessors in\nN. The direct p�nts of the node X; are denoted by the vector Xp(i)· A probability model and associated Bayesian network on a set X of variables can be completely characterized by its node models. A node model for variable Xi is the tuple\n(12)\nconsisting of the variable, its direct parents in N , and a set of conditional probability distributions for the node, one for each combination of values for its parent variables. In previous work (Laskey, 1990; see also Laskey, Cohen and Martin, 1989), I have suggested associating rebuttals with each node model.4 A rebuttal is a proposition expressing a condition under which the assessed probability distribution for the node model does not apply. Rebuttals make explicit the contextual assumptions underlying the model: in the terminology of Section 3, the truth of a rebuttal implies -.q. Thus, the system can monitor the rebuttal propositions and trigger model revision when a rebuttal is observed to be true. However, the system may not have the resources to check rebuttals routinely, or obtaining information about some rebuttals may be costly in time or other resources. Sometimes it might be desirable to associate a general \"ceteris non paribus\" rebuttal with a node model. That is, one allows for the possibility that the node model does not apply, but does not explicitly model the circumstances under which the assumptions are violated (unless the general rebuttal becomes sufficiently probable to warrant such effort).\n\"The tenn rebuttal is due to Toulmin (e.g., Toulmin et a!., 1984); its use in this context is due to Matvin Cohen (cf., Cohen. Laskey and Ulvila, 1987).\nLet Rj be a variable taking on values RiE { ti.fil, where ti means that at least one of the rebuttals to 1'ti is true, and fi = -,ti means that all rebuttals are false. The assessed node model distribution assumes fj. That is, p8(Xi I Xp(i)) = P(Xi I XJilil,fi). To completely specify a model including the rebuttal variable Ri would mean specifying two additional distributions: the distribution of Xi given its parents and tj, and the distribution of tj. Additional compications may arise: ti might imply that Xi depends directly on other nodes in addition to Xp(i)· and Ri might depend on other nodes or other rebuttals (some condition may invalidate more than one node model). The network might become computationally intractable if these complexities were included explicitly (in fact, assuming them away may have been a computationally motivated approximation). It may be useful to specify a straw model which does not cover all these complexities, but might be expected to fit better than the assessed model when one of the rebuttals is true. Consider the following simplified model. First, a rebuttal is assumed to \"break the link\" between a node and its parents, so that:\n(13)\nThat is, given tj, Xi is conditionally independent of all the nodes above it in N , and has the distribution P8(Xi I ti). Second, the rebuttals for different node models are assumed to be marginally independent of all other nodes in the network (i.e., Ri has no predecessors inN or among the other Rj). If the rebuttals are included explicitly in the inference network, the posterior probability P8(Ri I �) is computed as part of evidence propagation. If the general rebuttal Ri becomes probable, it may flag the system to build a more detailed model of the conditions under which the current node model for Xi is not valid. However, explicit representation of rebuttals doubles the number of nodes in the network.5 It is desirable to find a computationally simple method to determine when a rebuttal Ri has become probable. To determine the probability of Ri given evidence, one computes the posterior odds ratio by multiplying the likelihood ratio by the prior odds ratio:\nPs(ti I x.,) Ps(fi I x.,) Ps(x., I t) Ps(t) ---x--. Ps(x., I fi) Ps(fi)\n(14)\nWhen the likelihood ratio is large, the evidence Ke increases the posterior probability of ti relative to its prior probability. This likelihood ratio can be rewritten as:\n5No loops are added to the networl<, so algorithms to find loop cutsets or clique trees need work no harder.\nConflict and Surprise: Heuristics for Model Revision 203\np'� I�)\nP'� I fi)\nL P'� I xi�i)) P'(xi I ti) p'�(i)) (xi�J)\n• (15)\nL p'� I xi�(i)) !\"'(xi I �(i)) p'�(i)) (xi�J)\nNote that the first and last tenns in the summands are the same in numerator and denominator, and that these tenns are independent of Ri. If Ri is the only rebuttal in the model, ( 15) can be reexpressed as:\nL p\"� I xi�(i)) p'(xi I ti) p•�(i)) (xi�il)\nL p\"� I xi�(i)) !\"'(xi IJ>p(i)) p\"�(i)) (X;�J)\n'\"\"' P'(xi I �)\n£..J p\"� I Xi�i)) p\"(xi,Xp(i)) -. --- (x;�) P (xi I �(i))\np•�)\n'\"\"' P'(xi I ti)\n= £..J p\"(Xj.l>p(i) IJ>e) --- <x;�Jl !\" '(xi I xp(i)) (16)\nWhen there are rebuttals for other nodes as well, (16) is only approximate. It is a close approximation if the posterior probabilities of all rebuttals other than Ri are small.\nThe sum (16) will be large when the evidence x.e makes probable values of (Xi-Xp(i)) for which the straw model probability pS(Xi I ti) is much larger than the assessed probability p\"(xi IJ>p(i)). This might happen if Xi is a very atypical value of Xi given the values l>p(i) for the parent variables, but the evidence makes both Xi and l>p(i) very probable.\nClique tree algorithms automatically keep track of the joint probabilities of the (Xi, l>p(ij): other algorithms can be modified to do so. However, computation of (16) generally amounts to about as much work as explicitly modeling Ri. Computation can be reduced by pre-selecting a subset of (Xi.l>p(i)) for which pS(Xi I ti) is much larger than p•(xi I Xp(i)). and monitoring the posterior probability for onfy the selected subset of values for Xi and its parents.\n5 RARE CASES I have suggested that model reviSion should be considered when low probability evidence is observed. But even if the model is correct, things which it assigns low probability are occasionally expected to happen. Is there any way to distinguish between rare cases for which the model is correct and cases not covered by the model?\nSometimes the conflict can be explained by some rare hypothesis which is covered by the model. That is, there may be some variable Xk for which the value XkH would make the observed evidence x.e highly probable. The probability of the evidence can be written as:\np•�) = p\"(J>e I xk )p•(xk ) H H\n+ p\"(J>e 1 x.k )p•( x.k ) . H H\nThis can be very small even when Pa(l>e I XkH) is high if XkH was initially thought to be a very improbable value for Xk. If such a rare hypothesis could explain the conflict, the value of the conflict indicator cs will typically be decreased by independent evidence for XkH\" That is, one finds an observable variable Xf for which the assessed probability of Xf\nR is high given Xk = xkH but low given Xk * Xkw Observing Xf may resolve the issue. Precisely speaking, the value (x.e, XfH) was assessed to be much more probable relative to other values of (Xe, Xf) than was the value x.e relative to other values of X.,. Unless this is also true of the straw model, the conflict will be reduced upon observing l>fH·\nIn any case, it is generally wise to single out for special attention cases which occur but were initially assessed to be highly improbable. It may be that, being rare, they were not thought deserving of as close modeling attention. Or it may have been that a heuristic model construction algorithm pruned parts of the network that were independent of the variables of interest given the falsity of the improbable hypotheses. These modeling decisions may need to be reexamined if the hypotheses become more probable than originally thought.\n6 DISCUSSION If probabilistic reasoning methods are to be used on problems where flexible network reconfiguration is necessary, indicators must be constructed that tell the system when its current model appears to be inadequate. Even for static models, it is important to alert users to situations which the system's model was not designed to handle. An exact decision theoretic calculation would require actually revising the model to decide whether model revision is necessary. This is clearly infeasible. This paper presented a class of theoretically justified heuristic model revision indicators. The idea is to construct a computationally simple alternate model which, although not a tenable model for the\n204 Laskey\nphenomenon, is likely to fit the evidence better than the current model if the current model is incorrect. Model revision is initiated when the likelihood ratio of the evidence given the alternate model becomes unacceptably large.\nAcknowledgements\nThis work was supported by a grant from the Virginia Center for Innovative Technology to the Center of Excellence in Command, Control, Communications and Intelligence at George Mason University.\nReferences\nCohen, M.S., Laskey, K.B., and Ulvila, J.W., The Management of Uncertainty in Intelligence Data: A Self-Reconciling Evidential Database, Reston, VA: Decision Science Consortium, Inc., 1987 .\nJensen, F.V., Chamberlain, B., Nordahl, T. and Jensen F. (1990). Analysis in HUGIN of Data Conflict. In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, Boston, MA, 546-554. Association for Uncertainty in Artificial Intelligence, Mountain View, CA.\nHabbema, J.D.F. (1976). Models for Diagnosis and Detection of Combinations of Diseases. Decision Making and Medical Care. de Dombal et al. (eds), North Holland.\nKullback, S. (1959). Information Theory and Statistics. NY: Wiley. Laskey, K.B. (1990). A Probabilistic Reasoning Environment. In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, Boston, MA, 415-422 . Association for Uncertainty in Artificial Intelligence, Mountain View, CA.\nLaskey, K.B., Cohen, M.S., and Martin, A.W. (1989). Representing and Eliciting Knowledge about Uncertain Evidence and its Implications. IEEE Transactions on Systems, Man, and Cybernetics, 19, 536-545.\nSavage, L.J. (1954). The Foundations of Statistics. NY: Wiley.\nToulmin, S.E., Rieke, R., and Janik, A. (1984). An Introduction to Reasoning. NY: MacMillan."
    } ],
    "references" : [ {
      "title" : "The Management of Uncertainty in Intelligence Data: A Self-Reconciling Evidential Database, Reston, VA: Decision",
      "author" : [ "M.S. Cohen", "K.B. Laskey", "J.W. Ulvila" ],
      "venue" : "Science Consortium, Inc.,",
      "citeRegEx" : "Cohen et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 1987
    }, {
      "title" : "Analysis in HUGIN of Data Conflict",
      "author" : [ "F.V. Jensen", "B. Chamberlain", "T. Nordahl", "Jensen F." ],
      "venue" : "Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, Boston, MA, 546-554. Association for Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Jensen et al\\.,? 1990",
      "shortCiteRegEx" : "Jensen et al\\.",
      "year" : 1990
    }, {
      "title" : "Models for Diagnosis and Detection of Combinations of Diseases",
      "author" : [ "J.D.F. Habbema" ],
      "venue" : "Decision Making and Medical Care. de Dombal et al. (eds), North Holland.",
      "citeRegEx" : "Habbema,? 1976",
      "shortCiteRegEx" : "Habbema",
      "year" : 1976
    }, {
      "title" : "Information Theory and Statistics",
      "author" : [ "S. Kullback" ],
      "venue" : "NY: Wiley.",
      "citeRegEx" : "Kullback,? 1959",
      "shortCiteRegEx" : "Kullback",
      "year" : 1959
    }, {
      "title" : "Representing and Eliciting Knowledge about Uncertain Evidence and its Implications",
      "author" : [ "K.B. Laskey", "M.S. Cohen", "A.W. Martin" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, 19, 536-545.",
      "citeRegEx" : "Laskey et al\\.,? 1989",
      "shortCiteRegEx" : "Laskey et al\\.",
      "year" : 1989
    }, {
      "title" : "The Foundations of Statistics",
      "author" : [ "L.J. Savage" ],
      "venue" : "NY: Wiley.",
      "citeRegEx" : "Savage,? 1954",
      "shortCiteRegEx" : "Savage",
      "year" : 1954
    }, {
      "title" : "An Introduction to Reasoning",
      "author" : [ "S.E. Toulmin", "R. Rieke", "A. Janik" ],
      "venue" : "NY: MacMillan.",
      "citeRegEx" : "Toulmin et al\\.,? 1984",
      "shortCiteRegEx" : "Toulmin et al\\.",
      "year" : 1984
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Following Savage (1954), I term this restricted universe a small world (actually, a small set of",
      "startOffset" : 10,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "A well-known result from information theory states that ( 4) is never positive &nd is equal to zero only if the distributions P\"O and P'O are identical (Kullback, 1959).",
      "startOffset" : 152,
      "endOffset" : 168
    } ],
    "year" : 2011,
    "abstractText" : "Any probabilistic model of a problem is based on assumptions which, if violated, invalidate the model. Users of probability based decision aids need to be alerted when cases arise that are not covered by the aid's model. Diagnosis of model failure is also necessary to control dynamic model construction and revision. This paper presents a set of decision theoretically motivated heuristics for diagnosing situations in which a model is likely to provide an inadequate representation of the process being modeled.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}