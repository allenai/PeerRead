{
  "name" : "1709.05914.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Limitations of Cross-Lingual Learning from Image Search",
    "authors" : [ "Mareike Hartmann", "Anders Søgaard" ],
    "emails" : [ "hartmann@di.ku.dk", "soegaard@di.ku.dk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Typically, cross-lingual word representations are learned from word alignments, sentence alignments, or, more rarely, from aligned, comparable documents (Levy et al., 2017). Ammar et al. (2016) propose a method that learns multilingual word-embeddings from monolingual corpora and dictionaries containing translation pairs. However, for many languages such resources are not available.\nBergsma and Van Durme (2011) introduced an alternative idea, namely to learn bilingual representations from image data collected via web image search. The idea behind their approach is to represent words in a visual space and find valid translations between words based on similarities between their visual representations. Representa-\ntions of words in the visual space are built by representing a word by a set of images that are associated with that word, i.e., the word is a semantic tag for the images in the set.\nKiela et al. (2015) gain large performance improvements for the same task using a feature representation extracted from convolutional neural networks. However, both works only consider nouns, leaving open the question of whether learning cross-lingual representations for other partsof-speech from images is possible.\nIn order to evaluate whether this work scales to verbs and adjectives, we compile word lists containing these parts-of-speech in several languages based on a word association dataset (SimLex999). We collect image sets for each image word and represent all words in a visual space. Then, we rank translations computing similarities between image sets and evaluate performance on this task.\nAnother field of research that exploits image data for NLP applications is the induction of multi-modal embeddings, i.e. semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016). The work presented in our paper differs from these approaches, in that we do not use image data to improve semantic representations, but use images as a resource to learn cross-lingual representations."
    }, {
      "heading" : "1.1 Contributions",
      "text" : "We introduce a novel dataset for learning crosslingual word embeddings from images, and we evaluate the approaches by Bergsma and Van Durme (2011) and Kiela et al. (2015) on this data set, which apart from nouns includes both adjectives and verbs. In addition to considering representations of image words through sets of images, we also experiment with representations that\nar X\niv :1\n70 9.\n05 91\n4v 1\n[ cs\n.C L\n] 1\n8 Se\np 20\n17\nintegrate cross-lingual word-embeddings learned from text. Our results, nevertheless, suggest that none of the approaches involving image data are directly applicable to learning cross-lingual representations for adjectives and verbs. In sum, our paper is a negative result paper, showing that what seems like a promising approach to learning bilingual dictionaries for low-resource languages, does not scale beyond simple nouns."
    }, {
      "heading" : "2 Data",
      "text" : "Simlex-999 We use the Simlex-999 data set of English word pairs (Hill et al., 2014) to compile the word lists for our experiments. The dataset contains words of three part-of-speech classes (nouns, adjectives and verbs) comprising different levels of concreteness. We collect all distinct words in the dataset and randomly sample half of the words for each part-of-speech. The final word list comprises 557 English words (375 nouns, 116 verbs and 66 adjectives). We translate the word list into 5 languages (German, Danish, French, Russian, Italian) using the Yandex translation API1 and manually refine translations.2\nImage Data Sets Using the Bing Image Search API3, we represent each word in a word list by a set of images by collecting the first 50 jpeg images returned by the search engine when querying the word. This way, we compile image data sets for 6 languages. Identical images contained in image sets for two different languages are removed.4 Figure 1 shows examples for images associated with a word in two languages."
    }, {
      "heading" : "3 Approach",
      "text" : "The assumption underlying the approach is that semantically similar words in two languages are associated with similar images. Hence, in order to find the translation of a word, e.g. from English to German, we compare the images representing the English word with all the images representing\n1https://translate.yandex.com/ 2The Italian word list is compiled using the translated\nSimlex-999 data set (Leviant and Reichart, 2015). 3https://www.microsoft.com/cognitive-services/enus/bing-image-search-api 4We completely remove translation pairs for which more than 10 images are identical across languages. Even though identical images returned by the search engine are a strong hint that two words are translations of each other, we want to eliminate the effect of possible cross-lingual indexing internally applied by the search engine.\nGerman words, and pick as translation the German word represented by the most similar images. To compute similarities between images, we compute cosine similarities between their feature representations.\nFollowing previous work, we obtain two different feature representations for each image, color and SIFT features as proposed by Bergsma and Van Durme (2011) and convolutional network features as proposed by Kiela et al. (2015)."
    }, {
      "heading" : "3.1 SIFT and Color Features",
      "text" : "SIFT features (Lowe, 2004) are descriptors for points of interest in an image. By matching them with descriptor representations in a codebook, images can be represented by a bag-of-codewords from the codebook (Sivic and Zisserman, 2003). We apply the optimal hyperparameters reported by Bergsma and Van Durme and generate a codebook by clustering 20,000 codewords from 430,000 randomly sampled descriptors in the English image data. Color features are implemented as color histograms for the red, blue and green channels as well as a gray-scale histogram."
    }, {
      "heading" : "3.2 Convolutional Neural Network Feature Representations",
      "text" : "Following Kiela et al. (2015), we compute convolutional neural network (CNN) feature representations. We refer to this representation as VIS. For each image, we extract its pre-softmax layer representation in an AlexNet (Krizhevsky et al., 2012) pre-trained on the ImageNet classification task."
    }, {
      "heading" : "3.3 Representations from Cross-Lingual Word-Embeddings",
      "text" : "In order to investigate if it is possible to learn cross-lingual representations for verbs and adjectives from image data, we also want to know how well text-based methods perform in this task. Hence, we extend our experiments to crosslingual word-embeddings trained on text data. We use the pre-trained multilingual word-embeddings provided by Ammar et al. (2016).5 These wordembeddings are generated by first training wordembeddings in a monolingual space. Then, these representations are mapped into a joint multilingual space maximizing the similarity between representations that are translations according to a bilingual dictionary.\n5http://128.2.220.95/multilingual/data/\nNote that since this cross-lingual representation learning algorithm requires a dictionary in advance, it does not scale to the low-resource language scenario that we consider here. We nevertheless include these embeddings, because they strengthen our negative result. The imagebased representations do not scale to adjectives and verbs, not even as a supplement to text-based cross-lingual representations.\nFor our experiments, we generate a text-based representation for each image by extracting the multi-lingual word-embedding for the image word associated with the image. Note that this results in all images in an image set having the same textbased representation. We refer to this representation as TEX."
    }, {
      "heading" : "3.4 Combined Representations",
      "text" : "Besides purely image-based and purely textbased representations, we also consider combined representations that integrate the visual features with the information from text-based wordembeddings. In order to obtain an image’s combined representation, we concatenate its CNN feature vector with the text-based representation of its associated image word. This representation is referred to as COMBI.\nAs the CNN features have dimensionality 4096 and the word-embeddings have dimensionality 40, we also compute a version of the CNN features with reduced dimensionality (40 dimensions) using principal component analysis (referred to as VISPCA). We then generate a combined representation by concatenating the 40- dimensional CNN feature-vector with the 40- dimensional word-embedding, resulting in an 80- dimensional combined representation. This representation is referred to as COMBIPCA."
    }, {
      "heading" : "3.5 Similarity Computation",
      "text" : "We implement several methods to determine similarities between representations.\nSimilarities Between Individual Images Bergsma and Van Durme (2011) determine similarities between image sets based on similarities between all individual images. For each image in image set 1, the maximum similarity score for any image in image set 2 is computed. These maximum similarity scores are then either averaged (AVGMAX) or their maximum is taken (MAXMAX).\nSimilarities Between Aggregated Representations In addition to the above described methods, Kiela et al. (2015) generate an aggregated representation for each image set and then compute the similarity between image sets by computing the similarity between the aggregated representations. Aggregated representations for image sets are generated by either taking the componentwise average (CNN-MEAN) or the componentwise maximum (CNN-MAX) of all images in the set.\nK-Nearest Neighbor For each image in an image set in language 1, we compute its nearest neighbor across all image sets in language 2. Then, we find the image set in language 2 that contains the highest number of nearest neighbors. The image word is translated into the image word that is associated with that image 2 set. Ties between image sets containing an equivalent number of nearest neighbors are broken by computing the average distance between all members. We refer to the method as KNN.\nWhereas the other approaches described above provide a ranking of translations, this method determines only the one translation that is associated with the most similar image set.\nClustering Image Sets As we expect the retrieved image sets for a word to contain images associated with different senses of the word, we first cluster images into k clusters. This way, we hope to group images representing different word\nsenses. Then, we apply the KNN method as described above. We refer to this method as KNNC.\nLogistic Regression The above described methods are unsupervised and do not require parameter estimation. In order to check if performance can be increased using supervision, we train a logistic regression ranking model (LOGREGR) to predict rankings of translations. Training data is generated by pairing all image sets across two languages and computing difference vectors between the aggregated mean representations of each image set. Pairs of image sets that correspond to correct translation pairs are labeled as positive while all other pairs are labeled as negative. In the prediction step, translations are ranked based on the predicted distance to the hyperplane (Joachims, 2002)."
    }, {
      "heading" : "3.6 Evaluation Metrics",
      "text" : "Ranking performance is evaluated by computing the Mean Reciprocal Rank (MRR) as\nMRR = 1\nM M∑ i=1\n1\nrank(ws, wt)\nM is the number of words to be translated and rank(ws, wt) is the position the correct translation wt for source word ws is ranked on.\nIn addition to MRR, we also evaluate the crosslingual representations by means of precision at k (P@k). For the nearest neighbor approach, we can only compute P@1."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "We run experiments for 5 language pairs English– German, English–Danish, English–French, English–Russian and English–Italian. In the first experiment, we consider only the representations computed from image data (SIFT + color and CNN features) and compare the different methods for similarity computation described in Section 3.5. For each English word, we rank all the words in the corresponding target languages based on similarities between image sets and evaluate the models’ ability to identify correct translations, i.e. to rank the correct translation on a position near the top. We compare 4 settings that differ in the set of English words that are translated. In the setting ALL, all English words in the word list are translated. NN, VB and ADJ refer to the\nsettings where only nouns, verbs and adjectives are translated.\nIn a second experiment, we apply the best performing similarity computation method on the visual representations (AVGMAX) and the LOGREGR model to the visual, textual and combined representations."
    }, {
      "heading" : "4.1 Results",
      "text" : "We present results for the comparison between different similarity computation methods for purely visual representations in Tables 1 and 2. Figures 2 and 3 show the results for the performance of AVGMAX and LOGREGR on different representations.\nComparison of Similarity Computation Methods on Visual Representations Table 1 displays results for images represented by CNN features averaged over all language pairs. Table 2 shows results for images represented by SIFT and color features averaged over all language pairs.\nFirst, we observe that our results for translating nouns using CNN features are slightly lower than those reported in previous work (Kiela et al. (2015) report P@1 = 0.567 for a dataset containing 500 nouns). This may be due to a higher proportion of more abstract nouns in our dataset. Second, our experiments confirm that the CNN features outperform the color and SIFT features by a large margin, which is in line with the findings by Kiela et al.6\nMost importantly, we witness a very significant drop in performance when moving from nouns to verbs and adjectives. For verbs, we almost never pick the right translation based on the image-based word representations. This behavior applies across all methods for similarity computation. For the CNN features, AVGMAX outperforms the other methods in almost all cases.\nComparison of Representations Figures 2 and 3 compare the performance of the AVGMAX and LOGREGR methods applied to the combined representations (COMBI) and the representations after applying PCA (VISPCA and COMBIPCA), as well as the purely visual and textual representations (VIS and TEX).\n6Our results using SIFT and color features are very low, even when considering only nouns. This might be due to the fact that we did not tune any parameters but adopted the optimal parameters reported in previous work, while working with a dataset that contains more variance in the images.\nMost importantly, we see that the models involving images (VIS, COMBI and the corresponding PCA versions) are always outperformed by the model using representations learned from text data (except for English-Russian noun pairs). This observation applies to both the AVGMAX method and the LOGREGR model. As in Tables 1 and 2, we can see a significant drop in performance for the representations computed on visual data comparing nouns with adjectives and verbs. In contrast, the quality of the rankings based on text-based representations stays approximately the same for different parts-of-speech."
    }, {
      "heading" : "4.2 Analysis",
      "text" : "If we try to learn translations from images, integrating verbs and adjectives into the dataset worsens results compared to a dataset that contains only nouns. One possible explanation is that images associated with verbs and adjectives are less suited to represent the meaning of a concept than images associated with nouns.\nKiela et al. (2015) suppose that lexicon induction via image similarity performs worse for datasets containing words that are more abstract.\nIn order to approximate the degree of abstractness of a concept, they compute the image dispersion d for a word w as the average cosine distance between all image pairs in the image set {ij , . . . , in} associated with word w according to\nd(w) = 2 n(n− 1) ∑\nk<j≤n 1− ij · ik |ij ||ik|\nIn their analysis, Kiela et al. (2015) find that their model performs worse on datasets with a higher average image dispersion.\nComputing the average image dispersion for our data across languages shows that image sets associated with verbs and adjectives have a higher average image dispersion than image sets associated with nouns (nouns: d = 0.75, verbs: d = 0.83, adjectives: d = 0.82).\nTo gain insights into the relation between image dispersion, part-of-speech and the models ability to find correct translation pairs, we manually analyze the image sets with highest and lowest dispersion values in our dataset. Table 3 shows the image words associated with the image sets that have the highest and lowest dispersion values in our English image data.\nWe present examples of the image sets with the highest and the lowest dispersion values for each part-of-speech in Figures 4, 6 and 5. Along with the image sets for English words, we also present examples from the images sets representing the German translations of the English words. Among the first 10 images returned by the search engine, we manually choose the 4 images that seem to be most interesting.7\nNouns For the nouns (Figure 4), bicycle is represented by the image set with the lowest dispersion. The images in both the English and the German image sets are highly similar, mostly showing iconic versions of bicycles that might be advertised for sale. This observation seems to apply for the other languages, too, as the correct translation for bicycle is ranked on first position for all language pairs.\n7The 4 presented images are only a small part of the whole images set and cannot capture the whole variance in the images contained in a set. However, looking at these examples allows us to illustrate some weaknesses of our approach.\nConsidering the noun with highest dispersion, second, we find that the images in the associated image set display at least two different senses of the word: its adjectival sense as in second position and its sense as a fraction of a minute. The German translation Sekunde mainly refers to the time unit. This is reflected in the associated images showing a clock and sports settings (where Sekunde is associated with playing time).\nAdjectives The example images for the adjectives (Figure 5) are the most interesting, as they reveal some profound weaknesses of our approach. The adjective with lowest dispersion is bold, which we would expect to be represented by images associated with its most frequent sense (according to WordNet8: fearless, daring). However, the images in the bold image set display a mobile phone, as one of the phones by the BlackBerry brand is named BlackBerry Bold. In fact, 95% of the images in the image set display a mobile\n8http://wordnet.princeton.edu/\nphone. Here, the images provided by the search engine are clearly not associated with the adjectival meaning of bold. The adjective bold was translated by the translation system we use for dataset creation (see Section 2) into the German adjective fett, which is the translation of a very specific sense of bold (as in bold face type). The most frequent sense of the German fett is fat as in fat person. Hence, we would expect to find some images of fat people or animals in the German image set. However, we find that 90% of the images in the fett image set display the Star Wars character Boba Fett as shown in Figure 5. Similar as it is the case for the English word bold, the image search engine does not provide images associated with the adjectival sense of the queried word. For this word pair, even though the image dispersion is low, the model will fail to learn a correct translation based on the image data, as there is no semantic connection between the concepts in both image sets.\nFor the adjective with highest dispersion, necessary, we see various images, but at least some\nof them have an association with the meaning of the adjective. The same applies to the images for the German translation notwendig. Based on these observations, we conclude that even if an image word might be displayed by images with a low dispersion, this does not imply that it is easy to learn a translation for it.\nVerbs For the verbs (Figure 6), send has the lowest dispersion (which is considerably higher than the dispersion for the noun with lowest dispersion, see Table 3). 44% of the images in the send image set show a smartphone chat like the one in images 1 and 4 in Figure 6.\nFor the verb with highest dispersion, create, we find a lot of images that have no obvious relation to the verb, like the third and fourth image on the right side of Figure 6. The German translation, erstellen, is represented by many images that show the process of creating a document with a computer program.\nClearly, we observe that most of the images re-\nturned by the search engine are displaying a concept that is associated with the verb, rather than the activity the verb refers to itself."
    }, {
      "heading" : "4.3 Discussion",
      "text" : "The analysis described in the previous section gives some insights on why learning translations for adjectives and verbs from image search results works poorly. First, we observe that as soon as we move away from concrete nouns like bicycle, the images returned by the search engine are diverse and the collected image sets have a high dispersion. The images often display concepts that are associated with the queried words rather than the concepts behind the words themselves.\nWhile this problem seems to be inherent to the process of learning representations of verbs and adjectives from image data in general, our anal-\nysis also reveals issues that are related to our specific approach, namely to rely on an image search engine to provide us with resources to learn from. In many cases, the search engine does not capture the intended sense of the query word, resulting in search results that are completely unrelated to the images of the query’s translation. It is not clear how to query for verbs and adjectives and it is also not clear what mechanisms the search engine internally uses to group results. Even if we manage to enforce querying for a specific part-of-speech, the search engine might return images associated with an uncommon sense of that word. We depend on the search engine’s interpretation of our query, which is especially problematic if the query not a concrete noun. We conclude that performance in the lexicon induction task might increase if we eliminate the dependence on the search engine by moving to a different resource annotated with semantic tags or natural language captions, such as Flickr or Wikipedia."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We showed that existing work on learning crosslingual word representations from images obtained via web image search does not scale to other parts-of-speech than nouns. Even with supervised learning from image features, we are not able to obtain positive results for verbs and adjectives. In our analysis, we identify the dependence on the image search engine as one of the main problems of the approach, when applied to adjectives and verbs. Hence, we plan to switch to a different resource to collect image data in future work."
    } ],
    "references" : [ {
      "title" : "Massively multilingual word embeddings",
      "author" : [ "Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "arXiv preprint arXiv:1602.01925 .",
      "citeRegEx" : "Ammar et al\\.,? 2016",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning Bilingual Lexicons using the Visual Similarity of Labeled Web Images",
      "author" : [ "Shane Bergsma", "Benjamin Van Durme." ],
      "venue" : "IJCAI. Barcelona, Spain, pages 1764–1769.",
      "citeRegEx" : "Bergsma and Durme.,? 2011",
      "shortCiteRegEx" : "Bergsma and Durme.",
      "year" : 2011
    }, {
      "title" : "Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what I mean",
      "author" : [ "Felix Hill", "Anna Korhonen." ],
      "venue" : "EMNLP. Doha, Qatar, pages 255–265.",
      "citeRegEx" : "Hill and Korhonen.,? 2014",
      "shortCiteRegEx" : "Hill and Korhonen.",
      "year" : 2014
    }, {
      "title" : "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
      "author" : [ "Felix Hill", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "arXiv preprint arXiv:1408.3456 .",
      "citeRegEx" : "Hill et al\\.,? 2014",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "SIGKDD. ACM, New York, NY, USA, pages 133–142.",
      "citeRegEx" : "Joachims.,? 2002",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2002
    }, {
      "title" : "Learning image embeddings using convolutional neural networks for improved multi-modal semantics",
      "author" : [ "Douwe Kiela", "Léon Bottou." ],
      "venue" : "EMNLP. Doha, Qatar, pages 36–45.",
      "citeRegEx" : "Kiela and Bottou.,? 2014",
      "shortCiteRegEx" : "Kiela and Bottou.",
      "year" : 2014
    }, {
      "title" : "Improving multi-modal representations using image dispersion: Why less is sometimes more",
      "author" : [ "Douwe Kiela", "Felix Hill", "Anna Korhonen", "Stephen Clark." ],
      "venue" : "ACL. Association for Computational Linguistics, Baltimore, Maryland, pages 835–841.",
      "citeRegEx" : "Kiela et al\\.,? 2014",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2014
    }, {
      "title" : "Visual bilingual lexicon induction with transferred convnet features",
      "author" : [ "Douwe Kiela", "Ivan Vulic", "Stephen Clark." ],
      "venue" : "EMNLP. Lisbon, Portugal, pages 148–158.",
      "citeRegEx" : "Kiela et al\\.,? 2015",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton." ],
      "venue" : "NIPS. Lake Tahoe, Nevada, United States, pages 1106–1114.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Combining language and vision with a multimodal skip-gram model",
      "author" : [ "Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni." ],
      "venue" : "NAACL. Denver, Colorado, USA, pages 153–163.",
      "citeRegEx" : "Lazaridou et al\\.,? 2015",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2015
    }, {
      "title" : "Judgment language matters: Multilingual vector space models for judgment language aware lexical semantics",
      "author" : [ "Ira Leviant", "Roi Reichart." ],
      "venue" : "arXiv preprint arXiv:1508.00106 .",
      "citeRegEx" : "Leviant and Reichart.,? 2015",
      "shortCiteRegEx" : "Leviant and Reichart.",
      "year" : 2015
    }, {
      "title" : "A strong baseline for learning cross-lingual word embeddings from sentence alignments",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Anders Søgaard." ],
      "venue" : "EACL.",
      "citeRegEx" : "Levy et al\\.,? 2017",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2017
    }, {
      "title" : "Distinctive Image Features from Scale-Invariant Keypoints",
      "author" : [ "David G. Lowe." ],
      "venue" : "International Journal of Computer Vision 60(2):91–110.",
      "citeRegEx" : "Lowe.,? 2004",
      "shortCiteRegEx" : "Lowe.",
      "year" : 2004
    }, {
      "title" : "Visually grounded meaning representations",
      "author" : [ "Carina Silberer", "Vittorio Ferrari", "Mirella Lapata." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence To appear.",
      "citeRegEx" : "Silberer et al\\.,? 2016",
      "shortCiteRegEx" : "Silberer et al\\.",
      "year" : 2016
    }, {
      "title" : "Video google: A text retrieval approach to object matching in videos",
      "author" : [ "Josef Sivic", "Andrew Zisserman." ],
      "venue" : "null. IEEE, page 1470.",
      "citeRegEx" : "Sivic and Zisserman.,? 2003",
      "shortCiteRegEx" : "Sivic and Zisserman.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Typically, cross-lingual word representations are learned from word alignments, sentence alignments, or, more rarely, from aligned, comparable documents (Levy et al., 2017).",
      "startOffset" : 153,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "Ammar et al. (2016) propose a method that learns multilingual word-embeddings from monolingual corpora and dictionaries containing translation pairs.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).",
      "startOffset" : 88,
      "endOffset" : 204
    }, {
      "referenceID" : 2,
      "context" : "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).",
      "startOffset" : 88,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).",
      "startOffset" : 88,
      "endOffset" : 204
    }, {
      "referenceID" : 9,
      "context" : "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).",
      "startOffset" : 88,
      "endOffset" : 204
    }, {
      "referenceID" : 13,
      "context" : "semantic representations that are learned by integrating textual and visual information (Kiela et al., 2014; Hill and Korhonen, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015; Silberer et al., 2016).",
      "startOffset" : 88,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : "We introduce a novel dataset for learning crosslingual word embeddings from images, and we evaluate the approaches by Bergsma and Van Durme (2011) and Kiela et al. (2015) on this data set, which apart from nouns includes both adjectives and verbs.",
      "startOffset" : 151,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "Simlex-999 We use the Simlex-999 data set of English word pairs (Hill et al., 2014) to compile the word lists for our experiments.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "com/ The Italian word list is compiled using the translated Simlex-999 data set (Leviant and Reichart, 2015).",
      "startOffset" : 80,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "Following previous work, we obtain two different feature representations for each image, color and SIFT features as proposed by Bergsma and Van Durme (2011) and convolutional network features as proposed by Kiela et al. (2015).",
      "startOffset" : 207,
      "endOffset" : 227
    }, {
      "referenceID" : 12,
      "context" : "SIFT features (Lowe, 2004) are descriptors for points of interest in an image.",
      "startOffset" : 14,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "with descriptor representations in a codebook, images can be represented by a bag-of-codewords from the codebook (Sivic and Zisserman, 2003).",
      "startOffset" : 113,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "For each image, we extract its pre-softmax layer representation in an AlexNet (Krizhevsky et al., 2012) pre-trained on the ImageNet classification task.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "Following Kiela et al. (2015), we compute convolutional neural network (CNN) feature representations.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "We use the pre-trained multilingual word-embeddings provided by Ammar et al. (2016).5 These wordembeddings are generated by first training wordembeddings in a monolingual space.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "Similarities Between Aggregated Representations In addition to the above described methods, Kiela et al. (2015) generate an aggregated representation for each image set and then compute the similarity between image sets by comput-",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "First, we observe that our results for translating nouns using CNN features are slightly lower than those reported in previous work (Kiela et al. (2015) report P@1 = 0.",
      "startOffset" : 133,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "In their analysis, Kiela et al. (2015) find that their model performs worse on datasets with a higher average image dispersion.",
      "startOffset" : 19,
      "endOffset" : 39
    } ],
    "year" : 2017,
    "abstractText" : "Cross-lingual representation learning is an important step in making NLP scale to all the world’s languages. Recent work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused on the translation of selected nouns only. In our work, we investigate whether the meaning of other parts-of-speech, in particular adjectives and verbs, can be learned in the same way. We also experiment with combining the representations learned from visual data with embeddings learned from textual data. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.",
    "creator" : "LaTeX with hyperref package"
  }
}