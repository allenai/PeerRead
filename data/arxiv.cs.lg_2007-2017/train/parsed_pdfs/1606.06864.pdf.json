{
  "name" : "1606.06864.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n06 86\n4v 2\n[ cs\n.C L\n] 1\n6 Se\np 20\nIndex Terms— automatic speech recognition, recurrent neural networks, noise robustness, curriculum learning"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "The performance of automatic speech recognition (ASR) systems has increased significantly with the use of deep neural networks (DNNs) [1]. However, their performance in noisy environments still leaves room for improvement. Over the past decades, a multitude of methods to improve noise robustness of ASR systems has been proposed [2], with many methods being applicable to DNNs. These methods enhance noise robustness at various levels and are applied prior to feature extraction, at the feature level, and during training.\nSome example enhancement methods applied prior to feature extraction include denoising methods [3] and source separation methods [4] [5]. Methods applied at the feature level include methods that produce auditory-level features [6] and feature space adaptation methods [7]. Other approaches use DNNs, e.g. feature denoising with deep autoencoders [8] [9] or feature extraction from the raw waveform via convolutional neural networks (CNNs) [10] [11]. Many of these strategies add components to the speech recognition system that need careful optimization.\nThe training method itself can have a major influence on the performance of a neural network under noisy conditions. Training on noisy data is an established method of increasing the noise robustness of a network. Noisy training sets with a range of SNR values e.g. 10 dB - 20 dB [12] or 0 dB - 30 dB [10] are used during training.\nOther training methods such as dropout [13] - originally intended to improve regularisation - have been shown to also improve noise robustness [12]. The same is true for model adaptation/noise aware training techniques [12].\nThis paper presents general training methods for improving noise robustness in recurrent neural network (RNN)-based recognizers. RNNs are used here because they have demonstrated state-ofthe-art performance in tasks such as the common sequence labelling task in speech recognition [14] [15].\nIn particular, we introduce a new training strategy called accordion annealing (ACCAN) which exploits the benefits of curriculum-based training methods. By first training the network on low SNR levels down to 0 dB and gradually increasing the SNR range to encompass higher SNR levels, the trained network shows better noise robustness when tested under a wide range of SNR levels.\nThis work also investigates the usefulness of adding noise both at the acoustic waveform level and at the feature representation level during training. In particular, we exploit a method called per-epoch noise mixing (PEM), which is a waveform-level data augmentation method. It enables us to generate a new training set for every epoch of training, i.e. each training sample is mixed with a newly sampled noise segment at a randomly chosen SNR in every epoch. This form of data augmentation prevents networks from relying on constant noise segments for classification and helps in creating the necessary training samples over a wide SNR range. These steps lead to improved generalization and noise robustness of the trained network. Our results are evaluated on the Wall Street Journal corpus in a large-vocabulary continuous speech recognition (LVCSR) task. The testing is carried out on a large SNR range going from clean conditions (> 50 dB) down to -20 dB.\nThe paper is organized as follows: Section 2 presents our training methods for improved noise robustness. The evaluation setup is detailed in Section 3, with results given in Section 4 followed by discussions in Section 5 and concluding remarks in Section 6."
    }, {
      "heading" : "2. TRAINING METHODS FOR IMPROVED NOISE ROBUSTNESS",
      "text" : ""
    }, {
      "heading" : "2.1. Baseline",
      "text" : "Our baseline method takes advantage of multi-condition training [16] in order to increase the noise robustness of the network. Pink noise is added to a clean dataset to create samples with the desired SNR. Each training sample is randomly chosen to be of an SNR level in the range 0 to 50 dB with 5 dB steps. This wide range is larger than the SNR ranges used in previous work (e.g. 0 to 30 dB as in [10]). Our exhaustive simulations show that using such a large\nrange resulted in the best performance on the test datasets. The noise mixing is done once at the waveform-level before filterbank audio features are computed. This one set of training data is presented to the network over all training epochs. The resulting networks will be referred to as “noisy-baseline”. For completeness, we also include a “clean-baseline”, i.e. a network that is only trained on clean speech."
    }, {
      "heading" : "2.2. Gaussian noise injection",
      "text" : "Gaussian noise injection is a well-known method for improving generalisation in neural networks [17]. It is used here to improve the noise robustness of the network.\nDuring training, artificial Gaussian noise is added to the filterbank features created from the different SNR samples. The amount of additive noise is drawn from a zero-centered Gaussian distribution. Using a Gaussian with a standard deviation of σ = 0.6 yielded the best results. This method is referred to as the “Gauss-method” in the rest of the paper."
    }, {
      "heading" : "2.3. Per-epoch noise mixing (PEM)",
      "text" : "PEM is a method for adding noise to the waveform level during training. In every training epoch, each training sample is mixed with a randomly sampled noise segment at a randomly sampled SNR. The training procedure consists of the following steps:\n1. Mix every training sample with a randomly selected noise segment from a large pink noise pool to create a resulting sample at a randomly chosen SNR level between 0 to 50 dB.\n2. Extract audio features (e.g. filterbank features) for the noisecorrupted audio to obtain the training data for the current epoch.\n3. Optional: add Gaussian noise to the audio features.\n4. Train on newly generated training data for one epoch.\n5. Discard this training data after the epoch to free up storage.\n6. Repeat from step 1 until training terminates.\nThis method has several key advantages over conventional pretraining preprocessing methods. Firstly, it enables unlimited data augmentation on large speech datasets. With conventional methods, augmenting training data at the waveform level with real-world noise at various SNR values is prohibitively expensive in terms of processing time and training data size. PEM allows use to overcome these restrictions by training on the GPU and pre-processing the next-epoch training data in parallel on the CPU. After an epoch was trained on, the training data gets discarded to free storage for the next epoch.\nSecondly, PEM shows the network more unique training data: every training sample is presented at a selection of SNRs and with as many noise samples as can be extracted from the noise file and as needed by the number of epochs to reach a steady-state accuracy level. Thirdly, other noise types, different SNR training ranges, and even different audio features could be quickly tested as the training data can be easily augmented online. Finally, PEM enables us to dynamically change the SNR level during training, which renders advanced training paradigms such as curriculum learning (section 2.4) feasible.\nIn contrast to the Gauss-method, PEM permits more control over the training data. Real-world noise is added to the acoustic waveform at controlled SNRs, ensuring that the training data corresponds to realistic noise corruption with results that can be evaluated. Of\ncourse, PEM can be combined with Gaussian noise addition (optional step three in Section 2.3). We refer to PEM without Gaussian noise injection as “Vanilla-PEM” and to PEM with Gaussian noise injection as “Gauss-PEM”."
    }, {
      "heading" : "2.4. Curriculum learning",
      "text" : "Neural networks have been shown to optimize on the SNR they are trained on [16]. A network trained on clean conditions thus fares worse than a network trained on noisy conditions. Also, networks trained on a vast SNR range generally do worse on a single SNR than networks optimized for this specific SNR. In order to achieve high accuracy under both high and low SNR with a single network, we explored novel training paradigms based on curriculum learning. While curriculum learning has been used in image classification (scheduled denoising autoencoders, [18]) as well as speech recognition (SortaGrad [15], a method for faster accuracy convergence), this is the first work targeted at LVCSR under noisy conditions.\nOur novel ACCAN training method applies a multi-stage training schedule: in the first stage, the neural network is trained on the lowest SNR samples. In the following stages, the SNR training range is expanded in 5 dB steps towards higher SNR values. A typical schedule is shown in Table 1. In every stage, training repeats until the WER on the development set no longer improves. At the end of each stage, the weights of the best network are stored and used as the starting point for the next stage. Both training and validation sets share the same SNR range. The ACCAN approach seems counter-intuitive as noisy training data should be harder to train on than clean data. However, the noise allows the network to explore the parameter space more extensively at the beginning [19]. We also evaluated a method called “ACCAN-reversed”, which expands from high SNR to low SNR, but the results were very close to the standard “Gauss-PEM” approach."
    }, {
      "heading" : "3. SETUP",
      "text" : "Audio database All experiments were carried out on the Wall Street Journal (WSJ) corpus (LDC93S6B and LDC94S13B) in the following configuration:\n• training set: train-si84 (7138 samples),\n• development set: test-dev93 (503 samples),\n• test set: test-eval92 (333 samples).\nFor noise corruption, we used two different noise types: pink noise generated by the Audacity [20] software and babble noise from the NOISEX database [21].\nData preparation and language model The labels and transcriptions were extracted with EESEN [14] routines. All experiments were character-based and used 58 labels (letters, digits, punctuation marks etc.) During test time, the network output was decoded with the Weighted Finite State Transducer (WFST) approach from the EESEN framework, which allows us to apply a trigram language model. The language model used an expanded vocabulary in order\nto avoid out-of-vocabulary words occurring in the standard WSJ language model.\nAudio features We used 123-dimensional filterbank features that consisted of 40 filterbanks, 1 energy term and their respective first and second order derivatives. The features were generated by preprocessing routines from EESEN [14]. Each feature dimension is zero-mean and unit-variance normalized.\nNeural network configuration Our recognition pipeline is a endto-end solution that involves a RNN as the acoustic model. In order to automatically learn the alignments between speech frames and label sequences, the Connectionist Temporal Classification (CTC) [22] objective was adopted. The Lasagne library [23] enabled us to build and train our 5-layer neural network. The first 4 layers consisted of bidirectional long short-term memory (LSTM) [24] units with 250 units in each direction. The fifth and final layer was a non-flattening dense layer with 59 outputs, corresponding to the character labels + the blank label required by CTC. The network contained 8.5M tunable parameters. All layers were initialized with the Glorot uniform strategy [25]. Every experiment started with the exact same weight initialization. During training, the Adam [26] stochastic optimization method was used. To prevent overfitting and to increase noise robustness, dropout [13] was used (dropout probability=0.3). Every epoch of training, the WER on the development set was monitored with a simple best-path decoding approach.\nWith all training strategies except ACCAN, the network was trained for a generous 150 epochs. The networks weights from the epoch with the lowest WER were kept for evaluation. Generally, the improvements in WER saturated well before 150 epochs were reached. The ACCAN method used a patience of 5 as to switch between SNR stages, i.e. if the WER did not improve for 5 epochs on the current SNR stage, the training continued on the next SNR stage. By respecting the stage-switching policy, ACCAN reached the final SNR stage with the full SNR range at epoch 190. Saturation kicked in at epoch 240. While ACCAN trained for more epochs than the others, it only trained for 50 epochs on the full SNR range."
    }, {
      "heading" : "4. RESULTS",
      "text" : "The reported results are given for the ’test-eval92’ evaluation set for the Wall Street Journal corpus. The evaluation set was tested in clean condition and with added pink noise or babble noise at 15 SNR levels from 50dB to -20 dB in dB steps. We report in Table 2 the average WER over following SNR ranges:\n• Full SNR range: [clean signal, 50dB to -10dB]\n• High SNR range: [50dB to 0dB]\n• Low SNR range: [0dB to -10dB]\n• range of interest (ROI): [20dB to -10dB]\nWe choose to include the ROI, as our hearing tests showed that this range seems to well reflect common scenarios in public environments, where a clean speech signal is most often not found. Detailed results for each SNR individually are given in Table 3. Results for -15dB and -20dB are reported too, but should be considered as extreme cases. WER improvements are given as relative improvements in the text."
    }, {
      "heading" : "4.1. Noise addition methods",
      "text" : "This section summarizes results from the baseline, Gauss, VanillaPEM and Gauss-PEM methods, all trained on the SNR range from 0dB to 50dB. Our network trained on clean speech only (cleanbaseline) achieves 13.8% WER with a trigram language model and our 8.5M parameter network, while in literature [27], a 13.5% WER was achieved using a trigram language model and 3x larger (26.5M) parameter network. This confirms that our end-to-end speech recognition pipeline is fully functional.\nBaseline: The noisy-baseline network starts with a 25% higher WER on the clean test set than our clean-baseline network. For SNRs lower than 25dB, the noisy-baseline is significantly more noise robust. The WER seems to drastically increase at 25dB for the clean-baseline, while the noisy-baseline sees the increase at a lower 10dB SNR. However, all other methods outperform the noisybaseline by a significant margin at high and low SNRs.\nVanilla-PEM vs Gauss: Compared to the noisy-baseline, VanillaPEM achieves a 23% decrease in WER on high SNR, while Gauss only reduces WER by 15% (both pink noise and babble noise). This results in vanilla-PEM being able to outperform the clean-baseline on clean speech, while Gauss is not able to do the same. On low SNR, both methods reduce WER by around 20% on the pink noise test set. On babble noise, PEM results in a higher 22.5% WER decrease compared to the 15.5% decrease provided by Gauss.\nGauss-PEM: The Gauss-PEM method achieves the overall lowest WER on the high and low SNR range. It beats the noisy-baseline method by between 26.5% and 28.7% on high SNR, on low SNR and on the ROI for both pink noise and babble noise. The results on the high SNR range are notable: Gauss-PEM is able to outperform the clean-baseline network at every single SNR step in the high SNR range, even on clean speech. The network is much more noise robust while at the same time it even improves clean speech scores. Around 35dB to 25dB, Gauss-PEM (other methods, too) reaches its minimum WER. This is expected, as the mean SNR of the training SNR range is 25dB and the network seems to optimize for SNR levels close to this value [16]."
    }, {
      "heading" : "4.2. Curriculum learning",
      "text" : "To further increase the noise robustness, we developed a curriculum learning strategy for the Gauss-PEM method, resulting in our novel ACCAN method. We compare our results to Gauss-PEM, as this was the most noise robust non-curriculum method. Our test results show increased noise robustness for ACCAN on pink noise and babble noise: the WER decreases between 3.3% (ROI, pink noise) and 4.1% (ROI babble noise). For pink noise, the biggest decrease is seen at 0dB (10.5% WER decrease) and -5dB (11.3% decrease). For babble noise, the biggest WER decreases are found at 10dB (4.9%), 5dB (10.9%) and 0dB (10.7%).\nThe average WER of ACCAN on the high SNR range is worse on pink noise (relative 8.8% increase in WER), but better on babble noise (relative 0.4% decrease in WER). Ultimately, the absolute WER in clean speech of ACCAN (15.9%) is better than the noisybaseline (17.3%) but worse than Gauss-PEM (13.6%)."
    }, {
      "heading" : "5. DISCUSSION",
      "text" : "All proposed training methods lead to networks with increased noise robustness in the low SNR range in comparison with the standard noisy-baseline method. The noise robustness is increased on a network level and it does not rely on complex preprocessing frameworks.\nWe see increased noise robustness against both tested noise types. This is remarkable, as the networks only saw the pink noise type during training. The results show that waveform-level noise mixing (PEM) is especially strong in transferring noise robustness to noise types not seen in training. The feature level noise addition (Gauss) is less effective on unseen noise types. Also, PEM enabled us to train noise robust networks that - at the same time - achieve lower WER on clean speech than a network trained only on clean speech. The uncompromising data augmentation by PEM should be a decisive factor to achieve these results. While the noisy-baseline trained on 1.7GB of unique data (waveform level), the PEM-enabled networks trained on up to 408GB (240 epochs for ACCAN * 1.7GB) of unique training data (waveform level). By permanently sampling different noise segments, we force the network to not rely only on constant noise features for classification, but to develop a better internal representation of the speech data. This representation could be refined further by using other noise types besides pink noise for\ntraining, such as babble noise, street noise, restaurant noise. Also, SNR steps smaller than 5dB could be used to allow more than 11 different SNR levels during training.\nPEM enabled us to dynamically change the SNR during training. This facilitated the implementation of our novel ACCAN training strategy, that achieved the best noise robustness performance. The multi-stage training starts at low SNRs, where annealed networks are able to explore the parameters space with moderate influence of the speech signal. During gradual exposure to higher SNRs in the training process, accordion annealed networks refine their internal model of speech step by step, while they seemingly acquire higher noise robustness at the lower SNR levels. The inverse way of going through the SNR range does not yield increased noise robustness. The immediate presence of clean speech signals seems to force the network to converge faster to a complex acoustic model instead of exploring the parameter space."
    }, {
      "heading" : "6. CONCLUSION",
      "text" : "This work proposes new training methods for improving the noise robustness of RNNs for a LVCSR task. The networks are trained for a wide SNR range with the use of the Vanilla-PEM training method which adds noise at the waveform level and the Gauss method which injects Gaussian noise at the feature level. By combining the Gauss and Vanilla-PEM methods into the Gauss-PEM method, we achieve on average a 28% WER reduction on the 20dB to -10 dB SNR range when compared to a conventional multi-condition training method. At the same time, we achieve lower WER on clean speech than a network that is trained solely on clean speech. The ACCAN training strategy enhances the Gauss-PEM method with a curriculum learning strategy. The ACCAN training strategy results in performance up to 11.3% lower WER at low SNRs compared to Gauss-PEM method."
    }, {
      "heading" : "7. ACKNOWLEDGEMENTS",
      "text" : "The authors acknowledge Enea Ceolini and Joachim Ott for discussions on RNNs. They also acknowledge Ying Zhang and Yoshua Bengio (both from University of Montreal) for help in the setup of the language model. This work was partially supported by Samsung Advanced Institute of Technology and EU H2020 COCOHA #644732."
    }, {
      "heading" : "8. REFERENCES",
      "text" : "[1] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012.\n[2] Jinyu Li, Li Deng, Yifan Gong, and Reinhold Haeb-Umbach, “An overview of noise-robust automatic speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745–777, 2014.\n[3] Steven F Boll, “Suppression of acoustic noise in speech using spectral subtraction,” IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 27, no. 2, pp. 113–120, 1979.\n[4] Yusuke Fujita, Ryoichi Takashima, Takeshi Homma, Rintaro Ikeshita, Yohei Kawaguchi, Takashi Sumiyoshi, Takashi Endo, and Masahito Togami, “Unified asr system using lgm-based source separation, noise-robust feature extraction, and word hypothesis selection,” in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 416–422.\n[5] Takaaki Hori, Zhuo Chen, Hakan Erdogan, John R Hershey, JL Roux, Vikramjit Mitra, and Shinji Watanabe, “The merl/sri system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition,” in Proc. IEEE ASRU, 2015.\n[6] Hynek Hermansky, Nelson Morgan, Aruna Bayya, and Phil Kohn, “Compensation for the effect of the communication channel in auditory-like analysis of speech (rasta-plp),” in Second European Conference on Speech Communication and Technology, 1991.\n[7] Takaaki Hori, Zhuo Chen, Hakan Erdogan, John R Hershey, JL Roux, Vikramjit Mitra, and Shinji Watanabe, “The merl/sri system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition,” in Proc. IEEE ASRU, 2015.\n[8] Andrew L Maas, Quoc V Le, Tyler M O’Neil, Oriol Vinyals, Patrick Nguyen, and Andrew Y Ng, “Recurrent neural networks for noise reduction in robust asr.,” in INTERSPEECH, 2012, pp. 22–25.\n[9] Xue Feng, Yaodong Zhang, and James Glass, “Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition,” in 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2014, pp. 1759–1763.\n[10] Dimitri Palaz, Ronan Collobert, et al., “Analysis of cnn-based speech recognition system using raw speech as input,” in Proceedings of Interspeech, 2015.\n[11] George Trigeorgis, Fabien Ringeval, Raymond Brueckner, Erik Marchi, Mihalis A Nicolaou, Stefanos Zafeiriou, et al., “Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5200–5204.\n[12] Michael L Seltzer, Dong Yu, and Yongqiang Wang, “An investigation of deep neural networks for noise robust speech recognition,” in 2013 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2013, pp. 7398–7402.\n[13] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\n[14] Yajie Miao, Mohammad Gowayyed, and Florian Metze, “Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,” in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 167–174.\n[15] Dario Amodei et al., “Deep speech 2: End-to-end speech recognition in english and mandarin,” CoRR, vol. abs/1512.02595, 2015.\n[16] Shi Yin, Chao Liu, Zhiyong Zhang, Yiye Lin, Dong Wang, Javier Tejedor, Thomas Fang Zheng, and Yinguo Li, “Noisy training for deep neural networks in speech recognition,” EURASIP Journal on Audio, Speech, and Music Processing, vol. 2015, no. 1, pp. 1–14, 2015.\n[17] Petri Koistinen and Lasse Holmström, “Kernel regression and backpropagation training with noise,” in 1991 IEEE International Joint Conference on Neural Networks, 1991, pp. 367– 372.\n[18] Krzysztof J. Geras and Charles A. Sutton, “Scheduled denoising autoencoders,” CoRR, vol. abs/1406.3269, 2014.\n[19] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston, “Curriculum learning,” in Proceedings of the 26th Annual International Conference on Machine Learning, New York, NY, USA, 2009, ICML ’09, pp. 41–48, ACM.\n[20] Audacity team, “Audacity,” 2016.\n[21] Andrew Varga and Herman JM Steeneken, “Assessment for automatic speech recognition: Ii. noisex-92: A database and an experiment to study the effect of additive noise on speech recognition systems,” Speech Communication, vol. 12, no. 3, pp. 247–251, 1993.\n[22] Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber, “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd International Conference on Machine learning. ACM, 2006, pp. 369–376.\n[23] Lasagne team, “Lasagne: First release.,” Aug. 2015.\n[24] Sepp Hochreiter and Jürgen Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[25] Xavier Glorot and Yoshua Bengio, “Understanding the difficulty of training deep feedforward neural networks,” in International Conference on Artificial Intelligence and Statistics, 2010, pp. 249–256.\n[26] Diederik P. Kingma and Jimmy Ba, “Adam: A method for stochastic optimization,” CoRR, vol. abs/1412.6980, 2014.\n[27] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech recognition with recurrent neural networks.,” in ICML, 2014, vol. 14, pp. 1764–1772."
    } ],
    "references" : [ {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An overview of noise-robust automatic speech recognition",
      "author" : [ "Jinyu Li", "Li Deng", "Yifan Gong", "Reinhold Haeb-Umbach" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745–777, 2014.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Suppression of acoustic noise in speech using spectral subtraction",
      "author" : [ "Steven F Boll" ],
      "venue" : "IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 27, no. 2, pp. 113–120, 1979.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Unified asr system using lgm-based source separation, noise-robust feature extraction, and word hypothesis selection",
      "author" : [ "Yusuke Fujita", "Ryoichi Takashima", "Takeshi Homma", "Rintaro Ikeshita", "Yohei Kawaguchi", "Takashi Sumiyoshi", "Takashi Endo", "Masahito Togami" ],
      "venue" : "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 416–422.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The merl/sri system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition",
      "author" : [ "Takaaki Hori", "Zhuo Chen", "Hakan Erdogan", "John R Hershey", "JL Roux", "Vikramjit Mitra", "Shinji Watanabe" ],
      "venue" : "Proc. IEEE ASRU, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Compensation for the effect of the communication channel in auditory-like analysis of speech (rasta-plp)",
      "author" : [ "Hynek Hermansky", "Nelson Morgan", "Aruna Bayya", "Phil Kohn" ],
      "venue" : "Second European Conference on Speech Communication and Technology, 1991.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "The merl/sri system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition",
      "author" : [ "Takaaki Hori", "Zhuo Chen", "Hakan Erdogan", "John R Hershey", "JL Roux", "Vikramjit Mitra", "Shinji Watanabe" ],
      "venue" : "Proc. IEEE ASRU, 2015.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Recurrent neural networks for noise reduction in robust asr",
      "author" : [ "Andrew L Maas", "Quoc V Le", "Tyler M O’Neil", "Oriol Vinyals", "Patrick Nguyen", "Andrew Y Ng" ],
      "venue" : "INTERSPEECH, 2012, pp. 22–25.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition",
      "author" : [ "Xue Feng", "Yaodong Zhang", "James Glass" ],
      "venue" : "2014 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2014, pp. 1759–1763.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Analysis of cnn-based speech recognition system using raw speech as input",
      "author" : [ "Dimitri Palaz", "Ronan Collobert" ],
      "venue" : "Proceedings of Interspeech, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "author" : [ "George Trigeorgis", "Fabien Ringeval", "Raymond Brueckner", "Erik Marchi", "Mihalis A Nicolaou", "Stefanos Zafeiriou" ],
      "venue" : "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5200–5204.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "An investigation of deep neural networks for noise robust speech recognition",
      "author" : [ "Michael L Seltzer", "Dong Yu", "Yongqiang Wang" ],
      "venue" : "2013 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2013, pp. 7398–7402.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1929
    }, {
      "title" : "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding",
      "author" : [ "Yajie Miao", "Mohammad Gowayyed", "Florian Metze" ],
      "venue" : "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 167–174.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "author" : [ "Dario Amodei" ],
      "venue" : "CoRR, vol. abs/1512.02595, 2015.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Noisy training for deep neural networks in speech recognition",
      "author" : [ "Shi Yin", "Chao Liu", "Zhiyong Zhang", "Yiye Lin", "Dong Wang", "Javier Tejedor", "Thomas Fang Zheng", "Yinguo Li" ],
      "venue" : "EURASIP Journal on Audio, Speech, and Music Processing, vol. 2015, no. 1, pp. 1–14, 2015.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Kernel regression and backpropagation training with noise",
      "author" : [ "Petri Koistinen", "Lasse Holmström" ],
      "venue" : "1991 IEEE International Joint Conference on Neural Networks, 1991, pp. 367– 372.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Scheduled denoising autoencoders",
      "author" : [ "Krzysztof J. Geras", "Charles A. Sutton" ],
      "venue" : "CoRR, vol. abs/1406.3269, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning, New York, NY, USA, 2009, ICML ’09, pp. 41–48, ACM.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Audacity",
      "author" : [ "Audacity team" ],
      "venue" : "2016.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Assessment for automatic speech recognition: Ii. noisex-92: A database and an experiment to study the effect of additive noise on speech recognition systems",
      "author" : [ "Andrew Varga", "Herman JM Steeneken" ],
      "venue" : "Speech Communication, vol. 12, no. 3, pp. 247–251, 1993.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "Proceedings of the 23rd International Conference on Machine learning. ACM, 2006, pp. 369–376.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Lasagne: First release",
      "author" : [ "Lasagne team" ],
      "venue" : "Aug. 2015.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, 2010, pp. 249–256.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "CoRR, vol. abs/1412.6980, 2014.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Towards end-to-end speech recognition with recurrent neural networks",
      "author" : [ "Alex Graves", "Navdeep Jaitly" ],
      "venue" : "ICML, 2014, vol. 14, pp. 1764–1772.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The performance of automatic speech recognition (ASR) systems has increased significantly with the use of deep neural networks (DNNs) [1].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "Over the past decades, a multitude of methods to improve noise robustness of ASR systems has been proposed [2], with many methods being applicable to DNNs.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "Some example enhancement methods applied prior to feature extraction include denoising methods [3] and source separation methods [4] [5].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "Some example enhancement methods applied prior to feature extraction include denoising methods [3] and source separation methods [4] [5].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "Some example enhancement methods applied prior to feature extraction include denoising methods [3] and source separation methods [4] [5].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "Methods applied at the feature level include methods that produce auditory-level features [6] and feature space adaptation methods [7].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "Methods applied at the feature level include methods that produce auditory-level features [6] and feature space adaptation methods [7].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "feature denoising with deep autoencoders [8] [9] or feature extraction from the raw waveform via convolutional neural networks (CNNs) [10] [11].",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : "feature denoising with deep autoencoders [8] [9] or feature extraction from the raw waveform via convolutional neural networks (CNNs) [10] [11].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : "feature denoising with deep autoencoders [8] [9] or feature extraction from the raw waveform via convolutional neural networks (CNNs) [10] [11].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 10,
      "context" : "feature denoising with deep autoencoders [8] [9] or feature extraction from the raw waveform via convolutional neural networks (CNNs) [10] [11].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "10 dB - 20 dB [12] or 0 dB - 30 dB [10] are used during training.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "10 dB - 20 dB [12] or 0 dB - 30 dB [10] are used during training.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "Other training methods such as dropout [13] - originally intended to improve regularisation - have been shown to also improve noise robustness [12].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "Other training methods such as dropout [13] - originally intended to improve regularisation - have been shown to also improve noise robustness [12].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "The same is true for model adaptation/noise aware training techniques [12].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "RNNs are used here because they have demonstrated state-ofthe-art performance in tasks such as the common sequence labelling task in speech recognition [14] [15].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 14,
      "context" : "RNNs are used here because they have demonstrated state-ofthe-art performance in tasks such as the common sequence labelling task in speech recognition [14] [15].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 15,
      "context" : "Our baseline method takes advantage of multi-condition training [16] in order to increase the noise robustness of the network.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "0 to 30 dB as in [10]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : "Gaussian noise injection is a well-known method for improving generalisation in neural networks [17].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "ACCAN [0] [0, 5] .",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "Neural networks have been shown to optimize on the SNR they are trained on [16].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "While curriculum learning has been used in image classification (scheduled denoising autoencoders, [18]) as well as speech recognition (SortaGrad [15], a method for faster accuracy convergence), this is the first work targeted at LVCSR under noisy conditions.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "While curriculum learning has been used in image classification (scheduled denoising autoencoders, [18]) as well as speech recognition (SortaGrad [15], a method for faster accuracy convergence), this is the first work targeted at LVCSR under noisy conditions.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 18,
      "context" : "However, the noise allows the network to explore the parameter space more extensively at the beginning [19].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 19,
      "context" : "For noise corruption, we used two different noise types: pink noise generated by the Audacity [20] software and babble noise from the NOISEX database [21].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 20,
      "context" : "For noise corruption, we used two different noise types: pink noise generated by the Audacity [20] software and babble noise from the NOISEX database [21].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 13,
      "context" : "Data preparation and language model The labels and transcriptions were extracted with EESEN [14] routines.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "The features were generated by preprocessing routines from EESEN [14].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "In order to automatically learn the alignments between speech frames and label sequences, the Connectionist Temporal Classification (CTC) [22] objective was adopted.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 22,
      "context" : "The Lasagne library [23] enabled us to build and train our 5-layer neural network.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "The first 4 layers consisted of bidirectional long short-term memory (LSTM) [24] units with 250 units in each direction.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 24,
      "context" : "All layers were initialized with the Glorot uniform strategy [25].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "During training, the Adam [26] stochastic optimization method was used.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "To prevent overfitting and to increase noise robustness, dropout [13] was used (dropout probability=0.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "5M parameter network, while in literature [27], a 13.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "This is expected, as the mean SNR of the training SNR range is 25dB and the network seems to optimize for SNR levels close to this value [16].",
      "startOffset" : 137,
      "endOffset" : 141
    } ],
    "year" : 2016,
    "abstractText" : "The performance of automatic speech recognition systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple curriculum training strategy called accordion annealing (ACCAN). It uses a multi-stage training schedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are first added and samples at increasing higher SNR values are gradually added up to an SNR value of 50dB. We also use a method called per-epoch noise mixing (PEM) that generates noisy training samples online during training and thus enables dynamically changing the SNR of our training data. Both the ACCAN and the PEM methods are evaluated on a end-to-end speech recognition pipeline on the Wall Street Journal corpus. ACCAN decreases the average word error rate (WER) on the 20dB to -10dB SNR range by up to 31.4% when compared to a conventional multi-condition training method.",
    "creator" : "LaTeX with hyperref package"
  }
}