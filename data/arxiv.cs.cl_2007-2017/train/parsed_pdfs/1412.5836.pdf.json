{
  "name" : "1412.5836.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Daniel Fried" ],
    "emails" : [ "dfried@email.arizona.edu", "kevinduh@is.naist.jp" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "We are interested in algorithms for learning vector representations of words. Recent work has shown that such representations, also known as word embeddings, can successfully capture the semantic and syntactic regularities of words (Mikolov et al., 2013a) and improve the performance of various Natural Language Processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), parsing (Socher et al., 2013a), and semantic role labeling (Collobert et al., 2011).\nAlthough many kinds of representation learning algorithms have been proposed so far, they are all essentially based on the same premise of distributional semantics (Harris, 1954), embodied by J. R. Firth’s dictum: “You shall know a word by the company it keeps.” For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word. Intuitively, these algorithms learn to map words with similar context to nearby points in vector space.\nHowever, distributional semantics is by no means the only theory of word meaning. Relational semantics, exemplified by WordNet (Miller, 1995), defines a word by its relation with other words. Relations such as synonymy, hypernymy, and meronymy (Cruse, 1986) create a graph that links words in terms of our world knowledge and psychological predispositions. For example, stating a relation like “dog is-a mammal” gives a precise hierarchy between the two words, in a way that is very different from the distributional similarities observable from corpora. Arguably, the vector representation of “dog” ought be close to that of “mammal”, regardless of their distributional contexts.\nWe believe both distributional and relational semantics are valuable for word representations. Our goal is to explore how to combine these complementary approaches into a unified learning algorithm. We employ a general representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011). Its advantages include (a) flexibility in incorporating arbitrary objectives, and (b) relative ease of implementation. We show that ADMM effectively optimizes the joint objective and present preliminary results on several tasks. ∗Currently at the University of Cambridge.\nar X\niv :1\n41 2.\n58 36\nv1 [\ncs .C\nL ]\n1 8"
    }, {
      "heading" : "2 DISTRIBUTIONAL AND RELATIONAL OBJECTIVES",
      "text" : "Distributional Semantics Objective: We implement distributional semantics using the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi ∈ Rd, the word’s embedding. An n-length sequence of words (i1, i2, . . . , in) is represented as a vector x by concatenating the vector embeddings for each word, x = [wi1 ;wi2 . . . ;win ]. This vector x is then scored by feeding it through a two-layer neural network with h hidden nodes: SNLM (x) = u>(f(Ax + b)), where A ∈ Rh×(nd), b ∈ Rh, u ∈ Rh are network parameters and f is the sigmoid f(t) = 1/(1 + e−t) applied element-wise. The model is trained using noise contrastive estimation (NCE) (Mnih & Kavukcuoglu, 2013), where training text is corrupted by random replacement of random words to provide an implicit negative training example, xc. The loss function, optimized via stochastic gradient descent (SGD), is:\nLNLM (x,xc) = max(0, 1− SNLM (x) + SNLM (xc)) (1)\nRelational Semantics Objective: We investigate three different objectives. The Graph Distance loss, LGD, is based on the idea that words close together in WordNet should have similar embeddings. First, for a word pair (i, j), we define WordSim(i, j) based on the shortest path between word synsets (Leacock & Chodorow, 1998). Then, we encourage the cosine similarity between their embeddings vi and vj to match that of WordSim(i, j):\nLGD(i, j) =\n( vi · vj\n||vi||2||vj ||2 − [a×WordSim(i, j) + b]\n)2 (2)\nwhere a and b are parameters (learned during training) that scale WordSim(i, j) to be of the same range as the cosine.\nA different approach directly models WordNet relations as operations in vector space. These models score an input tuple (vl, R, vr) indicating a relation of type R between words vl and vr. The TransE model of Bordes et al. (2013) represents relations as translations: if the relationship R holds for two words vl and vr, then their embeddings vl,vr ∈ Rd should be close after translating vl by a relation vector R ∈ Rd: STransE(vl, R, vr) = −||vl +R− vr||2. Socher et al. (2013b) introduce a Neural Tensor Network (NTN) that models interaction between embeddings using tensors. The\nscoring function for a relation R is: SNTN (vl, R, vr) = U>f ( v>l WRvr +VR [ vl vr ] + bR ) , where U ∈ Rh, WR ∈ Rd×d×h, VR ∈ Rh×2d and bR ∈ Rk are parameters for relationship R. As in the NLM, parameters for these relational models are trained using NCE and SGD, using the hinge loss as defined in Eq. 1, with SNLM replaced by the STransE or SNTN scoring function.\nJoint Objective Optimization by ADMM: We now describe an ADMM formulation for joint optimization of the above objectives. Let w be the set of word embeddings {w1,w2, . . .wN ′} for the distributional objective, and v be the set of word embeddings {v1,v2, . . .vN ′′} for the relational objective, where N ′ and N ′′ are the vocabulary size of the corpus and WordNet, respectively. Let I be the set of N words that occur in both. Then we define a set of vectors y = {y1,y2, . . .yN}, which correspond to Lagrange multipliers, to penalize the difference (wi − vi) between sets of embeddings for each word i in the joint vocabulary I:\nLP (w,v) = ∑ i∈I ( y>i (wi − vi) ) + ρ 2 (∑ i∈I ||wi − vi||22 ) (3)\nIn the first term, y has same dimensionality as w and v, so a scalar penalty is maintained for each entry in every embedding vector. The second residual penalty term with hyperparameter ρ is added to avoid saddle points; ρ can be viewed as a step-size during the update of y. Finally, this augmented Lagrangian term (Eq. 3) is added to the sum of the loss terms for each objective (Eq. 1 and Eq. 2). Let θ = (u,A,b) be the parameters of the distributional objective, and φ be the parameters of the relational objective. The final loss function we optimize becomes:\nL = LNLM (w, θ) + LGD(v, φ) + LP (w,v) (4)\nThe ADMM algorithm proceeds by repeating the following three steps until convergence: (1) minimizew,θLNLM + LP ; (2) minimizev,φLGD + LP ; (3) For all embeddings i in joint vocabulary I , update the constraint vector: yi := yi + ρ(wi − vi). Since LNLM and LGD share no parameters, Steps (1) and (2) can be optimized easily using the same NCE and SGD procedures as the single objective case, with additional regularization term ρ (wi − vi)."
    }, {
      "heading" : "3 PRELIMINARY EXPERIMENTS & DISCUSSIONS",
      "text" : "The distributional objectiveLNLM is trained using 5-grams from the Google Books English corpus1, containing over 180 million 5-gram types. For training LGD, we sample 100k words and compute similarity to 5 other words per word in each ADMM iteration. For training LTransE and LNTN , we use the dataset of Socher et al. (2013b).\nWe first provide an analysis of the behavior of ADMM on the training set, to confirm that it effectively optimizes the joint objective. Fig. 1(left) plots the learning curve by training iteration for various values of the ρ hyperparameter. We see that ADMM attains a reasonable objective value relatively quickly in 100 iterations. Fig. 1(right) shows the averaged difference between the resulting sets of embeddings w and v, which decreases as desired.2\nNext, we compare the embeddings learned with different objectives on three standard benchmark tasks (Table 1). The Knowledge Base Completion task (Socher et al., 2013b) evaluates the models’ ability to classify relationship triplets from WordNet as correct or incorrect. The SemEval2012 Analogy Test is a relational word similarity task similar to SAT-style analogy questions (Jurgens et al., 2012). Given a set of word pairs, the model needs to select the pairs that most and least represent a particular relation. The Dependency Parsing task is based on the SANCL2012 data (Petrov & McDonald, 2012) and evaluates the accuracy of parsers trained on news domain adapted for web domain. We incorporate the embeddings as additional features in a standard MST parser to see whether embeddings improve generalization of out-of-domain words.\nFor both Knowledge Base and Parsing tasks, we observe that joint objective generally improves over single objectives: e.g. TransE+NLM (83.10%) > TransE (82.87%) for Knowledge Base, GD+NLM (76.18%) > GD (75.90%) for Parsing. The improvements are not large, but relatively consistent. For the Analogy Test, joint objectives did not improve over the single objective NLM baseline. We provide further analysis as well as extended descriptions of methods and experiments in a longer version of the paper here: http://arxiv.org/abs/1412.4369.\n1Berkeley distribution: tomato.banatao.berkeley.edu:8080/berkeleylm_binaries/ 2The reason for the peak around iteration 50 in Fig. 1 is that the embeddings begin with similar random initializations, so initially differences are small; as ADMM starts to see more data, w and v diverge, but converge eventually as y become large."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work is supported by a Microsoft Research CORE Grant and JSPS KAKENHI Grant Number 26730121. D.F. was supported by the Flinn Scholarship during the course of this work. We thank Haixun Wang, Jun’ichi Tsujii, Tim Baldwin, Yuji Matsumoto, and several anonymous reviewers for helpful discussions at various stages of the project."
    } ],
    "references" : [ {
      "title" : "A neural probabilistic language models",
      "author" : [ "Bengio", "Yoshua", "Ducharme", "Réjean", "Vincent", "Pascal", "Jauvin", "Christian" ],
      "venue" : null,
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Translating embeddings for modeling multi-relational data",
      "author" : [ "Bordes", "Antoine", "Usunier", "Nicolas", "Garcia-Duran", "Alberto", "Weston", "Jason", "Yakhnenko", "Oksana" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "Boyd", "Stephen", "Parikh", "Neal", "Chu", "Eric", "Peleato", "Borja", "Eckstein", "Jonathan" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Boyd et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boyd et al\\.",
      "year" : 2011
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Collobert et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Lexical Semantics",
      "author" : [ "Cruse", "Alan D" ],
      "venue" : null,
      "citeRegEx" : "Cruse and D.,? \\Q1986\\E",
      "shortCiteRegEx" : "Cruse and D.",
      "year" : 1986
    }, {
      "title" : "Semeval-2012 task 2: Measuring degrees of relational similarity",
      "author" : [ "Jurgens", "David A", "Turney", "Peter D", "Mohammad", "Saif M", "Holyoak", "Keith J" ],
      "venue" : "In Proceedings of the First Joint Conference on Lexical and Computational Semantics,",
      "citeRegEx" : "Jurgens et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jurgens et al\\.",
      "year" : 2012
    }, {
      "title" : "Combining local context and WordNet similarity for word sense identification",
      "author" : [ "Leacock", "Claudia", "Chodorow", "Martin" ],
      "venue" : "WordNet: An Electronic Lexical Database,",
      "citeRegEx" : "Leacock et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Leacock et al\\.",
      "year" : 1998
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey" ],
      "venue" : "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Mikolov", "Tomás", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "WordNet: A lexical database for English",
      "author" : [ "Miller", "George A" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Miller and A.,? \\Q1995\\E",
      "shortCiteRegEx" : "Miller and A.",
      "year" : 1995
    }, {
      "title" : "Learning word embeddings efficiently with noisecontrastive estimation",
      "author" : [ "Mnih", "Andriy", "Kavukcuoglu", "Koray" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mnih et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Overview of the 2012 shared task on parsing the web",
      "author" : [ "Petrov", "Slav", "McDonald", "Ryan" ],
      "venue" : "In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),",
      "citeRegEx" : "Petrov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Petrov et al\\.",
      "year" : 2012
    }, {
      "title" : "Continuous space language models",
      "author" : [ "Schwenk", "Holger" ],
      "venue" : "Computer Speech and Language,",
      "citeRegEx" : "Schwenk and Holger.,? \\Q2007\\E",
      "shortCiteRegEx" : "Schwenk and Holger.",
      "year" : 2007
    }, {
      "title" : "Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "author" : [ "Socher", "Richard", "Bauer", "John", "Manning", "Christopher D", "Ng", "Andrew Y" ],
      "venue" : null,
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Socher", "Richard", "Chen", "Danqi", "Manning", "Christopher D", "Ng", "Andrew Y" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Word representations: A simple and general method for semi-supervise learning",
      "author" : [ "Turian", "Joseph", "Ratinov", "Lev-Arie", "Bengio", "Yoshua" ],
      "venue" : "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Turian et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Effect of non-linear deep architecture in sequence labeling",
      "author" : [ "Wang", "Mengqiu", "Manning", "Christopher D" ],
      "venue" : "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,",
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : ", 2013a) and improve the performance of various Natural Language Processing systems, including information extraction (Turian et al., 2010; Wang & Manning, 2013), parsing (Socher et al.",
      "startOffset" : 118,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : ", 2013a), and semantic role labeling (Collobert et al., 2011).",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "” For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.",
      "startOffset" : 29,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "” For example, the models of (Bengio et al., 2003; Schwenk, 2007; Collobert et al., 2011; Mikolov et al., 2013b; Mnih & Kavukcuoglu, 2013) train word representations by exploiting the context window around the word.",
      "startOffset" : 29,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "We employ a general representation learning algorithm based on the Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011).",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "Distributional Semantics Objective: We implement distributional semantics using the Neural Language Model (NLM) of Collobert et al. (2011). Each word i in the vocabulary is associated with a d-dimensional vector wi ∈ R, the word’s embedding.",
      "startOffset" : 115,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "The TransE model of Bordes et al. (2013) represents relations as translations: if the relationship R holds for two words vl and vr, then their embeddings vl,vr ∈ R should be close after translating vl by a relation vector R ∈ R: STransE(vl, R, vr) = −||vl +R− vr||2.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "The TransE model of Bordes et al. (2013) represents relations as translations: if the relationship R holds for two words vl and vr, then their embeddings vl,vr ∈ R should be close after translating vl by a relation vector R ∈ R: STransE(vl, R, vr) = −||vl +R− vr||2. Socher et al. (2013b) introduce a Neural Tensor Network (NTN) that models interaction between embeddings using tensors.",
      "startOffset" : 20,
      "endOffset" : 289
    }, {
      "referenceID" : 5,
      "context" : "The SemEval2012 Analogy Test is a relational word similarity task similar to SAT-style analogy questions (Jurgens et al., 2012).",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 12,
      "context" : "For training LTransE and LNTN , we use the dataset of Socher et al. (2013b). We first provide an analysis of the behavior of ADMM on the training set, to confirm that it effectively optimizes the joint objective.",
      "startOffset" : 54,
      "endOffset" : 76
    } ],
    "year" : 2017,
    "abstractText" : "We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.",
    "creator" : "LaTeX with hyperref package"
  }
}