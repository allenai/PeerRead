{
  "name" : "1603.09457.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LSTM based Conversation Models",
    "authors" : [ "Yi Luan", "Yangfeng Ji", "Mari Ostendorf" ],
    "emails" : [ "luanyi@uw.edu,", "jiyfeng@gatech.edu,", "ostendor@uw.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "As automatic language understanding and generation technology improves, there is increasing interest in building humancomputer conversational systems, which can be used for a variety of applications such as travel planning, tutorial systems or chat-based technical support. Most work has emphasized understanding or generating a word sequence associated with a single sentence or speaker turn, potentially leveraging the previous turn. Beyond local context, language use in a goal-oriented conversation reflects the global topic of discussion, as well as the respective role of each participant. In this work, we introduce a conversational language model that incorporates both local and global context together with participant role.\nIn particular, participant roles (or speaker roles) impact content of a sentence in terms of both the information to be communicated and the interaction strategy, affecting both meaning and conversational structure. For example, in broadcast news, speaker roles are shown to be informative for discovering the story structures [1]; they impact speaking time and turn-taking [2]; and they are associated with particular phrase patterns [3]. In online discussions, speaker role is useful for detecting authority claims [4]. Other work shows that in casual conversations, speakers with different roles are likely to use different discourse markers [5]. For the Ubuntu technical support data used in this study, Table 1 illustrates differences in the distributions of frequent words for the poster vs. responder roles. The POSTER role tends to raise questions using words anyone, how. The RESPONDER role tends to use directive words (you, you’re), hedges (may, might) and words related to problem solving (sudo, check).\nSpecifically, we propose a neural network model that builds on recent work in response generation, integrating different methods that have been used for capturing local (previous sentence) context and more global context, and extending the network architecture to incorporate role information. The model can be used as a language model, as in speech recognition or\ntranslation, but our focus here is on response generation. Experiments are conducted with Ubuntu chat logs, using language model perplexity and response ranking, as well as qualitative analysis."
    }, {
      "heading" : "2. Related Work",
      "text" : "Data-driven methods are now widely used for building conversation systems. With the popularity of social media, such as Twitter, Sina Weibo, and online discussion forums, it is easier to collect conversation text [6, 7]. Several different data-driven models have been proposed to build conversation systems. Ritter et al. [8] present a statistical machine translation based conversation system. Recently, neural network models have been explored. The flexibility of neural network models opens the possibility of integrating different kinds of information into the generation procedure. For example, Sordoni et al. [9] present a way to integrate contextual information via feed-forward neural networks. Li et al. propose using Maximum Mutual Information (MMI) as the objective function in neural models in order to produce more diverse and interesting responses. Shang et al. [10] introduce the attention mechanism into an encoderdecoder network for a conversation model. Most similar to our work is the Semantic Controlled LSTM (SC-LSTM) proposed by Wan et al. [11], where a Dialog-act component is introduced into the LSTM cell to guide the generated content. In this work, we utilize the role information to bias response generation without modifying LSTM cells.\nEfficiently capturing local and global context remains a open problem in language modeling. Different ways of modeling document-level context has been explored in [12] and [13] based on the LSTM framework. Luan et al. [14] proposed a multi-scale recurrent architecture to incorporate both word and turn level context for spoken language understanding tasks. In this paper, we use a similar approach as [16], explicitly using Latent Dirichlet Analysis (LDA) as global-context feature to feed into RNNLM.\nEarly work on incorporating local context in conversational language modeling is described in [17] conditioned on the most recent word spoken by other speakers. Hutchinson et al. [18, 19] improve log-bilinear language model by introducing a multi-factor sparse matrix that could capture speaker role and topic information. In addition, Huang et al. [20] show that language models with role information significantly reduce word error rate in speech recognition. Our work differs from these approaches in using an LSTM. Recently, Li et al. propose using an additional vector to LSTM in order to capture personal characteristics of a speaker [21]. In this work, we utilize both a global topic vector and role information, where a role-specific weight matrix biases the word distributions for different roles.\nar X\niv :1\n60 3.\n09 45\n7v 1\n[ cs\n.C L\n] 3\n1 M\nar 2\n01 6"
    }, {
      "heading" : "3. Model",
      "text" : "In this section, we propose an LSTM based framework that integrating participant role and global topic of the conversation. As discussed in section 1, the assumption is, given the same context, each role has its own preference of picking words to generate a response. Each generated response should be both topically related to the current conversation and coherent with the local context."
    }, {
      "heading" : "3.1. Recurrent Neural Network Language Models",
      "text" : "We start building a response generation model [9] by using a recurrent neural network language model (RNNLM) [22]. In general, a RNNLM is a generative model of sentences. For a sentence consisted of word sequence x1, . . . , xI , the probability of xi given x1, . . . , xi−1 , x≤i−1 is\np(xi|x≤i−1) ∝ gτ (hi) (1)\nwhere hi is the current hidden state and gτ (·) is the probability function parameterized by τ :\ngτ (hi) = softmax(Wτhi), (2)\nwhere Wτ is the output layer parameter. The hidden state hi is computed recurrently as\nhi = fθ (xi,hi−1) . (3)\nfθ (·) is a nonlinear function parameterized by θ. We use an LSTM [23] since it is good at capturing long-term dependency, which is an objective for our conversation model."
    }, {
      "heading" : "3.2. Conversation Models with Speaker Roles",
      "text" : "To build a conversation model with different participant roles, we extend a RNNLM in two respects. First, to capture the variability from different participant roles, we incorporate rolebased information into the generation procedure. Second, to model a conversation instead of single turns, our model adjoins RNNLMs for all turns in sequence to model the whole conversation.\nMore specifically, consider two adjacent turns1 xt−1 = {xt−1,i} Nt−1 i=1 and xt = {xt,i} Nt i=1 with their participant role rt−1 and rt respectively. Nt is the number of words in the t-th turn. To build a single model for the entire conversation, we simply concatenate the RNNLMs for all sentences in order. Concatenation changes the way of computing the first hidden state in each utterance (except the first utterance in the conversation). Considering the two turns xt−1 and xt, after concatenation, the computation of the first hidden state in turn xt, ht,1, is\nht,1 = fθ ( xt,1,ht−1,Nt−1 ) . (4)\nAs we will see from section 4, this simple solution can capture the long-term contextual information.\n1In our formulation, we use one turn as the minimal unit as multiple sentences in one turn share the same role.\nWe introduce the role-based information by defining a roledependent function gτ,r (·). For example, the probability of xt,i given xt,i−1, . . . , x1,1 , x≤t,≤i−1 and its role rt is\np(xt,i|x≤t,≤i−1, rt) ∝ gτ,rt (ht,i) . (5)\nwhere the gτ,rt (·) is also parameterized by role rt. In our implementation, we use\ngτ,rt (ht,i) = softmax(Wτ (Wrtht,i)), (6)\nwhere Wτ ∈ RV×H , Wrt ∈ RH×H , V,H are the vocabulary size and hidden layer dimension respectively. Even Wτ is shared across the entire conversation model, Wrt is rolespecific. This linear transformation defined in Eq. 6 is easy to train in practice and appears to capture role information. This model is named the R-CONV model, as the role-based information is introduced in the output layer.\nDespite the difference between the two models, they can be learned in the same way, which is similar to training a RNNLM [22]. Following the way of training a language model, the parameters could be learned by maximizing the following objective function ∑\nk ∑ t `(xt,k+1,yt,k) (7)\nwhere yt,k is the prediction of xt,i+1. `(·, ·) can be any loss function for classification task. We choose cross entropy [24] as the loss function `(·, ·), because it is a popular objective function used in training neural language models.\nAs a final comment, if we eliminate the role information, RLDA-CONV will be reduced to an RNNLM. To demonstrate the utility of role-based information, we will use an RNNLM over conversations as a baseline model."
    }, {
      "heading" : "3.3. Incorporating global topic context",
      "text" : "In order to capture long-span context of the conversation, inspired by [16], we explicitly include a topic vector representing all previous dialog turns. We use Latent Dirichlet Allocation (LDA) to achieve a compact vector-space representation. This procedure maps a bag-of-words representation of a document into a low-dimensional vector which is conventionally interpreted as a topic representation. For each turn xt, we compute the LDA representation for all previous turns\nst = fLDA (x1,x2, . . . ,xt−1) (8)\nwhere fLDA (·) is the LDA inference function as in [15]. Then st is concatenated with hidden layer ht,i to predict xt,i.\np(xt,i|x≤t,≤i−1) ∝ gτ ( [h>t,i s > t ] > ) . (9)\nThis model is named LDA-CONV. We assume that by including st into output layer, the predicted word would be more topically related with the previous turns, thus allowing the recurrent part to learn more local context information.\nWhen incorporating both the global topic vector and the role factor, the conditional probability of xt,i is\np(xt,i|x≤t,≤i−1, rt) ∝ gτ,rt ( [h>t,i s > t ] > ) . (10)\nWe call this model, illustrated in Figure 1, R-LDA-CONV."
    }, {
      "heading" : "4. Experiments",
      "text" : "We evaluate our model from different aspects on the Ubuntu Dialogue Corpus [7], which provides one million two-person conversations extracted from Ubuntu chat logs. The conversations are about getting technical support for various Ubuntu-related problems. In this corpus, each conversation contains two users with different roles, POSTER: the user in this conversation who initializes the conversation by proposing a technical problem; RESPONDER: the other user who tries to provide technical support. For a conversation, we replace the user of each turn with the corresponding role."
    }, {
      "heading" : "4.1. Experimental setup",
      "text" : "Our models are trained in a subset of the Ubuntu Dialogue Corpus, in which each conversation contains 6 - 20 turns. The resulting data contains 216K conversations in the training set, 10k conversations in the test set and 13k conversations in the development set. We use a Twitter tokenizer [25] to parse all utterances in the conversations. The vocabulary is constructed on the training set with filtering out low-frequency tokens and replacing them with “UNKNOWN”. The vocabulary size is fixed to include 20K most frequent words. We did not filter out emoticons, instead we treat them as single tokens.\nThe LDA model is trained using all conversations in training data, where each conversation is treated as an individual training instance. We use Gensim [26] for both training and inference. There are three hyper-parameters in our models: the dimension of word representation K, the hidden dimension H and the number of topics M in LDA model. We use grid search over K,H ∈ {16, 32, 64, 128, 256}, M ∈ {50, 100}, and select the best combination for each model using the perplexity on\nthe development set. We use stochastic gradient descent with the initial learning rate λ = 0.1 to train all the models."
    }, {
      "heading" : "4.2. Evaluation Metrics",
      "text" : "Evaluation on response generation is an emerging research field in data-driven conversation modeling. Due to the variety of possible responses for a given context, it is too conservative to only compare the generated response with the ground truth. For a reasonable evaluation, the n-gram based evaluation metrics including BLEU [27] and ∆BLEU [28] require multiple references for one given context. One the other hand, there are indirect evaluation methods, for example, ranking based evaluation [7, 10] or qualitative analysis [29]. In this paper, we use both ranking-based evaluation (Recall@K [30]) across all models, and leave the n-gram based evaluation for future work. To compute the Recall@K metric of one model given K, the model is used to select the top-K candidates, and it is counted as correct if the ground-truth response is included. In addition to Recall@K, we also evaluate the different models based on test set perplexity.\nTo understand the chat conversations requires intensive knowledge of Ubuntu even for human readers. Therefore, the qualitative analysis focuses mainly on the capacity of capturing role information, not the justification of responses as valid answers to the technical questions."
    }, {
      "heading" : "4.3. Quantative Evaluation",
      "text" : "Experiments in this section compare the performance of LDACONV, R-CONV and R-LDA-CONV to the baseline LSTM system."
    }, {
      "heading" : "4.3.1. Perplexity",
      "text" : "The best perplexity numbers from the three models are shown in Table 2. R-LDA-CONV gives the lowest perplexity among the four models, nearly 8 points improvement over the baseline model. Comparing role vs. global topic, role has a bigger improvement on perplexity of 11% reduction for role vs. 7% for LDA topic. Combining both leads to a 15% reduction in perplexity. To simplify the comparison, in the following experiments, we only use the best configuration for each model."
    }, {
      "heading" : "4.3.2. Response ranking",
      "text" : "The task is to rank the ground-truth response with some randomly-selected sentences for a given context. For each test sample, we use the previous t − 1 sentences as context, try-\ning to select the best tth sentence. We randomly select 9 turns from other conversations in the dataset, replacing their role with the ground truth label. As we noticed that sentences from the background channel, like “yes”, “thank you”, could fit almost all the conversations with various context. To distinguish the background channel from some contentful sentences, we sample the negative examples with the ground-truth sentence length as a constraint — samples with the similar length (± 2 words) are selected as negative examples.\nThe Recall@K are shown in Table 3. Both R-CONV and LDA-CONV are better than baseline result, while R-LDACONV gives the best performance overall. Both role factors and topic feature are acting positively in ranking ground-truth responses. Even though no role information is explicitly used in the baseline model, the contextual information itself could be a useful hint to rank the ground-truth response higher. Therefore, the performance of the baseline model is still better than random guess. Again, role has a bigger effect than topic, and the combination gives the best results, but differences in Recall@K performance are small."
    }, {
      "heading" : "4.4. Qualitative Analysis",
      "text" : "For qualitative analysis, the best R-LDA-CONV model is used to generate role-specific responses, and we examined a number of examples to determine whether the generated response fit into the expected speaker role. We include two examples in Table 4 and Table 5 due to the page limitations. For each case, we have responses generated for each of the possible roles: a further question for the POSTER and a potential solution for the RESPONDER.\nAs we can see from the context part of Table 4, different roles clearly have different behaviors during the conversation. Ignoring the validity of this potential solution, this generated response is consistent with our expectation of the RESPONDER role. The response of POSTER seems quite plausible. The reply of RESPONDER is clearly the right style but more domain information in the topic vector could lead to a more useful solution.\nTable 5 shows another example to demonstrate the difference between the POSTER and RESPONDER roles. In this example, the response for the RESPONDER is not a potential solution but a question to the POSTER. Unlike the generated question for the POSTER role in the previous example, the purpose of RESPONDER’s question is to ask some further details in order to provide a simpler solution. The POSTER’s response also fits well in the local context as well as global topic of ubuntu installation, claiming the difficulty of implementing the POSTER’s suggestion. At the same time, the generated responses also show the necessity of incorporating certain domain knowledge into a domain-specific conversation system, which will be explored in future work."
    }, {
      "heading" : "5. Summary",
      "text" : "We propose an LSTM-based conversation model by incorporating role factor and topic feature to model different word distribution for different roles. We present three models: RCONV, LDA-CONV and R-LDA-CONV, by incorporating role factors and topic features into output layer. We evaluate the model using both perplexity and response ranking. Both RCONV and LDA-CONV outperform the baseline model on all tasks. The model R-LDA-CONV gives the best performance by combining the two components. In addition, the generation\nresults demonstrate the topical coherence and differences in responses associated with different roles. Besides role and topic, our model structure can be generalized to include more supervised information. For future work, we would incorporate supervised domain knowledge into our model to improve the topic relevance of the response."
    }, {
      "heading" : "6. References",
      "text" : "[1] Regina Barzilay, Michael Collins, Julia Hirschberg, and Steve\nWhittaker. The rules behind roles: Identifying speaker role in radio broadcasts. In AAAI/IAAI, pages 679–684, 2000.\n[2] B Hutchinson, B. Zhang, and M. Ostendorf. Unsupervised broadcast conversation speaker role labeling. In ICASSP, pages 5322– 5325, 2010.\n[3] B. Hutchinson B. Zhang, M. A. Marin and M. Ostendorf. Learning phrase patterns for text classification. IEEE Trans. Audio, Speech and Language Processing, 21(6):1180–1189, 2013.\n[4] Alex Marin, Bin Zhang, and Mari Ostendorf. Detecting forum authority claims in online discussions. In Proceedings of the Workshop on Languages in Social Media, pages 39–47. Association for Computational Linguistics, 2011.\n[5] Janet M Fuller. The influence of speaker roles on discourse marker use. Journal of Pragmatics, 35(1):23–45, 2003.\n[6] Hao Wang, Zhengdong Lu, Hang Li, and Enhong Chen. A Dataset for Research on Short-Text Conversations. In EMNLP, pages 935–945, 2013.\n[7] Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. arXiv preprint arXiv:1506.08909, 2015.\n[8] Alan Ritter, Colin Cherry, and William B Dolan. Data-driven response generation in social media. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 583–593. Association for Computational Linguistics, 2011.\n[9] Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. A neural network approach to contextsensitive generation of conversational responses. arXiv preprint arXiv:1506.06714, 2015.\n[10] Lifeng Shang, Zhengdong Lu, and Hang Li. Neural Responding Machine for Short-Text Conversation. arXiv preprint arXiv:1503.02364, 2015.\n[11] Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young. Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems. arXiv preprint arXiv:1508.01745, 2015.\n[12] Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein. Document context language models. arXiv preprint arXiv:1511.03962, 2015.\n[13] Rui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou, and Sheng Li. Hierarchical recurrent neural network for document modeling. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 899–907, 2015.\n[14] Yi Luan, Shinji Watanabe, and Bret Harsham. Efficient learning for spoken language understanding tasks with word embedding based pre-training. In Sixteenth Annual Conference of the International Speech Communication Association, 2015.\n[15] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:993– 1022, 2003.\n[16] Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model. In SLT, pages 234–239, 2012.\n[17] G. Ji and J. Bilmes. Multi-speaker language modeling. In HLT NAACL, 2004.\n[18] Brian Hutchinson. Rank and sparsity in language processing. PhD thesis, 2013.\n[19] Brian Hutchinson, Mari Ostendorf, and Maryam Fazel. A Sparse Plus Low-Rank Exponential Language Model for Limited Resource Scenarios. Audio, Speech, and Language Processing, IEEE/ACM Transactions on, 23(3):494–504, 2015.\n[20] Songfang Huang and Steve Renals. Modeling topic and role information in meetings using the hierarchical Dirichlet process. In Machine Learning for Multimodal Interaction, pages 214–225. Springer, 2008.\n[21] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A persona-based neural conversation model. arXiv preprint arXiv:1603.06155, 2016.\n[22] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045– 1048, 2010.\n[23] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n[24] Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.\n[25] Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. Improved part-of-speech tagging for online conversational text with word clusters. Association for Computational Linguistics, 2013.\n[26] Radim Řehůřek and Petr Sojka. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45– 50, Valletta, Malta, May 2010. ELRA. http://is.muni.cz/ publication/884893/en.\n[27] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics, 2002.\n[28] Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao, and Bill Dolan. deltaBLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets. arXiv preprint arXiv:1506.06863, 2015.\n[29] Oriol Vinyals and Quoc Le. A Neural Conversational Model. arXiv preprint arXiv:1506.05869, 2015.\n[30] Christopher D Manning, Prabhakar Raghavan, Hinrich Schütze, et al. Introduction to information retrieval, volume 1. Cambridge university press Cambridge, 2008."
    } ],
    "references" : [ {
      "title" : "The rules behind roles: Identifying speaker role in radio broadcasts",
      "author" : [ "Regina Barzilay", "Michael Collins", "Julia Hirschberg", "Steve Whittaker" ],
      "venue" : "In AAAI/IAAI,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2000
    }, {
      "title" : "Unsupervised broadcast conversation speaker role labeling",
      "author" : [ "B Hutchinson", "B. Zhang", "M. Ostendorf" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Learning phrase patterns for text classification",
      "author" : [ "B. Hutchinson B. Zhang", "M.A. Marin", "M. Ostendorf" ],
      "venue" : "IEEE Trans. Audio, Speech and Language Processing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Detecting forum authority claims in online discussions",
      "author" : [ "Alex Marin", "Bin Zhang", "Mari Ostendorf" ],
      "venue" : "In Proceedings of the Workshop on Languages in Social Media,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "The influence of speaker roles on discourse marker use",
      "author" : [ "Janet M Fuller" ],
      "venue" : "Journal of Pragmatics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "A Dataset for Research on Short-Text Conversations",
      "author" : [ "Hao Wang", "Zhengdong Lu", "Hang Li", "Enhong Chen" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau" ],
      "venue" : "arXiv preprint arXiv:1506.08909,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Data-driven response generation in social media",
      "author" : [ "Alan Ritter", "Colin Cherry", "William B Dolan" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "A neural network approach to contextsensitive generation of conversational responses",
      "author" : [ "Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "arXiv preprint arXiv:1506.06714,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Neural Responding Machine for Short-Text Conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li" ],
      "venue" : "arXiv preprint arXiv:1503.02364,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems",
      "author" : [ "Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young" ],
      "venue" : "arXiv preprint arXiv:1508.01745,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Document context language models",
      "author" : [ "Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein" ],
      "venue" : "arXiv preprint arXiv:1511.03962,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Hierarchical recurrent neural network for document modeling",
      "author" : [ "Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Efficient learning for spoken language understanding tasks with word embedding based pre-training",
      "author" : [ "Yi Luan", "Shinji Watanabe", "Bret Harsham" ],
      "venue" : "In Sixteenth Annual Conference of the International Speech Communication Association,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan" ],
      "venue" : "Journal of machine Learning research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2003
    }, {
      "title" : "Context dependent recurrent neural network language model",
      "author" : [ "Tomas Mikolov", "Geoffrey Zweig" ],
      "venue" : "In SLT,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Multi-speaker language modeling",
      "author" : [ "G. Ji", "J. Bilmes" ],
      "venue" : "In HLT NAACL,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Rank and sparsity in language processing",
      "author" : [ "Brian Hutchinson" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "A Sparse Plus Low-Rank Exponential Language Model for Limited Resource Scenarios",
      "author" : [ "Brian Hutchinson", "Mari Ostendorf", "Maryam Fazel" ],
      "venue" : "Audio, Speech, and Language Processing, IEEE/ACM Transactions on,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Modeling topic and role information in meetings using the hierarchical Dirichlet process",
      "author" : [ "Songfang Huang", "Steve Renals" ],
      "venue" : "In Machine Learning for Multimodal Interaction,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "A persona-based neural conversation model",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "arXiv preprint arXiv:1603.06155,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur" ],
      "venue" : "INTERSPEECH",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1997
    }, {
      "title" : "Machine learning: a probabilistic perspective",
      "author" : [ "Kevin P Murphy" ],
      "venue" : "MIT press,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Improved part-of-speech tagging for online conversational text with word clusters",
      "author" : [ "Olutobi Owoputi", "Brendan O’Connor", "Chris Dyer", "Kevin Gimpel", "Nathan Schneider", "Noah A Smith" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Software Framework for Topic Modelling with Large Corpora",
      "author" : [ "Radim Řehůřek", "Petr Sojka" ],
      "venue" : "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu" ],
      "venue" : "Association for Computational Linguistics,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2002
    }, {
      "title" : "deltaBLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets",
      "author" : [ "Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan" ],
      "venue" : "arXiv preprint arXiv:1506.06863,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Introduction to information retrieval, volume 1",
      "author" : [ "Christopher D Manning", "Prabhakar Raghavan", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For example, in broadcast news, speaker roles are shown to be informative for discovering the story structures [1]; they impact speaking time and turn-taking [2]; and they are associated with particular phrase patterns [3].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "For example, in broadcast news, speaker roles are shown to be informative for discovering the story structures [1]; they impact speaking time and turn-taking [2]; and they are associated with particular phrase patterns [3].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : "For example, in broadcast news, speaker roles are shown to be informative for discovering the story structures [1]; they impact speaking time and turn-taking [2]; and they are associated with particular phrase patterns [3].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 3,
      "context" : "In online discussions, speaker role is useful for detecting authority claims [4].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Other work shows that in casual conversations, speakers with different roles are likely to use different discourse markers [5].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "With the popularity of social media, such as Twitter, Sina Weibo, and online discussion forums, it is easier to collect conversation text [6, 7].",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "With the popularity of social media, such as Twitter, Sina Weibo, and online discussion forums, it is easier to collect conversation text [6, 7].",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "[8] present a statistical machine translation based conversation system.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] present a way to integrate contextual information via feed-forward neural networks.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] introduce the attention mechanism into an encoderdecoder network for a conversation model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11], where a Dialog-act component is introduced into the LSTM cell to guide the generated content.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Different ways of modeling document-level context has been explored in [12] and [13] based on the LSTM framework.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "Different ways of modeling document-level context has been explored in [12] and [13] based on the LSTM framework.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : "[14] proposed a multi-scale recurrent architecture to incorporate both word and turn level context for spoken language understanding tasks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "In this paper, we use a similar approach as [16], explicitly using Latent Dirichlet Analysis (LDA) as global-context feature to feed into RNNLM.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 16,
      "context" : "Early work on incorporating local context in conversational language modeling is described in [17] conditioned on the most recent word spoken by other speakers.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "[18, 19] improve log-bilinear language model by introducing a multi-factor sparse matrix that could capture speaker role and topic information.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 18,
      "context" : "[18, 19] improve log-bilinear language model by introducing a multi-factor sparse matrix that could capture speaker role and topic information.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 19,
      "context" : "[20] show that language models with role information significantly reduce word error rate in speech recognition.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "propose using an additional vector to LSTM in order to capture personal characteristics of a speaker [21].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "We start building a response generation model [9] by using a recurrent neural network language model (RNNLM) [22].",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "We start building a response generation model [9] by using a recurrent neural network language model (RNNLM) [22].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 22,
      "context" : "We use an LSTM [23] since it is good at capturing long-term dependency, which is an objective for our conversation model.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "Despite the difference between the two models, they can be learned in the same way, which is similar to training a RNNLM [22].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "We choose cross entropy [24] as the loss function `(·, ·), because it is a popular objective function used in training neural language models.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : "In order to capture long-span context of the conversation, inspired by [16], we explicitly include a topic vector representing all previous dialog turns.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "where fLDA (·) is the LDA inference function as in [15].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "We evaluate our model from different aspects on the Ubuntu Dialogue Corpus [7], which provides one million two-person conversations extracted from Ubuntu chat logs.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 24,
      "context" : "We use a Twitter tokenizer [25] to parse all utterances in the conversations.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 25,
      "context" : "We use Gensim [26] for both training and inference.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 26,
      "context" : "For a reasonable evaluation, the n-gram based evaluation metrics including BLEU [27] and ∆BLEU [28] require multiple references for one given context.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 27,
      "context" : "For a reasonable evaluation, the n-gram based evaluation metrics including BLEU [27] and ∆BLEU [28] require multiple references for one given context.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "One the other hand, there are indirect evaluation methods, for example, ranking based evaluation [7, 10] or qualitative analysis [29].",
      "startOffset" : 97,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "One the other hand, there are indirect evaluation methods, for example, ranking based evaluation [7, 10] or qualitative analysis [29].",
      "startOffset" : 97,
      "endOffset" : 104
    }, {
      "referenceID" : 28,
      "context" : "In this paper, we use both ranking-based evaluation (Recall@K [30]) across all models, and leave the n-gram based evaluation for future work.",
      "startOffset" : 62,
      "endOffset" : 66
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we present a conversational model that incorporates both context and participant role for two-party conversations. Different architectures are explored for integrating participant role and context information into a Long Short-term Memory (LSTM) language model. The conversational model can function as a language model or a language generation model. Experiments on the Ubuntu Dialog Corpus show that our model can capture multiple turn interaction between participants. The proposed method outperforms a traditional LSTM model as measured by language model perplexity and response ranking. Generated responses show characteristic differences between the two participant roles.",
    "creator" : "LaTeX with hyperref package"
  }
}