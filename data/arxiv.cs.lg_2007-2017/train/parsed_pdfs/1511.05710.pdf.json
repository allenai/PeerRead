{
  "name" : "1511.05710.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Complex-Valued Gaussian Processes for Regression: A Widely Non-Linear Approach",
    "authors" : [ "Rafael Boloix-Tortosa", "Eva Arias-de-Reyna", "F. Javier Payán-Somet", "Juan J. Murillo-Fuentes" ],
    "emails" : [ "rboloix@us.es." ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper we propose a novel Bayesian kernel based solution for regression in complex fields. We develop the formulation of the Gaussian process for regression (GPR) to deal with complex-valued outputs. Previous solutions for kernels methods usually assume a complexification approach, where the real-valued kernel is replaced by a complexvalued one. However, based on the results in complex-valued linear theory, we prove that both a kernel and a pseudo-kernel are to be included in the solution. This is the starting point to develop the new formulation for the complex-valued GPR. The obtained formulation resembles the one of the widely linear minimum mean-squared (WLMMSE) approach. Just in the particular case where the outputs are proper, the pseudo-kernel cancels and the solution simplifies to a real-valued GPR structure, as the WLMMSE does into a strictly linear solution. We include some numerical experiments to show that the novel solution, denoted as widely non-linear complex GPR (WCGPR), outperforms a strictly complex GPR where a pseudo-kernel is not included."
    }, {
      "heading" : "1 Introduction",
      "text" : "Complex-valued signals are present in the modeling of many systems in a wide range of fields such as optics, electromagnetics, acoustics and telecommunications, among others. The study of linear solutions for complex-valued signals has been addressed in detail in\nThis work was supported by the Spanish government (Ministerio de Educación y Ciencia, TEC2012-38800-C03-02), and by the European Union (FEDER) and Junta de Andalućıa (TIC-155). Copyright 2016 by the authors.\nthe literature. These solutions can be roughly classified into those that assume properness and those that do not. A proper complex random signal is uncorrelated with its complex conjugate (Neeser and Massey, 1993). In the proper scenario, solutions for the realvalued case can be usually rewritten for the complexvalued scenario by just replacing transpose by Hermitian. However, in the improper case, the solutions are more involved and the concept of widely linear is introduced. Accordingly, the linear minimum meansquared error (LMMSE) can be simply rewritten by taking into account the covariance between two random vectors. However, if the outputs are improper, an additional term must be added to include the pseudocovariance (Adali et al., 2011; Schreier and Scharf, 2010). Hence, both covariance and pseudo-covariance must be taken into account.\nMany non-linear tools for complex fields have been developed within the artificial neural network research community (Danilo P. Mandic, 2009; Hirose, 2013). In kernel methods, we may find a few results for kernel principal analysis (Papaioannou and Zafeiriou, 2014), classification (Steinwart et al., 2006) or regression (Ogunfunmi and Paul, 2011; Bouboulis et al., 2012; Tobar et al., 2012; Boloix-Tortosa et al., 2014). These solutions are usually introduced as a complexificacion of the kernel (Bouboulis et al., 2012). In the complexification approach, real-valued kernel tools are adapted to the complex-valued scenario by just rewriting the kernel to deal with complex-valued outputs, and inputs. However, as discussed above for linear solutions, this may suffice for the proper case, but not for the general one. Bearing this in mind, we investigate in this paper how pseudo-covariance matrices should be included in the solutions. In particular, we focus in Gaussian process for regression (GPR).\nGaussian processes (GPs) are kernel Bayesian tools for discriminative machine learning (O’Hagan and Kingman, 1978; Rasmussen and Williams, 2006; Pérez-Cruz et al., 2013). They have been successfully applied to regression, classification and dimensionality reduction.\nar X\niv :1\n51 1.\n05 71\n0v 1\n[ cs\n.L G\n] 1\n8 N\nov 2\n01 5\nGPs can be interpreted as a family of kernel methods with the additional advantage of providing a full conditional statistical description for the predicted variable. Also, hyperparameters can be learned by maximizing the marginal likelihood, avoiding cross-validation. For real fields, GPs applied to regression can be casted as a non-linear MMSE (Pérez-Cruz et al., 2013): they present a similar structure as the LMMSE, where we replace the linear covariances by kernels, and the regularization term also depends on the prior of the weights of the generalized regression (Rasmussen and Williams, 2006). In the following, we propose to develop a new formulation of GPR for complex-valued signals. We start analyzing the prediction for the real and imaginary part separately. Then we merge the results into a complex-valued formulation. In the general improper case, we show that the solution depends on both a kernel and a pseudo-kernel, to propose a widely complex GPR (WCGPR)."
    }, {
      "heading" : "2 Widely linear MMSE (WLMMSE) estimation",
      "text" : "In this section we review the widely concept for complex-valued signals by describing the widely linear minimum mean-squared error (WLMMSE) estimation. The WLMMSE estimation of a zero-mean signal f• : Ω → Cd from the zero-mean measurement y : Ω→ Cn is (Picinbono and Chevalier, 1995; Schreier and Scharf, 2010)\nf̂• = W1y + W2y ∗, (1)\nor by making use of the augmented notation, where the complex signals are stacked on their conjugates:\nf̂• = [ f̂• f̂∗• ] = W y = [ W1 W2 W∗2 W ∗ 1 ] [ y y∗ ] . (2)\nThe widely linear estimator is determined such that the mean-squared error is minimized, i.e., the error between the augmented estimator and the augmented signal, e = f̂• − f• , must be orthogonal to the augmented measurement, y, (Picinbono and Chevalier, 1995; Schreier and Scharf, 2010):\nW = Rf•yR −1 yy = [ Rf•y R̃f•y R̃∗f•y R ∗ f•y ] [ Ryy R̃yy R̃∗yy R ∗ yy ]−1 ,\n(3)\nwhere Ryy is the augmented covariance matrix of the\nmeasurements, with covariance matrix Ryy = E [ yyH ] and pseudo-covariance or complementary covariance matrix R̃yy = E [ yy> ] . Similarly, Rf•y is composed\nby Rf•y = E [ f•yH ] and R̃f•y = E [ f•y> ] . Now, by using the matrix-inversion lemma in (3), the WLMMSE\nestimation yields\nf̂• = [ Rf•y − R̃f•yR−∗yyR̃∗yy ] P−1yyy\n+ [ R̃f•y −Rf•yR−1yyR̃yy ] P−∗yyy ∗, (4)\nwhere Pyy = Ryy − R̃yyR−∗yyR̃∗yy is the error covariance matrix for linearly estimating y from y∗. Finally, the error covariance matrix Q = E [ eeH ] of the error vector e = f̂• − f• is (Schreier and Scharf, 2010)\nQ = Rf•f• − [ Rf•y − R̃f•yR−∗yyR̃∗yy ] P−1yyR H f•y\n− [ R̃f•y −Rf•yR−1yyR̃yy ] P−∗yyR̃ H f•y. (5)\nIt is important to note that the WLMMSE compared to the strictly linear MMSE commonly used fully exploits the dimensions of the problem, including the real and imaginary parts of every signal involved. Just in the case where the error of the LMMSE estimate is orthogonal to y∗ (Schreier and Scharf, 2010),\nR̃f•y −Rf•yR−1yyR̃yy = 0, (6)\nand both estimators provide the same solution f̂• = Rf•yR −1 yyy."
    }, {
      "heading" : "3 Composite Gaussian Processes for Regression",
      "text" : "Once we have defined the WLMMSE we next aim at developing the formulation for the GPR, to later relate both results. We first face the case where real and imaginary parts are estimated separately, to later merge the solutions into one complex-valued expression in the next section.\nGP for regression can be presented as a nonlinear regressor that expresses the input-output relation through function f(x), known as latent function, that follows a GP and underlies the regression problem\ny = f(x) + , (7)\nwhere the input vector is x ∈ Cd, and the error is modeled as additive zero-mean Gaussian noise.\nGiven a training set D = {(x(i), y(i))|i = 1, ..., n} = {Xn,yn}, we aggregate the input vectors as columns in matrix Xn and the outputs are stacked in the complex column vector y = [y(1), ..., y(n)] > = f(Xn) +\n= f + . The latent function provides the multidimensional Gaussian complex-valued random vector f = [f(x(1)), ..., f(x(n))] > , where f(x(i)) ∈ C. The goal of the regression is to predict the value of f• , [f(x•(1)), ..., f(x•(m))] > for new inputs X•m = [x•(1), ...,x•(m)].\nRafael Boloix-Tortosa, Eva Arias-de-Reyna, F. Javier Payán-Somet, Juan J. Murillo-Fuentes\nThe straightforward way of applying GPR to complex signals is to process a composite vector where we append the imaginary values to the real ones. Then two GPs can be learned, one for the real part and another for the imaginary part of the output, either independently or using a multi-output learning or vector scheme (Micchelli and Pontil, 2005; Boyle and Frean, 2005; Álvarez et al., 2012). The model in (7) can be rewritten in composite form as\nyR = [ yr yj ] = [ fr(Xn) fj(Xn) ] + [ r j ] = fR(Xn) + R,\n(8)\nwhere yR, fR and R are the composite (real) vectors for the outputs, the latent function and the noise, respectively. We assume that the real additive noise R is i.i.d. Gaussian with zero mean and variance Σ R . If we assume a zero mean process and specify the covariance function of the process kR(xi,xl), we can write out the corresponding 2n × 2n covariance matrix KR(Xn,Xn) elementwise from Xn, and generate the Gaussian prior fR ∼ N (0,KR(Xn,Xn)). Therefore, the observations are also Gaussian distributed, yR ∼ N (0,KR(Xn,Xn) + Σ R) = N (0,CR), and the joint distribution of the training outputs, yR, and the test predictions fR• = fR•(X•) according to the prior yield [\nyR fR•\n] ∼ N ( 0, [ CR KR(Xn,X•)\nKR(X•,Xn) KR(X•,X•)\n]) .\n(9)\nThe conditional distribution for the predictions fR• given the observations yields the predictive distribution\nfR•|X?,X,yR ∼ N (µfR• ,ΣfR•) , (10) and we arrive at the key predictive equations for GPR, the mean and variance given by:\nµfR• = KR(X•,Xn)C −1 R yR, (11) ΣfR• = KR(X•,X•)−KR(X•,Xn)C−1R KR(Xn,X•). (12)\nNote that in the predictions (11) and (12) we have matrices Krr, Krj, Kjr and Kjj, that are block matrices in the vector kernel matrix\nKR(Xn,Xn) = [ Krr(Xn,Xn) Krj(Xn,Xn) Kjr(Xn,Xn) Kjj(Xn,Xn) ] .\n(13)"
    }, {
      "heading" : "4 Widely Complex Gaussian Process Regression",
      "text" : "The model in (7) can also be rewritten in the augmented vector notation by stacking the complex sig-\nnals on their conjugates:\ny = [ y y∗ ] = [ f(Xn) f∗(Xn) ] + [ ∗ ] = f(Xn) +\n= f + , (14)\nwhere y, f and are the augmented vectors for the outputs, the latent function vector and the noise, respectively. There exists a simple relation between the composite vector (8) and the augmented vector (14): y = TyR, where\nT = [ In jIn In −jIn ] ∈ C2n×2n, (15)\nand TTH = THT = 2I2n. Also, = T R and f = TfR. This simple transformation allows us to calculate the augmented mean vector and the augmented covariance matrix of the prediction f• from (11) and (12), which are µ\nf• = TµfR• and Σf• = TΣfR•T\nH, respectively:\nµ f• = [ µf• µ∗f• ] = K(X•,Xn)C −1y, (16)\nΣf• = K(X•,X•)−K(X•,Xn)C −1K(Xn,X•),\n(17)\nwhere the augmented covariance matrix of the augmented observations, C = E [ yyH ] = TCRT\nH, is defined as\nC =\n[ C C̃\nC̃∗ C∗\n] = K(Xn,Xn) + Σ\n=\n[ K(Xn,Xn) K̃(Xn,Xn)\nK̃∗(Xn,Xn)∗ K∗(Xn,Xn)\n] + [ Σ Σ̃ Σ̃∗ Σ ∗ ] .\n(18)\nMatrix Σ = TΣ RT H is the augmented covariance matrix of the noise, and K(Xn,Xn) = TKR(Xn,Xn)T\nH is the augmented covariance matrix of f = f(Xn), composed by the covariance matrix K(Xn,Xn) = E [ f(Xn)f H(Xn) ]\nand the pseudo-covariance or complementary covariance matrix K̃(Xn,Xn) = E [ f(Xn)f >(Xn) ] . Notice that in the general complex case, two functions must be defined to calculate matrices K(Xn,Xn) and K̃(Xn,Xn), respectively, i.e., we need a covariance function or kernel k(xi,xl), and a pseudo-covariance function or pseudo-kernel, k̃(xi,xl). Using the matrix-inversion lemma to find C−1 in (16) yields the mean of the prediction\nµf• = [ K(X•,Xn)− K̃(X•,Xn)C−∗C̃∗ ] P−1y\n+ [ K̃(X•,Xn)−K(X•,Xn)C−1C̃ ] P−∗y∗, (19)\nwhere P = C− C̃C−∗C̃∗.\nComplex-Valued Gaussian Processes for Regression: A Widely Non-Linear Approach\nAlso, the covariance matrix yields\nΣf• = K(X•,X•)\n− [ K(X•,X)− K̃(X•,X)C−∗C̃∗ ] P−1K(X,X•) − [ K̃(X•,X)−K(X•,X)C−1C̃ ] P−∗K̃∗(X,X•).\n(20)"
    }, {
      "heading" : "4.1 Relation to the widely linear MMSE (WLMMSE) estimation",
      "text" : "At this point it is important to remark the similarity\nof the widely linear MMSE estimation (WLMMSE)\nwith the complex GPR developed above. Notice the\nsimilarity between the WLMMSE estimation in (4) and the mean of the complex GPR prediction in (19). The role of matrices Ryy, R̃yy, Rf•y and R̃f•y in the WLMMSE estimation in (4) is played in the complex GPR prediction in (19) by C, C̃, K and K̃, respectively, i.e., covariances and pseudo-covariances are replaced by kernels and pseudo-kernels. Therefore, the mean of the proposed complex GPR prediction can be cast as a nonlinear extension to the widely linear MMSE estimation, and we may denote it as widely nonlinear complex GPR (WCGPR). The same kind of similarity is found between the error covariance matrix Q in (5) and the WCGPR predictive covariance matrix in (20).\nIn Section 2 we stated that the WLMMSE estimate and the strictly linear MMSE estimate are identical, and equal to f̂• = Rf•yR −1 yyy, if and only if (6) holds. Similarly, in the context of WCGPR the prediction mean (19) simplifies to\nµf• = [ K(X•,Xn)− K̃(X•,Xn)C−∗C̃∗ ] P−1y\n= K(X•,Xn)C −1y, (21)\nif [ K̃(X•,Xn)−K(X•,Xn)C−1C̃ ] = 0. (22)\nThis takes place when, e.g. both f and are proper. In this scenario, since both K̃ and C̃ cancel the second term in (19) vanishes. This case is analogous to the strictly linear MMSE and this solution for proper complex GPR, that assumes a null pseudo-covariance, could be denoted as a strictly nonlinear complex GPR. This is the case studied in Boloix-Tortosa et al. (2014). Note that, in the same way that the WLMMSE compared to the strictly linear MMSE fully exploits the dimensions of the problem, the WCGPR presented in this paper also fully exploits the dimensions of the problems, while the complex GPR for the proper case in Boloix-Tortosa et al. (2014) does not. This advantage is highlighted in the next section devoted to experiments.\nAlso, the covariance matrix yields\nΣf• = K(X•,X•)\n− [ K(X•,X)− K̃(X•,X)C−∗C̃∗ ] P−1K(X,X•) − [ K̃(X•,X)−K(X•,X)C−1C̃ ] P−∗K̃∗(X,X•).\n(20)\n4.1 Relation to the widely linear MMSE (WLMMSE) estimation\nAt this point it is important to remark the sim larity of the widely linear MMSE estimation (WLMMSE) with the complex GPR developed above. Notice the similarity between the WLMMSE estimation in (4) and the mean of the complex GPR predicti n in (19). The role of matrices Ryy, R̃yy, Rf•y and R̃f•y in the WLMMSE estimation in (4) is played in the complex GPR prediction in (19) by C, C̃, K and K̃, respectively, i.e., covaria ces and seudo-covariances are replaced by kernel and pseudo-kernels. Therefore, the mean of the proposed complex GPR prediction can be cast as a nonlinear extension to the widely linear MMSE estimation, and we may denote it as widely nonlinear complex GPR (WCGPR). The same kind of similarity is found between the error covariance matrix Q in (5) and the WCGPR predictive covariance matrix in (20).\nIn Section 2 we stated that the WLMMSE estimate and the strictly linear MMSE estimate are identical, and equal to f̂• = Rf•yR −1 yyy, if and only if (6) holds. Similarly, in the context of WCGPR the prediction mean (19) simplifie o\nµf• = [ K(X•,Xn)− K̃(X•,Xn)C−∗C̃∗ ] P−1y\n= K(X•,Xn)C −1y, (21)\nif [ K̃(X•,Xn)−K(X•,Xn)C−1C̃ ] = 0. (22)\nThis takes place when, e.g. both f and are proper. In this scenario, since both K̃ and C̃ cancel the second term in (19) vanishes. This case is analogous to the strictly linear MMSE and this solution for proper complex GPR, that assumes a null pseudo-covariance, could be denoted as a strictly nonlinear complex GPR. This is the case studied in Boloix-Tortosa et al. (2014). Note that, in the same way that the WLMMSE compared to the strictly linear MMSE fully exploits the dimensions of the problem, the WCGPR presented in this paper also fully exploits the dimensions of the problems, while the complex GPR for the proper case in Boloix-Tortosa et al. (2014) does not. This advantage is highlighted in the next section devoted to experiments.\nFigure 1: Widely linear filtering model to generate a complex Gaussian process.\n5 Numerical Experiments\nWe propose the following example where we generated a sample function of a complex Gaussian process, then added a complex Gaussian noise to it, randomly chose training samples and tried to learn the sample function of the process by using (19). In order to generate a complex Gaussian process we followed the procedure in Picinbono and Bondon (1997), and the sample function of the process f(x) can be written as the output of a widely linear filter driven by complex proper white, zero-mean, unit-variance noise, S(x) = Sr(x)+ jSj(x):\nf(x) = (hr1(x) + jhj1(x)) ? S(x)\n+ (hr2(x) + jhj2(x)) ? S ∗(x). (23)\nThis procedure, sketched in Figure 1, allows for the generation of both proper or improper Gaussian processes with the desired second order statistics. In this example, the filters used were parameterized exponentials:\nh(x) = v exp ( −x Hx\nγ\n) , (24)\nwhere γ = 0.6 and v = 4 for hr1(x), v = 5 for hj1(x), v = 1 for hr2(x), and v = −3 for hj2(x). We generated 100 samples in [−5, 5] for both the real and the imaginary parts of the inputs to get a set of 10000 complex-valued inputs, and the filters were normalized to have unit norm. The real part of sample function f(x) obtained is shown in Figure 2 (top).\nComplex Gaussian noise with variance σ2 and complementary variance ρσ2 was added to represent measurement uncertainty. In this example we set σ = 0.0165 and ρ = 0.8 exp(j3π/2). A set of n = 500 training noisy samples were randomly chosen. These samples have been depicted as circles in Figure 2 (top).\nWe calculated the mean (19) and variance (20) of the predictive distribution using the training samples. The real part of the predictive mean (19) is depicted in Figure 2 (bottom). The mean squared error of the"
    }, {
      "heading" : "5 Numerical Experiments",
      "text" : "We propose the following example where we generated a sample function of a complex Gaussian process, then added a complex Gaussian noise to it, randomly chose training samples and tried to learn the sample function of the process by using (19). In order to generate a complex Gaussian process we followed the procedure in Picinbono and Bondon (1997), and the sample function of the process f(x) can be written as the output of a widely linear filter driven by complex proper white, zero-mean, unit-variance noise, S(x) = Sr(x) + jSj(x):\nf(x) = (hr1(x) + jhj1(x)) ? S(x)\n+ (hr2(x) + jhj2(x)) ? S ∗(x). (23)\nThis procedure, sketched in Figure 1, allows for the generation of both proper or improper Gaussian processes with the desired second order statistics. In this example, the filters used were parameterized exponentials:\nh(x) = v exp ( −x Hx\nγ\n) , (24)\nwhere γ = 0.6 and v = 4 for hr1(x), v = 5 for hj1(x), v = 1 for hr2(x), and v = −3 for hj2(x). We generated 100 samples in [−5, 5] for both the real and the imaginary parts of the inputs to get a set of 10000 complex-valued inputs, and the filters were normalized to have unit norm. The real part of sample function f(x) obtained is shown in Figure 2 (top).\nComplex Gaussian noise with variance σ2 and complementary variance ρσ2 was added to represent measurement uncertainty. In this example we set σ = 0.0165 and ρ = 0.8 exp(j3π/2). A set of n = 500 training noisy samples were randomly chosen. These samples have been epicted as circles in Figure 2 (top).\nWe calculated the mean (19) and variance (20) of the predictive distributio using the training samples. The\nRafael Boloix-Tortosa, Eva Arias-de-Reyna, F. Javier Payán-Somet, Juan J. Murillo-Fuentes\nreal part of the predictive mean (19) is depicted in Figure 2 (bottom). The mean squared error of the estimation was 10 log10MSE = −12.6 dB, computed for 10000 inputs. In Figure 3 a slice of the surface in Figure 2 is shown. The real part of the sample function of the process is plotted (black line) versus the real part of the input, the imaginary part of the input (x) was fixed to the value 0.4545. The real part of the prediction in (19) is depicted in red line, along with the grey shaded area that represents the pointwise mean plus and minus two times the standard deviation. The blue circles mark the training samples.\nWe have also compared the predictive capabilities of the proposed widely complex GPR in (19) with that of the prediction for the proper case in (21). In Figure 3 the mean of the prediction in (21) is plotted as a blue line. It is shown that the proposed WGPR prediction is always closer to the actual value of f(x) than the prediction for the proper case, as expected.\nFinally, in Figure 4 we compare the mean square error of the estimation of the same f(x) as before for the proposed WCGPR (19) and the proper case estimation (21) versus the number of training samples. The noise variance was increased to σ = 0.165 in order to check the good behavior of the proposed complexvalued regressor under a ten-fold higher noise level.\nAll other parameters were set to the same values used in the previous experiments. It can be seen in Figure 4 that the proposed widely complex GPR performs better that the proper case estimation, with a reduction in the MSE close to 2 dB at its best."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have shown that developing complex-valued nonlinear kernel based solutions does not suffice to replace kernels by its complex versions. In the general case, another kernel matrix, the so-called pseudokernel matrix must be included. We have focused on GPR to develop a novel formulation, denoted as\nwidely non-linear complex-valued GPR (WCGPR), after the widely linear MMSE, as it exhibits a quite similar structure. The pseudo-kernel or pseudo-covariance matrix in this formulation models the covariance between the outputs and their conjugates. If this pseudo-covariance cancels, i.e. the outputs are proper, WCGPR yields a strict non-linear complex formulation, as the WLMMSE yields a strict LMMSE. Other special cases can be also derived from this general solution. Through numerical experiments we show that the proposed formulation outperforms the strictly nonlinear complex-valued GPR when learning a complex Gaussian process generated using widely linear filters."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "In this paper we propose a novel Bayesian kernel based solution for regression in complex fields. We develop the formulation of the Gaussian process for regression (GPR) to deal with complex-valued outputs. Previous solutions for kernels methods usually assume a complexification approach, where the real-valued kernel is replaced by a complexvalued one. However, based on the results in complex-valued linear theory, we prove that both a kernel and a pseudo-kernel are to be included in the solution. This is the starting point to develop the new formulation for the complex-valued GPR. The obtained formulation resembles the one of the widely linear minimum mean-squared (WLMMSE) approach. Just in the particular case where the outputs are proper, the pseudo-kernel cancels and the solution simplifies to a real-valued GPR structure, as the WLMMSE does into a strictly linear solution. We include some numerical experiments to show that the novel solution, denoted as widely non-linear complex GPR (WCGPR), outperforms a strictly complex GPR where a pseudo-kernel is not included.",
    "creator" : "LaTeX with hyperref package"
  }
}