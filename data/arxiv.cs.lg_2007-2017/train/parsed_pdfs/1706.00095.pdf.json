{
  "name" : "1706.00095.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Using GPI-2 for Distributed Memory Paralleliziation of the Caffe Toolbox to Speed up Deep Neural Network Training",
    "authors" : [ "Martin Kuehn", "Janis Keuper" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Deep Neural Network (DNN) are currently of great interest in research and application. The training of these networks is a compute intensive and time consuming task. To reduce training times to a bearable amount at reasonable cost we extend the popular Caffe toolbox for DNN with an efficient distributed memory communication pattern. To achieve good scalability we emphasize the overlap of computation and communication and prefer fine granular synchronization patterns over global barriers. To implement these communication patterns we rely on the the ”Global address space Programming Interface” version 2 (GPI-2) communication library. This interface provides a light-weight set of asynchronous one-sided communication primitives supplemented by non-blocking fine granular data synchronization mechanisms. Therefore, CaffeGPI is the name of our parallel version of Caffe. First benchmarks demonstrate better scaling behavior compared with other extensions, e.g., the IntelTMCaffe. Even within a single symmetric multiprocessing machine with four graphics processing units, the CaffeGPI scales better than the standard Caffe toolbox. These first results demonstrate that the use of standard High Performance Computing (HPC) hardware is a valid cost saving approach to train large DDNs. I/O is an other bottleneck to work with DDNs in a standard parallel HPC setting, which we will consider in more detail in a forthcoming paper."
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep Neural Network (DNN) architectures have improved considerably the accuracy in data classification opening the door for a plethora of use cases in image classification, speech recognition or semantic text understanding. However, the training of DNNs is a very compute intensive task. So, the raising interest in these architectures created a tremendous demand for compute resources which is further intensified by a race to greater sizes of DNNs.\nAnother important factor is the time necessary for training DNNs. To train a popular architecture like, e.g., GoogLeNet can easily take several days on a Graphics Processing Unit (GPU). To make things worse the training usually is an iterative process of trials and modifications in the DNN architecture. So, keeping training times tolerably is a key requirement to actually apply DNNs in research and industry.\nIn response to this challenge, hardware vendors brought to market special hardware, e.g., the DGX-1 sold by NVIDIA or the S822LC (”Minsky”) sold by IBM. They try to integrate as much compute power in terms of floating point operations per second (FLOP/s) as possible in a single compute node. While this special hardware comes also with a special price it is also not as flexible to apply to other problems in computer science. On the other hand, there already exists a plethora of compute systems in the world used for High Performance Computing\nar X\niv :1\n70 6.\n00 09\n5v 2\n[ cs\n.L G\n] 1\n(HPC) [1]. Usually these consist of hundreds of nodes usually connected with high bandwidth, low latency networks like InfiniBand networks. A considerable number of them are even equipped with GPU accelerators.\nOur aim is to make these HPC resources available to the field of data analytics. The advantage of this approach is twofold. First, it provides the data analytics community access to the needed hardware quickly because it is already up and running. Secondly, in the long run it avoids the separation of resources that are used in the field of data analytics and in the traditional HPC field. This not only simplifies the buildup and the operation of these compute resources but it also increases the flexibility to mix data analytics and other compute jobs on the same cluster. The latter increases the load factor and reduces costs.\nThe toolbox Caffe [4] is very popular to build and train DNNs. It is easy to use and a wealth of predefined DNNs are available to get to results quickly. The popular Convolutional Neural Networks usually have a performance advantage on Caffe versus the TensorFlow framework. However, the parallelization of Caffe, as it is provided in its original version, is limited to a single Symmetric Multi Processing (SMP) compute node. IntelTMdeveloped a MPI based prallel version of Caffe, which we will use as benchmark. Our goal is to provide a parallel version of Caffe based on a Partitioned Global Address Space (PGAS) Application Programming Interface (API), that can exploit one sided commmunication more efficiently\nthan MPI. This speeds up the DNN training by distributing the computational load on several servers equipped with a GPU or a set of powerful Central Processing Units (CPUs).\nTo ensure high scalability it is crucial to organize the inter node communication efficiently. To exploit the precious network bandwidth to the maximum it is important to overlay as much of the inter node communication with computation as possible. The Global address space Programming Interface version 2 (GPI-2) library [5] developed by our group provides an efficient interface for one sided, asynchronous data transfers. This interface is the basis of an efficient and well scaling distributed memory parallelization of the Caffe toolbox. So, we call our parallel version of Caffe CaffeGPI.\nThe rest of the text is organized as follows. In Section 2 details on our parallelization approach and communication pattern are given. In Section 3 the implementation of the communication pattern is described. In Section 4 our first benchmarks are presented and in Section 5 the results are discussed."
    }, {
      "heading" : "2 Parallelization Approach",
      "text" : "Although the numerical operations involved in the training of DNNs are typically basic linear algebra operations on dense matrices, which have a rather good FLOP to byte ratio, the bandwidth of the communication networks must be used efficiently to keep the latency low between the training iterations."
    }, {
      "heading" : "2.1 Stochastic Gradient Descend",
      "text" : "The Stochastic Gradient Descend (SGD) [6] algorithm is a standard for training DNN and is implemented in the Caffe toolbox. It is the standard choice for such famous DNNs like, e.g., GoogLeNet or AlexNet. The training data xi is partitioned into batches which are iteratively applied to the DNN to modify the weights w which are also called the model. Each iteration consists of a forward propagation and a backward propagation. In the forward propagation, the data batch is inferred while during the backward propagation a gradient on the weights is computed and later on applied on the model.\nRequire: > 0 1: for all t = 0 . . . T do 2: randomly draw batch M ← B samples from X 3: Init∆wt = 0 4: for all x ∈M do 5: aggregate update ∆w ← ∂wxj(wt) 6: end for 7: update wt+1 ← wt − ∆wt 8: end for 9: Return wT\nFigure 1. Mini-Batch SGD with samples X = {x0, . . . , xm}, iterations T , step-size , batch size B\nThe two basic parallelization strategies commonly used for SGD are data parallelism or model parallelism [7]. In the model parallelism, the net and its weights are distributed among different ranks of the parallel computer, while the batch stays the same for all ranks. In data parallelism the DNN is duplicated on every rank while the data batch is split into equal fractions for each rank. The computed gradients on each rank are aggregated and then applied to the model which is distributed back to all the ranks. We concentrate on the data parallelism approach here which is favorable for DNN containing many convolutional layers like the popular AlexNet or GoogLeNet. An advantage of this approach is a constant aggregated IO turnover over the ranks fetching the training data.\nAs the DNN is organized in layers we write the model of layer l in the iteration k as w(l,k), the partial gradient on rank r as ∆w(l,k)r . Using this notation the iterative data parallel SGD algorithm can be shortly noted as\nwl,k+1 := wl,k − s∑\nr=1\n∆wl,kr . (1)\nHigher order terms are neglected here without loss of generality. These terms usually depend on the history of the models wl,k, which are inherently broadcasted to all the ranks to perform the next forward propagation phase."
    }, {
      "heading" : "2.2 Design Principles",
      "text" : "To exploit the full potential of this parallelization approach the data transfers of the model and gradients have\nto be designed very carefully, since the total amount of data that has to be transferred during each iteration grows with the number of compute ranks.\nAs a consequence a typical HPC interconnect, like EDR InfiniBand, should be used to provide enough bandwidth to exploit the scalability of the problem. These networks are commonly used to connect the nodes in current HPC clusters. Another important aspect is to use these interconnects efficiently. Foremost this means to overlap computation and communication to avoid that their run times add up fully. Instead both should ideally take place at the same time so that no additional time for communication is necessary. Unfortunately this is not the case for the standard Caffe which enters separate phases for computation and communication (see Figure 2). The second principle is to avoid global synchronization points between the ranks. Instead data dependencies are enforced locally and very fine granular. The communicated data chunks are pipelined to keep the ranks busy most of the time with either computing, communication or both. Barriers to synchronize the ranks are avoided and used only where absolutely necessary. However, it is important to note that our approach strictly regards all the data dependencies immanent of the SGD algorithm unlike the so called ”asynchronous SGD” algorithms described in literature [8]."
    }, {
      "heading" : "2.3 Overlapping Computation and Communication",
      "text" : "During the backward propagation the partial gradients ∆w (l,k) r are computed separately for each layer and in an inverse order. The next read access to the model of a certain layer is in the forward propagation of the following time step. So, especially for the layers at the bottom of\nthe DNN quite some time is available to reduce the partial gradients and to update the model of that layer (see Figure 4 for illustration).\nThe communication pattern is turn based, one layer of the DNN per turn. In each turn, a partial gradient ∆w(l,k)r is computed for the respective layer li and forwarded to the receiving ranks. Incoming partial gradients of previous layers from other ranks are checked, aggregated if available and forwarded to receiving ranks as well. In the same manner, updates on the model of previous layers are forwarded to the receiving nodes. In all these cases the data transfers are only triggered but not awaited for conclusion.\nIn the finalization phase, all loose ends of the communication to the local rank r are finalized. After that the local instance of the DNN is ready for the next iteration. Please note that even between iterations there is no global barrier applied. So, every rank that received a complete update of the model can immediately start with the next training phase without having to wait for other ranks to get their full update.\nParticularly, this scheme allows to overlap the computation of the gradient ∆w(l,k)r with the communication of gradients and models of previous layers. And it avoids global barriers between the ranks."
    }, {
      "heading" : "3 Implementation",
      "text" : "Our parallel version of the Caffe framework is done as minimally invasive as possible. Basically, the setup routine of the DDN and the backward propagation routines are modified. Additionally, a layer-wise model update is introduced in the Solver class.\nDuring the backward propagation over the layers, the calculations of the gradients are followed by calls to the newly added communication routines to reduce the local gradients and to broadcast the updated model."
    }, {
      "heading" : "3.1 Basics of GPI-2",
      "text" : "To implement the overlapping, one sided communication pattern the GPI-2 library has been used, which is a PGAS communication API for C/C++ and Fortran applications. Fulfilling the Global Address Space Programming Interface (GASPI) specification (see webpage [9]), it provides truly asynchronous one-sided communication primitives supplemented by a non-blocking light-weight and fine granular data synchronization mechanism. GPI-2 exploits interconnects supporting Remote Direct Memory Access (RDMA) as, e.g., InfininBand networks. On these networks the data transfers can be almost completely offloaded to the network infrastructure reducing the load on the computational resources to a minimum. No intermediate copies are necessary which saves memory bandwidth. Apart from that, GPI-2 is a very lean library and gives the user more direct control over the particular data transfers as, e.g. the usual Message Passing Interface (MPI) library.\nAll these features make GPI-2 a perfect match to implement the overlap of computation and inter node communication in Caffe. Being an open source library GPI-2 can be downloaded at [5]."
    }, {
      "heading" : "3.2 Implementation of Data Transfers",
      "text" : "In the Caffe data structures that define the DNN the arrays that carry the model and the gradient data are placed inside GPI-2 data segments. Providing the gaspi segment use function, GPI-2 cooperates perfectly with special memory regions in Caffe which are allocated by cudaMallocHost to enhance data transfers to the GPU. The GPI-2 library allows to write remotely (inter node) and directly to these segments. All the data transfers are triggered with a gaspi write notify call to the library. The receiver of the data chunk checks on the respective notification and acts on the received data if necessary.\nThe gradient data is reduced in a reduction tree pattern aggregating the final gradient on the master rank. The master rank performs the update on the current model to compute an update for the next iteration. Then the updated model is broadcasted to all the other ranks in another tree pattern. The gradient data is always sent from higher rank numbers to lower rank numbers while the updated model is broadcasted from lower rank numbers to higher rank numbers. As the size of the gradients equals the size of the models we take advantage of the full duplex feature of switched networks.\nThe reduction and the broadcast trees are build from scratch to keep control over the tree topolgy and to interwine closely the communication pattern with the computation."
    }, {
      "heading" : "3.2.1 Reduction of the Gradient",
      "text" : "To reduce the local gradients in a binomial, tree each rank checks for incoming gradient data in its receive buffer. If available the gradient data is reduced (added to) with its own gradient data of that layer. If the rank has a receiver in the tree pattern the reduced gradient is forwarded to this receiver using a call to gaspi write notify. The communication tasks are performed once in the loop over the layers as depicted in section 2.3. The gradient data is processed as available. No waiting takes place for a specific data chunk."
    }, {
      "heading" : "3.2.2 Broadcast of the model",
      "text" : "The broadcast of the model is performed similarly as the gradient reduction. As no reduction steps are necessary the incoming model data from the sender is just forwarded to its receiver ranks using a call to gaspi write notify."
    }, {
      "heading" : "4 First Benchmarks",
      "text" : "To evaluate our parallelization approach we start to compare CaffeGPI with the original Caffe on a SMP machine containing 4 GPUs. This setup is quite similar to specialized workstations produced to train DNNs. The original Caffe uses the standard thread parallel communication pattern in this benchmark. In the CaffeGPI benchmark, 4 independent processes are started on the same node, one for each GPU, communicating through the network card. The SMP machine is a single node of the Taurus cluster at the ZIH in Dresden containing 2 IntelTMXeon E5-2680v3 CPUs, 64GB Random-access memory (RAM) and 4 NVIDIA K80 GPU. The network is InfiniBand FDR. As DNN we choose the familiar AlexNet. Figure 5 depicts the scaling behavior at 1, 2, and 4 GPU.\nOn a first glance our CaffeGPI should have a small disadvantage in this benchmark compared with the standard Caffe toolbox because it needs to communicate via memory copies inside the node. At a closer look the reduction\nof the partial gradients puts a lot of load on the memory system, the PCIe bus and the QuickPath Interconnect between the CPUs. In both cases, memory copies between GPU-RAM and CPU-RAM have to be performed. However in the benchmark of the standard Caffe all the GPUs execute their communication phase at the same time leaving precious bandwidth idle during the computation phase. In the CaffeGPI benchmark, the memory copies between GPU-RAM and CPU-RAM are not overlapped but interleaved with computation of the partial gradients. As not all the GPUs perform their copies at the same time, the memory transfers are distributed over a longer period of time. The memory copies across the two sockets are performed by the network card and overlapped with the computation. Finally our implementation can demonstrate superior scaling behavior as depicted in Figure 5. The second benchmark compares CaffeGPI to IntelTMCaffe (see webpage [10]), a distributed memory extension of Caffe based on the MPI. In this benchmark, 1, 2 or 4 nodes of the same cluster were used, but only one GPU per node. Here distributed memory data transfers are performed in both scenarios. The benchmark in Figure 6 demonstrates a superior scaling behavior of our implementation in comparison to the IntelTMCaffe framework. A speedup of 2.4 on 4 nodes compared to one node delivers a reasonable performance figure to train AlexNet in a reasonable time frame on standard HPC hardware."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "The preliminary benchmarks presented in this work demonstrate that our distributed memory communication pattern implemented in the CaffeGPI framework scales well on four distributed memory nodes equipped with one GPU per node. The total performance is similar or even better than using the standard SMP-parallel approach of Caffe on a SMP node equipped with 4 GPU. Even on this single SMP node with 4 GPUs, our CaffeGPI scales much better than the standard Caffe framework.\nThese results demonstrate that data scientists can rely on available HPC compute resources to train their DNNs in a reasonable time frame. Our toolbox CaffeGPI can help to satisfy the need for more compute power in the area of data science without having to buy vast amounts of specialized hardware, which is difficult to apply economically for other tasks in computer science.\nWe will continue to benchmark various hardware configurations, e.g. a NVIDIA DGX-1 or an IBM S822LC (”Minsky”). Further benchmarks will be done to analyze the communication pattern introduced in CaffeGPI. Alternative patterns will be evaluated that might improve the reduction and the broadcast operations. We will also extend our benchmarks on more DNNs and to wider batch sizes to evaluate their scaling behavior and to find performance optimized training parameters."
    }, {
      "heading" : "Acknowledgment",
      "text" : "The authors thank the Center for Information Services and High Performance Computing (ZIH) at TU Dresden for generous allocations of computer time. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research."
    } ],
    "references" : [ {
      "title" : "Imagenet classification with deep convolutional neural  networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 1097–1105.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1–9.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia" ],
      "venue" : "arXiv preprint arXiv:1408.5093, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "Proceedings of COMPSTAT’2010. Springer, 2010, pp. 177–186.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Distributed training of deep neural networks: Theoretical and practical limits of parallel scalability",
      "author" : [ "J. Keuper", "F.-J. Preundt" ],
      "venue" : "Proceedings of the Workshop on Machine Learning in High Performance Computing Environments, ser. MLHPC ’16. Piscataway, NJ, USA: IEEE Press, 2016, pp. 19–26. [Online]. Available: https://doi.org/10.1109/MLHPC.2016.6",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Asynchronous parallel stochastic gradient descent: A numeric core for scalable distributed machine learning algorithms",
      "author" : [ "J. Keuper", "F.-J. Pfreundt" ],
      "venue" : "Proceedings of the Workshop on Machine Learning in High-Performance Computing Environments, ser. MLHPC ’15. New York, NY, USA: ACM, 2015, pp. 1:1–1:11. [Online]. Available: http://doi.acm.org/10.1145/2834892.2834893",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "AlexNet [2]: time per iteration 2s 0.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "6s time till convergence 250h 112h 13h 75h GoogLeNet [3]: time per iteration 1.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "The toolbox Caffe [4] is very popular to build and train DNNs.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "The Stochastic Gradient Descend (SGD) [6] algorithm is a standard for training DNN and is implemented in the Caffe toolbox.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "The two basic parallelization strategies commonly used for SGD are data parallelism or model parallelism [7].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "dencies immanent of the SGD algorithm unlike the so called ”asynchronous SGD” algorithms described in literature [8].",
      "startOffset" : 113,
      "endOffset" : 116
    } ],
    "year" : 2017,
    "abstractText" : "Deep Neural Network (DNN) are currently of great interest in research and application. The training of these networks is a compute intensive and time consuming task. To reduce training times to a bearable amount at reasonable cost we extend the popular Caffe toolbox for DNN with an efficient distributed memory communication pattern. To achieve good scalability we emphasize the overlap of computation and communication and prefer fine granular synchronization patterns over global barriers. To implement these communication patterns we rely on the the ”Global address space Programming Interface” version 2 (GPI-2) communication library. This interface provides a light-weight set of asynchronous one-sided communication primitives supplemented by non-blocking fine granular data synchronization mechanisms. Therefore, CaffeGPI is the name of our parallel version of Caffe. First benchmarks demonstrate better scaling behavior compared with other extensions, e.g., the IntelTMCaffe. Even within a single symmetric multiprocessing machine with four graphics processing units, the CaffeGPI scales better than the standard Caffe toolbox. These first results demonstrate that the use of standard High Performance Computing (HPC) hardware is a valid cost saving approach to train large DDNs. I/O is an other bottleneck to work with DDNs in a standard parallel HPC setting, which we will consider in more detail in a forthcoming paper.",
    "creator" : "LaTeX with hyperref package"
  }
}