{
  "name" : "1708.02383.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning how to Active Learn: A Deep Reinforcement Learning Approach",
    "authors" : [ "Meng Fang", "Yuan Li" ],
    "emails" : [ "meng.fang@unimelb.edu.au,", "yuanl4@student.unimelb.edu.au,", "t.cohn@unimelb.edu.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "For most Natural Language Processing (NLP) tasks, obtaining sufficient annotated text for training accurate models is a critical bottleneck. Thus active learning has been applied to NLP tasks to minimise the expense of annotating data (Thompson et al., 1999; Tong and Koller, 2001; Settles and Craven, 2008). Active learning aims to reduce cost by identifying a subset of unlabelled data for annotation, which is selected to maximise the accuracy of a supervised model trained on the data (Settles, 2010). There have been many successful applications to NLP, e.g., Tomanek et al. (2007) used an active learning algorithm for CoNLL corpus to get an F1 score 84% with a reduction of annotation cost of about 48%. In prior work most active learning algorithms are designed for English based on\nheuristics, such as using uncertainty or informativeness. There has been comparatively little work done about how to learn the active learning strategy itself.\nIt is no doubt that active learning is extremely important for other languages, particularly lowresource languages, where annotation is typically difficult to obtain, and annotation budgets more modest (Garrette and Baldridge, 2013). Such settings are a natural application for active learning, however there is little work to this end. A potential reason is that most active learning algorithms require a substantial ‘seed set’ of data for learning a basic classifier, which can then be used for active data selection. However, given the dearth of data in the low-resource setting, this assumption can make standard approaches infeasible.\nIn this paper,1 we propose PAL, short for Policy based Active Learning, a novel approach for learning a dynamic active learning strategy from data. This allows for the strategy to be applied in other data settings, such as cross-lingual applications. Our algorithm does not use a fixed heuristic, but instead learns how to actively select data, formalised as a reinforcement learning (RL) problem. An intelligent agent must decide whether or not to select data for annotation in a streaming setting, where the decision policy is learned using a deep Q-network (Mnih et al., 2015). The policy is informed by observations including sentences’ content information, the supervised model’s classifications and its confidence. Accordingly, a rich and dynamic policy can be learned for annotating new data based on the past sequence of annotation decisions.\nFurthermore, in order to reduce the dependence on the data in the target language, which may be low resource, we first learn the policy of active\n1Source code available at https://github.com/ mengf1/PAL\nar X\niv :1\n70 8.\n02 38\n3v 1\n[ cs\n.C L\n] 8\nA ug\n2 01\n7\nlearning on another language and then transfer it to the target language. It is easy to learn a policy on a high resource language, where there is plentiful data, such as English. We use cross-lingual word embeddings to learn compatible data representations for both languages, such that the learned policy can be easily ported into the other language.\nOur work is different for prior work in active learning for NLP. Most previous active learning algorithms developed for NER tasks is based on one language and then applied to the language itself. Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016). However, in our algorithm, we implicitly use uncertainty information as one kind of observations to the RL agent.\nThe remainder of this paper is organised as follows. In Section 2, we briefly review some related work. In Section 3, we present active learning algorithms, which cross multiple languages. The experimental results are presented in Section 4. We conclude our work in Section 5."
    }, {
      "heading" : "2 Related work",
      "text" : "As supervised learning methods often require a lot of training data, active learning is a technique that selects a subset of data to annotate for training the best classifier. Existing active learning (AL) algorithms can be generally considered as three categories: 1) uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2001), which selects the data about which the current classifier is the most uncertain; 2) query by committee (Seung et al., 1992), which selects the data about which the “committee” disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled. Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al., 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017). Qian et al. used uncertainty sampling to jointly perform on English and Chinese. Stratos and Collins and Zhang et al. deployed uncertainty-based AL algorithms for languages with the minimal supervision.\nDeep reinforcement learning (DRL) is a general-purpose framework for decision making based on representation learning. Recently, there are some notable examples include deep Qlearning (Mnih et al., 2015), deep visuomotor policies (Levine et al., 2016), attention with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al., 2015). Other important works include massively parallel frameworks (Nair et al., 2015), dueling architecture (Wang et al., 2016) and expert move prediction in the game of Go (Maddison et al., 2015), which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search (Silver et al., 2016). DRL has been also studied in NLP tasks. For example, recently, DRL has been studied for information extraction problem (Narasimhan et al., 2016). They designed a framework that can decide to acquire external evidence and the framework is under the reinforcement learning method. However, there has been fairly little work on using DRL to learn active learning strategies for language processing tasks, especially in cross-lingual settings.\nRecent deep learning work has also looked at transfer learning (Bengio, 2012). More recent work in deep learning has also considered transferring policies by reusing policy parameters between environments (Parisotto et al., 2016; Rusu et al., 2016), using either regularization or novel neural network architectures, though this work has not looked at transfer active learning strategies between languages with shared feature space in state."
    }, {
      "heading" : "3 Methodology",
      "text" : "We now show how active learning can be formalised as as a decision process, and then show how this allows for the active learning selection policy to be learned from data using deep reinforcement learning. Later we introduce a method for transferring the policy between languages."
    }, {
      "heading" : "3.1 Active learning as a decision process",
      "text" : "Active learning is a simple technique for labelling data, which involves first selecting some instances from an unlabelled dataset, which are then annotated by a human oracle, which is then repeated many times until a termination criterion is satisfied, e.g., the annotation budget is exhausted. Most often the selection function is based on the pre-\ndictions of a trained model, which has been fit to the labelled dataset at each stage in the algorithm, where datapoints are selected based on the model’s predictive uncertainty (Lewis and Gale, 1994), or divergence in predictions over an ensemble (Seung et al., 1992). The key idea of these methods is to find the instances on which the model is most likely to make errors, such that after their labelling and inclusion in the training set, the model becomes more robust to these types of errors on unseen data.\nThe steps in active learning can be viewed as a decision process, a means of formalising the active learning algorithm as a sequence of decisions, where the stages of active learning correspond to the state of the system. Accordingly, the state corresponds to the selected data for labelling and their labels, and each step in the active learning algorithm corresponds to a selection action, wherein the heuristic selects the next items from a pool. This process terminates when the budget is exhausted.\nEffectively the active learning heuristic is operating as a decision policy, a form of function taking as input the current state — comprising the labelled data, from which a model is trained — and a candidate unlabelled data point — e.g., the model uncertainty. This raises the opportunity to consider general policy functions, based on the state and data point inputs, and resulting in a labelling decision, and, accordingly a mechanism for learning such functions from data. We now elaborate on the components of this process, namely the formulation of the decision process, architecture of the policy function, and means of learning the decision policy automatically from data."
    }, {
      "heading" : "3.2 Stream-based learning",
      "text" : "For simplicity, we make a streaming assumption, whereby unlabelled data (sentences) arrive in a stream (Lewis and Gale, 1994).2 As each instance arrives, an agent must decide the action to take, namely whether or not the instance should be manually annotated. This process is illustrated in Figure 1, which illustrates the space of decision sequences for a small corpus. As part of this process, a separate model, pφ, is trained on the labelled data, and updated accordingly as the labelled dataset is expanded as new annotations ar-\n2This is different to pool-based active learning, where one of several options is chosen for annotation. Our setup permits simpler learning, while remaining sufficiently general.\nrive. This model is central to the policy for choosing the labelling actions at each stage, and for determining the reward for a sequence of actions.\nThis is a form of Markov Decision Process (MDP), which allows the learning of a policy that can dynamically select instances that are most informative. As illustrated in Figure 1 at each time, the agent observes the current state si which includes the sentence xi, and the learned model φ. The agent selects a binary action ai, denoting whether to label xi, according to the policy π. For ai = 1, the corresponding sentence is labelled and added to the labelled data, and the model pφ updated to include this new training point. The process then repeats, terminating when either the dataset is exhausted or a fixed annotation budget is reached. After termination a reward is computed based on the accuracy of the final model, φ. We represent the MDP framework as a tuple 〈S,A, Pr(si+1|si, a), R〉, where S = {s}\nis the space of all possible states, A = {0, 1} is the set of actions, R(s, a) is the reward function, and Pr(si+1|si, a) is the transition function."
    }, {
      "heading" : "3.2.1 State",
      "text" : "The state at time i comprises the candidate instance being considered for annotation and the labelled dataset constructed in steps 1 . . . i. We represent the state using a continuous vector, using the concatenation of the vector representation of xi, and outputs of the model pφ trained over the labelled data. These outputs use both the predictive marginal distributions of the model on the instance, and a representation of the model’s confidence. We now elaborate on each component.\nContent representation A key input to the agent is the content of the sentence, xi, which we encode using a convolutional neural network to arrive at a fixed sized vector representation, following Kim (2014). This involves embedding each of the n words in the sentence to produce a matrix Xi = {xi,1, xi,2, · · · , xi,n}, after which a series of wide convolutional filters is applied, using multiple filters with different gram sizes. Each filter uses a linear transformation with a rectified linear unit activation function. Finally the filter outputs are merged using a max-pooling operation to yield a hidden state hc, which is used to represent the sentence.\nRepresentation of marginals The prediction outputs of the training model, pφ(y|xi), are central to all active learning heuristics, and accordingly, we include this in our approach. In order to generalise existing techniques, we elect to use the predictive marginals directly, rather than only using statistics thereof, e.g., entropy. This generality allows for different and more nuanced concepts to be learned, including patterns of probabilities that span several adjacent positions in the sentence (e.g., the uncertainty about the boundary of a named entity).\nWe use another convolutional neural network to process the predictive marginals, as shown in Figure 2. The convolutional layer contains j filters with ReLU activation, based on a window of width 3 and height equal to the number of classes, and with a stride of one token. We use a wide convolution, by padding the input matrix to either size with vectors of zeros. These j feature maps are then subsampled with mean pooling, such that the network is easily able to capture the average un-\nPierre\nVinken\nwill\njoin\nthe\nboard\n…\nMarginals Convolutional layer\nRepresentation of marginals\nPER LOC ORG O\nFigure 2: The architecture for representing predictive marginal distributions, pφ(y|xi), as a fixed dimensional vector, to form part of the MDP state.\ncertainty in each window. The final hidden layer he is used to represent the predictive marginals.\nConfidence of sequential prediction The last component is a score C which indicates the confidence of the model prediction. This is defined based on the most probable label sequence under the model, e.g., using Viterbi algorithm with a CRF, and the probability of this sequence is used to represent the confidence, C = n √ maxy pφ(y|xi), where n = |xi| is the length of the sentence."
    }, {
      "heading" : "3.2.2 Action",
      "text" : "We now turn to the action, which denotes whether the human oracle must annotate the current sentence. The agent selects either to annotate xi, in which case ai = 1, or not, with ai = 0, after which the agent proceeds to consider the next instance, xi+1. When action ai = 1 is chosen, an oracle is requested to annotate the sentence, and the newly annotated sentence is added to the training data, and φ updated accordingly. A special ‘terminate’ option applies when no further data remains or the annotation budget is exhausted, which concludes the active learning run (referred to as an ‘episode’ or ‘game’ herein)."
    }, {
      "heading" : "3.2.3 Reward",
      "text" : "The training signal for learning the policy takes the form of a scalar ‘reward’, which provides feedback on the quality of the actions made by the agent. The most obvious reward is to wait for a game to conclude, then measure the held-out performance of the model, which has been trained\non the labelled data. However, this reward is delayed, and is difficult to related to individual actions after a long game. To compensate for this, we use reward shaping, whereby small intermediate rewards are assigned which speeds up the learning process (Ng, 2003; Lample and Chaplot, 2016). At each step, the intermediate reward is defined as the change in held-out performance, i.e., R(si−1, a) = Acc(φi) − Acc(φi−1), where Acc denotes predictive accuracy (here F1 score), and φi is the trained model after action a has take place, which may include an additional training instance. Accordingly, when considering the aggregate reward over a game, the intermediate terms cancel, such that the total reward measures the performance improvement over the whole game. Note that the value of R(s, a) can be positive or negative, indicating a beneficial or detrimental effect on the performance."
    }, {
      "heading" : "3.2.4 Budget",
      "text" : "There is a fixed budget B for the total number of instances annotated, which corresponds to the terminal state in the MDP. It is a predefined number and chosen according to time and cost constraints. A game is finished when the data is exhausted or the budget reached, and with the final result being the dataset thus created, upon which the final model is trained."
    }, {
      "heading" : "3.2.5 Reinforcement learning",
      "text" : "The remaining question is how the above components can be used to learn a good policy. Different policies make different data selections, and thus result in models with different performance. We adopt a reinforcement learning (RL) approach to learn a policy resulting a highly accurate model.\nHaving represented the problem as a MDP, episode as a sequence of transitions (si, a, r, si+1). One episode of active learning produces a finite sequence of states, actions and rewards. We use a deep Q-learning approach (Mnih et al., 2015), which formalises the policy using function Qπ(s, a)→ Rwhich determines the utility of taking a from state s according to a policy π. In Qlearning, the agent iteratively updates Q(s, a) using rewards obtained from each episode, with updates based on the recursive Bellman equation for the optimal Q:\nQπ(s, a) = E[Ri|si = s, ai = a, π]. (1)\nHere, Ri = ∑T t=i γ t−irt is the discounted fu-\nAlgorithm 1 Learn an active learning policy Input: data D, budget B Output: π\n1: for episode = 1, 2, . . . , N do 2: Dl ← ∅ and shuffle D 3: φ← Random 4: for i ∈ {0, 1, 2, . . . , |D|} do 5: Construct the state si using xi 6: The agent makes a decision according to\nai = argmaxQ π(si, a)\n7: if ai = 1 then 8: Obtain the annotation yi 9: Dl ← Dl + (xi,yi)\n10: Update model φ based on Dl 11: end if 12: Receive a reward ri using held-out set 13: if |Dl| = B then 14: Store (si, ai, ri,Terminate) inM 15: Break 16: end if 17: Construct the new state si+1 18: Store transition (si, ai, ri, si+1) inM 19: Sample random minibatch of transitions\n{(sj , aj , rj , sj+1)} from M, and perform gradient descent step on L(θ)\n20: Update policy π with θ 21: end for 22: end for 23: return the latest policy π\nture reward and γ ∈ [0, 1] is a factor discounting the value of future rewards and the expectation is taken over all transitions involving state s and action a.\nFollowing Deep Q-learning (Mnih et al., 2015), we make use of a deep neural network to compute the expected Q-value, in order to update the parameters. We implement the Q-function using a single hidden layer neural network, taking as input the state representation (hc,he, C) (defined in §3.2.1), and outputting two scalar values corresponding to the values Q(s, a) for a ∈ {0, 1}. This network uses a rectified linear unit (ReLU) activation function in its hidden layer.\nThe parameters in the DQN are learnt using stochastic gradient descent, based on a regression objective to match the Q-values predicted by the DQN and the expected Q-values from the Bellman equation, ri + γmaxaQ(si+1, a; θ). Following (Mnih et al., 2015), we use an experi-\nAlgorithm 2 Active learning by policy transfer Input: unlabelled data D, budget B, policy π Output: Dl\n1: Dl ← ∅ 2: φ← Random 3: for |Dl| 6= B and D not empty do 4: Randomly sample xi from the data pool D and construct the state si 5: The agent chooses an action ai according to\nai = argmaxQ π(si, a)\n6: if ai = 1 then 7: Obtain the annotation yi 8: Dl ← Dl + (xi,yi) 9: Update model φ based on Dl\n10: end if 11: D ← D\\xi 12: Receive a reward ri using held-out set 13: Update policy π 14: end for 15: return Dl\nence replay memory M to store each transition (s, a, r, s′) as it is used in an episode, after which we sample a mini-batch of transitions from the memory and then minimize the loss function:\nL(θ) = Es,a,r,s′ [( yi(r, s ′)−Q(s, a; θ) )2] , (2)\nwhere yi(r, s′) = r + γmaxa′ Q(s′, a′; θi−1) is the target Q-value, based on the current parameters θi−1, and the expectation is over the minibatch. Learning updates are made every training step, based on stochastic gradient descent to minimise Eq. 2 w.r.t. parameters θ.\nThe algorithm for learning is summarised in Algorithm 1. We train the policy by running multiple active learning episodes over the training data, where each episode is a simulated active learning run. For each episode, we shuffle the data, and hide the known labels, which are revealed as requested during the run. A disjoint held-out set is used to compute the reward, i.e., model accuracy, which is fixed over the episodes. Between each episode the model is reset to its initialisation condition, with the main changes being the different (random) data ordering and the evolving policy function."
    }, {
      "heading" : "3.3 Cross-lingual policy transfer",
      "text" : "We now turn to the question of how the learned policy can be applied to another dataset. Given\nAlgorithm 3 Active learning by policy and model transfer, for ‘cold-start’ scenario Input: unlabelled data D, budget B, policy π,\nmodel φ Output: Dl\n1: Dl ← ∅ 2: for |Dl| 6= B and D not empty do 3: Randomly sample xi from the data pool D and construct the state si 4: The agent chooses an action ai according to\nai = argmaxQ π(si, a)\n5: if ai = 1 then 6: Dl ← Dl + (xi,−) 7: end if 8: D ← D\\xi 9: end for\n10: Obtain all the annotations for Dl 11: return Dl\nthe extensive use of the training dataset, the policy application only makes sense when employed in a different data setting, e.g., where the domain, task or language is different. For this paper, we consider a cross-lingual application of the same task (NER), where we train a policy on a source language (e.g., English), and then transfer the learned policy to a different target language. Cross-lingual word embeddings provide a common shared representation to facilitate application of the policy to other languages.\nWe illustrate the policy transfer algorithm in Algorithm 2. This algorithm is broadly similar to Algorithm 1, but has two key differences. Firstly, Algorithm 2 makes only one pass over the data, rather than several passes, as befits an application to a low-resource language where oracle labelling is costly. Secondly, the algorithm also assumes an initial policy, π, which is fine tuned during the episode based on held-out performance such that the policy can adapt to the test scenario.3"
    }, {
      "heading" : "3.4 Cold-start transfer",
      "text" : "The above transfer algorithm has some limitations, which may not be realistic for low-resource settings: the requirement for held-out evaluation data and the embedding of the oracle annotator in-\n3Moreover, the algorithm can be extended to a traditional batch setting by evaluating a batch of data instances and selectinag the best k instances for labelling under the policy. This could be applied in either the transfer step (Algorithm 2) or initial policy training (Algorithm 1), or both.\nside the learning loop. The former implies more supervision than is ideal in a low-resource setting, while the latter places limitations on the communication with annotator as well as a necessity for real-time processing, both which are unlikely in a field linguistics setting.\nFor this data and- communication-impoverished setting, denoted as cold-start, we allow only one chance to request labels for the target data, and, having no held-out data, do not allow policy updates. The agent needs to select a batch of unlabelled target instances for annotations, but cannot use these resulting annotations or any other feedback to refine the selection. In this, more difficult cold-start setting, we bootstrap the process with an initial model, such that the agent can make informative decisions in the absence of feedback.\nThe procedure is outlined in Algorithm 3. Using the cross-lingual word embeddings, we transfer both a policy and a model into the target language. The model, φ, is trained on one source language, and the policy is learned on a different source language. Policy learning uses Alg 1, with the small change that in step 3 the model is initialised using φ. Consequently the learned policy can exploit the knowledge from cross-lingual initialisation, such that it can figure out which aspects that need to be corrected using target annotated data. Overall this allows for estimates and confidence values to be produced by the model, thus providing the agent with sufficient information for data selection."
    }, {
      "heading" : "4 Experiments",
      "text" : "We conduct experiments to validate the proposed active learning method in a cross-lingual setting, whereby an active learning policy trained on a source language is transferred to a target language. We allow repeated active learning simulations on the source language, where annotated corpora are plentiful, to learn a policy, while for target languages we only permit a single episode, to mimic a language without existing resources.\nWe use NER corpora from CoNLL2002/2003 shared tasks,4 which comprise NER annotated text in English (en), German (de), Spanish (es), and Dutch (nl), each annotated using the IOB1 labelling scheme, which we convert to the IO label-\n4 http://www.cnts.ua.ac.be/conll2002/ ner/, http://www.cnts.ua.ac.be/conll2003/ ner/\ning scheme. We use the existing corpus partions, with train used for policy training, testb used as held-out for computing rewards, and final results are reported on testa.\nWe consider three experimental conditions, as illustrated in Table 1:\nbilingual where English is the source (used for policy learning) and we vary the target language;\nmultilingual where several source languages are the used in joint learning of the policy, and a separate language is used as target; and\ncold-start where a pretrained English NER tagger is used to initialise policy learning on a source language, and in cold-start application to a separate target language.\nConfiguration We now outline the parameter settings for the experimental runs. For learning an active learning policy, we run N = 10, 000 episodes with budget B = 200 sentences using Alg. 1. Content representations use three convolutional filters of size 3, 4 and 5, using 128 filters for each size, while for predictive marginals, the convolutional filters are of width 3, using 20 filters. The size of the last hidden layer is 256. The discount factor is set to γ = 0.99. We used the ADAM algorithm with mini-batches of size 32 for training the neural network. To report performance, we apply the learned policy to the target training set (using Alg. 2 or 3, again with budget 200),5 after which we use the final trained model for which we report F1 score.\n5Although it is possible the policy may learn not to use the full budget, this does not occur in practise.\nFor word embeddings, we use off the shelf CCA trained multilingual embeddings (Ammar et al., 2016),6 using a 40 dimensional embedding and fixing these during training of both the policy and model. As the model, we use a standard linear chain CRF (Lafferty et al., 2001) for the first two sets of experiments, while for cold-start case we use a basic RNN classifier with the same multilingual embeddings as before, and a 128 dimensional hidden layer.\nThe proposed method is referred to as PAL, as shorthand Policy based Active Learning. Subscripts b,m, c are used to denote the bilingual, multilingual and cold-start experimental configurations. For comparative baselines, we use the following methods:\nUncertainty sampling we use the total token entropy measure (Settles and Craven, 2008), which takes the instance x maximising∑|x|\nt=1H(yt|x, φ), where H is the token entropy. We use the whole training set as the data pool, and select a single instance for labelling in each active learning step. This method was shown to achieve the best result among model-independent active learning methods on the CoNLL data.\nRandom sampling which randomly selects examples from the unlabelled pool.\nResults Figure 3 shows results the bilingual case, where PALb consistently outperforms the Random and Uncertainty baselines across the\n6http://128.2.220.95/multilingual\nthree target languages. Uncertainty sampling is ineffective, particularly towards the start of the run, as a consequence of its dependence on a high quality model. The use of content information allows PALb to make a stronger start, despite the poor initial model.\nAlso shown in Figure 3 are results for multilingual policy learning, PALm, which outperform all other approaches including PALb. This illustrates that the additional training over several languages gives rise to a better policy, than only using one source language. The superior performance is particularly marked in the early stages of the runs for Spanish and Dutch, which may indicate that the approach was better able to learn to exploit the sentence content information.\nWe evaluate the cold-start setting in Figure 4. Recall that in this setting there are no policy or model updates, as no heldout data is used, and all annotations arrive in a batch. The model, however, is initialised with a NER tagger trained on a different language, which explains why the performance for all methods starts from around 40% rather than 0%. Even in this challenging evaluation setting, our algorithm PALc outperforms both baseline methods, showing that deep Q learning allows for better exploitation of the pretrained classifier, alongside the sentence content.\nLastly, we report the results for all approaches in Table 2, based on training on the full 200 labelled sentences as selected under the different methods. It is clear that the PAL methods all outperform the baselines, and among these the multilingual training of PALm outperforms the bilingual setting in PALb. Surprisingly, PALc gives the over-\nall best results, despite using a static policy and model during target application, underscoring the importance of model pretraining. Table 2 also reports the cost reduction versus random sampling, showing that the PAL methods can reduce the annotation burden to as low as 10%."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have proposed a new active learning algorithm capable of learning active learning strategies from data. We formalise active learning under a Markov decision framework, whereby active learning corresponds to a sequence of binary annotation decisions applied to a stream of data. Based on this, we design an active learning algorithm as a policy based on deep reinforcement learning. We show how these learned active learn-\ning policies can be transferred between languages, which we empirically show provides consistent and sizeable improvements over baseline methods, including traditional uncertainty sampling. This holds true even in a very difficult cold-start setting, where no evaluation data is available, and there is no ability to react to annotations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was sponsored by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. Trevor Cohn was supported by an Australian Research Council Future Fellowship."
    } ],
    "references" : [ {
      "title" : "Massively multilingual word embeddings",
      "author" : [ "Waleed Ammar", "George Mulcaire", "Yulia Tsvetkov", "Guillaume Lample", "Chris Dyer", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:1602.01925 .",
      "citeRegEx" : "Ammar et al\\.,? 2016",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2016
    }, {
      "title" : "Multiple object recognition with visual attention",
      "author" : [ "Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Ba et al\\.,? 2015",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning of representations for unsupervised and transfer learning",
      "author" : [ "Yoshua Bengio." ],
      "venue" : "Proceedings of ICML Workshop on Unsupervised and Transfer Learning. pages 17–36.",
      "citeRegEx" : "Bengio.,? 2012",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2012
    }, {
      "title" : "Model transfer for tagging low-resource languages using a bilingual dictionary",
      "author" : [ "Meng Fang", "Trevor Cohn." ],
      "venue" : "Proceedings of the 55th Annual Meeting on Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Fang and Cohn.,? 2017",
      "shortCiteRegEx" : "Fang and Cohn.",
      "year" : 2017
    }, {
      "title" : "Learning a part-of-speech tagger from two hours of annotation",
      "author" : [ "Dan Garrette", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "citeRegEx" : "Garrette and Baldridge.,? 2013",
      "shortCiteRegEx" : "Garrette and Baldridge.",
      "year" : 2013
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing (EMNLP).",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Playing FPS games with deep reinforcement learning",
      "author" : [ "Guillaume Lample", "Devendra Singh Chaplot." ],
      "venue" : "arXiv preprint arXiv:1609.05521 .",
      "citeRegEx" : "Lample and Chaplot.,? 2016",
      "shortCiteRegEx" : "Lample and Chaplot.",
      "year" : 2016
    }, {
      "title" : "End-to-end training of deep visuomotor policies",
      "author" : [ "Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel." ],
      "venue" : "Journal of Machine Learning Research 17(39):1–40.",
      "citeRegEx" : "Levine et al\\.,? 2016",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2016
    }, {
      "title" : "A sequential algorithm for training text classifiers",
      "author" : [ "David D Lewis", "William A Gale." ],
      "venue" : "Proceedings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval. pages 3–12.",
      "citeRegEx" : "Lewis and Gale.,? 1994",
      "shortCiteRegEx" : "Lewis and Gale.",
      "year" : 1994
    }, {
      "title" : "Move evaluation in go using deep convolutional neural networks",
      "author" : [ "Chris J Maddison", "Aja Huang", "Ilya Sutskever", "David Silver." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Maddison et al\\.,? 2015",
      "shortCiteRegEx" : "Maddison et al\\.",
      "year" : 2015
    }, {
      "title" : "Employing em and pool-based active learning for text classification",
      "author" : [ "Andrew Kachites McCallumzy", "Kamal Nigamy." ],
      "venue" : "Proceedings of the 15th International Conference on Machine Learning (ICML). pages 359–367.",
      "citeRegEx" : "McCallumzy and Nigamy.,? 1998",
      "shortCiteRegEx" : "McCallumzy and Nigamy.",
      "year" : 1998
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : null,
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Massively parallel methods for deep reinforcement learning",
      "author" : [ "Silver." ],
      "venue" : "Proceedings of ICML Workshop on Deep Learning.",
      "citeRegEx" : "Silver.,? 2015",
      "shortCiteRegEx" : "Silver.",
      "year" : 2015
    }, {
      "title" : "Improving information extraction by acquiring external evidence with reinforcement learning",
      "author" : [ "Karthik Narasimhan", "Adam Yala", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods on Natural Language Processing",
      "citeRegEx" : "Narasimhan et al\\.,? 2016",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2016
    }, {
      "title" : "Shaping and Policy Search in Reinforcement Learning",
      "author" : [ "Andrew Y. Ng." ],
      "venue" : "Ph.D. thesis, University of California, Berkeley.",
      "citeRegEx" : "Ng.,? 2003",
      "shortCiteRegEx" : "Ng.",
      "year" : 2003
    }, {
      "title" : "Actor-mimic: Deep multitask and transfer reinforcement learning",
      "author" : [ "Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Parisotto et al\\.,? 2016",
      "shortCiteRegEx" : "Parisotto et al\\.",
      "year" : 2016
    }, {
      "title" : "Bilingual active learning for relation classification via pseudo parallel corpora",
      "author" : [ "Longhua Qian", "Haotian Hui", "Yanan Hu", "Guodong Zhou", "Qiaoming Zhu." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Qian et al\\.,? 2014",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2014
    }, {
      "title" : "Toward optimal active learning through monte carlo estimation of error reduction",
      "author" : [ "Nicholas Roy", "Andrew McCallum." ],
      "venue" : "Proceedings of the 18th International Conference on Machine Learning (ICML). pages 441–448.",
      "citeRegEx" : "Roy and McCallum.,? 2001",
      "shortCiteRegEx" : "Roy and McCallum.",
      "year" : 2001
    }, {
      "title" : "Progressive neural networks",
      "author" : [ "Andrei A Rusu", "Neil C Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell." ],
      "venue" : "arXiv preprint arXiv:1606.04671 .",
      "citeRegEx" : "Rusu et al\\.,? 2016",
      "shortCiteRegEx" : "Rusu et al\\.",
      "year" : 2016
    }, {
      "title" : "Active learning literature survey",
      "author" : [ "Burr Settles." ],
      "venue" : "University of Wisconsin, Madison 52(55-66):11.",
      "citeRegEx" : "Settles.,? 2010",
      "shortCiteRegEx" : "Settles.",
      "year" : 2010
    }, {
      "title" : "An analysis of active learning strategies for sequence labeling tasks",
      "author" : [ "Burr Settles", "Mark Craven." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1070–1079.",
      "citeRegEx" : "Settles and Craven.,? 2008",
      "shortCiteRegEx" : "Settles and Craven.",
      "year" : 2008
    }, {
      "title" : "Query by committee",
      "author" : [ "H Sebastian Seung", "Manfred Opper", "Haim Sompolinsky." ],
      "venue" : "Proceedings of the 5th annual workshop on Computational Learning Theory. pages 287–294.",
      "citeRegEx" : "Seung et al\\.,? 1992",
      "shortCiteRegEx" : "Seung et al\\.",
      "year" : 1992
    }, {
      "title" : "Multi-criteria-based active learning for named entity recognition",
      "author" : [ "Dan Shen", "Jie Zhang", "Jian Su", "Guodong Zhou", "Chew-Lim Tan." ],
      "venue" : "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Shen et al\\.,? 2004",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2004
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : null,
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Simple semisupervised pos tagging",
      "author" : [ "Karl Stratos", "Michael Collins." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the",
      "citeRegEx" : "Stratos and Collins.,? 2015",
      "shortCiteRegEx" : "Stratos and Collins.",
      "year" : 2015
    }, {
      "title" : "Active learning for natu",
      "author" : [ "mond J Mooney" ],
      "venue" : null,
      "citeRegEx" : "Mooney.,? \\Q1999\\E",
      "shortCiteRegEx" : "Mooney.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Thus active learning has been applied to NLP tasks to minimise the expense of annotating data (Thompson et al., 1999; Tong and Koller, 2001; Settles and Craven, 2008).",
      "startOffset" : 94,
      "endOffset" : 166
    }, {
      "referenceID" : 20,
      "context" : "Active learning aims to reduce cost by identifying a subset of unlabelled data for annotation, which is selected to maximise the accuracy of a supervised model trained on the data (Settles, 2010).",
      "startOffset" : 180,
      "endOffset" : 195
    }, {
      "referenceID" : 4,
      "context" : "It is no doubt that active learning is extremely important for other languages, particularly lowresource languages, where annotation is typically difficult to obtain, and annotation budgets more modest (Garrette and Baldridge, 2013).",
      "startOffset" : 202,
      "endOffset" : 232
    }, {
      "referenceID" : 12,
      "context" : "An intelligent agent must decide whether or not to select data for annotation in a streaming setting, where the decision policy is learned using a deep Q-network (Mnih et al., 2015).",
      "startOffset" : 162,
      "endOffset" : 181
    }, {
      "referenceID" : 13,
      "context" : "For most Natural Language Processing (NLP) tasks, obtaining sufficient annotated text for training accurate models is a critical bottleneck. Thus active learning has been applied to NLP tasks to minimise the expense of annotating data (Thompson et al., 1999; Tong and Koller, 2001; Settles and Craven, 2008). Active learning aims to reduce cost by identifying a subset of unlabelled data for annotation, which is selected to maximise the accuracy of a supervised model trained on the data (Settles, 2010). There have been many successful applications to NLP, e.g., Tomanek et al. (2007) used an active learning algorithm for CoNLL corpus to get an F1 score 84% with a reduction of annotation cost of about 48%.",
      "startOffset" : 19,
      "endOffset" : 587
    }, {
      "referenceID" : 21,
      "context" : "Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016).",
      "startOffset" : 131,
      "endOffset" : 204
    }, {
      "referenceID" : 25,
      "context" : "Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016).",
      "startOffset" : 131,
      "endOffset" : 204
    }, {
      "referenceID" : 9,
      "context" : "Existing active learning (AL) algorithms can be generally considered as three categories: 1) uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2001), which selects the data about which the current classifier is the most uncertain; 2) query by committee (Seung et al.",
      "startOffset" : 114,
      "endOffset" : 159
    }, {
      "referenceID" : 22,
      "context" : "Existing active learning (AL) algorithms can be generally considered as three categories: 1) uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2001), which selects the data about which the current classifier is the most uncertain; 2) query by committee (Seung et al., 1992), which selects the data about which the “committee” disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled.",
      "startOffset" : 264,
      "endOffset" : 284
    }, {
      "referenceID" : 18,
      "context" : ", 1992), which selects the data about which the “committee” disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled.",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al.",
      "startOffset" : 67,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al., 2014), and structured prediction (Shen et al.",
      "startOffset" : 145,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : ", 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).",
      "startOffset" : 35,
      "endOffset" : 128
    }, {
      "referenceID" : 21,
      "context" : ", 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).",
      "startOffset" : 35,
      "endOffset" : 128
    }, {
      "referenceID" : 25,
      "context" : ", 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).",
      "startOffset" : 35,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : ", 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).",
      "startOffset" : 35,
      "endOffset" : 128
    }, {
      "referenceID" : 12,
      "context" : "Recently, there are some notable examples include deep Qlearning (Mnih et al., 2015), deep visuomotor policies (Levine et al.",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : ", 2015), deep visuomotor policies (Levine et al., 2016), attention with recurrent networks (Ba et al.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : ", 2016), attention with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : ", 2016) and expert move prediction in the game of Go (Maddison et al., 2015), which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search (Silver et al.",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 24,
      "context" : ", 2015), which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search (Silver et al., 2016).",
      "startOffset" : 157,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "For example, recently, DRL has been studied for information extraction problem (Narasimhan et al., 2016).",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "Recent deep learning work has also looked at transfer learning (Bengio, 2012).",
      "startOffset" : 63,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "More recent work in deep learning has also considered transferring policies by reusing policy parameters between environments (Parisotto et al., 2016; Rusu et al., 2016), using either regularization or novel neural network architectures, though this work has not looked at transfer active learning strategies between languages with shared feature space in state.",
      "startOffset" : 126,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "More recent work in deep learning has also considered transferring policies by reusing policy parameters between environments (Parisotto et al., 2016; Rusu et al., 2016), using either regularization or novel neural network architectures, though this work has not looked at transfer active learning strategies between languages with shared feature space in state.",
      "startOffset" : 126,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "dictions of a trained model, which has been fit to the labelled dataset at each stage in the algorithm, where datapoints are selected based on the model’s predictive uncertainty (Lewis and Gale, 1994), or divergence in predictions over an ensemble (Seung et al.",
      "startOffset" : 178,
      "endOffset" : 200
    }, {
      "referenceID" : 22,
      "context" : "dictions of a trained model, which has been fit to the labelled dataset at each stage in the algorithm, where datapoints are selected based on the model’s predictive uncertainty (Lewis and Gale, 1994), or divergence in predictions over an ensemble (Seung et al., 1992).",
      "startOffset" : 248,
      "endOffset" : 268
    }, {
      "referenceID" : 9,
      "context" : "For simplicity, we make a streaming assumption, whereby unlabelled data (sentences) arrive in a stream (Lewis and Gale, 1994).",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Content representation A key input to the agent is the content of the sentence, xi, which we encode using a convolutional neural network to arrive at a fixed sized vector representation, following Kim (2014). This involves embedding each of the n words in the sentence to produce a matrix",
      "startOffset" : 197,
      "endOffset" : 208
    }, {
      "referenceID" : 15,
      "context" : "To compensate for this, we use reward shaping, whereby small intermediate rewards are assigned which speeds up the learning process (Ng, 2003; Lample and Chaplot, 2016).",
      "startOffset" : 132,
      "endOffset" : 168
    }, {
      "referenceID" : 7,
      "context" : "To compensate for this, we use reward shaping, whereby small intermediate rewards are assigned which speeds up the learning process (Ng, 2003; Lample and Chaplot, 2016).",
      "startOffset" : 132,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : "We use a deep Q-learning approach (Mnih et al., 2015), which formalises the policy using function Qπ(s, a)→ Rwhich determines the utility of taking a from state s according to a policy π.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "Following Deep Q-learning (Mnih et al., 2015), we make use of a deep neural network to compute the expected Q-value, in order to update the parameters.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "Following (Mnih et al., 2015), we use an experi-",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "For word embeddings, we use off the shelf CCA trained multilingual embeddings (Ammar et al., 2016),6 using a 40 dimensional embedding and fixing these during training of both the policy and model.",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "As the model, we use a standard linear chain CRF (Lafferty et al., 2001) for the first two sets of experiments, while for cold-start case we use a basic RNN classifier with the same multilingual embeddings as before, and a 128 dimensional hidden layer.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "Uncertainty sampling we use the total token entropy measure (Settles and Craven, 2008), which takes the instance x maximising ∑|x| t=1H(yt|x, φ), where H is the token entropy.",
      "startOffset" : 60,
      "endOffset" : 86
    } ],
    "year" : 2017,
    "abstractText" : "Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.",
    "creator" : "LaTeX with hyperref package"
  }
}