{
  "name" : "1412.7144.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "INSTANCE LEARNING", "Deepak Pathak", "Evan Shelhamer", "Jonathan Long", "Trevor Darrell" ],
    "emails" : [ "pathak@cs.berkeley.edu", "shelhamer@cs.berkeley.edu", "jonlong@cs.berkeley.edu", "trevor@cs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained endto-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, eliminating the need for object proposals based pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC 2011 segmentation data."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Convolutional networks (convnets) are achieving state-of-the-art performance on many computer vision tasks but require costly supervision. Following the ILSVRC12-winning image classifier of Krizhevsky et al. (2012), progress on detection (Girshick et al., 2014) and segmentation (Long et al., 2014) demonstrates that convnets can likewise address local tasks with structured output.\nMost deep learning methods rely on strongly annotated data that is highly time-consuming in these settings. Learning from weak supervision, though hard, would sidestep the annotation cost to scale up learning to available image level labels.\nIn this work, we propose a novel framework for multiple instance learning (MIL) with a fully convolutional network (FCN). The task is to learn pixel-level semantic segmentation from weak imagelevel labels that only signal the presence or absence of an object. Images that are not centered on the labeled object or contain multiple objects make the problem more difficult. The insight of this work is to drive the joint learning of the convnet representation and pixel classifier by multiple instance learning. Fully convolutional training learns the model end-to-end at each pixel. To learn the segmentation model from image labels, we cast each image as a bag of pixel-level-instances and define a pixelwise, multi-class adaptation of MIL for the loss.\nMIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation. Oquab et al. (2014) improve image classification by inferring latent object location, but do not evaluate the localization. Hoffman et al. (2014) does MIL finetuning on top but relies on bounding box proposals and supervised data for representation learning. Most MIL problems are framed as max-margin learning (Andrews et al., 2002; Felzenszwalb et al., 2010), while other approaches use boosting (Ali & Saenko, 2014) or Noisy-OR models (Heckerman, 2013). These approaches are limited by (1) fixed representations and (2) sensitivity to initial hypotheses of the latent instance-level labels. We aim to counter both shortcomings by simultaneously learning the representation to maximize the most confident inferred instances. We further aim to incorporate multi-class annotations by making multiple inferrences per image. When an image / bag contains multiple classes, then the competition of pixelwise models help to better infer the latent instance-level classes.\nIn this paper, we investigate the following ideas and carry out preliminary experiments to these ends.\nar X\niv :1\n41 2.\n71 44\nv1 [\ncs .C\nV ]\n2 2\nD ec\n2 01\n4\n• We perform MIL jointly with representation learning in fully-convolutional network, training everything end to end. This eliminates the need of instantiation of instance-label hypothesis. FCN allows to process varying size images without warping, and thus eliminates the need for any object proposal based pre-processing, resulting into a fast and convenient training.\n• We propose the multi-class pixel-level loss inspired from the binary MIL scenario. This tries to maximize the classification score based on each pixel-instance, while simultaneously take advantage of their competence in narrowing down the instance hypothesis.\n• We target the rarely studied problem of image segmentation in weakly supervised setting. Our belief is that the pixel-level consistency cues are quite helpful in disambiguating the presence of object in image, thus weak segmentation utilizes more information of image structure than bounding boxes."
    }, {
      "heading" : "2 FULLY CONVOLUTIONAL MIL",
      "text" : "A fully convolutional network (FCN) is a model designed for spatial prediction problems. Every layer in an FCN computes a local operation on relative spatial coordinates. In this way, an FCN can take an input of any size and produce an output of corresponding dimensions.\nFor the purpose of weakly supervised MIL learning, the FCN allows for the efficient selection of training instances. The FCN predicts an output map for all pixels, and has a corresponding loss map at every pixel. This loss map can be masked or otherwise manipulated to choose and select instances for computing the loss and back-propagation for learning.\nWe use the VGG net (Simonyan & Zisserman, 2014) architecture and adapt it to the fully convolutional form as suggested in Long et al. (2014) for semantic segmentation. The network is fine-tuned with the pre-trained model weights learned from ILSVRC 2012 classification data, with inclusion of a background class. Since there is no background in the classification problem, we zero initialize a background classifier."
    }, {
      "heading" : "3 MULTI-CLASS MIL LOSS",
      "text" : "We define multi-class MIL loss as the multi-class logistic loss computed at maximum predictions. This selection is enabled by the output map produced by FCN i.e. for arbitrary sized image, FCN outputs accordingly sized heat-map corresponding to each output class (including background). We identify the max scoring pixel in the coarse heat-maps of classes present in image, and background. TLoss is then computed only on these coarse pixels, and is back-propogated till the start. We ignore the loss at non-max scoring heat map pixels inspired by the alternating optimization in binary MIL problem. The background class is analogous to the negative instances, competing against the other positive object classes. Let input image be I , its label set be LI (including background label) and p̂l(x, y) be the output of heat-map corresponding to lth label at location location (x, y), then loss is defined as :\n(xl, yl) = arg max ∀(x,y)\np̂l(x, y) ∀l ∈ LI\n=⇒ MIL LOSS = −1 |LI | ∑ l∈LI log p̂l(xl, yl)\nIt is to emphasize that ignoring loss at all non-max scoring pixels helps in avoiding background biased learning of FCN. Moreover, simultaneous training has added advantage for multi-labeled images. Inter-class confusion helps in refining the intra-class pixel consistency. At test time, we simply take maximum across all classes per-pixel at the coarse heat-map level, and perform bilinear interpolation to obtain segmentation of the original image."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "We validate our approach through preliminary experimentation on PASCAL VOC 2011 Segmentation dataset. The VGG-FCN network is fine-tuned using MIL loss from model weights learned from ILSVRC12 Classification challenge with 1K categories. Long et al. (2014) fine-tunes only from the weights adapted from representation layers, and not classifier layer, as they have access to complete supervision. But in our setting, we observe significant improvement while starting with adapted classification layer weights for the classes common to both. Including this classifier layer prevents degenerate solutions of all background.\nTable 1: Results on PASCAL VOC 2011 val-segmentation data. Fine-tuning with MIL loss achieves 58% relative improvement over the baseline.\nApproach mean IOU\nBaseline (no classifier) 3.52% Baseline (w classifier) 12.96% FCN-MIL 20.46% Oracle (supervised) 64.03%\nFigure 1: Sample images from PASCAL VOC 2011 val-segmentation data. In each example, middle figure is the ground truth and right one is the output of FCN-MIL.\nTable 1 shows the result on validation data. FCN-MIL achieves 58% relative improvement over the baseline results when classifier layer weights are used for the common classes. The validation data was kept completely held-out, so we show all our results on it. These are encouraging results, but incomparable to oracle. Some example outputs from FCN-MIL are shown in Figure 1. Surprisingly, this network converges to background very quickly to background so we train it in small-steps with small learning rates."
    }, {
      "heading" : "5 DISCUSSION",
      "text" : "We propose a novel model of joint multiple instance and representation learning with a multi-class pixelwise loss inspired by binary MIL. This model is learned end-to-end as a fully convolutional network for the task of weakly supervised semantic segmentation. It precludes the requirement for initialization of instance hypothesis, and need for any sort of proposal mechanisms, with the benefit of much faster inference (≈ 1/3 sec). These results are encouraging, and can be improved further. Currently, we use bilinear implementation to scale up the image which gives rise to coarse outputs as in Figure 1. These can be improved with conditional random fields using super-pixel (Achanta et al., 2012) information. Moreover, controlling convnet learning by manipulating the loss map in this way could have further uses such as hard negative mining. We would investigate these possibilities further."
    } ],
    "references" : [ {
      "title" : "Slic superpixels compared to state-of-the-art superpixel methods",
      "author" : [ "Achanta", "Radhakrishna", "Shaji", "Appu", "Smith", "Kevin", "Lucchi", "Aurelien", "Fua", "Pascal", "Susstrunk", "Sabine" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Achanta et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Achanta et al\\.",
      "year" : 2012
    }, {
      "title" : "Confidence-rated multiple instance boosting for object detection",
      "author" : [ "K. Ali", "K. Saenko" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Ali and Saenko,? \\Q2014\\E",
      "shortCiteRegEx" : "Ali and Saenko",
      "year" : 2014
    }, {
      "title" : "Support vector machines for multiple-instance learning",
      "author" : [ "Andrews", "Stuart", "Tsochantaridis", "Ioannis", "Hofmann", "Thomas" ],
      "venue" : "In Proc. NIPS, pp",
      "citeRegEx" : "Andrews et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Andrews et al\\.",
      "year" : 2002
    }, {
      "title" : "Multi-fold mil training for weakly supervised object localization",
      "author" : [ "Cinbis", "Ramazan Gokberk", "Verbeek", "Jakob", "Schmid", "Cordelia" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Cinbis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cinbis et al\\.",
      "year" : 2014
    }, {
      "title" : "Object detection with discriminatively trained part-based models",
      "author" : [ "Felzenszwalb", "Pedro F", "Girshick", "Ross B", "McAllester", "David", "Ramanan", "Deva" ],
      "venue" : "IEEE Tran. PAMI,",
      "citeRegEx" : "Felzenszwalb et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Felzenszwalb et al\\.",
      "year" : 2010
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "In In Proc. CVPR,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2014
    }, {
      "title" : "A tractable inference algorithm for diagnosing multiple diseases",
      "author" : [ "Heckerman", "David" ],
      "venue" : "arXiv preprint arXiv:1304.1511,",
      "citeRegEx" : "Heckerman and David.,? \\Q2013\\E",
      "shortCiteRegEx" : "Heckerman and David.",
      "year" : 2013
    }, {
      "title" : "Detector discovery in the wild: Joint multiple instance and representation learning",
      "author" : [ "Hoffman", "Judy", "Pathak", "Deepak", "Darrell", "Trevor", "Saenko", "Kate" ],
      "venue" : "arXiv preprint arXiv:1412.1135,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2014
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In Proc. NIPS,",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Fully convolutional networks for semantic segmentation",
      "author" : [ "Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor" ],
      "venue" : "arXiv preprint arXiv:1411.4038,",
      "citeRegEx" : "Long et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2014
    }, {
      "title" : "Weakly supervised object recognition with convolutional neural networks",
      "author" : [ "Oquab", "Maxime", "Bottou", "Léon", "Laptev", "Ivan", "Sivic", "Josef" ],
      "venue" : "In Proc. NIPS,",
      "citeRegEx" : "Oquab et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Oquab et al\\.",
      "year" : 2014
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Simonyan", "Karen", "Zisserman", "Andrew" ],
      "venue" : "arXiv preprint arXiv:1409.1556,",
      "citeRegEx" : "Simonyan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Simonyan et al\\.",
      "year" : 2014
    }, {
      "title" : "Weakly-supervised discovery of visual pattern configurations",
      "author" : [ "Song", "Hyun Oh", "Lee", "Yong Jae", "Jegelka", "Stefanie", "Darrell", "Trevor" ],
      "venue" : "In Proc. NIPS,",
      "citeRegEx" : "Song et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "(2012), progress on detection (Girshick et al., 2014) and segmentation (Long et al.",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 9,
      "context" : ", 2014) and segmentation (Long et al., 2014) demonstrates that convnets can likewise address local tasks with structured output.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "MIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation.",
      "startOffset" : 53,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "MIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation.",
      "startOffset" : 53,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "Most MIL problems are framed as max-margin learning (Andrews et al., 2002; Felzenszwalb et al., 2010), while other approaches use boosting (Ali & Saenko, 2014) or Noisy-OR models (Heckerman, 2013).",
      "startOffset" : 52,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "Most MIL problems are framed as max-margin learning (Andrews et al., 2002; Felzenszwalb et al., 2010), while other approaches use boosting (Ali & Saenko, 2014) or Noisy-OR models (Heckerman, 2013).",
      "startOffset" : 52,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "Following the ILSVRC12-winning image classifier of Krizhevsky et al. (2012), progress on detection (Girshick et al.",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "MIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation. Oquab et al. (2014) improve image classification by inferring latent object location, but do not evaluate the localization.",
      "startOffset" : 54,
      "endOffset" : 160
    }, {
      "referenceID" : 2,
      "context" : "MIL can reduce the need for bounding box annotations (Cinbis et al., 2014; Song et al., 2014), but it is rarely attempted for segmentation. Oquab et al. (2014) improve image classification by inferring latent object location, but do not evaluate the localization. Hoffman et al. (2014) does MIL finetuning on top but relies on bounding box proposals and supervised data for representation learning.",
      "startOffset" : 54,
      "endOffset" : 286
    }, {
      "referenceID" : 9,
      "context" : "We use the VGG net (Simonyan & Zisserman, 2014) architecture and adapt it to the fully convolutional form as suggested in Long et al. (2014) for semantic segmentation.",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "Long et al. (2014) fine-tunes only from the weights adapted from representation layers, and not classifier layer, as they have access to complete supervision.",
      "startOffset" : 0,
      "endOffset" : 19
    } ],
    "year" : 2017,
    "abstractText" : "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained endto-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, eliminating the need for object proposals based pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC 2011 segmentation data.",
    "creator" : "LaTeX with hyperref package"
  }
}