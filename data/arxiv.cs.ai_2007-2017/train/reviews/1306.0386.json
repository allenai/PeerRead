{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration", "abstract": "Given a Markov Decision Process (MDP) with $n$ states and $m$ actions per state, we study the number of iterations needed by Policy Iteratio n (PI) algorithms to converge. We consider two variations of PI: Howard's PI that changes all the actions with a positive advantage, and Sim plex-PI that only changes one action with maximal advantage. We show that Howard's PI terminates after at most $ n(m-1) \\lceil \\frac{1}{1-\\gamma}\\log (\\frac{1}{1-\\gamma}) \\rceil $ iterations, improving by a factor $O(\\log n)$ a result by Hansen et al. (2013), while Simplex-PI terminates after at most $ n(m-1) \\lceil \\frac{n}{1-\\gamma} \\log (\\frac{n}{1-\\gamma})\\rceil $ iterations, improving by a factor 2 a result by Ye (2011). We then consider bounds that are independent of the discount factor $\\gamma$. When the MDP is deterministic, we show that Simplex-PI terminates after at most $ 2 n^2 m (m-1) \\lceil 2 (n-1) \\log n \\rceil \\lceil 2 n \\log n \\rceil = O(n^4 m^2 \\log^2 n) $ iterations, improving by a factor $O(n)$ a bound obtained by Post and Ye (2012). We generalize this result to general MDPs under some structural assumptions: given a measure of the maximal transient time $\\tau_t$ and the maximal time $\\tau_r$ to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most $ n^2 m (m-1) (\\lceil \\tau_r \\log (n \\tau_r) \\rceil +\\lceil \\tau_r \\log (n \\tau_t) \\rceil) \\lceil {\\tau_t} \\log (n (\\tau_t+1)) \\rceil = \\tilde O (n^2 \\tau_t \\tau_r m^2) $ iterations. We explain why similar results seem hard to derive for Howard's PI. Finally, under the additional (restrictive) assumption that the MDP is weakly-communicating, we show that Simplex-PI and Howard's PI terminate after at most $n(m-1) (\\lceil \\tau_t \\log n \\tau_t \\rceil + \\lceil \\tau_r \\log n \\tau_r \\rceil) =\\tilde O(nm (\\tau_t+\\tau_r))$ iterations.", "histories": [["v1", "Mon, 3 Jun 2013 12:48:27 GMT  (29kb)", "https://arxiv.org/abs/1306.0386v1", null], ["v2", "Thu, 6 Jun 2013 14:14:54 GMT  (29kb)", "http://arxiv.org/abs/1306.0386v2", "Markov decision processes ; Dynamic Programming ; Analysis of Algorithms"], ["v3", "Mon, 24 Jun 2013 14:09:56 GMT  (30kb)", "http://arxiv.org/abs/1306.0386v3", "Markov decision processes ; Dynamic Programming ; Analysis of Algorithms"], ["v4", "Wed, 10 Feb 2016 09:09:49 GMT  (33kb)", "http://arxiv.org/abs/1306.0386v4", "Markov decision processes, Dynamic Programming, Analysis of Algorithms, Mathematics of Operations Research, INFORMS, 2016"]], "reviews": [], "SUBJECTS": "math.OC cs.AI cs.DM cs.RO", "authors": ["bruno scherrer"], "accepted": true, "id": "1306.0386"}
