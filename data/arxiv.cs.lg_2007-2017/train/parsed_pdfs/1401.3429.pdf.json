{
  "name" : "1401.3429.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Latent Tree Models and Approximate Inference in Bayesian Networks",
    "authors" : [ "Yi Wang", "Nevin L. Zhang", "Tao Chen" ],
    "emails" : [ "wangyi@cse.ust.hk", "lzhang@cse.ust.hk", "csct@cse.ust.hk" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Latent tree models (LTMs) are tree-structured Bayesian networks where leaf nodes represent manifest variables which are observed, while internal nodes represent latent variables which are hidden. They are previously known as hierarchical latent class models (Zhang, 2004). In this paper, we do not distinguish between variables and nodes, and assume that all variables are categorical.\nPearl (1988) was the first to identify LTMs as a potentially useful class of models. There are two reasons. First, inference in LTMs takes time linear in the number of nodes, while it is intractable in general BNs. Second, the latent variables capture complex relationships among the manifest variables. In an LTM, the manifest variables are mutually independent given the latent variables, while eliminating all the latent variables results in a completely connected BN.\nWe study the possibility of exploiting those two properties for approximate inference in BNs. Here is the most natural idea:\n1. Offline: Obtain an LTM M that approximates a BN N in the sense that the joint distribution of the manifest variables inM approximately equals the joint distribution of the variables in N .\n2. Online: Use M instead of N to compute answers to probabilistic queries. The cardinalities of the latent variables play a crucial role in the approximation scheme. They determine inferential complexity and influence approximation accuracy. At one extreme, we can represent a BN exactly using an LTM by setting the cardinalities of the latent variables large enough. In this case, the inferential complexity is very high. At the other\nc©2008 AI Access Foundation. All rights reserved.\nextreme, we can set the cardinalities of the latent variables at 1. In this case, the manifest variables become mutually independent. The inferential complexity is the lowest and the approximation quality is the poorest. We seek an appropriate middle point between those two extremes.\nWe assume that there is a predetermined constraint on the cardinalities of the latent variables to control inferential complexity. We develop an algorithm for finding an LTM that satisfies the constraint and approximates the original BN well. The idea is to sample data from the BN, and learn an LTM from the data. The model structure is determined using hierarchical clustering of manifest variables. In each step, two closely correlated sets of manifest variables are grouped, and a new latent variable is introduced to account for the relationship between them. The cardinalities of the latent variables are set at the predetermined value. The parameters are optimized using the Expectation-Maximization (EM) algorithm (Dempster, Laird, & Rubin, 1977).\nWe have empirically evaluated our inference method on an array of networks. The possibility to tradeoff between inferential complexity and approximation accuracy has been demonstrated by adjusting the cardinality constraints. It turns out that our method is able to achieve good approximation accuracy before the cardinality becomes too high. We compared our method with loopy belief propagation (LBP) (Pearl, 1988), a standard approximate inference method which has been successfully used in many real world domains (Frey & MacKay, 1997; Murphy, Weiss, & Jordan, 1999). Given the same amount of time, our method achieves significantly higher accuracy than LBP in most cases. To achieve the same accuracy, LBP needs one to three orders of magnitude more time than our method.\nOur inference method is fast because LTM is tree-structured. One can also construct a Chow-Liu tree (Chow & Liu, 1968) to approximate the original BN and use it for inference. We refer to this approach as the CL-based method. In comparison with our method, CLbased method is always faster, but it is not as accurate as our method.\nOur scheme exploits the strong expressive capability of latent variable models. One can of course use other latent variable models instead of LTMs in the scheme. A straightforward choice is latent class model (LCM) (Hagenaars & McCutcheon, 2002). An LCM is an LTM with only one latent variable1. It assumes local independence, that is, the manifest variables are mutually independent conditioning on the latent variable. We also compare our method with this alternative. The results show that, under the same inferential complexity constraints, our method is more accurate than the LCM-based method.\nIt should be noted that our approximate scheme needs a lot of time in the offline phase. This is because that EM usually takes a long time to converge. Moreover, the time complexity of EM scales up linearly with the sample size, which should be set as large as possible to achieve high-quality approximation. Therefore, our method is suitable only for applications that allow a long offline phase.\nThe remainder of this paper is organized as follows. In Section 2, we review LTMs. In Section 3, we describe our method of constructing LTMs to approximate BNs. In Section 4, we describe our scheme for approximate inference formally. Section 5 reports empirical results. Section 6 discusses the relationship between our approach and existing work. Finally, in Section 7, we conclude this paper and point out some future directions.\n1. In machine learning community, LCM is also referred to as naive Bayes model with latent variable."
    }, {
      "heading" : "2. Latent Tree Model",
      "text" : "An LTM is a pair M = (m,θm). The first component m denotes the rooted tree and the set of cardinalities of the latent variables. We will refer to m as the model, and the rooted tree as the model structure. The second component θm denotes the collection of parameters in M. It contains a conditional probability table for each node given its parent.\nLet X and Y be the set of manifest variables and the set of latent variables in M, respectively. We use P (X,Y|m,θm), or PM(X,Y) in short, to denote the joint distribution represented by M. Two LTMs M and M′ are marginally equivalent if they share the same set of manifest variables X and PM(X) = PM′(X). A model m includes another model m′ if for any θm′ there exists θm such that (m,θm) and (m′,θm′) are marginally equivalent. Two models m and m′ are marginally equivalent if m includes m′ and vice versa.\nLet |Z| denote the cardinality of a variable Z. For a node Z in m, we use nb(Z) to denote the set of its neighbors. A model m is regular if for any latent node Y ,\n1. If Y has only two neighbors, then at least one of the neighbors must be a latent node and\n|Y | < ∏\nZ∈nb(Y )|Z| maxZ∈nb(Y )|Z| .\n2. If Y has more than two neighbors, then\n|Y | ≤ ∏\nZ∈nb(Y )|Z| maxZ∈nb(Y )|Z| .\nIf a model m is irregular, it is over-complicated. It can be reduced to a regular model m′ that is marginally equivalent to and contains fewer parameters than m (Zhang, 2004). In a regular model, a latent node Y is saturated if |Y | = Q\nZ∈nb(Y )|Z| maxZ∈nb(Y )|Z| . In this case, we say that\nY subsumes all its neighbors except the one with the largest cardinality."
    }, {
      "heading" : "3. Approximating Bayesian Networks with Latent Tree Models",
      "text" : "In this section, we study the problem of approximating a BN with an LTM. Let N be the BN to be approximated. Let X be the set of variables in N . For an LTM M to be an approximation of N , it should use X as its manifest variables, and the cardinalities of its latent variables should not exceed a predetermined threshold C. Figure 1(b), 1(c), and 1(d) show three example LTMs that approximate the BN in Figure 1(a). They will be used to illustrate various steps in our method.\nLet PN (X) be the joint distribution represented by N . An approximation M is of high quality if PM(X) is close to PN (X). We measure the quality of the approximation by the KL divergence (Cover & Thomas, 1991)\nD[PN (X)‖PM(X)] = ∑ X PN (X) log PN (X) PM(X) .\nOur objective is to find an LTM that minimizes the KL divergence, i.e.,\nM = argmin M D[PN (X)‖PM(X)].\nAn LTMM consists of two components, the modelm and the parameters θm. Therefore, the optimization problem can be naturally decomposed into two subproblems.\n1. Find an optimal model m .\n2. Optimize the parameters θ m for a given model m.\nIn the remainder of this section, we will discuss these two subproblems in details."
    }, {
      "heading" : "3.1 Parameter Optimization",
      "text" : "We start by addressing the second subproblem. Given a model m, our target is to find\nθ m = argmin θm\nD[PN (X)‖P (X|m,θm)].\nIt turns out that, due to the presence of latent variables, the KL divergence is difficult to directly minimize. This can be seen by expanding the KL divergence as follows,\nD[PN (X)‖P (X|m,θm)] = ∑ X PN (X) log PN (X)\nP (X|m,θm) =\n∑ X PN (X) logPN (X)− ∑ X PN (X) logP (X|m,θm)\n= ∑ X PN (X) logPN (X)− ∑ X PN (X) log ∑ Y P (X,Y|m,θm).\nThe first term on the last line can be neglected because it is independent of θm. The difficulty lies in maximizing the second term. The summation over latent variables Y appearing inside the logarithm makes this term indecomposable. Therefore, no closed-form solution can be obtained for θ m by taking the derivative of this term with respect to θm and setting it to zero.\nWe transform the problem into an asymptotically equivalent maximum likelihood estimation (MLE) problem. The idea is as follows.\n1. Generate a data set D with N independently and identically distributed samples from PN (X).\n2. Find the MLE of θm with respect to D, i.e., θ̂m = argmax\nθm P (D|m,θm).\nIt is well known that θ̂m converges almost surely to θ m as the sample size N approaches infinity (Huber, 1967).\nWe now discuss the implementation of this solution. We start by generating D from PN (X). Since PN (X) is represented by BN N , we use logic sampling (Henrion, 1988) for this task. Specifically, to generate a piece of sample from PN (X), we process the nodes in a topological ordering2. When handling node X, we sample its value according to the conditional distribution P (X|π(X) = j), where π(X) denotes the set of parents of X and j denote their values that have been sampled earlier. To obtain D, we repeat the procedure N times.\nGiven D, the next step is to find the MLE. Note that the values of latent variables Y are missing in D. We thus use the EM algorithm (Dempster et al., 1977). Starting with a random guess, the EM algorithm iteratively improves the estimate until the change in loglikelihoods of two consecutive iterations is smaller than a predetermined threshold. A practical issue is that EM can converge to local maxima on the likelihood surface. The local maxima can be far from the global maxima, and thus can be poor approximations to θ m. Fortunately, the local maxima issue is not severe for LTMs (Wang & Zhang, 2006). In practice, one can also use various techniques such as multiple restart (Chickering & Heckerman, 1997) and data permutation (Elidan et al., 2002) to alleviate this issue.\nNote that EM takes a long time to converge, especially when the sample size N is large. This is why our algorithm has an expensive offline phase.\n2. A topological ordering sorts the nodes in a DAG such that a node always precedes its children."
    }, {
      "heading" : "3.2 Exhaustive Search for the Optimal Model",
      "text" : "We now consider the first subproblem, i.e., to find the best model m . A straightforward way to solve this problem is to exhaust all possible models, find the optimal parameters θ m for each model m, compute the KL divergence D[PN (X)‖P (X|m,θ m)], and then return a model m with the minimum KL divergence.\nThe problem with this solution is its high computational complexity. Given a set of manifest variable X, there are infinitely many models. One can always obtain new models by inserting latent variables to an existing model. As we will show in Section 3.5, it is sufficient to consider a finite subspace, i.e., the subspace of regular models. However, there are still super-exponentially many regular models (Zhang, 2004). For each model, we need to optimize its parameters by running EM, which is a time-consuming process. Therefore, the exhaustive search is computationally infeasible. In the following 4 subsections, we will present a heuristic method."
    }, {
      "heading" : "3.3 Heuristic Construction of Model Structure",
      "text" : "We first present a heuristic for determining the model structure. In an LTM, two manifest variables are called siblings if they share the same parent. Our heuristic is based on two ideas: (1) In an LTM M, siblings are generally more closely correlated than variables that are located far apart; (2) If M is a good approximation of N , then two variables Xi and Xj are closely correlated in M if and only if they are closely correlated in N . So we can examine each pair of variables in N , pick the two variables that are most closely correlated, and introduce a latent variable as their parent in M.\nWe measure the strength of correlation between a pair of variables Xi and Xj by the mutual information (Cover & Thomas, 1991)\nIN (Xi;Xj) = ∑ Xi,Xj PN (Xi, Xj) log PN (Xi, Xj) PN (Xi)PN (Xj) .\nTo compute IN (Xi;Xj), one need to make inference in N . This could be computationally hard in the first place. So we use sampling technique to address this issue. Specifically, we generate a data set D with N samples from the BN N , and compute the empirical mutual information Î(Xi;Xj) using the empirical distribution P̂ (Xi, Xj) based on D. By the strong law of large numbers, Î(Xi;Xj) will almost surely converge to IN (Xi;Xj) as the sample size N goes to infinity.\nWe now use the BN shown in Figure 1(a) as an example to illustrate the idea. It contains 6 binary variables X1, X2, . . ., X6. Suppose the empirical mutual information based on some data set D is as presented in Table 1. As discussed above, we regard those as approximation to mutual information between variables in N and hence regard them as approximation to mutual information between variables in the final LTM M that we are to construct. We find that X4 and X6 are the pair with the largest mutual information. Therefore, we create a latent variable Y1 and make it the parent of X4 and X6.\nThe next step is to find, among Y1, X1, X2, X3, and X5, the pair of variables with the largest mutual information in M. There is one difficulty: Y1 is not in the original Bayesian network and hence not observed in the data set. The mutual information between Y1 and\nthe other variables cannot be computed directly. We hence seek an approximation. In the final model M, Y1 would d-separate X4 and X6 from the other variables. Therefore, for any X ∈ {X1, X2, X3, X5}, we have\nIM(Y1;X) ≥ IM(X4;X), IM(Y1;X) ≥ IM(X6;X). We hence approximate IM(Y1;X) using the lower bound\nmax{IM(X4;X), IM(X6;X)}. Back to our running example, the estimated mutual information between Y1 and X1, X2, X3, X5 is as presented in Table 2. We see that the next pair to pick is Y1 and X5. We introduce a latent variable Y2 as the parent of Y1 and X5. The process continues. The final model structure is a binary tree as shown in Figure 1(b)."
    }, {
      "heading" : "3.4 Cardinalities of Latent Variables",
      "text" : "After obtaining a model structure, the next step is to determine the cardinalities of the latent variables. We set the cardinalities of all the latent variables at a predetermined value C. In the following, we discuss how the choice of C influences quality of approximation and inferential efficiency.\nWe first discuss the impact of the value of C on the approximation quality. We start by considering the case when C equals to Cmax = ∏ X∈X |X|, i.e., the product of the cardinalities of all the manifest variables. In this case, each latent variable can be viewed as a joint variable of all the manifest variables. We can therefore set the parameters θm so that P (X|m,θm) = PN (X). That is, m can capture all the interactions among the manifest variables.\nWhat happens if we decrease C? It can be shown that the approximation quality will degrade. Let m be a model obtained with value C and m′ be another model obtained with a smaller value C ′. It is easy to see that m includes m′. The following lemma states that the approximation quality of m′ is no better than that of m.\nLemma 1 Let P (X) be a joint probability distribution of X. Let m and m′ be two models with manifest variables X. If m includes m′, then\nmin θm D[P (X)‖P (X|m,θm)] ≤ min θm′ D[P (X)‖P (X|m′,θm′)].\nProof: Define θ m′ = argmin\nθm′ D[P (X)‖P (X|m′,θm′)].\nBecause m includes m′, there must be parameters θ m of m such that\nP (X|m,θ m) = P (X|m′,θ m′).\nTherefore,\nmin θm D[P (X)‖P (X|m,θm)] ≤ D[P (X)‖P (X|m,θ m)] = D[P (X)‖P (X|m′,θ m′)] = min\nθm′ D[P (X)‖P (X|m′,θm′)]\nQ.E.D. As mentioned earlier, when C is large enough, model m can capture all the interactions among the manifest variables and hence can represent the joint distribution PN (X) exactly. If C is not large enough, we can only represent PN (X) approximately. According to the previous discussion, as C decreases, the approximation accuracy (in terms of KL divergence) will gradually degrade, indicating that modelm can capture less and less interactions among the manifest variables. The worst case occurs when C = 1. In this case, all the interactions are lost. The approximation accuracy is the poorest.\nThe parameter C also determines the computational cost of making inference in m. We use the clique tree propagation (CTP) algorithm for inference. So we measure the cost by the inferential complexity, which is defined to be the sum of the clique sizes in the clique tree of m. It is given by\n(|X| − 2) · C2 + ∑ X∈X |X| · C. (1)\nNote that |X| is the number of manifest variables, while |X| is the cardinality of a manifest variable X. Therefore, one can control the inferential complexity by changing the value of C. The smaller the value of C, the lower the complexity.\nIn summary, one can achieve a tradeoff between the approximation quality and the inferential complexity of the resultant model m by tuning the parameter C. In Figure 1(b), we set C = 8."
    }, {
      "heading" : "3.5 Model Regularization",
      "text" : "Suppose we have obtained a model m using the technique described in Section 3.3 and by setting the cardinalities of the latent variables at a certain value. In the following two subsections, we will show that it is sometimes possible to simplify m without compromising the approximation quality.\nWe first notice that m could be irregular. As an example, let us consider the model in Figure 1(b). It is constructed as an approximation to the BN N in Figure 1(a) with C = 8. By checking the latent variables, we find that Y5 violates the regularity condition. It has only two neighbors and |Y5| ≥ |X1|·|Y4|/max{|X1|, |Y4|}. Y1 and Y3 also violate the regularity condition because |Y1| > |X4|·|X6|·|Y2|/max{|X4|, |X6|, |Y2|} and |Y3| > |X2|·|X3|·|Y4|/max{|X2|, |X3|, |Y4|}. The following proposition suggests that irregular models should always be simplified until they become regular.\nProposition 1 If m is an irregular model, then there must exists a model m′ with lower inferential complexity such that\nmin θm D[PN (X)‖P (X|m,θm)] = min θm′ D[PN (X)‖P (X|m′,θm′)]. (2)\nProof: Let Y be a latent variable in m which violates the regularity condition. Denote its neighbors by Z1, Z2, . . . , Zk. We define another model m′ as follows:\n1. If Y has only two neighbors, then remove Y from m and connect Z1 with Z2.\n2. Otherwise, replace Y with a saturated latent variable Y ′, i.e.,\n|Y ′| = ∏k\ni=1|Zi| maxki=1 |Zi| .\nAs shown by Zhang (2004), for any parameters θm of m, there exists parameters θm′ of m′ such that (m,θm) and (m′,θm′) are marginally equivalent. The reverse is also true. Therefore, m and m′ are marginally equivalent. Equation 2 thus follows from Lemma 1.\nTo show that the inferential complexity of m′ is lower than that of m, we compare the clique trees of m and m′. Consider the aforementioned two cases:\n1. Y has only two neighbors. In this case, cliques {Y, Z1} and {Y, Z2} in the clique tree of m are replaced with {Z1, Z2} in the clique tree of m′. Assume |Z2| ≥ |Z1|. The difference in the sum of clique sizes is\nsum(m)− sum(m′) = |Y ||Z1|+ |Y ||Z2| − |Z1||Z2| ≥ |Z1||Z1|+ |Z1||Z2| − |Z1||Z2| = |Z1||Z1| > 0.\n2. Y has more than two neighbors. In this case, for all i = 1, 2, . . . , k, clique {Y, Zi} in the clique tree of m is replaced with a smaller clique {Y ′, Zi} in the clique tree of m′.\nIn both cases, the inferential complexity of m′ is lower than that of m. Q.E.D.\nThe proof of Proposition 1 presents a way to handle a latent variable that violates the regularity condition, i.e., either eliminating it or decreasing its cardinality. To regularize an irregular model, we handle all the latent variables in the order by which they are created in\nSection 3.3. In the following, we use the irregular model m in Figure 1(b) to demonstrate the regularization process.\nWe begin with latent variable Y1. It has three neighbors and violates the regularity condition. So we decrease its cardinality to |X4|·|X6|·|Y2|/max{|X4|, |X6|, |Y2|} = 4. Then we consider Y2. It satisfies the regularity condition and hence no changes are made. The next latent variable to examine is Y3. It violates the regularity condition. So we decrease its cardinality to |X2|·|X3|·|Y4|/max{|X2|, |X3|, |Y4|} = 4. We do not change Y4 because it does not cause irregularity. At last, we remove Y5, which has only two neighbors and violates the regularity condition, and connect Y4 with X1. We end up with the regular model m′ as shown in Figure 1(c)."
    }, {
      "heading" : "3.6 Further Simplifications",
      "text" : "After regularization, there are sometimes still opportunities for further model simplification. Take the model m′ in Figure 1(c) as an example. It contains two adjacent latent variables Y1 and Y2. Both variables are saturated. Y1 subsumes X4 and X6, and Y2 subsumes Y1 and X5. Y2 can be viewed as a joint variable of Y1 and X5, while Y1 can be in turn viewed as a joint variable of X4 and X6. Intuitively, we can eliminate Y1 and directly make Y2 the joint variable of X4, X5, and X6. This intuition is formalized by the following proposition.\nProposition 2 Let m be a model with more than one latent node. Let Y1 and Y2 be two adjacent latent nodes. If both Y1 and Y2 are saturated while Y2 subsumes Y1, then there exist another model m′ that is marginally equivalent to and has lower inferential complexity than m. Therefore,\nmin θm D[PN (X)‖P (X|m,θm)] = min θm′ D[PN (X)‖P (X|m′,θm′)].\nProof: We enumerate the neighbors of Y1 as Y2, Z11, Z12, . . . , Z1k, and the neighbors of Y2 as Y1, Z21, Z22, . . . , Z2l. Define another model m′ by removing Y1 from m and connecting Z11, Z12, . . . , Z1k to Y2. See Figure 2. We now prove that m and m′ are marginally equivalent, while the inferential complexity of m′ is lower than that of m.\nWe start by proving the marginal equivalence. For technical convenience, we will work with unrooted models. An unrooted model is a model with all directions on the edges dropped. Parameters of an unrooted model include a potential for each edge in the model. The potential is a non-negative function of the two variables that are connected by the edge. The concept of marginal equivalence can be defined the same way as for rooted models.\nAs shown by Zhang (2004), a model is marginally equivalent to its unrooted version. Therefore, to prove the marginal equivalence between m and m′, it is sufficient to show that the unrooted versions of m and m′ are marginally equivalent. For simplicity, we abuse m and m′ to denote the unrooted models. We also use f(·) to denote a potential in θm, and g(·) to denote a potential in θm′ .\nNote that Y1 and Y2 are saturated, while Y2 subsumes Y1. When all variables have no less than two states, this implies that:\n1. Y1 subsumes Z11, Z12, . . . , Z1k.\n2. Suppose that |Z2l| = maxlj=1 |Z2j |. Then Y2 subsumes Z21, Z22, . . . , Z2l−1.\nTherefore, a state of Y1 can be written as y1 =< z11, z12, . . . , z1k >, while a state of Y2 can be written as y2 =< y1, z21, z22, . . . , z2l−1 >. The latter can be further expanded as y2 =< z11, z12, . . . , z1k, z21, z22, . . . , z2l−1 >.\nWe first show that m′ includes m. Let θm be parameters of m. We define parameters θm′ of m′ as follows:\n• Potential for edge Y2 — Z2l: g(Y2 =< z11, z12, . . . , z1k, z21, z22, . . . , z2l−1 >,Z2l = z2l)\n= ∑ Y1,Y2 f(Y1, Y2) k∏ i=1 f(Y1, Z1i = z1i) l∏ j=1 f(Y2, Z2j = z2j).\n• Potential for edge Y2 — Z1i,∀i = 1, 2, . . . , k:\ng(Y2 =< z11, z12, . . . , z1k, z21, z22, . . . , z2l−1 >,Z1i = z′1i) = { 1 z1i = z′1i 0 otherwise\n• Potential for edge Y2 — Z2j ,∀j = 1, 2, . . . , l − 1:\ng(Y2 =< z11, z12, . . . , z1k, z21, z22, . . . , z2l−1 >,Z2j = z′2j) = { 1 z2j = z′2j 0 otherwise\n• Set the other potentials in θm′ the same as those in θm. It is easy to verify that\n∑ Y1,Y2 f(Y1, Y2) k∏ i=1 f(Y1, Z1i) l∏ j=1 f(Y2, Z2j) = ∑ Y2 k∏ i=1 g(Y2, Z1i) l∏ j=1 g(Y2, Z2j). (3)\nTherefore, P (X|m,θm) = P (X|m′,θm′). (4)\nNext, we prove that m includes m′. Given parameters θm′ of m′, we define parameters θm of m as follows:\n• Potential for edge Y1 — Y2:\nf(Y1 =< z11, z12, . . . , z1k >, Y2 = y2) = k∏\ni=1\ng(Y2 = y2, Z1i = z1i).\n• Potential for edge Y1 — Z1i,∀i = 1, 2, . . . , k:\nf(Y1 =< z11, z12, . . . , z1k >,Z1i = z′1i) = { 1 z1i = z′1i 0 otherwise\n• Set the other potentials in θm the same as those in θm′ . It can be verified that Equation 3 and 4 also hold. Therefore, m and m′ are marginally equivalent.\nWe now compare the inferential complexity of m and m′. According to the construction of m′, the clique tree of m′ is different from the clique tree of m in that it contains one less clique {Y1, Y2} and replaces clique {Y1, Z1i} with {Y2, Z1i} for all i = 1, 2, . . . , k. Therefore, the difference between the the sum of clique sizes is\nsum(m)− sum(m′) = |Y1||Y2|+ ∑ i |Y1||Z1i| − ∑ i |Y2||Z1i|\n= |Y2| ∏ i |Z1i|+ ∑ i |Y1||Z1i| − ∑ i |Y2||Z1i|\n= |Y2|( ∏ i |Z1i| − ∑ i |Z1i|) + ∑ i |Y1||Z1i|.\nThe first term on the last line is non-negative because ∏ i |Z1i| ≥ ∑\ni |Z1i| when |Z1i| ≥ 2 for all i = 1, 2, . . . , k. Therefore, the inferential complexity of m′ is always lower than that of m when Z1i is nontrivial. Q.E.D.\nGiven the regularized model, we check each pair of adjacent latent variables and apply Proposition 2 to eliminate redundant latent variables. We use the model m′ in Figure 1(c) as an example to demonstrate the process. The first pair to check are Y1 and Y2. Both of them are saturated while Y2 subsumes Y1. We thus remove Y1 and connect Y2 to X4 and X6. We then check Y3 and Y4. It turns out that Y3 is redundant. Therefore, we remove it and connect Y4 to X2 and X3. The last pair to check are Y2 and Y4. They are both saturated, but neither of them subsumes the other. Hence, they cannot be removed. The final model m′′ is shown in Figure 1(d)."
    }, {
      "heading" : "3.7 The Algorithm LTAB",
      "text" : "To summarize, we have outlined an algorithm for approximating BNs using LTMs. We call the algorithm LTAB, a shorthand for Latent Tree Approximation of Bayesian network. It has 3 inputs: a BN N , a predetermined cardinality C for latent variables, and a sample\nsize N . The output of LTAB is an LTM that approximates PN (X), the joint probability distribution represented by N . LTAB is briefly described as follows.\n1. Generate a data set D of N i.i.d. samples from PN (X). (Section 3.1) 2. Obtain an LTM structure by performing hierarchical clustering of variables, using\nempirical mutual information based on D as the similarity measure. (Section 3.3) 3. Set cardinalities of latent variables at C and simplify the model. (Section 3.4 – 3.6)\n4. Optimize parameters by running EM. (Section 3.1)\n5. Return the resultant LTM."
    }, {
      "heading" : "4. LTM-based Approximate Inference",
      "text" : "The focus of this paper is approximate inference in Bayesian networks. We propose the following two-phase method:\n1. Offline: Given a BN N , use LTAB to construct an approximation M. The sample size N should be set as large as possible, while the cardinality C should be determined to meet the requirement on inferential complexity.\n2. Online: Make inference in M instead of N . More specifically, given a piece of evidence E = e and a querying variable Q, return PM(Q|E = e) as an approximation to PN (Q|E = e)."
    }, {
      "heading" : "5. Empirical Results",
      "text" : "In this section, we empirically evaluate our approximate inference method. We first examine the impact of sample size N and cardinality C on the performance of our method. Then we compare the our method with CTP, LBP, the CL-based method, and the LCM-based method.\nWe used 8 networks in our experiments. They are listed in Table 3. CPCS54 is a subset of the CPCS network (Pradhan et al., 1994). The other networks are available at http://www.cs.huji.ac.il/labs/compbio/Repository/. Table 3 also reports the characteristics of the networks, including the number of nodes, the average/max indegree and cardinality of the nodes, and the inferential complexity (i.e., the sum of the clique sizes in the clique tree). The networks are sorted in ascending order with respect to the inferential complexity.\nFor each network, we simulated 500 pieces of evidence. Each piece of evidence was set on all the leaf nodes by sampling based on the joint probability distribution. Then we used the CTP algorithm and the approximate inference methods to compute the posterior distribution of each non-leaf node conditioned on each piece of evidence. The accuracy of an approximate method is measured by the average KL divergence between the exact and the approximate posterior distributions over all the query nodes and evidence.\nAll the algorithms in the experiments were implemented in Java and run on a machine with an Intel Pentium IV 3.2GHz CPU and 1GB RAM."
    }, {
      "heading" : "5.1 Impact of N and C",
      "text" : "We discussed the impact of N and C on the performance of our method in Section 3. This subsection empirically verifies the claims.\nThree sample sizes were chosen in the experiments: 1k, 10k, and 100k. For each network, we also chose a set of C. LTMs were then learned using LTAB with different combination of the values of N and C. For parameter learning, we terminated EM either when the improvement in loglikelihoods is smaller than 0.1, or when the algorithm ran for two months. The multiple restarting strategy by Chickering and Heckerman (1997) was used to avoid local maxima. The number of starting points was set at 16.\nThe running time of LTAB is plotted in Figure 3. The y-axes denote the time in hours, while the x-axes denote the parameter C for LTAB. The three curves correspond to different values of N . In general, the running time increases with N and C, ranging from seconds to weeks. For some settings, EM failed to converge in two months. Those settings are indicated by arrows in the plots. We emphasize that LTAB is executed offline and its running time should not be confused with the time for online inference, which will be reported next.\nAfter obtaining the LTMs, we used clique tree propagation to make inference. The approximation accuracy are shown in Figure 4. The y-axes denote the average KL divergence, while the x-axes still denote the parameter C for LTAB. There are five curves and one horizontal line in each plot. The three curves labeled as LTM are for our method, which correspond to the three sample sizes we used. The remaining two curves and the horizontal line are for the other approximate inference methods. We will discuss them in Sections 5.3 – 5.5.\nWe first examine the impact of sample size by comparing the corresponding curves in each plot. We find that, in general, the curves for larger samples are located below those for smaller ones. This shows that the approximation accuracy increases with the size of the training data.\nTo see the impact of C, we examine each individual curve from left to right. According to our discussion, the curve is expected to drop monotonically as C increases. This is generally true for the results with sample size 100k. For sample sizes 1k and 10k, however, there are cases in which the approximation becomes poorer as C increases. See Figure 4(e) and 4(f). This phenomenon does not conflict with our claims. As C increases, the\nexpressive power of the learned LTM increases. So it tends to overfit the data. On the other hand, the empirical distribution of a small set of data may significantly deviate from the joint distribution of the BN. This also suggests that the sample size should be set as large as possible.\nFinally, let us examine the impact of N and C on the inferential complexity. Figure 5 plots the running time for different methods to answer all the queries. For now, we only consider the three curves that are labeled as LTM. It can be seen that the three curves overlap in all plots. This implies that the running time is independent of the sample size N . On the other hand, all the curves are monotonically increasing. This confirms our claim that the inferential complexity is positively dependent on C.\nIn the following subsections, if not stated explicitly otherwise, we will only consider the results for N = 100k and the largest C. Under these settings, our method achieves the highest accuracy."
    }, {
      "heading" : "5.2 Comparison with CTP",
      "text" : "We now compare our method with CTP, a state-of-the-art exact inference algorithm. The first concern is that, how accurate is our method. By examining Figure 4, we argue that our method always achieves good approximation accuracy: For HAILFINDER, CPCS54, WATER, the average KL divergence of our method is around or less than 10−3; For the other networks, the average KL divergence is around or less than 10−2.\nWe next compare the inferential efficiency of our method and the CTP algorithm. The running time of CTP is denoted by dashed horizontal lines in the plots of Figure 5. It can be seen that our method is more efficient than the CTP algorithm. In particular, for the five networks with the highest inferential complexity, our method is faster than CTP by two to three orders of magnitude.\nTo summarize, the results suggest that our method can achieve good approximation accuracy at low computational cost."
    }, {
      "heading" : "5.3 Comparison with LBP",
      "text" : "We now compare our method with LBP. The latter is an iterative algorithm. It can be used as an anytime inference method by running a specific number of iterations. In our first set of experiments, we let LBP run as long as our method and compare their approximation accuracy. We did this for each network and each value of C. The accuracy of LBP are denoted by the curves labeled as LBP in Figure 4. By comparing those curves with the LTM curves for N = 100k, we see that our method achieves significantly higher accuracy than LBP in most cases: For WATER, the difference in average KL divergence is up to three orders of magnitude; For the other networks, the difference is up to one order of magnitude. For HAILFINDER with C = 32, LBP is two times more accurate than our method. However, our method also achieves good approximation accuracy in this case. The average KL divergence is smaller than 10−3. Finally, we noticed that LBP curves are horizontal lines for CPCS54, MILDEW, and BARLEY. Further investigation on those cases shows that LBP finished only one iteration in the given time period.\nWe next examine how much time it takes for LBP to achieve the same level of accuracy as our method. For each piece of evidence, we ran LBP until its average KL divergence\nis comparable with that of our method or the number of iterations exceeds 100. The running time of LBP are denoted by the curves labeled as LBP in Figure 5. Comparing those curves with the LTM curves, we found that LBP takes much more time than our method: For MILDEW, LBP is slower than our method by three orders of magnitude; For the other networks except HAILFINDER, LBP is slower by one to two orders of magnitude; For HAILFINDER with C = 32, the running time of the two methods are similar. The results show that our method compares more favorably to LBP in the networks that we examined."
    }, {
      "heading" : "5.4 Comparison with CL-based Method",
      "text" : "In this subsection, we compare our method with the CL-based method. More specifically, for each network, we learn a tree model from the 100k samples using the maximum spanning tree algorithm developed by Chow and Liu (1968). We then use the learned tree model to answer the queries.\nThe approximation accuracy of the CL-based method are shown as solid horizontal lines in the plots in Figure 4. Comparing with the CL-based method, our method achieves higher accuracy in all the networks except for MILDEW. For INSURANCE, WATER, and BARLEY, the differences are significant. For MILDEW, our method is competitive with the CL-based method. In the meantime, we notice that the CL-based method achieves good approximations in all the networks except for BARLEY. The average KL divergence is around or less than 10−2.\nAn obvious advantage of CL-based method is its high efficiency. This can be seen from the plots in Figure 5. In most of the plots, the CL line locates below the second data point on the LTM curve. The exception is MILDEW, for which the running time of the CL-based method is as long as our method with C = 16.\nIn summary, the results suggest that the CL-based method is a good choice for approximate inference if the online inference time is very limited. Otherwise, our method is more attractive because it is able to produce more accurate results when more time is allowed."
    }, {
      "heading" : "5.5 Comparison with LCM-based Method",
      "text" : "Lowd and Domingos (2005) have previously investigated the use of LCM for density estimation. Given a data set, they determine the cardinality of the latent variable using hold-out validation, and optimize the parameters using EM. It is shown that the learned LCM achieves good model fit on a separate testing set. The LCM was also used to answer simulated probabilistic queries and the results turn out to be good.\nInspired by their work, we also learned a set of LCMs from the 100k samples and compared them with LTMs on the approximate inference task. Our learning strategy is slightly different. Since LCM is a special case of LTM, its inferential complexity can also be controlled by changing the cardinality of the latent variable. In our experiments, we set the cardinality such that the sum of the clique sizes in the clique tree of the LCM is roughly the same as that for the LTM learned with a chosen C. In this way, the inferential complexity of the two models are comparable. This can be verified by examining the LCM curves in Figure 5. We then optimize the parameters of the LCM using EM with the same setting as in the case of LTM.\nAs shown in Figure 4, for ALARM, WIN95PTS, CPCS54, WATER, and BARLEY, the LCM curves are located above the LTM curves. That is, our method consistently outperforms\nthe LCM-based method for all C. For HAILFINDER and MILDEW, our method is worse than the LCM-based method when C is small. But when C becomes large, our method begins to win. For INSURANCE, the performance of the two methods are very close. The results suggest that unrestricted LTM is more suitable for approximation inference than LCM does."
    }, {
      "heading" : "6. Related Work",
      "text" : "The idea of approximating complex BNs by simple models and using the latter to make inference has been investigated previously. The existing work mainly falls into two categories. The work in the first category approximates the joint distributions of the BNs and uses the approximation to answer all probabilistic queries. In contrast, the work in the second category is query-specific. It assumes the evidence is known and directly approximates the posterior distribution of the querying nodes.\nOur method falls in the first category. We investigate the use of LTMs under this framework. This possibility has also been studied by Pearl (1988) and Sarkar (1995). Pearl (1988) develops an algorithm for constructing an LTM that is marginally equivalent to a joint distribution P (X), assuming such an LTM exists. Sarkar (1995) studies how to build good LTMs when only approximations are amenable. Their methods, however, can only deal with the cases of binary variables.\nResearchers have also explored the use of other models. Chow and Liu (1968) consider tree-structured BNs without latent variables. They develop a maximum spanning tree algorithm to efficiently construct the tree model that is closest to the original BN in terms of KL divergence. Lowd and Domingos (2005) learn an LCM to summarize a data set. The cardinality of the latent variable is determined so that the logscore on a hold-out set is maximized. They show that the learned model achieves good model fit on a separate testing set, and can provide accurate answers to simulated probabilistic queries. In both work, the approximation quality and the inferential complexity of the learned model are fixed. Our method, on the other hand, provides a parameter C to let users make the tradeoff between approximation quality and inferential complexity.\nThe work in the second category is mainly carried out under the variational framework. The mean field method (Saul, Jaakkola, & Jordan, 1996) assumes that the querying nodes are mutually independent. It constructs an independent model that is close to the posterior distribution. As an improvement to the mean field method, the structured mean field method (Saul & Jordan, 1996) preserves a tractable substructure among the querying nodes, rather than neglecting all interactions. Bishop et al. (1997) consider another improvement, i.e., mixtures of mean field distributions. It essentially fits an LCM to the posterior distribution. All these methods directly approximate posterior distributions. Therefore, they might be more accurate than our method when used to make inference. However, these methods are evidence-specific and construct approximations online. Moreover, they involve an iterative process for optimizing the variational parameters. Consequently, the online running time is unpredictable. With our method, in contrast, one can determine the inferential complexity beforehand."
    }, {
      "heading" : "7. Concluding Remarks",
      "text" : "We propose a novel scheme for BN approximate inference using LTMs. With our scheme one can trade off between the approximation accuracy and the inferential complexity. Our scheme achieves good accuracy at low costs in all the networks that we examined. In particular, it consistently outperforms LBP. We also show that LTMs are superior to LCMs when used for approximate inference.\nThe current bottleneck of the offline phase is parameter learning. We used EM algorithm to optimize parameters, which is known to be time consuming. The problem is especially severe when the parameter C and the sample size are large. One way to speed up parameter learning is to adapt the agglomerative clustering technique for learning the cardinality of a latent variable from data (Elidan & Friedman, 2001). The basic idea is to complete the training data by setting the cardinality of the latent variable large enough and assigning each record to a latent state. In each step, one selects two states of the latent variable to merge. The process repeats until the (penalized) likelihood ceases to improve. For our parameter learning problem, we can terminate the process when the desired cardinality C is achieved. We also need to deal with multiple latent variables. Since the data set is completed, we expect this method to yield a good starting point for EM in a very short time, which will in turn drastically shorten the offline phase."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Haipeng Guo and Yiping Ke for insightful discussions. We are also grateful to the anonymous reviewers for their valuable comments and suggestions on the earlier version of this paper. Research on this work was supported by Hong Kong Grants Council Grants #622105 and #622307, and the National Basic Research Program of China (aka the 973 Program) under project No. 2003CB517106. The work was completed when the first author was on leave at the HKUST Fok Ying Tung Graduate School, Guangzhou, China."
    } ],
    "references" : [ {
      "title" : "Approximating posterior distributions in belief networks using mixtures",
      "author" : [ "C.M. Bishop", "N. Lawrence", "T. Jaakkola", "M.I. Jordan" ],
      "venue" : "In Proceedings of the 10th Conference on Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bishop et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Bishop et al\\.",
      "year" : 1997
    }, {
      "title" : "Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables",
      "author" : [ "D.M. Chickering", "D. Heckerman" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Chickering and Heckerman,? \\Q1997\\E",
      "shortCiteRegEx" : "Chickering and Heckerman",
      "year" : 1997
    }, {
      "title" : "Approximating discrete probability distributions with dependence trees",
      "author" : [ "C.K. Chow", "C.N. Liu" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Chow and Liu,? \\Q1968\\E",
      "shortCiteRegEx" : "Chow and Liu",
      "year" : 1968
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.R. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "Learning the dimensionality of hidden variables",
      "author" : [ "G. Elidan", "N. Friedman" ],
      "venue" : "In Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Elidan and Friedman,? \\Q2001\\E",
      "shortCiteRegEx" : "Elidan and Friedman",
      "year" : 2001
    }, {
      "title" : "Data perturbation for escaping local maxima in learning",
      "author" : [ "G. Elidan", "M. Ninio", "N. Friedman", "D. Schuurmans" ],
      "venue" : "In Proceedings of the 18th National Conference on Artificial Intelligence,",
      "citeRegEx" : "Elidan et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Elidan et al\\.",
      "year" : 2002
    }, {
      "title" : "A revolution: belief propagation in graphs with cycles",
      "author" : [ "B.J. Frey", "D.J.C. MacKay" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Frey and MacKay,? \\Q1997\\E",
      "shortCiteRegEx" : "Frey and MacKay",
      "year" : 1997
    }, {
      "title" : "Applied Latent Class Analysis",
      "author" : [ "J.A. Hagenaars", "A.L. McCutcheon" ],
      "venue" : null,
      "citeRegEx" : "Hagenaars and McCutcheon,? \\Q2002\\E",
      "shortCiteRegEx" : "Hagenaars and McCutcheon",
      "year" : 2002
    }, {
      "title" : "Propagating uncertainty in Bayesian networks by probabilistic logic sampling",
      "author" : [ "M. Henrion" ],
      "venue" : "In Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Henrion,? \\Q1988\\E",
      "shortCiteRegEx" : "Henrion",
      "year" : 1988
    }, {
      "title" : "The behavior of maximum likelihood estimates under nonstandard conditions",
      "author" : [ "P.J. Huber" ],
      "venue" : "In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability,",
      "citeRegEx" : "Huber,? \\Q1967\\E",
      "shortCiteRegEx" : "Huber",
      "year" : 1967
    }, {
      "title" : "Naive Bayes models for probability estimation",
      "author" : [ "D. Lowd", "P. Domingos" ],
      "venue" : "In Proceedings of the 22nd International Conference on Machine Learning,",
      "citeRegEx" : "Lowd and Domingos,? \\Q2005\\E",
      "shortCiteRegEx" : "Lowd and Domingos",
      "year" : 2005
    }, {
      "title" : "Loopy belief propagation for approximate inference: an empirical study",
      "author" : [ "K.P. Murphy", "Y. Weiss", "M.I. Jordan" ],
      "venue" : "In Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Murphy et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Murphy et al\\.",
      "year" : 1999
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Knowledge engineering for large belief networks",
      "author" : [ "M. Pradhan", "G. Provan", "B. Middleton", "M. Henrion" ],
      "venue" : "In Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Pradhan et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Pradhan et al\\.",
      "year" : 1994
    }, {
      "title" : "Modeling uncertainty using enhanced tree structures in expert systems",
      "author" : [ "S. Sarkar" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics,",
      "citeRegEx" : "Sarkar,? \\Q1995\\E",
      "shortCiteRegEx" : "Sarkar",
      "year" : 1995
    }, {
      "title" : "Mean field theory for sigmoid belief networks",
      "author" : [ "L.K. Saul", "T. Jaakkola", "M.I. Jordan" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Saul et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Saul et al\\.",
      "year" : 1996
    }, {
      "title" : "Exploiting tractable substructures in intractable networks",
      "author" : [ "L.K. Saul", "M.I. Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Saul and Jordan,? \\Q1996\\E",
      "shortCiteRegEx" : "Saul and Jordan",
      "year" : 1996
    }, {
      "title" : "Severity of local maxima for the em algorithm: Experiences with hierarchical latent class models",
      "author" : [ "Y. Wang", "N.L. Zhang" ],
      "venue" : "In Proceedings of the 3rd European Workshop on Probabilistic Graphical Models,",
      "citeRegEx" : "Wang and Zhang,? \\Q2006\\E",
      "shortCiteRegEx" : "Wang and Zhang",
      "year" : 2006
    }, {
      "title" : "Hierarchical latent class models for cluster analysis",
      "author" : [ "N.L. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Zhang,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "They are previously known as hierarchical latent class models (Zhang, 2004).",
      "startOffset" : 62,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "Pearl (1988) was the first to identify LTMs as a potentially useful class of models.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 12,
      "context" : "We compared our method with loopy belief propagation (LBP) (Pearl, 1988), a standard approximate inference method which has been successfully used in many real world domains (Frey & MacKay, 1997; Murphy, Weiss, & Jordan, 1999).",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "It can be reduced to a regular model m′ that is marginally equivalent to and contains fewer parameters than m (Zhang, 2004).",
      "startOffset" : 110,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "It is well known that θ̂m converges almost surely to θ m as the sample size N approaches infinity (Huber, 1967).",
      "startOffset" : 98,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "Since PN (X) is represented by BN N , we use logic sampling (Henrion, 1988) for this task.",
      "startOffset" : 60,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "We thus use the EM algorithm (Dempster et al., 1977).",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : "In practice, one can also use various techniques such as multiple restart (Chickering & Heckerman, 1997) and data permutation (Elidan et al., 2002) to alleviate this issue.",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "However, there are still super-exponentially many regular models (Zhang, 2004).",
      "startOffset" : 65,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "As shown by Zhang (2004), for any parameters θm of m, there exists parameters θm′ of m′ such that (m,θm) and (m′,θm′) are marginally equivalent.",
      "startOffset" : 12,
      "endOffset" : 25
    }, {
      "referenceID" : 18,
      "context" : "As shown by Zhang (2004), a model is marginally equivalent to its unrooted version.",
      "startOffset" : 12,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "CPCS54 is a subset of the CPCS network (Pradhan et al., 1994).",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "The multiple restarting strategy by Chickering and Heckerman (1997) was used to avoid local maxima.",
      "startOffset" : 36,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "More specifically, for each network, we learn a tree model from the 100k samples using the maximum spanning tree algorithm developed by Chow and Liu (1968). We then use the learned tree model to answer the queries.",
      "startOffset" : 136,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "More specifically, for each network, we learn a tree model from the 100k samples using the maximum spanning tree algorithm developed by Chow and Liu (1968). We then use the learned tree model to answer the queries. The approximation accuracy of the CL-based method are shown as solid horizontal lines in the plots in Figure 4. Comparing with the CL-based method, our method achieves higher accuracy in all the networks except for MILDEW. For INSURANCE, WATER, and BARLEY, the differences are significant. For MILDEW, our method is competitive with the CL-based method. In the meantime, we notice that the CL-based method achieves good approximations in all the networks except for BARLEY. The average KL divergence is around or less than 10−2. An obvious advantage of CL-based method is its high efficiency. This can be seen from the plots in Figure 5. In most of the plots, the CL line locates below the second data point on the LTM curve. The exception is MILDEW, for which the running time of the CL-based method is as long as our method with C = 16. In summary, the results suggest that the CL-based method is a good choice for approximate inference if the online inference time is very limited. Otherwise, our method is more attractive because it is able to produce more accurate results when more time is allowed. 5.5 Comparison with LCM-based Method Lowd and Domingos (2005) have previously investigated the use of LCM for density estimation.",
      "startOffset" : 136,
      "endOffset" : 1382
    }, {
      "referenceID" : 9,
      "context" : "This possibility has also been studied by Pearl (1988) and Sarkar (1995).",
      "startOffset" : 42,
      "endOffset" : 55
    }, {
      "referenceID" : 9,
      "context" : "This possibility has also been studied by Pearl (1988) and Sarkar (1995). Pearl (1988) develops an algorithm for constructing an LTM that is marginally equivalent to a joint distribution P (X), assuming such an LTM exists.",
      "startOffset" : 42,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "This possibility has also been studied by Pearl (1988) and Sarkar (1995). Pearl (1988) develops an algorithm for constructing an LTM that is marginally equivalent to a joint distribution P (X), assuming such an LTM exists.",
      "startOffset" : 42,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "This possibility has also been studied by Pearl (1988) and Sarkar (1995). Pearl (1988) develops an algorithm for constructing an LTM that is marginally equivalent to a joint distribution P (X), assuming such an LTM exists. Sarkar (1995) studies how to build good LTMs when only approximations are amenable.",
      "startOffset" : 42,
      "endOffset" : 237
    }, {
      "referenceID" : 1,
      "context" : "Chow and Liu (1968) consider tree-structured BNs without latent variables.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Chow and Liu (1968) consider tree-structured BNs without latent variables. They develop a maximum spanning tree algorithm to efficiently construct the tree model that is closest to the original BN in terms of KL divergence. Lowd and Domingos (2005) learn an LCM to summarize a data set.",
      "startOffset" : 0,
      "endOffset" : 249
    }, {
      "referenceID" : 0,
      "context" : "Bishop et al. (1997) consider another improvement, i.",
      "startOffset" : 0,
      "endOffset" : 21
    } ],
    "year" : 2008,
    "abstractText" : "We propose a novel method for approximate inference in Bayesian networks (BNs). The idea is to sample data from a BN, learn a latent tree model (LTM) from the data offline, and when online, make inference with the LTM instead of the original BN. Because LTMs are tree-structured, inference takes linear time. In the meantime, they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good. Empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost.",
    "creator" : "dvips(k) 5.94b Copyright 2004 Radical Eye Software"
  }
}