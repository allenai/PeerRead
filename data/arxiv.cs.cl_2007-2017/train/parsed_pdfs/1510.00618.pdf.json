{
  "name" : "1510.00618.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Automatic Taxonomy Extraction from Query Logs with No Additional Sources of Information",
    "authors" : [ ],
    "emails" : [ "miguelfernandez@tuenti.com", "dani@uniovi.es" ],
    "sections" : [ {
      "heading" : null,
      "text" : "more people use search engines on a daily basis, important trails of users common knowledge are being recorded in those files. Previous research has shown that it is possible to extract concept taxonomies from full text documents, while other scholars have proposed methods to obtain similar queries from query logs. We propose a mixture of both lines of research, that is, mining query logs not to find related queries nor query hierarchies, but actual term taxonomies that could be used to improve search engine effectiveness and efficiency. As a result, in this study we have developed a method that combines lexical heuristics with a supervised classification model to successfully extract hyponymy relations from specialization search patterns revealed from log missions, with no additional sources of information, and in a language independent way.\nKeywords: Web search, query log, hyponymy relations, query reformulation, query classification, automatic taxonomy extraction.\n1 Introduction\nWeb search is becoming a common habit among internet users (Fallows, 2008). Hence, the amount of data in query logs is firmly increasing every day, recording a great deal of users’ common knowledge and interactions. As (Pasca and Van Durme, 2007) pointed out “If knowledge is generally prominent or relevant, people will (eventually) ask about it”. Nervertheless, searching is not a straightforward process, instead, the users gradually refine both their queries and their goals in a process referred by (Spink, 1998) as the successive search phenomenon. During this iterative process the users provide successive queries revealing different search patterns (Boldi et al., 2009). The most relevant ones for this proposal are the so-called Specialization pattern; and its antisymmetric operation, the Generalization pattern. Given a pair of queries (qi ,qi+1 ), Specialization occurs when a new query qi+1 is focused on increasing the precision of the previous query qi . In some cases a specialization can be automatically identified because qi is a substring of qi+1 (e.g. video game sales and arcade videogame sales), in others, some of the terms are shared among both queries and the difference is more specific in qi+1 (e.g. bird food and canary food); a specialization can even occur when none of the terms are shared among the\nquery (e.g.: outdoor activities and camping). Generalization comes to increase the previous query recall by looking for more generic information (e.g.: white-water rafting and extreme sports).\nIt must be noticed that when considering groups of queries we are not interested in all the queries issued by a user during one sitting (i.e. a searching episode) but in much shorter fragments where all the queries are topically related. The advantages of using such topical sessions are two-fold: (1) the data to be considered in order to find semantic relations between terms is much more focused on, and\n(2) such granularity level should dispel most of the privacy issues even if no de-identification was used (Xiong and Agichtein, 2007). In order to obtain such query log segmentation we have used a technique which has proved to attain similar results to those achieved by a human expert (Gayo-Avello, 2009). Such technique allows us to group topically related queries even when those queries do not share any common term.\n2 Motivation\nTaxonomies are made up of terms connected by hyponymy relations. The deductive power of hyponymy allows the application of reasoning schemas based upons structural subsumption (Baader, 2003), and hence, by using taxonomies it should be possible to greatly improve search engine effectiveness and efficiency by means of term disambiguation, and semantic query suggestion and expansion. For these same purposes, other lexical databases such as Wordnet could be applied (e.g. WordNet (Miller, 1990)) but we feel they present several lacks in order to be really useful. First, because WordNet is an English language project, parallel projects for other languages have been developed, such as EuroWordNet (Ellman, 2003), BalkaNet (Greek), Hebrew WordNet, Hindi WordNet and Japanese WordNet among others (Vossen and Fellbaum, 2004). Certainly we could rely on such different wordnets but the task of identifying the language in which queries are written is not trivial given the small number of terms usually employed. Additionally, there exist a huge gap between the lexicon used by Web users and the developers of wordnets. For instance, (Mandala et al., 1999) and (Gabrilovich and Markovitch, 2007) pointed out that most domain-specific relationships between words cannot be found in WordNet, and some kind of words, such as proper names, jargon or slang are just not included. Besides, (Mihalcea, 2003) also explained that due to the fact that professional linguists recognize minimal differences in word senses, common words such as “make” have too many different senses to be useful for IR tasks. Of course, these wordnets could be automatically enriched (Hearst, 1992) but such approach require a great effort (usually carried out by linguists) and, hence, wordnets remain as quite static data sources. On the other hand, most of the previous studies aimed at building term taxonomies – e.g. (Hearst, 1992), (Berland and Charniak, 1999), (Caraballo, 1999), (Girju et al., 2003), (Morin and Jacquemin, 2004). not only need large text corpora but they are also tightly coupled to the grammar rules of the target language. This would make their application to query logs extremely difficult (if not totally unfeasible) given the nature of the queries which are short and, many times, simply ungrammatical. Thus, we feel that taxonomies of terms and noun phrases collecting the common knowledge of search engine users, including typos, jargon and slang are a real need in order to improve the performance of Web search engines. Besides, we think that the only way to obtain such users’ mental model is by mining the query logs collecting the users queries. As a consequence, the following research questions come to light:\n(q1) Is it possible to automatically generate term taxonomies containing the common vocabulary em-\nployed by web users?\n(q2) If it is so, can it be done by using just the information contained in the query log, with no\nadditional sources of information?\n(q3) Is it possible to do all the above in a language-independent way?\nThroughout the following sections we describe our method to mine hyponymy relations from query logs, how we have applied it to the AOL and MSN datasets, the results we have obtained from its application, and finally, the implications and future lines of research of the study.\n3 Prior work\nThe idea depicted in this paper is somehow related to previous and on-going works. We will briefly review those which are most relevant, then, we will point out the main differences between such works and our approach. First, it must be said that the idea of automatically building term taxonomies is not new and several approaches have already been proposed to work on full text documents. Works such as (Hearst, 1992) , (Berland and Charniak, 1999), (Caraballo, 1999), (Girju et al., 2003), (Morin and Jacquemin, 2004), among others, are extremely relevant but they cannot be straightforwardly applied to query logs, because most of these techniques require lexico-syntactic patterns and POS tagging which are hardly useful when applied to Web search queries.\nWith regards to those works relying on query logs or folksonomies, there have been two main goals:\n(1) organizing the queries/tags in hierarchical arrangements (but not actual taxonomies), and (2) automatically obtaining similar queries/tags.\nThus, (Clough et al., 2005) and (Schmitz, 2006) applied subsumption to image tags in order to obtain tag hierarchies. Such hierarchies, however, were not taxonomies because no hyponymy relations were established; instead, the tags were arranged with regards to their specificity (e.g. church ← tower ← bell tower, sanfrancisco ← goldengate). (Heymann and Garcia-Molina, 2006), (Mika, 2005), and (Schwarzkopf et al., 2007) developed rather similar works; they also employed tag collections (although not image tags) and described different techniques to obtain concept hierarchies. Again, such hierarchies were not proper taxonomies. With regards to the field of query suggestion there exist abundant literature; we will just refer to two recent works that could be confused with our proposal. For instance, (Shen et al., 2007) and (Baeza-Yates and Tiberi, 2007) describe two methods to generate related queries for a given one by exploiting the data within the query log; however, neither of such methods produces a proper taxonomy the way we suggest.\nApproaches by other authors could be wrongly considered similar to our approach. For instance, (Chuang and Chien, 2003) describe a method to classify query terms into a predefined category system; thus, it is much closer to query topic classification than to taxonomy bootstrapping. Other works by the same authors such as (Chuang and Chien, 2004) and (Chuang and Chien, 2005), describe methods to obtain term hierarchies but such hierarchies are, in fact, clusters and not taxonomies. There also exist interesting works in the field of information extraction. For instance, (Pasca et al., 2006) and (Pasca, 2007) describe a technique to obtain class attributes from query logs (e.g. finding that population, flag\nor president are attributes for Country). The same author also provides a method to find named-entities (Pasca and Van Durme, 2007) which is related to (Sekine and Suzuki, 2007) and (Komachi and Suzuki, 2008). None of these works, however, are related to our approach because they do not generate term taxonomies.\nThus, our proposal, although somehow related to all the aforementioned research is different in several aspects. Different from classic works –e.g. (Hearst, 1992), (Berland and Charniak, 1999), (Caraballo, 1999), (Girju et al., 2003) and (Morin and Jacquemin, 2004) in that it does not rely on full text documents but on query logs. It also differs from (Clough et al., 2005), (Heymann and Garcia-Molina, 2006), (Schmitz, 2006), (Mika, 2005), (Baeza-Yates and Tiberi, 2007) and (Schwarzkopf et al., 2007) in the underlying goal: while those methods obtain tag or query hierarchies according to their specificity, we are interested in automatically building actual taxonomies (i.e. hierarchical arrangements according to hyponymy relations). We have also exposed that other works such as (Chuang and Chien, 2003), (Chuang and Chien, 2004), (Chuang and Chien, 2005), (Pasca et al., 2006), (Pasca and Van Durme, 2007), (Pasca, 2007), (Sekine and Suzuki, 2007) and (Komachi and Suzuki, 2008) are in fact dealing with problems which are totally unrelated to taxonomy construction.\n4 Method applied\nOur method relies on extracting pairs of terms or noun phrases from a series of query specialization patterns identified from topical query sessions. Broadly, the outline of this process consists of the following three activities:\n1. Sessionize and filter the log obtaining sets of non-navigational queries targeted at solving a par-\nticular information need.\n2. Identify query pairs revealing query specialization/generalization patterns from that sessions.\n3. For each of the above pairs, point out a group of hyponymy candidates over which, at most, one\ninstance will be chosen as a true hyponymy relation.\nDuring this work, we have developed two lines of experiments. The former was presented in (Fernandez-Fernandez and Gayo-Avello, 2009) and implemented the identification of specialization search patterns by only taking into account lexico-syntactic aspects about the queries – mainly, addition and subtraction of terms. The second one, evolved from the need of increasing the method’s performance, applies a supervised method devised by Bonchi et al. (Boldi et al., 2009) which takes into account the lexical and temporal features of query pairs, in addition to session-related information. Both lines of research are complementary: On the one side, lexical identification performs well in cases in which both queries of a pair share some terms – e.g. wild animal photographs and lion photographs, or naked celebrities and naked angelina jolie – yet it does not allow taxonomy extraction over query pairs that do not have any term in common, such as golden globe and film awards, that can be discovered using machine learning.\n4.1 Sessionization and navigational query removal\n4.1.1 Topical session detection\nTopical sessions, or missions (Boldi et al., 2009) are sets of queries submitted by the same user pursuing the resolution of a single information need. There exist a wide range of studies describing methods to reveal topical sessions from query logs. The work by Gayo-Avello (Gayo-Avello, 2009) surveys the state of the art on this field, and describes a new method that takes into account both the lexical and temporal dimensions of a pair of queries, in order to determine whether they belong to the same session or not. This method, called Geometric, performs better than the others surveyed by the author (F1.5 = 0.82), and hence it has been the one chosen to sessionized our data sets.\nProvided the character 3-gram vectors of a pair of queries, and their submission timestamps, our implementation calculates the (x, y) coordinates for the 2-dimensional space characterized by x) the lexical resemblance between both queries (cosine); and y) their temporal similarity, linearly normalized in the interval [0,1], given a maximum timespan of half-an-hour. Membership of the same session will be determined if the point (x,y) overtakes the boundaries of a circle with the center in (1,1) and radius being 1. (Figure 1).\n4.1.2 Removal of navigational queries\nAccording to (Broder, 2002), there are three kinds of queries in relation to their intent: (1) Navigational, when the immediate intent is to reach a particular site; (2) Informational, when the intent is to acquire some information assumed to be present on one or more web pages; and (3) Transactional, when the intent is to perform some web-mediated activity (e.g. to buy a product, or download a file). We think that navigational queries can reduce the accuracy of the hyponym extraction process and, thus, such queries should be removed. Because the intent behind navigational queries is to reach a particular site, most of them are lexically similar to the URL of the referred site and so, a simple heuristic to detect\nthem (Jansen et al., 2008) consists in marking a query as navigational if it matches at least one of the following criteria:\n• Contains company/business/organization/people names.\n• Contains domains suffixes.\n• Has “Web” as the source.\n• Its length (i.e., number of terms) is less than 3; and searcher clicked on the first results page.\nA straightforward application of this criteria leads to wrongly classify too many queries as navigational. Because of this, we have relaxed the heuristic and filtered as navigational only those queries containing 1) well known website names (e.g. google, wikipedia, etc), 2) domain suffixes (e.g. com, net, ... , co.uk, etc), or 3) strings frequently present in URLs such as www. or http://.\n4.2 Reformulation pattern identification\nThe output of the previous activity is a collection of non-navigational topically-related sets of queries. Over them we identify pairs of queries in which one of its elements asks for more precise information.\n4.2.1 Lexical identification\nA specialization occurs when a query q′ looks for information about the same topic as a previous query q, but in a more specific way. A generalization occurs when the user wants to increase search recall by reaching more relevant documents. Both patterns are antisymmetric, meaning that there exist a specialization in (q,q′) if, and only if (q′, q) make up a generalization.\nIn order to detect such patterns, works like (He et al., 2002) rely on the lexical similarity between both queries in such a way that one query specializes another if it adds terms to it. A trivial scenario occurs when q is a substring of q’ (e.g. fish food and tropical fish food). A not so trivial scenario occurs when q′ not only adds some terms to q, but also removes others, as it happens in the case of the queries celebrity scandals and charly sheen scandals. This kind of specialization –we call it specialization with reformulation– can be seen as a parallel move on the session, because the second query could look for slightly different information than the former, but if we pay attention to their number of results (11M vs. 0.4M respectively) we can see that the second one is much more specific, and thus, it is subsumed by the first one. A different case is that of the pair electronic repairs and iphone repairs, whose number of results swings in the same order of magnitude (340M vs. 550M)\n4.2.2 Supervised identification\nA third kind of specialization occurs when q and q′ do not share any term, but q′ looks for more specific information, as in the case of the pair outdoor activities and camping. These patterns cannot be identified by attending only to the lexical criteria described above.\nIn (Bonchi et al., 2009), the authors describe a machine learning approach that applying a queue of\nbinary classifiers in cascade, is able to classify a pair of queries into the following equivalence classes:\ngeneralization (lion, wild animals); specialization (ikea furniture, corner units), error correction (califrnia, california); parallel move, when queries look for something related, but not similar (hotel in Dublin, flights to Dublin); and session shift, when both queries are not aimed at solving the same information need and hence they do not belong to the same session.\nGiven that our input in this task are queries that belong to the same session, and were not previously identified as either trivial specializations or specializations with reformulation, our problem is reduced to the application of only two of the binary classifiers (those targeted at identifying specialization and generalization). The moment the query does not match any of the former categories, it is discarded. (Figure 2).\nThe classification algorithm used in our work is J48, an open source implementation of the wellknown C4.5 decision tree induction algorithm (Quinlan, 1993). As in the original paper, the algorithm was trained with a manually labeled sample of 3000 query pairs, but in our case, we limited them to those that 1) do not share any term, and 2) belong to the same session. The labeling process was accomplished by three judges that assigned one category among a) generalization, b) specialization and c) undefined. The result of this process was 421 query pairs labeled as either generalizations or specializations. Again, 1/3 of the labeled sample was used to evaluate the classifier’s performance.\nTo build the model from which the trees were induced, we calculated for each query pair the 27 lexical, temporal, and session related features proposed by Bonchi et al.with the exception of features [f21], [f22] y [f23], that in the original work were the cosine similarity, the Jaccard coefficient, and the overlapping of the stemmed query terms. We feel that applying stemming (Porter, 1980) ties the classifier to the language in which the queries were written, so we replaced that features in favor of the same measures applied to the Soundex codes of the terms, this way we obtain a set of similarity measures\nthat are independent of, at least, all the occidental languages.\n4.3 Hyponymy relation extraction\nThe last step in the process consists in, given an specialization (q,q′), identifying which terms from q and q′ act respectively as the hypernym and hyponym in the final relations. To do it, we first identify a set of candidate pairs and then we choose the most relevant candidate provided that its relevance is above a certain threshold. Relevance in this case is defined as a weight that is proportional to the probability of finding the same candidate in other specialization patterns across the log.\n( ) ( )\n( ) ( )\nIn the equation, P (t, t′) is the subset of the specialization patterns in which the term t appears in the more general query, and t′ in the more specific one; G(t′) is the subset of specialization patterns in which t′ appears only in the more general query; and finally S(t) is the subset in which t appears only in the more specific query. As a consequence, on the one side, a candidate relation appearing as it is in more patterns, will see its weight increased in a quadratic factor. On the other side, a candidate relation whose parts appear on the remaining set of specialization patterns, playing the opposite role (the hyponym in the more general query, and the hypernym in the more specific) will see its weight reduced by a linear factor.\nDepending on the kind of specialization pattern used as source, a concrete behavior is defined to select the candidate relations and, depending on the values of W assigned to it, to decide the instance that will be part of the taxonomy.\n4.3.1 Harnessing specializations with reformulation\nThis is the easiest case. A specialization with reformulation happens when in the pair (q,q′), some of the terms of qare replaced by others in q′(e.g. naked celebrities and naked angelina jolie). In this case, the intersection between both queries is removed, and the remaining terms in qand q′are taken as hyponym and hypernym of the candidate relation. Applying this to the previous example will give the relation celebrities ← angelina jolie as a result. It is easy to figure out that not all of this kind of specializations make up such a clear relation. (e.g. president bush, president of the united states). This is the reason why the candidate must have a positive weight (W > 0) in order to consider it as an actual hyponymy relation.\n4.3.2 Harnessing trivial and disjoint specializations\nA trivial specialization on a pair of queries (q,q′) is that in which q′only adds terms to q, or in other words, in which q is a substring of q′ (e.g. luxury cars, american luxury cars). We talk about disjoint specialization when no terms are shared between the two queries, but according to temporal, lexical and session related clues, a classifier determines that q′ has a narrower meaning than that of q. (e.g. marvel superheroes, wolverine). The candidate selection heuristic is the same in both cases:\n1. We compute the term n-gram vector for both queries. Following the luxury cars example, the\ngram vectors are g=[luxury, cars, luxury cars]; and g′=[american, luxury, cars, american luxury, luxury cars, american luxury cars].\n2. The set of candidates is built by combining each of the n-grams of g, with every n-gram of g′,\nprovided that the n-gram coming from g is the hypernym (t), the one coming from g′ is the hyponym (t′), and t′ is not a substring of t. (Table 1).\n3. For each candidate (t, t′) we compute W.\n4. Finally, we choose the candidate with the highest positive weight, but we do not choose any –and\nso, the extraction is considered barren–, if for every candidate W < 0.\n5 Research design\n5.1 Datasets used\nAs described by (Silvestri, 2010):\n“[. . . ] one of the main challenges in doing research with query logs is that query logs, themselves, are very difficult to obtain.”\nFor this research we have used two of the latest, publicly available, logs: AOL 2006 (Pass et al., 2006) and MSN 2006 (Zhang and Moffat, 2006).\nOn the one side, AOL 2006 contains more than 30 million records from about 650,000 users sampled from March to May 2006. Each record in the log contains 1) a user identifier, 2) the query string submitted by the user, 3) the timestamp of the submission; and if the user clicked on any result, then the record also includes 4) the position of the result clicked, and 5) the hostname portion of the visited URL.\nOn the other side, the MSN 2006 log was released as part of the “Microsoft Live Labs: Accelerating Search in Academic Research”1 incentive in 2006. This dataset contains about 15 million queries submitted by users from the United States during May 2006, as recorded by the MSN search engine. For each query, in addition to the same information provided by the AOL query log, this dataset also contains the number of search results that satisfied the query. One major difference between both logs is that, while AOL contains immutable user identifiers for every record originated by a certain user, the MSN log is anonymized in a way that user identifiers change each 30 minutes, preserving the users’ privacy.\nAs it was previously explained, we rely on the number of results to check query subsumption and identify specialization with reformulation patterns. This data can appear in every query log (as it appears in MSN), but has been omitted from the AOL one. To recreate this information, we have resubmitted each query to the Yahoo! BOSS2 API. And to reduce impedances between the results gathered from Yahoo!, and those present in the MSN log, we did the same operation with the latter.\nAt the end of this preprocessing activity, our dataset was comprised by near 45 million records containing 1) a user identifier, 2) the query string, 3) the submission timestamp, 4) the number of results of each query, and 5) click-through information.\n5.2 Method implementation\nThe experiments were applied by following a pipeline architecture in which the initial dataset is transformed into new data structures (figure 3). Some of the activities in the workflow are conceived to increase the method performance in two ways: 1) increasing effectiveness by reducing informational noise in the log; and 2) improving efficiency by generating new ad-hoc data structures that favor information extraction in the specialization detection, and in the relation extraction activities.\n5.2.1 Noise filtering\nBesides navigational queries, there exist others that – because of their nature – are not valid to mine semantic relations from them. For instance, the most frequent query in the AOL log is the “ ” query, which is believed to be the result of a masking strategy by the search engine (Brenes and Gayo-Avello, 2009). In the MSN log, those queries belonging to the longest hundred sessions are recorded in an average of less than three seconds each, and there is no click information at all associated to them. This suggests that the queries were sent by a software agent through the search API (Zhang and Moffat, 2006).\nTo reduce the amount of useless information in the log, we have defined the following criteria to\nconsider a query as spam:\n• The number of characters for all the terms in the query is lower than 3.\n• The number of characters in any term in the query is greater than 25.\n• The number of terms in the query is greater than 5.\n• The average time between query submissions from the same user is lower than 7 seconds.\n1http://research.microsoft.com/ur/us/fundingopps/RFPs/Search 2006 RFP.aspx 2http://developer.yahoo.com/search/boss\nA user who sends a query that matches at least one of the points above is considered a spammer, and the whole session where the query appears is invalidated. The product of this activity is in turn a set of sessions that are free of spam.\n5.2.2 Additional data structures\nApplying some of the algorithms depicted in the method require additional data structures, like indices and relations. For instance, for the supervised detection of specialization patterns, there is the need to compute session related features such as The average number of clicks in search results since session begin, among all sessions containing (q,q′). For this sake, from the sessionized logs we create the following indexes:\n• q → [r, {s, [{i, t}]}] given a query q, its number of results, the list of sessions in which qappears, and\ninside the sessions, the position in which q appears and the time at which is has been submitted.\n• s → [{q, {p, url}}] given a session, the list of queries that make it up, and the information associ-\nated to the visited results (position and target url).\nIn addition, during the relation extraction activity, we need to calculate the weight assigned to each candidate. To do so, we need to know in which queries a given n-gram appears, and its length and offset inside a query (t → [q, p, o]). With this information we can reenter the query index and reveal which role it plays inside a certain n-gram.\n5.3 Proposed evaluation method\nIn order to measure the performance of the method developed, we must determine whether an extracted relation t ← t′ has hyponymy semantics or, in other words, if any of the following statements are true: “t’ is a t ”, or “t’ is an instance of t ”.\nIn absence of another criterion of reference, we proposed a combined technique in which first, we test if a certain relation appears as an hyponymy relation in Wordnet (and therefore it is a valid one) or, if not, we delegate to a human judgement. To support this, we rely on the following assertions:\n• Most of AOL and MSN queries are written in English, the same language that Wordnet is built\nin.\n• Based on the previous one, we can apply stemming to cushion lexical deviations of the same term.\n• Some of the relations extracted capture common vocabulary of search engine users, in which is not\nunfrequent the use of jargon, slang, trade marks, and even typos. Although this kind of vocabulary does not appear in Wordnet, relations containing it can be identified by the judges.\nOnce taken all the above into account, the following evaluation process is implemented:\n1. We take a sample composed by an equal number of relations extracted (E), and discarded (D)\nin the relation extraction activity. As the extraction algorithm varies, and we want segmented performance numbers, the sample will also have an equal number of relations extracted from each kind of specialization pattern.\n2. We create a directed graph with the hyponymy relations present in Wordnet. In that graph, the\nvertices are the result of applying stemming over the terms that appear in the hyponymy relations, and there is an arc that goes from an hyponym to its direct hypernym.\n3. Given a candidate pair, we apply stemming to each side of the relation. Then we look whether\nthere exists a path in that graph, going from the specific side of the relation to the general side. If it does exist, then we will have finished, and the relation is correct.\n4. If the candidate pair is not present in Wordnet, two judges will evaluate if it is an actual hyponymy\nrelation, and it it is not, they will classify the candidate as either a) synonyms (co-hyponyms), b) terms related by another kind of relationship, or c) completely unrelated terms. In case that the judges don not agree on the verdict, the following rules apply:\n• If both judges determine that the relation is not an hyponymy, but differ on the category\nassigned, the result will be c) – both terms are unrelated.\n• If only one of the judges determines that the relation is a proper hyponymy, the result will\nbe the error category assigned by the other judge.\n5. The last step is to calculate precision (P) and recall (R) measures in the context of classification.\nP is the portion of relations properly classified by the extraction algorithm (true positives) from the whole set of relations extracted (true positives + false positives). Recall on the other side, is the portion of relations properly classified (true positives) from the whole set of existing relations (true positives + false negatives).\n6 Results\nFor each specialization type from which the relations were extracted (trivial specialization, specialization with reformulation, and disjoint specialization), we have taken 500 instances identified as valid hyponimy relations (E), and 500 instances discarded by the extraction heuristic, counting for a total of 3000 instances. Then we have applied the evaluation method described above, and obtained the following results.\n7 Implications and future directions\n7.1 Discussion\nFrom the initial dataset comprised of about 45 million queries, 367,495 were identified as specialization patterns. From these patterns, 51,300 hyponymy relations were extracted, from which only 7,714 were not repeated. This results show that only one out of 3,000 query pairs generate a valid hyponymy relation, and as a consequence a much bigger source of information is needed to effectively use this method in a large-scale search system. Despite all, a commercial search engine can serve up to 4,700 million queries a day (Comscore, 2012), and this is more than 100x the data we had for this study. We believe this vast amount of information is more than the necessary to build a useful taxonomy.\nRegarding performance, it can be seen that the method behaves better when dealing with disjoint specializations (P = 0.904). At the same time this is the less frequent kind of specializations, with less than 1% of contribution to the whole method’s performance (P = 0.731). This happens due to the fact that disjoint specializations do not share any term between the two queries, and as the sessionization algorithm takes into account query similarity, it is very uncommon that two dissimilar queries appear on the same topical-session, reducing the number of instances of this kind.\nAnother interesting aspect is that the number of false positives derived from a wrong interpretation of co-hyponymy relations (e.g. bill clinton ← monica lewinsky) is consistently higher in specializations with reformulation. This happens because the number of results for one of queries in the pattern is much higher than the other (i.e. the subsumption algorithm will determine an specialization) (figure 4), and the terms also appear in a wide variety of other patterns playing the same role (e.g. clinton scandal, lewinsky scandal) increasing the weight W of the candidate.\nThose relations whose terms appear as other kind of relations different from the hyponymy and co-hyponymy can be considered marginal (less than 0.5%). Other relations whose terms are determined as unrelated (91.29%) appear in a proportional way for each specialization type, and are caused by the\nexistence of stop words such as from, to or is, and also by others with an extremely high frequency (e.g. britney spears), which appear in different contexts increasing candidates’ weight. In order to cushion the impact of such terms, we could integratetf x idf measures into the weight calculus, thus reducing the overall score of a relation containing less relevant terms. This is left for future work.\nFinally, we can highlight the fact that a consistent ratio of the relations (29.6%) does not exist in Wordnet. Wordnet contains 155.287 words arranged in 117.000 sunsets Miller et al. (1990), covering almost an 85% of the lexicon in the Oxford Dictionary3. This ratio means that nearly 3 out of 10 relations contain terms that are common in users’ vocabulary, but do not appear in formal lexicons, such as brand names (e.g. briefs ← speedo); people names (celebrities ← angelina jolie), or even typos (britney spears ← brittney spears). This semantic information clearly serves to the purpose of increasing query suggestion and expansion effectiveness.\n7.2 Conclusions\nGiven the results shown and discussed above, we are now able to answer the research questions that motivated this study:\n(q1) Is it possible to automatically generate term taxonomies containing the common vocabulary em-\nployed by web users?\nYes, it is. And, in addition, our method has an acceptable performance (F0,5 = 0.754), and can be\nimplemented for large scale deployments with minimum effort.\n(q2) If it is so, can it be done by using just the information contained in the query log, with no\nadditional sources of information?\nYes, it can. The only information that is not contained in the query log is the training set used in the supervised identification activity. This information, like the rest of the code for the algorithms used, is only required at design time and does not need any additional maintenance. Other sources of information like dictionaries, search result snippets, text corpora, or any kind of semantic repositories\n3http://www.oup.com/online/oed/\nare not used at all, so we can conclude that our method only operates on information contained in the query log.\n(q3) Is it possible to do all the above in a language-independent way?\nMost of the queries in our dataset are written in English. Due to this, we could not measure the performance of our method in other languages. However, we have no evidence that it cannot work for other languages in which queries can be split into terms and these, in turn, into character n-grams as this is the only lexical information actually used. There is no evidence either, that our method could not be applied to Chinese and other languages in which text is not segmented, as other scholars (Yang et al., 2000; Gao et al., 2005) have developed methods to deal with text segmentation in these languages.\nTo sum up, we have developed a method for the automatic extraction of hyponoymy relations using query logs as the only source of information with independence of the language in which the queries are\nwritten –at least for the occidental ones– , and capturing the common vocabulary of web search users.\nThis will allow:\n• To increase web search engines effectiveness and efficiency by improving query suggestion and\nexpansion methods.\n• To organize the parole of web search users in a reasonable and fast way, reflecting everyday aspects\nof the language that are not covered by formal classification systems, such as linguistic dictionaries and wordnets.\n• To obtain term taxonomies for languages in which wordnets are scarce or do not even exist.\n• To serve as inspiration for future and ongoing works on semantic information extraction, and from\nother limited sources of information such as folksonomies or micro-posts.\n7.3 Future work\nIn the same research line, it would be interesting to:\n• Try to increase the ratio of disjoint specializations by applying supervised pattern identification\nalso for revealing topical sessions.\n• Identify other kind of relationships different from hyponymy.\n• Study the literature on text segmentation and entity recognition and try to apply them both to\nextract taxonomies in other languages, and to increase the accuracy of our method.\n• Develop a model for related query recommendation.\nOther related lines of research, would be:\n• Enriching other existing datasources with semantic information (e.g Wikipedia, DBPedia, Free-\nbase, etc.)\n• Filtering and curating taxonomies with the semantic information contained in the previous repos-\nitories.\n• Using taxonomies for semantic query tagging (i.e. determine which terms in the query describe a\nperson, a place, or a product, among others).\n• Given proper tagged queries, determine the user’s intention behind each one.\n• The application of the points above (semantic query tagging and identification of user intention)\nto filter search results, and to provide new ways of arranging them, further than document lists.\n• Apply what we have learnt to new user generated data sources, such as Twitter.\nReferences\nRicardo Baeza-Yates and Alessandro Tiberi. Extracting semantic relations from query logs. In Pro-\nceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,\nKDD ’07, page 7685, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-609-7. doi:\n10.1145/1281192.1281204. URL http://doi.acm.org/10.1145/1281192.1281204.\nMatthew Berland and Eugene Charniak. Finding parts in very large corpora. In Proceedings of the 37th\nannual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 57–64, Stroudsburg, PA, USA, 1999. Association for Computational Linguistics. ISBN 1- 55860- 609-3. doi: 10.3115/1034678.1034697. URL http://dx.doi.org/10.3115/1034678.1034697.\nP. Boldi, F. Bonchi, C. Castillo, and S. Vigna. From dango to japanese cakes: Query reformulation\nmodels and patterns. In Web Intelligence and Intelligent Agent Technologies, 2009. WI-IAT’09. IEEE/WIC/ACM International Joint Conferences on, volume 1, pages 183–190. IEEE, 2009.\nDavid J. Brenes and Daniel Gayo-Avello. Stratified analysis of AOL query log. Infor-\nmation Sciences, 179(12):1844–1858, May 2009. doi: 10.1016/j.ins.2009.01.027. URL http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V0C-4VJBTTF-1&_user= 10&_rdoc=1&_fmt=&_orig=search&_sort=d&_docanchor=&view=c&_acct=C000050221&_version= 1&_urlVersion=0&_userid=10&md5=7c7fa5cd6b6e1017500b4e7607393871.\nAndrei Broder. A taxonomy of web search. SIGIR Forum, 36(2):3–10, 2002. doi: 10.1145/792550.792552.\nURL http://portal.acm.org/citation.cfm?id=792552.\nS.A. Caraballo. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proceed- ings\nof the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages\n120–126. Association for Computational Linguistics, 1999.\nShui-Lung Chuang and Lee-Feng Chien. Enriching web taxonomies through subject categorization of\nquery terms from search engine logs. Decision Support Systems, 35(1):113–127, April 2003. ISSN 0167-9236. doi: 10.1016/S0167-9236(02)00099-4. URL http://www.sciencedirect.com/science/ article/pii/S0167923602000994.\nShui-Lung Chuang and Lee-Feng Chien. A practical web-based approach to generating topic hierarchy\nfor text segments. In Proceedings of the thirteenth ACM international conference on Information and knowledge management, CIKM ’04, page 127136, New York, NY, USA, 2004. ACM. ISBN 1-58113- 874-1. doi: 10.1145/1031171.1031193. URL http://doi.acm.org/10.1145/1031171.1031193.\nShui-Lung Chuang and Lee-Feng Chien. Taxonomy generation for text segments: A practical web-based\napproach. ACM Trans. Inf. Syst., 23(4):363396, October 2005. ISSN 1046-8188. doi: 10.1145/1095872. 1095873. URL http://doi.acm.org/10.1145/1095872.1095873.\nP. Clough, H. Joho, and M. Sanderson. Automatically organising images using concept hierarchies. In\nproceedings of the Multimedia Workshop running at ACM SIGIR conference, 2005.\nComscore. comScore Releases December 2011 U.S. Search Engine Rankings, 2012. URL\nhttp://www.comscore.com/Press_Events/Press_Releases/2012/1/comScore_Releases_ December_2011_U.S._Search_Engine_Rankings/.\nJ. Ellman. Eurowordnet: A multilingual database with lexical semantic networks: Edited by piek vossen.\nkluwer academic publishers. 1998. isbn 0792352955, 179 pages. Natural Language Engineering, 9(04): 427–430, 2003.\nD. Fallows. Almost half of all internet users now use search engines on a typical day. Pew Internet and\nAmerican Life Project Memo, 2008.\nMiguel Fernandez-Fernandez and Daniel Gayo-Avello. Hierarchical taxonomy extraction by mining\ntopical query sessions. In In proceedings of the International Conference of Knowledge Discovery and Information Retrieval 2009, pages 229–235, Funchal, Madeira (Portugal), 2009.\nE. Gabrilovich and S. Markovitch. Harnessing the expertise of 70,000 human editors: Knowledge-based\nfeature generation for text categorization. Journal of Machine Learning Research, 8:2297–2345, 2007.\nJianfeng Gao, Andi Wu, Mu Li, and Chang-ning Huang. Chinese word segmentation and named entity\nrecognition: a pragmatic approach. COMPUTATIONAL LINGUISTICS, 31:574, 2005. URL http: //citeseer.uark.edu:8080/citeseerx/viewdoc/summary?doi=10.1.1.69.7099.\nDaniel Gayo-Avello. A survey on session detection methods in query logs and a proposal for future\nevaluation. Inf. Sci., 179(12):1822–1843, 2009. URL http://portal.acm.org/citation.cfm?id= 1523556.\nR. Girju, A. Badulescu, and D. Moldovan. Learning semantic constraints for the automatic discovery\nof part-whole relations. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 1–8. Association for Computational Linguistics, 2003.\nDaqing He, Ayse Goker, and David Harper. Combining evidence for automatic web session identification.\nInf. Process. Manage., 38(5):727–742, 2002. ISSN 0306-4573. URL http://dx.doi.org/10.1016/ S0306-4573(01)00060-7.\nMarti A. Hearst. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th\nconference on Computational linguistics - Volume 2, COLING ’92, page 539545, Stroudsburg, PA, USA, 1992. Association for Computational Linguistics. doi: 10.3115/992133.992154. URL http:\n//dx.doi.org/10.3115/992133.992154.\nPaul Heymann and Hector Garcia-Molina. Collaborative creation of communal hierarchical taxonomies\nin social tagging systems. http://ilpubs.stanford.edu:8090/775/?auth=basic, April 2006. URL http:\n//ilpubs.stanford.edu:8090/775/?auth=basic.\nBernard J. Jansen, Danielle L. Booth, Am, and a Spink. Determining the informational, navigational,\nand transactional intent of web queries. Information Processing & Management, 44(3):1251–1266, May 2008. doi: 10.1016/j.ipm.2007.07.015. URL http://www.sciencedirect.com/science?\n_ob=ArticleURL&_udi=B6VC8-4PMT5X4-2&_user=10&_rdoc=1&_fmt=&_orig=search&_sort= d&_docanchor=&view=c&_searchStrId=1152600747&_rerunOrigin=scholar.google&_acct= C000050221&_version=1&_urlVersion=0&_userid=10&md5=07b115aab3727895f2274076aecb29ca.\nM. Komachi and H. Suzuki. Minimally supervised learning of semantic knowledge from query logs. In\nProceedings of the 3rd International Joint Conference on Natural Language Processing, pages 358–365, 2008.\nR. Mandala, T. Tokunaga, and H. Tanaka. Complementing wordnet with roget’s and corpus-based\nthesauri for information retrieval. In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, pages 94–101. Association for Computational Linguistics, 1999.\nR. Mihalcea. Turning wordnet into an information retrieval resource: Systematic polysemy and conver-\nsion to hierarchical codes. International journal of pattern recognition and artificial intelligence, 17 (5):689–704, 2003.\nPeter Mika. Ontologies are us: A unified model of social networks and semantics. In Yolanda Gil,\nEnrico Motta, V. Benjamins, and Mark Musen, editors, The Semantic Web ISWC 2005, volume 3729 of Lecture Notes in Computer Science, pages 522–536. Springer Berlin / Heidelberg, 2005. ISBN 978-3-\n540-29754-3. URL http://www.springerlink.com/content/f68p442351736187/abstract/.\nGeorge A Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. WordNet:\nan on-line lexical database. International Journal of Lexicography, 3:235–244, 1990. doi: 10.1.1.88. 6804. URL http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.88.6804.\nEmmanuel Morin and Christian Jacquemin. Automatic acquisition and expansion of hypernym\nlinks. Computers and the Humanities, 38(4):363–396, 2004. ISSN 0010-4817. doi: 10.1007/ s10579-004-1926-2. URL http://www.springerlink.com/content/tn7h6gg278x05431/abstract/.\nM. Pasca. Weakly-supervised discovery of named entities using web search queries. In Proceedings of the\nsixteenth ACM conference on Conference on information and knowledge management, pages 683–690. ACM, 2007.\nM. Pasca and B. Van Durme. What you seek is what you get: Extraction of class attributes from query\nlogs. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2832–2837, 2007.\nM. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. Organizing and searching the world wide web of\nfacts-step one: the one-million fact extraction challenge. In Proceedings of the National Conference on Artificial Intelligence, volume 21, page 1400. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.\nGreg Pass, Abdur Chowdhury, and Cayley Torgeson. A picture of search. In Proceedings of the 1st international\nconference on Scalable information systems, InfoScale ’06, New York, NY, USA, 2006. ACM. ISBN 1-59593- 428-6. doi: 10.1145/1146847.1146848. URL http://doi.acm.org/10.1145/ 1146847.1146848.\nM. F. Porter. An algorithm for suffix stripping. Program: electronic library and information sys- tems,\n14(3):130–137, December 1980. ISSN 0033-0337. doi: 10.1108/eb046814. URL http: //www.emeraldinsight.com/journals.htm?articleid=1670983&show=abstract.\nJohn Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993. ISBN 9781558602380.\nP. Schmitz. Inducing ontology from flickr tags. In Collaborative Web Tagging Workshop at WWW2006,\nEdinburgh, Scotland, pages 210–214, 2006.\nE. Schwarzkopf, D. Heckmann, D. Dengler, and A. Kröner. Mining the structure of tag spaces for user\nmodeling. In Complete On-Line Proceedings of the Workshop on Data Mining for User Modeling at the 11th International Conference on User Modeling. Corfu, Griechenland, pages 63–75, 2007.\nSatoshi Sekine and Hisami Suzuki. Acquiring ontological knowledge from query logs. In Proceedings of\nthe 16th international conference on World Wide Web, pages 1223–1224, Banff, Alberta, Canada, 2007. ACM. ISBN 978-1-59593-654-7. doi: 10.1145/1242572.1242777. URL http://portal.acm.\norg/citation.cfm?id=1242777.\nDou Shen, Min Qin, Weizhu Chen, Qiang Yang, and Zheng Chen. Mining web query hierarchies from\nclickthrough data. Artificial Intelligence, pages 341–346, 2007.\nFabrizio Silvestri. Mining query logs: Turning search usage data into knowledge. Foundations and Trends in\nInformation Retrieval, 4(1-2):1–174, 2010. ISSN 1554-0669, 1554-0677. doi: 10.1561/1500000013. URL http://www.nowpublishers.com/product.aspx?product=INR&doi=1500000013.\nAmanda Spink. Modeling users” successive searches in digital Environments:A national science Foun-\ndation/British library funded study. Technical report, Corporation for National Research Initiatives, 1998. URL http://portal.acm.org/citation.cfm?id=865316.\nP. Vossen and C. Fellbaum. Wordnets in the world. Technical report, 2004.\nL. Xiong and E. Agichtein. Towards privacy-preserving query log publishing. In Query Log Analysis: So- cial\nAnd Technological Challenges. A workshop at the 16th International World Wide Web Conference (WWW2007).,\n2007.\nChristopher C. Yang, Johnny W. K. Luk, Stanley K. Yung, and Jerome Yen. Combination and\nboundary detection approaches on chinese indexing. JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE, 51:340—351, 2000. URL http://citeseer.uark.edu: 8080/citeseerx/viewdoc/summary?doi=10.1.1.108.8593.\nY. Zhang and A. Moffat. Some observations on user search behavior. In Proceedings of the 11th Aus-\ntralasian Document Computing Symposium, volume 11, pages 1–8, 2006."
    } ],
    "references" : [ {
      "title" : "Stratified analysis of AOL query log",
      "author" : [ "David J. Brenes", "Daniel Gayo-Avello" ],
      "venue" : "IEEE/WIC/ACM International Joint Conferences on,",
      "citeRegEx" : "Brenes and Gayo.Avello.,? \\Q2009\\E",
      "shortCiteRegEx" : "Brenes and Gayo.Avello.",
      "year" : 2009
    }, {
      "title" : "A taxonomy of web search",
      "author" : [ "_urlVersion", "_userid" ],
      "venue" : "SIGIR Forum,",
      "citeRegEx" : "1 et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "1 et al\\.",
      "year" : 2002
    }, {
      "title" : "Eurowordnet: A multilingual database with lexical semantic networks: Edited by piek vossen",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Ellman.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ellman.",
      "year" : 2011
    }, {
      "title" : "Harnessing the expertise of 70,000 human editors: Knowledge-based feature generation for text categorization",
      "author" : [ "Funchal", "Madeira (Portugal", "2009. E. Gabrilovich", "S. Markovitch" ],
      "venue" : "Information Retrieval",
      "citeRegEx" : "Funchal et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Funchal et al\\.",
      "year" : 2009
    }, {
      "title" : "Automatic acquisition of hyponyms from large text corpora",
      "author" : [ "Marti A. Hearst" ],
      "venue" : "In Proceedings of the 14th conference on Computational linguistics - Volume 2,",
      "citeRegEx" : "Hearst.,? \\Q1992\\E",
      "shortCiteRegEx" : "Hearst.",
      "year" : 1992
    }, {
      "title" : "Collaborative creation of communal hierarchical taxonomies in social tagging systems",
      "author" : [ "Paul Heymann", "Hector Garcia-Molina" ],
      "venue" : "URL http: //ilpubs.stanford.edu:8090/775/?auth=basic",
      "citeRegEx" : "Heymann and Garcia.Molina.,? \\Q2006\\E",
      "shortCiteRegEx" : "Heymann and Garcia.Molina.",
      "year" : 2006
    }, {
      "title" : "Determining the informational, navigational, and transactional intent of web queries",
      "author" : [ "Bernard J. Jansen", "Danielle L. Booth", "Am", "a Spink" ],
      "venue" : "Information Processing & Management,",
      "citeRegEx" : "Jansen et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Jansen et al\\.",
      "year" : 2008
    }, {
      "title" : "Minimally supervised learning of semantic knowledge from query logs",
      "author" : [ "M. Komachi", "H. Suzuki" ],
      "venue" : "In Proceedings of the 3rd International Joint Conference on Natural Language Processing,",
      "citeRegEx" : "Komachi and Suzuki.,? \\Q2008\\E",
      "shortCiteRegEx" : "Komachi and Suzuki.",
      "year" : 2008
    }, {
      "title" : "Complementing wordnet with roget’s and corpus-based thesauri for information retrieval",
      "author" : [ "R. Mandala", "T. Tokunaga", "H. Tanaka" ],
      "venue" : "In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Mandala et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Mandala et al\\.",
      "year" : 1999
    }, {
      "title" : "Turning wordnet into an information retrieval resource: Systematic polysemy and conversion to hierarchical codes",
      "author" : [ "R. Mihalcea" ],
      "venue" : "International journal of pattern recognition and artificial intelligence,",
      "citeRegEx" : "Mihalcea.,? \\Q2003\\E",
      "shortCiteRegEx" : "Mihalcea.",
      "year" : 2003
    }, {
      "title" : "WordNet: an on-line lexical database",
      "author" : [ "George A Miller", "Richard Beckwith", "Christiane Fellbaum", "Derek Gross", "Katherine Miller" ],
      "venue" : "International Journal of Lexicography,",
      "citeRegEx" : "Miller et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 1990
    }, {
      "title" : "Automatic acquisition and expansion of hypernym links",
      "author" : [ "Emmanuel Morin", "Christian Jacquemin" ],
      "venue" : "Computers and the Humanities,",
      "citeRegEx" : "Morin and Jacquemin.,? \\Q2004\\E",
      "shortCiteRegEx" : "Morin and Jacquemin.",
      "year" : 2004
    }, {
      "title" : "Weakly-supervised discovery of named entities using web search queries",
      "author" : [ "M. Pasca" ],
      "venue" : "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,",
      "citeRegEx" : "Pasca.,? \\Q2007\\E",
      "shortCiteRegEx" : "Pasca.",
      "year" : 2007
    }, {
      "title" : "A picture of search",
      "author" : [ "Greg Pass", "Abdur Chowdhury", "Cayley Torgeson" ],
      "venue" : "In Proceedings of the 1st international conference on Scalable information systems, InfoScale ’06,",
      "citeRegEx" : "Pass et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Pass et al\\.",
      "year" : 2006
    }, {
      "title" : "Programs for Machine Learning",
      "author" : [ "John Ross Quinlan. C" ],
      "venue" : null,
      "citeRegEx" : "C4.5,? \\Q1993\\E",
      "shortCiteRegEx" : "C4.5",
      "year" : 1993
    }, {
      "title" : "Inducing ontology from flickr tags",
      "author" : [ "P. Schmitz" ],
      "venue" : "In Collaborative Web Tagging Workshop at WWW2006,",
      "citeRegEx" : "Schmitz.,? \\Q2006\\E",
      "shortCiteRegEx" : "Schmitz.",
      "year" : 2006
    }, {
      "title" : "Acquiring ontological knowledge from query logs",
      "author" : [ "Satoshi Sekine", "Hisami Suzuki" ],
      "venue" : "In Proceedings of the 16th international conference on World Wide Web,",
      "citeRegEx" : "Sekine and Suzuki.,? \\Q2007\\E",
      "shortCiteRegEx" : "Sekine and Suzuki.",
      "year" : 2007
    }, {
      "title" : "Mining query logs: Turning search usage data into knowledge",
      "author" : [ "Fabrizio Silvestri" ],
      "venue" : "Foundations and Trends in Information Retrieval,",
      "citeRegEx" : "Silvestri.,? \\Q2010\\E",
      "shortCiteRegEx" : "Silvestri.",
      "year" : 2010
    }, {
      "title" : "Wordnets in the world",
      "author" : [ "P. Vossen", "C. Fellbaum" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Vossen and Fellbaum.,? \\Q2004\\E",
      "shortCiteRegEx" : "Vossen and Fellbaum.",
      "year" : 2004
    }, {
      "title" : "Some observations on user search behavior",
      "author" : [ "Y. Zhang", "A. Moffat" ],
      "venue" : "In Proceedings of the 11th Australasian Document Computing Symposium,",
      "citeRegEx" : "Zhang and Moffat.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhang and Moffat.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "First, because WordNet is an English language project, parallel projects for other languages have been developed, such as EuroWordNet (Ellman, 2003), BalkaNet (Greek), Hebrew WordNet, Hindi WordNet and Japanese WordNet among others (Vossen and Fellbaum, 2004).",
      "startOffset" : 232,
      "endOffset" : 259
    }, {
      "referenceID" : 8,
      "context" : "For instance, (Mandala et al., 1999) and (Gabrilovich and Markovitch, 2007) pointed out that most domain-specific relationships between words cannot be found in WordNet, and some kind of words, such as proper names, jargon or slang are just not included.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "Besides, (Mihalcea, 2003) also explained that due to the fact that professional linguists recognize minimal differences in word senses, common words such as “make” have too many different senses to be useful for IR tasks.",
      "startOffset" : 9,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "Of course, these wordnets could be automatically enriched (Hearst, 1992) but such approach require a great effort (usually carried out by linguists) and, hence, wordnets remain as quite static data sources.",
      "startOffset" : 58,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "(Hearst, 1992), (Berland and Charniak, 1999), (Caraballo, 1999), (Girju et al.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : ", 2003), (Morin and Jacquemin, 2004).",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "Works such as (Hearst, 1992) , (Berland and Charniak, 1999), (Caraballo, 1999), (Girju et al.",
      "startOffset" : 14,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : ", 2003), (Morin and Jacquemin, 2004), among others, are extremely relevant but they cannot be straightforwardly applied to query logs, because most of these techniques require lexico-syntactic patterns and POS tagging which are hardly useful when applied to Web search queries.",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : ", 2005) and (Schmitz, 2006) applied subsumption to image tags in order to obtain tag hierarchies.",
      "startOffset" : 12,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "(Heymann and Garcia-Molina, 2006), (Mika, 2005), and (Schwarzkopf et al.",
      "startOffset" : 0,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : ", 2006) and (Pasca, 2007) describe a technique to obtain class attributes from query logs (e.",
      "startOffset" : 12,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "The same author also provides a method to find named-entities (Pasca and Van Durme, 2007) which is related to (Sekine and Suzuki, 2007) and (Komachi and Suzuki, 2008).",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "The same author also provides a method to find named-entities (Pasca and Van Durme, 2007) which is related to (Sekine and Suzuki, 2007) and (Komachi and Suzuki, 2008).",
      "startOffset" : 140,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "(Hearst, 1992), (Berland and Charniak, 1999), (Caraballo, 1999), (Girju et al.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : ", 2003) and (Morin and Jacquemin, 2004) in that it does not rely on full text documents but on query logs.",
      "startOffset" : 12,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : ", 2005), (Heymann and Garcia-Molina, 2006), (Schmitz, 2006), (Mika, 2005), (Baeza-Yates and Tiberi, 2007) and (Schwarzkopf et al.",
      "startOffset" : 9,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : ", 2005), (Heymann and Garcia-Molina, 2006), (Schmitz, 2006), (Mika, 2005), (Baeza-Yates and Tiberi, 2007) and (Schwarzkopf et al.",
      "startOffset" : 44,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : ", 2006), (Pasca and Van Durme, 2007), (Pasca, 2007), (Sekine and Suzuki, 2007) and (Komachi and Suzuki, 2008) are in fact dealing with problems which are totally unrelated to taxonomy construction.",
      "startOffset" : 38,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : ", 2006), (Pasca and Van Durme, 2007), (Pasca, 2007), (Sekine and Suzuki, 2007) and (Komachi and Suzuki, 2008) are in fact dealing with problems which are totally unrelated to taxonomy construction.",
      "startOffset" : 53,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : ", 2006), (Pasca and Van Durme, 2007), (Pasca, 2007), (Sekine and Suzuki, 2007) and (Komachi and Suzuki, 2008) are in fact dealing with problems which are totally unrelated to taxonomy construction.",
      "startOffset" : 83,
      "endOffset" : 109
    }, {
      "referenceID" : 6,
      "context" : "them (Jansen et al., 2008) consists in marking a query as navigational if it matches at least one of the following criteria:",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "As described by (Silvestri, 2010):",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "For this research we have used two of the latest, publicly available, logs: AOL 2006 (Pass et al., 2006) and MSN 2006 (Zhang and Moffat, 2006).",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : ", 2006) and MSN 2006 (Zhang and Moffat, 2006).",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "For instance, the most frequent query in the AOL log is the “ ” query, which is believed to be the result of a masking strategy by the search engine (Brenes and Gayo-Avello, 2009).",
      "startOffset" : 149,
      "endOffset" : 179
    }, {
      "referenceID" : 19,
      "context" : "This suggests that the queries were sent by a software agent through the search API (Zhang and Moffat, 2006).",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "000 sunsets Miller et al. (1990), covering almost an 85% of the lexicon in the Oxford Dictionary.",
      "startOffset" : 12,
      "endOffset" : 33
    } ],
    "year" : 2015,
    "abstractText" : "Search engine logs store detailed information on Web users interactions. Thus, as more and more people use search engines on a daily basis, important trails of users common knowledge are being recorded in those files. Previous research has shown that it is possible to extract concept taxonomies from full text documents, while other scholars have proposed methods to obtain similar queries from query logs. We propose a mixture of both lines of research, that is, mining query logs not to find related queries nor query hierarchies, but actual term taxonomies that could be used to improve search engine effectiveness and efficiency. As a result, in this study we have developed a method that combines lexical heuristics with a supervised classification model to successfully extract hyponymy relations from specialization search patterns revealed from log missions, with no additional sources of information, and in a language independent way.",
    "creator" : "Microsoft® Word 2010"
  }
}