{
  "name" : "1409.5209.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Pedestrian Detection with Spatially Pooled Features and Structured Ensemble Learning",
    "authors" : [ "Sakrapee Paisitkriangkrai", "Chunhua Shen", "Anton van den Hengel" ],
    "emails" : [ "chunhua.shen@adelaie.edu.au)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Pedestrian detection, boosting, ensemble learning, spatial pooling, structured learning.\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Pedestrian detection has gained a great deal of attention in the research community over the past decade. It is one of several fundamental topics in computer vision. The task of pedestrian detection is to identify visible pedestrians in a given image using knowledge gained through analysis of a set of labelled pedestrian and non-pedestrian exemplars. Significant progress has been made in the last decade in this area due to its practical use in many computer vision applications such as video surveillance, robotics and human computer interaction. The problem is made difficult by the inevitable variation in target appearance, lighting and pose, and by occlusion. In a recent literature survey on pedestrian detection, the authors evaluated several pedestrian detectors and concluded that combining multiple features can significantly boost the performance of pedestrian detection [1]. Hand-crafted low-level visual features have been applied to several computer vision applications and shown promising results [2], [3], [4], [5]. Inspired by the recent success of spatial pooling on object recognition and pedestrian detection problems [6], [7], [8], [9], we propose to perform the spatial pooling operation to create the new feature type for the task of pedestrian detection.\nOnce the detector has been trained, the most commonly adopted evaluation method by which to compare the detection performance of different algorithms is the Receiver Operating Characteristic (ROC) curve. The curve illustrates the varying performance of a binary classifier system as its discrimination threshold\nThe authors are with School of Computer Science, The University of Adelaide, SA 5005, Australia. C. Shen and A. van den Hengel are also with Australian Research Council Centre of Excellence for Robotic Vision.\nCorresponding author: C. Shen (e-mail: chunhua.shen@adelaie.edu.au).\nis altered. In the face and human detection literature, researchers are often interested in the low false positive area of the ROC curve since this region characterizes the performance needed for most real-world vision applications (see Fig. 1). An algorithm that achieves a high detection rate with many false positives would be less preferable than the algorithm that achieves a moderate detection rate with very few false positives. For human detection, researchers often report the partial area under the ROC curve (pAUC), typically over the range 0.01 and 1.0 false positives per image [1]. As the name implies, pAUC is calculated as the area under the ROC curve between two specified false positive rates (FPRs). It summarizes the practical performance of a detector and often is the primary performance measure of interest.\nAlthough pAUC is the metric of interest that has been adopted to evaluate detection performance, many classifiers do not directly optimize this evaluation criterion, and as a result, often under-perform. In this paper, we present a principled approach for learning an ensemble classifier which directly optimizes the partial area under the ROC curve, where the range over which the area is calculated may be selected according to the desired application. Built upon the structured learning framework, we thus propose here a novel form of ensemble classifier which directly optimizes the partial AUC score, which we call pAUCEnsT. As with all other boosting algorithms, our approach learns a predictor by building an ensemble of weak classification rules. It also relies on a sample re-weighting mechanism to pass the information between each iteration. However, unlike traditional boosting, at each iteration, the proposed approach places a greater emphasis on samples which have the incorrect\nar X\niv :1\n40 9.\n52 09\nv3 [\ncs .C\nV ]\n2 8\nJu n\n20 15\n2\nordering1 to achieve the optimal partial AUC score. The result is the ensemble learning method which yields the scoring function consistent with the correct relative ordering of positive and negative samples and optimizes the partial AUC score in a false positive rate range [α, β] where 0 ≤ α < β ≤ 1."
    }, {
      "heading" : "1.1 Main contributions",
      "text" : "The main contributions of our work can be summarized as follows. • We propose a novel approach to extract low-level\nvisual features based on spatial pooling for the problem of pedestrian detection. Spatial pooling has been successfully applied in sparse coding for generic image classification problems. We show that spatial pooling applied to commonly-used features such as covariance features [3] and LBP descriptors [4] improves accuracy of pedestrian detection. • We propose a new structured ensemble learning approach which explicitly optimizes the partial area under the ROC curve (pAUC) between any two given false positive rates. The method is of particular interest in the wide variety of applications where performance is most important over a particular range within the ROC curve. The proposed ensemble learning is termed pAUCEnsT (pAUC ENSemble learning with a Tight bound). The approach\n1. The positive sample has an incorrect ordering if it is ranked below the negative sample. In other words, we want all positive samples to be ranked above all negative samples.\nshares similarities with conventional boosting methods, but differs significantly in that the proposed method optimizes a multivariate performance measure using structured learning. Our design is simple and a conventional boosting-based visual detector can be transformed into a pAUCEnsT-based visual detector with few modifications to the existing code. Our approach is efficient since it exploits both the efficient weak classifier training and the efficient cutting plane solver for optimizing the partial AUC score in the structured SVM setting. To our knowledge, our approach is the first principled ensemble method that directly optimizes the partial AUC in an arbitrary false positive range [α, β]. • We build the best known pedestrian detector by combining the these two new techniques. Experimental results on several data sets, especially on challenging human detection data sets, demonstrate the effectiveness of the proposed approach. The new approach outperforms all previously reported pedestrian detection results and achieves state-ofthe-art performance on INRIA, ETH, TUD-Brussels and Caltech-USA pedestrian detection benchmarks.\nEarly versions of our work [10] introduced a pAUCbased node classifier for cascade classification, which optimizes the detection rate in the FPR range around [0.49, 0.51]. and the low-level visual features based on spatial pooling [11]. Here we train a single strong classifier with a new structured learning formulation which has a tighter convex upper bound on the partial AUC\n3 risk compared to [10]. A region proposals generation, known as binarized normed gradients (BING) [12], is applied to speed up the evaluation time of detector. We have also introduced new image features and a few careful design when learning the detector. This leads to a further improvement in accuracy and evaluation time as compared to [10], [11]. Our new detection framework outperforms all reported pedestrian detectors (at the time of submission), including several complex detectors such as LatSVM [13] (a part-based approach which models unknown parts as latent variables), ConvNet [7] (deep hierarchical models) and DBN-Mut [14] (discriminative deep model with mutual visibility relationship)."
    }, {
      "heading" : "1.2 Related work",
      "text" : "A few pedestrian detectors have been proposed over the past decade along with newly created pedestrian detection benchmarks such as INRIA, ETH, TUD-Brussels, Caltech and Daimler Pedestrian data sets. We refer readers to [1] for an excellent review on pedestrian detection frameworks and benchmark data sets. In this section, we briefly discuss some relevant work on object detection and review several recent state-of-the-art pedestrian detectors that are not covered in [1].\nRecent work in the field of object recognition has considered spatial pooling as one of crucial key components for computer vision system to achieve stateof-the-art performance on challenging benchmarks, e.g., Pascal VOC, Scene-15, Caltech, ImageNet [15], [16], [17], [18]. Spatial pooling is a method to extract visual representation based on encoded local features. In summary, visual features are extracted from a patch representing a small sub-window of an image. Feature values in each sub-window are spatially pooled and concatenate to form a final feature vector for classification. Invariant representation is generally obtained by pooling feature vectors over spatially local neighbourhoods.\nThe use of spatial pooling has long been part of recognition architectures such as convolutional networks [19], [20], [21], [22]. Spatial pooling (max pooling) is considered as one of critical key ingredients behind deep convolutional neural networks (CNN) which achieves the best performance in recent large scale visual recognition tasks. In CNN, max pooling has been used to reduce the computational complexity for upper layers and provide a form of translation invariance. Spatial pooling is general and can be applied to various coding methods, such as sparse coding, orthogonal matching pursuit and soft threshold [23]. Yang et al. propose to compute an image representation based on sparse codes of SIFT features with multi-scale spatial max pooling [15]. They conclude that the new representation significantly outperforms the linear spatial pyramid matching kernel. Max pooling achieves the best performance in their experiments compared with square root of mean squared statistics pooling and the mean of absolute values pooling due to its robustness to local spatial variations. To further\nimprove the performance of spatial pooling, Boureau et al. transform the pooling process to be more selective by applying pooling in both image space and descriptor space [24]. The authors show that this simple technique can significantly boost the recognition performance even with relatively small dictionaries.\nVarious ensemble classifiers have been proposed in the literature. Of these, AdaBoost is one the most well known as it has achieved tremendous success in computer vision and machine learning applications. In object detection, the cost of missing a true target is often higher than the cost of a false positive. Classifiers that are optimal under the symmetric cost, and thus treat false positives and negatives equally, cannot exploit this information [25], [26]. Several cost sensitive learning algorithms, where the classifier weights a positive class more heavily than a negative class, have thus been proposed.\nViola and Jones introduced the asymmetry property in Asymetric AdaBoost (AsymBoost) [25]. However, the authors reported that this asymmetry is immediately absorbed by the first weak classifier. Heuristics are then used to avoid this problem. In addition, one needs to carefully cross-validate this asymmetric parameter in order to achieve the desired result. Masnadi-Shirazi and Vasconcelos [27] proposed a cost-sensitive boosting algorithm based on the statistical interpretation of boosting. Their approach is to optimize the cost-sensitive loss by means of gradient descent. Most work along this line addresses the pAUC evaluation criterion indirectly. In addition, one needs to carefully cross-validate the asymmetric parameter in order to maximize the detection rate in a particular false positive range.\nSeveral algorithms that directly optimize the pAUC score have been proposed in bioinformatics [28], [29]. Dodd and Pepe propose a regression modeling framework based on the pAUC score [30]. Komori and Eguchi optimize the pAUC using boosting-based algorithms [29]. Their algorithm is heuristic in nature. Narasimhan and Agarwal develop structural SVM based methods which directly optimize the pAUC score [31], [32]. They demonstrate that their approaches significantly outperform several existing algorithms, including pAUCBoost [29] and asymmetric SVM [33]. Building on Narasimhan and Agarwal’s work, we propose the principled fullycorrective ensemble method which directly optimizes the pAUC evaluation criterion. The approach is flexible and can be applied to an arbitrary false positive range [α, β]. To our knowledge, our approach is the first principled ensemble learning method that directly optimizes the partial AUC in a false positive range not bounded by zero. It is important to emphasize here the difference between our approach and that of [31]. In [31] the authors train a linear structural SVM while our approach learns the ensemble of classifiers.\nA few recently proposed pedestrian detectors are as follows. Sermanet et al. train a pedestrian detector using a convolutional network model [7]. Instead of\n4 using hand designed features, they propose to use unsupervised sparse auto encoders to automatically learn features in a hierarchy. The features generated from a multi-scale convolutional network capture both global and local details such as shapes, silhouette and facial components. Experimental results show that their detector achieves competitive results on major benchmark data sets. Benenson et al. investigate different lowlevel aspects of pedestrian detection [34]. The authors show that by properly tuning low-level features, such as feature selection, pre-processing the raw image and classifier training, it is possible to reach state-of-theart results on major benchmarks. From their paper, one key observation that significantly improves the detection performance is to apply image normalization to the test image before extracting features. Park et al. propose new motion features for detecting pedestrians in a video sequence [8]. The authors use optical flow to align large objects in a sequence of image frames and use temporal difference features to capture the information that remains. By factoring out camera motion and combining their proposed motion features with channel features [35], the new detector achieves a five-fold reduction in false positives over previous best results on the Caltech pedestrian benchmark.\nAnother related work that applies structured SVM learning to object detection is the work of Desai et al. [36]. The authors train the model that captures the spatial arrangements of various object classes in the image by considering which spatial layouts of objects to suppress and which spatial layouts of objects to favor. Although both our approach and [36] cast the problem as a structured prediction and apply the cutting plane optimization, the underlying assumptions and resulting models are quite different. The formulation of [36] focuses on incorporating geometric configurations between multiple object classes instead of optimizing the detection rate within a prescribed false positive range. Our approach learns a function that optimizes the partial AUC risk between two false positive rates. In addition, it is not trivial to extend the formulation of [36] to boosting setting."
    }, {
      "heading" : "1.3 Notation",
      "text" : "Vectors are denoted by lower-case bold letters, e.g., x, matrices are denoted by upper-case bold letters, e.g., X and sets are denoted by calligraphic upper-case letters, e.g., X. All vectors are assumed to be column vectors. The (i, j) entry of X is xij . Let {x+i }mi=1 be a set of pedestrian training examples, {x−j }nj=1 be a set of non-pedestrian training examples and x ∈ Rd be a d dimensional feature vector. The tuple of all training samples is written as S = (S+,S−) where S+ = (x+1 , · · · ,x+m) ∈ Xm and S− = (x − 1 , · · · ,x−n ) ∈ Xn. We denote by H a set of all possible outputs of weak learners. Assuming that we have τ possible weak learners, the output of weak learners for positive and negative data can be represented as\nH = (H+,H−) where H+ ∈ Rτ×m and H− ∈ Rτ×n, respectively. Here h+ti is the label predicted by the weak learner ~t(·) on the positive training data x+i . Each column h:l of the matrix H represents the output of all weak learners when applied to the training instance xl. Each row ht: of the matrix H represents the output predicted by the weak learner ~t(·) on all the training data. In this paper, we are interested in the partial AUC (area under the ROC curve) within a specific false positive range [α, β]. Given n negative training samples, we let jα = dnαe and jβ = bnβc. Let Zβ = ( S− jβ ) denote the set of all subsets of negative training instances of size jβ . We define ζ = {x−kj} jβ j=1 ∈ Zβ as a given subset of negative instances, where k = [k1, . . . , kjβ ] is a vector indicating which elements of S− are included. The goal is to learn a set of binary weak learners and a scoring function, f : Rd → R, that acheive good performance in terms of the pAUC between some specified false positive rates α and β where 0 ≤ α < β ≤ 1. Here f(x) = ∑τ t=1 wt~t(x) where w ∈ Rτ is the linear coefficient vector, {~t(·)}τt=1 denote a set of binary weak learners and τ is the number of weak learners."
    }, {
      "heading" : "2 OUR APPROACH",
      "text" : "Despite several important work on object detection, the most practical and successful pedestrian detector is still the sliding-window based method of Viola and Jones [5]. Their method consists of two main components: feature extraction and the AdaBoost classifier. For pedestrian detection, the most commonly used features are HOG [2] and HOG+LBP [4]. Dollár et al. propose Aggregated Channel Features (ACF) which combine gradient histogram (a variant of HOG), gradients and LUV [37]. ACF uses the same channel features as ChnFtrs [35], which is shown to outperform HOG [34], [35].\nTo train the classifier, the procedure known as bootstrapping is often applied, which harvests hard negative examples and re-trains the classifier. Bootstrapping can be repeated several times. It is shown in [38] that at least two bootstrapping iterations are required for the classifier to achieve good performance. In this paper, we design the new pedestrian detection framework based on the new spatially pooled features, a novel form of ensemble classifier which directly optimizes the partial area under the ROC curve and an efficient region proposals generation. We first propose the new feature type based on a modified low-level descriptor and spatial pooling. In the next section, we discuss how the performance measure can be further improved using the proposed structured learning framework. Finally, we discuss our modifications to [37] in order to achieve state-of-theart detection results on Caltech pedestrian detection benchmark data sets."
    }, {
      "heading" : "2.1 Spatially pooled features",
      "text" : "Spatial pooling has been proven to be invariant to various image transformations and demonstrate better\n5 robustness to noise [16], [23], [24]. Several empirical results have indicated that a pooling operation can greatly improve the recognition performance. Pooling combines several visual descriptors obtained at nearby locations into some statistics that better summarize the features over some region of interest (pooling region). The new feature representation preserves visual information over a local neighbourhood while discarding irrelevant details and noises. Combining max-pooling with unsupervised feature learning methods have led to state-of-the art image recognition performance on several object recognition tasks. Although these feature learning methods have shown promising results over hand-crafted features, computing these features from learned dictionaries is still a time-consuming process for many real-time applications. In this section, we further improve the performance of low-level features by adopting the pooling operator commonly applied in unsupervised feature learning. This simple operation can enhance the feature robustness to noise and image transformation. In the following section, we investigate two visual descriptors which have shown to complement HOG in pedestrian detection, namely covariance descriptors and LBP. It is important to point out here that our approach is not limited to these two features, but can be applied to any low-level visual features.\nBackground A covariance matrix is positive semidefinite. It provides a measure of the relationship between two or more sets of variates. The diagonal entries of covariance matrices represent the variance of each feature and the non-diagonal entries represent the correlation between features. The variance measures the deviation of low-level features from the mean and provides information related to the distribution of lowlevel features. The correlation provides the relationship between multiple low-level features within the region. In this paper, we follow the feature representation as proposed in [3]. However, we introduce an additional edge orientation which considers the sign of intensity derivatives. Low-level features used in this paper are:\n[x, y, |Ix|, |Iy|, |Ixx|, |Iyy|, M, O1, O2]\nwhere x and y represent the pixel location, and Ix and Ixx are first and second intensity derivatives along the x-axis. The last three terms are the gradient magnitude (M = √ I2x + I 2 y ), edge orientation as in [3] (O1 = arctan(|Iy|/|Ix|)) and an additional edge orientation O2 in which,\nO2 = { atan2(Iy, Ix) if atan2(Iy, Ix) > 0, atan2(Iy, Ix) + π otherwise.\nThe orientation O2 is mapped over the interval [0, π]. Although some O1 features might be redundant after introducing O2, these features would not deteriorate the performance as they will not be selected by the weak learner. Our preliminary experiments show that using O1 alone yields slightly worse performance than\ncombining O1 and O2. With the defined mapping, the input image is mapped to a 9-dimensional feature image. The covariance descriptor of a region is a 9 × 9 matrix, and due to symmetry, only the upper triangular part is stored, which has only 45 different values.\nLocal Binary Pattern (LBP) is a texture descriptor that represents the binary code of each image patch into a feature histogram [39]. The standard version of LBP is formed by thresholding the 3×3-neighbourhood of each pixel with the centre pixel’s value. All binary results are combined to form an 8-bit binary value (28 different labels). The histogram of these 256 different labels can be used as texture descriptor. The LBP descriptor has shown to achieve good performance in many texture classification [39]. In this work, we adopt an extension of LBP, known as the uniform LBP, which can better filter out noises [4]. The uniform LBP is defined as the binary pattern that contains at most two bitwise transitions from 0 to 1 or vice versa.\nSpatially pooled covariance In this section, we improve the spatial invariance and robustness of the original covariance descriptor by applying the operator known as spatial pooling. There exist two common pooling strategies in the literature: average pooling and max-pooling. We use max-pooling as it has been shown to outperform average pooling in image classification [23], [24]. We divide the image window into small patches (refer to Fig. 2). For each patch, covariance features are calculated over pixels within the patch. For better invariance to translation and deformation, we perform spatial pooling over a pre-defined spatial region (pooling region) and use the obtained results to represent covariance features in the pooling region. The pooling operator thus summarizes multiple covariance matrices within each pooling region into a single matrix which represents covariance information. We refer to the feature extracted from each pooling region as spatially pooled covariance (sp-Cov) feature. Note that extracting covariance features in each patch can be computed efficiently using the integral image trick [40]. Our sp-Cov differs from covariance features in [3] in the following aspects:\n1. We apply spatial pooling to a set of covariance descriptors in the pooling region. To achieve this, we ignore the geometry of covariance matrix and stack the upper triangular part of the covariance matrix into a vector such that pooling is carried out on the vector space. For simplicity, we carry out pooling over a square image region of fixed resolution. Considering pooling over a set of arbitrary rectangular regions as in [41] is likely to further improve the performance of our features.\n2. Instead of normalizing the covariance descriptor of each patch based on the whole detection window [3], we calculate the correlation coefficient within each patch. The correlation coefficient returns the value in the range [−1, 1]. As each patch is now independent, the feature extraction can be done in parallel on the GPU.\nImplementation We extract sp-Cov using multi-scale\n6\npatches with the following sizes: 8 × 8, 16 × 16 and 32 × 32 pixels. Each scale will generate a different set of visual descriptors. Multi-scale patches have also been used in [42]. In this paper, the use of multi-scale patches is important as it expands the richness of our feature representations and enables us to capture human body parts at different scales. In our experiments, we set the patch spacing stride (step-size) to be 1 pixel. The pooling region is set to be 4 × 4-pixel and the pooling spacing stride is set to 4 pixels in our experiments.\nSpatially pooled LBP Similar to sp-Cov, we divide the image window into small patches and extract LBP over pixels within the patch. The histogram, which represents the frequency of each pattern occurring, is computed over the patch. For better invariance to translation, we perform spatial pooling over a pooling region and use the obtained results to represent the LBP histogram in the pooling region. We refer to the new feature as spatially pooled LBP (sp-LBP) feature.\nImplementation For the LBP operator, we use the 3 × 3-neighbourhood of each pixel and extract the local histogram using a patch size of 4× 4, 8× 8 and 16× 16 pixels. For sp-LBP, the patch spacing stride, the pooling region and the pooling spacing stride are set to 1 pixel, 8× 8-pixel and 4 pixels, respectively.\nDiscussion Although we make use of spatial pooling, our approach differs significantly from the unsupervised feature learning pipeline, which has been successfully applied to image classification problem [6], [42]. Instead of pooling encoded features over a pre-trained dictionary, we compute sp-Cov and sp-LBP by performing pooling directly on covariance and LBP features extracted from local patches. In other words, our proposed approach removes the dictionary learning and feature encoding from the conventional unsupervised feature learning [6], [42]. The advantage of our approach over conventional feature learning is that our features have much less dimensions than the size of visual words often used in generic image classification [6]. Using too few visual words can significantly degrade the recognition performance as reported in [16] and using too\nmany visual words would lead to very high-dimensional features and thus make the classifier training become computationally infeasible."
    }, {
      "heading" : "2.2 Optimizing partial AUC",
      "text" : "Structured learning approach Before we propose our approach, we briefly review the concept of SVM tightpAUC [α, β] [32], in which our ensemble learning approach is built upon. The area under the empirical ROC curve (AUC) can be defined as,\nAUC = 1\nmn m∑ i=1 n∑ j=1 1 ( f(x+i ) > f(x − j ) ) . (1)\nThe objective is to learn a scoring function f , f : Rd → R, that maximizes the AUC, or equivalently, minimizes the empirical risk,\nRAUC(f) = 1−AUC. (2)\nFor the partial AUC (pAUC) in the false positive range [α, β], the empirical pAUC risk can be written as [32]:\nRpAUC(f) = 1\nc\n∑m i=1 ∑jβ j=jα+1 1(f(x+i ) < f(x − (j)f|ζ )). (3)\nHere c is a constant, c = mn(β − α), x+i denotes the i-th positive training instance, x−(j)f|ζ denotes the j-th negative training instance sorted by the scoring function, f , in the set ζ = {x−(j)f|ζ} jβ j=1 ∈ Zβ , ζ denote the chosen\nsubset of negative instances and Zβ = ( S− jβ ) denote the set of all subsets of negative training instances of size jβ . In other words, we sort all negative instances based on their scoring values to obtain {x−(j)f } n j=1 in which f(x−(1)f ) ≥ f(x − (2)f ) ≥ . . . ≥ f(x−(jβ)f ) ≥ . . . ≥ f(x − (n)f ) and ζ = {x−(1)f|ζ , · · · ,x − (jβ)f|ζ\n}. Although the number of elements in ζ is jβ (there are jβ negative samples in the set), the empirical pAUC risk defined in (3) is computed from jβ − jα negative samples.\nClearly (3) is minimal when all positive samples, {x+i }mi=1, are ranked above {x − (j)f|ζ\n}jβj=jα+1, which represent negative samples in our prescribed false positive range [α, β] (in this case, the log-average miss rate would be zero). The structural SVM framework can be adopted to optimize the pAUC risk by considering a classification problem of all m × jβ pairs of positive and negative samples. We define a new label matrix π ∈ Πm,jβ = {0, 1}m×jβ (on the entire positive instances {x+i }mi=1 and a given subset of negative instances ζ = {x−kj} jβ j=1 ∈ Zβ where k = [k1, . . . , kjβ ] is a vector indicating which elements of S− are included) whose value for the pair (i, j) is defined as:\nπij =\n{ 0 if x+i is ranked above x − j\n1 otherwise. (4)\nThe true pair-wise label is defined as π∗ where π∗ij = 0 for all pairs (i, j). The pAUC loss is calculated from the number of pairs of examples that are ranked in the\n7 wrong order, i.e., negative examples are ranked before positive examples. Hence the pAUC loss between the prediction π and the true pair-wise label π∗ can be written as:\n∆(α,β)(π,π ∗) =\n1\nc\n∑m i=1 ∑jβ j=jα+1 ( πi,(j)π − π ∗ i,(j)π ) = 1\nc\n∑m i=1 ∑jβ j=jα+1 ( πi,(j)π − 0 ) = 1\nc\n∑m i=1 ∑jβ j=jα+1 πi,(j)π , (5)\nwhere (j)π denotes the index of the negative instance in S− ranked in the j-th position by any fixed ordering consistent with the matrix π. We define a joint feature map, φζ : (Xm × Xn) ×Πm,jβ → Rd, which takes a set of training instances (m positive samples and n negative samples) and an ordering matrix of dimension m × jβ and produce a vector output in Rd as:\nφζ(S,π) = 1\nc\n∑m i=1 ∑jβ j=1(1− πij)(x + i − x − kj ). (6)\nThis feature map ensures that the variable w (w ∈ Rd) that optimizes w>φζ(S,π) will also produce the optimal pAUC score for w>x. We can summarize the above problem as the following convex optimization problem [32]:\nmin w,ξ\n1 2 ‖w‖22 + ν ξ (7)\ns.t. w>(φζ(S,π ∗)− φζ(S,π)) ≥ ∆(α,β)(π,π∗)− ξ,\n∀ζ ∈ Zβ , ∀π ∈ Πm,jβ and ξ ≥ 0. Note that π∗ denotes the correct relative ordering and π denotes any arbitrary orderings and ν controls the amount of regularization.\nPartial AUC based ensemble classifier In order to design an ensemble-like algorithm for the pAUC, we first introduce a projection function, ~(·), which projects an instance vector x to {−1,+1}. This projection function is also known as the weak learner in boosting. In contrast to the previously described structured learning, we learn the scoring function, which optimizes the area under the curve between two false positive rates of the form: f(x) = ∑τ t=1 wt~t(x) where w ∈ Rτ is the linear coefficient vector, {~t(·)}τt=1 denote a set of binary weak learners and τ is the number of weak learners. Let us assume that we have already learned a set of all projection functions. By using the same pAUC loss, ∆(α,β)(·, ·), as in (5), and the same feature mapping, φζ(·, ·), as in (6), the optimization problem we want to solve is:\nmin w,ξ\n1 2 ‖w‖22 + ν ξ (8)\ns.t. w>(φζ(H,π ∗)− φζ(H,π)) ≥ ∆(α,β)(π,π∗)− ξ,\n∀π ∈ Πm,jβ and ξ ≥ 0. H = (H+,H−) is the projected output for positive and negative training samples. φζ(H,π) = [φζ(h1:,π), · · · , φζ(hτ :,π)] where φζ(ht:,π) :\n(Rm × Rn)×Πm,jβ → R and it is defined as,\nφζ(ht:,π) = 1\nc\n∑m i=1 ∑jβ j=1(1− πij) (9)(\n~t(x+i )− ~t(x − kj\n) ) ,\nwhere {x−kj} jβ j=1 is any given subsets of negative instances and k = [k1, . . . , kjβ ] is a vector indicating which elements of S− are included. The only difference between (7) and (8) is that the original data is now projected to a new non-linear feature space. The dual problem of (8) can be written as,\nmax λ\n∑ πλ(π)∆(α,β)(π\n∗,π)− (10) 1\n2\n∑ π,π̂λ(π)λ(π̂)〈φ∆(H,π), φ∆(H, π̂)〉\ns.t. 0 ≤ ∑ πλ(π) ≤ ν.\nwhere λ is the dual variable, λ(π) denotes the dual variable associated with the inequality constraint for π ∈ Πm,jβ and φ∆(H,π) = φζ(H,π∗) − φζ(H,π). To derive the Lagrange dual problem, the following KKT condition is used,\nw = ∑\nπ∈Πm,jβ\nλ(π) ( φζ(H,π ∗)− φζ(H,π) ) . (11)\nFinding best weak learners In this section, we show how one can explicitly learn the projection function, ~(·). We use the idea of column generation to derive an ensemble-like algorithm similar to LPBoost [43]. The condition for applying the column generation is that the duality gap between the primal and dual problem is zero (strong duality). By inspecting the KKT condition, at optimality, (11) must hold for all t = 1, · · · , τ . In other words, wt = ∑ π∈Πm,jβ λ(π) ( φζ(ht:,π ∗) − φζ(ht:,π) ) must hold for all t. For weak learners in the current working set, the corresponding condition in (11) is satisfied by the current solution. For weak learners that are not yet selected, they do not appear in the current restricted optimization problem and their corresponding coefficients are zero (wt = 0). It is easy to see that if∑ π∈Πm,jβ λ(π) ( φζ(ht:,π ∗) − φζ(ht:,π) )\n= 0 for all ~t(·) that are not in the current working set, then the current solution is already the globally optimal one. Hence the subproblem for selecting the best weak learner is:\n~∗(·) = argmax ~∈H ∣∣∣∑πλ(π)(φζ(h,π∗)− φζ(h,π))∣∣∣. (12) In other words, we pick the weak learner with the value | ∑ πλ(π) ( φζ(h,π ∗)−φζ(h,π) ) | most deviated from zero. Thus, a stopping condition for our algorithm is\n| ∑ πλ(π) ( φζ(h,π ∗)− φζ(h,π) ) | < ε, (13)\nwhere ε > 0 is a small precision constant (e.g., 10−4). To find the most optimal weak learner in H, we consider the relative ordering of all positive and negative instances, π ∈ Πm,jβ = {0, 1}m×jβ , whose value for the pair (i, j)\n8 is similar to (4). Given the weak learner ~(·), we define the joint feature map for the output of the weak learner ~(·) as,\nφζ(h,π) = 1\nc\n∑m i=1 ∑jβ j=1(1− πij) ( ~(x+i )− ~(x − kj ) ) .\n(14)\nThe subproblem for generating the optimal weak learner at iteration t considering the relative ordering of all positive and negative training instances, i.e. (12), can be re-written as,\n~∗t (·) = argmax ~∈H ∣∣∣∑πλ(π)(φζ(h,π∗)− φζ(h,π))∣∣∣ = argmax\n~∈H ∣∣∣∑ π λ(π) ∑ i,j πij ( ~(x+i )− ~(x − j ) )∣∣∣\n= argmax ~∈H ∣∣∣∑ i,j (∑ πλ(π)πij )( ~(x+i )− ~(x − j ) )∣∣∣\n= argmax ~∈H ∣∣∣∑lulyl~(xl)∣∣∣ = argmax\n~∈H\n∑ lulyl~(xl) (15)\nwhere i, j, l index the positive training samples (i = 1, · · · ,m), the negative training samples (j = 1, · · · , jβ) and the entire training samples (l = 1, 2,· · · ,m + jβ), respectively. Here yl is equal to +1 if xl is a positive sample and −1 otherwise, and\nul = {∑ π,j λ(π)πlj if xl is a positive sample∑ π,i λ(π)πil otherwise. (16)\nFor decision stumps and decision trees, the last equation in (15) is always valid since the weak learner set H is negation-closed [44]. In other words, if ~(·) ∈ H, then [−~](·) ∈ H, and vice versa. Here [−~](·) = −~(·). For decision stumps, one can flip the inequality sign such that ~(·) ∈ H and [−~](·) ∈ H. In fact, any linear classifiers of the form sign( ∑ t atxt + a0) are negationclosed. Using (15) to choose the best weak learner is not heuristic as the solution to (12) decreases the duality gap the most for the current solution.\nOptimizing weak learners’ coefficients We solve for the optimal w that minimizes our objective function (8). However, the optimization problem (8) has an exponential number of constraints, one for each matrix π ∈ Πm,jβ . As in [31], [45], we use the cutting plane method to solve this problem. The basic idea of the cutting plane is that a small subset of the constraints are sufficient to find an -approximate solution to the original problem. The algorithm starts with an empty constraint set and it adds the most violated constraint set at each iteration. The QP problem is solved using linear SVM and the process continues until no constraint is violated by more than . Since, the quadratic program is of constant size and the cutting plane method converges in a constant number of iterations, the major bottleneck lies in the combinatorial optimization (over Πm,jβ ) associated with finding the most violated constraint set at each iteration.\nNarasimhan and Agarwal show how this combinatorial problem can be solved efficiently in a polynomial time [31]. We briefly discuss their efficient algorithm in this section.\nThe combinatorial optimization problem associated with finding the most violated constraint can be written as,\nπ̄ = argmax π∈Πm,jβ Qw(π), (17)\nwhere\nQw(π) =∆(α,β)(π ∗,π)− (18)\n1 mn(β − α) ∑ i,j πijw >(h+:i − h − :kj ).\nThe trick to speed up (17) is to note that any ordering of the instances that is consistent with π yields the same objective value, Qw(π) in (18). In addition, one can break down (17) into smaller maximization problems by restricting the search space from Πm,jβ to the set Π w m,jβ where\nΠwm,jβ = { π ∈ Πm,jβ | ∀i, j1 < j2 : πi,(j1)w ≥ πi,(j2)w } .\nHere Πwm,jβ represents the set of all matrices π in which the ordering of the scores of two negative instances, w>h−:j1 and w\n>h−:j2 , is consistent. The new optimization problem is now easier to solve as the set of negative instances over which the loss term in (18) is computed is the same for all orderings in the search space. Interested reader may refer to [32]. We summarize the algorithm of our pAUCEnsT in Algorithm 1.\nA theoretical analysis of the convergence property for Algorithm 1 is as follows.\nProposition 1. At each iteration of Algorithm 1, the objective value decreases.\nProposition 2. The decrease of objective value between iterations t− 1 and t is not less than[\nφζ(ht:,π ∗)− φζ(ht:,π?[t])\n]2 .\nHere,\nπ?[t] = argmax π\n{ ∆(α,β)(π,π ∗) +w>[t]φζ(H,π) } ,\nSee supplementary material for the proofs. Computational complexity Each iteration in Algorithm 1 consists of 5 steps. Step ¬ learns the weak classifier with the minimal weighted error and add this weak learner to the ensemble set. In this step, we train a weak classifier using decision trees. We train the decision tree using the fast implementation of [46], in which feature values are quantized into 256 bins. This procedure costs O((m+n)FK) at each iteration. where m+n is the total number of samples, F is the number of features and K is the number of nodes in the tree.\nWe next analyze the time complexity of step ® which calls Algorithm 2. Algorithm 2 solves the structural SVM\n9\nAlgorithm 1 The training algorithm for pAUCEnsT.\nInput: 1) A set of training examples {xl, yl}, l = 1, · · · ,m+ n; 2) The maximum number of weak learners, tmax; stopping precision constant ε; 3) The regularization parameter, ν; 4) The learning objective based on the partial AUC, α and β; Output: The scoring function†, f(x) = ∑tmax t=1 wt~t(x),\nthat optimizes the pAUC score in the FPR range [α, β];\nInitialize: 1) t = 0; 2) Initilaize sample weights: ul = 0.5m if yl = +1, else ul =\n0.5 n ; 3) Extract low level features and store them in the cache memory for fast data access; while t < tmax and (13) is not met do\n¬ Train a new weak learner using (15). The weak learner corresponds to the weak classifier with the minimal weighted error (maximal edge);  Add the best weak learner into the current set; ® Solve the structured SVM problem using the cutting plane algorithm (Algorithm 2); ¯ Update sample weights, u, using (16); ° t← t+ 1; end † For a node in a cascade classifier, we introduce the threshold, b, and adjust b using the validation set such that sign ( f(x)− b ) achieves the node learning objective;\nproblem using the efficient cutting-plane algorithm. Step ¬ in Algorithm 2 costs O ( tmax(m + n) ) time since the linear kernel scales linearly with the number of training samples [47]. Here tmax is the maximum number of features (weak classifiers). Using the efficient algorithm of [32], step  costs O ( n log n+ (m+nβ) log(m+nβ) ) ≤\nO ( (m+n) log(m+n) ) time where nβ = nβ and β ≤ 1. As shown in [45], the number of iterations of Algorithm 2 is upper bounded by the value which is independent of the number of training samples. Here, we assume that the number of cutting-plane iterations required is bounded by r. In total, the time complexity of Algorithm 2 (Step ® in Algorithm 1) is O ( r ( log(m+n)+tmax ) (m+n) ) . Step ¯ updates the sample variables which can be executed in linear time. In summary, the total time complexity for training tmax boosting iterations using our approach is O ( tmax [ (m+n)(r log(m+n) + rtmax +FK) ]) . From this analysis, most of the training time is spent on training weak learners when FK log(m+ n). We summarizes the computational complexity of our approach in Table 1.\nAlgorithm 2 The cutting-plane algorithm\nInput: 1) A set of weak learners’ outputs H = (H+,H−); 2) The learning objective based on the partial AUC, α and β; 3) The regularization parameter, ν; 4) The cutting-plane termination threshold, ; Output: The weak learners’ coefficients w, the working set C and the dual variables λ, ρ; Initialize: C = ∅; Qw(π) = ∆(α,β)(π ∗,π)− 1 mn(β−α) ∑ i,j πijw >(h+:i − h − :kj ); Repeat ¬ Solve the dual problem using linear SVM,\nmin w,ξ\n1 2 ‖w‖22 + ν ξ s.t. Qw(π) ≤ ξ,∀π ∈ C;\n Compute the most violated constraint,\nπ̄ = argmax π∈Πm,jβ Qw(π);\n® C← C ∪ {π̄}; Until Qw(π̄) ≤ ξ + ;\nTable 1 also compares the computational complexity of our approach with AdaBoost. We discuss the difference between our approach and AdaBoost in the next section.\nDiscussion Our final ensemble classifier has a similar form as the AdaBoost-based object detector of [5]. Based on Algorithm 1, step ¬ and  of our algorithm are identical to the first two steps of AdaBoost adopted in [5]. Similar to AdaBoost, ul in step ¬ plays the role of sample weights associated to each training sample. The major difference between AdaBoost and our approach is in step ® and ¯ where the weak learner’s coefficient is computed and the sample weights are updated. In AdaBoost, the weak learner’s coefficient is calculated as wt = 1 2 log 1− t t where t = ∑ l ulI ( yl 6= ~t(xl) ) and I is the indicator function. The sample weights are updated with\nul = ul exp(−wtyl~t(xl))∑ l ul exp(−wtyl~t(xl) .\nWe point this out here since a minimal modification is required in order to transform the existing implementation of AdaBoost to pAUCEnsT, due to the high similarity.\nWe point out here the major difference between the ensemble classifier proposed in this paper and our earlier work [10]. In this work, we redefine the joint feature map (6) over subsets of negative instances. The new feature map leads to a tighter hinge relaxation on the partial AUC loss. In other words, the joint feature map defined\n10\nin [10] is computed over all the negative instances instead of subsets of negative instances ranked in positions jα + 1, · · · , jβ (corresponding to the FPR range [α, β] one is interested in). As a result, the new formulation is not only faster to train but also perform slightly better on the pAUC measure than [10]."
    }, {
      "heading" : "2.3 Region proposals generation",
      "text" : "The evaluation of our pedestrian detector outlined in the previous subsections can be computed efficiently with the use of integral channel features [35]. However the detector is still not efficient enough to be used in a sliding-window based framework. In order to improve the evaluation time, we adopt a cascaded approach in which classifiers are arranged based on their complexity [5]. In this paper, we adopt a two-stage approach, in which we place the fast to extract features in the first stage and our proposed features with pAUCEnsT in the second stage. In other words, our proposed detector is evaluated only on test samples which pass through the first stage.\nRecently the binarized normed gradients (BING) feature with a linear SVM classifier has been shown to speed up the classical sliding window object detection paradigm by discarding a large set of background patches [12]. On Pascal VOC2007, it achieves a comparable object detection rate to recently proposed Objectness [48] and Selective Search [49], while being three orders of magnitudes faster than these approaches. The detector of [12] is adopted in the first stage of our two-stage detector to filter out a large number of background patches. The underlying idea is to reduce the number of samples that our proposed detector needs to process.\nImplementation The original BING detector of [12] was trained for generic object detection. We make the following modifications to the original BING detector to improve its performance for pedestrian detection.\n1) Instead of resizing the training data to a resolution of 8×8 pixels, we resize the resolution of pedestrian samples to 8 × 16 pixels. This template has the same aspect ratio as the one adopted in [2]. Hence the BING features we use in our paper is 128 bit integer instead of 64 bit integer used in the original paper. The data type UINT128 is adopted to store BING features. In addition, we replace the Pascal VOC2007 training data with the Caltech training data to train the BING detector. 2) The original paper quantizes the test image to a resolution {(w, h)}where w, h ∈ {10, 20, 40, 80, 160, 320}. In this paper, we apply a multi-scale detection with a fixed aspect ratio. We scan the test image at 8 scales per octave (corresponding to a scale stride of 1.09)."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 Spatially pooled features",
      "text" : "We compare the performance of the proposed feature with and without spatial pooling. Our sp-Cov consists of 9 low-level image statistics. We exclude the mean and variance of two image statistics (pixel locations at x and y co-ordinates) since they do not capture discriminative information. We also exclude the correlation coefficient between pixel locations at x and y co-ordinates. Hence there is a total of 136 channels (7 low-level image statistics + 3 · 7 variances + 3 · 35 correlation coefficients + 3 LUV color channels)2. Experiments are carried out using AdaBoost with the shrinkage parameter of 0.1 [50] and level-3 decision trees as weak classifiers. We apply shrinkage to AdaBoost as it has been shown to improve the final classification accuracy [51]. We use the depth-3 decision tree as it offers better generalization performance as shown in [11]. We train three bootstrapping iterations and the final model consists of 2048 weak classifiers with soft cascade. We heuristically set the soft cascade’s rejection threshold to be −10 at every node. Log-average miss rates of detectors trained using covariance descriptors and LBP (without and with spatial pooling) are shown in Table 2. We observe that it is beneficial to apply spatial pooling as it increases the robustness of the features against small deformations and translations. We observe a reduction in miss rate by more than one percent on the INRIA test set. Since we did not combine sp-LBP with HOG as in [4], sp-LBP performs slightly worse than sp-Cov.\nCompared with other pedestrian detectors In this experiment, we compare the performance of our proposed sp-Cov with the original covariance descriptor proposed in [3]. [3] calculates the covariance distance in the Riemannian manifold. As eigen-decomposition is performed, the approach of [3] is computationally expensive. We speed up the weak learner training by proposing our modified covariance features and train the weak learner using the decision tree. The new weak learner is not only simpler than [3] but also highly effective. We compare our previously trained detector with the original covariance descriptor [3] in Fig. 3. We plot HOG [2] and HOG+LBP [4] as the baseline. Similar to the result reported in [34], where the authors show that HOG+Boosting reduces the average miss-rate over HOG+SVM by more than 30%, we observe that applying our sp-Cov features as the channel features significantly\n2. Note here that we extract covariance features at 3 different scales.\n11\nimproves the detection performance over the original covariance detector (a reduction of more than 5% miss rate at 10−4 false positives per window).\nNext we compare the proposed sp-Cov with ACF features (M+O+LUV) [37]. Since ACF uses fewer channels than sp-Cov, for a fair comparison, we increase ACF’s discriminative power by combining ACF features with LBP3 (M+O+LUV+LBP). The results are reported in Table 3. We observe that sp-Cov yields competitive results to M+O+LUV+LBP. From the table, sp-Cov performs better on the INRIA test set, worse on the ETH test set and on par with M+O+LUV+LBP on the TUD-Brussels test set. We observe that the best performance is achieved by combining sp-Cov and sp-LBP with M+O+LUV."
    }, {
      "heading" : "3.2 Ensemble classifier",
      "text" : "Synthetic data set In this experiment, we illustrate the effectiveness of our ensemble classifier on a synthetic data set similar to the one evaluated in [25]. The radius and angle of the positive data is drawn from a uniform distribution [0, 1.5] and [0, 2π], respectively. The radius of the negative data is drawn from a normal distribution with mean of 2 and the standard deviation of 0.4. The angle of the negative data is drawn from a uniform distribution similar to the positive data. We generate 400 positive data and 400 negative data for training and validation purposes (200 for training and 200 for validating the asymmetric parameter). For testing, we evaluate the learned classifier with 2000 positive and negative data. We compare pAUCEnsT against the baseline AdaBoost, Cost-Sensitive AdaBoost (CS-AdaBoost)\n3. In our implementation, we use an extension of LBP, known as the uniform LBP, which can better filter out noises [4]. Each LBP bin corresponds to each channel.\n[27] and Asymmetric AdaBoost (AsymBoost) [25]. For CS-AdaBoost, we set the cost for misclassifying positive and negative data as follows. We assign the asymmetric factor k = C1/C2 and restrict 0.5(C1 + C2) = 1. We then choose the best k which returns the highest partial AUC from {0.5, 0.6, · · · , 2.4, 2.5}. For AsymBoost, we choose the best asymmetric factor k which returns the highest partial AUC from {2−1, 2−0.8, · · · , 21.8, 22}. For our approach, the regularization parameter is chosen from {10−5, 10−4.8, · · · , 10−3.2, 10−3}. We use vertical and horizontal decision stumps as the weak classifier. For each algorithm, we train a strong classifier consisting of 10 weak classifiers. We evaluate the partial AUC of each algorithm at [0, 0.2] FPRs.\nFig. 4 illustrates the boundary decision4 and the pAUC score. Our approach outperforms all other asymmetric classifiers. We observe that pAUCEnsT places more emphasis on positive samples than negative samples to ensure the highest detection rate at the left-most part of the ROC curve (FPR < 0.2). Even though we choose the asymmetric parameter, k, from a large range of values, both CS-AdaBoost and AsymBoost perform slightly worse than our approach. AdaBoost performs worst on this toy data set since it optimizes the overall classification accuracy.\nProtein-protein interaction prediction In this experiment, we compare our approach with existing algorithms which optimize pAUC in bioinformatics. The problem we consider here is a protein-protein interaction prediction [54], in which the task is to predict whether a pair of proteins interact or not. We used the data set labelled ‘Physical Interaction Task in Detailed feature type’, which is publicly available on the internet5. The data set contains 2865 protein pairs known to be interacting (positive) and a random set of 237, 384 protein pairs labelled as non-interacting (negative). We use a subset of 85 features as in [31]. We randomly split the data into two groups: 10% for training/validation and 90% for evaluation. We choose the best regularization parameter form {1, 1/2, 1/5} by 5-fold cross validation. We repeat our experiments 10 times using the same regularization parameter. We train a linear classifier as our weak learner using LIBLINEAR [55]. We set the maximum number of\n4. We set the threshold such that the false positive rate is 0.2. 5. http://www.cs.cmu.edu/∼qyj/papers sulp/proteins05 pages/\nfeature-download.html\n12\nboosting iterations to 100 and report the pAUC score of our approach in Table 4. Baselines include pAUCEns, SVMpAUC, SVMAUC, pAUCBoost and Asymmetric SVM. Our approach outperforms all existing algorithms which optimize either AUC or pAUC . We attribute our improvement over SVMtightpAUC [0, 0.1] [32], as a result of introducing a non-linearity into the original problem. This phenomenon has also been observed in face detection as reported in [56].\nComparison to other asymmetric boosting Here we compare pAUCEnsT against existing asymmetric boosting algorithms, namely, AdaBoost with Fisher LDA postprocessing [56] and AsymBoost [25]. The results of AdaBoost are also presented as the baseline. For each algorithm, we train a strong classifier consisting of 100 weak classifiers (decision trees of depth 2). We then calculate the pAUC score by varying the threshold value in the FPR range [0, 0.1]. For each algorithm, the experiment is repeated 20 times and the average pAUC score is reported. For AsymBoost, we choose k from {2−0.5, 2−0.4, · · · , 20.5} by cross-validation. For our approach, the regularization parameter is chosen from {1, 0.5, 0.2, 0.1} by cross-validation. We evaluate the performance of all algorithms on 3 vision data sets: USPS digits, scenes and face data sets. For USPS, we use raw pixel values and categorize the data sets into two classes: even digits and odd digits. For scenes, we divide the 15-scene data sets used in [57] into 2 groups: indoor and outdoor scenes. We use CENTRIST as our feature descriptors\nand build 50 visual code words using the histogram intersection kernel [58]. Each image is represented in a spatial hierarchy manner. Each image consists of 31 subwindows. In total, there are 1550 feature dimensions per image. For faces, we use face data sets from [5] and randomly extract 5000 negative patches from background images. We apply principle component analysis (PCA) to preserve 95% total variation. The new data set has a dimension of 93. We report the experimental results in Table 5. From the table, pAUCEnsT demonstrates the best performance on all three vision data sets."
    }, {
      "heading" : "3.3 Pedestrian detection",
      "text" : "We evaluate the performance of our approach on the pedestrian detection task. We train the pedestrian detector on the KITTI vision benchmark suite [66] and Caltech-USA pedestrian data set [1]. The KITTI data set was captured from two high resolution stereo camera systems mounted to a vehicle. The vehicle was driven around a mid-size city. All images are color and saved in the portable network graphics (PNG) format. The data set consists of 7481 training images and 7518 test images. To obtain positive training data for the KITTI data set, we crop 2111 fully visible pedestrians from 7481 training images. We expand the positive training data by flipping cropped pedestrian patches along the vertical axis. Negative patches are collected from the KITTI training set with pedestrians, cyclists and ‘don’t care’ regions cropped out. To train the pAUC-based pedestrian detector, we set the resolution of the pedestrian model to 32×64 pixels. We extract visual features based on integral channel features approach. We use five different types of features: color (LUV), magnitude, orientation bins [35], the proposed sp-Cov and the proposed sp-LBP. We use decision trees as weak learners and set the depth of decision trees to be three. The regularization parameter ν is cross-validated from {1, 2−1, · · · , 2−4} using the KITTI training set (dividing the training set into training and validation splits). For FPR range [α, β], we set the α to 0 and again choose the value of β from {1, 2−1, · · · , 2−4} on the cross-validation data set. The pAUCEnsT detector is repeatedly trained with three bootstrapping iterations and the total number of negative samples collected is around 33, 000. The final classifier consists of 2048 weak\n13\nclassifiers. To obtain final detection results, greedy nonmaxima suppression is applied with the default parameter as described in the Addendum of [35]. We submit our detection results to the KITTI benchmark website and report the precision-recall curves of our detector in Fig. 5. The proposed approach outperforms all existing pedestrian detectors reported so far on the KITTI benchmark website.\nNext, we evaluate our classifier on the Caltech-USA benchmark data set. The Caltech-USA data set was collected from a video taken from a vehicle driving through regular traffic in an urban environment (greater Los Angeles metropolitan area). Images were captured at a resoltuion of 640× 480 pixels at 30 frames per second. Pedestrian are categorized into three scales. The near scale includes pedestrians over 80 pixels, the medium scale includes pedestrians at 30 − 80 pixels and the far scale includes pedestrian under 30 pixels. In this paper, we use every 30th frame (starting with the 30th\nframe) of the Caltech dataset. The data is split into training and test sets. For the positive training data, we use 1631 cropped pedestrian patches extracted from 4250 training images. We exclude occluded pedestrians from the Caltech training set [8]. Pedestrian patches are horizontally mirrored (flipped along the vertical axis) to expand the positive training data. Negative patches are collected from the Caltech-USA training set with pedestrians cropped out. To train the pAUC-based pedestrian detector, we set the resolution of the pedestrian model to 32× 64 pixels. We use six different types of features: color (LUV), magnitude, orientation bins [35], histogram of flow6 [68], sp-Cov and sp-LBP. We set the depth of decision trees, the number of bootstrapping iterations and the number of weak classifiers to be the same as in the previous experiment. We evaluate our pedestrian detectors on the conditions that pedestrians are at least 50 pixels in height and at least 65% visible. We use the publicly available evaluation software of Dollár et al. [1],\n6. We use the optical flow implementation of [67] which can be downloaded at http://people.csail.mit.edu/celiu/OpticalFlow/\n14\nwhich computes the AUC from 9 discrete points sampled between [0.01, 1.0] FPPI, to evaluate our experimental results. Fig. 6 compares the performance of our approach with other state-of-the-art algorithms.\nOn the Caltech data set, our approach outperforms all existing pedestrian detectors by a large margin. Spatial distribution of selected visual features is shown in Fig. 7 (left). Each pixel is normalized such that that white pixels indicate most frequently chosen regions. We observe that active regions focus mainly on head, shoulders and feet. Similar observation has also been reported in [34], in which the authors apply a multi-scale model for pedestrian detection. The training time of our approach is under 24 hours on a parallelized quad core Intel Xeon processor.\nRegion proposals generation In this section, we train a two-stage pedestrian detector by placing the efficient BING classifier in the first stage and the previously trained pAUCEnsT pedestrian detector in the second stage. To train the BING detector, the resolution of the pedestrian model is set to 8 × 16 pixels. The learned linear SVM model using BING features is shown in Fig. 7 (right). We observe that most active pixels (white pixels) are near the human contour. The SVM weights shown here are also similar to the learned SVM weights of HOG (Fig. 6b in [2]). We compare the performance of our two-stage pedestrian detector by varying the threshold value of the BING detector in the first stage (varying the number of region proposals being generated). We plot ROC curves of our detector with different BING threshold values in Fig. 8. From the figure, the performance starts to drop as we increase the BING threshold value (reducing the number of region proposals generated).\nHowever we observe that setting the BING threshold value in the range [−0.004, 0.008] results in similar pedestrian detection performance. This clearly demonstrates that the BING detector can be applied to discard a large number of background patches while retaining most pedestrian patches. Table 6 compares the number of region proposals discarded in the first stage, log-average miss rate and the average scanning time (excluding feature extraction and post-processing computation, e.g., non-maximum suppression) of our two-stage detector by varying the threshold value of the BING classifier on the Caltech-USA test set (640× 480-pixel images). From the table, setting the threshold value of the BING classifier to be 0.008 yields similar results to the original pAUCEnsT detector while reducing the window scanning time by half. We observe a slight improvement in the log-average miss rate of 0.1% when we set the BING threshold value to 0.004. We suspect that the BING detector might have discarded a few difficult-to-classify background patches that the pAUCEnsT detector fails to classify.\nNext we compare the performance and evaluation time of our two-stage detector with a soft cascade [69]. For soft cascade, we train AdaBoost with a combination of low-level visual features previously used. All other experimental settings are kept the same (e.g., a number of weak classifiers, a number of bootstrapping iterations, post-processing computation, etc.). We heuristically set the soft cascade’s rejection threshold at every node to be {−160,−80,−40,−20,−10,−1}. The performance and window scanning time of soft cascade with various rejection thresholds is shown in Table 7. We observe that the cascaded classifier performs worse than our two-stage detector (up to 5% worse in terms of the log-average miss rate on the Caltech-USA benchmark). Note that soft cascade (top row in Table 7) has a higher window scanning time than our two-stage approach (top row in Table 6). The reason is that, for soft cascade, the partial sum of weak classifiers’ coefficients is repeatedly compared with the rejection threshold. This additional comparison increases the window scanning time of soft cascade when the rejection threshold is set to be small.\nIt is important to point out that our performance gain comes at the cost of an increase in window scanning time. For example, our detector achieves an average miss rate of 23.9% with an average scan time of 2 seconds per 640×480-pixel image while soft cascade achieves an average miss rate of 27.1% with an average scan scan time of 0.3 seconds per image."
    }, {
      "heading" : "4 CONCLUSION",
      "text" : "In this paper, we have proposed an approach to strengthen the effectiveness of low-level visual features and formulated a new ensemble learning method for object detection. The proposed approach is combined with the efficient proposal generation, which results in the effective classifier which optimizes the average miss rate performance measure. Extensive experiments\n15\ndemonstrate the effectiveness of the proposed approach on both synthetic data and visual detection tasks. We plan to explore the possibility of applying the proposed approach to the multiple scales detector of [70] in order to improve the detection results of low resolution pedestrian images."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "This work was in part supported by the Data to Decisions Cooperative Research Centre. C. Shen’s participation was in part supported by an Australian Research Council Future Fellowship. C. Shen is the corresponding author.\n16\nAPPENDIX"
    }, {
      "heading" : "4.1 Convergence analysis of Algorithm 1",
      "text" : "In this Appendix, we provide a theoretical analysis of the convergence property for the structured ensemble learning in this paper.\nThe main result is as follows.\nProposition 3. At each iteration of Algorithm 1, the objective value decreases.\nProof: We assume that the current solution is a finite subset of weak learners and their corresponding coefficients are w. If at the next iteration one more different weak learner is added into the current weak learner subset, and we re-solve the primal optimization problem, and the corresponding ŵ is zero, then the objective value and the solution would be unchanged. If this happens, the current solution w is already the optimal solution—one is not able to find another weak learner to decrease the objective value.\nNow if the corresponding ŵ is not zero, we have added one more free variable into the primal master problem, and re-solving it must reduce the objective value.\nWith the next proposition, we show that the convergence of Algorithm 1 is guaranteed.\nProposition 4. The decrease of objective value between iterations t− 1 and t is not less than[ φζ(ht:,π ∗)− φζ(ht:,π?[t]) ]2 .\nHere, π?[t] = argmax\nπ\n{ ∆(α,β)(π,π ∗) +w>[t]φζ(H,π) } ,\nand the subscript [t] denotes the index at iteration t.\nProof: Recall that the optimization problem we want to solve is:\nmin w,ξ\n1 2 ‖w‖22 + ν ξ (19)\ns.t. w>(φζ(H,π ∗)− φζ(H,π)) ≥ ∆(α,β)(π,π∗)− ξ,\n∀π ∈ Πm,jβ and ξ ≥ 0. Here H = (H+,H−) is the projected output for positive and negative training samples. φζ(H,π) = [φζ(h1:,π), · · · , φζ(hk:,π)] where φζ(ht:,π) : (Rm × Rn)×Πm,jβ → R and it is defined as,\nφζ(ht:,π) = 1\nc m∑ i=1 jβ∑ j=1 (1− πij) ( ~t(x+i )− ~t(x − kj ) ) , (20)\nwhere {x−kj} jβ j=1 is any given subsets of negative instances and k = [k1, . . . , kjβ ] is a vector indicating which elements of S− are included. At iteration t in Algorithm 1, the primal objective in Equation (19) can be reformulated into:\nF (w[t]) = 1\n2 t∑ τ=1 w2[t],τ + ν · max π∈Πm,jβ { ∆(α,β)(π,π ∗)−w>[t] [ φζ(H,π ∗)− φζ(H,π) ]}\n= 1\n2 t∑ τ=1 w2[t],τ + ν ·maxπ { ∆(α,β)(π,π ∗) +w>[t]φζ(H,π) } − νw>[t]φζ(H,π ∗), (21)\nwhere w[t] denotes the optimal solution at iteration t and w[t] = [ w[t],1, w[t],2, · · · , w[t],t ]>. Let us define\nπ?[t] = argmax π\n{ ∆(α,β)(π,π ∗) +w>[t]φζ(H,π) } . (22)\nNow the objective function at iteration t is\nF (w[t],π ? [t]) =\n1\n2 t∑ τ=1 w2[t],τ + ν ·∆(α,β)(π ? [t],π ∗)−w>[t] [ φζ(H,π ∗)− φζ(H,π?[t]) ] . (23)\n17\nWe know that π?[t] is a sub-optimal maximization solution for iteration (t − 1). Therefore the following inequality must hold:\nF (w[t−1])− F (w[t]) = F (w[t−1],π?[t−1])− F (w[t],π ? [t]) ≥ F (w[t−1],π ? [t])− F (w[t],π ? [t]). (24)\nNow with an arbitrary value ω, we know that\nw̃[t] = [ w[t−1] ω ] is a sub-optimal solution for iteration t. Here w[t−1] is the optimal solution for iteration t−1. The inequality in (24) continues as\nF (w[t−1])− F (w[t]) = · · · ≥ · · · ≥ F (w[t−1],π?[t])− F (w̃[t],π ? [t]). (25)\nWith the above definition (23), We can greatly simplify (25), which is\nr.h.s. of (25) = − 12ω 2 + ω [ φζ(ht:,π ∗)− φζ(ht:,π?[t]) ] .\nIn summary, the objective decrease is lower bounded as:\nF (w[t−1])− F (w[t]) ≥ max ω { − 12ω 2 + ω [ φζ(ht:,π ∗)− φζ(ht:,π?[t]) ]}\n= [ φζ(ht:,π ∗)− φζ(ht:,π?[t]) ]2\nwhere π?[t] is calculated by (22). Note that, since the structured ensemble learning method in [10], the analysis here can be easily adapted so that it applies to [10].\n18"
    } ],
    "references" : [ {
      "title" : "Pedestrian detection: An evaluation of the state of the art",
      "author" : [ "P. Dollár", "C. Wojek", "B. Schiele", "P. Perona" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 4, pp. 743–761, 2012. 1, 3, 12, 13",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Histograms of oriented gradients for human detection",
      "author" : [ "N. Dalal", "B. Triggs" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., vol. 1, 2005. 1, 4, 10, 13, 14",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Pedestrian detection via classification on Riemannian manifolds",
      "author" : [ "O. Tuzel", "F. Porikli", "P. Meer" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 10, pp. 1713–1727, 2008. 1, 2, 5, 10, 11",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An HOG-LBP human detector with partial occlusion handling",
      "author" : [ "X. Wang", "T.X. Han", "S. Yan" ],
      "venue" : "Proc. IEEE Int. Conf. Comp. Vis., 2009. 1, 2, 4, 5, 10, 11",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Robust real-time face detection",
      "author" : [ "P. Viola", "M.J. Jones" ],
      "venue" : "Int. J. Comp. Vis., vol. 57, no. 2, pp. 137–154, 2004. 1, 4, 9, 10, 12, 13",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Linear spatial pyramid matching using sparse coding for image classification",
      "author" : [ "J. Yang", "K. Yu", "Y. Gong", "T. Huang" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2009. 1, 6",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Pedestrian detection with unsupervised multi-stage feature learning",
      "author" : [ "P. Sermanet", "K. Kavukcuoglu", "S. Chintala", "Y. LeCun" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2013. 1, 3",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Exploring weak stabilization for motion feature extraction",
      "author" : [ "D. Park", "C.L. Zitnick", "D. Ramanan", "P. Dollár" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2013. 1, 4, 13",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Regionlets for generic object detection",
      "author" : [ "X. Wang", "M. Yang", "S. Zhu", "Y. Lin" ],
      "venue" : "Proc. IEEE Int. Conf. Comp. Vis., 2013. 1",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient pedestrian detection by directly optimizing the partial area under the ROC curve",
      "author" : [ "S. Paisitkriangkrai", "C. Shen", "A. van den Hengel" ],
      "venue" : "Proc. IEEE Int. Conf. Comp. Vis., 2013. 2, 3, 9, 10, 11, 17",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Strengthening the effectiveness of pedestrian detection with spatially pooled features",
      "author" : [ "——" ],
      "venue" : "Proc. Eur. Conf. Comp. Vis., 2014. 2, 3, 10",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Bing: Binarized normed gradients for objectness estimation at 300fps",
      "author" : [ "M.-M. Cheng", "Z. Zhang", "W.-Y. Lin", "P. Torr" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2014. 3, 10",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Object detection with discriminatively trained part based models",
      "author" : [ "P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, pp. 1627– 1645, 2010. 3, 13",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Modeling mutual visibility relationship with a deep model in pedestrian detection",
      "author" : [ "W. Ouyang", "X. Zeng", "X. Wang" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2013. 3, 13",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Localityconstrained linear coding for image classification",
      "author" : [ "J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2010. 3",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The devil is in the details: an evaluation of recent feature encoding methods",
      "author" : [ "K. Chatfield", "V. Lempitsky", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "Proc. of British Mach. Vis. Conf., 2011. 3, 5, 6",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst., 2012. 3",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Return of the devil in the details: Delving deep into convolutional nets",
      "author" : [ "K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "Proc. British Conf. Mach. Vis., 2014. 3",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, no. 11, p. 22782324, 1998. 3",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Y. Bengio" ],
      "venue" : "Foundations and trends R  © in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009. 3",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Deep convolutional neural fields for depth estimation from a single image",
      "author" : [ "F. Liu", "C. Shen", "G. Lin" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2015. 3",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The treasure beneath convolutional layers: cross convolutional layer pooling for image classification",
      "author" : [ "L. Liu", "C. Shen", "A. van den Hengel" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2015. 3",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The importance of encoding versus training with sparse coding and vector quantization",
      "author" : [ "A. Coates", "A. Ng" ],
      "venue" : "Proc. Int. Conf. Mach. Learn., 2011. 3, 5",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Ask the locals: multi-way local pooling for image recognition",
      "author" : [ "Y. Boureau", "N.L. Roux", "F. Bach", "J. Ponce", "Y. LeCun" ],
      "venue" : "Proc. IEEE Int. Conf. Comp. Vis., 2011. 3, 5",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Fast and robust classification using asymmetric AdaBoost and a detector cascade",
      "author" : [ "P. Viola", "M. Jones" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst. MIT Press, 2002, pp. 1311–1318. 3, 11, 12",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Fast pedestrian detection using a cascade of boosted covariance features",
      "author" : [ "S. Paisitkriangkrai", "C. Shen", "J. Zhang" ],
      "venue" : "IEEE Trans. Circuits & Syst. for Vid. Tech., vol. 18, no. 8, 2008. 3",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Cost-sensitive boosting",
      "author" : [ "H. Masnadi-Shirazi", "N. Vasconcelos" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 2, pp. 294–309, 2011. 3, 11",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The linear combinations of biomarkers which maximize the partial area under the roc curves",
      "author" : [ "M.-J. Hsu", "H.-M. Hsueh" ],
      "venue" : "Comp. Stats., vol. 28, no. 2, pp. 1–20, 2012. 3",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A boosting method for maximizing the partial area under the roc curve",
      "author" : [ "O. Komori", "S. Eguchi" ],
      "venue" : "BMC Bioinformatics, vol. 11, no. 1, p. 314, 2010. 3, 11",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Combining diagnostic test results to increase accuracy",
      "author" : [ "M.S. Pepe", "M.L. Thompson" ],
      "venue" : "Biostatistics, vol. 1, no. 2, pp. 123–140, 2000. 3",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A structural svm based approach for optimizing partial AUC",
      "author" : [ "H. Narasimhan", "S. Agarwal" ],
      "venue" : "Proc. Int. Conf. Mach. Learn., 2013. 3, 8, 11",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "SVM  pAUC: a new support vector method for optimizing partial AUC based on a tight convex upper bound",
      "author" : [ "——" ],
      "venue" : "ACM Int. Conf. on Knowl. disc. and data mining, 2013. 3, 6, 7, 8, 9, 11, 12",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Asymmetric support vector machines: low false-positive learning under the user tolerance",
      "author" : [ "S.-H. Wu", "K.-P. Lin", "C.-M. Chen", "M.-S. Chen" ],
      "venue" : "Proc. of Intl. Conf. on Knowledge Discovery and Data Mining, 2008. 3, 11",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Seeking the strongest rigid detector",
      "author" : [ "R. Benenson", "M. Mathias", "T. Tuytelaars", "L.V. Gool" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2013. 4, 10, 13, 14",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Integral channel features",
      "author" : [ "P. Dollár", "Z. Tu", "P. Perona", "S. Belongie" ],
      "venue" : "Proc. of British Mach. Vis. Conf., 2009. 4, 10, 12, 13",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Discriminative models for multi-class object layout",
      "author" : [ "C. Desai", "D. Ramanan", "C. Fowlkes" ],
      "venue" : "Int. J. Comp. Vis., vol. 95, pp. 1–12, 2011. 4",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Fast feature pyramids for object detection",
      "author" : [ "P. Dollár", "R. Appel", "S. Belongie", "P. Perona" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. PP, no. 99, p. 1, 2014. 4, 11, 13",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "New features and insights for pedestrian detection",
      "author" : [ "S. Walk", "N. Majer", "K. Schindler", "B. Schiele" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., San Francisco, US, 2010. 4, 13",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multiresolution grayscale and rotation invariant texture classification with local binary patterns",
      "author" : [ "T. Ojala", "M. Pietikainen", "T. Maenpaa" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 7, pp. 971–987, 2002. 5",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Region covariance: A fast descriptor for detection and classification",
      "author" : [ "O. Tuzel", "F. Porikli", "P. Meer" ],
      "venue" : "Proc. Eur. Conf. Comp. Vis., 2006. 5",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Beyond spatial pyramids: Receptive field learning for pooled image features",
      "author" : [ "Y. Jia", "C. Huang", "T. Darrell" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2012. 5",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multipath sparse coding using hierarchical matching pursuit",
      "author" : [ "L. Bo", "X. Ren", "D. Fox" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2013. 6",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Linear programming boosting via column generation",
      "author" : [ "A. Demiriz", "K. Bennett", "J. Shawe-Taylor" ],
      "venue" : "Mach. Learn., vol. 46, no. 1–3, pp. 225–254, 2002. 7",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Boosting learning algorithm for pattern recognition and beyond",
      "author" : [ "O. Komori", "S. Eguchi" ],
      "venue" : "IEICE Trans. Infor. and Syst., vol. 94, no. 10, pp. 1863–1869, 2011. 8",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1863
    }, {
      "title" : "Cutting-plane training of structural svms",
      "author" : [ "T. Joachims", "T. Finley", "C.-N.J. Yu" ],
      "venue" : "Mach. Learn., vol. 77, no. 1, pp. 27–59, 2009. 8, 9, 11",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Quickly boosting decision trees pruning underachieving features early",
      "author" : [ "R. Appel", "T. Fuchs", "P. Dollár", "P. Perona" ],
      "venue" : "Proc. Int. Conf. Mach. Learn., 2013. 8",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Training linear svms in linear time",
      "author" : [ "T. Joachims" ],
      "venue" : "Proc. of Intl. Conf. on Knowledge Discovery and Data Mining, 2006. 9",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Measuring the objectness of image windows",
      "author" : [ "B. Alexe", "T. Deselaers", "V. Ferrari" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, pp. 2189–2202, 2012. 10",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Selective search for object recognition",
      "author" : [ "J. Uijlings", "K. van de Sande", "T. Gevers", "A.W. Smeulders" ],
      "venue" : "Int. J. Comp. Vis., vol. 104, no. 2, pp. 154–171, 2013. 10",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The Elements of Statistical Learning: Prediction, Inference and Data Mining",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2009
    }, {
      "title" : "Additive logistic regression: a statistical view of boosting",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Ann. Stat., vol. 28, no. 2, pp. 337–407, 2000. 10",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A mobile vision system for robust multi-person tracking",
      "author" : [ "A. Ess", "B. Leibe", "K. Schindler", "L. van Gool" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2008. 10",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Multi-cue onboard pedestrian detection",
      "author" : [ "C. Wojek", "S. Walk", "B. Schiele" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2009. 10",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Evaluation of different biological data and computational classification methods for use in protein interaction prediction",
      "author" : [ "Y. Qi", "Z. Bar-Joseph", "J. Klein-Seetharaman" ],
      "venue" : "Proteins: Struct., Func., and Bioinfor., vol. 63, no. 3, pp. 490–500, 2006. 11",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "J. Mach. Learn. Res., vol. 9, pp. 1871–1874, 2008. 11",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 1871
    }, {
      "title" : "Fast asymmetric learning for cascade face detection",
      "author" : [ "J. Wu", "S.C. Brubaker", "M.D. Mullin", "J.M. Rehg" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 3, pp. 369–382, 2008. 12",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
      "author" : [ "S. Lazebnik", "C. Schmid", "J. Ponce" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., New York City, USA, 2006. 12",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "CENTRIST: A visual descriptor for scene categorization",
      "author" : [ "J. Wu", "J.M. Rehg" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 8, pp. 1489–1501, 2011. 12",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Hierarchical adaptive structural svm for domain adaptation",
      "author" : [ "J. Xu", "S. Ramos", "D. Vazquez", "A. Lopez" ],
      "venue" : "arXiv preprint arXiv:1408.5400, 2014. 13",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Joint 3d estimation of objects and scene layout",
      "author" : [ "A. Geiger", "C. Wojek", "R. Urtasun" ],
      "venue" : "Proc. Adv. Neural Inf. Process. Syst., 2011. 13",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Laser-based segment classification using a mixture of bag-of-words",
      "author" : [ "J. Behley", "V. Steinhage", "A. Cremers" ],
      "venue" : "Proc. Int. Conf. Intel. Robots and Syst., 2013. 13",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Robust multiresolution pedestrian detection in traffic scenes",
      "author" : [ "J. Yan", "X. Zhang", "Z. Lei", "S. Liao", "S.Z. Li" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2013. 13",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Single-pedestrian detection aided by multi-pedestrian detection",
      "author" : [ "W. Ouyang", "X. Wang" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2013. 13",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Detection evolution with multi-order contextual co-occurrence.",
      "author" : [ "G. Chen", "Y. Ding", "J. Xiao", "T. Han" ],
      "venue" : "in Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2013
    }, {
      "title" : "Multiresolution models for object detection",
      "author" : [ "D. Park", "D. Ramanan", "C. Fowlkes" ],
      "venue" : "Proc. Eur. Conf. Comp. Vis., 2010. 13",
      "citeRegEx" : "65",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Are we ready for autonomous driving? The KITTI vision benchmark suite",
      "author" : [ "A. Geiger", "P. Lenz", "R. Urtasun" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2012. 12",
      "citeRegEx" : "66",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Beyond pixels: Exploring new representations and applications for motion analysis",
      "author" : [ "C. Liu" ],
      "venue" : "Ph.D. dissertation, Massachusetts Institute of Technology, 2009. 13",
      "citeRegEx" : "67",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Human detection using oriented histograms of flow and appearance",
      "author" : [ "N. Dalal", "B. Triggs", "C. Schmid" ],
      "venue" : "Proc. Eur. Conf. Comp. Vis., 2006. 13",
      "citeRegEx" : "68",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Robust object detection via soft cascade",
      "author" : [ "L. Bourdev", "J. Brandt" ],
      "venue" : "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2005. 14",
      "citeRegEx" : "69",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In a recent literature survey on pedestrian detection, the authors evaluated several pedestrian detectors and concluded that combining multiple features can significantly boost the performance of pedestrian detection [1].",
      "startOffset" : 217,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : "Hand-crafted low-level visual features have been applied to several computer vision applications and shown promising results [2], [3], [4], [5].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "Hand-crafted low-level visual features have been applied to several computer vision applications and shown promising results [2], [3], [4], [5].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "Hand-crafted low-level visual features have been applied to several computer vision applications and shown promising results [2], [3], [4], [5].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : "Hand-crafted low-level visual features have been applied to several computer vision applications and shown promising results [2], [3], [4], [5].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "Inspired by the recent success of spatial pooling on object recognition and pedestrian detection problems [6], [7], [8], [9], we propose to perform the spatial pooling operation to create the new feature type for the task of pedestrian detection.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 6,
      "context" : "Inspired by the recent success of spatial pooling on object recognition and pedestrian detection problems [6], [7], [8], [9], we propose to perform the spatial pooling operation to create the new feature type for the task of pedestrian detection.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "Inspired by the recent success of spatial pooling on object recognition and pedestrian detection problems [6], [7], [8], [9], we propose to perform the spatial pooling operation to create the new feature type for the task of pedestrian detection.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "Inspired by the recent success of spatial pooling on object recognition and pedestrian detection problems [6], [7], [8], [9], we propose to perform the spatial pooling operation to create the new feature type for the task of pedestrian detection.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "0 false positives per image [1].",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "We show that spatial pooling applied to commonly-used features such as covariance features [3] and LBP descriptors [4] improves accuracy of pedestrian detection.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "We show that spatial pooling applied to commonly-used features such as covariance features [3] and LBP descriptors [4] improves accuracy of pedestrian detection.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : "Early versions of our work [10] introduced a pAUCbased node classifier for cascade classification, which optimizes the detection rate in the FPR range around [0.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "and the low-level visual features based on spatial pooling [11].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "risk compared to [10].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "A region proposals generation, known as binarized normed gradients (BING) [12], is applied to speed up the evaluation time of detector.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "This leads to a further improvement in accuracy and evaluation time as compared to [10], [11].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "This leads to a further improvement in accuracy and evaluation time as compared to [10], [11].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Our new detection framework outperforms all reported pedestrian detectors (at the time of submission), including several complex detectors such as LatSVM [13] (a part-based approach which models unknown parts as latent variables), ConvNet [7] (deep hierarchical models) and DBN-Mut [14] (discriminative deep model with mutual visibility relationship).",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "Our new detection framework outperforms all reported pedestrian detectors (at the time of submission), including several complex detectors such as LatSVM [13] (a part-based approach which models unknown parts as latent variables), ConvNet [7] (deep hierarchical models) and DBN-Mut [14] (discriminative deep model with mutual visibility relationship).",
      "startOffset" : 239,
      "endOffset" : 242
    }, {
      "referenceID" : 13,
      "context" : "Our new detection framework outperforms all reported pedestrian detectors (at the time of submission), including several complex detectors such as LatSVM [13] (a part-based approach which models unknown parts as latent variables), ConvNet [7] (deep hierarchical models) and DBN-Mut [14] (discriminative deep model with mutual visibility relationship).",
      "startOffset" : 282,
      "endOffset" : 286
    }, {
      "referenceID" : 0,
      "context" : "We refer readers to [1] for an excellent review on pedestrian detection frameworks and benchmark data sets.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "In this section, we briefly discuss some relevant work on object detection and review several recent state-of-the-art pedestrian detectors that are not covered in [1].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : ", Pascal VOC, Scene-15, Caltech, ImageNet [15], [16], [17], [18].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : ", Pascal VOC, Scene-15, Caltech, ImageNet [15], [16], [17], [18].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : ", Pascal VOC, Scene-15, Caltech, ImageNet [15], [16], [17], [18].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : ", Pascal VOC, Scene-15, Caltech, ImageNet [15], [16], [17], [18].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "The use of spatial pooling has long been part of recognition architectures such as convolutional networks [19], [20], [21], [22].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : "The use of spatial pooling has long been part of recognition architectures such as convolutional networks [19], [20], [21], [22].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "The use of spatial pooling has long been part of recognition architectures such as convolutional networks [19], [20], [21], [22].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "The use of spatial pooling has long been part of recognition architectures such as convolutional networks [19], [20], [21], [22].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : "Spatial pooling is general and can be applied to various coding methods, such as sparse coding, orthogonal matching pursuit and soft threshold [23].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "propose to compute an image representation based on sparse codes of SIFT features with multi-scale spatial max pooling [15].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "transform the pooling process to be more selective by applying pooling in both image space and descriptor space [24].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "Classifiers that are optimal under the symmetric cost, and thus treat false positives and negatives equally, cannot exploit this information [25], [26].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "Classifiers that are optimal under the symmetric cost, and thus treat false positives and negatives equally, cannot exploit this information [25], [26].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 24,
      "context" : "Viola and Jones introduced the asymmetry property in Asymetric AdaBoost (AsymBoost) [25].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 26,
      "context" : "Masnadi-Shirazi and Vasconcelos [27] proposed a cost-sensitive boosting algorithm based on the statistical interpretation of boosting.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 27,
      "context" : "Several algorithms that directly optimize the pAUC score have been proposed in bioinformatics [28], [29].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 28,
      "context" : "Several algorithms that directly optimize the pAUC score have been proposed in bioinformatics [28], [29].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "Dodd and Pepe propose a regression modeling framework based on the pAUC score [30].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 28,
      "context" : "Komori and Eguchi optimize the pAUC using boosting-based algorithms [29].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 30,
      "context" : "Narasimhan and Agarwal develop structural SVM based methods which directly optimize the pAUC score [31], [32].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "Narasimhan and Agarwal develop structural SVM based methods which directly optimize the pAUC score [31], [32].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 28,
      "context" : "They demonstrate that their approaches significantly outperform several existing algorithms, including pAUCBoost [29] and asymmetric SVM [33].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 32,
      "context" : "They demonstrate that their approaches significantly outperform several existing algorithms, including pAUCBoost [29] and asymmetric SVM [33].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 30,
      "context" : "It is important to emphasize here the difference between our approach and that of [31].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 30,
      "context" : "In [31] the authors train a linear structural SVM while our approach learns the ensemble of classifiers.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "train a pedestrian detector using a convolutional network model [7].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 33,
      "context" : "investigate different lowlevel aspects of pedestrian detection [34].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "propose new motion features for detecting pedestrians in a video sequence [8].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 34,
      "context" : "By factoring out camera motion and combining their proposed motion features with channel features [35], the new detector achieves a five-fold reduction in false positives over previous best results on the Caltech pedestrian benchmark.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 35,
      "context" : "[36].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "Although both our approach and [36] cast the problem as a structured prediction and apply the cutting plane optimization, the underlying assumptions and resulting models are quite different.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 35,
      "context" : "The formulation of [36] focuses on incorporating geometric configurations between multiple object classes instead of optimizing the detection rate within a prescribed false positive range.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 35,
      "context" : "In addition, it is not trivial to extend the formulation of [36] to boosting setting.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "Despite several important work on object detection, the most practical and successful pedestrian detector is still the sliding-window based method of Viola and Jones [5].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "For pedestrian detection, the most commonly used features are HOG [2] and HOG+LBP [4].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "For pedestrian detection, the most commonly used features are HOG [2] and HOG+LBP [4].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 36,
      "context" : "propose Aggregated Channel Features (ACF) which combine gradient histogram (a variant of HOG), gradients and LUV [37].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 34,
      "context" : "ACF uses the same channel features as ChnFtrs [35], which is shown to outperform HOG [34], [35].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 33,
      "context" : "ACF uses the same channel features as ChnFtrs [35], which is shown to outperform HOG [34], [35].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 34,
      "context" : "ACF uses the same channel features as ChnFtrs [35], which is shown to outperform HOG [34], [35].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 37,
      "context" : "It is shown in [38] that at least two bootstrapping iterations are required for the classifier to achieve good performance.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 36,
      "context" : "Finally, we discuss our modifications to [37] in order to achieve state-of-theart detection results on Caltech pedestrian detection benchmark data sets.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 15,
      "context" : "robustness to noise [16], [23], [24].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 22,
      "context" : "robustness to noise [16], [23], [24].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "robustness to noise [16], [23], [24].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "In this paper, we follow the feature representation as proposed in [3].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "(M = √ I2 x + I 2 y ), edge orientation as in [3] (O1 = arctan(|Iy|/|Ix|)) and an additional edge orientation O2 in which,",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 38,
      "context" : "Local Binary Pattern (LBP) is a texture descriptor that represents the binary code of each image patch into a feature histogram [39].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 38,
      "context" : "The LBP descriptor has shown to achieve good performance in many texture classification [39].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : "In this work, we adopt an extension of LBP, known as the uniform LBP, which can better filter out noises [4].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 22,
      "context" : "We use max-pooling as it has been shown to outperform average pooling in image classification [23], [24].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 23,
      "context" : "We use max-pooling as it has been shown to outperform average pooling in image classification [23], [24].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 39,
      "context" : "Note that extracting covariance features in each patch can be computed efficiently using the integral image trick [40].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "Our sp-Cov differs from covariance features in [3] in the following aspects: 1.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 40,
      "context" : "Considering pooling over a set of arbitrary rectangular regions as in [41] is likely to further improve the performance of our features.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Instead of normalizing the covariance descriptor of each patch based on the whole detection window [3], we calculate the correlation coefficient within each patch.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 41,
      "context" : "Multi-scale patches have also been used in [42].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "Discussion Although we make use of spatial pooling, our approach differs significantly from the unsupervised feature learning pipeline, which has been successfully applied to image classification problem [6], [42].",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 41,
      "context" : "Discussion Although we make use of spatial pooling, our approach differs significantly from the unsupervised feature learning pipeline, which has been successfully applied to image classification problem [6], [42].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 5,
      "context" : "In other words, our proposed approach removes the dictionary learning and feature encoding from the conventional unsupervised feature learning [6], [42].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 41,
      "context" : "In other words, our proposed approach removes the dictionary learning and feature encoding from the conventional unsupervised feature learning [6], [42].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "The advantage of our approach over conventional feature learning is that our features have much less dimensions than the size of visual words often used in generic image classification [6].",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 15,
      "context" : "Using too few visual words can significantly degrade the recognition performance as reported in [16] and using too many visual words would lead to very high-dimensional features and thus make the classifier training become computationally infeasible.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 31,
      "context" : "Structured learning approach Before we propose our approach, we briefly review the concept of SVM tight pAUC [α, β] [32], in which our ensemble learning approach is built upon.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 31,
      "context" : "For the partial AUC (pAUC) in the false positive range [α, β], the empirical pAUC risk can be written as [32]:",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 31,
      "context" : "We can summarize the above problem as the following convex optimization problem [32]:",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 42,
      "context" : "We use the idea of column generation to derive an ensemble-like algorithm similar to LPBoost [43].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 43,
      "context" : "For decision stumps and decision trees, the last equation in (15) is always valid since the weak learner set H is negation-closed [44].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 30,
      "context" : "As in [31], [45], we use the cutting plane method to solve this problem.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 44,
      "context" : "As in [31], [45], we use the cutting plane method to solve this problem.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 30,
      "context" : "Narasimhan and Agarwal show how this combinatorial problem can be solved efficiently in a polynomial time [31].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 31,
      "context" : "Interested reader may refer to [32].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 45,
      "context" : "We train the decision tree using the fast implementation of [46], in which feature values are quantized into 256 bins.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 46,
      "context" : "Step ¬ in Algorithm 2 costs O ( tmax(m + n) ) time since the linear kernel scales linearly with the number of training samples [47].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 31,
      "context" : "Using the efficient algorithm of [32], step ­ costs O ( n log n+ (m+nβ) log(m+nβ) ) ≤",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 44,
      "context" : "As shown in [45], the number of iterations of Algorithm 2 is upper bounded by the value which is independent of the number of training samples.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 4,
      "context" : "Discussion Our final ensemble classifier has a similar form as the AdaBoost-based object detector of [5].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "Based on Algorithm 1, step ¬ and ­ of our algorithm are identical to the first two steps of AdaBoost adopted in [5].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "We point out here the major difference between the ensemble classifier proposed in this paper and our earlier work [10].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "in [10] is computed over all the negative instances instead of subsets of negative instances ranked in positions jα + 1, · · · , jβ (corresponding to the FPR range [α, β] one is interested in).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "As a result, the new formulation is not only faster to train but also perform slightly better on the pAUC measure than [10].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 34,
      "context" : "The evaluation of our pedestrian detector outlined in the previous subsections can be computed efficiently with the use of integral channel features [35].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 4,
      "context" : "In order to improve the evaluation time, we adopt a cascaded approach in which classifiers are arranged based on their complexity [5].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "Recently the binarized normed gradients (BING) feature with a linear SVM classifier has been shown to speed up the classical sliding window object detection paradigm by discarding a large set of background patches [12].",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 47,
      "context" : "On Pascal VOC2007, it achieves a comparable object detection rate to recently proposed Objectness [48] and Selective Search [49], while being three orders of magnitudes faster than these approaches.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 48,
      "context" : "On Pascal VOC2007, it achieves a comparable object detection rate to recently proposed Objectness [48] and Selective Search [49], while being three orders of magnitudes faster than these approaches.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "The detector of [12] is adopted in the first stage of our two-stage detector to filter out a large number of background patches.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "Implementation The original BING detector of [12] was trained for generic object detection.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "This template has the same aspect ratio as the one adopted in [2].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "INRIA [2] 14.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 51,
      "context" : "8% ETH [52] 42.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 52,
      "context" : "[53] 48.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 49,
      "context" : "1 [50] and level-3 decision trees as weak classifiers.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 50,
      "context" : "We apply shrinkage to AdaBoost as it has been shown to improve the final classification accuracy [51].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "We use the depth-3 decision tree as it offers better generalization performance as shown in [11].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "Since we did not combine sp-LBP with HOG as in [4], sp-LBP performs slightly worse than sp-Cov.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "Compared with other pedestrian detectors In this experiment, we compare the performance of our proposed sp-Cov with the original covariance descriptor proposed in [3].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "[3] calculates the covariance distance in the Riemannian manifold.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "As eigen-decomposition is performed, the approach of [3] is computationally expensive.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "The new weak learner is not only simpler than [3] but also highly effective.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "We compare our previously trained detector with the original covariance descriptor [3] in Fig.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "We plot HOG [2] and HOG+LBP [4] as the baseline.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "We plot HOG [2] and HOG+LBP [4] as the baseline.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 33,
      "context" : "Similar to the result reported in [34], where the authors show that HOG+Boosting reduces the average miss-rate over HOG+SVM by more than 30%, we observe that applying our sp-Cov features as the channel features significantly",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "3: ROC curves of our sp-Cov features and the conventional covariance detector [3] on INRIA test images.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 36,
      "context" : "Next we compare the proposed sp-Cov with ACF features (M+O+LUV) [37].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 24,
      "context" : "effectiveness of our ensemble classifier on a synthetic data set similar to the one evaluated in [25].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "In our implementation, we use an extension of LBP, known as the uniform LBP, which can better filter out noises [4].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 30,
      "context" : "Results marked by † were reported in [31].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 9,
      "context" : "1] [10] 56.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 31,
      "context" : "1] [32] 52.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 30,
      "context" : "1] [31] 51.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 28,
      "context" : "1]† [29] 48.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 32,
      "context" : "1]† [33] 44.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 44,
      "context" : "51% SVMAUC [45] 39.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 26,
      "context" : "[27] and Asymmetric AdaBoost (AsymBoost) [25].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[27] and Asymmetric AdaBoost (AsymBoost) [25].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 53,
      "context" : "The problem we consider here is a protein-protein interaction prediction [54], in which the task is to predict whether a pair of proteins interact or not.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 30,
      "context" : "We use a subset of 85 features as in [31].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 54,
      "context" : "We train a linear classifier as our weak learner using LIBLINEAR [55].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "02) [5] 20 0.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 55,
      "context" : "02) [56] 20 0.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : "02) [25] 20 0.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 31,
      "context" : "1] [32], as a result of introducing a non-linearity into the original problem.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 55,
      "context" : "This phenomenon has also been observed in face detection as reported in [56].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 55,
      "context" : "Comparison to other asymmetric boosting Here we compare pAUCEnsT against existing asymmetric boosting algorithms, namely, AdaBoost with Fisher LDA postprocessing [56] and AsymBoost [25].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 24,
      "context" : "Comparison to other asymmetric boosting Here we compare pAUCEnsT against existing asymmetric boosting algorithms, namely, AdaBoost with Fisher LDA postprocessing [56] and AsymBoost [25].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 56,
      "context" : "For scenes, we divide the 15-scene data sets used in [57] into 2 groups: indoor and outdoor scenes.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 57,
      "context" : "We use CENTRIST as our feature descriptors and build 50 visual code words using the histogram intersection kernel [58].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "For faces, we use face data sets from [5] and randomly extract 5000 negative patches from background images.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 65,
      "context" : "We train the pedestrian detector on the KITTI vision benchmark suite [66] and Caltech-USA pedestrian data set [1].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "We train the pedestrian detector on the KITTI vision benchmark suite [66] and Caltech-USA pedestrian data set [1].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 34,
      "context" : "We use five different types of features: color (LUV), magnitude, orientation bins [35], the proposed sp-Cov and the proposed sp-LBP.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 58,
      "context" : "5: Precision-recall curves of our approach and state-of-the-art detectors (DA-PDM [59], LSVM-MDPM-sv [60], LSVM-MDPM-us [13] and mBoW [61]) on the KITTI pedestrian detection test set.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 59,
      "context" : "5: Precision-recall curves of our approach and state-of-the-art detectors (DA-PDM [59], LSVM-MDPM-sv [60], LSVM-MDPM-us [13] and mBoW [61]) on the KITTI pedestrian detection test set.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : "5: Precision-recall curves of our approach and state-of-the-art detectors (DA-PDM [59], LSVM-MDPM-sv [60], LSVM-MDPM-us [13] and mBoW [61]) on the KITTI pedestrian detection test set.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 60,
      "context" : "5: Precision-recall curves of our approach and state-of-the-art detectors (DA-PDM [59], LSVM-MDPM-sv [60], LSVM-MDPM-us [13] and mBoW [61]) on the KITTI pedestrian detection test set.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 61,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 61,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 62,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 36,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 63,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 62,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 13,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 33,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 64,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 37,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 247,
      "endOffset" : 251
    }, {
      "referenceID" : 36,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 1,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 267,
      "endOffset" : 270
    }, {
      "referenceID" : 4,
      "context" : "6: ROC curves of our approach and several state-of-the-art detectors (ACF+SDt [8], MT-DPM+Context [62], MT-DPM [62], MultiResC+2Ped [63], ACF-Caltech [37], MOCO [64], MF+Motion+2Ped [63], DBN-Mut [14], Roerei [34], MultiResC [65], MultiFtr+Motion [38], ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian test set.",
      "startOffset" : 278,
      "endOffset" : 281
    }, {
      "referenceID" : 34,
      "context" : "To obtain final detection results, greedy nonmaxima suppression is applied with the default parameter as described in the Addendum of [35].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 1,
      "context" : "6b in [2]), i.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "We exclude occluded pedestrians from the Caltech training set [8].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 34,
      "context" : "We use six different types of features: color (LUV), magnitude, orientation bins [35], histogram of flow6 [68], sp-Cov and sp-LBP.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 67,
      "context" : "We use six different types of features: color (LUV), magnitude, orientation bins [35], histogram of flow6 [68], sp-Cov and sp-LBP.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "[1],",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 66,
      "context" : "We use the optical flow implementation of [67] which can be downloaded at http://people.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 33,
      "context" : "Similar observation has also been reported in [34], in which the authors apply a multi-scale model for pedestrian detection.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "6b in [2]).",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 68,
      "context" : "Next we compare the performance and evaluation time of our two-stage detector with a soft cascade [69].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 9,
      "context" : "Note that, since the structured ensemble learning method in [10], the analysis here can be easily adapted so that it applies to [10].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "Note that, since the structured ensemble learning method in [10], the analysis here can be easily adapted so that it applies to [10].",
      "startOffset" : 128,
      "endOffset" : 132
    } ],
    "year" : 2015,
    "abstractText" : "Many typical applications of object detection operate within a prescribed false-positive range. In this situation the performance of a detector should be assessed on the basis of the area under the ROC curve over that range, rather than over the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC). We propose a novel ensemble learning method which achieves a maximal detection rate at a user-defined range of false positive rates by directly optimizing the partial AUC using structured learning. In order to achieve a high object detection performance, we propose a new approach to extract low-level visual features based on spatial pooling. Incorporating spatial pooling improves the translational invariance and thus the robustness of the detection process. Experimental results on both synthetic and real-world data sets demonstrate the effectiveness of our approach, and we show that it is possible to train state-of-the-art pedestrian detectors using the proposed structured ensemble learning method with spatially pooled features. The result is the current best reported performance on the Caltech-USA pedestrian detection dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}