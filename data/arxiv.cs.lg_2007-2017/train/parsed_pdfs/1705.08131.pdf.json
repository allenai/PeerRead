{
  "name" : "1705.08131.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Black-Box Attacks against RNN based Malware Detection Algorithms",
    "authors" : [ "Weiwei Hu", "Ying Tan" ],
    "emails" : [ "ytan}@pku.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Machine learning has been widely used in various commercial and non-commercial products, and has brought great convenience and profits to human beings. However, recent researches on adversarial examples show that many machine learning algorithms are not robust at all when someone want to crack them on purpose [29, 6]. Adding some small perturbations to original samples will make a classifier unable to classify them correctly.\nIn some security related applications, attackers will try their best to attack any defensive systems to spread their malicious products such as malware. Existing machine learning based malware detection algorithms mainly represent programs as feature vectors with fixed dimension and classify them between benign programs and malware [15]. For example, a binary feature vector can be constructed according to the presences or absences of system APIs (i.e. application programming interfaces) in a program [26]. Grosse et al. [8] and Hu et al. [11] have shown that fixed dimensional feature based malware detection algorithms are very vulnerable under the attack of adversarial examples.\nRecently, as recurrent neural networks (RNN) become popular, some researchers have tried to use RNN for malware detection and classification [25, 30, 14]. The API sequence invoked by a program is used as the input of RNN. RNN will predict whether the program is benign or malware.\nThis paper tries to validate the security of a RNN based malware detection model when it is attacked by adversarial examples. We proposed a novel algorithm to generate sequential adversarial examples.\nExisting researches on adversarial samples mainly focus on images. Images are represented as matrices with fixed dimensions, and the values of the matrices are continuous. API sequences\n∗Prof. Ying Tan is the corresponding author.\nar X\niv :1\n70 5.\n08 13\n1v 1\n[ cs\n.L G\n] 2\n3 M\nay 2\nconsist of discrete symbols with variable lengths. Therefore, generating adversarial examples for API sequences will become quite different from generating adversarial examples for images.\nTo generate adversarial examples from API sequences we only consider to insert some irreverent APIs into the original sequences. Removing an API from the API sequence may make the program unable to work. How to insert irreverent APIs into the sequence will be the key to generate adversarial examples.\nWe propose a generative RNN based approach to generate irreverent APIs and insert them into the original API sequences. A substitute RNN is trained to fit the victim RNN. Gumbel-Softmax [12] is used to smooth the API symbols and deliver gradient information between the generative RNN and the substitute RNN."
    }, {
      "heading" : "2 Adversarial Examples",
      "text" : "Adversarial examples are usually generated by adding some perturbations to the original samples. Szegedy et al. used a box-constrained L-BFGS to search for an appropriate perturbation which can make a neural network misclassify an image [29]. They found that adversarial examples are able to transfer among different neural networks. Goodfellow et al. proposed the “fast gradient sign method” where added perturbations are determined by the gradients of the cost function with respect to inputs [6]. An iterative algorithm to generate adversarial examples was proposed by Papernot et al. [22]. At each iteration the algorithm only modifies one pixel or two pixels of the image.\nGrosse et al. used the iterative algorithm proposed by Papernot et al. [22] to add some adversarial perturbations to Android malware on about 545 thousand binary features [8]. For the best three malware detection models used in their experiments, about 60% to 70% malware will become undetected after their adversarial attacks.\nPrevious algorithms to generate adversarial examples mainly focused on attacking feed-forward neural networks. Papernot et al. migrated these algorithms to attack RNN [23]. RNN is unrolled along time and existing algorithms for feed-forward neural networks are used to generate adversarial examples for RNN. The limitation of their algorithm is that the perturbations are not truly sequential. For examples, if they want to generate adversarial examples from sentences, they can only replace existing words with others words, but cannot insert words to the original sentences or delete words form the original sentences.\nSometimes it is hard for the attackers to know the structures and parameters of the victim machine learning models. For example, many machine learning models are deployed in remote servers or compiled into binary executables. To attack a black-box victim neural network, Papernot et al. first got the outputs from the victim neural network on their training data, and then trained a substitute neural network to fit the victim neural network [21]. Adversarial examples are generated from the substitute neural network. They also showed that other kinds of machine learning models such as decision trees can also be attacked by using the substitute network to fit them [20].\nBesides substitute network based approaches, several direct algorithms for black-box attacks have been proposed recently. Narodytska et al. adopted a greedy local search to find a small set of pixels by observing the probability outputs of the victim network after applying perturbations [19]. Liu et al. used an ensemble-based algorithm to generate adversarial examples and the adversarial examples are able to attack other black-box models due to the transferability of adversarial examples [18] .\nSeveral defensive algorithms against adversarial examples have been proposed, such as feature selection [32], defensive distillation [24] and retraining [17]. However, it is found that the effectiveness of these defensive algorithms is limited, especially under repeated attacks [8, 4, 3]."
    }, {
      "heading" : "3 RNN for Malware Detection",
      "text" : "In this section we will show how to use RNN to detect malware. Malware detection is regarded as a sequential classification problem [25, 30, 14]. RNN is used to classify whether an API sequence comes from a benign program or malware.\nWe will also introduce some variants of RNN in this section. Malware detection model is usually a black box to malware authors, and they need to take the potential variants into consideration when developing attacking algorithms.\nEach API is represented as a one-hot vector. Assuming there are M APIs in total, these APIs are numbered from 0 to M − 1. The feature vector x of an API is an M -dimensional binary vector. If the API’s number is i, the i-th dimension of x is 1, and other dimensions are all zeros.\nAn API sequence is represented as x1,x2, ...,xT , where T is the length of the sequence. After feeding the input to RNN, the hidden states of RNN can be represented as h1,h2, ...,hT .\nIn the basic version of RNN, the hidden state of the last time step hT is used as the representation of the API sequence. The output layer uses hT to compute the probability distribution over the two classes. Then cross entropy is used as the loss function of API sequence classification.\nThe first variant of the RNN model introduced here is average pooling [2], which uses the average states across h1 to hT as the representation of the sequence, instead of the last state hT .\nAttention mechanism [1] is another variant, which uses weighted average of the hidden states to represent the sequence. Attention mechanism is inspired by the selective nature of human perception. For example, when faced with a picture human beings will focus on some meaningful objects in it, rather than every details of it. Attention mechanism in deep learning makes the model to focus on meaningful parts of inputs. It has shown to be very useful in machine translation [1] and image caption [31].\nAn attention function A is defined to map the hidden state to a scalar value, which indicates the importance of the corresponding time step. The attention function is usually a feed-forward neural network. The attention function values across the whole sequence are then normalized according to\nthe formula αt = exp(A(ht)))/ T∑ s=1 exp(A(hs)), where αt is the final weight of time step t.\nThe above RNN models only process the sequence in the forward direction, while some sequential patterns may lie in the backward direction. Bidirectional RNN tries to learn patterns from both directions [27]. In bidirectional RNN, an additional backward RNN is used to process the reversed sequence, i.e. from xT to x1. The concatenation of the hidden states from both directions is used to calculate the output probability."
    }, {
      "heading" : "4 Attacking RNN based Malware Detection Algorithms",
      "text" : "Papernot et al. [23] migrated the adversarial example generation algorithms for feed-forward neural networks to attack RNN by unrolling RNN along time and regarding it as a special kind of feedforward neural network. However, such model can only replace existing elements in the sequence with other elements, since the perturbations are not truly sequential. This algorithm cannot insert irreverent APIs to the original sequences. The main contribution of this paper is that we proposed a generative RNN based approach to generate sequential adversarial examples, which is able to effectively mine the vulnerabilities in the sequential patterns.\nThe proposed algorithm consists of a generative RNN and a substitute RNN, as shown in Figure 1 and Figure 2. The generative model is based on a modified version of the sequence to sequence model [28], which takes malware’s API sequence as input and generates an adversarial API sequence. The substitute RNN is trained on benign sequences and the Gumbel-Softmax [12] outputs of the generative RNN, in order to fit the black-box victim RNN. The Gumbel-Softmax enables the gradient to back propagate from the substitute RNN to the generative RNN."
    }, {
      "heading" : "4.1 The Generative RNN",
      "text" : "The input of the generative RNN is a malware API sequence, and the output is the generated sequential adversarial example for the input malware. The generative RNN generates a small piece of API sequence after each API and tries to insert the sequence piece after the API.\nFor the input sequence x1,x2, ...,xT , the hidden states of the recurrent layer are h1,h2, ...,hT . At time step t, a small sequence of Gumbel-Softmax output gt1, gt2, ..., gtL with length L is generated based on ht, where L is a hyper-parameter.\nSequence decoder [5] is used to generate the small sequence. The decoder RNN uses the formula hDτ = Dec(x D τ ,h D τ−1) to update hidden states, where x D τ is the input and h D τ is the hidden state of the decoder RNN which is initialized with zero.\nFormula 1 is used to get the hidden state when generating gt1.\nhD1 = Dec(ht,h D 0 = 0). (1)\nWhen generating the first element at time step t, the input is the hidden state ht.\nThen a softmax layer is followed to generate the API. Besides the M APIs, we introduce a special null API into the API set. If the null API is generated at time step τ , no API will be inserted to the original sequence at that moment. If we do not use the null API, too many generated APIs will be inserted into the sequence and the resulting sequence will become too long. Allowing null API will make the final sequence shorter. Since the M valid APIs have been numbered from 0 to M − 1, the null API is numbered as M .\nThe softmax layer will have M + 1 elements, which is calculated as πt1 = softmax(W shD1 ), whereW s is the weights to map the hidden state to the output layer.\nThen we can sample an API from πt1. Let the one-hot representation of the sampled API be at1.\nThe sampled API is a discrete symbol. If we give at1 to the substitute RNN, we are unable to get the gradients from the substitute RNN and thus unable to train the generative RNN.\nGumbel-Softmax is recently proposed to approximate one-hot vectors with differentiable representations [12]. The Gumbel-Softmax output gt1 has the same dimension with πt1. The i-th element of gt1 is calculated by Formula 2.\ngit1 = exp((log(πit1) + zi)/temp) M∑ j=0 exp((log(πjt1) + zj)/temp) , (2)\nwhere zi is a random number sampled from the Gumbel distribution [9] and temp is the temperature of Gumbel-Softmax. In this paper we use a superscript to index the element in a vector.\nTo generate the τ -th API at time step t when τ is greater than 1, the decoder RNN uses Formula 3 to update the hidden state.\nhDτ = Dec(W ggt(τ−1),h D τ−1). (3)\nThe decoder RNN takes the previous Gumbel-Softmax output as input. W g is used to map gt(τ−1) to a space with the same dimension as ht, in order to make the input dimension of the decoder RNN compatible with Formula 1.\nCalculating Gumbel-Softmax for τ > 1 can use the same way as τ = 1 (i.e. Formula 2). Therefore, we omit the formula here.\nAfter generating small sequences from t = 1 to T and inserting the generated sequences to the original sequence, we obtained two kinds of results.\nThe first kind of result is the one-hot representation of the final adversarial sequence Sadv:\nSadv = RemoveNull(x1,a11,a12, ...,a1L,x2,a21,a22, ...,a2L, ......,xT ,aT1,aT2, ...,aTL). (4)\nThe generated null APIs should be removed from the one-hot sequence.\nThe second kind of result uses Gumbel-Softmax outputs to replace one-hot representations:\nSGumbel = x1, g11, g12, ..., g1L,x2, g21, g22, ..., g2L, ......,xT , gT1, gT2, ..., gTL. (5)\nThe null APIs’ Gumbel-Softmax outputs are reserved in the sequence, in order to connect the gradients of loss function with null APIs. The loss function will be defined in the following sections."
    }, {
      "heading" : "4.2 The Substitute RNN",
      "text" : "Malware authors usually do not know the detailed structure of the victim RNN. They do not know whether the victim RNN uses bidirectional connection, average pooling and the attention mechanism. The weights of the victim RNN is also unavailable to malware authors.\nTo fit such victim RNN with unknown structure and weights, a neural network with strong representation ability should be used. Therefore, the substitute RNN uses bidirectional RNN with attention mechanism since it is able to learn complex sequential patterns. Bidirectional connection contains both the forward connection and the backward connection, and therefore it has the ability to represent the unidirectional connection. The attention mechanism is able to focus on different positions of the sequence. Therefore, RNN with attention mechanism can represent the cases without attention mechanism such as average pooling and the using of the last state to represent the sequence.\nTo fit the victim RNN, the substitute RNN should regard the output labels of the victim RNN on the training data as the target labels. The training data should contain both malware and benign programs.\nAs shown in Figure 1 and the previous section, for malware input two kinds of outputs are generated from the generative RNN, i.e. the one-hot adversarial example Sadv and the Gumbel-Softmax output SGumbel.\nWe use the victim RNN to detect the one-hot adversarial example, and get the resulting label v. v is a binary value where 0 represents benign label and 1 represents malware.\nThen the substitute RNN is used to classify the Gumbel-Softmax output SGumbel, and outputs the malicious probability pS .\nCross entropy is used as the loss function, as shown in Formula 6.\nLS = −v log(pS)− (1− v)log(1− pS). (6)\nFor a benign input sequence, it is directly fed into the victim RNN and the substitute RNN, as shown in Figure 2. The outputs of the two RNNs v and pS are used to calculate the loss function in the same way as Formula 6."
    }, {
      "heading" : "4.3 Training",
      "text" : "The training objective of the generative RNN is to minimize the predicted malicious probability pS on SGumbel. We also add a regularization term to restrict the number of inserted APIs in the adversarial sequence by maximizing the null API’s expectation probability. The final loss function of the generative RNN is defined in Formula 7.\nLG = log(pS)− γEt=1∼T,τ=1∼LπMtτ , (7)\nwhere γ is the regularization coefficient and M is the index of the null API.\nThe training process of the proposed model is summarized in Algorithm 1.\nAlgorithm 1 Training the Proposed Model 1: while terminal condition not satisfied do 2: Sample a minibatch of data, which contains malware and benign programs. 3: Calculate the outputs of the generative RNN for malware. 4: Get the outputs of the substitute RNN on benign programs and the Gumbel-Softmax output\nof malware. 5: Get the outputs of the victim RNN on the adversarial examples and benign programs. 6: Minimize LS on both benign and malware data by updating the substitute RNN’s weights. 7: Minimize LG on malware data by updating the generative RNN’s weights. 8: end while"
    }, {
      "heading" : "5 Experiments",
      "text" : "Adam [13] was used to train all of the models. LSTM unit was used for all of the RNNs presented in the experiments due to its good performance in processing long sequences [10, 7]."
    }, {
      "heading" : "5.1 Dataset",
      "text" : "We crawled 180 programs with corresponding behavior reports from a website for malware analysis (https://malwr.com/). On the website users can upload their programs and the website will execute\nthe programs in virtual machines. Then the API sequences called by the uploaded programs will be posted on the website. 70% of the crawled programs are malware.\nIn real-world applications, the adversarial example generation model and the victim RNN should be trained by malware authors and anti-virus vendors respectively. The datasets that they collected cannot be the same. Therefore, we use different training sets for the two models. We selected 30% of our dataset as the training set of the adversarial example generation model (i.e. the generative RNN and the substitute RNN), and selected 10% as the validation set of the adversarial example generation model. Then we selected another 30% and 10% as the training set and the validation set of the victim RNN respectively. The remaining 20% of our dataset was regarded as the test set."
    }, {
      "heading" : "5.2 The Victim RNNs",
      "text" : "To validate the representation ability of the substitute RNN, we used the several different structures for the black-box victim RNN, as shown in the first column of Table 1. In Table 1, the first LSTM model uses the last hidden state as the representation of the sequence. BiLSTM represents bidirectional LSTM. The suffixes “Average” and “Attention” in the last four rows indicate the use of average pooling and attention mechanism to represent the sequence.\nWe first tuned the hyper-parameters of BiLSTM-Attention on the validation set. The final learning rate was set to 0.001. The number of recurrent hidden layers and the number of attention hidden layers were both set to one and the layer sizes were both set to 128. We directly used the resulting hyperparameters to other victim models. We have tried to separately tune the hyper-parameters for other victim RNNs but the performance did not improve much compared with using BiLSTM-Attention’s hyper-parameters.\nTable 1 gives the area under curve (AUC) of these victim RNNs before adversarial attacks.\nOverall, the attention mechanism works better than non-attention approaches, since attention mechanism is able to learn the relative importance of different parts in sequences. LSTM and BiLSTM only use the last hidden state, and therefore the information delivered to the output layer is limited. In this case bidirectional connection delivers more information than unidirectional connection, and AUC of BiLSTM is higher than LSTM. For average pooling and attention mechanism, bidirectional LSTM does not outperform unidirectional LSTM in AUC. Average pooling and attention mechanism are able to capture the information of the whole API sequence. Unidirectional LSTM is enough to learn the sequential patterns. Compared with unidirectional LSTM, bidirectional LSTM has more parameters, which makes the learn process more difficult. Therefore, the bidirectional connection does not improve the performance for average pooling and attention mechanism."
    }, {
      "heading" : "5.3 Experimental Results of the Proposed Model",
      "text" : "The hyper-parameters of the generative RNN and the substitute RNN were tuned separately for each black-box victim RNN. The learning rate and the regularization coefficient were chosen by line search along the direction 0.01, 0.001, et al.. The Gumbel-Softmax temperature was searched in the range [1, 100]. Actually, the decoder length L in the generative RNN is also a kind of regularization coefficient. A large L will make the generative RNN have strong representation ability, but the whole adversarial sequences will become too long, and the generative RNN’s size may exceed the capacity of the GPU memory. Therefore, in our experiments we set L to 1.\nThe experimental results are show in Table 2.\nAfter adversarial attacks, all the victim RNNs fails to detect most of the malware. For different victim RNNs, the detection rates on adversarial examples range from 0.44% to 12.10%, while before adversarial attacks the detection rates range from 90.74% to 93.87%. That is to say, about 90% malware will bypass the detection algorithms under our proposed attack model.\nExcept LSTM, the detection rates on adversarial examples of all the victim models are smaller than 3.03%, which means that victim RNNs are almost unable to detect any malware. The victim model LSTM have detection rates of 12.10% and 11.95% on the training set and the test set respectively, which are higher than other victim RNNs. We can see that for the LSTM model the substitute RNN does not fit the victim RNN very well on the training data.\nThe differences in adversarial examples’ detection rates are very small between the training set and the test set for these victim RNNs. The generalization ability of the proposed model is quite well for unseen malware examples. The proposed adversarial example generation algorithm can be applied to both existing malware and unseen malware.\nIt can be seen that even if the adversarial example generation algorithm and the victim RNN use different RNN models and different training set, most of the adversarial examples are still able to attack the victim RNN successfully. The adversarial examples can transfer among different models and different training sets. The transferability makes it very easy for malware authors to attack RNN based malware detection algorithms."
    }, {
      "heading" : "6 Conclusions and Future Works",
      "text" : "A novel algorithm of generating sequential adversarial examples for malware is proposed in this paper. The generative network is based on the sequence to sequence model. A substitute RNN is trained to fit the black-box victim RNN. We use Gumbel-Softmax to approximate the generated discrete APIs, which is able to propagate the gradients from the substitute RNN to the generative RNN. The proposed model has successfully made most of the generated adversarial examples able to bypass several black-box victim RNNs with different structures.\nPrevious researches on adversarial examples mainly focused on images which have fixed input dimension. We have shown that the sequential machine models are also not safe under adversarial attacks. The problem of adversarial examples becomes more serious when it comes to malware detection. Robust defensive models are needed to deal with adversarial attacks.\nIn future works we will use the proposed model to attack convolutional neural network (CNN) based malware detection algorithms, since many researchers have begun to use CNN to process sequential data recently [33, 16]. We will validate whether a substitute RNN has enough capacity to fit a victim CNN, and whether a substitute CNN has enough capacity to fit a victim RNN. The research on the transferability of adversarial examples between RNN and CNN is very important to the practicability of sequential malware detection algorithms."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "A theoretical analysis of feature pooling in visual recognition",
      "author" : [ "Y-Lan Boureau", "Jean Ponce", "Yann LeCun" ],
      "venue" : "In Proceedings of the 27th international conference on machine learning",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Defensive distillation is not robust to adversarial examples",
      "author" : [ "Nicholas Carlini", "David Wagner" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Evaluation of defensive methods for dnns against multiple adversarial evasion models, 2016",
      "author" : [ "Xinyun Chen", "Bo Li", "Yevgeniy Vorobeychik" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using rnn encoderdecoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1406.1078,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1412.6572,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Lstm: A search space odyssey",
      "author" : [ "Klaus Greff", "Rupesh K Srivastava", "Jan Koutník", "Bas R Steunebrink", "Jürgen Schmidhuber" ],
      "venue" : "IEEE transactions on neural networks and learning systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Adversarial perturbations against deep neural networks for malware classification",
      "author" : [ "Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel" ],
      "venue" : "arXiv preprint arXiv:1606.04435,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Statistical theory of extreme values and some practical applications: a series of lectures",
      "author" : [ "Emil Julius Gumbel", "Julius Lieblein" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1954
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1997
    }, {
      "title" : "Generating adversarial malware examples for black-box attacks based on gan",
      "author" : [ "Weiwei Hu", "Ying Tan" ],
      "venue" : "arXiv preprint arXiv:1702.05983,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2017
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole" ],
      "venue" : "arXiv preprint arXiv:1611.01144,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Deep learning for classification of malware system call sequences",
      "author" : [ "Bojan Kolosnjaji", "Apostolis Zarras", "George Webster", "Claudia Eckert" ],
      "venue" : "In Australasian Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Learning to detect and classify malicious executables in the wild",
      "author" : [ "J Zico Kolter", "Marcus A Maloof" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Sequential short-text classification with recurrent and convolutional neural networks",
      "author" : [ "Ji Young Lee", "Franck Dernoncourt" ],
      "venue" : "arXiv preprint arXiv:1603.03827,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "A general retraining framework for scalable adversarial classification",
      "author" : [ "Bo Li", "Yevgeniy Vorobeychik", "Xinyun Chen" ],
      "venue" : "arXiv preprint arXiv:1604.02606,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Delving into transferable adversarial examples and black-box attacks",
      "author" : [ "Yanpei Liu", "Xinyun Chen", "Chang Liu", "Dawn Song" ],
      "venue" : "arXiv preprint arXiv:1611.02770,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Simple black-box adversarial perturbations for deep networks",
      "author" : [ "Nina Narodytska", "Shiva Prasad Kasiviswanathan" ],
      "venue" : "arXiv preprint arXiv:1612.06299,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow" ],
      "venue" : "arXiv preprint arXiv:1605.07277,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Practical black-box attacks against deep learning systems using adversarial examples",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "arXiv preprint arXiv:1602.02697,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "The limitations of deep learning in adversarial settings",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami" ],
      "venue" : "In Security and Privacy (EuroS&P),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Crafting adversarial input sequences for recurrent neural networks",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ananthram Swami", "Richard Harang" ],
      "venue" : "In Military Communications Conference, MILCOM 2016-2016",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Distillation as a defense to adversarial perturbations against deep neural networks",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami" ],
      "venue" : "In Security and Privacy (SP),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "Malware classification with recurrent networks",
      "author" : [ "Razvan Pascanu", "Jack W Stokes", "Hermineh Sanossian", "Mady Marinescu", "Anil Thomas" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Data mining methods for detection of new malicious executables",
      "author" : [ "Matthew G Schultz", "Eleazar Eskin", "Erez Zadok", "Salvatore J Stolfo" ],
      "venue" : "In Security and Privacy,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2001
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1997
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "arXiv preprint arXiv:1312.6199,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "Malware detection with deep neural network using process behavior",
      "author" : [ "Shun Tobiyama", "Yukiko Yamaguchi", "Hajime Shimada", "Tomonori Ikuse", "Takeshi Yagi" ],
      "venue" : "In Computer Software and Applications Conference (COMPSAC),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio" ],
      "venue" : "In ICML,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Adversarial feature selection against evasion attacks",
      "author" : [ "Fei Zhang", "Patrick PK Chan", "Battista Biggio", "Daniel S Yeung", "Fabio Roli" ],
      "venue" : "IEEE transactions on cybernetics,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "However, recent researches on adversarial examples show that many machine learning algorithms are not robust at all when someone want to crack them on purpose [29, 6].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "However, recent researches on adversarial examples show that many machine learning algorithms are not robust at all when someone want to crack them on purpose [29, 6].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : "Existing machine learning based malware detection algorithms mainly represent programs as feature vectors with fixed dimension and classify them between benign programs and malware [15].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 25,
      "context" : "application programming interfaces) in a program [26].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "[8] and Hu et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "[11] have shown that fixed dimensional feature based malware detection algorithms are very vulnerable under the attack of adversarial examples.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "Recently, as recurrent neural networks (RNN) become popular, some researchers have tried to use RNN for malware detection and classification [25, 30, 14].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 29,
      "context" : "Recently, as recurrent neural networks (RNN) become popular, some researchers have tried to use RNN for malware detection and classification [25, 30, 14].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : "Recently, as recurrent neural networks (RNN) become popular, some researchers have tried to use RNN for malware detection and classification [25, 30, 14].",
      "startOffset" : 141,
      "endOffset" : 153
    }, {
      "referenceID" : 11,
      "context" : "Gumbel-Softmax [12] is used to smooth the API symbols and deliver gradient information between the generative RNN and the substitute RNN.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 28,
      "context" : "used a box-constrained L-BFGS to search for an appropriate perturbation which can make a neural network misclassify an image [29].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "proposed the “fast gradient sign method” where added perturbations are determined by the gradients of the cost function with respect to inputs [6].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 21,
      "context" : "[22].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] to add some adversarial perturbations to Android malware on about 545 thousand binary features [8].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[22] to add some adversarial perturbations to Android malware on about 545 thousand binary features [8].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "migrated these algorithms to attack RNN [23].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "first got the outputs from the victim neural network on their training data, and then trained a substitute neural network to fit the victim neural network [21].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "They also showed that other kinds of machine learning models such as decision trees can also be attacked by using the substitute network to fit them [20].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "adopted a greedy local search to find a small set of pixels by observing the probability outputs of the victim network after applying perturbations [19].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "used an ensemble-based algorithm to generate adversarial examples and the adversarial examples are able to attack other black-box models due to the transferability of adversarial examples [18] .",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 31,
      "context" : "Several defensive algorithms against adversarial examples have been proposed, such as feature selection [32], defensive distillation [24] and retraining [17].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : "Several defensive algorithms against adversarial examples have been proposed, such as feature selection [32], defensive distillation [24] and retraining [17].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "Several defensive algorithms against adversarial examples have been proposed, such as feature selection [32], defensive distillation [24] and retraining [17].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "However, it is found that the effectiveness of these defensive algorithms is limited, especially under repeated attacks [8, 4, 3].",
      "startOffset" : 120,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "However, it is found that the effectiveness of these defensive algorithms is limited, especially under repeated attacks [8, 4, 3].",
      "startOffset" : 120,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "However, it is found that the effectiveness of these defensive algorithms is limited, especially under repeated attacks [8, 4, 3].",
      "startOffset" : 120,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "Malware detection is regarded as a sequential classification problem [25, 30, 14].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 29,
      "context" : "Malware detection is regarded as a sequential classification problem [25, 30, 14].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "Malware detection is regarded as a sequential classification problem [25, 30, 14].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "The first variant of the RNN model introduced here is average pooling [2], which uses the average states across h1 to hT as the representation of the sequence, instead of the last state hT .",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "Attention mechanism [1] is another variant, which uses weighted average of the hidden states to represent the sequence.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "It has shown to be very useful in machine translation [1] and image caption [31].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 30,
      "context" : "It has shown to be very useful in machine translation [1] and image caption [31].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 26,
      "context" : "Bidirectional RNN tries to learn patterns from both directions [27].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "[23] migrated the adversarial example generation algorithms for feed-forward neural networks to attack RNN by unrolling RNN along time and regarding it as a special kind of feedforward neural network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "The generative model is based on a modified version of the sequence to sequence model [28], which takes malware’s API sequence as input and generates an adversarial API sequence.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "The substitute RNN is trained on benign sequences and the Gumbel-Softmax [12] outputs of the generative RNN, in order to fit the black-box victim RNN.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "Sequence decoder [5] is used to generate the small sequence.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "Gumbel-Softmax is recently proposed to approximate one-hot vectors with differentiable representations [12].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "where zi is a random number sampled from the Gumbel distribution [9] and temp is the temperature of Gumbel-Softmax.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "Adam [13] was used to train all of the models.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 9,
      "context" : "LSTM unit was used for all of the RNNs presented in the experiments due to its good performance in processing long sequences [10, 7].",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "LSTM unit was used for all of the RNNs presented in the experiments due to its good performance in processing long sequences [10, 7].",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "The Gumbel-Softmax temperature was searched in the range [1, 100].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 32,
      "context" : "In future works we will use the proposed model to attack convolutional neural network (CNN) based malware detection algorithms, since many researchers have begun to use CNN to process sequential data recently [33, 16].",
      "startOffset" : 209,
      "endOffset" : 217
    }, {
      "referenceID" : 15,
      "context" : "In future works we will use the proposed model to attack convolutional neural network (CNN) based malware detection algorithms, since many researchers have begun to use CNN to process sequential data recently [33, 16].",
      "startOffset" : 209,
      "endOffset" : 217
    } ],
    "year" : 2017,
    "abstractText" : "Recent researches have shown that machine learning based malware detection algorithms are very vulnerable under the attacks of adversarial examples. These works mainly focused on the detection algorithms which use features with fixed dimension, while some researchers have begun to use recurrent neural networks (RNN) to detect malware based on sequential API features. This paper proposes a novel algorithm to generate sequential adversarial examples, which are used to attack a RNN based malware detection system. It is usually hard for malicious attackers to know the exact structures and weights of the victim RNN. A substitute RNN is trained to approximate the victim RNN. Then we propose a generative RNN to output sequential adversarial examples from the original sequential malware inputs. Experimental results showed that RNN based malware detection algorithms fail to detect most of the generated malicious adversarial examples, which means the proposed model is able to effectively bypass the detection algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}