{
  "name" : "1505.00322.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Using PCA to Efficiently Represent State Spaces",
    "authors" : [ "William Curran", "Tim Brys", "Matthew E. Taylor" ],
    "emails" : [ "CURRANW@ONID.OREGONSTATE.EDU", "TIMBRYS@VUB.AC.BE", "TAYLORM@EECS.WSU.EDU", "BILL.SMART@OREGONSTATE.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Reinforcement learning algorithms need to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces. This is known as the curse of dimensionality. By projecting the agent’s state onto a low-dimensional manifold, we can represent the state space in a smaller and more efficient representation. By using this representation during learning, the agent can converge to a good policy much faster. We test this approach in the Mario Benchmarking Domain. When using dimensionality reduction in Mario, learning converges much faster to a good policy. But, there is a critical convergence-performance trade-off. By projecting onto a low-dimensional manifold, we are ignoring important data. In this paper, we explore this trade-off of convergence and performance. We find that learning in as few as 4 dimensions (instead of 9), we can improve performance past learning in the full dimensional space at a faster convergence rate."
    }, {
      "heading" : "1. Introduction",
      "text" : "Learning in high dimensional spaces is necessary and difficult in robotic applications. State and action spaces in robotics become large, continuous, and scale exponentially with the number of joints. This leads to the curse of dimensionality.\nTo address this issue, researchers have developed transfer learning (Pan and Yang, 2010) and learning from demonstration (Argall et al., 2009) approaches. Transfer learning reduces computational complexity by learning in a simple domain, and transferring that knowledge to a more complex domain. In transfer learning there are three key research questions: What to transfer, how to transfer and when to transfer. Each of these questions are difficult to answer and all are completely domain dependent. When the source domain and the target domain are loosely related, straight forward transfer learning does not work, and can lead to worse performance (Pan and Yang, 2010).\nar X\niv :1\n50 5.\n00 32\n2v 2\n[ cs\n.L G\nTransfer learning also requires a computable mapping from the source domain to the target domain, which isn’t always possible.\nLearning from demonstration (LfD) methods speed up convergence time by bootstrapping learning with demonstrations (Argall et al., 2009). LfD learns a policy using examples or demonstrations provided by a human or a robotic teacher. This method extracts state-action pairs from these demonstrations to bootstrap learning. However, these demonstrations must be consistent and accurately represent solving the task. These methods also solve a specific complex task, rather than solve for general control (Argall et al., 2009).\nIn this work, we focus on the core problem of dealing with large state spaces. We approach this issue by projecting the full state space onto a low-dimensional manifold. We use Principal Component Analysis to find this transform, and use it during each learning iteration. In this way, we perform learning in only the low-dimensional space. This leads to a critical trade-off. By projecting onto a low-dimensional manifold, we are throwing out low variance, yet potentially important data. However, learning can converge to a good, yet suboptimal, policy much faster. In this paper, we explore this trade-off of convergence and performance.\nWe organize the rest of this paper as follows. Section 2 describes related work in dimensionality reduction in machine learning and reinforcement learning. In Section 3 we describe our work with PCA, dimensionality reduction and reinforcement learning. Section 4 introduces the Mario Benchmarking Domain and our learning approach. Experimental results are in Section 5, followed by conclusion and related work in Sections 6 and 7."
    }, {
      "heading" : "2. Related Work",
      "text" : "To motivate our approach, we introduce previous work performed in the field of dimensionality reduction."
    }, {
      "heading" : "2.1 Dimensionality Reduction",
      "text" : "Previous work in dimensionality reduction focuses on reducing the space for classification or function approximation. PCA is effective in many machine learning and data mining applications to extract features from large data sets (Pec; Turk and Pentland, 1991). Rather than using PCA for feature extraction in large data sets, we use PCA to reduce the dimensionality of the state space during learning.\nSwinehart and Abbott (2005) used a similar approach for function approximation with neural networks. They find that they can reduce convergence time for reinforcement-based, random walk learning can by reducing the dimension of the parameter space. Liu and Mahadevan (2011) also use dimensionality reduction to compute policies in a low dimensional subspace. Liu computes the low dimensional subspace from a high-dimensional space through random projections. They also reduce convergence time in continuous state spaces."
    }, {
      "heading" : "3. PCA+RL in High-Dimensional Spaces",
      "text" : "to be viable in many scenarios robots need to perform complex manipulation tasks. These complex manipulations need high degree-of-freedom arms and manipulators. For example, the PR2 robot has two 7 DoF arms. When learning position, velocity and acceleration control, this leads to a\n21 dimensional state space per arm. Learning in these high-dimensional spaces is computationally intractable without optimization techniques.\nTo learn in high-dimensional state spaces, our algorithm first computes a transform between the high-dimensional state space and a lower-dimensional space. To perform this computation, we need trajectories across a representative set of the agent’s state space. We can then use any dimensionality reduction technique to learn the transform. In this work, we use Principal Component Analysis (PCA) (Shlens, 2005).\nPCA identifies patterns in data and reduces the dimensions of the dataset with minimal loss of information. It does this by computing a transform to convert correlated data to linearly uncorrelated data. This transformation ensures that the first principal component has the largest possible variance. Each additional component has the largest possible variance uncorrelated with all previous components. Essentially, PCA represents as much as the demonstrated state space as possible in a lower dimension. This transform is given by:\nT = XW (1)\nwhere X is the demonstrated data, W is a p by p matrix whose columns are eigenvectors of XTX and p is the number of principle components (in this case, the number of dimensions).\nTo transform to any arbitrary dimension, k, we can choose k eigenvectors from W with the largest eigenvalues to form a p by k dimensional matrix Wk:\nTk = XWk (2)\nWe then use reinforcement learning to learn trajectories in the new manifold. All learning is in the lower-dimensional space. For each learning iteration, we project state x down to the lowerdimensional space k:\nxk =W T k x (3)\nWe can now compute the action using the chosen learning algorithm, and execute that action in the simulation. The simulation calculates the new state given the executed action, and we can project that state down to the same lower-dimensional space. We can then perform a learning update (Figure 1).\nBy learning in a smaller space, reinforcement learning algorithms will converge must faster. However, PCA cannot represent all the variance in all the demonstrations. Therefore, given infinite time, the converged learning performance will always be worse than learning in the full space. This leads to a critical trade-off. By projecting onto a low-dimensional manifold, we are throwing out low variance, yet possibly important data. Yet, learning can still converge to a good, yet suboptimal, policy much faster."
    }, {
      "heading" : "4. Mario Benchmark Problem",
      "text" : "The Mario benchmark problem (Karakovskiy and Togelius, 2012) is based on Infinite Mario Bros, which is a public reimplementation of the original 80’s game Super Mario Bros R©. In this task, Mario needs to collect as many points as possible, this is done by killing an enemy (10), devouring a mushroom (58) or a fireflower (64), grabbing a coin (16), finding a hidden block (24), finishing the level (1024), getting hurt by a creature (−42) or dying (−512). The actions available to Mario correspond to the buttons on the NES controller, which are (left, right, no direction), (jump, don’t\njump), and (run/fire, don’t run/fire). Mario can take one action from each of these groups simultaneously, resulting in 12 distinct combined or ‘super’ actions. The state space in Mario is quite complex, as Mario observes the exact locations of all enemies on the screen and their type. He also observes all information about himself, such as what mode he is in (small, big, fire). Lastly, he has a gridlike receptive field in which each cell indicates what type of object is in it (such as a brick, a coin, a mushroom, a goomba (enemy), etc.). A screenshot is shown in Figure 2.\nOur reinforcement learning agent for Mario is inspired by Liao’s and Brys’ previous work (Brys et al., 2014; Liao et al., 2012). We use aQ(λ)-learner with a tabular state representation and α=0.01, λ=0.5, γ=0.9 and =0.05. The part of the state space the agent considers consists of these variables:\n• is Mario able to jump (0− 1)\n• is Mario on the ground (0− 1)\n• is Mario able to shoot fireballs (0− 1)\n• Mario’s current direction, 8 directions and standing still (0− 8)\n• enemies closeby (within one gridcell) in 8 directions (28 → 0− 255)\n• enemies at midrange (within one to three gridcells) in 8 directions (0− 255)\n• whether there is an obstacle in four vertical gridcells in front of Mario (24 → 0− 16)\n• closest enemy position within 21x21 grid surrounding Mario + 1 for absent enemy (212+1→ 0− 441)\nThis makes for 3.24×1010 possible states, and 4×1011 Q-values (12 actions in each state). The size of the state space is not a problem computationally, because this state-space is sparsely visited, with the majority of the state-visits to a small set of states (Liao et al., 2012).\nPrevious work has investigated using demonstrations in the Mario domain to speed up reinforcement learning, albeit in different ways: by shaping the reward using the demonstration (Brys et al., 2015), or learning a reward function using inverse reinforcement learning (Lee et al., 2014).\nIn the experiments, we run every learning episode on a procedurally generated level based on a random seed ∈ [0, 106], with difficulty 0. Also, we randomly select the mode Mario starts in (small, large, fire) for each episode. Making an agent learn to play Mario this way helps avoid overfitting on a specific level, and makes for a more generally applicable Mario agent. Our results are always averaged over 100 different trials."
    }, {
      "heading" : "5. Results",
      "text" : "As a preliminary analysis, we calculated all the principle components of the Mario domain to see which dimensions PCA weighed the highest during learning. The jump, ground and current direction features are heavily represented in the first few principal components. This is intuitive, as this state changes frequently throughout a game of Mario. These features are also fundamental skills required to play a game of Mario.\nPCA also associated features related to enemies within close proximity to Mario entirely in the last few principal components. These features are only important in very specific scenarios where Mario needs to quickly react to many nearby enemies. If only one enemy is nearby, it is also represented in the closest enemy X and closest enemy y features.\nThis analysis legitimizes our approach in the Mario Benchmarking Domain. It demonstrates that we should initially learn using the fundamental skills required to play Mario. We will show that we can learn these skills quickly. The skills represented in the higher principal components are better for strict optimization in specific scenarios. Analyzing the principal components of the demonstrations is a sanity check as well as validation of the richness of the demonstration.\nWhen using our approach in the Mario domain, results were as expected. By projecting the state down to a low-dimensional manifold (less than 4), the learning algorithm converged quickly to bad policies. However, when using a manifold of 4 dimensions or greater, we converged quickly to a much better policy. These are promising results although these well-performing dimensions may still converge to a suboptimal policy after more than 5000 episodes.\nSince learning was poor in the first two manifolds, it shows us that the jump and ground features are not informative enough alone to learn an effective Mario policy. Yet, when projecting onto a 3 or 4 dimensional manifold, we see large increase in policy performance. In these manifolds, the\nfeatures jump, ground, current direction, shoot, closest enemy Y, and obstacles are all represented. It is intuitive that these features are important for having basic skill in Mario. The remaining features are important, but only for fine tuning policies."
    }, {
      "heading" : "6. Discussion",
      "text" : "The strengths of our approach is its simplicity and generality combined with fast convergence. Once we sample the domain, PCA can be ran on these samples. We can use the transforms given by PCA in the learning cycle whenever we compute a new state. This is a simple matrix operation and adds no additional computational cost. This preliminary work shows that learning can be efficiently performed in these low dimensional spaces at an increased convergence rate.\nBut, there are some fundamental issues. Our algorithm assumes that all states are important. It is possible for a state to be unnecessary to learning, but have high variance. We also stop learning once we converge in the lower-dimensional space. This ensures that given infinite time, the higher dimensional space will converge to better performance. We discuss our solution to these problems in Section 7 by using an iterative learning approach."
    }, {
      "heading" : "7. Future Work",
      "text" : "There are many areas of future work this approach can take. The next step we will take would be to perform additional analysis on the current results. Further experiments include varying the amount of training data given to PCA, or to use random demonstration data. We can then see how the quality and quantity of data affects learning.\nA major focus of future work is to improve upon the converged performance of the lowdimensional state representations. Work by Grzes and Kudenko has shown that mixed resolution function approximation works well in complex domains. They initially learned with a less expressive function approximation to provide early guidance during learning. They then learned using a more expressive function approximation to learn a high quality policy. We can leverage a similar\nidea. We propose in future work that we can learn until convergence in a n dimensional space. As shown in this work, the convergence time is low. We then transfer that knowledge to an n + 1 dimensional space. We hypothesize that this learning technique will converge faster than learning entirely in the full dimensional space."
    } ],
    "references" : [ {
      "title" : "A survey of robot learning from demonstration",
      "author" : [ "Brenna D. Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning" ],
      "venue" : "Robot. Auton. Syst.,",
      "citeRegEx" : "Argall et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Argall et al\\.",
      "year" : 2009
    }, {
      "title" : "Multi-objectivization of reinforcement learning problems by reward shaping",
      "author" : [ "Tim Brys", "Anna Harutyunyan", "Peter Vrancx", "Matthew E Taylor", "Daniel Kudenko", "Ann Nowé" ],
      "venue" : "In International Joint Conference on Neural Networks (IJCNN),",
      "citeRegEx" : "Brys et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Brys et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement learning from demonstration through shaping",
      "author" : [ "Tim Brys", "Anna Harutyunyan", "Halit Bener Suay", "Sonia Chernova", "Matthew E. Taylor", "Ann Nowé" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Brys et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Brys et al\\.",
      "year" : 2015
    }, {
      "title" : "The Mario AI benchmark and competitions",
      "author" : [ "Sergey Karakovskiy", "Julian Togelius" ],
      "venue" : "Computational Intelligence and AI in Games, IEEE Transactions on,",
      "citeRegEx" : "Karakovskiy and Togelius.,? \\Q2012\\E",
      "shortCiteRegEx" : "Karakovskiy and Togelius.",
      "year" : 2012
    }, {
      "title" : "Learning a super mario controller from examples of human play",
      "author" : [ "Geoffrey Lee", "Min Luo", "Fabio Zambetta", "Xiaodong Li" ],
      "venue" : "In Evolutionary Computation (CEC),",
      "citeRegEx" : "Lee et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2014
    }, {
      "title" : "Cs229 final report reinforcement learning to play mario",
      "author" : [ "Yizheng Liao", "Kun Yi", "Zhe Yang" ],
      "venue" : "Technical report, Stanford University,",
      "citeRegEx" : "Liao et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2012
    }, {
      "title" : "Compressive reinforcement learning with oblique random projections",
      "author" : [ "Bo Liu", "Sridhar Mahadevan" ],
      "venue" : null,
      "citeRegEx" : "Liu and Mahadevan.,? \\Q2011\\E",
      "shortCiteRegEx" : "Liu and Mahadevan.",
      "year" : 2011
    }, {
      "title" : "A survey on transfer learning. Knowledge and Data Engineering",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "Pan and Yang.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "A tutorial on principal component analysis",
      "author" : [ "Jonathon Shlens" ],
      "venue" : "In Systems Neurobiology Laboratory, Salk Institute for Biological Studies,",
      "citeRegEx" : "Shlens.,? \\Q2005\\E",
      "shortCiteRegEx" : "Shlens.",
      "year" : 2005
    }, {
      "title" : "Dimensional reduction for reward-based learning",
      "author" : [ "Christian D. Swinehart", "L.F. Abbott" ],
      "venue" : null,
      "citeRegEx" : "Swinehart and Abbott.,? \\Q2005\\E",
      "shortCiteRegEx" : "Swinehart and Abbott.",
      "year" : 2005
    }, {
      "title" : "Face recognition using eigenfaces",
      "author" : [ "M.A. Turk", "A.P. Pentland" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Turk and Pentland.,? \\Q1991\\E",
      "shortCiteRegEx" : "Turk and Pentland.",
      "year" : 1991
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "To address this issue, researchers have developed transfer learning (Pan and Yang, 2010) and learning from demonstration (Argall et al.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "To address this issue, researchers have developed transfer learning (Pan and Yang, 2010) and learning from demonstration (Argall et al., 2009) approaches.",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : "When the source domain and the target domain are loosely related, straight forward transfer learning does not work, and can lead to worse performance (Pan and Yang, 2010).",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "Learning from demonstration (LfD) methods speed up convergence time by bootstrapping learning with demonstrations (Argall et al., 2009).",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : "These methods also solve a specific complex task, rather than solve for general control (Argall et al., 2009).",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "PCA is effective in many machine learning and data mining applications to extract features from large data sets (Pec; Turk and Pentland, 1991).",
      "startOffset" : 112,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "Swinehart and Abbott (2005) used a similar approach for function approximation with neural networks.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "Liu and Mahadevan (2011) also use dimensionality reduction to compute policies in a low dimensional subspace.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "In this work, we use Principal Component Analysis (PCA) (Shlens, 2005).",
      "startOffset" : 56,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "The Mario benchmark problem (Karakovskiy and Togelius, 2012) is based on Infinite Mario Bros, which is a public reimplementation of the original 80’s game Super Mario Bros R ©.",
      "startOffset" : 28,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "The Mario benchmark problem (Karakovskiy and Togelius, 2012) is based on Infinite Mario Bros, which is a public reimplementation of the original 80’s game Super Mario Bros R ©. In this task, Mario needs to collect as many points as possible, this is done by killing an enemy (10), devouring a mushroom (58) or a fireflower (64), grabbing a coin (16), finding a hidden block (24), finishing the level (1024), getting hurt by a creature (−42) or dying (−512).",
      "startOffset" : 29,
      "endOffset" : 407
    }, {
      "referenceID" : 1,
      "context" : "Our reinforcement learning agent for Mario is inspired by Liao’s and Brys’ previous work (Brys et al., 2014; Liao et al., 2012).",
      "startOffset" : 89,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "Our reinforcement learning agent for Mario is inspired by Liao’s and Brys’ previous work (Brys et al., 2014; Liao et al., 2012).",
      "startOffset" : 89,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "The size of the state space is not a problem computationally, because this state-space is sparsely visited, with the majority of the state-visits to a small set of states (Liao et al., 2012).",
      "startOffset" : 171,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "Previous work has investigated using demonstrations in the Mario domain to speed up reinforcement learning, albeit in different ways: by shaping the reward using the demonstration (Brys et al., 2015), or learning a reward function using inverse reinforcement learning (Lee et al.",
      "startOffset" : 180,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : ", 2015), or learning a reward function using inverse reinforcement learning (Lee et al., 2014).",
      "startOffset" : 76,
      "endOffset" : 94
    } ],
    "year" : 2015,
    "abstractText" : "Reinforcement learning algorithms need to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces. This is known as the curse of dimensionality. By projecting the agent’s state onto a low-dimensional manifold, we can represent the state space in a smaller and more efficient representation. By using this representation during learning, the agent can converge to a good policy much faster. We test this approach in the Mario Benchmarking Domain. When using dimensionality reduction in Mario, learning converges much faster to a good policy. But, there is a critical convergence-performance trade-off. By projecting onto a low-dimensional manifold, we are ignoring important data. In this paper, we explore this trade-off of convergence and performance. We find that learning in as few as 4 dimensions (instead of 9), we can improve performance past learning in the full dimensional space at a faster convergence rate.",
    "creator" : "LaTeX with hyperref package"
  }
}