{
  "name" : "1605.04056.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Causal Discovery for Manufacturing Domains",
    "authors" : [ "Katerina Marazopoulou", "Rumi Ghosh", "Prasanth Lade", "David Jensen" ],
    "emails" : [ "kmarazo@cs.umass.edu", "rumi.ghosh@us.bosch.com", "prasanth.lade@us.bosch.com", "jensen@cs.umass.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Categories and Subject Descriptors H.2.8 [Database Management]: Database Applications— Data mining ; I.2.6 [Artificial Intelligence]: Learning— Concept learning, knowledge acquisition; J.1 [Computer Applications]: Administrative Data Processing—Manufacturing\nKeywords Causal Discovery, Manufacturing, Structure Learning\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00."
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "With increasing interest in the Internet of Things (IoT), the size and complexity of data sets collected from manufacturing processes has grown significantly. The ability to use networked devices and sensors in assembly lines has enabled collection of data from every stage of the manufacturing process. However, the collection of arbitrarily large amounts of data is meaningful only if it leads to actionable insights, especially on how to increase yield and efficiency. Our work is a step in this direction.\nThis work has two objectives. First, we aim to identify the joint causal structure of a specific manufacturing domain. Second, we focus on the the key causal factors that increase yield. To be more specific, given the measurements taken during the production and testing of a product, we aim to identify the causal relationships of the domain, which include the influential factors affecting yield.\nThe traditional approach to address this issue is through Design of Experiments (DoE) [3]. In the context of manufacturing, DoE usually includes carrying out experiments in the real production and testing environment. As a consequence, it can be costly and time-consuming. Therefore, practitioners limit the number of factors examined in each experiment. As the process of manufacturing becomes more complex, sometimes comprising hundreds of possible factors, it becomes very difficult to choose the appropriate factors for further investigation through DoE. Currently, most experts in manufacturing rely on domain knowledge and intuition along with basic statistics to guide their efforts to increase yield and improve quality.\nThis work aims to aid this process by using data mining and knowledge discovery. Specifically, we apply techniques for learning causal structure from real data collected from a manufacturing line in order to identify the key factors affecting yield and the complex interrelationships amongst them1.\nAlgorithms for learning causal structure construct a causal graphical model over the variables of a domain. Such models aim to identify the underlying causal mechanisms of a specific domain. Causal models are of great value, because knowing the causes of a target variable specifies interven-\n1Efforts are ongoing to make this data set public for the benefit of the knowledge discovery and data mining communities.\nar X\niv :1\n60 5.\n04 05\n6v 2\n[ cs\n.L G\n] 1\n3 Ju\nn 20\ntions on other variables that can change the target variable in a desired way. Causal models have been applied in areas such as planning, decision making, epidemiology, and social science, just to mention a few.\nIn what follows, we first describe the manufacturing domain, i.e., the manufacturing process and the structure of an assembly line. We then provide a short introduction to graphical models and causal discovery algorithms. We show qualitative results obtained from application of causal discovery algorithms on the data collected from a production line. We identify the specific challenges presented by manufacturing data and how the standard algorithms need to be modified in order to improve causal discovery. Finally, we present an evaluation of causal structure learning for manufacturing, based on domain expertise and synthetic data."
    }, {
      "heading" : "2. THE MANUFACTURING DOMAIN",
      "text" : "Typical assembly lines consist of multiple stations where different operations take place. In every station, several measurements are taken for the product up to that point. Components are added to an unfinished product in different production stations of an assembly line. A testing station is a station where a product passing through is inspected. Additionally, at the end of the assembly line, there are usually testing stations—called end-of-line (EOL) testing stations— which inspect the final product. Testing stations carry out a series of measurements to test the quality of the product. If the product does not meet the required quality criteria, it is usually rejected. A rejected product is called a scrap and an accepted product is called a good part.\nAn illustration of an assembly line is shown in Figure 1. Production stations are depicted with a rectangle and testing stations with a rhombus. The EOL testing stations are shown in gray. For example, in Figure 1, stations 1, 2, k and p are production stations depicted by blue rectangles. Station r is a test station represented by a blue rhombus. The measurements collected in the different stages of the production cycle are shown in orange rectangles.\nThe components that are used in the main assembly line to form the finished product are also assembled separately in assembly lines. For example, the substations in green in Figure 1 show the assembly line for the component that is added to the main product in station k. In the era of the Internet of Things and connected industry, the information gathered from the measurements in the main assembly line can be augmented with the information from the assembly lines for the individual sub-components. Measurements are also gathered externally, from suppliers of sub-components. We use the term manufacturing cycle data to cover data gathered from the assembly line of the component, as well as from the assembly lines of the sub-components and the supplier data.\nIn this work, the product under investigation is a fuel injector. Measurements were recorded from the assembly line producing the injector and from assembly lines producing the sub-components of the injector. Our goal is to leverage this plethora of measurements to learn the complex causal relationships between factors affecting yield in the production process. However, the methodology for root cause analysis proposed in this paper is generic and can easily be applied to any product manufactured in an assembly line.\n2.1 Description of the data used\nThe raw data produced by the assembly line for injectors consists of over 69,000 factors (or variables) and over one million parts (data points) manufactured within a period of a year. Thus, this is a massive data set from a manufacturing system, both in terms of data points and number of variables measured. For our analysis, the data was preprocessed to ignore features that are unique keys and features with zero variance. Moreover, we considered features from a single assembly line, one substation and one testing line. The clean data set consists of 431 continuous variables, normalized to 0 mean and variance 1.\nIt is worth mentioning that, even after preprocessing, the variables exhibit high pairwise linear correlation, as shown in Figure 5a, and non-linear relationships. These characteristics make the task of causal discovery challenging. However, there is a large amount of domain knowledge and inherent structure in the data that can be leveraged to improve the results."
    }, {
      "heading" : "3. PRELIMINARIES",
      "text" : "In this section we introduce the notation and basic definitions that we will use throughout the paper. Upper-case letters denote random variables (e.g., X, V ). Bold-face fonts denote sets of random variables (e.g., Z). First, we provide a short introduction to Bayesian networks and their causal interpretation. Then, we describe how to learn the structure\nof such models from data."
    }, {
      "heading" : "3.1 Bayesian Networks",
      "text" : "Bayesian networks are directed graphical models that compactly represent sets of probability distributions. A Bayesian network defined over a set of random variables V consists of:\n1. A directed acyclic graph G = 〈V,E〉, known as the structure of the network, and\n2. A set of parameters Θ, where every parameter θ ∈ Θ is a conditional distribution of a node given its parents in the graph.\nA graph G is a partially directed graph if it has both directed and undirected edges. The skeleton of a graph G is the undirected graph that can be obtained by substituting every edge of G with an undirected edge. A path between two nodes X and Y is a sequence of nodes X,V1, . . . , Vn, Y such that there exists an edge between every two consecutive nodes of the path. A v-structure or collider on a graph G is an ordered triple of nodes 〈X,Y, Z〉 such that X → Y ← Z and there is no edge between X and Z.\nThe structure of a Bayesian network represents a set of independencies according to the local Markov condition: a node is independent of its non-descendants given its parents in the graph. The same set of independencies can be derived through the use of the graphical criterion of d-separation. We say that two nodes V1 and V2 are d-separated given a disjoint set of variables Z if there are no d-connecting paths between V1 and V2 given Z. A path between V1 and V2 is d-connecting given a set Z, if every collider along the path is in Z or has a descendant in Z and no other nodes are in Z. It is worth noting that different directed acyclic graphs might represent the same set of independencies. That set of DAGs is known as the Markov equivalence class for that set of independencies. A Markov equivalence class can be graphically represented by a mixed graph (a graph that contains both directed and undirected edges). For every undirected edge, any of the two possible orientations does not change the set of conditional independencies induced by the graph."
    }, {
      "heading" : "3.2 Causal Bayesian Networks",
      "text" : "The notion of causality has been a subject of debate for philosophers and practitioners since ancient times. In this work, we focus on the semantics of causality based on probabilistic distributions and manipulations, following the work of Pearl [11] and Spirtes et al. [18]. In this framework, X is a direct cause of Y with respect to a set of variables Z if changing the value of X results in changes in the probability distribution of Y , assuming that the values of all other variables in Z are held constant [17].\nTo interpret graphs (and thus Bayesian networks) causally and use them for causal inference and discovery, we need to make an additional set of assumptions.\n1. Causal Sufficiency: There are no unmeasured common causes of a measured variable.\n2. Causal Markov condition: The Markov condition and d-separation provide a connection between the structure of a Bayesian network and the independencies that hold in the underlying distribution. If we want to\ninterpret Bayesian networks causally, a stronger form of the Markov condition is needed, the causal Markov condition:\nA variable is independent of its non-effects conditioned on its direct causes.\nThis implies that the independencies that can be read off the structure of the graph (IG) are a subset of the independencies that hold in the underlying distribution (ID): IG ⊆ ID.\n3. Faithfulness: This assumption states that the set of independencies that hold in the distribution, can be read from the structure of the graph: ID ⊆ ID.\nIn a causal Bayesian network, a directed edge X → Y between nodes X and Y denotes that X is a direct cause of Y with respect to the rest of the variables."
    }, {
      "heading" : "3.3 Learning causal models from observational data",
      "text" : "In this section we briefly review how to recover the structure of Bayesian networks from data. Let V = {V1, . . . , Vd} be a set of d random variables and P (V1, . . . , Vd} the joint distribution of these variables. Let D = {x1, . . . ,xn} be a data set consisting of n independent and identically distributed (i.i.d.) samples from the joint distribution over V. The task of causal discovery can be formalized as learning the structure of G when given data set D.\nThree main families of algorithms have been developed that learn the structure of causal models from data.\n1. Constraint-based algorithms operate in two phases. First, they use hypothesis tests to learn an undirected graph and then apply a series of orientation rules to retrieve a partially directed graph that corresponds to the Markov equivalence class of the true model. Algorithms in this category include PC [18], FCI [18], and Grow Shrink [7].\n2. Search-and-score methods search heuristically through the space of possible directed acyclic graphs and pick the one that maximizes a scoring function. Such algorithms include greedy hill climbing search or search using tabu lists. It is worth mentioning Greedy Equivalence Search [2], a score-based approach that searches\nX Y\nZ\nX Y\nZ Z 62 SeparatingSet(X, Y )\nIn this work, we explore the use of constraint-based methods for learning the structure of causal models for manufacturing domains.\n3.3.1 The PC algorithm In this work, we focus on constraint-based algorithms, and\nwe specifically focus on the PC algorithm [18]. PC operates in two phases (pseudocode for the standard algorithm is shown in Algorithm 1). Phase I starts with a fully connected undirected graph and tests pairs of variables for conditional independence given conditioning sets of increasing size. Once two variables X and Y are found to be independent given a separating set S (denoted as X |= Y |S), the edge between X and Y is removed and the separating set is recorded. The output of Phase I is an undirected graph and a list of separating sets for each missing edge. The absence of an edge between X and Y denotes that there exists a set of variables that render X and Y conditionally independent.\nPhase II of PC starts with the undirected graph produced by Phase I and aims to orient as many edges as possible. The first step is to orient the colliders (a schematic representation of how this is done is shown in Figure 3). After all colliders have been oriented, a set of four orientation rules2 is applied repetitively until no changes can be made [8]. The output of Phase II is a (partially) directed model that represents the Markov equivalence class of the underlying distribution.\nUnder the assumptions of causal Markov condition, faithfulness, and causal sufficiency, PC has been shown to be sound and complete in the large sample limit (i.e., with perfect tests of conditional independence). This implies that the algorithm is guaranteed to find the a maximally oriented graph that is consistent with the independencies inferred from the data, under the aforementioned conditions."
    }, {
      "heading" : "4. METHODOLOGY",
      "text" : "First, to obtain a baseline, we applied the standard PC algorithm in the data using significance value α = 0.05 for the tests of conditional independence. For the PC algorithm, we used the implementation provided in the pcalg package [5] (with appropriate modifications, as explained below). We used Fisher’s z-transformation of the partial correlation as the conditional independence test (throughout the paper).\nThe learned graph is shown in Figure 2. It consists of 431 nodes and 1164 edges. This simplistic approach demonstrates some of the challenges presented by manufacturing data. The learned model is hard for practitioners to interpret, both because of the large number of nodes and the density of the learned graphical model.\nWe now present modifications on the PC algorithm that leverage the properties of manufacturing data to learn an interpretable causal model of the domain. Specifically, we consider the following adjustments:\n1. Incorporating the prior domain knowledge regarding the temporal ordering of the stations in PC (discussed in 4.1).\n2. Finetuning the parameters of the conditional independence tests in the algorithm (see 4.2).\n3. Reducing the number of features to consider in the algorithm (see 4.3)."
    }, {
      "heading" : "4.1 Incorporating Prior Knowledge in PC",
      "text" : "The first modification aims to leverage the inherent temporal constraints of a manufacturing process by incorporating prior knowledge in the algorithm. Specifically, events in a manufacturing process are strongly sequential. There is a total ordering among the assembly and testing stations. For example, in Figure 1, station 1 precedes station 2. This induces a partial ordering on the variables measured across all stations. That is, all variables measured at station 1, precede all variables measured at station 2, Therefore, variables of station 2 cannot be causal for variables measured at preceding stations. However, among the variables measured in the same station, we have no information about their ordering.\nIn order to improve the results, we incorporated the available prior knowledge (partial ordering of the variables) in PC. Specifically, after phase I of PC, where we have an unoriented graph, we oriented all edges for which we have temporal information. Then, we ran the second phase of PC to orient as many of the remaining edges as possible. This does not produce a more sparse network, but it does produce a more accurate one.\n2The four rules are: known-non-colliders, cycle avoidance, Meek’s rule 3, and Meek’s rule 4. The latter is necessary for the completeness of the PC algorithm in the presence of prior knowledge."
    }, {
      "heading" : "4.2 Finetuning PC",
      "text" : "For the PC algorithm, there are two main parameters to adjust: the type of conditional independence tests used and the significance level (α value) for these tests. In this work, we only considered different values for the significance level. For the tests of conditional independence, if the computed statistic has a p-value lower than the specified significance threshold, the null hypothesis of independence is rejected, and thus the edge is kept in the model. The higher the α value, the more edges will be kept in the model. In other words, decreasing the α value results in sparser models.\nGiven the large sample size, the algorithm can detect even very weak dependencies from data. However, in many cases, weak dependencies are of no interest to researchers. In order to account for the effect weak dependencies, we incorporated a strength-of-effect cutoff in the conditional independence tests. We used the square of partial correlation to measure the strength of effect. Dependencies weaker than a specified threshold were ignored. Decreasing the strength of effect threshold results in denser models."
    }, {
      "heading" : "4.3 Clustering features",
      "text" : "The measurements taken along the assembly line exhibit high pairwise correlation, as shown in Figure 5a. This is especially true for measurements taken in the same station. This has two consequences for structure learning algorithms. First, features that are highly correlated will remain connected in the learned model. This results in a dense graph and hinders interpretability. Moreover, as noted above, the large number of features by itself is one of the factors that contribute to the uninterpretability of the learned causal model. The fact that certain features are almost perfectly correlated, provides a starting point for decreasing the number of variables.\nIn order to reduce the number of features to be included in the causal model, we clustered highly correlated features together using hierarchical clustering. The distance between two features X and Y was defined as\nd(X,Y ) = 1− |correlation(X,Y )|.\nThe distance between two clusters was computed as the maximum distance between their individual components (i.e.,\nmaximum linkage)3. From each cluster, we chose the most representative feature (the medoid) as the feature that exhibits the maximum average correlation with every other feature in the same cluster. The resulting clusters are coherent, in the sense that each cluster contains mostly variables from the same station.\nTo provide a qualitative evaluation for the quality of the resulting clustering, Figure 4a shows the minimum, maximum, and average correlation across clusters as the number of clusters increases. Similarly, Figure 4b shows how the size of the clusters varies when the number of clusters increases. Moreover, Figure 5b shows the pairwise correlation between the medoids of clusters. In this case, the selected medoids exhibit less pairwise correlation compared to the original features. Finally, the learned causal graph using only the cluster medoids is shown in Figure 6a. This is a significantly smaller model, compared to that of Figure 2, and one that is easier to interpret."
    }, {
      "heading" : "5. EVALUATION",
      "text" : "So far, we have presented qualitative results that show how clustering and domain knowledge can improve the interpretability of the learned causal model. Below, we provide a quantitative evaluation of the impact that prior knowledge, parameter fine-tuning, and feature clustering have on causal discovery.\nIdeally, the performance of a structure-learning algorithm would be evaluated through studying the effects of interventions. For example, to establish that X is a cause of Y , we would intervene on X and observe if the distribution of Y changes in the way predicted by the model. In practice, this type of direct interventions in an assembly line comes at a high cost. To evaluate the quality of the models learned through PC for the manufacturing process, we used two proxies for real interventions:\n1. We compared the learned model against certain “true causal relations”, as identified by domain experts. This establishes the usefulness of causal discovery methods for manufacturing domains.\n3Other linkage options were also used (average, median, centroid, Ward) with similar results.\n2. In order to evaluate different variants of the PC algorithm, we used synthetic models to generate data similar to the real data produced by the manufacturing line. We then compared the accuracy of models learned through different variants of PC against the generative model."
    }, {
      "heading" : "5.1 Evaluation through domain expertise",
      "text" : "One way to partially evaluate the effectiveness of causal structure learning in this manufacturing domain is through the use of domain knowledge. As noted earlier, Figure 6a contains causal relationships extracted by our model on a reduced feature set. Domain experts also provided partial ground truth where they identified nine critical features that are causal for the target variable of interest, based on their expertise and knowledge of the physical properties of the manufacturing line. We indexed each of these features using the medoids of the clusters from the hierarchical clustering that they belonged to. We found that these nine features belonged to three clusters with medoids F32, F25 and F9. From Figure 6a, we observe that F32, F25 and F9 indeed are identified as causes of the target variable F37. Figure 6b contains the exact paths extracted from the full causal model that contain these critical features. This confirms that the causal structure learned from the data agrees with the causal paths provided by the domain experts."
    }, {
      "heading" : "5.2 Evaluation through the use of synthetic models",
      "text" : "The fact that the learned causal model matches the intuition and knowledge of domain experts is very encouraging. However, it only provides validation for a small part of the model (the nine features identified as causes for the target variable). Unfortunately, the lack of ground truth makes the evaluation of the complete causal model impossible (at least without performing experiments directly on the production line). To quantify the performance of causal discovery techniques in the manufacturing domain given the absence of ground truth, we turn to synthetic models and simulated data. Specifically, we use a synthetic model to generate data\nand we treat the generating model as the ground truth. Ideally, the synthetic model will be similar to the actual model that describes the domain, and the generated data will resemble the real data produced by an assembly line.\n5.2.1 Generation of synthetic data The first step towards the evaluation through synthetic\ndata is the generation of the synthetic model and data. The procedure we followed is outlined below:\n• We used a data set Doriginal containing data produced by an actual production line to learn a causal model Mtrue ec using the PC algorithm with parameters α = 0.05 and strength of effect equal to 0.01. Note that we used the available temporal information when learning Mtrue eq . Mtrue eq represents a Markov equivalence class and thus, contains both directed and undirected edges. To generate data, we need a fully directed model. Therefore, we randomly picked a member of the Markov equivalence class Mgenerating . This is a fully directed model that, by definition, respects the orientations enforced by the prior knowledge.\n• We then learned parameters for Mgenerating using the original data set Doriginal . The parametric form of the conditional distributions was assumed to be Gaussian.\n• We used the learned modelMgenerating with the learned parameters to generate a set of 10 synthetic data sets, Disynthetic , i = {1, . . . , 10} with 50000 data points each4.\n• Finally, we ran the PC algorithm on the synthetic data using varying values for the significance level and the strength of effect:\nα = {0.001, 0.01, 0.05, 0.1} soe = {0, 0.05, 0.1}\n4To learn the parameters of the Bayesian network and generate data from the model, we used the bnlearn package for R [14].\nThe process we followed to generate synthetic data assumes that the features follow a Gaussian distribution. We evaluated this assumption by calculating the skewness and kurtosis for each feature distribution. Out of the 50 medoids we have extracted, we observed 26 features to have skewness and kurtosis values within the range of -2 to +2. This empirically indicates that 52% of the features are within an acceptable range that indicates normality. We compared the distribution of the generated features to that of the actual features, in order to demonstrate that the synthetic data we produce resemble the real data. Figure 7 depicts the distribution of the actual feature compared to that of the generated features for three features.\n5.2.2 Results on synthetic data To evaluate the performance of PC on synthetic data we\nused precision and recall after Phase I (undirected model) and after Phase II (partially directed model).\nUndirectedPrecision = # of true learned edges\n# of learned edges\nUndirectedRecall = # of true learned edges\n# of true edges\nDirectedPrecision = # of true learned oriented edges\n# of learned oriented edges\nDirectedRecall = # of true learned oriented edges\n# of true oriented edges\nThe average precision and recall values across the 10 synthetic data sets after Phase I are shown in Figure 8. The algorithm has almost perfect recall (it learns all the true edges). The precision of the algorithm drops as the significance level of the test increases. This is to be expected, because higher α values result in a denser graph, therefore, more edges will be incorrectly included in the output. The results after Phase II are presented in Figure 9. As the alpha values increases, the recall increases and the algorithm retrieves more than 50% of the true directions. However, the precision drops, suggesting that it is making more mistakes in the orientations. This can be explained because the learned model includes more spurious (directed) edges."
    }, {
      "heading" : "6. RELATED WORK",
      "text" : "In the area of causal discovery, the constraint-based algorithms we focused on retrieve models up to the Markov equivalence class (and thus might contain undirected edges). The authors of [20, 19] describe principled ways to choose a specific model from the Markov equivalence class. Moreover, apart from the structure learning algorithms that leverage conditional independence, there exist other techniques to retrieve causal relationships from observational data. In the past few years, a group of methods based on additive noise models (ANMs) has been proposed [16, 4, 13, 12]. ANMs leverage properties of the joint distribution other than conditional independence. In short, it has been shown that if\nthe observational distribution can be modeled as a structural equation model with an additive noise structure, then, under certain conditions, the directionality of all edges becomes identifiable.\nAnother line of related work focuses on learning graphical models for groupings of variables. Segal et al. [15] introduce module networks which construct groups of variables with “similar behaviour” (i.e., variables that share the same set of parents and the same parameters), called modules. They also present an algorithm to learn a dependency structure between modules from data. Moreover, recent work by Parviainen et al. [10] discusses the use of grouped variables for the specific task of causal discovery.\nRegarding the application of causal discovery techniques and Bayesian networks on domains similar to the manufacturing domain, Verron et al. [22, 23] use variations of a naive Bayes classifier (simple naive Bayes, tree-augmented Bayes classifier) to detect errors in manufacturing processes. Their task is however distinct than ours, as they aim to classify/predict if a part is faulty, as opposed to building a joint causal model of the domain. More closely related is the work of Li et al. [6], that applied causal discovery techniques for process control data. The above paper focuses on domain expertise for feature selection. Moreover, the authors discretize their data and use domain knowledge to limit the number of states for every discrete variable. In our work, we use statistical methods for feature selection/construction from the abundant raw features. Finally, Nannapaneni et al. [9] use Bayesian networks for uncertainty quantification for manufacturing domains. However, they are using scorebased methods for construction of Bayesian networks from data."
    }, {
      "heading" : "7. CONCLUSIONS AND FUTURE WORK",
      "text" : "In this work, we apply causal structure learning techniques to the field of manufacturing. We highlight the challenges and opportunities presented by manufacturing data and show how causal algorithms can be adapted to such data. This implies a large feature space and the variables\nmeasured might be highly correlated. These characteristics make the task of causal discovery challenging. On the other hand, there exists a large amount of domain knowledge, including both temporal and physical properties of the assembly line as well as knowledge from domain experts, that can be leveraged to improve the results. We presented results from the application of the PC algorithm on real data produced by manufacturing lines. We incorporated prior knowledge and clustered the feature space to improve the precision of the model. The structures learned by these models have been partially evaluated by the domain experts and thus can be used to identify influential factors effecting production yield. We have also explored the behavior of the algorithm on synthetic data generated to be similar to the original data. We show that, in the absence of a ground truth, we can use the synthetic data to evaluate the accuracy of different algorithms for learning causal structure.\nData produced by manufacturing lines provide many opportunities for future work. One potential direction is to verify some of the learned causal relationships through experimentation in the production line. This intervention in assembly-line operations to determine if the observed changes match those predicted by the model. The cost associated with this approach is potentially very high. The assembly line will have to stop for interventions to take place and only a few causal relationships could be tested in this manner. However, it would provide very strong evidence about the validity of the causal relationships that have been learned from data. Another direction for future work is to use information-theoretic measures of dependence instead of linear correlation (for example, mutual information on discretized data) and more advanced tests of conditional independence (such as the kernel-based conditional independence test [26]). This will allow the models to identify nonlinear relationships in the data. Finally, the partial ordering of variables that arises naturally in manufacturing domains provides an ideal setting for application of top-down (or recursive) causal discovery algorithms [25, 24, 1].\n8. REFERENCES\n[1] R. Cai, Z. Zhang, and Z. Hao. Sada: A general framework to support robust causation discovery. In Proceedings of the 30th international conference on machine learning, pages 208–216, 2013.\n[2] D. M. Chickering. Optimal structure identification with greedy search. The Journal of Machine Learning Research, 3:507–554, 2003.\n[3] S. R. A. Fisher, R. A. Fisher, S. Genetiker, R. A. Fisher, S. Genetician, R. A. Fisher, and S. Généticien. The design of experiments. Oliver and Boyd Edinburgh, 1960.\n[4] P. O. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Schölkopf. Nonlinear causal discovery with additive noise models. In Advances in neural information processing systems, pages 689–696, 2009.\n[5] M. Kalisch, M. Mächler, D. Colombo, M. H. Maathuis, and P. Bühlmann. Causal inference using graphical models with the R package pcalg. Journal of Statistical Software, 47(11):1–26, 2012.\n[6] J. Li and J. Shi. Knowledge discovery from observational data for process control using causal Bayesian networks. IIE Transactions, 39(6):681–690, June 2007.\n[7] D. Margaritis. Learning Bayesian network model structure from data. PhD thesis, US Army, 2003.\n[8] C. Meek. Causal inference and causal explanation with background knowledge. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, pages 403–410. Morgan Kaufmann Publishers Inc., 1995.\n[9] S. Nannapaneni, S. Mahadevan, D. Lechevalier, A. Narayanan, and S. Rachuri. Automated uncertainty quantification analysis using a system model and data. In Big Data (Big Data), 2015 IEEE International Conference on, pages 1408–1417. IEEE, 2015.\n[10] P. Parviainen and S. Kaski. Bayesian networks for variable groups. arXiv preprint arXiv:1508.07753, 2015.\n[11] J. Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, New York, NY, USA, 2nd edition, 2009.\n[12] J. Peters, D. Janzing, and B. Schölkopf. Causal inference on discrete data using additive noise models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(12):2436–2450, 2011.\n[13] J. Peters, J. M. Mooij, D. Janzing, and B. Schölkopf. Identifiability of causal graphs using functional models. In F. G. Cozman and A. Pfeffer, editors, Proceedings of the 27th Annual Conference on Uncertainty in Artificial Intelligence, pages 589–598. AUAI Press, 2011.\n[14] M. Scutari. Learning bayesian networks with the bnlearn R package. Journal of Statistical Software, 35(3):1–22, 2010.\n[15] E. Segal, D. Pe’er, A. Regev, D. Koller, and N. Friedman. Learning module networks. Journal of Machine Learning Research, 6:557–588, April 2005.\n[16] S. Shimizu, P. O. Hoyer, A. Hyvärinen, and A. Kerminen. A linear non-gaussian acyclic model for causal discovery. The Journal of Machine Learning\nResearch, 7:2003–2030, 2006.\n[17] P. Spirtes. Introduction to causal inference. J. Mach. Learn. Res., 11:1643–1662, Aug. 2010.\n[18] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction and Search. MIT Press, Cambridge, MA, 2nd edition, 2000.\n[19] X. Sun, D. Janzing, and B. Schölkopf. Causal inference by choosing graphs with most plausible markov kernels. In International Symposium on Artificial Intelligence and Mathematics, 2006.\n[20] X. Sun, D. Janzing, and B. Schölkopf. Distinguishing between cause and effect via kernel-based complexity measures for conditional probability densities. Neurocomputing, pages 1248–1256, 2008.\n[21] I. Tsamardinos, L. E. Brown, and C. F. Aliferis. The max-min hill-climbing bayesian network structure learning algorithm. Machine Learning, 65(1):31–78, 2006.\n[22] S. Verron, T. Tiplica, and A. Kobi. Fault detection with bayesian network. In Frontiers in Robotics, Automation and Control, pages 341–356. InTech Education and Publishing, 2008.\n[23] S. Verron, T. Tiplica, and A. Kobi. Monitoring of complex processes with bayesian networks. In Bayesian Networks. InTech Education and Publishing, 2010.\n[24] X. Xie and Z. Geng. A recursive method for structural learning of directed acyclic graphs. The Journal of Machine Learning Research, 9:459–483, 2008.\n[25] R. Yehezkel and B. Lerner. Bayesian network structure learning by recursive autonomy identification. The Journal of Machine Learning Research, 10:1527–1570, 2009.\n[26] K. Zhang, J. Peters, D. Janzing, and B. Schölkopf. Kernel-based conditional independence test and application in causal discovery. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 804–813, 2011."
    } ],
    "references" : [ {
      "title" : "Sada: A general framework to support robust causation discovery",
      "author" : [ "R. Cai", "Z. Zhang", "Z. Hao" ],
      "venue" : "In Proceedings of the 30th international conference on machine learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Optimal structure identification with greedy search",
      "author" : [ "D.M. Chickering" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Généticien. The design of experiments",
      "author" : [ "S.R.A. Fisher", "R.A. Fisher", "S. Genetiker", "S. Genetician" ],
      "venue" : "Oliver and Boyd Edinburgh,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1960
    }, {
      "title" : "Nonlinear causal discovery with additive noise models",
      "author" : [ "P.O. Hoyer", "D. Janzing", "J.M. Mooij", "J. Peters", "B. Schölkopf" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Causal inference using graphical models with the R package pcalg",
      "author" : [ "M. Kalisch", "M. Mächler", "D. Colombo", "M.H. Maathuis", "P. Bühlmann" ],
      "venue" : "Journal of Statistical Software,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Knowledge discovery from observational data for process control using causal Bayesian networks",
      "author" : [ "J. Li", "J. Shi" ],
      "venue" : "IIE Transactions,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Learning Bayesian network model structure from data",
      "author" : [ "D. Margaritis" ],
      "venue" : "PhD thesis, US Army,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Causal inference and causal explanation with background knowledge",
      "author" : [ "C. Meek" ],
      "venue" : "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1995
    }, {
      "title" : "Automated uncertainty quantification analysis using a system model and data",
      "author" : [ "S. Nannapaneni", "S. Mahadevan", "D. Lechevalier", "A. Narayanan", "S. Rachuri" ],
      "venue" : "In Big Data (Big Data),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Bayesian networks for variable groups",
      "author" : [ "P. Parviainen", "S. Kaski" ],
      "venue" : "arXiv preprint arXiv:1508.07753,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Causality: Models, Reasoning and Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Causal inference on discrete data using additive noise models",
      "author" : [ "J. Peters", "D. Janzing", "B. Schölkopf" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Identifiability of causal graphs using functional models",
      "author" : [ "J. Peters", "J.M. Mooij", "D. Janzing", "B. Schölkopf" ],
      "venue" : "Proceedings of the 27th Annual Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Learning bayesian networks with the bnlearn R package",
      "author" : [ "M. Scutari" ],
      "venue" : "Journal of Statistical Software,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Learning module networks",
      "author" : [ "E. Segal", "D. Pe’er", "A. Regev", "D. Koller", "N. Friedman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "A linear non-gaussian acyclic model for causal discovery",
      "author" : [ "S. Shimizu", "P.O. Hoyer", "A. Hyvärinen", "A. Kerminen" ],
      "venue" : "The Journal of Machine Learning  Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "Introduction to causal inference",
      "author" : [ "P. Spirtes" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Causation, Prediction and Search",
      "author" : [ "P. Spirtes", "C. Glymour", "R. Scheines" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "Causal inference by choosing graphs with most plausible markov kernels",
      "author" : [ "X. Sun", "D. Janzing", "B. Schölkopf" ],
      "venue" : "In International Symposium on Artificial Intelligence and Mathematics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Distinguishing between cause and effect via kernel-based complexity measures for conditional probability densities",
      "author" : [ "X. Sun", "D. Janzing", "B. Schölkopf" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "The max-min hill-climbing bayesian network structure learning algorithm",
      "author" : [ "I. Tsamardinos", "L.E. Brown", "C.F. Aliferis" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Fault detection with bayesian network. In Frontiers in Robotics, Automation and Control, pages 341–356",
      "author" : [ "S. Verron", "T. Tiplica", "A. Kobi" ],
      "venue" : "InTech Education and Publishing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Monitoring of complex processes with bayesian networks",
      "author" : [ "S. Verron", "T. Tiplica", "A. Kobi" ],
      "venue" : "In Bayesian Networks. InTech Education and Publishing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "A recursive method for structural learning of directed acyclic graphs",
      "author" : [ "X. Xie", "Z. Geng" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2008
    }, {
      "title" : "Bayesian network structure learning by recursive autonomy identification",
      "author" : [ "R. Yehezkel", "B. Lerner" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Kernel-based conditional independence test and application in causal discovery",
      "author" : [ "K. Zhang", "J. Peters", "D. Janzing", "B. Schölkopf" ],
      "venue" : "In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "The traditional approach to address this issue is through Design of Experiments (DoE) [3].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "In this work, we focus on the semantics of causality based on probabilistic distributions and manipulations, following the work of Pearl [11] and Spirtes et al.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "[18].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "In this framework, X is a direct cause of Y with respect to a set of variables Z if changing the value of X results in changes in the probability distribution of Y , assuming that the values of all other variables in Z are held constant [17].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 17,
      "context" : "Algorithms in this category include PC [18], FCI [18], and Grow Shrink [7].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "Algorithms in this category include PC [18], FCI [18], and Grow Shrink [7].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "Algorithms in this category include PC [18], FCI [18], and Grow Shrink [7].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "It is worth mentioning Greedy Equivalence Search [2], a score-based approach that searches",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "An example of a hybrid algorithm is MMHC [21].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "In this work, we focus on constraint-based algorithms, and we specifically focus on the PC algorithm [18].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "After all colliders have been oriented, a set of four orientation rules is applied repetitively until no changes can be made [8].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "For the PC algorithm, we used the implementation provided in the pcalg package [5] (with appropriate modifications, as explained below).",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "To learn the parameters of the Bayesian network and generate data from the model, we used the bnlearn package for R [14].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "The authors of [20, 19] describe principled ways to choose a specific model from the Markov equivalence class.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "The authors of [20, 19] describe principled ways to choose a specific model from the Markov equivalence class.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 15,
      "context" : "In the past few years, a group of methods based on additive noise models (ANMs) has been proposed [16, 4, 13, 12].",
      "startOffset" : 98,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "In the past few years, a group of methods based on additive noise models (ANMs) has been proposed [16, 4, 13, 12].",
      "startOffset" : 98,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "In the past few years, a group of methods based on additive noise models (ANMs) has been proposed [16, 4, 13, 12].",
      "startOffset" : 98,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "In the past few years, a group of methods based on additive noise models (ANMs) has been proposed [16, 4, 13, 12].",
      "startOffset" : 98,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "[15] introduce module networks which construct groups of variables with “similar behaviour” (i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[10] discusses the use of grouped variables for the specific task of causal discovery.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22, 23] use variations of a naive Bayes classifier (simple naive Bayes, tree-augmented Bayes classifier) to detect errors in manufacturing processes.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "[22, 23] use variations of a naive Bayes classifier (simple naive Bayes, tree-augmented Bayes classifier) to detect errors in manufacturing processes.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "[6], that applied causal discovery techniques for process control data.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] use Bayesian networks for uncertainty quantification for manufacturing domains.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 25,
      "context" : "Another direction for future work is to use information-theoretic measures of dependence instead of linear correlation (for example, mutual information on discretized data) and more advanced tests of conditional independence (such as the kernel-based conditional independence test [26]).",
      "startOffset" : 281,
      "endOffset" : 285
    }, {
      "referenceID" : 24,
      "context" : "Finally, the partial ordering of variables that arises naturally in manufacturing domains provides an ideal setting for application of top-down (or recursive) causal discovery algorithms [25, 24, 1].",
      "startOffset" : 187,
      "endOffset" : 198
    }, {
      "referenceID" : 23,
      "context" : "Finally, the partial ordering of variables that arises naturally in manufacturing domains provides an ideal setting for application of top-down (or recursive) causal discovery algorithms [25, 24, 1].",
      "startOffset" : 187,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : "Finally, the partial ordering of variables that arises naturally in manufacturing domains provides an ideal setting for application of top-down (or recursive) causal discovery algorithms [25, 24, 1].",
      "startOffset" : 187,
      "endOffset" : 198
    } ],
    "year" : 2016,
    "abstractText" : "Increasing yield and improving quality are of paramount importance to any manufacturing company. One of the ways to achieve this is through discovery of the causal factors that affect these quantities. In this work, we use data-driven causal models to identify causal relationships in manufacturing. Specifically, we apply causal structure learning techniques on real data collected from a production line. Emphasis is given to the interpretability of the learned causal models, so that they can be used by practitioners to take meaningful actions. We highlight the challenges presented by assemblyline data and propose ways to address those challenges. We also identify unique characteristics of data originating from assembly lines and how to leverage those characteristics to improve causal discovery. Standard evaluation techniques for causal structure learning show that the learned models closely match the underlying causal relationships between different factors in the production process. These results were also validated by manufacturing domain experts, who found them promising. This work demonstrates how data mining and knowledge discovery can be used for root cause analysis in the domain of manufacturing and connected industry.",
    "creator" : "LaTeX with hyperref package"
  }
}