{
  "name" : "1705.03800.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Hybrid Isolation Forest - Application to Intrusion Detection",
    "authors" : [ "PIERRE-FRANÇCOIS MARTEAU" ],
    "emails" : [ "permissions@acm.org." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Hybrid Isolation Forest - Application to Intrusion Detection\nPIERRE-FRANÇCOIS MARTEAU, IRISA, CNRS, Universite Bretagne Sud\nSAEID SOHEILY-KHAH, IRISA, CNRS, Universite Bretagne Sud\nNICOLAS BÉCHET, IRISA, CNRS, Universite Bretagne Sud\nFrom the identification of a drawback in the Isolation Forest (IF) algorithm that limits its use in the scope of anomaly detection, we propose two extensions that allow to firstly overcome the previously mention limitation and secondly to provide it with some supervised learning capability. The resulting Hybrid Isolation Forest (HIF) that we propose is first evaluated on a synthetic dataset to analyze the effect of the new meta-parameters that are introduced and verify that the addressed limitation of the IF algorithm is effectively overcame. We hen compare the two algorithms on the ISCX benchmark dataset, in the context of a network intrusion detection application. Our experiments show that HIF outperforms IF, but also challenges the 1-class and 2-classes SVM baselines with computational efficiency.\nAdditional Key Words and Phrases: Isolation Forest, Machine Learning, Semi-supervised Learning, Anomaly Detection, Intrusion\nDetection\nACM Reference format: Pierre-Françcois Marteau, Saeid Soheily-Khah, and Nicolas Béchet. 2017. Hybrid Isolation Forest - Application to Intrusion Detection. 1, 1, Article 1 (May 2017), 24 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Anomaly detection has been a hot topic for several decades and has led to numerous applications in a wide range of domains, such as fault tolerance in industry, crisis detection in finance and economy, health diagnosis, extreme phenomena in earth science and meteorology, atypical celestial object detection in astronomy or astrophysics, system intrusion in cyber-security, etc.\nAnomaly detection is generally defined as the problem of identifying patterns that deviates from a ’normality’ behavioral model. In the literature, most approaches can be categorized either according to the model of normality that is involved or to the way they address the abnormality characterization and its identification. A quite exhaustive, although a bit old, review in anomaly detection has been proposed in [8], completed by a more recent comparative study [14]. According to these studies, the state of the art methods can be distributed into five main categories:\n(1) Near neighbors and clustering based methods [4]: Near Neighbors methods rely on the assumption that a ’normal’ instance occurs close to its near neighbors while an anomaly occurs far from its near neighbors.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2017 Association for Computing Machinery. Manuscript submitted to ACM\nManuscript submitted to ACM 1\nar X\niv :1\n70 5.\n03 80\n0v 1\n[ cs\n.L G\n] 1\n0 M\nay 2\n01 7\nSimilarly, cluster based methods rely on the assumption that a ’normal’ instance occurs near its closest cluster centroid while an anomaly will occur far from its nearest cluster centroid [5, 20]. (2) Classification based method: in this paradigm, several classes of ’normal’ data are learned by a set of one against all classifiers (each classifier is associated to a class and is trained to separate it from the others classes). An instance that is not categorized as ’normal’ by any of these classifiers is considered as an anomaly. A peculiar case occurs when a single class is used to model the ’normal’ data. Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection. (3) Statistical based methods rely on the assumption that ’normal’ data are associated to high probability states of an underlying stochastic process while anomalies are associated to low probability states of this process. Popular approaches in this category are kernel based density models and the Gaussian Mixture Model (GMM), including recent advance in one-class GMM [18], (4) Information theoretic based methods use information theoretic measures [19], such as the entropy, the Kolmogorof complexity, the minimum description length, etc, to estimate the ’complexity’ of the ’normal’ dataset (or equivalently the complexity of the process behind the production of these data) [15]. If C(D) estimates the complexity of dataset D, the minimal subset of instance I that maximizes C(D) − C(I ) is considered as the anomaly subset. (5) Spectral based method rely on the assumption that it is possible to embed the data into a lower dimensional subspace in which ’normal’ data and anomalies are supposedly well separated [2]. PCA and graph (of similarity) clustering are among the most popular methods in this category.\nA recent review [1], though much less detailed, identifies a supplemental category that stands in between supervised and unsupervised methods, and that authors refer to as ’hybrid methods’. Such approaches combine supervised, semi-supervised and unsupervised techniques.\nIn 2008, Isolation Forest (IF) [22], a quite conceptually different approach to the previously referenced methods, has been proposed that went strangely below the radar of the previous review. The IF paradigm is based on the difficulty to isolate a particular instance inside the whole set of instances when using (random) tree structures. It relies on the assumption that an anomaly is in general much easier to isolate than a ’normal’ data instance. Hence, IF is an unsupervised approach that relates somehow to the information theoretic based methods since the isolation difficulty is addressed through an algorithmic complexity scheme. IF has been successfully used in some applications, [23], [11] and extended recently in [28] to improve the selection of attributes and their split values when constructing the tree. The main advantage of IF based algorithms is their capability to process large amount of data in high dimension spaces with computational and spatial controlled efficiency compared to other unsupervised methods. Unfortunately, as we shall see in the next section, IF suffers from what we call \"blind spots\", namely empty portions of the space in which data instances are embedded, that are nevertheless considered as normality spots by the IF algorithm.\nApart from identifying this drawback, the aim of this paper is to propose, firstly, a semi-supervised extension of the IF algorithm to overcome the ’blind spot’ limitation, and secondly a supervised extension in order to benefit from known anomalies if any. The Hybrid Isolation Forest (HIF) is thus a hybrid method that takes advantage of the unsupervised paradigm of IF while correcting a weakness that could have very damaging effects, and furthermore benefits from a supervised learning capability to enhance the effectiveness of the anomaly detection.\nManuscript submitted to ACM\nWe detail the IF algorithm in the second section of this paper, and give some highlights about the occurrence of the so-called ’blind spots’ by using a synthetic dataset. The third section presents the extension of the IF algorithm that we propose and shows, on the previous synthetic dataset, how this extension can be used to get rid of blind spots. The supervised functionality that we add to the IF is also described by the end of this section. The fourth section addresses an application in the domain of intrusion detection that assesses in a close to real-life situation the benefits brought by the HIF algorithm. Our results show that the proposed HIF algorithm compares advantageously with the state of the art baselines in anomaly detection that we have considered, namely one-class and two-classes SVM."
    }, {
      "heading" : "2 ISOLATION FOREST AND ITS ’BLIND SPOT’",
      "text" : "The simple idea behind the isolation forest approach, is that it is (in general) much simpler to isolate an ’outlier’ from the rest of the data than to isolate an ’inlier’ from the rest of the data.\nThis leads, in the context of a binary tree partitioning algorithm, to expect a shorter path to locate an ’outlier’ and a longer path to locate an ’inlier’. This is exemplified in Fig.1, which shows that, for a 2D normally distributed dataset, more separating lines are needed to separate the ’inlier’ xi from the rest of the data comparatively to the number of separating lines needed to isolate the ’outlier’ x0."
    }, {
      "heading" : "2.1 The Isolation Forest algorithm",
      "text" : "We reproduce hereinafter the description of the isolation forest algorithm as presented in [22].\n2.1.1 Building the isolation forest: Let X ⊂ Rd be the set of instances. The IF algorithm is an ensemble based approach that builds a forest of random binary trees. Given a sample S randomly drawn from X , an isolation tree iT (S) is recursively built according to the (iTree) algorithm 1:\nManuscript submitted to ACM\nAlgorithm 1: Function iTree(S, l , lmax ) Input :S ⊂ X , l the current depth level, lmax the maximal depth limit Output :an iTree\n1 if l ≥ lmax or |S | ≤ 1 then 2 return exNode(S) 3 end 4 else 5 randomly select a dimension q ∈ {1, · · · ,n} 6 randomly select a split value p between max and min values along dimension q in S 7 Sl ← filter(S,q < p) 8 Sr ← filter(S,q ≥ p) 9 return inNode(Left← iTree(Sl , l + 1, lmax ), 10 Right← iTree(Sr , l + 1, lmax ), 11 splitDim← q, 12 splitVal← p) 13 end\nTwo meta-parameters are required to build an isolation forest:ψ , the size of the subsets S that are used to build the trees, and t , the number of trees. Parameter lmax , the maximum height of the trees, is empirically set up to ⌈loд2ψ ⌉.\nFinally, the isolation forest iF = {iT (S1), iT (S2), · · · , iT (St )} is obtained by randomly selecting {S1, S2, · · · , St }, t samples in X with |Si | = ψ for all i , and constructing an iTree on each of these samples, as depicted in Algorithm 1.\n2.1.2 Constructing the anomaly score: The anomaly score for the isolation forest is constructed from the analysis of unsuccessful search in a Binary Search Tree (BST). The path length h(x) for data point x is defined as the number of edges x that traverses an iTree from the root node until the traversal is terminated at an external node. For a data set of n instances, Section 10.3.3 of [25] gives the average path length of unsuccessful search in BST as:\nc(n) = 2H (n − 1) − (2(n − 1)/n) (1)\nwhere H (i) is the harmonic number and that can be estimated by loд(i) + 0.5772156649 (Euler’s constant). As c(n) is the average of h(x) given n, we use it to normalize h(x). The anomaly score s of an instance x is defined as:\ns(x ,n) = 2− E(h(x )) c (n) (2)\nwhere E(h(x)) is the mean of h(x) taken over a collection of isolation trees. One can easily establish that:\n• when E(h(x)) → c(n), s → 0.5; • when E(h(x)) → 0, s → 1; • and when E(h(x)) → n − 1, with n large, s → 0.\nFinally, once the isolation forest is constructed, given any x ∈ Rd , an anomaly score for x , s(x), is calculated according to Eq. 2, where E(h(x)) is evaluated on the set of iTrees of the forest.\nManuscript submitted to ACM\nThe Algorithm 2 presents the recursive evaluation of h(x) given x and an iTree, e , the current path length, being initialized with 0.\nAlgorithm 2: Function PathLength(x ,T , e) Input :x an instance, T an iTree, e the current path length; to be initialized to zero when first called Output :h(x), path length for x\n1 if T is an external node then 2 return e + c(T .size) (c(.) is defined in Equation (1)) 3 end 4 a ← T .splitAtt 5 if x[a] < T .splitValue then 6 return PathLength(x ,T .le f t , e + 1) 7 end 8 else 9 return PathLength(x ,T .riдht , e + 1) 10 end"
    }, {
      "heading" : "2.2 Existence of ’blind spots’ in IF based anomaly detection",
      "text" : "The assumption behind the IF algorithm is that anomalies will be associated to short paths in the iTrees, leading to a high anomaly score s , while ’normal’ data will be associated to longer paths, leading to a low anomaly score s . Unfortunately, if this is true for normally distributed data for instance, this is not true in general. In particular, this assumption is not verified for data distributed in a concave set such as a tore or a set with a ’horse shoe’ shape. To demonstrate this, we develop the following test based on synthetic data.\n2.2.1 Synthetic experiment. For this setting, ’normal’ data belongs to a 2D-tore centered in (0, 0) and delimited by two concentric circles whose radius are respectively 1.5 and 4. A training (Xn ) and a testing (Xnt ) sets of normal data are uniformly drawn from this 2D-tore, each one containing n = 1000 instances, as depicted in Fig. 2 (a).\nA first ’anomaly’ set (Xr ) is drawn from a Normal distribution with mean (3., 3.) and covariance ((.25, 0), (0, .25)), as depicted in red square dots in 2 (b). These anomalies intersect the 2D-tore at its top right side.\nA second ’anomaly’ set (Xд ) is drawn from a Normal distribution with mean (0., 0.) and covariance ((.5, 0), (0, .5)), as depicted in green diamond dots in 2 (c). These anomalies are located at the center of the 2D-tore.\nThen we build the IF (withψ = 64 and t = 512) from the ’normal’ dataset Xn and evaluate the distributions of the anomaly scores obtained for the ’normal’ ’blue’ test dataset Xnt , the ’red’ anomalies Xr and the ’green’ anomalies Xд . Fig.3 presents the ’normal’ v.s. ’red’ anomalies (a) distributions, and with the addition of the green anomaly distribution (b).\nAt this point, we clearly show that the ’green’ anomaly distribution is in large intersection with the ’normal’ data distribution, which is not the case for the ’red’ anomaly distribution. Hence anomalies located at the center of the tore are likely to be much more mis-detected by the IF algorithm than anomalies located at the periphery of the tore.\nFig.4 presents the Receiver Operating Curve (ROC) for an anomaly detector based on the scores provided by an IF trained on ’normal’ data only. On this test, the Area Under the Curve measures (AUC) are respectively 0.925 and 0.441\nManuscript submitted to ACM\n(a)\n(b)\n(c)\n(a)\n(b)\nfor the ’red’ and ’green’ anomalies respectively. Hence, the IF does not perform better than a random classifier when it comes to separate ’green’ anomalies from ’normal’ data. This is what we call a ’blind spot’ effect.\nTo fix this mis-detection problem, several approaches can be carried out. The first one consists in finding a new embedding of the data in which ’blind spots’ are not present. For the 2D-tore experiment, one can easily replace the 2D Cartesian coordinate system by a 2D polar system and get rid of the ’blind spot’. However, such explicit embedding does not exists or is difficult to elicit in general. In the same line of approaches, Kernel based methods define an implicit embedding that allows for projecting the data in a higher dimensional space (possibly an infinite dimensional one) in Manuscript submitted to ACM\n(a)\n(b)\nwhich the hope is that the tackled problem becomes (more) linearly separable. The kernel PCA [27] is a good example of such method. The difficulty is that the implicit embedding is defined through the so-called kernel gram matrix, that can be very hard to compute in the context of large volume of data.\nAs ’blind spots’ are obviously associated to some correlation into the variables that describe the data, the second avenue to avoid them is to find a transformation so that the new variables are statistically independent, or as independent as possible. Unfortunately, if linear correlation is easily removable, nonlinear correlation is much more difficult to remove. This can be feasible to some extend and in some framework, such as blind source separation, though it requires some knowledge about the data, such as the number of sources [17]. For problems expressed in high dimension it is often possible to reduce the dimension using an appropriate embedding that allows for describing the data with variables that are weakly correlated, but in general at the cost of losing information (the embedding is not invertible) [REFs] [6, 7, 16, 24] ."
    }, {
      "heading" : "3 HYBRID ISOLATION FOREST",
      "text" : "Complementary to the previously mentioned approaches, we propose to overcome the ’blind spot’ drawback of IF by adding, directly into the IF framework itself, two simple sources of information that will allow for designing a more sophisticated anomaly score:\n(1) the first one is an unsupervised extension that exploits a distance knowledge to neighboring ’normal’ data, (2) the second one is a supervised-based extension that exploits a distance knowledge to neighboring ’anomaly’\ndata."
    }, {
      "heading" : "3.1 Adding a distance-based score",
      "text" : "In the 2D-tore experiment, the ’green’ anomalies are undetected because they are located inside an area that is as difficult to ’isolate’ using a BST than ’normal’ data. However, most of these data are located at a greater distance to the\nManuscript submitted to ACM\n’normal’ data associated to the external node bucket found by the BST algorithm when evaluating the path length (see line 2 of Algorithm 2).\nHence, the simple idea we propose to overcome the ’blind spot’ effect consists in integrating this distance based information into the IF score as follows:\n• first the centroid of the data associated to each external node of the hiTree is evaluated when building the hiTree (training phase). This corresponds to lines 2-5 in Algorithm 3. • for each tested data, its distance to the centroid of the external node found by the BST algorithm is evaluated and reported as a second score element. This corresponds to line 3 in Algorithm 6.\nThe final score sc (x) provided at the forest level is simply the expectation of the δ (x) scores evaluated on each of the iTrees of the forest.\nsc (x) = E(δ (x))) (3)\nAlgorithm 3: Function hiTree(S, SLab, l , lmax ) Input :S ⊂ X , l the current depth level, lmax the maximal depth limit Output :an hiTree\n1 if l ≥ lmax or |S | ≤ 1 then 2 CS=None 3 if |S | ≥ 0 then 4 CS=Centroid(S) 5 end 6 return exNode(S , SLab) 7 end 8 else 9 randomly select a dimension q ∈ {1, · · · ,n} 10 randomly select a split value p between max and min values along dimension q in S 11 Sl ← filter(S,q < p) 12 Sr ← filter(S,q ≥ p) 13 return inNode(Left← iTree(Sl , SLabl , l + 1, lmax ), 14 Right← iTree(Sr , SLabr , l + 1,max), 15 splitDim← q, 16 splitVal← p) 17 end"
    }, {
      "heading" : "3.2 Adding known anomalies",
      "text" : "In some situations, some anomalies may have been detected and documented. This is the case for any monitored environment in which incident are reported such as in network supervision and intrusion detection for instance. In such situations, incorporating this kind of expert knowledge into the IF framework can be a source for improvements for the detection of new anomalies. Furthermore, this could offer a way to better balance, according to the targeted application, the anomaly mis-detection rate and the false alarm rate.\nTo implement this supervised extension to the isolation forest algorithm, at the training phase, we develop a function, detailed in algorithm 4, that assigns a known (labeled) anomaly xa to the external nodes (the leaf of the iTree) that is Manuscript submitted to ACM\nAlgorithm 4: Function addAnomaly(x , xlab , T )\nInput :x ∈ Rd : an anomaly, xlab : a label attached to the anomaly, T : a node in an iTree Output :none\n1 if T is an external node then 2 append xlab to the list of anomaly labels attached to T (T .labels) 3 append x to list of anomalies attached to (T .Xa ) 4 end 5 else 6 a ← T .splitAtt 7 if x[a] < T .splitValue then 8 return addAnomaly(x , xlab , T .le f t ) 9 end 10 else 11 return addAnomaly(x , xlab , T .riдht ) 12 end 13 end\nreturned when searching the BST for xa . By the end of this process, each external node stores in a list of all its assigned anomalies and associated labels (line 2-3 of the algorithm).\nOnce all the anomalies have been introduced in the iTrees, to complete the training, for each external node we evaluate the centroid of the anomalies that have been attached to it, as depicted in algorithm 5.\nAt the testing phase, we finally evaluate a dedicated score δa (x) for each iTree as shown in line 4 of the scoring algorithm (Algorithm 6). Given the external node T of an iTree found when searching for the tested instance x , δa (x) corresponds to the distance between x and the centroid of the set of anomalies assigned to T (T .Xc ).\nThe final supervised score contribution sa (x) provided by the HIF is simply the ratio of the expectation of δ (x) (the distance of x to the local centroid of ’normal’ data, Eq. 3) over the expectation of δa (x), basically the ratio of the mean of the scores δa (x) over the mean of the scores δa (x) evaluated on each of the iTrees of the forest as stated in Eq. 4. If E(δa (x)) = 0, then we simply enforce sa (x) = 0.\nsa (x) = E(δ (x)) E(δa (x)) = MeaniT reeδ (x , iTree) MeaniT reeδa (x , iTree)\n(4)"
    }, {
      "heading" : "3.3 Aggregating the scores",
      "text" : "The IF score s(x ,n) for data x and dataset size n, given in Eq.(2) need to be aggregated with the two new scores sc (x) and sa (x). We adopt a simple aggregation in two steps:\n(1) first, all the three scores are normalized such as to fit into the unit interval [0; 1]. On the train data we evaluate for s(x) ∈ {s(x ,n), sc (x), sa (x)}\ns̃(x) = s(x) −min(s(x)) max(s(x)) −min(s(x)) (5)\n(2) then we introduce two meta-parameters α1 and α2 ∈ [0; 1] and evaluate the following linear model: shif (x ,n) = α2 · ( α1 · s̃(x ,n) + (1 − α1) · ˜sc (x) ) + (1 − α2) · ˜sa (x)) (6)\nWe discuss the selection or optimization of these meta parameters in section 3.5. Manuscript submitted to ACM\nAlgorithm 5: Function computeAnomalyCentroid(T ) Input :T : a node in an iTree Output :none\n1 if T is an external node then 2 Ca=None 3 if |T .Xa | ≥ 0 then 4 Ca=Centroid(T .Xa ) 5 end 6 end 7 else 8 computeAnomalyCentroid(T .le f t ) 9 computeAnomalyCentroid(T .riдht ) 10 end\nAlgorithm 6: Function hiScore(x ,T , e) Input :x an instance, T an hiTree, e the current path length; to be initialized to zero when first called Output :h(x), the path length for x , δ (x), the Euclidean distance between x and the centroid associated to the\nexternal node, and δa (x) the minimal Euclidean distance between x and the centroid of the anomalies attached to T , T .Xa\n1 if T is an external node then 2 h(x) = e + c(T .size) (c(.) is defined in Equation (1)) 3 δ (x) = EuclideanDistance(x ,T .CS ) 4 δa (x) = EuclideanDistance(x ,T .Ca ) 5 return h(x), δ (x), δa (x) 6 end 7 a ← T .splitAtt 8 if x[a] < T .splitValue then 9 return hiScore(x ,T .le f t , e + 1) 10 end 11 else 12 return hiScore(x ,T .riдht , e + 1) 13 end"
    }, {
      "heading" : "3.4 Back to the synthetic experiment",
      "text" : "We go back to the synthetic experiment to assess the impact of the new scoring involved in the HIF algorithm. To the previous setting (Fig.2) we add a third anomaly cluster constructed from the Normal distribution with mean (−3.,−3.) and covariance ((.25, 0), (0, .25)) (Xc , containing also 1000 anomalies) in cyan star dots at the bottom left of the 2D-tore, as shown in Fig. 5. Some labeled ’red’ anomalies are also represented in black dots in the figure.\nWe construct the HIF with the same meta parameter setting that we previously used for the construction of the IF, namelyψ = 64 and t = 512.\n3.4.1 Impact of the sc (x) score. : we consider at this stage that α2 = 1 in the HIF score, which leads to the reduced aggregation score given in Eq. 7. We refer to this configuration as the HIF1 algorithm. Hence none anomaly are added Manuscript submitted to ACM\nto the HIF yet. Figure 6 presents the AUC value as a function of α1 when all the test data are considered (’normal’ test data are mixed up with all kind of anomalies). An optimum AUC value can be found near α1 = .3.\nshif 1(x ,n) = α1 · s̃(x ,n) + (1 − α1) · ˜sc (x) (7)\nFig. 7 presents the distributions of the scores obtained by the IF algorithm (top) and the HIF1 algorithm (bottom). Clearly, the ’green’ anomaly cluster located at the center of the 2D-tore is much more separated for HIF1 than for IF. The ’red’ and ’cyan’ anomaly clusters distributions remain apparently well separated by the two algorithms from the ’normal’ data distribution in blue.\nThis first finding is confirmed by the ROC curves presented in Fig.12 obtained for the best α1 value for HIF1. If the ROC curves for the peripheral ’red’ and ’cyan’ anomaly clusters are close for the two algorithms and apparently rather good, for the ’green’ anomalies, the ROC curve for IF is very poor (IF will not perform better than a random classifier) while it is quite good for HIF1. Table 1, in which the global AUC values for the two algorithms are reported, supports this claim. In addition, Fig. 8 visualizes the variation of these AUC values when varying parameter α1 in [0; 1]. As\nManuscript submitted to ACM\n(a)\n(b)\n(c)\nexpected, when α1 increases, the AUC value for the ’green’ anomalies regularly decreases, while the AUC values for the ’red’ and ’cyan’ anomalies increases to reach a plateau when α1 > .4 and α1 > .7 respectively. This explains the optimum value for α1 that corresponds to a compromise between the two scores of different nature that are involved.\n3.4.2 Impact of the sa (x) score. We consider at this stage that α1 = 1 in the HIF score, which leads to the reduced aggregation score given in Eq. 8.\nshif 2(x ,n) = α2 · s̃(x ,n) + (1 − α2) · ˜sa (x) (8)\nThus, 5 (.05%) ’red’ labeled anomalies are now added into the HIF score. Manuscript submitted to ACM\nIt can be shown on Fig.9 that when α2 increases, the AUC value corresponding to the detection of the ’red’ anomalies slightly decreases while it increases from near .4 to .9 for the ’cyan’ anomalies: this is expected, because no ’cyan’ labeled anomalies have been added to the HIF. For the ’green’ anomalies, when α2 increases, the AUC value increases slightly with mean value near .5, basically the chance level. Note that for α2 = 1 the AUC values correspond to the IF scoring.\nConsidering the separation of the ’red’ anomalies from the ’normal’ test data, Fig. 10 gives the best AUC value (obtained when optimizing the meta parameters α1) as a function of the percentage of anomalies (compared to the normal train data) used to train the HIF. We observe that the addition of a single anomaly (.001%) is enough to increase the AUC from .9 to near .99. Indeed, to optimize in practice the meta parameters, we will require more labeled anomalies than a single one.\n3.4.3 Evaluating the complete HIF score. We consider here the full HIF score which corresponds to the aggregated scoring given in Equation 6. We refer to this configuration as the HIF2 algorithm.\nManuscript submitted to ACM\nThe 3D plot presented in Fig. 11 shows that a maximal AUC value can be obtained when optimizing the two meta parameters α1 and α2.\nTable 2 presents the mean and standard deviatoion of the AUC values for IF and HIF2 evaluated on ten independent tests (for which the ’normal’ and anomaly data are randomly drawn). Manuscript submitted to ACM\nA mean optimal AUC value is obtained for α1 ≈ .2 and α2 ≈ .7which means that all the individual scores s(x ,n), sc (x) and sa (x) play a role. This shows that when some anomalies can be used for training, a simple grid search optimization process can easily be implemented to set up the HIF2 meta parameters α1 and α2.\nThe final global AUC values obtained when all the test data (’normal’ test, ’red’, ’green’ and ’cyan’ anomalies) are evaluated in a single test are .793 for the IF algorithm, .910 for the HIF1 algorithm when no labeled anomalies are added, and .928 when 0.5% of the red anomalies are added into the HIF2.\nAs a preliminary conclusion, according to this synthetic experiment, we can state that the ’blind spot’ drawback of the IF algorithm has been apparently correctly solved by introducing a distance-to-normality based paradigm into the scoring of the algorithm. Furthermore, adding some few labeled anomalies into the HIF structure provides a supervised complementary source of potential improvements.\n3.4.4 Comparison with the one-class and two-classes SVM. In order to assess the IF andHIF (HIF1 andHIF2) algorithms comparatively to the state of the art in anomaly detection, we consider here the one-class Support Vector Machine (1C-SVM), and the supervised two-classes Support Vector Machine (2C-SVM) as two baselines. We constructed 15 random instances of the synthetic dataset. We have selected for the 1C-SVM and the 2C-SVM the radial basis kernel that is very well adapted for separating such kind of non linearly separable uniformly and normally distributed data. On each test, we optimized the (ν and γ ) meta parameters of the SVM such as maximizing the AUC value of the ROC curve obtained when using the distance to the separating hyper-plane as the classification score for the SVM. The same number of randomly drawn 5 ’red’ anomalies have been used to train the HIF2 and the 2-classes SVM. IF and HIF structures are built usingψ = 64 and t = 2048. Similarly to the SVM, we optimize for HIF1 and HIF2 α1 and α2 meta parameters such as maximizing the AUC value that is reported in the following tables.\nTable 3 presents the mean AUC values and associated standard deviation obtained for each algorithm. Table 4 gives the p-values of the Wilcoxon signed rank tests carried out to evaluate the significance of the pairwise AUC differences for each pair of classifiers. This experiment shows that the 1C-SVM and the HIF1 algorithms perform very comparatively since their pairwise differences are not significant. On the other hand the supervised functionality of HIF2, when adding randomly .5% of labeled anomalies (namely 5 among 1000 samples), brings a slight improvement compared to the HIF1 algorithm and the 1C-SVM (although the difference between HIF2 and 1C-SVM seems to be not significant here). Nonetheless, the 2C-SVM performs significantly worse than the 1C-SVM and HIF1 and HIF2. According to Fig. 12, the 2C-SVM, while learning the separation frontier between the normal and red anomaly data, looses the ability to identify outliers in the center of the tore.\n3.4.5 Dependence to the sample size (ψ ) in each tree and to the number of trees (t ) in the forest. : The dependence of the AUC values to the other meta parameter settings, namely the number of iTrees and the sample size assigned to each iTree, is presented in Fig 13.\n(a)\n(b)\nFor this experiment that is characterized with a dataset size of n = 1000 samples used to train the isolation forest, one can see that the HIF reaches good and relatively stable AUC values with fewer trees ((a) sub-figure) and lower sample size ((b) sub-figure) which is quite advantageous in terms of memory space and response time. Manuscript submitted to ACM"
    }, {
      "heading" : "3.5 Setting up meta parameters α1 and α2",
      "text" : "We consider here that ’normal’ data are available for training.\n3.5.1 No labeled anomaly is available. : in this case we only need to setup α1. Figures 6 and 8 shows that a default value for α1 can be setup in [.2; .5]. If numerous unlabeled anomalies exist in the test data as well as ’normal’ test data, then an exploratory analysis of the distribution of the HIF1 score may be used to isolate anomaly modes from the ’normal’ distribution. By varying α1, the separation of the distribution modes may vary and the seek for an ’optimal’ separation may be carried out.\nAs an example, Fig. 14 shows, for α1 = .4 the distribution HIF1 scores for ’normal’ train data distribution (in blue) and the test data (in red) in which ’normal’ and ’red’ anomalies are mixed up. Clearly, the tail of the test data distribution is much longer towards the high scores than for the train data. This is obviously cues for anomaly identification. Hence, one can sample few data with high score and expertise them to decide whether they correspond to some kind of anomalies, and doing so, we may be able to provide some labeled anomaly data. This is the basis for designing an some active learning process.\n3.5.2 Few labeled anomalies are available. : in that case, labeled anomalies can be used in a supervised learning process. A simple grid search can be setup to optimize the detection of the anomalies as we did in our previous test to estimate the impact of the HIF additional scores.\nManuscript submitted to ACM\nIf several distinct kind of anomalies exist, an iterative procedure can be set up to progressively isolate the anomalies according to their distinct patterns."
    }, {
      "heading" : "3.6 Complexity of the HIF algorithm",
      "text" : "Basically, the HIF algorithm has the same overall complexity than the IF algorithm, although some extra computational costs are involved during training and testing stages.\nIF has time complexities of O(t ·ψ · loд(ψ )) in the training stage and O(n · t · loд(ψ )) in the testing stage. At training stage, the HIF algorithm requires to evaluate the centroids of the data attached to each of the external nodes, henceψ centroids in average need to be evaluated. The evaluation of a centroid is dependent on the number of elements contained in the external buckets (neb ). It seems difficult to estimate precisely the expectation of neb , nevertheless, Fig.15 presents the result of an empirical study that shows that if the maximal height of the iTree is lmax = ⌈loд2(ψ )⌉, then the average neb value increases slightly faster than a logarithmic law. For this test, the random forest has been build from a normally distributed dataset with (0.0, 0.0)mean, and ((3, 0), (0, 3)) covariance matrix. Hence, to maintain the overall algorithmic complexity at training stage close toO(ln(ψ )) one may increase slightly the maximal height of the iTrees. One can use for instance lmax = ⌈1.2 · loд2(ψ )⌉ that empirically maintains a sub-logarithmic growth a shown in Fig.15 .\nFurthermore, at training stage, the HIF2 algorithm requires to evaluate the centroids of the anomalies that are attached to each of the external nodes. As in general very few anomalies are introduced into the HIF, this extra cost is marginal compared to the others.\nAt testing time, HIF has the same ’big-O’ complexity than the IF, namelyO(n · t · loдψ ), although some extra distances to centroid costs are involved in HIF. The increase of lmax will increase slightly the number of comparisons during the test stage, but this number will still be proportional to loд(ψ ).\nIn the following application we have used lmax = ⌈1.1 · loд2(ψ )⌉. Manuscript submitted to ACM"
    }, {
      "heading" : "4 APPLICATION TO INTRUSION DETECTION IN NETWORK SYSTEMS",
      "text" : "We here present the results of the experiments performed. We first describe the dataset retained to conduct our experiments, then specify the pre-processing procedure of data, prior to present and discuss about the results obtained.\nIn this research work, we are interested in the packet payloads as well as the packet header information. While packet headers generally constitute only a small part of whole network traffic data, packet payloads are more complicated. Accordingly, packet payloads analysis is more costly than the analysis of packet header data as it needs more computations and pre-processing. In this way, considering a suitable pre-processing process of network traffic data is vital."
    }, {
      "heading" : "4.1 The ISCX dataset",
      "text" : "The ISCX dataset 2012 [29], which has been prepared at the Information Security Centre of Excellence at the University of New Brunswick, is used to perform this experiments and evaluate the performance of our proposed approaches. The entire ISCX labeled dataset comprises over two million traffic packets characterized with 20 features and covered seven days of network activities (i.e. normal and attack). Four different attack types, called as Brute Force SSH, Infiltrating, HTTP DoS, and DDoS are conducted on different days. Despite some minor disadvantages, the ISCX dataset 2012 is the most up-to-date available dataset compared to the other aging explored datasets for intrusion detection [3, 21, 29].\nAs input to the detection process, we make use of the pre-processed ISCX flows consisting of header and payload packets which will be detailed later. To do so, first of all, flows are classified according to their application layers types such as HTTP Web, POP, IMAP, SSH, etc. Then we do the experiments separately on each data subset. This choice has been adopted because the ’normal’ traffic flows look very different depending on the application or service layer they relate to. It is also a way to reduce the complexity of the supervision task."
    }, {
      "heading" : "4.2 Pre-processing of the data",
      "text" : "Data pre-processing is a crucial task which can be even considered as a fundamental building block of intrusion detection. Pre-processing involves cleaning the data and removing redundant and unnecessary entries. It also involves converting the features of the dataset into numerical data and saving in a machine-readable format. To convert categorical features into entirely numerical ones, we adopt the binary number representation where we usem binary numbers to represent am-category feature. However, when a categorical feature takes its values in an infinite set of categories, we need to consider another conversion approach. To do so, we use histogram of distributions.\nIn the next step, we add the number of source-destination pairs in a pre-defined window size of flows to the features. This added feature helps to detect network and IP scans as well as distributed attacks. Table 5 presents the final features after pre-processing which be used in the experiments.\nLast step, but certainly not the least one, is to normalize the data. This step is crucial when dealing with features of different units and scales. Without normalization, features with extremely greater values dominate the features with smaller values. Here we use min-max normalization approachaccording to which we fit all the features into the unit segment [0; 1]."
    }, {
      "heading" : "4.3 Results",
      "text" : "We compare in Table 6 the AUC values obtained, for the various tested application layers, IF, HIF1 and HIF2 algorithms and the two baselines 1C-SVM and 2C-SVM. In this table, #LA column gives the absolute number and Labeled Anomalies\nManuscript submitted to ACM\nthat are introduced in the HIF2 and used to train the 2C-SVM (Eq.6). In this same column, the value in parenthesis corresponds to the total number of anomalies observed for the given application layer. For IF, HIF1 and HIF2, the forests comprise 1024 trees and each tree is associated to a data sample containing 512 instances. For HIF and SVM approaches, the meta parameters have been selected such as maximizing the AUC values.\nThe obtained AUC values are in general relatively high, except for the HTTPImage transfer application layer for which the AUC values for IF, HIF1 and HIF2 are respectively .55, .56 and .60. For this application layer, the detection of anomalies are much difficult because the attacks are deeply encoded into the image data packets that are transferred. The features we are using to represent the network data flows are not sufficiently discriminant to detect such anomalies. We note that the two baselines perform slightly better than the isolation forest based algorithms since 1C-SVM and 2C-SVM get respectively .64 and .69 AUC values for this application layer.\nAccording to table 6, the resolution of the ’blind spots’ seems to improve slightly the AUC values in general, although the significance of these improvements is not always established. Clearly, improvements are mostly driven by the incorporation of some labeled anomalies into the HIF. It is particularly visible for the HTTPWeb application layer for which we have added 400 anomalies representing 1% of all HTTPWeb anomalies. For this application layer, the AUC values are .86 for IF and .99 for HIF2 which outperforms largely the two baselines.\nManuscript submitted to ACM\nManuscript submitted to ACM\n(a)\n(b)\nFigure 16 shows the ROC curves for the 5 tested methods and the 9 application layers. According to these curves, the HIF1 performs similarly or better than the 1C-SVM and the HIF2 algorithm performs similarly or better than the 2C-SVM, except for the HTTPImageTransfer application layer.\nFigure 17 presents the distributions of the IF (a) and HIF (b) scores obtained on the ISCX HTTPWeb subset. The learning of 1% of the labeled anomalies generates a shifting of the distributions that better separates anomalies from ’normal’ data. This is also well shown in Figure 18 which presents the ROC curves for the IF and HIF algorithms.\nFurthermore, in Figure 17, one can identify a residual anomaly peak that overlaps the ’normal’ data distribution. One may try to isolate the data around this peak and ask an experts to labeled some of them to further train the HIF.\nAn exploratory data analysis based on the HIF score distributions can thus be used to set up efficiently an interactive semi-supervised procedure, in the scope of an active learning setting.\nHere again, according to Fig.19, adding very few labeled anomalies (as low as 2) into the HIF is enough to significantly improve the accuracy of the anomaly detection. This shows the supervised functionality of HIF can be particularly useful when some expertise is available."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "From the construction of a synthetic dataset we have identified an apparently quite penalizing drawback in the Isolation Forest algorithm, the so-called ’blind spots’, that characterize unoccupied areas in the data embedding as ’normal’ areas even if they are far from the ’normal’ data distribution.\nTo overcome this problem we have proposed a first extension that introduces some centroid based distance calculation in the IF algorithm and shown that the ’blind spot’ effect disappears at a very low computational cost. Manuscript submitted to ACM\nFurthermore, we have introduced a second extension that provides a supervised capability to the IF, enabling to introduce known anomaly locations into the trees of the isolation forest. A distance based score to these locations is the source to an additional scoring provided by the HIF. A simple linear model has been implemented to aggregate the three anomaly sub-scores proposed in the final Hybrid Isolation Forest (HIF) algorithm.\nExtensive testing on a synthetic dataset has been conducted to verify the resolution of the ’blind-spot’ effect and to study the impact of the meta parameters used to aggregate the composite scores of HIF, or to estimate the algorithmic complexity of the proposed extensions.\nIn addition, the HIF algorithm has been tested on an intrusion detection task. The experimentation carried out on the ISCX benchmark dataset shows that significant improvements in the detection accuracy can be made by incorporating few known anomalies as train data for the HIF.\nFurthermore, we have shown that the HIF algorithms compare relatively advantageously with the SVM baselines (1C-SVM and 2C-SVM) on the synthetic and ISCX datasets. The apparently successful (although simple) combination of\nManuscript submitted to ACM\nthe anomaly detection principle with a supervised classification capability, which is missing in our two baselines, is what makes the HIF original and very competitive at a low complexity cost.\nFinally we believe that the analysis of the distributions of the HIF scores can be used to set up an interactive semi-supervised or active learning procedure that can help to identify the emergence of new anomaly ’spots’ occurring in the data."
    } ],
    "references" : [ {
      "title" : "Survey on anomaly detection using data mining techniques",
      "author" : [ "Shikha Agrawal", "Jitendra Agrawal" ],
      "venue" : "Procedia Computer Science,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Graph based anomaly detection and description: A survey",
      "author" : [ "Leman Akoglu", "Hanghang Tong", "Danai Koutra" ],
      "venue" : "Data Min. Knowl. Discov.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Design of multilevel hybrid classifier with variant feature sets for intrusion detection system",
      "author" : [ "Aslıhan Akyol", "Mehmet Hacibeyoğlu", "Bekir Karlik" ],
      "venue" : "IEICE Transactions on Information and Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Nearest-neighbor and clustering based anomaly detection algorithms for rapidminer",
      "author" : [ "Mennatallah Amer", "Markus Goldstein" ],
      "venue" : "Proceedings of the 3rd RapidMiner Community Meeting and Conferernce (RCOMM",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Article: Network intrusion detection using clustering: A data mining approach",
      "author" : [ "S.Sathya Bama", "M.S.Irfan Ahmed", "A.Saravanan" ],
      "venue" : "International Journal of Computer Applications,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Stochastic neighbor embedding (sne) for dimension reduction and visualization using arbitrary divergences",
      "author" : [ "Kerstin Bunte", "Sven Haase", "Michael Biehl", "Thomas Villmann" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Carreira-perpiñán. The elastic embedding algorithm for dimensionality reduction",
      "author" : [ "Á. Miguel" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Anomaly detection: A survey",
      "author" : [ "Varun Chandola", "Arindam Banerjee", "Vipin Kumar" ],
      "venue" : "ACM Comput. Surv.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "A neural network component for an intrusion detection system",
      "author" : [ "H. Debar", "M. Becker", "D. Siboni" ],
      "venue" : "IEEE Computer Society Symposium on Research in Security and Privacy,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1992
    }, {
      "title" : "One class random forests",
      "author" : [ "Chesner Désir", "Simon Bernard", "Caroline Petitjean", "Laurent Heutte" ],
      "venue" : "Pattern Recogn.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "An anomaly detection approach based on isolation forest algorithm for streaming data using sliding window",
      "author" : [ "Zhiguo Ding", "Minrui Fei" ],
      "venue" : "IFAC Proceedings Volumes,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Anomaly detection support vector machine and its application to fault diagnosis",
      "author" : [ "R. Fujimaki" ],
      "venue" : "Eighth IEEE International Conference on Data Mining,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "A study in using neural networks for anomaly and misuse detection",
      "author" : [ "Anup K. Ghosh", "Aaron Schwartzbard" ],
      "venue" : "In Proceedings of the 8th Conference on USENIX Security Symposium - Volume 8,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data",
      "author" : [ "Markus Goldstein", "Seiichi Uchida" ],
      "venue" : "PLOS ONE, 11(4):1–31,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Towards an information-theoretic framework for analyzing intrusion detection systems",
      "author" : [ "Guofei Gu", "Prahlad Fogla", "David Dagon", "Wenke Lee", "Boris Škorić" ],
      "venue" : "In 11th European Symposium on Research in Computer Security (ESORICS),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Dimensionality reduction by supervised neighbor embedding using laplacian search",
      "author" : [ "Carlo Cattani Jianwei Zheng", "Hangke Zhang", "Wanliang Wang" ],
      "venue" : "Computational and Mathematical Methods in Medicine,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Blind separation of sources, part 1: An adaptive algorithm based on neuromimetic architecture",
      "author" : [ "Christian Jutten", "Jeanny Herault" ],
      "venue" : "Signal Process.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1991
    }, {
      "title" : "One-class classification with gaussian processes",
      "author" : [ "Michael Kemmler", "Erik Rodner", "Esther-Sabrina Wacker", "Joachim Denzler" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Information-theoretic measures for anomaly detection",
      "author" : [ "Wenke Lee", "Dong Xiang" ],
      "venue" : "In Proceedings of the 2001 IEEE Symposium on Security and Privacy, SP",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2001
    }, {
      "title" : "Cann: An intrusion detection system based on combining cluster centers and nearest neighbors",
      "author" : [ "Wei-Chao Lin", "Shih-Wen Ke", "Chih-Fong Tsai" ],
      "venue" : "Knowledge-Based Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Pcaplib: A system of extracting, classifying, and anonymizing real packet traces",
      "author" : [ "Ying-Dar Lin", "Po-Ching Lin", "Sheng-Hao Wang", "I-Wei Chen", "Yuan-Cheng Lai" ],
      "venue" : "IEEE Systems Journal,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Isolation forest",
      "author" : [ "F.T. Liu", "K.M. Ting", "Z.H. Zhou" ],
      "venue" : "In Proceedings of the 8th IEEE International Conference on Data Mining (ICDM’08),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Isolation-based anomaly detection",
      "author" : [ "Fei Tony Liu", "Kai Ming Ting", "Zhi-Hua Zhou" ],
      "venue" : "ACM Trans. Knowl. Discov. Data,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Spherical stochastic neighbor embedding of hyperspectral data",
      "author" : [ "Dalton Lunga", "Okan K. Ersoy" ],
      "venue" : "IEEE Trans. Geoscience and Remote Sensing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Data Structures and Algorithms with Object-Oriented Design",
      "author" : [ "Bruno Richard Preiss" ],
      "venue" : "Patterns in Java. wiley,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1999
    }, {
      "title" : "Intrusion detection with neural networks",
      "author" : [ "Jake Ryan", "Meng-Jang Lin", "Risto Miikkulainen" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1998
    }, {
      "title" : "Nonlinear component analysis as a kernel eigenvalue problem",
      "author" : [ "Bernhard Schölkopf", "Alexander Smola", "Klaus-Robert Müller" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1998
    }, {
      "title" : "A Novel Isolation-Based Outlier Detection Method, pages 446–456",
      "author" : [ "Yanhui Shen", "Huawen Liu", "Yanxia Wang", "Zhongyu Chen", "Guanghua Sun" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Toward developing a systematic approach to generate benchmark datasets for intrusion detection",
      "author" : [ "Ali Shiravi", "Hadi Shiravi", "Mahbod Tavallaee", "Ali A. Ghorbani" ],
      "venue" : "Computer Security,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "A quite exhaustive, although a bit old, review in anomaly detection has been proposed in [8], completed by a more recent comparative study [14].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "A quite exhaustive, although a bit old, review in anomaly detection has been proposed in [8], completed by a more recent comparative study [14].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "According to these studies, the state of the art methods can be distributed into five main categories: (1) Near neighbors and clustering based methods [4]: Near Neighbors methods rely on the assumption that a ’normal’ instance occurs close to its near neighbors while an anomaly occurs far from its near neighbors.",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "Similarly, cluster based methods rely on the assumption that a ’normal’ instance occurs near its closest cluster centroid while an anomaly will occur far from its nearest cluster centroid [5, 20].",
      "startOffset" : 188,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "Similarly, cluster based methods rely on the assumption that a ’normal’ instance occurs near its closest cluster centroid while an anomaly will occur far from its nearest cluster centroid [5, 20].",
      "startOffset" : 188,
      "endOffset" : 195
    }, {
      "referenceID" : 9,
      "context" : "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.",
      "startOffset" : 155,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.",
      "startOffset" : 155,
      "endOffset" : 166
    }, {
      "referenceID" : 25,
      "context" : "Random Forest, including recent advances on one-class random forest [10], multi-class and one-class Support Vector Machine (SVM) [12], and neural networks [9, 13, 26], are the most used classifiers for anomaly detection.",
      "startOffset" : 155,
      "endOffset" : 166
    }, {
      "referenceID" : 17,
      "context" : "Popular approaches in this category are kernel based density models and the Gaussian Mixture Model (GMM), including recent advance in one-class GMM [18], (4) Information theoretic based methods use information theoretic measures [19], such as the entropy, the Kolmogorof complexity, the minimum description length, etc, to estimate the ’complexity’ of the ’normal’ dataset (or equivalently the complexity of the process behind the production of these data) [15].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "Popular approaches in this category are kernel based density models and the Gaussian Mixture Model (GMM), including recent advance in one-class GMM [18], (4) Information theoretic based methods use information theoretic measures [19], such as the entropy, the Kolmogorof complexity, the minimum description length, etc, to estimate the ’complexity’ of the ’normal’ dataset (or equivalently the complexity of the process behind the production of these data) [15].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 14,
      "context" : "Popular approaches in this category are kernel based density models and the Gaussian Mixture Model (GMM), including recent advance in one-class GMM [18], (4) Information theoretic based methods use information theoretic measures [19], such as the entropy, the Kolmogorof complexity, the minimum description length, etc, to estimate the ’complexity’ of the ’normal’ dataset (or equivalently the complexity of the process behind the production of these data) [15].",
      "startOffset" : 457,
      "endOffset" : 461
    }, {
      "referenceID" : 1,
      "context" : "(5) Spectral based method rely on the assumption that it is possible to embed the data into a lower dimensional subspace in which ’normal’ data and anomalies are supposedly well separated [2].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "A recent review [1], though much less detailed, identifies a supplemental category that stands in between supervised and unsupervised methods, and that authors refer to as ’hybrid methods’.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "In 2008, Isolation Forest (IF) [22], a quite conceptually different approach to the previously referenced methods, has been proposed that went strangely below the radar of the previous review.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "IF has been successfully used in some applications, [23], [11] and extended recently in [28] to improve the selection of attributes and their split values when constructing the tree.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "IF has been successfully used in some applications, [23], [11] and extended recently in [28] to improve the selection of attributes and their split values when constructing the tree.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "IF has been successfully used in some applications, [23], [11] and extended recently in [28] to improve the selection of attributes and their split values when constructing the tree.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "Principle of the IF algorithm (Figure is from [22]).",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 21,
      "context" : "1 The Isolation Forest algorithm We reproduce hereinafter the description of the isolation forest algorithm as presented in [22].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 24,
      "context" : "3 of [25] gives the average path length of unsuccessful search in BST as:",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 26,
      "context" : "The kernel PCA [27] is a good example of such method.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "This can be feasible to some extend and in some framework, such as blind source separation, though it requires some knowledge about the data, such as the number of sources [17].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : "For problems expressed in high dimension it is often possible to reduce the dimension using an appropriate embedding that allows for describing the data with variables that are weakly correlated, but in general at the cost of losing information (the embedding is not invertible) [REFs] [6, 7, 16, 24] .",
      "startOffset" : 286,
      "endOffset" : 300
    }, {
      "referenceID" : 6,
      "context" : "For problems expressed in high dimension it is often possible to reduce the dimension using an appropriate embedding that allows for describing the data with variables that are weakly correlated, but in general at the cost of losing information (the embedding is not invertible) [REFs] [6, 7, 16, 24] .",
      "startOffset" : 286,
      "endOffset" : 300
    }, {
      "referenceID" : 15,
      "context" : "For problems expressed in high dimension it is often possible to reduce the dimension using an appropriate embedding that allows for describing the data with variables that are weakly correlated, but in general at the cost of losing information (the embedding is not invertible) [REFs] [6, 7, 16, 24] .",
      "startOffset" : 286,
      "endOffset" : 300
    }, {
      "referenceID" : 23,
      "context" : "For problems expressed in high dimension it is often possible to reduce the dimension using an appropriate embedding that allows for describing the data with variables that are weakly correlated, but in general at the cost of losing information (the embedding is not invertible) [REFs] [6, 7, 16, 24] .",
      "startOffset" : 286,
      "endOffset" : 300
    }, {
      "referenceID" : 28,
      "context" : "1 The ISCX dataset The ISCX dataset 2012 [29], which has been prepared at the Information Security Centre of Excellence at the University of New Brunswick, is used to perform this experiments and evaluate the performance of our proposed approaches.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "Despite some minor disadvantages, the ISCX dataset 2012 is the most up-to-date available dataset compared to the other aging explored datasets for intrusion detection [3, 21, 29].",
      "startOffset" : 167,
      "endOffset" : 178
    }, {
      "referenceID" : 20,
      "context" : "Despite some minor disadvantages, the ISCX dataset 2012 is the most up-to-date available dataset compared to the other aging explored datasets for intrusion detection [3, 21, 29].",
      "startOffset" : 167,
      "endOffset" : 178
    }, {
      "referenceID" : 28,
      "context" : "Despite some minor disadvantages, the ISCX dataset 2012 is the most up-to-date available dataset compared to the other aging explored datasets for intrusion detection [3, 21, 29].",
      "startOffset" : 167,
      "endOffset" : 178
    } ],
    "year" : 2017,
    "abstractText" : "From the identification of a drawback in the Isolation Forest (IF) algorithm that limits its use in the scope of anomaly detection, we propose two extensions that allow to firstly overcome the previously mention limitation and secondly to provide it with some supervised learning capability. The resulting Hybrid Isolation Forest (HIF) that we propose is first evaluated on a synthetic dataset to analyze the effect of the new meta-parameters that are introduced and verify that the addressed limitation of the IF algorithm is effectively overcame. We hen compare the two algorithms on the ISCX benchmark dataset, in the context of a network intrusion detection application. Our experiments show that HIF outperforms IF, but also challenges the 1-class and 2-classes SVM baselines with computational efficiency.",
    "creator" : "LaTeX with hyperref package"
  }
}