{
  "name" : "1306.1091.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep Generative Stochastic Networks Trainable by Backprop",
    "authors" : [ "Yoshua Bengio", "Éric Thibodeau-Laufer", "Guillaume Alain" ],
    "emails" : [ "FIND.US@ON.THE.WEB" ],
    "sections" : [ {
      "heading" : null,
      "text" : "P(X) X\nC(X̃|X)\nX̃\nP(X|X̃)\nP(X) X\nP(H|X)\nH\nP(X|H)\nFigure 1. Top: A denoising auto-encoder defines an estimated Markov chain where the transition operator first samples a corrupted X̃ from C(X̃|X) and then samples a reconstruction from Pθ(X|X̃), which is trained to estimate the ground truth P (X|X̃). Note how for any given X̃ , P (X|X̃) is a much simpler (roughly unimodal) distribution than the ground truth P (X) and its partition function is thus easier to approximate. Bottom: More generally, a GSN allows the use of arbitrary latent variables H in addition to X , with the Markov chain state (and mixing) involving both X and H . Here H is the angle about the origin. The GSN inherits the benefit of a simpler conditional and adds latent variables, which allow far more powerful deep representations in which mixing is easier (Bengio et al., 2013b).\nar X\niv :1\n30 6.\n10 91\nv5 [\ncs .L\nG ]\n2 4\nM ay"
    }, {
      "heading" : "1 Introduction",
      "text" : "Research in deep learning (see Bengio (2009) and Bengio et al. (2013a) for reviews) grew from breakthroughs in unsupervised learning of representations, based mostly on the Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoder variants (Bengio et al., 2007; Vincent et al., 2008), and sparse coding variants (Lee et al., 2007; Ranzato et al., 2007). However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al., 2012). The latest breakthrough in object recognition (Krizhevsky et al., 2012) was achieved with fairly deep convolutional networks with a form of noise injection in the input and hidden layers during training, called dropout (Hinton et al., 2012). In all of these cases, the availability of large quantities of labeled data was critical.\nOn the other hand, progress with deep unsupervised architectures has been slower, with the established options with a probabilistic footing being the Deep Belief Network (DBN) (Hinton et al., 2006) and the Deep Boltzmann Machine (DBM) (Salakhutdinov & Hinton, 2009). Although single-layer unsupervised learners are fairly well developed and used to pre-train these deep models, jointly training all the layers with respect to a single unsupervised criterion remains a challenge, with a few techniques arising to reduce that difficulty (Montavon & Muller, 2012; Goodfellow et al., 2013). In contrast to recent progress toward joint supervised training of models with many layers, joint unsupervised training of deep models remains a difficult task.\nThough the goal of training large unsupervised networks has turned out to be more elusive than its supervised counterpart, the vastly larger available volume of unlabeled data still beckons for efficient methods to model it. Recent progress in training supervised models raises the question: can we take advantage of this progress to improve our ability to train deep, generative, unsupervised, semi-supervised or structured output models?\nThis paper lays theoretical foundations for a move in this direction through the following main contributions:\n1 – Intuition: In Section 2 we discuss what we view as basic motivation for studying alternate ways of training unsupervised probabilistic models, i.e., avoiding the intractable sums or maximization involved in many approaches.\n2 – Training Framework: We generalize recent work on the generative view of denoising autoencoders (Bengio et al., 2013c) by introducing latent variables in the framework to define Generative Stochastic Networks (GSNs) (Section 3). GSNs aim to estimate the data generating distribution indirectly, by parametrizing the transition op-\nerator of a Markov chain rather than directly parametrizing P (X). Most critically, this framework transforms the unsupervised density estimation problem into one which is more similar to supervised function approximation. This enables training by (possibly regularized) maximum likelihood and gradient descent computed via simple backpropagation, avoiding the need to compute intractable partition functions. Depending on the model, this may allow us to draw from any number of recently demonstrated supervised training tricks. For example, one could use a convolutional architecture with max-pooling for parametric parsimony and computational efficiency, or dropout (Hinton et al., 2012) to prevent co-adaptation of hidden representations.\n3 – General theory: Training the generative (decoding / denoising) component of a GSN P (X|h) with noisy representation h is often far easier than modeling P (X) explicitly (compare the blue and red distributions in Figure 1). We prove that if our estimated P (X|h) is consistent (e.g. through maximum likelihood), then the stationary distribution of the resulting chain is a consistent estimator of the data generating density, P (X) (Section 3.2). We strengthen the consistency theorems introduced in Bengio et al. (2013c) by showing that the corruption distribution may be purely local, not requiring support over the whole domain of the visible variables (Section 3.1).\n4 – Consequences of theory: We show that the model is general and extends to a wide range of architectures, including sampling procedures whose computation can be unrolled as a Markov Chain, i.e., architectures that add noise during intermediate computation in order to produce random samples of a desired distribution (Theorem 2). An exciting frontier in machine learning is the problem of modeling so-called structured outputs, i.e., modeling a conditional distribution where the output is high-dimensional and has a complex multimodal joint distribution (given the input variable). We show how GSNs can be used to support such structured output and missing values (Section 3.4).\n5 – Example application: In Section 4 we show an example application of the GSN theory to create a deep GSN whose computational graph resembles the one followed by Gibbs sampling in deep Boltzmann machines (with continuous latent variables), but that can be trained efficiently with back-propagated gradients and without layerwise pretraining. Because the Markov Chain is defined over a state (X,h) that includes latent variables, we reap the dual advantage of more powerful models for a given number of parameters and better mixing in the chain as we add noise to variables representing higher-level information, first suggested by the results obtained by Bengio et al. (2013b) and Luo et al. (2013). The experimental results show that such a model with latent states indeed mixes better than\nshallower models without them (Table 1).\n6 – Dependency networks: Finally, an unexpected result falls out of the GSN theory: it allows us to provide a novel justification for dependency networks (Heckerman et al., 2000) and for the first time define a proper joint distribution between all the visible variables that is learned by such models (Section 3.5)."
    }, {
      "heading" : "2 Summing over too many major modes",
      "text" : "Many of the computations involved in graphical models (inference, sampling, and learning) are made intractable and difficult to approximate because of the large number of non-negligible modes in the modeled distribution (either directly P (x) or a joint distribution P (x, h) involving latent variables h). In all of these cases, what is intractable is the computation or approximation of a sum (often weighted by probabilities), such as a marginalization or the estimation of the gradient of the normalization constant. If only a few terms in this sum dominate (corresponding to the dominant modes of the distribution), then many good approximate methods can be found, such as Monte-Carlo Markov chains (MCMC) methods.\nSimilarly difficult tasks arise with structured output problems where one wants to sample from P (y, h|x) and both y and h are high-dimensional and have a complex highly multimodal joint distribution (given x).\nDeep Boltzmann machines (Salakhutdinov & Hinton, 2009) combine the difficulty of inference (for the positive phase where one tries to push the energies associated with the observed x down) and also that of sampling (for the negative phase where one tries to push up the energies associated with x’s sampled from P (x)). Unfortunately, using an MCMC method to sample from P (x, h) in order to estimate the gradient of the partition function may be seriously hurt by the presence of a large number of important modes, as argued below.\nTo evade the problem of highly multimodal joint or posterior distributions, the currently known approaches to dealing with the above intractable sums make very strong explicit assumptions (in the parametrization) or implicit assumptions (by the choice of approximation methods) on the form of the distribution of interest. In particular, MCMC methods are more likely to produce a good estimator if the number of non-negligible modes is small: otherwise the chains would require at least as many MCMC steps as the number of such important modes, times a factor that accounts for the mixing time between modes. Mixing time itself can be very problematic as a trained model becomes sharper, as it approaches a data generating distribution that may have well-separated and sharp modes (i.e., manifolds).\nWe propose to make another assumption that might suffice\nto bypass this multimodality problem: the effectiveness of function approximation.\nIn particular, the GSN approach presented in the next section relies on estimating the transition operator of a Markov chain, e.g. P (xt|xt−1) or P (xt, ht|xt−1, ht−1). Because each step of the Markov chain is generally local, these transition distributions will often include only a very small number of important modes (those in the neighbourhood of the previous state). Hence the gradient of their partition function will be easy to approximate. For example consider the denoising transitions studied by Bengio et al. (2013c) and illustrated in Figure 1, where x̃t−1 is a stochastically corrupted version of xt−1 and we learn the denoising distribution P (x|x̃). In the extreme case (studied empirically here) where P (x|x̃) is approximated by a unimodal distribution, the only form of training that is required involves function approximation (predicting the clean x from the corrupted x̃).\nAlthough having the true P (x|x̃) turn out to be unimodal makes it easier to find an appropriate family of models for it, unimodality is by no means required by the GSN framework itself. One may construct a GSN using any multimodal model for output (e.g. mixture of Gaussians, RBMs, NADE, etc.), provided that gradients for the parameters of the model in question can be estimated (e.g. log-likelihood gradients).\nThe approach proposed here thus avoids the need for a poor approximation of the gradient of the partition function in the inner loop of training, but still has the potential of capturing very rich distributions by relying mostly on “function approximation”.\nBesides the approach discussed here, there may well be other very different ways of evading this problem of intractable marginalization, including approaches such as sum-product networks (Poon & Domingos, 2011), which are based on learning a probability function that has a tractable form by construction and yet is from a flexible enough family of distributions."
    }, {
      "heading" : "3 Generative Stochastic Networks",
      "text" : "Assume the problem we face is to construct a model for some unknown data-generating distribution P (X) given only examples of X drawn from that distribution. In many cases, the unknown distribution P (X) is complicated, and modeling it directly can be difficult.\nA recently proposed approach using denoising autoencoders transforms the difficult task of modeling P (X) into a supervised learning problem that may be much easier to solve. The basic approach is as follows: given a clean example data point X from P (X), we obtain a corrupted version X̃ by sampling from some corruption distribution\nC(X̃|X). For example, we might take a clean image, X , and add random white noise to produce X̃ . We then use supervised learning methods to train a function to reconstruct, as accurately as possible, any X from the data set given only a noisy version X̃ . As shown in Figure 1, the reconstruction distribution P (X|X̃) may often be much easier to learn than the data distribution P (X), because P (X|X̃) tends to be dominated by a single or few major modes (such as the roughly Gaussian shaped density in the figure).\nBut how does learning the reconstruction distribution help us solve our original problem of modeling P (X)? The two problems are clearly related, because if we knew everything about P (X), then our knowledge of the C(X̃|X) that we chose would allow us to precisely specify the optimal reconstruction function via Bayes rule: P (X|X̃) = 1 zC(X̃|X)P (X), where z is a normalizing constant that does not depend on X . As one might hope, the relation is also true in the opposite direction: once we pick a method of adding noise, C(X̃|X), knowledge of the corresponding reconstruction distribution P (X|X̃) is sufficient to recover the density of the data P (X).\nThis intuition was borne out by proofs in two recent papers. Alain & Bengio (2013) showed that denoising autoencoders with small Gaussian corruption and squared error loss estimated the score (derivative of the log-density with respect to the input) of continuous observed random variables. More recently, Bengio et al. (2013c) generalized this to arbitrary variables (discrete, continuous or both), arbitrary corruption (not necessarily asymptotically small), and arbitrary loss function (so long as they can be seen as a loglikelihood).\nBeyond proving that P (X|X̃) is sufficient to reconstruct the data density, Bengio et al. (2013c) also demonstrated a method of sampling from a learned, parametrized model of the density, Pθ(X), by running a Markov chain that alternately adds noise using C(X̃|X) and denoises by sampling from the learned Pθ(X|X̃), which is trained to approximate the true P (X|X̃). The most important contribution of that paper was demonstrating that if a learned, parametrized reconstruction function Pθ(X|X̃) converges to the true P (X|X̃), then under some relatively benign conditions the stationary distribution π(X) of the resulting Markov chain will exist and will indeed converge to the data distribution P (X).\nBefore moving on, we should pause to make an important point clear. Alert readers may have noticed that P (X|X̃) and P (X) can each be used to reconstruct the other given knowledge of C(X̃|X). Further, if we assume that we have chosen a simple C(X̃|X) (say, a uniform Gaussian with a single width parameter), then P (X|X̃) and P (X) must both be of approximately the same complexity. Put another way, we can never hope to combine a simple C(X̃|X) and a\nsimple P (X|X̃) to model a complex P (X). Nonetheless, it may still be the case that P (X|X̃) is easier to model than P (X) due to reduced computational complexity in computing or approximating the partition functions of the conditional distribution mapping corrupted input X̃ to the distribution of corresponding clean input X . Indeed, because that conditional is going to be mostly assigning probability to X locally around X̃ , P (X|X̃) has only one or a few modes, while P (X) can have a very large number.\nSo where did the complexity go? P (X|X̃) has fewer modes than P (X), but the location of these modes depends on the value of X̃ . It is precisely this mapping from X̃ → mode location that allows us to trade a difficult density modeling problem for a supervised function approximation problem that admits application of many of the usual supervised learning tricks.\nIn the next four sections, we extend previous results in several directions."
    }, {
      "heading" : "3.1 Generative denoising autoencoders with local noise",
      "text" : "The main theorem in Bengio et al. (2013c), reproduced below, requires that the Markov chain be ergodic. A set of conditions guaranteeing ergodicity is given in the aforementioned paper, but these conditions are restrictive in requiring that C(X̃|X) > 0 everywhere that P (X) > 0. Here we show how to relax these conditions and still guarantee ergodicity through other means.\nLet Pθn(X|X̃) be a denoising auto-encoder that has been trained on n training examples. Pθn(X|X̃) assigns a probability to X , given X̃ , when X̃ ∼ C(X̃|X). This estimator defines a Markov chain Tn obtained by sampling alternatively an X̃ from C(X̃|X) and an X from Pθ(X|X̃). Let πn be the asymptotic distribution of the chain defined by Tn, if it exists. The following theorem is proven by Bengio et al. (2013c). Theorem 1. If Pθn(X|X̃) is a consistent estimator of the true conditional distribution P (X|X̃) and Tn defines an ergodic Markov chain, then as n → ∞, the asymptotic distribution πn(X) of the generated samples converges to the data-generating distribution P (X).\nIn order for Theorem 1 to apply, the chain must be ergodic. One set of conditions under which this occurs is given in the aforementioned paper. We slightly restate them here: Corollary 1. If the support for both the data-generating distribution and denoising model are contained in and non-zero in a finite-volume region V (i.e., ∀X̃ , ∀X /∈ V, P (X) = 0, Pθ(X|X̃) = 0 and ∀X̃ , ∀X ∈ V, P (X) > 0, Pθ(X|X̃) > 0, C(X̃|X) > 0) and these statements remain true in the limit of n→∞, then the chain defined by Tn will be ergodic.\nIf conditions in Corollary 1 apply, then the chain will be ergodic and Theorem 1 will apply. However, these conditions are sufficient, not necessary, and in many cases they may be artificially restrictive. In particular, Corollary 1 defines a large region V containing any possible X allowed by the model and requires that we maintain the probability of jumping between any two points in a single move to be greater than 0. While this generous condition helps us easily guarantee the ergodicity of the chain, it also has the unfortunate side effect of requiring that, in order for Pθn(X|X̃) to converge to the conditional distribution P (X|X̃), it must have the capacity to model every mode of P (X), exactly the difficulty we were trying to avoid. The left two plots in Figure 2 show this difficulty: because C(X̃|X) > 0 everywhere in V , every mode of P (X) will leak, perhaps attenuated, into P (X|X̃).\nFortunately, we may seek ergodicity through other means. The following corollary allows us to choose a C(X̃|X) that only makes small jumps, which in turn only requires Pθ(X|X̃) to model a small part of the space V around each X̃ .\nLet Pθn(X|X̃) be a denoising auto-encoder that has been trained on n training examples and C(X̃|X) be some corruption distribution. Pθn(X|X̃) assigns a probability to X , given X̃ , when X̃ ∼ C(X̃|X) and X ∼ P(X). Define a Markov chain Tn by alternately sampling an X̃ from C(X̃|X) and an X from Pθ(X|X̃).\nCorollary 2. If the data-generating distribution is con-\ntained in and non-zero in a finite-volume region V (i.e., ∀X /∈ V, P (X) = 0, and ∀X ∈ V, P (X) > 0) and all pairs of points in V can be connected by a finite-length path through V and for some > 0, ∀X̃ ∈ V,∀X ∈ V within of each other, C(X̃|X) > 0 and Pθ(X|X̃) > 0 and these statements remain true in the limit of n → ∞, then the chain defined by Tn will be ergodic.\nProof. Consider any two points Xa and Xb in V . By the assumptions of Corollary 2, there exists a finite length path between Xa and Xb through V . Pick one such finite length path P . Chose a finite series of points x = {x1, x2, . . . , xk} along P , with x1 = Xa and xk = Xb such that the distance between every pair of consecutive points (xi, xi+1) is less than as defined in Corollary 2. Then the probability of sampling X̃ = xi+1 from C(X̃|xi)) will be positive, because C(X̃|X)) > 0 for all X̃ within of X by the assumptions of Corollary 2. Further, the probability of sampling X = X̃ = xi+1 from Pθ(X|X̃) will be positive from the same assumption on P . Thus the probability of jumping along the path from xi to xi+1, Tn(Xt+1 = xi+1|Xt = xi), will be greater than zero for all jumps on the path. Because there is a positive probability finite length path between all pairs of points in V , all states commute, and the chain is irreducible. If we consider Xa = Xb ∈ V , by the same arguments Tn(Xt = Xa|Xt−1 = Xa) > 0. Because there is a positive probability of remaining in the same state, the chain will be aperiodic. Because the chain is irreducible and over a finite state space, it will be positive recurrent as well. Thus, the chain defined by Tn is ergodic.\nAlthough this is a weaker condition that has the advantage of making the denoising distribution even easier to model (probably having less modes), we must be careful to choose the ball size large enough to guarantee that one can jump often enough between the major modes of P (X) when these are separated by zones of tiny probability. must be larger than half the largest distance one would have to travel across a desert of low probability separating two nearby modes (which if not connected in this way would make V not anymore have a single connected component). Practically, there would be a trade-off between the difficulty of estimating P (X|X̃) and the ease of mixing between major modes separated by a very low density zone.\nThe generalization of the above results presented in the next section is meant to help deal with this mixing problem. It is inspired by the recent work (Bengio et al., 2013b) showing that mixing between modes can be a serious problem for RBMs and DBNs, and that well-trained deeper models can greatly alleviate it by allowing the mixing to happen at a more abstract level of representation (e.g., where some bits can actually represent which mode /\nclass / manifold is considered)."
    }, {
      "heading" : "3.2 Generalizing the denoising autoencoder to GSNs",
      "text" : "The denoising auto-encoder Markov chain is defined by X̃t ∼ C(X̃|Xt) and Xt+1 ∼ Pθ(X|X̃t), where Xt alone can serve as the state of the chain. The GSN framework generalizes this by defining a Markov chain with both a visible Xt and a latent variable Ht as state variables, of the form\nHt+1 ∼ Pθ1(H|Ht, Xt) Xt+1 ∼ Pθ2(X|Ht+1).\nX2X0 X1\nH0 H1 H2\nDenoising auto-encoders are thus a special case of GSNs. Note that, given that the distribution of Ht+1 depends on a previous value of Ht, we find ourselves with an extra H0 variable added at the beginning of the chain. This H0 complicates things when it comes to training, but when we are in a sampling regime we can simply wait a sufficient number of steps to burn in.\nThe next theoretical results give conditions for making the stationary distributions of the above Markov chain match a target data generating distribution. Theorem 2. Let (Ht, Xt) ∞ t=0 be the Markov chain defined by the following graphical model.\nX2X0 X1\nH0 H1 H2\nIf we assume that the chain has a stationary distribution πX,H , and that for every value of (x, h) we have that\n• all the P (Xt = x|Ht = h) = g(x, h) share the same density for t ≥ 1\n• all the P (Ht+1 = h|Ht = h′, Xt = x) = f(h, h′, x) share the same density for t ≥ 0\n• P (H0 = h|X0 = x) = P (H1 = h|X0 = x)\n• P (X1 = x|H1 = h) = P (X0 = x|H1 = h)\nthen for every value of (x, h) we get that\n• P (X0 = x|H0 = h) = g(x, h) holds, which is something that was assumed only for t ≥ 1\n• P (Xt = x,Ht = h) = P (X0 = x,H0 = h) for all t ≥ 0\n• the stationary distribution πH,X has a marginal distribution πX such that π (x) = P (X0 = x).\nThose conclusions show that our Markov chain has the property that its samples in X are drawn from the same distribution as X0.\nProof. The proof hinges on a few manipulations done with the first variables to show that P (Xt = x|Ht = h) = g(x, h), which is assumed for t ≥ 1, also holds for t = 0.\nFor all h we have that\nP (H0 = h) = ∫ P (H0 = h|X0 = x)P (X0 = x)dx\n= ∫ P (H1 = h|X0 = x)P (X0 = x)dx\n= P (H1 = h).\nThe equality in distribution between (X1, H1) and (X0, H0) is obtained with\nP (X1 = x,H1 = h) = P (X1 = x|H1 = h)P (H1 = h) = P (X0 = x|H1 = h)P (H1 = h)\n(by hypothesis) = P (X0 = x,H1 = h)\n= P (H1 = h|X0 = x)P (X0 = x) = P (H0 = h|X0 = x)P (X0 = x)\n(by hypothesis) = P (X0 = x,H0 = h).\nThen we can use this to conclude that\nP (X0 = x,H0 = h) = P (X1 = x,H1 = h)\n=⇒ P (X0 = x|H0 = h) = P (X1 = x|H1 = h) = g(x, h) so, despite the arrow in the graphical model being turned the other way, we have that the density of P (X0 = x|H0 = h) is the same as for all other P (Xt = x|Ht = h) with t ≥ 1.\nNow, since the distribution of H1 is the same as the distribution of H0, and the transition probability P (H1 = h|H0 = h′) is entirely defined by the (f, g) densities which are found at every step for all t ≥ 0, then we know that (X2, H2) will have the same distribution as (X1, H1). To make this point more explicitly,\nP (H1 = h|H0 = h′)\n= ∫ P (H1 = h|H0 = h′, X0 = x)P (X0 = x|H0 = h′)dx\n= ∫ f(h, h′, x)g(x, h′)dx\n= ∫ P (H2 = h|H1 = h′, X1 = x)P (X1 = x|H1 = h′)dx\n=P (H2 = h|H1 = h′)\nThis also holds for P (H3|H2) and for all subsequent P (Ht+1|Ht). This relies on the crucial step where we demonstrate that P (X0 = x|H0 = h) = g(x, h). Once this was shown, then we know that we are using the same transitions expressed in terms of (f, g) at every step.\nSince the distribution of H0 was shown above to be the same as the distribution of H1, this forms a recursive argument that shows that all the Ht are equal in distribution to H0. Because g(x, h) describes every P (Xt = x|Ht = h), we have that all the joints (Xt, Ht) are equal in distribution to (X0, H0).\nThis implies that the stationary distribution πX,H is the same as that of (X0, H0). Their marginals with respect to X are thus the same.\nTo apply Theorem 2 in a context where we use experimental data to learn a model, we would like to have certain guarantees concerning the robustness of the stationary density πX . When a model lacks capacity, or when it has seen only a finite number of training examples, that model can be viewed as a perturbed version of the exact quantities found in the statement of Theorem 2.\nA good overview of results from perturbation theory discussing stationary distributions in finite state Markov chains can be found in (Cho et al., 2000). We reference here only one of those results. Theorem 3. Adapted from (Schweitzer, 1968)\nLet K be the transition matrix of a finite state, irreducible, homogeneous Markov chain. Let π be its stationary distribution vector so that Kπ = π. Let A = I − K and Z = (A+ C)\n−1 where C is the square matrix whose columns all contain π. Then, if K̃ is any transition matrix (that also satisfies the irreducible and homogeneous conditions) with stationary distribution π̃, we have that\n‖π − π̃‖1 ≤ ‖Z‖∞ ∥∥∥K − K̃∥∥∥\n∞ .\nThis theorem covers the case of discrete data by showing how the stationary distribution is not disturbed by a great amount when the transition probabilities that we learn are close to their correct values. We are talking here about the transition between steps of the chain (X0, H0), (X1, H1), . . . , (Xt, Ht), which are defined in Theorem 2 through the (f, g) densities.\nWe avoid discussing the training criterion for a GSN. Various alternatives exist, but this analysis is for future work. Right now Theorem 2 suggests the following rules :\n• Pick the transition distribution f(h, h′, x) to be use-\nful. There is no bad f when g can be trained perfectly with infinite capacity. However, the choice of f can put a great burden on g, and using a simple f, such as one that represents additive gaussian noise, will lead to less difficulties in training g. In practice, we have also found good results by training f at the same time by back-propagating the errors from g into f . In this way we simultaneously train g to model the distribution implied by f and train f to make its implied distribution easy to model by g.\n• Make sure that during training P (H0 = h|X0 = x) → P (H1 = h|X0 = x). One interesting way to achieve this is, for each X0 in the training set, iteratively sample H1|(H0, X0) and substitute the value of H1 as the updated value of H0. Repeat until you have achieved a kind of “burn in”. Note that, after the training is completed, when we use the chain for sampling, the samples that we get from its stationary distribution do not depend on H0. This technique of substituting theH1 intoH0 does not apply beyond the training step.\n• Define g(x, h) to be your estimator for P (X0 = x|H1 = h), e.g. by training an estimator of this conditional distribution from the samples (X0, H1).\n• The rest of the chain for t ≥ 1 is defined in terms of (f, g).\nAs much as we would like to simply learn g from pairs (H0, X0), the problem is that the training samples X (i) 0 are descendants of the corresponding values of H(i)0 in the original graphical model that describes the GSN. Those H\n(i) 0 are hidden quantities in GSN and we have to find a way to deal with them. Setting them all to be some default value would not work because the relationship between H0 and X0 would not be the same as the relationship later between Ht and Xt in the chain."
    }, {
      "heading" : "3.3 Alternate parametrization with deterministic functions of random quantities",
      "text" : "There are several equivalent ways of expressing a GSN. One of the interesting formulations is to use deterministic functions of random variables to express the densities (f, g) used in Theorem 2. With that approach, we define Ht+1 = fθ1(Xt, Zt, Ht) for some independent noise source Zt, and we insist that Xt cannot be recovered exactly from Ht+1. The advantage of that formulation is that one can directly back-propagated the reconstruction loglikelihood logP (X1 = x0|H1 = f(X0, Z0, H0)) into all the parameters of f and g (a similar idea was independently proposed in (Kingma, 2013) and also exploited in (Rezende et al., 2014)).\nFor the rest of this paper, we will use such a deterministic function f instead of having f refer to a probability density function. We apologize if it causes any confusion.\nIn the setting described at the beginning of section 3, the function playing the role of the “encoder” was fixed for the purpose of the theorem, and we showed that learning only the “decoder” part (but a sufficiently expressive one) sufficed. In this setting we are learning both, for which some care is needed.\nOne problem would be if the created Markov chain failed to converge to a stationary distribution. Another such problem could be that the function f(Xt, Zt, Ht) learned would try to ignore the noise Zt, or not make the best use out of it. In that case, the reconstruction distribution would simply converge to a Dirac at the inputX . This is the analogue of the constraint on auto-encoders that is needed to prevent them from learning the identity function. Here, we must design the family from which f and g are learned such that when the noise Z is injected, there are always several possible values of X that could have been the correct original input.\nAnother extreme case to think about is when f(X,Z,H) is overwhelmed by the noise and has lost all information about X . In that case the theorems are still applicable while giving uninteresting results: the learner must capture the full distribution of X in Pθ2(X|H) because the latter is now equivalent to Pθ2(X), since f(X,Z,H) no longer contains information about X . This illustrates that when the noise is large, the reconstruction distribution (parametrized by θ2) will need to have the expressive power to represent multiple modes. Otherwise, the reconstruction will tend to capture an average output, which would visually look like a fuzzy combination of actual modes. In the experiments performed here, we have only considered unimodal reconstruction distributions (with factorized outputs), because we expect that even if P (X|H) is not unimodal, it would be dominated by a single mode when the noise level is small. However, future work should investigate multimodal alternatives.\nA related element to keep in mind is that one should pick the family of conditional distributions Pθ2(X|H) so that one can sample from them and one can easily train them when given (X,H) pairs, e.g., by maximum likelihood."
    }, {
      "heading" : "3.4 Handling missing inputs or structured output",
      "text" : "In general, a simple way to deal with missing inputs is to clamp the observed inputs and then apply the Markov chain with the constraint that the observed inputs are fixed and not resampled at each time step, whereas the unobserved inputs are resampled each time, conditioned on the clamped inputs.\nOne readily proves that this procedure gives rise to sampling from the appropriate conditional distribution:\nProposition 1. If a subset x(s) of the elements of X is kept fixed (not resampled) while the remainderX(−s) is updated stochastically during the Markov chain of Theorem 2, but using P (Xt|Ht, X(s)t = x(s)), then the asymptotic distribution πn of the Markov chain produces samples of X(−s) from the conditional distribution πn(X(−s)|X(s) = x(s)).\nProof. Without constraint, we know that at convergence, the chain produces samples of πn. A subset of these samples satisfies the conditionX = x(s), and these constrained samples could equally have been produced by samplingXt from\nPθ2(Xt|fθ1(Xt−1, Zt−1, Ht−1), X (s) t = X (s)),\nby definition of conditional distribution. Therefore, at convergence of the chain, we have that using the constrained distribution P (Xt|f(Xt−1, Zt−1, Ht−1), X(s)t = x(s)) produces a sample from πn under the condition X(s) = x(s).\nPractically, it means that we must choose an output (reconstruction) distribution from which it is not only easy to sample from, but also from which it is easy to sample a subset of the variables in the vector X conditioned on the rest being known. In the experiments below, we used a factorial distribution for the reconstruction, from which it is trivial to sample conditionally a subset of the input variables. In general (with non-factorial output distributions) one must use the proper conditional for the theorem to apply, i.e., it is not sufficient to clamp the inputs, one must also sample the reconstructions from the appropriate conditional distribution (conditioning on the clamped values).\nThis method of dealing with missing inputs can be immediately applied to structured outputs. If X(s) is viewed as an “input” and X(−s) as an “output”, then sampling from X\n(−s) t+1 ∼ P (X(−s)|f((X(s), X (−s) t ), Zt, Ht), X\n(s)) will converge to estimators of P (X(−s)|X(s)). This still requires good choices of the parametrization (for f as well as for the conditional probability P ), but the advantages of this approach are that there is no approximate inference of latent variables and the learner is trained with respect to simpler conditional probabilities: in the limit of small noise, we conjecture that these conditional probabilities can be well approximated by unimodal distributions. Theoretical evidence comes from Alain & Bengio (2013): when the amount of corruption noise converges to 0 and the input variables have a smooth continuous density, then a unimodal Gaussian reconstruction density suffices to fully capture the joint distribution.\nX0\nH1\nX1\nH2 H3\nX2\nH0\nW1 W1 W1\nW2 W2 W2\nW3 W3\nX1\nH1\nX2 X3\nH2 H3\nW3\nX0data target target target\nFigure 3. Left: Generic GSN Markov chain with state variables Xt and Ht. Right: GSN Markov chain inspired by the unfolded computational graph of the Deep Boltzmann Machine Gibbs sampling process, but with backprop-able stochastic units at each layer. The training example X = x0 starts the chain. Either odd or even layers are stochastically updated at each step, and all downward weight matrices are fixed to the transpose of the corresponding upward weight matrix. All xt’s are corrupted by salt-and-pepper noise before entering the graph. Each xt for t > 0 is obtained by sampling from the reconstruction distribution for that step, Pθ2(Xt|Ht). The walkback training objective is the sum over all steps of log-likelihoods of target X = x0 under the reconstruction distribution. In the special case of a unimodal Gaussian reconstruction distribution, maximizing the likelihood is equivalent to minimizing reconstruction error; in general one trains to maximum likelihood, not simply minimum reconstruction error."
    }, {
      "heading" : "3.5 Dependency Networks as GSNs",
      "text" : "Dependency networks (Heckerman et al., 2000) are models in which one estimates conditionals Pi(xi|x−i), where x−i denotes x \\ xi, i.e., the set of variables other than the i-th one, xi. Note that each Pi may be parametrized separately, thus not guaranteeing that there exists a joint of which they are the conditionals. Instead of the ordered pseudo-Gibbs sampler defined in Heckerman et al. (2000), which resamples each variable xi in the order x1, x2, . . ., we can view dependency networks in the GSN framework by defining a proper Markov chain in which at each step one randomly chooses which variable to resample. The corruption process therefore just consists of H = f(X,Z) = X−s where X−s is the complement of Xs, with s a randomly chosen subset of elements of X (possibly constrained to be of size 1). Furthermore, we parametrize the reconstruction distribution as Pθ2(X = x|H) = δx−s=X−sPθ2,s(Xs = xs|x−s) where the estimated conditionals Pθ2,s(Xs = xs|x−s) are not constrained to be consistent conditionals of some joint distribution over all of X .\nProposition 2. If the above GSN Markov chain has a stationary distribution, then the dependency network defines a joint distribution (which is that stationary distribution), which does not have to be known in closed form. Furthermore, if the conditionals are consistent estimators of the ground truth conditionals, then that stationary distribution is a consistent estimator of the ground truth joint distribution.\nThe proposition can be proven by immediate application of Theorem 1 from Bengio et al. (2013c) with the above definitions of the GSN. This joint stationary distribution can exist even if the conditionals are not consistent. To show that, assume that some choice of (possibly inconsistent)\nconditionals gives rise to a stationary distribution π. Now let us consider the set of all conditionals (not necessarily consistent) that could have given rise to that π. Clearly, the conditionals derived from π is part of that set, but there are infinitely many others (a simple counting argument shows that the fixed point equation of π introduces fewer constraints than the number of degrees of freedom that define the conditionals). To better understand why the ordered pseudo-Gibbs chain does not benefit from the same properties, we can consider an extended case by adding an extra component of the state X , being the index of the next variable to resample. In that case, the Markov chain associated with the ordered pseudo-Gibbs procedure would be periodic, thus violating the ergodicity assumption of the theorem. However, by introducing randomness in the choice of which variable(s) to resample next, we obtain aperiodicity and ergodicity, yielding as stationary distribution a mixture over all possible resampling orders. These results also show in a novel way (see e.g. Hyvärinen (2006) for earlier results) that training by pseudolikelihood or generalized pseudolikelihood provides a consistent estimator of the associated joint, so long as the GSN Markov chain defined above is ergodic. This result can be applied to show that the multi-prediction deep Boltzmann machine (MPDBM) training procedure introduced by Goodfellow et al. (2013) also corresponds to a GSN. This has been exploited in order to obtain much better samples using the associated GSN Markov chain than by sampling from the corresponding DBM (Goodfellow et al., 2013). Another interesting conclusion that one can draw from this paper and its GSN interpretation is that state-of-the-art classification error can thereby be obtained: 0.91% on MNIST without fine-tuning (best comparable previous DBM results was well above 1%) and 10.6% on permutation-invariant NORB (best previous DBM results was 10.8%)."
    }, {
      "heading" : "4 Experimental Example of GSN",
      "text" : "The theoretical results on Generative Stochastic Networks (GSNs) open for exploration a large class of possible parametrizations which will share the property that they can capture the underlying data distribution through the GSN Markov chain. What parametrizations will work well? Where and how should one inject noise? We present results of preliminary experiments with specific selections for each of these choices, but the reader should keep in mind that the space of possibilities is vast.\nAs a conservative starting point, we propose to explore families of parametrizations which are similar to existing deep stochastic architectures such as the Deep Boltzmann Machine (DBM) (Salakhutdinov & Hinton, 2009). Basically, the idea is to construct a computational graph that is similar to the computational graph for Gibbs sampling or variational inference in Deep Boltzmann Machines. However, we have to diverge a bit from these architectures in order to accommodate the desirable property that it will be possible to back-propagate the gradient of reconstruction log-likelihood with respect to the parameters θ1 and θ2. Since the gradient of a binary stochastic unit is 0 almost everywhere, we have to consider related alternatives. An interesting source of inspiration regarding this question is a recent paper on estimating or propagating gradients through stochastic neurons (Bengio, 2013). Here we consider the following stochastic non-linearities: hi = ηout + tanh(ηin + ai) where ai is the linear activation for unit i (an affine transformation applied to the input of the unit, coming from the layer below, the layer above, or both) and ηin and ηout are zero-mean Gaussian noises.\nTo emulate a sampling procedure similar to Boltzmann machines in which the filled-in missing values can depend on the representations at the top level, the computational graph allows information to propagate both upwards (from input to higher levels) and downwards, giving rise to the computational graph structure illustrated in Figure 3, which is similar to that explored for deterministic recurrent auto-encoders (Seung, 1998; Behnke, 2001; Savard, 2011). Downward weight matrices have been fixed to the transpose of corresponding upward weight matrices.\nThe walkback algorithm was proposed in Bengio et al. (2013c) to make training of generalized denoising autoencoders (a special case of the models studied here) more efficient. The basic idea is that the reconstruction is obtained after not one but several steps of the sampling Markov chain. In this context it simply means that the computational graph from X to a reconstruction probability actually involves generating intermediate samples as if we were running the Markov chain starting at X . In the experiments, the graph was unfolded so that 2D sampled reconstructions would be produced, where D is the depth\n(number of hidden layers). The training loss is the sum of the reconstruction negative log-likelihoods (of target X) over all those reconstruction steps.\nExperiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013b). Networks with 2 and 3 hidden layers were evaluated and compared to regular denoising auto-encoders (just 1 hidden layer, i.e., the computational graph separates into separate ones for each reconstruction step in the walkback algorithm). They all have tanh hidden units and pre- and post-activation Gaussian noise of standard deviation 2, applied to all hidden layers except the first. In addition, at each step in the chain, the input (or the resampled Xt) is corrupted with salt-and-pepper noise of 40% (i.e., 40% of the pixels are corrupted, and replaced with a 0 or a 1 with probability 0.5). Training is over 100 to 600 epochs at most, with good results obtained after around 100 epochs. Hidden layer sizes vary between 1000 and 1500 depending on the experiments, and a learning rate of 0.25 and momentum of 0.5 were selected to approximately minimize the reconstruction negative log-likelihood. The learning rate is reduced multiplicatively by 0.99 after each epoch. Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10000 consecutively generated samples (using the real-valued mean-field reconstructions as the training data for the Parzen density estimator). This can be seen as an lower bound on the true log-likelihood, with the bound converging to the true likelihood as we consider more samples and appropriately set the smoothing parameter of the Parzen estimator1 Results are summarized in Table 1. The test set Parzen log-likelihood bound was not used to select among model architectures, but visual inspection of samples generated did guide the preliminary search reported here. Optimization hyper-parameters (learning rate, momentum, and learning rate reduction schedule) were selected based on the reconstruction loglikelihood training objective. The Parzen log-likelihood bound obtained with a two-layer model on MNIST is 214 (± standard error of 1.1), while the log-likelihood bound obtained by a single-layer model (regular denoising auto-encoder, DAE in the table) is substantially worse, at -152±2.2. In comparison, Bengio et al. (2013b) report a log-likelihood bound of -244±54 for RBMs and 138±2 for a 2-hidden layer DBN, using the same setup. We have also evaluated a 3-hidden layer DBM (Salakhutdinov &\n1However, in this paper, to be consistent with the numbers given in Bengio et al. (2013b) we used a Gaussian Parzen density, which makes the numbers not comparable with the AIS loglikelihood upper bounds for binarized images reported in other papers for the same data.\nHinton, 2009), using the weights provided by the author, and obtained a Parzen log-likelihood bound of 32±2. See http://www.mit.edu/˜rsalakhu/DBM.html for details.\nInterestingly, the GSN and the DBN-2 actually perform slightly better than when using samples directly coming from the MNIST training set, maybe because they generate more “prototypical” samples (we are using mean-field outputs).\nFigure 4 shows a single run of consecutive samples from\nthis trained model (see Figure 6 for longer runs), illustrating that it mixes quite well (better than RBMs) and produces rather sharp digit images. The figure shows that it can also stochastically complete missing values: the left half of the image was initialized to random pixels and the right side was clamped to an MNIST image. The Markov chain explores plausible variations of the completion according to the trained conditional distribution.\nA smaller set of experiments was also run on TFD, yielding a test set Parzen log-likelihood bound of 1890 ±29. The setup is exactly the same and was not tuned after the MNIST experiments. A DBN-2 yields a Parzen loglikelihood bound of 1908 ±66, which is indistinguishable statistically, while an RBM yields 604 ± 15. One out of every 2 consecutive samples from the GSN-3 model are shown in Figure 5 (see Figure 8 for longer runs without skips)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have introduced a new approach to training generative models, called Generative Stochastic Networks (GSN), that is an alternative to maximum likelihood, with the objective of avoiding the intractable marginalizations and the danger of poor approximations of these marginalizations. The training procedure is more similar to function approximation than to unsupervised learning because the reconstruction distribution is simpler than the data distribution, often unimodal (provably so in the limit of very small noise).\nThis makes it possible to train unsupervised models that capture the data-generating distribution simply using backprop and gradient descent (in a computational graph that includes noise injection). The proposed theoretical results state that under mild conditions (in particular that the noise injected in the networks prevents perfect reconstruction), training the model to denoise and reconstruct its observations (through a powerful family of reconstruction distributions) suffices to capture the data-generating distribution through a simple Markov chain. Another way to put it is that we are training the transition operator of a Markov chain whose stationary distribution estimates the data distribution, and it turns out that this is a much easier learning problem because the normalization constant for this conditional distribution is generally dominated by fewer modes. These theoretical results are extended to the case where the corruption is local but still allows the chain to mix and to the case where some inputs are missing or constrained (thus allowing to sample from a conditional distribution on a subset of the observed variables or to learned structured output models). The GSN framework is shown to lend to dependency networks a valid estimator of the joint distribution of the observed variables even when the learned conditionals are not consistent, also allowing to prove consistency of generalized pseudolikelihood training, associated with the stationary distribution of the corresponding GSN (that randomly chooses a subset of variables and then resamples it). Experiments have been conducted to validate the theory, in the case where the GSN architecture emulates the Gibbs sampling process of a Deep Boltzmann Machine, on two datasets. A quantitative evaluation of the samples confirms that the training procedure works very well (in this case allowing us to train a deep generative model without layerwise pretraining) and can be used to perform conditional sampling of a subset of variables given the rest."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to acknowledge the stimulating discussions and help from Vincent Dumoulin, Pascal Vincent, Yao Li, Aaron Courville, Ian Goodfellow, and Hod Lipson, as well as funding from NSERC, CIFAR (YB is a CIFAR\nSenior Fellow), NASA (JY is a Space Technology Research Fellow), and the Canada Research Chairs."
    }, {
      "heading" : "A Supplemental Experimental Results",
      "text" : "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013c). Theorem 2 requires H0 to have the same distribution as H1 (given X0) during training, and the main paper suggests a way to achieve this by initializing each training chain with H0 set to the previous value of H1 when the same example X0 was shown. However, we did not implement that procedure in the experiments below, so that is left for future work to explore.\nNetworks with 2 and 3 hidden layers were evaluated and compared to regular denoising auto-encoders (just 1 hidden layer, i.e., the computational graph separates into separate ones for each reconstruction step in the walkback algorithm). They all have tanh hidden units and pre- and post-activation Gaussian noise of standard deviation 2, applied to all hidden layers except the first. In addition, at each step in the chain, the input (or the resampled Xt) is corrupted with salt-and-pepper noise of 40% (i.e., 40% of the pixels are corrupted, and replaced with a 0 or a 1 with probability 0.5). Training is over 100 to 600 epochs at most, with good results obtained after around 100 epochs, using stochastic gradient descent (minibatch size = 1). Hidden layer sizes vary between 1000 and 1500 depending on the experiments, and a learning rate of 0.25 and momentum of 0.5 were selected to approximately minimize the reconstruction negative log-likelihood. The learning rate is reduced multiplicatively by 0.99 after each epoch. Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10000 consecutively generated samples (using the real-valued mean-field reconstructions as the training data for the Parzen density estimator). This can be seen as an lower bound on the true log-likelihood, with the bound converging to the true likelihood as we consider more\nsamples and appropriately set the smoothing parameter of the Parzen estimator2. Results are summarized in Table 1. The test set Parzen log-likelihood bound was not used to select among model architectures, but visual inspection of samples generated did guide the preliminary search reported here. Optimization hyper-parameters (learning rate, momentum, and learning rate reduction schedule) were selected based on the reconstruction log-likelihood training objective. The Parzen log-likelihood bound obtained with a two-layer model on MNIST is 214 (± standard error of 1.1), while the log-likelihood bound obtained by a single-layer model (regular denoising auto-encoder, DAE in the table) is substantially worse, at -152±2.2. In comparison, Bengio et al. (2013c) report a log-likelihood bound of -244±54 for RBMs and 138±2 for a 2-hidden layer DBN, using the same setup. We have also evaluated a 3-hidden layer DBM (Salakhutdinov & Hinton, 2009), using the weights provided by the author, and obtained a Parzen log-likelihood bound of 32±2. See http://www.mit.edu/˜rsalakhu/DBM.html for details. Figure 6 shows two runs of consecutive samples from this trained model, illustrating that it mixes quite well (better than RBMs) and produces rather sharp digit images. The figure shows that it can also stochastically complete missing values: the left half of the image was initialized to random pixels and the right side was clamped to an MNIST image. The Markov chain explores plausible variations of the completion according to the trained conditional distribution.\nA smaller set of experiments was also run on TFD, yielding for a GSN a test set Parzen log-likelihood bound of 1890 ±29. The setup is exactly the same and was not tuned after the MNIST experiments. A DBN-2 yields a Parzen loglikelihood bound of 1908 ±66, which is undistinguishable statistically, while an RBM yields 604 ± 15. A run of consecutive samples from the GSN-3 model are shown in Figure 8. Figure 7 shows consecutive samples obtained early on during training, after only 5 and 25 epochs respectively, illustrating the fast convergence of the training procedure."
    } ],
    "references" : [ {
      "title" : "2013c) we used a Gaussian Parzen density, which (in addition to being lower rather than upper bounds) makes the numbers not comparable with the AIS log-likelihood upper bounds for binarized images reported in some papers for the same",
      "author" : [ "Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bengio,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio",
      "year" : 2013
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle" ],
      "venue" : "In NIPS’2006,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Bengio", "Yoshua" ],
      "venue" : "Now Publishers,",
      "citeRegEx" : "Bengio and Yoshua.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio and Yoshua.",
      "year" : 2009
    }, {
      "title" : "Estimating or propagating gradients through stochastic neurons",
      "author" : [ "Bengio", "Yoshua" ],
      "venue" : "Technical Report arXiv:1305.2982, Universite de Montreal,",
      "citeRegEx" : "Bengio and Yoshua.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio and Yoshua.",
      "year" : 2013
    }, {
      "title" : "Unsupervised feature learning and deep learning: A review and new perspectives",
      "author" : [ "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI),",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Better mixing via deep representations",
      "author" : [ "Bengio", "Yoshua", "Mesnil", "Grégoire", "Dauphin", "Yann", "Rifai", "Salah" ],
      "venue" : "In ICML’13,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Generalized denoising auto-encoders as generative models. In NIPS26",
      "author" : [ "Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal" ],
      "venue" : "Nips Foundation,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Quickly generating representative samples from an RBM-derived process",
      "author" : [ "Breuleux", "Olivier", "Bengio", "Yoshua", "Vincent", "Pascal" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Breuleux et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Breuleux et al\\.",
      "year" : 2011
    }, {
      "title" : "Comparison of perturbation bounds for the stationary distribution of a markov chain",
      "author" : [ "Cho", "Grace E", "Meyer", "Carl D", "Carl", "D. Meyer" ],
      "venue" : "Linear Algebra Appl,",
      "citeRegEx" : "Cho et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2000
    }, {
      "title" : "Phone recognition with the mean-covariance restricted Boltzmann machine",
      "author" : [ "Dahl", "George E", "Ranzato", "Marc’Aurelio", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E" ],
      "venue" : "In NIPS’2010,",
      "citeRegEx" : "Dahl et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dahl et al\\.",
      "year" : 2010
    }, {
      "title" : "Binary coding of speech spectrograms using a deep auto-encoder",
      "author" : [ "L. Deng", "M. Seltzer", "D. Yu", "A. Acero", "A. Mohamed", "G. Hinton" ],
      "venue" : "In Interspeech",
      "citeRegEx" : "Deng et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2010
    }, {
      "title" : "Multi-prediction deep Boltzmann machines. In NIPS26",
      "author" : [ "Goodfellow", "Ian J", "Mirza", "Mehdi", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "Nips Foundation,",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2013
    }, {
      "title" : "Dependency networks for inference, collaborative filtering, and data visualization",
      "author" : [ "Heckerman", "David", "Chickering", "David Maxwell", "Meek", "Christopher", "Rounthwaite", "Robert", "Kadie", "Carl" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Heckerman et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Heckerman et al\\.",
      "year" : 2000
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee Whye" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Consistency of pseudolikelihood estimation of fully visible boltzmann machines",
      "author" : [ "Hyvärinen", "Aapo" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hyvärinen and Aapo.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hyvärinen and Aapo.",
      "year" : 2006
    }, {
      "title" : "Fast gradient-based inference with continuous latent variable models in auxiliary form",
      "author" : [ "Kingma", "Diederik P" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Kingma and P.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and P.",
      "year" : 2013
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "In NIPS’2012",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient sparse coding algorithms",
      "author" : [ "Lee", "Honglak", "Battle", "Alexis", "Raina", "Rajat", "Ng", "Andrew" ],
      "venue" : "In NIPS’06,",
      "citeRegEx" : "Lee et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2007
    }, {
      "title" : "Texture modeling with convolutional spikeand-slab RBMs and deep extensions",
      "author" : [ "Luo", "Heng", "Carrier", "Pierre Luc", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "In AISTATS’2013,",
      "citeRegEx" : "Luo et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2013
    }, {
      "title" : "Sum-product networks: A new deep architecture",
      "author" : [ "Poon", "Hoifung", "Domingos", "Pedro" ],
      "venue" : "In UAI’2011, Barcelona,",
      "citeRegEx" : "Poon et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Poon et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient learning of sparse representations with an energybased model",
      "author" : [ "M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun" ],
      "venue" : "In NIPS’2006,",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2007
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "A generative process for sampling contractive auto-encoders",
      "author" : [ "Rifai", "Salah", "Bengio", "Yoshua", "Dauphin", "Yann", "Vincent", "Pascal" ],
      "venue" : "In ICML’12,",
      "citeRegEx" : "Rifai et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rifai et al\\.",
      "year" : 2012
    }, {
      "title" : "Réseaux de neurones à relaxation entraı̂nés par critère d’autoencodeur débruitant",
      "author" : [ "Savard", "François" ],
      "venue" : "Master’s thesis, U. Montréal,",
      "citeRegEx" : "Savard and François.,? \\Q2011\\E",
      "shortCiteRegEx" : "Savard and François.",
      "year" : 2011
    }, {
      "title" : "Perturbation theory and finite markov chains",
      "author" : [ "Schweitzer", "Paul J" ],
      "venue" : "Journal of Applied Probability,",
      "citeRegEx" : "Schweitzer and J.,? \\Q1968\\E",
      "shortCiteRegEx" : "Schweitzer and J.",
      "year" : 1968
    }, {
      "title" : "Conversational speech transcription using context-dependent deep neural networks",
      "author" : [ "Seide", "Frank", "Li", "Gang", "Yu", "Dong" ],
      "venue" : "In Interspeech",
      "citeRegEx" : "Seide et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Seide et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning continuous attractors in recurrent networks",
      "author" : [ "Seung", "Sebastian H" ],
      "venue" : "In NIPS’97,",
      "citeRegEx" : "Seung and H.,? \\Q1998\\E",
      "shortCiteRegEx" : "Seung and H.",
      "year" : 1998
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine" ],
      "venue" : "ICML",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "(2013a) for reviews) grew from breakthroughs in unsupervised learning of representations, based mostly on the Restricted Boltzmann Machine (RBM) (Hinton et al., 2006), auto-encoder variants (Bengio et al.",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : ", 2006), auto-encoder variants (Bengio et al., 2007; Vincent et al., 2008), and sparse coding variants (Lee et al.",
      "startOffset" : 31,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : ", 2006), auto-encoder variants (Bengio et al., 2007; Vincent et al., 2008), and sparse coding variants (Lee et al.",
      "startOffset" : 31,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : ", 2008), and sparse coding variants (Lee et al., 2007; Ranzato et al., 2007).",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 21,
      "context" : ", 2008), and sparse coding variants (Lee et al., 2007; Ranzato et al., 2007).",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al.",
      "startOffset" : 162,
      "endOffset" : 220
    }, {
      "referenceID" : 10,
      "context" : "However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al.",
      "startOffset" : 162,
      "endOffset" : 220
    }, {
      "referenceID" : 26,
      "context" : "However, the most impressive recent results have been obtained with purely supervised learning techniques for deep networks, in particular for speech recognition (Dahl et al., 2010; Deng et al., 2010; Seide et al., 2011) and object recognition (Krizhevsky et al.",
      "startOffset" : 162,
      "endOffset" : 220
    }, {
      "referenceID" : 17,
      "context" : ", 2011) and object recognition (Krizhevsky et al., 2012).",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "The latest breakthrough in object recognition (Krizhevsky et al., 2012) was achieved with fairly deep convolutional networks with a form of noise injection in the input and hidden layers during training, called dropout (Hinton et al.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : ", 2012) was achieved with fairly deep convolutional networks with a form of noise injection in the input and hidden layers during training, called dropout (Hinton et al., 2012).",
      "startOffset" : 155,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : "Research in deep learning (see Bengio (2009) and Bengio et al.",
      "startOffset" : 31,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Research in deep learning (see Bengio (2009) and Bengio et al. (2013a) for reviews) grew from breakthroughs in unsupervised learning of representations, based mostly on the Restricted Boltzmann Machine (RBM) (Hinton et al.",
      "startOffset" : 31,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, progress with deep unsupervised architectures has been slower, with the established options with a probabilistic footing being the Deep Belief Network (DBN) (Hinton et al., 2006) and the Deep Boltzmann Machine (DBM) (Salakhutdinov & Hinton, 2009).",
      "startOffset" : 176,
      "endOffset" : 197
    }, {
      "referenceID" : 11,
      "context" : "Although single-layer unsupervised learners are fairly well developed and used to pre-train these deep models, jointly training all the layers with respect to a single unsupervised criterion remains a challenge, with a few techniques arising to reduce that difficulty (Montavon & Muller, 2012; Goodfellow et al., 2013).",
      "startOffset" : 268,
      "endOffset" : 318
    }, {
      "referenceID" : 14,
      "context" : "For example, one could use a convolutional architecture with max-pooling for parametric parsimony and computational efficiency, or dropout (Hinton et al., 2012) to prevent co-adaptation of hidden representations.",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "We strengthen the consistency theorems introduced in Bengio et al. (2013c) by showing that the corruption distribution may be purely local, not requiring support over the whole domain of the visible variables (Section 3.",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "Because the Markov Chain is defined over a state (X,h) that includes latent variables, we reap the dual advantage of more powerful models for a given number of parameters and better mixing in the chain as we add noise to variables representing higher-level information, first suggested by the results obtained by Bengio et al. (2013b) and Luo et al.",
      "startOffset" : 313,
      "endOffset" : 335
    }, {
      "referenceID" : 0,
      "context" : "Because the Markov Chain is defined over a state (X,h) that includes latent variables, we reap the dual advantage of more powerful models for a given number of parameters and better mixing in the chain as we add noise to variables representing higher-level information, first suggested by the results obtained by Bengio et al. (2013b) and Luo et al. (2013). The experimental results show that such a model with latent states indeed mixes better than",
      "startOffset" : 313,
      "endOffset" : 357
    }, {
      "referenceID" : 12,
      "context" : "6 – Dependency networks: Finally, an unexpected result falls out of the GSN theory: it allows us to provide a novel justification for dependency networks (Heckerman et al., 2000) and for the first time define a proper joint distribution between all the visible variables that is learned by such models (Section 3.",
      "startOffset" : 154,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "For example consider the denoising transitions studied by Bengio et al. (2013c) and illustrated in Figure 1, where x̃t−1 is a stochastically corrupted version of xt−1 and we learn the denoising distribution P (x|x̃).",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "Alain & Bengio (2013) showed that denoising autoencoders with small Gaussian corruption and squared error loss estimated the score (derivative of the log-density with respect to the input) of continuous observed random variables.",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Alain & Bengio (2013) showed that denoising autoencoders with small Gaussian corruption and squared error loss estimated the score (derivative of the log-density with respect to the input) of continuous observed random variables. More recently, Bengio et al. (2013c) generalized this to arbitrary variables (discrete, continuous or both), arbitrary corruption (not necessarily asymptotically small), and arbitrary loss function (so long as they can be seen as a loglikelihood).",
      "startOffset" : 8,
      "endOffset" : 267
    }, {
      "referenceID" : 0,
      "context" : "Beyond proving that P (X|X̃) is sufficient to reconstruct the data density, Bengio et al. (2013c) also demonstrated a method of sampling from a learned, parametrized model of the density, Pθ(X), by running a Markov chain that alternately adds noise using C(X̃|X) and denoises by sampling from the learned Pθ(X|X̃), which is trained to approximate the true P (X|X̃).",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "The main theorem in Bengio et al. (2013c), reproduced below, requires that the Markov chain be ergodic.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "The following theorem is proven by Bengio et al. (2013c).",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "A good overview of results from perturbation theory discussing stationary distributions in finite state Markov chains can be found in (Cho et al., 2000).",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : "The advantage of that formulation is that one can directly back-propagated the reconstruction loglikelihood logP (X1 = x0|H1 = f(X0, Z0, H0)) into all the parameters of f and g (a similar idea was independently proposed in (Kingma, 2013) and also exploited in (Rezende et al., 2014)).",
      "startOffset" : 260,
      "endOffset" : 282
    }, {
      "referenceID" : 0,
      "context" : "Theoretical evidence comes from Alain & Bengio (2013): when the amount of corruption noise converges to 0 and the input variables have a smooth continuous density, then a unimodal Gaussian reconstruction density suffices to fully capture the joint distribution.",
      "startOffset" : 40,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "Dependency networks (Heckerman et al., 2000) are models in which one estimates conditionals Pi(xi|x−i), where x−i denotes x \\ xi, i.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "Dependency networks (Heckerman et al., 2000) are models in which one estimates conditionals Pi(xi|x−i), where x−i denotes x \\ xi, i.e., the set of variables other than the i-th one, xi. Note that each Pi may be parametrized separately, thus not guaranteeing that there exists a joint of which they are the conditionals. Instead of the ordered pseudo-Gibbs sampler defined in Heckerman et al. (2000), which resamples each variable xi in the order x1, x2, .",
      "startOffset" : 21,
      "endOffset" : 399
    }, {
      "referenceID" : 11,
      "context" : "This has been exploited in order to obtain much better samples using the associated GSN Markov chain than by sampling from the corresponding DBM (Goodfellow et al., 2013).",
      "startOffset" : 145,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "The proposition can be proven by immediate application of Theorem 1 from Bengio et al. (2013c) with the above definitions of the GSN.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "The proposition can be proven by immediate application of Theorem 1 from Bengio et al. (2013c) with the above definitions of the GSN. This joint stationary distribution can exist even if the conditionals are not consistent. To show that, assume that some choice of (possibly inconsistent) conditionals gives rise to a stationary distribution π. Now let us consider the set of all conditionals (not necessarily consistent) that could have given rise to that π. Clearly, the conditionals derived from π is part of that set, but there are infinitely many others (a simple counting argument shows that the fixed point equation of π introduces fewer constraints than the number of degrees of freedom that define the conditionals). To better understand why the ordered pseudo-Gibbs chain does not benefit from the same properties, we can consider an extended case by adding an extra component of the state X , being the index of the next variable to resample. In that case, the Markov chain associated with the ordered pseudo-Gibbs procedure would be periodic, thus violating the ergodicity assumption of the theorem. However, by introducing randomness in the choice of which variable(s) to resample next, we obtain aperiodicity and ergodicity, yielding as stationary distribution a mixture over all possible resampling orders. These results also show in a novel way (see e.g. Hyvärinen (2006) for earlier results) that training by pseudolikelihood or generalized pseudolikelihood provides a consistent estimator of the associated joint, so long as the GSN Markov chain defined above is ergodic.",
      "startOffset" : 73,
      "endOffset" : 1388
    }, {
      "referenceID" : 0,
      "context" : "The proposition can be proven by immediate application of Theorem 1 from Bengio et al. (2013c) with the above definitions of the GSN. This joint stationary distribution can exist even if the conditionals are not consistent. To show that, assume that some choice of (possibly inconsistent) conditionals gives rise to a stationary distribution π. Now let us consider the set of all conditionals (not necessarily consistent) that could have given rise to that π. Clearly, the conditionals derived from π is part of that set, but there are infinitely many others (a simple counting argument shows that the fixed point equation of π introduces fewer constraints than the number of degrees of freedom that define the conditionals). To better understand why the ordered pseudo-Gibbs chain does not benefit from the same properties, we can consider an extended case by adding an extra component of the state X , being the index of the next variable to resample. In that case, the Markov chain associated with the ordered pseudo-Gibbs procedure would be periodic, thus violating the ergodicity assumption of the theorem. However, by introducing randomness in the choice of which variable(s) to resample next, we obtain aperiodicity and ergodicity, yielding as stationary distribution a mixture over all possible resampling orders. These results also show in a novel way (see e.g. Hyvärinen (2006) for earlier results) that training by pseudolikelihood or generalized pseudolikelihood provides a consistent estimator of the associated joint, so long as the GSN Markov chain defined above is ergodic. This result can be applied to show that the multi-prediction deep Boltzmann machine (MPDBM) training procedure introduced by Goodfellow et al. (2013) also corresponds to a GSN.",
      "startOffset" : 73,
      "endOffset" : 1740
    }, {
      "referenceID" : 0,
      "context" : "An interesting source of inspiration regarding this question is a recent paper on estimating or propagating gradients through stochastic neurons (Bengio, 2013).",
      "startOffset" : 145,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "The walkback algorithm was proposed in Bengio et al. (2013c) to make training of generalized denoising autoencoders (a special case of the models studied here) more efficient.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013b). Networks with 2 and 3 hidden layers were evaluated and compared to regular denoising auto-encoders (just 1 hidden layer, i.",
      "startOffset" : 147,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013b). Networks with 2 and 3 hidden layers were evaluated and compared to regular denoising auto-encoders (just 1 hidden layer, i.e., the computational graph separates into separate ones for each reconstruction step in the walkback algorithm). They all have tanh hidden units and pre- and post-activation Gaussian noise of standard deviation 2, applied to all hidden layers except the first. In addition, at each step in the chain, the input (or the resampled Xt) is corrupted with salt-and-pepper noise of 40% (i.e., 40% of the pixels are corrupted, and replaced with a 0 or a 1 with probability 0.5). Training is over 100 to 600 epochs at most, with good results obtained after around 100 epochs. Hidden layer sizes vary between 1000 and 1500 depending on the experiments, and a learning rate of 0.25 and momentum of 0.5 were selected to approximately minimize the reconstruction negative log-likelihood. The learning rate is reduced multiplicatively by 0.99 after each epoch. Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10000 consecutively generated samples (using the real-valued mean-field reconstructions as the training data for the Parzen density estimator).",
      "startOffset" : 147,
      "endOffset" : 1175
    }, {
      "referenceID" : 0,
      "context" : "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013b). Networks with 2 and 3 hidden layers were evaluated and compared to regular denoising auto-encoders (just 1 hidden layer, i.e., the computational graph separates into separate ones for each reconstruction step in the walkback algorithm). They all have tanh hidden units and pre- and post-activation Gaussian noise of standard deviation 2, applied to all hidden layers except the first. In addition, at each step in the chain, the input (or the resampled Xt) is corrupted with salt-and-pepper noise of 40% (i.e., 40% of the pixels are corrupted, and replaced with a 0 or a 1 with probability 0.5). Training is over 100 to 600 epochs at most, with good results obtained after around 100 epochs. Hidden layer sizes vary between 1000 and 1500 depending on the experiments, and a learning rate of 0.25 and momentum of 0.5 were selected to approximately minimize the reconstruction negative log-likelihood. The learning rate is reduced multiplicatively by 0.99 after each epoch. Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10000 consecutively generated samples (using the real-valued mean-field reconstructions as the training data for the Parzen density estimator). This can be seen as an lower bound on the true log-likelihood, with the bound converging to the true likelihood as we consider more samples and appropriately set the smoothing parameter of the Parzen estimator1 Results are summarized in Table 1. The test set Parzen log-likelihood bound was not used to select among model architectures, but visual inspection of samples generated did guide the preliminary search reported here. Optimization hyper-parameters (learning rate, momentum, and learning rate reduction schedule) were selected based on the reconstruction loglikelihood training objective. The Parzen log-likelihood bound obtained with a two-layer model on MNIST is 214 (± standard error of 1.1), while the log-likelihood bound obtained by a single-layer model (regular denoising auto-encoder, DAE in the table) is substantially worse, at -152±2.2. In comparison, Bengio et al. (2013b) report a log-likelihood bound of -244±54 for RBMs and 138±2 for a 2-hidden layer DBN, using the same setup.",
      "startOffset" : 147,
      "endOffset" : 2373
    }, {
      "referenceID" : 0,
      "context" : "However, in this paper, to be consistent with the numbers given in Bengio et al. (2013b) we used a Gaussian Parzen density, which makes the numbers not comparable with the AIS loglikelihood upper bounds for binarized images reported in other papers for the same data.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "Gaussian mixture rather than a Bernoulli mixture to compute the likelihood, but we can compare with Rifai et al. (2012); Bengio et al.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "Experiments evaluating the ability of the GSN models to generate good samples were performed on the MNIST and TFD datasets, following the setup in Bengio et al. (2013c). Theorem 2 requires H0 to have the same distribution as H1 (given X0) during training, and the main paper suggests a way to achieve this by initializing each training chain with H0 set to the previous value of H1 when the same example X0 was shown.",
      "startOffset" : 147,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : "Following Breuleux et al. (2011), the quality of the samples was also estimated quantitatively by measuring the log-likelihood of the test set under a Parzen density estimator constructed from 10000 consecutively generated samples (using the real-valued mean-field reconstructions as the training data for the Parzen density estimator).",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "In comparison, Bengio et al. (2013c) report a log-likelihood bound of -244±54 for RBMs and 138±2 for a 2-hidden layer DBN, using the same setup.",
      "startOffset" : 15,
      "endOffset" : 37
    } ],
    "year" : 2014,
    "abstractText" : "We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining. P(X)",
    "creator" : "LaTeX with hyperref package"
  }
}