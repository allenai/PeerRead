{
  "name" : "1511.04646.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Word Embedding Based Correlation Model for Question/Answer Matching",
    "authors" : [ "Yikang Shen", "Wenge Rong", "Nan Jiang", "Baolin Peng", "Jie Tang", "Zhang Xiong" ],
    "emails" : [ "xiongz}@buaa.edu.cn,", "blpeng@se.cuhk.edu.hk,", "jietang@tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Community Question Answering (CQA) services are websites that enable users to share knowledge by asking and answering different kinds of questions. Over the last decade, websites, such as Yahoo! Answers, Baidu Zhidao, Quora, and Zhihu, have accumulated large scale question and answer (Q&A) archives, which are usually organised as a question with a list of candidate answers and associated with metadata including user tagged subject categories, answer popularity votes, and selected correct answer (Zhou et al. 2015). This user-generated content is an important information repository on the web and makes Q&A archives invaluable resources for various tasks such as question-answering (Jeon et al. 2005; Xue et al. 2008; Nakov et al. 2015; 2016) and knowledge mining (Adamic et al. 2008).\nTo make better use of information stored in CQA systems, a fundamental task is to properly matching potential candidate answers to the question, since many questions recur enough to allow for at least a few new questions to be answered by past materials (Shtok et al. 2012). There are several challenges for this task among which the lexical gap or lexical chasm between the question and candidate answers is a difficult one (Berger et al. 2000). Lexical gap describes the distance between dissimilar but potentiality related words\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nin questions and answers. For example, given the question “What is the fastest car in the world?”, a good answer might be “The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet.” This Q&A pair share no more than 4 words in common, including “the” and “is”, but they are strongly associated by synonyms, hyponyms, or other weaker semantic associations (Yih et al. 2014). Due to the heterogeneity of question and answer, the lexical gap is more significant in Q&A matching task than other paraphrases detection task or information retrieval task.\nA possible approach for the lexical gap problem is to employ translation model, which will leverage the Q&A pairs to learn the semantically related words (Jeon et al. 2005; Xue et al. 2008; Zhou et al. 2012; Guzmán et al. 2016). The basic assumption is that Q&A pairs are “parallel text” and relationship between words (or phrases) can be established through word-to-word (or phrase-to-phrase) translation probabilities by representing words in a discrete space. In spite of its wide use in many natural language processing tasks, discrete space representation has two majors disadvantages: 1) the curse of dimensionality (Bengio et al. 2003), for a natural language with a vocabulary V of size N , we need to learn at most NN word-to-word translation probabilities; 2) the generalisation structure is not obvious: it is difficult to estimate the probability of exact word if they are rare in the training parallel text (Zou et al. 2013).\nAn alternative method is to use a semantic-based model. Some work proposed to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, with the assumption that a question and its answer should share similar topic distribution (Cai et al. 2011; Ji et al. 2012). Recently, inspired by the success of word embedding, some papers propose to leverage the advantage of the vector representation of words to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015) by using similarity of word vector to represent the word-to-word relation. In other words, this method calculates Q&A matching probability based on semantic similarities between words. Because local smoothness properties of continuous space word representations, generalisation can be obtain more easily (Bengio et al. 2003). However question and answers are heterogeneous in many aspects, semantic similarities can be weak between questions and answers (Zhou et al. 2015).\nInspired by the pros and cons of the translation model\nar X\niv :1\n51 1.\n04 64\n6v 2\n[ cs\n.C L\n] 2\n6 N\nov 2\n01 6\nand semantic model, in this paper we propose a Word Embedding Correlation (WEC) model, which integrates the advantages of both the translation model (Jeon et al. 2005; Xue et al. 2008) and word embedding (Bengio et al. 2003; Mikolov et al. 2013a; Pennington et al. 2014). In this model, a word-level correlations function C(qi, aj) is designed to capture the word-to-word relation. Similar to traditional translation probability, this function calculates words co-occurrence probability in parallel text (Q&A pairs). Instead of using word’s discrete representation and maintaining a big and sparse translation probability matrix, we map input words qi and aj into vectors and use a low dimension dense translation matrix M to capture the co-occurrence relationship of words. If co-occurrences of exact words are rare in the training parallel text, C(qi, aj) can also estimate their correlations strength because of the local smoothness properties of continuous space word representations (Bengio et al. 2003). Based on the word-level correlations function, we propose a sentence-level correlations functions C(q, a) to calculate the relevance between question and answer. This sentence-level correlation function also makes it possible to learn the translation matrix M directly from parallel corpus. Furthermore, we combine our model with convolution neural network (CNN) (LeCun et al. 1998; Shen et al. 2015) to integrate both lexical and syntactical information stored in Q&A to estimate the matching probability. Experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset has shown WEC model’s potential.\nThe proposed model will be illustrated in detail in Section 2. Section 3 will elaborate on the experimental study. Section 4 will present related work in solving the CQA matching problem and Section 5 concludes the paper and highlights possible future research directions."
    }, {
      "heading" : "Methodology",
      "text" : "Problem Definition Given a question q = q1...qn, where qi is the i-th word in the question, and a set of candidate answers A = {a1, a2, ..., an}, where aj = aj1...ajm and a j k is the k-th word in j-th candidate answer, the goal is to identify the most relevant answer abest.\nIn order to solve this problem, we calculate the matching probability between q and each answer ai, and then rank candidate answers by their matching probabilities, which are calculated through three steps: 1) words in questions and answers are represented by vectors in a continuous space; 2) word-to-word correlation score is calculated by using a word-level correlation function; 3) Q&A matching probability is obtained by employing a phrase-level correlation function. Furthermore, we also propose to incorporate the proposed WEC model with convolution neural network (CNN) to achieve a better matching precision."
    }, {
      "heading" : "Word Embedding",
      "text" : "In order to properly represent words in a continuous space, the idea of a neural language model (Bengio et al. 2003) is employed to enable jointly learn embedding of words into an n-dimensional vector space and to use these vectors to predict how likely a word is given its context. Skip-gram model\n(Mikolov et al. 2013a) is a widely used approach to compute such an embedding. When skip-gram networks are optimised via gradient ascent, the derivatives modify the word embedding matrixL ∈ R(n×|V |), where |V | is the size of the vocabulary. The word vectors inside the embedding matrix capture distributional syntactic and semantic information via the word co-occurrence statistics (Bengio et al. 2003; Mikolov et al. 2013a). Once this matrix is learned on an unlabelled corpus, it can be used for subsequent tasks by using each word’s vector vw (a column in L) to represent that word."
    }, {
      "heading" : "Word Embedding based Correlation (WEC) Model",
      "text" : "Word-level Correlation Function In this paper we try to discover a correlation scoring function that uses word embedding as input and can also model the co-occurrence of words at the same time. In order to achieve this goal, we use a translation matrix M to transform words in the answer into words in the question. Given a pair of words (qi, aj), their WEC scoring function is defined as:\nC(qi, aj) = cos < vqi ,Mvaj >= vTqi ||vqi || Mvaj ||Mvaj || (1)\nwhere vqi and vaj represent qi and aj’s d-dimensional word embedding vectors; || · || is Euclidean norm; correlations matrix M ∈ Rd×d. M is called translation matrix, because it maps word in the answer into a possible correlated word in the question. Then the cosine function will be used to capture the semantic similarity between origin words in the question and the mapped words. Previous cosine similarity is a special case of WEC Scoring Function when M is set to identity matrix. Meanwhile, C(qi, aj) does not necessarily equal to C(aj , qi), because the probability of qi existing in question and aj existing in answer may not equal to the probability of aj existing in question and qi existing in answer.\nSentence-level Correlation Function Based on the wordlevel correlation function, we further propose the sentencelevel correlation function, which integrates word-to-word correlation scores into the Q&A pair correlation score. For a Q&A pair (q, a), their correlation score is defined as:\nC(q, a) = 1 |a| ∑ j max i C(qi, aj) (2)\nwhere |a| represents the length of answer a, C(qi, aj) is the correlations score of the i-th word in question and the jth word in the answer. The max-operator choose one most related word in question for each word in answer. Sentence level correlation score is calculated by averaging selected word-level scores. According to our experiments, this max-average function perform better than simply average all word-level correlation score, and maximizing more efficiently."
    }, {
      "heading" : "WEC + Convolution Neural Networks (CNN)",
      "text" : "WEC is based on the bag-of-word schema, which puts the syntactical information aside, e.g., the word sequence information. As such in worst cases, two phrases may have\nsame bag-of-words representation, their real meaning could be completely opposite (Socher et al. 2011).\nTo overcome this limitation, several approaches have been proposed and one possible solution is to use the convolution neural network (CNN) model (He et al. 2015; Mou et al. 2016). (Kalchbrenner et al. 2014) proposed that the convolutional and dynamic pooling layer in CNN can relate phrases far apart in the input sentence. In (Shen et al. 2015), S+CNN model is proposed for Q&A matching to integrate both syntactical and lexical information to estimate the matching probability. In their model, the input Q&A pair is transformed into a similarity matrix S, generated through function:\nSij = cos(qi mod |q|, aj mod |a|) (3)\nwhere |q| and |a| are the respective lengths of question and answer, S is a nf × mf fix size matrix, and nf and mf are the number of rows and columns respectively. Thus, the maximum length of questions and answers should be limited to be no longer than nf and mf . Then the similarity matrix is used as an input of a CNN instead of an image in (LeCun et al. 1998), the output of the CNN is the matching score of the Q&A pair. Fig. 1 shows the architecture of the employed CNN.\nInstead of word embedding cosine similarity, the wordlevel correlation scores in WEC can be used in the formation of the input matrix of CNN. Similar to the S-matrix, we propose a correlations matrix C, generated through function:\nCij = C(qi mod |q|, aj mod |a|) (4)\nwhere C is an nf × mf fixed size matrix, and used as the input matrix of CNN. In this way, we obtain a new combination model, called the WEC+CNN model.\nThe complete training process comprises two supervised pre-training steps and a supervised fine-tuning step. In the first supervised pre-training step, we maximize the margin of the output of WEC function to pre-trainM . In the second supervised pre-training step, we fix M , then maximize the margin of the output of CNN to train the CNN part. In the fine-tuning step, we maximize the margin of the output of CNN to fine-tune all parameters in WEC and CNN."
    }, {
      "heading" : "Experimental Study",
      "text" : ""
    }, {
      "heading" : "Dataset",
      "text" : "To evaluate the proposed WEC model, two datasets Yahoo! Answer and Baidu Zhidao are employed in this research. The dataset from Yahoo! Answers is available in Yahoo! Webscope1, including a large number of questions and their corresponding answers. In addition, the corpus contains a small amount of meta data, such as, which answer was selected as the best answer, and the category and sub-category assigned to this question (Surdeanu et al. 2008). To validate the proposed WEC model, we generate three different sub-datasets. As shown in Table 1, the first subset contains 53,261 questions which are categorized as being about travel. The second subset contains 57,576 questions that are categorized as being about relationships. The third subset contains 66,129 questions that are categorized as being about finance. Because the CNN model needs to limit the maximum length of questions and answers, all selected questions are no longer than 50 words, selected answers a no longer than 100 words. Thus, for the Yahoo! Answer dataset, nf = 50 and mf = 100. We also limit the minimum length of an answer to 5 words, to avoid answers like “Yes”, “It doesn’t work” or simply a URL. More than half of the questions and answers in Yahoo! Answers satisfy these limitations.\nThe Baidu Zhidao dataset is provided in (Shen et al. 2015), and contains 99,909 questions and their best answers. Following their settings, 4 different datasets are generated, each contains questions from a single category: ’Computers & Internet’, ’Education & Science’, ’Games’, and ’Entertainment & Recreation’. each dataset contains 90,000 training triples, the random category dataset contains 10,000 test questions, other datasets contain 1,000 test questions each. In this dataset nf = 30 and mf = 50."
    }, {
      "heading" : "Experimental Settings",
      "text" : "Evaluation Metrics We use the same evaluation method employed by (Lu and Li 2013; Shen et al. 2015) to evaluate the accuracy of matching questions and answers. A set of candidate answers is created with size 6 (one positive + five negative) for each question in the test data. We compare the performance of our approach in ranking quality of the six candidate answers against that of others baselines. Discounted cumulative gain (DCG) (Rvelin and Inen 2000) is employed to evaluate the ranking quality. The premise of DCG is that highly relevant documents appearing lower in a ranking list should be penalised as the graded relevance value is reduced logarithmically proportional to the position of the result. DCG accumulated at a particular rank position p is defined as:\nDCG@p = rel1 + p∑ i=2 reli log2(i)\n(5)\nwhere the best answer rel = 1, for other answers rel = 0. We choose DCG@1 to evaluate the precision of choosing the best answer and DCG@6 to evaluate the quality of ranking.\n1http://research.yahoo.com/Academic Relations\nBaseline We compare WEC model against Translation model (TM) (Jeon et al. 2005), Translation based language model (TRLM) (Xue et al. 2008), Okapi model (Jeon et al. 2005) and Language model (LM) (Jeon et al. 2005). TM and TRLM use translation probabilities to overcome the lexical gap, while Okapi and LM only consider words that exist in both question and answer.\nGiven a question q and answer a, TM (Jeon et al. 2005) can be define as:\nSq,a = ∏ t∈q ((1− λ) ∑ w∈a P (t|w)Pml(w|a)\n+ λPml(t|Coll)) (6)\nTRLM (Xue et al. 2008) can be define as: Sq,a = ∏ t∈q ((1− λ)(β ∑ w∈a P (t|w)Pml(w|a)\n+ (1− β)Pml(t|a) + λPml(t|Coll))) (7)\nwhere P (t|w) denotes the probability that question word t is the translation of answer wordw. IBM translation model 1 is used to learn P (t|w) with answer as the source and question as the target. In (Shen et al. 2015), word vector cosine similarity is used as word translation probabilities in TM and TRLM. Their results are also included in the experimental study.\nHyperparameter To train the skip-gram model, we use the hyper-parameters recommended in (Mikolov et al. 2013a): the dimension of word embedding is set to 500, and the window size is 10.\nCNN model contains two convolution layers labelled as C1 and C2, each convolution layer is followed by a maxpooling layers P1 and P2, and fully connected layer F. Each unit in a convolution layer is connected to a 5 × 5 neighbourhood in the input. Each unit in max-pooling layer is connected to a 2 × 2 neighbourhood in the corresponding feature map in C1. Layer C1 and M1 contains 20 feature maps each. Layer C2 and M2 contains 50 feature maps each. The fully connected layer contains 500 units and is fully connected to M2."
    }, {
      "heading" : "Experiment Results",
      "text" : "Table 2 shows the Q&A matching performance of WEC based methods, translation probability based methods and traditional retrieval methods on the Yahoo! Answer dataset. For top candidate answer precision, WEC slightly outperforms the translation probability based models. For candidate answer ranking qualities, WEC outperforms TRLM and\nTM. By adding CNN into the model, WEC+CNN outperforms all other models. It is possible to interpret that WEC model can perform better than TRLM and TM model, but merely using lexical level information limits its ability in selecting the best answer. Thus, WEC+CNN is able to improve the result by adding syntactical information into the model.\nTable 3 shows the Q&A matching performance of different approaches on Baidu Zhidao dataset. We find that WEC and WEC+CNN outperform all other models. Furthermore, IBM-1 based models outperform cos-similarity based models. This is possibly is due to the heterogeneity of question and answer, since both WEC and IBM translation model 1 can directly model the word-to-word co-occurrence probability instead of semantic similarity. On both datasets, traditional retrieval models obtain the worst result because they suffer from the lexical gap problem."
    }, {
      "heading" : "Examples",
      "text" : "To better understand the behavior of WEC, we illustrated a number of example translations from answer words to a given question word in Table 4. Three different methods, e.g., WEC, cos-similarity, IBM Model 1, are employed to estimate the translation probabilities. Interestingly, these methods provide semantically related target words with different characters. To clarify this difference, consider the word “where”. IBM model 1 provides “hamlets”, “prefecture”, “foxborough” and “berea”. They are rarely appeared (comparing with “middle” and “southern”) generic or specific name for settlement. Cos-similarity provides “what”, “how”, and “which”. They are question words like “where”. WEC model provides “middle”, “southern”, “southeastern”, and “situated”. These words are semantically related to the target word, and likely to appear in a suitable answer. The difference between three models reflect differences in their learning processes.\nIBM translation model 1 leverage the co-occurrence of words in parallel corpus to learn translation probabilities. The sum of translation probabilities for a question word equal to 1. Therefore, answer words with low document frequency get relatively higher translation probabilities with certain question words, because these words co-occur with a smell set of question words, hence its translation probabilities concentrate on this set of words. Skip-gram model learn embedding of words into an n-dimensional vector and to use these vectors to predict how likely a word is given its context. Thus, the cos-similarity captures the probability that a pair of words appear with similar contexts.\nTable 2: Performance of different approaches on Yahoo! Answers dataset (WEC denote sentence-level WEC function)\nApproach Travel Relationships FinanceDCG@1 DCG@6 DCG@1 DCG@6 DCG@1 DCG@6 WEC + CNN 0.761 0.946 0.709 0.938 0.780 0.952 WEC 0.734 0.946 0.698 0.936 0.761 0.949 TRLM 0.727 0.922 0.683 0.910 0.755 0.927 TM 0.698 0.914 0.676 0.912 0.742 0.926 Okapi 0.631 0.875 0.517 0.823 0.646 0.866 LM 0.592 0.848 0.525 0.825 0.595 0.838\nTable 3: Performance of different approaches on Baidu Zhidao dataset. IBM-1 denotes that translation probabilities are learned using IBM translation model 1, cos denotes that translation probabilities are calculated through word vector’s cosine-similarity\nWEC tries to combines advantages of IBM model 1 and word embedding. The word vector capture distribu-\ntional syntactic and semantic information via the word cooccurrence statistics (Bengio et al. 2003; Mikolov et al.\n2013a). Word-to-word correlation score are learned via maximizing the result of sentence-level correlation function Eq. (2). Meanwhile, WEC do not normalize the correlation score, which is more feasible for QA tasks.\nThe sentence-level correlation function Eq. (2) is also capable of identifying important relevance between questions and answers. As shown in the Fig. 2, for each word in an answer, the max operator in Eq. (2) chooses the most relevant word in question, based on the correlation score calculated by Eq. (1). Interestingly, both words “try” and “looking” are linked with “find”, while the relevance between “try” and “find” is more obscure than the obvious relevance between “looking” and “find”. Although, the link between “a” and “the” is inappropriate in this context, but in many other contexts, this relationship may be correct. The relation between words given a certain context is left for future work."
    }, {
      "heading" : "Related Work",
      "text" : ""
    }, {
      "heading" : "Lexical Gap Problem in CQA",
      "text" : "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al. 2000; Surdeanu et al. 2008; Lu and Li 2013; Shen et al. 2015). One major challenge in both tasks is the lexical gap (chasm) problem (Chen et al. 2016). Potential solutions to overcome this difficulty include 1) query expansion, 2) statistical translation, 3) latent variable models (Berger et al. 2000).\nMuch importance has been attached to statistical translation in the literature. Classical methods include translation model (TM) (Jeon et al. 2005) and translation language model (TRLM) (Xue et al. 2008). Both use IBM translation model 1 to learn the translation probabilities between question and answer words. Apart from word-level translation, phrase-level translation for question and answer retrieval has also achieved promising results (Cai et al. 2011).\nLatent variable models also attract much research in recent years. Proposals have been made to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, on the assumption that question and its answer should share a similar topic distribution (Cai et al. 2011; Ji et al. 2012). Furthermore, inspired by the recent success of word embedding, several approaches have been proposed to leverage the advantages of the vector representation to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015).\nDifferent from previous models, our work aims at combining the idea of both statistical translation and latent variable model. We proposed a latent variable model, but parameters are learned to model the word-level translation probabilities. As a result, we can keep the generalisability of latent variable model, while achieving better precision than a brutal statistical translation model and provide more reasonable results in word-to-word correlation examples."
    }, {
      "heading" : "Translation Matrix",
      "text" : "Distributed representations for words have proven its success in many domain applications. Its main advantage is that the representations of similar words are close in the vector space, which makes generalisation to novel patterns easier and model estimation more robust. Successful follow-up work includes application to statistical language modelling (Bengio et al. 2003; Mikolov et al. 2013a).\nInspired by vector representation of words, the translation matrix has been proposed to map vector representation x from one language space to another language space, using cosine similarity as a distance metric (Mikolov et al. 2013b). Our word-level WEC model uses the same translation functions to map vector x from answer semantic space to question semantic space. We further propose a sentence-level WEC model to calculate the Q&A matching probability, and a method to learn the translation matrix through maximising the matching accuracy in a parallel Q&A corpus.\nSimilarly neural tensor network (NTN) is also implemented to model relational information (Socher et al. 2013; Qiu and Huang 2015). A tensor matrix is employed to seize the relationship between vectors. The NTN’s main advantage is that it can relate two inputs multiplicatively instead of only implicitly through non-linearity as with standard neural networks where the entity vectors are simply concatenated (Socher et al. 2013). Our model is conceptually similar to NTN and use a translation matrix to model the word-to-word relation in Q&A pairs. Similar to NTN, the translation matrix in our model makes it possible to explicitly relate the two inputs, and cos in Eq. (1) adds non-linearity."
    }, {
      "heading" : "Conclusion and Future Work",
      "text" : "This paper presents a new approach for Q&A matching in CQA services. In order to solve the lexical gap between question and answer, a word embedding based correlation (WEC) model is proposed, where the co-occurrence relation between words in parallel text is represented as a matrix (or a set of matrices). Given a random pair of words, WEC model can score their co-occurrence probability in Q&A pairs like the previous translation model based approach. And it also leverages the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. Our experiments show that WEC and WEC+CNN outperform state-of-the-art models.\nThere are several interesting directions which deserve further exploration in the future. It is possible to apply this model in question-question matching tasks, or multilanguage question retrieval task. It is also interesting to explore the possibility of using this approach to solve other parallel detection problems (e.g., comment selection on a given tweet)."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work was partially supported by the National Natural Science Foundation of China (No. 61332018), the National Department Public Benefit Research Foundation of China (No. 201510209), and the Fundamental Research Funds for the Central Universities."
    } ],
    "references" : [ {
      "title" : "Knowledge sharing and yahoo answers: everyone knows something",
      "author" : [ "Lada A. Adamic", "Jun Zhang", "Eytan Bakshy", "Mark S. Ackerman" ],
      "venue" : "In Proc. WWW,",
      "citeRegEx" : "Adamic et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Adamic et al\\.",
      "year" : 2008
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Bridging the lexical chasm: Statistical approaches to answer-finding",
      "author" : [ "Adam Berger", "Rich Caruana", "David Cohn", "Dayne Freitag", "Vibhu Mittal" ],
      "venue" : "In Proc. SIGIR,",
      "citeRegEx" : "Berger et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Berger et al\\.",
      "year" : 2000
    }, {
      "title" : "Learning the latent topics for question retrieval in community QA",
      "author" : [ "Li Cai", "Guangyou Zhou", "Kang Liu", "Jun Zhao" ],
      "venue" : "In Proc. IJCNLP,",
      "citeRegEx" : "Cai et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2011
    }, {
      "title" : "A generalized framework of exploring category information for question retrieval in community question answer archives",
      "author" : [ "Xin Cao", "Gao Cong", "Bin Cui", "Christian S. Jensen" ],
      "venue" : "In Proc. WWW,",
      "citeRegEx" : "Cao et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2010
    }, {
      "title" : "A semantic graph based topic model for question retrieval in community question answering",
      "author" : [ "Long Chen", "Joemon M. Jose", "Haitao Yu", "Fajie Yuan", "Dell Zhang" ],
      "venue" : "In Proc. ICDM,",
      "citeRegEx" : "Chen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Machine translation evaluation meets community question answering",
      "author" : [ "Francisco Guzmán", "Lluı́s Màrquez", "Preslav Nakov" ],
      "venue" : "In Proc. ACL,",
      "citeRegEx" : "Guzmán et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Guzmán et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-perspective sentence similarity modeling with convolutional neural networks",
      "author" : [ "Hua He", "Kevin Gimpel", "Jimmy Lin" ],
      "venue" : "In Proc. EMNLP,",
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Finding similar questions in large question and answer archives",
      "author" : [ "Jiwoon Jeon", "W. Bruce Croft", "Joon Ho Lee" ],
      "venue" : "In Proc. CIKM,",
      "citeRegEx" : "Jeon et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Jeon et al\\.",
      "year" : 2005
    }, {
      "title" : "Questionanswer topic model for question retrieval in community question answering",
      "author" : [ "Zongcheng Ji", "Fei Xu", "Bin Wang", "Ben He" ],
      "venue" : "In Proc. CIKM,",
      "citeRegEx" : "Ji et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2012
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom" ],
      "venue" : "In Proc. ACL,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "A deep architecture for matching short texts",
      "author" : [ "Zhengdong Lu", "Hang Li" ],
      "venue" : "In Proc. NIPS,",
      "citeRegEx" : "Lu and Li.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lu and Li.",
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "CoRR, abs/1301.3781,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting similarities among languages for machine translation",
      "author" : [ "Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever" ],
      "venue" : "CoRR, abs/1309.4168,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Convolutional neural networks over tree structures for programming language processing",
      "author" : [ "Lili Mou", "Ge Li", "Lu Zhang", "Tao Wang", "Zhi Jin" ],
      "venue" : "In Proc. AAAI,",
      "citeRegEx" : "Mou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2016
    }, {
      "title" : "Semeval-2015 task 3: Answer selection in community question answering",
      "author" : [ "Preslav Nakov", "Lluı́s Màrquez", "Walid Magdy", "Alessandro Moschitti", "James Glass", "Bilal Randeree" ],
      "venue" : "In Proc. SemEval,",
      "citeRegEx" : "Nakov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2015
    }, {
      "title" : "SemEval-2016 task 3: Community question answering",
      "author" : [ "Preslav Nakov", "Lluı́s Màrquez", "Alessandro Moschitti", "Walid Magdy", "Hamdy Mubarak", "Abed Alhakim Freihat", "James Glass", "Bilal Randeree" ],
      "venue" : "In Proc. SemEval,",
      "citeRegEx" : "Nakov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning" ],
      "venue" : "In Proc. EMNLP,",
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural tensor network architecture for community-based question answering",
      "author" : [ "Xipeng Qiu", "Xuanjing Huang" ],
      "venue" : "In Proc. IJCAI,",
      "citeRegEx" : "Qiu and Huang.,? \\Q2015\\E",
      "shortCiteRegEx" : "Qiu and Huang.",
      "year" : 2015
    }, {
      "title" : "Inen. IR evaluation methods for retrieving highly relevant documents",
      "author" : [ "Kalervo J.A. Rvelin", "Jaana Kek A. L" ],
      "venue" : "In Proc. SIGIR,",
      "citeRegEx" : "Rvelin and L.,? \\Q2000\\E",
      "shortCiteRegEx" : "Rvelin and L.",
      "year" : 2000
    }, {
      "title" : "Question/answer matching for CQA system via combining lexical and sequential information",
      "author" : [ "Yikang Shen", "Wenge Rong", "Zhiwei Sun", "Yuanxin Ouyang", "Zhang Xiong" ],
      "venue" : "In Proc. AAAI,",
      "citeRegEx" : "Shen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning from the past: answering new questions with past answers",
      "author" : [ "Anna Shtok", "Gideon Dror", "Yoelle Maarek", "Idan Szpektor" ],
      "venue" : "In Proc. WWW,",
      "citeRegEx" : "Shtok et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shtok et al\\.",
      "year" : 2012
    }, {
      "title" : "Semi-supervised recursive autoencoders for predicting sentiment distributions",
      "author" : [ "Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning" ],
      "venue" : "In Proc. EMNLP,",
      "citeRegEx" : "Socher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Ng" ],
      "venue" : "In Proc. NIPS,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning to rank answers on large online QA collections",
      "author" : [ "Mihai Surdeanu", "Massimiliano Ciaramita", "Hugo Zaragoza" ],
      "venue" : "In Proc. ACL,",
      "citeRegEx" : "Surdeanu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Surdeanu et al\\.",
      "year" : 2008
    }, {
      "title" : "Retrieval models for question and answer archives",
      "author" : [ "Xiaobing Xue", "Jiwoon Jeon", "W. Bruce Croft" ],
      "venue" : "In Proc. SIGIR,",
      "citeRegEx" : "Xue et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2008
    }, {
      "title" : "Semantic parsing for single-relation question answering",
      "author" : [ "Wen-tau Yih", "Xiaodong He", "Christopher Meek" ],
      "venue" : "In Proc. ACL,",
      "citeRegEx" : "Yih et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2014
    }, {
      "title" : "A classification-based approach to question routing in community question answering",
      "author" : [ "Tom Chao Zhou", "Michael R. Lyu", "Irwin King" ],
      "venue" : "In Proc. WWW,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning continuous word embedding with metadata for question retrieval in community question answering",
      "author" : [ "Guangyou Zhou", "Tingting He", "Jun Zhao", "Po Hu" ],
      "venue" : "In Proc. ACL,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    }, {
      "title" : "Bilingual word embeddings for phrasebased machine translation",
      "author" : [ "Will Y. Zou", "Richard Socher", "Daniel M. Cer", "Christopher D. Manning" ],
      "venue" : "In Proc. EMNLP,",
      "citeRegEx" : "Zou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Over the last decade, websites, such as Yahoo! Answers, Baidu Zhidao, Quora, and Zhihu, have accumulated large scale question and answer (Q&A) archives, which are usually organised as a question with a list of candidate answers and associated with metadata including user tagged subject categories, answer popularity votes, and selected correct answer (Zhou et al. 2015).",
      "startOffset" : 352,
      "endOffset" : 370
    }, {
      "referenceID" : 8,
      "context" : "This user-generated content is an important information repository on the web and makes Q&A archives invaluable resources for various tasks such as question-answering (Jeon et al. 2005; Xue et al. 2008; Nakov et al. 2015; 2016) and knowledge mining (Adamic et al.",
      "startOffset" : 167,
      "endOffset" : 227
    }, {
      "referenceID" : 26,
      "context" : "This user-generated content is an important information repository on the web and makes Q&A archives invaluable resources for various tasks such as question-answering (Jeon et al. 2005; Xue et al. 2008; Nakov et al. 2015; 2016) and knowledge mining (Adamic et al.",
      "startOffset" : 167,
      "endOffset" : 227
    }, {
      "referenceID" : 16,
      "context" : "This user-generated content is an important information repository on the web and makes Q&A archives invaluable resources for various tasks such as question-answering (Jeon et al. 2005; Xue et al. 2008; Nakov et al. 2015; 2016) and knowledge mining (Adamic et al.",
      "startOffset" : 167,
      "endOffset" : 227
    }, {
      "referenceID" : 0,
      "context" : "2015; 2016) and knowledge mining (Adamic et al. 2008).",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "To make better use of information stored in CQA systems, a fundamental task is to properly matching potential candidate answers to the question, since many questions recur enough to allow for at least a few new questions to be answered by past materials (Shtok et al. 2012).",
      "startOffset" : 254,
      "endOffset" : 273
    }, {
      "referenceID" : 2,
      "context" : "There are several challenges for this task among which the lexical gap or lexical chasm between the question and candidate answers is a difficult one (Berger et al. 2000).",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 27,
      "context" : "” This Q&A pair share no more than 4 words in common, including “the” and “is”, but they are strongly associated by synonyms, hyponyms, or other weaker semantic associations (Yih et al. 2014).",
      "startOffset" : 174,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "A possible approach for the lexical gap problem is to employ translation model, which will leverage the Q&A pairs to learn the semantically related words (Jeon et al. 2005; Xue et al. 2008; Zhou et al. 2012; Guzmán et al. 2016).",
      "startOffset" : 154,
      "endOffset" : 227
    }, {
      "referenceID" : 26,
      "context" : "A possible approach for the lexical gap problem is to employ translation model, which will leverage the Q&A pairs to learn the semantically related words (Jeon et al. 2005; Xue et al. 2008; Zhou et al. 2012; Guzmán et al. 2016).",
      "startOffset" : 154,
      "endOffset" : 227
    }, {
      "referenceID" : 28,
      "context" : "A possible approach for the lexical gap problem is to employ translation model, which will leverage the Q&A pairs to learn the semantically related words (Jeon et al. 2005; Xue et al. 2008; Zhou et al. 2012; Guzmán et al. 2016).",
      "startOffset" : 154,
      "endOffset" : 227
    }, {
      "referenceID" : 6,
      "context" : "A possible approach for the lexical gap problem is to employ translation model, which will leverage the Q&A pairs to learn the semantically related words (Jeon et al. 2005; Xue et al. 2008; Zhou et al. 2012; Guzmán et al. 2016).",
      "startOffset" : 154,
      "endOffset" : 227
    }, {
      "referenceID" : 1,
      "context" : "In spite of its wide use in many natural language processing tasks, discrete space representation has two majors disadvantages: 1) the curse of dimensionality (Bengio et al. 2003), for a natural language with a vocabulary V of size N , we need to learn at most N word-to-word translation probabilities; 2) the generalisation structure is not obvious: it is difficult to estimate the probability of exact word if they are rare in the training parallel text (Zou et al.",
      "startOffset" : 159,
      "endOffset" : 179
    }, {
      "referenceID" : 30,
      "context" : "2003), for a natural language with a vocabulary V of size N , we need to learn at most N word-to-word translation probabilities; 2) the generalisation structure is not obvious: it is difficult to estimate the probability of exact word if they are rare in the training parallel text (Zou et al. 2013).",
      "startOffset" : 282,
      "endOffset" : 299
    }, {
      "referenceID" : 3,
      "context" : "Some work proposed to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, with the assumption that a question and its answer should share similar topic distribution (Cai et al. 2011; Ji et al. 2012).",
      "startOffset" : 205,
      "endOffset" : 238
    }, {
      "referenceID" : 9,
      "context" : "Some work proposed to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, with the assumption that a question and its answer should share similar topic distribution (Cai et al. 2011; Ji et al. 2012).",
      "startOffset" : 205,
      "endOffset" : 238
    }, {
      "referenceID" : 21,
      "context" : "Recently, inspired by the success of word embedding, some papers propose to leverage the advantage of the vector representation of words to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015) by using similarity of word vector to represent the word-to-word relation.",
      "startOffset" : 165,
      "endOffset" : 201
    }, {
      "referenceID" : 29,
      "context" : "Recently, inspired by the success of word embedding, some papers propose to leverage the advantage of the vector representation of words to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015) by using similarity of word vector to represent the word-to-word relation.",
      "startOffset" : 165,
      "endOffset" : 201
    }, {
      "referenceID" : 1,
      "context" : "Because local smoothness properties of continuous space word representations, generalisation can be obtain more easily (Bengio et al. 2003).",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 29,
      "context" : "However question and answers are heterogeneous in many aspects, semantic similarities can be weak between questions and answers (Zhou et al. 2015).",
      "startOffset" : 128,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "and semantic model, in this paper we propose a Word Embedding Correlation (WEC) model, which integrates the advantages of both the translation model (Jeon et al. 2005; Xue et al. 2008) and word embedding (Bengio et al.",
      "startOffset" : 149,
      "endOffset" : 184
    }, {
      "referenceID" : 26,
      "context" : "and semantic model, in this paper we propose a Word Embedding Correlation (WEC) model, which integrates the advantages of both the translation model (Jeon et al. 2005; Xue et al. 2008) and word embedding (Bengio et al.",
      "startOffset" : 149,
      "endOffset" : 184
    }, {
      "referenceID" : 1,
      "context" : "2008) and word embedding (Bengio et al. 2003; Mikolov et al. 2013a; Pennington et al. 2014).",
      "startOffset" : 25,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "2008) and word embedding (Bengio et al. 2003; Mikolov et al. 2013a; Pennington et al. 2014).",
      "startOffset" : 25,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "If co-occurrences of exact words are rare in the training parallel text, C(qi, aj) can also estimate their correlations strength because of the local smoothness properties of continuous space word representations (Bengio et al. 2003).",
      "startOffset" : 213,
      "endOffset" : 233
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, we combine our model with convolution neural network (CNN) (LeCun et al. 1998; Shen et al. 2015) to integrate both lexical and syntactical information stored in Q&A to estimate the matching probability.",
      "startOffset" : 72,
      "endOffset" : 109
    }, {
      "referenceID" : 21,
      "context" : "Furthermore, we combine our model with convolution neural network (CNN) (LeCun et al. 1998; Shen et al. 2015) to integrate both lexical and syntactical information stored in Q&A to estimate the matching probability.",
      "startOffset" : 72,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "In order to properly represent words in a continuous space, the idea of a neural language model (Bengio et al. 2003) is employed to enable jointly learn embedding of words into an n-dimensional vector space and to use these vectors to predict how likely a word is given its context.",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "The word vectors inside the embedding matrix capture distributional syntactic and semantic information via the word co-occurrence statistics (Bengio et al. 2003; Mikolov et al. 2013a).",
      "startOffset" : 141,
      "endOffset" : 183
    }, {
      "referenceID" : 23,
      "context" : "same bag-of-words representation, their real meaning could be completely opposite (Socher et al. 2011).",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "To overcome this limitation, several approaches have been proposed and one possible solution is to use the convolution neural network (CNN) model (He et al. 2015; Mou et al. 2016).",
      "startOffset" : 146,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "To overcome this limitation, several approaches have been proposed and one possible solution is to use the convolution neural network (CNN) model (He et al. 2015; Mou et al. 2016).",
      "startOffset" : 146,
      "endOffset" : 179
    }, {
      "referenceID" : 10,
      "context" : "(Kalchbrenner et al. 2014) proposed that the convolutional and dynamic pooling layer in CNN can relate phrases far apart in the input sentence.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "In (Shen et al. 2015), S+CNN model is proposed for Q&A matching to integrate both syntactical and lexical information to estimate the matching probability.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "Then the similarity matrix is used as an input of a CNN instead of an image in (LeCun et al. 1998), the output of the CNN is the matching score of the Q&A pair.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "In addition, the corpus contains a small amount of meta data, such as, which answer was selected as the best answer, and the category and sub-category assigned to this question (Surdeanu et al. 2008).",
      "startOffset" : 177,
      "endOffset" : 199
    }, {
      "referenceID" : 21,
      "context" : "The Baidu Zhidao dataset is provided in (Shen et al. 2015), and contains 99,909 questions and their best answers.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "Evaluation Metrics We use the same evaluation method employed by (Lu and Li 2013; Shen et al. 2015) to evaluate the accuracy of matching questions and answers.",
      "startOffset" : 65,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Baseline We compare WEC model against Translation model (TM) (Jeon et al. 2005), Translation based language model (TRLM) (Xue et al.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "2005), Translation based language model (TRLM) (Xue et al. 2008), Okapi model (Jeon et al.",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "2008), Okapi model (Jeon et al. 2005) and Language model (LM) (Jeon et al.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "2005) and Language model (LM) (Jeon et al. 2005).",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Given a question q and answer a, TM (Jeon et al. 2005) can be define as:",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 26,
      "context" : "TRLM (Xue et al. 2008) can be define as:",
      "startOffset" : 5,
      "endOffset" : 22
    }, {
      "referenceID" : 21,
      "context" : "In (Shen et al. 2015), word vector cosine similarity is used as word translation probabilities in TM and TRLM.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.",
      "startOffset" : 213,
      "endOffset" : 300
    }, {
      "referenceID" : 26,
      "context" : "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.",
      "startOffset" : 213,
      "endOffset" : 300
    }, {
      "referenceID" : 4,
      "context" : "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.",
      "startOffset" : 213,
      "endOffset" : 300
    }, {
      "referenceID" : 3,
      "context" : "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.",
      "startOffset" : 213,
      "endOffset" : 300
    }, {
      "referenceID" : 29,
      "context" : "To fully use Q&A archives in CQA systems, there are two important tasks for a newly submitted question, including question retrieval, which focuses on matching new questions with archived questions in CQA systems (Jeon et al. 2005; Xue et al. 2008; Cao et al. 2010; Cai et al. 2011; Zhou et al. 2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al.",
      "startOffset" : 213,
      "endOffset" : 300
    }, {
      "referenceID" : 2,
      "context" : "2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al. 2000; Surdeanu et al. 2008; Lu and Li 2013; Shen et al. 2015).",
      "startOffset" : 120,
      "endOffset" : 196
    }, {
      "referenceID" : 25,
      "context" : "2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al. 2000; Surdeanu et al. 2008; Lu and Li 2013; Shen et al. 2015).",
      "startOffset" : 120,
      "endOffset" : 196
    }, {
      "referenceID" : 21,
      "context" : "2015) and answer locating, which focuses finding a potentially suitable answer within a collection of candidate answers (Berger et al. 2000; Surdeanu et al. 2008; Lu and Li 2013; Shen et al. 2015).",
      "startOffset" : 120,
      "endOffset" : 196
    }, {
      "referenceID" : 5,
      "context" : "One major challenge in both tasks is the lexical gap (chasm) problem (Chen et al. 2016).",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Potential solutions to overcome this difficulty include 1) query expansion, 2) statistical translation, 3) latent variable models (Berger et al. 2000).",
      "startOffset" : 130,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "Classical methods include translation model (TM) (Jeon et al. 2005) and translation language model (TRLM) (Xue et al.",
      "startOffset" : 49,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "2005) and translation language model (TRLM) (Xue et al. 2008).",
      "startOffset" : 44,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "Apart from word-level translation, phrase-level translation for question and answer retrieval has also achieved promising results (Cai et al. 2011).",
      "startOffset" : 130,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "Proposals have been made to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, on the assumption that question and its answer should share a similar topic distribution (Cai et al. 2011; Ji et al. 2012).",
      "startOffset" : 209,
      "endOffset" : 242
    }, {
      "referenceID" : 9,
      "context" : "Proposals have been made to learn the latent topics aligned across the question-answer pairs to bridge the lexical gap, on the assumption that question and its answer should share a similar topic distribution (Cai et al. 2011; Ji et al. 2012).",
      "startOffset" : 209,
      "endOffset" : 242
    }, {
      "referenceID" : 21,
      "context" : "Furthermore, inspired by the recent success of word embedding, several approaches have been proposed to leverage the advantages of the vector representation to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015).",
      "startOffset" : 185,
      "endOffset" : 221
    }, {
      "referenceID" : 29,
      "context" : "Furthermore, inspired by the recent success of word embedding, several approaches have been proposed to leverage the advantages of the vector representation to overcome the lexical gap (Shen et al. 2015; Zhou et al. 2015).",
      "startOffset" : 185,
      "endOffset" : 221
    }, {
      "referenceID" : 1,
      "context" : "Successful follow-up work includes application to statistical language modelling (Bengio et al. 2003; Mikolov et al. 2013a).",
      "startOffset" : 81,
      "endOffset" : 123
    }, {
      "referenceID" : 24,
      "context" : "Similarly neural tensor network (NTN) is also implemented to model relational information (Socher et al. 2013; Qiu and Huang 2015).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 24,
      "context" : "The NTN’s main advantage is that it can relate two inputs multiplicatively instead of only implicitly through non-linearity as with standard neural networks where the entity vectors are simply concatenated (Socher et al. 2013).",
      "startOffset" : 206,
      "endOffset" : 226
    } ],
    "year" : 2016,
    "abstractText" : "The large scale of Q&A archives accumulated in community based question answering (CQA) servivces are important information and knowledge resource on the web. Question and answer matching task has been attached much importance to for its ability to reuse knowledge stored in these systems: it can be useful in enhancing user experience with recurrent questions. In this paper, a Word Embedding based Correlation (WEC) model is proposed by integrating advantages of both the translation model and word embedding. Given a random pair of words, WEC can score their co-occurrence probability in Q&A pairs, while it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. An experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new method’s promising",
    "creator" : "TeX"
  }
}