{
  "name" : "1510.02892.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Survey on Feature Selection",
    "authors" : [ "Tarek Amr", "Beatriz de La Iglesia" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 0.\n02 89\n2v 1\n[ cs\n.L G\n] 1\n0 O\nct 2\n01 5\nFeature selection plays an important role in the data mining process. It is needed to deal with the excessive number of features, which can become a computational burden on the learning algorithms. It is also necessary, even when computational resources are not scarce, since it improves the accuracy of the machine learning tasks, as we will see in the upcoming sections. In this review, we discuss the different feature selection approaches, and the relation between them and the various machine learning algorithms."
    }, {
      "heading" : "1 Introduction",
      "text" : "According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones. Classification is an example of predictive models. Friedman (1997) described it as a model where discrete output values (class labels) are learnt from the different variables (features) of the input data. Clustering, on the other hand, is categorised by Dunham (2002) as a descriptive task. The features of the input data are used to categorize it without supervised training. In both cases, the choice of the feature-set plays an important role in the performance of the data mining problem. Liu et al. (2010) listed three advantages for removing irrelevant and redundant features: it makes the data mining task more efficient, improves its accuracy and simplifies the inferred model, making it more comprehensible.\nFor an accurate classifier, it is needed to reduce both bias and variance of the model (Friedman, 1997). As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa. This is known as “bias-variance trade-off”. Hence, as noted by Kohavi and John (1997), classifiers faced with limited data has to find an optimum point where they can actually estimate the statistical distribution of fewer features (variance reduction) versus less accurate estimation of more features (bias reduction); ergo, Munson and Caruana (2009) summarized the feature selection process as the process of finding the best bias-variance trade-off point.\nWhen it comes to unsupervised learning algorithms, such as clustering, Janecek et al. (2008) highlighted that the problem with high dimensional data (more features) is that it makes the proximity measures between the records more uniform, hence metrics such as distance and density become harder to obtain.\nIn the next sections we explain the different feature selection approaches."
    }, {
      "heading" : "2 The selection process",
      "text" : "In its simplest form, the feature selection process can evaluate individual features and rank them based on their correlation with class labels (Yu and Liu, 2004). However, Hall (1999) reported that studies had proven a good feature subset to be the one whose features are not correlated to each other, besides them being correlated to class labels. Hence, they are better evaluated as subset rather than individually. Liu et al. (2010) summarized subset feature selection process into three main steps:\n• Search: Generating a subset of the available features to be evaluated.\n• Evaluation: Evaluating the utility of the generated subset\n• Stop: Deciding whether to stop or continue the search till a stopping criterion is reached\nFor N-dimension feature-space, there are 2N possible subsets. Thus, the generation step uses different approaches to traverse the available subsets. Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003).\nJohn et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones.\nHall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space.\nYang and Honavar (1998) argued against the monotonicity assumption, presenting genetic algorithms as an alternative to escape the local minimas. Pintelas (2004) explained it as follows: The features are represented as a binary string where zeros represent the absence of features and ones represent their presence. Genetic operations such as mutation (adding or removing a feature by reversing the value of the bit representing it) and crossover (combining two subsets together) take place on the strings and better feature subsets have more chance to produce newer subsets via more mutations and crossovers.\nOne idea proposed by Yu and Liu (2004), is to start with individual feature selection first, to eliminating irrelevant features Then subset selection is performed later to remove redundant features. By decoupling the two processes, they downsize the search space for the subset selection, hence improving its performance. However, this contradicts with\nwhat Kohavi-1997 warned of, where an irrelevant feature on its own can still form, among others, an optimal subset.\nIn each iteration, the generated subset has to be evaluated. Dash and Liu (2003) explained that this step compares the new subset with the previously acquired ones, or with predefined optimum threshold to decide (i) whether the new subset should replace the previous best subset, and also (ii) whether a stopping criterion has been reached to prevent the algorithm from doing exhaustive search.\nThe way evaluation is done is what subdivides feature selection into to two main categories: filters and wrappers (Hall, 1999; Liu et al., 2010). The two approaches are discussed in the next section."
    }, {
      "heading" : "3 Filters and Wrappers",
      "text" : "Filters and wrappers are two evaluation strategies. In filters, individual features or subsets are evaluated independently of the learning algorithms, while wrappers use the learning algorithm to evaluate feature subsets (Estévez et al., 2009).\nGheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al., 2005; Estévez et al., 2009), chi-square test (Jin et al., 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007).\nFor individual selection, Lewis (1992) measures the mutual information (MI) between each feature and the target class label. Then features are ranked accordingly, selecting the top n features. Hamming (1986) stated the following equation to calculate the MI between two variables, A = [a1, a− 2, ..an] and B = [b1, b2, ..bn]:\nI(A,B) = ΣiΣjPr(ai, bj) log Pr(ai, bj)\nPr(ai) ∗ Pr(bj)\nIt is clear from the previous equation that for features with equal conditional probability with a class, the rare ones get higher scores than common ones (Yang and Pedersen, 1997). On contrary, Yang and Pedersen (1997) added that Chi-Squared values are normalized, but it is not suitably for rare features, since they hardly follow X2 distribution. Linear correlation coefficient is another option, however Yu and Liu (2004); Gómez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used.\nIt worth mentioning here that some papers, such as Yang and Pedersen (1997), discriminate between Information Gain (IG) and Mutual Information (MI), however Cover and Thomas (2006, p. 21) showed that the MI formula mentioned above is the same one referred to by Quinlan (1986); Hall (1999) as IG.\nTraditionally, filter methods select features individually. One idea is to calculate MI between class labels and subsets instead of individual features. However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a “minimalredundancy-maximal-relevance” (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Estévez et al. (2009) built on this idea. Similarly, Torkkola (2003) proposed the use of Renyi’s entropy as an alternative to Shannon et al. (1949)’s entropy to solve the multivariate issue, whereas Markov blanket,\npresented Koller and Sahami (1996), is one other solution.\nThe absence of target labels in unsupervised learning encouraged He et al. (2006) to use Laplacian Score (LS). LS assumes that a relevant feature is the one where neighbouring records across the whole feature space are also close across this feature vector (He et al., 2006). They added that LS yields to Fisher Criterion Score (FCS) when target labels are available. Yan et al. (2007); Fu et al. (2008) highlighted that these methods assume classes to be normally distribution across the data-space. Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI\nAlthough filters are normally faster than wrappers, John et al. (1994) warned that it doesn’t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm’s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation. Compared to filters, Gheyas and Smith (2010) stated that wrapper’s effectiveness comes at the expense of their computational cost. Because of their cost, Pintelas (2004) noticed that the forward selection algorithms (mentioned earlier) might be more common with wrappers, even if it is less effective than the backward selection. He et al. (2006) also added that wrappers are common in unsupervised learning scenarios, since filters, other than Laplacian Score, usually rely on class labels to calculate the correlations between features and those labels."
    }, {
      "heading" : "4 FS and Learning Algorithms",
      "text" : "We have stated earlier that good features are not only the ones highly correlated with the target class, but also the ones not correlated with each other. Kohavi and John (1997) highlighted that the accuracy of instance-based algorithms is vulnerable to the former, while Naive-Bayes is more robust when faced with the former yet vulnerable to the latter. Nearest neighbour (NN) algorithm is an examples of instance based learning. (Witten and Frank, 2005, p. 116) added that the adoption of k-NN, where (k > 1), can smooth the effect of noisy data a bit, hence variance.\nLal et al. (2006) remarked that some learning algorithms, such as decision trees (DT), select relevant features implicitly. Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step. Nevertheless, decision tree still need earlier feature selection, as noted by Kohavi and John (1997).\nAdditionally, Kohavi and John (1997) noticed in their experiments that different search algorithms work better with different learning algorithms as well as datasets. Similarly, experiments by Hua et al. (2009) proved different feature-selection methods to give various accuracy across different sample sizes and data nature."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have seen that wrappers are generally more accurate then filters, yet the latter is more computational efficient. Similar trade-offs exist between selecting the features individually or as a subset, as well as between the different search algorithms. However, experiments\nshowed that the nature of the dataset, the robustness of the classifier and the nature of the learning problem dictates our choices between those trade-offs. Additionally, there are efforts being put to make filter methods suitable to subset selection and unsupervised learning scenarios."
    } ],
    "references" : [ {
      "title" : "Feature selection for high-dimensional data, a pearson redundancy based filter",
      "author" : [ "J. Biesiada", "W. Duch" ],
      "venue" : "Computer Recognition Systems",
      "citeRegEx" : "Biesiada and Duch.,? \\Q2007\\E",
      "shortCiteRegEx" : "Biesiada and Duch.",
      "year" : 2007
    }, {
      "title" : "Consistency-based search in feature selection",
      "author" : [ "M. Dash", "H. Liu" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Dash and Liu.,? \\Q2003\\E",
      "shortCiteRegEx" : "Dash and Liu.",
      "year" : 2003
    }, {
      "title" : "Hybrid feature selection: combining fisher criterion and mutual information for efficient feature selection",
      "author" : [ "C. Dhir", "S. Lee" ],
      "venue" : "Advances in Neuro-Information Processing,",
      "citeRegEx" : "Dhir and Lee.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dhir and Lee.",
      "year" : 2009
    }, {
      "title" : "Minimum redundancy feature selection from microarray gene expression data",
      "author" : [ "C. Ding", "H. Peng" ],
      "venue" : "Journal of bioinformatics and computational biology,",
      "citeRegEx" : "Ding and Peng.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ding and Peng.",
      "year" : 2005
    }, {
      "title" : "The role of occam’s razor in knowledge discovery",
      "author" : [ "P. Domingos" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "Domingos.,? \\Q1999\\E",
      "shortCiteRegEx" : "Domingos.",
      "year" : 1999
    }, {
      "title" : "Data Mining: Introductory and Advanced Topics",
      "author" : [ "Margaret H. Dunham" ],
      "venue" : null,
      "citeRegEx" : "Dunham.,? \\Q2002\\E",
      "shortCiteRegEx" : "Dunham.",
      "year" : 2002
    }, {
      "title" : "Normalized mutual information feature selection",
      "author" : [ "P.A. Estévez", "M. Tesmer", "C.A. Perez", "J.M. Zurada" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "Estévez et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Estévez et al\\.",
      "year" : 2009
    }, {
      "title" : "On bias, variance, 0/1loss, and the curse-of-dimensionality",
      "author" : [ "J.H. Friedman" ],
      "venue" : "Data mining and knowledge discovery,",
      "citeRegEx" : "Friedman.,? \\Q1997\\E",
      "shortCiteRegEx" : "Friedman.",
      "year" : 1997
    }, {
      "title" : "Classification and feature extraction by simplexization",
      "author" : [ "Y. Fu", "S. Yan", "T.S. Huang" ],
      "venue" : "Information Forensics and Security, IEEE Transactions on,",
      "citeRegEx" : "Fu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2008
    }, {
      "title" : "Feature subset selection in large dimensionality domains",
      "author" : [ "I.A. Gheyas", "L.S. Smith" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Gheyas and Smith.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gheyas and Smith.",
      "year" : 2010
    }, {
      "title" : "Information-theoretic feature selection for functional data classification",
      "author" : [ "V. Gómez-Verdejo", "M. Verleysen", "J. Fleury" ],
      "venue" : null,
      "citeRegEx" : "Gómez.Verdejo et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gómez.Verdejo et al\\.",
      "year" : 2009
    }, {
      "title" : "An introduction to variable and feature selection",
      "author" : [ "I. Guyon", "A. Elisseeff" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Guyon and Elisseeff.,? \\Q2003\\E",
      "shortCiteRegEx" : "Guyon and Elisseeff.",
      "year" : 2003
    }, {
      "title" : "Correlation-based feature selection for machine learning",
      "author" : [ "M.A. Hall" ],
      "venue" : "PhD thesis, The University of Waikato,",
      "citeRegEx" : "Hall.,? \\Q1999\\E",
      "shortCiteRegEx" : "Hall.",
      "year" : 1999
    }, {
      "title" : "Coding and information theory",
      "author" : [ "R.W. Hamming" ],
      "venue" : "Prentice-Hall, Inc.,",
      "citeRegEx" : "Hamming.,? \\Q1986\\E",
      "shortCiteRegEx" : "Hamming.",
      "year" : 1986
    }, {
      "title" : "Laplacian score for feature selection",
      "author" : [ "X. He", "D. Cai", "P. Niyogi" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "He et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2006
    }, {
      "title" : "Performance of feature-selection methods in the classification of high-dimension data",
      "author" : [ "J. Hua", "W.D. Tembe", "E.R. Dougherty" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Hua et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hua et al\\.",
      "year" : 2009
    }, {
      "title" : "On the relationship between feature selection and classification accuracy",
      "author" : [ "A.G.K. Janecek", "W.N. Gansterer", "M. Demel", "G.F. Ecker" ],
      "venue" : "In JMLR: Workshop and Conference Proceedings,",
      "citeRegEx" : "Janecek et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Janecek et al\\.",
      "year" : 2008
    }, {
      "title" : "Machine learning techniques and chi-square feature selection for cancer classification using sage gene expression profiles",
      "author" : [ "X. Jin", "A. Xu", "R. Bie", "P. Guo" ],
      "venue" : "Data Mining for Biomedical Applications,",
      "citeRegEx" : "Jin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2006
    }, {
      "title" : "Irrelevant features and the subset selection problem",
      "author" : [ "G.H. John", "R. Kohavi", "K. Pfleger" ],
      "venue" : "In Proceedings of the eleventh international conference on machine learning,",
      "citeRegEx" : "John et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "John et al\\.",
      "year" : 1994
    }, {
      "title" : "Wrappers for feature subset selection",
      "author" : [ "R. Kohavi", "G.H. John" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Kohavi and John.,? \\Q1997\\E",
      "shortCiteRegEx" : "Kohavi and John.",
      "year" : 1997
    }, {
      "title" : "Toward optimal feature selection",
      "author" : [ "D. Koller", "M. Sahami" ],
      "venue" : null,
      "citeRegEx" : "Koller and Sahami.,? \\Q1996\\E",
      "shortCiteRegEx" : "Koller and Sahami.",
      "year" : 1996
    }, {
      "title" : "Feature selection and feature extraction for text categorization",
      "author" : [ "D.D. Lewis" ],
      "venue" : "In Proceedings of the workshop on Speech and Natural Language,",
      "citeRegEx" : "Lewis.,? \\Q1992\\E",
      "shortCiteRegEx" : "Lewis.",
      "year" : 1992
    }, {
      "title" : "Feature selection: An ever evolving frontier in data mining",
      "author" : [ "H. Liu", "H. Motoda", "R. Setiono", "Z. Zhao" ],
      "venue" : "In Proc. The Fourth Workshop on Feature Selection in Data Mining,",
      "citeRegEx" : "Liu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "On feature selection, bias-variance, and bagging",
      "author" : [ "M. Munson", "R. Caruana" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "Munson and Caruana.,? \\Q2009\\E",
      "shortCiteRegEx" : "Munson and Caruana.",
      "year" : 2009
    }, {
      "title" : "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy",
      "author" : [ "H. Peng", "F. Long", "C. Ding" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Peng et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2005
    }, {
      "title" : "On the selection of classifier-specific feature selection algorithms",
      "author" : [ "S.B.K.P.E. Pintelas" ],
      "venue" : "In Proceedings of International Conference on Intelligent Knowledge Systems (IKS-2004),",
      "citeRegEx" : "Pintelas.,? \\Q2004\\E",
      "shortCiteRegEx" : "Pintelas.",
      "year" : 2004
    }, {
      "title" : "Induction of decision trees",
      "author" : [ "J.R. Quinlan" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Quinlan.,? \\Q1986\\E",
      "shortCiteRegEx" : "Quinlan.",
      "year" : 1986
    }, {
      "title" : "The mathematical theory of communication, volume 117",
      "author" : [ "C.E. Shannon", "W. Weaver", "R.E. Blahut", "B. Hajek" ],
      "venue" : "University of Illinois press Urbana,",
      "citeRegEx" : "Shannon et al\\.,? \\Q1949\\E",
      "shortCiteRegEx" : "Shannon et al\\.",
      "year" : 1949
    }, {
      "title" : "Feature extraction by non parametric mutual information maximization",
      "author" : [ "K. Torkkola" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Torkkola.,? \\Q2003\\E",
      "shortCiteRegEx" : "Torkkola.",
      "year" : 2003
    }, {
      "title" : "Data Mining: Practical machine learning tools and techniques",
      "author" : [ "I.H. Witten", "E. Frank" ],
      "venue" : null,
      "citeRegEx" : "Witten and Frank.,? \\Q2005\\E",
      "shortCiteRegEx" : "Witten and Frank.",
      "year" : 2005
    }, {
      "title" : "Graph embedding and extensions: A general framework for dimensionality reduction",
      "author" : [ "S. Yan", "D. Xu", "B. Zhang", "H.J. Zhang", "Q. Yang", "S. Lin" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Yan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2007
    }, {
      "title" : "Feature subset selection using a genetic algorithm",
      "author" : [ "J. Yang", "V. Honavar" ],
      "venue" : "Intelligent Systems and Their Applications, IEEE,",
      "citeRegEx" : "Yang and Honavar.,? \\Q1998\\E",
      "shortCiteRegEx" : "Yang and Honavar.",
      "year" : 1998
    }, {
      "title" : "A comparative study on feature selection in text categorization",
      "author" : [ "Y. Yang", "J.O. Pedersen" ],
      "venue" : "In MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-,",
      "citeRegEx" : "Yang and Pedersen.,? \\Q1997\\E",
      "shortCiteRegEx" : "Yang and Pedersen.",
      "year" : 1997
    }, {
      "title" : "Efficient feature selection via analysis of relevance and redundancy",
      "author" : [ "L. Yu", "H. Liu" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Yu and Liu.,? \\Q2004\\E",
      "shortCiteRegEx" : "Yu and Liu.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "For an accurate classifier, it is needed to reduce both bias and variance of the model (Friedman, 1997).",
      "startOffset" : 87,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "1 Introduction According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones.",
      "startOffset" : 28,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "1 Introduction According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones. Classification is an example of predictive models. Friedman (1997) described it as a model where discrete output values (class labels) are learnt from the different variables (features) of the input data.",
      "startOffset" : 28,
      "endOffset" : 180
    }, {
      "referenceID" : 4,
      "context" : "1 Introduction According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones. Classification is an example of predictive models. Friedman (1997) described it as a model where discrete output values (class labels) are learnt from the different variables (features) of the input data. Clustering, on the other hand, is categorised by Dunham (2002) as a descriptive task.",
      "startOffset" : 28,
      "endOffset" : 381
    }, {
      "referenceID" : 4,
      "context" : "1 Introduction According to Dunham (2002), machine learning tasks can be seen as predictive or descriptive ones. Classification is an example of predictive models. Friedman (1997) described it as a model where discrete output values (class labels) are learnt from the different variables (features) of the input data. Clustering, on the other hand, is categorised by Dunham (2002) as a descriptive task. The features of the input data are used to categorize it without supervised training. In both cases, the choice of the feature-set plays an important role in the performance of the data mining problem. Liu et al. (2010) listed three advantages for removing irrelevant and redundant features: it makes the data mining task more efficient, improves its accuracy and simplifies the inferred model, making it more comprehensible.",
      "startOffset" : 28,
      "endOffset" : 624
    }, {
      "referenceID" : 4,
      "context" : "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it.",
      "startOffset" : 16,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa.",
      "startOffset" : 16,
      "endOffset" : 441
    }, {
      "referenceID" : 4,
      "context" : "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa. This is known as “bias-variance trade-off”. Hence, as noted by Kohavi and John (1997), classifiers faced with limited data has to find an optimum point where they can actually estimate the statistical distribution of fewer features (variance reduction) versus less accurate estimation of more features (bias reduction); ergo, Munson and Caruana (2009) summarized the feature selection process as the process of finding the best bias-variance trade-off point.",
      "startOffset" : 16,
      "endOffset" : 694
    }, {
      "referenceID" : 4,
      "context" : "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa. This is known as “bias-variance trade-off”. Hence, as noted by Kohavi and John (1997), classifiers faced with limited data has to find an optimum point where they can actually estimate the statistical distribution of fewer features (variance reduction) versus less accurate estimation of more features (bias reduction); ergo, Munson and Caruana (2009) summarized the feature selection process as the process of finding the best bias-variance trade-off point.",
      "startOffset" : 16,
      "endOffset" : 960
    }, {
      "referenceID" : 4,
      "context" : "As described by Domingos (1999), the bias is a systematic error that occurs when inferring a more generalized model for the data, hence increasing the training data will not improve it. Variance, on the other hand, results when the model tries to cope with the variations of the noisy data sample. Increasing the sample size in this case can balance the effect of the noise and reduce the variance accordingly. Nevertheless, Friedman (1997) stresses that during the training process, the more sensitive the model is to the training data, the lower the bias in exchange for a higher variance, and vice versa. This is known as “bias-variance trade-off”. Hence, as noted by Kohavi and John (1997), classifiers faced with limited data has to find an optimum point where they can actually estimate the statistical distribution of fewer features (variance reduction) versus less accurate estimation of more features (bias reduction); ergo, Munson and Caruana (2009) summarized the feature selection process as the process of finding the best bias-variance trade-off point. When it comes to unsupervised learning algorithms, such as clustering, Janecek et al. (2008) highlighted that the problem with high dimensional data (more features) is that it makes the proximity measures between the records more uniform, hence metrics such as distance and density become harder to obtain.",
      "startOffset" : 16,
      "endOffset" : 1160
    }, {
      "referenceID" : 33,
      "context" : "In its simplest form, the feature selection process can evaluate individual features and rank them based on their correlation with class labels (Yu and Liu, 2004).",
      "startOffset" : 144,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "However, Hall (1999) reported that studies had proven a good feature subset to be the one whose features are not correlated to each other, besides them being correlated to class labels.",
      "startOffset" : 9,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "However, Hall (1999) reported that studies had proven a good feature subset to be the one whose features are not correlated to each other, besides them being correlated to class labels. Hence, they are better evaluated as subset rather than individually. Liu et al. (2010) summarized subset feature selection process into three main steps:",
      "startOffset" : 9,
      "endOffset" : 273
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003).",
      "startOffset" : 203,
      "endOffset" : 223
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms.",
      "startOffset" : 204,
      "endOffset" : 244
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa.",
      "startOffset" : 204,
      "endOffset" : 463
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process.",
      "startOffset" : 204,
      "endOffset" : 614
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process.",
      "startOffset" : 204,
      "endOffset" : 627
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features.",
      "startOffset" : 204,
      "endOffset" : 739
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search.",
      "startOffset" : 204,
      "endOffset" : 1196
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space.",
      "startOffset" : 204,
      "endOffset" : 1438
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space. Yang and Honavar (1998) argued against the monotonicity assumption, presenting genetic algorithms as an alternative to escape the local minimas.",
      "startOffset" : 204,
      "endOffset" : 1652
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space. Yang and Honavar (1998) argued against the monotonicity assumption, presenting genetic algorithms as an alternative to escape the local minimas. Pintelas (2004) explained it as follows: The features are represented as a binary string where zeros represent the absence of features and ones represent their presence.",
      "startOffset" : 204,
      "endOffset" : 1789
    }, {
      "referenceID" : 1,
      "context" : "Additionally, instead of searching within all possible subsets, it can stop after reaching certain number of features or iterations, or when an optimum subset is reached according to the evaluation step (Dash and Liu, 2003). John et al. (1994) listed two search algorithms. The forward selection algorithms starts with an empty set and keeps adding features, while the backward elimination starts with all the features and keeps on removing ones. Pintelas (2004) explained that in the two algorithms, once a feature is added it cannot be removed and vice versa. Thus, they are described by Yang and Honavar (1998); Hall (1999) as greedy hill-climbing algorithms, where they assume monotonicity of the whole process. Kohavi and John (1997) added that dealing with smaller subset in the beginning makes the forward selection faster than the backward elimination algorithm and can reach relatively fewer features more quickly, yet the latter usually selects more interactive features. This makes Forward Selection preferable with high dimensional data. Generally, hill-climbing algorithms might get caught in local minima and fail to include useful features, or exclude irrelevant ones. Hall (1999) mentioned another algorithm; best first search. Unlike the hill climbing algorithms, at each step it generates all possible moves and allow for backtracking once the path it is traversing is not adding any improvement. Kohavi and John (1997) highlighted the importance of having a stoppage criterion, to limit the generation of possible moves at each step, in order to prevent the algorithm from traversing the entire search space. Yang and Honavar (1998) argued against the monotonicity assumption, presenting genetic algorithms as an alternative to escape the local minimas. Pintelas (2004) explained it as follows: The features are represented as a binary string where zeros represent the absence of features and ones represent their presence. Genetic operations such as mutation (adding or removing a feature by reversing the value of the bit representing it) and crossover (combining two subsets together) take place on the strings and better feature subsets have more chance to produce newer subsets via more mutations and crossovers. One idea proposed by Yu and Liu (2004), is to start with individual feature selection first, to eliminating irrelevant features Then subset selection is performed later to remove redundant features.",
      "startOffset" : 204,
      "endOffset" : 2276
    }, {
      "referenceID" : 12,
      "context" : "The way evaluation is done is what subdivides feature selection into to two main categories: filters and wrappers (Hall, 1999; Liu et al., 2010).",
      "startOffset" : 114,
      "endOffset" : 144
    }, {
      "referenceID" : 22,
      "context" : "The way evaluation is done is what subdivides feature selection into to two main categories: filters and wrappers (Hall, 1999; Liu et al., 2010).",
      "startOffset" : 114,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "Dash and Liu (2003) explained that this step compares the new subset with the previously acquired ones, or with predefined optimum threshold to decide (i) whether the new subset should replace the previous best subset, and also (ii) whether a stopping criterion has been reached to prevent the algorithm from doing exhaustive search.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "In filters, individual features or subsets are evaluated independently of the learning algorithms, while wrappers use the learning algorithm to evaluate feature subsets (Estévez et al., 2009).",
      "startOffset" : 169,
      "endOffset" : 191
    }, {
      "referenceID" : 21,
      "context" : "Gheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al., 2005; Estévez et al., 2009), chi-square test (Jin et al.",
      "startOffset" : 79,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "Gheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al., 2005; Estévez et al., 2009), chi-square test (Jin et al.",
      "startOffset" : 79,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "Gheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al., 2005; Estévez et al., 2009), chi-square test (Jin et al.",
      "startOffset" : 79,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : ", 2009), chi-square test (Jin et al., 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007).",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : ", 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007).",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "In filters, individual features or subsets are evaluated independently of the learning algorithms, while wrappers use the learning algorithm to evaluate feature subsets (Estévez et al., 2009). Gheyas and Smith (2010) listed some filter methods such as: mutual information (Lewis, 1992; Peng et al.",
      "startOffset" : 170,
      "endOffset" : 217
    }, {
      "referenceID" : 0,
      "context" : ", 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007). For individual selection, Lewis (1992) measures the mutual information (MI) between each feature and the target class label.",
      "startOffset" : 46,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : ", 2006) and Pearson correlation coefficients (Biesiada and Duch, 2007). For individual selection, Lewis (1992) measures the mutual information (MI) between each feature and the target class label. Then features are ranked accordingly, selecting the top n features. Hamming (1986) stated the following equation to calculate the MI between two variables, A = [a1, a− 2, .",
      "startOffset" : 46,
      "endOffset" : 280
    }, {
      "referenceID" : 32,
      "context" : "I(A,B) = ΣiΣjPr(ai, bj) log Pr(ai, bj) Pr(ai) ∗ Pr(bj) It is clear from the previous equation that for features with equal conditional probability with a class, the rare ones get higher scores than common ones (Yang and Pedersen, 1997).",
      "startOffset" : 210,
      "endOffset" : 235
    }, {
      "referenceID" : 24,
      "context" : "I(A,B) = ΣiΣjPr(ai, bj) log Pr(ai, bj) Pr(ai) ∗ Pr(bj) It is clear from the previous equation that for features with equal conditional probability with a class, the rare ones get higher scores than common ones (Yang and Pedersen, 1997). On contrary, Yang and Pedersen (1997) added that Chi-Squared values are normalized, but it is not suitably for rare features, since they hardly follow X distribution.",
      "startOffset" : 211,
      "endOffset" : 275
    }, {
      "referenceID" : 24,
      "context" : "I(A,B) = ΣiΣjPr(ai, bj) log Pr(ai, bj) Pr(ai) ∗ Pr(bj) It is clear from the previous equation that for features with equal conditional probability with a class, the rare ones get higher scores than common ones (Yang and Pedersen, 1997). On contrary, Yang and Pedersen (1997) added that Chi-Squared values are normalized, but it is not suitably for rare features, since they hardly follow X distribution. Linear correlation coefficient is another option, however Yu and Liu (2004); Gómez-Verdejo et al.",
      "startOffset" : 211,
      "endOffset" : 480
    }, {
      "referenceID" : 8,
      "context" : "Linear correlation coefficient is another option, however Yu and Liu (2004); Gómez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used.",
      "startOffset" : 77,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "Linear correlation coefficient is another option, however Yu and Liu (2004); Gómez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used. It worth mentioning here that some papers, such as Yang and Pedersen (1997), discriminate between Information Gain (IG) and Mutual Information (MI), however Cover and Thomas (2006, p.",
      "startOffset" : 77,
      "endOffset" : 315
    }, {
      "referenceID" : 8,
      "context" : "Linear correlation coefficient is another option, however Yu and Liu (2004); Gómez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used. It worth mentioning here that some papers, such as Yang and Pedersen (1997), discriminate between Information Gain (IG) and Mutual Information (MI), however Cover and Thomas (2006, p. 21) showed that the MI formula mentioned above is the same one referred to by Quinlan (1986); Hall (1999) as IG.",
      "startOffset" : 77,
      "endOffset" : 516
    }, {
      "referenceID" : 8,
      "context" : "Linear correlation coefficient is another option, however Yu and Liu (2004); Gómez-Verdejo et al. (2009) warned that the assumption of linear relation between variables and classes is not usually valid; therefore, MI is still widely used. It worth mentioning here that some papers, such as Yang and Pedersen (1997), discriminate between Information Gain (IG) and Mutual Information (MI), however Cover and Thomas (2006, p. 21) showed that the MI formula mentioned above is the same one referred to by Quinlan (1986); Hall (1999) as IG.",
      "startOffset" : 77,
      "endOffset" : 529
    }, {
      "referenceID" : 3,
      "context" : "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a “minimalredundancy-maximal-relevance” (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Estévez et al.",
      "startOffset" : 9,
      "endOffset" : 332
    }, {
      "referenceID" : 3,
      "context" : "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a “minimalredundancy-maximal-relevance” (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Estévez et al. (2009) built on this idea.",
      "startOffset" : 9,
      "endOffset" : 358
    }, {
      "referenceID" : 3,
      "context" : "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a “minimalredundancy-maximal-relevance” (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Estévez et al. (2009) built on this idea. Similarly, Torkkola (2003) proposed the use of Renyi’s entropy as an alternative to Shannon et al.",
      "startOffset" : 9,
      "endOffset" : 405
    }, {
      "referenceID" : 3,
      "context" : "However, Ding and Peng (2005) explained that the more variables in our joint probabilities, the harder it is for our limited sample to cover the multivariate density. Hence, they proposed a “minimalredundancy-maximal-relevance” (mRMR) formula, which accounts for both inter-features and feature-to-class MI. Both Peng et al. (2005) and Estévez et al. (2009) built on this idea. Similarly, Torkkola (2003) proposed the use of Renyi’s entropy as an alternative to Shannon et al. (1949)’s entropy to solve the multivariate issue, whereas Markov blanket,",
      "startOffset" : 9,
      "endOffset" : 484
    }, {
      "referenceID" : 14,
      "context" : "LS assumes that a relevant feature is the one where neighbouring records across the whole feature space are also close across this feature vector (He et al., 2006).",
      "startOffset" : 146,
      "endOffset" : 163
    }, {
      "referenceID" : 18,
      "context" : "Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm’s accuracy while using that specific subset (John et al., 1994).",
      "startOffset" : 193,
      "endOffset" : 212
    }, {
      "referenceID" : 14,
      "context" : "presented Koller and Sahami (1996), is one other solution.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "The absence of target labels in unsupervised learning encouraged He et al. (2006) to use Laplacian Score (LS).",
      "startOffset" : 65,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "The absence of target labels in unsupervised learning encouraged He et al. (2006) to use Laplacian Score (LS). LS assumes that a relevant feature is the one where neighbouring records across the whole feature space are also close across this feature vector (He et al., 2006). They added that LS yields to Fisher Criterion Score (FCS) when target labels are available. Yan et al. (2007); Fu et al.",
      "startOffset" : 65,
      "endOffset" : 386
    }, {
      "referenceID" : 7,
      "context" : "(2007); Fu et al. (2008) highlighted that these methods assume classes to be normally distribution across the data-space.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn’t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness.",
      "startOffset" : 7,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn’t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm’s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation.",
      "startOffset" : 7,
      "endOffset" : 529
    }, {
      "referenceID" : 2,
      "context" : "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn’t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm’s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation. Compared to filters, Gheyas and Smith (2010) stated that wrapper’s effectiveness comes at the expense of their computational cost.",
      "startOffset" : 7,
      "endOffset" : 684
    }, {
      "referenceID" : 2,
      "context" : "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn’t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm’s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation. Compared to filters, Gheyas and Smith (2010) stated that wrapper’s effectiveness comes at the expense of their computational cost. Because of their cost, Pintelas (2004) noticed that the forward selection algorithms (mentioned earlier) might be more common with wrappers, even if it is less effective than the backward selection.",
      "startOffset" : 7,
      "endOffset" : 809
    }, {
      "referenceID" : 2,
      "context" : "Hence, Dhir and Lee (2009) proposed a hybrid measurement based on FCS and MI Although filters are normally faster than wrappers, John et al. (1994) warned that it doesn’t take into its consideration the biases of the learning algorithm during subset selection, after showing the wrappers effectiveness. Wrapper methods use the learning algorithm, during the evaluation step, to determine the utility of a certain features subset based on the algorithm’s accuracy while using that specific subset (John et al., 1994). Hall (1999) added that the training data is usually divided into folds and accuracy is determined using cross-validation. Compared to filters, Gheyas and Smith (2010) stated that wrapper’s effectiveness comes at the expense of their computational cost. Because of their cost, Pintelas (2004) noticed that the forward selection algorithms (mentioned earlier) might be more common with wrappers, even if it is less effective than the backward selection. He et al. (2006) also added that wrappers are common in unsupervised learning scenarios, since filters, other than Laplacian Score, usually rely on class labels to calculate the correlations between features and those labels.",
      "startOffset" : 7,
      "endOffset" : 986
    }, {
      "referenceID" : 17,
      "context" : "Kohavi and John (1997) highlighted that the accuracy of instance-based algorithms is vulnerable to the former, while Naive-Bayes is more robust when faced with the former yet vulnerable to the latter.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "Kohavi and John (1997) highlighted that the accuracy of instance-based algorithms is vulnerable to the former, while Naive-Bayes is more robust when faced with the former yet vulnerable to the latter. Nearest neighbour (NN) algorithm is an examples of instance based learning. (Witten and Frank, 2005, p. 116) added that the adoption of k-NN, where (k > 1), can smooth the effect of noisy data a bit, hence variance. Lal et al. (2006) remarked that some learning algorithms, such as decision trees (DT), select relevant features implicitly.",
      "startOffset" : 0,
      "endOffset" : 435
    }, {
      "referenceID" : 11,
      "context" : "Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step. Nevertheless, decision tree still need earlier feature selection, as noted by Kohavi and John (1997). Additionally, Kohavi and John (1997) noticed in their experiments that different search algorithms work better with different learning algorithms as well as datasets.",
      "startOffset" : 0,
      "endOffset" : 283
    }, {
      "referenceID" : 11,
      "context" : "Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step. Nevertheless, decision tree still need earlier feature selection, as noted by Kohavi and John (1997). Additionally, Kohavi and John (1997) noticed in their experiments that different search algorithms work better with different learning algorithms as well as datasets.",
      "startOffset" : 0,
      "endOffset" : 321
    }, {
      "referenceID" : 11,
      "context" : "Guyon and Elisseeff (2003) added that in those embedded methods of feature selection, the selection process takes place during the training phase, rather than in preprocessing step. Nevertheless, decision tree still need earlier feature selection, as noted by Kohavi and John (1997). Additionally, Kohavi and John (1997) noticed in their experiments that different search algorithms work better with different learning algorithms as well as datasets. Similarly, experiments by Hua et al. (2009) proved different feature-selection methods to give various accuracy across different sample sizes and data nature.",
      "startOffset" : 0,
      "endOffset" : 495
    } ],
    "year" : 2015,
    "abstractText" : "Feature selection plays an important role in the data mining process. It is needed to deal with the excessive number of features, which can become a computational burden on the learning algorithms. It is also necessary, even when computational resources are not scarce, since it improves the accuracy of the machine learning tasks, as we will see in the upcoming sections. In this review, we discuss the different feature selection approaches, and the relation between them and the various machine learning algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}