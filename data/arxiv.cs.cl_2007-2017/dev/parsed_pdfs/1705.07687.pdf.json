{
  "name" : "1705.07687.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "W2VLDA: Almost Unsupervised System for Aspect Based Sentiment Analysis",
    "authors" : [ "Aitor Garćıa-Pablosa", "Montse Cuadros", "German Rigau" ],
    "emails" : [ "agarciap@vicomtech.org", "mcuadros@vicomtech.org", "german.rigau@ehu.eus" ],
    "sections" : [ {
      "heading" : null,
      "text" : "With the increase of online customer opinions in specialised websites and social networks, the necessity of automatic systems to help to organise and classify customer reviews by domain-specific aspect/categories and sentiment polarity is more important than ever. Supervised approaches for Aspect Based Sentiment Analysis obtain good results for the domain/language they are trained on, but having manually labelled data for training supervised systems for all domains and languages is usually very costly and time consuming. In this work we describe W2VLDA, an almost unsupervised system based on topic modelling, that combined with some other unsupervised methods and a minimal configuration, performs aspect/category classification, aspectterms/opinion-words separation and sentiment polarity classification for any given domain and language. We evaluate the performance of the aspect and sentiment classification in the multilingual SemEval 2016 task 5 (ABSA) dataset. We show competitive results for several languages (English, Spanish, French and Dutch) and domains (hotels, restaurants, electronic devices).\nKeywords: Sentiment Analysis, Almost Unsupervised, Multilingual, Multidomain"
    }, {
      "heading" : "1. Introduction",
      "text" : "During the last decade, the Web has become one of the most important sources for customers and providers to evaluate and compare products\n∗Corresponding author Email addresses: agarciap@vicomtech.org (Aitor Garćıa-Pablos),\nmcuadros@vicomtech.org (Montse Cuadros), german.rigau@ehu.eus (German Rigau)\nPreprint submitted to Expert Systems with Applications July 19, 2017\nar X\niv :1\n70 5.\n07 68\n7v 2\n[ cs\n.C L\n] 1\n8 Ju\nand services. The vast amount of content generated every day in countless websites and social networks keeps growing and requires automated ways to handle and classify all these opinions. Because of that, many different algorithms and approaches have been developed in the area of Opinion Mining.\nOpinion Mining is a subfield of Natural Language Processing (NLP) that deals with the automatic analysis of opinions shared by humans in different contexts, like in customer reviews (Pang and Lee, 2008; Liu, 2012). Aspect Based Sentiment Analysis (ABSA) refers to the systems that determine the opinions or sentiments expressed on different features or aspects of the products/services under evaluation (e.g. battery or performance for a laptop). An ABSA system should be capable of classifying each opinion according to the aspects relevant for each domain in addition to classifying its sentiment polarity (usually positive, negative or neutral), as depicted in figure 1.\nBest performing ABSA systems generally use manually labelled data and language specific resources for training on a particular domain and for a particular language (Pontiki et al., 2014, 2015, 2016). This is the case of deep-learning based systems, that provide very good performance but require a significant amount of labelled data for training (Chen et al., 2017; Araque et al., 2017).\nOn the other hand, weakly-supervised systems do not require labelled data for training, but they usually need some language specific resources, such as carefully curated lists of seed words or language dependent tools to preprocess the input (Lin et al., 2011; Jo and Oh, 2011; Kim et al., 2013). In addition, most of these works only report results for English.\nIn this work, we present W2VLDA, an almost unsupervised system for multilingual and multidomain ABSA, that works leveraging large quantities of unlabelled textual data and an initial configuration consisting of a minimal set of seed words. Figure 2 shows an schema of W2VLDA. Imagine the following scenario. The owners of a famous restaurant want to monitor the opinion of their costumers with respect to a set of aspects. In particular,\nthey want to know the opinion about its food, service, price, ambience, location, etc. The input of W2VLDA is a corpus of customer reviews and an example word per aspect they want to monitor (for instance, chicken for the aspect food, service for the aspect service, etc.)1. With this input, W2VLDA produces two main outputs. First, a weighted list of words per aspect (for instance, chicken, salad, burger, etc. for the aspect food), a weighted list of positive words (tasty, yummy, homemade, etc.) and weighted list of negative words (soggy, tasteless, burnt, etc.) for every selected aspect. Thus, our system performs at a word level three subtasks simultaneously: aspect classification, aspect-term/opinion-word separation, and sentiment polarity classification. Second, W2VLDA also produces a weighted list of sentences for every selected domain aspect and polarity.\nThe system is based on a topic modelling approach combined with continuous word embeddings and a Maximum Entropy classifier. It runs over\n1W2VLDA also needs an example of a positive and a negative word (for instance, excellent and horrible).\nan unlabelled corpus of the target language and domain just by defining the desired aspects with a single seed-word per aspect. We show results for different domains (restaurants, hotels, electronic devices) and languages (English, Spanish, French and Dutch). We compare its performance with other topic modelling based approaches, and we evaluate the performance of this approach on the SemEval2016 task 5 dataset, which provides a manually labelled set of restaurant reviews for several languages, including English, Spanish, French and Dutch. The contributions of this work are the minimal need of supervision (just one seed word per aspect/polarity) to perform ABSA over any unlabelled corpus of customer reviews. The lack of language or domain specific requirements allows the system to be readily used for other languages and domains. Another contribution is the automatic separation of the topic words into aspect-terms, positive words and negative words to improve the readability of the generated topics. We will leave the source code publicly available2.\nAfter this short introduction, the paper is structured as follows. First, section 2 reviews previous related work. Then, section 3 describes our system, including the seed-word based configuration, the aspect-term/opinion-word separation and the topic modelling part. After that, section 4 shows the results and evaluation. Finally, section 5 describes the conclusions and future work."
    }, {
      "heading" : "2. Related work",
      "text" : "During the last decade the research community has addressed the problem of analysing user opinions, particularly focused on online customer reviews (Liu et al., 2012; Chen et al., 2014). The problem of customer opinion analysis can be divided into several subtasks, such as detecting the aspect (aspect classification) and detecting the opinion about the aspect of the product being evaluated.\nA common approach in the literature is to identify frequent nouns, lexical patterns, dependency relations applying supervised machine learning approaches (Hu and Liu, 2004; Popescu and Etzioni, 2007; Blair-Goldensohn et al., 2008; Wu et al., 2009; Qiu et al., 2011). Some works focus on automatically deriving the most likely polarity for words, constructing a so-called sentiment lexicon (Mostafa, 2013). The typical approaches use different variants\n2https://bitbucket.org/aitor-garcia-p/w2vlda-last/overview\nof bootstrapping or polarity propagation leveraging some base dictionaries and pre-existing linguistic resources (Rao and Ravichandran, 2009; Jijkoun et al., 2010; Huang et al., 2014).\nA well-known unsupervised method for text modelling documents is Latent Dirichlet Allocation (LDA). LDA is a generative model introduced by (Blei et al., 2003) that quickly gained popularity because it is an unsupervised, flexible and extensible technique to model documents. LDA models documents as multinomial distributions of so-called topics. Topics are multinomial distributions of words over a fixed vocabulary. Topics can be interpreted as the categories from which each document is built up, and they can be used for several kinds of tasks, like dimensionality reduction or unsupervised clustering. Due to its flexibility, LDA has been extended and combined with other approaches, obtaining topic models that improve the resulting topics or that model additional information (Mcauliffe and Blei, 2008; Ramage et al., 2009).\nTopic models have been applied to Sentiment Analysis to jointly model topics and sentiment of words (Lin et al., 2009, 2011; Jo and Oh, 2011; Lu et al., 2011; Kim et al., 2013; Alam et al., 2016). A usual way to guide a topic modelling process towards a particular objective is to bias the LDA hyperparameters using certain apriori information. In the case of modelling the polarity of the documents, it usually means using a carefully selected set of seed words. Our method follows this idea, but replaces the need for a carefully crafted list of language or domain polarity words by only a single domain independent positive word (e.g. excellent for English) and a single domain independent negative word (e.g. horrible for English).\nIn general, topics coming from a topic modelling approach are anonymous word distributions, requiring an additional step to map them to a meaningful domain category. This task requires a manual inspection by an expert or a mapping calculation to an existing resource (Bhatia et al., 2016). Our approach relies on a minimal topic configuration to define the topics for the target domain the user wants to monitor. Thus, the resulting topics match the ones defined initially by the user. This is done by leveraging semantic word similarities to guide the topic modelling towards the defined topics. This semantic word similarity is obtained using continuous word embeddings over the domain words. Continuous word embeddings are known for capturing semantic regularities of words (Mikolov et al., 2013a; Collobert and Weston, 2008). Some works have made use of this fact to improve the resulting topics (Das et al., 2015; Nguyen et al., 2015; Qiang et al., 2016), but\ntheir objective is to improve the unsupervised modelling of a corpus instead of guiding the model towards a predefined set of topics. There are works that exploit word embeddings in a supervised machine learning setting to perform sentiment analysis (Tang et al., 2014; Giatsoglou et al., 2017).\nSome authors have also attempted an automatic aspect-term/opinionword separation within the topic modelling process (Zhao et al., 2010; Mukherjee and Liu, 2012). Aspect terms are the words that are used to speak about the aspect being evaluated (e.g. waiter or waitstaff when speaking about the service of a restaurant). On the other hand, opinion words express the sentiment about an aspect, such as attentive or terrible. The separation of these two kinds of words might be useful because it eases the interpretation of the resulting topics, and the sentiment classification can be focused on the opinion-words which are more likely to bear sentiment information. Zhao et al. (2010) attempted this separation training a supervised classifier on a small manually labelled dataset and using Part-of-Speech tagging. Mukherjee and Liu (2012) elaborated on this idea trying a similar approach but substituting the manually labelled dataset with an existing lexicon of opinion words for English. Instead, we apply Brown clustering (Brown et al., 1992) to a set of training instances from an unlabelled corpus in order to train an aspect-term/opinion-word classifier that is later integrated into the topic modelling process. Following this approach, no additional languagedependent resources are required, and the full process could be applied to any language and domain.\nIn summary, combining topic modelling, continuous word embeddings and a minimal topic definition, our proposed approach can model customer reviews in different languages and domains performing three subtasks at the same time: aspect classification, sentiment classification and aspectterms/opinion-words separation. To our knowledge, no other almost unsupervised system tries to perform these three tasks at the same time and without requiring any pre-existing language or domain dependent resource."
    }, {
      "heading" : "3. System description",
      "text" : "The main objective of the W2VLDA system is to perform the three tasks (detecting aspects, opinions and their polarity) of Aspect Based Sentiment Analysis at the same time. That is, to classify pieces of text into a predefined set of domain aspects and classify their sentiment polarity as positive or negative. In addition, our system separates opinion words from aspect terms\nwithout requiring additional resources or supervision. The system at its core consists of an LDA-based topic model extended with additional variables, with biased topic modelling hyperparameters based on continuous word embeddings, and combined with unsupervised pre-trained classification model for aspect-term/opinion-word separation."
    }, {
      "heading" : "3.1. Topics and sentiment configuration",
      "text" : "W2VLDA only requires a minimal domain aspect and sentiment polarity configuration per language and domain. The configuration consists of a single seed to define each desired domain aspect, plus a single general positive seed word and a single general negative seed word valid for all domain aspects. This simple configuration is the only language and domain dependent information required by W2VLDA 3. Therefore, a simple translation of the seeds should suffice to make the system work for another language or domain, as long as each translated seed has an equivalent meaning and use in the target language. Table 1 shows an example of a domain aspect and polarity configuration for the restaurant domain in several languages."
    }, {
      "heading" : "3.2. Aspect-term and opinion-word separation",
      "text" : "Part of the outcome of the system consists of the aspect-term/opinionword separation into differentiated word classes. In order to achieve this separation without adding any language dependent tool or resource, the system\n3A list of general stopwords for each target language is also necessary in order to obtain better results. We use the stopword lists from Apache Lucene.\nuses Brown clusters (Brown et al., 1992) to model examples of aspect-terms and opinion-words and train a MaxEnt-based classification model. Brown clusters have been used as unsupervised features with good results in supervised Part-of-Speech tagging (Turian et al., 2010) and Named Entity Recognition (Agerri and Rigau, 2016). Brown clusters are computed4 from the domain unlabelled corpus with no additional supervision, and are used as the features for the two words context window, [-2,+2], of each training example. The training instances are obtained leveraging the occurrences of the initial configuration with aspects and polarity seed words, assuming that domain aspect seed words are aspect-terms and polarity-words are opinion-words.\nFigure 3 describes the process to obtain the classification model. First domain aspect seed words and polarity seed words are used as gold aspect-terms and gold opinion-words respectively. Then the occurrences of these words are bootstrapped from the domain corpus and they are modelled according to their context window. Next, context words are replaced by their corresponding Brown cluster to build each training instance. Finally, a MaxEnt model\n4We use the Brown clustering implementation at https://github.com/ koendeschacht/brown-cluster\nis trained using these generated training instances. We have experimented with a different number of Brown clusters (100, 200, 500, 1000 and 2000) but the impact of this parameter was negligible for this purpose. The reported results have been obtained using 200 clusters.\nA drawback of this approach is that every word in the vocabulary will be classified as aspect-term or as opinion-word. There are words that do not belong to any of these categories. It would be interesting to have a third class (e.g. ”other”), but it would require labelling training instances for that additional class, introducing a manual supervision that we want to keep to a minimum. We assume that the words that are not clearly aspect-terms or opinion-words will be spread across both classes, losing relevance during the topic modelling process."
    }, {
      "heading" : "3.3. Combining everything in a topic model",
      "text" : "The core of the system consists of an LDA-based topic model, extended to include the aspect-term/opinion-word separation and the positive/negative separation for each topic. In addition, the aspect-term/opinion-word separation is guided by a pre-trained classifier as explained at section 3.2, while the topic and polarity modelling are guided by biasing certain hyper-parameters according to the given topic configuration.\nFigure 4 shows the proposed model in plate notation and the generative story modelled by the algorithm.\nThe generative hypothesis described by the model is the following. For each document d a distribution of topics, θd, is sampled from a Dirichlet distribution with parameter αd, which is a vector with asymmetric topic priors for that document. Note that in this context each document corresponds to individual sentences instead of full texts. Then for each word n in document d a topic value is drawn: zd,n ∼ Multi(θd), z ∈ {1..T}. Then a aspect-term/opinion switch variable is sampled: yd,n ∼ Bernoulli(πd,n), y ∈ {A,O}. Depending on yd,n, the word wd,n is emitted from the topic aspect terms distribution (φzd,n,A) or else, a polarity value vd,n is sampled from Ωd to choose if the word has to be drawn from φzd,n,P or φzd,n,N (positive and negative words respectively).\nThe model guides the topic and polarity modelling towards the desired values by biasing the hyper-parameters that govern the Dirichlet distributions from which the topics and words are sampled. In a standard LDA setting those hyper-parameters (commonly named α and β) are symmetric\nbecause no apriori information about the topic and word distributions is assumed. In our model, these hyper-parameters are biased using a similarity calculation among the words of the domain corpus and the topic seed words of the initial configuration. This similarity measure is based on the cosine distance between the dense vector representation of the topic defining seeds and each word of the vocabulary. Such a dense vector representation of the words over a particular vocabulary, commonly referred as word embeddings, could be obtained using any distributional semantics approach, but in this work we stick to the well-known word2vec (Mikolov et al., 2013a). Word embeddings are a very popular way of representing words as the input for a variety of machine learning techniques and are known for encoding interesting syntactic and semantic properties (Mikolov et al., 2013b). In this case, we exploit the semantic similarity among words that can be calculated using the cosine distance of the resulting word vectors. The similarity, sim, is the value between a word and a set of words (e.g. some topic defining seeds), and it is calculated using 1.\nsim(w, t) = argmax v∈t sim(w, v) (1)\nWhere w is any word found in the domain corpus, v is any of the seed words chosen to define topic t, and sim stands for the cosine distance between\ntwo word vectors. The α hyper-parameters control the topic probability distribution for each document as in the original LDA. But instead of having a single symmetric α value, each document has a biased α for each topic, based on semantic word similarity, as described in 2.\nαd,t =\nNd∑ i sim(wd,i, t)\nT∑ t′ Nd∑ i sim(wd,i, t′) ∗ αbase (2)\nOn the other hand, the β hyper-parameters, which control the distribution of words for each topic, are calculated in a similar way, as shown in 3 and 4.\nβt,w = sim(w, t) ∗ βbase (3)\nβq,w = sim(w, q) ∗ βbase q ∈ {P,N} (4)\nFinally, the δ hyper-parameters control the polarity distribution for each document, and they are calculated for each document as shown in 5.\nδd,q =\nNd∑ i sim(wd,i, q)\n∑ q′ {P,N} Nd∑ i sim(wd,i, q′) ∗ δbase (5)\nIn the formulas wd,i is the i-th word of the document d, Nd is the number of words in that document, t is a topic from the set of defined topics T . Similarly q is a pre-defined polarity words set, P for positives and N for negatives (in our experiments P only contains excellent and N only contains horrible for English, or their equivalents for other languages).\nαbase, βbase and δbase are configurable hyper-parameters, analogous to the symmetric α and β in the original LDA model.\nIn addition to the bias of these hyper-parameters, the distribution π that governs each binary aspect-term/opinion-word switching variable, y, is set from the pre-trained aspect-term/opinion-word classifier probabilities applied to each word and its context features as described in section 3.2.\nThe posterior inference of the model is obtained via Gibbs sampling (Griffiths and Steyvers, 2004). Let wd,n be the n-th word of the d-th document, given the assignment of all other variables, its topic assignment zd,n is sampled using (6). Analogously, the aspect-term/opinion-word assignment yd,n and the polarity of the opinion-words, vd,n are sampled using (7) and (8) respectively.\np(zd,n = t|z−d,n, y−d,n, v−d,n, ·) ∝ nt,Awd,n + β t,A wd,n\nV∑ v nt,Av + β t,A v\n× nt,Pwd,n + β t,P wd,n\nV∑ v nt,Pv + β t,P v\n× nt,Nwd,n + β t,N wd,n\nV∑ v nt,Nv + β t,N v ×(nd,t+αd,t)\n(6)\np(yd,n = u|zd,n = t, ·) ∝ nt,uwd,n + β t,u wd,n\nV∑ v nt,uv + β t,u v\n× exp(λu × xd,n)∑\nu′∈{A,O} exp(λu′ ∗ xd,n) (7)\np(vd,n = q|zd,n = t, ·) ∝ nt,qwd,n + β t,q wd,n\nV∑ v′ nt,q v′d,n + βt,q v′d,n × (nd,q + δd,q) (8)\nIn these formulas, nt,uwd,n is the number of times the vocabulary term corresponding to wd,n has been assigned to topic t and word-type u ∈ {A,O} (i.e. Aspect-terms or Opinion-words), nd,t is the number of words in the document d assigned to topic t, λu are the pre-trained aspect-term/opinionword classifier model weights for word-type u and xd,n is the feature vector for wd,n, composed by the Brown clusters of the context words. Analogously, nt,qwd,n is the number of times wd,n has been assigned to topic t and polarity q ∈ {P,N} and nd,q is the number of words in the document d assigned to polarity q."
    }, {
      "heading" : "4. Evaluation",
      "text" : "We evaluate W2VLDA for the three different subtasks that it performs: topic (aspect) classification, sentiment classification, and aspect-term/opinionword separation. First, we compare W2VLDA with other LDA-based methods. Then, we also evaluate W2VLDA in a multilingual ABSA dataset comparing its performance classifying topics (aspects) and sentiment with some supervised machine learning approaches trained on labelled data.\nWe show results for several datasets, demonstrating how the system works for different languages and domains just by changing the topic configuration, composed of a single seed word per each desired topic, language and domain.\nFor instance, table 2 shows some of the resulting words for several domains (restaurants and electronic devices), topics (food, service, ambience for restaurants, and warranty, design and price for electronic devices) for English customer reviews, including the automatic separation of aspect-terms from positive and negative words per topic. Table 3 shows the equivalent information for restaurants and hotel reviews in Spanish and French.\nLikewise, table 4 shows examples of sentences classified under different topics (food, service, ambience for restaurants, and staff, ambience and location for hotels) for several domains (restaurants and hotels) and languages (Spanish and French)."
    }, {
      "heading" : "4.1. Resources and experimental setting",
      "text" : "In order to evaluate W2VLDA, we use the following resources. For topic classification we use the dataset from (Ganu et al., 2009) which contains restaurant reviews labelled with domain-related categories (e.g. food, staff, ambience) for English. For sentiment classification, we use the Laptops and DIGITAL-SLR dataset (Jo and Oh, 2011), consisting of English reviews of electronic products with their corresponding 5-star rating.\nAdditional multilingual experiments have been performed using the SemEval2016 task 5 datasets (Pontiki et al., 2016). In particular, the restaurant reviews datasets which are labelled with domain-related categories and polarity for six languages.\nIn order to compute the topic model and the word embeddings, we have automatically gathered additional customer reviews about restaurants from some popular customer review websites. These unlabelled domain corpora consist of a few thousand restaurant reviews in English, Spanish, French and\nDutch. We use word2vec to compute the word embeddings that are used for the\nword similarity calculation. In particular, we use the Apache Spark MLlib 5 implementation with default parameters to compute the domain-based word embeddings.\nTable 1 shows the topic definition used in the experiments for the domain of restaurants, just one word per topic. Unless stated otherwise, the polarity seeds for every domain are excellent and horrible or their equivalents in other languages.\nThe values for αbase, βbase and δbase mentioned in 3.3, which play a similar role to α and β in the original LDA, are set to the values commonly recommended in the literature (Griffiths and Steyvers, 2004): 50/T for αbase and δbase being T the number of topics, and 0.01 for βbase. The topic modelling process runs for 500 iterations in every experiment with a burn-in period of 100 iterations and a sampling lag of 10 iterations."
    }, {
      "heading" : "4.2. Comparison with other LDA based approaches",
      "text" : "First, we evaluate W2VLDA in a topic classification setting using the restaurant reviews dataset from (Ganu et al., 2009). This dataset contains few thousand reviews from restaurants, classified into several categories but the authors report results only for the three main categories: food, ambience and staff. We compare W2VLDA against the results reported in (Zhao et al., 2010) for two LDA-based approaches, LocLDA (Brody and Elhadad, 2010) and ME-LDA (Zhao et al., 2010).\nLocLDA and ME-LDA are LDA-based approaches, and thus, unsupervised. But the results reported in the experiment involved some supervision as described in Zhao et al. (2010). First, the authors computed a topic model of 14 topics. Then the authors examine each topic and manually set a label according to their judgment. W2VLDA provides already named topics at the end of the process, so no manual topic inspection and labelling are required. In order to assign a topic label to a particular sentence, we use the resulting topic distribution for that sentence (θd) selecting the topic with highest posterior probability.\nTable 5 shows the results of the experiment and the comparison with the other systems. Despite not requiring human intervention to relabel the obtained topics unlike the other two systems, W2VLDA obtains slightly better overall results.\n5http://spark.apache.org/mllib/\nWe also evaluate the ability of W2VLDA to assign correct polarities to customer reviews. We use the estimated polarity distribution of a sentence (Ωd) to assign to a review the polarity with the highest probability. We compare our polarity classification results with respect to those from JST (Lin et al., 2011), ASUM (Jo and Oh, 2011) and HASM (Kim et al., 2013). The evaluation runs over the laptops and digital SLRs subset obtained from the Amazon Electronics dataset6. As explained at (Kim et al., 2013) two datasets are used, a small dataset containing 1000 reviews with 1 star rating (strong negative) and 1000 5 stars (strong positive), and a large dataset with additional 1000 reviews of 2 stars (negative) as well as 1000 reviews of 4 stars (positive). The baseline consists of a simple polarity seed word count, using the polarity seed words from (Turney and Littman, 2003), assigning to the sentence the polarity with the greatest proportion. As stated in previous sections, W2VLDA uses just a single polarity seed for each sentiment polarity, excellent and horrible respectively.\nFigure 5 shows the result of this comparison. W2VLDA obtains comparable results for the small dataset and better results for the big dataset despite using only a single seed word to define each polarity."
    }, {
      "heading" : "4.3. Multilingual evaluation on SemEval2016 dataset",
      "text" : "We use the SemEval 2016 task 5 datasets (Pontiki et al., 2016) in order to perform a multilingual evaluation of W2VLDA. SemEval 2016 datasets consist of restaurant reviews in several languages. The reviews are split by sentence and labelled with the explicit aspect term mentions, the coarsegrained category they belong to, and the polarity for that category.\nSemEval 2016 restaurants datasets are annotated for six coarse-grained categories: food, service, ambience, drinks, location, and restaurant. The last\n6Available at http://uilab.kaist.ac.kr/research/WSDM11/\ncategory, restaurant acts as a miscellaneous category that is used when the sentence does not refer to any other specific category but to the restaurant as a whole. Such an abstract concept cannot be represented by a seed word, so we omit this category from the evaluation. To avoid ambiguities and simplify the classification of a sentence, we only keep sentences with a single category label. Finally, since the categories drinks and location have very little representation in the datasets (below the 5% of the instances), we keep only the three main categories: food, service and ambience.\nTable 6 and table 7 show the distribution of categories and polarities respectively for the resulting datasets, for four languages: English, Spanish, French and Dutch.\nSince W2VLDA is a topic modelling, it needs a reasonable amount of domain documents to build the statistical model. To cope with this require-\nment, we have implemented a script to automatically extract restaurant reviews of the required languages from an online customer reviews website. Due to copyright permissions, we cannot share these reviews, but table 8 shows the number of reviews used to feed the algorithm. The polarity mentioned on the table is based on the number of the stars from the 5-star rating (as usual, 1-2 stars meaning negative and 4-5 starts meaning positive). As it can be observed in the table, for some languages the script has not found an equal number of positive and negative reviews. We tried to compensate this fact with oversampling, to pair the number of positive and negative reviews before running the algorithm. In this case we oversample negative examples for each language until they equal in number the positive ones (i.e. 10k). Note that in the case of Dutch this may lead to an excessive oversampling\ndue to the small number of available negatives examples. Also note that these polarities are just to get an insight of the polarity distribution of the datasets, but they are not used for any sort of supervised training.\nThe evaluation experiment is done as follows. For each language, we use the downloaded reviews to run the algorithm. It includes calculating the domain word embeddings, Brown clusters and the topic model estimation. Using the generated model for each language the topic and polarity distributions, θ and Ω, are estimated for each of the sentences of the evaluation set. The topic with the highest probability in the estimated topic distribution for that sentence is assigned as the category label (i.e. domain aspect). Analogously, the polarity with the highest probability in the estimated polarity distribution for that sentence is assigned as the polarity label. The assigned category is compared to the gold category, and the accuracy (ratio of correctly labelled examples) is calculated. The same process is followed to calculate the polarity classification accuracy.\nThe obtained accuracy is compared to several baselines. First, two supervised baselines are used. One is a Naive-Bayes classifier (NB), trained using the labelled sentences. The sentences are transformed to bag-of-words vectors with a vocabulary size of 80k words and normalised using tf-idf weights. The other supervised baseline is a Multilayer Perceptron algorithm (MLP), with two hidden layers, and the same tf-idf vector as input. Another baseline is the majority baseline, that shows the accuracy that can be obtained in the case of choosing the most frequent class. This is only to ensure that the datasets are not excessively unbalanced and the algorithms are really learning relevant information. Finally, the last baseline (W2VLDA NO) is the same W2VLDA but replacing the word-embeddings similarity mechanism to bias the topic modelling hyper-priors. Instead of using the word-embedding"
    }, {
      "heading" : "NB 0.492 0.497 0.472 0.457",
      "text" : "similarity to calculate a bias for every word, only the configured seed words receive a strong bias for their corresponding topic or polarity.\nTable 9 shows the evaluation results for the domain aspects classification (food, service, ambience). Since the evaluation datasets are not completely balanced for each of the domain aspects (see table 6), we run the evaluation on several balanced subsets created by random sampling the base datasets for each language. Each balanced subset contains 100 sentences from each domain aspect. We do this five times generating five different subsets, and we use these subsets to evaluate the baselines and W2VLDA. The results on each individual subset are obtained using the average accuracy applying a 10-fold cross validation. We calculate the average and standard deviation of the results on each subset to perform a t-test of statistical significance. W2VLDA outperforms the baselines with a 95% of confidence for all the languages except for Dutch, which despite obtaining better results than the baselines it only achieves a 80% on confidence in the statistical significance test.\nTable 10 shows the evaluation results for the polarity classification (positive and negative). The calculation of the results and the statistical significance tests have been performed in the same way than for the domain aspect classification. Again, W2VLDA outperforms the baselines with a 95% on confidence in the statistical test, except for Dutch. A possible reason for this is that the oversampling performed for the unlabelled Dutch reviews for the topic modelling was excessive, or the data contained in it was less representative than for other languages (see table 8). Studying which are the lower"
    }, {
      "heading" : "NB 0.672 0.577 0.587 0.563",
      "text" : "bounds of the required amount of data would be an interesting problem that we let for future research."
    }, {
      "heading" : "4.4. Assessing the seed words impact",
      "text" : "Since the proposed approach heavily relies on the seed words (i.e. seeds words are the only source of supervision to guide the algorithm to the desired goal), it is interesting to evaluate the impact of different seed words and their combination.\nWe perform some experiments for English using the SemEval 2016 restaurant reviews dataset and several combinations of seed words for the target domain aspects and sentiment polarities. In the first experiment group, for each run we only change the seed words that define the domain aspects. The polarity seeds remain the same.\nWe use three different seed words for each domain aspect, in particular: food, chicken and burger for domain aspect FOOD ; service, staff and waiter for domain aspect SERVICE ; and ambience, atmosphere and décor for domain aspect AMBIENCE. We try different permutations and combinations of the seed words, including pairs of seed words for each domain aspect, and finally also the combination of the three seed words together. Table 11 show the results for this experiment. The results show that the accuracy is stable across all the combinations regardless of the chosen seed words. As expected, some combinations perform better than others but overall the average if high and the standard deviation is below 5% of the accuracy. The best result is obtained using all the seed words at the same time. This last fact is not\nsurprising, since with more seeds the semantic coverage to guide the algorithm to the desired domain aspects is increased (as long as the seed words are semantically coherent with the domain aspect they are defining).\nAnother fact that can be observed in the table is that domain aspect seed words do not affect the polarity results, as it would be expected. The polarity results show minor variations among the experiments, but the standard deviation is only a 0.8% of the accuracy.\nAnalogously to the domain aspect seed words, we have performed some experiments with the polarity words. We have tested several combinations of seeds with opposed polarity: excellent - horrible, awesome - awful, etc. Table 12 show the results. Even with seed words of less extreme polarity, like good - bad, the results are quite stable. We also test combining more than a single word for each polarity, and as the results table shows, combining the three seed words for each polarity achieves the best result. The standard deviation for all the experiment runs is just a 1.2% of the accuracy. Similarly to what was observed for the domain aspects, the polarity seed words do not seem to affect the domain aspect classification accuracy, with only a 1.2% on standard deviation for all the runs.\nFinally, in order to perform a sanity check to evaluate if the sentiment polarity classification is really depending on the correct selection of the polarity seed words, we perform two more runs using misleading words as polarity seeds. In particular, we use cat and waitress for positives and dog and waiter for negatives. The use of these words as polarity seeds is obviously incorrect,\nand what we want to check is if using such meaningless words (for polarity) leads to bad polarity classification results. Table 13 shows the results for this experiment, confirming that the election of representative polarity seed words is relevant to correctly guide the algorithm."
    }, {
      "heading" : "4.5. Aspect-term/Opinion-word separation evaluation",
      "text" : "Finally we experiment with the aspect-term and opinion-word separation. As described in section 3.2, W2VLDA models the domain words into separated word distributions: aspect terms or opinion words.\nIn order to evaluate the accuracy of this words separation, we use Bing Liu’s polarity lexicon for English (Hu and Liu, 2004). Since sentiment lexicons contain words bearing some specific sentiment, we treat the words contained in this lexicon as a ground-truth for opinion-words. In addition, we use the gold aspect-terms labelled in the SemEval 2016 dataset as a ground-truth for aspect-terms.\nThe experiment now consists of running the W2VLDA again on the restaurant review dataset and counting how many times a word from the opinion words ground-truth is classified as an opinion word, and how many times each word from the aspect terms ground-truth is classified as an aspect term. Then the proportion of correct assignments is calculated. If the automatic aspect-term / opinion-word separation is correct, the proportion of opinion words and aspect terms correctly classified should be high.\nWe perform several experiments varying the number of Brown clusters involved in the process (see section 3.2) to evaluate if it has a noticeable impact on the word separation. Figure 7 shows the resulting proportions of correctly assigned aspect terms and opinion words for English. In general, the correct proportions are high compared to a random assignment, which indicates that the aspect-term/opinion-word separation performs correctly most of the times. Interestingly, aspect-terms are better distinguished than opinion-words."
    }, {
      "heading" : "5. Conclusions and future work",
      "text" : "In this document, we have presented W2VLDA, a system that performs aspect and sentiment classification with almost no supervision and without\nthe need of language or domain specific resources. In order to do that, the system combines different unsupervised approaches, like word embeddings or Latent Dirichlet Allocation (LDA), to bootstrap information from a domain corpus. The only supervision required by the user is a single seed word per desired aspect and polarity. Because of that, the system can be applied to datasets of different languages and domains with almost no adaptation. The resulting topics and polarities are directly paired with the aspect names selected by the user at the beginning, so the output can be used to perform Aspect Based Sentiment Analysis. In addition, the system tries to separate automatically aspect terms and opinion words, providing more clear information and insight to the resulting domain aspects vocabulary. We evaluate W2VLDA for aspect classification using customer reviews of several domains and compare it against other LDA-based approaches. We also evaluate its performance using a subset of the multilingual SemEval 2016 task 5 ABSA dataset. As future work, it would we interesting to include an automated way to deal with stop-words and other words that do not carry information for the ABSA task. A better-integrated handling of multi-word and negation expressions could also improve the results. On the other hand, the are more specialised word embeddings related to sentiment analysis (Rothe et al., 2016), and it would be interesting to study if different word embeddings bring improvements to the method keeping a minimal supervision."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by Vicomtech-IK4 and by the project TUNER - TIN2015-65308-C5-1-R (MINECO/FEDER, UE)."
    } ],
    "references" : [ {
      "title" : "Robust multilingual named entity recognition with shallow semi-supervised features",
      "author" : [ "R. Agerri", "G. Rigau" ],
      "venue" : "Artificial Intelligence, 238:63 – 82.",
      "citeRegEx" : "Agerri and Rigau,? 2016",
      "shortCiteRegEx" : "Agerri and Rigau",
      "year" : 2016
    }, {
      "title" : "Joint multi-grain topic sentiment: Modeling semantic aspects for online reviews",
      "author" : [ "M.H. Alam", "W.J. Ryu", "S.K. Lee" ],
      "venue" : "Information Sciences, 339:206–223. 26",
      "citeRegEx" : "Alam et al\\.,? 2016",
      "shortCiteRegEx" : "Alam et al\\.",
      "year" : 2016
    }, {
      "title" : "Enhancing deep learning sentiment analysis with ensemble techniques in social applications",
      "author" : [ "O. Araque", "I. Corcuera-Platas", "J.F. Sánchez-Rada", "C.A. Iglesias" ],
      "venue" : "Expert Systems with Applications, 77:236– 246.",
      "citeRegEx" : "Araque et al\\.,? 2017",
      "shortCiteRegEx" : "Araque et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic labelling of topics with neural embeddings",
      "author" : [ "S. Bhatia", "J.H. Lau", "T. Baldwin" ],
      "venue" : "arXiv preprint arXiv:1612.05340.",
      "citeRegEx" : "Bhatia et al\\.,? 2016",
      "shortCiteRegEx" : "Bhatia et al\\.",
      "year" : 2016
    }, {
      "title" : "Building a sentiment summarizer for local service reviews",
      "author" : [ "S. Blair-Goldensohn", "K. Hannan", "R. McDonald", "T. Neylon", "G.A. Reis", "J. Reynar" ],
      "venue" : "WWW workshop on NLP in the information explosion era, volume 14, pages 339–348.",
      "citeRegEx" : "Blair.Goldensohn et al\\.,? 2008",
      "shortCiteRegEx" : "Blair.Goldensohn et al\\.",
      "year" : 2008
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "the Journal of machine Learning research, 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "An unsupervised aspect-sentiment model for online reviews",
      "author" : [ "S. Brody", "N. Elhadad" ],
      "venue" : "The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, (June):804–812.",
      "citeRegEx" : "Brody and Elhadad,? 2010",
      "shortCiteRegEx" : "Brody and Elhadad",
      "year" : 2010
    }, {
      "title" : "Class-based n-gram models of natural language",
      "author" : [ "P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai" ],
      "venue" : "Computational linguistics, 18(4):467–479.",
      "citeRegEx" : "Brown et al\\.,? 1992",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1992
    }, {
      "title" : "Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN",
      "author" : [ "T. Chen", "R. Xu", "Y. He", "X. Wang" ],
      "venue" : "Expert Systems with Applications, 72:221–230.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Aspect extraction with automated prior knowledge learning",
      "author" : [ "Z. Chen", "A. Mukherjee", "B. Liu" ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 347–358.",
      "citeRegEx" : "Chen et al\\.,? 2014",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "R. Collobert", "J. Weston" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.",
      "citeRegEx" : "Collobert and Weston,? 2008",
      "shortCiteRegEx" : "Collobert and Weston",
      "year" : 2008
    }, {
      "title" : "Gaussian LDA for Topic Models with Word Embeddings",
      "author" : [ "R. Das", "M. Zaheer", "C. Dyer" ],
      "venue" : "Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics, pages 795–804. 27",
      "citeRegEx" : "Das et al\\.,? 2015",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2015
    }, {
      "title" : "Beyond the stars: Improving rating predictions using review text content",
      "author" : [ "G. Ganu", "N. Elhadad", "A. Marian" ],
      "venue" : "WebDB, volume 9, pages 1–6. Citeseer.",
      "citeRegEx" : "Ganu et al\\.,? 2009",
      "shortCiteRegEx" : "Ganu et al\\.",
      "year" : 2009
    }, {
      "title" : "Sentiment analysis leveraging emotions and word embeddings",
      "author" : [ "M. Giatsoglou", "M.G. Vozalis", "K. Diamantaras", "A. Vakali", "G. Sarigiannidis", "K.C. Chatzisavvas" ],
      "venue" : "Expert Systems with Applications, 69:214–224.",
      "citeRegEx" : "Giatsoglou et al\\.,? 2017",
      "shortCiteRegEx" : "Giatsoglou et al\\.",
      "year" : 2017
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "T.L. Griffiths", "M. Steyvers" ],
      "venue" : "Proceedings of the National Academy of Sciences, 101(suppl 1):5228–5235.",
      "citeRegEx" : "Griffiths and Steyvers,? 2004",
      "shortCiteRegEx" : "Griffiths and Steyvers",
      "year" : 2004
    }, {
      "title" : "Mining opinion features in customer reviews",
      "author" : [ "M. Hu", "B. Liu" ],
      "venue" : "AAAI, volume 4, pages 755–760.",
      "citeRegEx" : "Hu and Liu,? 2004",
      "shortCiteRegEx" : "Hu and Liu",
      "year" : 2004
    }, {
      "title" : "Automatic construction of domain-specific sentiment lexicon based on constrained label propagation",
      "author" : [ "S. Huang", "Z. Niu", "C. Shi" ],
      "venue" : "Knowledge-Based Systems, 56:191–200.",
      "citeRegEx" : "Huang et al\\.,? 2014",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating focused topic-specific sentiment lexicons",
      "author" : [ "V. Jijkoun", "M. de Rijke", "W. Weerkamp" ],
      "venue" : "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Jijkoun et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jijkoun et al\\.",
      "year" : 2010
    }, {
      "title" : "Aspect and sentiment unification model for online review analysis",
      "author" : [ "Y. Jo", "A.H. Oh" ],
      "venue" : "Proceedings of the fourth ACM international conference on Web search and data mining, pages 815–824. ACM.",
      "citeRegEx" : "Jo and Oh,? 2011",
      "shortCiteRegEx" : "Jo and Oh",
      "year" : 2011
    }, {
      "title" : "A Hierarchical Aspect-Sentiment Model for Online Reviews",
      "author" : [ "S. Kim", "J. Zhang", "Z. Chen", "A. Oh", "S. Liu" ],
      "venue" : "Proceedings of the TwentySeventh AAAI Conference on Artificial Intelligence, pages 526–533.",
      "citeRegEx" : "Kim et al\\.,? 2013",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2013
    }, {
      "title" : "Weakly supervised joint sentiment-topic detection from text",
      "author" : [ "C. Lin", "Y. He", "R. Everson", "S. Rüger" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 24:1134–1145.",
      "citeRegEx" : "Lin et al\\.,? 2011",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2011
    }, {
      "title" : "Joint Sentiment / Topic Model for Sentiment Analysis",
      "author" : [ "C. Lin", "N.P. Road", "E. Ex" ],
      "venue" : "Cikm, pages 375–384.",
      "citeRegEx" : "Lin et al\\.,? 2009",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2009
    }, {
      "title" : "Sentiment analysis and opinion mining",
      "author" : [ "B. Liu" ],
      "venue" : "Synthesis Lectures on Human Language Technologies, 5(1):1–167. 28",
      "citeRegEx" : "Liu,? 2012",
      "shortCiteRegEx" : "Liu",
      "year" : 2012
    }, {
      "title" : "Opinion target extraction using wordbased translation model",
      "author" : [ "K. Liu", "L. Xu", "J. Zhao" ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, (July):1346–1356.",
      "citeRegEx" : "Liu et al\\.,? 2012",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2012
    }, {
      "title" : "Multi-aspect sentiment analysis with topic models",
      "author" : [ "B. Lu", "M. Ott", "C. Cardie", "B.K. Tsou" ],
      "venue" : "Proceedings - IEEE International Conference on Data Mining, ICDM, pages 81–88.",
      "citeRegEx" : "Lu et al\\.,? 2011",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2011
    }, {
      "title" : "Supervised topic models",
      "author" : [ "J.D. Mcauliffe", "D.M. Blei" ],
      "venue" : "Advances in neural information processing systems, pages 121–128.",
      "citeRegEx" : "Mcauliffe and Blei,? 2008",
      "shortCiteRegEx" : "Mcauliffe and Blei",
      "year" : 2008
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781, pages 1–12.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "T. Mikolov", "Yih", "W.-t.", "G. Zweig" ],
      "venue" : "Proceedings of NAACL-HLT, pages 746–751.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "More than words: Social networks text mining for consumer brand sentiments",
      "author" : [ "M.M. Mostafa" ],
      "venue" : "Expert Systems with Applications, 40(10):4241–4251.",
      "citeRegEx" : "Mostafa,? 2013",
      "shortCiteRegEx" : "Mostafa",
      "year" : 2013
    }, {
      "title" : "Aspect extraction through semi-supervised modeling",
      "author" : [ "A. Mukherjee", "B. Liu" ],
      "venue" : "ACL ’12 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, (July):339– 348.",
      "citeRegEx" : "Mukherjee and Liu,? 2012",
      "shortCiteRegEx" : "Mukherjee and Liu",
      "year" : 2012
    }, {
      "title" : "Improving topic models with latent feature word representations",
      "author" : [ "D.Q. Nguyen", "R. Billingsley", "L. Du", "M. Johnson" ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:299–313.",
      "citeRegEx" : "Nguyen et al\\.,? 2015",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "B. Pang", "L. Lee" ],
      "venue" : "Foundations and trends in information retrieval, 2(1-2):1–135.",
      "citeRegEx" : "Pang and Lee,? 2008",
      "shortCiteRegEx" : "Pang and Lee",
      "year" : 2008
    }, {
      "title" : "Semeval-2015 task 12: Aspect based sentiment analysis",
      "author" : [ "M. Pontiki", "D. Galanis", "H. Papageorgiou", "S. Manandhar", "I. Androutsopoulos" ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Association for Computational Linguistics, Denver, Colorado, pages 486–495.",
      "citeRegEx" : "Pontiki et al\\.,? 2015",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2015
    }, {
      "title" : "Semeval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "M. Pontiki", "D. Galanis", "J. Pavlopoulos", "H. Papageorgiou", "I. Androutsopoulos", "S. Manandhar" ],
      "venue" : "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), Association for Computational Linguistics, Dublin, Ireland, pages 27–35.",
      "citeRegEx" : "Pontiki et al\\.,? 2014",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "Extracting product features and opinions from reviews",
      "author" : [ "Popescu", "A.-M.", "O. Etzioni" ],
      "venue" : "Natural language processing and text mining, pages 9–28. Springer.",
      "citeRegEx" : "Popescu et al\\.,? 2007",
      "shortCiteRegEx" : "Popescu et al\\.",
      "year" : 2007
    }, {
      "title" : "Topic Modeling over Short Texts by Incorporating Word Embeddings",
      "author" : [ "J. Qiang", "P. Chen", "T. Wang", "X. Wu" ],
      "venue" : "arXiv preprint arXiv: 1609.08496v1, page 10.",
      "citeRegEx" : "Qiang et al\\.,? 2016",
      "shortCiteRegEx" : "Qiang et al\\.",
      "year" : 2016
    }, {
      "title" : "Opinion word expansion and target extraction through double propagation",
      "author" : [ "G. Qiu", "B. Liu", "J. Bu", "C. Chen" ],
      "venue" : "Computational linguistics, 37(1):9–27.",
      "citeRegEx" : "Qiu et al\\.,? 2011",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2011
    }, {
      "title" : "Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora",
      "author" : [ "D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning" ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 248–256. Association for Computational Linguistics.",
      "citeRegEx" : "Ramage et al\\.,? 2009",
      "shortCiteRegEx" : "Ramage et al\\.",
      "year" : 2009
    }, {
      "title" : "Semi-supervised polarity lexicon induction",
      "author" : [ "D. Rao", "D. Ravichandran" ],
      "venue" : "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 675–682. Association for Computational Linguistics.",
      "citeRegEx" : "Rao and Ravichandran,? 2009",
      "shortCiteRegEx" : "Rao and Ravichandran",
      "year" : 2009
    }, {
      "title" : "Ultradense word embeddings by orthogonal transformation",
      "author" : [ "S. Rothe", "S. Ebert", "H. Schütze" ],
      "venue" : "arXiv preprint arXiv:1602.07572.",
      "citeRegEx" : "Rothe et al\\.,? 2016",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning Sentiment-Specific Word Embedding",
      "author" : [ "D. Tang", "F. Wei", "N. Yang", "M. Zhou", "T. Liu", "B. Qin" ],
      "venue" : "Acl, pages 1555–1565.",
      "citeRegEx" : "Tang et al\\.,? 2014",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2014
    }, {
      "title" : "Word representations: a simple and general method for semi-supervised learning",
      "author" : [ "J. Turian", "L. Ratinov", "Y. Bengio" ],
      "venue" : "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.",
      "citeRegEx" : "Turian et al\\.,? 2010",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Measuring praise and criticism: Inference of semantic orientation from association",
      "author" : [ "P.D. Turney", "M.L. Littman" ],
      "venue" : "ACM Transactions on Information Systems (TOIS), 21(4):315–346.",
      "citeRegEx" : "Turney and Littman,? 2003",
      "shortCiteRegEx" : "Turney and Littman",
      "year" : 2003
    }, {
      "title" : "Phrase dependency parsing for opinion mining",
      "author" : [ "Y. Wu", "Q. Zhang", "X. Huang", "L. Wu" ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, volume 3, pages 1533– 1541. Association for Computational Linguistics.",
      "citeRegEx" : "Wu et al\\.,? 2009",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2009
    }, {
      "title" : "Jointly Modeling Aspects and Opinions with a MaxEnt-LDA Hybrid",
      "author" : [ "W.X. Zhao", "J. Jiang", "H. Yan", "X. Li" ],
      "venue" : "Computational Linguistics, 16(October):56–65. 31",
      "citeRegEx" : "Zhao et al\\.,? 2010",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Opinion Mining is a subfield of Natural Language Processing (NLP) that deals with the automatic analysis of opinions shared by humans in different contexts, like in customer reviews (Pang and Lee, 2008; Liu, 2012).",
      "startOffset" : 182,
      "endOffset" : 213
    }, {
      "referenceID" : 22,
      "context" : "Opinion Mining is a subfield of Natural Language Processing (NLP) that deals with the automatic analysis of opinions shared by humans in different contexts, like in customer reviews (Pang and Lee, 2008; Liu, 2012).",
      "startOffset" : 182,
      "endOffset" : 213
    }, {
      "referenceID" : 8,
      "context" : "This is the case of deep-learning based systems, that provide very good performance but require a significant amount of labelled data for training (Chen et al., 2017; Araque et al., 2017).",
      "startOffset" : 147,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "This is the case of deep-learning based systems, that provide very good performance but require a significant amount of labelled data for training (Chen et al., 2017; Araque et al., 2017).",
      "startOffset" : 147,
      "endOffset" : 187
    }, {
      "referenceID" : 20,
      "context" : "On the other hand, weakly-supervised systems do not require labelled data for training, but they usually need some language specific resources, such as carefully curated lists of seed words or language dependent tools to preprocess the input (Lin et al., 2011; Jo and Oh, 2011; Kim et al., 2013).",
      "startOffset" : 242,
      "endOffset" : 295
    }, {
      "referenceID" : 18,
      "context" : "On the other hand, weakly-supervised systems do not require labelled data for training, but they usually need some language specific resources, such as carefully curated lists of seed words or language dependent tools to preprocess the input (Lin et al., 2011; Jo and Oh, 2011; Kim et al., 2013).",
      "startOffset" : 242,
      "endOffset" : 295
    }, {
      "referenceID" : 19,
      "context" : "On the other hand, weakly-supervised systems do not require labelled data for training, but they usually need some language specific resources, such as carefully curated lists of seed words or language dependent tools to preprocess the input (Lin et al., 2011; Jo and Oh, 2011; Kim et al., 2013).",
      "startOffset" : 242,
      "endOffset" : 295
    }, {
      "referenceID" : 23,
      "context" : "During the last decade the research community has addressed the problem of analysing user opinions, particularly focused on online customer reviews (Liu et al., 2012; Chen et al., 2014).",
      "startOffset" : 148,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : "During the last decade the research community has addressed the problem of analysing user opinions, particularly focused on online customer reviews (Liu et al., 2012; Chen et al., 2014).",
      "startOffset" : 148,
      "endOffset" : 185
    }, {
      "referenceID" : 15,
      "context" : "A common approach in the literature is to identify frequent nouns, lexical patterns, dependency relations applying supervised machine learning approaches (Hu and Liu, 2004; Popescu and Etzioni, 2007; Blair-Goldensohn et al., 2008; Wu et al., 2009; Qiu et al., 2011).",
      "startOffset" : 154,
      "endOffset" : 265
    }, {
      "referenceID" : 4,
      "context" : "A common approach in the literature is to identify frequent nouns, lexical patterns, dependency relations applying supervised machine learning approaches (Hu and Liu, 2004; Popescu and Etzioni, 2007; Blair-Goldensohn et al., 2008; Wu et al., 2009; Qiu et al., 2011).",
      "startOffset" : 154,
      "endOffset" : 265
    }, {
      "referenceID" : 43,
      "context" : "A common approach in the literature is to identify frequent nouns, lexical patterns, dependency relations applying supervised machine learning approaches (Hu and Liu, 2004; Popescu and Etzioni, 2007; Blair-Goldensohn et al., 2008; Wu et al., 2009; Qiu et al., 2011).",
      "startOffset" : 154,
      "endOffset" : 265
    }, {
      "referenceID" : 36,
      "context" : "A common approach in the literature is to identify frequent nouns, lexical patterns, dependency relations applying supervised machine learning approaches (Hu and Liu, 2004; Popescu and Etzioni, 2007; Blair-Goldensohn et al., 2008; Wu et al., 2009; Qiu et al., 2011).",
      "startOffset" : 154,
      "endOffset" : 265
    }, {
      "referenceID" : 28,
      "context" : "Some works focus on automatically deriving the most likely polarity for words, constructing a so-called sentiment lexicon (Mostafa, 2013).",
      "startOffset" : 122,
      "endOffset" : 137
    }, {
      "referenceID" : 38,
      "context" : "of bootstrapping or polarity propagation leveraging some base dictionaries and pre-existing linguistic resources (Rao and Ravichandran, 2009; Jijkoun et al., 2010; Huang et al., 2014).",
      "startOffset" : 113,
      "endOffset" : 183
    }, {
      "referenceID" : 17,
      "context" : "of bootstrapping or polarity propagation leveraging some base dictionaries and pre-existing linguistic resources (Rao and Ravichandran, 2009; Jijkoun et al., 2010; Huang et al., 2014).",
      "startOffset" : 113,
      "endOffset" : 183
    }, {
      "referenceID" : 16,
      "context" : "of bootstrapping or polarity propagation leveraging some base dictionaries and pre-existing linguistic resources (Rao and Ravichandran, 2009; Jijkoun et al., 2010; Huang et al., 2014).",
      "startOffset" : 113,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "LDA is a generative model introduced by (Blei et al., 2003) that quickly gained popularity because it is an unsupervised, flexible and extensible technique to model documents.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : "Due to its flexibility, LDA has been extended and combined with other approaches, obtaining topic models that improve the resulting topics or that model additional information (Mcauliffe and Blei, 2008; Ramage et al., 2009).",
      "startOffset" : 176,
      "endOffset" : 223
    }, {
      "referenceID" : 37,
      "context" : "Due to its flexibility, LDA has been extended and combined with other approaches, obtaining topic models that improve the resulting topics or that model additional information (Mcauliffe and Blei, 2008; Ramage et al., 2009).",
      "startOffset" : 176,
      "endOffset" : 223
    }, {
      "referenceID" : 18,
      "context" : "Topic models have been applied to Sentiment Analysis to jointly model topics and sentiment of words (Lin et al., 2009, 2011; Jo and Oh, 2011; Lu et al., 2011; Kim et al., 2013; Alam et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 195
    }, {
      "referenceID" : 24,
      "context" : "Topic models have been applied to Sentiment Analysis to jointly model topics and sentiment of words (Lin et al., 2009, 2011; Jo and Oh, 2011; Lu et al., 2011; Kim et al., 2013; Alam et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "Topic models have been applied to Sentiment Analysis to jointly model topics and sentiment of words (Lin et al., 2009, 2011; Jo and Oh, 2011; Lu et al., 2011; Kim et al., 2013; Alam et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 195
    }, {
      "referenceID" : 1,
      "context" : "Topic models have been applied to Sentiment Analysis to jointly model topics and sentiment of words (Lin et al., 2009, 2011; Jo and Oh, 2011; Lu et al., 2011; Kim et al., 2013; Alam et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "This task requires a manual inspection by an expert or a mapping calculation to an existing resource (Bhatia et al., 2016).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 26,
      "context" : "Continuous word embeddings are known for capturing semantic regularities of words (Mikolov et al., 2013a; Collobert and Weston, 2008).",
      "startOffset" : 82,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "Continuous word embeddings are known for capturing semantic regularities of words (Mikolov et al., 2013a; Collobert and Weston, 2008).",
      "startOffset" : 82,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "Some works have made use of this fact to improve the resulting topics (Das et al., 2015; Nguyen et al., 2015; Qiang et al., 2016), but",
      "startOffset" : 70,
      "endOffset" : 129
    }, {
      "referenceID" : 30,
      "context" : "Some works have made use of this fact to improve the resulting topics (Das et al., 2015; Nguyen et al., 2015; Qiang et al., 2016), but",
      "startOffset" : 70,
      "endOffset" : 129
    }, {
      "referenceID" : 35,
      "context" : "Some works have made use of this fact to improve the resulting topics (Das et al., 2015; Nguyen et al., 2015; Qiang et al., 2016), but",
      "startOffset" : 70,
      "endOffset" : 129
    }, {
      "referenceID" : 40,
      "context" : "There are works that exploit word embeddings in a supervised machine learning setting to perform sentiment analysis (Tang et al., 2014; Giatsoglou et al., 2017).",
      "startOffset" : 116,
      "endOffset" : 160
    }, {
      "referenceID" : 13,
      "context" : "There are works that exploit word embeddings in a supervised machine learning setting to perform sentiment analysis (Tang et al., 2014; Giatsoglou et al., 2017).",
      "startOffset" : 116,
      "endOffset" : 160
    }, {
      "referenceID" : 44,
      "context" : "Some authors have also attempted an automatic aspect-term/opinionword separation within the topic modelling process (Zhao et al., 2010; Mukherjee and Liu, 2012).",
      "startOffset" : 116,
      "endOffset" : 160
    }, {
      "referenceID" : 29,
      "context" : "Some authors have also attempted an automatic aspect-term/opinionword separation within the topic modelling process (Zhao et al., 2010; Mukherjee and Liu, 2012).",
      "startOffset" : 116,
      "endOffset" : 160
    }, {
      "referenceID" : 7,
      "context" : "Instead, we apply Brown clustering (Brown et al., 1992) to a set of training instances from an unlabelled corpus in order to train an aspect-term/opinion-word classifier that is later integrated into the topic modelling process.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : ", 2014; Giatsoglou et al., 2017). Some authors have also attempted an automatic aspect-term/opinionword separation within the topic modelling process (Zhao et al., 2010; Mukherjee and Liu, 2012). Aspect terms are the words that are used to speak about the aspect being evaluated (e.g. waiter or waitstaff when speaking about the service of a restaurant). On the other hand, opinion words express the sentiment about an aspect, such as attentive or terrible. The separation of these two kinds of words might be useful because it eases the interpretation of the resulting topics, and the sentiment classification can be focused on the opinion-words which are more likely to bear sentiment information. Zhao et al. (2010) attempted this separation training a supervised classifier on a small manually labelled dataset and using Part-of-Speech tagging.",
      "startOffset" : 8,
      "endOffset" : 719
    }, {
      "referenceID" : 12,
      "context" : ", 2014; Giatsoglou et al., 2017). Some authors have also attempted an automatic aspect-term/opinionword separation within the topic modelling process (Zhao et al., 2010; Mukherjee and Liu, 2012). Aspect terms are the words that are used to speak about the aspect being evaluated (e.g. waiter or waitstaff when speaking about the service of a restaurant). On the other hand, opinion words express the sentiment about an aspect, such as attentive or terrible. The separation of these two kinds of words might be useful because it eases the interpretation of the resulting topics, and the sentiment classification can be focused on the opinion-words which are more likely to bear sentiment information. Zhao et al. (2010) attempted this separation training a supervised classifier on a small manually labelled dataset and using Part-of-Speech tagging. Mukherjee and Liu (2012) elaborated on this idea trying a similar approach but substituting the manually labelled dataset with an existing lexicon of opinion words for English.",
      "startOffset" : 8,
      "endOffset" : 874
    }, {
      "referenceID" : 7,
      "context" : "uses Brown clusters (Brown et al., 1992) to model examples of aspect-terms and opinion-words and train a MaxEnt-based classification model.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 41,
      "context" : "Brown clusters have been used as unsupervised features with good results in supervised Part-of-Speech tagging (Turian et al., 2010) and Named Entity Recognition (Agerri and Rigau, 2016).",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : ", 2010) and Named Entity Recognition (Agerri and Rigau, 2016).",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : "Such a dense vector representation of the words over a particular vocabulary, commonly referred as word embeddings, could be obtained using any distributional semantics approach, but in this work we stick to the well-known word2vec (Mikolov et al., 2013a).",
      "startOffset" : 232,
      "endOffset" : 255
    }, {
      "referenceID" : 27,
      "context" : "Word embeddings are a very popular way of representing words as the input for a variety of machine learning techniques and are known for encoding interesting syntactic and semantic properties (Mikolov et al., 2013b).",
      "startOffset" : 192,
      "endOffset" : 215
    }, {
      "referenceID" : 14,
      "context" : "The posterior inference of the model is obtained via Gibbs sampling (Griffiths and Steyvers, 2004).",
      "startOffset" : 68,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "For topic classification we use the dataset from (Ganu et al., 2009) which contains restaurant reviews labelled with domain-related categories (e.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 18,
      "context" : "For sentiment classification, we use the Laptops and DIGITAL-SLR dataset (Jo and Oh, 2011), consisting of English reviews of electronic products with their corresponding 5-star rating.",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 14,
      "context" : "3, which play a similar role to α and β in the original LDA, are set to the values commonly recommended in the literature (Griffiths and Steyvers, 2004): 50/T for αbase and δbase being T the number of topics, and 0.",
      "startOffset" : 122,
      "endOffset" : 152
    }, {
      "referenceID" : 12,
      "context" : "Comparison with other LDA based approaches First, we evaluate W2VLDA in a topic classification setting using the restaurant reviews dataset from (Ganu et al., 2009).",
      "startOffset" : 145,
      "endOffset" : 164
    }, {
      "referenceID" : 44,
      "context" : "We compare W2VLDA against the results reported in (Zhao et al., 2010) for two LDA-based approaches, LocLDA (Brody and Elhadad, 2010) and ME-LDA (Zhao et al.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : ", 2010) for two LDA-based approaches, LocLDA (Brody and Elhadad, 2010) and ME-LDA (Zhao et al.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 44,
      "context" : ", 2010) for two LDA-based approaches, LocLDA (Brody and Elhadad, 2010) and ME-LDA (Zhao et al., 2010).",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : ", 2010) for two LDA-based approaches, LocLDA (Brody and Elhadad, 2010) and ME-LDA (Zhao et al., 2010). LocLDA and ME-LDA are LDA-based approaches, and thus, unsupervised. But the results reported in the experiment involved some supervision as described in Zhao et al. (2010). First, the authors computed a topic model of 14 topics.",
      "startOffset" : 46,
      "endOffset" : 275
    }, {
      "referenceID" : 20,
      "context" : "We compare our polarity classification results with respect to those from JST (Lin et al., 2011), ASUM (Jo and Oh, 2011) and HASM (Kim et al.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : ", 2011), ASUM (Jo and Oh, 2011) and HASM (Kim et al.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : ", 2011), ASUM (Jo and Oh, 2011) and HASM (Kim et al., 2013).",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "As explained at (Kim et al., 2013) two datasets are used, a small dataset containing 1000 reviews with 1 star rating (strong negative) and 1000 5 stars (strong positive), and a large dataset with additional 1000 reviews of 2 stars (negative) as well as 1000 reviews of 4 stars (positive).",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 42,
      "context" : "The baseline consists of a simple polarity seed word count, using the polarity seed words from (Turney and Littman, 2003), assigning to the sentence the polarity with the greatest proportion.",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "In order to evaluate the accuracy of this words separation, we use Bing Liu’s polarity lexicon for English (Hu and Liu, 2004).",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 39,
      "context" : "On the other hand, the are more specialised word embeddings related to sentiment analysis (Rothe et al., 2016), and it would be interesting to study if different word embeddings bring improvements to the method keeping a minimal supervision.",
      "startOffset" : 90,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "With the increase of online customer opinions in specialised websites and social networks, the necessity of automatic systems to help to organise and classify customer reviews by domain-specific aspect/categories and sentiment polarity is more important than ever. Supervised approaches for Aspect Based Sentiment Analysis obtain good results for the domain/language they are trained on, but having manually labelled data for training supervised systems for all domains and languages is usually very costly and time consuming. In this work we describe W2VLDA, an almost unsupervised system based on topic modelling, that combined with some other unsupervised methods and a minimal configuration, performs aspect/category classification, aspectterms/opinion-words separation and sentiment polarity classification for any given domain and language. We evaluate the performance of the aspect and sentiment classification in the multilingual SemEval 2016 task 5 (ABSA) dataset. We show competitive results for several languages (English, Spanish, French and Dutch) and domains (hotels, restaurants, electronic devices).",
    "creator" : "LaTeX with hyperref package"
  }
}