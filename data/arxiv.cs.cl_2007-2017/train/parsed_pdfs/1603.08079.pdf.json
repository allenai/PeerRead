{
  "name" : "1603.08079.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities",
    "authors" : [ "Yevgeni Berzak", "Andrei Barbu", "Daniel Harari" ],
    "emails" : [ "berzak@mit.edu", "andrei@0xab.com", "hararid@mit.edu", "boris@mit.edu", "shimon.ullman@weizmann.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Ambiguity is one of the defining characteristics of human languages, and language understanding crucially relies on the ability to obtain unambiguous representations of linguistic content. While some ambiguities can be resolved using intra-linguistic contextual cues, the disambiguation of many linguistic constructions requires integration of world knowledge and perceptual information obtained from other modalities.\nIn this work, we focus on the problem of grounding language in the visual modality, and introduce a novel task for language understanding which requires resolving linguistic ambiguities by utilizing the visual context in which the linguistic content is expressed. This type of inference is frequently called for in human communication that occurs in a visual environment, and is crucial for language acquisition, when much of the linguistic content refers to the visual surroundings of the child (Snow, 1972).\nOur task is also fundamental to the problem of grounding vision in language, by focusing on phenomena of linguistic ambiguity, which are prevalent in language, but typically overlooked when using language as a medium for expressing understanding of visual content. Due to such ambiguities, a superficially appropriate description of a visual scene may in fact not be sufficient for demonstrating a correct understanding of the relevant visual content. Our task addresses this issue by introducing a deep validation protocol for visual understanding, requiring not only providing a surface description of a visual activity but also demonstrating structural understanding at the levels of syntax, semantics and discourse.\nTo enable the systematic study of visually grounded processing of ambiguous language, we create a new corpus, LAVA (Language and Vision Ambiguities). This corpus contains sentences with linguistic ambiguities that can only be resolved using external information. The sentences are paired with short videos that visualize different interpretations of each sentence. Our sentences encompass a wide range of syntactic, semantic and dis-\nar X\niv :1\n60 3.\n08 07\n9v 1\n[ cs\n.C V\n] 2\n6 M\nar 2\ncourse ambiguities, including ambiguous prepositional and verb phrase attachments, conjunctions, logical forms, anaphora and ellipsis. Overall, the corpus contains 237 sentences, with 2 to 3 interpretations per sentence, and an average of 3.37 videos that depict visual variations of each sentence interpretation, corresponding to a total of 1679 videos.\nUsing this corpus, we address the problem of selecting the interpretation of an ambiguous sentence that matches the content of a given video. Our approach for tackling this task extends the sentence tracker introduced in (Siddharth et al., 2014). The sentence tracker produces a score which determines if a sentence is depicted by a video. This earlier work had no concept of ambiguities; it assumed that every sentence had a single interpretation. We extend this approach to represent multiple interpretations of a sentence, enabling us to pick the interpretation that is most compatible with the video.\nTo summarize, the contributions of this paper are threefold. First, we introduce a new task for visually grounded language understanding, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence’s content. Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%."
    }, {
      "heading" : "2 Related Work",
      "text" : "Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities.\nPrevious work relating ambiguity in language to the visual modality addressed the problem of word\nsense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014). Our work expands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision.\nThe interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995). A considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans. Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972). Over time, children develop mechanisms for grounded disambiguation of language, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic content (Kidd and Holler, 2009). Our study leverages such insights to develop a complementary framework that enables addressing the challenge of visually grounded disambiguation of language in the realm of artificial intelligence."
    }, {
      "heading" : "3 Task",
      "text" : "In this work we provide a concrete framework for the study of language understanding with visual context by introducing the task of grounded language disambiguation. This task requires to choose the correct linguistic representation of a sentence given a visual context depicted in a video. Specifically, provided with a sentence, n candidate interpretations of that sentence and a video that depicts the content of the sentence, one needs to choose the interpretation that corresponds to the content of the video.\nTo illustrate this task, consider the example in figure 1, where we are given the sentence “Sam approached the chair with a bag” along with two different linguistic interpretations. In the first in-\nterpretation, which corresponds to parse 1(a), Sam has the bag. In the second interpretation associated with parse 1(b), the bag is on the chair rather than with Sam. Given the visual context from figure 1(c), the task is to choose which interpretation is most appropriate for the sentence."
    }, {
      "heading" : "4 Approach Overview",
      "text" : "To address the grounded language disambiguation task, we use a compositional approach for determining if a specific interpretation of a sentence is depicted by a video. In this framework, described in detail in section 6, a sentence and an accompanying interpretation encoded in first order logic, give rise to a grounded model that matches a video against the provided sentence interpretation.\nThe model is comprised of Hidden Markov Models (HMMs) which encode the semantics of words, and trackers which locate objects in video frames. To represent an interpretation of a sentence, word models are combined with trackers through a cross-product which respects the semantic representation of the sentence to create a single model which recognizes that interpretation.\nGiven a sentence, we construct an HMM based representation for each interpretation of that sentence. We then detect candidate locations for objects in every frame of the video. Together the re-\nS\nNP NNP Bill\nVP\nVBD held\nNP\nDT the\nNP\nJJ\ngreen\nNP\nNN chair CC and NN bag\n(a) First interpretation\nS\nNP NNP Bill\nVP\nVBD held\nNP\nDT the\nNP\nNP\nJJ\ngreen\nNN chair\nCC and NN bag\n(b) Second interpretation\nforestation for the sentence and the candidate object locations are combined to form a model which can determine if a given interpretation is depicted by the video. We test each interpretation and report the interpretation with highest likelihood."
    }, {
      "heading" : "5 Corpus",
      "text" : "To enable a systematic study of linguistic ambiguities that are grounded in vision, we compiled a corpus with ambiguous sentences describing visual actions. The sentences are formulated such that the correct linguistic interpretation of each sentence can only be determined using external, non-linguistic, information about the depicted activity. For example, in the sentence “Bill held the green chair and bag”, the correct scope of “green” can only be determined by integrating additional information about the color of the bag. This information is provided in the accompanying videos, which visualize the possible interpretations of each sentence. Figure 2 presents the syntactic parses for this example along with frames from the respective videos. Although our videos contain visual uncertainty, they are not ambiguous with respect to the linguistic interpretation they are presenting, and hence a video always corresponds to a single candidate representation of a sentence.\nThe corpus covers a wide range of well\nknown syntactic, semantic and discourse ambiguity classes. While the ambiguities are associated with various types, different sentence interpretations always represent distinct sentence meanings, and are hence encoded semantically using first order logic. For syntactic and discourse ambiguities we also provide an additional, ambiguity type specific encoding as described below.\n• Syntax Syntactic ambiguities include Prepositional Phrase (PP) attachments, Verb Phrase (VP) attachments, and ambiguities in the interpretation of conjunctions. In addition to logical forms, sentences with syntactic ambiguities are also accompanied with Context Free Grammar (CFG) parses of the candidate interpretations, generated from a deterministic CFG parser.\n• Semantics The corpus addresses several classes of semantic quantification ambiguities, in which a syntactically unambiguous sentence may correspond to different logical forms. For each such sentence we provide the respective logical forms.\n• Discourse The corpus contains two types of discourse ambiguities, Pronoun Anaphora and Ellipsis, offering examples comprising two sentences. In anaphora ambiguity cases, an ambiguous pronoun in the second sentence is given its candidate antecedents in the first sentence, as well as a corresponding logical form for the meaning of the second sentence. In ellipsis cases, a part of the second sentence, which can constitute either the subject and the verb, or the verb and the object, is omitted. We provide both interpretations of the omission in the form of a single unambiguous sentence, and its logical form, which combines the meanings of the first and the second sentences.\nTable 2 lists examples of the different ambiguity classes, along with the candidate interpretations of each example.\nThe corpus is generated using Part of Speech (POS) tag sequence templates. For each template, the POS tags are replaced with lexical items from the corpus lexicon, described in table 3, using all the visually applicable assignments. This generation process yields an overall of 237 sentences,\nof which 213 sentences have 2 candidate interpretations, and 24 sentences have 3 interpretations. Table 1 presents the corpus templates for each ambiguity class, along with the number of sentences generated from each template.\nThe corpus videos are filmed in an indoor environment containing background objects and pedestrians. To account for the manner of performing actions, videos are shot twice with different actors. Whenever applicable, we also filmed the actions from two different directions (e.g. approach from the left, and approach from the right). Finally, all videos were shot with two cameras from two different view points. Taking these variations into account, the resulting video corpus contains 7.1 videos per sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos. The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage (152434 frames).\nA custom corpus is required for this task because no existing corpus, containing either videos or images, systematically covers multimodal ambiguities. Datasets such as UCF Sports (Rodriguez et al., 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accompanied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed. Datasets for image and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al., 2013),\naim to control for more aspects of the videos than just the main action being performed but they do not provide the range of ambiguities discussed here. The closest dataset is that of Siddharth et al. (2014) as it controls for object appearance, color, action, and direction of motion, making it more likely to be suitable for evaluating disambiguation tasks. Unfortunately, that dataset was designed to avoid ambiguities, and therefore is not suitable for evaluating the work described here."
    }, {
      "heading" : "6 Model",
      "text" : "To perform the disambiguation task, we extend the sentence recognition model of Siddharth et al. (2014) which represents sentences as compositions of words. Given a sentence, its first order logic interpretation and a video, our model produces a score which determines if the sentence is depicted by the video. It simultaneously tracks the participants in the events described by the sentence while recognizing the events themselves. This al-\nlows it to be flexible in the presence of noise by integrating top-down information from the sentence with bottom-up information from object and property detectors. Each word in the query sentence is represented by an HMM (Baum et al., 1970), which recognizes tracks (i.e. paths of detections in a video for a specific object) that satisfy the semantics of the given word. In essence, this model can be described as having two layers, one in which object tracking occurs and one in which words observe tracks and filter tracks that do not satisfy the word constraints.\nGiven a sentence interpretation, we construct a sentence-specific model which recognizes if a video depicts the sentence as follows. Each predicate in the first order logic formula has a corresponding HMM, which can recognize if that predicate is true of a video given its arguments. Each variable has a corresponding tracker which attempts to physically locate the bounding box corresponding to that variable in each frame of a\nvideo. This creates a bipartite graph: HMMs that represent predicates are connected to trackers that represent variables. The trackers themselves are similar to the HMMs, in that they comprise a lattice of potential bounding boxes in every frame. To construct a joint model for a sentence interpretation, we take the cross product of HMMs and trackers, taking only those cross products dictated by the structure of the formula corresponding to the desired interpretation. Given a video, we employ an object detector to generate candidate detections in each frame, construct trackers which select one of these detections in each frame, and finally construct the overall model from HMMs and trackers.\nProvided an interpretation and its corresponding formula composed of P predicates and V variables, along with a collection of object detections, bframedetection index, in each frame of a video of length T the model computes the score of the videosentence pair by finding the optimal detection for each participant in every frame. This is in essence the Viterbi algorithm (Viterbi, 1971), the MAP algorithm for HMMs, applied to finding optimal object detections jframevariable for each participant, and the optimal state kframepredicate for each predicate HMM, in every frame. Each detection is scored by its confidence from the object detector, f and each object track is scored by a motion coherence metric g which determines if the motion of the track agrees with the underlying optical flow. Each predicate,\np, is scored by the probability of observing a particular detection in a given state hp, and by the probability of transitioning between states ap. The structure of the formula and the fact that multiple predicates often refer to the same variables is recorded by θ, a mapping between predicates and their arguments. The model computes the MAP estimate as:\nmax j11 ,..., j T 1\n... j1V ,..., j T V\nmax k11,..., k T 1\n... k1P ,..., k T P\nV∑ v=1 T∑ t=1 f(btjtv ) + T∑ t=2 g(bt−1 jt−1v , btjtv )+\nP∑ p=1 T∑ t=1 hp(k t p, b t jt θ1p , btjt θ2p ) + T∑ t=2 ap(k t−1 p , k t p)\nfor sentences which have words that refer to at most two tracks (i.e. transitive verbs or binary predicates) but is trivially extended to arbitrary arities. Figure 3 provides a visual overview of the model as a cross-product of tracker models and word models.\nOur model extends the approach of Siddharth et al. (2014) in several ways. First, we depart from the dependency based representation used in that work, and recast the model to encode first order logic formulas. Note that some complex first order logic formulas cannot be directly encoded in the model and require additional inference steps. This extension enables us to represent ambiguities in which a given sentence has multiple logical interpretations for the same syntactic parse.\nSecond, we introduce several model components which are not specific to disambiguation, but are required to encode linguistic constructions that are present in our corpus and could not be handled by the model of Siddharth et al. (2014). These new components are the predicate “not equal”, disjunction, and conjunction. The key addition among these components is support for the new predicate “not equal”, which enforces that two tracks, i.e. objects, are distinct from each other. For example, in the sentence “Claire and Bill moved a chair” one would want to ensure that the two movers are distinct entities. In earlier work, this was not required because the sentences tested in that work were designed to distinguish objects based on constraints rather than identity. In other words, there might have been two different people but they were distinguished in the sentence by their actions or appearance. To faithfully recognize that two actors are moving the chair in the earlier example, we must ensure that they are disjoint from each other. In order to do this we create a new HMM for this predicate, which assigns low probability to tracks that heavily overlap, forcing the model to fit two different actors in the previous example. By combining the new first order logic based semantic representation in lieu of a syntactic representation with a more expressive model, we can encode the sentence interpretations required to perform the disambiguation task.\nFigure 3(left) shows an example of two different interpretations of the above discussed sentence “Claire and Bill moved a chair”. Object trackers, which correspond to variables in the first order logic representation of the sentence interpretation, are shown in red. Predicates which constrain the possible bindings of the trackers, corresponding to predicates in the representation of the sentence, are shown in blue. Links represent the argument structure of the first order logic formula, and determine the cross products that are taken between the predicate HMMs and tracker lattices in order to form the joint model which recognizes the entire interpretation in a video.\nThe resulting model provides a single unified formalism for representing all the ambiguities in table 2. Moreover, this approach can be tuned to different levels of specificity. We can create models that are specific to one interpretation of a sentence or that are generic, and accept multiple interpretations by eliding constraints that are not com-\nmon between the different interpretations. This allows the model, like humans, to defer deciding on a particular interpretation or to infer that multiple interpretation of the sentence are plausible."
    }, {
      "heading" : "7 Experimental Results",
      "text" : "We tested the performance of the model described in the previous section on the LAVA dataset presented in section 5. Each video in the dataset was pre-processed with object detectors for humans, bags, chairs, and telescopes. We employed a mixture of CNN (Krizhevsky et al., 2012) and DPM (Felzenszwalb et al., 2010) detectors, trained on held out sections of our corpus. For each object class we generated proposals from both the CNN and the DPM detectors, and trained a scoring function to map both results into the same space. The scoring function consisted of a sigmoid over the confidence of the detectors trained on the same held out portion of the training set. As none of the disambiguation examples discussed here rely on the specific identity of the actors, we did not detect their identity. Instead, any sentence which contains names was automatically converted to one which contains arbitrary “person” labels.\nThe sentences in our corpus have either two or three interpretations. Each interpretation has one or more associated videos where the scene was shot from a different angle, carried out either by different actors, with different objects, or in different directions of motion. For each sentence-video pair, we performed a 1-out-of-2 or 1-out-of-3 classification task to determine which of the interpretations of the corresponding sentence best fits that video. Overall chance performance on our dataset is 49.04%, slightly lower than 50% due to the 1- out-of-3 classification examples.\nThe model presented here achieved an accuracy of 75.36% over the entire corpus averaged across all error categories. This demonstrates that the model is largely capable of capturing the underlying task and that similar compositional crossmodal models may do the same. For each of the 3 major ambiguity classes we had an accuracy of 84.26% for syntactic ambiguities, 72.28% for semantic ambiguities, and 64.44% for discourse ambiguities.\nThe most significant source of model failures are poor object detections. Objects are often rotated and presented at angles that are difficult to recognize. Certain object classes like the telescope\nare much more difficult to recognize due to their small size and the fact that hands tend to largely occlude them. This accounts for the degraded performance of the semantic ambiguities relative to the syntactic ambiguities, as many more semantic ambiguities involved the telescope. Object detector performance is similarly responsible for the lower performance of the discourse ambiguities which relied much more on the accuracy of the person detector as many sentences involve only people interacting with each other without any additional objects. This degrades performance by removing a helpful constraint for inference, according to which people tend to be close to the objects they are manipulating. In addition, these sentences introduced more visual uncertainty as they often involved three actors.\nThe remaining errors are due to the event models. HMMs can fixate on short sequences of events which seem as if they are part of an action, but in fact are just noise or the prefix of another action. Ideally, one would want an event model which has a global view of the action, if an object went up from the beginning to the end of the video while a person was holding it, it’s likely that the object was being picked up. The event models used here cannot enforce this constraint, they merely assert that the object was moving up for some number of frames; an event which can happen due to noise in the object detectors. Enforcing such local constraints instead of the global constraint of the motion of the object over the video makes joint tracking and event recognition tractable in the framework presented here but can lead to errors. Finding models which strike a better balance between local information and global constraints while maintaining tractable inference remains an area of future work."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We present a novel framework for studying ambiguous utterances expressed in a visual context. In particular, we formulate a new task for resolving structural ambiguities using visual signal. This is a fundamental task for humans, involving complex cognitive processing, and is a key challenge for language acquisition during childhood. We release a multimodal corpus that enables to address this task, as well as support further investigation of ambiguity related phenomena in visually grounded language processing. Finally, we\npresent a unified approach for resolving ambiguous descriptions of videos, achieving good performance on our corpus.\nWhile our current investigation focuses on structural inference, we intend to extend this line of work to learning scenarios, in which the agent has to deduce the meaning of words and sentences from structurally ambiguous input. Furthermore, our framework can be beneficial for image and video retrieval applications in which the query is expressed in natural language. Given an ambiguous query, our approach will enable matching and clustering the retrieved results according to the different query interpretations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM), funded by NSF STC award CCF1231216. SU was also supported by ERC Advanced Grant 269627 Digital Baby."
    } ],
    "references" : [ {
      "title" : "Word sense disambiguation with pictures",
      "author" : [ "Matthew Johnson", "David Forsyth" ],
      "venue" : "In Proceedings of the HLTNAACL 2003 workshop on Learning word meaning from non-linguistic data-Volume",
      "citeRegEx" : "Barnard et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Barnard et al\\.",
      "year" : 2003
    }, {
      "title" : "A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains",
      "author" : [ "Baum et al.1970] L.E. Baum", "T. Petrie", "G. Soules", "N. Weiss" ],
      "venue" : null,
      "citeRegEx" : "Baum et al\\.,? \\Q1970\\E",
      "shortCiteRegEx" : "Baum et al\\.",
      "year" : 1970
    }, {
      "title" : "Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages",
      "author" : [ "Bruni et al.2012] Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran" ],
      "venue" : null,
      "citeRegEx" : "Bruni et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bruni et al\\.",
      "year" : 2012
    }, {
      "title" : "The interaction of visual and linguistic saliency during syntactic ambiguity resolution",
      "author" : [ "Coco", "Keller2015] Moreno I Coco", "Frank Keller" ],
      "venue" : null,
      "citeRegEx" : "Coco et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Coco et al\\.",
      "year" : 2015
    }, {
      "title" : "Every picture tells a story: Generating sentences from images",
      "author" : [ "Farhadi et al.2010] Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth" ],
      "venue" : null,
      "citeRegEx" : "Farhadi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Farhadi et al\\.",
      "year" : 2010
    }, {
      "title" : "Object detection with discriminatively trained part-based models",
      "author" : [ "Ross B Girshick", "David McAllester", "Deva Ramanan" ],
      "venue" : "Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Felzenszwalb et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Felzenszwalb et al\\.",
      "year" : 2010
    }, {
      "title" : "Improving image-sentence embeddings using large weakly annotated photo collections",
      "author" : [ "Gong et al.2014] Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik" ],
      "venue" : "In Computer Vision–ECCV",
      "citeRegEx" : "Gong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Karpathy", "Fei-Fei2014] Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "arXiv preprint arXiv:1412.2306",
      "citeRegEx" : "Karpathy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2014
    }, {
      "title" : "Children’s use of gesture to resolve lexical ambiguity",
      "author" : [ "Kidd", "Holler2009] Evan Kidd", "Judith Holler" ],
      "venue" : "Developmental Science,",
      "citeRegEx" : "Kidd et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kidd et al\\.",
      "year" : 2009
    }, {
      "title" : "What are you talking about? text-to-image coreference",
      "author" : [ "Kong et al.2014] Chen Kong", "Dahua Lin", "Mayank Bansal", "Raquel Urtasun", "Sanja Fidler" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Kong et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2014
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105",
      "author" : [ "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Hmdb: a large video database for human motion recognition",
      "author" : [ "Hueihan Jhuang", "Estı́baliz Garrote", "Tomaso Poggio", "Thomas Serre" ],
      "venue" : "In Computer Vision (ICCV),",
      "citeRegEx" : "Kuehne et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kuehne et al\\.",
      "year" : 2011
    }, {
      "title" : "Baby talk: Understanding and generating simple image descriptions",
      "author" : [ "Kulkarni et al.2011] G Kulkarni", "V Premraj", "S Dhar", "Siming Li", "Yejin Choi", "AC Berg", "TL Berg" ],
      "venue" : "In Proceedings of the 2011 IEEE Conference on Computer Vision and",
      "citeRegEx" : "Kulkarni et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kulkarni et al\\.",
      "year" : 2011
    }, {
      "title" : "Combining language and vision with a multimodal skip-gram model. CoRR, abs/1501.02598",
      "author" : [ "Nghia The Pham", "Marco Baroni" ],
      "venue" : null,
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2015
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick" ],
      "venue" : null,
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Recognizing realistic actions from videos in the wild",
      "author" : [ "Liu et al.2009] Jingen Liu", "Jiebo Luo", "Mubarak Shah" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Liu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Midge: Generating image descriptions from computer vision detections",
      "author" : [ "Xufeng Han", "Jesse Dodge", "Alyssa Mensch", "Amit Goyal", "Alex Berg", "Kota Yamaguchi", "Tamara Berg", "Karl Stratos", "Hal Daumé III" ],
      "venue" : null,
      "citeRegEx" : "Mitchell et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2012
    }, {
      "title" : "Linking people in videos with their names using coreference resolution",
      "author" : [ "Armand Joulin", "Percy Liang", "Li Fei-Fei" ],
      "venue" : "In Computer Vision–ECCV",
      "citeRegEx" : "Ramanathan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ramanathan et al\\.",
      "year" : 2014
    }, {
      "title" : "Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics",
      "author" : [ "Marcus Rohrbach", "Dominikus Wetzel", "Stefan Thater", "Bernt Schiele", "Manfred Pinkal" ],
      "venue" : null,
      "citeRegEx" : "Regneri et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Regneri et al\\.",
      "year" : 2013
    }, {
      "title" : "Action MACH A Spatio-temporal Maximum Average Correlation",
      "author" : [ "Javed Ahmed", "Mubarak Shah" ],
      "venue" : null,
      "citeRegEx" : "Rodriguez et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rodriguez et al\\.",
      "year" : 2008
    }, {
      "title" : "Seeing what you’re told: Sentence-guided activity recognition in video",
      "author" : [ "Andrei Barbu", "Jeffrey Mark Siskind" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Siddharth et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Siddharth et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning grounded meaning representations with autoencoders",
      "author" : [ "Silberer", "Lapata2014] Carina Silberer", "Mirella Lapata" ],
      "venue" : "In Proceedings of ACL,",
      "citeRegEx" : "Silberer et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Silberer et al\\.",
      "year" : 2014
    }, {
      "title" : "Mothers’ speech to children learning language",
      "author" : [ "Catherine E Snow" ],
      "venue" : "Child development,",
      "citeRegEx" : "Snow.,? \\Q1972\\E",
      "shortCiteRegEx" : "Snow.",
      "year" : 1972
    }, {
      "title" : "Zero-shot learning through cross-modal transfer",
      "author" : [ "Milind Ganjoo", "Christopher D Manning", "Andrew Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Grounded compositional semantics for finding and describing images",
      "author" : [ "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : null,
      "citeRegEx" : "Socher et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2014
    }, {
      "title" : "Eye movements and spoken language comprehension: Effects of visual context on syntactic ambiguity resolution",
      "author" : [ "Michael K Tanenhaus", "Kathleen M Eberhard", "Julie C Sedivy" ],
      "venue" : "Cognitive psychology,",
      "citeRegEx" : "Spivey et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Spivey et al\\.",
      "year" : 2002
    }, {
      "title" : "Integration of visual and linguistic information in spoken language comprehension",
      "author" : [ "Michael J Spivey-Knowlton", "Kathleen M Eberhard", "Julie C Sedivy" ],
      "venue" : null,
      "citeRegEx" : "Tanenhaus et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Tanenhaus et al\\.",
      "year" : 1995
    }, {
      "title" : "Integrating language and vision to generate natural language descriptions of videos in the wild",
      "author" : [ "Subhashini Venugopalan", "Sergio Guadarrama", "Kate Saenko", "Raymond Mooney" ],
      "venue" : null,
      "citeRegEx" : "Thomason et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Thomason et al\\.",
      "year" : 2014
    }, {
      "title" : "Translating videos to natural language using deep recurrent neural networks",
      "author" : [ "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko" ],
      "venue" : null,
      "citeRegEx" : "Venugopalan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Venugopalan et al\\.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Alexander Toshev", "Samy Bengio", "Dumitru Erhan" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Convolutional codes and their performance in communication systems",
      "author" : [ "A.J. Viterbi" ],
      "venue" : "Communications of the IEEE,",
      "citeRegEx" : "Viterbi.,? \\Q1971\\E",
      "shortCiteRegEx" : "Viterbi.",
      "year" : 1971
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "This type of inference is frequently called for in human communication that occurs in a visual environment, and is crucial for language acquisition, when much of the linguistic content refers to the visual surroundings of the child (Snow, 1972).",
      "startOffset" : 232,
      "endOffset" : 244
    }, {
      "referenceID" : 20,
      "context" : "Our approach for tackling this task extends the sentence tracker introduced in (Siddharth et al., 2014).",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al.",
      "startOffset" : 112,
      "endOffset" : 223
    }, {
      "referenceID" : 23,
      "context" : "Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al.",
      "startOffset" : 112,
      "endOffset" : 223
    }, {
      "referenceID" : 6,
      "context" : "Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al.",
      "startOffset" : 112,
      "endOffset" : 223
    }, {
      "referenceID" : 13,
      "context" : "Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al.",
      "startOffset" : 112,
      "endOffset" : 223
    }, {
      "referenceID" : 4,
      "context" : ", 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 293
    }, {
      "referenceID" : 12,
      "context" : ", 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 293
    }, {
      "referenceID" : 16,
      "context" : ", 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 293
    }, {
      "referenceID" : 24,
      "context" : ", 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 293
    }, {
      "referenceID" : 27,
      "context" : ", 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 293
    }, {
      "referenceID" : 20,
      "context" : ", 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 293
    }, {
      "referenceID" : 28,
      "context" : ", 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 293
    }, {
      "referenceID" : 29,
      "context" : ", 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",
      "startOffset" : 81,
      "endOffset" : 293
    }, {
      "referenceID" : 0,
      "context" : "Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003).",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014).",
      "startOffset" : 91,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014).",
      "startOffset" : 91,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995).",
      "startOffset" : 164,
      "endOffset" : 188
    }, {
      "referenceID" : 25,
      "context" : "A considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans.",
      "startOffset" : 85,
      "endOffset" : 129
    }, {
      "referenceID" : 22,
      "context" : "Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972).",
      "startOffset" : 164,
      "endOffset" : 176
    }, {
      "referenceID" : 19,
      "context" : "Datasets such as UCF Sports (Rodriguez et al., 2008), YouTube (Liu et al.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : ", 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : ", 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accompanied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "Datasets for image and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : ", 2014) and TACOS (Regneri et al., 2013),",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "The closest dataset is that of Siddharth et al. (2014) as it controls for object appearance, color, action, and direction of motion, making it more likely to be suitable for evaluating disambiguation tasks.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "Each word in the query sentence is represented by an HMM (Baum et al., 1970), which recognizes tracks (i.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 19,
      "context" : "To perform the disambiguation task, we extend the sentence recognition model of Siddharth et al. (2014) which represents sentences as compositions of words.",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 30,
      "context" : "This is in essence the Viterbi algorithm (Viterbi, 1971), the MAP algorithm for HMMs, applied to finding optimal object detections jframe variable for each participant, and the optimal state kframe predicate for each predicate HMM, in every frame.",
      "startOffset" : 41,
      "endOffset" : 56
    }, {
      "referenceID" : 20,
      "context" : "Our model extends the approach of Siddharth et al. (2014) in several ways.",
      "startOffset" : 34,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "Second, we introduce several model components which are not specific to disambiguation, but are required to encode linguistic constructions that are present in our corpus and could not be handled by the model of Siddharth et al. (2014). These new components are the predicate “not equal”, disjunction, and conjunction.",
      "startOffset" : 212,
      "endOffset" : 236
    }, {
      "referenceID" : 10,
      "context" : "We employed a mixture of CNN (Krizhevsky et al., 2012) and DPM (Felzenszwalb et al.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : ", 2012) and DPM (Felzenszwalb et al., 2010) detectors, trained on held out sections of our corpus.",
      "startOffset" : 16,
      "endOffset" : 43
    } ],
    "year" : 2016,
    "abstractText" : "Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.",
    "creator" : "LaTeX with hyperref package"
  }
}