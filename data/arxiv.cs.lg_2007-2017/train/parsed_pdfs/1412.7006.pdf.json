{
  "name" : "1412.7006.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-modal Sensor Registration for Vehicle Perception via Deep Neural Networks",
    "authors" : [ "Michael Giering", "Vivek Venugopalan" ],
    "emails" : [ "reddykk}@utrc.utc.com" ],
    "sections" : [ {
      "heading" : "I. MOTIVATION",
      "text" : "Navigation and situational awareness of optionally manned vehicles requires the integration of multiple sensing modalities such as Light Detection and Ranging (LiDAR) and video, but could just as easily be extended to other modalities including Radio Detection And Ranging (RADAR), ShortWavelength Infrared (SWIR) and Global Positioning System (GPS). Spatio-temporal registration of information from multimodal sensors is technically challenging in its own right. For many tasks such as pedestrian and object detection tasks that make use of multiple sensors, decision support methods rest on the assumption of proper registration. Most approaches [1] in LiDAR-video for instance, build separate vision and LiDAR feature extraction methods and identify common anchor points in both. Alternatively, by generating a single feature set on LiDAR, Video and optical flow, it enables the system to to capture mutual information among modalities more efficiently. The ability to dynamically register information from the available data channels for perception related tasks can alleviate the need for anchor points between sensor modalities. We see auto-registration as a prerequisite need for operating on multimodal information with confidence.\nDeep neural networks (DNN) lend themselves in a seamless manner for data fusion on time series data. For some challenges in which the modalities share significant mutual information, the features generated on the fused information can provide insight that neither input alone can [2]. In effect the ML version of, ”the whole is greater than the sum of it’s parts”.\nAutonomous navigation places significant constraints on the speed of perception algorithms and their ability to drive decision making in real-time. Though computationally intensive to train, our implemented DCNN run easily within our real-time frame rates of 8 fps and could accommodate more standard rates of 30 fps. With most research in deep neural networks focused on algorithmic improvements and novel applications, a significant benefit to applied researchers is sometimes under appreciated. The automated feature generation of DNNs enables us to create mutli-modal systems with far less overhead. The need for domain experts and hand-crafted feature design are lessened, allowing more rapid prototyping and testing. The generalization of auto-registration across multiple assets is clearly a path to be explored.\nIn this paper, the main contributions are: (i) formulation of an image registration problem as a fusion of modalities from different sensors, namely LIDAR (L), video (Grayscale or R,G,B) and optical flow (U,V); (ii) performance evaluation of deep convolutional neural networks (DCNN) with various input parameters, such as kernel filter size and different combinations of input channels (R,G,B,Gr,L,U,V); (iii) fusion of patch-level and image-level predictions to generate alignment at the frame-level. The experiments were conducted using a publicly available dataset from FORD and the University of Michigan [3]. The DCNN implementation was executed on an NVIDIA Tesla K40 GPU with 2880 cores and compute power of 5 TFLOPS (single precision). The paper is organized into the following sections: Section I describes the introduction and motivation for this work; Section II provides a survey of the related work; the problem formulation along with the dataset description and the preprocessing is explained in Section III; Section IV gives the details of the DCNN setup for the different experiments; Section V describes the experiments and the post-processing steps for visualizing the qualitative results; finally Section VI summarizes the paper and concludes with future research thrusts."
    }, {
      "heading" : "II. PREVIOUS WORK",
      "text" : "A great amount has been published on various multi-modal fusion methods [4], [5], [6], [7]. The most common approaches taken generate features of interest in each modality separately and create a decision support mechanism that aggregates features across modalities. If spatial alignment is required across modalities, as it is for LiDAR-video such filter methods\nar X\niv :1\n41 2.\n70 06\nv2 [\ncs .C\nV ]\n8 J\nul 2\n01 5\n[8] are required to ensure proper inter-modal registration. These filter methods for leveraging 3D LiDAR and 2D images are often geometric in nature and make use of projections between the different data spaces.\nAutomatic registration of 2D video and 3D LiDAR has been a widely researched topic for over a decade [9], [10], [11], [1]. Its application in real-time autonomous navigation makes it a challenging problem. Majority of the 2D-3D registration algorithms are based on feature matching. Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16]. Feature based approaches generally rely on dense 3D point cloud and additional knowledge of relative sun position and GPS/inertial navigation system (INS). Another approach used for video and LiDAR auto-registration is to reconstruct 3D point cloud from video sequences using structure from motion (SFM) and performing 3D-3D registration [17], [18]. 3D-3D registration is more difficult and computationally expensive compared to 2D-3D registration.\nThe use of deep neural networks to analyze multi-modal sensor inputs has increased sharply in just the last few years, including audio-video [2], [19], image/text [20], image/depth [21] and LiDAR-video To the best of our knowledge the use of multi-modal deep neural networks for dynamic LiDAR-video registration has not been presented.\nA common challenge for data fusion methods is deciding at what level features from the differing sensor streams should be brought together. The deep neural network (DNN) approach most similar to the more traditional data fusion methods is to train DNNs independently on sensor modalities and then use the high-level outputs of those networks as inputs to a subsequent aggregator, which could also be a DNN. This is analogous to the earlier example of learning 3D/2D features and the process of identifying common geometric features.\nIt is possible however to apply DNNs with a more agnostic view enabling a unified set of features to be learned across multi-modal data. In these cases the input channels aren’t differentiated. Unsupervised methods including deep Boltzmann machines and deep auto-encoders for learning such joint representations have been successful.\nDeep convolutional neural networks (DCNN) enable a similar agnostic approach to input channels. A significant difference is that target data is required to train them as classifiers. This is the approach chosen by us for automating the registration of LiDAR-video and optical-flow, in which we are combining 1D/3D/2D data representations respectively to learn a unified model across as many as 6D."
    }, {
      "heading" : "III. PROBLEM STATEMENT",
      "text" : "Being able to detect and correct the misalignment (registration, calibration) among sensors of the same or different kinds, is critical for decision support systems operating on their fused information streams. For our work DCNNs were implemented for the detection of small spatial misalignments in LiDAR and Video frames. The methodology is directly applicable to temporal registration as well. LiDAR-video data\ncollected from a driverless car was chosen for the multi-modal fusion test case. LiDAR-video is a common combination for providing perception capabilities to many types of ground and airborne platforms including driverless cars [8]."
    }, {
      "heading" : "A. Ford LiDAR-video Dataset and Experimental Setup",
      "text" : "The FORD LiDAR-video dataset [3] is collected by an autonomous Ford F-250 vehicle integrated with the following perception and navigation sensors as follows:\n• Velodyne HDL-64E LiDAR with two blocks of lasers spinning at 10 Hz and a maximum range of 120m. • Point Grey Ladybug3 omni-directional camera system with six 2-Mega-pixel cameras collecting video data at 8fps with 1600× 1600 resolution. • Two Riegl LMS-Q120 LIDAR sensors installed in the front of the vehicle generating range and intensity data when the laser sweeps its 80° field of view (FOV). • Applanix POS-LV420 INS with Trimble GPS system providing the 6 degrees of freedom (DOF) estimates at 100 Hz. • Xsens MTi-G sensor consisting of accelerometer, gyroscope, magnetometer, integrated GPS receiver, static pressure sensor and temperature sensor. It measures the GPS co-ordinates of the vehicle and also provides the 3D velocity and 3D rate of turn.\nThis dataset is generated by the vehicle while driving in and around the Ford research campus and downtown Michigan. The data includes feature rich downtown areas as well as featureless empty parking lots. As shown in Figure 1, we divided the data set into training and testing sections A to B and C to D respectively. They were chosen in a manner that minimizes the likelihood of contamination between training and testing. Because of this, the direction of the light source is never the same in the testing and training sets."
    }, {
      "heading" : "B. Optical Flow",
      "text" : "In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25]. Optical flow provides information of the scene dynamics and is expressed as an estimate of velocity at each\npixel from two consecutive frames, denoted by ~u and ~v. The motion field from these two frames is measured by the motion of the pixel brightness pattern, where the changes in image brightness is due to the camera or object motion. [26] describes an algorithm for computing optical flow from images, which is used during the preprocessing step. Figure 2 shows an example of the optical flow computed using two consecutive frames from the Ford LiDAR-video dataset. By including optical flow as input channels, we imbue the DCNN with information on the dynamics observed across time steps."
    }, {
      "heading" : "C. Preprocessing",
      "text" : "At each video frame timestep, the inputs to our model consist of C channels of data with C ranging from 3-6 channels. Channels consist of grayscale Gr or (R,G,B) information from the video, horizontal and vertical components of optical flow (U,V) and depth information L from LiDAR The data from each modality is reshaped to a fixed size of 800 × 256 values, which are partitioned into p × p patches at a prescribed stride. Each patch p × p is stacked across C channels, effectively generating a vector of C dimensions. The different preprocessing parameters are denoted by patch size p, stride s and the number of input channels C.\nPreprocessing is repeated N times, where N is the number of offset classes. For each offset class, the video (R,G,B) and optical flow (U,V) channels are kept static and the depth (L) channel from the LiDAR is moved by the offset simulating a misalignment between the video and the LiDAR sensors. In order to accurately detect the misalignment in the LiDAR and Video sensor data, a threshold is set to limit the information available in each channel. The LiDAR data has regions of sparsity and hence the LiDAR patches with a variance (σ2 < 15%) are dropped from the final dataset. This leads to the elimination of the majority of foreground patches in the data set, reducing the size of the training and testing set by approximately 80%. Figure 3a shows a N = 9 class elliptically distributed set of offsets and Figure 3b shows a p× p patch stacked across all the different C channels."
    }, {
      "heading" : "IV. MODEL DESCRIPTION",
      "text" : "Our models for auto-registration are DCNNs trained to classify the current misalignment of the LiDAR-video data streams into one of a predefined set of offsets. DCNNs are probably the most successful deep learning model to date on\nfielded applications. The fact that the algorithm shares weights in the training phase, results in fewer model parameters and more efficient training. DCNNs are particularly useful for problems in which local structure is important, such as object recognition in images and temporal information for voice recognition. The alternating steps of convolution and pooling generates features at multiple scales which in turn imbues DCNN’s with scale invariant characteristics.\nThe model shown in Figure 4 consists of 3 pairs of convolution-pooling layers, that estimates the offset between the LiDAR-video inputs at each time step. For each patch within a timestep, there are N variants with the LiDAR-videooptical flow inputs offset by the predetermined amounts. The CNN outputs to a softmax layer, thereby providing an offset classification value for each patch of the frame. As described in Section III-C, 32 × 32 patches were stacked across the different channels and provided as the input to the DCNN. All the 6 channels RGBLUV were used for the majority of the experiments, whereas only 4 channels were required for the RGBL and the GrLUV experiments. The first convolutional layer uses 32 filters (or kernels) of size 5×5×C with a stride of 1 pixel and padding of 2 pixels on the edges. The following pooling layer generates the input data (of size 16×16×32) for\nthe second convolutional layer. This layer uses 32 filters of size 5×5×32 with a stride of 1 pixel and padding of 2 pixels on the edges. A second pooling layer, similar to the first one is used to generate input with size 8× 8× 32 for the third convolutional layer that uses 64 filters of size 5 × 5 × 32 with the stride and padding same as previous convolutional layer. The third pooling layer with similar configuration as the two previous pooling layers connects to an output softmax layer with labels corresponding to the N = 9 classes. The DCNN described above was trained using stochastic gradient descent with a mini-batch size of 100 epochs. The DCNN is configured with Rectified Linear Units (ReLUs), as they train several times faster than their equivalents with tanh connections [27]\nThe NVIDIA Kepler series K40 GPUs [28] are very FLOPS/Watt efficient and are being used to drive real-time image processing capabilities [29]. These GPUs consist of 2880 cores with 12 GB of on-board device memory (RAM). Deep Learning applications have been targeted on GPUs previously in [30] and these implementations are both compute and memory bound. Stacking of channels results in a vector of 32 × 32 × C , which is suitable for the Single Instruction Multiple Datapath (SIMD) architecture of the GPUs. At the same time, the training batch size caches in the GPU memory, so the utilization of the K40 GPU’s memory is very high. This also results in our experiments to run successfully on a single GPU instead of partitioning the different layers over multiple GPUs."
    }, {
      "heading" : "V. EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "A. Dataset using elliptically distributed offsets",
      "text" : "In our experiments, elliptically distributed set of N = 9 offsets of the LiDAR-video data were considered. The LiDAR data is displaced along an ellipse with a major axis of 32 pixels and a minor axis of 16 pixels rotated clockwise from x-axis by 45° as shown in Figure 3a. Separate training and testing sets were generated from two different tracks as shown in Figure 1 for all the N = 9 offsets of LiDAR data. Training and testing tracks have never seen regions and also have different lighting conditions. Our preprocessing step described in Section III-C results in 223, 371 and 126, 513 patches for testing and training extracted from 469 and 224 images respectively.\nIn the testing phase, for each frame a simple voting scheme is used to aggregate the patch level offset predictions to a\nsingle frame level prediction. A sample histogram of the patch level predictions is show in Figure 5. We color each patch of the frame with a color corresponding to the predicted class as shown in Figure 5."
    }, {
      "heading" : "B. Experimental results",
      "text" : "Table I lists the inputs and CNN parameters explored ranked in the order of increasing accuracy. We averaged the values across the diagonal of the confusion matrix to determine the image level and patch level accuracy. Patch level accuracy is the individual performance of all the 32 × 32 patches from the testing images. Classification of patches belonging to a single time-step are voted to predict the shift for image level accuracy. In Table I, the first 3 columns show the results for different number of filter combinations in the convolutional layers with fixed number of filters and input channels RGBLUV. We observed that the image and patch level accuracy decreased with the increase in the number of filters. For experiments shown in columns 4 and 5, the filter size was increased, with the number of filters constant at (32, 32, 64). We observed that for the 6 channels RGBLUV, filter size of 9 gave the best image level accuracy of 63.03%. Column 6 shows the results of our experiment after dropping the optical flow UV channels. The image and patch level accuracy decreased for this case, indicating that optical flow contributed significantly towards image registration. The remaining experiments utilized the Grayscale information Gr instead of RGB and produced the best results with 76.69% and 41.05% image and patch level accuracy respectively. Table II shows that by using information from consecutive frames the performance increases significantly."
    }, {
      "heading" : "VI. CONCLUSIONS AND FUTURE WORK",
      "text" : "In this paper, we proposed a deep learning method to do LiDAR-Video registration. We demonstrated the effect of filter size, number of filters and different channels. We also showed the advantage of using temporal information, optical flow and grayscale. The next step in taking this work forward is to complete our development of a deep auto-registration method for ground and aerial platforms requiring no a priori calibration ground truth. Our aerospace applications in particular present noisier data with an increased number of degrees of freedom. The extension of these methods to simultaneously register\ninformation across multiple platforms and larger numbers of modalities will provide interesting challenges that we look forward to working on."
    } ],
    "references" : [ {
      "title" : "Real-time 2D Video 3D LiDAR Registration",
      "author" : [ "C. Bodensteiner", "M. Arens" ],
      "venue" : "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 2206–2209.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multimodal Deep Learning",
      "author" : [ "J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 689–696.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Ford Campus Vision And Lidar Data Set",
      "author" : [ "G. Pandey", "J.R. McBride", "R.M. Eustice" ],
      "venue" : "The International Journal of Robotics Research, vol. 30, no. 13, pp. 1543–1552, 2011.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Information Fusion In Biometrics",
      "author" : [ "A. Ross", "A. Jain" ],
      "venue" : "Pattern recognition letters, vol. 24, no. 13, pp. 2115–2125, 2003.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Learning Representations By Maximizing Compression",
      "author" : [ "K. Gregor", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1108.1169, 2011.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Optimal Multimodal Fusion For Multimedia Data Analysis",
      "author" : [ "Y. Wu", "E.Y. Chang", "K.C.-C. Chang", "J.R. Smith" ],
      "venue" : "Proceedings of the 12th annual ACM international conference on Multimedia. ACM, 2004, pp. 572–579.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The Challenge Problem For Automated Detection Of 101 Semantic Concepts In Multimedia",
      "author" : [ "C.G. Snoek", "M. Worring", "J.C. Van Gemert", "J.-M. Geusebroek", "A.W. Smeulders" ],
      "venue" : "Proceedings of the 14th annual ACM international conference on Multimedia. ACM, 2006, pp. 421–430.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Google’s driverless car",
      "author" : [ "S. Thrun" ],
      "venue" : "Ted Talk, Ed, 2011.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A Robust Approach For Automatic Registration Of Aerial Images With Untextured Aerial LIDAR Data",
      "author" : [ "L. Wang", "U. Neumann" ],
      "venue" : "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 2623–2630.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Automatic registration of lidar and optical imagery using depth map stereo",
      "author" : [ "H. Kim", "C.D. Correa", "N. Max" ],
      "venue" : "Computational Photography (ICCP), 2014 IEEE International Conference on. IEEE, 2014, pp. 1–8.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automatic Registration of LIDAR and Optical Images of Urban Scenes",
      "author" : [ "A. Mastin", "J. Kepner", "J. Fisher" ],
      "venue" : "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 2639–2646.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A systematic approach for 2d-image to 3d-range registration in urban environments",
      "author" : [ "L. Liu", "I. Stamos" ],
      "venue" : "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, Oct 2007, pp. 1–8.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Automatic registration of aerial imagery with untextured 3d lidar models",
      "author" : [ "M. Ding", "K. Lyngbaek", "A. Zakhor" ],
      "venue" : "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, June 2008, pp. 1–8.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Automated texture mapping of 3d city models with oblique aerial imagery",
      "author" : [ "C. Frueh", "R. Sammon", "A. Zakhor" ],
      "venue" : "3D Data Processing, Visualization and Transmission, 2004. 3DPVT 2004. Proceedings. 2nd International Symposium on, Sept 2004, pp. 396–403.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Integrating automated range registration with multiview geometry for the photorealistic modeling of large-scale scenes",
      "author" : [ "I. Stamos", "L. Liu", "C. Chen", "G. Wolberg", "G. Yu", "S. Zokai" ],
      "venue" : "International Journal of Computer Vision, vol. 78, no. 2-3, pp. 237–260, 2008. [Online]. Available: http://dx.doi.org/10.1007/s11263-007-0089-1",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A shadow based method for image to model registration",
      "author" : [ "A.J. Troccoli", "P.K. Allen" ],
      "venue" : "In IEEE Workshop on Image and Video Registration, Conf. on Comp. Vision and, 2004.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Alignment of continuous video onto 3d point clouds",
      "author" : [ "W. Zhao", "D. Nister", "S. Hsu" ],
      "venue" : "Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, vol. 2, June 2004, pp. II–II.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multiview geometry for texture mapping 2d images onto 3d range data",
      "author" : [ "L. Liu", "I. Stamos", "G. Yu", "G. Wolberg", "S. Zokai" ],
      "venue" : "Computer Vision  (a) Patch level confusion matrix (41.05% accuracy) (b) Image level confusion matrix (76.69% accuracy) Fig. 6: Confusion Matrix for elliptically distributed N = 9 classes using Greyscale, Optical Flow and LiDAR channels with a filter size of 9 and Pattern Recognition, 2006 IEEE Computer Society Conference on, vol. 2, June 2006, pp. 2293–2300.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Deep Learning for Robust Feature Generation in Audiovisual Emotion Recognition",
      "author" : [ "Y. Kim", "H. Lee", "E.M. Provost" ],
      "venue" : "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3687–3691.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multimodal Learning With Deep Boltzmann Machines",
      "author" : [ "N. Srivastava", "R. Salakhutdinov" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 2222–2230.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Deep Learning for Detecting Robotic Grasps",
      "author" : [ "I. Lenz", "H. Lee", "A. Saxena" ],
      "venue" : "arXiv preprint arXiv:1301.3592, 2013.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Egomotion and relative depth map from optical flow",
      "author" : [ "K. Prazdny" ],
      "venue" : "Biological Cybernetics, vol. 36, no. 2, pp. 87–102, 1980. [Online]. Available: http://dx.doi.org/10.1007/BF00361077",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Robust depth estimation from optical flow",
      "author" : [ "B. Shahraray", "M. Brown" ],
      "venue" : "Computer Vision., Second International Conference on, Dec 1988, pp. 641–650.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Dynamic 3D Scene Depth Reconstruction via Optical Flow Field Rectification",
      "author" : [ "Y. Yang", "Q. Liu", "R. Ji", "Y. Gao" ],
      "venue" : "PLoS ONE, vol. 7, p. 47041, Nov. 2012.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Efficient moving object segmentation algorithm using background registration technique",
      "author" : [ "S.-Y. Chien", "S.-Y. Ma", "L.-G. Chen" ],
      "venue" : "Circuits  and Systems for Video Technology, IEEE Transactions on, vol. 12, no. 7, pp. 577–586, Jul 2002.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Beyond Pixels: Exploring New Representations and Applications for Motion Analysis",
      "author" : [ "C. Liu" ],
      "venue" : "Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, MA, USA, 2009, aAI0822221.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Rectified Linear Units Improve Restricted Boltzmann Machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807–814.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "NVIDIA’s Next Generation CUDA Compute Architecture: Kepler TM GK110",
      "author" : [ "NVIDIA Inc." ],
      "venue" : "Whitepaper, May 2012.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Accelerating real-time lidar data processing using gpus",
      "author" : [ "V. Venugopal", "S. Kannan" ],
      "venue" : "Circuits and Systems (MWSCAS), 2013 IEEE 56th International Midwest Symposium on, August 2013, pp. 1168–1171.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Imagenet Classification With Deep Convolutional Neural Networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, 2012, pp. 1097–1105.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Most approaches [1] in LiDAR-video for instance, build separate vision and LiDAR",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "For some challenges in which the modalities share significant mutual information, the features generated on the fused information can provide insight that neither input alone can [2].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 2,
      "context" : "The experiments were conducted using a publicly available dataset from FORD and the University of Michigan [3].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "A great amount has been published on various multi-modal fusion methods [4], [5], [6], [7].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "A great amount has been published on various multi-modal fusion methods [4], [5], [6], [7].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "A great amount has been published on various multi-modal fusion methods [4], [5], [6], [7].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "A great amount has been published on various multi-modal fusion methods [4], [5], [6], [7].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "[8] are required to ensure proper inter-modal registration.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "Automatic registration of 2D video and 3D LiDAR has been a widely researched topic for over a decade [9], [10], [11], [1].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "Automatic registration of 2D video and 3D LiDAR has been a widely researched topic for over a decade [9], [10], [11], [1].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "Automatic registration of 2D video and 3D LiDAR has been a widely researched topic for over a decade [9], [10], [11], [1].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "Automatic registration of 2D video and 3D LiDAR has been a widely researched topic for over a decade [9], [10], [11], [1].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "Geometric features like corners and edges are extracted from detected vanishing points [12], [13], line segments [14], [15], and shadows [16].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "Another approach used for video and LiDAR auto-registration is to reconstruct 3D point cloud from video sequences using structure from motion (SFM) and performing 3D-3D registration [17], [18].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 17,
      "context" : "Another approach used for video and LiDAR auto-registration is to reconstruct 3D point cloud from video sequences using structure from motion (SFM) and performing 3D-3D registration [17], [18].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : "The use of deep neural networks to analyze multi-modal sensor inputs has increased sharply in just the last few years, including audio-video [2], [19], image/text [20], image/depth [21] and LiDAR-video To the best of our knowledge the use of multi-modal deep neural networks for dynamic LiDAR-video registration has not been presented.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "The use of deep neural networks to analyze multi-modal sensor inputs has increased sharply in just the last few years, including audio-video [2], [19], image/text [20], image/depth [21] and LiDAR-video To the best of our knowledge the use of multi-modal deep neural networks for dynamic LiDAR-video registration has not been presented.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 19,
      "context" : "The use of deep neural networks to analyze multi-modal sensor inputs has increased sharply in just the last few years, including audio-video [2], [19], image/text [20], image/depth [21] and LiDAR-video To the best of our knowledge the use of multi-modal deep neural networks for dynamic LiDAR-video registration has not been presented.",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 20,
      "context" : "The use of deep neural networks to analyze multi-modal sensor inputs has increased sharply in just the last few years, including audio-video [2], [19], image/text [20], image/depth [21] and LiDAR-video To the best of our knowledge the use of multi-modal deep neural networks for dynamic LiDAR-video registration has not been presented.",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "LiDAR-video is a common combination for providing perception capabilities to many types of ground and airborne platforms including driverless cars [8].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "The FORD LiDAR-video dataset [3] is collected by an autonomous Ford F-250 vehicle integrated with the following perception and navigation sensors as follows:",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 21,
      "context" : "In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 24,
      "context" : "In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [22], depth maps [23], reconstruct dynamic 3D scene depth [24], and segment moving objects [25].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 25,
      "context" : "[26] describes an algorithm for computing optical flow from images, which is used during the preprocessing step.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "The DCNN is configured with Rectified Linear Units (ReLUs), as they train several times faster than their equivalents with tanh connections [27] The NVIDIA Kepler series K40 GPUs [28] are very FLOPS/Watt efficient and are being used to drive real-time image processing capabilities [29].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 27,
      "context" : "The DCNN is configured with Rectified Linear Units (ReLUs), as they train several times faster than their equivalents with tanh connections [27] The NVIDIA Kepler series K40 GPUs [28] are very FLOPS/Watt efficient and are being used to drive real-time image processing capabilities [29].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 28,
      "context" : "The DCNN is configured with Rectified Linear Units (ReLUs), as they train several times faster than their equivalents with tanh connections [27] The NVIDIA Kepler series K40 GPUs [28] are very FLOPS/Watt efficient and are being used to drive real-time image processing capabilities [29].",
      "startOffset" : 282,
      "endOffset" : 286
    }, {
      "referenceID" : 29,
      "context" : "Deep Learning applications have been targeted on GPUs previously in [30] and these implementations are both compute and memory bound.",
      "startOffset" : 68,
      "endOffset" : 72
    } ],
    "year" : 2015,
    "abstractText" : "The ability to simultaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle’s physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisite to analyzing the fused data. The persistence and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LiDAR-video systems like on those many driverless cars are a common example of where keeping the LiDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LiDARvideo inputs. A number of variations were tested on the Ford LiDAR-video driving test data set and will be discussed. To the best of our knowledge the use of multi-modal deep convolutional neural networks for dynamic real-time LiDAR-video registration has not been presented.",
    "creator" : "LaTeX with hyperref package"
  }
}