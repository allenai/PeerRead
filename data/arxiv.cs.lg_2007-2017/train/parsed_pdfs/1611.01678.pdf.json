{
  "name" : "1611.01678.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Comparing learning algorithms in neural network for diagnosing cardiovascular disease",
    "authors" : [ "Mirmorsal Madani" ],
    "emails" : [ "mt_madani@yahoo.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "medical science for diagnosing, overcoming and treating diseases. Neural network is one of the techniques which are widely used for diagnosis in medical field. In this article efficiency of nine algorithms, which are basis of neural network learning in diagnosing cardiovascular diseases, will be assessed. Algorithms are assessed in terms of accuracy, sensitivity, transparency, AROC and convergence rate by means of 10 fold cross validation. The results suggest that in training phase, Lonberg-M algorithm has the best efficiency in terms of all metrics, algorithm OSS has maximum accuracy in testing phase, algorithm SCG has the maximum transparency and algorithm CGB has the maximum sensitivity.\nKeywords— cardiovascular disease; neural network; learning algorithms.\nI. INTRODUCTION\nCardiovascular disease is any kind of disease which influences on circulatory system. It mainly includes heart diseases, cerebrovascular diseases, kidney and arterial diseases. According to statistics in 2006, 26% of death rates in the USA are because of heart diseases [3]. Data mining in medical science has been used very much and in recent years it has been widely studied. A wide range of problems in medical science is associated with diagnosis and they are solved through various experiments. From view of data mining, prediction in diagnosis is among data classification problems. Classification includes studying features of a new object and allocating it to one of pre-determined sets.\nNeural network has obtained significant importance as a classification technique in recent years among pattern classification algorithm and machinery learning. Neural network is preferred to other methods in terms of its high acceleration, accuracy and efficiency during colliding with large data basis. Learning is a key ability of neural network. Learning rules are algorithms for finding suitable weights or other parameters of network. There are various algorithms for training neural network. It is a hard task to select an\nappropriate learning algorithm for neural network and it depends on many factors.\nIn this article we seek to review efficiency of 9 algorithms of neural network learning for diagnosis and classification of those who suffer from heart diseases. Efficiency of the algorithms will be reviewed in testing and training phases in terms of accuracy, sensitivity, transparency, and AROC and convergence rate. In the first section we describe neural network. In the second section various algorithms of neural network learning are briefly explained. In the third section experiment results are explained and in the end of the article conclusion and future works are presented.\nII. NEURAL NETWORK\nArtificial neural network originated from biological systems [5]. Neural network was composed of too many neurons and it has the ability of learning from samples, such as human brain. Neural network can do tasks that cannot be performed by means of linear planning. In the network information is available in communication between neurons directly. The data were obtained from biological systems through learning [5]. The data which are achieved through learning can be applied for decision-making on new samples. Multi-layered perceptron neural network includes one input layers, hidden layer and output layer. In some articles efficiency of neural network was widely approved in diagnosing various diseases, such as skin disease [1], oncology [4], and radiology [2] and so on. According to figure 1, neural network was used with three layers, 13 input layers in the first layer, 7 neurons in the hidden layer and one neuron in output layer. Sigmoid transfer function was used in the hidden and output layers.\nNeural network design is highly dependent on type of neural network learning algorithm. Leaning algorithms are used for obtaining optimum parameters by means of efficiency function (mean square error). The algorithms are mainly located in two groups: first-time algorithms (based on gradient reduction methods) and second-time algorithm. In table 1, nine algorithms with neural network training basis along with acronym of each algorithm were presented. 9 algorithms will then be described briefly [5].\n Gradient reduction algorithms\nThis algorithm is a first-time algorithm and uses the first derivate of the error function for finding minimum in error space. According to equation (1), gradient g can be defined in form of first-time derivative from E total error function.\n(1)\nDefining gradient g, rules of updating gradient reduction algorithm can be written as equation (2).\n(2)\n GDX variable learning rate\nIn standard gradient reduction methods, LR rate is constant in all the training phases, while algorithm efficiency is highly dependent on LR rate. With large LR value, the algorithm has many fluctuations and with small LR value the algorithm needs much time for convergence. It is recommended to change LR level during training process based on level of efficiency in order to improve efficiency of algorithm. Size of a flexible LR should make sufficient stability for algorithm [5].\n Reactionary propagation (RP)\nMulti-layered networks often use sigmoid transfer function in their hidden layer. The functions compress inputs, which are in maximum range, into a small range. Generally, in these functions gradient becomes zero if the input is large. This leads to a problem in gradient reduction algorithm, since at this point the gradient will become less and makes some quantitative changes in weights and biases. As a result weights and biases will remain far from their optimum value. The aim of reactionary propagation algorithms is to remove side effects in minor derivatives. Only derived mark is used for specifying update direction of weights. Size of derivative will leave no effect on weight`s updating. Changes of weight can be determined by means of a single updating value. Weight update and bias values will increase if derivative value does not change its signal in sequenced repetition. Also updating value will be reduced with one factor where derivative changes its mark to previous repetition. Updating value does not change with zero derivatives. With fluctuation of weights, weights will change in small amount. If weight keeps changing with the same direction for several repetitions, value of change will increase. The algorithm has high efficiency to standard algorithm of gradient reduction. Also, the algorithm needs less memory [4].\n Combined gradient algorithms\nIn most algorithms, learning rate is used for specifying size of steps in updating weights. Size of each step is adjusted for each repetition in most of the algorithms. Hence, a searching operation is conducted between all the gradients where efficiency function is minimized along the line. In equation (3) all the algorithms start searching in the first repetition for maximum gradient reduction.\nIn equation (4) a linear search was conducted to determine optimum distance with linear search.\n(3)\n(4)\nThen, direction of next search is accompanied with previous direction. According to equation (5), general procedure for specifying a new search direction is a combination of new search direction and the previous one.\nDifferent accompanied gradients are separated for calculating Pk by means of their behavior. For FletcherReeves, Pk is calculated by means of equation (6).\n(5)\n(6)\nCombined gradient algorithms are very quick and sometimes they are even quicker than reactionary propagation. Of course every problem has different results. These algorithms need more memory compared to simple algorithms.\nPolak-Ribiere is another algorithm in which parameters are calculated in terms of equations (7) and\n(7,8)\nIn all the algorithms, search direction is reset in certain repetitions to negative gradient. Standard reset point is where number of repetitions is equal with number of network parameters. But there are some methods for specifying these points which increase efficiency. In Powell-Beale Restars algorithm, when there is not enough balance between current and previous gradients, reset operation is conducted for search. The problem is checked out in equation (9).\n(9)\nIf above condition exists, search direction is reset. The algorithm has more efficiency in some problems compared to CGP. But it is hard to comment for every problem. In return the memory level used in this method is more than CGF [5].\n Scale combined gradient\nAll the algorithms discussed so fare regarding combined gradient require a linear search. The linear search is expensive to be calculated, since network should react to\nall the training inputs and calculate several parameters for each search. Scale combined gradient algorithm is designed in a way that it does not need any timeconsuming linear search. The algorithm is very complex and cannot be explained here. But it is based on combining two methods of combined gradient and Lonberg-M. The algorithm needs more repetition for convergence compared to other combined gradient algorithms. But calculation in each repetition is reduced significantly. Since linear search was not conducted in this method. Memory space needed in this algorithm is similar to Fletcher-Reeves [5].\n Quasi-Newton algorithm\nIn this method all the elements of g1, g2….gN are functions of weights and all the weights are independent of each other in terms of being linear. Therefore, updating rules for Newton method is calculated by means of equation 10.\n(10)\nIn equation (10), H is Matrix Hessian.\n(11)\nNewton methods usually have more suitable and quicker convergence to gradient reduction algorithms. But this improvement happens only when second class approximation of error function is logical. Otherwise, the algorithm becomes divergent. In order to obtain Matrix Hessian H, second derivative of total error function should be calculated and this could be very complex and expensive to calculate. As a result they are not suitable for neural network. In quasi-Newton method which is a Newton-based method algorithm, the second derivative is not required to be calculated and it needs less calculation costs. In this method Jakobian matrix was applied instead of Matrix Hessian. In Newton-Gaussian algorithm, updating rules are calculated by means of equation (12).\n(12)\nIn this equation e and J is Jakobian matrix. Newton-Gaussian algorithm is still facing convergence difficulties such as Newton algorithm for optimizing complex error level [4].\n Lonberg-M algorithms\nThis algorithm belongs to gradient reduction algorithm and Newton-Gaussian. Fortunately, this algorithm inherits convergence rate of Newton-Gaussian algorithm and\nconsistency of gradient reduction methods. Although LonbergM algorithm has slower convergence than Newton-Gaussian algorithms, it has quicker convergence than gradient reduction methods. In order to make sure that Hessian matrix JTJ is inversion allowed, algorithm Lonberg-M introduces estimation for Hessian matrix in terms of equation (13):\n(13)\nWhere μ is always positive which is called combination coefficient. And I am identity matrix. It can be learnt from equation (13) that elements of main diameter in Hessian matrix is more than zero. Therefore, this makes matrix H to be inversion allowed all the time.\nRules of updating Lonberg-M algorithm can be calculated by means of formula (14).\n(14)\nCombining two algorithms of gradient reduction and NewtonGaussian, algorithm Lonberg-M is awitched between the two algorithms during leaning process. Once combination coefficient U becomes very small, Newton-Gaussian algorithm is used. When the coefficient is very big, gradient reduction algorithm is used [6].\n Algorithm BFGS\nThis algorithm needs more calculations and space compared to other combined gradient methods. Approximation matrix is hessian n*n where n is equal to weights and network bias. High size of matrix makes some problems for storing in terms of level of memory. Therefore, very big networks are recommended to use Rprop methods or combining gradient rather than this method. For smaller networks the algorithm can have better efficiency to other methods [3].\n Algorithm One Step Secant\nSince BFGS needs too much space and many calculations, another quasi-Newton method was founded with less space and calculations. Algorithm OSS is in fact a bridge between combining gradient and quasi-Newton. The algorithm does not save full hessian matrix, it supposes that previous hessian matrix is valid in each repletion and this led to reduction of calculation and space used [3].\nIV. EXPERIMENTS\nIt is a difficult task to specify which of the algorithms are suitable for neural network training for classifying medical\ndata, since this problem depends on many factors such as problem complexity, number of training data, data quality, weights and bias in network and so on. To review efficiency of neural network learning algorithms, we may use previous data available in University of California at Irvine (UCI) [4]. We need to normalize inputs and objective before neural network training so that they are scaled in a certain range. In these experiments, data applied are normalized in ranges _1 and 1. The algorithms are assessed in terms of accuracy, sensitivity, transparency, AROC and convergence rate by means of Cross validation 10 fold."
    }, {
      "heading" : "GRADIENT REDUCTION ALGORITHMS",
      "text" : "In figure 2, values of AROC exist in testing and training phases for cardiovascular data. In testing algorithm, OSS achieved 0.8378 and in training algorithm LM received 0.9472 which are maximum level of AROC.\nSpeed of neural network training algorithms are presented in figure 3. Objective function was mean square error and it sought to reduce MSE in repetition of neural training algorithms, in order to reach the value required. According to figure 3, error of algorithm LM is reduced with the most speed than other algorithms.\nV. CONCLUSION\nIn this paper, 9 algorithms of neural network learning basis were reviewed for recognizing heart diseases. Neural network with architectural record 1_7_13 was applied (13 neurons in input layer, 7 neurons in hidden layer, and 1 neuron in output layer). In neural network training, algorithm Lonberg-M is in the first place with maximum accuracy, sensitivity, transparency, and AROC and convergence rate. Algorithm\nOSS has 77.78 accuracy, SCG has 72.17 transparencies and CGB has 83.67 sensitivity which all have maximum average in testing phase. According to the results, neural network has great accuracy in diagnosing those suffering from cardiovascular diseases. This is due to wide communication between non-linear elements which makes interpretation hard for people. As a result, in future works process of discovering rules could be reviewed."
    } ],
    "references" : [ {
      "title" : "Melanoma. Diagnosis by Raman spectroscopy and neural networks: structure alterations in proteins and lipids in intact cancer tissue",
      "author" : [ "D. Christensen", "M. Gniadecka", "P.A. Philipsen", "S.Sigurdsson", "S.Wessel", "O.F. Nielsen", "J. Hercogova" ],
      "venue" : "J Invest Dermatol,2006, pp. 443–449.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Performance evaluation of radiologists with artificial neural network for differential diagnosis of intra-axial cerebral tumors on MR images",
      "author" : [ "K. Yamashita", "T.Yoshiura", "H. Arimura", "F. Mihara", "T. Noguchi", "A. Hiwatashi", "O. Togao", "Y.Yamashita", "T. Shono", "S. Kumazawa", "Y. Higashida", "H. Honda" ],
      "venue" : "AJNR Am J Neuroradiol , 2009, PP.1153–1158.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Deaths: Final data for 2006,",
      "author" : [ "M. Heron", "D.L. Hoyert", "S.L. Murphy", "J. Xu", "K.D. Kochanek", "B. Tejada-Vera" ],
      "venue" : "National Vital Statistics Reports,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Artificial neural networks for decision making in urologic oncology",
      "author" : [ "M. Remzi", "B. Djavan" ],
      "venue" : "Ann Urol (Paris), 2013,pp.110–115.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Neural Networks: A Comprehensive Foundation, “Prentice Hall, USA,2001",
      "author" : [ "S. Haykin" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "An Experimental Evaluation of Apple Siri and Google Speech Recognition",
      "author" : [ "M. Assefi", "G. Liu", "M.P. Wittie", "C. Izurieta" ],
      "venue" : "Proccedings of the 2015 ISCA SEDE",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Impact of Network Performance on Cloud Speech Recognition",
      "author" : [ "M. Assefi", "M. Wittie", "Knight", "August" ],
      "venue" : "In 2015 24th International Conference on Computer Communication and Networks (ICCCN) (pp. 1-6)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Molecular Solutions for the Minimum Edge Dominating Set Problem on DNAbased Supercomputing",
      "author" : [ "S. Safaei", "B. Dalvand", "B. Esmaeili", "V. Safaei" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Molecular Solutions for the Bin-packing and Minimum Makespan Scheduling Problems on DNA-based Supercomputing",
      "author" : [ "S. Safaei", "B. Dalvand", "Z. Derakhshandeh", "V. Safaei" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "According to statistics in 2006, 26% of death rates in the USA are because of heart diseases [3].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "NEURAL NETWORK Artificial neural network originated from biological systems [5].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "The data were obtained from biological systems through learning [5].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "In some articles efficiency of neural network was widely approved in diagnosing various diseases, such as skin disease [1], oncology [4], and radiology [2] and so on.",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "In some articles efficiency of neural network was widely approved in diagnosing various diseases, such as skin disease [1], oncology [4], and radiology [2] and so on.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "In some articles efficiency of neural network was widely approved in diagnosing various diseases, such as skin disease [1], oncology [4], and radiology [2] and so on.",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : "9 algorithms will then be described briefly [5].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "Size of a flexible LR should make sufficient stability for algorithm [5].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Also, the algorithm needs less memory [4].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "In return the memory level used in this method is more than CGF [5].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "Memory space needed in this algorithm is similar to Fletcher-Reeves [5].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "Newton-Gaussian algorithm is still facing convergence difficulties such as Newton algorithm for optimizing complex error level [4].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "When the coefficient is very big, gradient reduction algorithm is used [6].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "For smaller networks the algorithm can have better efficiency to other methods [3].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "The algorithm does not save full hessian matrix, it supposes that previous hessian matrix is valid in each repletion and this led to reduction of calculation and space used [3].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "To review efficiency of neural network learning algorithms, we may use previous data available in University of California at Irvine (UCI) [4].",
      "startOffset" : 139,
      "endOffset" : 142
    } ],
    "year" : 2016,
    "abstractText" : "Today data mining techniques are exploited in medical science for diagnosing, overcoming and treating diseases. Neural network is one of the techniques which are widely used for diagnosis in medical field. In this article efficiency of nine algorithms, which are basis of neural network learning in diagnosing cardiovascular diseases, will be assessed. Algorithms are assessed in terms of accuracy, sensitivity, transparency, AROC and convergence rate by means of 10 fold cross validation. The results suggest that in training phase, Lonberg-M algorithm has the best efficiency in terms of all metrics, algorithm OSS has maximum accuracy in testing phase, algorithm SCG has the maximum transparency and algorithm CGB has the maximum sensitivity. Keywords— cardiovascular disease; neural network; learning algorithms.",
    "creator" : "Microsoft® Word 2016"
  }
}