{
  "name" : "1506.01273.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Summarization of Films and Documentaries Based on Subtitles and Scripts",
    "authors" : [ "Marta Aparı́cioa", "Paulo Figueiredoa", "Francisco Raposoa", "David Martins de Matosa", "Ricardo Ribeiroa", "Luı́s Marujoa" ],
    "emails" : [ "david.matos@inesc-id.pt" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We assess the performance of generic text summarization algorithms applied to films and documentaries, using extracts from news articles produced by reference models of extractive summarization. We use three datasets: (i) news articles, (ii) film scripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics are used for comparing generated summaries against news abstracts, plot summaries, and synopses. We show that the best performing algorithms are LSA, for news articles and documentaries, and LexRank and Support Sets, for films. Despite the different nature of films and documentaries, their relative behavior is in accordance with that obtained for news articles.\nc© 2016 Elsevier Ltd. All rights reserved."
    }, {
      "heading" : "1. Introduction",
      "text" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7]. Nevertheless, application areas within the entertainment industry are gaining attention: e.g. summarization of literary short stories [12], music summarization [31], summarization of books [24], or inclusion of character analyses in movie summaries [36]. We follow this direction, creating extractive, text-driven video summaries for films and documentaries.\nDocumentaries started as cinematic portrayals of reality [10]. Today, they continue to portray historical events, argumentation, and research. They are commonly understood as capturing reality and therefore, seen as inherently non-fictional. Films, in contrast, are usually associated with fiction. However, films and documentaries do not fundamentally differ: many of the strategies and narrative structures employed in films are also used in documentaries [27].\nIn the context of our work, films (fictional) tell stories based on fictive events, whereas documentaries (non-fictional) address, mostly, scientific subjects. We study the parallelism between the information carried in subtitles and scripts of both films and documentaries. Extractive summarization methods\n∗∗Corresponding author e-mail: david.matos@inesc-id.pt (David Martins de Matos)\nhave been extensively explored for news documents [16, 22, 37, 29, 30, 23]. Our main goal is to understand the quality of automatic summaries, produced for films and documentaries, using the well-known behavior of news articles as reference. Generated summaries are evaluated against manual abstracts using ROUGE metrics, which correlate with human judgements [15, 17].\nThis article is organized as follows: Section 2 presents the summarization algorithms; Section 3 presents the collected datasets; Section 4 presents the evaluation setup; Section 5 discusses our results; Section 6 presents conclusions and directions for future work."
    }, {
      "heading" : "2. Generic Summarization",
      "text" : "Six text-based summarization approaches were used to summarize newspaper articles, subtitles, and scripts. They are described in the following sections."
    }, {
      "heading" : "2.1. Maximal Marginal Relevance (MMR)",
      "text" : "MMR is a query-based summarization method [4]. It iteratively selects sentences via Equation 1 (Q is a query; Sim1 and Sim2 are similarity metrics; Si and Sj are non-selected and previously selected sentences, respectively). λ balances relevance and novelty. MMR can generate generic summaries by considering the input sentences centroid as a query [25, 38].\narg max Si\n[ λSim1 (Si, Q)− (1− λ) max\nSj Sim2 (Si, Sj)\n] (1)\nar X\niv :1\n50 6.\n01 27\n3v 3\n[ cs\n.C L\n] 9\nM ar\n2 01\n2"
    }, {
      "heading" : "2.2. LexRank",
      "text" : "LexRank [6] is a centrality-based method based on Google’s PageRank [3]. A graph is built using sentences, represented by TF-IDF vectors, as vertexes. Edges are created when the cosine similarity exceeds a threshold. Equation 2 is computed at each vertex until the error rate between two successive iterations is lower than a certain value. In this equation, d is a damping factor to ensure the method’s convergence, N is the number of vertexes, and S (Vi) is the score of the ith vertex.\nS (Vi) = (1− d) N\n+d× ∑\nVj∈adj[Vi]\nSim (Vi, Vj)∑ Vk∈adj[Vj ] Sim (Vj , Vk) S (Vj)\n(2)"
    }, {
      "heading" : "2.3. Latent Semantic Analysis (LSA)",
      "text" : "LSA infers contextual usage of text based on word cooccurrence [13, 14]. Important topics are determined without the need for external lexical resources [9]: each word’s occurrence context provides information concerning its meaning, producing relations between words and sentences that correlate with the way humans make associations. Singular Value Decomposition (SVD) is applied to each document, represented by a t×n term-by-sentences matrix A, resulting in its decomposition UΣV T . Summarization consists of choosing the k highest singular values from Σ, giving Σk. U and V T are reduced to Uk and V Tk , respectively, approximating A by Ak = UkΣkV T k . The most important sentences are selected from V Tk ."
    }, {
      "heading" : "2.4. Support Sets",
      "text" : "Documents are typically composed by a mixture of subjects, involving a main and various minor themes. Support sets are defined based on this observation [35]. Important content is determined by creating a support set for each passage, by comparing it with all others. The most semantically-related passages, determined via geometric proximity, are included in the support set. Summaries are composed by selecting the most relevant passages, i.e., the ones present in the largest number of support sets. For a segmented information source I , p1, p2, . . . , pN , support sets Si for each passage pi are defined by Equation 3, where Sim is a similarity function, and i is a threshold. The most important passages are selected by Equation 4.\nSi , {s ∈ I : Sim(s, pi) > i ∧ s 6= pi} (3)\narg max s∈Uni=1Si\n|{Si : s ∈ Si}| (4)"
    }, {
      "heading" : "2.5. Key Phrase-based Centrality (KP-Centrality)",
      "text" : "Ribeiro et al. [32] proposed an extension of the centrality algorithm described in Section 2.4, which uses a two-stage important passage retrieval method. The first stage consists of a feature-rich supervised key phrase extraction step, using the MAUI toolkit with additional semantic features: the detection of rhetorical signals, the number of Named Entities, Part-OfSpeech (POS) tags, and 4 n-gram domain model probabilities [20, 19]. The second stage consists of the extraction of the most important passages, where key phrases are considered regular passages."
    }, {
      "heading" : "2.6. Graph Random-walk with Absorbing StateS that HOPs among PEaks for Ranking (GRASSHOPPER)",
      "text" : "GRASSHOPPER [40] is a re-ranking algorithm that maximizes diversity and minimizes redundancy. It takes a weighted graph W (n × n: n vertexes representing sentences; weights are defined by a similarity measure), a probability distribution r (representing a prior ranking), and λ ∈ [0, 1], that balances the relative importance of W and r. If there is no prior ranking, a uniform distribution can be used. Sentences are ranked by applying the teleporting random walks method in an absorbing Markov chain, based on the n × n transition matrix P̃ (calculated by normalizing the rows of W ), i.e., P = λP̃ + (1− λ) 1r>. The first sentence to be scored is the one with the highest stationary probability arg maxni=1 πi according to the stationary distribution of P : π = P>π. Already selected sentences may never be visited again, by defining Pgg = 1 and Pgi = 0,∀i 6= g. The expected number of visits is given by matrix N = (I −Q)−1 (where Nij is the expected number of visits to the sentence j, if the random walker began at sentence i). We obtain the average of all possible starting sentences to get the expected number of visits to the jth sentence, vj . The sentence to be selected is the one that satisfies arg maxni=|G|+1 vi."
    }, {
      "heading" : "3. Datasets",
      "text" : "We use three datasets: newspaper articles (baseline data), films, and documentaries. Film data consists of subtitles and scripts, containing scene descriptions and dialog. Documentary data consists of subtitles containing mostly monologue. Reference data consists of manual abstracts (for newspaper articles), plot summaries (for films and documentaries), and synopses (for films). Plot summaries are concise descriptions, sufficient for the reader to get a sense of what happens in the film or documentary. Synopses are much longer and may contain important details concerning the turn of events in the story. All datasets were normalized by removing punctuation inside sentences and timestamps from subtitles."
    }, {
      "heading" : "3.1. Newspaper Articles",
      "text" : "TeMário [28] is composed by 100 newspaper articles in Brazilian Portuguese (Table 1), covering domains such as “world”, “politics”, and “foreign affairs”. Each article has a human-made reference summary (abstract)."
    }, {
      "heading" : "3.2. Films",
      "text" : "We collected 100 films, with an average of 4 plot summaries (minimum of 1, maximum of 7) and 1 plot synopsis per film\n3\n(Table 2). Table 3 presents the properties of the subtitles, scripts, and the concatenation of both. Not all the information present in the scripts was used: dialogs were removed in order to make them more similar to plot summaries."
    }, {
      "heading" : "3.3. Documentaries",
      "text" : "We collected 98 documentaries. Table 4 presents the properties of their subtitles: note that the number of sentences is smaller than in films, influencing ROUGE (recall-based) scores.\nWe collected 223 manual plot summaries and divided them into four classes (Table 5): 143 “Informative”, 63 “Interrogative”, 9 “Inviting”, and 8 “Challenge”. “Informative” summaries contain factual information about the program; “Interrogative” summaries contain questions that arouse viewer curiosity, e.g. “What is the meaning of life?”; “Inviting” are invitations, e.g. “Got time for a 24 year vacation?”; and, “Challenge” entice viewers on a personal basis, e.g. “are you ready for...?”. We chose “Informative” summaries due to their resemblance to the sentences extracted by the summarization algorithms. On average, there are 2 informative plot summaries per documentary (minimum of 1, maximum of 3)."
    }, {
      "heading" : "4. Experimental Setup",
      "text" : "For news articles, summaries were generated with the average size of the manual abstracts (≈ 31% of their size).\nFor each film, two summaries were generated, by selecting a number of sentences equal to (i) the average length of its manual plot summaries, and (ii) the length of its synopsis. In contrast with news articles and documentaries, three types of input were considered: script, subtitles, script+subtitles.\nFor each documentary, a summary was generated with the same average number of sentences of its manual plot summaries (≈ 1% of the documentary’s size).\nContent quality of summaries is based on word overlap (as defined by ROUGE) between generated summaries and their references. ROUGE-N computes the fraction of selected words that are correctly identified by the summarization algorithms (cf. Equation 5: RS are reference summaries, gramn is the ngram length, and countmatch(gramn) is the maximum number of n-grams of a candidate summary that co-occur with a set of reference summaries). ROUGE-SU measures the overlap of skip-bigrams (any pair of words in their sentence order, with the addition of unigrams as counting unit). ROUGE-SU4 limits the maximum gap length of skip-bigrams to 4.\nROUGE-N =\n∑ S∈RS ∑ gramn∈S\ncountmatch(gramn)∑ S∈RS ∑ gramn∈S count(gramn) (5)"
    }, {
      "heading" : "5. Results and Discussion",
      "text" : "Subtitles and scripts were evaluated against manual plot summaries and synopses to define an optimal performance reference. The following sections present averaged ROUGE-1, ROUGE-2, and ROUGE-SU4 scores (henceforth R-1, R-2, and R-SU4), and the performance of each summarization algorithm, as a ratio between the score of the generated summaries and this reference (relative performance). Several parametrizations of the algorithms were used (we present only the best results). Concerning MMR, we found that the best λ corresponds to a higher average number of words per summary. Concerning GRASSHOPPER, we used the uniform distribution as prior."
    }, {
      "heading" : "5.1. Newspaper Articles (TeMário)",
      "text" : "Table 6 presents the scores for each summarization algorithm. LSA achieved the best scores for R-1, R-2, and R-SU4. Figure 1 shows the relative performance results.\n4"
    }, {
      "heading" : "5.2. Films",
      "text" : "Table 7 presents the scores for the film data combinations against plot summaries. Overall, Support Sets, LSA, and LexRank, capture the most relevant sentences for plot summaries. It would be expected, for algorithms such as GRASSHOPPER and MMR, that maximize diversity, to perform well in this context, because plot summaries are relatively small and focus on the more important aspects of the film, ideally, without redundant content. However, our results show otherwise. For scripts, LSA and LexRank are the best approaches in terms of R-1 and R-SU4.\nTable 8 presents the scores for the film data combinations against plot synopses. The size of synopses is very different from that of plot summaries. Although synopses also focus on the major events of the story, their larger size allows for a more refined description of film events. Additionally, because summaries are created with the same number of sentences of the corresponding synopsis, higher scores are expected. From all algorithms, LexRank clearly stands out with the highest scores for all metrics (except for R-SU4, for scripts).\n5 The script+subtitles combination was used in order to determine whether the inclusion of redundant content would improve the scores, over the separate use of scripts or subtitles. However, in all cases (Figure 4), script+subtitles leads to worse scores, when compared to scripts alone. The same behavior is observed when using subtitles except for Support Sets-based methods (Support Sets and KP-Centrality). For plot synopses, the best scores are achieved by LexRank and GRASSHOPPER, while, for plot summaries, the best scores are achieved by LexRank and LSA. By inspection of the summaries produced by each algorithm, we observed that MMR chooses sentences with fewer words in comparison with all other algorithms (normally, leading to lower scores). Overall, the algorithms behave similarly for both subtitles and scripts."
    }, {
      "heading" : "5.3. Documentaries",
      "text" : "From all algorithms (Table 9), LSA achieved the best results for R-1 and R-SU4, along with LexRank for R-1. KP-Centrality achieved the best results for R-2. It is important to notice that LSA also produces the summaries with the highest word count (favoring recall). Figure 2 shows the relative performance results: LSA outperformed all other algorithms for R-1 and RSU4, and KP-Centrality was the best for R-2; Support Sets and KP-Centrality performed closely to LSA for R-SU4; the best MMR results were consistently worse across all metrics (MMR summaries have the lowest word count)."
    }, {
      "heading" : "5.4. Discussion",
      "text" : "News articles intend to answer basic questions about a particular event: who, what, when, where, why, and often, how. Their structure is sometimes referred to as “inverted pyramid”, where the most essential information comes first. Typically, the first sentences provide a good overview of the entire article and are more likely to be chosen when composing the final summary. Although documentaries follow a narrative structure similar to films, they can be seen as more closely related to news than films, especially regarding their intrinsic informative nature. In spite of their different natures, however, summaries created by humans produce similar scores for all of them. It is possible to observe this behavior in Figure 3. Note that documentaries achieve higher scores than news articles or films, when using the original subtitles documents against the corresponding manual plot summaries.\nFigure 4 presents an overview of the performance of each summarization algorithm across all domains. The results concerning news articles were the best out of all three datasets for all experiments. However, summaries for this dataset preserve, approximately, 31% of the original articles, in terms of sentences, which is significantly higher than for films and documentaries (which preserve less than 1%), necessarily leading to higher scores. Nonetheless, we can observe the differences in behavior between these domains. Notably, documentaries achieve the best results for plot summaries, in comparison with films, using scripts, subtitles, or the combination of both. The relative scores on the films dataset are influenced by two major aspects: the short sentences found in the films dialogs; and, since the generated summaries are extracts from subtitles and scripts, they are not able to represent the film as a whole, in contrast with what happens with plot summaries or synopses. Additionally, the experiments conducted for script+subtitles for films, in general, do not improve scores above those of scripts alone, except for Support Sets for R-1. Overall, LSA performed consistently better for news articles and documentaries. Similar relatively good behavior had already been observed for meeting\n6 recordings, where the best summarizer was also LSA [26]. One possible reason for these results is that LSA tries to capture the relation between words in sentences. By inferring contextual usage of text based on these relations, high scores, apart from R-1, are produced for R-2 and R-SU4. For films, LexRank was the best performing algorithm for subtitles, scripts and the combination of both, using plot synopses, followed by LSA and Support Sets for plot summaries. MMR has the lowest scores for all metrics and all datasets. We observed that sentences closer to the centroid typically contain very few words, thus leading to shorter summaries and the corresponding low scores.\nInterestingly, by observing the average of R-1, R-2, and RSU4, it is possible to notice that it follows very closely the values of R-SU4. These results suggest that R-SU4 adequately reflects the scores of both R-1 and R-2, capturing the concepts derived from both unigrams and bigrams.\nOverall, considering plot summaries, documentaries achieved higher results in comparison with films. However, in general, the highest score for these two domains is achieved using films scripts against plot synopses. Note that synopses have a significant difference in terms of sentences in comparison with plot summaries. The average synopsis has 120 sentences, while plot summaries have, on average, 5 sentences for films, and 4 for documentaries. This gives synopses a clear advantage in terms of ROUGE (recall-based) scores, due to the high count of words."
    }, {
      "heading" : "6. Conclusions and Future Work",
      "text" : "We analyzed the impact of the six summarization algorithms on three datasets. The newspaper articles dataset was used as a reference. The other two datasets, consisting of films and documentaries, were evaluated against plot summaries, for films and documentaries, and synopses, for films. Despite the different nature of these domains, the abstractive summaries created by humans, used for evaluation, share similar scores across metrics.\nThe best performing algorithms are LSA, for news and documentaries, and LexRank for films. Moreover, we conducted experiments combining scripts and subtitles for films, in order to assess the performance of generic algorithms by inclusion of redundant content. Our results suggest that this combination is unfavorable. Additionally, it is possible to observe that all algorithms behave similarly for both subtitles and scripts. As previously mentioned, the average of the scores follows closely the values of R-SU4, suggesting that R-SU4 is able to capture concepts derived from both unigrams and bigrams.\nWe plan to use subtitles as a starting point to perform video summaries of films and documentaries. For films, the results from our experiments using plot summaries show that the summarization of scripts only marginally improved performance, in comparison with subtitles. This suggests that subtitles are a viable approach for text-driven film and documentary summarization. This positive aspect is compounded by their being broadly available, as opposed to scripts."
    }, {
      "heading" : "7. Acknowledgements",
      "text" : "This work was supported by national funds through Fundação para a Ciência e a Tecnologia (FCT) with reference UID/CEC/50021/2013."
    } ],
    "references" : [ {
      "title" : "Video summarization: Techniques and classification, in: Computer Vision and Graphics",
      "author" : [ "M. Ajmal", "M. Ashraf", "M. Shakir", "Y. Abbas", "F. Shah" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Inferring strategies for sentence ordering in multidocument news summarization",
      "author" : [ "R. Barzilay", "N. Elhadad", "K. McKeown" ],
      "venue" : "Journal of Artificial Intelligence",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "The Anatomy of a Large-Scale Hypertextual Web Search Engine",
      "author" : [ "S. Brin", "L. Page" ],
      "venue" : "in: Proc. of the 7th Intl. Conf. on World Wide Web,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries",
      "author" : [ "J. Carbonell", "J. Goldstein" ],
      "venue" : "in: Proc. of the 21st Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1998
    }, {
      "title" : "New methods in automatic abstracting",
      "author" : [ "H.P. Edmundson" ],
      "venue" : "Journal of the Association for Computing Machinery",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1969
    }, {
      "title" : "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
      "author" : [ "G. Erkan", "D.R. Radev" ],
      "venue" : "Journal of Artificial Intelligence Research ,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Automatic extraction of cue phrases for important sentences in lecture speech and automatic lecture speech summarization",
      "author" : [ "Y. Fujii", "N. Kitaoka", "S. Nakagawa" ],
      "venue" : "in: Proc. of INTERSPEECH",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Cluster- Rank: A Graph Based Method for Meeting Summarization",
      "author" : [ "N. Garg", "B. Favre", "K. Reidhammer", "D. Hakkani-Tür" ],
      "venue" : "in: Proc. of INTERSPEECH",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis",
      "author" : [ "Y. Gong", "X. Liu" ],
      "venue" : "in: Proc. of the 24th Annual Intl. ACM SIGIR Conf. on Research and Development in Information Retrieval,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Documenting the Documentary: Close Readings of Documentary Film and Video",
      "author" : [ "B.K. Grant", "J. Sloniowski" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "A repository of state of the art and competitive baseline summaries for generic news summarization",
      "author" : [ "K. Hong", "J.M. Conroy", "B. Favre", "A. Kulesza", "H. Lin", "A. Nenkova" ],
      "venue" : "in: Proc. of the Ninth Intl. Conf. on Language Resources and Evaluation (LREC-2014),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Summarizing short stories",
      "author" : [ "A. Kazantseva", "S. Szpakowicz" ],
      "venue" : "Computational Linguistics",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
      "author" : [ "T.K. Landauer", "S.T. Dutnais" ],
      "venue" : "Psychological Review",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1997
    }, {
      "title" : "An introduction to latent semantic analysis",
      "author" : [ "T.K. Landauer", "P.W. Foltz", "D. Laham" ],
      "venue" : "Discourse processes",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "ROUGE: A Package for Automatic Evaluation of Summaries, in: Text Summ",
      "author" : [ "C.Y. Lin" ],
      "venue" : "Branches Out: Proc. of the ACL-04",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "The automated acquisition of topic signatures for text summarization",
      "author" : [ "C.Y. Lin", "E. Hovy" ],
      "venue" : "in: Proc. of the 18th Conf. on Computational Linguistics - Volume",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "Exploring correlation between rouge and human evaluation on meeting summaries",
      "author" : [ "F. Liu", "Y. Liu" ],
      "venue" : "IEEE Transactions on Audio, Speech & Language Processing",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "The Automatic Creation of Literature Abstracts",
      "author" : [ "H.P. Luhn" ],
      "venue" : "IBM Journal of Research and Development",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1958
    }, {
      "title" : "Supervised topical key phrase extraction of news stories using crowdsourcing, light filtering and co-reference normalization",
      "author" : [ "L. Marujo", "A. Gershman", "J. Carbonell", "R. Frederking", "J.P. Neto" ],
      "venue" : "Proceedings of the Eight International Conference on Language Resources and Evaluation",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Keyphrase cloud generation of broadcast news., in: INTERSPEECH, ISCA",
      "author" : [ "L. Marujo", "M. Viveiros", "J.P. Neto" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Comparing Lexical, Acoustic/Prosodic, Structural and Discourse Features for Speech Summarization",
      "author" : [ "S.R. Maskey", "J. Hirschberg" ],
      "venue" : "in: Proc. of the 9th EUROSPEECH - INTERSPEECH",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2005
    }, {
      "title" : "From text to speech summarization",
      "author" : [ "K. McKeown", "J. Hirschberg", "M. Galley", "S. Maskey" ],
      "venue" : "in: Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Tracking and summarizing news on a daily basis with columbia’s newsblaster",
      "author" : [ "K.R. McKeown", "R. Barzilay", "D. Evans", "V. Hatzivassiloglou", "J.L. Klavans", "A. Nenkova", "C. Sable", "B. Schiffman", "S. Sigelman", "M. Summarization" ],
      "venue" : "in: Proc. of HLT",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2002
    }, {
      "title" : "Explorations in automatic book summarization, in: EMNLP-CoNLL’07",
      "author" : [ "R. Mihalcea", "H. Ceylan" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    }, {
      "title" : "Extractive Summarization of Meeting Recordings",
      "author" : [ "G. Murray", "S. Renals", "J. Carletta" ],
      "venue" : "in: Proc. of the 9th European Conf. on Speech Communication and Technology,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2005
    }, {
      "title" : "Extractive Summarization of Meeting Records",
      "author" : [ "G. Murray", "S. Renals", "J. Carletta" ],
      "venue" : "in: Proc. of the 9th EUROSPEECH - INTERSPEECH",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2005
    }, {
      "title" : "Representing Reality: Issues and Concepts in Documentary",
      "author" : [ "B. Nichols" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1991
    }, {
      "title" : "NewsInEssence: A System For Domain-Independent, Real-Time News Clustering and Multi-Document Summarization",
      "author" : [ "D.R. Radev", "S. Blair-goldensohn", "Z. Zhang", "R.S. Raghavan" ],
      "venue" : "in: Proc. of the First Intl. Conf. on Human Language Technology Research,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2001
    }, {
      "title" : "NewsInEssence: Summarizing Online News Topics",
      "author" : [ "D.R. Radev", "J. Otterbacher", "A. Winkel", "S. Blair-Goldensohn" ],
      "venue" : "Communications of the ACM",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2005
    }, {
      "title" : "On the Application of Generic Summarization Algorithms to Music",
      "author" : [ "F. Raposo", "R. Ribeiro", "D.M. de Matos" ],
      "venue" : "IEEE Signal Processing",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Self reinforcement for important passage retrieval",
      "author" : [ "R. Ribeiro", "L. Marujo", "D. de Matos", "J.P. Neto", "A. Gershman", "J. Car-  bonell" ],
      "venue" : "Digital. URL:",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "Extractive summarization of broadcast news: Comparing strategies for european portuguese",
      "author" : [ "R. Ribeiro", "D. de Matos" ],
      "venue" : "in: TSD,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2007
    }, {
      "title" : "Summarizing speech by contextual reinforcement of important passages",
      "author" : [ "R. Ribeiro", "D. de Matos" ],
      "venue" : "in: Proc. of PROPOR",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2012
    }, {
      "title" : "Revisiting Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity",
      "author" : [ "R. Ribeiro", "D.M. de Matos" ],
      "venue" : "Journal of Artificial Intelligence",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2011
    }, {
      "title" : "Character-based movie summarization",
      "author" : [ "J. Sang", "C. Xu" ],
      "venue" : "in: Proc. of the Intl. Conf. on Multimedia,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2010
    }, {
      "title" : "Automatic summarising: The state of the art",
      "author" : [ "K. Spärck Jones" ],
      "venue" : "Inf. Process. Manage",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2007
    }, {
      "title" : "Using corpus and knowledge-based similarity measure in maximum marginal relevance for meeting summarization",
      "author" : [ "S. Xie", "Y. Liu" ],
      "venue" : "in: Proc. - ICASSP, IEEE Intl. Conf. on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2008
    }, {
      "title" : "Extractive Speech Summarization Using Shallow Rhetorical Structure Modeling",
      "author" : [ "J.J. Zhang", "R.H.Y. Chan", "P. Fung" ],
      "venue" : "IEEE Transactions on Audio Speech and Language Processing",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2010
    }, {
      "title" : "Improving Diversity in Ranking using Absorbing Random Walks",
      "author" : [ "X. Zhu", "A.B. Goldberg", "J.V. Gael", "D. Andrzejewski" ],
      "venue" : "in: Proc. of the 5th NAACL - HLT,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 61,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 61,
      "endOffset" : 68
    }, {
      "referenceID" : 20,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 37,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 32,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 28,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 31,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 10,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 25,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 222,
      "endOffset" : 229
    }, {
      "referenceID" : 7,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 222,
      "endOffset" : 229
    }, {
      "referenceID" : 6,
      "context" : "Input media for automatic summarization has varied from text [18, 5] to speech [21, 39, 34] and video [1], but the application domain has been, in general, restricted to informative sources: news [2, 30, 33, 11], meetings [26, 8], or lectures [7].",
      "startOffset" : 243,
      "endOffset" : 246
    }, {
      "referenceID" : 11,
      "context" : "summarization of literary short stories [12], music summarization [31], summarization of books [24], or inclusion of character analyses in movie summaries [36].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 29,
      "context" : "summarization of literary short stories [12], music summarization [31], summarization of books [24], or inclusion of character analyses in movie summaries [36].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 23,
      "context" : "summarization of literary short stories [12], music summarization [31], summarization of books [24], or inclusion of character analyses in movie summaries [36].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 34,
      "context" : "summarization of literary short stories [12], music summarization [31], summarization of books [24], or inclusion of character analyses in movie summaries [36].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 9,
      "context" : "Documentaries started as cinematic portrayals of reality [10].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : "However, films and documentaries do not fundamentally differ: many of the strategies and narrative structures employed in films are also used in documentaries [27].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 15,
      "context" : "pt (David Martins de Matos) have been extensively explored for news documents [16, 22, 37, 29, 30, 23].",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "pt (David Martins de Matos) have been extensively explored for news documents [16, 22, 37, 29, 30, 23].",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 35,
      "context" : "pt (David Martins de Matos) have been extensively explored for news documents [16, 22, 37, 29, 30, 23].",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 27,
      "context" : "pt (David Martins de Matos) have been extensively explored for news documents [16, 22, 37, 29, 30, 23].",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "pt (David Martins de Matos) have been extensively explored for news documents [16, 22, 37, 29, 30, 23].",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : "pt (David Martins de Matos) have been extensively explored for news documents [16, 22, 37, 29, 30, 23].",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "Generated summaries are evaluated against manual abstracts using ROUGE metrics, which correlate with human judgements [15, 17].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 16,
      "context" : "Generated summaries are evaluated against manual abstracts using ROUGE metrics, which correlate with human judgements [15, 17].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "MMR is a query-based summarization method [4].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "considering the input sentences centroid as a query [25, 38].",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 36,
      "context" : "considering the input sentences centroid as a query [25, 38].",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "LexRank [6] is a centrality-based method based on Google’s PageRank [3].",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 2,
      "context" : "LexRank [6] is a centrality-based method based on Google’s PageRank [3].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "LSA infers contextual usage of text based on word cooccurrence [13, 14].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "LSA infers contextual usage of text based on word cooccurrence [13, 14].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "Important topics are determined without the need for external lexical resources [9]: each word’s occurrence context provides information concerning its meaning, producing relations between words and sentences that correlate with the way humans make associations.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 33,
      "context" : "Support sets are defined based on this observation [35].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : "[32] proposed an extension of the centrality algorithm described in Section 2.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "Speech (POS) tags, and 4 n-gram domain model probabilities [20, 19].",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "Speech (POS) tags, and 4 n-gram domain model probabilities [20, 19].",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 38,
      "context" : "GRASSHOPPER [40] is a re-ranking algorithm that maximizes diversity and minimizes redundancy.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "It takes a weighted graph W (n × n: n vertexes representing sentences; weights are defined by a similarity measure), a probability distribution r (representing a prior ranking), and λ ∈ [0, 1], that balances the relative importance of W and r.",
      "startOffset" : 186,
      "endOffset" : 192
    }, {
      "referenceID" : 25,
      "context" : "recordings, where the best summarizer was also LSA [26].",
      "startOffset" : 51,
      "endOffset" : 55
    } ],
    "year" : 2016,
    "abstractText" : "We assess the performance of generic text summarization algorithms applied to films and documentaries, using extracts from news articles produced by reference models of extractive summarization. We use three datasets: (i) news articles, (ii) film scripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics are used for comparing generated summaries against news abstracts, plot summaries, and synopses. We show that the best performing algorithms are LSA, for news articles and documentaries, and LexRank and Support Sets, for films. Despite the different nature of films and documentaries, their relative behavior is in accordance with that obtained for news articles. c © 2016 Elsevier Ltd. All rights reserved.",
    "creator" : "LaTeX with hyperref package"
  }
}