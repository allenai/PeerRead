{
  "name" : "1511.07118.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cascading Denoising Auto-Encoder as a Deep Directed Generative Model",
    "authors" : [ "Dong-Hyun Lee" ],
    "emails" : [ "DONGHYUN.LEE.DL@GMAIL.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 1.\n07 11\n8v 1\n[ cs\n.L G\n] 2\n3 N\nov 2\nWe consider a directed model with an stochastic identity mapping (simple corruption process) as an inference model and a DAE as a generative model. By cascading these models, we propose Cascading Denoising AutoEncoders(CDAE) which can generate samples of data distribution from tractable prior distribution under the assumption that probabilistic distribution of corrupted data approaches tractable prior distribution as the level of corruption increases.\nThis work tries to answer two questions. On the one hand, can deep directed models be successfully trained without intractable posterior inference and difficult optimization of very deep neural networks in inference and generative models? These are unavoidable when recent successful directed model like VAE (Kingma & Welling, 2014) is trained on complex dataset like real images. On the other hand, can DAEs get clean samples of data distribution from heavily corrupted samples which can be considered of tractable prior distribution far from data manifold? so-called global denoising scheme.\nOur results show positive responses of these questions and this work can provide fairly simple framework for generative models of very complex dataset.\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s)."
    }, {
      "heading" : "1. Introduction",
      "text" : "Denoising Auto-Encoders were originally designed for feature learning algorithm or layer-wise pre-training of deep neural network (Vincent et al., 2010). Recent work (Bengio et al., 2013) has shown that DAEs can also be probabilistic generative models in the context of pure unsupervised learning. They interpret a DAE as a transition operator inducing a Markov chain whose stationary distribution is the data distribution.\nHowever, there are several drawbacks on the framework. At first, sampling process via MCMC chains always suffers from a mixing problem. Starting from 1 data sample, the chain walks through the data manifold, where the walk-step is made up of corruption of a sample and denoising the corrupted sample. If we can de-noise only slightly corrupted samples, that is, with local denoising scheme, the chain can walk only a small step per iteration. This causes a mixing problem in that it is hard to escape from a mode. We can easily guess that generated samples comes from only few modes near the initial sample in spite of good quality of the samples. And sometimes the chain falls in spurious modes if the corruption is too local. (Bengio et al., 2013) suggested the walkback training to avoid it.\nSecondly, lack of direct method to estimate test loglikelihood makes it difficult to compare DAEs to other generative models. Though parzen window estimator could be used for any type of generative models, test loglikelihood is still major measure for quality of generative models.\nRecently, directed generative models have been the focus of attention, as for example, Variational AutoEncoder (Kingma & Welling, 2014) which showed great success both as generative models and in the context of semi-supervised learning (Kingma et al., 2014). Directed generative models are appealing because of their clear probabilistic interpretation which allows ancestral sampling and direct methods to estimate test loglikelihood.\nSo we interpret DAEs as directed generative models with an stochastic identity mapping (corruption process) as an\ninference model and a DAE as a generative model in order to alleviate problems mentioned above under the assumption that distribution of corrupted data approaches tractable prior distribution as the level of corruption increases. But it is difficult for single DAE to generate clean samples of data distribution from tractable prior distribution like gaussian because ususally DAE can de-noise slightly corrupted samples near data manifold, local denoising scheme. For global denoising scheme, we should use cascading DAEs in which single DAE only need to make a transition to more probable distribution of data.\nOur model can provide much better method to generate samples from DAEs and a direct method to estimate test log-likelihood. Moreover, it can provide new ”exhaustive” training method for DAE and it is also helpful for representation learning.\nThis work tries to answer two questions. On the one hand, can deep directed models be successfully trained without intractable posterior inference and difficult optimization of very deep neural networks in inference and generative models? These are unavoidable when recent successful directed model like VAE (Kingma & Welling, 2014) is trained on complex dataset like real images. On the other hand, can DAEs get clean samples of data distribution from heavily corrupted samples which can be considered of tractable prior distribution far from data manifold? so-called global denoising scheme.\nOur results show positive responses of these questions and this work can provide fairly simple framework for generative models of very complex dataset."
    }, {
      "heading" : "2. Method",
      "text" : ""
    }, {
      "heading" : "2.1. Denoising Auto-Encoder as a Directed Generative Model",
      "text" : "In general, features obtained from encoder in AutoEncoder have no probabilistic interpretation without any sampling process on it to match prior distribution. Moreover, encoder and decoder are not distinguished in generalized version of DAE (Bengio et al., 2013). Because sampling process as a corruption is on the observed input variable, we can consider corrupted input x̃ as a latent variable z with an inference model of a stochastic identity mapping.\nWe have the inference model q(x, z) and the generative model p(x, z) as following,\nq(x, z) = q(z|x)q(x) = c(z = x̃|x)q(x) p(x, z) = pθ(x|z)p(z) (1)\nwhere q(x) is data distribution, p(z) is prior, c(z = x̃|x) is a corruption process, pθ(x|z) is parametric reconstruction mapping in DAE.\nTo maximize a variational lower bound on the loglikelihood,\nlog p(x) = log ∑\nz\nq(z|x) p(x, z)\nq(z|x) ≥\n∑\nz\nq(z|x) log p(x, z)\nq(z|x)\n= E z∼c(z=x̃|x) log pθ(x|z) −DKL(c(z = x̃|x)||p(z))\n= Lθ(x)\n(2)\nwhere z ∼ c(z = x̃|x) is a fixed corruption on the same space as the observed variable. Unlike typical variational training, we use a fixed stochastic encoder so that we don’t train inference model to approximate the posterior. Instead, we rely on the simple fact that the distribution of corrupted samples become similar to tractable prior distribution like gaussian as the corruption level increases. It can alleviate difficulty of training in that we only need to train reconstruction part, whereas it is challenging to de-noise heavily corrupted sample which is almost same as tractable distribution, so called global denoising scheme."
    }, {
      "heading" : "2.2. Cascading DAE with gradually increasing corruption",
      "text" : "Inspired by (Sohl-Dickstein et al., 2015), we introduce Cascading Denoising Auto-Encoders(CDAE). At the first step, we don’t need to recover clean data samples at once, but only have to make a transition from prior distribution to more probable distribution of data. Through several steps, we eventually can get clean data samples.\nAt first, we have corrupted samples with gradually increasing corruption level x(1), ... x(k) of data x(0).\nq(x(0), ..., z = x(k)) = q(x(0))\nk ∏\ni=1\nq(x(i)|x(0))\np(x(0), ..., z = x(k)) = p(x(k))\nk ∏\ni=1\npθ(x (i−1)|x(i))\n(3)\nwhere q(x(0)) is a data distribution and p(z = x(k)) is a prior distribution. To maximize a variational lower bound\non the log-likelihood,\nlog p(x(0)) ≥ ∑\nx (1...k)\nq(x(1...k)|x(0)) log p(x(0),x(1...k))\nq(x(1...k)|x(0))\n= E x (1...k) log\n[\np(x(k))p(x(0)|x(1))\nq(x(k)|x(0))\nk−1 ∏\ni=1\npθ(x (i)|x(i+1))\nq(x(i)|x(0))\n]\n= Lθ(x)\n= E x (1)\nlog pθ(x (0)|x(1))\n− k−1 ∑\ni=1\nE x (i+1)\nDKL(q(x (i)|x(0))||pθ(x (i)|x(i+1)))\n−DKL(q(x (k)|x(0))||p(z = x(k)))\n(4)\nwhere all expectations are x(i) ∼ q(x(i)|x(0)). Different from diffusion process in (Sohl-Dickstein et al., 2015), we use the same DAE in all pθ(x(i)|x(i+1))’s (that is, we use shared weights for all steps like Recurrent Neural Networks), and conditionally independent intermediate corruption distribution given the data sample. These enable us to consider this model as a series of DAE. As you shall see, this objective is merely the sum of denoising reconstruction errors with several corruption levels in the gaussian case."
    }, {
      "heading" : "2.3. Gaussian CDAE for continuous data",
      "text" : "For continuous data, isotropic gaussian is the simplest and typical choice as well as it can show how this framework is related with the typical training objective of DAE, mean squared reconstruction error with gaussian corruption.\npθ(x (i)|x(i+1)) = N (rθ(x (i+1)), σ(i)r 2 )\nq(x(i)|x(0)) = N (x(0), σ(i)c 2 )\n(5)\nThe negative variational lower bound is\n−Lθ(x) = k ∑\ni=1\nE x (i)\n1\n2σ (i−1) r\n2 ‖x (0) − rθ(x (i))‖2 + φ(σ(i)c , σ (i) r )\n+DKL(q(x (k)|x(0))||p(z = x(k)))\n(6)\nwhere the first term is merely denoising reconstruction error with several levels of corruption x(i) ∼ c(x(i)|x(0)), the second term φ(σ(i)c , σ (i) r ) is independent from θ so that it disappears the gradient of this objective, the third term is to match between heavily corrupted samples on the top DAE and the prior as a tractable distribution. We can train the parameters of prior (for example, the mean and variance of gaussian) with this term even before training DAE because we use a fixed identity encoder independent from\nthe parameters of DAE. So this is directly connected with typical DAE in that the terms related with the parameter of DAE θ are only reconstruction errors. Moreover, it makes training our model much easier under the assumption that corrupted samples become closer to tractable distribution as the level of corruption increases.\nMoreover, the terms of 6 related with θ can shrink even to single reconstruction error.\nLshrinkθ (x) = E x (c)∼Q(x(c)|x(0)) ‖x(0) − rθ(x (c))‖2 (7)\nwhere\nQ(x(c)|x(0)) = ∑\ni\n1\n2σ (i−1) r\n2 qi(x (c)|x(0)) (8)\nwhere qi(x(c)|x(0)) is the same distribution as q(x(i)|x(0)).\nBecause our DAE is probabilistic models with sampling after reconstruction, we need to determine σ(i)r ’s. these variables can be trained with θ, instead, we calculate them before every training epoch thanks to following analytic solution of ∂Lθ(x)/∂ ( σ (i) r −2) = 0\nσ(i)r 2 = σ(i)c 2 +\n1 d ‖x(0) − rθ(x (i+1))‖2 (9)\nwhere d is dimensionality of data and σ(i)c ’s are gradually increasing values as hyperparameters."
    }, {
      "heading" : "2.4. Estimating test loglikelihood",
      "text" : "Due to lack of a direct method to estimate test loglikelihood of generative DAE (Bengio et al., 2013), they use parzen window estimator. It is convenient and unique method for generative models in which the exact loglikelihood is not tractable. But the estimator has several drawbacks (....) Our framework provides the direct method to estimate test loglikelihood which is the most popular and historic one to measure quality of generative models.\nWe can calculate the variational lower bound of loglikelihood using equation 4, The more the number of step is, the larger the gap between the exact value and the bound is. The gap is\nDKL ( q(x(1..k)|x(0))||p(x(1..k)|x(0)) )\n(10)\nand this value can be larger as the number of steps k increases.\nIn RAISE (Burda et al., 2015) and AIS (Neal, 2001), they estimate the exact loglikelihood through importance weights. We also can do it as following,\nlog p(x (0) test) = log\n∑\nx (1...k)\nq(x(1...k)|x (0) test)\np(x (0) test,x (1...k))\nq(x(1...k)|x(0))\n= log E x (1...k)\n[\np(z = x(k))\nk−1 ∏\ni=0\npθ(x (i)|x(i+1)) q(x(i+1)|x (0) test)\n]\n(11)"
    }, {
      "heading" : "Acknowledgments",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Generalized denoising auto-encoders as generative models",
      "author" : [ "Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal" ],
      "venue" : "In NIPS’2013,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Accurate and conservative estimates of mrf loglikelihood using reverse annealing",
      "author" : [ "Burda", "Yuri", "Grosse", "Roger B", "Salakhutdinov", "Ruslan" ],
      "venue" : "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "Burda et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Burda et al\\.",
      "year" : 2015
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling" ],
      "venue" : "In NIPS’2014,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Kingma", "Durk P", "Welling", "Max" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Annealed importance sampling",
      "author" : [ "Neal", "Radford M" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "Neal and M.,? \\Q2001\\E",
      "shortCiteRegEx" : "Neal and M.",
      "year" : 2001
    }, {
      "title" : "Deep unsupervised learning using nonequilibrium thermodynamics",
      "author" : [ "Sohl-Dickstein", "Jascha", "Weiss", "Eric A", "Maheswaranathan", "Niru", "Ganguli", "Surya" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning",
      "citeRegEx" : "Sohl.Dickstein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sohl.Dickstein et al\\.",
      "year" : 2015
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "Vincent", "Pascal", "Larochelle", "Hugo", "Lajoie", "Isabelle", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine" ],
      "venue" : "J. Machine Learning Res.,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Abstract Recent work (Bengio et al., 2013) has shown how Denoising Auto-Encoders(DAE) become generative models as a density estimator.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "Introduction Denoising Auto-Encoders were originally designed for feature learning algorithm or layer-wise pre-training of deep neural network (Vincent et al., 2010).",
      "startOffset" : 143,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "Recent work (Bengio et al., 2013) has shown that DAEs can also be probabilistic generative models in the context of pure unsupervised learning.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "(Bengio et al., 2013) suggested the walkback training to avoid it.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "Recently, directed generative models have been the focus of attention, as for example, Variational AutoEncoder (Kingma & Welling, 2014) which showed great success both as generative models and in the context of semi-supervised learning (Kingma et al., 2014).",
      "startOffset" : 236,
      "endOffset" : 257
    }, {
      "referenceID" : 0,
      "context" : "Moreover, encoder and decoder are not distinguished in generalized version of DAE (Bengio et al., 2013).",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Cascading DAE with gradually increasing corruption Inspired by (Sohl-Dickstein et al., 2015), we introduce Cascading Denoising Auto-Encoders(CDAE).",
      "startOffset" : 63,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Different from diffusion process in (Sohl-Dickstein et al., 2015), we use the same DAE in all pθ(x|x)’s (that is, we use shared weights for all steps like Recurrent Neural Networks), and conditionally independent intermediate corruption distribution given the data sample.",
      "startOffset" : 36,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Estimating test loglikelihood Due to lack of a direct method to estimate test loglikelihood of generative DAE (Bengio et al., 2013), they use parzen window estimator.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "In RAISE (Burda et al., 2015) and AIS (Neal, 2001), they estimate the exact loglikelihood through importance weights.",
      "startOffset" : 9,
      "endOffset" : 29
    } ],
    "year" : 2017,
    "abstractText" : "Recent work (Bengio et al., 2013) has shown how Denoising Auto-Encoders(DAE) become generative models as a density estimator. However, in practice, the framework suffers from a mixing problem in the MCMC sampling process and no direct method to estimate the test loglikelihood. We consider a directed model with an stochastic identity mapping (simple corruption process) as an inference model and a DAE as a generative model. By cascading these models, we propose Cascading Denoising AutoEncoders(CDAE) which can generate samples of data distribution from tractable prior distribution under the assumption that probabilistic distribution of corrupted data approaches tractable prior distribution as the level of corruption increases. This work tries to answer two questions. On the one hand, can deep directed models be successfully trained without intractable posterior inference and difficult optimization of very deep neural networks in inference and generative models? These are unavoidable when recent successful directed model like VAE (Kingma & Welling, 2014) is trained on complex dataset like real images. On the other hand, can DAEs get clean samples of data distribution from heavily corrupted samples which can be considered of tractable prior distribution far from data manifold? so-called global denoising scheme. Our results show positive responses of these questions and this work can provide fairly simple framework for generative models of very complex dataset. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).",
    "creator" : "LaTeX with hyperref package"
  }
}