{
  "name" : "1703.02239.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Functions that Emerge through End-to-endReinforcement Learning",
    "authors" : [ "Katsunari Shibata" ],
    "emails" : [ "shibata@oita-u.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n02 23\n9v 1\n[ cs\n.A I]\n7 M\nar 2\n01 7\n“Function Modularization” approach is deeply penetrated subconsciously. The inputs and outputs for a learning system can be raw sensor signals and motor commands. “State space” or “action space” generally used in RL show the existence of functional modules. That has limited reinforcement learning to learning only for the action-planning module. In order to extend reinforcement learning to learning of the entire function on a huge degree of freedom of a massively parallel learning system and to explain or develop human-like intelligence, the author has believed that end-to-end RL from sensors to motors using a recurrent NN (RNN) becomes an essential key. Especially in the higher functions, since their inputs or outputs are difficult to decide, this approach is very effective by being free from the need to decide them.\nThe functions that emerge, we have confirmed, through RL using a NN cover a broad range from real robot learning with raw camera pixel inputs to acquisition of dynamic functions in a RNN. Those are (1)image recognition, (2)color constancy (optical illusion), (3)sensor motion (active recognition), (4)hand-eye coordination and hand reaching movement, (5)explanation of brain activities, (6)communication, (7)knowledge transfer, (8)memory, (9)selective attention, (10)prediction, (11)exploration. The end-to-end RL enables the emergence of very flexible comprehensive functions that consider many things in parallel although it is difficult to give the boundary of each function clearly.\nKeywords: function emergence, end-to-end reinforcement learning (RL), recurrent neural network (RNN), higher functions, artificial general intelligence (AGI)"
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research has been supported by JSPS KAKENHI Grant Numbers JP07780305, JP08233204, JP13780295, JP15300064, JP19300070 and many our group members\n∗http://shws.cc.oita-u.ac.jp/˜shibata/home.html"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions. One remarkable point especially in the results in TV-games is the gap such that even though the inputs of a deep NN are raw image pixels without any pre-processing and the NN is just learned through RL, the ability acquired through learning extends to the excellent strategies for several games. The learning do not need special knowledge about learned tasks and necessary functions emerge through learning, and so it is expected to open the way to the AGI (Artificial General Intelligence) or Strong AI.\nIt has been general that a NN is considered as just a non-linear function approximator for RL, and a recurrent neural network (RNN) is used to avoid POMDP (Partially Observable Markov Decision Problem). Under such circumstances, the origin of the end-to-end RL can be found in the Tesauro’s work called TD-gammon[4]. The author’s group is the only one who has propounded this framework for around 20 years using the symbolic name of “Direct-Vision-based Reinforcement Learning”[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.\nIn this paper, the author’s unwavering direction that end-to-end RL becomes an important key for explaining human intelligence or developing human-like intelligence especially for the higher functions is introduced at first. It is also shown that a variety of functions emerge through end-to-end (oriented) RL; from real robot learning with raw camera pixel inputs to acquisition of dynamic functions in an RNN. All of the works here have been published already, but the author believes that it is worthwhile to know what functions emerge and what functions hardly emerge at this timing when the end-to-end RL begins to be focused on."
    }, {
      "heading" : "2 The Direction for Human-like Intelligence",
      "text" : "There is probably little doubt that human intelligence is realized thanks to the massively parallel and cohesively flexible processing on a huge degree of freedom in our brain. On the other hand, unlike in the case of unconsciousness, our consciousness looks linguistic and so it is not parallel but sequential. Therefore, it is impossible to completely understand the brain functions through our consciousness. Nevertheless, the researchers have tried to understand the brain or develop human-like intelligence by hands. We are likely to divide a difficult problem into sub-problems expecting each divided one is easier to solve. Then “Function Modularization” approach has been deeply penetrated subconsciously. However, for each module, we have to decide what are the inputs and outputs at first. It is easily known that to decide the information that comes and goes between divided modules, it is necessary to understand the entire function. Then to decide the information, some simple frame is set in advance and that causes the “Frame Problem”. It can be also thought that the division into “symbolic (logical) processing” and “pattern processing” causes the “Symbol Grounding Problem”. In our brain, they may be processed in different areas, but they are both processed in the same brain as one NN.\n“State space” or “action space”, which is generally used in RL, can be another aspect of the function modularization. Researchers have limited the learning only to the actions that make the mapping from state space to action space, and have tried to develop the way of construction of state space from sensor signals and given the way of generating motor commands from each action separately from RL. It has also been taken for granted that classification categories are given in recognition function and reference trajectories are given in control function. However, we can recognize complicated situations from a picture, but it is clear that all of them cannot be given as classification targets in advance. There is no evidence that reference trajectory is explicitly generated in our brain. From the viewpoint of understanding and developing human-like intelligence, they are by-products of the function modularization. They give unnecessary constraints on a huge degree of freedom, and disturb the flexible and comprehensive learning despite the intension of the researchers.\nBased on the above, the author has thought that the interruption of human designers should be cut off and the development of functions should be committed to learning of high-dimensional parallel systems as much as possible. The inputs for a learning system can be raw sensor signals, the outputs can be raw actuator signals, and the entire process from sensors to actuators (motors) should be learned without dividing into functional modules. A NN has an ability to optimize the parallel process according to some value or cost function through learning. If training signals are given by humans, they can be constraints for learning. However, RL has an ability to learn autonomously without receiving training signals directly based on trials and errors using a motion-perception feedback loop with the environment.\nFor the higher functions, which are different from the recognition or control, it is difficult to decide either inputs or outputs, and that has disturbed the progress of the research on them. The author is expecting that higher functions also emerge through comprehensive end-to-end RL from sensors to motors using a recurrent NN. In image recognition, the use of deep learning makes the performance better than the conventional approaches that need to design a feature space. In speech recognition, to leave the learning to an RNN makes the performance better than the combination with conventional methods like HMM. They seem to support the author’s direction.\nThe author has thought that what emerges should be called “functions” rather than “representation” because it is not just a choice of representation, but the result of the processing to get there. Furthermore to be more accurate, the function that emerges cannot be localized clearly in the NN, but it is just a label for us to understand through our consciousness."
    }, {
      "heading" : "3 Functions that Emerge through Reinforcement Learning (RL)",
      "text" : "Here, functions that have been observed to emerge in a NN through RL in our works are introduced. The NN is trained using the training signals produced automatically from RL; Q-learning, actor-critic or Actor-Q (for learning of both continuous motion and discrete action) based on TD learning. By using actor outputs in actor-critic or actor-Q directly as motion commands, continuous motions have been learned. The convolutional structure is not used except for [20]. The details can be seen in each reference.\n3.1 Static Functions that Emerge in a Layered Neural Network (NN)\nA layered neural network is used and trained according to error backpropagation (BP), and static functions emerge as follows.\n§ Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6]. That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig. 1 and Fig. 2.\n§ Color Constancy (Optical Illusion) Motion control of a colored object to the goal location decided by the object color was learned. The top view image covered by a randomlyappearing colored filter was the input. From the internal representation, we tried to explain the optical illusion of color constancy[11]. § Sensor Motion (Active Recognition) A pattern image was the input, and from the reward that indicates whether the recognition result is correct or not, both recognition and camera motion for better recognition were acquired through RL[12]. § Hand-Eye Coordination and Hand ReachingMovement\nA NN whose inputs were joint angles of an arm and also image on which its hand can be seen, and whose outputs were the joint torques learned to reach the hand to the randomly-located target that could be also seen on the image. No explicit reference trajectory was given. Furthermore, adaptation of force field and its after effect were observed[13]. § Explanation of the Brain Activations during Tool Use From the internal representation after learning of a reaching task with variable link length, we tried to explain the emergence of the activities observed in the monkey brain when the monkey used a tool to get a food[14]. § Knowledge Transfer between Different Sensors An agent has two kinds of sensors and two kinds of motors. There are four sensor-and-motor combinations. There is a task that could be achieved using either of the combinations. After the agent learned the task using 3 combinations, learning for the remainder sensor-and-motor combination was drastically accelerated[15]. § Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])"
    }, {
      "heading" : "3.2 Dynamic Functions that Emerge in a Recurrent Neural Network (RNN)",
      "text" : "Here, the emergence of dynamic functions is introduced in which expansion along the time axis is required. In this case, a RNN is used to deal with dynamics, and is trained by BPTT(Back Propagation Through Time) with the training signals generated automatically based on RL. For both acquisition of memory and error propagation, the feedback connection weights are set so that the transition matrix is the identity matrix or close to it when being linearly approximated. § Memory There are someworks in which necessary informationwas extracted,memorized and reflected to behaviors after learning almost only from the reward or punishment at each goal. In [16], a very interesting behavior in which if unexpected results occurred, an agent went back to check the state in the previous stage without any direction could be observed. In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned. Purposive associative memory could be also observed[17]. § Selective Attention It was learned that based on the previously-presented image, the attended area of the next presented pattern is changed without any special structure for attention and the area of the image was correctly classified[19]. Here, TD-based RL was not used, but learning was done by the reinforcement signal representing only whether the final answer was correct or not. Purposive associative memory could be also observed. § Prediction (explained in the next subsection[20]) § Explanation of the Emergence of Reward Expectancy Neurons From a simulation result of RL using an RNN, we tried to explain the emergence of reward expectancy neurons, which responded only in the non-reward trials in a multi-trial task observed in the monkey brain[21]. § Exploration Effective and deterministic exploration behavior for ambiguous or invisible goal considering past experiencewas learned and temporal abstraction was discussed[22, 23]. § Communication (introduced in another paper[24])\nAlthough each function has been examined in a very simple task, it is known that a variety of functions emerge based on extraction and memory of necessary information using an RNN. However, the emergence of “thinking” or “symbol processing” that needs multi-step state transition was very difficult."
    }, {
      "heading" : "3.3 Frame-free Function Emergence for AGI (Artificial General Intelligence)",
      "text" : "As mentioned before, through end-to-end RL from sensors to motors, entire process is learned and comprehensive function is acquired. For example, in the learning of [20], an agent who has a visual sensor and an RNN as shown in Fig. 3 learned both motion and capture timing of a moving object that sometimes becomes invisible randomly.\nIn a general approach, the object motion is estimated from some frames of image using some givenmodel, the future object location is predicted, the capture point and time are decided by some optimizationmethod, a reference trajectory is derived from the capture point, and the motions are controlled to follow the trajectory.\nFig. 4 shows four examples after RL based on the reward given for object capture. The agent did not know in advance the way to predict the motion or even the fact that prediction is necessary to catch it. Nevertheless, it moved to the very front of its range of motion, waited the object, and when the object came to close, the agent moved backward with it and caught it. Though the object became invisible or visible again suddenly, the agent could behave appropriately. Since the moving direction of the object changed sometimes when it was invisible during learning, the agent learned to wait close to the center (y = 1.5) where it can react the unexpected object motion. As shown in case 4, when the object changed its direction unexpectedly, the agent could catch it though the timing is a bit later than the case of expected motion (case 3)."
    } ],
    "references" : [ {
      "title" : "Playing Atari with Deep Reinforcement Learning,NIPS",
      "author" : [ "V. Minh", "K Kavukcuoglu" ],
      "venue" : "Deep Learning Workshop",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Minh", "K. Kavukcuoglu", "D Silver" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search,Nature",
      "author" : [ "D. Silver", "A Huang" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Practical Issues in Temporal Difference",
      "author" : [ "G. Tesauro" ],
      "venue" : "Learning,Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1992
    }, {
      "title" : "Reinforcement Learning When Visual Sensory Signals are Directly Given as Inputs",
      "author" : [ "K. Shibata", "Y. Okabe" ],
      "venue" : "Proc. of ICNN(Int’l Conf. on Neural Networks)97,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1997
    }, {
      "title" : "Direct-Vision-Based Reinforcement Learning in ”Going to an Target” Task with an Obstacle and with a Variety of Target Sizes",
      "author" : [ "K. Shibata", "Y. Okabe", "K. Ito" ],
      "venue" : "Proc. of NEURAP(Neural Networks and their Applications)’98,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Emergence of Intelligence through Reinforcement Learning with a Neural Network, Advances in Reinforcement Learning, Intech, 99–120",
      "author" : [ "K. Shibata" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Acquisition of Box Pushing by Direct-Vision-Based Reinforcement Learning",
      "author" : [ "K. Shibata", "M. Iida" ],
      "venue" : "Proc. of SICE Annual Conf",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Learning of Action Generation from Raw Camera Images in a Real-World-like Environment by Simple Coupling of Reinforcement Learning and a Neural Network",
      "author" : [ "K. Shibata", "T. Kawano" ],
      "venue" : "Adv. in Neuro-Information Processing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Acquisition of Flexible Image Recognition by Coupling of Reinforcement Learning and a Neural Network",
      "author" : [ "K. Shibata", "T. Kawano" ],
      "venue" : "SICE J. of Control, Measurement, and System Integration,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Emergence of Color Constancy Illusion through Reinforcement Learning with a Neural Network",
      "author" : [ "K. Shibata", "S. Kurizaki" ],
      "venue" : "Proc. of ICDL-EpiRob(Int’l Conf. on Developmental Learning & Epigenetic Robotics)2012,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Effect of Force Load in Hand Reaching Movement Acquired by Reinforcement Learning",
      "author" : [ "K. Shibata", "K. Ito" ],
      "venue" : "Proc. of ICONIP(Int’l Conf. on Neural Information",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Hidden Representation after Reinforcement Learning of Hand Reaching Movement with Variable Link Length",
      "author" : [ "K. Shibata", "K. Ito" ],
      "venue" : "Proc. of IJCNN (Int’l Joint Conf. on Neural Networks)",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2003
    }, {
      "title" : "Spatial Abstraction and Knowledge Transfer in Reinforcement Learning Using a Multi-Layer Neural Network",
      "author" : [ "K. Shibata" ],
      "venue" : "Proc. of ICDL (5th Int’l Conf. on Development and Learning)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Contextual Behavior and Internal Representations Acquired by Reinforcement Learning with a Recurrent Neural Network",
      "author" : [ "H. Utsunomiya", "K. Shibata" ],
      "venue" : "Adv. in Neuro-Information Processing, Lecture Notes in Comp. Sci., Proc. of ICONIP",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Discovery of Pattern Meaning from Delayed Rewards by Reinforcement Learning with a Recurrent Neural Network",
      "author" : [ "K. Shibata", "H. Utsunomiya" ],
      "venue" : "Proc. of IJCNN (Int’l Joint Conf. on Neural Networks)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Acquisition of Context-Based Active Word Recognition by Q-Learning Using a Recurrent Neural Network, Robot",
      "author" : [ "A.A.M. Faudzi", "K. Shibata" ],
      "venue" : "Intelligent Technology and Applications,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "2004)Dynamics of a RecurrentNeural NetworkAcquired throughLearning of a Context-basedAttention Task",
      "author" : [ "K. Shibata", "M. Sugisaka" ],
      "venue" : "Artificial Life and Robotics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "Emergence of Flexible Prediction-Based Discrete Decision Making and Continuous Motion Generation through Actor-Q-Learning",
      "author" : [ "K. Shibata", "K. Goto" ],
      "venue" : "Proc. of ICDL-Epirob (Int’l Conf. on Developmental Learning & Epigenetic Robotics)",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "A model to explain the emergence of reward expectancy neurons using reinforcement learning and neural network,Neurocomputing",
      "author" : [ "Ishii S", "M. Shidara", "K. Shibata" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Learning of Deterministic Exploration and Temporal Abstraction in Reinforcement Learning",
      "author" : [ "K. Shibata" ],
      "venue" : "Proc. of SICE-ICCAS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2006
    }, {
      "title" : "Acquisition of Deterministic Exploration and Purposive Memory through Reinforcement Learning with a Recurrent Neural Network",
      "author" : [ "K. Goto", "K. Shibata" ],
      "venue" : "Proc. of SICE Annual Conf. 2010,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Communications that Emerge through Reinforcement Learning Using a Neural Network, RLDM2017",
      "author" : [ "K. Shibata" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions.",
      "startOffset" : 57,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions.",
      "startOffset" : 57,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "Under such circumstances, the origin of the end-to-end RL can be found in the Tesauro’s work called TD-gammon[4].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "The author’s group is the only one who has propounded this framework for around 20 years using the symbolic name of “Direct-Vision-based Reinforcement Learning”[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.",
      "startOffset" : 160,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "The author’s group is the only one who has propounded this framework for around 20 years using the symbolic name of “Direct-Vision-based Reinforcement Learning”[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.",
      "startOffset" : 160,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "The author’s group is the only one who has propounded this framework for around 20 years using the symbolic name of “Direct-Vision-based Reinforcement Learning”[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.",
      "startOffset" : 249,
      "endOffset" : 252
    }, {
      "referenceID" : 18,
      "context" : "The convolutional structure is not used except for [20].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "(2003)[8] A layered neural network is used and trained according to error backpropagation (BP), and static functions emerge as follows.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "§ Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "§ Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "§ Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "§ Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig.",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig.",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 10,
      "context" : "From the internal representation, we tried to explain the optical illusion of color constancy[11].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, adaptation of force field and its after effect were observed[13].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "§ Explanation of the Brain Activations during Tool Use From the internal representation after learning of a reaching task with variable link length, we tried to explain the emergence of the activities observed in the monkey brain when the monkey used a tool to get a food[14].",
      "startOffset" : 271,
      "endOffset" : 275
    }, {
      "referenceID" : 13,
      "context" : "After the agent learned the task using 3 combinations, learning for the remainder sensor-and-motor combination was drastically accelerated[15].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "§ Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "§ Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "§ Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "§ Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "(2008)[9]",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 14,
      "context" : "In [16], a very interesting behavior in which if unexpected results occurred, an agent went back to check the state in the previous stage without any direction could be observed.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 15,
      "context" : "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 16,
      "context" : "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "Purposive associative memory could be also observed[17].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "§ Selective Attention It was learned that based on the previously-presented image, the attended area of the next presented pattern is changed without any special structure for attention and the area of the image was correctly classified[19].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 18,
      "context" : "§ Prediction (explained in the next subsection[20]) § Explanation of the Emergence of Reward Expectancy Neurons From a simulation result of RL using an RNN, we tried to explain the emergence of reward expectancy neurons, which responded only in the non-reward trials in a multi-trial task observed in the monkey brain[21].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "§ Prediction (explained in the next subsection[20]) § Explanation of the Emergence of Reward Expectancy Neurons From a simulation result of RL using an RNN, we tried to explain the emergence of reward expectancy neurons, which responded only in the non-reward trials in a multi-trial task observed in the monkey brain[21].",
      "startOffset" : 317,
      "endOffset" : 321
    }, {
      "referenceID" : 20,
      "context" : "§ Exploration Effective and deterministic exploration behavior for ambiguous or invisible goal considering past experiencewas learned and temporal abstraction was discussed[22, 23].",
      "startOffset" : 172,
      "endOffset" : 180
    }, {
      "referenceID" : 21,
      "context" : "§ Exploration Effective and deterministic exploration behavior for ambiguous or invisible goal considering past experiencewas learned and temporal abstraction was discussed[22, 23].",
      "startOffset" : 172,
      "endOffset" : 180
    }, {
      "referenceID" : 22,
      "context" : "§ Communication (introduced in another paper[24]) Although each function has been examined in a very simple task, it is known that a variety of functions emerge based on extraction and memory of necessary information using an RNN.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "For example, in the learning of [20], an agent who has a visual sensor and an RNN as shown in Fig.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 18,
      "context" : "(2013)[20]",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 18,
      "context" : "(2013)[20] In a general approach, the object motion is estimated from some frames of image using some givenmodel, the future object location is predicted, the capture point and time are decided by some optimizationmethod, a reference trajectory is derived from the capture point, and the motions are controlled to follow the trajectory.",
      "startOffset" : 6,
      "endOffset" : 10
    } ],
    "year" : 2017,
    "abstractText" : "Recently, triggered by the impressive results in TV-games or game of Go by Google DeepMind, end-to-end reinforcement learning (RL) is collecting attentions. Although little is known, the author’s group has propounded this framework for around 20 years and already has shown a variety of functions that emerge in a neural network (NN) through RL. In this paper, they are introduced again at this timing. “Function Modularization” approach is deeply penetrated subconsciously. The inputs and outputs for a learning system can be raw sensor signals and motor commands. “State space” or “action space” generally used in RL show the existence of functional modules. That has limited reinforcement learning to learning only for the action-planning module. In order to extend reinforcement learning to learning of the entire function on a huge degree of freedom of a massively parallel learning system and to explain or develop human-like intelligence, the author has believed that end-to-end RL from sensors to motors using a recurrent NN (RNN) becomes an essential key. Especially in the higher functions, since their inputs or outputs are difficult to decide, this approach is very effective by being free from the need to decide them. The functions that emerge, we have confirmed, through RL using a NN cover a broad range from real robot learning with raw camera pixel inputs to acquisition of dynamic functions in a RNN. Those are (1)image recognition, (2)color constancy (optical illusion), (3)sensor motion (active recognition), (4)hand-eye coordination and hand reaching movement, (5)explanation of brain activities, (6)communication, (7)knowledge transfer, (8)memory, (9)selective attention, (10)prediction, (11)exploration. The end-to-end RL enables the emergence of very flexible comprehensive functions that consider many things in parallel although it is difficult to give the boundary of each function clearly.",
    "creator" : "LaTeX with hyperref package"
  }
}